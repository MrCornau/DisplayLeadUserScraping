{"interestingcomments": [{"Unnamed: 0": 73, "autor": 73, "date": null, "content": "Eclipse Paho Java Client\nThe Paho Java Client is an MQTT client library written in Java for developing applications that run on the JVM or other Java compatible platforms such as Android\nThe Paho Java Client provides two APIs: MqttAsyncClient provides a fully asynchronous API where completion of activities is notified via registered callbacks. MqttClient is a synchronous wrapper around MqttAsyncClient where functions appear synchronous to the application.\nProject description:\nThe Paho project has been created to provide reliable open-source implementations of open and standard messaging protocols aimed at new, existing, and emerging applications for Machine-to-Machine (M2M) and Internet of Things (IoT). Paho reflects the inherent physical and cost constraints of device connectivity. Its objectives include effective levels of decoupling between devices and applications, designed to keep markets open and encourage the rapid growth of scalable Web and Enterprise middleware and applications.\nLinks\nProject Website: https://www.eclipse.org/paho\nEclipse Project Information: https://projects.eclipse.org/projects/iot.paho\nPaho Java Client Page: https://eclipse.org/paho/clients/java/\nGitHub: https://github.com/eclipse/paho.mqtt.java\nTwitter: @eclipsepaho\nIssues: https://github.com/eclipse/paho.mqtt.java/issues\nMailing-list: https://dev.eclipse.org/mailman/listinfo/paho-dev\nUsing the Paho Java Client\nDownloading\nEclipse hosts a Nexus repository for those who want to use Maven to manage their dependencies. The released libraries are also available in the Maven Central repository.\nAdd the repository definition and the dependency definition shown below to your pom.xml.\nReplace %REPOURL% with either https://repo.eclipse.org/content/repositories/paho-releases/ for the official releases, or https://repo.eclipse.org/content/repositories/paho-snapshots/ for the nightly snapshots. Replace %VERSION% with the level required .\nThe latest release version is 1.2.5 and the current snapshot version is 1.2.6-SNAPSHOT.\n** Dependency definition for MQTTv3 client **\n<project ...>\n<repositories>\n<repository>\n<id>Eclipse Paho Repo</id>\n<url>%REPOURL%</url>\n</repository>\n</repositories>\n...\n<dependencies>\n<dependency>\n<groupId>org.eclipse.paho</groupId>\n<artifactId>org.eclipse.paho.client.mqttv3</artifactId>\n<version>%VERSION%</version>\n</dependency>\n</dependencies>\n</project>\n** Dependency definition for MQTTv5 client **\n<project ...>\n<repositories>\n<repository>\n<id>Eclipse Paho Repo</id>\n<url>%REPOURL%</url>\n</repository>\n</repositories>\n...\n<dependencies>\n<dependency>\n<groupId>org.eclipse.paho</groupId>\n<artifactId>org.eclipse.paho.mqttv5.client</artifactId>\n<version>%VERSION%</version>\n</dependency>\n</dependencies>\n</project>\nIf you find that there is functionality missing or bugs in the release version, you may want to try using the snapshot version to see if this helps before raising a feature request or an issue.\nBuilding from source\nThere are two active branches on the Paho Java git repository, master which is used to produce stable releases, and develop where active development is carried out. By default cloning the git repository will download the master branch, to build from develop make sure you switch to the remote branch: git checkout -b develop remotes/origin/develop\nTo then build the library run the following maven command: mvn package -DskipTests\nThis will build the client library without running the tests. The jars for the library, source and javadoc can be found in the following directories:\norg.eclipse.paho.client.mqttv3/target\norg.eclipse.paho.mqttv5.client/target\nDocumentation\nMQTTv3 reference documentation is online at: http://www.eclipse.org/paho/files/javadoc/index.html\nLog and Debug in the Java Client: https://wiki.eclipse.org/Paho/Log_and_Debug_in_the_Java_client\nGetting Started\nThe included code below is a very basic sample that connects to a server and publishes a message using the MQTTv3 synchronous API. More extensive samples demonstrating the use of the MQTTv3 and MQTTv5 Asynchronous API can be found in the org.eclipse.paho.sample directory of the source.\nimport org.eclipse.paho.client.mqttv3.MqttClient;\nimport org.eclipse.paho.client.mqttv3.MqttConnectOptions;\nimport org.eclipse.paho.client.mqttv3.MqttException;\nimport org.eclipse.paho.client.mqttv3.MqttMessage;\nimport org.eclipse.paho.client.mqttv3.persist.MemoryPersistence;\npublic class MqttPublishSample {\npublic static void main(String[] args) {\nString topic = \"MQTT Examples\";\nString content = \"Message from MqttPublishSample\";\nint qos = 2;\nString broker = \"tcp://iot.eclipse.org:1883\";\nString clientId = \"JavaSample\";\nMemoryPersistence persistence = new MemoryPersistence();\ntry {\nMqttClient sampleClient = new MqttClient(broker, clientId, persistence);\nMqttConnectOptions connOpts = new MqttConnectOptions();\nconnOpts.setCleanSession(true);\nSystem.out.println(\"Connecting to broker: \"+broker);\nsampleClient.connect(connOpts);\nSystem.out.println(\"Connected\");\nSystem.out.println(\"Publishing message: \"+content);\nMqttMessage message = new MqttMessage(content.getBytes());\nmessage.setQos(qos);\nsampleClient.publish(topic, message);\nSystem.out.println(\"Message published\");\nsampleClient.disconnect();\nSystem.out.println(\"Disconnected\");\nSystem.exit(0);\n} catch(MqttException me) {\nSystem.out.println(\"reason \"+me.getReasonCode());\nSystem.out.println(\"msg \"+me.getMessage());\nSystem.out.println(\"loc \"+me.getLocalizedMessage());\nSystem.out.println(\"cause \"+me.getCause());\nSystem.out.println(\"excep \"+me);\nme.printStackTrace();\n}\n}\n}", "link": "https://github.com/eclipse/paho.mqtt.java", "origin": "Github", "suborigin": "Iot", "result": true, "Selector": "branches", "selectorShort": "branch", "MarkedSent": "eclipse paho java client\nthe paho java client is an mqtt client library written in java for developing applications that run on the jvm or other java compatible platforms such as android\nthe paho java client provides two apis: mqttasyncclient provides a fully asynchronous api where completion of activities is notified via registered callbacks. mqttclient is a synchronous wrapper around mqttasyncclient where functions appear synchronous to the application.\nproject description:\nthe paho project has been created to provide reliable open-source implementations of open and standard messaging protocols aimed at new, existing, and emerging applications for machine-to-machine (m2m) and internet of things (iot). paho reflects the inherent physical and cost constraints of device connectivity. its objectives include effective levels of decoupling between devices and applications, designed to keep markets open and encourage the rapid growth of scalable web and enterprise middleware and applications.\nlinks\nproject website: https://www.eclipse.org/paho\neclipse project information: https://projects.eclipse.org/projects/iot.paho\npaho java client page: https://eclipse.org/paho/clients/java/\ngithub: https://github.com/eclipse/paho.mqtt.java\ntwitter: @eclipsepaho\nissues: https://github.com/eclipse/paho.mqtt.java/issues\nmailing-list: https://dev.eclipse.org/mailman/listinfo/paho-dev\nusing the paho java client\ndownloading\neclipse hosts a nexus repository for those who want to use maven to manage their dependencies. the released libraries are also available in the maven central repository.\nadd the repository definition and the dependency definition shown below to your pom.xml.\nreplace %repourl% with either https://repo.eclipse.org/content/repositories/paho-releases/ for the official releases, or https://repo.eclipse.org/content/repositories/paho-snapshots/ for the nightly snapshots. replace %version% with the level required .\nthe latest release version is 1.2.5 and the current snapshot version is 1.2.6-snapshot.\n** dependency definition for mqttv3 client **\n<project ...>\n<repositories>\n<repository>\n<id>eclipse paho repo</id>\n<url>%repourl%</url>\n</repository>\n</repositories>\n...\n<dependencies>\n<dependency>\n<groupid>org.eclipse.paho</groupid>\n<artifactid>org.eclipse.paho.client.mqttv3</artifactid>\n<version>%version%</version>\n</dependency>\n</dependencies>\n</project>\n** dependency definition for mqttv5 client **\n<project ...>\n<repositories>\n<repository>\n<id>eclipse paho repo</id>\n<url>%repourl%</url>\n</repository>\n</repositories>\n...\n<dependencies>\n<dependency>\n<groupid>org.eclipse.paho</groupid>\n<artifactid>org.eclipse.paho.mqttv5.client</artifactid>\n<version>%version%</version>\n</dependency>\n</dependencies>\n</project>\nif you find that there is functionality missing or bugs in the release version, you may want to try using the snapshot version to see if this helps before raising a feature request or an issue.\nbuilding from source\nthere are two active -----> branches !!!  on the paho java git repository, master which is used to produce stable releases, and develop where active development is carried out. by default cloning the git repository will download the master branch, to build from develop make sure you switch to the remote branch: git checkout -b develop remotes/origin/develop\nto then build the library run the following maven command: mvn package -dskiptests\nthis will build the client library without running the tests. the jars for the library, source and javadoc can be found in the following directories:\norg.eclipse.paho.client.mqttv3/target\norg.eclipse.paho.mqttv5.client/target\ndocumentation\nmqttv3 reference documentation is online at: http://www.eclipse.org/paho/files/javadoc/index.html\nlog and debug in the java client: https://wiki.eclipse.org/paho/log_and_debug_in_the_java_client\ngetting started\nthe included code below is a very basic sample that connects to a server and publishes a message using the mqttv3 synchronous api. more extensive samples demonstrating the use of the mqttv3 and mqttv5 asynchronous api can be found in the org.eclipse.paho.sample directory of the source.\nimport org.eclipse.paho.client.mqttv3.mqttclient;\nimport org.eclipse.paho.client.mqttv3.mqttconnectoptions;\nimport org.eclipse.paho.client.mqttv3.mqttexception;\nimport org.eclipse.paho.client.mqttv3.mqttmessage;\nimport org.eclipse.paho.client.mqttv3.persist.memorypersistence;\npublic class mqttpublishsample {\npublic static void main(string[] args) {\nstring topic = \"mqtt examples\";\nstring content = \"message from mqttpublishsample\";\nint qos = 2;\nstring broker = \"tcp://iot.eclipse.org:1883\";\nstring clientid = \"javasample\";\nmemorypersistence persistence = new memorypersistence();\ntry {\nmqttclient sampleclient = new mqttclient(broker, clientid, persistence);\nmqttconnectoptions connopts = new mqttconnectoptions();\nconnopts.setcleansession(true);\nsystem.out.println(\"connecting to broker: \"+broker);\nsampleclient.connect(connopts);\nsystem.out.println(\"connected\");\nsystem.out.println(\"publishing message: \"+content);\nmqttmessage message = new mqttmessage(content.getbytes());\nmessage.setqos(qos);\nsampleclient.publish(topic, message);\nsystem.out.println(\"message published\");\nsampleclient.disconnect();\nsystem.out.println(\"disconnected\");\nsystem.exit(0);\n} catch(mqttexception me) {\nsystem.out.println(\"reason \"+me.getreasoncode());\nsystem.out.println(\"msg \"+me.getmessage());\nsystem.out.println(\"loc \"+me.getlocalizedmessage());\nsystem.out.println(\"cause \"+me.getcause());\nsystem.out.println(\"excep \"+me);\nme.printstacktrace();\n}\n}\n}", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000073, "year": null}, {"Unnamed: 0": 132, "autor": 132, "date": null, "content": "Note\nThe sole purpose of this repository is to help me organize recent academic papers related to fuzzing, binary analysis, IoT security, and general exploitation. This is a non-exhausting list, even though I'll try to keep it updated... Feel free to suggest decent papers via a PR.\nRead & Tagged\n2021 - An Empirical Study of OSS-Fuzz Bugs\nTags: flaky bugs, clusterfuzz, sanitizer, bug detection, bug classification, time-to-fix, time-to-detect\n2020 - Corpus Distillation for Effective Fuzzing\nTags: corpus minimization, afl-cmin, google fuzzer test suite, FTS, minset, AFL\n2020 - Symbolic execution with SymCC: Don't interpret, compile!\nTags: KLEE, QSYM, LLVM, C, C++, compiler, symbolic execution, concolic execution, source code level, IR, angr, Z3, DARPA corpus, AFL\n2020 - WEIZZ: Automatic Grey-Box Fuzzing for Structured Binary Formats\nTags: REDQUEEN, chunk-based formats, AFLSmart, I2S, checksums, magix bytes, QEMU, Eclipser, short fuzzing runs,\n2020 - Efficient Binary-Level Coverage Analysis\nTags: bcov, detour + trampoline, basic block coverage, sliced microexecution, superblocks, strongly connected components, dominator graph, BAP, angr, IDA, DynamoRIO, Intel PI, BAP, angr, IDA, DynamoRIO, Intel PIN\n2020 - Test-Case Reduction via Test-Case Generation: Insights From the Hypothesis Reducer\nTags: Test case reducer, property based testing, CSmith, test case generation, hierachical delta debugging\n2020 - AFL++: Combining Incremental Steps of Fuzzing Research\nTags: AFL++, AFL, MOpt, LAF-Intel, Fuzzbench, Ngram, RedQueen, Unicorn, QBDI, CmpLog, AFLFast\n2020 - FirmXRay: Detecting Bluetooth Link Layer Vulnerabilities From Bare-Metal Firmware\nTags: Ghdira, static analysis, sound disassembly, base address finder, BLE, vulnerability discovery\n2020 - P2IM: Scalable and Hardware-independent Firmware Testing via Automatic Peripheral Interface Modeling\nTags: HALucinator, emulation, firmware, QEMU, AFL, requires source, MCU, peripheral abstraction\n2020 - What Exactly Determines the Type? Inferring Types with Context\nTags: context assisted type inference, stripped binaries, variable and type reconstruction, IDA Pro, Word2Vec, CNN,\n2020 - Causal Testing: Understanding Defects\u2019 Root Causes\nTags: Defects4J, causal relationships, Eclipse plugin, unit test mutation, program trace diffing, static value diffing, user study\n2020 - AURORA: Statistical Crash Analysis for Automated Root Cause Explanation\nTags: RCA, program traces, input diversification, Intel PIN, Rust, CFG,\n2020 - ParmeSan: Sanitizer-guided Greybox Fuzzing\nTags: interprocedural CFG, data flow analysis, directed fuzzing (DGF), disregarding 'hot paths', LAVA-M based primitives, LLVM, Angora, AFLGo, ASAP, santizer dependent\n2020 - Magma: A Ground-Truth Fuzzing Benchmark\nTags: best practices, fuzzer benchmarking, ground truth, Lava-M\n2020 - Fitness Guided Vulnerability Detection with Greybox Fuzzing\nTags: AFL, vuln specific fitness metric (headroom), buffer/integer overflow detection, AFLGo, pointer analysis, CIL, bad benchmarking\n2020 - GREYONE: Data Flow Sensitive Fuzzing\nTags: data-flow fuzzing, taint-guided mutation, input prioritization, constraint conformance, REDQUEEN, good evaluation, VUzzer\n2020 - FairFuzz-TC: a fuzzer targeting rare branches\nTags: AFL, required seeding, branch mask\n2020 - Fitness Guided Vulnerability Detection with Greybox Fuzzing\nTags: AFL, vuln specific fitness metric (headroom), buffer/integer overflow detection, AFLGo, pointer analysis, CIL, bad evaluation\n2020 - TOFU: Target-Oriented FUzzer\nTags: DGF, structured mutations, staged fuzzing/learning of cli args, target fitness, structure aware, Dijkstra for priority, AFLGo, Superion\n2020 - FuZZan: Efficient Sanitizer Metadata Design for Fuzzing\nTags:: sanitizer metadata, optimization, ASAN, MSan, AFL\n2020 - Boosting Fuzzer Efficiency: An Information Theoretic Perspective\nTags:: Shannon entropy, seed power schedule, libfuzzer, active SLAM, DGF, fuzzer efficiency\n2020 - Learning Input Tokens for Effective Fuzzing\nTags: dynamic taint tracking, parser checks, magic bytes, creation of dict inputs for fuzzers\n2020 - A Review of Memory Errors Exploitation in x86-64\nTags: NX, canaries, ASLR, new mitigations, mitigation evaluation, recap on memory issues\n2020 - SoK: The Progress, Challenges, and Perspectives of Directed Greybox Fuzzing\nTags: SoK, directed grey box fuzzing, AFL, AFL mutation operators, DGF vs CGF\n2020 - MemLock: Memory Usage Guided Fuzzing\nTags: memory consumption, AFL, memory leak, uncontrolled-recursion, uncontrolled-memory-allocation, static analysis\n2019 - Matryoshka: Fuzzing Deeply Nested Branches\nTags: AFL, QSYM, Angora, path constraints, nested conditionals, (post) dominator trees, gradient descent, REDQUEEN, LAVA-M\n2019 - Building Fast Fuzzers\nTags: grammar based fuzzing, optimization, bold claims, comparison to badly/non-optimized fuzzers, python, lots of micro-optimizations, nice protocolling of failures, bad ASM optimization\n2019 - Not All Bugs Are the Same: Understanding, Characterizing, and Classifying the Root Cause of Bugs\nTags: RCA via bug reports, classification model, F score,\n2019 - AntiFuzz: Impeding Fuzzing Audits of Binary Executables\nTags: anti fuzzing, prevent crashes, delay executions, obscure coverage information, overload symbolic execution\n2019 - MOpt: Optimized Mutation Scheduling for Fuzzers\nTags: mutation scheduling, particle swarm optimization (PSO), AFL, AFL mutation operators, VUzzer,\n2019 - FuzzFactory: Domain-Specific Fuzzing with Waypoints\nTags: domain-specific fuzzing, AFL, LLVM, solve hard constraints like cmp, find dynamic memory allocations, binary-based\n2019 - Fuzzing File Systems via Two-Dimensional Input Space Exploration\nTags: Ubuntu, file systems, library OS, ext4, brtfs, meta block mutations, edge cases\n2019 - REDQUEEN: Fuzzing with Input-to-State Correspondence\nTags: feedback-driven, AFL, magic-bytes, nested contraints, input-to-state correspondence, I2S\n2019 - PeriScope: An Effective Probing and Fuzzing Framework for the Hardware-OS Boundary\nTags: kernel, android, userland, embedded, hardware, Linux, device driver, WiFi\n2019 - FirmFuzz: Automated IoT Firmware Introspection and Analysis\nTags: emulation, firmadyne, BOF, XSS, CI, NPD, semi-automatic\n2019 - Firm-AFL: High-Throughput Greybox Fuzzing of IoT Firmware via Augmented Process Emulation\nTags: emulation, qemu, afl, full vs user mode, syscall redirect, \"augmented process emulation\", firmadyne\n2018 - A Survey of Automated Root Cause Analysisof Software Vulnerability\nTags: Exploit mitigations, fuzzing basics, symbolic execution basics, fault localization, high level\n2018 - PhASAR: An Inter-procedural Static Analysis Framework for C/C++\nTags: LLVM, (inter-procedural) data-flow analysis, call-graph, points-to, class hierachy, CFG, IR\n2018 - INSTRIM: Lightweight Instrumentation for Coverage-guided Fuzzing\nTags: LLVM, instrumentation optimization, graph algorithms, selective instrumentation, coverage calculation\n2018 - What You Corrupt Is Not What You Crash: Challenges in Fuzzing Embedded Devices\nTags: embedded, challenges, heuristics, emulation, crash classification, fault detection\n2018 - Evaluating Fuzz Testing\nTags: fuzzing evaluation, good practices, bad practices\n2017 - Root Cause Analysis of Software Bugs using Machine Learning Techniques\nTags: ML, RC prediction for filed bug reports, unsupervised + supervised combination, RC categorisation, F score\n2017 - kAFL: Hardware-Assisted Feedback Fuzzing for OS Kernels\nTags: intel PT, kernel, AFL, file systems, Windows, NTFS, Linux, ext, macOS, APFS, driver, feedback-driven\n2016 - Driller: Argumenting Fuzzing Through Selective Symbolic Execution\nTags: DARPA, CGC, concolic execution, hybrid fuzzer, binary based\n2015 - Challenges with Applying Vulnerability Prediction Models\nTags: VPM vs DPM, prediction models on large scale systems, files with frequent changes leave more vulns, older code exhibits more vulns\n2014 - Optimizing Seed Selection for Fuzzing\nTags: BFF, (weighted) minset, peach, cover set problem, seed transferabilty, time minset, size minset, round robin\n2013 - Automatic Recovery of Root Causes from Bug-Fixing Changes\nTags: ML + SCA, F score, AST, PPA, source tree analysis\nUnread\nUnread papers categorized by a common main theme.\nGeneral fuzzing implementations\n2022 - BEACON : Directed Grey-Box Fuzzing with Provable Path Pruning\n2021 - Same Coverage, Less Bloat: Accelerating Binary-only Fuzzing with Coverage-preserving Coverage-guided Tracing\n2021 - Facilitating Parallel Fuzzing with Mutually-exclusive Task Distribution\n2021 - PATA: Fuzzing with Path Aware Taint Analysis\n2021 - BSOD: Binary-only Scalable fuzzing Of device Drivers\n2021 - FuzzBench: An Open Fuzzer Benchmarking Platform and Service\n2021 - My Fuzzer Beats Them All! Developing a Framework for Fair Evaluation and Comparison of Fuzzers\n2021 - Scalable Fuzzing of Program Binaries with E9AFL\n2021 - BigMap: Future-proofing Fuzzers with Efficient Large Maps\n2021 - Token-Level Fuzzing\n2021 - Hashing Fuzzing: Introducing Input Diversity to Improve Crash Detection\n2021 - LeanSym: Efficient Hybrid Fuzzing Through Conservative Constraint Debloating\n2021 - ESRFuzzer: an enhanced fuzzing framework for physical SOHO router devices to discover multi-Type vulnerabilities\n2021 - FIRM-COV: High-Coverage Greybox Fuzzing for IoT Firmware via Optimized Process Emulation\n2021 - KCFuzz: Directed Fuzzing Based on Keypoint Coverage\n2021 - TCP-Fuzz: Detecting Memory and Semantic Bugs in TCP Stacks with Fuzzing\n2021 - Fuzzing with optimized grammar-aware mutation strategies\n2021 - Directed Fuzzing for Use-After-FreeVulnerabilities Detection\n2021 - RapidFuzz: Accelerating Fuzzing via Generative Adversarial Networks\n2021 - DIFUZZRTL: Differential Fuzz Testing to FindCPU Bugs\n2021 - Z-Fuzzer: device-agnostic fuzzing of Zigbee protocol implementation\n2021 - Fuzzing with Multi-dimensional Control of Mutation Strategy\n2021 - Using a Guided Fuzzer and Preconditions to Achieve Branch Coverage with Valid Inputs\n2021 - RIFF: Reduced Instruction Footprint for Coverage-Guided Fuzzing\n2021 - CoCoFuzzing: Testing Neural Code Models with Coverage-Guided Fuzzing\n2021 - Seed Selection for Successful Fuzzing\n2021 - Gramatron: Effective Grammar-Aware Fuzzing\n2021 - Hyntrospect: a fuzzer for Hyper-V devices\n2021 - FUZZOLIC: mixing fuzzing and concolic execution\n2021 - QFuzz: Quantitative Fuzzing for Side Channels\n2021 - Revizor: Fuzzing for Leaks in Black-box CPUs\n2021 - Unleashing Fuzzing Through Comprehensive, Efficient, and Faithful Exploitable-Bug Exposing\n2021 - Constraint-guided Directed Greybox Fuzzing\n2021 - Test-Case Reduction and Deduplication Almost forFree with Transformation-Based Compiler Testing\n2021 - RULF: Rust Library Fuzzing via API Dependency Graph Traversal\n2021 - STOCHFUZZ: Sound and Cost-effective Fuzzing of Stripped Binaries by Incremental and Stochastic Rewriting\n2021 - PS-Fuzz: Efficient Graybox Firmware Fuzzing Based on Protocol State\n2021 - MuDelta: Delta-Oriented Mutation Testing at Commit Time\n2021 - CollabFuzz: A Framework for Collaborative Fuzzing\n2021 - MUTAGEN: Faster Mutation-Based Random Testing\n2021 - Inducing Subtle Mutations with Program Repair\n2021 - Differential Analysis of X86-64 Instruction Decoders\n2021 - On Introducing Automatic Test Case Generation in Practice: A Success Story and Lessons Learned\n2021 - A Priority Based Path Searching Method for Improving Hybrid Fuzzing\n2021 - IntelliGen: Automatic Driver Synthesis for Fuzz Testing\n2021 - icLibFuzzer: Isolated-context libFuzzer for Improving Fuzzer Comparability\n2021 - SN4KE: Practical Mutation Testing at Binary Level\n2021 - One Engine to Fuzz \u2019em All: Generic Language Processor Testing with Semantic Validation\n2021 - Growing A Test Corpus with Bonsai Fuzzing\n2021 - Fuzzing Symbolic Expressions\n2021 - JMPscare: Introspection for Binary-Only Fuzzing\n2021 - An Improved Directed Grey-box Fuzzer\n2021 - A Binary Protocol Fuzzing Method Based on SeqGAN\n2021 - Refined Grey-Box Fuzzing with Sivo\n2021 - PSOFuzzer: A Target-Oriented Software Vulnerability Detection Technology Based on Particle Swarm Optimization\n2021 - MooFuzz: Many-Objective Optimization Seed Schedule for Fuzzer\n2021 - CMFuzz: context-aware adaptive mutation for fuzzers\n2021 - GTFuzz: Guard Token Directed Grey-Box Fuzzing\n2021 - ProFuzzBench: A Benchmark for Stateful Protocol Fuzzing\n2021 - SymQEMU:Compilation-based symbolic execution for binaries\n2021 - CONCOLIC EXECUTION TAILORED FOR HYBRID FUZZING THESIS\n2021 - Breaking Through Binaries: Compiler-quality Instrumentationfor Better Binary-only Fuzzing\n2021 - AlphaFuzz: Evolutionary Mutation-based Fuzzing as Monte Carlo Tree Search\n2020 - Fuzzing with Fast Failure Feedback\n2020 - LAFuzz: Neural Network for Efficient Fuzzing\n2020 - MaxAFL: Maximizing Code Coverage with a Gradient-Based Optimization Technique\n2020 - Program State Abstraction for Feedback-Driven Fuzz Testing using Likely Invariants\n2020 - PMFuzz: Test Case Generation for Persistent Memory Programs\n2020 - FuSeBMC: A White-Box Fuzzer for Finding Security Vulnerabilities in C Programs\n2020 - Integrity: Finding Integer Errors by Targeted Fuzzing\n2020 - ConFuzz: Coverage-guided Property Fuzzing for Event-driven Programs\n2020 - AFLTurbo: Speed up Path Discovery for Greybox Fuzzing\n2020 - Fuzzing Channel-Based Concurrency Runtimes using Types and Effects\n2020 - DeFuzz: Deep Learning Guided Directed Fuzzing\n2020 - CrFuzz: Fuzzing Multi-purpose Programs through InputValidation\n2020 - EPfuzzer: Improving Hybrid Fuzzing with Hardest-to-reach Branch Prioritization\n2020 - Fuzzing Based on Function Importance by Attributed Call Graph\n2020 - UNIFUZZ: A Holistic and Pragmatic Metrics-Driven Platform for Evaluating Fuzzers\n2020 - PathAFL: Path-Coverage Assisted Fuzzing\n2020 - Path Sensitive Fuzzing for Native Applications\n2020 - UniFuzz: Optimizing Distributed Fuzzing via Dynamic Centralized Task Scheduling\n2020 - Fuzzing Error Handling Code using Context-Sensitive Software Fault Injection\n2020 - SpecFuzz: Bringing Spectre-type vulnerabilities to the surface\n2020 - Zeror: Speed Up Fuzzing with Coverage-sensitive Tracing and Scheduling\n2020 - MUZZ: Thread-aware Grey-box Fuzzing for Effective Bug Hunting in Multithreaded Programs\n2020 - Evolutionary Grammar-Based Fuzzing\n2020 - AFLpro: Direction sensitive fuzzing\n2020 - CSI-Fuzz: Full-speed Edge Tracing Using Coverage Sensitive Instrumentation\n2020 - Scalable Greybox Fuzzing for Effective Vulnerability Management DISS\n2020 - HotFuzz Discovering Algorithmic Denial-of-Service Vulnerabilities through Guided Micro-Fuzzing\n2020 - Fuzzing Binaries for Memory Safety Errors with QASan\n2020 - Suzzer: A Vulnerability-Guided Fuzzer Based on Deep Learning\n2020 - IJON: Exploring Deep State Spaces via Fuzzing\n2020 - Binary-level Directed Fuzzing for Use-After-Free Vulnerabilities\n2020 - AFLNET: A Greybox Fuzzer for Network Protocols\n2020 - PANGOLIN: Incremental Hybrid Fuzzing with Polyhedral Path Abstraction\n2020 - UEFI Firmware Fuzzing with Simics Virtual Platform\n2020 - Finding Security Vulnerabilities in Network Protocol Implementations\n2020 - Typestate-Guided Fuzzer for Discovering Use-after-Free Vulnerabilities\n2020 - FuzzGuard: Filtering out Unreachable Inputs in Directed Grey-box Fuzzing through Deep Learning\n2020 - HyDiff: Hybrid Differential Software Analysis\n2019 - Engineering a Better Fuzzer with SynergicallyIntegrated Optimizations\n2019 - Superion: Grammar-Aware Greybox Fuzzing\n2019 - ProFuzzer: On-the-fly Input Type Probing for Better Zero-day Vulnerability Discovery\n2019 - Grimoire: Synthesizing Structure while Fuzzing\n2019 - Ptrix: Efficient Hardware-Assisted Fuzzing for COTS Binary\n2019 - SAVIOR: Towards Bug-Driven Hybrid Testing\n2019 - FUDGE: Fuzz Driver Generation at Scale\n2019 - NAUTILUS: Fishing for Deep Bugs with Grammars\n2019 - Send Hardest Problems My Way: Probabilistic Path Prioritization for Hybrid Fuzzing\n2019 - EnFuzz: Ensemble Fuzzing with Seed Synchronization among Diverse Fuzzers\n2018 - Fuzz Testing in Practice: Obstacles and Solutions\n2018 - PAFL: Extend Fuzzing Optimizations of Single Mode to Industrial Parallel Mode\n2018 - PTfuzz: Guided Fuzzing with Processor Trace Feedback\n2018 - Angora: Efficient Fuzzing by Principled Search\n2018 - FairFuzz: A Targeted Mutation Strategy for Increasing Greybox Fuzz Testing Coverage\n2018 - NEUZZ: Efficient Fuzzing with Neural Program Smoothing\n2018 - CollAFL: path Sensitive Fuzzing\n2018 - Full-speed Fuzzing: Reducing Fuzzing Overhead through Coverage-guided Tracing\n2018 - QSYM: A Practical Concolic Execution Engine Tailored for Hybrid Fuzzing\n2018 - Coverage-based Greybox Fuzzing as Markov Chain\n2018 - MoonShine: Optimizing OS Fuzzer Seed Selection with Trace Distillation\n2018 - Singularity: Pattern Fuzzing for Worst Case Complexity\n2018 - Smart Greybox Fuzzing\n2018 - Hawkeye: Towards a Desired Directed Grey-box Fuzzer\n2018 - PerfFuzz: Automatically Generating Pathological Inputs\n2018 - FairFuzz: A Targeted Mutation Strategy for Increasing Greybox Fuzz Testing Coverage\n2018 - Enhancing Memory Error Detection forLarge-Scale Applications and Fuzz Testing\n2018 - T-Fuzz: fuzzing by program transformation\n2017 - Evaluating and improving fault localization\n2017 - IMF: Inferred Model-based Fuzzer\n2017 - Synthesizing Program Input Grammars\n2017 - Stateful Fuzzing of Wireless Device Drivers in an Emulated Environment\n2017 - Steelix: Program-State Based Binary Fuzzing\n2017 - Designing New Operating Primitives to ImproveFuzzing Performance\n2017 - VUzzer: Application-aware Evolutionary Fuzzing\n2017 - DIFUZE: Interface Aware Fuzzing for Kernel Drivers\n2017 - Instruction Punning: Lightweight Instrumentation for x86-64\n2017 - Designing New Operating Primitives to Improve Fuzzing Performance\n2014 - A Large-Scale Analysis of the Security of Embedded Firmwares\n2013 - Scheduling Black-box Mutational Fuzzing\n2013 - Dowsing for Overflows: A Guided Fuzzer to Find Buffer Boundary Violations\n2013 - RPFuzzer: A Framework for Discovering Router Protocols Vulnerabilities Based on Fuzzing\n2011 - Offset-Aware Mutation based Fuzzing for Buffer Overflow Vulnerabilities: Few Preliminary Results\n2010 - TaintScope: A Checksum-Aware Directed Fuzzing Tool for Automatic Software Vulnerability Detection\n2009 - Taint-based Directed Whitebox Fuzzing\n2009 - Dynamic Test Generation To Find Integer Bugs in x86 Binary Linux Programs\n2008 - Grammar-based Whitebox Fuzzing\n2008 - Vulnerability Analysis for X86 Executables Using Genetic Algorithm and Fuzzing\n2008 - Fuzzing Wi-Fi Drivers to Locate Security Vulnerabilities\n2008 - KLEE: Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs\n2008 - Automated Whitebox Fuzz Testing\n2005 - DART: Directed Automated Random Testing\n1994 - Dominators, Super Blocks, and Program Coverage\nIoT fuzzing\n2022 - Fuzzware: Using Precise MMIO Modeling for Effective Firmware Fuzzing\n2021 - Large-scale Firmware Vulnerability Analysis Based on Code Similarity\n2021 - Towards Fast and Scalable Firmware Fuzzing with Dual-Level Peripheral Modeling\n2021 - Zero WFuzzer: Target-Oriented Fuzzing for Web Interface of Embedded Devices\n2021 - StFuzzer: Contribution-Aware Coverage-Guided Fuzzing for Smart Devices\n2021 - Rtkaller: State-aware Task Generation for RTOS Fuzzing\n2021 - IFIZZ: Deep-State and Efficient Fault-Scenario Generation to Test IoT Firmware\n2021 - Automatic Vulnerability Detection in Embedded Devices and Firmware: Survey and Layered Taxonomies\n2021 - Fuzzing the Internet of Things: A Review on the Techniques and Challenges for Efficient Vulnerability Discovery in Embedded Systems\n2020 - Verification of Embedded Software Binaries using Virtual Prototypes\n2020 - \u03bcSBS: Static Binary Sanitization of Bare-metal Embedded Devices forFault Observability\n2020 - Device-agnostic Firmware Execution is Possible: A Concolic Execution Approach for Peripheral Emulation\n2020 - Vulnerability Detection in SIoT Applications: A Fuzzing Method on their Binaries\n2020 - FirmAE: Towards Large-Scale Emulation of IoT Firmware forDynamic Analysis\n2020 - FIRMNANO: Toward IoT Firmware Fuzzing Through Augmented Virtual Execution\n2020 - ARM-AFL: Coverage-Guided Fuzzing Framework for ARM-Based IoT Devices\n2020 - Bug detection in embedded environments by fuzzing and symbolic execution\n2020 - FirmXRay: Detecting Bluetooth Link Layer Vulnerabilities From Bare-Metal Firmware\n2020 - EM-Fuzz: Augmented Firmware Fuzzing via Memory Checking\n2020 - Verification of Embedded Binaries using Coverage-guided Fuzzing with System C-based Virtual Prototypes\n2020 - DICE: Automatic Emulation of DMA Input Channels for Dynamic Firmware Analysis\n2020 - Fw\u2010fuzz: A code coverage\u2010guided fuzzing framework for network protocols on firmware\n2020 - TAINT-DRIVEN FIRMWARE FUZZING OF EMBEDDED SYSTEMS THESIS\n2020 - A Dynamic Instrumentation Technology for IoT Devices\n2020 - Vulcan: a state-aware fuzzing tool for wear OS ecosystem\n2020 - A Novel Concolic Execution Approach on Embedded Device\n2020 - HFuzz: Towards automatic fuzzing testing of NB-IoT core network protocols implementations\n2020 - FIRMCORN: Vulnerability-Oriented Fuzzing of IoT Firmware via Optimized Virtual Execution\n2018 - IoTFuzzer: Discovering Memory Corruptions in IoT Through App-based Fuzzing\n2017 - Towards Automated Dynamic Analysis for Linux-based Embedded Firmware\n2016 - Scalable Graph-based Bug Search for Firmware Images\n2015 - SURROGATES: Enabling Near-Real-Time Dynamic Analyses of Embedded Systems\n2015 - Firmalice - Automatic Detection of Authentication Bypass Vulnerabilities in Binary Firmware\n2014 - A Large-Scale Analysis of the Security of Embedded Firmwares\n2013 - RPFuzzer: A Framework for Discovering Router Protocols Vulnerabilities Based on Fuzzing\nFirmware Emulation\n2021 - FIRMGUIDE: Boosting the Capability of Rehosting Embedded Linux Kernels through Model-Guided Kernel Execution\n2021 - Automatic Firmware Emulation through Invalidity-guided Knowledge Inference(Extended Version)\n2021 - Firmware Re-hosting Through Static Binary-level Porting\n2021 - Jetset: Targeted Firmware Rehosting for Embedded Systems\n2021 - Automatic Firmware Emulation through Invalidity-guided Knowledge Inference\nNetwork fuzzing\n2021 - StateAFL: Greybox Fuzzing for Stateful Network Servers\nKernel fuzzing\n2021 - ACHyb: a hybrid analysis approach to detect kernel access control vulnerabilities\n2021 - CVFuzz: Detecting complexity vulnerabilities in OpenCL kernels via automated pathological input generation\n2021 - HEALER: Relation Learning Guided Kernel Fuzzing\n2021 - SyzVegas: Beating Kernel Fuzzing Odds with Reinforcement Learning\n2021 - NTFUZZ: Enabling Type-Aware Kernel Fuzzing on Windows with Static Binary Analysis\n2021 - Undo Workarounds for Kernel Bugs\n2020 - A Hybrid Interface Recovery Method for Android Kernels Fuzzing\n2020 - FINDING RACE CONDITIONS IN KERNELS:FROM FUZZING TO SYMBOLIC EXECUTION - THESIS\n2020 - Agamotto: Accelerating Kernel Driver Fuzzing with Lightweight Virtual Machine Checkpoints\n2020 - X-AFL: a kernel fuzzer combining passive and active fuzzing\n2020 - Identification of Kernel Memory Corruption Using Kernel Memory Secret Observation Mechanism\n2020 - HFL: Hybrid Fuzzing on the Linux Kernel\n2020 - Realistic Error Injection for System Calls\n2020 - KRACE: Data Race Fuzzing for Kernel File Systems\n2020 - USBFuzz: A Framework for Fuzzing USB Drivers by Device Emulation\n2019 - Fuzzing File Systems via Two-Dimensional Input Space Exploration\n2019 - Razzer: Finding Kernel Race Bugs through Fuzzing\n2019 - Unicorefuzz: On the Viability of Emulation for Kernel space Fuzzing\n2017 - Stateful Fuzzing of Wireless Device Drivers in an Emulated Environment\n2017 - DIFUZE: Interface Aware Fuzzing for Kernel Drivers\n2008 - Fuzzing Wi-Fi Drivers to Locate Security Vulnerabilities\nFormat specific fuzzing\n2021 - V-Shuttle: Scalable and Semantics-Aware Hypervisor Virtual Device Fuzzing\n2021 - FormatFuzzer: Effective Fuzzing of Binary File Formats\n2020 - NYX: Greybox Hypervisor Fuzzing using Fast Snapshots and Affine Types\n2020 - Tree2tree Structural Language Modeling for Compiler Fuzzing\n2020 - Detecting Critical Bugs in SMT Solvers Using Blackbox Mutational Fuzzing\n2020 - JS Engine - Montage: A Neural Network Language Model-Guided JavaScript Engine Fuzzer\n2020 - JS Engine - Fuzzing JavaScript Engines with Aspect-preserving Mutation\n2020 - CUDA Compiler - CUDAsmith: A Fuzzer for CUDA Compilers\n2020 - Smart Contracts - sFuzz: An Efficient Adaptive Fuzzer for Solidity Smart Contracts\n2019 - Compiler Fuzzing: How Much Does It Matter?\n2019 - Smart Contracts - Harvey: A Greybox Fuzzer for Smart Contracts\n2017 - XML - Skyfire: Data-Driven Seed Generation for Fuzzing\nExploitation\n2021 - A Novel Method for the Automatic Generation of JOP Chain Exploits\n2021 - V0Finder: Discovering the Correct Origin of Publicly Reported Software Vulnerabilities\n2021 - Identifying Valuable Pointers in Heap Data\n2021 - OCTOPOCS: Automatic Verification of Propagated Vulnerable Code Using Reformed Proofs of Concept\n2021 - Characterizing Vulnerabilities in a Major Linux Distribution\n2021 - MAZE: Towards Automated Heap Feng Shui\n2021 - Vulnerability Detection in C/C++ Source Code With Graph Representation Learning\n2021 - mallotROPism: a metamorphic engine for malicious software variation development\n2020 - Automatic Techniques to Systematically Discover New Heap Exploitation Primitives\n2020 - Shadow-Heap: Preventing Heap-based Memory Corruptions by Metadata Validation\n2020 - Practical Fine-Grained Binary Code Randomization\n2020 - Tiny-CFA: Minimalistic Control-Flow Attestation UsingVerified Proofs of Execution\n2020 - Greybox Automatic Exploit Generation for Heap Overflows in Language Interpreters - PHD THESIS\n2020 - ABCFI: Fast and Lightweight Fine-Grained Hardware-Assisted Control-Flow Integrity\n2020 - HeapExpo: Pinpointing Promoted Pointers to Prevent Use-After-Free Vulnerabilities\n2020 - Localizing Patch Points From One Exploit\n2020 - Speculative Dereferencing of Registers: Reviving Foreshadow\n2020 - HAEPG: An Automatic Multi-hop Exploitation Generation Framework\n2020 - Exploiting More Binaries by Using Planning to Assemble ROP Exploiting More Binaries by Using Planning to Assemble ROP Attacks Attacks\n2020 - ROPminer: Learning-Based Static Detection of ROP Chain Considering Linkability of ROP Gadgets\n2020 - KOOBE: Towards Facilitating Exploit Generation of Kernel Out-Of-Bounds Write Vulnerabilities\n2020 - Preventing Return Oriented Programming Attacks By Preventing Return Instruction Pointer Overwrites\n2020 - KASLR: Break It, Fix It, Repeat\n2020 - ShadowGuard : Optimizing the Policy and Mechanism of Shadow Stack Instrumentation using Binary Static Analysis\n2020 - VulHunter: An Automated Vulnerability Detection System Based on Deep Learning and Bytecode\n2020 - Analysis and Evaluation of ROPInjector\n2020 - API Misuse Detection in C Programs: Practice on SSL APIs\n2020 - KOOBE: Towards Facilitating Exploit Generation of Kernel Out-Of-Bounds Write Vulnerabilities\n2020 - Egalito: Layout-Agnostic Binary Recompilation\n2020 - Verifying Software Vulnerabilities in IoT Cryptographic Protocols\n2020 - \u03bcRAI: Securing Embedded Systems with Return Address Integrity\n2020 - Preventing Return Oriented Programming Attacks By Preventing Return Instruction Pointer Overwrites\n2019 - Kernel Protection Against Just-In-Time Code Reuse\n2019 - Kernel Exploitation Via Uninitialized Stack\n2019 - KEPLER: Facilitating Control-flow Hijacking Primitive Evaluation for Linux Kernel Vulnerabilities\n2019 - SLAKE: Facilitating Slab Manipulation for Exploiting Vulnerabilities in the Linux Kernel\n2018 - HeapHopper: Bringing Bounded Model Checkingto Heap Implementation Security\n2018 - K-Miner: Uncovering Memory Corruption in Linux\n2017 - HAIT: Heap Analyzer with Input Tracing\n2017 - DROP THE ROP: Fine-grained Control-flow Integrity for the Linux Kernel\n2017 - kR^X: Comprehensive Kernel Protection against Just-In-Time Code Reuse\n2017 - Unleashing Use-Before-Initialization Vulnerabilities in the Linux Kernel Using Targeted Stack Spraying\n2017 - Towards Automated Dynamic Analysis for Linux-based Embedded Firmware\n2016 - Scalable Graph-based Bug Search for Firmware Images\n2015 - Cross-Architecture Bug Search in Binary Executables\n2015 - SURROGATES: Enabling Near-Real-Time Dynamic Analyses of Embedded Systems\n2015 - From Collision To Exploitation: Unleashing Use-After-Free Vulnerabilities in Linux Kernel\n2015 - PIE: Parser Identification in Embedded Systems\n2014 - ret2dir: Rethinking Kernel Isolation\n2014 - Make It Work, Make It Right, Make It Fast: Building a Platform-Neutral Whole-System Dynamic Binary Analysis Platform\n2012 - Anatomy of a Remote Kernel Exploit\n2012 - A Heap of Trouble: Breaking the LinuxKernel SLOB Allocator\n2011 - Linux kernel vulnerabilities: state-of-the-art defenses and open problems\n2011 - Protecting the Core: Kernel Exploitation Mitigations\n2015 - From Collision To Exploitation: Unleashing Use-After-Free Vulnerabilities in Linux Kernel\n2014 - ret2dir: Rethinking Kernel Isolation\n2012 - Anatomy of a Remote Kernel Exploit\n2012 - A Heap of Trouble: Breaking the Linux Kernel SLOB Allocator\n2011 - Linux kernel vulnerabilities: state-of-the-art defenses and open problems\n2011 - Protecting the Core: Kernel Exploitation Mitigations\nStatic Binary Analysis\n2021 - VIVA: Binary Level Vulnerability Identification via Partial Signature\n2021 - Overview of the advantages and disadvantages of static code analysis tools\n2021 - Multi-Level Cross-Architecture Binary Code Similarity Metric\n2020 - VulDetector: Detecting Vulnerabilities using Weighted Feature Graph Comparison\n2020 - DEEPBINDIFF: Learning Program-Wide Code Representations for Binary Diffing\n2020 - BinDeep: A Deep Learning Approach to Binary Code Similarity Detection\n2020 - Revisiting Binary Code Similarity Analysis using Interpretable Feature Engineering and Lessons Learned\n2020 - iDEA: Static Analysis on the Security of Apple Kernel Drivers\n2020 - HART: Hardware-Assisted Kernel Module Tracing on Arm\n2020 - AN APPROACH TO COMPARING CONTROL FLOW GRAPHS BASED ON BASIC BLOCK MATCHING\n2020 - How Far We Have Come: Testing Decompilation Correctness of C Decompilers\n2020 - Dynamic Binary Lifting and Recompilation DISS\n2020 - Similarity Based Binary Backdoor Detection via Attributed Control Flow Graph\n2020 - IoTSIT: A Static Instrumentation Tool for IoT Devices\n2019 - Code Similarity Detection using AST and Textual Information\n2018 - CodEX: Source Code Plagiarism DetectionBased on Abstract Syntax Trees\n2017 - rev.ng: a unified binary analysis framework to recover CFGs and function boundaries\n2017 - Angr: The Next Generation of Binary Analysis\n2016 - Binary code is not easy\n2015 - Cross-Architecture Bug Search in Binary Executables\n2014 - A platform for secure static binary instrumentation\n2013 - MIL: A language to build program analysis tools through static binary instrumentation\n2013 - Binary Code Analysis\n2013 - A compiler-level intermediate representation based binary analysis and rewriting system\n2013 - Protocol reverse engineering through dynamic and static binary analysis\n2013 - BinaryPig: Scalable Static Binary Analysis Over Hadoop\n2011 - BAP: A Binary Analysis Platform\n2009 - Syntax tree fingerprinting for source code similarity detection\n2008 - BitBlaze: A New Approach to Computer Security via Binary Analysis\n2005 - Practical analysis of stripped binary code\n2004 - Detecting kernel-level rootkits through binary analysis\nMisc\n2021 - yzGen: Automated Generation of Syscall Specification of Closed-Source macOS Drivers\n2021 - Igor: Crash Deduplication Through Root-Cause Clustering\n2021 - UAFSan: an object-identifier-based dynamic approach for detecting use-after-free vulnerabilities\n2021 - SyML: Guiding Symbolic Execution Toward Vulnerable States Through Pattern Learning\n2021 - LLSC: A Parallel Symbolic Execution Compiler for LLVM IR\n2021 - FuzzSplore: Visualizing Feedback-Driven Fuzzing Techniques\n2020 - Memory Error Detection Based on Dynamic Binary Translation\n2020 - Sydr: Cutting Edge Dynamic Symbolic Execution\n2020 - DrPin: A dynamic binary instumentator for multiple processor architectures\n2020 - MVP: Detecting Vulnerabilities using Patch-Enhanced Vulnerability Signatures\n2020 - Collecting Vulnerable Source Code from Open-Source Repositories for Dataset Generation\n2020 - LEOPARD: Identifying Vulnerable Code for Vulnerability Assessment through Program Metrics\n2020 - Dynamic Program Analysis Tools in GCC and CLANG Compilers\n2020 - On Using k-means Clustering for Test Suite Reduction\n2020 - Optimizing the Parameters of an Evolutionary Algorithm for Fuzzing and Test Data Generation\n2020 - Inputs from Hell: Learning Input Distributions for Grammar-Based Test Generation\n2020 - IdSan: An identity-based memory sanitizer for fuzzing binaries\n2020 - An experimental study oncombining automated andstochastic test data generation - MASTER THESIS\n2020 - FuzzGen: Automatic Fuzzer Generation\n2020 - Fuzzing: On the Exponential Cost of Vulnerability Discovery\n2020 - Poster: Debugging Inputs\n2020 - API Misuse Detection in C Programs: Practice on SSL APIs\n2020 - Egalito: Layout-Agnostic Binary Recompilation\n2020 - Verifying Software Vulnerabilities in IoT Cryptographic Protocols\n2020 - \u03bcRAI: Securing Embedded Systems with Return Address Integrity\n2020 - Fast Bit-Vector Satisfiability\n2020 - MARDU: Efficient and Scalable Code Re-randomization\n2020 - Towards formal verification of IoT protocols: A Review\n2020 - Automating the fuzzing triage process\n2020 - COMPARING AFL SCALABILITY IN VIRTUAL-AND NATIVE ENVIRONMENT\n2020 - SYMBION: Interleaving Symbolic with Concrete Execution\n2020 - Not All Coverage Measurements Are Equal: Fuzzing by Coverage Accounting for Input Prioritization\n2019 - Toward the Analysis of Embedded Firmware through Automated Re-hosting\n2019 - FUZZIFICATION: Anti-Fuzzing Techniques\n2018 - VulinOSS: A Dataset of Security Vulnerabilities in Open-source Systems\n2018 - HDDr: A Recursive Variantof the Hierarchical Delta Debugging Algorithm\n2017 - Coarse Hierarchical Delta Debugging\n2017 - VUDDY: A Scalable Approach for Vulnerable CodeClone Discovery\n2017 - Postmortem Program Analysis with Hardware-Enhanced Post-Crash Artifacts\n2017 - Synthesizing Program Input Grammars\n2017 - Designing New Operating Primitives to Improve Fuzzing Performance\n2017 - Instruction Punning: Lightweight Instrumentation for x86-64\n2016 - Modernizing Hierarchical Delta Debugging\n2016 - VulPecker: An Automated Vulnerability Detection SystemBased on Code Similarity Analysis\n2016 - CREDAL: Towards Locating a Memory Corruption Vulnerability with Your Core Dump\n2016 - RETracer: Triaging Crashes by Reverse Execution fromPartial Memory Dumps\n2015 - PIE: Parser Identification in Embedded Systems\n2010 - Iterative Delta Debugging\n2009 - Dynamic Test Generation To Find Integer Bugs in x86 Binary Linux Programs\n2006 - HDD: Hierarchical Delta Debugging\nSurveys, SoKs, and Studies\n2021 - Exploratory Review of Hybrid Fuzzing for Automated Vulnerability Detection\n2021 - A Systematic Review of Network Protocol Fuzzing Techniques\n2021 - Vulnerability Detection is Just the Beginning\n2021 - Evaluating Synthetic Bugs\n2020 - A Practical, Principled Measure of Fuzzer Appeal:A Preliminary Study\n2020 - A Systemic Review of Kernel Fuzzing\n2020 - A Survey of Hybrid Fuzzing based on Symbolic Execution\n2020 - A Study on Using Code Coverage Information Extracted from Binary to Guide Fuzzing\n2020 - Study of Security Flaws in the Linux Kernel by Fuzzing\n2020 - Dynamic vulnerability detection approaches and tools: State of the Art\n2020 - Fuzzing: Challenges and Reflections\n2020 - The Relevance of Classic Fuzz Testing: Have We Solved This One?\n2020 - A Practical, Principled Measure of Fuzzer Appeal:A Preliminary Study\n2020 - SoK: All You Ever Wanted to Know About x86/x64 Binary Disassembly But Were Afraid to Ask\n2020 - A Quantitative Comparison of Coverage-Based Greybox Fuzzers\n2020 - A Survey of Security Vulnerability Analysis, Discovery, Detection, and Mitigation on IoT Devices\n2020 - A systematic review of fuzzing based on machine learning techniques\n2019 - A Survey of Binary Code Similarity\n2019 - The Art, Science, and Engineering of Fuzzing: A Survey\n2012 - Regression testingminimization, selection and prioritization: a survey", "link": "https://github.com/0xricksanchez/paper_collection", "origin": "Github", "suborigin": "Iot", "result": true, "Selector": "branches", "selectorShort": "branch", "MarkedSent": "note\nthe sole purpose of this repository is to help me organize recent academic papers related to fuzzing, binary analysis, iot security, and general exploitation. this is a non-exhausting list, even though i'll try to keep it updated... feel free to suggest decent papers via a pr.\nread & tagged\n2021 - an empirical study of oss-fuzz bugs\ntags: flaky bugs, clusterfuzz, sanitizer, bug detection, bug classification, time-to-fix, time-to-detect\n2020 - corpus distillation for effective fuzzing\ntags: corpus minimization, afl-cmin, google fuzzer test suite, fts, minset, afl\n2020 - symbolic execution with symcc: don't interpret, compile!\ntags: klee, qsym, llvm, c, c++, compiler, symbolic execution, concolic execution, source code level, ir, angr, z3, darpa corpus, afl\n2020 - weizz: automatic grey-box fuzzing for structured binary formats\ntags: redqueen, chunk-based formats, aflsmart, i2s, checksums, magix bytes, qemu, eclipser, short fuzzing runs,\n2020 - efficient binary-level coverage analysis\ntags: bcov, detour + trampoline, basic block coverage, sliced microexecution, superblocks, strongly connected components, dominator graph, bap, angr, ida, dynamorio, intel pi, bap, angr, ida, dynamorio, intel pin\n2020 - test-case reduction via test-case generation: insights from the hypothesis reducer\ntags: test case reducer, property based testing, csmith, test case generation, hierachical delta debugging\n2020 - afl++: combining incremental steps of fuzzing research\ntags: afl++, afl, mopt, laf-intel, fuzzbench, ngram, redqueen, unicorn, qbdi, cmplog, aflfast\n2020 - firmxray: detecting bluetooth link layer vulnerabilities from bare-metal firmware\ntags: ghdira, static analysis, sound disassembly, base address finder, ble, vulnerability discovery\n2020 - p2im: scalable and hardware-independent firmware testing via automatic peripheral interface modeling\ntags: halucinator, emulation, firmware, qemu, afl, requires source, mcu, peripheral abstraction\n2020 - what exactly determines the type? inferring types with context\ntags: context assisted type inference, stripped binaries, variable and type reconstruction, ida pro, word2vec, cnn,\n2020 - causal testing: understanding defects\u2019 root causes\ntags: defects4j, causal relationships, eclipse plugin, unit test mutation, program trace diffing, static value diffing, user study\n2020 - aurora: statistical crash analysis for automated root cause explanation\ntags: rca, program traces, input diversification, intel pin, rust, cfg,\n2020 - parmesan: sanitizer-guided greybox fuzzing\ntags: interprocedural cfg, data flow analysis, directed fuzzing (dgf), disregarding 'hot paths', lava-m based primitives, llvm, angora, aflgo, asap, santizer dependent\n2020 - magma: a ground-truth fuzzing benchmark\ntags: best practices, fuzzer benchmarking, ground truth, lava-m\n2020 - fitness guided vulnerability detection with greybox fuzzing\ntags: afl, vuln specific fitness metric (headroom), buffer/integer overflow detection, aflgo, pointer analysis, cil, bad benchmarking\n2020 - greyone: data flow sensitive fuzzing\ntags: data-flow fuzzing, taint-guided mutation, input prioritization, constraint conformance, redqueen, good evaluation, vuzzer\n2020 - fairfuzz-tc: a fuzzer targeting rare -----> branches !!! \ntags: afl, required seeding, branch mask\n2020 - fitness guided vulnerability detection with greybox fuzzing\ntags: afl, vuln specific fitness metric (headroom), buffer/integer overflow detection, aflgo, pointer analysis, cil, bad evaluation\n2020 - tofu: target-oriented fuzzer\ntags: dgf, structured mutations, staged fuzzing/learning of cli args, target fitness, structure aware, dijkstra for priority, aflgo, superion\n2020 - fuzzan: efficient sanitizer metadata design for fuzzing\ntags:: sanitizer metadata, optimization, asan, msan, afl\n2020 - boosting fuzzer efficiency: an information theoretic perspective\ntags:: shannon entropy, seed power schedule, libfuzzer, active slam, dgf, fuzzer efficiency\n2020 - learning input tokens for effective fuzzing\ntags: dynamic taint tracking, parser checks, magic bytes, creation of dict inputs for fuzzers\n2020 - a review of memory errors exploitation in x86-64\ntags: nx, canaries, aslr, new mitigations, mitigation evaluation, recap on memory issues\n2020 - sok: the progress, challenges, and perspectives of directed greybox fuzzing\ntags: sok, directed grey box fuzzing, afl, afl mutation operators, dgf vs cgf\n2020 - memlock: memory usage guided fuzzing\ntags: memory consumption, afl, memory leak, uncontrolled-recursion, uncontrolled-memory-allocation, static analysis\n2019 - matryoshka: fuzzing deeply nested -----> branches !!! \ntags: afl, qsym, angora, path constraints, nested conditionals, (post) dominator trees, gradient descent, redqueen, lava-m\n2019 - building fast fuzzers\ntags: grammar based fuzzing, optimization, bold claims, comparison to badly/non-optimized fuzzers, python, lots of micro-optimizations, nice protocolling of failures, bad asm optimization\n2019 - not all bugs are the same: understanding, characterizing, and classifying the root cause of bugs\ntags: rca via bug reports, classification model, f score,\n2019 - antifuzz: impeding fuzzing audits of binary executables\ntags: anti fuzzing, prevent crashes, delay executions, obscure coverage information, overload symbolic execution\n2019 - mopt: optimized mutation scheduling for fuzzers\ntags: mutation scheduling, particle swarm optimization (pso), afl, afl mutation operators, vuzzer,\n2019 - fuzzfactory: domain-specific fuzzing with waypoints\ntags: domain-specific fuzzing, afl, llvm, solve hard constraints like cmp, find dynamic memory allocations, binary-based\n2019 - fuzzing file systems via two-dimensional input space exploration\ntags: ubuntu, file systems, library os, ext4, brtfs, meta block mutations, edge cases\n2019 - redqueen: fuzzing with input-to-state correspondence\ntags: feedback-driven, afl, magic-bytes, nested contraints, input-to-state correspondence, i2s\n2019 - periscope: an effective probing and fuzzing framework for the hardware-os boundary\ntags: kernel, android, userland, embedded, hardware, linux, device driver, wifi\n2019 - firmfuzz: automated iot firmware introspection and analysis\ntags: emulation, firmadyne, bof, xss, ci, npd, semi-automatic\n2019 - firm-afl: high-throughput greybox fuzzing of iot firmware via augmented process emulation\ntags: emulation, qemu, afl, full vs user mode, syscall redirect, \"augmented process emulation\", firmadyne\n2018 - a survey of automated root cause analysisof software vulnerability\ntags: exploit mitigations, fuzzing basics, symbolic execution basics, fault localization, high level\n2018 - phasar: an inter-procedural static analysis framework for c/c++\ntags: llvm, (inter-procedural) data-flow analysis, call-graph, points-to, class hierachy, cfg, ir\n2018 - instrim: lightweight instrumentation for coverage-guided fuzzing\ntags: llvm, instrumentation optimization, graph algorithms, selective instrumentation, coverage calculation\n2018 - what you corrupt is not what you crash: challenges in fuzzing embedded devices\ntags: embedded, challenges, heuristics, emulation, crash classification, fault detection\n2018 - evaluating fuzz testing\ntags: fuzzing evaluation, good practices, bad practices\n2017 - root cause analysis of software bugs using machine learning techniques\ntags: ml, rc prediction for filed bug reports, unsupervised + supervised combination, rc categorisation, f score\n2017 - kafl: hardware-assisted feedback fuzzing for os kernels\ntags: intel pt, kernel, afl, file systems, windows, ntfs, linux, ext, macos, apfs, driver, feedback-driven\n2016 - driller: argumenting fuzzing through selective symbolic execution\ntags: darpa, cgc, concolic execution, hybrid fuzzer, binary based\n2015 - challenges with applying vulnerability prediction models\ntags: vpm vs dpm, prediction models on large scale systems, files with frequent changes leave more vulns, older code exhibits more vulns\n2014 - optimizing seed selection for fuzzing\ntags: bff, (weighted) minset, peach, cover set problem, seed transferabilty, time minset, size minset, round robin\n2013 - automatic recovery of root causes from bug-fixing changes\ntags: ml + sca, f score, ast, ppa, source tree analysis\nunread\nunread papers categorized by a common main theme.\ngeneral fuzzing implementations\n2022 - beacon : directed grey-box fuzzing with provable path pruning\n2021 - same coverage, less bloat: accelerating binary-only fuzzing with coverage-preserving coverage-guided tracing\n2021 - facilitating parallel fuzzing with mutually-exclusive task distribution\n2021 - pata: fuzzing with path aware taint analysis\n2021 - bsod: binary-only scalable fuzzing of device drivers\n2021 - fuzzbench: an open fuzzer benchmarking platform and service\n2021 - my fuzzer beats them all! developing a framework for fair evaluation and comparison of fuzzers\n2021 - scalable fuzzing of program binaries with e9afl\n2021 - bigmap: future-proofing fuzzers with efficient large maps\n2021 - token-level fuzzing\n2021 - hashing fuzzing: introducing input diversity to improve crash detection\n2021 - leansym: efficient hybrid fuzzing through conservative constraint debloating\n2021 - esrfuzzer: an enhanced fuzzing framework for physical soho router devices to discover multi-type vulnerabilities\n2021 - firm-cov: high-coverage greybox fuzzing for iot firmware via optimized process emulation\n2021 - kcfuzz: directed fuzzing based on keypoint coverage\n2021 - tcp-fuzz: detecting memory and semantic bugs in tcp stacks with fuzzing\n2021 - fuzzing with optimized grammar-aware mutation strategies\n2021 - directed fuzzing for use-after-freevulnerabilities detection\n2021 - rapidfuzz: accelerating fuzzing via generative adversarial networks\n2021 - difuzzrtl: differential fuzz testing to findcpu bugs\n2021 - z-fuzzer: device-agnostic fuzzing of zigbee protocol implementation\n2021 - fuzzing with multi-dimensional control of mutation strategy\n2021 - using a guided fuzzer and preconditions to achieve branch coverage with valid inputs\n2021 - riff: reduced instruction footprint for coverage-guided fuzzing\n2021 - cocofuzzing: testing neural code models with coverage-guided fuzzing\n2021 - seed selection for successful fuzzing\n2021 - gramatron: effective grammar-aware fuzzing\n2021 - hyntrospect: a fuzzer for hyper-v devices\n2021 - fuzzolic: mixing fuzzing and concolic execution\n2021 - qfuzz: quantitative fuzzing for side channels\n2021 - revizor: fuzzing for leaks in black-box cpus\n2021 - unleashing fuzzing through comprehensive, efficient, and faithful exploitable-bug exposing\n2021 - constraint-guided directed greybox fuzzing\n2021 - test-case reduction and deduplication almost forfree with transformation-based compiler testing\n2021 - rulf: rust library fuzzing via api dependency graph traversal\n2021 - stochfuzz: sound and cost-effective fuzzing of stripped binaries by incremental and stochastic rewriting\n2021 - ps-fuzz: efficient graybox firmware fuzzing based on protocol state\n2021 - mudelta: delta-oriented mutation testing at commit time\n2021 - collabfuzz: a framework for collaborative fuzzing\n2021 - mutagen: faster mutation-based random testing\n2021 - inducing subtle mutations with program repair\n2021 - differential analysis of x86-64 instruction decoders\n2021 - on introducing automatic test case generation in practice: a success story and lessons learned\n2021 - a priority based path searching method for improving hybrid fuzzing\n2021 - intelligen: automatic driver synthesis for fuzz testing\n2021 - iclibfuzzer: isolated-context libfuzzer for improving fuzzer comparability\n2021 - sn4ke: practical mutation testing at binary level\n2021 - one engine to fuzz \u2019em all: generic language processor testing with semantic validation\n2021 - growing a test corpus with bonsai fuzzing\n2021 - fuzzing symbolic expressions\n2021 - jmpscare: introspection for binary-only fuzzing\n2021 - an improved directed grey-box fuzzer\n2021 - a binary protocol fuzzing method based on seqgan\n2021 - refined grey-box fuzzing with sivo\n2021 - psofuzzer: a target-oriented software vulnerability detection technology based on particle swarm optimization\n2021 - moofuzz: many-objective optimization seed schedule for fuzzer\n2021 - cmfuzz: context-aware adaptive mutation for fuzzers\n2021 - gtfuzz: guard token directed grey-box fuzzing\n2021 - profuzzbench: a benchmark for stateful protocol fuzzing\n2021 - symqemu:compilation-based symbolic execution for binaries\n2021 - concolic execution tailored for hybrid fuzzing thesis\n2021 - breaking through binaries: compiler-quality instrumentationfor better binary-only fuzzing\n2021 - alphafuzz: evolutionary mutation-based fuzzing as monte carlo tree search\n2020 - fuzzing with fast failure feedback\n2020 - lafuzz: neural network for efficient fuzzing\n2020 - maxafl: maximizing code coverage with a gradient-based optimization technique\n2020 - program state abstraction for feedback-driven fuzz testing using likely invariants\n2020 - pmfuzz: test case generation for persistent memory programs\n2020 - fusebmc: a white-box fuzzer for finding security vulnerabilities in c programs\n2020 - integrity: finding integer errors by targeted fuzzing\n2020 - confuzz: coverage-guided property fuzzing for event-driven programs\n2020 - aflturbo: speed up path discovery for greybox fuzzing\n2020 - fuzzing channel-based concurrency runtimes using types and effects\n2020 - defuzz: deep learning guided directed fuzzing\n2020 - crfuzz: fuzzing multi-purpose programs through inputvalidation\n2020 - epfuzzer: improving hybrid fuzzing with hardest-to-reach branch prioritization\n2020 - fuzzing based on function importance by attributed call graph\n2020 - unifuzz: a holistic and pragmatic metrics-driven platform for evaluating fuzzers\n2020 - pathafl: path-coverage assisted fuzzing\n2020 - path sensitive fuzzing for native applications\n2020 - unifuzz: optimizing distributed fuzzing via dynamic centralized task scheduling\n2020 - fuzzing error handling code using context-sensitive software fault injection\n2020 - specfuzz: bringing spectre-type vulnerabilities to the surface\n2020 - zeror: speed up fuzzing with coverage-sensitive tracing and scheduling\n2020 - muzz: thread-aware grey-box fuzzing for effective bug hunting in multithreaded programs\n2020 - evolutionary grammar-based fuzzing\n2020 - aflpro: direction sensitive fuzzing\n2020 - csi-fuzz: full-speed edge tracing using coverage sensitive instrumentation\n2020 - scalable greybox fuzzing for effective vulnerability management diss\n2020 - hotfuzz discovering algorithmic denial-of-service vulnerabilities through guided micro-fuzzing\n2020 - fuzzing binaries for memory safety errors with qasan\n2020 - suzzer: a vulnerability-guided fuzzer based on deep learning\n2020 - ijon: exploring deep state spaces via fuzzing\n2020 - binary-level directed fuzzing for use-after-free vulnerabilities\n2020 - aflnet: a greybox fuzzer for network protocols\n2020 - pangolin: incremental hybrid fuzzing with polyhedral path abstraction\n2020 - uefi firmware fuzzing with simics virtual platform\n2020 - finding security vulnerabilities in network protocol implementations\n2020 - typestate-guided fuzzer for discovering use-after-free vulnerabilities\n2020 - fuzzguard: filtering out unreachable inputs in directed grey-box fuzzing through deep learning\n2020 - hydiff: hybrid differential software analysis\n2019 - engineering a better fuzzer with synergicallyintegrated optimizations\n2019 - superion: grammar-aware greybox fuzzing\n2019 - profuzzer: on-the-fly input type probing for better zero-day vulnerability discovery\n2019 - grimoire: synthesizing structure while fuzzing\n2019 - ptrix: efficient hardware-assisted fuzzing for cots binary\n2019 - savior: towards bug-driven hybrid testing\n2019 - fudge: fuzz driver generation at scale\n2019 - nautilus: fishing for deep bugs with grammars\n2019 - send hardest problems my way: probabilistic path prioritization for hybrid fuzzing\n2019 - enfuzz: ensemble fuzzing with seed synchronization among diverse fuzzers\n2018 - fuzz testing in practice: obstacles and solutions\n2018 - pafl: extend fuzzing optimizations of single mode to industrial parallel mode\n2018 - ptfuzz: guided fuzzing with processor trace feedback\n2018 - angora: efficient fuzzing by principled search\n2018 - fairfuzz: a targeted mutation strategy for increasing greybox fuzz testing coverage\n2018 - neuzz: efficient fuzzing with neural program smoothing\n2018 - collafl: path sensitive fuzzing\n2018 - full-speed fuzzing: reducing fuzzing overhead through coverage-guided tracing\n2018 - qsym: a practical concolic execution engine tailored for hybrid fuzzing\n2018 - coverage-based greybox fuzzing as markov chain\n2018 - moonshine: optimizing os fuzzer seed selection with trace distillation\n2018 - singularity: pattern fuzzing for worst case complexity\n2018 - smart greybox fuzzing\n2018 - hawkeye: towards a desired directed grey-box fuzzer\n2018 - perffuzz: automatically generating pathological inputs\n2018 - fairfuzz: a targeted mutation strategy for increasing greybox fuzz testing coverage\n2018 - enhancing memory error detection forlarge-scale applications and fuzz testing\n2018 - t-fuzz: fuzzing by program transformation\n2017 - evaluating and improving fault localization\n2017 - imf: inferred model-based fuzzer\n2017 - synthesizing program input grammars\n2017 - stateful fuzzing of wireless device drivers in an emulated environment\n2017 - steelix: program-state based binary fuzzing\n2017 - designing new operating primitives to improvefuzzing performance\n2017 - vuzzer: application-aware evolutionary fuzzing\n2017 - difuze: interface aware fuzzing for kernel drivers\n2017 - instruction punning: lightweight instrumentation for x86-64\n2017 - designing new operating primitives to improve fuzzing performance\n2014 - a large-scale analysis of the security of embedded firmwares\n2013 - scheduling black-box mutational fuzzing\n2013 - dowsing for overflows: a guided fuzzer to find buffer boundary violations\n2013 - rpfuzzer: a framework for discovering router protocols vulnerabilities based on fuzzing\n2011 - offset-aware mutation based fuzzing for buffer overflow vulnerabilities: few preliminary results\n2010 - taintscope: a checksum-aware directed fuzzing tool for automatic software vulnerability detection\n2009 - taint-based directed whitebox fuzzing\n2009 - dynamic test generation to find integer bugs in x86 binary linux programs\n2008 - grammar-based whitebox fuzzing\n2008 - vulnerability analysis for x86 executables using genetic algorithm and fuzzing\n2008 - fuzzing wi-fi drivers to locate security vulnerabilities\n2008 - klee: unassisted and automatic generation of high-coverage tests for complex systems programs\n2008 - automated whitebox fuzz testing\n2005 - dart: directed automated random testing\n1994 - dominators, super blocks, and program coverage\niot fuzzing\n2022 - fuzzware: using precise mmio modeling for effective firmware fuzzing\n2021 - large-scale firmware vulnerability analysis based on code similarity\n2021 - towards fast and scalable firmware fuzzing with dual-level peripheral modeling\n2021 - zero wfuzzer: target-oriented fuzzing for web interface of embedded devices\n2021 - stfuzzer: contribution-aware coverage-guided fuzzing for smart devices\n2021 - rtkaller: state-aware task generation for rtos fuzzing\n2021 - ifizz: deep-state and efficient fault-scenario generation to test iot firmware\n2021 - automatic vulnerability detection in embedded devices and firmware: survey and layered taxonomies\n2021 - fuzzing the internet of things: a review on the techniques and challenges for efficient vulnerability discovery in embedded systems\n2020 - verification of embedded software binaries using virtual prototypes\n2020 - \u03bcsbs: static binary sanitization of bare-metal embedded devices forfault observability\n2020 - device-agnostic firmware execution is possible: a concolic execution approach for peripheral emulation\n2020 - vulnerability detection in siot applications: a fuzzing method on their binaries\n2020 - firmae: towards large-scale emulation of iot firmware fordynamic analysis\n2020 - firmnano: toward iot firmware fuzzing through augmented virtual execution\n2020 - arm-afl: coverage-guided fuzzing framework for arm-based iot devices\n2020 - bug detection in embedded environments by fuzzing and symbolic execution\n2020 - firmxray: detecting bluetooth link layer vulnerabilities from bare-metal firmware\n2020 - em-fuzz: augmented firmware fuzzing via memory checking\n2020 - verification of embedded binaries using coverage-guided fuzzing with system c-based virtual prototypes\n2020 - dice: automatic emulation of dma input channels for dynamic firmware analysis\n2020 - fw\u2010fuzz: a code coverage\u2010guided fuzzing framework for network protocols on firmware\n2020 - taint-driven firmware fuzzing of embedded systems thesis\n2020 - a dynamic instrumentation technology for iot devices\n2020 - vulcan: a state-aware fuzzing tool for wear os ecosystem\n2020 - a novel concolic execution approach on embedded device\n2020 - hfuzz: towards automatic fuzzing testing of nb-iot core network protocols implementations\n2020 - firmcorn: vulnerability-oriented fuzzing of iot firmware via optimized virtual execution\n2018 - iotfuzzer: discovering memory corruptions in iot through app-based fuzzing\n2017 - towards automated dynamic analysis for linux-based embedded firmware\n2016 - scalable graph-based bug search for firmware images\n2015 - surrogates: enabling near-real-time dynamic analyses of embedded systems\n2015 - firmalice - automatic detection of authentication bypass vulnerabilities in binary firmware\n2014 - a large-scale analysis of the security of embedded firmwares\n2013 - rpfuzzer: a framework for discovering router protocols vulnerabilities based on fuzzing\nfirmware emulation\n2021 - firmguide: boosting the capability of rehosting embedded linux kernels through model-guided kernel execution\n2021 - automatic firmware emulation through invalidity-guided knowledge inference(extended version)\n2021 - firmware re-hosting through static binary-level porting\n2021 - jetset: targeted firmware rehosting for embedded systems\n2021 - automatic firmware emulation through invalidity-guided knowledge inference\nnetwork fuzzing\n2021 - stateafl: greybox fuzzing for stateful network servers\nkernel fuzzing\n2021 - achyb: a hybrid analysis approach to detect kernel access control vulnerabilities\n2021 - cvfuzz: detecting complexity vulnerabilities in opencl kernels via automated pathological input generation\n2021 - healer: relation learning guided kernel fuzzing\n2021 - syzvegas: beating kernel fuzzing odds with reinforcement learning\n2021 - ntfuzz: enabling type-aware kernel fuzzing on windows with static binary analysis\n2021 - undo workarounds for kernel bugs\n2020 - a hybrid interface recovery method for android kernels fuzzing\n2020 - finding race conditions in kernels:from fuzzing to symbolic execution - thesis\n2020 - agamotto: accelerating kernel driver fuzzing with lightweight virtual machine checkpoints\n2020 - x-afl: a kernel fuzzer combining passive and active fuzzing\n2020 - identification of kernel memory corruption using kernel memory secret observation mechanism\n2020 - hfl: hybrid fuzzing on the linux kernel\n2020 - realistic error injection for system calls\n2020 - krace: data race fuzzing for kernel file systems\n2020 - usbfuzz: a framework for fuzzing usb drivers by device emulation\n2019 - fuzzing file systems via two-dimensional input space exploration\n2019 - razzer: finding kernel race bugs through fuzzing\n2019 - unicorefuzz: on the viability of emulation for kernel space fuzzing\n2017 - stateful fuzzing of wireless device drivers in an emulated environment\n2017 - difuze: interface aware fuzzing for kernel drivers\n2008 - fuzzing wi-fi drivers to locate security vulnerabilities\nformat specific fuzzing\n2021 - v-shuttle: scalable and semantics-aware hypervisor virtual device fuzzing\n2021 - formatfuzzer: effective fuzzing of binary file formats\n2020 - nyx: greybox hypervisor fuzzing using fast snapshots and affine types\n2020 - tree2tree structural language modeling for compiler fuzzing\n2020 - detecting critical bugs in smt solvers using blackbox mutational fuzzing\n2020 - js engine - montage: a neural network language model-guided javascript engine fuzzer\n2020 - js engine - fuzzing javascript engines with aspect-preserving mutation\n2020 - cuda compiler - cudasmith: a fuzzer for cuda compilers\n2020 - smart contracts - sfuzz: an efficient adaptive fuzzer for solidity smart contracts\n2019 - compiler fuzzing: how much does it matter?\n2019 - smart contracts - harvey: a greybox fuzzer for smart contracts\n2017 - xml - skyfire: data-driven seed generation for fuzzing\nexploitation\n2021 - a novel method for the automatic generation of jop chain exploits\n2021 - v0finder: discovering the correct origin of publicly reported software vulnerabilities\n2021 - identifying valuable pointers in heap data\n2021 - octopocs: automatic verification of propagated vulnerable code using reformed proofs of concept\n2021 - characterizing vulnerabilities in a major linux distribution\n2021 - maze: towards automated heap feng shui\n2021 - vulnerability detection in c/c++ source code with graph representation learning\n2021 - mallotropism: a metamorphic engine for malicious software variation development\n2020 - automatic techniques to systematically discover new heap exploitation primitives\n2020 - shadow-heap: preventing heap-based memory corruptions by metadata validation\n2020 - practical fine-grained binary code randomization\n2020 - tiny-cfa: minimalistic control-flow attestation usingverified proofs of execution\n2020 - greybox automatic exploit generation for heap overflows in language interpreters - phd thesis\n2020 - abcfi: fast and lightweight fine-grained hardware-assisted control-flow integrity\n2020 - heapexpo: pinpointing promoted pointers to prevent use-after-free vulnerabilities\n2020 - localizing patch points from one exploit\n2020 - speculative dereferencing of registers: reviving foreshadow\n2020 - haepg: an automatic multi-hop exploitation generation framework\n2020 - exploiting more binaries by using planning to assemble rop exploiting more binaries by using planning to assemble rop attacks attacks\n2020 - ropminer: learning-based static detection of rop chain considering linkability of rop gadgets\n2020 - koobe: towards facilitating exploit generation of kernel out-of-bounds write vulnerabilities\n2020 - preventing return oriented programming attacks by preventing return instruction pointer overwrites\n2020 - kaslr: break it, fix it, repeat\n2020 - shadowguard : optimizing the policy and mechanism of shadow stack instrumentation using binary static analysis\n2020 - vulhunter: an automated vulnerability detection system based on deep learning and bytecode\n2020 - analysis and evaluation of ropinjector\n2020 - api misuse detection in c programs: practice on ssl apis\n2020 - koobe: towards facilitating exploit generation of kernel out-of-bounds write vulnerabilities\n2020 - egalito: layout-agnostic binary recompilation\n2020 - verifying software vulnerabilities in iot cryptographic protocols\n2020 - \u03bcrai: securing embedded systems with return address integrity\n2020 - preventing return oriented programming attacks by preventing return instruction pointer overwrites\n2019 - kernel protection against just-in-time code reuse\n2019 - kernel exploitation via uninitialized stack\n2019 - kepler: facilitating control-flow hijacking primitive evaluation for linux kernel vulnerabilities\n2019 - slake: facilitating slab manipulation for exploiting vulnerabilities in the linux kernel\n2018 - heaphopper: bringing bounded model checkingto heap implementation security\n2018 - k-miner: uncovering memory corruption in linux\n2017 - hait: heap analyzer with input tracing\n2017 - drop the rop: fine-grained control-flow integrity for the linux kernel\n2017 - kr^x: comprehensive kernel protection against just-in-time code reuse\n2017 - unleashing use-before-initialization vulnerabilities in the linux kernel using targeted stack spraying\n2017 - towards automated dynamic analysis for linux-based embedded firmware\n2016 - scalable graph-based bug search for firmware images\n2015 - cross-architecture bug search in binary executables\n2015 - surrogates: enabling near-real-time dynamic analyses of embedded systems\n2015 - from collision to exploitation: unleashing use-after-free vulnerabilities in linux kernel\n2015 - pie: parser identification in embedded systems\n2014 - ret2dir: rethinking kernel isolation\n2014 - make it work, make it right, make it fast: building a platform-neutral whole-system dynamic binary analysis platform\n2012 - anatomy of a remote kernel exploit\n2012 - a heap of trouble: breaking the linuxkernel slob allocator\n2011 - linux kernel vulnerabilities: state-of-the-art defenses and open problems\n2011 - protecting the core: kernel exploitation mitigations\n2015 - from collision to exploitation: unleashing use-after-free vulnerabilities in linux kernel\n2014 - ret2dir: rethinking kernel isolation\n2012 - anatomy of a remote kernel exploit\n2012 - a heap of trouble: breaking the linux kernel slob allocator\n2011 - linux kernel vulnerabilities: state-of-the-art defenses and open problems\n2011 - protecting the core: kernel exploitation mitigations\nstatic binary analysis\n2021 - viva: binary level vulnerability identification via partial signature\n2021 - overview of the advantages and disadvantages of static code analysis tools\n2021 - multi-level cross-architecture binary code similarity metric\n2020 - vuldetector: detecting vulnerabilities using weighted feature graph comparison\n2020 - deepbindiff: learning program-wide code representations for binary diffing\n2020 - bindeep: a deep learning approach to binary code similarity detection\n2020 - revisiting binary code similarity analysis using interpretable feature engineering and lessons learned\n2020 - idea: static analysis on the security of apple kernel drivers\n2020 - hart: hardware-assisted kernel module tracing on arm\n2020 - an approach to comparing control flow graphs based on basic block matching\n2020 - how far we have come: testing decompilation correctness of c decompilers\n2020 - dynamic binary lifting and recompilation diss\n2020 - similarity based binary backdoor detection via attributed control flow graph\n2020 - iotsit: a static instrumentation tool for iot devices\n2019 - code similarity detection using ast and textual information\n2018 - codex: source code plagiarism detectionbased on abstract syntax trees\n2017 - rev.ng: a unified binary analysis framework to recover cfgs and function boundaries\n2017 - angr: the next generation of binary analysis\n2016 - binary code is not easy\n2015 - cross-architecture bug search in binary executables\n2014 - a platform for secure static binary instrumentation\n2013 - mil: a language to build program analysis tools through static binary instrumentation\n2013 - binary code analysis\n2013 - a compiler-level intermediate representation based binary analysis and rewriting system\n2013 - protocol reverse engineering through dynamic and static binary analysis\n2013 - binarypig: scalable static binary analysis over hadoop\n2011 - bap: a binary analysis platform\n2009 - syntax tree fingerprinting for source code similarity detection\n2008 - bitblaze: a new approach to computer security via binary analysis\n2005 - practical analysis of stripped binary code\n2004 - detecting kernel-level rootkits through binary analysis\nmisc\n2021 - yzgen: automated generation of syscall specification of closed-source macos drivers\n2021 - igor: crash deduplication through root-cause clustering\n2021 - uafsan: an object-identifier-based dynamic approach for detecting use-after-free vulnerabilities\n2021 - syml: guiding symbolic execution toward vulnerable states through pattern learning\n2021 - llsc: a parallel symbolic execution compiler for llvm ir\n2021 - fuzzsplore: visualizing feedback-driven fuzzing techniques\n2020 - memory error detection based on dynamic binary translation\n2020 - sydr: cutting edge dynamic symbolic execution\n2020 - drpin: a dynamic binary instumentator for multiple processor architectures\n2020 - mvp: detecting vulnerabilities using patch-enhanced vulnerability signatures\n2020 - collecting vulnerable source code from open-source repositories for dataset generation\n2020 - leopard: identifying vulnerable code for vulnerability assessment through program metrics\n2020 - dynamic program analysis tools in gcc and clang compilers\n2020 - on using k-means clustering for test suite reduction\n2020 - optimizing the parameters of an evolutionary algorithm for fuzzing and test data generation\n2020 - inputs from hell: learning input distributions for grammar-based test generation\n2020 - idsan: an identity-based memory sanitizer for fuzzing binaries\n2020 - an experimental study oncombining automated andstochastic test data generation - master thesis\n2020 - fuzzgen: automatic fuzzer generation\n2020 - fuzzing: on the exponential cost of vulnerability discovery\n2020 - poster: debugging inputs\n2020 - api misuse detection in c programs: practice on ssl apis\n2020 - egalito: layout-agnostic binary recompilation\n2020 - verifying software vulnerabilities in iot cryptographic protocols\n2020 - \u03bcrai: securing embedded systems with return address integrity\n2020 - fast bit-vector satisfiability\n2020 - mardu: efficient and scalable code re-randomization\n2020 - towards formal verification of iot protocols: a review\n2020 - automating the fuzzing triage process\n2020 - comparing afl scalability in virtual-and native environment\n2020 - symbion: interleaving symbolic with concrete execution\n2020 - not all coverage measurements are equal: fuzzing by coverage accounting for input prioritization\n2019 - toward the analysis of embedded firmware through automated re-hosting\n2019 - fuzzification: anti-fuzzing techniques\n2018 - vulinoss: a dataset of security vulnerabilities in open-source systems\n2018 - hddr: a recursive variantof the hierarchical delta debugging algorithm\n2017 - coarse hierarchical delta debugging\n2017 - vuddy: a scalable approach for vulnerable codeclone discovery\n2017 - postmortem program analysis with hardware-enhanced post-crash artifacts\n2017 - synthesizing program input grammars\n2017 - designing new operating primitives to improve fuzzing performance\n2017 - instruction punning: lightweight instrumentation for x86-64\n2016 - modernizing hierarchical delta debugging\n2016 - vulpecker: an automated vulnerability detection systembased on code similarity analysis\n2016 - credal: towards locating a memory corruption vulnerability with your core dump\n2016 - retracer: triaging crashes by reverse execution frompartial memory dumps\n2015 - pie: parser identification in embedded systems\n2010 - iterative delta debugging\n2009 - dynamic test generation to find integer bugs in x86 binary linux programs\n2006 - hdd: hierarchical delta debugging\nsurveys, soks, and studies\n2021 - exploratory review of hybrid fuzzing for automated vulnerability detection\n2021 - a systematic review of network protocol fuzzing techniques\n2021 - vulnerability detection is just the beginning\n2021 - evaluating synthetic bugs\n2020 - a practical, principled measure of fuzzer appeal:a preliminary study\n2020 - a systemic review of kernel fuzzing\n2020 - a survey of hybrid fuzzing based on symbolic execution\n2020 - a study on using code coverage information extracted from binary to guide fuzzing\n2020 - study of security flaws in the linux kernel by fuzzing\n2020 - dynamic vulnerability detection approaches and tools: state of the art\n2020 - fuzzing: challenges and reflections\n2020 - the relevance of classic fuzz testing: have we solved this one?\n2020 - a practical, principled measure of fuzzer appeal:a preliminary study\n2020 - sok: all you ever wanted to know about x86/x64 binary disassembly but were afraid to ask\n2020 - a quantitative comparison of coverage-based greybox fuzzers\n2020 - a survey of security vulnerability analysis, discovery, detection, and mitigation on iot devices\n2020 - a systematic review of fuzzing based on machine learning techniques\n2019 - a survey of binary code similarity\n2019 - the art, science, and engineering of fuzzing: a survey\n2012 - regression testingminimization, selection and prioritization: a survey", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000132, "year": null}, {"Unnamed: 0": 156, "autor": 156, "date": null, "content": "HiveMQ Community Edition\nHiveMQ CE is a Java-based open source MQTT broker that fully supports MQTT 3.x and MQTT 5.\nHiveMQ CE is the foundation of the HiveMQ enterprise-connectivity and messaging platform and implements all MQTT features. This project is the technical core of many large MQTT deployments and is now available as open source software under the Apache 2 license.\nWebsite: https://www.hivemq.com/\nDocumentation: https://github.com/hivemq/hivemq-community-edition/wiki\nCommunity Forum: https://community.hivemq.com/\nContribution guidelines: Contributing.adoc\nLicense: The source files in this repository are made available under the Apache License Version 2.0.\nFeatures\nAll MQTT 3.1, 3.1.1 and MQTT 5.0 features\nMQTT over TCP, TLS, WebSocket and Secure WebSocket transport\nJava Extension SDK for:\nAuthentication\nAuthorization\nClient Initializers\nMQTT Packet Interceptors\nInteracting with Publishes, Retained Messages, Clients and Subscriptions\nRunning on Windows, Linux and MacOS (Linux is recommended)\nEmbedded Mode\nHiveMQ CE is compatible with all MQTT 3 and MQTT 5 clients, including Eclipse Paho and HiveMQ MQTT Client.\nDocumentation\nThe documentation for the HiveMQ CE can be found here.\nMQTT Resources\nMQTT Essentials\nMQTT 5 Essentials\nHiveMQ Community Forum\nThe ideal place for questions or discussions about the HiveMQ Community Edition is our brand new HiveMQ Community Forum.\nHow to Use\nQuick Start\nDownload the latest HiveMQ CE binary package.\nUnzip the package.\nRun the run.sh (Linux/OSX) or run.bat (Windows) in the bin folder of the package.\ncd hivemq-ce-<version>\nbin/run.sh\nImportant\nAt least Java version 11 is required to run HiveMQ CE. If you are in doubt, you can check the installed Java version by entering java -version on your command line.\nYou can now connect MQTT clients to <ip address>:1883.\nCaution\nIf you want to connect devices on external networks to HiveMQ CE, please make sure your server is reachable from those networks and the required ports (default: 1883) are accessible through your firewall.\nJust in Time Builds\nJust in time builds for current branches on this repository and for specific commits are available here.\nRun with Docker\nAll releases as well as the current state of the master branch are available in the hivemq/hivemq-ce repository on DockerHub. To execute this image, simply run the following command:\ndocker run --name hivemq-ce -d -p 1883:1883 hivemq/hivemq-ce\nTo change the default log level you can set the environment variable HIVEMQ_LOG_LEVEL when running the container:\ndocker run --name hivemq-ce -e HIVEMQ_LOG_LEVEL=INFO -d -p 1883:1883 hivemq/hivemq-ce\nBuilding from Source\nBuilding the Binary Package\nCheck out the git repository and build the binary package.\ngit clone https://github.com/hivemq/hivemq-community-edition.git\ncd hivemq-community-edition\n./gradlew hivemqZip\nThe package hivemq-ce-<version>.zip is created in the sub-folder build/zip/.\nBuilding the Docker Image\nCheck out the git repository and build the Docker image.\ngit clone https://github.com/hivemq/hivemq-community-edition.git\ncd hivemq-community-edition\ndocker/build.sh\nThe Docker image hivemq/hivemq-ce:snapshot is created locally.\nFor further development instructions see the contribution guidelines.\nEmbedded Mode\nHiveMQ Community Edition offers an embedded mode and a programmatic API for integrating with Java/Java EE software.\nGradle\nIf you use Gradle, include the following code in your build.gradle(.kts) file.\ndependencies {\nimplementation(\"com.hivemq:hivemq-community-edition-embedded:2021.3\")\n}\nMaven\nIf you use Maven, include the following code in your pom.xml file.\n<project>\n...\n<dependencies>\n<dependency>\n<groupId>com.hivemq</groupId>\n<artifactId>hivemq-community-edition-embedded</artifactId>\n<version>2021.3</version>\n</dependency>\n</dependencies>\n...\n</project>\nNote\nYou must set the compiler version to 11 or higher.\nUsage\nEntry into the embedded mode is done with the com.hivemq.embedded.EmbeddedHiveMQBuilder.\npublic class Main {\npublic static void main(String[] args) {\nfinal EmbeddedHiveMQBuilder embeddedHiveMQBuilder = EmbeddedHiveMQ.builder()\n.withConfigurationFolder(Path.of(\"/path/to/embedded-config-folder\"))\n.withDataFolder(Path.of(\"/path/to/embedded-data-folder\"))\n.withExtensionsFolder(Path.of(\"/path/to/embedded-extensions-folder\"));\n...\n}\n}\nOnce built, an EmbeddedHiveMQ can be started with start().\npublic class Main {\npublic static void main(String[] args) {\nfinal EmbeddedHiveMQBuilder embeddedHiveMQBuilder = EmbeddedHiveMQ.builder();\n...\ntry (final EmbeddedHiveMQ hiveMQ = embeddedHiveMQBuilder.build()) {\nhiveMQ.start().join();\n...\n} catch (final Exception ex) {\nex.printStackTrace();\n}\n}\n}\nA running EmbeddedHiveMQ can be stopped with stop().\npublic class Main {\npublic static void main(String[] args) {\n...\ntry (final EmbeddedHiveMQ hiveMQ = embeddedHiveMQBuilder.build()) {\n...\nhiveMQ.stop().join();\n} catch (final Exception ex) {\nex.printStackTrace();\n}\n}\n}\nSimilar to the embedded HiveMQ an embedded extension can be built with the com.hivemq.embedded.EmbeddedExtensionBuilder.\nThen add the embedded extension to the embedded HiveMQ builder.\npublic class Main {\npublic static void main(String[] args) {\nfinal EmbeddedExtension embeddedExtension = EmbeddedExtension.builder()\n.withId(\"embedded-ext-1\")\n.withName(\"Embedded Extension\")\n.withVersion(\"1.0.0\")\n.withPriority(0)\n.withStartPriority(1000)\n.withAuthor(\"Me\")\n.withExtensionMain(new MyEmbeddedExtensionMain())\n.build();\nfinal EmbeddedHiveMQBuilder builder = EmbeddedHiveMQ.builder()\n.withConfigurationFolder(Path.of(\"/path/to/embedded-config-folder\"))\n.withDataFolder(Path.of(\"/path/to/embedded-data-folder\"))\n.withExtensionsFolder(Path.of(\"/path/to/embedded-extensions-folder\"))\n.withEmbeddedExtension(embeddedExtension);\ntry (final EmbeddedHiveMQ hiveMQ = builder.build()) {\nhiveMQ.start().join();\n//do something with hivemq\n} catch (final Exception ex) {\nex.printStackTrace();\n}\n}\nprivate static class MyEmbeddedExtensionMain implements ExtensionMain {\n@Override\npublic void extensionStart(final @NotNull ExtensionStartInput extensionStartInput, final @NotNull ExtensionStartOutput extensionStartOutput) {\n// my extension start code\n}\n@Override\npublic void extensionStop(final @NotNull ExtensionStopInput extensionStopInput, final @NotNull ExtensionStopOutput extensionStopOutput) {\n// my extension stop code\n}\n}\n}\nNote\nAn EmbeddedHiveMQ is a resource that is similar to a e.g. network connection and implements the java.lang.AutoCloseable interface. Always use ARM (try with resources) or ensure a call to close().\nExclusions\nWhen you deploy an application that includes EmbeddedHiveMQ, it can be useful to exclude some dependencies. One way to exclude dependencies is with the maven shade plugin.\n<project>\n...\n<build>\n<plugins>\n<plugin>\n<groupId>org.apache.maven.plugins</groupId>\n<artifactId>maven-shade-plugin</artifactId>\n<version>3.2.1</version>\n<executions>\n<execution>\n<phase>package</phase>\n<goals>\n<goal>shade</goal>\n</goals>\n<configuration>\n<artifactSet>\n<excludes>\n<!--Exclude the undesired dependencies-->\n<exclude>org.rocksdb:rocksdbjni</exclude>\n<exclude>ch.qos.logback:logback-classic</exclude>\n</excludes>\n</artifactSet>\n<filters>\n<filter>\n<artifact>*:*</artifact>\n<excludes>\n<exclude>META-INF/*.SF</exclude>\n<exclude>META-INF/*.DSA</exclude>\n<exclude>META-INF/*.RSA</exclude>\n</excludes>\n</filter>\n</filters>\n</configuration>\n</execution>\n</executions>\n</plugin>\n</plugins>\n</build>\n...\n</project>\nRocksDB Exclusion\nTo exclude the org.rocksdb:rocksdbjni dependency, two internal configurations must be changed before you call start().\npublic class Main {\npublic static void main(String[] args) {\n...\ntry (final EmbeddedHiveMQ hiveMQ = embeddedHiveMQBuilder.build()) {\nInternalConfigurations.PAYLOAD_PERSISTENCE_TYPE.set(PersistenceType.FILE);\nInternalConfigurations.RETAINED_MESSAGE_PERSISTENCE_TYPE.set(PersistenceType.FILE);\nhiveMQ.start().join();\n...\n} catch (final Exception ex) {\nex.printStackTrace();\n}\n}\n}\nContributing\nIf you want to contribute to HiveMQ CE, see the contribution guidelines.\nLicense\nHiveMQ Community Edition is licensed under the APACHE LICENSE, VERSION 2.0. A copy of the license can be found here.", "link": "https://github.com/hivemq/hivemq-community-edition", "origin": "Github", "suborigin": "Iot", "result": true, "Selector": "branches", "selectorShort": "branch", "MarkedSent": "hivemq community edition\nhivemq ce is a java-based open source mqtt broker that fully supports mqtt 3.x and mqtt 5.\nhivemq ce is the foundation of the hivemq enterprise-connectivity and messaging platform and implements all mqtt features. this project is the technical core of many large mqtt deployments and is now available as open source software under the apache 2 license.\nwebsite: https://www.hivemq.com/\ndocumentation: https://github.com/hivemq/hivemq-community-edition/wiki\ncommunity forum: https://community.hivemq.com/\ncontribution guidelines: contributing.adoc\nlicense: the source files in this repository are made available under the apache license version 2.0.\nfeatures\nall mqtt 3.1, 3.1.1 and mqtt 5.0 features\nmqtt over tcp, tls, websocket and secure websocket transport\njava extension sdk for:\nauthentication\nauthorization\nclient initializers\nmqtt packet interceptors\ninteracting with publishes, retained messages, clients and subscriptions\nrunning on windows, linux and macos (linux is recommended)\nembedded mode\nhivemq ce is compatible with all mqtt 3 and mqtt 5 clients, including eclipse paho and hivemq mqtt client.\ndocumentation\nthe documentation for the hivemq ce can be found here.\nmqtt resources\nmqtt essentials\nmqtt 5 essentials\nhivemq community forum\nthe ideal place for questions or discussions about the hivemq community edition is our brand new hivemq community forum.\nhow to use\nquick start\ndownload the latest hivemq ce binary package.\nunzip the package.\nrun the run.sh (linux/osx) or run.bat (windows) in the bin folder of the package.\ncd hivemq-ce-<version>\nbin/run.sh\nimportant\nat least java version 11 is required to run hivemq ce. if you are in doubt, you can check the installed java version by entering java -version on your command line.\nyou can now connect mqtt clients to <ip address>:1883.\ncaution\nif you want to connect devices on external networks to hivemq ce, please make sure your server is reachable from those networks and the required ports (default: 1883) are accessible through your firewall.\njust in time builds\njust in time builds for current -----> branches !!!  on this repository and for specific commits are available here.\nrun with docker\nall releases as well as the current state of the master branch are available in the hivemq/hivemq-ce repository on dockerhub. to execute this image, simply run the following command:\ndocker run --name hivemq-ce -d -p 1883:1883 hivemq/hivemq-ce\nto change the default log level you can set the environment variable hivemq_log_level when running the container:\ndocker run --name hivemq-ce -e hivemq_log_level=info -d -p 1883:1883 hivemq/hivemq-ce\nbuilding from source\nbuilding the binary package\ncheck out the git repository and build the binary package.\ngit clone https://github.com/hivemq/hivemq-community-edition.git\ncd hivemq-community-edition\n./gradlew hivemqzip\nthe package hivemq-ce-<version>.zip is created in the sub-folder build/zip/.\nbuilding the docker image\ncheck out the git repository and build the docker image.\ngit clone https://github.com/hivemq/hivemq-community-edition.git\ncd hivemq-community-edition\ndocker/build.sh\nthe docker image hivemq/hivemq-ce:snapshot is created locally.\nfor further development instructions see the contribution guidelines.\nembedded mode\nhivemq community edition offers an embedded mode and a programmatic api for integrating with java/java ee software.\ngradle\nif you use gradle, include the following code in your build.gradle(.kts) file.\ndependencies {\nimplementation(\"com.hivemq:hivemq-community-edition-embedded:2021.3\")\n}\nmaven\nif you use maven, include the following code in your pom.xml file.\n<project>\n...\n<dependencies>\n<dependency>\n<groupid>com.hivemq</groupid>\n<artifactid>hivemq-community-edition-embedded</artifactid>\n<version>2021.3</version>\n</dependency>\n</dependencies>\n...\n</project>\nnote\nyou must set the compiler version to 11 or higher.\nusage\nentry into the embedded mode is done with the com.hivemq.embedded.embeddedhivemqbuilder.\npublic class main {\npublic static void main(string[] args) {\nfinal embeddedhivemqbuilder embeddedhivemqbuilder = embeddedhivemq.builder()\n.withconfigurationfolder(path.of(\"/path/to/embedded-config-folder\"))\n.withdatafolder(path.of(\"/path/to/embedded-data-folder\"))\n.withextensionsfolder(path.of(\"/path/to/embedded-extensions-folder\"));\n...\n}\n}\nonce built, an embeddedhivemq can be started with start().\npublic class main {\npublic static void main(string[] args) {\nfinal embeddedhivemqbuilder embeddedhivemqbuilder = embeddedhivemq.builder();\n...\ntry (final embeddedhivemq hivemq = embeddedhivemqbuilder.build()) {\nhivemq.start().join();\n...\n} catch (final exception ex) {\nex.printstacktrace();\n}\n}\n}\na running embeddedhivemq can be stopped with stop().\npublic class main {\npublic static void main(string[] args) {\n...\ntry (final embeddedhivemq hivemq = embeddedhivemqbuilder.build()) {\n...\nhivemq.stop().join();\n} catch (final exception ex) {\nex.printstacktrace();\n}\n}\n}\nsimilar to the embedded hivemq an embedded extension can be built with the com.hivemq.embedded.embeddedextensionbuilder.\nthen add the embedded extension to the embedded hivemq builder.\npublic class main {\npublic static void main(string[] args) {\nfinal embeddedextension embeddedextension = embeddedextension.builder()\n.withid(\"embedded-ext-1\")\n.withname(\"embedded extension\")\n.withversion(\"1.0.0\")\n.withpriority(0)\n.withstartpriority(1000)\n.withauthor(\"me\")\n.withextensionmain(new myembeddedextensionmain())\n.build();\nfinal embeddedhivemqbuilder builder = embeddedhivemq.builder()\n.withconfigurationfolder(path.of(\"/path/to/embedded-config-folder\"))\n.withdatafolder(path.of(\"/path/to/embedded-data-folder\"))\n.withextensionsfolder(path.of(\"/path/to/embedded-extensions-folder\"))\n.withembeddedextension(embeddedextension);\ntry (final embeddedhivemq hivemq = builder.build()) {\nhivemq.start().join();\n//do something with hivemq\n} catch (final exception ex) {\nex.printstacktrace();\n}\n}\nprivate static class myembeddedextensionmain implements extensionmain {\n@override\npublic void extensionstart(final @notnull extensionstartinput extensionstartinput, final @notnull extensionstartoutput extensionstartoutput) {\n// my extension start code\n}\n@override\npublic void extensionstop(final @notnull extensionstopinput extensionstopinput, final @notnull extensionstopoutput extensionstopoutput) {\n// my extension stop code\n}\n}\n}\nnote\nan embeddedhivemq is a resource that is similar to a e.g. network connection and implements the java.lang.autocloseable interface. always use arm (try with resources) or ensure a call to close().\nexclusions\nwhen you deploy an application that includes embeddedhivemq, it can be useful to exclude some dependencies. one way to exclude dependencies is with the maven shade plugin.\n<project>\n...\n<build>\n<plugins>\n<plugin>\n<groupid>org.apache.maven.plugins</groupid>\n<artifactid>maven-shade-plugin</artifactid>\n<version>3.2.1</version>\n<executions>\n<execution>\n<phase>package</phase>\n<goals>\n<goal>shade</goal>\n</goals>\n<configuration>\n<artifactset>\n<excludes>\n<!--exclude the undesired dependencies-->\n<exclude>org.rocksdb:rocksdbjni</exclude>\n<exclude>ch.qos.logback:logback-classic</exclude>\n</excludes>\n</artifactset>\n<filters>\n<filter>\n<artifact>*:*</artifact>\n<excludes>\n<exclude>meta-inf/*.sf</exclude>\n<exclude>meta-inf/*.dsa</exclude>\n<exclude>meta-inf/*.rsa</exclude>\n</excludes>\n</filter>\n</filters>\n</configuration>\n</execution>\n</executions>\n</plugin>\n</plugins>\n</build>\n...\n</project>\nrocksdb exclusion\nto exclude the org.rocksdb:rocksdbjni dependency, two internal configurations must be changed before you call start().\npublic class main {\npublic static void main(string[] args) {\n...\ntry (final embeddedhivemq hivemq = embeddedhivemqbuilder.build()) {\ninternalconfigurations.payload_persistence_type.set(persistencetype.file);\ninternalconfigurations.retained_message_persistence_type.set(persistencetype.file);\nhivemq.start().join();\n...\n} catch (final exception ex) {\nex.printstacktrace();\n}\n}\n}\ncontributing\nif you want to contribute to hivemq ce, see the contribution guidelines.\nlicense\nhivemq community edition is licensed under the apache license, version 2.0. a copy of the license can be found here.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000156, "year": null}, {"Unnamed: 0": 174, "autor": 174, "date": null, "content": "Californium (Cf) - CoAP for Java\nEclipse Californium is a Java implementation of RFC7252 - Constrained Application Protocol for IoT Cloud services. Thus, the focus is on scalability and usability instead of resource-efficiency like for embedded devices. Yet Californium is also suitable for embedded JVMs.\nMore information can be found at http://www.eclipse.org/californium/ and http://coap.technology/.\nBuild using Maven\nYou need to have a working maven installation to build Californium. Then simply run the following from the project's root directory:\n$ mvn clean install\nExecutable JARs of the examples with all dependencies can be found in the demo-apps/run folder.\nThe build-process in branch master is tested for jdk 7, jdk 8, jdk 11, jdk 15 and jdk 16. For jdk 7 the revapi maven-plugin is disabled, it requires at least java 8.\nTo generate the javadocs, add \"-DcreateJavadoc=true\" to the command line and set the JAVA_HOME.\n$ mvn clean install -DcreateJavadoc=true\nBuild earlier release\n!!! Since 29. October 2021 !!!\nThe hostname \"non-existing.host\" is now existing and all builds of version and tags before that date will fail.\nTo (re-)build versions before that date the unit tests must therefore be skipped.\n$ mvn clean install -DskipTests\nEarlier versions (3.0.0-Mx, 2.6.5 and before) may also fail to build with newer JDKs, especially, if java 16 is used! That is cause by the unit test dependency to a deprecated version of \"mockito\". If such a (re-)build is required, the unit tests must be skipped (which is in the meantime anyway required caused by the \"non-existing.host\").\nIn combination with the \"non-existing.host\" now existing, the build with unit test only works for the current heads of the branches 2.6.x and master!\nBuild jdk7 compliant\nCalifornium 2.x and newer can be used with java 7 or newer. If you want to build it with a jdk 7, but use also plugins which are only supported for newer jdks, the toolchain plugin could be used. That requires a toolchains configuration in \"toolchains.xml\" in your maven \".m2\" folder\n<?xml version=\"1.0\" encoding=\"UTF8\"?>\n<toolchains>\n<!-- JDK toolchains -->\n<toolchain>\n<type>jdk</type>\n<provides>\n<version>1.7</version>\n</provides>\n<configuration>\n<jdkHome>path..to..jdk7...home</jdkHome>\n</configuration>\n</toolchain>\n</toolchains>\nTo use the jdk7 toolchain, add \"-DuseToolchain=true\" to the command line.\n$ mvn clean install -DuseToolchain=true\nTo use the jdk7 toolchain and create javadocs, add \"-DuseToolchainJavadoc=true\" to the command line (JAVA_HOME is not required).\n$ mvn clean install -DuseToolchainJavadoc=true\nBuild with jdk11 and EdDSA support\nTo support EdDSA, either java 15, java 16, or java 11 with ed25519-java is required at runtime. Using java 15 to build Californium, leaves out ed25519-java, using java 11 for building, includes ed25519-java by default. If ed25519-java should NOT be included into the californium's jars, add -Dno.net.i2p.crypto.eddsa=true to maven's arguments.\n$ mvn clean install -Dno.net.i2p.crypto.eddsa=true\nIn that case, it's still possible to use ed25519-java, if the eddsa-0.3.0.jar is provided to the classpath separately.\nRun unit tests using Bouncy Castle as alternative JCE provider\nWith 3.0 a first, experimental support for using Bouncy Castle (1.69, bcprov-jdk15on, bcpkix-jdk15on, and, for tls, bctls-jdk15on) is implemented.\nTo demonstrate the basic functions, run the unit-tests using the profile bc-tests\n$ mvn clean install -Pbc-tests\nSupporting Bouncy Castle for the unit test uncovers a couple of differences, which required to adapt the implementation. It is assumed, that more will be found and more adaption will be required. If you find some, don't hesitate to report issues, perhaps research and analysis, and fixes. On the other hand, the project Californium will for now not be able to provide support for Bouncy Castle questions with or without relation to Californium. You may create issues, but it may be not possible for us to answer them.\nOn issue seems to be the SecureRandom generator, which shows in some environments strange CPU/time consumption.\nWith that, it gets very time consuming to test all combinations. Therefore, if you need a specific one, please test it on your own. If you consider, that some adaption is required, let us know by creating an issue or PR.\nUsing Californium in Maven Projects\nWe are publishing Californium's artifacts for milestones and releases to Maven Central. To use the latest released version as a library in your projects, add the following dependency to your pom.xml (without the dots):\n<dependencies>\n...\n<dependency>\n<groupId>org.eclipse.californium</groupId>\n<artifactId>californium-core</artifactId>\n<version>3.0.0</version>\n</dependency>\n...\n</dependencies>\n...\nCurrent Builds\nYou can also be bold and try out the most recent build from master. However, we are not publishing those to Maven Central but to Californium's project repository at Eclipse only. You will therefore need to add the Eclipse Repository to your pom.xml first:\n<repositories>\n...\n<repository>\n<id>repo.eclipse.org</id>\n<name>Californium Repository</name>\n<url>https://repo.eclipse.org/content/repositories/californium/</url>\n</repository>\n...\n</repositories>\nYou can then simply depend on 3.1.0-SNAPSHOT.\nEclipse\nThe project can be easily imported into a recent version of the Eclipse IDE. Make sure to have the following before importing the Californium (Cf) projects:\nEclipse EGit (should be the case with every recent Eclipse version)\nm2e - Maven Integration for Eclipse (should be the case with every recent Eclipse version)\nUTF-8 workspace text file encoding (Preferences \u00bb General \u00bb Workspace)\nThen choose [Import... \u00bb Maven \u00bb Existing Maven Projects] to import californium into Eclipse.\nIntelliJ\nThe project can also be imported to IntelliJ as follows:\nIn IntelliJ, choose [File.. \u00bb Open] then select the location of the cloned repository in your filesystem. IntelliJ will then automatically import all projects and resolve required Maven dependencies.\nInterop Server\nA test server is running at coap://californium.eclipseprojects.io:5683/\nIt is an instance of the cf-plugtest-server from the demo-apps. The root resource responds with its current version.\nMore information can be found at http://www.eclipse.org/californium and technical details at https://projects.eclipse.org/projects/iot.californium.\nAnother interop server with a different implementation can be found at coap://coap.me:5683/. More information can be found at http://coap.me/.\nAdapter Selection\nFor some systems (particularly when multicasting), it may be necessary to specify/restrict californium to a particular network interface, or interfaces. This can be achieved by setting the COAP_NETWORK_INTERFACES JVM parameter to a suitable regex, for example:\njava -DCOAP_NETWORK_INTERFACES='.*wpan0' -jar target/cf-helloworld-server-3.0.0.jar MulticastTestServer\nContact\nA bug, an idea, an issue? Join the Mailing list or create an issue here on GitHub.\nContributing\nPlease check out our contribution guidelines", "link": "https://github.com/eclipse/californium", "origin": "Github", "suborigin": "Iot", "result": true, "Selector": "branches", "selectorShort": "branch", "MarkedSent": "californium (cf) - coap for java\neclipse californium is a java implementation of rfc7252 - constrained application protocol for iot cloud services. thus, the focus is on scalability and usability instead of resource-efficiency like for embedded devices. yet californium is also suitable for embedded jvms.\nmore information can be found at http://www.eclipse.org/californium/ and http://coap.technology/.\nbuild using maven\nyou need to have a working maven installation to build californium. then simply run the following from the project's root directory:\n$ mvn clean install\nexecutable jars of the examples with all dependencies can be found in the demo-apps/run folder.\nthe build-process in branch master is tested for jdk 7, jdk 8, jdk 11, jdk 15 and jdk 16. for jdk 7 the revapi maven-plugin is disabled, it requires at least java 8.\nto generate the javadocs, add \"-dcreatejavadoc=true\" to the command line and set the java_home.\n$ mvn clean install -dcreatejavadoc=true\nbuild earlier release\n!!! since 29. october 2021 !!!\nthe hostname \"non-existing.host\" is now existing and all builds of version and tags before that date will fail.\nto (re-)build versions before that date the unit tests must therefore be skipped.\n$ mvn clean install -dskiptests\nearlier versions (3.0.0-mx, 2.6.5 and before) may also fail to build with newer jdks, especially, if java 16 is used! that is cause by the unit test dependency to a deprecated version of \"mockito\". if such a (re-)build is required, the unit tests must be skipped (which is in the meantime anyway required caused by the \"non-existing.host\").\nin combination with the \"non-existing.host\" now existing, the build with unit test only works for the current heads of the -----> branches !!!  2.6.x and master!\nbuild jdk7 compliant\ncalifornium 2.x and newer can be used with java 7 or newer. if you want to build it with a jdk 7, but use also plugins which are only supported for newer jdks, the toolchain plugin could be used. that requires a toolchains configuration in \"toolchains.xml\" in your maven \".m2\" folder\n<?xml version=\"1.0\" encoding=\"utf8\"?>\n<toolchains>\n<!-- jdk toolchains -->\n<toolchain>\n<type>jdk</type>\n<provides>\n<version>1.7</version>\n</provides>\n<configuration>\n<jdkhome>path..to..jdk7...home</jdkhome>\n</configuration>\n</toolchain>\n</toolchains>\nto use the jdk7 toolchain, add \"-dusetoolchain=true\" to the command line.\n$ mvn clean install -dusetoolchain=true\nto use the jdk7 toolchain and create javadocs, add \"-dusetoolchainjavadoc=true\" to the command line (java_home is not required).\n$ mvn clean install -dusetoolchainjavadoc=true\nbuild with jdk11 and eddsa support\nto support eddsa, either java 15, java 16, or java 11 with ed25519-java is required at runtime. using java 15 to build californium, leaves out ed25519-java, using java 11 for building, includes ed25519-java by default. if ed25519-java should not be included into the californium's jars, add -dno.net.i2p.crypto.eddsa=true to maven's arguments.\n$ mvn clean install -dno.net.i2p.crypto.eddsa=true\nin that case, it's still possible to use ed25519-java, if the eddsa-0.3.0.jar is provided to the classpath separately.\nrun unit tests using bouncy castle as alternative jce provider\nwith 3.0 a first, experimental support for using bouncy castle (1.69, bcprov-jdk15on, bcpkix-jdk15on, and, for tls, bctls-jdk15on) is implemented.\nto demonstrate the basic functions, run the unit-tests using the profile bc-tests\n$ mvn clean install -pbc-tests\nsupporting bouncy castle for the unit test uncovers a couple of differences, which required to adapt the implementation. it is assumed, that more will be found and more adaption will be required. if you find some, don't hesitate to report issues, perhaps research and analysis, and fixes. on the other hand, the project californium will for now not be able to provide support for bouncy castle questions with or without relation to californium. you may create issues, but it may be not possible for us to answer them.\non issue seems to be the securerandom generator, which shows in some environments strange cpu/time consumption.\nwith that, it gets very time consuming to test all combinations. therefore, if you need a specific one, please test it on your own. if you consider, that some adaption is required, let us know by creating an issue or pr.\nusing californium in maven projects\nwe are publishing californium's artifacts for milestones and releases to maven central. to use the latest released version as a library in your projects, add the following dependency to your pom.xml (without the dots):\n<dependencies>\n...\n<dependency>\n<groupid>org.eclipse.californium</groupid>\n<artifactid>californium-core</artifactid>\n<version>3.0.0</version>\n</dependency>\n...\n</dependencies>\n...\ncurrent builds\nyou can also be bold and try out the most recent build from master. however, we are not publishing those to maven central but to californium's project repository at eclipse only. you will therefore need to add the eclipse repository to your pom.xml first:\n<repositories>\n...\n<repository>\n<id>repo.eclipse.org</id>\n<name>californium repository</name>\n<url>https://repo.eclipse.org/content/repositories/californium/</url>\n</repository>\n...\n</repositories>\nyou can then simply depend on 3.1.0-snapshot.\neclipse\nthe project can be easily imported into a recent version of the eclipse ide. make sure to have the following before importing the californium (cf) projects:\neclipse egit (should be the case with every recent eclipse version)\nm2e - maven integration for eclipse (should be the case with every recent eclipse version)\nutf-8 workspace text file encoding (preferences \u00bb general \u00bb workspace)\nthen choose [import... \u00bb maven \u00bb existing maven projects] to import californium into eclipse.\nintellij\nthe project can also be imported to intellij as follows:\nin intellij, choose [file.. \u00bb open] then select the location of the cloned repository in your filesystem. intellij will then automatically import all projects and resolve required maven dependencies.\ninterop server\na test server is running at coap://californium.eclipseprojects.io:5683/\nit is an instance of the cf-plugtest-server from the demo-apps. the root resource responds with its current version.\nmore information can be found at http://www.eclipse.org/californium and technical details at https://projects.eclipse.org/projects/iot.californium.\nanother interop server with a different implementation can be found at coap://coap.me:5683/. more information can be found at http://coap.me/.\nadapter selection\nfor some systems (particularly when multicasting), it may be necessary to specify/restrict californium to a particular network interface, or interfaces. this can be achieved by setting the coap_network_interfaces jvm parameter to a suitable regex, for example:\njava -dcoap_network_interfaces='.*wpan0' -jar target/cf-helloworld-server-3.0.0.jar multicasttestserver\ncontact\na bug, an idea, an issue? join the mailing list or create an issue here on github.\ncontributing\nplease check out our contribution guidelines", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000174, "year": null}, {"Unnamed: 0": 187, "autor": 187, "date": null, "content": "HiveMQ MQTT Client\nMQTT 5.0 and 3.1.1 compatible and feature-rich high-performance Java client library with different API flavours and backpressure support.\nDocumentation: https://hivemq.github.io/hivemq-mqtt-client/\nCommunity forum: https://community.hivemq.com/\nHiveMQ website: https://www.hivemq.com/\nContribution guidelines: CONTRIBUTING.md\nLicense: LICENSE\nMQTT resources:\nMQTT Essentials\nMQTT 5 Essentials\nFeatures\nAll MQTT 3.1.1 and MQTT 5.0 features\nAPI flavors:\nReactive: Reactive Streams compatible, RxJava and Reactor APIs available\nAsynchronous API: futures and callbacks\nBlocking API: for quick start and testing\nSwitch flexibly between flavours and use them concurrently\nFlavours are clearly separated but have a consistent API style\nBackpressure support:\nQoS 1 and 2\nQoS 0 (dropping incoming messages, if necessary)\nBringing MQTT flow control and reactive pull backpressure together\nTransports:\nTCP\nSSL/TLS\nAll TLS versions up to TLS 1.3 are supported\nTLS mutual authentication\nTLS Server Name Indication (SNI)\nTLS Session Resumption\nDefault and customizable hostname verification\nWebSocket, Secure WebSocket\nProxy: SOCKS4, SOCKS5, HTTP CONNECT\nAll possible combinations\nAutomatic and configurable thread management\nAutomatic and configurable reconnect handling and message redelivery\nAutomatic and configurable resubscribe if the session expired\nManual message acknowledgment\nSelectively enable manual acknowledgment for specific streams\nAcknowledge messages that are emitted to multiple streams independently per stream (the client aggregates the acknowledgments before sending MQTT acknowledgments)\nOrder of manual acknowledgment does not matter (the client automatically ensures the order of MQTT acknowledgments for 100% compatibility with the MQTT specification)\nLifecycle listeners\nWhen connected\nWhen disconnected or connection failed\nMQTT 5 specific:\nPluggable Enhanced Authentication support (additional to MQTT specification: server-triggered re-authentication)\nAutomatic Topic Alias mapping\nInterceptors for QoS flows\nUsers\nIf you use the HiveMQ MQTT Client in a project that is not listed here, feel free to open an issue or pull request.\nHow to use\nJava 8 or higher is required.\nDependency\nGradle\nIf you use Gradle, just include the following inside your build.gradle(.kts) file.\ndependencies {\nimplementation(\"com.hivemq:hivemq-mqtt-client:1.3.0\")\n}\nFor optional features you can choose to include additional modules:\ndependencies {\nimplementation(platform(\"com.hivemq:hivemq-mqtt-client-websocket:1.3.0\"))\nimplementation(platform(\"com.hivemq:hivemq-mqtt-client-proxy:1.3.0\"))\nimplementation(platform(\"com.hivemq:hivemq-mqtt-client-epoll:1.3.0\"))\nimplementation(\"com.hivemq:hivemq-mqtt-client-reactor:1.3.0\")\n}\nMaven\nIf you use Maven, just include the following inside your pom.xml file.\n<project>\n...\n<dependencies>\n<dependency>\n<groupId>com.hivemq</groupId>\n<artifactId>hivemq-mqtt-client</artifactId>\n<version>1.3.0</version>\n</dependency>\n</dependencies>\n...\n</project>\nNOTE: You have to set the compiler version to 1.8 or higher.\n<project>\n...\n<properties>\n<maven.compiler.source>1.8</maven.compiler.source>\n<maven.compiler.target>1.8</maven.compiler.target>\n</properties>\n...\n</project>\nFor optional features you can choose to include additional modules:\n<project>\n...\n<dependencies>\n<dependency>\n<groupId>com.hivemq</groupId>\n<artifactId>hivemq-mqtt-client-websocket</artifactId>\n<version>1.3.0</version>\n<type>pom</type>\n</dependency>\n</dependencies>\n<dependencies>\n<dependency>\n<groupId>com.hivemq</groupId>\n<artifactId>hivemq-mqtt-client-proxy</artifactId>\n<version>1.3.0</version>\n<type>pom</type>\n</dependency>\n</dependencies>\n<dependencies>\n<dependency>\n<groupId>com.hivemq</groupId>\n<artifactId>hivemq-mqtt-client-epoll</artifactId>\n<version>1.3.0</version>\n<type>pom</type>\n</dependency>\n</dependencies>\n<dependencies>\n<dependency>\n<groupId>com.hivemq</groupId>\n<artifactId>hivemq-mqtt-client-reactor</artifactId>\n<version>1.3.0</version>\n</dependency>\n</dependencies>\n...\n</project>\nShaded version\nIf you are experiencing problems with transitive dependencies, you can try the shaded version. This version packs the transitive dependencies which are only used internal under a different package name. The shaded version includes the websocket, proxy and epoll modules. To use the shaded version just append -shaded to the artifact name.\nGradle\ndependencies {\nimplementation(\"com.hivemq:hivemq-mqtt-client-shaded:1.3.0\")\n}\nMaven\n<project>\n...\n<dependencies>\n<dependency>\n<groupId>com.hivemq</groupId>\n<artifactId>hivemq-mqtt-client-shaded</artifactId>\n<version>1.3.0</version>\n</dependency>\n</dependencies>\n...\n</project>\nSnapshots\nSnapshots can be obtained using JitPack.\nGradle\nrepositories {\n...\nmaven { url 'https://jitpack.io' }\n}\ndependencies {\nimplementation(\"com.github.hivemq.hivemq-mqtt-client:hivemq-mqtt-client:develop-SNAPSHOT\")\n// snapshots for optional modules\nimplementation(platform(\"com.github.hivemq.hivemq-mqtt-client:hivemq-mqtt-client-websocket:develop-SNAPSHOT\"))\nimplementation(platform(\"com.github.hivemq.hivemq-mqtt-client:hivemq-mqtt-client-proxy:develop-SNAPSHOT\"))\nimplementation(platform(\"com.github.hivemq.hivemq-mqtt-client:hivemq-mqtt-client-epoll:develop-SNAPSHOT\"))\nimplementation(\"com.github.hivemq.hivemq-mqtt-client:hivemq-mqtt-client-reactor:develop-SNAPSHOT\")\n}\nMaven\n<project>\n...\n<repositories>\n<repository>\n<id>jitpack.io</id>\n<url>https://jitpack.io</url>\n</repository>\n</repositories>\n<dependencies>\n<dependency>\n<groupId>com.github.hivemq.hivemq-mqtt-client</groupId>\n<artifactId>hivemq-mqtt-client</artifactId>\n<version>develop-SNAPSHOT</version>\n</dependency>\n</dependencies>\n<!-- snapshots for optional modules -->\n<dependencies>\n<dependency>\n<groupId>com.github.hivemq.hivemq-mqtt-client</groupId>\n<artifactId>hivemq-mqtt-client-websocket</artifactId>\n<version>develop-SNAPSHOT</version>\n<type>pom</type>\n</dependency>\n</dependencies>\n<dependencies>\n<dependency>\n<groupId>com.github.hivemq.hivemq-mqtt-client</groupId>\n<artifactId>hivemq-mqtt-client-proxy</artifactId>\n<version>develop-SNAPSHOT</version>\n<type>pom</type>\n</dependency>\n</dependencies>\n<dependencies>\n<dependency>\n<groupId>com.github.hivemq.hivemq-mqtt-client</groupId>\n<artifactId>hivemq-mqtt-client-epoll</artifactId>\n<version>develop-SNAPSHOT</version>\n<type>pom</type>\n</dependency>\n</dependencies>\n<dependencies>\n<dependency>\n<groupId>com.github.hivemq.hivemq-mqtt-client</groupId>\n<artifactId>hivemq-mqtt-client-reactor</artifactId>\n<version>develop-SNAPSHOT</version>\n</dependency>\n</dependencies>\n...\n</project>\nChange the artifact name to hivemq-mqtt-client-shaded to get snapshots of the shaded version.\nJitPack works for all branches and also specific commits. Just specify <branch>-SNAPSHOT or the first 10 digits of the commit id in the version.\nGeneral principles\nAPI and implementation are clearly separated. All classes inside internal packages must not be used directly.\nThe API is mostly fluent and uses fluent builders to create clients, configurations and messages.\nThe API is designed to be consistent:\nThe same principles are used throughout the library.\nThe MQTT 3 and 5 interfaces are as consistent as possible with only version-specific differences.\nCreation of clients\nBase classes: Mqtt3Client, Mqtt5Client\nMqtt5Client client = MqttClient.builder()\n.identifier(UUID.randomUUID().toString())\n.serverHost(\"broker.hivemq.com\")\n.useMqttVersion5()\n.build();\nMqtt3Client client = MqttClient.builder()...useMqttVersion3().build();\nOr if the version is known upfront:\nMqtt5Client client = Mqtt5Client.builder()...build();\nMqtt3Client client = Mqtt3Client.builder()...build();\nFor each API style exists a specific build...() method.\nAPI flavours\nEach API style has its own interface to separate them clearly. At any time it is possible to switch the API style.\nBlocking API\nBuilder method: buildBlocking()\nSwitch method: client.toBlocking()\nExamples\nSubscribe example\nfinal Mqtt5BlockingClient client = Mqtt5Client.builder()\n.identifier(UUID.randomUUID().toString())\n.serverHost(\"broker.hivemq.com\")\n.buildBlocking();\nclient.connect();\ntry (final Mqtt5Publishes publishes = client.publishes(MqttGlobalPublishFilter.ALL)) {\nclient.subscribeWith().topicFilter(\"test/topic\").qos(MqttQos.AT_LEAST_ONCE).send();\npublishes.receive(1, TimeUnit.SECONDS).ifPresent(System.out::println);\npublishes.receive(100, TimeUnit.MILLISECONDS).ifPresent(System.out::println);\n} finally {\nclient.disconnect();\n}\nPublish example\nMqtt5BlockingClient client = Mqtt5Client.builder()\n.identifier(UUID.randomUUID().toString())\n.serverHost(\"broker.hivemq.com\")\n.buildBlocking();\nclient.connect();\nclient.publishWith().topic(\"test/topic\").qos(MqttQos.AT_LEAST_ONCE).payload(\"1\".getBytes()).send();\nclient.disconnect();\nConnect\nclient.connect();\nOr with customized properties of the Connect message:\nclient.connectWith().keepAlive(10).send();\nOr with pre-built Connect message:\nMqtt5Connect connectMessage = Mqtt5Connect.builder().keepAlive(10).build();\nclient.connect(connectMessage);\nPublish\nclient.publishWith()\n.topic(\"test/topic\")\n.qos(MqttQos.AT_LEAST_ONCE)\n.payload(\"payload\".getBytes())\n.send();\nOr with pre-built Publish message:\nMqtt5Publish publishMessage = Mqtt5Publish.builder()\n.topic(\"test/topic\")\n.qos(MqttQos.AT_LEAST_ONCE)\n.payload(\"payload\".getBytes())\n.build();\nclient.publish(publishMessage);\nSubscribe\nclient.subscribeWith().topicFilter(\"test/topic\").qos(MqttQos.EXACTLY_ONCE).send();\nOr with pre-built Subscribe message:\nMqtt5Subscribe subscribeMessage = Mqtt5Subscribe.builder()\n.topicFilter(\"test/topic\")\n.qos(MqttQos.EXACTLY_ONCE)\n.build();\nclient.subscribe(subscribeMessage);\nUnsubscribe\nclient.unsubscribeWith().topicFilter(\"test/topic\").send();\nOr with pre-built Unsubscribe message:\nMqtt5Unsubscribe unsubscribeMessage = Mqtt5Unsubscribe.builder().topicFilter(\"test/topic\").build();\nclient.unsubscribe(unsubscribeMessage);\nConsume messages\ntry (Mqtt5BlockingClient.Mqtt5Publishes publishes = client.publishes(MqttGlobalPublishFilter.ALL)) {\nMqtt5Publish publishMessage = publishes.receive();\n// or with timeout\nOptional<Mqtt5Publish> publishMessage = publishes.receive(10, TimeUnit.SECONDS);\n// or without blocking\nOptional<Mqtt5Publish> publishMessage = publishes.receiveNow();\n}\npublishes must be called before subscribe to ensure no message is lost. It can be called before connect to receive messages of a previous session.\nDisconnect\nclient.disconnect();\nOr with customized properties of the DISCONNECT message (only MQTT 5):\nclient.disconnectWith().reasonString(\"test\").send();\nOr with pre-built Disconnect message (only MQTT 5):\nMqtt5Disconnect disconnectMessage = Mqtt5Disconnect.builder().reasonString(\"test\").build();\nclient.disconnect(disconnectMessage);\nReauth (only MQTT 5)\nclient.reauth();\nAsync API\nBuilder method: buildAsync()\nSwitch method: client.toAsync()\nExamples\nSubscribe example\nMqtt5BlockingClient client = Mqtt5Client.builder()\n.identifier(UUID.randomUUID().toString())\n.serverHost(\"broker.hivemq.com\")\n.buildBlocking();\nclient.connect();\nclient.toAsync().subscribeWith()\n.topicFilter(\"test/topic\")\n.qos(MqttQos.AT_LEAST_ONCE)\n.callback(System.out::println)\n.send();\nPublish example\nMqtt5AsyncClient client = Mqtt5Client.builder()\n.identifier(UUID.randomUUID().toString())\n.serverHost(\"broker.hivemq.com\")\n.buildAsync();\nclient.connect()\n.thenCompose(connAck -> client.publishWith().topic(\"test/topic\").payload(\"1\".getBytes()).send())\n.thenCompose(publishResult -> client.disconnect());\nConnect\nconnect(), connectWith() and connect(Mqtt3/5Connect) method calls are analog to the Blocking API but return CompletableFuture.\nPublish\npublishWith() and publish(Mqtt3/5Publish) method calls are analog to the Blocking API but return CompletableFuture.\nSubscribe\nsubscribeWith() and subscribe(Mqtt3/5Subscribe) method calls are analog to the Blocking API but return CompletableFuture.\nAdditionally messages can be consumed per subscribe:\nclient.subscribeWith()\n.topicFilter(\"test/topic\")\n.qos(MqttQos.EXACTLY_ONCE)\n.callback(System.out::println)\n.executor(executor) // optional\n.send();\nOr with pre-built Subscribe message:\nMqtt5Subscribe subscribeMessage = Mqtt5Subscribe.builder()\n.topicFilter(\"test/topic\")\n.qos(MqttQos.EXACTLY_ONCE)\n.build();\nclient.subscribe(subscribeMessage, System.out::println);\nclient.subscribe(subscribeMessage, System.out::println, executor);\nUnsubscribe\nunsubscribeWith() and unsubscribe(Mqtt3/5Unsubscribe) method calls are analog to the Blocking API but return CompletableFuture.\nConsume messages\nMessages can either be consumed per subscribe (described above) or globally:\nclient.publishes(MqttGlobalPublishFilter.ALL, System.out::println);\nOr with executing the callback on a specified executor:\nclient.publishes(MqttGlobalPublishFilter.ALL, System.out::println, executor);\npublishes must be called before subscribe to ensure no message is lost. It can be called before connect to receive messages of a previous session.\nDisconnect\ndisconnect(), disconnectWith() and disconnect(Mqtt5Disconnect) method calls are analog to the Blocking API but return CompletableFuture.\nReauth (only MQTT 5)\nreauth() method call is analog to the Blocking API but returns CompletableFuture.\nReactive API\nBuilder method: buildRx()\nSwitch method: client.toRx()\nExamples\nSubscribe example\nMqtt5RxClient client = Mqtt5Client.builder()\n.identifier(UUID.randomUUID().toString())\n.serverHost(\"broker.hivemq.com\")\n.buildRx();\n// As we use the reactive API, the following line does not connect yet, but returns a reactive type.\n// e.g. Single is something like a lazy and reusable future. Think of it as a source for the ConnAck message.\nSingle<Mqtt5ConnAck> connAckSingle = client.connect();\n// Same here: the following line does not subscribe yet, but returns a reactive type.\n// FlowableWithSingle is a combination of the single SubAck message and a Flowable of Publish messages.\n// A Flowable is an asynchronous stream that enables backpressure from the application over the client to the broker.\nFlowableWithSingle<Mqtt5Publish, Mqtt5SubAck> subAckAndMatchingPublishes = client.subscribeStreamWith()\n.topicFilter(\"a/b/c\").qos(MqttQos.AT_LEAST_ONCE)\n.addSubscription().topicFilter(\"a/b/c/d\").qos(MqttQos.EXACTLY_ONCE).applySubscription()\n.applySubscribe();\n// The reactive types offer many operators that will not be covered here.\n// Here we register callbacks to print messages when we received the CONNACK, SUBACK and matching PUBLISH messages.\nCompletable connectScenario = connAckSingle\n.doOnSuccess(connAck -> System.out.println(\"Connected, \" + connAck.getReasonCode()))\n.doOnError(throwable -> System.out.println(\"Connection failed, \" + throwable.getMessage()))\n.ignoreElement();\nCompletable subscribeScenario = subAckAndMatchingPublishes\n.doOnSingle(subAck -> System.out.println(\"Subscribed, \" + subAck.getReasonCodes()))\n.doOnNext(publish -> System.out.println(\n\"Received publish\" + \", topic: \" + publish.getTopic() + \", QoS: \" + publish.getQos() +\n\", payload: \" + new String(publish.getPayloadAsBytes())))\n.ignoreElements();\n// Reactive types can be easily and flexibly combined\nconnectScenario.andThen(subscribeScenario).blockingAwait();\nPublish example\nMqtt5RxClient client = Mqtt5Client.builder()\n.identifier(UUID.randomUUID().toString())\n.serverHost(\"broker.hivemq.com\")\n.buildRx();\n// As we use the reactive API, the following line does not connect yet, but returns a reactive type.\nCompletable connectScenario = client.connect()\n.doOnSuccess(connAck -> System.out.println(\"Connected, \" + connAck.getReasonCode()))\n.doOnError(throwable -> System.out.println(\"Connection failed, \" + throwable.getMessage()))\n.ignoreElement();\n// Fake a stream of Publish messages with an incrementing number in the payload\nFlowable<Mqtt5Publish> messagesToPublish = Flowable.range(0, 10_000)\n.map(i -> Mqtt5Publish.builder()\n.topic(\"a/b/c\")\n.qos(MqttQos.AT_LEAST_ONCE)\n.payload((\"test \" + i).getBytes())\n.build())\n// Emit 1 message only every 100 milliseconds\n.zipWith(Flowable.interval(100, TimeUnit.MILLISECONDS), (publish, i) -> publish);\n// As we use the reactive API, the following line does not publish yet, but returns a reactive type.\nCompletable publishScenario = client.publish(messagesToPublish)\n.doOnNext(publishResult -> System.out.println(\n\"Publish acknowledged: \" + new String(publishResult.getPublish().getPayloadAsBytes())))\n.ignoreElements();\n// As we use the reactive API, the following line does not disconnect yet, but returns a reactive type.\nCompletable disconnectScenario = client.disconnect().doOnComplete(() -> System.out.println(\"Disconnected\"));\n// Reactive types can be easily and flexibly combined\nconnectScenario.andThen(publishScenario).andThen(disconnectScenario).blockingAwait();\nConnect\nconnect(), connectWith() and connect(Mqtt3/5Connect) method calls are analog to the Async and Blocking API but return Single<ConnAck>.\nPublish\npublish takes a reactive stream of Publish messages (Flowable) and returns a reactive stream of Publish results (Flowable).\nThe Reactive API is usually not used for publishing single messages. Nevertheless it is possible with the following code.\nSingle<Mqtt5PublishResult> result =\nclient.publish(Flowable.just(Mqtt5Publish.builder()\n.topic(\"test/topic\")\n.qos(MqttQos.AT_LEAST_ONCE)\n.payload(\"payload\".getBytes())\n.build())).singleOrError();\nSubscribe\nsubscribeWith() and subscribe(Mqtt3/5Subscribe) method calls are analog to the Async and Blocking API but return Single<SubAck>.\nAdditionally messages can be consumed per subscribe:\nFlowable<Mqtt5Publish> result =\nclient.subscribeStreamWith()\n.topicFilter(\"test/topic\")\n.qos(MqttQos.EXACTLY_ONCE)\n.applySubscribe()\n.doOnSingle(subAck -> System.out.println(\"subscribed\"))\n.doOnNext(publish -> System.out.println(\"received publish\"));\nOr with pre-built Subscribe message:\nMqtt5Subscribe subscribeMessage = Mqtt5Subscribe.builder()\n.topicFilter(\"test/topic\")\n.qos(MqttQos.EXACTLY_ONCE)\n.build();\nFlowable<Mqtt5Publish> result =\nclient.subscribeStreamWith(subscribeMessage)\n.doOnSingle(subAck -> System.out.println(\"subscribed\"))\n.doOnNext(publish -> System.out.println(\"received publish\"));\nUnsubscribe\nunsubscribeWith() and unsubscribe(Mqtt3/5Unsubscribe) method calls are analog to the Async and Blocking API but return Single<UnsubAck>.\nConsume messages\nMessages can either be consumed per subscribe (described above) or globally:\nFlowable<Mqtt5Publish> result =\nclient.publishes(MqttGlobalPublishFilter.ALL).doOnNext(System.out::println);\npublishes must be called before subscribe to ensure no message is lost. It can be called before connect to receive messages of a previous session.\nDisconnect\ndisconnect(), disconnectWith() and disconnect(Mqtt5Disconnect) method calls are analog to the Async and Blocking API but return Completable.\nReauth (only MQTT 5)\nreauth() method call is analog to the Async and Blocking API but returns Completable.\nVersioning\nSemantic Versioning is used.\nAll code inside com.hivemq.client.internal packages must not be used directly. It can change at any time and is not part of the public API.\nInterfaces annotated with DoNotImplement must not be implemented. The implementation is provided by the library. This allows the library to later add methods to the interface without breaking backwards compatibility with implementing classes.\nContributing\nIf you want to contribute to HiveMQ MQTT Client, see the contribution guidelines.\nLicense\nHiveMQ MQTT Client is licensed under the APACHE LICENSE, VERSION 2.0. A copy of the license can be found here.", "link": "https://github.com/hivemq/hivemq-mqtt-client", "origin": "Github", "suborigin": "Iot", "result": true, "Selector": "branches", "selectorShort": "branch", "MarkedSent": "hivemq mqtt client\nmqtt 5.0 and 3.1.1 compatible and feature-rich high-performance java client library with different api flavours and backpressure support.\ndocumentation: https://hivemq.github.io/hivemq-mqtt-client/\ncommunity forum: https://community.hivemq.com/\nhivemq website: https://www.hivemq.com/\ncontribution guidelines: contributing.md\nlicense: license\nmqtt resources:\nmqtt essentials\nmqtt 5 essentials\nfeatures\nall mqtt 3.1.1 and mqtt 5.0 features\napi flavors:\nreactive: reactive streams compatible, rxjava and reactor apis available\nasynchronous api: futures and callbacks\nblocking api: for quick start and testing\nswitch flexibly between flavours and use them concurrently\nflavours are clearly separated but have a consistent api style\nbackpressure support:\nqos 1 and 2\nqos 0 (dropping incoming messages, if necessary)\nbringing mqtt flow control and reactive pull backpressure together\ntransports:\ntcp\nssl/tls\nall tls versions up to tls 1.3 are supported\ntls mutual authentication\ntls server name indication (sni)\ntls session resumption\ndefault and customizable hostname verification\nwebsocket, secure websocket\nproxy: socks4, socks5, http connect\nall possible combinations\nautomatic and configurable thread management\nautomatic and configurable reconnect handling and message redelivery\nautomatic and configurable resubscribe if the session expired\nmanual message acknowledgment\nselectively enable manual acknowledgment for specific streams\nacknowledge messages that are emitted to multiple streams independently per stream (the client aggregates the acknowledgments before sending mqtt acknowledgments)\norder of manual acknowledgment does not matter (the client automatically ensures the order of mqtt acknowledgments for 100% compatibility with the mqtt specification)\nlifecycle listeners\nwhen connected\nwhen disconnected or connection failed\nmqtt 5 specific:\npluggable enhanced authentication support (additional to mqtt specification: server-triggered re-authentication)\nautomatic topic alias mapping\ninterceptors for qos flows\nusers\nif you use the hivemq mqtt client in a project that is not listed here, feel free to open an issue or pull request.\nhow to use\njava 8 or higher is required.\ndependency\ngradle\nif you use gradle, just include the following inside your build.gradle(.kts) file.\ndependencies {\nimplementation(\"com.hivemq:hivemq-mqtt-client:1.3.0\")\n}\nfor optional features you can choose to include additional modules:\ndependencies {\nimplementation(platform(\"com.hivemq:hivemq-mqtt-client-websocket:1.3.0\"))\nimplementation(platform(\"com.hivemq:hivemq-mqtt-client-proxy:1.3.0\"))\nimplementation(platform(\"com.hivemq:hivemq-mqtt-client-epoll:1.3.0\"))\nimplementation(\"com.hivemq:hivemq-mqtt-client-reactor:1.3.0\")\n}\nmaven\nif you use maven, just include the following inside your pom.xml file.\n<project>\n...\n<dependencies>\n<dependency>\n<groupid>com.hivemq</groupid>\n<artifactid>hivemq-mqtt-client</artifactid>\n<version>1.3.0</version>\n</dependency>\n</dependencies>\n...\n</project>\nnote: you have to set the compiler version to 1.8 or higher.\n<project>\n...\n<properties>\n<maven.compiler.source>1.8</maven.compiler.source>\n<maven.compiler.target>1.8</maven.compiler.target>\n</properties>\n...\n</project>\nfor optional features you can choose to include additional modules:\n<project>\n...\n<dependencies>\n<dependency>\n<groupid>com.hivemq</groupid>\n<artifactid>hivemq-mqtt-client-websocket</artifactid>\n<version>1.3.0</version>\n<type>pom</type>\n</dependency>\n</dependencies>\n<dependencies>\n<dependency>\n<groupid>com.hivemq</groupid>\n<artifactid>hivemq-mqtt-client-proxy</artifactid>\n<version>1.3.0</version>\n<type>pom</type>\n</dependency>\n</dependencies>\n<dependencies>\n<dependency>\n<groupid>com.hivemq</groupid>\n<artifactid>hivemq-mqtt-client-epoll</artifactid>\n<version>1.3.0</version>\n<type>pom</type>\n</dependency>\n</dependencies>\n<dependencies>\n<dependency>\n<groupid>com.hivemq</groupid>\n<artifactid>hivemq-mqtt-client-reactor</artifactid>\n<version>1.3.0</version>\n</dependency>\n</dependencies>\n...\n</project>\nshaded version\nif you are experiencing problems with transitive dependencies, you can try the shaded version. this version packs the transitive dependencies which are only used internal under a different package name. the shaded version includes the websocket, proxy and epoll modules. to use the shaded version just append -shaded to the artifact name.\ngradle\ndependencies {\nimplementation(\"com.hivemq:hivemq-mqtt-client-shaded:1.3.0\")\n}\nmaven\n<project>\n...\n<dependencies>\n<dependency>\n<groupid>com.hivemq</groupid>\n<artifactid>hivemq-mqtt-client-shaded</artifactid>\n<version>1.3.0</version>\n</dependency>\n</dependencies>\n...\n</project>\nsnapshots\nsnapshots can be obtained using jitpack.\ngradle\nrepositories {\n...\nmaven { url 'https://jitpack.io' }\n}\ndependencies {\nimplementation(\"com.github.hivemq.hivemq-mqtt-client:hivemq-mqtt-client:develop-snapshot\")\n// snapshots for optional modules\nimplementation(platform(\"com.github.hivemq.hivemq-mqtt-client:hivemq-mqtt-client-websocket:develop-snapshot\"))\nimplementation(platform(\"com.github.hivemq.hivemq-mqtt-client:hivemq-mqtt-client-proxy:develop-snapshot\"))\nimplementation(platform(\"com.github.hivemq.hivemq-mqtt-client:hivemq-mqtt-client-epoll:develop-snapshot\"))\nimplementation(\"com.github.hivemq.hivemq-mqtt-client:hivemq-mqtt-client-reactor:develop-snapshot\")\n}\nmaven\n<project>\n...\n<repositories>\n<repository>\n<id>jitpack.io</id>\n<url>https://jitpack.io</url>\n</repository>\n</repositories>\n<dependencies>\n<dependency>\n<groupid>com.github.hivemq.hivemq-mqtt-client</groupid>\n<artifactid>hivemq-mqtt-client</artifactid>\n<version>develop-snapshot</version>\n</dependency>\n</dependencies>\n<!-- snapshots for optional modules -->\n<dependencies>\n<dependency>\n<groupid>com.github.hivemq.hivemq-mqtt-client</groupid>\n<artifactid>hivemq-mqtt-client-websocket</artifactid>\n<version>develop-snapshot</version>\n<type>pom</type>\n</dependency>\n</dependencies>\n<dependencies>\n<dependency>\n<groupid>com.github.hivemq.hivemq-mqtt-client</groupid>\n<artifactid>hivemq-mqtt-client-proxy</artifactid>\n<version>develop-snapshot</version>\n<type>pom</type>\n</dependency>\n</dependencies>\n<dependencies>\n<dependency>\n<groupid>com.github.hivemq.hivemq-mqtt-client</groupid>\n<artifactid>hivemq-mqtt-client-epoll</artifactid>\n<version>develop-snapshot</version>\n<type>pom</type>\n</dependency>\n</dependencies>\n<dependencies>\n<dependency>\n<groupid>com.github.hivemq.hivemq-mqtt-client</groupid>\n<artifactid>hivemq-mqtt-client-reactor</artifactid>\n<version>develop-snapshot</version>\n</dependency>\n</dependencies>\n...\n</project>\nchange the artifact name to hivemq-mqtt-client-shaded to get snapshots of the shaded version.\njitpack works for all -----> branches !!!  and also specific commits. just specify <branch>-snapshot or the first 10 digits of the commit id in the version.\ngeneral principles\napi and implementation are clearly separated. all classes inside internal packages must not be used directly.\nthe api is mostly fluent and uses fluent builders to create clients, configurations and messages.\nthe api is designed to be consistent:\nthe same principles are used throughout the library.\nthe mqtt 3 and 5 interfaces are as consistent as possible with only version-specific differences.\ncreation of clients\nbase classes: mqtt3client, mqtt5client\nmqtt5client client = mqttclient.builder()\n.identifier(uuid.randomuuid().tostring())\n.serverhost(\"broker.hivemq.com\")\n.usemqttversion5()\n.build();\nmqtt3client client = mqttclient.builder()...usemqttversion3().build();\nor if the version is known upfront:\nmqtt5client client = mqtt5client.builder()...build();\nmqtt3client client = mqtt3client.builder()...build();\nfor each api style exists a specific build...() method.\napi flavours\neach api style has its own interface to separate them clearly. at any time it is possible to switch the api style.\nblocking api\nbuilder method: buildblocking()\nswitch method: client.toblocking()\nexamples\nsubscribe example\nfinal mqtt5blockingclient client = mqtt5client.builder()\n.identifier(uuid.randomuuid().tostring())\n.serverhost(\"broker.hivemq.com\")\n.buildblocking();\nclient.connect();\ntry (final mqtt5publishes publishes = client.publishes(mqttglobalpublishfilter.all)) {\nclient.subscribewith().topicfilter(\"test/topic\").qos(mqttqos.at_least_once).send();\npublishes.receive(1, timeunit.seconds).ifpresent(system.out::println);\npublishes.receive(100, timeunit.milliseconds).ifpresent(system.out::println);\n} finally {\nclient.disconnect();\n}\npublish example\nmqtt5blockingclient client = mqtt5client.builder()\n.identifier(uuid.randomuuid().tostring())\n.serverhost(\"broker.hivemq.com\")\n.buildblocking();\nclient.connect();\nclient.publishwith().topic(\"test/topic\").qos(mqttqos.at_least_once).payload(\"1\".getbytes()).send();\nclient.disconnect();\nconnect\nclient.connect();\nor with customized properties of the connect message:\nclient.connectwith().keepalive(10).send();\nor with pre-built connect message:\nmqtt5connect connectmessage = mqtt5connect.builder().keepalive(10).build();\nclient.connect(connectmessage);\npublish\nclient.publishwith()\n.topic(\"test/topic\")\n.qos(mqttqos.at_least_once)\n.payload(\"payload\".getbytes())\n.send();\nor with pre-built publish message:\nmqtt5publish publishmessage = mqtt5publish.builder()\n.topic(\"test/topic\")\n.qos(mqttqos.at_least_once)\n.payload(\"payload\".getbytes())\n.build();\nclient.publish(publishmessage);\nsubscribe\nclient.subscribewith().topicfilter(\"test/topic\").qos(mqttqos.exactly_once).send();\nor with pre-built subscribe message:\nmqtt5subscribe subscribemessage = mqtt5subscribe.builder()\n.topicfilter(\"test/topic\")\n.qos(mqttqos.exactly_once)\n.build();\nclient.subscribe(subscribemessage);\nunsubscribe\nclient.unsubscribewith().topicfilter(\"test/topic\").send();\nor with pre-built unsubscribe message:\nmqtt5unsubscribe unsubscribemessage = mqtt5unsubscribe.builder().topicfilter(\"test/topic\").build();\nclient.unsubscribe(unsubscribemessage);\nconsume messages\ntry (mqtt5blockingclient.mqtt5publishes publishes = client.publishes(mqttglobalpublishfilter.all)) {\nmqtt5publish publishmessage = publishes.receive();\n// or with timeout\noptional<mqtt5publish> publishmessage = publishes.receive(10, timeunit.seconds);\n// or without blocking\noptional<mqtt5publish> publishmessage = publishes.receivenow();\n}\npublishes must be called before subscribe to ensure no message is lost. it can be called before connect to receive messages of a previous session.\ndisconnect\nclient.disconnect();\nor with customized properties of the disconnect message (only mqtt 5):\nclient.disconnectwith().reasonstring(\"test\").send();\nor with pre-built disconnect message (only mqtt 5):\nmqtt5disconnect disconnectmessage = mqtt5disconnect.builder().reasonstring(\"test\").build();\nclient.disconnect(disconnectmessage);\nreauth (only mqtt 5)\nclient.reauth();\nasync api\nbuilder method: buildasync()\nswitch method: client.toasync()\nexamples\nsubscribe example\nmqtt5blockingclient client = mqtt5client.builder()\n.identifier(uuid.randomuuid().tostring())\n.serverhost(\"broker.hivemq.com\")\n.buildblocking();\nclient.connect();\nclient.toasync().subscribewith()\n.topicfilter(\"test/topic\")\n.qos(mqttqos.at_least_once)\n.callback(system.out::println)\n.send();\npublish example\nmqtt5asyncclient client = mqtt5client.builder()\n.identifier(uuid.randomuuid().tostring())\n.serverhost(\"broker.hivemq.com\")\n.buildasync();\nclient.connect()\n.thencompose(connack -> client.publishwith().topic(\"test/topic\").payload(\"1\".getbytes()).send())\n.thencompose(publishresult -> client.disconnect());\nconnect\nconnect(), connectwith() and connect(mqtt3/5connect) method calls are analog to the blocking api but return completablefuture.\npublish\npublishwith() and publish(mqtt3/5publish) method calls are analog to the blocking api but return completablefuture.\nsubscribe\nsubscribewith() and subscribe(mqtt3/5subscribe) method calls are analog to the blocking api but return completablefuture.\nadditionally messages can be consumed per subscribe:\nclient.subscribewith()\n.topicfilter(\"test/topic\")\n.qos(mqttqos.exactly_once)\n.callback(system.out::println)\n.executor(executor) // optional\n.send();\nor with pre-built subscribe message:\nmqtt5subscribe subscribemessage = mqtt5subscribe.builder()\n.topicfilter(\"test/topic\")\n.qos(mqttqos.exactly_once)\n.build();\nclient.subscribe(subscribemessage, system.out::println);\nclient.subscribe(subscribemessage, system.out::println, executor);\nunsubscribe\nunsubscribewith() and unsubscribe(mqtt3/5unsubscribe) method calls are analog to the blocking api but return completablefuture.\nconsume messages\nmessages can either be consumed per subscribe (described above) or globally:\nclient.publishes(mqttglobalpublishfilter.all, system.out::println);\nor with executing the callback on a specified executor:\nclient.publishes(mqttglobalpublishfilter.all, system.out::println, executor);\npublishes must be called before subscribe to ensure no message is lost. it can be called before connect to receive messages of a previous session.\ndisconnect\ndisconnect(), disconnectwith() and disconnect(mqtt5disconnect) method calls are analog to the blocking api but return completablefuture.\nreauth (only mqtt 5)\nreauth() method call is analog to the blocking api but returns completablefuture.\nreactive api\nbuilder method: buildrx()\nswitch method: client.torx()\nexamples\nsubscribe example\nmqtt5rxclient client = mqtt5client.builder()\n.identifier(uuid.randomuuid().tostring())\n.serverhost(\"broker.hivemq.com\")\n.buildrx();\n// as we use the reactive api, the following line does not connect yet, but returns a reactive type.\n// e.g. single is something like a lazy and reusable future. think of it as a source for the connack message.\nsingle<mqtt5connack> connacksingle = client.connect();\n// same here: the following line does not subscribe yet, but returns a reactive type.\n// flowablewithsingle is a combination of the single suback message and a flowable of publish messages.\n// a flowable is an asynchronous stream that enables backpressure from the application over the client to the broker.\nflowablewithsingle<mqtt5publish, mqtt5suback> subackandmatchingpublishes = client.subscribestreamwith()\n.topicfilter(\"a/b/c\").qos(mqttqos.at_least_once)\n.addsubscription().topicfilter(\"a/b/c/d\").qos(mqttqos.exactly_once).applysubscription()\n.applysubscribe();\n// the reactive types offer many operators that will not be covered here.\n// here we register callbacks to print messages when we received the connack, suback and matching publish messages.\ncompletable connectscenario = connacksingle\n.doonsuccess(connack -> system.out.println(\"connected, \" + connack.getreasoncode()))\n.doonerror(throwable -> system.out.println(\"connection failed, \" + throwable.getmessage()))\n.ignoreelement();\ncompletable subscribescenario = subackandmatchingpublishes\n.doonsingle(suback -> system.out.println(\"subscribed, \" + suback.getreasoncodes()))\n.doonnext(publish -> system.out.println(\n\"received publish\" + \", topic: \" + publish.gettopic() + \", qos: \" + publish.getqos() +\n\", payload: \" + new string(publish.getpayloadasbytes())))\n.ignoreelements();\n// reactive types can be easily and flexibly combined\nconnectscenario.andthen(subscribescenario).blockingawait();\npublish example\nmqtt5rxclient client = mqtt5client.builder()\n.identifier(uuid.randomuuid().tostring())\n.serverhost(\"broker.hivemq.com\")\n.buildrx();\n// as we use the reactive api, the following line does not connect yet, but returns a reactive type.\ncompletable connectscenario = client.connect()\n.doonsuccess(connack -> system.out.println(\"connected, \" + connack.getreasoncode()))\n.doonerror(throwable -> system.out.println(\"connection failed, \" + throwable.getmessage()))\n.ignoreelement();\n// fake a stream of publish messages with an incrementing number in the payload\nflowable<mqtt5publish> messagestopublish = flowable.range(0, 10_000)\n.map(i -> mqtt5publish.builder()\n.topic(\"a/b/c\")\n.qos(mqttqos.at_least_once)\n.payload((\"test \" + i).getbytes())\n.build())\n// emit 1 message only every 100 milliseconds\n.zipwith(flowable.interval(100, timeunit.milliseconds), (publish, i) -> publish);\n// as we use the reactive api, the following line does not publish yet, but returns a reactive type.\ncompletable publishscenario = client.publish(messagestopublish)\n.doonnext(publishresult -> system.out.println(\n\"publish acknowledged: \" + new string(publishresult.getpublish().getpayloadasbytes())))\n.ignoreelements();\n// as we use the reactive api, the following line does not disconnect yet, but returns a reactive type.\ncompletable disconnectscenario = client.disconnect().dooncomplete(() -> system.out.println(\"disconnected\"));\n// reactive types can be easily and flexibly combined\nconnectscenario.andthen(publishscenario).andthen(disconnectscenario).blockingawait();\nconnect\nconnect(), connectwith() and connect(mqtt3/5connect) method calls are analog to the async and blocking api but return single<connack>.\npublish\npublish takes a reactive stream of publish messages (flowable) and returns a reactive stream of publish results (flowable).\nthe reactive api is usually not used for publishing single messages. nevertheless it is possible with the following code.\nsingle<mqtt5publishresult> result =\nclient.publish(flowable.just(mqtt5publish.builder()\n.topic(\"test/topic\")\n.qos(mqttqos.at_least_once)\n.payload(\"payload\".getbytes())\n.build())).singleorerror();\nsubscribe\nsubscribewith() and subscribe(mqtt3/5subscribe) method calls are analog to the async and blocking api but return single<suback>.\nadditionally messages can be consumed per subscribe:\nflowable<mqtt5publish> result =\nclient.subscribestreamwith()\n.topicfilter(\"test/topic\")\n.qos(mqttqos.exactly_once)\n.applysubscribe()\n.doonsingle(suback -> system.out.println(\"subscribed\"))\n.doonnext(publish -> system.out.println(\"received publish\"));\nor with pre-built subscribe message:\nmqtt5subscribe subscribemessage = mqtt5subscribe.builder()\n.topicfilter(\"test/topic\")\n.qos(mqttqos.exactly_once)\n.build();\nflowable<mqtt5publish> result =\nclient.subscribestreamwith(subscribemessage)\n.doonsingle(suback -> system.out.println(\"subscribed\"))\n.doonnext(publish -> system.out.println(\"received publish\"));\nunsubscribe\nunsubscribewith() and unsubscribe(mqtt3/5unsubscribe) method calls are analog to the async and blocking api but return single<unsuback>.\nconsume messages\nmessages can either be consumed per subscribe (described above) or globally:\nflowable<mqtt5publish> result =\nclient.publishes(mqttglobalpublishfilter.all).doonnext(system.out::println);\npublishes must be called before subscribe to ensure no message is lost. it can be called before connect to receive messages of a previous session.\ndisconnect\ndisconnect(), disconnectwith() and disconnect(mqtt5disconnect) method calls are analog to the async and blocking api but return completable.\nreauth (only mqtt 5)\nreauth() method call is analog to the async and blocking api but returns completable.\nversioning\nsemantic versioning is used.\nall code inside com.hivemq.client.internal packages must not be used directly. it can change at any time and is not part of the public api.\ninterfaces annotated with donotimplement must not be implemented. the implementation is provided by the library. this allows the library to later add methods to the interface without breaking backwards compatibility with implementing classes.\ncontributing\nif you want to contribute to hivemq mqtt client, see the contribution guidelines.\nlicense\nhivemq mqtt client is licensed under the apache license, version 2.0. a copy of the license can be found here.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000187, "year": null}, {"Unnamed: 0": 312, "autor": 312, "date": null, "content": "Anchorage House\nHome Assistant Configuration\nThis repo contains the working Home Assistant configuration for Anchorage House. Below are links to the devices currently being used, blog posts, and other HA enthusists that provided inspiration and configs to help build this config. All of the code is free to use.\nBe sure to follow me on twitter @thejeffreystone and on YouTube where I am starting to post videos of my Home Automation journey. I also post articles about this config and other Home Automation topics at slacker-labs.com\nYou can also follow Anchorage House on Twitter @anchoragehouse2 where it tweets about the cool stuff it does throughout the day as well as links to some of the best Home Assistent content creators out there.\nAnd of course, if you have hit that \u2b50 at the top to follow this repo what are you waiting for? You can get updates right in your notification feed everytime I push updates. Which is at least once a week and some weeks quite a bit more.\nAnchorage House's Three Laws of Home Automation\nWhen designing Anchorage House's automations I have made every effort to prioritize the solution based on the following three laws.\nFirst Law: Every automation or action should be the result of a passive sensor or indirect action.\nSecond Law: An automation can be triggered by voice command only when Law 1 cannot be achieved.\nThird Law: An automation or action can be trigger by a physical switch or as the result of a direct iteraction only when Law 1 and Law 2 cannot be achieved.\nFor more about how they are used visit https://slacker-labs.com/2020/04/02/the-three-laws-of-home-automation/\nThis is V5 of my config.\nMajor Changes in v5:\nMigrated from Hassbian to Hassio\nMigrated off Smartthings, which had been used a device hub. My zwave and zigbee devices are now using zwave2mqtt and zigbee2mqtt.\nSwapped the Honeywell Wifi Thermostat out for an Ecobee. This actually happened over the summer, and I am just now updating the readme...\nGeneral Information about Anchorage House\nHassio is the flavor of Home Assistant powering Anchorage House these days. Currently it is running on a RaspberryPi 3. I also have a Ubuntu Server that handles running some other things like my 433mhz data collection from varios temperature sensors, and Splunk for Home Assistant data analytics.\nThe old configurations are stored in branches for anyone that wants to see the previous iterations. However, the only branch guaranteed to work with the current version of Home Assistant is master.\nRead about the continuing work to automate everything over on slacker-labs.com. Feel free to reach out if you have questions. I love this stuff.\nOne last thing. Everything in this configuration is a combination of the things I want out of a home automation system flavored with the inspiration from others using Home Assistant. In some cases I took someone else's idea and made it my own, and in some I just completely stole it. So I would be remiss if I didn't acknowledge those that inpired this journey. If you like what you see here, please checkout their configs as well. I owe them thanks for sharing their work.\nCCOSTAN / https://www.vCloudInfo.com\nIsabella Gross Alstr\u00f6m\nMahasri Kalavala\nDevices:\nVarious Apple Devices (Macbooks, Mac Mini, iPhones, iPads)\nNetworking\nEero Mesh Network Routers\nAeotec Z-Stick Gen 5\nMedia\nAppleTv\nGoogle Home Hub / Home Mini\nAmazon Echo Dot\nAmazon Echo\nChromeCast\nRoku Premiere\nRoku Streaming Stick\nCameras\nArlo Pro 2 Cameras\nFoscam FI8918W\nWyze Cam\nSwitches / Plugs\nMyQ Garage Door Opener\nGE Z-Wave Switches\nSonoff WiFi Smart Switch\nLevitron Plug in ZWave Dimmer\nLevitron Z-Wave Switch\nWemo Wifi SmartPlug\nHoneywell Z-Wave Plug\nSylvania Smart Zigbee Plug\nWyze Plug\nHoneywell UltraPro Z-Wave Plus Smart Light Switch\nInnr Zigbee Plugs\nLights\nSengled LED Color Plus (Zigbee)\nCree Connect Bulbs (Zigbee)\nSylvania BR30 LED Bulb\nSensors\nZ-Wave Door Sensor\nZ-Wave Garage Door Tilt Sensor\nDome Motion Z-Wave Sensor\nAqara Motion Sensor\nAqara Temp / Humidity Sensor\nClimate\nEcobee Thermostat\nEcobee Temp and Motion Sensors\nUSB SDR For getting data from Accurite temperture and humidty sensors\nAccurite Temp and Humidty Sensors\nAcuRite Lightning Detector Sensor\nSecurity\nFirst Alert Z-Wave Smoke/CO2\nIntegrations:\nAmazon Echo - Voice Control and Music\nIFTTT - Automations\nLife360 - Presence Detection\nSpotify - Music\nGoogle (Calendar API, GMail for sensors, and Google Home) - Voice Control, Automations based on Calendar, USPS Informed Delivery\nWaze API - Transit time and Automations\nDropbox - Config backup\nMQTT - For Zwave, Zigbee, various sensors running on a different host.\nRTL_433 - Indoor and Outdoor Temperature Sensors\nCloudflare - 15 year SSL Cert\nSplunk - Data Analytics\nVarious other services and APIs\nWant to support this project?\nBuy me a coffee\nFind your next Home Automation device on Amazon using my affiliate link\nAffiliate Disclosure", "link": "https://github.com/thejeffreystone/home-assistant-configuration", "origin": "Github", "suborigin": "Iot", "result": true, "Selector": "branches", "selectorShort": "branch", "MarkedSent": "anchorage house\nhome assistant configuration\nthis repo contains the working home assistant configuration for anchorage house. below are links to the devices currently being used, blog posts, and other ha enthusists that provided inspiration and configs to help build this config. all of the code is free to use.\nbe sure to follow me on twitter @thejeffreystone and on youtube where i am starting to post videos of my home automation journey. i also post articles about this config and other home automation topics at slacker-labs.com\nyou can also follow anchorage house on twitter @anchoragehouse2 where it tweets about the cool stuff it does throughout the day as well as links to some of the best home assistent content creators out there.\nand of course, if you have hit that \u2b50 at the top to follow this repo what are you waiting for? you can get updates right in your notification feed everytime i push updates. which is at least once a week and some weeks quite a bit more.\nanchorage house's three laws of home automation\nwhen designing anchorage house's automations i have made every effort to prioritize the solution based on the following three laws.\nfirst law: every automation or action should be the result of a passive sensor or indirect action.\nsecond law: an automation can be triggered by voice command only when law 1 cannot be achieved.\nthird law: an automation or action can be trigger by a physical switch or as the result of a direct iteraction only when law 1 and law 2 cannot be achieved.\nfor more about how they are used visit https://slacker-labs.com/2020/04/02/the-three-laws-of-home-automation/\nthis is v5 of my config.\nmajor changes in v5:\nmigrated from hassbian to hassio\nmigrated off smartthings, which had been used a device hub. my zwave and zigbee devices are now using zwave2mqtt and zigbee2mqtt.\nswapped the honeywell wifi thermostat out for an ecobee. this actually happened over the summer, and i am just now updating the readme...\ngeneral information about anchorage house\nhassio is the flavor of home assistant powering anchorage house these days. currently it is running on a raspberrypi 3. i also have a ubuntu server that handles running some other things like my 433mhz data collection from varios temperature sensors, and splunk for home assistant data analytics.\nthe old configurations are stored in -----> branches !!!  for anyone that wants to see the previous iterations. however, the only branch guaranteed to work with the current version of home assistant is master.\nread about the continuing work to automate everything over on slacker-labs.com. feel free to reach out if you have questions. i love this stuff.\none last thing. everything in this configuration is a combination of the things i want out of a home automation system flavored with the inspiration from others using home assistant. in some cases i took someone else's idea and made it my own, and in some i just completely stole it. so i would be remiss if i didn't acknowledge those that inpired this journey. if you like what you see here, please checkout their configs as well. i owe them thanks for sharing their work.\nccostan / https://www.vcloudinfo.com\nisabella gross alstr\u00f6m\nmahasri kalavala\ndevices:\nvarious apple devices (macbooks, mac mini, iphones, ipads)\nnetworking\neero mesh network routers\naeotec z-stick gen 5\nmedia\nappletv\ngoogle home hub / home mini\namazon echo dot\namazon echo\nchromecast\nroku premiere\nroku streaming stick\ncameras\narlo pro 2 cameras\nfoscam fi8918w\nwyze cam\nswitches / plugs\nmyq garage door opener\nge z-wave switches\nsonoff wifi smart switch\nlevitron plug in zwave dimmer\nlevitron z-wave switch\nwemo wifi smartplug\nhoneywell z-wave plug\nsylvania smart zigbee plug\nwyze plug\nhoneywell ultrapro z-wave plus smart light switch\ninnr zigbee plugs\nlights\nsengled led color plus (zigbee)\ncree connect bulbs (zigbee)\nsylvania br30 led bulb\nsensors\nz-wave door sensor\nz-wave garage door tilt sensor\ndome motion z-wave sensor\naqara motion sensor\naqara temp / humidity sensor\nclimate\necobee thermostat\necobee temp and motion sensors\nusb sdr for getting data from accurite temperture and humidty sensors\naccurite temp and humidty sensors\nacurite lightning detector sensor\nsecurity\nfirst alert z-wave smoke/co2\nintegrations:\namazon echo - voice control and music\nifttt - automations\nlife360 - presence detection\nspotify - music\ngoogle (calendar api, gmail for sensors, and google home) - voice control, automations based on calendar, usps informed delivery\nwaze api - transit time and automations\ndropbox - config backup\nmqtt - for zwave, zigbee, various sensors running on a different host.\nrtl_433 - indoor and outdoor temperature sensors\ncloudflare - 15 year ssl cert\nsplunk - data analytics\nvarious other services and apis\nwant to support this project?\nbuy me a coffee\nfind your next home automation device on amazon using my affiliate link\naffiliate disclosure", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000312, "year": null}, {"Unnamed: 0": 340, "autor": 340, "date": null, "content": "Hi!\nThis is an IEEE802.15.4 O-QPSK transceiver for GNU Radio, based on Thomas Schmid's implementation.\nDevelopment\nLike GNU Radio, this module uses maint branches for development. These branches are supposed to be used with the corresponding GNU Radio branches. This means: the maint-3.7 branch is compatible with GNU Radio 3.7, maint-3.8 is compatible with GNU Radio 3.8, etc.\nFeatures\nThe O-QPSK PHY encapsulated in a hierarchical block.\nThe CSS PHY, also encapsulated in a hierarchical block (Limitation: Packets need to have a fixed length).\nA block that implements the Rime communication stack. Rime is a lightweight communication stack designed for Wireless Sensor Networks and is part of the Contiki Operating System.\nA transceiver flow graph with USRP <-> PHY <-> MAC <-> Network layer (Rime) <-> UDP Socket / APP which resembles pretty well the ISO/OSI structure.\nA sample application which visualizes sensor values. The application shows how easy it is to connect an external program to the flow graph by using Socket PDU blocks.\nAn IEEE 802.15.4 and Rime dissector for Wireshark.\nSome interesting properties:\nPackets can be piped to Wireshark.\nThe complete physical modulation is done with plain GNU Radio blocks.\nIt is interoperable with TelosB sensor motes.\nIt is interoperable with Contiki.\nIt uses a block to tag packet bursts with tx_sob and tx_eob tags. This tags are understood by the UHD blocks and allow fast switching between transmission and reception.\nYou can find the firmware that I used to test interoperability with TelosB motes in the contiki folder. The firmware is based on Contiki v2.6. There is another README file in the Contiki folder that describes how to compile and use the firmware.\nDependencies\nGNU Radio\ngr-foo (Wireshark Connector, Packet Pad and Burst Tagger blocks)\nhttps://github.com/bastibl/gr-foo.git\npython-matplotlib (if you want to run the GUI sample application)\nsudo apt-get install python-matplotlib\nInstallation\nPlease see www.wime-project.net for installation instructions.\nUsage\nOpen the examples/transceiver_*.grc flow graph with gnuradio-companion and check if all blocks are connected. Enable either the UHD blocks to interface with real hardware or the Packet Pad block to loop back the samples. Open some Rime connections and connect messages sources or Socket PDUs. You can easily connect to the Socket PDU blocks with netcat. Netcat can be started for example with\nnc -u localhost 52001\nThere are also startup scripts in the apps folder.\nHave fun!", "link": "https://github.com/bastibl/gr-ieee802-15-4", "origin": "Github", "suborigin": "Iot", "result": true, "Selector": "branches", "selectorShort": "branch", "MarkedSent": "hi!\nthis is an ieee802.15.4 o-qpsk transceiver for gnu radio, based on thomas schmid's implementation.\ndevelopment\nlike gnu radio, this module uses maint -----> branches !!!  for development. these branches are supposed to be used with the corresponding gnu radio branches. this means: the maint-3.7 branch is compatible with gnu radio 3.7, maint-3.8 is compatible with gnu radio 3.8, etc.\nfeatures\nthe o-qpsk phy encapsulated in a hierarchical block.\nthe css phy, also encapsulated in a hierarchical block (limitation: packets need to have a fixed length).\na block that implements the rime communication stack. rime is a lightweight communication stack designed for wireless sensor networks and is part of the contiki operating system.\na transceiver flow graph with usrp <-> phy <-> mac <-> network layer (rime) <-> udp socket / app which resembles pretty well the iso/osi structure.\na sample application which visualizes sensor values. the application shows how easy it is to connect an external program to the flow graph by using socket pdu blocks.\nan ieee 802.15.4 and rime dissector for wireshark.\nsome interesting properties:\npackets can be piped to wireshark.\nthe complete physical modulation is done with plain gnu radio blocks.\nit is interoperable with telosb sensor motes.\nit is interoperable with contiki.\nit uses a block to tag packet bursts with tx_sob and tx_eob tags. this tags are understood by the uhd blocks and allow fast switching between transmission and reception.\nyou can find the firmware that i used to test interoperability with telosb motes in the contiki folder. the firmware is based on contiki v2.6. there is another readme file in the contiki folder that describes how to compile and use the firmware.\ndependencies\ngnu radio\ngr-foo (wireshark connector, packet pad and burst tagger blocks)\nhttps://github.com/bastibl/gr-foo.git\npython-matplotlib (if you want to run the gui sample application)\nsudo apt-get install python-matplotlib\ninstallation\nplease see www.wime-project.net for installation instructions.\nusage\nopen the examples/transceiver_*.grc flow graph with gnuradio-companion and check if all blocks are connected. enable either the uhd blocks to interface with real hardware or the packet pad block to loop back the samples. open some rime connections and connect messages sources or socket pdus. you can easily connect to the socket pdu blocks with netcat. netcat can be started for example with\nnc -u localhost 52001\nthere are also startup scripts in the apps folder.\nhave fun!", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000340, "year": null}, {"Unnamed: 0": 497, "autor": 497, "date": null, "content": "Azure SDK for Embedded C\nThe Azure SDK for Embedded C is designed to allow small embedded (IoT) devices to communicate with Azure services. Since we expect our client library code to run on microcontrollers, which have very limited amounts of flash and RAM, and have slower CPUs, our C SDK does things very differently than the SDKs we offer for other languages.\nWith this in mind, there are many tenets or principles that we follow in order to properly address this target audience:\nCustomers of our SDK compile our source code along with their own.\nWe target the C99 programming language and test with gcc, clang, & MS Visual C compilers.\nWe offer very few abstractions making our code easy to understand and debug.\nOur SDK is non allocating. That is, customers must allocate our data structures where they desire (global memory, heap, stack, etc.) and then pass the address of the allocated structure into our functions to initialize them and in order to perform various operations.\nUnlike our other language SDKs, many things (such as composing an HTTP pipeline of policies) are done in source code as opposed to runtime. This reduces code size, improves execution speed and locks-in behavior, reducing the chance of bugs at runtime.\nWe support microcontrollers with no operating system, microcontrollers with a real-time operating system (like Azure RTOS), Linux, and Windows. Customers can implement custom platform layers to use our SDK on custom devices. We provide some platform layers, and encourage the community to submit platform layers to increase the out-of-the-box supported platforms.\nFor higher level abstractions built on top of this repo, please see the following:\nAzure IoT middleware for Azure RTOS builds on top of the embedded SDK and tightly couples with the Azure RTOS family of networking and OS products. This gives you very performant and small applications for real-time, constrained devices.\nAzure IoT middleware for FreeRTOS builds on top of the embedded SDK and takes care of the MQTT stack while integrating with FreeRTOS. This maintains the focus on constrained devices and gives users a distilled Azure IoT feature set while allowing for flexibility with their networking stack.\nTable of Contents\nAzure SDK for Embedded C\nTable of Contents\nDocumentation\nThe GitHub Repository\nServices\nStructure\nMain Branch\nRelease Branches and Release Tagging\nGetting Started Using the SDK\nCMake\nCMake Options\nConsume SDK for C as Dependency with CMake\nVisual Studio Code\nSource Files (IDE, command line, etc)\nRunning Samples\nStorage Sample\nLibcurl Global Init and Global Clean Up\nIoT samples\nDevelopment Environment\nWindows\nVisual Studio 2019\nLinux\nvcpkg\nDebian\nBuild\nMac\nvcpkg\nBuild\nUsing your own HTTP stack implementation\nLink your application with your own HTTP stack\nSDK Architecture\nContributing\nAdditional Helpful Links for Contributors\nCommunity\nReporting Security Issues and Security Bugs\nLicense\nDocumentation\nWe use doxygen to generate documentation for source code. You can find the generated, versioned documentation here.\nThe GitHub Repository\nTo get help with the SDK:\nFile a Github Issue.\nAsk new questions or see others' questions on Stack Overflow using the azure and c tags.\nServices\nThe Azure SDK for Embedded C repo has been structured around the service libraries it provides:\nIoT - Library to connect Embedded Devices to Azure IoT services\nStructure\nThis repo is structured with two priorities:\nSeparation of services/features to make it easier to find relevant information and resources.\nSimplified source file structuring to easily integrate features into a user's project.\n/sdk - folder containing docs, sources, samples, tests for all SDK packages\n/docs - documentation for each service (iot, etc)\n/inc - include directory - can be singularly included in your project to resolve all headers\n/samples - samples for each service\n/src - source files for each service\n/tests - tests for each service\nFor instructions on how to consume the libraries via CMake, please see here. For instructions on how consume the source code in an IDE, command line, or other build systems, please see here.\nMain Branch\nThe main branch has the most recent code with new features and bug fixes. It does not represent the latest General Availability (GA) release of the SDK.\nRelease Branches and Release Tagging\nWhen we make an official release, we will create a unique git tag containing the name and version to mark the commit. We'll use this tag for servicing via hotfix branches as well as debugging the code for a particular beta or stable release version. A release tag looks like this:\n<package-name>_<package-version>\nThe latest release can be found in the release section of this repo.\nFor more information, please see this branching strategy document.\nGetting Started Using the SDK\nThe SDK can be conveniently consumed either via CMake or other non-CMake methods (IDE workspaces, command line, and others).\nCMake\nInstall the required prerequisites:\nCMake version 3.10 or later\nC compiler: MSVC, gcc or clang are recommended\ngit to clone our Azure SDK repository with the desired tag\nClone our Azure SDK repository, optionally using the desired version tag.\ngit clone https://github.com/Azure/azure-sdk-for-c\ngit checkout <tag_name>\nFor information about using a specific client library, see the README file located in the client library's folder which is a subdirectory under the /sdk/docs folder.\nEnsure the SDK builds correctly.\nCreate an output directory for your build artifacts (in this example, we named it build, but you can pick any name).\nmkdir build\nNavigate to that newly created directory.\ncd build\nRun cmake pointing to the sources at the root of the repo to generate the builds files.\ncmake ..\nLaunch the underlying build system to compile the libraries.\ncmake --build .\nThis results in building each library as a static library file, placed in the output directory you created (for example build\\sdk\\core\\az_core\\Debug). At a minimum, you must have an Azure Core library, a Platform library, and an HTTP library. Then, you can build any additional Azure service client library you intend to use from within your application (for example build\\sdk\\iot\\Debug). To use our client libraries in your application, just #include our public header files and then link your application's object files with our library files.\nProvide platform-specific implementations for functionality required by Azure Core. For more information, see the Azure Core Porting Guide.\nCMake Options\nBy default, when building the project with no options, the following static libraries are generated:\nLibraries:\naz_core\naz_span, az_http, az_json, etc.\naz_iot\niot_provisioning, iot_hub, etc.\naz_noplatform\nA platform abstraction which will compile but returns AZ_ERROR_DEPENDENCY_NOT_PROVIDED from all its functions. This ensures the project can be compiled without the need to provide any specific platform implementation. This is useful if you want to use az_core without platform specific functions like time or sleep.\naz_nohttp\nLibrary that provides a no-op HTTP stack, returning AZ_ERROR_DEPENDENCY_NOT_PROVIDED. Similar to az_noplatform, this library ensures the project can be compiled without requiring any HTTP stack implementation. This is useful if you want to use az_core without az_http functionality.\nThe following CMake options are available for adding/removing project features.\nOption Description Default Value\nUNIT_TESTING Generates Unit Test for compilation. When turning this option ON, cmocka is a required dependency for compilation.\nAfter Compiling, use `ctest` to run Unit Test. OFF\nUNIT_TESTING_MOCKS This option works only with GCC. It uses -ld option from linker to mock functions during unit test. This is used to test platform or HTTP functions by mocking the return values. OFF\nPRECONDITIONS Turning this option OFF would remove all method contracts. This is typically for shipping libraries for production to make it as optimized as possible. ON\nTRANSPORT_CURL This option requires Libcurl dependency to be available. It generates an HTTP stack with libcurl for az_http to be able to send requests thru the wire. This library would replace the no_http. OFF\nTRANSPORT_PAHO This option requires paho-mqtt dependency to be available. Provides Paho MQTT support for IoT. OFF\nAZ_PLATFORM_IMPL This option can be set to any of the next values:\n- No_value: default value is used and no_platform library is used.\n- \"POSIX\": Provides implementation for Linux and Mac systems.\n- \"WIN32\": Provides platform implementation for Windows based system\n- \"CUSTOM\": Tells cmake to use an specific implementation provided by user. When setting this option, user must provide an implementation library and set option `AZ_CUSTOM_PLATFORM_IMPL_NAME` with the name of the library (i.e. -DAZ_PLATFORM_IMPL=CUSTOM -DAZ_CUSTOM_PLATFORM_IMPL_NAME=user_platform_lib). cmake will look for this library to link az_core No_value\nSamples: Storage Samples are built by default using the default PAL and HTTP adapter (see running samples). This means that running samples without building an HTTP transport adapter would throw errors like:\n./blobs_client_example.exe\nRunning sample with no_op HTTP implementation.\nRecompile az_core with an HTTP client implementation like CURL to see sample sending network requests.\ni.e. cmake -DTRANSPORT_CURL=ON ..\nConsume SDK for C as Dependency with CMake\nAzure SDK for C can be automatically checked out by cmake and become a build dependency. This is done by using FetchContent.\nUsing this option would skip manually getting the Azure SDK for C source code to build and installing it (or making it available from some include path). Instead, CMake would do this for us.\nAzure SDK for C provides a CMake module that can be copied and used for this purpose.\nVisual Studio Code\nFor convenience, you can quickly get started using Visual Studio Code and the CMake Extension by Microsoft. Included in the repo is a settings.json file here which the extension will use to configure a CMake project. To use it, copy the settings.json file from .vscode-config to your own .vscode directory. With this, you can run and debug samples and tests. Modify the variables in the file to your liking or as instructed by sample documentation and then select the following button in the extension:\nFrom there you can select targets to build and debug.\nNOTE: Especially on Windows, make sure you select a compiler platform version that matches the dependencies installed via vcpkg (i.e. x64 or x86). Additionally, the triplet to use should be specified in the VCPKG_DEFAULT_TRIPLET field in settings.json.\nSource Files (IDE, command line, etc)\nWe have set up the repo for easy integration into other projects which don't use CMake. Two main features make this possible:\nTo resolve all header file relative paths, you only need to include sdk/inc in your project. All header files are included in the sdk with relative paths to clearly demarcate the services they belong to. A couple examples being:\n#include <azure/core/az_span.h>\n#include <azure/iot/az_iot_hub_client.h>\nAll source files are placed in a directory structure similar to the headers: sdk/src. Each service has its own subdirectory to separate files which you may be singularly interested in.\nTo use a specific service/feature, you may include the header file with the function declaration and compile the according .c containing the function implementation with your project.\nThe specific dependencies of each service may vary, but a couple rules of thumb should resolve the most typical of issues.\nAll services depend on core (source files here). You may compile these files with your project to resolve core dependencies.\nMost services will require a platform file to be compiled with your project (see here for porting instructions). We have provided several implementations already here for windows, posix, and a no_platform for no-op stubs. Please compile one of these, for your respective platform, with your project.\nThe following compilation, preprocessor options will add or remove functionality in the SDK.\nOption Description\nAZ_NO_PRECONDITION_CHECKING Turns off precondition checks to maximize performance with removal of function precondition checking.\nAZ_NO_LOGGING Removes all logging code and artifacts from the SDK (helps reduce code size).\nRunning Samples\nSee cmake options to learn about how to build an HTTP transport adapter, how to build IoT samples, and to turn logging on.\nStorage Sample\nThe storage sample expects a storage account with a container and SaS token used for authentication to be set in an environment variable AZURE_STORAGE_URL.\nNote: Building samples can be disabled by setting AZ_SDK_C_NO_SAMPLES environment variable.\n# On linux, set env var like this. For Windows, do it from advanced settings/ env variables\nexport ENV_URL=\"https://??????????????\"\nLibcurl Global Init and Global Clean Up\nWhen you select to build the libcurl http stack implementation, you have to make sure to call curl_global_init before using SDK client to send HTTP request to Azure.\nYou need to also call curl_global_cleanup once you no longer need to perform SDk client API calls.\nNote how you can use function atexit() to set libcurl global clean up.\nThe reason for this is the fact of this functions are not thread-safe, and a customer can use libcurl not only for Azure SDK library but for some other purpose. More info here.\nThis is libcurl specific only.\nIoT samples\nSamples for IoT will be built only when CMake option TRANSPORT_PAHO is set. See compiler options. For more information about IoT APIs and samples, see Azure IoT Clients.\nDevelopment Environment\nProject contains files to work on Windows, Mac or Linux based OS.\nNote For any environment variables set to use with CMake, the environment variables must be set BEFORE the first cmake generation command (cmake ..). The environment variables will NOT be picked up if you have already generated the build files, set environment variables, and then regenerate. In that case, you must either delete the CMakeCache.txt file or delete the folder in which you are generating build files and start again.\nWindows\nvcpkg is the easiest way to have dependencies installed. It downloads packages sources, headers and build libraries for whatever TRIPLET is set up (platform/arq). vcpkg maintains any installed package inside its own folder, allowing to have multiple vcpkg folder with different dependencies installed on each. This is also great because you don't have to install dependencies globally on your system.\nUse the following steps to install vcpkg and have it linked to CMake.\nNote: The Azure SDK is only officially supported against certain versions of vcpkg. Use the commit in vcpkg-commit.txt to get a known working version.\n# Clone vcpkg:\ngit clone https://github.com/Microsoft/vcpkg.git\n# (consider this path as PATH_TO_VCPKG)\ncd vcpkg\n# Checkout the vcpkg commit from the vcpkg-commit.txt file (link above)\ngit checkout <vcpkg commit>\n# build vcpkg\n.\\bootstrap-vcpkg.bat\n# install dependencies and update triplet\n.\\vcpkg.exe install --triplet x64-windows-static curl[winssl] cmocka paho-mqtt\n# Add this environment variables to link this vcpkg folder with cmake:\n# VCPKG_DEFAULT_TRIPLET=x64-windows-static\n# VCPKG_ROOT=PATH_TO_VCPKG (replace PATH_TO_VCPKG for where vcpkg is installed)\nNote: Setting up a development environment in windows without vcpkg is not supported. It requires installing all dev-dependencies globally and manually setting cmake files to link each of them.\nFollow next steps to build project from command prompt:\n# cd to project folder\ncd azure-sdk-for-c\n# create a new folder to generate cmake files for building (i.e. build)\nmkdir build\ncd build\n# generate files\n# cmake will automatically detect what C compiler is used by system by default and will generate files for it\ncmake ..\n# compile files. Cmake would call compiler and linker to generate libs\ncmake --build .\nNote: The steps above would compile and generate the default output for azure-sdk-for-c which includes static libraries only. See section CMake Options\nVisual Studio 2019\nOpen project folder with Visual Studio. If vcpkg has been previously installed and set up like mentioned above. Everything will be ready to build. Right after opening project, Visual Studio will read cmake files and generate cache files automatically.\nLinux\nvcpkg\nvcpkg can be used to download packages sources, headers and build libraries for whatever TRIPLET is set up (platform/architecture). vcpkg maintains any installed package inside its own folder, allowing to have multiple vcpkg folder with different dependencies installed on each. This is also great because you don't have to install dependencies globally on your system.\nUse the following steps to install vcpkg and have it linked to CMake.\nNote: The Azure SDK is only officially supported against certain versions of vcpkg. Use the commit in vcpkg-commit.txt to get a known working version.\n# Clone vcpkg:\ngit clone https://github.com/Microsoft/vcpkg.git\n# (consider this path as PATH_TO_VCPKG)\ncd vcpkg\n# Checkout the vcpkg commit from the vcpkg-commit.txt file (link above)\ngit checkout <vcpkg commit>\n# build vcpkg\n./bootstrap-vcpkg.sh\n./vcpkg install --triplet x64-linux curl cmocka paho-mqtt\nexport VCPKG_DEFAULT_TRIPLET=x64-linux\nexport VCPKG_ROOT=PATH_TO_VCPKG #replace PATH_TO_VCPKG for where vcpkg is installed\nDebian\nAlternatively, for Ubuntu 18.04 you can use:\nsudo apt install build-essential cmake libcmocka-dev libcmocka0 gcovr lcov doxygen curl libcurl4-openssl-dev libssl-dev ca-certificates\nBuild\n# cd to project folder\ncd azure-sdk-for-c\n# create a new folder to generate cmake files for building (i.e. build)\nmkdir build\ncd build\n# generate files\n# cmake will automatically detect what C compiler is used by system by default and will generate files for it\ncmake ..\n# compile files. Cmake would call compiler and linker to generate libs\nmake\nNote: The steps above would compile and generate the default output for azure-sdk-for-c which includes static libraries only. See section CMake Options\nMac\nvcpkg\nvcpkg can be used to download packages sources, headers and build libraries for whatever TRIPLET is set up (platform/architecture). vcpkg maintains any installed package inside its own folder, allowing to have multiple vcpkg folder with different dependencies installed on each. This is also great because you don't have to install dependencies globally on your system.\nFirst, ensure that you have the latest gcc installed:\nbrew update\nbrew upgrade\nbrew info gcc\nbrew install gcc\nbrew cleanup\nUse the following steps to install vcpkg and have it linked to CMake.\nNote: The Azure SDK is only officially supported against certain versions of vcpkg. Use the commit in vcpkg-commit.txt to get a known working version.\n# Clone vcpkg:\ngit clone https://github.com/Microsoft/vcpkg.git\n# (consider this path as PATH_TO_VCPKG)\ncd vcpkg\n# Checkout the vcpkg commit from the vcpkg-commit.txt file (link above)\ngit checkout <vcpkg commit>\n# build vcpkg\n./bootstrap-vcpkg.sh\n./vcpkg install --triplet x64-osx curl cmocka paho-mqtt\nexport VCPKG_DEFAULT_TRIPLET=x64-osx\nexport VCPKG_ROOT=PATH_TO_VCPKG #replace PATH_TO_VCPKG for where vcpkg is installed\nBuild\n# cd to project folder\ncd azure-sdk-for-c\n# create a new folder to generate cmake files for building (i.e. build)\nmkdir build\ncd build\n# generate files\n# cmake will automatically detect what C compiler is used by system by default and will generate files for it\ncmake ..\n# compile files. Cmake would call compiler and linker to generate libs\nmake\nNote: The steps above would compile and generate the default output for azure-sdk-for-c which includes static libraries only. See section CMake Options\nUsing your own HTTP stack implementation\nYou can create and use your own HTTP stack and adapter. This is to avoid the libcurl implementation from Azure SDK.\nThe first step is to understand the two components that are required. The first one is an HTTP stack implementation that is capable of sending bits through the wire. Some examples of these are libcurl, win32, etc.\nThe second component is an HTTP transport adapter. This is the implementation code which takes an http request from Azure SDK Core and uses it to send it using the specific HTTP stack implementation. Azure SDK Core provides the next contract that this component needs to implement:\nAZ_NODISCARD az_result\naz_http_client_send_request(az_http_request const* request, az_http_response* ref_response);\nFor example, Azure SDK provides a cmake target az_curl (find it here) with the implementation code for the contract function mentioned before. It uses an az_http_request reference to create an specific libcurl request and send it though the wire. Then it uses libcurl response to fill the az_http_response reference structure.\nLink your application with your own HTTP stack\nCreate your own http adapter for an Http stack and then use the following cmake command to have it linked to your application\ntarget_link_libraries(your_application_target PRIVATE lib_adapter http_stack_lib)\n# For instance, this is how we link libcurl and its adapter\ntarget_link_libraries(blobs_client_example PRIVATE az_curl CURL::libcurl)\nSee the complete cmake file and how to link your own library here\nSDK Architecture\nAt the heart of our SDK is, what we refer to as, Azure Core. This code defines several data types and functions for use by the client libraries that build on top of us such as the Azure IoT client libraries. Here are some of the features that customers use directly:\nSpans: A span represents a byte buffer and is used for string manipulations, HTTP requests/responses, reading/writing JSON payloads. It allows us to return a substring within a larger string without any memory allocations. See the Working With Spans section of the Azure Core README for more information.\nLogging: As our SDK performs operations, it can send log messages to a customer-defined callback. Customers can enable this to assist with debugging and diagnosing issues when leveraging our SDK code. See the Logging SDK Operations section of the Azure Core README for more information.\nContexts: Contexts offer an I/O cancellation mechanism. Multiple contexts can be composed together in your application's call tree. When a context is canceled, its children are also canceled. See the Canceling an Operation section of the Azure Core README for more information.\nJSON: Non-allocating JSON reading and JSON writing data structures and operations.\nHTTP: Non-allocating HTTP request and HTTP response data structures and operations.\nArgument Validation: The SDK validates function arguments and invokes a callback when validation fails. By default, this callback suspends the calling thread forever. However, you can override this behavior and, in fact, you can disable all argument validation to get smaller and faster code. See the SDK Function Argument Validation section of the Azure Core README for more information.\nIn addition to the above features, Azure Core provides features available to client libraries written to access other Azure services. Customers use these features indirectly by way of interacting with a client library. By providing these features in Azure Core, the client libraries built on top of us will share a common implementation and many features will behave identically across client libraries. For example, Azure Core offers a standard set of credential types and an HTTP pipeline with logging, retry, and telemetry policies.\nContributing\nFor details on contributing to this repository, see the contributing guide.\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.microsoft.com.\nWhen you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repositories using our CLA.\nThis project has adopted the Microsoft Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.\nAdditional Helpful Links for Contributors\nMany people all over the world have helped make this project better. You'll want to check out:\nWhat are some good first issues for new contributors to the repo?\nHow to build and test your change\nHow you can make a change happen!\nCommunity\nChat with other community members\nReporting Security Issues and Security Bugs\nSecurity issues and bugs should be reported privately, via email, to the Microsoft Security Response Center (MSRC) secure@microsoft.com. You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Further information, including the MSRC PGP key, can be found in the Security TechCenter.\nLicense\nAzure SDK for Embedded C is licensed under the MIT license.", "link": "https://github.com/Azure/azure-sdk-for-c", "origin": "Github", "suborigin": "Iot", "result": true, "Selector": "branches", "selectorShort": "branch", "MarkedSent": "azure sdk for embedded c\nthe azure sdk for embedded c is designed to allow small embedded (iot) devices to communicate with azure services. since we expect our client library code to run on microcontrollers, which have very limited amounts of flash and ram, and have slower cpus, our c sdk does things very differently than the sdks we offer for other languages.\nwith this in mind, there are many tenets or principles that we follow in order to properly address this target audience:\ncustomers of our sdk compile our source code along with their own.\nwe target the c99 programming language and test with gcc, clang, & ms visual c compilers.\nwe offer very few abstractions making our code easy to understand and debug.\nour sdk is non allocating. that is, customers must allocate our data structures where they desire (global memory, heap, stack, etc.) and then pass the address of the allocated structure into our functions to initialize them and in order to perform various operations.\nunlike our other language sdks, many things (such as composing an http pipeline of policies) are done in source code as opposed to runtime. this reduces code size, improves execution speed and locks-in behavior, reducing the chance of bugs at runtime.\nwe support microcontrollers with no operating system, microcontrollers with a real-time operating system (like azure rtos), linux, and windows. customers can implement custom platform layers to use our sdk on custom devices. we provide some platform layers, and encourage the community to submit platform layers to increase the out-of-the-box supported platforms.\nfor higher level abstractions built on top of this repo, please see the following:\nazure iot middleware for azure rtos builds on top of the embedded sdk and tightly couples with the azure rtos family of networking and os products. this gives you very performant and small applications for real-time, constrained devices.\nazure iot middleware for freertos builds on top of the embedded sdk and takes care of the mqtt stack while integrating with freertos. this maintains the focus on constrained devices and gives users a distilled azure iot feature set while allowing for flexibility with their networking stack.\ntable of contents\nazure sdk for embedded c\ntable of contents\ndocumentation\nthe github repository\nservices\nstructure\nmain branch\nrelease -----> branches !!!  and release tagging\ngetting started using the sdk\ncmake\ncmake options\nconsume sdk for c as dependency with cmake\nvisual studio code\nsource files (ide, command line, etc)\nrunning samples\nstorage sample\nlibcurl global init and global clean up\niot samples\ndevelopment environment\nwindows\nvisual studio 2019\nlinux\nvcpkg\ndebian\nbuild\nmac\nvcpkg\nbuild\nusing your own http stack implementation\nlink your application with your own http stack\nsdk architecture\ncontributing\nadditional helpful links for contributors\ncommunity\nreporting security issues and security bugs\nlicense\ndocumentation\nwe use doxygen to generate documentation for source code. you can find the generated, versioned documentation here.\nthe github repository\nto get help with the sdk:\nfile a github issue.\nask new questions or see others' questions on stack overflow using the azure and c tags.\nservices\nthe azure sdk for embedded c repo has been structured around the service libraries it provides:\niot - library to connect embedded devices to azure iot services\nstructure\nthis repo is structured with two priorities:\nseparation of services/features to make it easier to find relevant information and resources.\nsimplified source file structuring to easily integrate features into a user's project.\n/sdk - folder containing docs, sources, samples, tests for all sdk packages\n/docs - documentation for each service (iot, etc)\n/inc - include directory - can be singularly included in your project to resolve all headers\n/samples - samples for each service\n/src - source files for each service\n/tests - tests for each service\nfor instructions on how to consume the libraries via cmake, please see here. for instructions on how consume the source code in an ide, command line, or other build systems, please see here.\nmain branch\nthe main branch has the most recent code with new features and bug fixes. it does not represent the latest general availability (ga) release of the sdk.\nrelease branches and release tagging\nwhen we make an official release, we will create a unique git tag containing the name and version to mark the commit. we'll use this tag for servicing via hotfix branches as well as debugging the code for a particular beta or stable release version. a release tag looks like this:\n<package-name>_<package-version>\nthe latest release can be found in the release section of this repo.\nfor more information, please see this branching strategy document.\ngetting started using the sdk\nthe sdk can be conveniently consumed either via cmake or other non-cmake methods (ide workspaces, command line, and others).\ncmake\ninstall the required prerequisites:\ncmake version 3.10 or later\nc compiler: msvc, gcc or clang are recommended\ngit to clone our azure sdk repository with the desired tag\nclone our azure sdk repository, optionally using the desired version tag.\ngit clone https://github.com/azure/azure-sdk-for-c\ngit checkout <tag_name>\nfor information about using a specific client library, see the readme file located in the client library's folder which is a subdirectory under the /sdk/docs folder.\nensure the sdk builds correctly.\ncreate an output directory for your build artifacts (in this example, we named it build, but you can pick any name).\nmkdir build\nnavigate to that newly created directory.\ncd build\nrun cmake pointing to the sources at the root of the repo to generate the builds files.\ncmake ..\nlaunch the underlying build system to compile the libraries.\ncmake --build .\nthis results in building each library as a static library file, placed in the output directory you created (for example build\\sdk\\core\\az_core\\debug). at a minimum, you must have an azure core library, a platform library, and an http library. then, you can build any additional azure service client library you intend to use from within your application (for example build\\sdk\\iot\\debug). to use our client libraries in your application, just #include our public header files and then link your application's object files with our library files.\nprovide platform-specific implementations for functionality required by azure core. for more information, see the azure core porting guide.\ncmake options\nby default, when building the project with no options, the following static libraries are generated:\nlibraries:\naz_core\naz_span, az_http, az_json, etc.\naz_iot\niot_provisioning, iot_hub, etc.\naz_noplatform\na platform abstraction which will compile but returns az_error_dependency_not_provided from all its functions. this ensures the project can be compiled without the need to provide any specific platform implementation. this is useful if you want to use az_core without platform specific functions like time or sleep.\naz_nohttp\nlibrary that provides a no-op http stack, returning az_error_dependency_not_provided. similar to az_noplatform, this library ensures the project can be compiled without requiring any http stack implementation. this is useful if you want to use az_core without az_http functionality.\nthe following cmake options are available for adding/removing project features.\noption description default value\nunit_testing generates unit test for compilation. when turning this option on, cmocka is a required dependency for compilation.\nafter compiling, use `ctest` to run unit test. off\nunit_testing_mocks this option works only with gcc. it uses -ld option from linker to mock functions during unit test. this is used to test platform or http functions by mocking the return values. off\npreconditions turning this option off would remove all method contracts. this is typically for shipping libraries for production to make it as optimized as possible. on\ntransport_curl this option requires libcurl dependency to be available. it generates an http stack with libcurl for az_http to be able to send requests thru the wire. this library would replace the no_http. off\ntransport_paho this option requires paho-mqtt dependency to be available. provides paho mqtt support for iot. off\naz_platform_impl this option can be set to any of the next values:\n- no_value: default value is used and no_platform library is used.\n- \"posix\": provides implementation for linux and mac systems.\n- \"win32\": provides platform implementation for windows based system\n- \"custom\": tells cmake to use an specific implementation provided by user. when setting this option, user must provide an implementation library and set option `az_custom_platform_impl_name` with the name of the library (i.e. -daz_platform_impl=custom -daz_custom_platform_impl_name=user_platform_lib). cmake will look for this library to link az_core no_value\nsamples: storage samples are built by default using the default pal and http adapter (see running samples). this means that running samples without building an http transport adapter would throw errors like:\n./blobs_client_example.exe\nrunning sample with no_op http implementation.\nrecompile az_core with an http client implementation like curl to see sample sending network requests.\ni.e. cmake -dtransport_curl=on ..\nconsume sdk for c as dependency with cmake\nazure sdk for c can be automatically checked out by cmake and become a build dependency. this is done by using fetchcontent.\nusing this option would skip manually getting the azure sdk for c source code to build and installing it (or making it available from some include path). instead, cmake would do this for us.\nazure sdk for c provides a cmake module that can be copied and used for this purpose.\nvisual studio code\nfor convenience, you can quickly get started using visual studio code and the cmake extension by microsoft. included in the repo is a settings.json file here which the extension will use to configure a cmake project. to use it, copy the settings.json file from .vscode-config to your own .vscode directory. with this, you can run and debug samples and tests. modify the variables in the file to your liking or as instructed by sample documentation and then select the following button in the extension:\nfrom there you can select targets to build and debug.\nnote: especially on windows, make sure you select a compiler platform version that matches the dependencies installed via vcpkg (i.e. x64 or x86). additionally, the triplet to use should be specified in the vcpkg_default_triplet field in settings.json.\nsource files (ide, command line, etc)\nwe have set up the repo for easy integration into other projects which don't use cmake. two main features make this possible:\nto resolve all header file relative paths, you only need to include sdk/inc in your project. all header files are included in the sdk with relative paths to clearly demarcate the services they belong to. a couple examples being:\n#include <azure/core/az_span.h>\n#include <azure/iot/az_iot_hub_client.h>\nall source files are placed in a directory structure similar to the headers: sdk/src. each service has its own subdirectory to separate files which you may be singularly interested in.\nto use a specific service/feature, you may include the header file with the function declaration and compile the according .c containing the function implementation with your project.\nthe specific dependencies of each service may vary, but a couple rules of thumb should resolve the most typical of issues.\nall services depend on core (source files here). you may compile these files with your project to resolve core dependencies.\nmost services will require a platform file to be compiled with your project (see here for porting instructions). we have provided several implementations already here for windows, posix, and a no_platform for no-op stubs. please compile one of these, for your respective platform, with your project.\nthe following compilation, preprocessor options will add or remove functionality in the sdk.\noption description\naz_no_precondition_checking turns off precondition checks to maximize performance with removal of function precondition checking.\naz_no_logging removes all logging code and artifacts from the sdk (helps reduce code size).\nrunning samples\nsee cmake options to learn about how to build an http transport adapter, how to build iot samples, and to turn logging on.\nstorage sample\nthe storage sample expects a storage account with a container and sas token used for authentication to be set in an environment variable azure_storage_url.\nnote: building samples can be disabled by setting az_sdk_c_no_samples environment variable.\n# on linux, set env var like this. for windows, do it from advanced settings/ env variables\nexport env_url=\"https://??????????????\"\nlibcurl global init and global clean up\nwhen you select to build the libcurl http stack implementation, you have to make sure to call curl_global_init before using sdk client to send http request to azure.\nyou need to also call curl_global_cleanup once you no longer need to perform sdk client api calls.\nnote how you can use function atexit() to set libcurl global clean up.\nthe reason for this is the fact of this functions are not thread-safe, and a customer can use libcurl not only for azure sdk library but for some other purpose. more info here.\nthis is libcurl specific only.\niot samples\nsamples for iot will be built only when cmake option transport_paho is set. see compiler options. for more information about iot apis and samples, see azure iot clients.\ndevelopment environment\nproject contains files to work on windows, mac or linux based os.\nnote for any environment variables set to use with cmake, the environment variables must be set before the first cmake generation command (cmake ..). the environment variables will not be picked up if you have already generated the build files, set environment variables, and then regenerate. in that case, you must either delete the cmakecache.txt file or delete the folder in which you are generating build files and start again.\nwindows\nvcpkg is the easiest way to have dependencies installed. it downloads packages sources, headers and build libraries for whatever triplet is set up (platform/arq). vcpkg maintains any installed package inside its own folder, allowing to have multiple vcpkg folder with different dependencies installed on each. this is also great because you don't have to install dependencies globally on your system.\nuse the following steps to install vcpkg and have it linked to cmake.\nnote: the azure sdk is only officially supported against certain versions of vcpkg. use the commit in vcpkg-commit.txt to get a known working version.\n# clone vcpkg:\ngit clone https://github.com/microsoft/vcpkg.git\n# (consider this path as path_to_vcpkg)\ncd vcpkg\n# checkout the vcpkg commit from the vcpkg-commit.txt file (link above)\ngit checkout <vcpkg commit>\n# build vcpkg\n.\\bootstrap-vcpkg.bat\n# install dependencies and update triplet\n.\\vcpkg.exe install --triplet x64-windows-static curl[winssl] cmocka paho-mqtt\n# add this environment variables to link this vcpkg folder with cmake:\n# vcpkg_default_triplet=x64-windows-static\n# vcpkg_root=path_to_vcpkg (replace path_to_vcpkg for where vcpkg is installed)\nnote: setting up a development environment in windows without vcpkg is not supported. it requires installing all dev-dependencies globally and manually setting cmake files to link each of them.\nfollow next steps to build project from command prompt:\n# cd to project folder\ncd azure-sdk-for-c\n# create a new folder to generate cmake files for building (i.e. build)\nmkdir build\ncd build\n# generate files\n# cmake will automatically detect what c compiler is used by system by default and will generate files for it\ncmake ..\n# compile files. cmake would call compiler and linker to generate libs\ncmake --build .\nnote: the steps above would compile and generate the default output for azure-sdk-for-c which includes static libraries only. see section cmake options\nvisual studio 2019\nopen project folder with visual studio. if vcpkg has been previously installed and set up like mentioned above. everything will be ready to build. right after opening project, visual studio will read cmake files and generate cache files automatically.\nlinux\nvcpkg\nvcpkg can be used to download packages sources, headers and build libraries for whatever triplet is set up (platform/architecture). vcpkg maintains any installed package inside its own folder, allowing to have multiple vcpkg folder with different dependencies installed on each. this is also great because you don't have to install dependencies globally on your system.\nuse the following steps to install vcpkg and have it linked to cmake.\nnote: the azure sdk is only officially supported against certain versions of vcpkg. use the commit in vcpkg-commit.txt to get a known working version.\n# clone vcpkg:\ngit clone https://github.com/microsoft/vcpkg.git\n# (consider this path as path_to_vcpkg)\ncd vcpkg\n# checkout the vcpkg commit from the vcpkg-commit.txt file (link above)\ngit checkout <vcpkg commit>\n# build vcpkg\n./bootstrap-vcpkg.sh\n./vcpkg install --triplet x64-linux curl cmocka paho-mqtt\nexport vcpkg_default_triplet=x64-linux\nexport vcpkg_root=path_to_vcpkg #replace path_to_vcpkg for where vcpkg is installed\ndebian\nalternatively, for ubuntu 18.04 you can use:\nsudo apt install build-essential cmake libcmocka-dev libcmocka0 gcovr lcov doxygen curl libcurl4-openssl-dev libssl-dev ca-certificates\nbuild\n# cd to project folder\ncd azure-sdk-for-c\n# create a new folder to generate cmake files for building (i.e. build)\nmkdir build\ncd build\n# generate files\n# cmake will automatically detect what c compiler is used by system by default and will generate files for it\ncmake ..\n# compile files. cmake would call compiler and linker to generate libs\nmake\nnote: the steps above would compile and generate the default output for azure-sdk-for-c which includes static libraries only. see section cmake options\nmac\nvcpkg\nvcpkg can be used to download packages sources, headers and build libraries for whatever triplet is set up (platform/architecture). vcpkg maintains any installed package inside its own folder, allowing to have multiple vcpkg folder with different dependencies installed on each. this is also great because you don't have to install dependencies globally on your system.\nfirst, ensure that you have the latest gcc installed:\nbrew update\nbrew upgrade\nbrew info gcc\nbrew install gcc\nbrew cleanup\nuse the following steps to install vcpkg and have it linked to cmake.\nnote: the azure sdk is only officially supported against certain versions of vcpkg. use the commit in vcpkg-commit.txt to get a known working version.\n# clone vcpkg:\ngit clone https://github.com/microsoft/vcpkg.git\n# (consider this path as path_to_vcpkg)\ncd vcpkg\n# checkout the vcpkg commit from the vcpkg-commit.txt file (link above)\ngit checkout <vcpkg commit>\n# build vcpkg\n./bootstrap-vcpkg.sh\n./vcpkg install --triplet x64-osx curl cmocka paho-mqtt\nexport vcpkg_default_triplet=x64-osx\nexport vcpkg_root=path_to_vcpkg #replace path_to_vcpkg for where vcpkg is installed\nbuild\n# cd to project folder\ncd azure-sdk-for-c\n# create a new folder to generate cmake files for building (i.e. build)\nmkdir build\ncd build\n# generate files\n# cmake will automatically detect what c compiler is used by system by default and will generate files for it\ncmake ..\n# compile files. cmake would call compiler and linker to generate libs\nmake\nnote: the steps above would compile and generate the default output for azure-sdk-for-c which includes static libraries only. see section cmake options\nusing your own http stack implementation\nyou can create and use your own http stack and adapter. this is to avoid the libcurl implementation from azure sdk.\nthe first step is to understand the two components that are required. the first one is an http stack implementation that is capable of sending bits through the wire. some examples of these are libcurl, win32, etc.\nthe second component is an http transport adapter. this is the implementation code which takes an http request from azure sdk core and uses it to send it using the specific http stack implementation. azure sdk core provides the next contract that this component needs to implement:\naz_nodiscard az_result\naz_http_client_send_request(az_http_request const* request, az_http_response* ref_response);\nfor example, azure sdk provides a cmake target az_curl (find it here) with the implementation code for the contract function mentioned before. it uses an az_http_request reference to create an specific libcurl request and send it though the wire. then it uses libcurl response to fill the az_http_response reference structure.\nlink your application with your own http stack\ncreate your own http adapter for an http stack and then use the following cmake command to have it linked to your application\ntarget_link_libraries(your_application_target private lib_adapter http_stack_lib)\n# for instance, this is how we link libcurl and its adapter\ntarget_link_libraries(blobs_client_example private az_curl curl::libcurl)\nsee the complete cmake file and how to link your own library here\nsdk architecture\nat the heart of our sdk is, what we refer to as, azure core. this code defines several data types and functions for use by the client libraries that build on top of us such as the azure iot client libraries. here are some of the features that customers use directly:\nspans: a span represents a byte buffer and is used for string manipulations, http requests/responses, reading/writing json payloads. it allows us to return a substring within a larger string without any memory allocations. see the working with spans section of the azure core readme for more information.\nlogging: as our sdk performs operations, it can send log messages to a customer-defined callback. customers can enable this to assist with debugging and diagnosing issues when leveraging our sdk code. see the logging sdk operations section of the azure core readme for more information.\ncontexts: contexts offer an i/o cancellation mechanism. multiple contexts can be composed together in your application's call tree. when a context is canceled, its children are also canceled. see the canceling an operation section of the azure core readme for more information.\njson: non-allocating json reading and json writing data structures and operations.\nhttp: non-allocating http request and http response data structures and operations.\nargument validation: the sdk validates function arguments and invokes a callback when validation fails. by default, this callback suspends the calling thread forever. however, you can override this behavior and, in fact, you can disable all argument validation to get smaller and faster code. see the sdk function argument validation section of the azure core readme for more information.\nin addition to the above features, azure core provides features available to client libraries written to access other azure services. customers use these features indirectly by way of interacting with a client library. by providing these features in azure core, the client libraries built on top of us will share a common implementation and many features will behave identically across client libraries. for example, azure core offers a standard set of credential types and an http pipeline with logging, retry, and telemetry policies.\ncontributing\nfor details on contributing to this repository, see the contributing guide.\nthis project welcomes contributions and suggestions. most contributions require you to agree to a contributor license agreement (cla) declaring that you have the right to, and actually do, grant us the rights to use your contribution. for details, visit https://cla.microsoft.com.\nwhen you submit a pull request, a cla-bot will automatically determine whether you need to provide a cla and decorate the pr appropriately (e.g., label, comment). simply follow the instructions provided by the bot. you will only need to do this once across all repositories using our cla.\nthis project has adopted the microsoft open source code of conduct. for more information see the code of conduct faq or contact opencode@microsoft.com with any additional questions or comments.\nadditional helpful links for contributors\nmany people all over the world have helped make this project better. you'll want to check out:\nwhat are some good first issues for new contributors to the repo?\nhow to build and test your change\nhow you can make a change happen!\ncommunity\nchat with other community members\nreporting security issues and security bugs\nsecurity issues and bugs should be reported privately, via email, to the microsoft security response center (msrc) secure@microsoft.com. you should receive a response within 24 hours. if for some reason you do not, please follow up via email to ensure we received your original message. further information, including the msrc pgp key, can be found in the security techcenter.\nlicense\nazure sdk for embedded c is licensed under the mit license.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000497, "year": null}, {"Unnamed: 0": 582, "autor": 582, "date": null, "content": "This repository contains the code of the openSenseMap frontend running at https://opensensemap.org. To get more information about openSenseMap and senseBox visit the before mentioned links or have a look at this video or read the openSenseMap chapter in our documentation. openSenseMap is part of the senseBox project.\nOriginally, this frontend has been built as part of the bachelor thesis of @mpfeil at the ifgi (Institute for Geoinformatics, WWU M\u00fcnster) and is currently maintained by @mpfeil.\nThe easiest way to get up and running with your own copy is clicking the Deploy to Netlify button below. It will clone the repository into your own account, and deploy the site to Netlify. It is going to ask for Maptiles url and API endpoint. Please use the default values listed under configuration or use your own.\nConfiguration\nYou can configure the API endpoint and/or map tiles using the following environmental variables:\nENV Default value\nOPENSENSEMAP_API_URL https://api.testing.opensensemap.org\nOPENSENSEMAP_STYLE_URL <YOUR_MAPBOX_STYLE_URL>\nOPENSENSEMAP_ACCESS_TOKEN <YOUR_MAPBOX_ACCESS_TOKEN>\nYou can set them in your terminal or create a file called .env and set the values.\nDevelopment\nHave Node.js v10, grunt and bower installed\nCheck out a branch for your feature (git checkout my-aweseome-feature)\nRun npm install and npx bower install\nRun frontend in development mode (npx grunt serve)\nCommit your changes to your branch and push it to your fork\nCreate a pull request against the master branch\nSee also: CONTRIBUTING\nRelated projects\nServices\nopenSenseMap Backend\nopenSenseMap translations\nDeployment\nOSeM-compose\nopenSenseMap-infrastructure\nTechnologies\nAngularJS\nOrganization\nBranches\nmaster (runs on testing server)\nIs used for production container build tags\nDocker\nopenSenseMap including openSenseMap-API\nFor installing openSenseMap and openSenseMap-API with Docker check out our oSeM-compose repository.\nopenSenseMap\nTo build just openSenseMap you can run: docker build -t osem .\nFollowing build-args are availble:\nBuild Arg Default value\nOPENSENSEMAP_API_URL https://api.opensensemap.org\nOPENSENSEMAP_STYLE_URL <YOUR_MAPBOX_STYLE_URL>\nOPENSENSEMAP_ACCESS_TOKE <YOUR_MAPBOX_ACCESS_TOKEN>\nLicense\nMIT - Matthias Pfeil 2015 - now", "link": "https://github.com/sensebox/openSenseMap", "origin": "Github", "suborigin": "Iot", "result": true, "Selector": "branches", "selectorShort": "branch", "MarkedSent": "this repository contains the code of the opensensemap frontend running at https://opensensemap.org. to get more information about opensensemap and sensebox visit the before mentioned links or have a look at this video or read the opensensemap chapter in our documentation. opensensemap is part of the sensebox project.\noriginally, this frontend has been built as part of the bachelor thesis of @mpfeil at the ifgi (institute for geoinformatics, wwu m\u00fcnster) and is currently maintained by @mpfeil.\nthe easiest way to get up and running with your own copy is clicking the deploy to netlify button below. it will clone the repository into your own account, and deploy the site to netlify. it is going to ask for maptiles url and api endpoint. please use the default values listed under configuration or use your own.\nconfiguration\nyou can configure the api endpoint and/or map tiles using the following environmental variables:\nenv default value\nopensensemap_api_url https://api.testing.opensensemap.org\nopensensemap_style_url <your_mapbox_style_url>\nopensensemap_access_token <your_mapbox_access_token>\nyou can set them in your terminal or create a file called .env and set the values.\ndevelopment\nhave node.js v10, grunt and bower installed\ncheck out a branch for your feature (git checkout my-aweseome-feature)\nrun npm install and npx bower install\nrun frontend in development mode (npx grunt serve)\ncommit your changes to your branch and push it to your fork\ncreate a pull request against the master branch\nsee also: contributing\nrelated projects\nservices\nopensensemap backend\nopensensemap translations\ndeployment\nosem-compose\nopensensemap-infrastructure\ntechnologies\nangularjs\norganization\n-----> branches !!! \nmaster (runs on testing server)\nis used for production container build tags\ndocker\nopensensemap including opensensemap-api\nfor installing opensensemap and opensensemap-api with docker check out our osem-compose repository.\nopensensemap\nto build just opensensemap you can run: docker build -t osem .\nfollowing build-args are availble:\nbuild arg default value\nopensensemap_api_url https://api.opensensemap.org\nopensensemap_style_url <your_mapbox_style_url>\nopensensemap_access_toke <your_mapbox_access_token>\nlicense\nmit - matthias pfeil 2015 - now", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000582, "year": null}, {"Unnamed: 0": 648, "autor": 648, "date": null, "content": "theCore: C++ Embedded Framework\ntheCore is the C++ embedded framework for various microcontroller applications, with or without RTOS (bare-metal).\ntheCore tries to provide all that is necessary to create portable application code:\nStartup, initialization and periphery code for each supported platform.\nBuild system support for different platform and target configurations.\nPlatform-independent device drivers, like temperature sensors or displays.\nOptional OS-related abstractions: semaphores, mutexes and threads.\nEasy-to-deploy development environment.\nDifferent libraries and utilities.\nDocumentation\ntheCore documentation hosted on GitHub pages and available in two formats: multi-page HTML for ease of navigation and single-page HTML for ease of manual searching.\nDoxygen documentation is also available (trough it is not complete).\nWhere to get theCore\ntheCore is now available only from source and hosted solely on GitHub. There are different branches for development and stable versions. Check Branching section for information about branches purpose.\nGetting started\nYou can start using theCore by completing guides and tutorials in the Guides section or check the source code of the standalone blinky example project.\nIt is also worth to look at examples that are placed in examples dir and read the Examples section to understand how to build and launch them.\nSupported platforms\nSupported platform list along with information about each platform is located in Platforms section.\nProject links\nHere a couple of additional references that you might be interest in:\nMailing list. For large discussions and announcements.\ntheCore Telegram group. For discussions.\ntheCore Telegram channel. For announcements.\ntheCore Twitter. You can check some announcements there too.\nOpenHub page. Pretty good and explanatory.\nLicense\nThe source code is distributed under MPL v2.0 License (Mozilla Public License Version 2.0).\nMPL is like LGPL, but with static linking exception.\nFor many practical cases it means that you can use theCore in your proprietary embedded applications without disclosing the application source code.\nKeep it private, if needed. MPL allows you to do that.\nPay note though, that MPL is still a copy-left license. So you are obligated to distribute any changes of theCore itself if there were any.\nIt is strongly recommended to read MPL 2.0 FAQ to get more familiar with it.\nTechnologies and projects used\nMost notable are:\nC++11 and C++14 - as a main programming language standards.\nPython 3 and cog - code generation.\nDoxygen and Sphinx - in-source and project-wide documentation.\nCMake - build-system.\nNix - development environment management.\nCppUTest and Unity - unit and on-device testing.\nTravis - continuos integration.\nCheck the Credits section for more.\nContribution & Using\nThe project is on its very beginning, so any help is more than welcome and highly appreciated. If you\u2019d like to take a part in the project growth and (or) have any questions \u2013 take a look at Community guidelines section, leave message at theCore Telegram group, mailing list, gitter, contact me directly at forgge@gmail.com, or simply check out a list of theCore issues.\nIf you have any suggestions on theCore improvement or just like it how it is, don\u2019t keep silence! I\u2019ll be happy to read your reviews.\nYou are welcome to use theCore in your own projects. If there is something that it\u2019s not enough for this, please let me know via email or open some issues and we will do our best to handle this ASAP. Good luck!", "link": "https://github.com/theCore-embedded/theCore", "origin": "Github", "suborigin": "Iot", "result": true, "Selector": "branches", "selectorShort": "branch", "MarkedSent": "thecore: c++ embedded framework\nthecore is the c++ embedded framework for various microcontroller applications, with or without rtos (bare-metal).\nthecore tries to provide all that is necessary to create portable application code:\nstartup, initialization and periphery code for each supported platform.\nbuild system support for different platform and target configurations.\nplatform-independent device drivers, like temperature sensors or displays.\noptional os-related abstractions: semaphores, mutexes and threads.\neasy-to-deploy development environment.\ndifferent libraries and utilities.\ndocumentation\nthecore documentation hosted on github pages and available in two formats: multi-page html for ease of navigation and single-page html for ease of manual searching.\ndoxygen documentation is also available (trough it is not complete).\nwhere to get thecore\nthecore is now available only from source and hosted solely on github. there are different -----> branches !!!  for development and stable versions. check branching section for information about branches purpose.\ngetting started\nyou can start using thecore by completing guides and tutorials in the guides section or check the source code of the standalone blinky example project.\nit is also worth to look at examples that are placed in examples dir and read the examples section to understand how to build and launch them.\nsupported platforms\nsupported platform list along with information about each platform is located in platforms section.\nproject links\nhere a couple of additional references that you might be interest in:\nmailing list. for large discussions and announcements.\nthecore telegram group. for discussions.\nthecore telegram channel. for announcements.\nthecore twitter. you can check some announcements there too.\nopenhub page. pretty good and explanatory.\nlicense\nthe source code is distributed under mpl v2.0 license (mozilla public license version 2.0).\nmpl is like lgpl, but with static linking exception.\nfor many practical cases it means that you can use thecore in your proprietary embedded applications without disclosing the application source code.\nkeep it private, if needed. mpl allows you to do that.\npay note though, that mpl is still a copy-left license. so you are obligated to distribute any changes of thecore itself if there were any.\nit is strongly recommended to read mpl 2.0 faq to get more familiar with it.\ntechnologies and projects used\nmost notable are:\nc++11 and c++14 - as a main programming language standards.\npython 3 and cog - code generation.\ndoxygen and sphinx - in-source and project-wide documentation.\ncmake - build-system.\nnix - development environment management.\ncpputest and unity - unit and on-device testing.\ntravis - continuos integration.\ncheck the credits section for more.\ncontribution & using\nthe project is on its very beginning, so any help is more than welcome and highly appreciated. if you\u2019d like to take a part in the project growth and (or) have any questions \u2013 take a look at community guidelines section, leave message at thecore telegram group, mailing list, gitter, contact me directly at forgge@gmail.com, or simply check out a list of thecore issues.\nif you have any suggestions on thecore improvement or just like it how it is, don\u2019t keep silence! i\u2019ll be happy to read your reviews.\nyou are welcome to use thecore in your own projects. if there is something that it\u2019s not enough for this, please let me know via email or open some issues and we will do our best to handle this asap. good luck!", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000648, "year": null}, {"Unnamed: 0": 837, "autor": 837, "date": null, "content": "This repository contains the code of the openSenseMap API, which is the HTTP REST API used by https://opensensemap.org running at https://api.opensensemap.org. To get more information about openSenseMap and senseBox visit the before mentioned links or have a look at this video, read the API docs or the openSenseMap chapter in our documentation. openSenseMap is part of the senseBox project.\nOriginally, this API has been built as part of the bachelor thesis of @mpfeil at the ifgi (Institute for Geoinformatics, WWU M\u00fcnster). Developers and previous maintainer include @umut0, @felixerdy, @noerw, @chk1 and @ubergesundheit.\nYou'll find that the repostiory uses yarn workspaces to separate the API and the database models for reuse in other projects. While the API is not published on npm, the package @sensebox/opensensemap-api-models is published from packages/models folder.\nConfiguration\nConfiguration of both the api and the models is done using mechanisms provided by lorenwest/node-config. You can find an annotated example configuration with all keys in config/config.example.json.\nDevelopment\nHave Node.js v14, yarn, Docker and docker-compose installed\nStart your development database (docker-compose up -d db)\nCreate branch for your feature (git checkout my-awesome-feature)\nRun yarn install\nCommit your changes to your branch and push it to your fork\nCreate a pull request against the master branch\nSee also: CONTRIBUTING\nRunning Tests\nYou can run the tests in containers using Docker and docker-compose.\n# Run this the first time or every time you change dependencies in package.json\nyarn build-test-env\nyarn test\nRelated projects\nServices\nopenSenseMap Frontend\nttn-osem-integration\nmqtt-osem-integration\nsensebox-mailer\nLibraries\nsketch-templater\nopenSenseMap-API-models\nDeployment\nOSeM-compose\nopenSenseMap-infrastructure\nTechnologies\nNode.js\nMongoDB\nOrganization\nBranches\nmaster (runs on testing server)\nIs used for container build tags\ngh-pages\nHosts API docs for https://docs.opensensemap.org/\nIs generated and pushed to GitHub by GitHub Actions file\nTags and Versions\nGit Tags are used for Docker hub builds (like v1). Version number is increased by one for each new version. Docker images are built automatically by the Docker hub for all tags starting with v\nVersioned container images\nCheck out master branch\nGo to root directory\nRun tests\nOptional: Build docker image locally\nCommit everything needed for the container image\nRun yarn tag-container\nRun git push origin master\nLicense\nMIT - Matthias Pfeil 2015 - now", "link": "https://github.com/sensebox/openSenseMap-API", "origin": "Github", "suborigin": "Iot", "result": true, "Selector": "branches", "selectorShort": "branch", "MarkedSent": "this repository contains the code of the opensensemap api, which is the http rest api used by https://opensensemap.org running at https://api.opensensemap.org. to get more information about opensensemap and sensebox visit the before mentioned links or have a look at this video, read the api docs or the opensensemap chapter in our documentation. opensensemap is part of the sensebox project.\noriginally, this api has been built as part of the bachelor thesis of @mpfeil at the ifgi (institute for geoinformatics, wwu m\u00fcnster). developers and previous maintainer include @umut0, @felixerdy, @noerw, @chk1 and @ubergesundheit.\nyou'll find that the repostiory uses yarn workspaces to separate the api and the database models for reuse in other projects. while the api is not published on npm, the package @sensebox/opensensemap-api-models is published from packages/models folder.\nconfiguration\nconfiguration of both the api and the models is done using mechanisms provided by lorenwest/node-config. you can find an annotated example configuration with all keys in config/config.example.json.\ndevelopment\nhave node.js v14, yarn, docker and docker-compose installed\nstart your development database (docker-compose up -d db)\ncreate branch for your feature (git checkout my-awesome-feature)\nrun yarn install\ncommit your changes to your branch and push it to your fork\ncreate a pull request against the master branch\nsee also: contributing\nrunning tests\nyou can run the tests in containers using docker and docker-compose.\n# run this the first time or every time you change dependencies in package.json\nyarn build-test-env\nyarn test\nrelated projects\nservices\nopensensemap frontend\nttn-osem-integration\nmqtt-osem-integration\nsensebox-mailer\nlibraries\nsketch-templater\nopensensemap-api-models\ndeployment\nosem-compose\nopensensemap-infrastructure\ntechnologies\nnode.js\nmongodb\norganization\n-----> branches !!! \nmaster (runs on testing server)\nis used for container build tags\ngh-pages\nhosts api docs for https://docs.opensensemap.org/\nis generated and pushed to github by github actions file\ntags and versions\ngit tags are used for docker hub builds (like v1). version number is increased by one for each new version. docker images are built automatically by the docker hub for all tags starting with v\nversioned container images\ncheck out master branch\ngo to root directory\nrun tests\noptional: build docker image locally\ncommit everything needed for the container image\nrun yarn tag-container\nrun git push origin master\nlicense\nmit - matthias pfeil 2015 - now", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000837, "year": null}], "name": "branchIot"}
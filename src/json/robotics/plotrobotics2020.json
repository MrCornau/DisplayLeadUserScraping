{"interestingcomments": [{"Unnamed: 0": 646, "autor": "hhemken", "date": 1599504257000, "content": "A New Path to Humanoid Robots? /!/ I have a few free ebooks to give away! Message me if you\u2019re interested.\n\nIt has to do with creating virtual humans and physical robots by capturing human behavior and putting it into a machine learning model. It's fundamentally an embodied cognition approach.\n\nIt\u2019s written in the style of a *noir* techno-thriller, and features plot elements based on the following patents:\n\n[U.S. Patent 9,676,098](https://patents.google.com/patent/US9676098)   Data collection from living subjects and controlling an autonomous robot using the data\n\n[U.S. Patent 10,166,680](https://patents.google.com/patent/US10166680)  Autonomous robot using data captured from a living subject\n\n[U.S. Patent 10,195,738](https://patents.google.com/patent/US10195738)  Data collection from a subject using a sensor apparatus\n\n[https://www.amazon.com/dp/0578715902](https://www.amazon.com/dp/0578715902)", "link": "https://www.reddit.com/r/robotics/comments/iodc55/a_new_path_to_humanoid_robots/", "origin": "Reddit", "suborigin": "robotics", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "a new path to humanoid robots? /!/ i have a few free ebooks to give away! message me if you\u2019re interested.\n\nit has to do with creating virtual humans and physical robots by capturing human behavior and putting it into a machine learning model. it's fundamentally an embodied cognition approach.\n\nit\u2019s written in the style of a *noir* techno-thriller, and features -----> plot !!!  elements based on the following patents:\n\n[u.s. patent 9,676,098](https://patents.google.com/patent/us9676098)   data collection from living subjects and controlling an autonomous robot using the data\n\n[u.s. patent 10,166,680](https://patents.google.com/patent/us10166680)  autonomous robot using data captured from a living subject\n\n[u.s. patent 10,195,738](https://patents.google.com/patent/us10195738)  data collection from a subject using a sensor apparatus\n\n[https://www.amazon.com/dp/0578715902](https://www.amazon.com/dp/0578715902)", "sortedWord": "None", "removed": null, "score": 1, "comments": 2, "media": null, "medialink": "https://www.reddit.com/r/robotics/comments/iodc55/a_new_path_to_humanoid_robots/", "identifyer": 3500653, "year": "2020"}, {"Unnamed: 0": 2079, "autor": "User4158", "date": 1594244263000, "content": "Inertial Measurement Unit Data to Position Estimation /!/ Hello,\n\nI am trying to estimate the motion and position of an IMU. Before working on my own data, I tried using a great dataset of Visual-Inertial Dataset by TUM Computer Vision Group, please see:  \n[https://vision.in.tum.de/data/datasets/visual-inertial-dataset](https://vision.in.tum.de/data/datasets/visual-inertial-dataset)\n\nA 3 axes IMU is used at 200 Hz for measuring accelerations and angular velocities. I do not mention stereo setup cameras or other hardware. \n\nAs sequance, I used \"**corridor4**\" because of its property of being relatively small in terms of size. \n\nI downloaded the .bag file and extracted acceleration and angular velocity data for samples of total number of 19k.\n\nGraphs: [imgur.com/a/tEBdwvZ](https://imgur.com/a/tEBdwvZ)\n\nAcceleration data was provided in terms of g (**1st Graph**). I converted it to m/s\\^2 and substracted gravitational acceleration of earth from z axes (**2nd Graph**).\n\nNow, in order to be able to determine linear velocity, I tried numerical integration with the method of Cumulative Trapezoidal Rule - I initially tried Simpson 3/8, however it yielded an unsolvable system of equations, but total sum may be determined-  \n\nPlease see the **3rd Graph** and notice the drift of Z axis. I was expecting this since the issue of non zero offset is present. \n\nTo compensate this, I used High Pass Filter (1st order, 0.1 cut off). Please see the **4th Graph**.\n\nAgain I used cumulative trapezoids for the integration of HP Filtered Linear Velocity, to be able to find Positions, **5th Graph**.\n\nFinally I tried to plot position estimation, **6th Graph**. \n\nThe problem is that ground truth path and the positions I estimated are totally different. \n\nI would be quite happy to see some recommendations. \n\nI know that it is so difficult to estimate highly accurate positions with acceleration data since it drifts really fast. But I would love to have opinions regarding what I may be doing wrong.\n\nThanks.", "link": "https://www.reddit.com/r/robotics/comments/hnq5wt/inertial_measurement_unit_data_to_position/", "origin": "Reddit", "suborigin": "robotics", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "inertial measurement unit data to position estimation /!/ hello,\n\ni am trying to estimate the motion and position of an imu. before working on my own data, i tried using a great dataset of visual-inertial dataset by tum computer vision group, please see:  \n[https://vision.in.tum.de/data/datasets/visual-inertial-dataset](https://vision.in.tum.de/data/datasets/visual-inertial-dataset)\n\na 3 axes imu is used at 200 hz for measuring accelerations and angular velocities. i do not mention stereo setup cameras or other hardware. \n\nas sequance, i used \"**corridor4**\" because of its property of being relatively small in terms of size. \n\ni downloaded the .bag file and extracted acceleration and angular velocity data for samples of total number of 19k.\n\ngraphs: [imgur.com/a/tebdwvz](https://imgur.com/a/tebdwvz)\n\nacceleration data was provided in terms of g (**1st graph**). i converted it to m/s\\^2 and substracted gravitational acceleration of earth from z axes (**2nd graph**).\n\nnow, in order to be able to determine linear velocity, i tried numerical integration with the method of cumulative trapezoidal rule - i initially tried simpson 3/8, however it yielded an unsolvable system of equations, but total sum may be determined-  \n\nplease see the **3rd graph** and notice the drift of z axis. i was expecting this since the issue of non zero offset is present. \n\nto compensate this, i used high pass filter (1st order, 0.1 cut off). please see the **4th graph**.\n\nagain i used cumulative trapezoids for the integration of hp filtered linear velocity, to be able to find positions, **5th graph**.\n\nfinally i tried to -----> plot !!!  position estimation, **6th graph**. \n\nthe problem is that ground truth path and the positions i estimated are totally different. \n\ni would be quite happy to see some recommendations. \n\ni know that it is so difficult to estimate highly accurate positions with acceleration data since it drifts really fast. but i would love to have opinions regarding what i may be doing wrong.\n\nthanks.", "sortedWord": "None", "removed": null, "score": 1, "comments": 3, "media": "self", "medialink": "https://www.reddit.com/r/robotics/comments/hnq5wt/inertial_measurement_unit_data_to_position/", "identifyer": 3502097, "year": "2020"}, {"Unnamed: 0": 3498, "autor": "adiego73", "date": 1592159293000, "content": "Transformation matrix and state update /!/ Hello guys! I am writing my first post here to ask you something that is blowing my mind (in the bad sense of the sentence :|)\n\nI am trying to develop a EKF-SLAM algorithm for a drone, and I want to estimate its position in the inertial/world reference frame. I have as control variables the linear and angular velocities in the drone-reference frame. To do so, I am following this paper: [http://ingmec.ual.es/\\~jlblanco/papers/RangeBearingSLAM6D.pdf](http://ingmec.ual.es/~jlblanco/papers/RangeBearingSLAM6D.pdf) in which it states that the position update is:\n\n    x_ = x_ + delta_t * (vx * T(0,0) + vy * T(0,1) + vz * T(0,2));\n    y_ = y_ + delta_t * (vx * T(1,0) + vy * T(1,1) + vz * T(1,2));\n    z_ = z_ + delta_t * (vx * T(2,0) + vy * T(2,1) + vz * T(2,2));\n    yaw_ = yaw_ + delta_t * omega_z;\n\nWhere `T` is the following transformation matrix:\n\n    [ cos(yaw) * cos(pitch), cos(yaw) * sin(pitch) * sin(roll) - sin(yaw) * cos(roll), cos(yaw) * sin(pitch) * cos(roll) + sin(yaw) * sin(roll), 0 ]\n    [ sin(yaw) * cos(pitch), sin(yaw) * sin(pitch) * sin(roll) + cos(yaw) * cos(roll), sin(yaw) * sin(pitch) * cos(roll) - cos(yaw) * sin(roll), 0 ]\n    [ -sin(pitch), cos(pitch) * sin(roll), cos(pitch) * cos(roll), 0 ]\n    [ 0, 0, 0, 1 ] \n\nI have triple-check the transformation matrix and it seems to be fine, also the position updates make sense (or I think it does)..\n\nThe problem comes when I plot the position to compare ground truth with my estimation, and I see the following:\n\n&amp;#x200B;\n\n[Green line is the ground truth, and blue line the estimate position](https://preview.redd.it/ej6kxjw33x451.png?width=597&amp;format=png&amp;auto=webp&amp;s=53a6f98c09931ca645991fc74b89dbd539defa6c)\n\nAs you can see in the point where the arrow points, the ground truth (green) goes to some place, and my update (blue) goes to the opposite direction.. after this divergence it continues more or less fine, considering that it drifts (which is the expected). That moment in which one line goes up and the other down, is when the drone starts rotating 180 degrees, that is why I think the problem is in the transformation matrix. \n\nSomething that worth mentioning is that this is pure prediction, so no correction is done.  \n\n\nCan you spot any error in my reasoning? Is there something that I didn't mention that may worth quadruple-check? Or do you think this is more or less expected and I am creating a problem from nothing?\n\n&amp;#x200B;\n\nHope you can help me and sorry for the long post :D", "link": "https://www.reddit.com/r/robotics/comments/h8yy23/transformation_matrix_and_state_update/", "origin": "Reddit", "suborigin": "robotics", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "transformation matrix and state update /!/ hello guys! i am writing my first post here to ask you something that is blowing my mind (in the bad sense of the sentence :|)\n\ni am trying to develop a ekf-slam algorithm for a drone, and i want to estimate its position in the inertial/world reference frame. i have as control variables the linear and angular velocities in the drone-reference frame. to do so, i am following this paper: [http://ingmec.ual.es/\\~jlblanco/papers/rangebearingslam6d.pdf](http://ingmec.ual.es/~jlblanco/papers/rangebearingslam6d.pdf) in which it states that the position update is:\n\n    x_ = x_ + delta_t * (vx * t(0,0) + vy * t(0,1) + vz * t(0,2));\n    y_ = y_ + delta_t * (vx * t(1,0) + vy * t(1,1) + vz * t(1,2));\n    z_ = z_ + delta_t * (vx * t(2,0) + vy * t(2,1) + vz * t(2,2));\n    yaw_ = yaw_ + delta_t * omega_z;\n\nwhere `t` is the following transformation matrix:\n\n    [ cos(yaw) * cos(pitch), cos(yaw) * sin(pitch) * sin(roll) - sin(yaw) * cos(roll), cos(yaw) * sin(pitch) * cos(roll) + sin(yaw) * sin(roll), 0 ]\n    [ sin(yaw) * cos(pitch), sin(yaw) * sin(pitch) * sin(roll) + cos(yaw) * cos(roll), sin(yaw) * sin(pitch) * cos(roll) - cos(yaw) * sin(roll), 0 ]\n    [ -sin(pitch), cos(pitch) * sin(roll), cos(pitch) * cos(roll), 0 ]\n    [ 0, 0, 0, 1 ] \n\ni have triple-check the transformation matrix and it seems to be fine, also the position updates make sense (or i think it does)..\n\nthe problem comes when i -----> plot !!!  the position to compare ground truth with my estimation, and i see the following:\n\n&amp;#x200b;\n\n[green line is the ground truth, and blue line the estimate position](https://preview.redd.it/ej6kxjw33x451.png?width=597&amp;format=png&amp;auto=webp&amp;s=53a6f98c09931ca645991fc74b89dbd539defa6c)\n\nas you can see in the point where the arrow points, the ground truth (green) goes to some place, and my update (blue) goes to the opposite direction.. after this divergence it continues more or less fine, considering that it drifts (which is the expected). that moment in which one line goes up and the other down, is when the drone starts rotating 180 degrees, that is why i think the problem is in the transformation matrix. \n\nsomething that worth mentioning is that this is pure prediction, so no correction is done.  \n\n\ncan you spot any error in my reasoning? is there something that i didn't mention that may worth quadruple-check? or do you think this is more or less expected and i am creating a problem from nothing?\n\n&amp;#x200b;\n\nhope you can help me and sorry for the long post :d", "sortedWord": "None", "removed": null, "score": 2, "comments": 5, "media": null, "medialink": "https://www.reddit.com/r/robotics/comments/h8yy23/transformation_matrix_and_state_update/", "identifyer": 3503532, "year": "2020"}, {"Unnamed: 0": 3671, "autor": "Vickyn1103", "date": 1582379485000, "content": "Robotic Arm Configuration /!/ &amp;#x200B;\n\n![img](644xmpw7chi41 \"Need to know what type of Robotic arm configuration is this? It would be great if any one can let me know how to write a MATLAB code for the inverse kinematics of the arm shown in the figure.  I need to write a sequence for\u00a0 movement of the arm through three selected points  and plot it in 3D. \")\n\nNeed to know what type of Robotic arm configuration is this? \n\nIt would be great if any one can let me know how to write a MATLAB code for the inverse kinematics of the arm shown in the figure.  I need to write a sequence for\u00a0 movement of the arm through three selected points  and plot it in 3D. \n\nAny  help would be encouraged  :) , thanks in advance for your help", "link": "https://www.reddit.com/r/robotics/comments/f7srjk/robotic_arm_configuration/", "origin": "Reddit", "suborigin": "robotics", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "robotic arm configuration /!/ &amp;#x200b;\n\n![img](644xmpw7chi41 \"need to know what type of robotic arm configuration is this? it would be great if any one can let me know how to write a matlab code for the inverse kinematics of the arm shown in the figure.  i need to write a sequence for\u00a0 movement of the arm through three selected points  and -----> plot !!!  it in 3d. \")\n\nneed to know what type of robotic arm configuration is this? \n\nit would be great if any one can let me know how to write a matlab code for the inverse kinematics of the arm shown in the figure.  i need to write a sequence for\u00a0 movement of the arm through three selected points  and -----> plot !!!  it in 3d. \n\nany  help would be encouraged  :) , thanks in advance for your help", "sortedWord": "None", "removed": null, "score": 1, "comments": 1, "media": null, "medialink": "https://www.reddit.com/r/robotics/comments/f7srjk/robotic_arm_configuration/", "identifyer": 3503706, "year": "2020"}, {"Unnamed: 0": 3812, "autor": "eee_bume", "date": 1600424757000, "content": "Best Python package to plot &amp; analyze 3D trajectories? /!/ Starting my master thesis regarding a UAV project and never had to deal with plotting 3D trajectories until now. For everything else I am a pure Matplotlib-Guy, but Plotly's ability to easily change the viewing point kind arouses me...\n\nI want to know the opinion of you peeps!\n\n[View Poll](https://www.reddit.com/poll/iv3jrg)", "link": "https://www.reddit.com/r/robotics/comments/iv3jrg/best_python_package_to_plot_analyze_3d/", "origin": "Reddit", "suborigin": "robotics", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "best python package to -----> plot !!!  &amp; analyze 3d trajectories? /!/ starting my master thesis regarding a uav project and never had to deal with plotting 3d trajectories until now. for everything else i am a pure matplotlib-guy, but plotly's ability to easily change the viewing point kind arouses me...\n\ni want to know the opinion of you peeps!\n\n[view poll](https://www.reddit.com/poll/iv3jrg)", "sortedWord": "None", "removed": null, "score": 1, "comments": 3, "media": null, "medialink": "https://www.reddit.com/r/robotics/comments/iv3jrg/best_python_package_to_plot_analyze_3d/", "identifyer": 3503852, "year": "2020"}], "name": "plotrobotics2020"}
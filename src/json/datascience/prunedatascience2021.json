{"interestingcomments": [{"autor": "teachMeSensei96", "date": 1621035461000, "content": "Please help me prune my career decision tree /!/ Hi, I hope everyone is doing well in these testing times! \n\nI am an Electronics and Communications engineer and have been working as a data analyst for the past 3 years. Most of my work also entails predictive models like XGBoost and recently I have also picked up some NLP. \n\nNow I am looking to apply for a Masters degree. I feel that there are other MS in CS vs MS in DS reddit posts out there but my confusion is a bit more deep, so I'll try my best to structure it well. \n\n1. I do not have a background in CS, most of the folks I've seen do well in Data Science (Analytics or ML Engineer) usually have a bachelor's in CS. So should I do a MS in CS. \n\n2. Almost everyone with a MS in Data Science is able to get a job as a data scientist. So I should apply to MS in DS, but it'll blow away my cover for Software Engineering roles which I would not like but are a good backup in the long run. \n\n3. Since most software Devs or data scientists will be in management positions later on in their career, it does not matter that I don't have backup software engineer roles because I will be a manager.\n\n4. MS DS programs are few in number and my overall profile is not that good to get into great programs. My background is not in CS so it also limits my options for MS CS. \n\n5. I applied to a mix of MSCS and MSDS programs this year and got into UT Dallas MSCS and USFCA MSDS. I am considering building a stronger profile and applying again but only in either MSCS or MSDS. \n\nAs you can see I'm using a lot of variables to think about this. I am probably overthinking, yes. But I seriously want to make a good decision. \n\nI would really appreciate perspectives that would help me think and opinions that don't digress a lot from the mentioned points. \n\nThankyou so much for reading through, I really appreciate your help! Thanks!", "link": "https://www.reddit.com/r/datascience/comments/ncm0uu/please_help_me_prune_my_career_decision_tree/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "prune", "selectorShort": "prune", "MarkedSent": "please help me -----> prune !!!  my career decision tree /!/ hi, i hope everyone is doing well in these testing times! \n\ni am an electronics and communications engineer and have been working as a data analyst for the past 3 years. most of my work also entails predictive models like xgboost and recently i have also picked up some nlp. \n\nnow i am looking to apply for a masters degree. i feel that there are other ms in cs vs ms in ds reddit posts out there but my confusion is a bit more deep, so i'll try my best to structure it well. \n\n1. i do not have a background in cs, most of the folks i've seen do well in data science (analytics or ml engineer) usually have a bachelor's in cs. so should i do a ms in cs. \n\n2. almost everyone with a ms in data science is able to get a job as a data scientist. so i should apply to ms in ds, but it'll blow away my cover for software engineering roles which i would not like but are a good backup in the long run. \n\n3. since most software devs or data scientists will be in management positions later on in their career, it does not matter that i don't have backup software engineer roles because i will be a manager.\n\n4. ms ds programs are few in number and my overall profile is not that good to get into great programs. my background is not in cs so it also limits my options for ms cs. \n\n5. i applied to a mix of mscs and msds programs this year and got into ut dallas mscs and usfca msds. i am considering building a stronger profile and applying again but only in either mscs or msds. \n\nas you can see i'm using a lot of variables to think about this. i am probably overthinking, yes. but i seriously want to make a good decision. \n\ni would really appreciate perspectives that would help me think and opinions that don't digress a lot from the mentioned points. \n\nthankyou so much for reading through, i really appreciate your help! thanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/ncm0uu/please_help_me_prune_my_career_decision_tree/',)", "identifyer": 5587600, "year": "2021"}, {"autor": "grid_world", "date": 1626850473000, "content": "Prune Neural Networks layers for f% sparsity - TensorFlow2 /!/ I am using TensorFlow 2.5 and Python3.8 where I have a simple TF2 CNN having one conv layer and an output layer for binary classification as follows:\n\n&amp;#x200B;\n\n        num_filters = 32    \n        def cnn_model():\n                model = Sequential()\n                \n                model.add(\n                    InputLayer(input_shape = (32, 32, 3))\n                )\n                \n                model.add(\n                    Conv2D(\n                        filters = num_filters, kernel_size = (3, 3),\n                        activation = 'relu', kernel_initializer = tf.initializers.he_normal(),\n                        strides = (1, 1), padding = 'same',\n                        use_bias = True, \n                        bias_initializer = RandomNormal(mean = 0.0, stddev = 0.05)\n                        # kernel_regularizer = regularizers.l2(weight_decay)\n                    )\n                )\n                \n                model.add(Flatten())\n                \n                model.add(\n                    Dense(\n                        units = 1, activation = 'sigmoid'\n                    )\n                )\n                \n                return model\n    \n        \n        # I then instantiate two instances of it:\n    \n        model = cnn_model()\n        model2 = cnn_model()\n    \n        model.summary()\n        '''\n        Model: \"sequential_2\"\n        _________________________________________________________________\n        Layer (type)                 Output Shape              Param #   \n        =================================================================\n        conv2d_5 (Conv2D)            (None, 32, 32, 32)        896       \n        _________________________________________________________________\n        flatten_2 (Flatten)          (None, 32768)             0         \n        _________________________________________________________________\n        dense_2 (Dense)              (None, 1)                 32769     \n        =================================================================\n        Total params: 33,665\n        Trainable params: 33,665\n        Non-trainable params: 0\n        '''\n    \n        def count_nonzero_params(model):\n            # Count number of non-zero parameters in each layer and in total-\n            model_sum_params = 0\n            \n            for layer in model.trainable_weights:\n                loc_param = tf.math.count_nonzero(layer, axis = None).numpy()\n                model_sum_params += loc_param\n            \n            # print(\"Total number of trainable parameters = {0}\\n\".format(model_sum_params))\n            \n            return model_sum_params\n    \n        # Sanity check-\n        count_nonzero_params(model)\n        # 33664\n\nA random input is used to make predictions using the two models-\n\n&amp;#x200B;\n\n        x = tf.random.normal(shape = (5, 32, 32, 3))\n        pred = model(x)\n        pred2 = model2(x)\n        pred.shape, pred.shape\n        # (TensorShape([5, 1]), TensorShape([5, 1]))\n\n&amp;#x200B;\n\nA pruning function has been defined to prune f% of smallest magnitude weights **for model1** *for each layer* such that:\n\n&gt;for connections in model, **only those connections are pruned** (per layer) **which are f% of smallest magnitude weights in both the models** viz., model and model2\n\n&amp;#x200B;\n\n        def custom_pruning(model1, model2, p):\n            \"\"\"\n            Function to prune p% of smallest magnitude weights of \n            a given CNN model globally.\n            \n            Input:\n            model1            TF2 Convolutional Neural Network model\n            model2            TF2 Convolutional Neural Network model\n            \n                              \n            p                 Prune p% of smallest magnitude weights globally\n            \n            Output:\n            Returns a Python3 list containing layer-wise pruned weights.    \n            \"\"\"\n            \n            # Python3 list to hold weights of model1-\n            model1_np_wts = []\n            \n            for layer in model1.weights:\n                model1_np_wts.append(layer.numpy())\n            \n            # Python3 list to hold flattened weights-\n            flattened_wts = []\n        \n            for layer in model1_np_wts:\n                flattened_wts.append(np.abs(layer.flatten()))\n        \n            # Compute pth percentile threshold using all weights from model1-\n            threshold_weights1 = np.percentile(np.concatenate(flattened_wts), p)\n            \n            del flattened_wts\n            \n            \n            # Python3 list to hold weights of model2-\n            model2_np_wts = []\n        \n            for layer in model2.weights:\n                model2_np_wts.append(layer.numpy())\n        \n            # Python3 list to hold flattened weights for model2-\n            flattened_wts2 = []\n        \n            for layer in model2_np_wts:\n                flattened_wts2.append(np.abs(layer.flatten()))\n        \n            # Compute pth percentile threshold using all weights from model2-\n            threshold_weights2 = np.percentile(np.concatenate(flattened_wts2), p)\n            \n            del flattened_wts2\n            \n            \n            # Python3 list to contain pruned weights-\n            pruned_wts = []\n            \n            for layer_model1, layer_model2 in zip(model1_np_wts, model2_np_wts):\n                if len(layer_model1.shape) == 4:\n                    layer_wts_abs = np.abs(layer_model1)\n                    layer_wts2_abs = np.abs(layer_model2)\n                    layer_wts_abs[(layer_wts_abs &lt; threshold_weights1) &amp; (layer_wts2_abs &lt; threshold_weights2)] = 0\n                    layer_mod = np.where(layer_wts_abs == 0, 0, layer_model1)\n                    pruned_wts.append(layer_mod)\n                elif len(layer_model1.shape) == 2:\n                    layer_wts_abs = np.abs(layer_model1)\n                    layer_wts2_abs = np.abs(layer_model2)\n                    layer_wts_abs[(layer_wts_abs &lt; threshold_weights1) &amp; (layer_wts2_abs &lt; threshold_weights2)] = 0\n                    layer_mod = np.where(layer_wts_abs == 0, 0, layer_model1)\n                    pruned_wts.append(layer_mod)\n                else:\n                    pruned_wts.append(layer_model1)\n                \n                \n            return pruned_wts\n            \n    \n        # Prune 15% of smallest magnitude weights-\n        pruned_wts = custom_pruning(model1 = model, model2 = model2, p = 15)\n    \n        # Initialize and load weights for pruned model-\n        new_model = cnn_model()\n        new_model.set_weights(pruned_wts)\n        \n        # Count original and unpruned parameters-\n        orig_params = count_nonzero_params(model)\n        \n        # Count pruned parameters-\n        pruned_params = count_nonzero_params(new_model)\n        \n        # Compute actual sparsity-\n        sparsity = ((orig_params - pruned_params) / orig_params) * 100\n        \n        print(f\"actual sparsity = {sparsity:.2f}% for a given sparsity = 15%\")\n        # actual sparsity = 2.22% for a given sparsity = 15%\n\nThe problem is that for a given sparsity of 15%, only 2.22% connections are pruned. To achieve the desired 15% sparsity, a hit and trial method to find 'p' parameter's value-\n\n        # Prune 15% of smallest magnitude weights-\n        pruned_wts = custom_pruning(model1 = model, model2 = model2, p = 38)\n    \n        # Initialize and load weights for pruned model-\n        new_model = cnn_model()\n        new_model.set_weights(pruned_wts)\n        \n        # Count pruned parameters-\n        pruned_params = count_nonzero_params(new_model)\n        \n        # Compute actual sparsity-\n        sparsity = ((orig_params - pruned_params) / orig_params) * 100\n        \n        print(f\"actual sparsity = {sparsity:.2f}% for a given sparsity = 15%\")\n        # actual sparsity = 14.40% for a given sparsity = 15%\n\nDue to two conditions while filtering in 'custom\\_pruning()', this difference between desired and actual sparsity levels are occurring.\n\n&amp;#x200B;\n\nIs there some other better way to achieve this that I am missing out?\n\n&amp;#x200B;\n\nThanks!", "link": "https://www.reddit.com/r/datascience/comments/ookq8g/prune_neural_networks_layers_for_f_sparsity/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "prune", "selectorShort": "prune", "MarkedSent": "-----> prune !!!  neural networks layers for f% sparsity - tensorflow2 /!/ i am using tensorflow 2.5 and python3.8 where i have a simple tf2 cnn having one conv layer and an output layer for binary classification as follows:\n\n&amp;#x200b;\n\n        num_filters = 32    \n        def cnn_model():\n                model = sequential()\n                \n                model.add(\n                    inputlayer(input_shape = (32, 32, 3))\n                )\n                \n                model.add(\n                    conv2d(\n                        filters = num_filters, kernel_size = (3, 3),\n                        activation = 'relu', kernel_initializer = tf.initializers.he_normal(),\n                        strides = (1, 1), padding = 'same',\n                        use_bias = true, \n                        bias_initializer = randomnormal(mean = 0.0, stddev = 0.05)\n                        # kernel_regularizer = regularizers.l2(weight_decay)\n                    )\n                )\n                \n                model.add(flatten())\n                \n                model.add(\n                    dense(\n                        units = 1, activation = 'sigmoid'\n                    )\n                )\n                \n                return model\n    \n        \n        # i then instantiate two instances of it:\n    \n        model = cnn_model()\n        model2 = cnn_model()\n    \n        model.summary()\n        '''\n        model: \"sequential_2\"\n        _________________________________________________________________\n        layer (type)                 output shape              param #   \n        =================================================================\n        conv2d_5 (conv2d)            (none, 32, 32, 32)        896       \n        _________________________________________________________________\n        flatten_2 (flatten)          (none, 32768)             0         \n        _________________________________________________________________\n        dense_2 (dense)              (none, 1)                 32769     \n        =================================================================\n        total params: 33,665\n        trainable params: 33,665\n        non-trainable params: 0\n        '''\n    \n        def count_nonzero_params(model):\n            # count number of non-zero parameters in each layer and in total-\n            model_sum_params = 0\n            \n            for layer in model.trainable_weights:\n                loc_param = tf.math.count_nonzero(layer, axis = none).numpy()\n                model_sum_params += loc_param\n            \n            # print(\"total number of trainable parameters = {0}\\n\".format(model_sum_params))\n            \n            return model_sum_params\n    \n        # sanity check-\n        count_nonzero_params(model)\n        # 33664\n\na random input is used to make predictions using the two models-\n\n&amp;#x200b;\n\n        x = tf.random.normal(shape = (5, 32, 32, 3))\n        pred = model(x)\n        pred2 = model2(x)\n        pred.shape, pred.shape\n        # (tensorshape([5, 1]), tensorshape([5, 1]))\n\n&amp;#x200b;\n\na pruning function has been defined to prune f% of smallest magnitude weights **for model1** *for each layer* such that:\n\n&gt;for connections in model, **only those connections are pruned** (per layer) **which are f% of smallest magnitude weights in both the models** viz., model and model2\n\n&amp;#x200b;\n\n        def custom_pruning(model1, model2, p):\n            \"\"\"\n            function to prune p% of smallest magnitude weights of \n            a given cnn model globally.\n            \n            input:\n            model1            tf2 convolutional neural network model\n            model2            tf2 convolutional neural network model\n            \n                              \n            p                 prune p% of smallest magnitude weights globally\n            \n            output:\n            returns a python3 list containing layer-wise pruned weights.    \n            \"\"\"\n            \n            # python3 list to hold weights of model1-\n            model1_np_wts = []\n            \n            for layer in model1.weights:\n                model1_np_wts.append(layer.numpy())\n            \n            # python3 list to hold flattened weights-\n            flattened_wts = []\n        \n            for layer in model1_np_wts:\n                flattened_wts.append(np.abs(layer.flatten()))\n        \n            # compute pth percentile threshold using all weights from model1-\n            threshold_weights1 = np.percentile(np.concatenate(flattened_wts), p)\n            \n            del flattened_wts\n            \n            \n            # python3 list to hold weights of model2-\n            model2_np_wts = []\n        \n            for layer in model2.weights:\n                model2_np_wts.append(layer.numpy())\n        \n            # python3 list to hold flattened weights for model2-\n            flattened_wts2 = []\n        \n            for layer in model2_np_wts:\n                flattened_wts2.append(np.abs(layer.flatten()))\n        \n            # compute pth percentile threshold using all weights from model2-\n            threshold_weights2 = np.percentile(np.concatenate(flattened_wts2), p)\n            \n            del flattened_wts2\n            \n            \n            # python3 list to contain pruned weights-\n            pruned_wts = []\n            \n            for layer_model1, layer_model2 in zip(model1_np_wts, model2_np_wts):\n                if len(layer_model1.shape) == 4:\n                    layer_wts_abs = np.abs(layer_model1)\n                    layer_wts2_abs = np.abs(layer_model2)\n                    layer_wts_abs[(layer_wts_abs &lt; threshold_weights1) &amp; (layer_wts2_abs &lt; threshold_weights2)] = 0\n                    layer_mod = np.where(layer_wts_abs == 0, 0, layer_model1)\n                    pruned_wts.append(layer_mod)\n                elif len(layer_model1.shape) == 2:\n                    layer_wts_abs = np.abs(layer_model1)\n                    layer_wts2_abs = np.abs(layer_model2)\n                    layer_wts_abs[(layer_wts_abs &lt; threshold_weights1) &amp; (layer_wts2_abs &lt; threshold_weights2)] = 0\n                    layer_mod = np.where(layer_wts_abs == 0, 0, layer_model1)\n                    pruned_wts.append(layer_mod)\n                else:\n                    pruned_wts.append(layer_model1)\n                \n                \n            return pruned_wts\n            \n    \n        # prune 15% of smallest magnitude weights-\n        pruned_wts = custom_pruning(model1 = model, model2 = model2, p = 15)\n    \n        # initialize and load weights for pruned model-\n        new_model = cnn_model()\n        new_model.set_weights(pruned_wts)\n        \n        # count original and unpruned parameters-\n        orig_params = count_nonzero_params(model)\n        \n        # count pruned parameters-\n        pruned_params = count_nonzero_params(new_model)\n        \n        # compute actual sparsity-\n        sparsity = ((orig_params - pruned_params) / orig_params) * 100\n        \n        print(f\"actual sparsity = {sparsity:.2f}% for a given sparsity = 15%\")\n        # actual sparsity = 2.22% for a given sparsity = 15%\n\nthe problem is that for a given sparsity of 15%, only 2.22% connections are pruned. to achieve the desired 15% sparsity, a hit and trial method to find 'p' parameter's value-\n\n        # prune 15% of smallest magnitude weights-\n        pruned_wts = custom_pruning(model1 = model, model2 = model2, p = 38)\n    \n        # initialize and load weights for pruned model-\n        new_model = cnn_model()\n        new_model.set_weights(pruned_wts)\n        \n        # count pruned parameters-\n        pruned_params = count_nonzero_params(new_model)\n        \n        # compute actual sparsity-\n        sparsity = ((orig_params - pruned_params) / orig_params) * 100\n        \n        print(f\"actual sparsity = {sparsity:.2f}% for a given sparsity = 15%\")\n        # actual sparsity = 14.40% for a given sparsity = 15%\n\ndue to two conditions while filtering in 'custom\\_pruning()', this difference between desired and actual sparsity levels are occurring.\n\n&amp;#x200b;\n\nis there some other better way to achieve this that i am missing out?\n\n&amp;#x200b;\n\nthanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/ookq8g/prune_neural_networks_layers_for_f_sparsity/',)", "identifyer": 5591176, "year": "2021"}, {"autor": "gauravc2796", "date": 1632244816000, "content": "New Research Paper video Explainer: Block Pruning For Faster Transformers by Hugging Face /!/ Hi,\n\nA new research paper explainer has been released:\n\nBlock Pruning For Faster Transformers by Hugging Face [https://youtu.be/CyJdzkcdGl0](https://youtu.be/CyJdzkcdGl0)\n\nVideo explains the basics of pruning, distillation, paper overview, and also codes.\n\nTLDR:\n\n1. Large pre-trained NN to have billions of parameters to train. It is computationally expensive to load or download these models.\n2. Here comes pruning and distillation. Both of these techniques try to eliminate parameters (weights) with very little drop in accuracy.\n3. If we want to prune weights when finetuning our model, then movement pruning works best for that. However, the time and space requirements to train these models again to drop unnecessary could be expensive.\n4. Here comes block pruning where it creates square blocks in the weight matrix and then tries to perform movement pruning on the blocks rather than weights.\n\nFor an in-depth overview refer to the video explainer.\n\n&amp;#x200B;\n\nIf you like this content, do share it with friends and support the channel. If any opinions, clarifications, or anything you can mention in the comments.", "link": "https://www.reddit.com/r/datascience/comments/psne85/new_research_paper_video_explainer_block_pruning/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "prune", "selectorShort": "prune", "MarkedSent": "new research paper video explainer: block pruning for faster transformers by hugging face /!/ hi,\n\na new research paper explainer has been released:\n\nblock pruning for faster transformers by hugging face [https://youtu.be/cyjdzkcdgl0](https://youtu.be/cyjdzkcdgl0)\n\nvideo explains the basics of pruning, distillation, paper overview, and also codes.\n\ntldr:\n\n1. large pre-trained nn to have billions of parameters to train. it is computationally expensive to load or download these models.\n2. here comes pruning and distillation. both of these techniques try to eliminate parameters (weights) with very little drop in accuracy.\n3. if we want to -----> prune !!!  weights when finetuning our model, then movement pruning works best for that. however, the time and space requirements to train these models again to drop unnecessary could be expensive.\n4. here comes block pruning where it creates square blocks in the weight matrix and then tries to perform movement pruning on the blocks rather than weights.\n\nfor an in-depth overview refer to the video explainer.\n\n&amp;#x200b;\n\nif you like this content, do share it with friends and support the channel. if any opinions, clarifications, or anything you can mention in the comments.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/psne85/new_research_paper_video_explainer_block_pruning/',)", "identifyer": 5598381, "year": "2021"}], "name": "prunedatascience2021"}
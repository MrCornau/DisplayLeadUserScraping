{"interestingcomments": [{"autor": "Camjw1123", "date": 1625177960000, "content": "Building a tool with GLT-3 to write your resume for you, and tailor it to the job spec! What do you think?", "link": "https://www.reddit.com/r/datascience/comments/obwojn/building_a_tool_with_glt3_to_write_your_resume/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "building a -----> tool !!!  with glt-3 to write your resume for you, and tailor it to the job spec! what do you think?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 47, "media": "('rich:video',)", "medialink": "('https://gfycat.com/ambitioushauntingagama',)", "identifyer": 5586753, "year": "2021"}, {"autor": "next_gen_hub", "date": 1625165790000, "content": "Will alteryx continue to be relevant or is it getting crowded out by other products? /!/ My org tries to be forward looking and we are looking at alteryx right now. We are entertaining alteryx because a number of people in our department aren't proficient on SQL/python.\n\nI'm just trying to make sure we don't get a tool that will become less relevant over time. In the past i feel like a lot of people were using alteryx for data prep and data blending, but it seems like a lot of people are doing data prep/blending/movement in their platform instead, like databricks or dataiku.\n\nIs this what you're seeing? Alteryx is kinda being cut out? Or is alteryx becoming more and more relevant.\n\n[View Poll](https://www.reddit.com/poll/obsjwm)", "link": "https://www.reddit.com/r/datascience/comments/obsjwm/will_alteryx_continue_to_be_relevant_or_is_it/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "will alteryx continue to be relevant or is it getting crowded out by other products? /!/ my org tries to be forward looking and we are looking at alteryx right now. we are entertaining alteryx because a number of people in our department aren't proficient on sql/python.\n\ni'm just trying to make sure we don't get a -----> tool !!!  that will become less relevant over time. in the past i feel like a lot of people were using alteryx for data prep and data blending, but it seems like a lot of people are doing data prep/blending/movement in their platform instead, like databricks or dataiku.\n\nis this what you're seeing? alteryx is kinda being cut out? or is alteryx becoming more and more relevant.\n\n[view poll](https://www.reddit.com/poll/obsjwm)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 15, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/obsjwm/will_alteryx_continue_to_be_relevant_or_is_it/',)", "identifyer": 5586761, "year": "2021"}, {"autor": "WalterDragan", "date": 1625112040000, "content": "Job schedulers and orchestration? /!/ Curious to hear how other people in the data science community schedule jobs and orchestrate tasks. We use a tool called Control-M which is GUI based and produces XML files as job definitions. I'd rate it pretty low in terms of being able to be dynamic and flexible. A lot of jobs end up being created that are nearly identical with just a different parameter getting passed in.\n\nHow are you doing it at your work?", "link": "https://www.reddit.com/r/datascience/comments/obdqzj/job_schedulers_and_orchestration/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "job schedulers and orchestration? /!/ curious to hear how other people in the data science community schedule jobs and orchestrate tasks. we use a -----> tool !!!  called control-m which is gui based and produces xml files as job definitions. i'd rate it pretty low in terms of being able to be dynamic and flexible. a lot of jobs end up being created that are nearly identical with just a different parameter getting passed in.\n\nhow are you doing it at your work?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/obdqzj/job_schedulers_and_orchestration/',)", "identifyer": 5586790, "year": "2021"}, {"autor": "sundayp26", "date": 1617271641000, "content": "I applied for a job that had data analysis as the main requirement of the job. They asked me to pick any tool I want (R/Python/Excel) and proceeded to give me a task that could be solved through excel only. Is Excel something every aspiring Data scientist should know? /!/ They gave me a workbook with 3 sheets. The first said, use excel create a graph and marked a big area in yellow within the same sheet. There was a table on the left side and the question as well as the yellow area on the right\n\nThe second one asked me to change the date time format. It was written that I could use any excel funtion I could want. Again, the table on the left side of the screen and the yellow area to its right.\n\nThe third one had a formula which was in excel with the whole $ and other excel syntax. I was supposed to decipher it and describe it below in a specified area in english.\n\nThe problem was that I had opened jupyter notebook thinking I would get some data to clean, prepare. Maybe even fit a model and predict some thing.  \nI thought I would be making charts using ggplot2 in R or matplotlib in python.\n\nIs excel really the whole thing recruiters are looking for when they put data analysis in their job description? And should I learn it well before applying to other jobs?", "link": "https://www.reddit.com/r/datascience/comments/mhs3r8/i_applied_for_a_job_that_had_data_analysis_as_the/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "i applied for a job that had data analysis as the main requirement of the job. they asked me to pick any -----> tool !!!  i want (r/python/excel) and proceeded to give me a task that could be solved through excel only. is excel something every aspiring data scientist should know? /!/ they gave me a workbook with 3 sheets. the first said, use excel create a graph and marked a big area in yellow within the same sheet. there was a table on the left side and the question as well as the yellow area on the right\n\nthe second one asked me to change the date time format. it was written that i could use any excel funtion i could want. again, the table on the left side of the screen and the yellow area to its right.\n\nthe third one had a formula which was in excel with the whole $ and other excel syntax. i was supposed to decipher it and describe it below in a specified area in english.\n\nthe problem was that i had opened jupyter notebook thinking i would get some data to clean, prepare. maybe even fit a model and predict some thing.  \ni thought i would be making charts using ggplot2 in r or matplotlib in python.\n\nis excel really the whole thing recruiters are looking for when they put data analysis in their job description? and should i learn it well before applying to other jobs?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 8, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/mhs3r8/i_applied_for_a_job_that_had_data_analysis_as_the/',)", "identifyer": 5586874, "year": "2021"}, {"autor": "anomalias", "date": 1617239853000, "content": "Wanting to transition from auditing to data analyst (and maybe science in the long run) /!/ I've been a full time financial auditor at my firm for the past 3 years auditing (We audit governmental ministries). Starting from June only 50% of my work will be financial auditing and the other 50% will be what they call \"data analyst\". It's a small newly created team. However, this isn't your traditional data analyst role with Tableau/Power BI as your main tool. It's pretty much \"Try and do something clever with excel/VBA to trivialise some of the more repetitive auditing tasks\" (such as salary). To be honest, I am not entirely sure what I'll be doing in this role, since I haven't started yet.\n\nTools I know they have \"created\" so far is one for picking random samples, one for analysing what number the invoices start at ([Benford's law](https://en.wikipedia.org/wiki/Benford%27s_law)), and some others as well.\n\nI have been toying with the idea of changing career from financial auditing to data analyst in a few years. Currently I'm taking some Tableau, Power BI and Machine Learning courses (I am quite decent at Python and SQL).\n\nThis new role will not have any python and tableau/power bi, and won't be analysing data in the \"traditional way\" a given data analyst does for \"normal\" companies, if you will.\n\nSo now to my question: will it be misleading future employers putting \"Data analyst 50%\" and \"Financial Audting 50%\" on my resume/linkedin?", "link": "https://www.reddit.com/r/datascience/comments/mhkf3l/wanting_to_transition_from_auditing_to_data/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "wanting to transition from auditing to data analyst (and maybe science in the long run) /!/ i've been a full time financial auditor at my firm for the past 3 years auditing (we audit governmental ministries). starting from june only 50% of my work will be financial auditing and the other 50% will be what they call \"data analyst\". it's a small newly created team. however, this isn't your traditional data analyst role with tableau/power bi as your main -----> tool !!! . it's pretty much \"try and do something clever with excel/vba to trivialise some of the more repetitive auditing tasks\" (such as salary). to be honest, i am not entirely sure what i'll be doing in this role, since i haven't started yet.\n\ntools i know they have \"created\" so far is one for picking random samples, one for analysing what number the invoices start at ([benford's law](https://en.wikipedia.org/wiki/benford%27s_law)), and some others as well.\n\ni have been toying with the idea of changing career from financial auditing to data analyst in a few years. currently i'm taking some tableau, power bi and machine learning courses (i am quite decent at python and sql).\n\nthis new role will not have any python and tableau/power bi, and won't be analysing data in the \"traditional way\" a given data analyst does for \"normal\" companies, if you will.\n\nso now to my question: will it be misleading future employers putting \"data analyst 50%\" and \"financial audting 50%\" on my resume/linkedin?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 8, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/mhkf3l/wanting_to_transition_from_auditing_to_data/',)", "identifyer": 5586891, "year": "2021"}, {"autor": "Vinothd19", "date": 1617222710000, "content": "Need Help in creating points /!/ Hi Guys,\n\nI am a Mechanical Engineering Graduate and now doing masters in Data Science.\n\nright now i am trying to land a part time job in BI/DA, even though i edited my resume to match the Job Description and required skills i couldnt land a single interview for past one month.\n\nkindly find my experience summary below.\n\n    SKILLS\n    \n    Power BI, Tableau, SQL, Python (Pandas, NumPy, Seaborn), R, ETL(Pentaho), SPSS, \n    Project Management, Statistical Analysis, Data Modelling.\n    \n    EXPERIENCE\t\n    \n    \n    DATA ANALYST EXECUTIVE\n    \u2022\tWorking in Technical Support team parse and visualize data helping in \n    \n    better decision making.\n    \u2022\tCollaborated with sales and product team to create an internal analytical\n     reporting section/view in Power BI that allows all stakeholders to see insights \n    at any time.\n    \u2022\tGenerated insights, which helped in reducing the warranty loss by 20%\n     saving $150K per year.\n    \u2022\tHandled Digital Transformation Projects including concept and scope, \n    business process document, feasibility studies, execution plans, and on-site \n    tests with end user, project closing. \n    \n    PROJECTS\n    \n    Implementing Microsoft Dynamics as CRM tool.\n    A feedback tool for customers for product survey\n    \n    DATA ANALYST EXECUTIVE\n    \u2022\tCreating Month-wise, Quarter-wise, Year-wise parts sales report. \n    (Power BI)\n    \u2022\tSupporting demand planning team generally with cross-functional \n    analytics.\n    \u2022\tGenerated /KPI\u2019s which helped in inventory planning and reducing the\n     operational cost by 10%.\n    \u2022\tWorking with stakeholders and helping them take better business decisions\n     by providing insights.\n    \u2022\tChanged the process of purchasing the parts from the supplier to help\n     speed up the parts delivery to the customers.\n    \u2022\tLed small team to automate the reports and few it projects which helped \n     in increasing revenue.\n    \u2022\tAutomated various manual procedures and reports\n    \u2022\tWork primarily done in Excel, Power BI, and MYSQL\n    \n    PROJECTS\n    \n    An AMC which helped in retaining the new customers by 30% up to 5 years.\n    A Campaign tool to understand the parts sales and service in various regions. \n    Monitoring an E-commerce Website and Android app development.", "link": "https://www.reddit.com/r/datascience/comments/mhf0oe/need_help_in_creating_points/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "need help in creating points /!/ hi guys,\n\ni am a mechanical engineering graduate and now doing masters in data science.\n\nright now i am trying to land a part time job in bi/da, even though i edited my resume to match the job description and required skills i couldnt land a single interview for past one month.\n\nkindly find my experience summary below.\n\n    skills\n    \n    power bi, tableau, sql, python (pandas, numpy, seaborn), r, etl(pentaho), spss, \n    project management, statistical analysis, data modelling.\n    \n    experience\t\n    \n    \n    data analyst executive\n    \u2022\tworking in technical support team parse and visualize data helping in \n    \n    better decision making.\n    \u2022\tcollaborated with sales and product team to create an internal analytical\n     reporting section/view in power bi that allows all stakeholders to see insights \n    at any time.\n    \u2022\tgenerated insights, which helped in reducing the warranty loss by 20%\n     saving $150k per year.\n    \u2022\thandled digital transformation projects including concept and scope, \n    business process document, feasibility studies, execution plans, and on-site \n    tests with end user, project closing. \n    \n    projects\n    \n    implementing microsoft dynamics as crm -----> tool !!! .\n    a feedback tool for customers for product survey\n    \n    data analyst executive\n    \u2022\tcreating month-wise, quarter-wise, year-wise parts sales report. \n    (power bi)\n    \u2022\tsupporting demand planning team generally with cross-functional \n    analytics.\n    \u2022\tgenerated /kpi\u2019s which helped in inventory planning and reducing the\n     operational cost by 10%.\n    \u2022\tworking with stakeholders and helping them take better business decisions\n     by providing insights.\n    \u2022\tchanged the process of purchasing the parts from the supplier to help\n     speed up the parts delivery to the customers.\n    \u2022\tled small team to automate the reports and few it projects which helped \n     in increasing revenue.\n    \u2022\tautomated various manual procedures and reports\n    \u2022\twork primarily done in excel, power bi, and mysql\n    \n    projects\n    \n    an amc which helped in retaining the new customers by 30% up to 5 years.\n    a campaign tool to understand the parts sales and service in various regions. \n    monitoring an e-commerce website and android app development.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/mhf0oe/need_help_in_creating_points/',)", "identifyer": 5586899, "year": "2021"}, {"autor": "dcastm", "date": 1622582199000, "content": "Why gatekeep? (and the reason why it won't work!) /!/ Whenever I hear someone saying, \"to be a real data scientist, you need to know X,\" I cringe. In most cases, that's not useful: data scientists do different things and use various tools in different companies. There's no point in saying that you need to know X, Y, or Z. Learn the tool that gets the job done, whichever that is.\n\nThough, I've often wondered why this kind of gatekeeping is so frequent in our industry. After thinking about it for a while, I came to the following conclusion: for well-paid professions, there's an inverse relationship between barriers to entry and the amount of gatekeeping.\n\nFor example, take the healthcare industry. This industry is heavily regulated, so there are high barriers to entry: you cannot go around calling yourself a medical doctor; you need to get a specific degree for that. Which makes sense: you don't want to put your life in the hands of someone you don't know can tell the difference between a heart attack and a headache.\n\nOn the other spectrum, you have the technological industry. In it, there are fewer regulations: you don't need a specific degree or permission to be part of it. So, you'll find a broader range of professions in its participants. It's not rare to find a programmer whose background is in arts or humanities. This industry has low barriers to entry.\n\nFor people outside of these industries, low and high barriers to entry create different incentives. In tech, there's little risk in saying you're a software engineer without having the required knowledge. You might be kicked out of a company, or get a bad reputation. In healthcare, if you fake your credentials, you might end up in prison.\n\nBecause of this fact, people in these industries behave differently.\n\nMy wife is a medical doctor, so I get to hang around many people from healthcare. I've never heard anyone in that area say \"if you don't know X, then you are not a real medical doctor.\" They don't seem to worry too much about that. They know there's no shortcut to practice medicine. You need to get a degree and pass the required exams.\n\nI work as a data scientist, so I also get to spent time with a few techies. Opposite to healthcare, in tech, I often see people talking about what a real data scientist/software engineer/data engineer should know.\n\nI believe many people in tech behave this way to protect their current status. Well-paid jobs provide you with a high social standing and a good salary. If you see any of those at risk, you look for ways to mitigate those risks.\n\nThere's no problem in industries with high barriers to entry: outsiders won't try very hard to get into it. Hardly anyone quits their job, studies for six years, and then becomes a medical doctor. But, if you combine low barriers to entry with well-paid jobs, then people will come running at it like if they were escaping from an army of zombies. It's no by chance that new boot camps pop up every day.\n\nThat's where gatekeepers enter the game. They have one goal: keep their salaries and status high. How do they do it? They create artificial barriers to entry by making what they do look harder than it really is.\n\nBut don't get me wrong, I'm not stating that that's an evil goal. It makes total sense. I'm yet to see someone fighting for a lower salary or a decrease in their social standing!\n\nI just question the means. Will gatekeeping really make sure you keep a good salary and high social standing?\n\nI bet it won't. In the last twenty years, we've seen three things increase quickly:\n\n* People working remotely\n* Free or low-cost access to high-quality education\n* Democratization of technology\n\nSo there are way too many people living in cheaper areas wanting to get a higher paying job that will have access to more affordable and better education and can access the same technology you can. If this continues, whatever attempt people put into creating artificial barriers will fall short.\n\nSo, don't gatekeep. It won't work.", "link": "https://www.reddit.com/r/datascience/comments/nq4uwe/why_gatekeep_and_the_reason_why_it_wont_work/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "why gatekeep? (and the reason why it won't work!) /!/ whenever i hear someone saying, \"to be a real data scientist, you need to know x,\" i cringe. in most cases, that's not useful: data scientists do different things and use various tools in different companies. there's no point in saying that you need to know x, y, or z. learn the -----> tool !!!  that gets the job done, whichever that is.\n\nthough, i've often wondered why this kind of gatekeeping is so frequent in our industry. after thinking about it for a while, i came to the following conclusion: for well-paid professions, there's an inverse relationship between barriers to entry and the amount of gatekeeping.\n\nfor example, take the healthcare industry. this industry is heavily regulated, so there are high barriers to entry: you cannot go around calling yourself a medical doctor; you need to get a specific degree for that. which makes sense: you don't want to put your life in the hands of someone you don't know can tell the difference between a heart attack and a headache.\n\non the other spectrum, you have the technological industry. in it, there are fewer regulations: you don't need a specific degree or permission to be part of it. so, you'll find a broader range of professions in its participants. it's not rare to find a programmer whose background is in arts or humanities. this industry has low barriers to entry.\n\nfor people outside of these industries, low and high barriers to entry create different incentives. in tech, there's little risk in saying you're a software engineer without having the required knowledge. you might be kicked out of a company, or get a bad reputation. in healthcare, if you fake your credentials, you might end up in prison.\n\nbecause of this fact, people in these industries behave differently.\n\nmy wife is a medical doctor, so i get to hang around many people from healthcare. i've never heard anyone in that area say \"if you don't know x, then you are not a real medical doctor.\" they don't seem to worry too much about that. they know there's no shortcut to practice medicine. you need to get a degree and pass the required exams.\n\ni work as a data scientist, so i also get to spent time with a few techies. opposite to healthcare, in tech, i often see people talking about what a real data scientist/software engineer/data engineer should know.\n\ni believe many people in tech behave this way to protect their current status. well-paid jobs provide you with a high social standing and a good salary. if you see any of those at risk, you look for ways to mitigate those risks.\n\nthere's no problem in industries with high barriers to entry: outsiders won't try very hard to get into it. hardly anyone quits their job, studies for six years, and then becomes a medical doctor. but, if you combine low barriers to entry with well-paid jobs, then people will come running at it like if they were escaping from an army of zombies. it's no by chance that new boot camps pop up every day.\n\nthat's where gatekeepers enter the game. they have one goal: keep their salaries and status high. how do they do it? they create artificial barriers to entry by making what they do look harder than it really is.\n\nbut don't get me wrong, i'm not stating that that's an evil goal. it makes total sense. i'm yet to see someone fighting for a lower salary or a decrease in their social standing!\n\ni just question the means. will gatekeeping really make sure you keep a good salary and high social standing?\n\ni bet it won't. in the last twenty years, we've seen three things increase quickly:\n\n* people working remotely\n* free or low-cost access to high-quality education\n* democratization of technology\n\nso there are way too many people living in cheaper areas wanting to get a higher paying job that will have access to more affordable and better education and can access the same technology you can. if this continues, whatever attempt people put into creating artificial barriers will fall short.\n\nso, don't gatekeep. it won't work.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 17, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nq4uwe/why_gatekeep_and_the_reason_why_it_wont_work/',)", "identifyer": 5586938, "year": "2021"}, {"autor": "curious_sapient", "date": 1622554636000, "content": "What BI tools does a marketing team use and how do they push data from various sources to the BI tool and make it analysis-ready?", "link": "https://www.reddit.com/r/datascience/comments/npu8k8/what_bi_tools_does_a_marketing_team_use_and_how/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what bi tools does a marketing team use and how do they push data from various sources to the bi -----> tool !!!  and make it analysis-ready?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/npu8k8/what_bi_tools_does_a_marketing_team_use_and_how/',)", "identifyer": 5586961, "year": "2021"}, {"autor": "Kokubo-ubo", "date": 1622545373000, "content": "Data Manipulation layer - where? /!/ Hello hello - After months of reading posts, blogs, watching youtube videos, etc... I am still wondering where do people do the data manipulation in their data stack. I know it depends on what are your goals, the overall data stack and so on. However, how do you guys solve the issue of manipulating the data before analysis?\n\n  \nMy typical example is: Data source: company DB (a copy created for data team) - Output: Dashaboard in BI tool - Data manipulation needed: cleaning, formatting, calculated fields, etc...  \n\n\nAt the moment we are doing everything in our BI tool, and it is fine. However, we have some limitations, since we can't use python in it and also, we are creating a lot of similar calculated tables in it (fortunately there is the possibility of query what has been imported from DB to build new tables inside the BI tool).  \n\n\nI guess that the answer to this is probably to set up a data warehouse, but even in this case, where do you do manipulate data? Do you create a table in the DWH each time you want a new analysis?  \n\n\nThanks!", "link": "https://www.reddit.com/r/datascience/comments/nprcsy/data_manipulation_layer_where/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "data manipulation layer - where? /!/ hello hello - after months of reading posts, blogs, watching youtube videos, etc... i am still wondering where do people do the data manipulation in their data stack. i know it depends on what are your goals, the overall data stack and so on. however, how do you guys solve the issue of manipulating the data before analysis?\n\n  \nmy typical example is: data source: company db (a copy created for data team) - output: dashaboard in bi -----> tool !!!  - data manipulation needed: cleaning, formatting, calculated fields, etc...  \n\n\nat the moment we are doing everything in our bi -----> tool !!! , and it is fine. however, we have some limitations, since we can't use python in it and also, we are creating a lot of similar calculated tables in it (fortunately there is the possibility of query what has been imported from db to build new tables inside the bi tool).  \n\n\ni guess that the answer to this is probably to set up a data warehouse, but even in this case, where do you do manipulate data? do you create a table in the dwh each time you want a new analysis?  \n\n\nthanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nprcsy/data_manipulation_layer_where/',)", "identifyer": 5586972, "year": "2021"}, {"autor": "TruePositive6", "date": 1622536742000, "content": "data pipeline versioning /!/ Hey all,\n\nMy team has a postgres DB with multiple raw data tables. Almost each table has its own pipeline for normalizing, feature extraction etc... A pipeline for example can be: \n\n Read Raw Table \u2192 One hot conversion \u2192 Normalization  \u2192 ... \n\nEach stage in the pipeline outputs an intermediate result:\n\n Raw\\_Table \u2192 One\\_hot\\_conversion\\_table \u2192 Normalized\\_one\\_hot\\_conversion\\_table \u2192 ... \n\nIn one small scale project we tried to use [DVC](https://dvc.org/) and really liked the pipeline interface and the caching feature. The downside of DVC is that it only works with local files whereas in other projects we load and output data in batches from/to tables in the remote DB. \n\nIs there a tool which have this kind of pipeline interface, caching of the intermediate results and supports remote databases as well?\n\nHow do you keep track of your intermediate data results in your pre-training phase of the project?", "link": "https://www.reddit.com/r/datascience/comments/nppahh/data_pipeline_versioning/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "data pipeline versioning /!/ hey all,\n\nmy team has a postgres db with multiple raw data tables. almost each table has its own pipeline for normalizing, feature extraction etc... a pipeline for example can be: \n\n read raw table \u2192 one hot conversion \u2192 normalization  \u2192 ... \n\neach stage in the pipeline outputs an intermediate result:\n\n raw\\_table \u2192 one\\_hot\\_conversion\\_table \u2192 normalized\\_one\\_hot\\_conversion\\_table \u2192 ... \n\nin one small scale project we tried to use [dvc](https://dvc.org/) and really liked the pipeline interface and the caching feature. the downside of dvc is that it only works with local files whereas in other projects we load and output data in batches from/to tables in the remote db. \n\nis there a -----> tool !!!  which have this kind of pipeline interface, caching of the intermediate results and supports remote databases as well?\n\nhow do you keep track of your intermediate data results in your pre-training phase of the project?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nppahh/data_pipeline_versioning/',)", "identifyer": 5586977, "year": "2021"}, {"autor": "vietlinh12hoa", "date": 1622482681000, "content": "What is your thought on SAS as a tool for data science /!/  Cheer everyone,\n\nI just moved from Python to SAS for 4 months due to new job requirements. I wonder how you think SAS compared with other languages, any future.\n\nMine:\n\n\\- SAS is not so complex. The only problem is we have to memorize weird syntax\n\n\\- Lots of problem can be solved with proc sql. Unfortunately, proc sql has some different characteristics compared with standard SQL (e.g. why row\\_number is missing in proc sql????). I likely use SAS mainly for practicing SQL.\n\n\\- The syntax is unique and not transferrable. If you're in SAS industry for too long, then it's likely hard to move to other jobs with different tool. Unlike if you know MATLAB or Python, you can easily move to R, or even C/C++ (They're interconnected with each other very well, SAS is a standalone hero)\n\n\\- Company uses SAS likely for security purposes (need an organization who is responsible for the tool if anything bad happened)\n\n\\- Then SAS Visual Analytics is another story and if you program for Advanced Filter in SAS Viya, then again it's more or less different systax compared with SAS Guide.\n\nWhat's your thought?", "link": "https://www.reddit.com/r/datascience/comments/np8uqk/what_is_your_thought_on_sas_as_a_tool_for_data/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what is your thought on sas as a -----> tool !!!  for data science /!/  cheer everyone,\n\ni just moved from python to sas for 4 months due to new job requirements. i wonder how you think sas compared with other languages, any future.\n\nmine:\n\n\\- sas is not so complex. the only problem is we have to memorize weird syntax\n\n\\- lots of problem can be solved with proc sql. unfortunately, proc sql has some different characteristics compared with standard sql (e.g. why row\\_number is missing in proc sql????). i likely use sas mainly for practicing sql.\n\n\\- the syntax is unique and not transferrable. if you're in sas industry for too long, then it's likely hard to move to other jobs with different tool. unlike if you know matlab or python, you can easily move to r, or even c/c++ (they're interconnected with each other very well, sas is a standalone hero)\n\n\\- company uses sas likely for security purposes (need an organization who is responsible for the tool if anything bad happened)\n\n\\- then sas visual analytics is another story and if you program for advanced filter in sas viya, then again it's more or less different systax compared with sas guide.\n\nwhat's your thought?", "sortedWord": "None", "removed": "('nan',)", "score": 5, "comments": 145, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/np8uqk/what_is_your_thought_on_sas_as_a_tool_for_data/',)", "identifyer": 5587000, "year": "2021"}, {"autor": "Beautiful_Blood", "date": 1622401786000, "content": "Create a website to consult the data of all the central banks in the world /!/ Good afternoon community:\n\nI have recently been consulting some basic data on central banks and I have realized that in all central banks in the world they are very messy, hidden in \"weird\" tabs, in menus that take forever to reach after clicking on 10 drop-downs, etc. .\n\nTherefore, it has occurred to me that maybe I could make a website, [Streamlit](https://streamlit.io/) ([example of Streamlit here](https://share.streamlit.io/crosstabkite/worst-case-analysis/app.py)) or [Dash](https://dash-gallery.plotly.host/Portal/) ([example of Dash here](https://dash-gallery.plotly.host/dash-manufacture-spc-dashboard/)) (I'm Data Scientist, don't know fullstack web developing tools) in which to do two things:\n\n1- Automate data collection either through API or through web scraping so that they are always updated.\n\n2- Create a visualization tool that is easy, simple and for the whole family (with any of the tools mentioned above) that is capable of reflecting this data in graphics. I would like to take things from basic things like year-on-year inflation or interest rates to more complex things like hedonic inflation adjustment.\n\n&amp;#x200B;\n\nMY QUESTIONS ARE:\n\n1- Do you think that the web can have some traffic and be interesting for professionals who research or work in economics?\n\n2- What data could be considered \"rare\" within the statistics that a central bank can offer? I do this with the intention of putting not so common data and giving the greatest added value that I can to my website.\n\nIf you have read this far, thank you very much, if not:\n\n&amp;#x200B;\n\nTLDR: I want to make a website that reflects central bank statistics and I do not know what visualization tool or what \"complex\" statistics I could put to give more value to my project.\n\n&amp;#x200B;\n\nThanks for your help!\n\nP.D: Even if you are not interested if you give me an upvote, I greatly appreciate your help. Thanks!", "link": "https://www.reddit.com/r/datascience/comments/noi4lb/create_a_website_to_consult_the_data_of_all_the/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "create a website to consult the data of all the central banks in the world /!/ good afternoon community:\n\ni have recently been consulting some basic data on central banks and i have realized that in all central banks in the world they are very messy, hidden in \"weird\" tabs, in menus that take forever to reach after clicking on 10 drop-downs, etc. .\n\ntherefore, it has occurred to me that maybe i could make a website, [streamlit](https://streamlit.io/) ([example of streamlit here](https://share.streamlit.io/crosstabkite/worst-case-analysis/app.py)) or [dash](https://dash-gallery.plotly.host/portal/) ([example of dash here](https://dash-gallery.plotly.host/dash-manufacture-spc-dashboard/)) (i'm data scientist, don't know fullstack web developing tools) in which to do two things:\n\n1- automate data collection either through api or through web scraping so that they are always updated.\n\n2- create a visualization -----> tool !!!  that is easy, simple and for the whole family (with any of the tools mentioned above) that is capable of reflecting this data in graphics. i would like to take things from basic things like year-on-year inflation or interest rates to more complex things like hedonic inflation adjustment.\n\n&amp;#x200b;\n\nmy questions are:\n\n1- do you think that the web can have some traffic and be interesting for professionals who research or work in economics?\n\n2- what data could be considered \"rare\" within the statistics that a central bank can offer? i do this with the intention of putting not so common data and giving the greatest added value that i can to my website.\n\nif you have read this far, thank you very much, if not:\n\n&amp;#x200b;\n\ntldr: i want to make a website that reflects central bank statistics and i do not know what visualization tool or what \"complex\" statistics i could put to give more value to my project.\n\n&amp;#x200b;\n\nthanks for your help!\n\np.d: even if you are not interested if you give me an upvote, i greatly appreciate your help. thanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/noi4lb/create_a_website_to_consult_the_data_of_all_the/',)", "identifyer": 5587034, "year": "2021"}, {"autor": "kenfury", "date": 1627645009000, "content": "Looking to scrape a subreddit for trends /!/ There is a subreddit I was hoping to scrape for the last two years or so to see how the upvotes and comments on four weekly threads compare.  These four threads all have a unique string in the title that is consistent from week to week.  Once done i would like to export the data in the form of a excel/csv to see week over week trends of the four threads.   Any pointers on if there is already a tool that exists?\n\nThanks", "link": "https://www.reddit.com/r/datascience/comments/ouiwwm/looking_to_scrape_a_subreddit_for_trends/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "looking to scrape a subreddit for trends /!/ there is a subreddit i was hoping to scrape for the last two years or so to see how the upvotes and comments on four weekly threads compare.  these four threads all have a unique string in the title that is consistent from week to week.  once done i would like to export the data in the form of a excel/csv to see week over week trends of the four threads.   any pointers on if there is already a -----> tool !!!  that exists?\n\nthanks", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/ouiwwm/looking_to_scrape_a_subreddit_for_trends/',)", "identifyer": 5587134, "year": "2021"}, {"autor": "specialkender", "date": 1627643781000, "content": "State of the art tools in Data science /!/ Just entering the data science job world now after my PhD in computational chemistry. I've been using Python in Jupyter notebooks for most of my research to extract trends and perform statistical analysis. Pandas is my comfort zone using plotly/seaborn/matplotlib to produce plots.\n\nThis was all fine for the task i had to manage before, but now that I'll have to deal with request of companies I'm looking to expand my array of tools at my disposal. I'd want to keep Python as my main programming language, so to choose tools that specifically can interface with Python. My question is, what are some state of the art, versatile tools that are \"must know\" or \"you really wanna know\" nowadays if you want to work with big companies?\n\nI think I understand that I'll forcefully have to learn SQL, which doesn't seem too bad. Glue seems an amazing library that I started to learn. I guess I'd need to learn some data warehousing tool and so on, also I hear that Apache Spark is highly sought, but is it essential?\n\nAny thought opinion is welcome!", "link": "https://www.reddit.com/r/datascience/comments/ouin0t/state_of_the_art_tools_in_data_science/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "state of the art tools in data science /!/ just entering the data science job world now after my phd in computational chemistry. i've been using python in jupyter notebooks for most of my research to extract trends and perform statistical analysis. pandas is my comfort zone using plotly/seaborn/matplotlib to produce plots.\n\nthis was all fine for the task i had to manage before, but now that i'll have to deal with request of companies i'm looking to expand my array of tools at my disposal. i'd want to keep python as my main programming language, so to choose tools that specifically can interface with python. my question is, what are some state of the art, versatile tools that are \"must know\" or \"you really wanna know\" nowadays if you want to work with big companies?\n\ni think i understand that i'll forcefully have to learn sql, which doesn't seem too bad. glue seems an amazing library that i started to learn. i guess i'd need to learn some data warehousing -----> tool !!!  and so on, also i hear that apache spark is highly sought, but is it essential?\n\nany thought opinion is welcome!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/ouin0t/state_of_the_art_tools_in_data_science/',)", "identifyer": 5587137, "year": "2021"}, {"autor": "Fishcork", "date": 1633032636000, "content": "Probability for a deal to win /!/ Hi all,\n\nI currently have a large dataset of deals that are either 'open', 'lost', or 'won'. Most deals that are won will take approximately 2-3 months. \n\nI want to essentially forecast the subtotal of won deals in the next few months, but I'm having trouble setting up my model. Namely, I'm having trouble incorporating the 2-3 month average time for deals to be won. Because the probability for a deal to win increases with time, before it hits around 3 months and then it starts decreasing with time. \n\n&amp;#x200B;\n\nDoes anyone have any ideas? I managed to implement a super basic model within my BI tool that follows a normal distribution, but I'm looking for something more robust. Thanks in advance!", "link": "https://www.reddit.com/r/datascience/comments/pyrtf2/probability_for_a_deal_to_win/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "probability for a deal to win /!/ hi all,\n\ni currently have a large dataset of deals that are either 'open', 'lost', or 'won'. most deals that are won will take approximately 2-3 months. \n\ni want to essentially forecast the subtotal of won deals in the next few months, but i'm having trouble setting up my model. namely, i'm having trouble incorporating the 2-3 month average time for deals to be won. because the probability for a deal to win increases with time, before it hits around 3 months and then it starts decreasing with time. \n\n&amp;#x200b;\n\ndoes anyone have any ideas? i managed to implement a super basic model within my bi -----> tool !!!  that follows a normal distribution, but i'm looking for something more robust. thanks in advance!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 15, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pyrtf2/probability_for_a_deal_to_win/',)", "identifyer": 5587169, "year": "2021"}, {"autor": "andrewtakesphotos", "date": 1633023735000, "content": "Cleaning Up Excel Data /!/ **Background Information**\n\nI am not a data scientist - in fact I am a chemical engineer by degree and complete n00b when it comes to most things related to data . I am proficient at best with Microsoft Excel, and have no programming experience (unless you count the HTML I learned and forgot about 16 years ago). \n\nThat being said  I do completely understand the value of being able to collect, clean up, interpret, and present large amounts of information in a meaningful way. In my new job role, I will have the opportunity to wear many hats at my new company, where I will help create an American organization for a well-established European company. \n\nDuring this process, it would be great if I could help our company grow by developing a broader skill set and tool chest for myself.  To start I would like to get more experience in Excel and learn how to use ArcGIS for visualization and trending. I am sure this will just be the very beginning - but I think it's a good place to start!   \n\nOh - I believe that it will help in my future hiring if I can at least develop a foundational knowledge of data science and systems. I don't expect to become a great data scientist - but if I can learn how to do the basics myself, and understand what can be done - that will help tremendously with finding the right hires as we expand!\n\n&amp;#x200B;\n\n**Project #1: Request for Help**\n\n**Help with my first small project - Market Research data**: I have been provided with a rather clunky Excel file of market research (see screenshot attached). This includes qualitative information including a lengthy list of companies and location data, plus quantitative values including physical size. My current thinking is to attack this project in the following steps:\n\n&amp;#x200B;\n\n1. **Clean Up Data** \\- in particular, I would like to:\n   1. Clean up the address data into a format that I can later more easily import into **ArcGIS / Power BI**  (I am also learning how to use ArcGIS, and our current Market Researcher uses PowerBI already). \n   2. **Validate (location) data** (this may need to be more manual for now, or I may need to enlist the help of a market researcher)\n   3. Generate **Latitude and Longitude** coordinates for the physical addresses\n2. **Import Data in ArcGIS** \\- I would like to visually represent this data as a function of key quantitative measures by using ArcGIS. Eventually, I would like to move into data trends and forecasting using ArcGIS with PowerBI, but that's a problem for future me...\n\nPlease let me know if you can help me to get started with Step #1 of this! Any tutorials or other recommendations on ways to expand my knowledge base or skills would also be much appreciated. Thanks in advance!", "link": "https://www.reddit.com/r/datascience/comments/pyov6q/cleaning_up_excel_data/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "cleaning up excel data /!/ **background information**\n\ni am not a data scientist - in fact i am a chemical engineer by degree and complete n00b when it comes to most things related to data . i am proficient at best with microsoft excel, and have no programming experience (unless you count the html i learned and forgot about 16 years ago). \n\nthat being said  i do completely understand the value of being able to collect, clean up, interpret, and present large amounts of information in a meaningful way. in my new job role, i will have the opportunity to wear many hats at my new company, where i will help create an american organization for a well-established european company. \n\nduring this process, it would be great if i could help our company grow by developing a broader skill set and -----> tool !!!  chest for myself.  to start i would like to get more experience in excel and learn how to use arcgis for visualization and trending. i am sure this will just be the very beginning - but i think it's a good place to start!   \n\noh - i believe that it will help in my future hiring if i can at least develop a foundational knowledge of data science and systems. i don't expect to become a great data scientist - but if i can learn how to do the basics myself, and understand what can be done - that will help tremendously with finding the right hires as we expand!\n\n&amp;#x200b;\n\n**project #1: request for help**\n\n**help with my first small project - market research data**: i have been provided with a rather clunky excel file of market research (see screenshot attached). this includes qualitative information including a lengthy list of companies and location data, plus quantitative values including physical size. my current thinking is to attack this project in the following steps:\n\n&amp;#x200b;\n\n1. **clean up data** \\- in particular, i would like to:\n   1. clean up the address data into a format that i can later more easily import into **arcgis / power bi**  (i am also learning how to use arcgis, and our current market researcher uses powerbi already). \n   2. **validate (location) data** (this may need to be more manual for now, or i may need to enlist the help of a market researcher)\n   3. generate **latitude and longitude** coordinates for the physical addresses\n2. **import data in arcgis** \\- i would like to visually represent this data as a function of key quantitative measures by using arcgis. eventually, i would like to move into data trends and forecasting using arcgis with powerbi, but that's a problem for future me...\n\nplease let me know if you can help me to get started with step #1 of this! any tutorials or other recommendations on ways to expand my knowledge base or skills would also be much appreciated. thanks in advance!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pyov6q/cleaning_up_excel_data/',)", "identifyer": 5587173, "year": "2021"}, {"autor": "Nosuma666", "date": 1633018107000, "content": "What tools to use for small to medium datasets? /!/ I currently am in the need for a tool to automatically create diffrent reports daily. I have used Scala running Apache Spark in the past to work with giant datasets (250 Million rows) but using it to analyse small datasets feels like getting rid of an anthill with an orbital cannon. \nWhat tools do you guys use and recomend?\n\nPlease excuse any bad grammar as English is not my first language.", "link": "https://www.reddit.com/r/datascience/comments/pymymo/what_tools_to_use_for_small_to_medium_datasets/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what tools to use for small to medium datasets? /!/ i currently am in the need for a -----> tool !!!  to automatically create diffrent reports daily. i have used scala running apache spark in the past to work with giant datasets (250 million rows) but using it to analyse small datasets feels like getting rid of an anthill with an orbital cannon. \nwhat tools do you guys use and recomend?\n\nplease excuse any bad grammar as english is not my first language.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pymymo/what_tools_to_use_for_small_to_medium_datasets/',)", "identifyer": 5587178, "year": "2021"}, {"autor": "Ayy_Im_Walkin_Here", "date": 1632957114000, "content": "Tool Needed For Operations Problem /!/ Hi there, sorry if this is the wrong sub for this sort of question. I work at a large warehouse that ships a bunch of unqiue products (SKUs) and am trying to find a way to group them together to make it more efficient to pick. I want to create a tool or visual that shows clusters of SKUs that are commonly on the same customer orders. That way I can place them close to each other and the pickers will have their walking time reduced.\n\nI have the data set up like this:\n\n|aSKU|bSKU|OrderCount|\n|:-|:-|:-|\n|A1|B1|5|\n|B1|C1|10|\n|A1|C1|12|\n\n&amp;#x200B;\n\nThe \"OrderCount\" is the amount of orders that both SKUs are on. In my head I'd like a visual that shows each individual SKU as a dot. The more orders that SKU ships on, the larger the dot (that would come from a different table). And the stronger the connection between two SKUs, or the \"OrderCount\", the closer together the dots would be. I'm hoping to find groups or clusters of several SKUs that are commonly picked on the same order and be able to place them closer together.\n\nAny ideas or help would be greatly appreciated!", "link": "https://www.reddit.com/r/datascience/comments/py73hs/tool_needed_for_operations_problem/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "-----> tool !!!  needed for operations problem /!/ hi there, sorry if this is the wrong sub for this sort of question. i work at a large warehouse that ships a bunch of unqiue products (skus) and am trying to find a way to group them together to make it more efficient to pick. i want to create a tool or visual that shows clusters of skus that are commonly on the same customer orders. that way i can place them close to each other and the pickers will have their walking time reduced.\n\ni have the data set up like this:\n\n|asku|bsku|ordercount|\n|:-|:-|:-|\n|a1|b1|5|\n|b1|c1|10|\n|a1|c1|12|\n\n&amp;#x200b;\n\nthe \"ordercount\" is the amount of orders that both skus are on. in my head i'd like a visual that shows each individual sku as a dot. the more orders that sku ships on, the larger the dot (that would come from a different table). and the stronger the connection between two skus, or the \"ordercount\", the closer together the dots would be. i'm hoping to find groups or clusters of several skus that are commonly picked on the same order and be able to place them closer together.\n\nany ideas or help would be greatly appreciated!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/py73hs/tool_needed_for_operations_problem/',)", "identifyer": 5587208, "year": "2021"}, {"autor": "secodaHQ", "date": 1615819578000, "content": "We built an easy-to-use data discovery tool. Curious if anyone is interested in giving it a try! /!/ At our old company, data grew way too fast and it seemed like no one outside our data team knew what anything meant. The business struggled because of COVID and we had to let go of our data lead. This meant that we were left with tons of undocumented data that was useless. We tried to understand what everything meant, but this took months of effort.\n\nWe decided to build an easy-to-use data discovery tool to help other small teams, which is free to use by following this link [secoda](https://www.secoda.co/). We're in our beta right now, but the tool should be functional enough to help anyone looking for a simple solution to this problem.", "link": "https://www.reddit.com/r/datascience/comments/m5m8sb/we_built_an_easytouse_data_discovery_tool_curious/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "we built an easy-to-use data discovery -----> tool !!! . curious if anyone is interested in giving it a try! /!/ at our old company, data grew way too fast and it seemed like no one outside our data team knew what anything meant. the business struggled because of covid and we had to let go of our data lead. this meant that we were left with tons of undocumented data that was useless. we tried to understand what everything meant, but this took months of effort.\n\nwe decided to build an easy-to-use data discovery tool to help other small teams, which is free to use by following this link [secoda](https://www.secoda.co/). we're in our beta right now, but the tool should be functional enough to help anyone looking for a simple solution to this problem.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/m5m8sb/we_built_an_easytouse_data_discovery_tool_curious/',)", "identifyer": 5587391, "year": "2021"}, {"autor": "rajvosa07", "date": 1619739118000, "content": "Need recommendations for best tool to understand, visualize and potentially query a bunch of JSON files in a few directories /!/ I have a collection of JSON files which represent varying types of objects describing personal data (social media posts, links to images, social media comments, financial transactions, medical data like vaccinations, observations, fitness data like daily steps, achievement, etc).\n\nI am in search of tools that can help a non-programer understand what data is in these files, ideally visualize it and allow some queries to be run. Are there tools that you know of that are capable of this? I've played a little bit with Tableau, but the JSON files don't contain time-series data as there is some nesting of objects and it wasn't quite apparent how to accomplish this without going through mapping and transforming the data first.\n\nCan you think of ways to allow BD people (non programers) to point a tool at a directory and be able to understand (or at least get a sense of) what they are looking at?\n\nAppreciate any pointers / help.\n\nThanks", "link": "https://www.reddit.com/r/datascience/comments/n1h9tc/need_recommendations_for_best_tool_to_understand/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "need recommendations for best -----> tool !!!  to understand, visualize and potentially query a bunch of json files in a few directories /!/ i have a collection of json files which represent varying types of objects describing personal data (social media posts, links to images, social media comments, financial transactions, medical data like vaccinations, observations, fitness data like daily steps, achievement, etc).\n\ni am in search of tools that can help a non-programer understand what data is in these files, ideally visualize it and allow some queries to be run. are there tools that you know of that are capable of this? i've played a little bit with tableau, but the json files don't contain time-series data as there is some nesting of objects and it wasn't quite apparent how to accomplish this without going through mapping and transforming the data first.\n\ncan you think of ways to allow bd people (non programers) to point a tool at a directory and be able to understand (or at least get a sense of) what they are looking at?\n\nappreciate any pointers / help.\n\nthanks", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/n1h9tc/need_recommendations_for_best_tool_to_understand/',)", "identifyer": 5587537, "year": "2021"}, {"autor": "ReedCube", "date": 1621171664000, "content": "need advice on the best web scraping tool/approach for this job /!/ Hey everyone, I first posted this on r/analytics but realized it doesn't fit very well there. I need to scrape some elections data from a website. It has JavaScript and around a 1000 individual pages that all have the same format and variables, stored in a table. I'm new to scraping but have read a bit today and it seems like Python is my best bet. I was wondering if this is the type of thing I should use a full crawler on, like Scrapy. The URLs for the pages i need all have this format:\n\nhttps://elections.amo.on.ca/web/en/municipal/XXXXX \n\nWhere XXXXX seems to be an ID code for each page.  I don't know which 5 digit codes actually correspond to the pages I need but its certainly not all 5 digit combinations because the number of possible pages is much smaller than 99,999.\n\nShould I just get started with learning Scrapy in Python, or do you think there is a better tool for this task?", "link": "https://www.reddit.com/r/datascience/comments/ndobxv/need_advice_on_the_best_web_scraping_toolapproach/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "need advice on the best web scraping -----> tool !!! /approach for this job /!/ hey everyone, i first posted this on r/analytics but realized it doesn't fit very well there. i need to scrape some elections data from a website. it has javascript and around a 1000 individual pages that all have the same format and variables, stored in a table. i'm new to scraping but have read a bit today and it seems like python is my best bet. i was wondering if this is the type of thing i should use a full crawler on, like scrapy. the urls for the pages i need all have this format:\n\nhttps://elections.amo.on.ca/web/en/municipal/xxxxx \n\nwhere xxxxx seems to be an id code for each page.  i don't know which 5 digit codes actually correspond to the pages i need but its certainly not all 5 digit combinations because the number of possible pages is much smaller than 99,999.\n\nshould i just get started with learning scrapy in python, or do you think there is a better tool for this task?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 15, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/ndobxv/need_advice_on_the_best_web_scraping_toolapproach/',)", "identifyer": 5587547, "year": "2021"}, {"autor": "monkeyunited", "date": 1635533508000, "content": "Confluence as self-servicing analytics tool? /!/ I'm trying to understand if it's possible to use Confluence as self-servicing analytics tool.\n\nMy goal is to have a knowledge database where you can search for past analytics projects or routine processes. Within the page, there will be either github link or Excel spreadsheet attached to the page with instructions on filters and parameters so anyone with the right access can reproduce the process.\n\nI chose Confluence because I also plan to set up a ticketing system using Jira. The Confluence page will have past Jira tickets attached to provide context.\n\nFirst of all, do you think this is feasible? Will this scale well into the future or becomes a swamp to maintain? What are some of the potential downfalls and how could those downfalls be avoided if possible?\n\nFor context, I'm the only analyst in a startup. I want to set this up so 1. we have documentation 2. business side can run simple things on their own, 3. new analyst can onboard faster, and 4. eliminating myself as the single point of failure.", "link": "https://www.reddit.com/r/datascience/comments/qijnta/confluence_as_selfservicing_analytics_tool/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "confluence as self-servicing analytics -----> tool !!! ? /!/ i'm trying to understand if it's possible to use confluence as self-servicing analytics tool.\n\nmy goal is to have a knowledge database where you can search for past analytics projects or routine processes. within the page, there will be either github link or excel spreadsheet attached to the page with instructions on filters and parameters so anyone with the right access can reproduce the process.\n\ni chose confluence because i also plan to set up a ticketing system using jira. the confluence page will have past jira tickets attached to provide context.\n\nfirst of all, do you think this is feasible? will this scale well into the future or becomes a swamp to maintain? what are some of the potential downfalls and how could those downfalls be avoided if possible?\n\nfor context, i'm the only analyst in a startup. i want to set this up so 1. we have documentation 2. business side can run simple things on their own, 3. new analyst can onboard faster, and 4. eliminating myself as the single point of failure.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/qijnta/confluence_as_selfservicing_analytics_tool/',)", "identifyer": 5587706, "year": "2021"}, {"autor": "Headybouffant", "date": 1635467130000, "content": "What\u2019s the best ERD tool/system/template that can be just as easy to use for stakeholders as it is fully detailed for Engineers?", "link": "https://www.reddit.com/r/datascience/comments/qi0fhe/whats_the_best_erd_toolsystemtemplate_that_can_be/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what\u2019s the best erd -----> tool !!! /system/template that can be just as easy to use for stakeholders as it is fully detailed for engineers?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('/r/SQL/comments/qi01ai/whats_the_best_erd_toolsystemtemplate_that_can_be/',)", "identifyer": 5587734, "year": "2021"}, {"autor": "quite--average", "date": 1624994035000, "content": "For someone who's trying to break into data science, what percentage of their recent experience should be ML related on their resume? /!/ Hello!\n\nI have a Masters with Stats as major. Currently, I'm working as a BI developer in a F500 company. I have just finished my first year at this job after Masters. Majority of my job involves making dashboards in a BI tool. Lately, I have been successful in integrating things like Python or R in the dashboard making process. I use those to clean, manipulate the data before the dashboard uses it.\n\nOn top of that I have an ongoing Machine learning project that I'm working on using R. I have made significant amount of process in this project and gotten a good grasp on R.\n\nMy question, will this experience be enough to get interviews for junior data scientist role or for now should I apply for more data analyst roles?\n\nThanks!", "link": "https://www.reddit.com/r/datascience/comments/oaf42i/for_someone_whos_trying_to_break_into_data/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "for someone who's trying to break into data science, what percentage of their recent experience should be ml related on their resume? /!/ hello!\n\ni have a masters with stats as major. currently, i'm working as a bi developer in a f500 company. i have just finished my first year at this job after masters. majority of my job involves making dashboards in a bi -----> tool !!! . lately, i have been successful in integrating things like python or r in the dashboard making process. i use those to clean, manipulate the data before the dashboard uses it.\n\non top of that i have an ongoing machine learning project that i'm working on using r. i have made significant amount of process in this project and gotten a good grasp on r.\n\nmy question, will this experience be enough to get interviews for junior data scientist role or for now should i apply for more data analyst roles?\n\nthanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 8, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/oaf42i/for_someone_whos_trying_to_break_into_data/',)", "identifyer": 5587760, "year": "2021"}, {"autor": "KyotaLangard", "date": 1624988432000, "content": "I built a free online tool that lets you visually organize your SQL queries /!/  Link: [https://visualsql.net](https://visualsql.net/)\n\nWhen I was working at my previous job, some queries were getting pretty big and ugly, and I started thinking about some kind of tool that could make building my SQL queries cleaner and more convenient. I thought that there must be a better way to organize queries other than in a wall of text.\n\nSomeone pointed to me that there were already some software like Alteryx and Knime that are visual, but for my objective (to just visually organize SQL queries) these were overkill and overbudget. Also, their dark theme is either non-existent or ugly as hell.\n\nThere are also other tools like Chartio's Visual SQL and Active Query Builder that are kinda \"visual\", but not the kind of visual that I wanted. So I decided to create my own.\n\nSupports all modern browsers. Also supports tablets and IE11. Dark mode by default, light mode isn't even a consideration.\n\nDon't know if it will be useful to you guys, but this would certainly be a game changer for my past self. I'll keep refining this project over time so feedback is much appreciated!", "link": "https://www.reddit.com/r/datascience/comments/oad8gj/i_built_a_free_online_tool_that_lets_you_visually/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "i built a free online -----> tool !!!  that lets you visually organize your sql queries /!/  link: [https://visualsql.net](https://visualsql.net/)\n\nwhen i was working at my previous job, some queries were getting pretty big and ugly, and i started thinking about some kind of tool that could make building my sql queries cleaner and more convenient. i thought that there must be a better way to organize queries other than in a wall of text.\n\nsomeone pointed to me that there were already some software like alteryx and knime that are visual, but for my objective (to just visually organize sql queries) these were overkill and overbudget. also, their dark theme is either non-existent or ugly as hell.\n\nthere are also other tools like chartio's visual sql and active query builder that are kinda \"visual\", but not the kind of visual that i wanted. so i decided to create my own.\n\nsupports all modern browsers. also supports tablets and ie11. dark mode by default, light mode isn't even a consideration.\n\ndon't know if it will be useful to you guys, but this would certainly be a game changer for my past self. i'll keep refining this project over time so feedback is much appreciated!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/oad8gj/i_built_a_free_online_tool_that_lets_you_visually/',)", "identifyer": 5587768, "year": "2021"}, {"autor": "KyotaLangard", "date": 1624903548000, "content": "[Self-Promotion] I developed visualsql.net, a free online tool for visual SQL query building /!/  When I was working at my previous job, some queries were getting pretty big and ugly, and I started thinking about some kind of tool that could make building my SQL queries cleaner and more convenient. I thought that there must be a better way to organize queries other than in a wall of text.\n\nSomeone pointed to me that there were already some software like Alteryx and Knime that are visual, but for my objective (to just visually organize SQL queries) these were overkill and overbudget. Also, their dark theme is either non-existent or ugly as hell.\n\nThere are also other tools like Chartio's Visual SQL and Active Query Builder that are kinda \"visual\", but not the kind of visual that I wanted. So I decided to create my own.\n\n&amp;#x200B;\n\nLink: [https://visualsql.net](https://visualsql.net/)\n\n&amp;#x200B;\n\nSupports all modern browsers. Also supports tablets and IE11.\n\nDark mode by default. Light mode isn't even a consideration.\n\n&amp;#x200B;\n\nDon't know if it will be useful to you guys, but this would certainly be a game changer for my past self.\n\nI'll keep refining this project over time so feedback is much appreciated!", "link": "https://www.reddit.com/r/datascience/comments/o9ptot/selfpromotion_i_developed_visualsqlnet_a_free/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[self-promotion] i developed visualsql.net, a free online -----> tool !!!  for visual sql query building /!/  when i was working at my previous job, some queries were getting pretty big and ugly, and i started thinking about some kind of -----> tool !!!  that could make building my sql queries cleaner and more convenient. i thought that there must be a better way to organize queries other than in a wall of text.\n\nsomeone pointed to me that there were already some software like alteryx and knime that are visual, but for my objective (to just visually organize sql queries) these were overkill and overbudget. also, their dark theme is either non-existent or ugly as hell.\n\nthere are also other tools like chartio's visual sql and active query builder that are kinda \"visual\", but not the kind of visual that i wanted. so i decided to create my own.\n\n&amp;#x200b;\n\nlink: [https://visualsql.net](https://visualsql.net/)\n\n&amp;#x200b;\n\nsupports all modern browsers. also supports tablets and ie11.\n\ndark mode by default. light mode isn't even a consideration.\n\n&amp;#x200b;\n\ndon't know if it will be useful to you guys, but this would certainly be a game changer for my past self.\n\ni'll keep refining this project over time so feedback is much appreciated!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/o9ptot/selfpromotion_i_developed_visualsqlnet_a_free/',)", "identifyer": 5587814, "year": "2021"}, {"autor": "tcbjj", "date": 1623708165000, "content": "Suggestions for BI visualization tool that supports live feed connection from BQ? /!/ I was looking into tableau online which seems fantastic, but is a little expensive for my needs.  Ive been using google data studio, but its fastest data freshness is 15min, which isnt good enough.\n\nAnyone have another suggestions?\n\n&amp;#x200B;\n\nThanks", "link": "https://www.reddit.com/r/datascience/comments/nzyady/suggestions_for_bi_visualization_tool_that/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "suggestions for bi visualization -----> tool !!!  that supports live feed connection from bq? /!/ i was looking into tableau online which seems fantastic, but is a little expensive for my needs.  ive been using google data studio, but its fastest data freshness is 15min, which isnt good enough.\n\nanyone have another suggestions?\n\n&amp;#x200b;\n\nthanks", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 16, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nzyady/suggestions_for_bi_visualization_tool_that/',)", "identifyer": 5587904, "year": "2021"}, {"autor": "Xxyjoel", "date": 1617050707000, "content": "ML + Infrastructure /!/ Hey All,\n\nI have been in the data science and machine learning space for the majority of my career, but have more recently spent time in meddling around with infrastructure.  Could be the naivety, though the complexity (and inefficiency) of bundled cloud tools bothered me, so I built a tool to help manage costs.\n\nIt still requires some policy finagling, so it's not self service yet, however, I'd love y'alls candid feedback on the tool - [BlueArch.io](https://bluearch.io/)\n\nApologies  if this is against the sub's rules... sharing your work can be scary but I'm pretty excited about project.", "link": "https://www.reddit.com/r/datascience/comments/mfz32k/ml_infrastructure/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "ml + infrastructure /!/ hey all,\n\ni have been in the data science and machine learning space for the majority of my career, but have more recently spent time in meddling around with infrastructure.  could be the naivety, though the complexity (and inefficiency) of bundled cloud tools bothered me, so i built a -----> tool !!!  to help manage costs.\n\nit still requires some policy finagling, so it's not self service yet, however, i'd love y'alls candid feedback on the tool - [bluearch.io](https://bluearch.io/)\n\napologies  if this is against the sub's rules... sharing your work can be scary but i'm pretty excited about project.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/mfz32k/ml_infrastructure/',)", "identifyer": 5587977, "year": "2021"}, {"autor": "gare_it", "date": 1622389162000, "content": "SQL \"Take Home\" Exercises /!/ I'm interviewing for a Product Analyst role that I'm a little lukewarm on. They gave me some data in a SQLite db and csv format, and asked me to explore it for some insights, and visualize my findings using SQL. If I didn't use SQL they said they'd get back to me later with a different SQL exercise. They told me this should take around 3 hours.\n\nIs there a common tool that I can load SQLite db files into, then visualize solely using SQL? Is this amount of time spent on a take home, or this type of exercise common? \n\nI've done some other take homes that were pure SQL exercises, and that came with either just a list of questions, or some data already in an env where I could just write my statements and see the results. \n\nI'm happy to load these up and explore/visualize with Python (and some embedded SQL statements within my Python), but if that's not the focus I don't know why I would.", "link": "https://www.reddit.com/r/datascience/comments/nodr10/sql_take_home_exercises/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "sql \"take home\" exercises /!/ i'm interviewing for a product analyst role that i'm a little lukewarm on. they gave me some data in a sqlite db and csv format, and asked me to explore it for some insights, and visualize my findings using sql. if i didn't use sql they said they'd get back to me later with a different sql exercise. they told me this should take around 3 hours.\n\nis there a common -----> tool !!!  that i can load sqlite db files into, then visualize solely using sql? is this amount of time spent on a take home, or this type of exercise common? \n\ni've done some other take homes that were pure sql exercises, and that came with either just a list of questions, or some data already in an env where i could just write my statements and see the results. \n\ni'm happy to load these up and explore/visualize with python (and some embedded sql statements within my python), but if that's not the focus i don't know why i would.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 9, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nodr10/sql_take_home_exercises/',)", "identifyer": 5588239, "year": "2021"}, {"autor": "adgezaza87", "date": 1622289044000, "content": "What is your current process like to source, clean/prepare, and collaborate with datasets? /!/ As part of a bigger project, I'm looking to put some effort into open sourcing a data sourcing and data collaboration cli tool.  The utility of the tool has been great limited enterprise and research settings as a sort of a shadow IT tool that replaces \"emailing CSVs\" around\".\n\nPlease note, I'm not a data scientists or engineer so I'm looking to understand this use case further.  Thanks.", "link": "https://www.reddit.com/r/datascience/comments/nnlunt/what_is_your_current_process_like_to_source/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what is your current process like to source, clean/prepare, and collaborate with datasets? /!/ as part of a bigger project, i'm looking to put some effort into open sourcing a data sourcing and data collaboration cli -----> tool !!! .  the utility of the tool has been great limited enterprise and research settings as a sort of a shadow it tool that replaces \"emailing csvs\" around\".\n\nplease note, i'm not a data scientists or engineer so i'm looking to understand this use case further.  thanks.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 14, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nnlunt/what_is_your_current_process_like_to_source/',)", "identifyer": 5588275, "year": "2021"}, {"autor": "purplebootyfox", "date": 1626368913000, "content": "Any recommendations for Kaggle notebooks to learn from? (specifically around scikit-learn pipelines and feature engineering) /!/ Being able to review and work through a kaggle notebooks has been really helpful for me as a learning tool. Right now I am trying to learn more applied or practical knowledge about featuring engineering and best practices while working with sklearn. Does anyone know of any gold standard notebooks for doing feature engineering and building pipelines?\n\nOne thing I am having trouble with is doing custom feature engineering and then including that step in a gridsearch. For example, take categorical variable X and keep the 10 most frequent categories while assigning 'Other' to the remaining categories before one hot encoding. I'd like to be able to send this step to a grid search but haven't been able to figure out how to get custom logic like this in the pipeline format. \n\nI have other questions but in general I'd just like to see a more applied/practical example of working with sklearn rather than toy examples/tutorials.", "link": "https://www.reddit.com/r/datascience/comments/okx6nb/any_recommendations_for_kaggle_notebooks_to_learn/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "any recommendations for kaggle notebooks to learn from? (specifically around scikit-learn pipelines and feature engineering) /!/ being able to review and work through a kaggle notebooks has been really helpful for me as a learning -----> tool !!! . right now i am trying to learn more applied or practical knowledge about featuring engineering and best practices while working with sklearn. does anyone know of any gold standard notebooks for doing feature engineering and building pipelines?\n\none thing i am having trouble with is doing custom feature engineering and then including that step in a gridsearch. for example, take categorical variable x and keep the 10 most frequent categories while assigning 'other' to the remaining categories before one hot encoding. i'd like to be able to send this step to a grid search but haven't been able to figure out how to get custom logic like this in the pipeline format. \n\ni have other questions but in general i'd just like to see a more applied/practical example of working with sklearn rather than toy examples/tutorials.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/okx6nb/any_recommendations_for_kaggle_notebooks_to_learn/',)", "identifyer": 5588466, "year": "2021"}, {"autor": "chr15m", "date": 1626350189000, "content": "Have you analyzed Twitter data in your work? /!/ Hi all, hopefully this is the right place to ask about this. If not, please let me know and I'll remove this post.\n\nI am working on a tool to make it easy to download Twitter data in CSV &amp; JSON format without fiddling around with API keys etc. I would like to know from people who actually do this as part of their job, what kinds of data should I make sure I support?\n\nSo far I have implemented search, which lets you download the results of a Twitter search as a dataset. I was also going to implement user timeline download, thread download, likes by user, retweets by user, people search, follower/following data, and list-member data. Is there anything I have missed that you have needed in the past? Are any of these particularly higher priority than other data sets?\n\nThank you so much for reading and have a nice day.", "link": "https://www.reddit.com/r/datascience/comments/okrbhy/have_you_analyzed_twitter_data_in_your_work/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "have you analyzed twitter data in your work? /!/ hi all, hopefully this is the right place to ask about this. if not, please let me know and i'll remove this post.\n\ni am working on a -----> tool !!!  to make it easy to download twitter data in csv &amp; json format without fiddling around with api keys etc. i would like to know from people who actually do this as part of their job, what kinds of data should i make sure i support?\n\nso far i have implemented search, which lets you download the results of a twitter search as a dataset. i was also going to implement user timeline download, thread download, likes by user, retweets by user, people search, follower/following data, and list-member data. is there anything i have missed that you have needed in the past? are any of these particularly higher priority than other data sets?\n\nthank you so much for reading and have a nice day.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 8, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/okrbhy/have_you_analyzed_twitter_data_in_your_work/',)", "identifyer": 5588479, "year": "2021"}, {"autor": "vietlinh12hoa", "date": 1626290451000, "content": "Working with business people /!/ I would like to take this opportunity to outline some of my experience working with business side.\n\n* They might be not aware system, so a simple business request can take long to execute\n* They prefer percentage in number \n* They don't like too much digits number (i.e. 10b instead of 10.123.456.789)\n* The like story telling\n* They prefer fancy dashboards, button bar, dropdown, point click, dynamic update, with multiple KPIs\n* They would think their questions for DS are clear (from DS viewpoint, they can be ambiguous and lots of edge cases) \n* Some people prefer the desired outputs rather than factual numbers\n* They believe in DS intelligence, and hardly makes mistakes.\n* They believe the efficiency of multitasking = number\\_task \\* the efficiency of each single tasking \n* They seems hardly seeing value of a library, preproc tool, or documentation\n* Many will prioritize more customer satisfactory than employee satisfactory\n\nAnyone here would like share some other experience?", "link": "https://www.reddit.com/r/datascience/comments/okbizr/working_with_business_people/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "working with business people /!/ i would like to take this opportunity to outline some of my experience working with business side.\n\n* they might be not aware system, so a simple business request can take long to execute\n* they prefer percentage in number \n* they don't like too much digits number (i.e. 10b instead of 10.123.456.789)\n* the like story telling\n* they prefer fancy dashboards, button bar, dropdown, point click, dynamic update, with multiple kpis\n* they would think their questions for ds are clear (from ds viewpoint, they can be ambiguous and lots of edge cases) \n* some people prefer the desired outputs rather than factual numbers\n* they believe in ds intelligence, and hardly makes mistakes.\n* they believe the efficiency of multitasking = number\\_task \\* the efficiency of each single tasking \n* they seems hardly seeing value of a library, preproc -----> tool !!! , or documentation\n* many will prioritize more customer satisfactory than employee satisfactory\n\nanyone here would like share some other experience?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/okbizr/working_with_business_people/',)", "identifyer": 5588495, "year": "2021"}, {"autor": "balackdynamite", "date": 1627639548000, "content": "Looking for the right visualization tool /!/ I have a time series database (AWS Timestream) and I'm looking for a good visualization tool.\n\n\n1) It needs to be able to be embedded (plugin) to my web application\n\n\n2) I'd like to allow users to dynamically change and create their own dashboards using their data from the DB\n\n\nI've looked at a few so far but it's hard to find one that satisfies both requirements.\n\nAnyone have any recommendations?", "link": "https://www.reddit.com/r/datascience/comments/ouhrv9/looking_for_the_right_visualization_tool/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "looking for the right visualization -----> tool !!!  /!/ i have a time series database (aws timestream) and i'm looking for a good visualization tool.\n\n\n1) it needs to be able to be embedded (plugin) to my web application\n\n\n2) i'd like to allow users to dynamically change and create their own dashboards using their data from the db\n\n\ni've looked at a few so far but it's hard to find one that satisfies both requirements.\n\nanyone have any recommendations?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/ouhrv9/looking_for_the_right_visualization_tool/',)", "identifyer": 5588541, "year": "2021"}, {"autor": "morningshower", "date": 1627585912000, "content": "Does anyone know which tool was used to create this US population migration visualization? /!/ Hi! I'm planning to create a visualization similar to the one below. I was curious what tool might have been used to create it. Is it possible to create such visualization with R or Tableau? Or will I need to use something like PhotoShop and manually create the image?\n\n[https://www.census.gov/dataviz/visualizations/051/](https://www.census.gov/dataviz/visualizations/051/)\n\nhttps://preview.redd.it/nk0vkiyma7e71.png?width=880&amp;format=png&amp;auto=webp&amp;s=b8d4174f1bfc161f367d6ff80009a7e5041d7c92", "link": "https://www.reddit.com/r/datascience/comments/ou43x2/does_anyone_know_which_tool_was_used_to_create/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "does anyone know which -----> tool !!!  was used to create this us population migration visualization? /!/ hi! i'm planning to create a visualization similar to the one below. i was curious what tool might have been used to create it. is it possible to create such visualization with r or tableau? or will i need to use something like photoshop and manually create the image?\n\n[https://www.census.gov/dataviz/visualizations/051/](https://www.census.gov/dataviz/visualizations/051/)\n\nhttps://preview.redd.it/nk0vkiyma7e71.png?width=880&amp;format=png&amp;auto=webp&amp;s=b8d4174f1bfc161f367d6ff80009a7e5041d7c92", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/ou43x2/does_anyone_know_which_tool_was_used_to_create/',)", "identifyer": 5588560, "year": "2021"}, {"autor": "awaymsg", "date": 1627512991000, "content": "Educational background for data scientists of reddit? /!/ I'm yet another young post-bac looking to eventually transition into data science, and I was exploring m.s. programs when I found [this](https://analytics.ncsu.edu/?page_id=4184) tool that shows different degrees from around the USA broken into data science, data analytics, and business analytics.   \n\n\nIt got me thinking a lot on the differences between the degree \"types.\" I'm sure the milage varies program to program, and there's many paths up the mountain so to speak, but I'm curious how many people here studied pure data science vs learning data science through a business perspective. \n\nSorry if this is super lame, I'm just really curious.\n\n[View Poll](https://www.reddit.com/poll/otl5p7)", "link": "https://www.reddit.com/r/datascience/comments/otl5p7/educational_background_for_data_scientists_of/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "educational background for data scientists of reddit? /!/ i'm yet another young post-bac looking to eventually transition into data science, and i was exploring m.s. programs when i found [this](https://analytics.ncsu.edu/?page_id=4184) -----> tool !!!  that shows different degrees from around the usa broken into data science, data analytics, and business analytics.   \n\n\nit got me thinking a lot on the differences between the degree \"types.\" i'm sure the milage varies program to program, and there's many paths up the mountain so to speak, but i'm curious how many people here studied pure data science vs learning data science through a business perspective. \n\nsorry if this is super lame, i'm just really curious.\n\n[view poll](https://www.reddit.com/poll/otl5p7)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 10, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/otl5p7/educational_background_for_data_scientists_of/',)", "identifyer": 5588593, "year": "2021"}, {"autor": "VictorAVB", "date": 1627475821000, "content": "How to Create a Price Comparison Tool With Python BeautifulSoup", "link": "https://www.reddit.com/r/datascience/comments/ot9132/how_to_create_a_price_comparison_tool_with_python/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "how to create a price comparison -----> tool !!!  with python beautifulsoup", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('link',)", "medialink": "('https://webautomation.io/blog/how-to-create-price-comparison-tool-with-beautiful-soup/',)", "identifyer": 5588611, "year": "2021"}, {"autor": "datahan", "date": 1627462815000, "content": "What are some of your epiphanies when learning Python, Numpy, Pandas, Sklearn, Tensorflow, Pytorch? /!/ Here I came up with a list of 7 for python, numpy, pandas: [https://community.dataquest.io/t/reflections-from-teaching-a-data-science-bootcamp/555220](https://community.dataquest.io/t/reflections-from-teaching-a-data-science-bootcamp/555220), but still hoping to collect more ideas.   \n1. List of numbers vs list of objects  \n2. Iterable (list, tuple, set, dict, pandas)  \n3. Functions can be passed around  \n4. Subclassing to tweak others\u2019 code  \n5. Dunders and OOP  \n6. axis = 0,1,None  \n7. Vectorization and Broadcasting  \nI feel it's hard to come up with such things because once someone gets familiar with a tool, it becomes second nature rather than a big idea. They are also things that are hard for short-form tutorials to talk about because it requires bridging gaps with out-of-scope concepts. However, such thinking scaffolding are critical to someone new to a tool.", "link": "https://www.reddit.com/r/datascience/comments/ot64iz/what_are_some_of_your_epiphanies_when_learning/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what are some of your epiphanies when learning python, numpy, pandas, sklearn, tensorflow, pytorch? /!/ here i came up with a list of 7 for python, numpy, pandas: [https://community.dataquest.io/t/reflections-from-teaching-a-data-science-bootcamp/555220](https://community.dataquest.io/t/reflections-from-teaching-a-data-science-bootcamp/555220), but still hoping to collect more ideas.   \n1. list of numbers vs list of objects  \n2. iterable (list, tuple, set, dict, pandas)  \n3. functions can be passed around  \n4. subclassing to tweak others\u2019 code  \n5. dunders and oop  \n6. axis = 0,1,none  \n7. vectorization and broadcasting  \ni feel it's hard to come up with such things because once someone gets familiar with a -----> tool !!! , it becomes second nature rather than a big idea. they are also things that are hard for short-form tutorials to talk about because it requires bridging gaps with out-of-scope concepts. however, such thinking scaffolding are critical to someone new to a tool.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/ot64iz/what_are_some_of_your_epiphanies_when_learning/',)", "identifyer": 5588621, "year": "2021"}, {"autor": "sonalg", "date": 1631696833000, "content": "Open source Zingg: Entity Resolution for unified customer views, deduplication, AML and KYC /!/   \nSuper excited to tell that we have just open sourced Zingg - a Spark based entity resolution tool to rapidly remove duplicates, build households, enrich and unify customer and supplier data. I hope you will check us out at [https://github.com/zinggAI/zingg](https://github.com/zinggAI/zingg) \n\nLooking forward to any feedback or comments. S\n\nSonal", "link": "https://www.reddit.com/r/datascience/comments/pomhvi/open_source_zingg_entity_resolution_for_unified/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "open source zingg: entity resolution for unified customer views, deduplication, aml and kyc /!/   \nsuper excited to tell that we have just open sourced zingg - a spark based entity resolution -----> tool !!!  to rapidly remove duplicates, build households, enrich and unify customer and supplier data. i hope you will check us out at [https://github.com/zinggai/zingg](https://github.com/zinggai/zingg) \n\nlooking forward to any feedback or comments. s\n\nsonal", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pomhvi/open_source_zingg_entity_resolution_for_unified/',)", "identifyer": 5588638, "year": "2021"}, {"autor": "trapspeed3000", "date": 1631647721000, "content": "Does anyone else feel Python is immensely more difficult than R? /!/ So I've been doing this for about 7 years now + a couple years of screwing around in grad school. At the same time, somehow I've just been able to stay primarily in R all these years. It's suboptimal, but I've never made an effort to transition to Python. I'm also bad in terms of best practices and staying on top of new developments. So that should be kept in mind.\n\nCaveats out of the way... am I the only one who finds Python massively more difficult to use? I'm the only member of the DS team at my company at the moment and we just had someone leave who was primarily in Python. Some of his code broke and now I'm having to troubleshoot it. I can't make heads or tails. With my R code things are basically sequential and you can tell what's happening by searching object names and following changes to those objects. Whenever I look at my colleagues' python code it rarely resembles that. And it's cumbersome to run pieces of code and see where it's breaking. In R I can easily run this piece, then the next piece, then look inside constituent functions.\n\nI've also never seen anything in my colleagues' code that resembles tidyverse. I'm ridiculously more efficient in data munging than I was in base R. Most of my data comes from the wild and cleaning data seems like a nightmare in Python. It could just be a skillset thing, but my when my Python colleagues would be responsible for cleaning data I would regularly find some troubling errors. I can't help but think it may have to do with the tool they're using.\n\nMy other gripe is environments. My old boss had some great projects, but the shit rarely runs for me. I spend a huge amount of time trying to set up the appropriate environments.\n\nA more constructive question is how can I get my hands around this? Any time I sit down to learn Python I spend a lot of time on minor concepts that I would likely pick up anyway. After a few weeks I'm so bored that when other things come up I just drop it. The way I learned R involved adapting other people's code and understanding how it works. I'd pick apart something advanced and figure out simpler concepts as a part of that process. I have a ton of Python code available to me that's relevant to my job but I can't get it to run at all, much less truly understand it and build my own version.\n\nUgh. Rant over, thanks for listening.", "link": "https://www.reddit.com/r/datascience/comments/po9qpo/does_anyone_else_feel_python_is_immensely_more/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "does anyone else feel python is immensely more difficult than r? /!/ so i've been doing this for about 7 years now + a couple years of screwing around in grad school. at the same time, somehow i've just been able to stay primarily in r all these years. it's suboptimal, but i've never made an effort to transition to python. i'm also bad in terms of best practices and staying on top of new developments. so that should be kept in mind.\n\ncaveats out of the way... am i the only one who finds python massively more difficult to use? i'm the only member of the ds team at my company at the moment and we just had someone leave who was primarily in python. some of his code broke and now i'm having to troubleshoot it. i can't make heads or tails. with my r code things are basically sequential and you can tell what's happening by searching object names and following changes to those objects. whenever i look at my colleagues' python code it rarely resembles that. and it's cumbersome to run pieces of code and see where it's breaking. in r i can easily run this piece, then the next piece, then look inside constituent functions.\n\ni've also never seen anything in my colleagues' code that resembles tidyverse. i'm ridiculously more efficient in data munging than i was in base r. most of my data comes from the wild and cleaning data seems like a nightmare in python. it could just be a skillset thing, but my when my python colleagues would be responsible for cleaning data i would regularly find some troubling errors. i can't help but think it may have to do with the -----> tool !!!  they're using.\n\nmy other gripe is environments. my old boss had some great projects, but the shit rarely runs for me. i spend a huge amount of time trying to set up the appropriate environments.\n\na more constructive question is how can i get my hands around this? any time i sit down to learn python i spend a lot of time on minor concepts that i would likely pick up anyway. after a few weeks i'm so bored that when other things come up i just drop it. the way i learned r involved adapting other people's code and understanding how it works. i'd pick apart something advanced and figure out simpler concepts as a part of that process. i have a ton of python code available to me that's relevant to my job but i can't get it to run at all, much less truly understand it and build my own version.\n\nugh. rant over, thanks for listening.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 261, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/po9qpo/does_anyone_else_feel_python_is_immensely_more/',)", "identifyer": 5588658, "year": "2021"}, {"autor": "pirate7777777", "date": 1631527005000, "content": "Data platform, data infrastructure, and data stack. Same concept but different perceptions? /!/ Yesterday I was discussing with a couple of friends (a data scientist and a data analyst) about terminology to better communicate with stakeholders, and although we all agreed that Data platform, Data Infrastructure, and Data Stack are often used interchangeably, these terms seem to lead to different perceptions:\n\n* To the data scientist: data stack and data infra are pretty much the same, while data platform seems more for ingestion and move data around (i.e. tools like Segment and Snowplow)\n* To the data analyst: data stack feels the most complete and updated term, data platform seems specific to tools and sounds old, data infra seems the most low-end and most specific to DevOps/DataOps.\n* To me (PM): data infra seems the most complete, data stack seems the more high-level, mostly tool specific, and data platform seems the most business-friendly term with the focus on tools.\n\nWhat are your thoughts?", "link": "https://www.reddit.com/r/datascience/comments/pncd84/data_platform_data_infrastructure_and_data_stack/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "data platform, data infrastructure, and data stack. same concept but different perceptions? /!/ yesterday i was discussing with a couple of friends (a data scientist and a data analyst) about terminology to better communicate with stakeholders, and although we all agreed that data platform, data infrastructure, and data stack are often used interchangeably, these terms seem to lead to different perceptions:\n\n* to the data scientist: data stack and data infra are pretty much the same, while data platform seems more for ingestion and move data around (i.e. tools like segment and snowplow)\n* to the data analyst: data stack feels the most complete and updated term, data platform seems specific to tools and sounds old, data infra seems the most low-end and most specific to devops/dataops.\n* to me (pm): data infra seems the most complete, data stack seems the more high-level, mostly -----> tool !!!  specific, and data platform seems the most business-friendly term with the focus on tools.\n\nwhat are your thoughts?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pncd84/data_platform_data_infrastructure_and_data_stack/',)", "identifyer": 5588721, "year": "2021"}, {"autor": "meta-pirate", "date": 1631525458000, "content": "Data platform, data infrastructure, and data stack. Same concept but different perceptions? /!/ Yesterday I was discussing with a couple of friends (a data scientist and a data analyst) about terminology to better communicate with stakeholders, and although we all agreed that Data platform, Data Infrastructure, and Data Stack are often used interchangeably, these terms seem to lead to different perceptions:\n\n* To the data scientist: data stack and data infra are pretty much the same, while data platform seems more for ingestion and move data around (i.e. tools like Segment and Snowplow)\n* To the data analyst: data stack feels the most complete and updated term, data platform seems specific to tools and sounds old, data infra seems the most low-end and most specific to DevOps/DataOps.\n* To me (PM): data infra seems the most complete, data stack seems the more high-level, mostly tool specific, and data platform seems the most business-friendly term with the focus on tools.\n\nWhat are your thoughts?", "link": "https://www.reddit.com/r/datascience/comments/pnc2ma/data_platform_data_infrastructure_and_data_stack/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "data platform, data infrastructure, and data stack. same concept but different perceptions? /!/ yesterday i was discussing with a couple of friends (a data scientist and a data analyst) about terminology to better communicate with stakeholders, and although we all agreed that data platform, data infrastructure, and data stack are often used interchangeably, these terms seem to lead to different perceptions:\n\n* to the data scientist: data stack and data infra are pretty much the same, while data platform seems more for ingestion and move data around (i.e. tools like segment and snowplow)\n* to the data analyst: data stack feels the most complete and updated term, data platform seems specific to tools and sounds old, data infra seems the most low-end and most specific to devops/dataops.\n* to me (pm): data infra seems the most complete, data stack seems the more high-level, mostly -----> tool !!!  specific, and data platform seems the most business-friendly term with the focus on tools.\n\nwhat are your thoughts?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pnc2ma/data_platform_data_infrastructure_and_data_stack/',)", "identifyer": 5588724, "year": "2021"}, {"autor": "the_growth_guy", "date": 1632901529000, "content": "How do you decide on the tools needed for a ML project? /!/ Hey folks, I am an amateur in the field of data science and since some days I have been overwhelmed by all the tools available.\n\nI understand that every tool has it's unique offering. But I want to understand what parameters do you'll evaluate a tool on. What information you'll consume to make the decision of choosing one tool over another.", "link": "https://www.reddit.com/r/datascience/comments/pxqj00/how_do_you_decide_on_the_tools_needed_for_a_ml/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "how do you decide on the tools needed for a ml project? /!/ hey folks, i am an amateur in the field of data science and since some days i have been overwhelmed by all the tools available.\n\ni understand that every -----> tool !!!  has it's unique offering. but i want to understand what parameters do you'll evaluate a tool on. what information you'll consume to make the decision of choosing one tool over another.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 10, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pxqj00/how_do_you_decide_on_the_tools_needed_for_a_ml/',)", "identifyer": 5588747, "year": "2021"}, {"autor": "datahard", "date": 1632857992000, "content": "What is Gaio? /!/ Hi guys\n\nDoes anyone already know about the release of the free data analysis tool?\n\nhelp me understand better?\n\nthe name is Gaionet\n\n[en.gaio.io](https://en.gaio.io)", "link": "https://www.reddit.com/r/datascience/comments/pxe34d/what_is_gaio/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what is gaio? /!/ hi guys\n\ndoes anyone already know about the release of the free data analysis -----> tool !!! ?\n\nhelp me understand better?\n\nthe name is gaionet\n\n[en.gaio.io](https://en.gaio.io)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pxe34d/what_is_gaio/',)", "identifyer": 5588769, "year": "2021"}, {"autor": "captnpepsi", "date": 1632846615000, "content": "Approaches to a Spend Regulation System /!/ I'm facing the following problem and work and thought I might see if someone here as a brilliant approach to it to solve me a headache!  (heads up: there is already an implementation in place for this method but i'm not sure if there is not a better way to approach it)\n\nCircumstance:\n\nA meta-search website exists with 100 shops that post their products.   A customer can scroll through products and make selections (click) at which points a shop is charged for that customer's traffic. \n\nEach shop defines ahead of time with the website how much they would like to spend in total during a specific calendar month.   \n\nIt is the goal of the website and of the shop to \n\n1. use the total amount that the shop would like to spend within a month\n2. ideally only reach the maximum that a shop is willing to spend on the very last day of the month.  This way the shop stays online for as long as possible...in contrast to using all of the shop's available funds in one day and seeing the customer go offline\n\n&amp;#x200B;\n\nYour only tool to control spend to each shop is to adjust the ranking for products. \n\nYou can assume for simplicity that all shops are on the same page and that there is only one page.", "link": "https://www.reddit.com/r/datascience/comments/pxa4vh/approaches_to_a_spend_regulation_system/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "approaches to a spend regulation system /!/ i'm facing the following problem and work and thought i might see if someone here as a brilliant approach to it to solve me a headache!  (heads up: there is already an implementation in place for this method but i'm not sure if there is not a better way to approach it)\n\ncircumstance:\n\na meta-search website exists with 100 shops that post their products.   a customer can scroll through products and make selections (click) at which points a shop is charged for that customer's traffic. \n\neach shop defines ahead of time with the website how much they would like to spend in total during a specific calendar month.   \n\nit is the goal of the website and of the shop to \n\n1. use the total amount that the shop would like to spend within a month\n2. ideally only reach the maximum that a shop is willing to spend on the very last day of the month.  this way the shop stays online for as long as possible...in contrast to using all of the shop's available funds in one day and seeing the customer go offline\n\n&amp;#x200b;\n\nyour only -----> tool !!!  to control spend to each shop is to adjust the ranking for products. \n\nyou can assume for simplicity that all shops are on the same page and that there is only one page.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pxa4vh/approaches_to_a_spend_regulation_system/',)", "identifyer": 5588771, "year": "2021"}, {"autor": "Eightstream", "date": 1632806199000, "content": "Opinions on ORMs? /!/ Recently I have been playing around with SQLAlchemy. I was wondering if anyone finds ORMs useful for data science work, and if so what particular advantages do you like?\n\nI like the idea of an object model for databases in principle, but in reality I am pretty comfortable with raw SQL and a lot of the touted advantages of ORMs (security, deployability, etc.) are not things that matter a whole lot to me when iterating on models. Seems a bit more like a tool for SWEs.", "link": "https://www.reddit.com/r/datascience/comments/pwzbgt/opinions_on_orms/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "opinions on orms? /!/ recently i have been playing around with sqlalchemy. i was wondering if anyone finds orms useful for data science work, and if so what particular advantages do you like?\n\ni like the idea of an object model for databases in principle, but in reality i am pretty comfortable with raw sql and a lot of the touted advantages of orms (security, deployability, etc.) are not things that matter a whole lot to me when iterating on models. seems a bit more like a -----> tool !!!  for swes.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pwzbgt/opinions_on_orms/',)", "identifyer": 5588800, "year": "2021"}, {"autor": "nancybotwinnn", "date": 1632777407000, "content": "Changing job title on resume? /!/ So I am a \u2018Data Analyst\u2019 at a tech company that creates enterprise ML software (competitor to alteryx). \n\nMost of my job though is not super typical of an analyst. I am integrating new data sources, creating/deploying/automating analytical tools for different teams, creating/deploying/automating ML models to serve business purposes, maintenance of our instance, and doing other types of advanced analytics or data engineering tasks. Most of my job is done in Python, sql, or the company\u2019s software. \n\nMy team is responsible for all the internal data needs of the company. \n\nFor instance we started using a new tool for HR for recruiting and hiring called workday. It was my responsibility to integrate the workday data into our own system with the API, map our old hiring data to workday, set up a process to sync the data to our databases, then create different analytical/reporting tools for HR &amp; our recruiters. Then the Financial Planning team asked me to use all of our hiring data and create a ML model to predict how long it will take to hire new roles so they can better plan salaries. I then took the model and set up an automated process where it retrains every month on the new hires data and predicts on new open positions everyday and sends them to Financial planning to forecast their budgets. \n\nEveryone on my team has a data analyst or specialist title (they do the same things though). \n\nWe have data scientists however they serve a different purpose. They are customer facing, so they provide DS consulting to companies who use our ML software. They don\u2019t serve any internal function.\n\nThis is my first job out of college. Would it be deceitful to call myself a data scientist working in analytics on a resume?", "link": "https://www.reddit.com/r/datascience/comments/pwqtoi/changing_job_title_on_resume/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "changing job title on resume? /!/ so i am a \u2018data analyst\u2019 at a tech company that creates enterprise ml software (competitor to alteryx). \n\nmost of my job though is not super typical of an analyst. i am integrating new data sources, creating/deploying/automating analytical tools for different teams, creating/deploying/automating ml models to serve business purposes, maintenance of our instance, and doing other types of advanced analytics or data engineering tasks. most of my job is done in python, sql, or the company\u2019s software. \n\nmy team is responsible for all the internal data needs of the company. \n\nfor instance we started using a new -----> tool !!!  for hr for recruiting and hiring called workday. it was my responsibility to integrate the workday data into our own system with the api, map our old hiring data to workday, set up a process to sync the data to our databases, then create different analytical/reporting tools for hr &amp; our recruiters. then the financial planning team asked me to use all of our hiring data and create a ml model to predict how long it will take to hire new roles so they can better plan salaries. i then took the model and set up an automated process where it retrains every month on the new hires data and predicts on new open positions everyday and sends them to financial planning to forecast their budgets. \n\neveryone on my team has a data analyst or specialist title (they do the same things though). \n\nwe have data scientists however they serve a different purpose. they are customer facing, so they provide ds consulting to companies who use our ml software. they don\u2019t serve any internal function.\n\nthis is my first job out of college. would it be deceitful to call myself a data scientist working in analytics on a resume?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pwqtoi/changing_job_title_on_resume/',)", "identifyer": 5588817, "year": "2021"}, {"autor": "Diehauser", "date": 1632757742000, "content": "Naive Econ to DS, put off by DS community /!/ Sorry for the long post, I wanted to get this off my chest.\n\nAfter finishing my bachelor's in Economics, I entered a relatively long and decently-paid internship in a well-respected company as an analyst (more of a generic title, really). \n\nMy primary goal was to help management analyze, optimize and automate certain costly periodical work processes as much as possible. To that effect, I worked extensively with the ones in charge in order to understand the process: painfully extracting data from their archaic database system software, cleaning the messy data (much of which was simply missing or comically wrong, I had a good laugh at certain points), dealing with the sheer dispersion of key data both inside and outside their own database (some critical points had to be shared and clarified between departments, often through e-mail), working around data silos and silo mentality (requests often outright ignored and acting like they were just doing favours, seriously f\\*\\*\\* you George). \n\nWhat should have been simple tasks took hours, and the more complex tasks took days if not weeks of work. Certain required data was so large that it took a long time to extract from the database (and then sometimes the software decided to call it quits) and completely crashed Excel (which was nearly the only tool everyone used), so it had to be processed in chunks. I could feel the exasperation of those who got saddled with this stupid role day in and day out, and they were a lot of times simply disregarded.\n\nNearing the end of the internship, I delivered a report identifying points of failure (unsurprisingly, the data gathering failed right at the source) and proposing solutions to mitigate and/or correct them, optimizations that could be done in the data cleaning and analysis process itself, and how they could proceed from there. Unfortunately, I could not continue at the company because of circumstance even though my boss thought I was sorely needed.\n\nIt was there that I begun to really understand the concept of data maturity so, despite everything, I quite enjoyed the internship and found it very worthwhile.  I then thought to myself that it might be wise to acquire more technical skills to help companies with data maturity and analysis (plus no matter how much I like Excel, it's simply not good enough sometimes), that it would be valuable no matter what type of business job I ended up in.\n\nSo I enrolled in a MSc for DS &amp; Advanced Analytics from one of the top schools in the country for the area (I'm in Europe), thinking it the best fit for what I was looking for. Talking to a few classmates and reading this subreddit, **apparently there's this whole hype about DS that I somehow managed to completely miss** and the field's a mess right now. I thought it was a job/discipline like all the rest, supported by the fact that when I've spoken to people about it they shrugged. However, hearing those \"sexiest job\" words from one my classmate's lips, everything suddenly clicked and I started wondering if I'd made a mistake.\n\nDon't get me wrong, I'm very much enjoying the MSc and it's been demanding. I've also begun working at a multinational company you might have heard of in a new small department with considerable leeway, one of my colleagues is finishing his MSc in DS from another school and he's been helping improve data literacy and maturity at the company. Lord knows there's a lot of work to do in that sense, the resistance to it is frankly hilarious.\n\nTL;DR - I didn't go into DS to become a DS, just like I didn't go into Econ to become an Economist. Those are special, prestigious roles in my mind reserved for PhDs. I only wanted to acquire the mindset and tools that I thought relevant for business and data-related work. However, I'm being put off and stressed by how weird this \"field\" is. I'm also aware that it seems to be splitting into many different roles, and that Data Engineering might be the closer to the core of what I've been doing so maybe I should try focusing on that too.", "link": "https://www.reddit.com/r/datascience/comments/pwjwvb/naive_econ_to_ds_put_off_by_ds_community/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "naive econ to ds, put off by ds community /!/ sorry for the long post, i wanted to get this off my chest.\n\nafter finishing my bachelor's in economics, i entered a relatively long and decently-paid internship in a well-respected company as an analyst (more of a generic title, really). \n\nmy primary goal was to help management analyze, optimize and automate certain costly periodical work processes as much as possible. to that effect, i worked extensively with the ones in charge in order to understand the process: painfully extracting data from their archaic database system software, cleaning the messy data (much of which was simply missing or comically wrong, i had a good laugh at certain points), dealing with the sheer dispersion of key data both inside and outside their own database (some critical points had to be shared and clarified between departments, often through e-mail), working around data silos and silo mentality (requests often outright ignored and acting like they were just doing favours, seriously f\\*\\*\\* you george). \n\nwhat should have been simple tasks took hours, and the more complex tasks took days if not weeks of work. certain required data was so large that it took a long time to extract from the database (and then sometimes the software decided to call it quits) and completely crashed excel (which was nearly the only -----> tool !!!  everyone used), so it had to be processed in chunks. i could feel the exasperation of those who got saddled with this stupid role day in and day out, and they were a lot of times simply disregarded.\n\nnearing the end of the internship, i delivered a report identifying points of failure (unsurprisingly, the data gathering failed right at the source) and proposing solutions to mitigate and/or correct them, optimizations that could be done in the data cleaning and analysis process itself, and how they could proceed from there. unfortunately, i could not continue at the company because of circumstance even though my boss thought i was sorely needed.\n\nit was there that i begun to really understand the concept of data maturity so, despite everything, i quite enjoyed the internship and found it very worthwhile.  i then thought to myself that it might be wise to acquire more technical skills to help companies with data maturity and analysis (plus no matter how much i like excel, it's simply not good enough sometimes), that it would be valuable no matter what type of business job i ended up in.\n\nso i enrolled in a msc for ds &amp; advanced analytics from one of the top schools in the country for the area (i'm in europe), thinking it the best fit for what i was looking for. talking to a few classmates and reading this subreddit, **apparently there's this whole hype about ds that i somehow managed to completely miss** and the field's a mess right now. i thought it was a job/discipline like all the rest, supported by the fact that when i've spoken to people about it they shrugged. however, hearing those \"sexiest job\" words from one my classmate's lips, everything suddenly clicked and i started wondering if i'd made a mistake.\n\ndon't get me wrong, i'm very much enjoying the msc and it's been demanding. i've also begun working at a multinational company you might have heard of in a new small department with considerable leeway, one of my colleagues is finishing his msc in ds from another school and he's been helping improve data literacy and maturity at the company. lord knows there's a lot of work to do in that sense, the resistance to it is frankly hilarious.\n\ntl;dr - i didn't go into ds to become a ds, just like i didn't go into econ to become an economist. those are special, prestigious roles in my mind reserved for phds. i only wanted to acquire the mindset and tools that i thought relevant for business and data-related work. however, i'm being put off and stressed by how weird this \"field\" is. i'm also aware that it seems to be splitting into many different roles, and that data engineering might be the closer to the core of what i've been doing so maybe i should try focusing on that too.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 20, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pwjwvb/naive_econ_to_ds_put_off_by_ds_community/',)", "identifyer": 5588836, "year": "2021"}, {"autor": "the_emcee", "date": 1611079767000, "content": "What does the applicant profile/interview process look like for data analyst roles, even internships? /!/ I know that this sub kinda looks down on the actual day-to-day of that role, but I\u2019d probably really enjoy doing \u201csimple\u201d data analysis in Python/R or learning to use a business intelligence tool or SQL. even if i have a working knowledge of NLP and some NN architectures, I know I\u2019m not so proficient with the math to actually do any of that stuff on a job, and I recognize that the business need doesn\u2019t even require it all the time, and I recognize that I just wanna get started and don\u2019t know how to signal to companies that I\u2019d probably LOVE a \u201clesser\u201d role where i\u2019m not expected to be an ML/distributed systems/cloud provider/dev ops/dashboard expert bc that\u2019s more stressful given my skillset anyway", "link": "https://www.reddit.com/r/datascience/comments/l0pgvw/what_does_the_applicant_profileinterview_process/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what does the applicant profile/interview process look like for data analyst roles, even internships? /!/ i know that this sub kinda looks down on the actual day-to-day of that role, but i\u2019d probably really enjoy doing \u201csimple\u201d data analysis in python/r or learning to use a business intelligence -----> tool !!!  or sql. even if i have a working knowledge of nlp and some nn architectures, i know i\u2019m not so proficient with the math to actually do any of that stuff on a job, and i recognize that the business need doesn\u2019t even require it all the time, and i recognize that i just wanna get started and don\u2019t know how to signal to companies that i\u2019d probably love a \u201clesser\u201d role where i\u2019m not expected to be an ml/distributed systems/cloud provider/dev ops/dashboard expert bc that\u2019s more stressful given my skillset anyway", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/l0pgvw/what_does_the_applicant_profileinterview_process/',)", "identifyer": 5588874, "year": "2021"}, {"autor": "Local_Indication9669", "date": 1611068760000, "content": "Business analytics professor seeking career advice /!/ I\u2019ve been teaching (and developing) my university\u2019s business analytics curriculum for about three years now. I have a PhD (in marketing) and absolutely love teaching analytics, data science, and market research. However I\u2019m stuck at the adjunct level and unsure if I\u2019ll be able to move to full time. I was curious what types of jobs I might be qualified for in the real world. Maybe salary expectations?\n\nThings I teach: Linear optimization, non linear optimizations, discriminate analysis, simple regression, multiple regression, anova, Monte Carlo simulations, hypothesis testing, chi square tests for independence and normality. SPSS, R, Python (I\u2019m at least knowledgeable), Excel\u2019s various analytics tools. I also worked with a colleague at Harvard University (not where I work) to develop a machine learning algorithm in the RNC genetics field related to cancer research and was developing a standardized SEM reporting tool in R (editors weren\u2019t interested in standardizing SEM reports). \n\nThank you.", "link": "https://www.reddit.com/r/datascience/comments/l0lloc/business_analytics_professor_seeking_career_advice/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "business analytics professor seeking career advice /!/ i\u2019ve been teaching (and developing) my university\u2019s business analytics curriculum for about three years now. i have a phd (in marketing) and absolutely love teaching analytics, data science, and market research. however i\u2019m stuck at the adjunct level and unsure if i\u2019ll be able to move to full time. i was curious what types of jobs i might be qualified for in the real world. maybe salary expectations?\n\nthings i teach: linear optimization, non linear optimizations, discriminate analysis, simple regression, multiple regression, anova, monte carlo simulations, hypothesis testing, chi square tests for independence and normality. spss, r, python (i\u2019m at least knowledgeable), excel\u2019s various analytics tools. i also worked with a colleague at harvard university (not where i work) to develop a machine learning algorithm in the rnc genetics field related to cancer research and was developing a standardized sem reporting -----> tool !!!  in r (editors weren\u2019t interested in standardizing sem reports). \n\nthank you.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/l0lloc/business_analytics_professor_seeking_career_advice/',)", "identifyer": 5588882, "year": "2021"}, {"autor": "weber_stephen", "date": 1611001161000, "content": "Would you use a tool to help clean data if it generated Python code? /!/ I spend too much time cleaning data. Talking to others, it\u2019s a common problem. Expensive tools lock you in and don\u2019t give you code. I am exploring making a desktop app (inexpensive) to help. Would love your insight!\n\n[View Poll](https://www.reddit.com/poll/l02ttk)", "link": "https://www.reddit.com/r/datascience/comments/l02ttk/would_you_use_a_tool_to_help_clean_data_if_it/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "would you use a -----> tool !!!  to help clean data if it generated python code? /!/ i spend too much time cleaning data. talking to others, it\u2019s a common problem. expensive tools lock you in and don\u2019t give you code. i am exploring making a desktop app (inexpensive) to help. would love your insight!\n\n[view poll](https://www.reddit.com/poll/l02ttk)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/l02ttk/would_you_use_a_tool_to_help_clean_data_if_it/',)", "identifyer": 5588911, "year": "2021"}, {"autor": "Heil_Ashoka", "date": 1611637545000, "content": "Is Alteryx really worth it to learn with spending $400 to learn? /!/ I am currently working as BI Developer using Microstrategy tool. I am planning to step up in my career and as I have experience in BI , I chose Data Science because I somewhat became a fan of Data.  \n\nI learnt Data Science and Machine learning with python but I heard Alteryx is hot in the market now.\n\nIs Alteryx really hot in the market? Is it worth it to learn?\n\nPlease help with taking a decision on this.", "link": "https://www.reddit.com/r/datascience/comments/l56s7l/is_alteryx_really_worth_it_to_learn_with_spending/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "is alteryx really worth it to learn with spending $400 to learn? /!/ i am currently working as bi developer using microstrategy -----> tool !!! . i am planning to step up in my career and as i have experience in bi , i chose data science because i somewhat became a fan of data.  \n\ni learnt data science and machine learning with python but i heard alteryx is hot in the market now.\n\nis alteryx really hot in the market? is it worth it to learn?\n\nplease help with taking a decision on this.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/l56s7l/is_alteryx_really_worth_it_to_learn_with_spending/',)", "identifyer": 5588939, "year": "2021"}, {"autor": "[deleted]", "date": 1611637164000, "content": "AutoViz: A New Tool For Automated Visualization in Data Science /!/ [deleted]", "link": "https://www.reddit.com/r/datascience/comments/l56o32/autoviz_a_new_tool_for_automated_visualization_in/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "autoviz: a new -----> tool !!!  for automated visualization in data science /!/ [deleted]", "sortedWord": "None", "removed": "('deleted',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.marktechpost.com/2020/05/03/autoviz-a-new-tool-for-automated-visualization-in-data-science/',)", "identifyer": 5588940, "year": "2021"}, {"autor": "maxToTheJ", "date": 1611612493000, "content": "Bad habits migrated from tech / software dev culture into data science? /!/ Anyone have any favorite examples?\n\nMost frustrating is how many contradicting ideas are all over tech . software dev\n\nOver engineering happens a lot to make something \"Scalable\" on day 1\n\nThis is a meritocracy but we are all about \"culture fit\" with no clear definition what the latter is.\n\nTool worship", "link": "https://www.reddit.com/r/datascience/comments/l4ywu2/bad_habits_migrated_from_tech_software_dev/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "bad habits migrated from tech / software dev culture into data science? /!/ anyone have any favorite examples?\n\nmost frustrating is how many contradicting ideas are all over tech . software dev\n\nover engineering happens a lot to make something \"scalable\" on day 1\n\nthis is a meritocracy but we are all about \"culture fit\" with no clear definition what the latter is.\n\n-----> tool !!!  worship", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/l4ywu2/bad_habits_migrated_from_tech_software_dev/',)", "identifyer": 5588951, "year": "2021"}, {"autor": "skinfirst", "date": 1611585152000, "content": "Need help on interpretation /!/ Hello, I am having a hard time interpreting and explaining the results on a test campaign A/B test result. \n\nProblem:\nMembers call into customer Care for three reasons :\nR1- Make a new order\nR2- Track an order\nR3- questions on cost of products/services\n\nCombinations of reasons of call being tested( S depicts scenario) \nS1: R1 only\nS2: R2 only\nS3: R3 only\nS4: R1 and R2 only\nS5: R2 and r3 only\nS6: r1 and r3 only\nS7: r1, r2, r3 \n\nMy colleague designed an A/B test where the callers were sent an email after every call guiding them ti the online portal where they can do the corresponding action . \n\nAim of the test: To understand if guiding members to self-service tools will reduce subsequent calls to our Call Center\n\nTest group design;\nA/B test with \n90% test split into two groups\n45% received the email with webpage address only\n45% received email with webpage address and a \u2018how-to\u2019 video on the tool on the webpage\n10% control , do not receive any email\n\nThe test results yield the following:\n\nThere is no statistical reduction in calls for any scenario except Scenario 7 (S7)and increases for Scenario 6(S6). The results hold true when either test 1 is compared to control or test2 is compared to control.\n\nThis is baffling to me on how to explain that when members call for all three reasons and are sent an email for all three reasons they do not call in the future\n\nBut if members are called for reason R2 and R3 and are sent content for these two reasons they call more.\n\nAll comparisons where done at scenario level.\n\nCan someone help me explain. \nThanks in advance", "link": "https://www.reddit.com/r/datascience/comments/l4oyxd/need_help_on_interpretation/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "need help on interpretation /!/ hello, i am having a hard time interpreting and explaining the results on a test campaign a/b test result. \n\nproblem:\nmembers call into customer care for three reasons :\nr1- make a new order\nr2- track an order\nr3- questions on cost of products/services\n\ncombinations of reasons of call being tested( s depicts scenario) \ns1: r1 only\ns2: r2 only\ns3: r3 only\ns4: r1 and r2 only\ns5: r2 and r3 only\ns6: r1 and r3 only\ns7: r1, r2, r3 \n\nmy colleague designed an a/b test where the callers were sent an email after every call guiding them ti the online portal where they can do the corresponding action . \n\naim of the test: to understand if guiding members to self-service tools will reduce subsequent calls to our call center\n\ntest group design;\na/b test with \n90% test split into two groups\n45% received the email with webpage address only\n45% received email with webpage address and a \u2018how-to\u2019 video on the -----> tool !!!  on the webpage\n10% control , do not receive any email\n\nthe test results yield the following:\n\nthere is no statistical reduction in calls for any scenario except scenario 7 (s7)and increases for scenario 6(s6). the results hold true when either test 1 is compared to control or test2 is compared to control.\n\nthis is baffling to me on how to explain that when members call for all three reasons and are sent an email for all three reasons they do not call in the future\n\nbut if members are called for reason r2 and r3 and are sent content for these two reasons they call more.\n\nall comparisons where done at scenario level.\n\ncan someone help me explain. \nthanks in advance", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/l4oyxd/need_help_on_interpretation/',)", "identifyer": 5588966, "year": "2021"}, {"autor": "sk2977", "date": 1611488855000, "content": "Data as a Service /!/ Problem\n\nYou don\u2019t know how to make better decisions.\n\nSolution\n\nData as a Service (DaaS) bridges the gap between your predictions and reality. The lower your spread, the better your decisions.\n\nData is a means to an end, if consumed as a product. However, insights are closer to the \"job to be done\", if consumed as a service. In this article I'll cover aspects of both.\n\nIn competitive environments, the more data is shared, the less valuable it becomes. For example, shared lead lists for marketers have low close rates, publicly shared transactions are already priced in to the value of stocks. However, in cooperative environments, like public health, data sharing is extremely valuable. Consider the case of covid at the moment. Keep this distinction in mind for the rest of the article.\n\nMotivation\n\nSo what is powering DaaS? Very few companies had the ability to make use of raw data in the past. An order of magnitude more companies are buying data today than did 5 years ago. That's because a good engineer with a tool like Snowflake can be as productive as a great engineer 5 years ago.\n\nWe are seeing a commoditisation of data-warehousing with Snowflake, data lakes with AWS S3, and even Lakehouses with Databricks. The end user has ease of data ingestion with Fivetran, aggregation and monitoring with dbt.\n\nThese solutions are driving a rise to the market of alternative data, where companies are not only interested in internal (private) data sources but also external (public) data sources.\n\nOne example is hedge funds. Less than 3 years ago, just ten hedge funds were buying alternative data. Today there are 500-700 funds currently making investments to ingest large amounts of data.\n\nData as a Product (DaaP) vs Data as a Service (DaaS)\n\nIt is the difference between providing \"data\" and providing \"insights\"\n\nThere are two broad mandates (I'm being overly simplistic on purpose) that data teams are formed with:\n\nProvide data to a company\nProvide insights to a company\n\nIn a DaaS model, the data team partners with stakeholder groups to tackle specific problems using data. Team members have more functional experience, and are responsible for providing insights as opposed to raw data.\n\nChallenge\n\nThere are hundreds of companies that sell software and tools (traditional SaaS) to data scientists and machine learning teams. But there aren't many that specialise in selling data to teams that require it, in whatever shape or form (DaaP or DaaS).\n\nBeing a data-only business is less exciting because data is a supporting role.\n\nData companies support the true innovators.\n\nIt's like selling high quality butter to pastry chefs. The end consumer of the pastry may not even know there is butter in it, but the chef knows how important the ingredient was. Of course, the most important player in making a tasty pastry is the chef. A data company is just one of the important ingredients that goes into its creation.\n\nPrediction\n\nThe last 15 years have been about how companies get insights from their own data -- Tableau, PowerBI, Snowflake, Palantir have played into that trend.\n\nCompanies that are far along the curve of getting insights from their own data need external data if they want to continue getting value from their data teams. Because even the largest companies only knows about 0.1% of the world from their internal data alone.\n\nNonetheless, there are still very few data buyers today. Most companies want applications (answers), not data (collection of facts). The only reason to start a data business today is if you believe the number of data buyers will go up an order of magnitude in the next five to ten years.\n\nIt's a great time to build a pure-play DaaS business if you\u2019re bullish on this trend.\n\nSelling intelligent data to teams is starting to be a big business.", "link": "https://www.reddit.com/r/datascience/comments/l3y0yx/data_as_a_service/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "data as a service /!/ problem\n\nyou don\u2019t know how to make better decisions.\n\nsolution\n\ndata as a service (daas) bridges the gap between your predictions and reality. the lower your spread, the better your decisions.\n\ndata is a means to an end, if consumed as a product. however, insights are closer to the \"job to be done\", if consumed as a service. in this article i'll cover aspects of both.\n\nin competitive environments, the more data is shared, the less valuable it becomes. for example, shared lead lists for marketers have low close rates, publicly shared transactions are already priced in to the value of stocks. however, in cooperative environments, like public health, data sharing is extremely valuable. consider the case of covid at the moment. keep this distinction in mind for the rest of the article.\n\nmotivation\n\nso what is powering daas? very few companies had the ability to make use of raw data in the past. an order of magnitude more companies are buying data today than did 5 years ago. that's because a good engineer with a -----> tool !!!  like snowflake can be as productive as a great engineer 5 years ago.\n\nwe are seeing a commoditisation of data-warehousing with snowflake, data lakes with aws s3, and even lakehouses with databricks. the end user has ease of data ingestion with fivetran, aggregation and monitoring with dbt.\n\nthese solutions are driving a rise to the market of alternative data, where companies are not only interested in internal (private) data sources but also external (public) data sources.\n\none example is hedge funds. less than 3 years ago, just ten hedge funds were buying alternative data. today there are 500-700 funds currently making investments to ingest large amounts of data.\n\ndata as a product (daap) vs data as a service (daas)\n\nit is the difference between providing \"data\" and providing \"insights\"\n\nthere are two broad mandates (i'm being overly simplistic on purpose) that data teams are formed with:\n\nprovide data to a company\nprovide insights to a company\n\nin a daas model, the data team partners with stakeholder groups to tackle specific problems using data. team members have more functional experience, and are responsible for providing insights as opposed to raw data.\n\nchallenge\n\nthere are hundreds of companies that sell software and tools (traditional saas) to data scientists and machine learning teams. but there aren't many that specialise in selling data to teams that require it, in whatever shape or form (daap or daas).\n\nbeing a data-only business is less exciting because data is a supporting role.\n\ndata companies support the true innovators.\n\nit's like selling high quality butter to pastry chefs. the end consumer of the pastry may not even know there is butter in it, but the chef knows how important the ingredient was. of course, the most important player in making a tasty pastry is the chef. a data company is just one of the important ingredients that goes into its creation.\n\nprediction\n\nthe last 15 years have been about how companies get insights from their own data -- tableau, powerbi, snowflake, palantir have played into that trend.\n\ncompanies that are far along the curve of getting insights from their own data need external data if they want to continue getting value from their data teams. because even the largest companies only knows about 0.1% of the world from their internal data alone.\n\nnonetheless, there are still very few data buyers today. most companies want applications (answers), not data (collection of facts). the only reason to start a data business today is if you believe the number of data buyers will go up an order of magnitude in the next five to ten years.\n\nit's a great time to build a pure-play daas business if you\u2019re bullish on this trend.\n\nselling intelligent data to teams is starting to be a big business.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/l3y0yx/data_as_a_service/',)", "identifyer": 5589018, "year": "2021"}, {"autor": "Jfpalomeque", "date": 1611448225000, "content": "My first streamlite web scrapper! /!/ [https://share.streamlit.io/jfpalomeque/indeed\\_scrapper/main/scrapper.py](https://share.streamlit.io/jfpalomeque/indeed_scrapper/main/scrapper.py)\n\nThis is a scrapper for Indeed.co.uk, the job ads website. This project has three parts, an advance webscrapper, a little exploratory analysis of those ads and a visualization tool using streamlite. Code in [https://github.com/jfpalomeque/indeed\\_scrapper](https://github.com/jfpalomeque/indeed_scrapper)", "link": "https://www.reddit.com/r/datascience/comments/l3ogji/my_first_streamlite_web_scrapper/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "my first streamlite web scrapper! /!/ [https://share.streamlit.io/jfpalomeque/indeed\\_scrapper/main/scrapper.py](https://share.streamlit.io/jfpalomeque/indeed_scrapper/main/scrapper.py)\n\nthis is a scrapper for indeed.co.uk, the job ads website. this project has three parts, an advance webscrapper, a little exploratory analysis of those ads and a visualization -----> tool !!!  using streamlite. code in [https://github.com/jfpalomeque/indeed\\_scrapper](https://github.com/jfpalomeque/indeed_scrapper)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/l3ogji/my_first_streamlite_web_scrapper/',)", "identifyer": 5589026, "year": "2021"}, {"autor": "Tender_Figs", "date": 1614868560000, "content": "Is the allure of data science compared to data engineering a function of discovering answers and solving problems compared to engineering a system someone else will use? /!/ I struggle with determining which I want to be when I grow up (I'm 35, HA!)... and I feel a pull to provide answers and make discoveries than to build a tool for someone else to use. My background is corporate finance, and I excelled as describing data to business folk as well as arriving at a final answer on an initiative (usually, margin, ROI). \n\nWould this mean I probably have a proclivity to going to deeper into statistics than something like CS?\n\nI understand the flawed thinking here, because machine learning models are also products someone else would be using. I just see it as closer to the mantra of \"seeking and providing answers\" that seems to drive me.\n\nThoughts?", "link": "https://www.reddit.com/r/datascience/comments/lxlvev/is_the_allure_of_data_science_compared_to_data/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "is the allure of data science compared to data engineering a function of discovering answers and solving problems compared to engineering a system someone else will use? /!/ i struggle with determining which i want to be when i grow up (i'm 35, ha!)... and i feel a pull to provide answers and make discoveries than to build a -----> tool !!!  for someone else to use. my background is corporate finance, and i excelled as describing data to business folk as well as arriving at a final answer on an initiative (usually, margin, roi). \n\nwould this mean i probably have a proclivity to going to deeper into statistics than something like cs?\n\ni understand the flawed thinking here, because machine learning models are also products someone else would be using. i just see it as closer to the mantra of \"seeking and providing answers\" that seems to drive me.\n\nthoughts?", "sortedWord": "None", "removed": "('nan',)", "score": 2, "comments": 31, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/lxlvev/is_the_allure_of_data_science_compared_to_data/',)", "identifyer": 5589081, "year": "2021"}, {"autor": "ahmedbesbes", "date": 1615634150000, "content": "How to use MLflow to better track and structure your machine learning experiments /!/ Hello everyone!\n\nWant to better organize your machine learning experiments and make them reproducible when working individually or within a team?  \n\n\nMLflow is a great solution that you may look at.  \n\n\nI made a small video to present this tool and how to properly configure it either locally or on AWS.  \n\n\n[https://www.youtube.com/watch?v=osYRsBVId-A&amp;ab\\_channel=AhmedBesbes](https://www.youtube.com/watch?v=osYRsBVId-A&amp;ab_channel=AhmedBesbes)  \n\n\nTell me what you think :)", "link": "https://www.reddit.com/r/datascience/comments/m44ct0/how_to_use_mlflow_to_better_track_and_structure/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "how to use mlflow to better track and structure your machine learning experiments /!/ hello everyone!\n\nwant to better organize your machine learning experiments and make them reproducible when working individually or within a team?  \n\n\nmlflow is a great solution that you may look at.  \n\n\ni made a small video to present this -----> tool !!!  and how to properly configure it either locally or on aws.  \n\n\n[https://www.youtube.com/watch?v=osyrsbvid-a&amp;ab\\_channel=ahmedbesbes](https://www.youtube.com/watch?v=osyrsbvid-a&amp;ab_channel=ahmedbesbes)  \n\n\ntell me what you think :)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/m44ct0/how_to_use_mlflow_to_better_track_and_structure/',)", "identifyer": 5589163, "year": "2021"}, {"autor": "tuliosarmento", "date": 1615566393000, "content": "[D] Interpreting log loss in a specific context /!/ Hi! I'm studying data science and got a project here as a homework. I made a logistic regression model to predict some rare events (3% or less). This event is labeled as my \"1\". Then, instead of getting an ideal model with perfect balance sensitivity and specificity, I did what every shithead would do and focused on creating a model who would predict the \"0\" or non-event with an \"marvellous\" precision, ignoring all the false positives. (To do that, I filtered all the scenarios where the probability of 0 would be grater than a high value).\nResult: I got a specificity of 99.95% (yaaay!) and a sensitivity of 2% or something like that. \nBut now, I'm looking for the probabilities for each scenario being 0 or 1 (I'm using python and the predict_proba() method, if it's a useful info).\nI would like to assess these probabilities and I was told the log loss would be the ideal tool. I did it for the model and got a really shitty value (0.98 or something like that). This got me thinking: Is this value specific for the probabilities of \"1\" and, therefore, I could use the (1 - 0.98) to assess the probabilities of non-event (\"0\")? I guess probably not but that's my doubt.\n\nSo in cases like this, where just specificity matters (and it shows a semi-perfect value), how should I assess the probabilities (outputs of the model) individually?\n\nThanks!", "link": "https://www.reddit.com/r/datascience/comments/m3l521/d_interpreting_log_loss_in_a_specific_context/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] interpreting log loss in a specific context /!/ hi! i'm studying data science and got a project here as a homework. i made a logistic regression model to predict some rare events (3% or less). this event is labeled as my \"1\". then, instead of getting an ideal model with perfect balance sensitivity and specificity, i did what every shithead would do and focused on creating a model who would predict the \"0\" or non-event with an \"marvellous\" precision, ignoring all the false positives. (to do that, i filtered all the scenarios where the probability of 0 would be grater than a high value).\nresult: i got a specificity of 99.95% (yaaay!) and a sensitivity of 2% or something like that. \nbut now, i'm looking for the probabilities for each scenario being 0 or 1 (i'm using python and the predict_proba() method, if it's a useful info).\ni would like to assess these probabilities and i was told the log loss would be the ideal -----> tool !!! . i did it for the model and got a really shitty value (0.98 or something like that). this got me thinking: is this value specific for the probabilities of \"1\" and, therefore, i could use the (1 - 0.98) to assess the probabilities of non-event (\"0\")? i guess probably not but that's my doubt.\n\nso in cases like this, where just specificity matters (and it shows a semi-perfect value), how should i assess the probabilities (outputs of the model) individually?\n\nthanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/m3l521/d_interpreting_log_loss_in_a_specific_context/',)", "identifyer": 5589195, "year": "2021"}, {"autor": "secodaHQ", "date": 1615506090000, "content": "Looking for beta testers for our new product /!/ After months of work and countless customer interviews, our team is excited to launch the beta of our data discovery tool at [secoda.co](http://secoda.co/). We've made the tool completely self-service to allow any organization to get value from a better data discovery tool.  \n\n\nCompanies have started to generate an enormous amount of data at an earlier stage, but traditional catalogues tools are priced for the enterprise consumer.  \n\n\nIf you're a small team that just wants a simple, affordable, automated data discovery tool that doesn't cost an arm and a leg, we would love to have you try out [https://www.secoda.co](https://www.secoda.co/)", "link": "https://www.reddit.com/r/datascience/comments/m341ey/looking_for_beta_testers_for_our_new_product/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "looking for beta testers for our new product /!/ after months of work and countless customer interviews, our team is excited to launch the beta of our data discovery -----> tool !!!  at [secoda.co](http://secoda.co/). we've made the tool completely self-service to allow any organization to get value from a better data discovery tool.  \n\n\ncompanies have started to generate an enormous amount of data at an earlier stage, but traditional catalogues tools are priced for the enterprise consumer.  \n\n\nif you're a small team that just wants a simple, affordable, automated data discovery tool that doesn't cost an arm and a leg, we would love to have you try out [https://www.secoda.co](https://www.secoda.co/)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/m341ey/looking_for_beta_testers_for_our_new_product/',)", "identifyer": 5589221, "year": "2021"}, {"autor": "gabegabe6", "date": 1618492803000, "content": "Finding patterns in stock data with similarity matching - Stock Pattern Analyze /!/ I built this tool with which anyone can find patterns in historical stock data and use that to \"forecast\" a trend for a given stock.\n\nYou can define the symbol and the time window, and you'll receive the most similar patterns. I hope you'll like it, or even use it, and I am open for any idea.\n\n- Blog post: https://www.gaborvecsei.com/Stock-Pattern-Analyzer/\n- GitHub repo: https://github.com/gaborvecsei/Stocks-Pattern-Analyzer", "link": "https://www.reddit.com/r/datascience/comments/mrei73/finding_patterns_in_stock_data_with_similarity/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "finding patterns in stock data with similarity matching - stock pattern analyze /!/ i built this -----> tool !!!  with which anyone can find patterns in historical stock data and use that to \"forecast\" a trend for a given stock.\n\nyou can define the symbol and the time window, and you'll receive the most similar patterns. i hope you'll like it, or even use it, and i am open for any idea.\n\n- blog post: https://www.gaborvecsei.com/stock-pattern-analyzer/\n- github repo: https://github.com/gaborvecsei/stocks-pattern-analyzer", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/mrei73/finding_patterns_in_stock_data_with_similarity/',)", "identifyer": 5589262, "year": "2021"}, {"autor": "milehighonlife", "date": 1618439672000, "content": "Best tool for simple data joins/cleaning? /!/ I routinely have to compare sets of data that share a unique ID, either to find where the two datasets are different (IDs present in the first file and not the second, values that are different in one of two tables) or to append values from one file in another based on ID. I\u2019ve used Excel to try to accomplish this for a few years, but my datasets are now typically 1 million+ rows. I asked my company for a tool to better handle this level of data and they told me to use Power BI. I\u2019m finding now that this is also not the best tool, as it\u2019s taking 4 hours or more to run a full join by ID on my datasets. \n\nI\u2019d like to go back to my company to recommend a new tool to use, and am looking for advice on which tool is best for my use case. I know some SQL and Python, but I\u2019m open to tools using other languages since I think I can pick them up relatively quickly. My only requirement is that I can use Excel or CSV files as a data source, but bonus points for a free software for one user. \n\nThanks in advance!", "link": "https://www.reddit.com/r/datascience/comments/mr1re0/best_tool_for_simple_data_joinscleaning/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "best -----> tool !!!  for simple data joins/cleaning? /!/ i routinely have to compare sets of data that share a unique id, either to find where the two datasets are different (ids present in the first file and not the second, values that are different in one of two tables) or to append values from one file in another based on id. i\u2019ve used excel to try to accomplish this for a few years, but my datasets are now typically 1 million+ rows. i asked my company for a tool to better handle this level of data and they told me to use power bi. i\u2019m finding now that this is also not the best tool, as it\u2019s taking 4 hours or more to run a full join by id on my datasets. \n\ni\u2019d like to go back to my company to recommend a new tool to use, and am looking for advice on which tool is best for my use case. i know some sql and python, but i\u2019m open to tools using other languages since i think i can pick them up relatively quickly. my only requirement is that i can use excel or csv files as a data source, but bonus points for a free software for one user. \n\nthanks in advance!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/mr1re0/best_tool_for_simple_data_joinscleaning/',)", "identifyer": 5589276, "year": "2021"}, {"autor": "Fun-Ant-5808", "date": 1619704596000, "content": "Thinking to create a tool to get datasets /!/ [removed]", "link": "https://www.reddit.com/r/datascience/comments/n14w8x/thinking_to_create_a_tool_to_get_datasets/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "thinking to create a -----> tool !!!  to get datasets /!/ [removed]", "sortedWord": "None", "removed": "('reddit',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/n14w8x/thinking_to_create_a_tool_to_get_datasets/',)", "identifyer": 5589370, "year": "2021"}, {"autor": "seuqaj114", "date": 1620308577000, "content": "I developed a tool to train models on AWS with a single command /!/ Hey everyone,\n\nMy friend and I developed [Nimbo](https://nimbo.sh/), a dead-simple CLI that wraps AWS CLI, allowing you to run code on AWS as if you were running it locally. GitHub: [https://github.com/nimbo-sh/nimbo](https://github.com/nimbo-sh/nimbo). Docs: [https://docs.nimbo.sh](https://docs.nimbo.sh/).\n\nWe decided to build this because we were frustrated with how cumbersome using AWS was, and we just wanted to be able to run jobs on AWS as easily as we run them locally. All in all, we didn't like the current AWS user experience, and we thought we could drastically simplify it for the machine learning/scientific computing niche.\n\nFor this reason, we also provide many useful commands to make it faster and easier to work with AWS, such as one-command Jupyter notebooks on EC2, easily checking prices, logging onto an instance, or syncing data to/from S3 (you can see some useful commands [here](https://docs.nimbo.sh/useful-commands)).\n\nUnlike other similar services, we are solely client-side, meaning that the code runs on your EC2 instances and data is stored in your S3 buckets (we don't have a server; all the infrastructure orchestration happens in the Nimbo package).\n\nWe have tons of ideas for Nimbo, such as docker support and one-command model deployments.  \n.\n\nWe are happy to receive any feedback and suggestions you have.", "link": "https://www.reddit.com/r/datascience/comments/n67kt0/i_developed_a_tool_to_train_models_on_aws_with_a/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "i developed a -----> tool !!!  to train models on aws with a single command /!/ hey everyone,\n\nmy friend and i developed [nimbo](https://nimbo.sh/), a dead-simple cli that wraps aws cli, allowing you to run code on aws as if you were running it locally. github: [https://github.com/nimbo-sh/nimbo](https://github.com/nimbo-sh/nimbo). docs: [https://docs.nimbo.sh](https://docs.nimbo.sh/).\n\nwe decided to build this because we were frustrated with how cumbersome using aws was, and we just wanted to be able to run jobs on aws as easily as we run them locally. all in all, we didn't like the current aws user experience, and we thought we could drastically simplify it for the machine learning/scientific computing niche.\n\nfor this reason, we also provide many useful commands to make it faster and easier to work with aws, such as one-command jupyter notebooks on ec2, easily checking prices, logging onto an instance, or syncing data to/from s3 (you can see some useful commands [here](https://docs.nimbo.sh/useful-commands)).\n\nunlike other similar services, we are solely client-side, meaning that the code runs on your ec2 instances and data is stored in your s3 buckets (we don't have a server; all the infrastructure orchestration happens in the nimbo package).\n\nwe have tons of ideas for nimbo, such as docker support and one-command model deployments.  \n.\n\nwe are happy to receive any feedback and suggestions you have.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/n67kt0/i_developed_a_tool_to_train_models_on_aws_with_a/',)", "identifyer": 5589525, "year": "2021"}, {"autor": "Imaginary_Eagle3751", "date": 1624300684000, "content": "Feedback for open source tool Cuelake (similar to data bricks) /!/ [removed]", "link": "https://www.reddit.com/r/datascience/comments/o524de/feedback_for_open_source_tool_cuelake_similar_to/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "feedback for open source -----> tool !!!  cuelake (similar to data bricks) /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/o524de/feedback_for_open_source_tool_cuelake_similar_to/',)", "identifyer": 5589649, "year": "2021"}, {"autor": "turquoise8", "date": 1624298401000, "content": "I'm looking for a simple tool that i can color 100 dots/symbols that represent 100 people. /!/ I'm not a data scientist so sorry if this is inappropriate. I want to have 100 symbols(small circles?) to visualize demographics. I can't find such charts in Excel or other simple chart tools. What's the easiest way i can achieve this?\n\nAnd if this is inappropriate here, which sub should i ask for it?", "link": "https://www.reddit.com/r/datascience/comments/o518jf/im_looking_for_a_simple_tool_that_i_can_color_100/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "i'm looking for a simple -----> tool !!!  that i can color 100 dots/symbols that represent 100 people. /!/ i'm not a data scientist so sorry if this is inappropriate. i want to have 100 symbols(small circles?) to visualize demographics. i can't find such charts in excel or other simple chart tools. what's the easiest way i can achieve this?\n\nand if this is inappropriate here, which sub should i ask for it?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 5, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/o518jf/im_looking_for_a_simple_tool_that_i_can_color_100/',)", "identifyer": 5589653, "year": "2021"}, {"autor": "morningtundra", "date": 1624277138000, "content": "Digitizing Printed Archives of Data Tables /!/ Does anyone have advice or ideas on how to digitize several hundred pages of printed tables of data (engineering measurements)?\n\nTable headers are a mix of English, French, and German. Data is mostly measurement data (imperial measures) and fractions, in a courier-style font of the typewriter era.\n\nI know I have to OCR all this but I can\u2019t find a tool well suited to tables of data. Let alone engineering notations.\n\nWhat\u2019s an efficient way to scan all this? Kinos or OfficeMax perhaps? But then how do I get this into something like Excel?\n\nAm I really going to have to hand key all this? It could take me months if I can stay sane long enough.\n\nAny advice welcome.", "link": "https://www.reddit.com/r/datascience/comments/o4tjvs/digitizing_printed_archives_of_data_tables/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "digitizing printed archives of data tables /!/ does anyone have advice or ideas on how to digitize several hundred pages of printed tables of data (engineering measurements)?\n\ntable headers are a mix of english, french, and german. data is mostly measurement data (imperial measures) and fractions, in a courier-style font of the typewriter era.\n\ni know i have to ocr all this but i can\u2019t find a -----> tool !!!  well suited to tables of data. let alone engineering notations.\n\nwhat\u2019s an efficient way to scan all this? kinos or officemax perhaps? but then how do i get this into something like excel?\n\nam i really going to have to hand key all this? it could take me months if i can stay sane long enough.\n\nany advice welcome.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 5, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/o4tjvs/digitizing_printed_archives_of_data_tables/',)", "identifyer": 5589677, "year": "2021"}, {"autor": "Imaginary_Eagle3751", "date": 1624263523000, "content": "Feedback for open source tool Cuelake (similar to data bricks) /!/ We have been working on this open source project since 2 months, needed some feedback from data scientists so posting it here. In current state Cuelake can be installed on a Kubernetes cluster and can run spark and python code via zeppelin notebooks. You can check out the repo here: [https://github.com/cuebook/cuelake](https://github.com/cuebook/cuelake)\n\nHere are some benefits from Cuelake that I have assumed:\n\n1. **One tool for engineer/analyst/scientist:** Data scientist can run their own spark or python clusters, train and test data, create models. These models then can be used for scoring in ETL pipelines and the code can be scheduled to run on any frequency. Data analyst can use spark sql or jdbc interpreter to analyze data on adhoc basis. Even data analyst can schedule SQL queries for basic data engineering. Complex use cases can be handled by creating workflows from multiple notebooks for example: on success of notebook A, B &amp; C run notebook X or else run notebook Y\n2. **Cloud Cost optimization:** Spark Clusters are shut down after 5 minutes of inactivity by default. You can change the timeout according to your need. \n3. **Scalability using Kubernetes Cluster Autoscaler:** You can set a default spark cluster memory and also have a separate cluster per notebook and you can specify the number of executors you need and memory per master and executor at the starting of notebook in spark.conf. \n4. **Cost savings from Data Warehouse to Lakehouse:** Instead of putting all data in data warehouses you can build a lakehouse or data lake using Cuelake. With lakehouse you get all the features of a data warehouse, its just slower in performance. So you can move your data that is not used for analytics but you still need to store it anyway. \n\nPlease let me know if my assumptions are wrong or very absurd :p.", "link": "https://www.reddit.com/r/datascience/comments/o4qcw7/feedback_for_open_source_tool_cuelake_similar_to/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "feedback for open source -----> tool !!!  cuelake (similar to data bricks) /!/ we have been working on this open source project since 2 months, needed some feedback from data scientists so posting it here. in current state cuelake can be installed on a kubernetes cluster and can run spark and python code via zeppelin notebooks. you can check out the repo here: [https://github.com/cuebook/cuelake](https://github.com/cuebook/cuelake)\n\nhere are some benefits from cuelake that i have assumed:\n\n1. **one tool for engineer/analyst/scientist:** data scientist can run their own spark or python clusters, train and test data, create models. these models then can be used for scoring in etl pipelines and the code can be scheduled to run on any frequency. data analyst can use spark sql or jdbc interpreter to analyze data on adhoc basis. even data analyst can schedule sql queries for basic data engineering. complex use cases can be handled by creating workflows from multiple notebooks for example: on success of notebook a, b &amp; c run notebook x or else run notebook y\n2. **cloud cost optimization:** spark clusters are shut down after 5 minutes of inactivity by default. you can change the timeout according to your need. \n3. **scalability using kubernetes cluster autoscaler:** you can set a default spark cluster memory and also have a separate cluster per notebook and you can specify the number of executors you need and memory per master and executor at the starting of notebook in spark.conf. \n4. **cost savings from data warehouse to lakehouse:** instead of putting all data in data warehouses you can build a lakehouse or data lake using cuelake. with lakehouse you get all the features of a data warehouse, its just slower in performance. so you can move your data that is not used for analytics but you still need to store it anyway. \n\nplease let me know if my assumptions are wrong or very absurd :p.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/o4qcw7/feedback_for_open_source_tool_cuelake_similar_to/',)", "identifyer": 5589681, "year": "2021"}, {"autor": "toaarushi", "date": 1624209811000, "content": "What kind of project can a beginner do on a software like environics analytics? /!/ I have to student access to environics analytics tool which has vast data on the Canadian demographic, income, buying habits etc. \n\nWhat do you think about softwares like these?\nWhat kind of project would you recommend doing that could be relevant for a job search?", "link": "https://www.reddit.com/r/datascience/comments/o4ahkf/what_kind_of_project_can_a_beginner_do_on_a/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what kind of project can a beginner do on a software like environics analytics? /!/ i have to student access to environics analytics -----> tool !!!  which has vast data on the canadian demographic, income, buying habits etc. \n\nwhat do you think about softwares like these?\nwhat kind of project would you recommend doing that could be relevant for a job search?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/o4ahkf/what_kind_of_project_can_a_beginner_do_on_a/',)", "identifyer": 5589713, "year": "2021"}, {"autor": "quite--average", "date": 1624207328000, "content": "If Data Scientist is not an entry level job, how much and what kind of experience can be a stepping stone to become a Data Scientist? /!/ Hello!\n\nI have often seen on this sub people saying that Data Scientist is not entry level job. I previously didn't use to believe that since I knew people who became \"Data Scientist\" right after school. However, now I understand why it's not an entry level job, thanks to the seniors on this sub.\n\nMy question, what are some jobs that can be used as a stepping stone to become a Data Scientist? And Am I on the right track?\n\nI have a Masters degree with Statistics as a major. Graduated in 2020 and have been  working as a BI developer ever since. For the first half of the year more than 90% of my job was to build dashboards in a BI tool, got a bit monotonous. But recently, I have picked up an ML project and have been using R to explore the data, feature Engineer and build ML models. Apart from that, building dashboards is pretty much continuous but lately along with that I have started using pandas or tidyverse to clean, reshape the data and turn it into a data source for the BI tool.\n\nCan I look forward to apply for Data Science roles in a year or so?\n\nThank you!", "link": "https://www.reddit.com/r/datascience/comments/o49mn3/if_data_scientist_is_not_an_entry_level_job_how/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "if data scientist is not an entry level job, how much and what kind of experience can be a stepping stone to become a data scientist? /!/ hello!\n\ni have often seen on this sub people saying that data scientist is not entry level job. i previously didn't use to believe that since i knew people who became \"data scientist\" right after school. however, now i understand why it's not an entry level job, thanks to the seniors on this sub.\n\nmy question, what are some jobs that can be used as a stepping stone to become a data scientist? and am i on the right track?\n\ni have a masters degree with statistics as a major. graduated in 2020 and have been  working as a bi developer ever since. for the first half of the year more than 90% of my job was to build dashboards in a bi -----> tool !!! , got a bit monotonous. but recently, i have picked up an ml project and have been using r to explore the data, feature engineer and build ml models. apart from that, building dashboards is pretty much continuous but lately along with that i have started using pandas or tidyverse to clean, reshape the data and turn it into a data source for the bi tool.\n\ncan i look forward to apply for data science roles in a year or so?\n\nthank you!", "sortedWord": "None", "removed": "('nan',)", "score": 2, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/o49mn3/if_data_scientist_is_not_an_entry_level_job_how/',)", "identifyer": 5589716, "year": "2021"}, {"autor": "Little_Sun_Jill", "date": 1624720217000, "content": "Should I Switch Job? Need Advices... /!/ Hello, thanks for reading this post. I really need advices about whether I should change job. \n\n**Background** \n\nHere is my background. I majored in Statistics in school and currently work in the data science team of a large company. My main mandate is to do segmentation models and deep dive analysis. SAS is the main tool the team uses. \n\n**Situation**\n\nAs I work here for four years, I realized that the team is falling much behind the data science trend outside. We don't have access to Python/R, and the company doesn't have the latest SAS licenses for machine learning modules. \n\nI tried to keep learning at Coursera but I can hardly apply what I learn to the work projects as almost  all those fancy techniques need Python/R to implement. Therefore, the longer I work here, the more I think I'm falling behind and losing a competitive position in job market. \n\n**Concerns**\n\nI hesitate to change job for some reasons. Here are the main ones:\n\n* My manager treats me well (no overtime, take days off as needed).\n* I'm working on a long-term strategic project which would be beneficial for my career growth. \n* Based on my tenure and performance, I will likely get promotion to senior in a year.  \n* The team just got a new director, and there is plan to transform the team from the ground up. Just don't know when and what would that be. \n\n**Questions**\n\nQ1. Should I quit this job and look for one that's closer to data science trend? \n\nI found that the jobs that I'm interested usually require experiences in Python and Neural Network which I have little. I'm worried that I would need to lower my salary expectation and restart my career like a junior. \n\nQ2. If yes, should I wait until I promote to senior?  \n\n&amp;#x200B;\n\nThank you for your time and advices!", "link": "https://www.reddit.com/r/datascience/comments/o8clh9/should_i_switch_job_need_advices/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "should i switch job? need advices... /!/ hello, thanks for reading this post. i really need advices about whether i should change job. \n\n**background** \n\nhere is my background. i majored in statistics in school and currently work in the data science team of a large company. my main mandate is to do segmentation models and deep dive analysis. sas is the main -----> tool !!!  the team uses. \n\n**situation**\n\nas i work here for four years, i realized that the team is falling much behind the data science trend outside. we don't have access to python/r, and the company doesn't have the latest sas licenses for machine learning modules. \n\ni tried to keep learning at coursera but i can hardly apply what i learn to the work projects as almost  all those fancy techniques need python/r to implement. therefore, the longer i work here, the more i think i'm falling behind and losing a competitive position in job market. \n\n**concerns**\n\ni hesitate to change job for some reasons. here are the main ones:\n\n* my manager treats me well (no overtime, take days off as needed).\n* i'm working on a long-term strategic project which would be beneficial for my career growth. \n* based on my tenure and performance, i will likely get promotion to senior in a year.  \n* the team just got a new director, and there is plan to transform the team from the ground up. just don't know when and what would that be. \n\n**questions**\n\nq1. should i quit this job and look for one that's closer to data science trend? \n\ni found that the jobs that i'm interested usually require experiences in python and neural network which i have little. i'm worried that i would need to lower my salary expectation and restart my career like a junior. \n\nq2. if yes, should i wait until i promote to senior?  \n\n&amp;#x200b;\n\nthank you for your time and advices!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/o8clh9/should_i_switch_job_need_advices/',)", "identifyer": 5589781, "year": "2021"}, {"autor": "safemath", "date": 1624697508000, "content": "Where do I start learning so I can hit the ground running with this new DBT/BQ work assignment? /!/ I have been assigned a new role. My manager knowing I am the most 'technical' person in the team asked if I could help and take a stab at it. I am not entirely sure of the workflow, but I know the tools:\n\nDBT, BigQuery and SQL.\n\nI  know the basics of SQL but I want to familiarize myself with all the   above tool kits and how to link them. I am a reasonably competent   self-taught programmer (from the perspective of C# for GameDev), but   these toolkits are new to me.\n\nAs I   feel like a total outsider, can someone point me to some practical resources I can watch or read so I can do a good job here and impress management? I dont even know the basics of DBT or BQ, so dont know where   to start learning...\n\nAny pointers would be really appreciated.", "link": "https://www.reddit.com/r/datascience/comments/o873up/where_do_i_start_learning_so_i_can_hit_the_ground/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "where do i start learning so i can hit the ground running with this new dbt/bq work assignment? /!/ i have been assigned a new role. my manager knowing i am the most 'technical' person in the team asked if i could help and take a stab at it. i am not entirely sure of the workflow, but i know the tools:\n\ndbt, bigquery and sql.\n\ni  know the basics of sql but i want to familiarize myself with all the   above -----> tool !!!  kits and how to link them. i am a reasonably competent   self-taught programmer (from the perspective of c# for gamedev), but   these toolkits are new to me.\n\nas i   feel like a total outsider, can someone point me to some practical resources i can watch or read so i can do a good job here and impress management? i dont even know the basics of dbt or bq, so dont know where   to start learning...\n\nany pointers would be really appreciated.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/o873up/where_do_i_start_learning_so_i_can_hit_the_ground/',)", "identifyer": 5589790, "year": "2021"}, {"autor": "Affectionate3375", "date": 1624695746000, "content": "Where do I start learning so I can hit the ground running with this new DBT/BQ work assignment? /!/ I have been assigned a new role. My manager knowing I am the most 'technical' person in the team asked if I could help and take a stab at it. I am not entirely sure of the workflow, but I know the tools:\n\nDBT, BigQuery and SQL. \n\nI know the basics of SQL but I want to familiarize myself with all the above tool kits and how to link them. I am a reasonably competent self-taught programmer (from the perspective of C# for GameDev), but these toolkits are new to me. \n\nAs I feel like a total outsider, can someone point me to some practical resources I can watch or read so I can do a good job here and impress management? \n\nAny pointers would be really appreciated.", "link": "https://www.reddit.com/r/datascience/comments/o86r7b/where_do_i_start_learning_so_i_can_hit_the_ground/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "where do i start learning so i can hit the ground running with this new dbt/bq work assignment? /!/ i have been assigned a new role. my manager knowing i am the most 'technical' person in the team asked if i could help and take a stab at it. i am not entirely sure of the workflow, but i know the tools:\n\ndbt, bigquery and sql. \n\ni know the basics of sql but i want to familiarize myself with all the above -----> tool !!!  kits and how to link them. i am a reasonably competent self-taught programmer (from the perspective of c# for gamedev), but these toolkits are new to me. \n\nas i feel like a total outsider, can someone point me to some practical resources i can watch or read so i can do a good job here and impress management? \n\nany pointers would be really appreciated.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/o86r7b/where_do_i_start_learning_so_i_can_hit_the_ground/',)", "identifyer": 5589791, "year": "2021"}, {"autor": "kittythief", "date": 1609961028000, "content": "What tool helped you succeed and understand your work? /!/ I recently accepted a data analyst internship and I am completely lost. With the added pandemic, everything is being done remotely and I have had bare minimum training. ( think youtube videos defining definitions).\n\nI am trying my best to comprehend the work but there is no training manual, there is two people on the team who are already exasperated with my questions. \n\nSo far I can see we use funnel.io to add dimensions and metrics of who knows what. Upload it to data studio/ google analytics and google tag manager. \n\nDo you have any tools that helped you understand your work? I am well and truly lost, I don't even know what to google to do research on my own.", "link": "https://www.reddit.com/r/datascience/comments/krv3vw/what_tool_helped_you_succeed_and_understand_your/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what -----> tool !!!  helped you succeed and understand your work? /!/ i recently accepted a data analyst internship and i am completely lost. with the added pandemic, everything is being done remotely and i have had bare minimum training. ( think youtube videos defining definitions).\n\ni am trying my best to comprehend the work but there is no training manual, there is two people on the team who are already exasperated with my questions. \n\nso far i can see we use funnel.io to add dimensions and metrics of who knows what. upload it to data studio/ google analytics and google tag manager. \n\ndo you have any tools that helped you understand your work? i am well and truly lost, i don't even know what to google to do research on my own.", "sortedWord": "None", "removed": "('nan',)", "score": 3, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/krv3vw/what_tool_helped_you_succeed_and_understand_your/',)", "identifyer": 5589848, "year": "2021"}, {"autor": "bo-de-gas", "date": 1609956207000, "content": "Are \"bootcamps\" diploma mills? /!/ Hey all, I'm wondering how competitive or exclusive the admission process for bootcamps really is (specifically in the Data Science field). \n\nRight now I'm going through it at 2 different institutions which seem like the most reputable ones accessible to me in my local area. I've completed a pre admission challenge at one and working on the other right now. \n\nThey both seem pretty eager to have me join, but I'm getting a pretty strong \"used car salesman\" meets \"apple genius\" vibe from both of them if that makes any sense.\n\nThese are my observations: \n\n\\-So far I've received one admission offer with a 20% discount (or \"scholarship\" in thier words) from the listed tuition cost, but it wouldn't surprise me if they offered that to everybody. \n\n\\-They told me it was because the work on my technical challenge was impressive, but I couldn't get them give me any kind of critical feedback (I know my coding work had deficiencies that I just didn't have time to fix, and some of my approach seemed a bit dodgy to me at least). \n\n\\-They wouldn't tell me the rate at which they reject applicants. \n\n\\-I'm feeling a moderate amount of pressure to sign on ASAP, and being told how competitive things are. But they're not giving me any real deadline beyond the actual start date for the late February cohort I'm interested in. They're offering for me to join an earlier cohort even. It doesn't sound like they're filling up..\n\n\\-As I was writing this I received an email from my point of contact and they forgot to remove a note indicating that they were using an email tracking app to see how many times I looked at their message in my inbox. This is a bit invasive, and seems like a sales tool plain and simple. (I read it 3 times, triggering them to follow up with me)\n\nI have no illusions in my mind that I'm enrolling at MIT or Harvard. I have a pretty respectable educational and professional background that I think would make me a desirable candidate for these courses - I want to learn some new skills that I can apply to areas I'm already experienced in, which come with some kind of credentials.\n\nI don't want to throw away a large chunk of my savings on a diploma mill though. I have already learned a lot of cool stuff on my own since I started looking into these courses. Are these institutions just taking in anybody with deep enough pockets? \n\nAny general thoughts or advice would be welcome!", "link": "https://www.reddit.com/r/datascience/comments/krtb03/are_bootcamps_diploma_mills/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "are \"bootcamps\" diploma mills? /!/ hey all, i'm wondering how competitive or exclusive the admission process for bootcamps really is (specifically in the data science field). \n\nright now i'm going through it at 2 different institutions which seem like the most reputable ones accessible to me in my local area. i've completed a pre admission challenge at one and working on the other right now. \n\nthey both seem pretty eager to have me join, but i'm getting a pretty strong \"used car salesman\" meets \"apple genius\" vibe from both of them if that makes any sense.\n\nthese are my observations: \n\n\\-so far i've received one admission offer with a 20% discount (or \"scholarship\" in thier words) from the listed tuition cost, but it wouldn't surprise me if they offered that to everybody. \n\n\\-they told me it was because the work on my technical challenge was impressive, but i couldn't get them give me any kind of critical feedback (i know my coding work had deficiencies that i just didn't have time to fix, and some of my approach seemed a bit dodgy to me at least). \n\n\\-they wouldn't tell me the rate at which they reject applicants. \n\n\\-i'm feeling a moderate amount of pressure to sign on asap, and being told how competitive things are. but they're not giving me any real deadline beyond the actual start date for the late february cohort i'm interested in. they're offering for me to join an earlier cohort even. it doesn't sound like they're filling up..\n\n\\-as i was writing this i received an email from my point of contact and they forgot to remove a note indicating that they were using an email tracking app to see how many times i looked at their message in my inbox. this is a bit invasive, and seems like a sales -----> tool !!!  plain and simple. (i read it 3 times, triggering them to follow up with me)\n\ni have no illusions in my mind that i'm enrolling at mit or harvard. i have a pretty respectable educational and professional background that i think would make me a desirable candidate for these courses - i want to learn some new skills that i can apply to areas i'm already experienced in, which come with some kind of credentials.\n\ni don't want to throw away a large chunk of my savings on a diploma mill though. i have already learned a lot of cool stuff on my own since i started looking into these courses. are these institutions just taking in anybody with deep enough pockets? \n\nany general thoughts or advice would be welcome!", "sortedWord": "None", "removed": "('nan',)", "score": 2, "comments": 169, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/krtb03/are_bootcamps_diploma_mills/',)", "identifyer": 5589854, "year": "2021"}, {"autor": "TuataraTim", "date": 1609949847000, "content": "Am I even a Data Analyst? /!/ Had trouble finding a SWE/DS job (CS grad) in the pandemic so I jumped on a Data Analyst role thinking it could be a good springboard.\n\n80% of what I do is work on migrating reports from one system to another, newer, more tableau-like system. Lots of drag and drop GUIs. Every couple of reports I come across doesn't have the right data in our Data Mart, so I look at the SQL to figure out if a measure is being calculated incorrectly, if we're even bringing in the right thing from the data warehouse, etc. I look through a lot of excel files looking for needle in the haystack discrepancies in numbers as well.\n\nBesides that, I do support for the users using our reporting tool (adding them to the system and giving them perms). I've volunteered to get myself involved in a little bit of python scripting in pandas to help automate excel reports, but I don't have much time at work to do that.\n\nIs this a typical Data Analyst job? I've seen people on this sub mention data analysts presenting findings and stuff to management, which is nothing like I do. If not, what would you even call this job? It's not a terrible job, I just want to program a lot more...", "link": "https://www.reddit.com/r/datascience/comments/krqzqj/am_i_even_a_data_analyst/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "am i even a data analyst? /!/ had trouble finding a swe/ds job (cs grad) in the pandemic so i jumped on a data analyst role thinking it could be a good springboard.\n\n80% of what i do is work on migrating reports from one system to another, newer, more tableau-like system. lots of drag and drop guis. every couple of reports i come across doesn't have the right data in our data mart, so i look at the sql to figure out if a measure is being calculated incorrectly, if we're even bringing in the right thing from the data warehouse, etc. i look through a lot of excel files looking for needle in the haystack discrepancies in numbers as well.\n\nbesides that, i do support for the users using our reporting -----> tool !!!  (adding them to the system and giving them perms). i've volunteered to get myself involved in a little bit of python scripting in pandas to help automate excel reports, but i don't have much time at work to do that.\n\nis this a typical data analyst job? i've seen people on this sub mention data analysts presenting findings and stuff to management, which is nothing like i do. if not, what would you even call this job? it's not a terrible job, i just want to program a lot more...", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 20, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/krqzqj/am_i_even_a_data_analyst/',)", "identifyer": 5589858, "year": "2021"}, {"autor": "SprinkleData", "date": 1609944198000, "content": "The best in class Dashboard and SQL Reporting tools in 2021 /!/ The struggle is real isn't it? Most Established companies could have various tools for various levels of data operations where there isn't any constrain in terms of funds. This isn't the case when it comes to Startups and SMEs right? There is a constrain for almost everything and funds are the most alarming ones.\n\nStartups that look to make key business decisions with their data would mostly look for a very much affordable tool in the market and we have shortlisted the best five Dashboarding and SQL reporting tools for 2021.\n\n[**Top 5 Free or Affordable SQL reporting and Dashboard tools in 2021**](https://www.sprinkledata.com/docs/top-5-free-or-affordable-dashboard-and-sql-reporting-tools/index.html?utm_source=reddit_060121&amp;utm_medium=sqlreportingtool)", "link": "https://www.reddit.com/r/datascience/comments/krp5om/the_best_in_class_dashboard_and_sql_reporting/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "the best in class dashboard and sql reporting tools in 2021 /!/ the struggle is real isn't it? most established companies could have various tools for various levels of data operations where there isn't any constrain in terms of funds. this isn't the case when it comes to startups and smes right? there is a constrain for almost everything and funds are the most alarming ones.\n\nstartups that look to make key business decisions with their data would mostly look for a very much affordable -----> tool !!!  in the market and we have shortlisted the best five dashboarding and sql reporting tools for 2021.\n\n[**top 5 free or affordable sql reporting and dashboard tools in 2021**](https://www.sprinkledata.com/docs/top-5-free-or-affordable-dashboard-and-sql-reporting-tools/index.html?utm_source=reddit_060121&amp;utm_medium=sqlreportingtool)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/krp5om/the_best_in_class_dashboard_and_sql_reporting/',)", "identifyer": 5589866, "year": "2021"}, {"autor": "mrnerdy59", "date": 1609838021000, "content": "Finally, a DS Project in Production. Predictive Lead Scoring. Possible a SAAS product in the future /!/ Just wanted to talk about one of my DS/ML based project that I've pushed into production. Looking for some feedback/reviews on it, what could have been done better, or if it breaks at some point or DM me on how it really works but I'll summarise it here anyway:\n\nThe app's idea is to do lead scoring, where \"Lead\" is basically a visitor on your website. What this tool does is, given that you had a CRM like Hubspot attached to your website that keeps a history/track of leads, the app will pull data, train a model, deploy it only for your account and predict on new leads for their probability to convert into customers.\n\nThe concept isn't new, there are lot of companies doing predictive scoring, but doing a model for every business individually without any real human intervention except for linking Hubspot (Yh, It works with just this CRM for now) is not many are doing it.\n\nIf someone is interested to take this forward in terms of converting this into a product, do DM me. TIA\n\n[trainhubspot.deepneurals.com](https://trainhubspot.deepneurals.com)", "link": "https://www.reddit.com/r/datascience/comments/kqurtb/finally_a_ds_project_in_production_predictive/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "finally, a ds project in production. predictive lead scoring. possible a saas product in the future /!/ just wanted to talk about one of my ds/ml based project that i've pushed into production. looking for some feedback/reviews on it, what could have been done better, or if it breaks at some point or dm me on how it really works but i'll summarise it here anyway:\n\nthe app's idea is to do lead scoring, where \"lead\" is basically a visitor on your website. what this -----> tool !!!  does is, given that you had a crm like hubspot attached to your website that keeps a history/track of leads, the app will pull data, train a model, deploy it only for your account and predict on new leads for their probability to convert into customers.\n\nthe concept isn't new, there are lot of companies doing predictive scoring, but doing a model for every business individually without any real human intervention except for linking hubspot (yh, it works with just this crm for now) is not many are doing it.\n\nif someone is interested to take this forward in terms of converting this into a product, do dm me. tia\n\n[trainhubspot.deepneurals.com](https://trainhubspot.deepneurals.com)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/kqurtb/finally_a_ds_project_in_production_predictive/',)", "identifyer": 5589929, "year": "2021"}, {"autor": "Responsible-Fly-1792", "date": 1635361998000, "content": "Using MLFlow to Track and Version Machine Learning Models /!/ MLFlow is a powerful MLOps tool for tracking and versioning machine learning models. Would you like to learn how it works? Check out my new blog here:\n\n&amp;#x200B;\n\nUsing MLFlow to Track and Version Machine Learning Models\n\n[https://towardsdatascience.com/using-mlflow-to-track-and-version-machine-learning-models-efd5daa08df0](https://towardsdatascience.com/using-mlflow-to-track-and-version-machine-learning-models-efd5daa08df0)", "link": "https://www.reddit.com/r/datascience/comments/qh4610/using_mlflow_to_track_and_version_machine/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "using mlflow to track and version machine learning models /!/ mlflow is a powerful mlops -----> tool !!!  for tracking and versioning machine learning models. would you like to learn how it works? check out my new blog here:\n\n&amp;#x200b;\n\nusing mlflow to track and version machine learning models\n\n[https://towardsdatascience.com/using-mlflow-to-track-and-version-machine-learning-models-efd5daa08df0](https://towardsdatascience.com/using-mlflow-to-track-and-version-machine-learning-models-efd5daa08df0)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/qh4610/using_mlflow_to_track_and_version_machine/',)", "identifyer": 5589992, "year": "2021"}, {"autor": "ricke813", "date": 1623127796000, "content": "Tableau vs Amazon QuickSights /!/ I know Tableau very well and is my preferred tool. But, I noticed there's a cheaper option from Amazon. Any input from your experience is appreciated.", "link": "https://www.reddit.com/r/datascience/comments/nuwn27/tableau_vs_amazon_quicksights/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "tableau vs amazon quicksights /!/ i know tableau very well and is my preferred -----> tool !!! . but, i noticed there's a cheaper option from amazon. any input from your experience is appreciated.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 13, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nuwn27/tableau_vs_amazon_quicksights/',)", "identifyer": 5590037, "year": "2021"}, {"autor": "Apprehensive-Web-987", "date": 1623115592000, "content": "[rant from a hiring manager] MS in Buzzword programs are a waste of money /!/ I'm a first time hiring manager and I'm hiring a Data Analyst. I've reviewed hundreds of resumes and talked to dozens of people to calibrate myself on what people from different backgrounds are like.\n\nCall me a purist, but I don't think it's possible to do good quantitative work if you don't understand how the models work from a basic mathematical perspective. For example, you should be able to answer the following questions about any given model you use in your work and mention in an interview: how should residuals be distributed in any given model you're using? What does a p-value actually represent in mathematical terms? What are the assumptions around independence of your predictor variables for any given model you're using? What diagnostic tests should you run for any given model and what hypothesis are each of these testing? And more. If you don't understand these things, then you are going to build an overly complicated model that is completely overfitted, as opposed to a more generalizable model that can be applied to multiple datasets.\n\nThe number of people I've talked to who have a MS in Business Analytics who say they're doing ML but then can't explain these basic underlying assumptions of the models they build is astounding and pitiful. It doesn't matter how prestigious the school they went to--I've talked to people with degrees like this from both Ivy League schools and from lesser known schools and it is all the same. I'm not trying to trick people with obscure trivia or anything; when I ask them to tell me about a project they worked on, they tell me \"I built \\[insert model\\] so I could \\[solve whatever business problem\\]\" and when I follow up with \"what are the underlying assumptions for \\[insert model\\]? What kinds of diagnostics did you run for that model?\", they get completely tongue tied, even for the basics like logistic and linear regression. The only things these people ever check for with diagnostics is \"sensitivity and specificity\" and \"area under the curve\", which barely scratches the surface in determining whether or not your model is good.\n\nI've looked into the requirements on these programs (both when I was choosing a masters program a few years back and also more recently as I've been interviewing people) and none of these programs require people to take any courses that would actually prepare them to build models thoughtfully--just courses like \"SQL for Analytics\", \"Tableau for Visualization\", and super tool specific classes that (a) would be super easy for a reasonably smart person to learn on the job and (b) are not going to give them skills that will be longterm valuable because the tools we use change all the time--underlying mathematical principles do not.\n\nSo.... if you're thinking about getting an advanced degree to get more out of your career in this field, do yourself a favor and choose a program in a REAL ACADEMIC FIELD (ex: statistics, math, computer science, engineering) and not Some Buzzword That's Super Hot Right Now. No one will care about your bullshit buzzword degree 10 years from now. At the bare minimum, choose a program that requires you to take classes that are well founded in probability and mathematical statistics and not just plugging random shit into R or Python hoping that you'll be able to \\~predict the future\\~.", "link": "https://www.reddit.com/r/datascience/comments/nusz8e/rant_from_a_hiring_manager_ms_in_buzzword/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[rant from a hiring manager] ms in buzzword programs are a waste of money /!/ i'm a first time hiring manager and i'm hiring a data analyst. i've reviewed hundreds of resumes and talked to dozens of people to calibrate myself on what people from different backgrounds are like.\n\ncall me a purist, but i don't think it's possible to do good quantitative work if you don't understand how the models work from a basic mathematical perspective. for example, you should be able to answer the following questions about any given model you use in your work and mention in an interview: how should residuals be distributed in any given model you're using? what does a p-value actually represent in mathematical terms? what are the assumptions around independence of your predictor variables for any given model you're using? what diagnostic tests should you run for any given model and what hypothesis are each of these testing? and more. if you don't understand these things, then you are going to build an overly complicated model that is completely overfitted, as opposed to a more generalizable model that can be applied to multiple datasets.\n\nthe number of people i've talked to who have a ms in business analytics who say they're doing ml but then can't explain these basic underlying assumptions of the models they build is astounding and pitiful. it doesn't matter how prestigious the school they went to--i've talked to people with degrees like this from both ivy league schools and from lesser known schools and it is all the same. i'm not trying to trick people with obscure trivia or anything; when i ask them to tell me about a project they worked on, they tell me \"i built \\[insert model\\] so i could \\[solve whatever business problem\\]\" and when i follow up with \"what are the underlying assumptions for \\[insert model\\]? what kinds of diagnostics did you run for that model?\", they get completely tongue tied, even for the basics like logistic and linear regression. the only things these people ever check for with diagnostics is \"sensitivity and specificity\" and \"area under the curve\", which barely scratches the surface in determining whether or not your model is good.\n\ni've looked into the requirements on these programs (both when i was choosing a masters program a few years back and also more recently as i've been interviewing people) and none of these programs require people to take any courses that would actually prepare them to build models thoughtfully--just courses like \"sql for analytics\", \"tableau for visualization\", and super -----> tool !!!  specific classes that (a) would be super easy for a reasonably smart person to learn on the job and (b) are not going to give them skills that will be longterm valuable because the tools we use change all the time--underlying mathematical principles do not.\n\nso.... if you're thinking about getting an advanced degree to get more out of your career in this field, do yourself a favor and choose a program in a real academic field (ex: statistics, math, computer science, engineering) and not some buzzword that's super hot right now. no one will care about your bullshit buzzword degree 10 years from now. at the bare minimum, choose a program that requires you to take classes that are well founded in probability and mathematical statistics and not just plugging random shit into r or python hoping that you'll be able to \\~predict the future\\~.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 51, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nusz8e/rant_from_a_hiring_manager_ms_in_buzzword/',)", "identifyer": 5590040, "year": "2021"}, {"autor": "Namur007", "date": 1623068706000, "content": "Visual Cleaning tool - Time Series /!/ Hi! Looking for a suggestion on a visual cleaning tool for time series (or any) data. \n\nI am running my raw data through a time series decomposition to pick out some potential errors in the source, but ideally it would be nice to have something interactive to work with to validate the errors and add more. \n\nTrying to avoid writing a bespoke thing in dash but I can ultimately do that if it comes to it. \n\nNo bound to a platform but python is preferred, or even something electron based. \n\nThanks!", "link": "https://www.reddit.com/r/datascience/comments/nub8j8/visual_cleaning_tool_time_series/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "visual cleaning -----> tool !!!  - time series /!/ hi! looking for a suggestion on a visual cleaning tool for time series (or any) data. \n\ni am running my raw data through a time series decomposition to pick out some potential errors in the source, but ideally it would be nice to have something interactive to work with to validate the errors and add more. \n\ntrying to avoid writing a bespoke thing in dash but i can ultimately do that if it comes to it. \n\nno bound to a platform but python is preferred, or even something electron based. \n\nthanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nub8j8/visual_cleaning_tool_time_series/',)", "identifyer": 5590077, "year": "2021"}, {"autor": "surgeai", "date": 1615999041000, "content": "Supercharge your NLP project with a free, self-serve NER tool /!/ Hey [r/datascience](https://www.reddit.com/r/MachineLearning/) readers!\n\nWe heard from Redditors that a lot of people want a free, self-serve NLP labeling tool. We heard your feedback - and we\u2019re excited to share with you our new-and-improved data labeling platform! It comes with a free suite of tools for customizing your labeling projects, as well as a wide variety of labeling questions: multiple choice, free response, named entity recognition (NER), etc.\n\nSo how\u2019s our platform different from others? With Surge, you have a quick way to kick off labeling projects, access to free team management capabilities, and fully-customizable templates to support a wide range of labeling needs. You can read more about the details [here](https://www.surgehq.ai/blog/named-entity-recognition).\n\nIf you\u2019d like to check it out, go to [surgehq.ai](https://www.surgehq.ai/) and create an account. You and your team can start labeling right away for free. Or, if you don\u2019t want to do the labeling yourself, you can use our high-quality, paid workforce.\n\nAny kind of feedback would really help us shape our product roadmap; you can email us at team@surgehq.ai. We hope that you enjoy using our tool, and we\u2019re looking forward to hearing your thoughts!", "link": "https://www.reddit.com/r/datascience/comments/m74hr0/supercharge_your_nlp_project_with_a_free/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "supercharge your nlp project with a free, self-serve ner -----> tool !!!  /!/ hey [r/datascience](https://www.reddit.com/r/machinelearning/) readers!\n\nwe heard from redditors that a lot of people want a free, self-serve nlp labeling tool. we heard your feedback - and we\u2019re excited to share with you our new-and-improved data labeling platform! it comes with a free suite of tools for customizing your labeling projects, as well as a wide variety of labeling questions: multiple choice, free response, named entity recognition (ner), etc.\n\nso how\u2019s our platform different from others? with surge, you have a quick way to kick off labeling projects, access to free team management capabilities, and fully-customizable templates to support a wide range of labeling needs. you can read more about the details [here](https://www.surgehq.ai/blog/named-entity-recognition).\n\nif you\u2019d like to check it out, go to [surgehq.ai](https://www.surgehq.ai/) and create an account. you and your team can start labeling right away for free. or, if you don\u2019t want to do the labeling yourself, you can use our high-quality, paid workforce.\n\nany kind of feedback would really help us shape our product roadmap; you can email us at team@surgehq.ai. we hope that you enjoy using our tool, and we\u2019re looking forward to hearing your thoughts!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/m74hr0/supercharge_your_nlp_project_with_a_free/',)", "identifyer": 5590172, "year": "2021"}, {"autor": "michael_htx", "date": 1615992670000, "content": "[P] Label Studio v1.0 \u2013 an open source data labeling tool that helps you prepare ML training data and improve the quality of your datasets, works for computer vision, NLP, speech processing, time series analysis, and more", "link": "https://www.reddit.com/r/datascience/comments/m724do/p_label_studio_v10_an_open_source_data_labeling/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] label studio v1.0 \u2013 an open source data labeling -----> tool !!!  that helps you prepare ml training data and improve the quality of your datasets, works for computer vision, nlp, speech processing, time series analysis, and more", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('/r/MachineLearning/comments/m6eoru/p_label_studio_v10_an_open_source_data_labeling/',)", "identifyer": 5590183, "year": "2021"}, {"autor": "sergiimk", "date": 1615945463000, "content": "Introducing Kamu - World's first global collaborative data pipeline /!/ [kamu](https://github.com/kamu-data/kamu-cli)\u00a0is a new data management tool for rapid exchange and global collaboration on structured data.\n\nIt can be described as:\n\n* Global supply chain for structured data\n* Decentralized stream-processing data pipeline\n* Git for data (think collaboration, not diffs/branches)\n* Blockchain for Big Data\n\nIt\u2019s an ambitious project that takes a very different perspective at\u00a0*what data is*\u00a0and how it should to be handled.\n\nPlease check out:\n\n* [Quick project intro](https://medium.com/kamu-data/kamu-update-were-in-alpha-8c86e4c4f1a9)\n* [Introductory post to Open Data Fabric](https://medium.com/kamu-data/introducing-open-data-fabric-eaf9fdcd3903) \\- an open protocol the tool is based on\n* [The demo video](http://www.youtube.com/watch?v=UpT2tvf3r0Y)\n\nWe've been developing it for 2.5 years and sharing our prototype with a broad audience for the first time, so I'll be grateful for your feedback and happy to answer questions!", "link": "https://www.reddit.com/r/datascience/comments/m6p3su/introducing_kamu_worlds_first_global/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "introducing kamu - world's first global collaborative data pipeline /!/ [kamu](https://github.com/kamu-data/kamu-cli)\u00a0is a new data management -----> tool !!!  for rapid exchange and global collaboration on structured data.\n\nit can be described as:\n\n* global supply chain for structured data\n* decentralized stream-processing data pipeline\n* git for data (think collaboration, not diffs/branches)\n* blockchain for big data\n\nit\u2019s an ambitious project that takes a very different perspective at\u00a0*what data is*\u00a0and how it should to be handled.\n\nplease check out:\n\n* [quick project intro](https://medium.com/kamu-data/kamu-update-were-in-alpha-8c86e4c4f1a9)\n* [introductory post to open data fabric](https://medium.com/kamu-data/introducing-open-data-fabric-eaf9fdcd3903) \\- an open protocol the tool is based on\n* [the demo video](http://www.youtube.com/watch?v=upt2tvf3r0y)\n\nwe've been developing it for 2.5 years and sharing our prototype with a broad audience for the first time, so i'll be grateful for your feedback and happy to answer questions!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/m6p3su/introducing_kamu_worlds_first_global/',)", "identifyer": 5590211, "year": "2021"}, {"autor": "wuyuanyi135", "date": 1615938136000, "content": "A simple Jupyter/IPython magic extension to help reuse the cell /!/ Hello,\n\nAfter googling the similar concept without success, I wrote a simple magic that will automate the cell -&gt; reusable function process. I wish to hear your opinion of how you guys reuse the code in Jupyter notebook.\n\n# Long story\n\nThere is one thing that has bothered me for a while is to reuse the cell in Jupyter notebook. For example, I write some data processing code in a cell then realize that I need to parameterize a few variables and do a case study (for optimization or visualization). \n\nUsually what I do is to copy and paste the code to a new cell and change the necessary parts. This is bad for maintainability. For example, if I need to apply a modification I have to apply it to every copy. Also, the variable name conflict will overwrite the previous result. I have to refactor a lot of code to ensure the two versions can work in parallel. \n\nWrapping the cell as a function is the way to go, but debugging a function in Jupyter is much more difficult due to the variable scope and lack of robust debugging tools (it's getting better though). So, what I usually do is to first write the code in the cell. After everything is working, I put the function header, indent the code, write the return line and this cell becomes a function. Later when the function doesn't work properly, I comment out the header and return lines, then remove the indent, and start over from the pure cell code again.\n\nWhat I have been wishing for, is something like \"cell arguments\" that allows a cell to be reused like a function, by passing in a few arguments that override the behavior. Therefore, I create this tool that \"functionize\" a cell as a function so that it could be called later with the specified arguments. \n\nI would like to see your opinion about reusing the cell and reducing code duplication in Jupyter notebook. \n\nThank you!", "link": "https://www.reddit.com/r/datascience/comments/m6mqjx/a_simple_jupyteripython_magic_extension_to_help/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "a simple jupyter/ipython magic extension to help reuse the cell /!/ hello,\n\nafter googling the similar concept without success, i wrote a simple magic that will automate the cell -&gt; reusable function process. i wish to hear your opinion of how you guys reuse the code in jupyter notebook.\n\n# long story\n\nthere is one thing that has bothered me for a while is to reuse the cell in jupyter notebook. for example, i write some data processing code in a cell then realize that i need to parameterize a few variables and do a case study (for optimization or visualization). \n\nusually what i do is to copy and paste the code to a new cell and change the necessary parts. this is bad for maintainability. for example, if i need to apply a modification i have to apply it to every copy. also, the variable name conflict will overwrite the previous result. i have to refactor a lot of code to ensure the two versions can work in parallel. \n\nwrapping the cell as a function is the way to go, but debugging a function in jupyter is much more difficult due to the variable scope and lack of robust debugging tools (it's getting better though). so, what i usually do is to first write the code in the cell. after everything is working, i put the function header, indent the code, write the return line and this cell becomes a function. later when the function doesn't work properly, i comment out the header and return lines, then remove the indent, and start over from the pure cell code again.\n\nwhat i have been wishing for, is something like \"cell arguments\" that allows a cell to be reused like a function, by passing in a few arguments that override the behavior. therefore, i create this -----> tool !!!  that \"functionize\" a cell as a function so that it could be called later with the specified arguments. \n\ni would like to see your opinion about reusing the cell and reducing code duplication in jupyter notebook. \n\nthank you!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/m6mqjx/a_simple_jupyteripython_magic_extension_to_help/',)", "identifyer": 5590217, "year": "2021"}, {"autor": "blkcatsusie", "date": 1623542614000, "content": "Any advice for Engineering PhD who wants to get a data scientist position? /!/ I am PhD in Mechanical Engineering and currently work at a startup engineering software company. The company is experiencing a financial crisis so I should leave work very soon. So, I should find a new job and I am thinking of transitioning from engineer to data scientist. Please see my current status and provide me advice.\n\nMy graduate study and the current job responsibilities are actually very different from the conventional mechanical engineering job duties. Many of my job responsibilities are similar to those of data scientist. My specific area is 'optimization algorithm considering uncertainties in engineering system'. Statistics, probability, bayesian inference, regression, classification, stochastic process, gradient-based and heuristic optimization methods, and so on. I don't have deep understandings of mechanical engineering such as fluid mechanics and solid mechanics. Mine is just undergraduate level.\n\nHere is the summary about me:\n\n1. My main tool is not Python but Matlab because Matlab is the tool used in the mechanical engineering optimization community. I used R in my phd research but I am not fluent in R. I have limited experience in Python at work.\n2. My degrees are from Mechanical Engineering.\n3. Worked at one of three US automotive companies as a mechanical engineer.\n4. Currently research engineer at startup software company.\n5. Master's study was about control algorithm for autonomous vehicle\n6. Phd study was... very different from mechanical engineering. It was to find the best business solutions considering uncertainties in human choice behavior. Hmm.. I don't understand what I did in my phd research.\n7. I want to get a data scientist position at a traditional engineering company. For example, automotive or manufacturing. not software company or FAANG. No startup anymore. Never.\n\nSo, my plan is\n\n* Make the transition from Matlab to Python first.\n* and then, take Data Science Bootcamp (probably online courses)\n* I need to learn how to use Python libraries and SQL.\n\nI am thinking of spending up to 6 months on this transition. Please give me your valuable advice. Thank you!!", "link": "https://www.reddit.com/r/datascience/comments/nyk8bl/any_advice_for_engineering_phd_who_wants_to_get_a/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "any advice for engineering phd who wants to get a data scientist position? /!/ i am phd in mechanical engineering and currently work at a startup engineering software company. the company is experiencing a financial crisis so i should leave work very soon. so, i should find a new job and i am thinking of transitioning from engineer to data scientist. please see my current status and provide me advice.\n\nmy graduate study and the current job responsibilities are actually very different from the conventional mechanical engineering job duties. many of my job responsibilities are similar to those of data scientist. my specific area is 'optimization algorithm considering uncertainties in engineering system'. statistics, probability, bayesian inference, regression, classification, stochastic process, gradient-based and heuristic optimization methods, and so on. i don't have deep understandings of mechanical engineering such as fluid mechanics and solid mechanics. mine is just undergraduate level.\n\nhere is the summary about me:\n\n1. my main -----> tool !!!  is not python but matlab because matlab is the -----> tool !!!  used in the mechanical engineering optimization community. i used r in my phd research but i am not fluent in r. i have limited experience in python at work.\n2. my degrees are from mechanical engineering.\n3. worked at one of three us automotive companies as a mechanical engineer.\n4. currently research engineer at startup software company.\n5. master's study was about control algorithm for autonomous vehicle\n6. phd study was... very different from mechanical engineering. it was to find the best business solutions considering uncertainties in human choice behavior. hmm.. i don't understand what i did in my phd research.\n7. i want to get a data scientist position at a traditional engineering company. for example, automotive or manufacturing. not software company or faang. no startup anymore. never.\n\nso, my plan is\n\n* make the transition from matlab to python first.\n* and then, take data science bootcamp (probably online courses)\n* i need to learn how to use python libraries and sql.\n\ni am thinking of spending up to 6 months on this transition. please give me your valuable advice. thank you!!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nyk8bl/any_advice_for_engineering_phd_who_wants_to_get_a/',)", "identifyer": 5590290, "year": "2021"}, {"autor": "lljc00", "date": 1623538722000, "content": "Using Jupyter Notebook vs something else? /!/ Noob here.  I have very basic skills in Python using PyCharm.  \n\nI just picked up Python for Data Science for Dummies - was in the library (yeah, open for in-person browsing!) and it looked interesting.\n\nIn this book, the author uses Jupyter Notebook.  Before I go and install another program and head down the path of learning it, I'm wondering if this is the right tool to be using. \n\nMy goals: Well, I guess I'd just like to expand my knowledge of Python.  I don't use it for work or anything, yet...  I'd like to move into an FP&amp;A role and I know understanding Python is sometimes advantageous.  I do realize that doing data science with Python is probably more than would be needed in an FP&amp;A role, and that's OK.  I think I may just like to learn how to use Python more because I'm just a very analytical person by nature and maybe someday I'll use it to put together analyses of Coronavirus data.  But since I am new with learning coding languages, if Jupyter is good as a starting point, that's OK too.  Have to admit that the CLI screenshots in the book intimidated me,  but I'm OK learning it since I know CLI is kind of a part of being a techy and it's probably about time I got more comfortable with it.", "link": "https://www.reddit.com/r/datascience/comments/nyizb5/using_jupyter_notebook_vs_something_else/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "using jupyter notebook vs something else? /!/ noob here.  i have very basic skills in python using pycharm.  \n\ni just picked up python for data science for dummies - was in the library (yeah, open for in-person browsing!) and it looked interesting.\n\nin this book, the author uses jupyter notebook.  before i go and install another program and head down the path of learning it, i'm wondering if this is the right -----> tool !!!  to be using. \n\nmy goals: well, i guess i'd just like to expand my knowledge of python.  i don't use it for work or anything, yet...  i'd like to move into an fp&amp;a role and i know understanding python is sometimes advantageous.  i do realize that doing data science with python is probably more than would be needed in an fp&amp;a role, and that's ok.  i think i may just like to learn how to use python more because i'm just a very analytical person by nature and maybe someday i'll use it to put together analyses of coronavirus data.  but since i am new with learning coding languages, if jupyter is good as a starting point, that's ok too.  have to admit that the cli screenshots in the book intimidated me,  but i'm ok learning it since i know cli is kind of a part of being a techy and it's probably about time i got more comfortable with it.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 112, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nyizb5/using_jupyter_notebook_vs_something_else/',)", "identifyer": 5590292, "year": "2021"}, {"autor": "quite--average", "date": 1623431672000, "content": "To what extent should you lie on your resume and have you done that? /!/ Hello!\n\nPlease hear me out before bashing or downvoting.\n\nHere's an example of a situation currently at work:\n\nThere's a excel workbook and my senior made an excel macro that pulls data from all the sheets and then basically transforms the data from wide to long. A quarterly report is made using a not so popular BI tool from the macro generated data. This report is then shown to the executives. Mind you, this is an extremely important report\n\nNow, you people know in the industry no one cares about excel macros and a random BI tool. I want to do everything in Python from using pandas to dash plotly. \nI have done the pandas part already. \n\nProblem is that I'm pretty sure this is going to be shot down and the seniors would want to keep things the old way. So this Python part won't really be used by anyone.\n\nHere comes the lying part, Can I write on my resume that I used pandas and dashplotly to make a quarterly report for executives? I'm doing this to learn new stuff on real data and as you know how the recruitment system is now a days, they want to see \"pandas\" on your resume or else the resume is in the trash.\n\nThanks!", "link": "https://www.reddit.com/r/datascience/comments/nxl3s7/to_what_extent_should_you_lie_on_your_resume_and/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "to what extent should you lie on your resume and have you done that? /!/ hello!\n\nplease hear me out before bashing or downvoting.\n\nhere's an example of a situation currently at work:\n\nthere's a excel workbook and my senior made an excel macro that pulls data from all the sheets and then basically transforms the data from wide to long. a quarterly report is made using a not so popular bi -----> tool !!!  from the macro generated data. this report is then shown to the executives. mind you, this is an extremely important report\n\nnow, you people know in the industry no one cares about excel macros and a random bi tool. i want to do everything in python from using pandas to dash plotly. \ni have done the pandas part already. \n\nproblem is that i'm pretty sure this is going to be shot down and the seniors would want to keep things the old way. so this python part won't really be used by anyone.\n\nhere comes the lying part, can i write on my resume that i used pandas and dashplotly to make a quarterly report for executives? i'm doing this to learn new stuff on real data and as you know how the recruitment system is now a days, they want to see \"pandas\" on your resume or else the resume is in the trash.\n\nthanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 8, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nxl3s7/to_what_extent_should_you_lie_on_your_resume_and/',)", "identifyer": 5590328, "year": "2021"}, {"autor": "hungry4hungary", "date": 1620973888000, "content": "Looking for a little help with my survey /!/ Hey everybody,\n\nPlease please please fill out my survey so I can graduate!\n\nLink to the [SURVEY](https://erasmusuniversity.eu.qualtrics.com/jfe/form/SV_a5x9eEFWUtcBS98) &lt;--\n\nI\u2019m doing my master's at Erasmus University Rotterdam. \nTo be able to graduate I\u2019m currently trying to collect data from people who work with or study predictive analytics, machine learning, or something connected to the topic.\nMy research is about underlying biases in algorithms that discriminate against minority groups. \n\nThere are many articles concerning the issue I will leave them here if you\u2019d like to read them:\n\n- [Reuters - Amazon scraps secret AI recruiting tool that showed bias against women](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G)\n- [Healthaffairs - Discrimination By Artificial Intelligence In A Commercial Electronic Health Record\u2014A Case Study](https://www.healthaffairs.org/do/10.1377/hblog20200128.626576/full/)\n- [Reuters - U.S. government study finds racial bias in facial recognition tools](https://www.reuters.com/article/us-usa-crime-face-idUSKBN1YN2V1)\n\nI am so close to acquiring the required amount of responses so each response is greatly appreciated! :)", "link": "https://www.reddit.com/r/datascience/comments/nc223j/looking_for_a_little_help_with_my_survey/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "looking for a little help with my survey /!/ hey everybody,\n\nplease please please fill out my survey so i can graduate!\n\nlink to the [survey](https://erasmusuniversity.eu.qualtrics.com/jfe/form/sv_a5x9eefwutcbs98) &lt;--\n\ni\u2019m doing my master's at erasmus university rotterdam. \nto be able to graduate i\u2019m currently trying to collect data from people who work with or study predictive analytics, machine learning, or something connected to the topic.\nmy research is about underlying biases in algorithms that discriminate against minority groups. \n\nthere are many articles concerning the issue i will leave them here if you\u2019d like to read them:\n\n- [reuters - amazon scraps secret ai recruiting -----> tool !!!  that showed bias against women](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-iduskcn1mk08g)\n- [healthaffairs - discrimination by artificial intelligence in a commercial electronic health record\u2014a case study](https://www.healthaffairs.org/do/10.1377/hblog20200128.626576/full/)\n- [reuters - u.s. government study finds racial bias in facial recognition tools](https://www.reuters.com/article/us-usa-crime-face-iduskbn1yn2v1)\n\ni am so close to acquiring the required amount of responses so each response is greatly appreciated! :)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nc223j/looking_for_a_little_help_with_my_survey/',)", "identifyer": 5590332, "year": "2021"}, {"autor": "Jaypal17", "date": 1620919160000, "content": "Sentiment Analysis Recommendations on Review data /!/ I'm looking for recommendations on my project, currently, we have a couple of hundred rows of health care review data. My project manager wants me to find a sentiment analysis tool that gives a compound score that correlates accurately to the stars given for the review. My first attempt I used vaderSentiment and it was around 55% accurate at the score to start rating, my second attempt I used texBlob and that was less accurate (35%). I want to know if there is any off the shelf models or other libraries I can use with python, especially if it understand Healthcare lingo. We hope that we can find something that is about 60-70% accurate from compound score to star rating. Eventually, we will build our own model once we have more data and time. For now, we just want to demo the data we have. Also if you think I'm going about this all wrong please let me know. I am relatively new to data science and this is a part-time project for my job.", "link": "https://www.reddit.com/r/datascience/comments/nbj29s/sentiment_analysis_recommendations_on_review_data/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "sentiment analysis recommendations on review data /!/ i'm looking for recommendations on my project, currently, we have a couple of hundred rows of health care review data. my project manager wants me to find a sentiment analysis -----> tool !!!  that gives a compound score that correlates accurately to the stars given for the review. my first attempt i used vadersentiment and it was around 55% accurate at the score to start rating, my second attempt i used texblob and that was less accurate (35%). i want to know if there is any off the shelf models or other libraries i can use with python, especially if it understand healthcare lingo. we hope that we can find something that is about 60-70% accurate from compound score to star rating. eventually, we will build our own model once we have more data and time. for now, we just want to demo the data we have. also if you think i'm going about this all wrong please let me know. i am relatively new to data science and this is a part-time project for my job.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 10, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nbj29s/sentiment_analysis_recommendations_on_review_data/',)", "identifyer": 5590355, "year": "2021"}, {"autor": "KiwiD_1618", "date": 1634231460000, "content": "ETL and ELT /!/ Ok I mean I got it. I completely understand what they are from the experience I have. But does it require any specific tool? Like SQL or some other tools that ITs use? Does the following procedure consider an ETL pipeline?\n\nUsing R/Python download raw data from multiple clients through sftp servers, manipulate the data so that all have the same structure, save them into a database (nothing special - just csv txt or rds files), when the time comes open the files on R/Python and make various of statistical analysis, export specifically structure outputs that can fit very well on a BI tool and transfer them on the servers. All of this being automatically done.\n\nI see a lot of companies asking for ETL. Can I add on my CV that I have ETL experience based on the above procedures?", "link": "https://www.reddit.com/r/datascience/comments/q844ek/etl_and_elt/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "etl and elt /!/ ok i mean i got it. i completely understand what they are from the experience i have. but does it require any specific -----> tool !!! ? like sql or some other tools that its use? does the following procedure consider an etl pipeline?\n\nusing r/python download raw data from multiple clients through sftp servers, manipulate the data so that all have the same structure, save them into a database (nothing special - just csv txt or rds files), when the time comes open the files on r/python and make various of statistical analysis, export specifically structure outputs that can fit very well on a bi tool and transfer them on the servers. all of this being automatically done.\n\ni see a lot of companies asking for etl. can i add on my cv that i have etl experience based on the above procedures?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 15, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/q844ek/etl_and_elt/',)", "identifyer": 5590449, "year": "2021"}, {"autor": "Sonny-Orkidea", "date": 1634199763000, "content": "how to get data from API automatically and make a custom dashboard for myself? /!/ Hi everyone, We are working with CDP tool, which has its own database (limited), data structure and you can create models, custom dashboards,reports, funnels for customer behaviors etc.\n\ni miss 2 things: correlations and dashboard for data check (daily check of tracked events etc.). I know some pandas, matplotlib, seaborn fundamentals and i can code my own dashboard/report for this, but i dont know where, what tool to use - i know only jupyter notebook, and how to deploy it. how to automatically get data everyday from API etc.\n\nCan you show me some directions? Deploying is not necessary, it can be just on my pc for a start.", "link": "https://www.reddit.com/r/datascience/comments/q7vemx/how_to_get_data_from_api_automatically_and_make_a/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "how to get data from api automatically and make a custom dashboard for myself? /!/ hi everyone, we are working with cdp -----> tool !!! , which has its own database (limited), data structure and you can create models, custom dashboards,reports, funnels for customer behaviors etc.\n\ni miss 2 things: correlations and dashboard for data check (daily check of tracked events etc.). i know some pandas, matplotlib, seaborn fundamentals and i can code my own dashboard/report for this, but i dont know where, what tool to use - i know only jupyter notebook, and how to deploy it. how to automatically get data everyday from api etc.\n\ncan you show me some directions? deploying is not necessary, it can be just on my pc for a start.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/q7vemx/how_to_get_data_from_api_automatically_and_make_a/',)", "identifyer": 5590472, "year": "2021"}, {"autor": "no-mans-tech", "date": 1627430420000, "content": "Google Cloud Releases The Data Validation Tool (DVT), An Open Sourced Python CLI Tool That Provides An Automated Solution For Validation Across Different Environments /!/ Data validation is an essential step of any data migration project. It ensures that the new system will be able to read and map all necessary fields as they are migrated from one table into another, making sure your datasets always match up correctly with each other before proceeding on to the next phase of a process.\n\nGoogle announced the launch of a new open-source Python command-line tool, [Data Validation Tool](https://github.com/GoogleCloudPlatform/professional-services-data-validator)\u00a0 (DVT), to automate data verification across different environments. The [Data Validation Tool](https://github.com/GoogleCloudPlatform/professional-services-data-validator)\u00a0 (DVT) uses Ibis to connect with large databases such as BigQuery, Cloud Spanner and more.\n\nQuick Read: [https://techsucker.news/2021/07/27/google-cloud-releases-the-data-validation-tool-dvt-an-open-sourced-python-cli-tool-that-provides-an-automated-solution-for-validation-across-different-environments/](https://techsucker.news/2021/07/27/google-cloud-releases-the-data-validation-tool-dvt-an-open-sourced-python-cli-tool-that-provides-an-automated-solution-for-validation-across-different-environments/)\n\nGoogle Blog: https://cloud.google.com/blog/products/databases/automate-data-validation-with-dvt", "link": "https://www.reddit.com/r/datascience/comments/osyfqe/google_cloud_releases_the_data_validation_tool/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "google cloud releases the data validation -----> tool !!!  (dvt), an open sourced python cli -----> tool !!!  that provides an automated solution for validation across different environments /!/ data validation is an essential step of any data migration project. it ensures that the new system will be able to read and map all necessary fields as they are migrated from one table into another, making sure your datasets always match up correctly with each other before proceeding on to the next phase of a process.\n\ngoogle announced the launch of a new open-source python command-line tool, [data validation tool](https://github.com/googlecloudplatform/professional-services-data-validator)\u00a0 (dvt), to automate data verification across different environments. the [data validation tool](https://github.com/googlecloudplatform/professional-services-data-validator)\u00a0 (dvt) uses ibis to connect with large databases such as bigquery, cloud spanner and more.\n\nquick read: [https://techsucker.news/2021/07/27/google-cloud-releases-the-data-validation-tool-dvt-an-open-sourced-python-cli-tool-that-provides-an-automated-solution-for-validation-across-different-environments/](https://techsucker.news/2021/07/27/google-cloud-releases-the-data-validation-tool-dvt-an-open-sourced-python-cli-tool-that-provides-an-automated-solution-for-validation-across-different-environments/)\n\ngoogle blog: https://cloud.google.com/blog/products/databases/automate-data-validation-with-dvt", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/osyfqe/google_cloud_releases_the_data_validation_tool/',)", "identifyer": 5590538, "year": "2021"}, {"autor": "VictorAVB", "date": 1627392220000, "content": "How to Create a Price Comparison Tool With Python- Beautiful Soup /!/ Hi Guys, I thought I would share as I get a alot of requests for how to do this. Check out my step by step guide", "link": "https://www.reddit.com/r/datascience/comments/osmr7p/how_to_create_a_price_comparison_tool_with_python/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "how to create a price comparison -----> tool !!!  with python- beautiful soup /!/ hi guys, i thought i would share as i get a alot of requests for how to do this. check out my step by step guide", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/osmr7p/how_to_create_a_price_comparison_tool_with_python/',)", "identifyer": 5590560, "year": "2021"}, {"autor": "DataChaz", "date": 1627337058000, "content": "Generate FAQs for your pages automatically with What The FAQ, a new mighty Streamlit app by DataChaz! /!/ Wouldn't it be nice to auto-generate qualitative FAQs for your pages?\n\nWhat The FAQ leverages the power of [Huggingface Transformers](https://huggingface.co/transformers/) &amp; [Google T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) &amp; to generate quality question &amp; answer pairs from URLs!\n\nTry the app [here](https://share.streamlit.io/charlywargnier/what-the-faq/main/app.py) and read the [blog post](https://www.charlywargnier.com/post/wtfaq-an-mighty-app-to-generate-quality-question-answer-pairs)!\n\nHow to use it:\n\n1. Input a URL and the tool will suggest Q&amp;As\n2. Select the Questions and answers that \\*make sense\\*\n3. Download the Q&amp;As to create your FAQs!", "link": "https://www.reddit.com/r/datascience/comments/os9an5/generate_faqs_for_your_pages_automatically_with/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "generate faqs for your pages automatically with what the faq, a new mighty streamlit app by datachaz! /!/ wouldn't it be nice to auto-generate qualitative faqs for your pages?\n\nwhat the faq leverages the power of [huggingface transformers](https://huggingface.co/transformers/) &amp; [google t5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) &amp; to generate quality question &amp; answer pairs from urls!\n\ntry the app [here](https://share.streamlit.io/charlywargnier/what-the-faq/main/app.py) and read the [blog post](https://www.charlywargnier.com/post/wtfaq-an-mighty-app-to-generate-quality-question-answer-pairs)!\n\nhow to use it:\n\n1. input a url and the -----> tool !!!  will suggest q&amp;as\n2. select the questions and answers that \\*make sense\\*\n3. download the q&amp;as to create your faqs!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/os9an5/generate_faqs_for_your_pages_automatically_with/',)", "identifyer": 5590584, "year": "2021"}, {"autor": "quite--average", "date": 1627327665000, "content": "Data Scientists, is every bullet point on your resume a Machine Learning related task/project? /!/ Hello!\n\nI'm a BI Developer with one year of experience after Masters (Major: Statistics). I'm trying to land a junior data scientist role but have not been able to get any call backs. I'm guessing the problem is that my resume has only 1 Machine learning related work project and one school project (didn't list more because I have read school projects matter much). Rest of the bullet points include, using Pandas/ Tidyverse for analyzing data and finding why some part of business process this week went wrong (over simplifying) or building dashboards using a BI tool after data wrangling. Another reason I'm guessing is that I'm on a visa. \n\nI was wondering if you guys could tell me if all of your tasks/projects on your resume are related to machine learning. If not, what other kind of projects can help land a junior data scientist role?\n\nThanks!", "link": "https://www.reddit.com/r/datascience/comments/os62h7/data_scientists_is_every_bullet_point_on_your/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "data scientists, is every bullet point on your resume a machine learning related task/project? /!/ hello!\n\ni'm a bi developer with one year of experience after masters (major: statistics). i'm trying to land a junior data scientist role but have not been able to get any call backs. i'm guessing the problem is that my resume has only 1 machine learning related work project and one school project (didn't list more because i have read school projects matter much). rest of the bullet points include, using pandas/ tidyverse for analyzing data and finding why some part of business process this week went wrong (over simplifying) or building dashboards using a bi -----> tool !!!  after data wrangling. another reason i'm guessing is that i'm on a visa. \n\ni was wondering if you guys could tell me if all of your tasks/projects on your resume are related to machine learning. if not, what other kind of projects can help land a junior data scientist role?\n\nthanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/os62h7/data_scientists_is_every_bullet_point_on_your/',)", "identifyer": 5590596, "year": "2021"}, {"autor": "mwmwmw01", "date": 1627289413000, "content": "Rapid conversion of PDF tables to excel tables [I have researched - any help appreciated!] - what program? /!/ Hi all,\n\nFirst off, thanks for any help. I've tried to Google this to no end but I haven't found a great solution. **I'm looking for a program/solution to quickly convert pdf tables to excel data via a 'selection' tool of the table**\n\n**Context:** I do a lot of work with company Annual Reports. Although often digital versions of these reports are available, they are often inaccurate or incomplete. I often need to grab individual tables and put them in excel. But the workflow of doing this is pretty clunky.\n\n**Problem to solve:** Basically what I want to do is be reading a pdf, think \"I want this table\" in excel, and take a screenshot (ideally using a 'selection' tool) and have that copied to clipboard and then to Excel. \n\n**Question:** Any ideas on solutions? I'm aware of Adobe Professional but is pretty dear for only this functionality. Aware of Table Capture for Chrome which is awesome, but does not work for PDFs. There are multiple conversion tools online, but the workflow is very clunky to have to upload the document first, wait, download, then copy. I think there is something out there to do this, but just can't find on researching.\n\n**Apologies if I'm posting in the wrong place, please kindly redirect me if so.**\n\nThanks", "link": "https://www.reddit.com/r/datascience/comments/orutaw/rapid_conversion_of_pdf_tables_to_excel_tables_i/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "rapid conversion of pdf tables to excel tables [i have researched - any help appreciated!] - what program? /!/ hi all,\n\nfirst off, thanks for any help. i've tried to google this to no end but i haven't found a great solution. **i'm looking for a program/solution to quickly convert pdf tables to excel data via a 'selection' -----> tool !!!  of the table**\n\n**context:** i do a lot of work with company annual reports. although often digital versions of these reports are available, they are often inaccurate or incomplete. i often need to grab individual tables and put them in excel. but the workflow of doing this is pretty clunky.\n\n**problem to solve:** basically what i want to do is be reading a pdf, think \"i want this table\" in excel, and take a screenshot (ideally using a 'selection' tool) and have that copied to clipboard and then to excel. \n\n**question:** any ideas on solutions? i'm aware of adobe professional but is pretty dear for only this functionality. aware of table capture for chrome which is awesome, but does not work for pdfs. there are multiple conversion tools online, but the workflow is very clunky to have to upload the document first, wait, download, then copy. i think there is something out there to do this, but just can't find on researching.\n\n**apologies if i'm posting in the wrong place, please kindly redirect me if so.**\n\nthanks", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/orutaw/rapid_conversion_of_pdf_tables_to_excel_tables_i/',)", "identifyer": 5590614, "year": "2021"}, {"autor": "TimboCA", "date": 1621622017000, "content": "What is ONE single essential tool/program/skill that a new person absolutely must master when transitioning into a data science/analyst role? /!/ \n\n[View Poll](https://www.reddit.com/poll/nhzi43)", "link": "https://www.reddit.com/r/datascience/comments/nhzi43/what_is_one_single_essential_toolprogramskill/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what is one single essential -----> tool !!! /program/skill that a new person absolutely must master when transitioning into a data science/analyst role? /!/ \n\n[view poll](https://www.reddit.com/poll/nhzi43)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 14, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nhzi43/what_is_one_single_essential_toolprogramskill/',)", "identifyer": 5590653, "year": "2021"}, {"autor": "kvnhn", "date": 1630967360000, "content": "Dud: a lightweight tool for versioning data alongside source code and building data pipelines.", "link": "https://www.reddit.com/r/datascience/comments/pja3ea/dud_a_lightweight_tool_for_versioning_data/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "dud: a lightweight -----> tool !!!  for versioning data alongside source code and building data pipelines.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('link',)", "medialink": "('https://github.com/kevin-hanselman/dud',)", "identifyer": 5590734, "year": "2021"}, {"autor": "BombastusBlomquist", "date": 1630953700000, "content": "Problems with the evaluation of code analysis tools (Classification Problem) /!/ So I am relatively new to datascience and hope this is a valid question for this subreddit. I am currently working on evaluating so called Static Analysis Security Tools for a project and want to compare their performance. This turns out to not be as trivial as I first thought.\n\n These tools scan program code and return, without running the code, places which could be relevant for security concerns. For this purpose I am using the [Software Assurance Reference Dataset (SARD)](https://samate.nist.gov/SRD/testsuite.php) which contains code pieces and the corresponding CWE number (which stands for Common Weakness Enumeration and is a way of categorizing different types of vulnerabilities) for each error. They can be looked up [here](https://cwe.mitre.org/). In the dataset I use, every file contains either exactly one error, or no error at all and the correct CWE number is written into the file name. So if tool X scans the file called CWE\\_89\\_blabla.php, it should ideally return only the CWE-89 error. There are also files which are deliberate error free examples in order to be able to test for true negative results.\n\nThe problem I'm facing right now is, that each of the tools do often times return multiple results for every file, effectively ruining the statistic, even if they do return the correct error category within these results.\n\nAdditionally there is a decent chance that the multiple other returned values for a single file are not false positives at all. For instance, having a password or username in the program code is considered an error which is of course a valid concern but this is not reflected in the dataset since each files is supposed to only contain one error. Thus I cannot know whether the other returned values are valid if I do not look manually at all about 65000 results (per tool), which is just not possible.\n\nI then mark every result as the corresponding result class (true positive, false positive, true negative and false negative) and calculated the metrics accuracy, precision, recall and the f-measure. The resulting values are worse than I thought, with much higher false positive rates than is reasonably practical. I suspect the many multiple outputs per file to be the problem here. Is there maybe a better metric to use for data like this?\n\nAlso, what if I want to create a confusion matrix? There I compare only one value from the test dataset with one prediction of the tool, but since the tool returns multiple results per point in the dataset, the number of results returned by the tool and the size of the original dataset are different. I could however repeat each value in the dataset as many times as there are results for the value.\n\nFurthermore, I noticed that the dataset is also imbalanced. It has about 12000 instances for true positive results and about 29000 instances for true negative results. I know this does have an influence in some cases, so is this also relevant in this case? Does it influence the validity of the results if every tool gets the same dataset to scan?\n\nI thought about maybe consolidating multiple results per file into one, meaning, for instance, if there is a correct result within the set of returned results, then this would be registered as a true positive whereas, if there were only false results it would be registered as a false positive. Would maybe weighting the values be a good approach?\n\nAlso keep in mind that every tool I want to look at does return different amounts of results for each file.\n\nI hope this was at least somewhat coherent and if you have some pointers for me, it would be very much welcome. Even just some keywords for me to search for would be appreciated. Also if there is a better approach for this kind of problem, please let me know!", "link": "https://www.reddit.com/r/datascience/comments/pj5lao/problems_with_the_evaluation_of_code_analysis/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "problems with the evaluation of code analysis tools (classification problem) /!/ so i am relatively new to datascience and hope this is a valid question for this subreddit. i am currently working on evaluating so called static analysis security tools for a project and want to compare their performance. this turns out to not be as trivial as i first thought.\n\n these tools scan program code and return, without running the code, places which could be relevant for security concerns. for this purpose i am using the [software assurance reference dataset (sard)](https://samate.nist.gov/srd/testsuite.php) which contains code pieces and the corresponding cwe number (which stands for common weakness enumeration and is a way of categorizing different types of vulnerabilities) for each error. they can be looked up [here](https://cwe.mitre.org/). in the dataset i use, every file contains either exactly one error, or no error at all and the correct cwe number is written into the file name. so if -----> tool !!!  x scans the file called cwe\\_89\\_blabla.php, it should ideally return only the cwe-89 error. there are also files which are deliberate error free examples in order to be able to test for true negative results.\n\nthe problem i'm facing right now is, that each of the tools do often times return multiple results for every file, effectively ruining the statistic, even if they do return the correct error category within these results.\n\nadditionally there is a decent chance that the multiple other returned values for a single file are not false positives at all. for instance, having a password or username in the program code is considered an error which is of course a valid concern but this is not reflected in the dataset since each files is supposed to only contain one error. thus i cannot know whether the other returned values are valid if i do not look manually at all about 65000 results (per tool), which is just not possible.\n\ni then mark every result as the corresponding result class (true positive, false positive, true negative and false negative) and calculated the metrics accuracy, precision, recall and the f-measure. the resulting values are worse than i thought, with much higher false positive rates than is reasonably practical. i suspect the many multiple outputs per file to be the problem here. is there maybe a better metric to use for data like this?\n\nalso, what if i want to create a confusion matrix? there i compare only one value from the test dataset with one prediction of the tool, but since the tool returns multiple results per point in the dataset, the number of results returned by the tool and the size of the original dataset are different. i could however repeat each value in the dataset as many times as there are results for the value.\n\nfurthermore, i noticed that the dataset is also imbalanced. it has about 12000 instances for true positive results and about 29000 instances for true negative results. i know this does have an influence in some cases, so is this also relevant in this case? does it influence the validity of the results if every tool gets the same dataset to scan?\n\ni thought about maybe consolidating multiple results per file into one, meaning, for instance, if there is a correct result within the set of returned results, then this would be registered as a true positive whereas, if there were only false results it would be registered as a false positive. would maybe weighting the values be a good approach?\n\nalso keep in mind that every tool i want to look at does return different amounts of results for each file.\n\ni hope this was at least somewhat coherent and if you have some pointers for me, it would be very much welcome. even just some keywords for me to search for would be appreciated. also if there is a better approach for this kind of problem, please let me know!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pj5lao/problems_with_the_evaluation_of_code_analysis/',)", "identifyer": 5590741, "year": "2021"}, {"autor": "KDeven2000", "date": 1630785852000, "content": "What Data Analysis Tools Support Data Extraction from Word Documents? /!/ I was assigned the Data Analysis part of an Intelligent Network research. It focuses on developing a **Database System for Intelligent Networks**. It's based on the **Big Data** concept.\n\nI was told to find some **ELT(Extract-Load-Transform)** **tools** to extract data (mostly **unstructured** data) from **MS Word Documents** for **Data Analysis and Visualisation** purposes. The tool should also integrate with a platform called **Hadoop**.\n\nMost of the tools I found focused on the **ETL(Extract-Transform-Load)** concept. They did not emphasise on the **ELT** concept much, and did not mention any option to extract data from **Word Documents** specifically.\n\nDoes anyone know any tools that can do all that?", "link": "https://www.reddit.com/r/datascience/comments/phylhq/what_data_analysis_tools_support_data_extraction/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what data analysis tools support data extraction from word documents? /!/ i was assigned the data analysis part of an intelligent network research. it focuses on developing a **database system for intelligent networks**. it's based on the **big data** concept.\n\ni was told to find some **elt(extract-load-transform)** **tools** to extract data (mostly **unstructured** data) from **ms word documents** for **data analysis and visualisation** purposes. the -----> tool !!!  should also integrate with a platform called **hadoop**.\n\nmost of the tools i found focused on the **etl(extract-transform-load)** concept. they did not emphasise on the **elt** concept much, and did not mention any option to extract data from **word documents** specifically.\n\ndoes anyone know any tools that can do all that?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/phylhq/what_data_analysis_tools_support_data_extraction/',)", "identifyer": 5590798, "year": "2021"}, {"autor": "KDeven2000", "date": 1630785188000, "content": "What are the Data Analysis and Visualisation tools that specifically support data extraction from Word documents? /!/ I was assigned the Data Analysis part of an Intelligent Network research. It focuses on developing a **Database System for Intelligent Networks**. It's based on the **Big Data** concept.\n\nI was told to find some **ELT(Extract-Load-Transform)** **tools** to extract data (mostly **unstructured** data) from **MS Word Documents** for **Data Analysis and Visualisation** purposes. The tool should also integrate with a platform called **Hadoop**.\n\nI'm new to this subject and I haven't had any experience on **Data Analysis/Visualisation** or so called **ELT tools** before. I searched on the internet, but most of the tools I found focused on the **ETL(Extract-Transform-Load)** concept. They did not emphasise on the **ELT** concept much, and did not mention any option to extract data from **Word Documents** specifically.\n\nDoes anyone know any tools that can do all that?", "link": "https://www.reddit.com/r/datascience/comments/phyeh9/what_are_the_data_analysis_and_visualisation/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what are the data analysis and visualisation tools that specifically support data extraction from word documents? /!/ i was assigned the data analysis part of an intelligent network research. it focuses on developing a **database system for intelligent networks**. it's based on the **big data** concept.\n\ni was told to find some **elt(extract-load-transform)** **tools** to extract data (mostly **unstructured** data) from **ms word documents** for **data analysis and visualisation** purposes. the -----> tool !!!  should also integrate with a platform called **hadoop**.\n\ni'm new to this subject and i haven't had any experience on **data analysis/visualisation** or so called **elt tools** before. i searched on the internet, but most of the tools i found focused on the **etl(extract-transform-load)** concept. they did not emphasise on the **elt** concept much, and did not mention any option to extract data from **word documents** specifically.\n\ndoes anyone know any tools that can do all that?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/phyeh9/what_are_the_data_analysis_and_visualisation/',)", "identifyer": 5590799, "year": "2021"}, {"autor": "SubtleCoconut", "date": 1626103561000, "content": "A/B testing skillset catch-22 /!/ Hi all,\n\nI\u2019m trying to transition to a role in the business analytics space while my current job is mostly data wrangling. I recently got rejected from a position due to my lack of experience with A/B testing, even though I felt like I did a pretty good job understanding and completing their take-home assignment on it. Normally, when I don\u2019t have experience in a certain tool/process, I\u2019d start a personal project involving it. However, with A/B testing, I\u2019m not 100% sure where to begin. I don\u2019t really have the ability to conduct experiments and collect a bunch of customer data based on adjusted user experiences on a website/app. Does anyone have any suggestions on how I can improve my skillset in this area without on-the-job experience? Thanks!", "link": "https://www.reddit.com/r/datascience/comments/oitjak/ab_testing_skillset_catch22/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "a/b testing skillset catch-22 /!/ hi all,\n\ni\u2019m trying to transition to a role in the business analytics space while my current job is mostly data wrangling. i recently got rejected from a position due to my lack of experience with a/b testing, even though i felt like i did a pretty good job understanding and completing their take-home assignment on it. normally, when i don\u2019t have experience in a certain -----> tool !!! /process, i\u2019d start a personal project involving it. however, with a/b testing, i\u2019m not 100% sure where to begin. i don\u2019t really have the ability to conduct experiments and collect a bunch of customer data based on adjusted user experiences on a website/app. does anyone have any suggestions on how i can improve my skillset in this area without on-the-job experience? thanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/oitjak/ab_testing_skillset_catch22/',)", "identifyer": 5590890, "year": "2021"}, {"autor": "ploomber-io", "date": 1610470713000, "content": "Open-source Workflow Management Tools: A Survey /!/ There are a lot workflow orchestrators available for ML/DS projects. I reviewed the most popular ones and found important differences, hope this helps you find the best tool for the job! \n\nReviewed tools (in alphabetical order):\n\nAirflow, Dagster, DVC (Pipelines), Elyra, Flyte, Kale, Kedro, Kubeflow pipelines, Luigi, Metaflow, Ploomber, Prefect, TFX.\n\n[https://ploomber.io/posts/survey/](https://ploomber.io/posts/survey/)", "link": "https://www.reddit.com/r/datascience/comments/kvvj97/opensource_workflow_management_tools_a_survey/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "open-source workflow management tools: a survey /!/ there are a lot workflow orchestrators available for ml/ds projects. i reviewed the most popular ones and found important differences, hope this helps you find the best -----> tool !!!  for the job! \n\nreviewed tools (in alphabetical order):\n\nairflow, dagster, dvc (pipelines), elyra, flyte, kale, kedro, kubeflow pipelines, luigi, metaflow, ploomber, prefect, tfx.\n\n[https://ploomber.io/posts/survey/](https://ploomber.io/posts/survey/)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/kvvj97/opensource_workflow_management_tools_a_survey/',)", "identifyer": 5590978, "year": "2021"}, {"autor": "HugoRAS", "date": 1610443779000, "content": "What do people use BI Tools for /!/ Hi all, we're playing around with powerBI and it seems that BI tools in general (not specifically powerBI) can be used for everything from data exploration to business logic.\n\nOur initial hands-on experience isn't quite as rosy though - it's struggling to load some fairly modest datasets (12M rows, 8 small columns), which our existing tools would be fine with (jupyter for analysis flask or dash for presenting dashboards).\n\nCan I ask you all what you use powerBI or other BI Tools for?\n\n* Firstly, do you use them at all?\n* Do you use them to present data that's already been analysed (eg. summary stats saved to a table, and then displayed using the BI Tool)\n* Do you use them to do analysis (eg. load the whole dataset into the bi tool, and then put the analysis logic in the BI Tool)\n* Do you put business logic into them\n* Do you host APIs from them?\n* What's the effective size limit - the number of rows or gigabytes - where you wouldn't really expect the tool to work well?\n* Do you hate it or love it and if so why?", "link": "https://www.reddit.com/r/datascience/comments/kvogm6/what_do_people_use_bi_tools_for/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what do people use bi tools for /!/ hi all, we're playing around with powerbi and it seems that bi tools in general (not specifically powerbi) can be used for everything from data exploration to business logic.\n\nour initial hands-on experience isn't quite as rosy though - it's struggling to load some fairly modest datasets (12m rows, 8 small columns), which our existing tools would be fine with (jupyter for analysis flask or dash for presenting dashboards).\n\ncan i ask you all what you use powerbi or other bi tools for?\n\n* firstly, do you use them at all?\n* do you use them to present data that's already been analysed (eg. summary stats saved to a table, and then displayed using the bi -----> tool !!! )\n* do you use them to do analysis (eg. load the whole dataset into the bi tool, and then put the analysis logic in the bi tool)\n* do you put business logic into them\n* do you host apis from them?\n* what's the effective size limit - the number of rows or gigabytes - where you wouldn't really expect the tool to work well?\n* do you hate it or love it and if so why?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 5, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/kvogm6/what_do_people_use_bi_tools_for/',)", "identifyer": 5590993, "year": "2021"}, {"autor": "Distinct_Scallion850", "date": 1610407888000, "content": "obscure data science tricks? /!/ What's your favorite data science trick, hack, tool, software, coding practice, etc. that they'll never teach in school! Super curious to know what DS practitioners have discovered that's likely more pragmatically useful than a college class.", "link": "https://www.reddit.com/r/datascience/comments/kvf2dr/obscure_data_science_tricks/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "obscure data science tricks? /!/ what's your favorite data science trick, hack, -----> tool !!! , software, coding practice, etc. that they'll never teach in school! super curious to know what ds practitioners have discovered that's likely more pragmatically useful than a college class.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 5, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/kvf2dr/obscure_data_science_tricks/',)", "identifyer": 5591017, "year": "2021"}, {"autor": "basicallybrittt", "date": 1610397308000, "content": "Best ways to deploy Machine Learning models /!/ I\u2019m a Data Scientist and have recently built a text classification tool that has been put into production using the AWS sagemaker infrastructure. \nI was just wondering how the rest of y\u2019all deploy your models in a production environment and if you have any tips for a n00b. I\u2019d like to explore some alternative deployment techniques this year so I\u2019m very interested to hear how other people do this!", "link": "https://www.reddit.com/r/datascience/comments/kvbg3y/best_ways_to_deploy_machine_learning_models/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "best ways to deploy machine learning models /!/ i\u2019m a data scientist and have recently built a text classification -----> tool !!!  that has been put into production using the aws sagemaker infrastructure. \ni was just wondering how the rest of y\u2019all deploy your models in a production environment and if you have any tips for a n00b. i\u2019d like to explore some alternative deployment techniques this year so i\u2019m very interested to hear how other people do this!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 12, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/kvbg3y/best_ways_to_deploy_machine_learning_models/',)", "identifyer": 5591027, "year": "2021"}, {"autor": "TellMeAreYouFree", "date": 1626893065000, "content": "Migrating analysis from Jupyter Notebook to _______? What's next? /!/ Relatively new to DS techniques but I've built a very useful tool for my sales team using Jupyter Notebook/python.\n\nI just installed VS Code and want to learn how to use the IDE.  What is a useful next step for   \n(1) me continuing to learn python/DS (creating modules/.py files/structuring a real application, defining reusable functions and classes, etc), and   \n(2) delivering useful information to my team (who struggle to even use excel, let alone a Jupyter notebook)\n\nIf I transition to a .py file(s) that is executed all at once, what are my options for \"displaying\" summary data frames and tables to the end user?  I currently have them display after cells in the notebook, and then I have to screenshot them and email them to the end user. ugh. not ideal, I know!", "link": "https://www.reddit.com/r/datascience/comments/oow7u1/migrating_analysis_from_jupyter_notebook_to_whats/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "migrating analysis from jupyter notebook to _______? what's next? /!/ relatively new to ds techniques but i've built a very useful -----> tool !!!  for my sales team using jupyter notebook/python.\n\ni just installed vs code and want to learn how to use the ide.  what is a useful next step for   \n(1) me continuing to learn python/ds (creating modules/.py files/structuring a real application, defining reusable functions and classes, etc), and   \n(2) delivering useful information to my team (who struggle to even use excel, let alone a jupyter notebook)\n\nif i transition to a .py file(s) that is executed all at once, what are my options for \"displaying\" summary data frames and tables to the end user?  i currently have them display after cells in the notebook, and then i have to screenshot them and email them to the end user. ugh. not ideal, i know!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 7, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/oow7u1/migrating_analysis_from_jupyter_notebook_to_whats/',)", "identifyer": 5591158, "year": "2021"}, {"autor": "charlescad", "date": 1626860694000, "content": "Been assigned a new position. My manager asked about my needs. What do you think? Short and long version or the more courageous readers. /!/ **Short version of my question:**\n\nQuestion 1: I am assigned a new position in a support unit of my company that will help people manage their Extract Transform Load processes. What tools should I use/learn?\n\nQuestion 2: My new manager asked me: Should you need new computer for your tasks. What would it be? In terms of processing power, operating system, etc.\n\nObjectives:\n\n* better organise the data from many different sources within the department\n* Reduce teams' data processing time through better managemnt of the data but as well through more efficient tools (like parallel computing).\n* Provide some dynamic data visualization tools\n\n&amp;#x200B;\n\n**Long version of my question:**\n\nIf you like to read about people's life, here is a longer version of the question: I provide more background about my position. I would then ask broader questions: what come through your mind when reading this? Do you have in mind tools, formation that I should start using or learning?\n\nI am a statistician working in a company that is somewhat rigid in terms of data project processes. Rigid in the sense that the security team hardly allows users to install and test new programs; that we can solely work on Windows; that processing power is deployed on internal servers without the possibility to subscribe to any cloud computing service.\n\nStill, our analysts' main objective is to write evidence based reports... Which requires data, data processing, data analysis tools. Analysts can work on many available languages and programs among which R, python, Stata, SAS, Excel, etc. But still, I would not be able to install Apache Airflow for some task scheduling jobs when needed for instance.\n\nI have been assigned a new role in my department: I am now in a support unit and in charge of providing support to all the data analysts on how to manage data, where to find it, how to automatically update databases. \n\nIn a nutshell, I think we can resume it to providing tools for the Extract Transform Load processes on a per project basis. Why per project basis? In my departments, different teams use different tools, different sources of data. I can influence users using a tool if it really helps management of their data. But I won't change the mind and reeducate the whole team around a new imposed tool. \n\n**Some more pieces of information**\n\nThe company is developing new tools to better manage data with a structure depending on whether data is confidential, whether it is large (HDFS) or not (NTFS) format. The IT team is trying to implement Spark on a cluster of internal (Windows) server (which does not work for now). I think the technology behind this will be Spark/Python/Hive. \n\n**My background and how I work**\n\nStatistician (with master degree in economics department with specialization in econometrics). I have started my career ten years ago with SAS and Stata, now using python and R for data processing. Emacs as a text editor. I work on the internal servers of my organization. 80% of my work time is to manage databases: fetch different sources, cleanse, harmonize, predict. I love learning new things and I keep trying new things, sometimes in a hacky maneer!\n\nData format: I use many different sources of data from SQL servers, Excel files, CSV files, API calls. It is hardly higher than 500 gb. I am not sure this fits for big data. But what I am sure of is that I always try to minimize the time spent processing the data.\n\nThis being said, if I were to use the new job nomenclature that people nowadays use, I think I would be closer to a data scientist than to a data engineer. \n\nAt home: linux/ubuntu and manjaro.\n\nThank you for reading! Questions are at the beginning of the text :-)", "link": "https://www.reddit.com/r/datascience/comments/ooml41/been_assigned_a_new_position_my_manager_asked/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "been assigned a new position. my manager asked about my needs. what do you think? short and long version or the more courageous readers. /!/ **short version of my question:**\n\nquestion 1: i am assigned a new position in a support unit of my company that will help people manage their extract transform load processes. what tools should i use/learn?\n\nquestion 2: my new manager asked me: should you need new computer for your tasks. what would it be? in terms of processing power, operating system, etc.\n\nobjectives:\n\n* better organise the data from many different sources within the department\n* reduce teams' data processing time through better managemnt of the data but as well through more efficient tools (like parallel computing).\n* provide some dynamic data visualization tools\n\n&amp;#x200b;\n\n**long version of my question:**\n\nif you like to read about people's life, here is a longer version of the question: i provide more background about my position. i would then ask broader questions: what come through your mind when reading this? do you have in mind tools, formation that i should start using or learning?\n\ni am a statistician working in a company that is somewhat rigid in terms of data project processes. rigid in the sense that the security team hardly allows users to install and test new programs; that we can solely work on windows; that processing power is deployed on internal servers without the possibility to subscribe to any cloud computing service.\n\nstill, our analysts' main objective is to write evidence based reports... which requires data, data processing, data analysis tools. analysts can work on many available languages and programs among which r, python, stata, sas, excel, etc. but still, i would not be able to install apache airflow for some task scheduling jobs when needed for instance.\n\ni have been assigned a new role in my department: i am now in a support unit and in charge of providing support to all the data analysts on how to manage data, where to find it, how to automatically update databases. \n\nin a nutshell, i think we can resume it to providing tools for the extract transform load processes on a per project basis. why per project basis? in my departments, different teams use different tools, different sources of data. i can influence users using a -----> tool !!!  if it really helps management of their data. but i won't change the mind and reeducate the whole team around a new imposed tool. \n\n**some more pieces of information**\n\nthe company is developing new tools to better manage data with a structure depending on whether data is confidential, whether it is large (hdfs) or not (ntfs) format. the it team is trying to implement spark on a cluster of internal (windows) server (which does not work for now). i think the technology behind this will be spark/python/hive. \n\n**my background and how i work**\n\nstatistician (with master degree in economics department with specialization in econometrics). i have started my career ten years ago with sas and stata, now using python and r for data processing. emacs as a text editor. i work on the internal servers of my organization. 80% of my work time is to manage databases: fetch different sources, cleanse, harmonize, predict. i love learning new things and i keep trying new things, sometimes in a hacky maneer!\n\ndata format: i use many different sources of data from sql servers, excel files, csv files, api calls. it is hardly higher than 500 gb. i am not sure this fits for big data. but what i am sure of is that i always try to minimize the time spent processing the data.\n\nthis being said, if i were to use the new job nomenclature that people nowadays use, i think i would be closer to a data scientist than to a data engineer. \n\nat home: linux/ubuntu and manjaro.\n\nthank you for reading! questions are at the beginning of the text :-)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/ooml41/been_assigned_a_new_position_my_manager_asked/',)", "identifyer": 5591173, "year": "2021"}, {"autor": "narkgarfie", "date": 1613146916000, "content": "Courses for ETL and API Connections /!/ Hi all! I have really enjoyed the process of 1) understanding where data originates, 2) building an automated ETL process for this data, then 3) displaying the data/insights via a BI tool (or however). I've had help from developer friends on building out point number 2, yet I was curious if anyone had any recommendations on foundational courses related to this (preferably in Python). Where I've asked developers plenty of dumb questions, I feel like I must not be asking the *right* dumb questions for it to fundamentally click for me.  \n\n\nTo provide more context, I'm now looking at a site that drops a txt file once a day. I'm curious to learn best practices in automating an ETL tool to pull this file on a daily basis and incorporate the data into an established dataset. Although I know Python (and R) as a data analyst, I'm obviously not a developer. Learning through specific examples has been the easiest way for things to click for me. Thanks in advance!", "link": "https://www.reddit.com/r/datascience/comments/liefc4/courses_for_etl_and_api_connections/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "courses for etl and api connections /!/ hi all! i have really enjoyed the process of 1) understanding where data originates, 2) building an automated etl process for this data, then 3) displaying the data/insights via a bi -----> tool !!!  (or however). i've had help from developer friends on building out point number 2, yet i was curious if anyone had any recommendations on foundational courses related to this (preferably in python). where i've asked developers plenty of dumb questions, i feel like i must not be asking the *right* dumb questions for it to fundamentally click for me.  \n\n\nto provide more context, i'm now looking at a site that drops a txt file once a day. i'm curious to learn best practices in automating an etl tool to pull this file on a daily basis and incorporate the data into an established dataset. although i know python (and r) as a data analyst, i'm obviously not a developer. learning through specific examples has been the easiest way for things to click for me. thanks in advance!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/liefc4/courses_for_etl_and_api_connections/',)", "identifyer": 5591254, "year": "2021"}, {"autor": "highwayman07", "date": 1631475169000, "content": "What does customer segmentation (pre-paid telecom industry) look like on a practical level? Questions from a noob /!/ I'm trying to dive into customer segmentation and clustering for a potential job at a telecom provider. I've been watching tutorials on unsupervised learning algorithms such as kmeans. It is starting to make sense, but my question is, if the kmeans algorithm analyzes many (5-10) factors in order to uncover patterns which would otherwise be unapparent, how would one be able to extract actionable insights out of these segments? \n\nAlso, I apparently they use Qlik Sense. I understand Qlik Sense to be a data visualization/exploration tool. But does it run clustering algorithms?\n\nSeparately, I am looking at a different tactics for maximizing Average Revenue Per User and/or minimizing churn. Some of them include:\n\n* Identifying users who place international calls and offer them an international call bundle\n* Offer roaming packages to users who have traveled recently\n* Use geolocation data to estimate income and promote offerings that match said income estimate\n* Reach out to users who are about to lose their line due to non-payment and issue them a one-time offer (ie recharge for 2 months and get the 3rd month free)\n\nAm I thinking in the right direction? \n\n&amp;#x200B;\n\nThanks!", "link": "https://www.reddit.com/r/datascience/comments/pmzen1/what_does_customer_segmentation_prepaid_telecom/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what does customer segmentation (pre-paid telecom industry) look like on a practical level? questions from a noob /!/ i'm trying to dive into customer segmentation and clustering for a potential job at a telecom provider. i've been watching tutorials on unsupervised learning algorithms such as kmeans. it is starting to make sense, but my question is, if the kmeans algorithm analyzes many (5-10) factors in order to uncover patterns which would otherwise be unapparent, how would one be able to extract actionable insights out of these segments? \n\nalso, i apparently they use qlik sense. i understand qlik sense to be a data visualization/exploration -----> tool !!! . but does it run clustering algorithms?\n\nseparately, i am looking at a different tactics for maximizing average revenue per user and/or minimizing churn. some of them include:\n\n* identifying users who place international calls and offer them an international call bundle\n* offer roaming packages to users who have traveled recently\n* use geolocation data to estimate income and promote offerings that match said income estimate\n* reach out to users who are about to lose their line due to non-payment and issue them a one-time offer (ie recharge for 2 months and get the 3rd month free)\n\nam i thinking in the right direction? \n\n&amp;#x200b;\n\nthanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pmzen1/what_does_customer_segmentation_prepaid_telecom/',)", "identifyer": 5591339, "year": "2021"}, {"autor": "Osamabinbush", "date": 1631296393000, "content": "Data Analysis using JavaScript /!/ We\u2019ve been working on a data analysis tool to give JavaScript developers super-powers; it combines the ease of drag-and-drop with the freedom of writing arbitrary code. \n\nYou can play with it here: https:/alpha.hal9.ai.\n\nWe\u2019ve built a gallery of example where you can see the app in action.\n\nLet me know if you have suggestions!", "link": "https://www.reddit.com/r/datascience/comments/plq2sf/data_analysis_using_javascript/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "data analysis using javascript /!/ we\u2019ve been working on a data analysis -----> tool !!!  to give javascript developers super-powers; it combines the ease of drag-and-drop with the freedom of writing arbitrary code. \n\nyou can play with it here: https:/alpha.hal9.ai.\n\nwe\u2019ve built a gallery of example where you can see the app in action.\n\nlet me know if you have suggestions!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/plq2sf/data_analysis_using_javascript/',)", "identifyer": 5591412, "year": "2021"}, {"autor": "KDeven2000", "date": 1631293667000, "content": "Loading a .docx file into ETL/ELT tool? /!/ [removed]", "link": "https://www.reddit.com/r/datascience/comments/plp7dx/loading_a_docx_file_into_etlelt_tool/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "loading a .docx file into etl/elt -----> tool !!! ? /!/ [removed]", "sortedWord": "None", "removed": "('reddit',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/plp7dx/loading_a_docx_file_into_etlelt_tool/',)", "identifyer": 5591415, "year": "2021"}, {"autor": "djkaffe123", "date": 1625670247000, "content": "Machine learning in Azure /!/ Hi everybody,\n\nI recently got hired in a company using azure, and I expect to be building ml models on that platform. I am currently looking into their ml based offerings, and right now feel somewhat disappointed that the main ML tool is centered around drag and drop modelling (??). Has anybody worked with ML in azure doing more custom things?  \n\n\nIn AWS I would build ml or data pipeline using infrastructure as code and pointing to my python scripts doing the actual logic. This gave me perfect flexibility for what I needed to do, and I am wondering how to do this in azure without building everything myself.  \n\n\nDo I do arm templates pointing at my python scripts or what? I would like to use some of the ml services provided (easy monitoring and deployment ) but perhaps with custom feature engineering and modelling.", "link": "https://www.reddit.com/r/datascience/comments/ofkotx/machine_learning_in_azure/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "machine learning in azure /!/ hi everybody,\n\ni recently got hired in a company using azure, and i expect to be building ml models on that platform. i am currently looking into their ml based offerings, and right now feel somewhat disappointed that the main ml -----> tool !!!  is centered around drag and drop modelling (??). has anybody worked with ml in azure doing more custom things?  \n\n\nin aws i would build ml or data pipeline using infrastructure as code and pointing to my python scripts doing the actual logic. this gave me perfect flexibility for what i needed to do, and i am wondering how to do this in azure without building everything myself.  \n\n\ndo i do arm templates pointing at my python scripts or what? i would like to use some of the ml services provided (easy monitoring and deployment ) but perhaps with custom feature engineering and modelling.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 12, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/ofkotx/machine_learning_in_azure/',)", "identifyer": 5591459, "year": "2021"}, {"autor": "ajinkyajawale", "date": 1625638114000, "content": "What tool you use for data dictionary /!/ We are heavy data engineering and data science team who regularly interacts with product and Marketing teams\n\nWe don't want to Product team and marketing team to get involved into dwh or schema or any data level asking hell what's this what's that... Rather we providing the proper data dictionary like what's the table what's it's meaning fields at High level view..\n\nIt would be highly appreciated if you have used any similar relevant tools..\n\nPs.. we are not looking for data governance tools more as a data dictionary documention tool", "link": "https://www.reddit.com/r/datascience/comments/ofd3fp/what_tool_you_use_for_data_dictionary/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what -----> tool !!!  you use for data dictionary /!/ we are heavy data engineering and data science team who regularly interacts with product and marketing teams\n\nwe don't want to product team and marketing team to get involved into dwh or schema or any data level asking hell what's this what's that... rather we providing the proper data dictionary like what's the table what's it's meaning fields at high level view..\n\nit would be highly appreciated if you have used any similar relevant tools..\n\nps.. we are not looking for data governance tools more as a data dictionary documention tool", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 5, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/ofd3fp/what_tool_you_use_for_data_dictionary/',)", "identifyer": 5591476, "year": "2021"}, {"autor": "m2rik", "date": 1614737285000, "content": "Excel model to advanced tool /!/ [removed]", "link": "https://www.reddit.com/r/datascience/comments/lwil6g/excel_model_to_advanced_tool/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "excel model to advanced -----> tool !!!  /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 0, "comments": 10, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/lwil6g/excel_model_to_advanced_tool/',)", "identifyer": 5591538, "year": "2021"}, {"autor": "VSK-1", "date": 1632595338000, "content": "Is it ok to use Google sheets in stead of excel now? /!/ I am new to big data and principles concern is analytics, however, I am really not that familiar with any element of the ma ecosystem. I know the excel has traditionally been viewed as the de facto tool for all things data. But on further inspection, it seems to be the go-to tool for all things finance related, in particular for some reason (is this right?). So I was wondering whether sheets is a viable substitute in 2021. I really would feel more comfortable using sheets but bot at the expense as any important functionality that only excel may have.", "link": "https://www.reddit.com/r/datascience/comments/pvcwdx/is_it_ok_to_use_google_sheets_in_stead_of_excel/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "is it ok to use google sheets in stead of excel now? /!/ i am new to big data and principles concern is analytics, however, i am really not that familiar with any element of the ma ecosystem. i know the excel has traditionally been viewed as the de facto -----> tool !!!  for all things data. but on further inspection, it seems to be the go-to tool for all things finance related, in particular for some reason (is this right?). so i was wondering whether sheets is a viable substitute in 2021. i really would feel more comfortable using sheets but bot at the expense as any important functionality that only excel may have.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 21, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pvcwdx/is_it_ok_to_use_google_sheets_in_stead_of_excel/',)", "identifyer": 5591630, "year": "2021"}, {"autor": "gemag", "date": 1632562053000, "content": "What do you use as a whiteboarding tool during your remote meetings? /!/ Hi!\n\nI often need to sketch diagrams or write down simple equations during my remote meetings.\n\nUnfortunately, I don\u2019t have a touchscreen laptop, and using the trackpad to draw charts sucks (I have a MacBook and mostly use Zoom for remote meetings).\n\n Do you guys have any recommendations?", "link": "https://www.reddit.com/r/datascience/comments/pv3fxx/what_do_you_use_as_a_whiteboarding_tool_during/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what do you use as a whiteboarding -----> tool !!!  during your remote meetings? /!/ hi!\n\ni often need to sketch diagrams or write down simple equations during my remote meetings.\n\nunfortunately, i don\u2019t have a touchscreen laptop, and using the trackpad to draw charts sucks (i have a macbook and mostly use zoom for remote meetings).\n\n do you guys have any recommendations?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 73, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pv3fxx/what_do_you_use_as_a_whiteboarding_tool_during/',)", "identifyer": 5591648, "year": "2021"}, {"autor": "damiannelus", "date": 1632556541000, "content": "Tool/approach selection advice /!/ Hey, before I start implementing my idea, I thought it would save me a lot of time to ask for help where it comes to tools selection.\n\nHere\u2019s **the context**. \n\nI have two datasets, *employees* (with their features like category, seniority, \u2026) and *projects* on which those employees worked (with some features like name, client, \u2026). \n\nI **want to**: \n\n1. Prepare **an interactive diagram that groups employees by the same feature**. Let\u2019s say I select \u201ccategory\u201d as the grouping feature and I want to see dots representing devs gathered in one circle, QAs in another circle, and so on. \n2. Despite the selected grouping from the first point, **visualize projects on which those resources worked as connections between those dots**.\n\nAs if it wasn\u2019t enough\u2026 \n\n* Same employees can work on a few projects together. Thus, each project, a connection, has to be distinguishable. \n* I\u2019d also like to have this data accessible for other analysis (e.g. to answer the question of what 2 employees has the biggest number of common projects)\n\nAnd know, **please advise**: \n\n1. Is Python the right language for that? I have some basic skills and for the future, I\u2019d prefer to stick to Python rather than R. \n2. What libs could help to visualize the above? Any extra tips?\n\nThe above is probably noob questions but wanted to ask them to avoid digging too deep into libs/solutions that will turn out to be dead ends.", "link": "https://www.reddit.com/r/datascience/comments/pv2c53/toolapproach_selection_advice/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "-----> tool !!! /approach selection advice /!/ hey, before i start implementing my idea, i thought it would save me a lot of time to ask for help where it comes to tools selection.\n\nhere\u2019s **the context**. \n\ni have two datasets, *employees* (with their features like category, seniority, \u2026) and *projects* on which those employees worked (with some features like name, client, \u2026). \n\ni **want to**: \n\n1. prepare **an interactive diagram that groups employees by the same feature**. let\u2019s say i select \u201ccategory\u201d as the grouping feature and i want to see dots representing devs gathered in one circle, qas in another circle, and so on. \n2. despite the selected grouping from the first point, **visualize projects on which those resources worked as connections between those dots**.\n\nas if it wasn\u2019t enough\u2026 \n\n* same employees can work on a few projects together. thus, each project, a connection, has to be distinguishable. \n* i\u2019d also like to have this data accessible for other analysis (e.g. to answer the question of what 2 employees has the biggest number of common projects)\n\nand know, **please advise**: \n\n1. is python the right language for that? i have some basic skills and for the future, i\u2019d prefer to stick to python rather than r. \n2. what libs could help to visualize the above? any extra tips?\n\nthe above is probably noob questions but wanted to ask them to avoid digging too deep into libs/solutions that will turn out to be dead ends.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pv2c53/toolapproach_selection_advice/',)", "identifyer": 5591651, "year": "2021"}, {"autor": "happysealND", "date": 1628807784000, "content": "How should I link my code for reproduction and automation? /!/ So, this is more of a thought experiment I've had whilst working on my dissertation.\n\nCurrently I'm pulling asset information from an API for a set time horizon in the past using python. using this asset data I then feed it into R to do a bit more manipulation. Simultaneously I use another API to pull Reddit data to get sentiments for the same firms and feed that into a sentiment analysis tool to then be processed in R also, which is all then used in some regression in STATA. This happens using a bunch of write.csv and read.csv commands, not going to lie it gets a bit messy and I do lose track since I'm using multiple scripts in multiple languages. \n\nWhat I think I'm thinking of is a pipeline (correct me if I'm wrong) in order to smoothly get the data from the sources I need, to then be cleaned and then modelled. If this is the right approach how should I consider doing this, otherwise what process/tools am I actually looking for to make this a swift process?", "link": "https://www.reddit.com/r/datascience/comments/p3amxn/how_should_i_link_my_code_for_reproduction_and/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "how should i link my code for reproduction and automation? /!/ so, this is more of a thought experiment i've had whilst working on my dissertation.\n\ncurrently i'm pulling asset information from an api for a set time horizon in the past using python. using this asset data i then feed it into r to do a bit more manipulation. simultaneously i use another api to pull reddit data to get sentiments for the same firms and feed that into a sentiment analysis -----> tool !!!  to then be processed in r also, which is all then used in some regression in stata. this happens using a bunch of write.csv and read.csv commands, not going to lie it gets a bit messy and i do lose track since i'm using multiple scripts in multiple languages. \n\nwhat i think i'm thinking of is a pipeline (correct me if i'm wrong) in order to smoothly get the data from the sources i need, to then be cleaned and then modelled. if this is the right approach how should i consider doing this, otherwise what process/tools am i actually looking for to make this a swift process?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/p3amxn/how_should_i_link_my_code_for_reproduction_and/',)", "identifyer": 5591748, "year": "2021"}, {"autor": "Sonny-Orkidea", "date": 1630232839000, "content": "My job position name not describing my daily agenda /!/ Hello. I am working with CDP tool Bloomreach (former Exponea). My responsibility is data integration process, planning of web tracking and data structure. We are tracking customer and events on website and this is used in analytics and marketing communication channels (email, sms, push notifications etc..). I clean data, debug some tracking mistakes, templating in Jinja and coding web layers (banners, ribbons, small notifications ...), optimize campaigns etc. All the technical stuff and my collegues are doing designs and campaigns, all content.\n\nMy job position is marketing specialist. It sound bad and its not my job. We are doing some marketing, yes. But i am more data driven. How would you re-name my position? also. How would you describe this whole process. Everybody here calls it marketing automation, but marketing automation is something what mailchimp does. Not this.", "link": "https://www.reddit.com/r/datascience/comments/pdsuzu/my_job_position_name_not_describing_my_daily/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "my job position name not describing my daily agenda /!/ hello. i am working with cdp -----> tool !!!  bloomreach (former exponea). my responsibility is data integration process, planning of web tracking and data structure. we are tracking customer and events on website and this is used in analytics and marketing communication channels (email, sms, push notifications etc..). i clean data, debug some tracking mistakes, templating in jinja and coding web layers (banners, ribbons, small notifications ...), optimize campaigns etc. all the technical stuff and my collegues are doing designs and campaigns, all content.\n\nmy job position is marketing specialist. it sound bad and its not my job. we are doing some marketing, yes. but i am more data driven. how would you re-name my position? also. how would you describe this whole process. everybody here calls it marketing automation, but marketing automation is something what mailchimp does. not this.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pdsuzu/my_job_position_name_not_describing_my_daily/',)", "identifyer": 5591767, "year": "2021"}, {"autor": "Somespecialcharacter", "date": 1630017939000, "content": "[E] Repeated Measure /!/ Hello folks,\n\nI am a newbie with predictive modeling and honestly I have a minor in statistics and I am moving more towards machine learning concepts. \nNow, this may be completely absurd but I just had a thought and needed an opinion.\nI worked on a research project where we had a binary outcome Yes/No looking at weather they visited a store for a set of 1699 participants. Total number of stores in our study were 49. We created a tool that gives us the quality of the stores on a percentage scale.\n\nOut hypotheses was, the better the store score the more the users.\nThe way we linked 1699 participants to 49 stores was by the zipcode the lived in and the zipcode the store was in. And to make sure of the sampling we ascertain the percentages of participants from each zipcode and store. \n\nThe dataset looked something like 1699 rows with either yes or no on the visit to store and the score for the store repeated for participants in each zip code.\n\nFor eg\n\n1 yes 87.55 zipcode x\n2 no  87.55 zipcode x \n3 yes 91.77 zipcode y\n4 yes 91.77 zipcode y\n5 yes 87.55 zipcode x\n6 no  73.01  zipcode z\n\u2026\u2026.\n1699 \n\n\nI hope the data makes sense now. We used a GEE model to adjust for repeated measures. But our model was not successful. (Any suggestions on this would also be helpful)\n\nNow, while learning ML concepts I was wondering if there could potentially be any algorithm that kind of fits our data and study design to build a predictive model that predicts the usage from the score that we build for the store.\nSorry for the long post and thank you for making till the end and showing interest. \nYour help is highly appreciated!", "link": "https://www.reddit.com/r/datascience/comments/pcaguk/e_repeated_measure/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[e] repeated measure /!/ hello folks,\n\ni am a newbie with predictive modeling and honestly i have a minor in statistics and i am moving more towards machine learning concepts. \nnow, this may be completely absurd but i just had a thought and needed an opinion.\ni worked on a research project where we had a binary outcome yes/no looking at weather they visited a store for a set of 1699 participants. total number of stores in our study were 49. we created a -----> tool !!!  that gives us the quality of the stores on a percentage scale.\n\nout hypotheses was, the better the store score the more the users.\nthe way we linked 1699 participants to 49 stores was by the zipcode the lived in and the zipcode the store was in. and to make sure of the sampling we ascertain the percentages of participants from each zipcode and store. \n\nthe dataset looked something like 1699 rows with either yes or no on the visit to store and the score for the store repeated for participants in each zip code.\n\nfor eg\n\n1 yes 87.55 zipcode x\n2 no  87.55 zipcode x \n3 yes 91.77 zipcode y\n4 yes 91.77 zipcode y\n5 yes 87.55 zipcode x\n6 no  73.01  zipcode z\n\u2026\u2026.\n1699 \n\n\ni hope the data makes sense now. we used a gee model to adjust for repeated measures. but our model was not successful. (any suggestions on this would also be helpful)\n\nnow, while learning ml concepts i was wondering if there could potentially be any algorithm that kind of fits our data and study design to build a predictive model that predicts the usage from the score that we build for the store.\nsorry for the long post and thank you for making till the end and showing interest. \nyour help is highly appreciated!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pcaguk/e_repeated_measure/',)", "identifyer": 5591842, "year": "2021"}, {"autor": "NicoleJaneway", "date": 1617615822000, "content": "Validate my NLP / Streamlit project idea /!/ Posting here so the good people of r/datascience can tell me whether this is a decent idea or a crap idea.  \n\n**Objective:**\n\nUsing Streamlit as the front end, I'd like to create an NLP tool that intakes a user's question and outputs a suggested answer.  Basically, an extremely simple chatbot.\n\nAdvanced version:  the user can input their own corpus of Frequently Asked Questions to fine-tune the model for their own use case.\n\n**Outcome**:\n\nI'm thinking this tool could help out my teammates who are tasked with replying to emails to our company's public-facing email inbox.  They paste the text of the email into one box of the UI, then copy the FAQ response text from the other box into the response email.\n\n**Approach**:\n\nI was thinking of using a Hugging Face Transformer model and then fine-tuning on a csv of FAQ text where the answers are features and the questions are labels.  I'm curious if I'll need to generate a bunch of fake data (e.g. a bunch of different ways of phrasing the question for each answer) or whether the latest Transformers work decently well with a smaller dataset.\n\n**Question**:\n\nI know chatbots have been done before.  Thoughts from this group on resources I could use to start working on this side project?", "link": "https://www.reddit.com/r/datascience/comments/mkgw07/validate_my_nlp_streamlit_project_idea/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "validate my nlp / streamlit project idea /!/ posting here so the good people of r/datascience can tell me whether this is a decent idea or a crap idea.  \n\n**objective:**\n\nusing streamlit as the front end, i'd like to create an nlp -----> tool !!!  that intakes a user's question and outputs a suggested answer.  basically, an extremely simple chatbot.\n\nadvanced version:  the user can input their own corpus of frequently asked questions to fine-tune the model for their own use case.\n\n**outcome**:\n\ni'm thinking this tool could help out my teammates who are tasked with replying to emails to our company's public-facing email inbox.  they paste the text of the email into one box of the ui, then copy the faq response text from the other box into the response email.\n\n**approach**:\n\ni was thinking of using a hugging face transformer model and then fine-tuning on a csv of faq text where the answers are features and the questions are labels.  i'm curious if i'll need to generate a bunch of fake data (e.g. a bunch of different ways of phrasing the question for each answer) or whether the latest transformers work decently well with a smaller dataset.\n\n**question**:\n\ni know chatbots have been done before.  thoughts from this group on resources i could use to start working on this side project?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/mkgw07/validate_my_nlp_streamlit_project_idea/',)", "identifyer": 5591888, "year": "2021"}, {"autor": "ReactCereals", "date": 1617554331000, "content": "Catching up: how to learn the foundations I missed starting in DS without a CS background /!/ Hello Community,\n\nSo, I was fortunate enough to be in one of the first generations to directly study data science in my area (even though I was learning towards that field years ago while studying physics).\n\nNow I have been working for a few years and....it\u2019s cool. I mean I do my little Python scripts, cool graphs, setup Spark clusters, run my data driven web apps to production on a Kubernetes cluster, write my retrieval algorithms for my little data warehouse, and will tinker around with AI a bit to see if I can improve it further later this year.\n\nOverall I could call myself a happy \u201clittle bit of everything\u201d data scientist, yay :)\n\nNow I wanted to move away from business and production and more towards the \u201cscience\u201d part in our job description: I want to do research (well that\u2019s my root coming from physics anyway I guess).\n\nAnd while trying to get there I had a bitter realization: I have no freaking idea how a computer works or what I am even \u201creally\u201d doing under the hood.\n\nI have written Django Webapps and don\u2019t really know how a web server works. I have used Kubernetes without even knowing what the architecture profoundly does. I have rolled out Spark clusters to improve performance but, besides the raw theory, I have no clue why the way it improves things really works on a down to compiled \u201cRDD code\u201d level.\n\nDon\u2019t get me wrong; with all of that I of course know what the theory behind it is. But I feel like my theory ends really quickly at the scope of the tool I am using. That my theoretical knowledge is limited to the knowledge about how a tool itself works, but not how it really runs or interacts with everything else.\n\nAlso I realized when switching over between programming languages that I fundamentally have no clue: I don\u2019t know how compilers, linkers, registry, garbage collection, etc works, sometimes not even what it really is.\n\nOverall I studied mathematics for quite a while and I for sure know my DS tools. But between me and my little CPU (well, more like a server rack but whatever) running all this jobs for me is a huge gap of cluelessness.\n\nMakes me feel quite bad realizing this over the past few weeks.\nSo i would like to ask maybe the people here coming from CS or Engineering or something: any ideas where I should start to \u201ccatch up\u201d?\nI was thinking about getting a book about ARM architecture, learning a bit assembly, maybe refreshing my FORTRAN basics, write a very little binary compiler in C, and maybe play together a little Webserver and even a small web framework in Julia.\n\nIf doing research I will probably write algorithms as part of it. And I think I can\u2019t write an efficient or proper algorithm without knowing what I am even doing here.\n\nThat would be my ideas for a lot of small projects over the course of this year or maybe even next year to get myself down to the concepts.\n\nI would highly appreciate every link/keyword/etc or whatever you could get me as to where I should start or what you believe would be important.\n\nAlso, i would be happy to hear about your opinion: do you think I am too hard to myself for feeling bad for not knowing all of this? Or do you maybe even believe we don\u2019t really need that foundation for DS?\n\nThanks for reading and have a great week :)", "link": "https://www.reddit.com/r/datascience/comments/mjzppv/catching_up_how_to_learn_the_foundations_i_missed/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "catching up: how to learn the foundations i missed starting in ds without a cs background /!/ hello community,\n\nso, i was fortunate enough to be in one of the first generations to directly study data science in my area (even though i was learning towards that field years ago while studying physics).\n\nnow i have been working for a few years and....it\u2019s cool. i mean i do my little python scripts, cool graphs, setup spark clusters, run my data driven web apps to production on a kubernetes cluster, write my retrieval algorithms for my little data warehouse, and will tinker around with ai a bit to see if i can improve it further later this year.\n\noverall i could call myself a happy \u201clittle bit of everything\u201d data scientist, yay :)\n\nnow i wanted to move away from business and production and more towards the \u201cscience\u201d part in our job description: i want to do research (well that\u2019s my root coming from physics anyway i guess).\n\nand while trying to get there i had a bitter realization: i have no freaking idea how a computer works or what i am even \u201creally\u201d doing under the hood.\n\ni have written django webapps and don\u2019t really know how a web server works. i have used kubernetes without even knowing what the architecture profoundly does. i have rolled out spark clusters to improve performance but, besides the raw theory, i have no clue why the way it improves things really works on a down to compiled \u201crdd code\u201d level.\n\ndon\u2019t get me wrong; with all of that i of course know what the theory behind it is. but i feel like my theory ends really quickly at the scope of the -----> tool !!!  i am using. that my theoretical knowledge is limited to the knowledge about how a tool itself works, but not how it really runs or interacts with everything else.\n\nalso i realized when switching over between programming languages that i fundamentally have no clue: i don\u2019t know how compilers, linkers, registry, garbage collection, etc works, sometimes not even what it really is.\n\noverall i studied mathematics for quite a while and i for sure know my ds tools. but between me and my little cpu (well, more like a server rack but whatever) running all this jobs for me is a huge gap of cluelessness.\n\nmakes me feel quite bad realizing this over the past few weeks.\nso i would like to ask maybe the people here coming from cs or engineering or something: any ideas where i should start to \u201ccatch up\u201d?\ni was thinking about getting a book about arm architecture, learning a bit assembly, maybe refreshing my fortran basics, write a very little binary compiler in c, and maybe play together a little webserver and even a small web framework in julia.\n\nif doing research i will probably write algorithms as part of it. and i think i can\u2019t write an efficient or proper algorithm without knowing what i am even doing here.\n\nthat would be my ideas for a lot of small projects over the course of this year or maybe even next year to get myself down to the concepts.\n\ni would highly appreciate every link/keyword/etc or whatever you could get me as to where i should start or what you believe would be important.\n\nalso, i would be happy to hear about your opinion: do you think i am too hard to myself for feeling bad for not knowing all of this? or do you maybe even believe we don\u2019t really need that foundation for ds?\n\nthanks for reading and have a great week :)", "sortedWord": "None", "removed": "('nan',)", "score": 0, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/mjzppv/catching_up_how_to_learn_the_foundations_i_missed/',)", "identifyer": 5591908, "year": "2021"}, {"autor": "uuuuuiiiuuuuuuu", "date": 1617476413000, "content": "Looking for software/tool recommendations for detailed forecasting (have a few options) /!/ [removed]", "link": "https://www.reddit.com/r/datascience/comments/mjeuri/looking_for_softwaretool_recommendations_for/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "looking for software/-----> tool !!!  recommendations for detailed forecasting (have a few options) /!/ [removed]", "sortedWord": "None", "removed": "('reddit',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/mjeuri/looking_for_softwaretool_recommendations_for/',)", "identifyer": 5591947, "year": "2021"}, {"autor": "SnooPaintings5866", "date": 1632062086000, "content": "The ultimate guide to getting a job in data science. /!/ Organisation is Key\n\nI\u2019ve interviewed at Google (and DeepMind), Uber, Facebook, Amazon for roles that lie under the \u201cData Scientist\u201d umbrella and this is the typical interview construction theme I\u2019ve observed:\n\nSoftware Engineering\nApplied Statistics\nMachine Learning\nData Wrangling, Manipulation and Visualisation\n\nNow nobody is expecting some super graduate level competency in all of these topics, but you need to know enough to convince your interviewer that you\u2019re capable of delivering if they offered you the job. How much you need to know depends on the job spec, but in this increasingly competitive market, no knowledge is lost.\n\nI recommend using Notion to organise your job prep. It\u2019s extremely versatile, and enables you to utilise the Spaced Repetition and Active Recall principles to nail down learning and deploying key topics that come up time and time again in a Data Scientist interview. Ali Abdaal has a great tutorial on note taking with Notion to maximise your learning potential during the interview process.\n\nI used to run through my Notion notes over and over, but in particular, right before my interview. This ensured that key topics and definitions were loaded into my working memory and I didn\u2019t waste precious \ntime \u201cummmmmm\u201ding when hit with some question.\n\n2. Software Engineering\nNot all Data Scientist roles will grill you on the time complexity of an algorithm, but all of these roles will expect you to write code. Data Science isn\u2019t one job, but a collection of jobs that attracts talent from a variety of industries, including the software engineering world. As such you\u2019re competing with guys that know the ins and outs of writing efficient code and I would recommend spending at least 1\u20132 hours a day in the lead-up to your interview practicing the following concepts:\n\nArrays\nHash Tables\nLinked Lists\nTwo-Pointer based algorithms\nString algorithms (interviewers LOVE these)\nBinary Search\nDivide and Conquer Algorithms\nSorting Algorithms\nDynamic Programming\nRecursion\n\nDO NOT LEARN THE ALGORITHMS OFF BY HEART. This approach is useless, because the interviewer can question you on any variation of the algorithm and you will be lost. Instead learn the strategy behind how each algorithm works. Learn what computational and spatial complexity are, and learn why they are so fundamental to building efficient code.\n\nLeetCode was my best friend during interview preparation and is well worth the $35 per month in my opinion. Your interviewers only have so many algorithm questions to sample from, and this website covers a host of algorithm concepts including companies that are likely or are known to have asked these questions in the past. There\u2019s also a great community who discuss each problem in detail, and helped me during the myriad of \u201cstuck\u201d moments I encountered. LeetCode has a \u201clite\u201d version with a smaller question bank if the $35 price tag is too steep, as do HackerRank and geeksforgeeks which are other great resources.\n\nWhat you should do is attempt each question, even if it\u2019s a brute force approach that takes ages to run. Then look at the model solution, and try to figure out what the optimal strategy is. Then read up what the optimal strategy is and try to understand why this is the optimal strategy. Ask yourself questions like \u201cwhy is Quicksort O(n\u00b2) average time complexity?\u201d, why do two pointers and one for loop make more sense than three for loops?\n\n3. Applied Statistics\nData science has an implicit dependence on applied statistics, and how implicit that will be depends on the role you\u2019ve applied for. Where do we use applied statistics? It pops up just about anywhere where we need to organise, interpret and derive insights from data.\n\nI studied the following topics intensely during my interviews, and you bet your bottom dollar that I was grilled about each topic:\n\nDescriptive statistics (What distribution does my data follow, what are the modes of the distribution, the expectation, the variance)\nProbability theory (Given my data follows a Binomial distribution, what is the probability of observing 5 paying customers in 10 click-through events)\n\nHypothesis testing (forming the basis of any question on A/B testing, T-tests, anova, chi-squared tests, etc).\n\nRegression (Is the relationship between my variables linear, what are potential sources of bias, what are the assumptions behind the ordinary least squares solution)\n\nBayesian Inference (What are some advantages/disadvantages vs frequentist methods)\n\nIf you think this is a lot of material you are not alone, I was massively overwhelmed with the volume of knowledge expected in these kinds of interviews and the plethora of information on the internet that could help me. Two invaluable resources come to mind when I was revising for interviews.\n\nIntroduction to Probability and Statistics, an open course on everything listed above including questions and an exam to help you test your knowledge.\n\nMachine Learning: A Bayesian and Optimization Perspective by Sergios Theodoridis. This is more a machine learning text than a specific primer on applied statistics, but the linear algebra approaches outlined here really help drive home the key statistical concepts on regression.\n\nThe way you\u2019re going to remember this stuff isn\u2019t through memorisation, you need to solve as many problems as you can get your hands on. Glassdoor is a great repo for the sorts of applied stats questions typically asked in interviews. The most challenging interview I had by far was with G-Research, but I really enjoyed studying for the exam, and their sample exam papers were fantastic resources when it came to testing how far I was getting in my applied statistics revision.\n\n4. Machine Learning\nNow we come to the beast, the buzzword of our millennial era, and a topic so broad that it can be easy to get so lost in revision that you want to give up.\nThe applied statistics part of this study guide will give you a very very strong foundation to get started with machine learning (which is basically just applied applied statistics written in fancy linear algebra), but there are certain key concepts that came up over and over again during my interviews. Here is a (by no means exhaustive) set of concepts organised by topic:\n\nMetrics \u2014 Classification\nConfusion Matrices, Accuracy, Precision, Recall, Sensitivity\nF1 Score\nTPR, TNR, FPR, FNR\nType I and Type II errors\nAUC-ROC Curves\n\nMetrics \u2014 Regression\nTotal sum of squares, explained sum of squares, residual sum of squares\nCoefficient of determination and its adjusted form\nAIC and BIC\nAdvantages and disadvantages of RMSE, MSE, MAE, MAPE\n\nBias-Variance Tradeoff, Over/Under-Fitting\nK Nearest Neighbours algorithm and the choice of k in bias-variance trade-off\nRandom Forests\nThe asymptotic property\nCurse of dimensionality\nModel Selection\nK-Fold Cross Validation\nL1 and L2 Regularisation\nBayesian Optimization\n\nSampling\nDealing with class imbalance when training classification models\nSMOTE for generating pseudo observations for an underrepresented class\nClass imbalance in the independent variables\nSampling methods\nSources of sampling bias\nMeasuring Sampling Error\n\nHypothesis Testing\nThis really comes under under applied statistics, but I cannot stress enough the importance of learning about statistical power. It\u2019s enormously important in A/B testing.\n\nRegression Models\nOrdinary Linear Regression, its assumptions, estimator derivation and limitations are covered in significant detail in the sources cited in the applied statistics section. Other regression models you should be familiar with are:\nDeep Neural Networks for Regression\nRandom Forest Regression\nXGBoost Regression\nTime Series Regression (ARIMA/SARIMA)\nBayesian Linear Regression\nGaussian Process Regression\n\nClustering Algorithms\nK-Means\nHierarchical Clustering\nDirichlet Process Mixture Models\n\nClassification Models\nLogistic Regression (Most important one, revise well)\nMultiple Regression\nXGBoost Classification\nSupport Vector Machines\n\nIt\u2019s a lot, but much of the content will be trivial if your applied statistics foundation is strong enough. I would recommend knowing the ins and outs of at least three different classification/regression/clustering methods, because the interviewer could always (and has previously) asked \u201cwhat other methods could we have used, what are some advantages/disadvantages\u201d? This is a small subset of the machine learning knowledge in the world, but if you know these important examples, the interviews will flow a lot more smoothly.\n\n5. Data Manipulation and Visualisation\n\u201cWhat are some of the steps for data wrangling and data cleaning before applying machine learning algorithms\u201d?\n\nWe are given a new dataset, the first thing you\u2019ll need to prove is that you can perform an exploratory data analysis (EDA). Before you learn anything realise that there is one path to success in data wrangling: Pandas. The Pandas IDE, when used correctly, is the most powerful tool in a data scientists toolbox. The best way to learn how to use Pandas for data manipulation is to download many, many datasets and learn how to do the following set of tasks as confidently as you making your morning cup of coffee.\n\nOne of my interviews involved downloading a dataset, cleaning it, visualising it, performing feature selection, building and evaluating a model all in one hour. It was a crazy hard task, and I felt overwhelmed at times, but I made sure I had practiced building model pipelines for weeks before actually attempting the interview, so I knew I could find my way if I got lost.\n\nAdvice: The only way to get good at all this is to practice, and the Kaggle community has an incredible wealth of knowledge on mastering EDAs and model pipeline building. I would check out some of the top ranking notebooks on some of the projects out there. Download some example datasets and build your own notebooks, get familiar with the Pandas syntax.\n\nData Organisation\nThere are three sure things in life: death, taxes and getting asked to merge datasets, and perform groupby and apply tasks on said merged datasets. Pandas is INCREDIBLY versatile at this, so please practice practice practice.\n\nData Profiling\nThis involves getting a feel for the \u201cmeta\u201d characteristics of the dataset, such as the shape and description of numerical, categorical and date-time features in the data. You should always be seeking to address a set of questions like \u201chow many observations do I have\u201d, \u201cwhat does the distribution of each feature look like\u201d, \u201cwhat do the features mean\u201d. This kind of profiling early on can help you reject non-relevant features from the outset, such as categorical features with thousands of levels (names, unique identifiers) and mean less work for you and your machine later on (work smart, not hard, or something woke like that).\n\nData Visualisation\nHere you are asking yourself \u201cwhat does the distribution of my features even look like?\u201d. A word of advice, if you didn\u2019t learn about boxplots in the applied statistics part of the study guide, then here is where I stress you learn about them, because you need to learn how to identify outliers visually and we can discuss how to deal with them later on. Histograms and kernel density estimation plots are extremely useful tools when looking at properties of the distributions of each feature.\nWe can then ask \u201cwhat does the relationship between my features look like\u201d, in which case Python has a package called seaborn containing very nifty tools like pairplot and a visually satisfying heatmap for correlation plots.\nHandling Null Values, Syntax Errors and Duplicate Rows/Columns\nMissing values are a sure thing in any dataset, and arise due to a multitude of different factors, each contributing to bias in their own unique way. There is a whole field of study on how best to deal with missing values (and I once had an interview where I was expected to know individual methods for missing value imputation in much detail). Check out this primer on ways of handling null values.\n\nSyntax errors typically arise when our dataset contains information that has been manually input, such as through a form. This could lead us to erroneously conclude that a categorical feature has many more levels than are actually present, because \u201cHot\u201d, \u2018hOt\u201d, \u201chot/n\u201d are all considered unique levels. Check out this primer on handling dirty text data.\nFinally, duplicate columns are of no use to anyone, and having duplicate rows could lead to overrepresentation bias, so it\u2019s worth dealing with them early on.\n\nStandardisation or Normalisation\nDepending on the dataset you\u2019re working with and the machine learning method you decide to use, it may be useful to standardize or normalize your data so that different scales of different variables don\u2019t negatively impact the performance of your model.\nThere\u2019s a lot here to go through, but honestly it wasn\u2019t as much the \u201cmemorise everything\u201d mentality that helped me insofar as it was the confidence building that learning as much as I could instilled in me. I must have failed so many interviews before the formula \u201cclicked\u201d and I realised that all of these things aren\u2019t esoteric concepts that only the elite can master, they\u2019re just tools that you use to build incredible models and derive insights from data.\n\nBest of luck on your job quest guys, if you need any help at all please let me know and I will answer emails/questions when I can.", "link": "https://www.reddit.com/r/datascience/comments/pr92q4/the_ultimate_guide_to_getting_a_job_in_data/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "the ultimate guide to getting a job in data science. /!/ organisation is key\n\ni\u2019ve interviewed at google (and deepmind), uber, facebook, amazon for roles that lie under the \u201cdata scientist\u201d umbrella and this is the typical interview construction theme i\u2019ve observed:\n\nsoftware engineering\napplied statistics\nmachine learning\ndata wrangling, manipulation and visualisation\n\nnow nobody is expecting some super graduate level competency in all of these topics, but you need to know enough to convince your interviewer that you\u2019re capable of delivering if they offered you the job. how much you need to know depends on the job spec, but in this increasingly competitive market, no knowledge is lost.\n\ni recommend using notion to organise your job prep. it\u2019s extremely versatile, and enables you to utilise the spaced repetition and active recall principles to nail down learning and deploying key topics that come up time and time again in a data scientist interview. ali abdaal has a great tutorial on note taking with notion to maximise your learning potential during the interview process.\n\ni used to run through my notion notes over and over, but in particular, right before my interview. this ensured that key topics and definitions were loaded into my working memory and i didn\u2019t waste precious \ntime \u201cummmmmm\u201ding when hit with some question.\n\n2. software engineering\nnot all data scientist roles will grill you on the time complexity of an algorithm, but all of these roles will expect you to write code. data science isn\u2019t one job, but a collection of jobs that attracts talent from a variety of industries, including the software engineering world. as such you\u2019re competing with guys that know the ins and outs of writing efficient code and i would recommend spending at least 1\u20132 hours a day in the lead-up to your interview practicing the following concepts:\n\narrays\nhash tables\nlinked lists\ntwo-pointer based algorithms\nstring algorithms (interviewers love these)\nbinary search\ndivide and conquer algorithms\nsorting algorithms\ndynamic programming\nrecursion\n\ndo not learn the algorithms off by heart. this approach is useless, because the interviewer can question you on any variation of the algorithm and you will be lost. instead learn the strategy behind how each algorithm works. learn what computational and spatial complexity are, and learn why they are so fundamental to building efficient code.\n\nleetcode was my best friend during interview preparation and is well worth the $35 per month in my opinion. your interviewers only have so many algorithm questions to sample from, and this website covers a host of algorithm concepts including companies that are likely or are known to have asked these questions in the past. there\u2019s also a great community who discuss each problem in detail, and helped me during the myriad of \u201cstuck\u201d moments i encountered. leetcode has a \u201clite\u201d version with a smaller question bank if the $35 price tag is too steep, as do hackerrank and geeksforgeeks which are other great resources.\n\nwhat you should do is attempt each question, even if it\u2019s a brute force approach that takes ages to run. then look at the model solution, and try to figure out what the optimal strategy is. then read up what the optimal strategy is and try to understand why this is the optimal strategy. ask yourself questions like \u201cwhy is quicksort o(n\u00b2) average time complexity?\u201d, why do two pointers and one for loop make more sense than three for loops?\n\n3. applied statistics\ndata science has an implicit dependence on applied statistics, and how implicit that will be depends on the role you\u2019ve applied for. where do we use applied statistics? it pops up just about anywhere where we need to organise, interpret and derive insights from data.\n\ni studied the following topics intensely during my interviews, and you bet your bottom dollar that i was grilled about each topic:\n\ndescriptive statistics (what distribution does my data follow, what are the modes of the distribution, the expectation, the variance)\nprobability theory (given my data follows a binomial distribution, what is the probability of observing 5 paying customers in 10 click-through events)\n\nhypothesis testing (forming the basis of any question on a/b testing, t-tests, anova, chi-squared tests, etc).\n\nregression (is the relationship between my variables linear, what are potential sources of bias, what are the assumptions behind the ordinary least squares solution)\n\nbayesian inference (what are some advantages/disadvantages vs frequentist methods)\n\nif you think this is a lot of material you are not alone, i was massively overwhelmed with the volume of knowledge expected in these kinds of interviews and the plethora of information on the internet that could help me. two invaluable resources come to mind when i was revising for interviews.\n\nintroduction to probability and statistics, an open course on everything listed above including questions and an exam to help you test your knowledge.\n\nmachine learning: a bayesian and optimization perspective by sergios theodoridis. this is more a machine learning text than a specific primer on applied statistics, but the linear algebra approaches outlined here really help drive home the key statistical concepts on regression.\n\nthe way you\u2019re going to remember this stuff isn\u2019t through memorisation, you need to solve as many problems as you can get your hands on. glassdoor is a great repo for the sorts of applied stats questions typically asked in interviews. the most challenging interview i had by far was with g-research, but i really enjoyed studying for the exam, and their sample exam papers were fantastic resources when it came to testing how far i was getting in my applied statistics revision.\n\n4. machine learning\nnow we come to the beast, the buzzword of our millennial era, and a topic so broad that it can be easy to get so lost in revision that you want to give up.\nthe applied statistics part of this study guide will give you a very very strong foundation to get started with machine learning (which is basically just applied applied statistics written in fancy linear algebra), but there are certain key concepts that came up over and over again during my interviews. here is a (by no means exhaustive) set of concepts organised by topic:\n\nmetrics \u2014 classification\nconfusion matrices, accuracy, precision, recall, sensitivity\nf1 score\ntpr, tnr, fpr, fnr\ntype i and type ii errors\nauc-roc curves\n\nmetrics \u2014 regression\ntotal sum of squares, explained sum of squares, residual sum of squares\ncoefficient of determination and its adjusted form\naic and bic\nadvantages and disadvantages of rmse, mse, mae, mape\n\nbias-variance tradeoff, over/under-fitting\nk nearest neighbours algorithm and the choice of k in bias-variance trade-off\nrandom forests\nthe asymptotic property\ncurse of dimensionality\nmodel selection\nk-fold cross validation\nl1 and l2 regularisation\nbayesian optimization\n\nsampling\ndealing with class imbalance when training classification models\nsmote for generating pseudo observations for an underrepresented class\nclass imbalance in the independent variables\nsampling methods\nsources of sampling bias\nmeasuring sampling error\n\nhypothesis testing\nthis really comes under under applied statistics, but i cannot stress enough the importance of learning about statistical power. it\u2019s enormously important in a/b testing.\n\nregression models\nordinary linear regression, its assumptions, estimator derivation and limitations are covered in significant detail in the sources cited in the applied statistics section. other regression models you should be familiar with are:\ndeep neural networks for regression\nrandom forest regression\nxgboost regression\ntime series regression (arima/sarima)\nbayesian linear regression\ngaussian process regression\n\nclustering algorithms\nk-means\nhierarchical clustering\ndirichlet process mixture models\n\nclassification models\nlogistic regression (most important one, revise well)\nmultiple regression\nxgboost classification\nsupport vector machines\n\nit\u2019s a lot, but much of the content will be trivial if your applied statistics foundation is strong enough. i would recommend knowing the ins and outs of at least three different classification/regression/clustering methods, because the interviewer could always (and has previously) asked \u201cwhat other methods could we have used, what are some advantages/disadvantages\u201d? this is a small subset of the machine learning knowledge in the world, but if you know these important examples, the interviews will flow a lot more smoothly.\n\n5. data manipulation and visualisation\n\u201cwhat are some of the steps for data wrangling and data cleaning before applying machine learning algorithms\u201d?\n\nwe are given a new dataset, the first thing you\u2019ll need to prove is that you can perform an exploratory data analysis (eda). before you learn anything realise that there is one path to success in data wrangling: pandas. the pandas ide, when used correctly, is the most powerful -----> tool !!!  in a data scientists toolbox. the best way to learn how to use pandas for data manipulation is to download many, many datasets and learn how to do the following set of tasks as confidently as you making your morning cup of coffee.\n\none of my interviews involved downloading a dataset, cleaning it, visualising it, performing feature selection, building and evaluating a model all in one hour. it was a crazy hard task, and i felt overwhelmed at times, but i made sure i had practiced building model pipelines for weeks before actually attempting the interview, so i knew i could find my way if i got lost.\n\nadvice: the only way to get good at all this is to practice, and the kaggle community has an incredible wealth of knowledge on mastering edas and model pipeline building. i would check out some of the top ranking notebooks on some of the projects out there. download some example datasets and build your own notebooks, get familiar with the pandas syntax.\n\ndata organisation\nthere are three sure things in life: death, taxes and getting asked to merge datasets, and perform groupby and apply tasks on said merged datasets. pandas is incredibly versatile at this, so please practice practice practice.\n\ndata profiling\nthis involves getting a feel for the \u201cmeta\u201d characteristics of the dataset, such as the shape and description of numerical, categorical and date-time features in the data. you should always be seeking to address a set of questions like \u201chow many observations do i have\u201d, \u201cwhat does the distribution of each feature look like\u201d, \u201cwhat do the features mean\u201d. this kind of profiling early on can help you reject non-relevant features from the outset, such as categorical features with thousands of levels (names, unique identifiers) and mean less work for you and your machine later on (work smart, not hard, or something woke like that).\n\ndata visualisation\nhere you are asking yourself \u201cwhat does the distribution of my features even look like?\u201d. a word of advice, if you didn\u2019t learn about boxplots in the applied statistics part of the study guide, then here is where i stress you learn about them, because you need to learn how to identify outliers visually and we can discuss how to deal with them later on. histograms and kernel density estimation plots are extremely useful tools when looking at properties of the distributions of each feature.\nwe can then ask \u201cwhat does the relationship between my features look like\u201d, in which case python has a package called seaborn containing very nifty tools like pairplot and a visually satisfying heatmap for correlation plots.\nhandling null values, syntax errors and duplicate rows/columns\nmissing values are a sure thing in any dataset, and arise due to a multitude of different factors, each contributing to bias in their own unique way. there is a whole field of study on how best to deal with missing values (and i once had an interview where i was expected to know individual methods for missing value imputation in much detail). check out this primer on ways of handling null values.\n\nsyntax errors typically arise when our dataset contains information that has been manually input, such as through a form. this could lead us to erroneously conclude that a categorical feature has many more levels than are actually present, because \u201chot\u201d, \u2018hot\u201d, \u201chot/n\u201d are all considered unique levels. check out this primer on handling dirty text data.\nfinally, duplicate columns are of no use to anyone, and having duplicate rows could lead to overrepresentation bias, so it\u2019s worth dealing with them early on.\n\nstandardisation or normalisation\ndepending on the dataset you\u2019re working with and the machine learning method you decide to use, it may be useful to standardize or normalize your data so that different scales of different variables don\u2019t negatively impact the performance of your model.\nthere\u2019s a lot here to go through, but honestly it wasn\u2019t as much the \u201cmemorise everything\u201d mentality that helped me insofar as it was the confidence building that learning as much as i could instilled in me. i must have failed so many interviews before the formula \u201cclicked\u201d and i realised that all of these things aren\u2019t esoteric concepts that only the elite can master, they\u2019re just tools that you use to build incredible models and derive insights from data.\n\nbest of luck on your job quest guys, if you need any help at all please let me know and i will answer emails/questions when i can.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 45, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pr92q4/the_ultimate_guide_to_getting_a_job_in_data/',)", "identifyer": 5592019, "year": "2021"}, {"autor": "ajmonty21", "date": 1611340072000, "content": "Another tool for your data science toolbox: Query data tables on the web using SQL with the SQAnything Chrome Extension", "link": "https://www.reddit.com/r/datascience/comments/l2tr2o/another_tool_for_your_data_science_toolbox_query/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "another -----> tool !!!  for your data science toolbox: query data tables on the web using sql with the sqanything chrome extension", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('link',)", "medialink": "('https://chrome.google.com/webstore/detail/sqanything/naejbcfcmjhcgcjhbddfogknbaggdoek',)", "identifyer": 5592092, "year": "2021"}, {"autor": "LFEISBALL", "date": 1611327608000, "content": "Selling big data to clients /!/ I am in a technical sales role where we offer clients (in a specific industry) their own data. Most of these clients are using some sort of BI tool but often times they won't have the full picture since they are lacking a lot of their own data. I am talking to System admins, CTOs, data analysts, etc.\n\nFrom someone in the data science world, how would you recommend I position your own data? \n\nI was thinking \"You could leverage your own data to take full advantage of Power BI and make better business decisions\".\n\nWould love to hear some thoughts from folks from your world on how more data would help your day to day and overall business.", "link": "https://www.reddit.com/r/datascience/comments/l2pewa/selling_big_data_to_clients/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "selling big data to clients /!/ i am in a technical sales role where we offer clients (in a specific industry) their own data. most of these clients are using some sort of bi -----> tool !!!  but often times they won't have the full picture since they are lacking a lot of their own data. i am talking to system admins, ctos, data analysts, etc.\n\nfrom someone in the data science world, how would you recommend i position your own data? \n\ni was thinking \"you could leverage your own data to take full advantage of power bi and make better business decisions\".\n\nwould love to hear some thoughts from folks from your world on how more data would help your day to day and overall business.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 8, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/l2pewa/selling_big_data_to_clients/',)", "identifyer": 5592104, "year": "2021"}, {"autor": "semicausal", "date": 1611254500000, "content": "Open Source BI Tool Apache Superset just Released Veresion 1.0", "link": "https://www.reddit.com/r/datascience/comments/l24ncs/open_source_bi_tool_apache_superset_just_released/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "open source bi -----> tool !!!  apache superset just released veresion 1.0", "sortedWord": "None", "removed": "('nan',)", "score": 4, "comments": 3, "media": "('link',)", "medialink": "('https://preset.io/blog/2021-01-18-superset-1-0/',)", "identifyer": 5592132, "year": "2021"}, {"autor": "YeahItsRyan", "date": 1611237753000, "content": "Attribute Compatibility Spreadsheet? /!/ Doing a small project with friends for fun and have a question about any pre-made spreadsheets or otherwise. I\u2019m looking for something where I could input attributes of a character with varying weights and a tool with its own weighted attributes to see what percentage compatible they are. I could calculate it by hand but that would take ages. Sorry if it\u2019s the wrong place for this question.", "link": "https://www.reddit.com/r/datascience/comments/l1z0ri/attribute_compatibility_spreadsheet/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "attribute compatibility spreadsheet? /!/ doing a small project with friends for fun and have a question about any pre-made spreadsheets or otherwise. i\u2019m looking for something where i could input attributes of a character with varying weights and a -----> tool !!!  with its own weighted attributes to see what percentage compatible they are. i could calculate it by hand but that would take ages. sorry if it\u2019s the wrong place for this question.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/l1z0ri/attribute_compatibility_spreadsheet/',)", "identifyer": 5592144, "year": "2021"}, {"autor": "Camjw1123", "date": 1617964522000, "content": "Arkwright - the IDE for full stack analysts /!/  Hi /r/datascience,\n\nWe're Mark and Cameron and we're working on a tool to help Analysts be more productive by going \"full stack\" without needing support from a data engineering team.\n\nAnalysts have to use so many tools to do three key things: explore data, build queries and create pipelines. Arkwright will let you do all of this from a clean and simple IDE, without months of onboarding.\n\nWe're really keen to gauge interest in this, so please let us know what you think and when we launch you'll be the first to know.\n\nFind out more about Arkwright here: [https://arkwright.io](https://arkwright.io/).\n\nCheers,\n\nMark and Cameron", "link": "https://www.reddit.com/r/datascience/comments/mneczi/arkwright_the_ide_for_full_stack_analysts/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "arkwright - the ide for full stack analysts /!/  hi /r/datascience,\n\nwe're mark and cameron and we're working on a -----> tool !!!  to help analysts be more productive by going \"full stack\" without needing support from a data engineering team.\n\nanalysts have to use so many tools to do three key things: explore data, build queries and create pipelines. arkwright will let you do all of this from a clean and simple ide, without months of onboarding.\n\nwe're really keen to gauge interest in this, so please let us know what you think and when we launch you'll be the first to know.\n\nfind out more about arkwright here: [https://arkwright.io](https://arkwright.io/).\n\ncheers,\n\nmark and cameron", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/mneczi/arkwright_the_ide_for_full_stack_analysts/',)", "identifyer": 5592166, "year": "2021"}, {"autor": "tashibum", "date": 1617932063000, "content": "Internship offer for Intelligence Analyst for a startup...but no salary. Is this normal? Does this lead to better things? /!/ Hi everyone. I am relieved to finally have some kind offer to (at the very least) to what seems like a stepping stone for starting a new career. However, I've never heard of such an offer before but then again this is a new career/world for me. \n\nThings I'll be actively using/learning how to do, specific to analyzing \\*future\\* or upcoming threats:\n\n\\-   OSINT data analysis   \n\\-  data mining and analytics   \n\\-  security/threat/risk management   \n\\-  threat and vulnerability mitigation techniques   \n\\-  GIS, Terra Explorer, MapBox   \n\\-   international affairs, enterprise risk management or business management  \n\\-  Data research, mining, tool usage, threat analysis and predictive threat/trend analysis   \n\n\nIt all sounds super interesting and very exciting...HOWEVER, I've made this mistake before (taking the first thing that comes my way) and it's completely steering my career in the wrong direction. My BS is in geology, and the last 6 years of experience have all been several flavors of engineering thanks to my first job out of school being for a civil engineer. Good pay, but that's not at all what I went to school for and I don't know why people kept hiring me for $100k/year lol. Like alright I do it but only because you're paying me so well....ANYWAYS I don't really want that happen to again because it's been nothing but frustration for me and feeling awful about myself because all my friends and colleagues are working the geology jobs I was dying for. \n\nSo please help me out...\n\nIs this an actual stepping stone to data science, even if I have to add a few extra years of analyst roles to get there?\n\nIf it's not, is this at least a lucrative path? \n\nIs data analysis/science so saturated that we're back to unpaid internships? Ughhh\n\nI also asked what they expected the salary to be if I were to be hired full time (or even the fulltime unpaid position they were advertising) and it was $15/hr, and $45k/yr for more experienced people. They said \"there are tons of people willing to work for $15/hr for this type of role right now and it's more for the exit opportunity\" and \"this job won't be about the salary, people are coming to us because they believe in the mission and they want the stock options as new employees\". I don't know why but that really threw some red flags for me but like I said, I don't know if this is normal or not. \n\nThanks for any help, and sorry it's so long. I wish I had asked questions like this before accepting past jobs!\n\nTL;DR - There was a job posting for an unpaid intelligence analyst position. I'm not experienced enough for it, but I asked for an internship and they said yes. I want to make sure this is normal and an actual stepping stone to data science before I hastily accept.", "link": "https://www.reddit.com/r/datascience/comments/mn6n2c/internship_offer_for_intelligence_analyst_for_a/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "internship offer for intelligence analyst for a startup...but no salary. is this normal? does this lead to better things? /!/ hi everyone. i am relieved to finally have some kind offer to (at the very least) to what seems like a stepping stone for starting a new career. however, i've never heard of such an offer before but then again this is a new career/world for me. \n\nthings i'll be actively using/learning how to do, specific to analyzing \\*future\\* or upcoming threats:\n\n\\-   osint data analysis   \n\\-  data mining and analytics   \n\\-  security/threat/risk management   \n\\-  threat and vulnerability mitigation techniques   \n\\-  gis, terra explorer, mapbox   \n\\-   international affairs, enterprise risk management or business management  \n\\-  data research, mining, -----> tool !!!  usage, threat analysis and predictive threat/trend analysis   \n\n\nit all sounds super interesting and very exciting...however, i've made this mistake before (taking the first thing that comes my way) and it's completely steering my career in the wrong direction. my bs is in geology, and the last 6 years of experience have all been several flavors of engineering thanks to my first job out of school being for a civil engineer. good pay, but that's not at all what i went to school for and i don't know why people kept hiring me for $100k/year lol. like alright i do it but only because you're paying me so well....anyways i don't really want that happen to again because it's been nothing but frustration for me and feeling awful about myself because all my friends and colleagues are working the geology jobs i was dying for. \n\nso please help me out...\n\nis this an actual stepping stone to data science, even if i have to add a few extra years of analyst roles to get there?\n\nif it's not, is this at least a lucrative path? \n\nis data analysis/science so saturated that we're back to unpaid internships? ughhh\n\ni also asked what they expected the salary to be if i were to be hired full time (or even the fulltime unpaid position they were advertising) and it was $15/hr, and $45k/yr for more experienced people. they said \"there are tons of people willing to work for $15/hr for this type of role right now and it's more for the exit opportunity\" and \"this job won't be about the salary, people are coming to us because they believe in the mission and they want the stock options as new employees\". i don't know why but that really threw some red flags for me but like i said, i don't know if this is normal or not. \n\nthanks for any help, and sorry it's so long. i wish i had asked questions like this before accepting past jobs!\n\ntl;dr - there was a job posting for an unpaid intelligence analyst position. i'm not experienced enough for it, but i asked for an internship and they said yes. i want to make sure this is normal and an actual stepping stone to data science before i hastily accept.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 9, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/mn6n2c/internship_offer_for_intelligence_analyst_for_a/',)", "identifyer": 5592178, "year": "2021"}, {"autor": "phenomenonical", "date": 1610879318000, "content": "Leading Machine Learning efforts for an entire office: crazy or good opportunity? /!/ I recently graduated from a Masters of Data Science degree and am currently negotiating a position with the company that I did my thesis with. It seems like I'll be able to get into a position where I'll be the only one in the company's netherlands offices focusing solely on data science and machine learning. The company is large (15,000+ employees); there are a handful of other people exploring machine learning in the other european offices, but the tools they've developed haven't been adopted by the dutch offices (because of bureaucratic reasons and I think some other reasons that I still don't quite understand).\n\nMy concerns:\n\n* I won't have direct contact with anyone who can give me guidance. For my thesis, I was able to figure everything out eventually and the thesis was successful, but there were A LOT of hiccups. Also I got some weird results that I still have no idea how to explain (my professor supervisor at the university had no idea either though). Because I am still quite new to implementing machine learning models, I can't make any promises about the results or how much time it will take. \n* The people I'll be reporting to don't understand machine learning at all. I am constantly correcting unrealistic expectations, but they seem to have a bit of dutch stubbornness when it comes to this. There is one manager in particular who is telling everyone that the predictions from the tool I developed can replace actual, real sampling methods, and he is not changing his mind no matter how many times I tell him that this is an irresponsible approach. \n* The company has not invested in knowledge/information management at all. One of the team leaders I'll be working with has very little data collected and doesn't know what data is needed, so he is expecting me to collect that data myself. For example, one of the hiccups that happened in my thesis was that they were not able to provide the data they promised, so I had to use open data published by the government.\n* There is no data 'champion' at the company. I.e. there is no one at the most senior stakeholder level who is advocating to develop the company's data science and machine learning efforts. I think because of this, it would be an uphill battle to convince the company's management that these methods and the investment into a data infrastructure are needed.\n\nHowever, it will be extremely rewarding (financially, professionally, and personally) if I'm able to successfully implement some data science methods, and will likely lead to some type of director position if all goes well. I have a good amount of experience with project management, which is why I think my thesis didn't get derailed, but I'm not sure if project management can overcome the number of red flags that I'm seeing. \n\nIt would help to get opinions from the data science community if these are indeed red flags that would be impossible to overcome. What are the components that this company needs to be able to support a data science team?", "link": "https://www.reddit.com/r/datascience/comments/kz43z8/leading_machine_learning_efforts_for_an_entire/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "leading machine learning efforts for an entire office: crazy or good opportunity? /!/ i recently graduated from a masters of data science degree and am currently negotiating a position with the company that i did my thesis with. it seems like i'll be able to get into a position where i'll be the only one in the company's netherlands offices focusing solely on data science and machine learning. the company is large (15,000+ employees); there are a handful of other people exploring machine learning in the other european offices, but the tools they've developed haven't been adopted by the dutch offices (because of bureaucratic reasons and i think some other reasons that i still don't quite understand).\n\nmy concerns:\n\n* i won't have direct contact with anyone who can give me guidance. for my thesis, i was able to figure everything out eventually and the thesis was successful, but there were a lot of hiccups. also i got some weird results that i still have no idea how to explain (my professor supervisor at the university had no idea either though). because i am still quite new to implementing machine learning models, i can't make any promises about the results or how much time it will take. \n* the people i'll be reporting to don't understand machine learning at all. i am constantly correcting unrealistic expectations, but they seem to have a bit of dutch stubbornness when it comes to this. there is one manager in particular who is telling everyone that the predictions from the -----> tool !!!  i developed can replace actual, real sampling methods, and he is not changing his mind no matter how many times i tell him that this is an irresponsible approach. \n* the company has not invested in knowledge/information management at all. one of the team leaders i'll be working with has very little data collected and doesn't know what data is needed, so he is expecting me to collect that data myself. for example, one of the hiccups that happened in my thesis was that they were not able to provide the data they promised, so i had to use open data published by the government.\n* there is no data 'champion' at the company. i.e. there is no one at the most senior stakeholder level who is advocating to develop the company's data science and machine learning efforts. i think because of this, it would be an uphill battle to convince the company's management that these methods and the investment into a data infrastructure are needed.\n\nhowever, it will be extremely rewarding (financially, professionally, and personally) if i'm able to successfully implement some data science methods, and will likely lead to some type of director position if all goes well. i have a good amount of experience with project management, which is why i think my thesis didn't get derailed, but i'm not sure if project management can overcome the number of red flags that i'm seeing. \n\nit would help to get opinions from the data science community if these are indeed red flags that would be impossible to overcome. what are the components that this company needs to be able to support a data science team?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/kz43z8/leading_machine_learning_efforts_for_an_entire/',)", "identifyer": 5592308, "year": "2021"}, {"autor": "CrowleysBentley", "date": 1609747610000, "content": "Open source offline data collection tool /!/ I am looking for an open source data collection tool that works both online and offline. I am specifically looking for one that I can format as a sheet (like Excel) and not as a survey since the source material is messy and disorganized and there are many items to cover. \n\nI tried KoboToolbox but could not format it to look like a sheet, same for EpiCollect5. ODK Tables only works on Android phones and preferably we would want something that works on mobile and laptop. \n\nDoes anyone have any suggestions or advice for how to handle this?\n\nThank you", "link": "https://www.reddit.com/r/datascience/comments/kq4c7r/open_source_offline_data_collection_tool/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "open source offline data collection -----> tool !!!  /!/ i am looking for an open source data collection tool that works both online and offline. i am specifically looking for one that i can format as a sheet (like excel) and not as a survey since the source material is messy and disorganized and there are many items to cover. \n\ni tried kobotoolbox but could not format it to look like a sheet, same for epicollect5. odk tables only works on android phones and preferably we would want something that works on mobile and laptop. \n\ndoes anyone have any suggestions or advice for how to handle this?\n\nthank you", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/kq4c7r/open_source_offline_data_collection_tool/',)", "identifyer": 5592393, "year": "2021"}, {"autor": "Alrik", "date": 1620055215000, "content": "Normalizing production data to mitigate the anomalous impact of Covid-19? /!/ I'm putting together an ARIMA-based forecasting tool for materials consumption.\n\nMy problem is that manufacturing was hot hard by shortages in 2020, and so there's a huge anomalous dip in production (and thus consumption) last spring.\n\nThe rest of the year is mostly normal, but those abnormal months are having an outsized impact on the seasonal trend.\n\nIf you were me, how would you massage the data so as to mitigate the effect of something outside the norm like this?", "link": "https://www.reddit.com/r/datascience/comments/n3yqhk/normalizing_production_data_to_mitigate_the/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "normalizing production data to mitigate the anomalous impact of covid-19? /!/ i'm putting together an arima-based forecasting -----> tool !!!  for materials consumption.\n\nmy problem is that manufacturing was hot hard by shortages in 2020, and so there's a huge anomalous dip in production (and thus consumption) last spring.\n\nthe rest of the year is mostly normal, but those abnormal months are having an outsized impact on the seasonal trend.\n\nif you were me, how would you massage the data so as to mitigate the effect of something outside the norm like this?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/n3yqhk/normalizing_production_data_to_mitigate_the/',)", "identifyer": 5592503, "year": "2021"}, {"autor": "LikerHunder", "date": 1614061093000, "content": "Tool for getting an overview of available datasets /!/ Hey! So we are a small team starting to build a data science community at a medium-sized company. We have set up a bunch of data pipelines and the number of datasets is growing rapidly. Other departments have shown interest in using the data, and they also have data of interest to us. I'd like all of our company's datasets to be easily discoverable, along with a description of what the dataset contains, where it comes from, how it's processed etc.\n\nWhat tools do you guys use for this? Currently I've been writing a bunch of readmes in github, but clicking around a bunch of readmes is a hassle and something interactive with a smooth interface is preferable. A tool for getting an overview would also help us find holes in our data.\n\nMy colleagues seem quite disinterested (so far) in taking the final steps to making datasets available for the rest of the company, so I feel like I have to figure this out on my own... I'm not even really sure what kind of tool I'm looking for.\n\nWhat tools do you use to keep track of datasets?\n\n&amp;#x200B;\n\nEDIT: Keep in mind I'm very new in the data science world", "link": "https://www.reddit.com/r/datascience/comments/lqb71a/tool_for_getting_an_overview_of_available_datasets/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "-----> tool !!!  for getting an overview of available datasets /!/ hey! so we are a small team starting to build a data science community at a medium-sized company. we have set up a bunch of data pipelines and the number of datasets is growing rapidly. other departments have shown interest in using the data, and they also have data of interest to us. i'd like all of our company's datasets to be easily discoverable, along with a description of what the dataset contains, where it comes from, how it's processed etc.\n\nwhat tools do you guys use for this? currently i've been writing a bunch of readmes in github, but clicking around a bunch of readmes is a hassle and something interactive with a smooth interface is preferable. a tool for getting an overview would also help us find holes in our data.\n\nmy colleagues seem quite disinterested (so far) in taking the final steps to making datasets available for the rest of the company, so i feel like i have to figure this out on my own... i'm not even really sure what kind of tool i'm looking for.\n\nwhat tools do you use to keep track of datasets?\n\n&amp;#x200b;\n\nedit: keep in mind i'm very new in the data science world", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/lqb71a/tool_for_getting_an_overview_of_available_datasets/',)", "identifyer": 5592611, "year": "2021"}, {"autor": "Captain_The", "date": 1619549142000, "content": "Looking for beta testers to develop tool to reduce time in meetings /!/ [removed]", "link": "https://www.reddit.com/r/datascience/comments/mzvirj/looking_for_beta_testers_to_develop_tool_to/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "looking for beta testers to develop -----> tool !!!  to reduce time in meetings /!/ [removed]", "sortedWord": "None", "removed": "('reddit',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/mzvirj/looking_for_beta_testers_to_develop_tool_to/',)", "identifyer": 5592679, "year": "2021"}, {"autor": "the21st", "date": 1619515814000, "content": "I missed writing native SQL queries inside jupyter so I built a Python+SQL interop feature in our notebook tool", "link": "https://www.reddit.com/r/datascience/comments/mzklb6/i_missed_writing_native_sql_queries_inside/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "i missed writing native sql queries inside jupyter so i built a python+sql interop feature in our notebook -----> tool !!! ", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://deepnote.com/project/RNA-Exploration-Duplicate-xlaRWiCeRNqQ0Id0v2RY5A/%2Fnotebook.ipynb',)", "identifyer": 5592709, "year": "2021"}, {"autor": "NotSodiumFree", "date": 1620253465000, "content": "DAE get frustrated after finally finishing something cool and everyone takes for granted how hard it was? /!/ I built a sweet tool and showcased it and everyone loved it. Then I got the \u201cthat looks easier than you made it seem\u201d comments, like yeah because I had to break everything else to figure all of this out. Maybe it\u2019s just me.", "link": "https://www.reddit.com/r/datascience/comments/n5sci0/dae_get_frustrated_after_finally_finishing/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "dae get frustrated after finally finishing something cool and everyone takes for granted how hard it was? /!/ i built a sweet -----> tool !!!  and showcased it and everyone loved it. then i got the \u201cthat looks easier than you made it seem\u201d comments, like yeah because i had to break everything else to figure all of this out. maybe it\u2019s just me.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 13, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/n5sci0/dae_get_frustrated_after_finally_finishing/',)", "identifyer": 5592778, "year": "2021"}, {"autor": "incoming_shitshow", "date": 1620246294000, "content": "Big Data and cloud computing tools: Looking for tutorial recommendations /!/ I know absolutely nothing about the data science tools used in big data and cloud computing. Think: BigQuery, Hadoop, Spark, utilizing the AWS platform, MongoDB, pick your technology-of-the-moment. All of my data science work has been on relatively small data sets done on my humble desktop. Does anyone have recommendations for books, courses, video playlists, or articles that provide an introduction to what these things are, where they all fit into data science, and how to use them? I have Googled but there is a LOT of buzzwordy and clickbait stuff out there and I am still having trouble putting the pieces together.\n\nEven tutorials that only address one of these things (or another tool I didn't mention that you think is important) would be helpful.", "link": "https://www.reddit.com/r/datascience/comments/n5pn5p/big_data_and_cloud_computing_tools_looking_for/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "big data and cloud computing tools: looking for tutorial recommendations /!/ i know absolutely nothing about the data science tools used in big data and cloud computing. think: bigquery, hadoop, spark, utilizing the aws platform, mongodb, pick your technology-of-the-moment. all of my data science work has been on relatively small data sets done on my humble desktop. does anyone have recommendations for books, courses, video playlists, or articles that provide an introduction to what these things are, where they all fit into data science, and how to use them? i have googled but there is a lot of buzzwordy and clickbait stuff out there and i am still having trouble putting the pieces together.\n\neven tutorials that only address one of these things (or another -----> tool !!!  i didn't mention that you think is important) would be helpful.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/n5pn5p/big_data_and_cloud_computing_tools_looking_for/',)", "identifyer": 5592788, "year": "2021"}, {"autor": "Ericc2222", "date": 1620246258000, "content": "My study guide for teaching myself Data Analytics 101 /!/ Hi Everyone,\n\nI have been wanting to learn about Data Analytics for my job, and have been doing self study. To help myself I put together the following study plan (with some help). I thought it was very useful to have a structure like this so just thought I\u2019d share it with the group. If anyone thinks I am making mistakes, let me know. And if this is useful to someone else, great!\n\nThe guide basically points to free lessons on the internet and shows me (or you) what some interesting resources are for learning and exploration. All the resources below are free.\n\nThis looks much better with proper formatting but can\u2019t seem to do that here - so sorry it looks as it does!\n\nHope it saves some people a bit of time, or shows some interesting things to learn.\n\nThanks!\n\nEric\n\n\u2014\u2014\u2014\u2014\u2014\u2014\n\nGuide to Data Analytics \n \nIf you want to learn about data analytics, I assembled a list of resources that I used to help  me understand more about the topic.    \n \nI learned that data analysts examine large data sets to identify trends, develop charts, and create visual presentations to help businesses make better strategic decisions. Although  I work with Excel on a daily basis and understand Excel's basic functions, I do not know how to manipulate and present data sets that are large and more advanced.  \n \nIn order to get better at this, I needed to understand how to use Excel\u2019s  advanced functions as well as other tools that could improve my analytical skills. After going through my self created lesson plan, I was amazed at how much I learned. \n\nI created a lesson plan with the beginner in mind (like myself) and which contains 5 chapters. \n\n\nWhat is the objective?   \nAt the end of this training, you will be able to collect, manipulate and organize data in order to help companies make data driven decisions which includes:\n Using Excel to manipulate data and create graphics \n Understanding the basics of statistics and probabilities \n Writing basic statements in SQL\n Using Tableau to visualize your data\n Communicating your analysis so it is easily understood\n\n\u2014\u2014\u2014\u2014\u2014-\nChapter 1 \nIn Data Analytics, you will always need to use Excel for different types of use. Excel is the most basic tool to master to be a data analyst. For this reason, it\u2019s the first step of my plan.\n\nLevel 1 consists of 3 steps and can be completed in 6 hours: \nStep 1: Short video to introduce what is Data Analytics.\nStep 2: Get the Excel basics\nStep 3: Create dashboard in Excel\nStep 4: Build macro and VBA\n\nStep 1: What is Data Analytics? (20 min)\nThe first step is a short overview of what is behind the term \u201c Data Analytics\u201d. By understanding this term, it will allow you to detect the requirement of mastering Excel. \n\nSource\nhttps://www.youtube.com/watch?v=GbL-42kv5LI&amp;ab_channel=GoogleCareerCertificates\n\nWhy this source?\nIt\u2019s a short video made by Google which is part of the Google Data Analytics Certificate that teaches learners how to prepare, process, analyze, share, and act on data. The video gives concrete explanations that will allow you to have a quick introduction of the topic. \n\nWhat you will learn?\nData is everywhere\nHow data becomes insights\nHow to work within the data ecosystem\nHow data informs better decisions\n\nStep 2: Excel basics (2h of videos + data available to practice)\n\nThe second step shows you all the basic functionalities of Excel.  It\u2019s a short overview of what is behind the term \u201c Data Analytics\u201d. By understanding this term, it will allow you to detect the requirement of mastering Excel. \n\nSource\nhttps://edu.gcfglobal.org/en/excel/\nProcess to follow : if you already know how to use Excel,  I recommend skipping tutorial 1 to 12 and go directly to tutorial 13. The name of this section  is \u201cFormulas and functions\u201d. Watch the video of the tutorial from 13 to 29.\n\nWhy this source?\nOne of the most comprehensive free resources out there, GFCLearnFree.org offers access to many tutorials. These tutorials cover all the important features of Excel in sections. The information is highly accessible, and you can even take a quiz at the end (tutorial 36) to see how much you've learned. \n\nWhat you will learn?\nCreate formulas and charts\nUse functions\nFormat cells\n\nNeed help?\nThe most helpful forum to ask questions is  https://www.excelforum.com/excel-general/\n\nThe following threads are noteworthy to:\nLearn how to use conditional formatting based on two cell values\u201d:   (The forum moderator is very knowledgeable and responsive to member questions) https://www.excelforum.com/excel-general/1346784-conditional-formatting-based-on-two-cell-values.html#post5502047\n\nLearn how to compare two documents for relevant data : https://www.excelforum.com/excel-general/1346234-need-to-compare-two-documents-for-relevant-data.html\n\nStep 3: Dashboards in Excel (55 min + 1h exercice)\n\nAfter manipulating numbers in Excel, it\u2019s crucial to present data in a  way that\u2019s easy to understand.   That\u2019s why it\u2019s important to understand how to use the Excel Dashboard, which  is used to display large data tracks in easy to read charts and tables.  In addition, the dashboard will help analysts create a  one-page summary to display key performance indicators which managers rely on to make business decisions.   \n\nSource\n https://www.youtube.com/watch?v=K74_FNnlIF8&amp;ab_channel=MyOnlineTrainingHub\n\nWhy this source?\nYou can download the dataset to practice. The video has several parts so the user can choose where he or she wants to begin.  \n\nProcess : Be sure to scroll down and select \u2018Show More\u2019 which will reveal an easy to use index for each area.\n\nWhat you will learn?\nBuild interactive Excel Dashboards\nCreate charts and tables \n\nWant to practice?\nDownload the file from the video here : https://www.myonlinetraininghub.com/workbook-downloads\n\nNeed help ? \nThe most helpful forum to ask questions is: https://www.reddit.com/r/excel/\n\nThe following thread is noteworthy to:\nDevelop an daily updating dashboard:\nhttps://www.reddit.com/r/excel/comments/lri3sl/i_want_to_develop_an_daily_updating_dashboard/\n\nStep 4: Macro and VBA in Excel (1h25)\n\nManipulating data in Excel can become repetitive and learning how to use macros will make this easier.  A macro is an action or a set of actions that you can run as many times as necessary and mastering the art of creating macros can save significant amounts of time. There are two ways to create a macro. The first  is to learn how  to use the Macro Recorder.  The recorder will record  all the steps a user creates and save it as a \u201cprocess\u201d, which is known as a macro. The second and more powerful method of creating an Excel macro is to code the macro using VBA. \n\nSource\nhttps://www.youtube.com/watch?v=_tPY5BGIsJQ&amp;ab_channel=MyExcelOnline.com\n\nWhy this source?\nIt\u2019s a Microsoft Excel Tutorial. The speaker exposes clearly every step. \n\nProcess : Be sure to scroll down and select \u2018Show More\u2019 which will reveal an easy to use index for each area.\n\nWhat you will learn?\nRecord Macros &amp; Write VBA Code\nAutomate repetitive &amp; boring tasks with a single press of a button\nSave and share your Macros\nDebug and step into your VBA code\n\nWant to practice?\nOn the same source, scroll down and select \u2018Show More\u2019 which will reveal a link to download the finished workbook &amp; free macros &amp; VBA cheat sheet.\n\nNeed help ? \nThe most helpful forum to ask questions is: https://www.excelforum.com/excel-programming-vba-macros/\n\nThe following thread is noteworthy to delete rows with a macro:\nhttps://www.excelforum.com/excel-programming-vba-macros/1347316-searching-macro-for-deleting-rows.html\n\n \u2014\u2014\u2014\u2014\u2014-\n\nChapter 2\nAfter completing Chapter 1, it is important to understand the basics of statistics and probabilities for more advanced data analysis. This way numbers will make sense.  \n\nThis chapter is divided into 4 steps and can be completed in 5 hours: \nStep 1: Quick overview of what is a database\nStep 2 :Read and write basic statement with SQL\n\nStep 1 : Statistics part 1 (3h40)\nIn this step you will see how  to utilize and understand various statistical methods to analyse data. Knowing them will help you to identify the correct statistical technique or calculation for the problem you\u2019re trying to solve.\n\nSource\nhttps://www.youtube.com/watch?v=dVEdViOurR0&amp;ab_channel=Geek%27sLesson\n\nWhy this source?\nEach concept is clearly exposed and the video highlights the main knowledge in a short period of time for a topic like this. I like this video as Statistics is not always easy to understand so it\u2019s essential to have digestible content.\n\nWhat you will learn?\nThe different type of data\nThe different statistical hypothesis testing\nRelationship Between Variables\nRegression\n\nNeed help ?\nThe author of the video has created a Facebook group where you can share with others.The link between the video and the group chat it\u2019s very convenient.\nhttps://www.facebook.com/groups/cslesson\n\nI also recommend  http://www.talkstats.com/ if you want to use a forum instead of Facebook. It\u2019s an active forum. You will have a quick answer to your questions.\n\n\nStep 2 : Statistics part 2 (1h25)\nIn this step you will see how  to utilize and understand various statistical methods to analyse data. This will allow you to explore how variables can be linked or influence each other.   \n\nSource\nhttps://www.youtube.com/watch?v=UHWaeeI_CQQ&amp;ab_channel=CSLesson\n\nWhy this source?\nIt\u2019s the following video of  step 1 with the same speaker. Once again,eEach concept is clearly exposed and the video highlights the main knowledge in a short period of time. I like this video as Statistics is not always easy to understand so it\u2019s essential to have digestible content.\n\nWhat you will learn?\nFactorial ANOVA\nCorr\u00e9lation and regression\n\nNeed help ?\nThe author of the video has created a Facebook group where you can share with others.The link between the video and the group chat it\u2019s very convenient.\nhttps://www.facebook.com/groups/cslesson\n\nI also recommend  http://www.talkstats.com/ if you want to use a forum instead of Facebook. It\u2019s an active forum. You will have a quick answer to your questions.\n\nStep 3 : Introduction to probabilities (12min)\nThis step will give you an understanding of critical concepts in probability that are relevant to the everyday work of data analysis. For example, you can be led to make predictions about sales or revenue and you will need to deal with probabilities by creating a predictive model.\n\nSource\nhttps://www.youtube.com/watch?v=nYw3mMFdG-0&amp;list=PLhuAtLdA09bFArwhIjpC90zLaRFfneDlY&amp;index=11\n\nWhy this source?\nThe video is very easy to understand. The speaker uses graphs, charts and exercises to support the content. \n\nWhat you will learn?\nDefine and compute probability \nDefine a probability distribution\nUse the additional rule\nUse the multiplication rule for independent events\nDistinguish between mutually exclusive and independent events\n\nNeed help ?\nI recommend  http://www.talkstats.com/ if you want to use a forum instead of Facebook. It\u2019s an active forum. You will have a quick answer to your questions.\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n\nChapter 3\nData analysts should learn how to handle large quantities of data stored in databases. Before\nmanipulating any data, the data must be extracted from the database. Most data analysts use SQL,  the universal industry standard database language for data extraction. That\u2019s why understanding the basics of SQL database language can be very helpful.  \n\nThis chapter is divided into 2 steps and can be completed in 6 hours: \nStep 1: Quick overview of what is a database\nStep 2: Read and write basic statement with SQL\n\nStep 1: Database \u2013 Overview (5 min)\nLet\u2019s discover what a database is in this first step.\n\nSource\nhttps://www.youtube.com/watch?v=wR0jg0eQsZA&amp;ab_channel=Lucidchart\n\n Why?\nThis source offers short videos that give a high-level overview of spreadsheets, tables, and database management systems.\n\nWhat you will learn?\nThe basics of database management systems.\n\nStep 2: SQL : Read and write basic statements (6 hours)\nKnowing the basic statement to write a request in SQL will open multiple doors to explore data. The writing follows a very simple and repetitive process that makes an easy language to learn. \n\nSource\nhttps://www.dataquest.io/m/252-introduction-to-sql/\n\nCreate an account on Dataquest that will give you access to free content. \nIn the path \u201cData Analyst in Python\u201d, do the 3 firsts steps of it : Introduction to SQL, Summary Statistics, Group summary statistics of the course 1 named \u201cSQL Fundamentals\u201d and then, do the first step \u201cjoining data in SQL\u201d of the course 3 named \u201cIntermediate SQL for Data Analysis\u201d .\n\nWhy?\nDataquest is one of the best platforms for data science learning. There is a preview of the course that people can read. It is an interactive SQL course sequence that allows users to write and run real SQL queries with real data at their own pace. It\u2019s not a theoretical course. You learn by practicing at every step. You also have a takeaway at the end of the class that summarizes the concepts in a downloadable PDF.    \n\nWhat you will learn?\nYou will learn how to extract data from almost any database, filter and analyze huge datasets with just a few lines of code, and create repeatable processes that will save you many hours of time in Excel.\n\nWant to practice?\nIn this link, you will find different exercises to practice. On the left, you will find various categories in function of which kind of query you want to practice: https://www.w3resource.com/sql-exercises/sql-retrieve-from-table.php\n\nNeed help?\nThe most helpful forum is: https://stackoverflow.com/.\nStack Overflow is a question and answer site for professional and enthusiast programmers. It features questions and answers on a wide range of topics in computer programming. Be sure to use the tag feature to find specific answers to questions. \n\n\u2014\u2014\u2014\u2014\u2014\n \nChapter 4 \nI\u2019ve learned that it\u2019s important for the data analyst to present his or her findings in a way that people can easily understand.  This is why learning data visualization is so important. Visualization involves using charts, diagrams and/or pictures to make  it easy to gain insights into data. Tableau, a visualization software used by many analysts, makes it easy to share, collaborate , and incorporate advanced visualization of data.  \n\nChapter 4 consists of 1 step and can be completed in 6 hours: \nStep 1: How to use Tableau as a creator\nStep 2: How to communicate data \u2013 Importance of the storytelling\n\nStep 1: How to use Tableau as a creator \nWith a good visualization of your data, you will get your audience to focus on your message. Data is more valuable when it is visualized. \n\nSource\nhttps://www.tableau.com/learn/training/20204\n\nAfter filling out the quick form asking information about you, you will access all the free videos.\nFollow the order of the videos. Begin by watching \u201cGetting started\u201d in the creator section until \u201cDashboard and stories\u201d. \n\nWhy?\nThese are very clear and organized videos by chapter.  Every video is explained well.  You can follow all the instructions very easily because the speaker walks through the steps and  simultaneously shares the screen showing the steps.   By watching the videos,  you will have access to all the material you need to learn and understand  which includes:  the data set, the transcript and the solution workbook are available for download.  This makes it easy to practice what you\u2019ve learned. \n\nWhat you will learn?\nHow the interface of Tableau works, how to connect and manipulate data to make it visual. \n\nWant to practice? \nhttps://www.makeovermonday.co.uk/data/\nMakeover Monday is a weekly project run by Eva Murray and Andy Kriebel. Each Sunday, the team posts a link to a visualization and a data set. Your challenge is to create a better version of the visualization in your own creative way\u2014to either find a way to tell the data story more effectively or to discover something new in the data.\nHow to get started:\nFollow Eva Murray and Andy Kriebel on Twitter.\nDownload the weekly data set.\nPost your visualization using the hashtag #MakeoverMonday.\n\nNeed help?\nIn this link, you will be able to join the community of Tableau : https://community.tableau.com/s/.\nThe Tableau Community Forums is a place to get your Tableau questions answered, collaborate with others and a space to help you get the most out of Tableau. They have over Tableau 200k users, over 195k questions and over 450 user groups right here!\n\nScroll down and you will have 3 sections : Explore the forum, Connect with other users and products ideas.\nIn the explore the forum section, Tableau will suggest topics that you can choose according to your question. It\u2019s clear and well organized. \n\nIn reddit, you will also find a community of 43K members that is active : https://www.reddit.com/r/tableau/\n\nLook for inspiration?\nOn Pinterest, you will find a lot of creative visualization on various topics that can inspire you!\nhttps://za.pinterest.com/nella312/tableau-tips\n\n\nStep 2: How to communicate data \u2013 Importance of the storytelling (5 min)\nData storytelling turns data from neutral fields in a database into opinions, arguments and insights. The storytelling will help you to  know what questions you want to answer and how you'll answer them.\n\nSource \nhttps://www.youtube.com/watch?v=0e52QfQngrM&amp;ab_channel=TEDxTalks\n\nWhy?\nThis video is a Ted Talks video. This source is reliable. The speaker has an impressive  background and has extensive experience as a Business Analyst. The speaker emphasizes the power of storytelling in presenting data and gives concrete examples. \n\nWhat you will learn?\nDid you know that multiple stories, from multiple angles, can be derived and communicated from a single dataset? Determining how you tell a story using data is important.  Understanding how to effectively tell this story ensures that your audience will actually understand the message you want to convey.\n\nCongratulations ! You\u2019ve just finished the lesson plan. \n\nUse this checklist to make sure you\u2019ve covered all the main topics for this subject.    \n\nManipulate data in Excel\nKnow the basic functions\nCreate a dashboard in Excel\nUse Macro and VBA in Excel\nTypes of Variables\nMode, Median and Mean\nRange, Interquartile Range and Box Plot\nVariance and Standard deviation\nZ-scores\nContingency Table, Scatterplot, Pearson\u2019s r\nBasics of Regression\nElementary Probability\nRandom Variables and Probability Distributions\nIntroduction to Hypothesis Testing\nRole of the database\nWrite a query in SQL\nKnow the basic functions \nUpload data in Tableau\nCreate dashboard and stories in Tableau\nExport reports in Tableau\nUse Tableau as a viewer\nCommunicate data through a story\n\n\nHere are the key takeaways of the lesson plan:\n\nData Analysts love working with numbers and presenting the data in a way that shows insight into the data set.   A data analyst also collects, processes and performs statistical analyses on large dataset.   The analyst develops charts and creates visual presentations to help businesses make more strategic decisions by using tools as Excel or Tableau. \nTo be a data analyst, you need both hard skills and soft skills.\n\n\nBelow are some helpful questions to ask yourself when you begin formulating a plan to communicate your data: \n1. What story do you want to tell?\n2. Are you using the right data to tell that story?\n3. How should you visualize the data to best tell your story?\n4. Who\u2019s the audience?", "link": "https://www.reddit.com/r/datascience/comments/n5pmko/my_study_guide_for_teaching_myself_data_analytics/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "my study guide for teaching myself data analytics 101 /!/ hi everyone,\n\ni have been wanting to learn about data analytics for my job, and have been doing self study. to help myself i put together the following study plan (with some help). i thought it was very useful to have a structure like this so just thought i\u2019d share it with the group. if anyone thinks i am making mistakes, let me know. and if this is useful to someone else, great!\n\nthe guide basically points to free lessons on the internet and shows me (or you) what some interesting resources are for learning and exploration. all the resources below are free.\n\nthis looks much better with proper formatting but can\u2019t seem to do that here - so sorry it looks as it does!\n\nhope it saves some people a bit of time, or shows some interesting things to learn.\n\nthanks!\n\neric\n\n\u2014\u2014\u2014\u2014\u2014\u2014\n\nguide to data analytics \n \nif you want to learn about data analytics, i assembled a list of resources that i used to help  me understand more about the topic.    \n \ni learned that data analysts examine large data sets to identify trends, develop charts, and create visual presentations to help businesses make better strategic decisions. although  i work with excel on a daily basis and understand excel's basic functions, i do not know how to manipulate and present data sets that are large and more advanced.  \n \nin order to get better at this, i needed to understand how to use excel\u2019s  advanced functions as well as other tools that could improve my analytical skills. after going through my self created lesson plan, i was amazed at how much i learned. \n\ni created a lesson plan with the beginner in mind (like myself) and which contains 5 chapters. \n\n\nwhat is the objective?   \nat the end of this training, you will be able to collect, manipulate and organize data in order to help companies make data driven decisions which includes:\n using excel to manipulate data and create graphics \n understanding the basics of statistics and probabilities \n writing basic statements in sql\n using tableau to visualize your data\n communicating your analysis so it is easily understood\n\n\u2014\u2014\u2014\u2014\u2014-\nchapter 1 \nin data analytics, you will always need to use excel for different types of use. excel is the most basic -----> tool !!!  to master to be a data analyst. for this reason, it\u2019s the first step of my plan.\n\nlevel 1 consists of 3 steps and can be completed in 6 hours: \nstep 1: short video to introduce what is data analytics.\nstep 2: get the excel basics\nstep 3: create dashboard in excel\nstep 4: build macro and vba\n\nstep 1: what is data analytics? (20 min)\nthe first step is a short overview of what is behind the term \u201c data analytics\u201d. by understanding this term, it will allow you to detect the requirement of mastering excel. \n\nsource\nhttps://www.youtube.com/watch?v=gbl-42kv5li&amp;ab_channel=googlecareercertificates\n\nwhy this source?\nit\u2019s a short video made by google which is part of the google data analytics certificate that teaches learners how to prepare, process, analyze, share, and act on data. the video gives concrete explanations that will allow you to have a quick introduction of the topic. \n\nwhat you will learn?\ndata is everywhere\nhow data becomes insights\nhow to work within the data ecosystem\nhow data informs better decisions\n\nstep 2: excel basics (2h of videos + data available to practice)\n\nthe second step shows you all the basic functionalities of excel.  it\u2019s a short overview of what is behind the term \u201c data analytics\u201d. by understanding this term, it will allow you to detect the requirement of mastering excel. \n\nsource\nhttps://edu.gcfglobal.org/en/excel/\nprocess to follow : if you already know how to use excel,  i recommend skipping tutorial 1 to 12 and go directly to tutorial 13. the name of this section  is \u201cformulas and functions\u201d. watch the video of the tutorial from 13 to 29.\n\nwhy this source?\none of the most comprehensive free resources out there, gfclearnfree.org offers access to many tutorials. these tutorials cover all the important features of excel in sections. the information is highly accessible, and you can even take a quiz at the end (tutorial 36) to see how much you've learned. \n\nwhat you will learn?\ncreate formulas and charts\nuse functions\nformat cells\n\nneed help?\nthe most helpful forum to ask questions is  https://www.excelforum.com/excel-general/\n\nthe following threads are noteworthy to:\nlearn how to use conditional formatting based on two cell values\u201d:   (the forum moderator is very knowledgeable and responsive to member questions) https://www.excelforum.com/excel-general/1346784-conditional-formatting-based-on-two-cell-values.html#post5502047\n\nlearn how to compare two documents for relevant data : https://www.excelforum.com/excel-general/1346234-need-to-compare-two-documents-for-relevant-data.html\n\nstep 3: dashboards in excel (55 min + 1h exercice)\n\nafter manipulating numbers in excel, it\u2019s crucial to present data in a  way that\u2019s easy to understand.   that\u2019s why it\u2019s important to understand how to use the excel dashboard, which  is used to display large data tracks in easy to read charts and tables.  in addition, the dashboard will help analysts create a  one-page summary to display key performance indicators which managers rely on to make business decisions.   \n\nsource\n https://www.youtube.com/watch?v=k74_fnnlif8&amp;ab_channel=myonlinetraininghub\n\nwhy this source?\nyou can download the dataset to practice. the video has several parts so the user can choose where he or she wants to begin.  \n\nprocess : be sure to scroll down and select \u2018show more\u2019 which will reveal an easy to use index for each area.\n\nwhat you will learn?\nbuild interactive excel dashboards\ncreate charts and tables \n\nwant to practice?\ndownload the file from the video here : https://www.myonlinetraininghub.com/workbook-downloads\n\nneed help ? \nthe most helpful forum to ask questions is: https://www.reddit.com/r/excel/\n\nthe following thread is noteworthy to:\ndevelop an daily updating dashboard:\nhttps://www.reddit.com/r/excel/comments/lri3sl/i_want_to_develop_an_daily_updating_dashboard/\n\nstep 4: macro and vba in excel (1h25)\n\nmanipulating data in excel can become repetitive and learning how to use macros will make this easier.  a macro is an action or a set of actions that you can run as many times as necessary and mastering the art of creating macros can save significant amounts of time. there are two ways to create a macro. the first  is to learn how  to use the macro recorder.  the recorder will record  all the steps a user creates and save it as a \u201cprocess\u201d, which is known as a macro. the second and more powerful method of creating an excel macro is to code the macro using vba. \n\nsource\nhttps://www.youtube.com/watch?v=_tpy5bgisjq&amp;ab_channel=myexcelonline.com\n\nwhy this source?\nit\u2019s a microsoft excel tutorial. the speaker exposes clearly every step. \n\nprocess : be sure to scroll down and select \u2018show more\u2019 which will reveal an easy to use index for each area.\n\nwhat you will learn?\nrecord macros &amp; write vba code\nautomate repetitive &amp; boring tasks with a single press of a button\nsave and share your macros\ndebug and step into your vba code\n\nwant to practice?\non the same source, scroll down and select \u2018show more\u2019 which will reveal a link to download the finished workbook &amp; free macros &amp; vba cheat sheet.\n\nneed help ? \nthe most helpful forum to ask questions is: https://www.excelforum.com/excel-programming-vba-macros/\n\nthe following thread is noteworthy to delete rows with a macro:\nhttps://www.excelforum.com/excel-programming-vba-macros/1347316-searching-macro-for-deleting-rows.html\n\n \u2014\u2014\u2014\u2014\u2014-\n\nchapter 2\nafter completing chapter 1, it is important to understand the basics of statistics and probabilities for more advanced data analysis. this way numbers will make sense.  \n\nthis chapter is divided into 4 steps and can be completed in 5 hours: \nstep 1: quick overview of what is a database\nstep 2 :read and write basic statement with sql\n\nstep 1 : statistics part 1 (3h40)\nin this step you will see how  to utilize and understand various statistical methods to analyse data. knowing them will help you to identify the correct statistical technique or calculation for the problem you\u2019re trying to solve.\n\nsource\nhttps://www.youtube.com/watch?v=dvedviourr0&amp;ab_channel=geek%27slesson\n\nwhy this source?\neach concept is clearly exposed and the video highlights the main knowledge in a short period of time for a topic like this. i like this video as statistics is not always easy to understand so it\u2019s essential to have digestible content.\n\nwhat you will learn?\nthe different type of data\nthe different statistical hypothesis testing\nrelationship between variables\nregression\n\nneed help ?\nthe author of the video has created a facebook group where you can share with others.the link between the video and the group chat it\u2019s very convenient.\nhttps://www.facebook.com/groups/cslesson\n\ni also recommend  http://www.talkstats.com/ if you want to use a forum instead of facebook. it\u2019s an active forum. you will have a quick answer to your questions.\n\n\nstep 2 : statistics part 2 (1h25)\nin this step you will see how  to utilize and understand various statistical methods to analyse data. this will allow you to explore how variables can be linked or influence each other.   \n\nsource\nhttps://www.youtube.com/watch?v=uhwaeei_cqq&amp;ab_channel=cslesson\n\nwhy this source?\nit\u2019s the following video of  step 1 with the same speaker. once again,eeach concept is clearly exposed and the video highlights the main knowledge in a short period of time. i like this video as statistics is not always easy to understand so it\u2019s essential to have digestible content.\n\nwhat you will learn?\nfactorial anova\ncorr\u00e9lation and regression\n\nneed help ?\nthe author of the video has created a facebook group where you can share with others.the link between the video and the group chat it\u2019s very convenient.\nhttps://www.facebook.com/groups/cslesson\n\ni also recommend  http://www.talkstats.com/ if you want to use a forum instead of facebook. it\u2019s an active forum. you will have a quick answer to your questions.\n\nstep 3 : introduction to probabilities (12min)\nthis step will give you an understanding of critical concepts in probability that are relevant to the everyday work of data analysis. for example, you can be led to make predictions about sales or revenue and you will need to deal with probabilities by creating a predictive model.\n\nsource\nhttps://www.youtube.com/watch?v=nyw3mmfdg-0&amp;list=plhuatlda09bfarwhijpc90zlarffnedly&amp;index=11\n\nwhy this source?\nthe video is very easy to understand. the speaker uses graphs, charts and exercises to support the content. \n\nwhat you will learn?\ndefine and compute probability \ndefine a probability distribution\nuse the additional rule\nuse the multiplication rule for independent events\ndistinguish between mutually exclusive and independent events\n\nneed help ?\ni recommend  http://www.talkstats.com/ if you want to use a forum instead of facebook. it\u2019s an active forum. you will have a quick answer to your questions.\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n\nchapter 3\ndata analysts should learn how to handle large quantities of data stored in databases. before\nmanipulating any data, the data must be extracted from the database. most data analysts use sql,  the universal industry standard database language for data extraction. that\u2019s why understanding the basics of sql database language can be very helpful.  \n\nthis chapter is divided into 2 steps and can be completed in 6 hours: \nstep 1: quick overview of what is a database\nstep 2: read and write basic statement with sql\n\nstep 1: database \u2013 overview (5 min)\nlet\u2019s discover what a database is in this first step.\n\nsource\nhttps://www.youtube.com/watch?v=wr0jg0eqsza&amp;ab_channel=lucidchart\n\n why?\nthis source offers short videos that give a high-level overview of spreadsheets, tables, and database management systems.\n\nwhat you will learn?\nthe basics of database management systems.\n\nstep 2: sql : read and write basic statements (6 hours)\nknowing the basic statement to write a request in sql will open multiple doors to explore data. the writing follows a very simple and repetitive process that makes an easy language to learn. \n\nsource\nhttps://www.dataquest.io/m/252-introduction-to-sql/\n\ncreate an account on dataquest that will give you access to free content. \nin the path \u201cdata analyst in python\u201d, do the 3 firsts steps of it : introduction to sql, summary statistics, group summary statistics of the course 1 named \u201csql fundamentals\u201d and then, do the first step \u201cjoining data in sql\u201d of the course 3 named \u201cintermediate sql for data analysis\u201d .\n\nwhy?\ndataquest is one of the best platforms for data science learning. there is a preview of the course that people can read. it is an interactive sql course sequence that allows users to write and run real sql queries with real data at their own pace. it\u2019s not a theoretical course. you learn by practicing at every step. you also have a takeaway at the end of the class that summarizes the concepts in a downloadable pdf.    \n\nwhat you will learn?\nyou will learn how to extract data from almost any database, filter and analyze huge datasets with just a few lines of code, and create repeatable processes that will save you many hours of time in excel.\n\nwant to practice?\nin this link, you will find different exercises to practice. on the left, you will find various categories in function of which kind of query you want to practice: https://www.w3resource.com/sql-exercises/sql-retrieve-from-table.php\n\nneed help?\nthe most helpful forum is: https://stackoverflow.com/.\nstack overflow is a question and answer site for professional and enthusiast programmers. it features questions and answers on a wide range of topics in computer programming. be sure to use the tag feature to find specific answers to questions. \n\n\u2014\u2014\u2014\u2014\u2014\n \nchapter 4 \ni\u2019ve learned that it\u2019s important for the data analyst to present his or her findings in a way that people can easily understand.  this is why learning data visualization is so important. visualization involves using charts, diagrams and/or pictures to make  it easy to gain insights into data. tableau, a visualization software used by many analysts, makes it easy to share, collaborate , and incorporate advanced visualization of data.  \n\nchapter 4 consists of 1 step and can be completed in 6 hours: \nstep 1: how to use tableau as a creator\nstep 2: how to communicate data \u2013 importance of the storytelling\n\nstep 1: how to use tableau as a creator \nwith a good visualization of your data, you will get your audience to focus on your message. data is more valuable when it is visualized. \n\nsource\nhttps://www.tableau.com/learn/training/20204\n\nafter filling out the quick form asking information about you, you will access all the free videos.\nfollow the order of the videos. begin by watching \u201cgetting started\u201d in the creator section until \u201cdashboard and stories\u201d. \n\nwhy?\nthese are very clear and organized videos by chapter.  every video is explained well.  you can follow all the instructions very easily because the speaker walks through the steps and  simultaneously shares the screen showing the steps.   by watching the videos,  you will have access to all the material you need to learn and understand  which includes:  the data set, the transcript and the solution workbook are available for download.  this makes it easy to practice what you\u2019ve learned. \n\nwhat you will learn?\nhow the interface of tableau works, how to connect and manipulate data to make it visual. \n\nwant to practice? \nhttps://www.makeovermonday.co.uk/data/\nmakeover monday is a weekly project run by eva murray and andy kriebel. each sunday, the team posts a link to a visualization and a data set. your challenge is to create a better version of the visualization in your own creative way\u2014to either find a way to tell the data story more effectively or to discover something new in the data.\nhow to get started:\nfollow eva murray and andy kriebel on twitter.\ndownload the weekly data set.\npost your visualization using the hashtag #makeovermonday.\n\nneed help?\nin this link, you will be able to join the community of tableau : https://community.tableau.com/s/.\nthe tableau community forums is a place to get your tableau questions answered, collaborate with others and a space to help you get the most out of tableau. they have over tableau 200k users, over 195k questions and over 450 user groups right here!\n\nscroll down and you will have 3 sections : explore the forum, connect with other users and products ideas.\nin the explore the forum section, tableau will suggest topics that you can choose according to your question. it\u2019s clear and well organized. \n\nin reddit, you will also find a community of 43k members that is active : https://www.reddit.com/r/tableau/\n\nlook for inspiration?\non pinterest, you will find a lot of creative visualization on various topics that can inspire you!\nhttps://za.pinterest.com/nella312/tableau-tips\n\n\nstep 2: how to communicate data \u2013 importance of the storytelling (5 min)\ndata storytelling turns data from neutral fields in a database into opinions, arguments and insights. the storytelling will help you to  know what questions you want to answer and how you'll answer them.\n\nsource \nhttps://www.youtube.com/watch?v=0e52qfqngrm&amp;ab_channel=tedxtalks\n\nwhy?\nthis video is a ted talks video. this source is reliable. the speaker has an impressive  background and has extensive experience as a business analyst. the speaker emphasizes the power of storytelling in presenting data and gives concrete examples. \n\nwhat you will learn?\ndid you know that multiple stories, from multiple angles, can be derived and communicated from a single dataset? determining how you tell a story using data is important.  understanding how to effectively tell this story ensures that your audience will actually understand the message you want to convey.\n\ncongratulations ! you\u2019ve just finished the lesson plan. \n\nuse this checklist to make sure you\u2019ve covered all the main topics for this subject.    \n\nmanipulate data in excel\nknow the basic functions\ncreate a dashboard in excel\nuse macro and vba in excel\ntypes of variables\nmode, median and mean\nrange, interquartile range and box plot\nvariance and standard deviation\nz-scores\ncontingency table, scatterplot, pearson\u2019s r\nbasics of regression\nelementary probability\nrandom variables and probability distributions\nintroduction to hypothesis testing\nrole of the database\nwrite a query in sql\nknow the basic functions \nupload data in tableau\ncreate dashboard and stories in tableau\nexport reports in tableau\nuse tableau as a viewer\ncommunicate data through a story\n\n\nhere are the key takeaways of the lesson plan:\n\ndata analysts love working with numbers and presenting the data in a way that shows insight into the data set.   a data analyst also collects, processes and performs statistical analyses on large dataset.   the analyst develops charts and creates visual presentations to help businesses make more strategic decisions by using tools as excel or tableau. \nto be a data analyst, you need both hard skills and soft skills.\n\n\nbelow are some helpful questions to ask yourself when you begin formulating a plan to communicate your data: \n1. what story do you want to tell?\n2. are you using the right data to tell that story?\n3. how should you visualize the data to best tell your story?\n4. who\u2019s the audience?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/n5pmko/my_study_guide_for_teaching_myself_data_analytics/',)", "identifyer": 5592789, "year": "2021"}, {"autor": "data_quality", "date": 1620194504000, "content": "What do you use as your SQL IDE? /!/ Hi all,\n\nWhat do you use as your SQL IDE? \n\nMy name is Joseph, and I've been a data scientist for well over a decade. I've been frustrated with the lack of productivity tools and dev tools around my data science workflow, especially around SQL and the data warehouse. I decided to scratch my own itch and create a tool to solve my needs.\n\nI'm calling it Prequel, \"a prequel to SQL\". You can visit prequel.ai if you are interested in learning more.  \n\nPrequel solves the following three problems that I've experienced first hand as a data analyst and scientist:\n\n1. **Data Discovery**. Especially in larger organizations, it's too difficult to find the right data I need. There are oftentimes many versions of the same data, and institutional knowledge about what data I should be using to get to a result is in people's brains. Prequel has a native data discovery tool that aggregates metadata (schema, lineage, query logs) and backlinks to queries that reference that data so that you can easily find context without bothering your co-workers.\n2. **Writing and organizing queries.** Code has a home in github. Design has a home in Figma. But there is no home for SQL. Adhoc queries don't belong in github, and analyses are most often adhoc, and not put into production. It is not easy to share the SQL you've written to co-workers to troubleshoot a problem together, arrive at conclusions, or to generally share findings. It is also not possible to share documentation around the SQL you've written, and oftentimes you want to join business context (the purpose of this metric as it pertains to the marketing team or a particular project) with the query itself. This is not possible within the current IDE landscape (think DataGrip).\n3. **Context switching**. I don't want to have to switch between different tools and tabs when I'm writing SQL. I want to view things such as: what are the commonly joined tables to this particular table? What is the lineage diagram for this table? What are popular tables pertaining to this keyword? Has another person done this analysis before? I want this context on the same page as the SQL that I'm writing.\n\nPrequel solves these three problems as follows:\n\n1. **Data Discovery and context engine**. Prequel automatically aggregates metadata (schema, lineage, query logs, audit logs) directly from your data warehouse. It also automatically creates backlinks between the data (tables, columns) to the queries that everyone has written in your organization.\n2. **Query + Docs = QueryDocs.** I've invented a new pattern called the QueryDoc that is very similar to the Notion pattern + embeddable and runnable SQL. This way, you can take rich notes as you write your SQL and you are able to hierarchically organize your private or shared workspaces.\n3. **Context Sidebar = \"Magic Sidebar\"**. A second brain as your write SQL. Inspired by the outlines pattern in Roam, the magic sidebar lets you browse commonly joined tables, lineage, social context, etc as you are writing your queries in the QueryDoc workspace.\n\nI would love your feedback here, positive or negative, and would love you to tear apart my idea. I want to serve the data science and analytics community and build a productivity tool that you all deserve, so any feedback is much appreciated.\n\nThanks for reading!\n\nJoseph", "link": "https://www.reddit.com/r/datascience/comments/n58t9r/what_do_you_use_as_your_sql_ide/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what do you use as your sql ide? /!/ hi all,\n\nwhat do you use as your sql ide? \n\nmy name is joseph, and i've been a data scientist for well over a decade. i've been frustrated with the lack of productivity tools and dev tools around my data science workflow, especially around sql and the data warehouse. i decided to scratch my own itch and create a -----> tool !!!  to solve my needs.\n\ni'm calling it prequel, \"a prequel to sql\". you can visit prequel.ai if you are interested in learning more.  \n\nprequel solves the following three problems that i've experienced first hand as a data analyst and scientist:\n\n1. **data discovery**. especially in larger organizations, it's too difficult to find the right data i need. there are oftentimes many versions of the same data, and institutional knowledge about what data i should be using to get to a result is in people's brains. prequel has a native data discovery tool that aggregates metadata (schema, lineage, query logs) and backlinks to queries that reference that data so that you can easily find context without bothering your co-workers.\n2. **writing and organizing queries.** code has a home in github. design has a home in figma. but there is no home for sql. adhoc queries don't belong in github, and analyses are most often adhoc, and not put into production. it is not easy to share the sql you've written to co-workers to troubleshoot a problem together, arrive at conclusions, or to generally share findings. it is also not possible to share documentation around the sql you've written, and oftentimes you want to join business context (the purpose of this metric as it pertains to the marketing team or a particular project) with the query itself. this is not possible within the current ide landscape (think datagrip).\n3. **context switching**. i don't want to have to switch between different tools and tabs when i'm writing sql. i want to view things such as: what are the commonly joined tables to this particular table? what is the lineage diagram for this table? what are popular tables pertaining to this keyword? has another person done this analysis before? i want this context on the same page as the sql that i'm writing.\n\nprequel solves these three problems as follows:\n\n1. **data discovery and context engine**. prequel automatically aggregates metadata (schema, lineage, query logs, audit logs) directly from your data warehouse. it also automatically creates backlinks between the data (tables, columns) to the queries that everyone has written in your organization.\n2. **query + docs = querydocs.** i've invented a new pattern called the querydoc that is very similar to the notion pattern + embeddable and runnable sql. this way, you can take rich notes as you write your sql and you are able to hierarchically organize your private or shared workspaces.\n3. **context sidebar = \"magic sidebar\"**. a second brain as your write sql. inspired by the outlines pattern in roam, the magic sidebar lets you browse commonly joined tables, lineage, social context, etc as you are writing your queries in the querydoc workspace.\n\ni would love your feedback here, positive or negative, and would love you to tear apart my idea. i want to serve the data science and analytics community and build a productivity tool that you all deserve, so any feedback is much appreciated.\n\nthanks for reading!\n\njoseph", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/n58t9r/what_do_you_use_as_your_sql_ide/',)", "identifyer": 5592829, "year": "2021"}, {"autor": "AI52487963", "date": 1624039241000, "content": "There has to be an online tool for text generation right? /!/ Friend of mine wants to import a list of names and generate fake names that are similar based on the input training data. He's been having trouble with installing tensorflow on his M1 mac. I was sure there would be an online utility to do something like that without the need to train locally, but I can't seem to find a decent resource for him to use.\n\nAny suggestions?", "link": "https://www.reddit.com/r/datascience/comments/o2w4qb/there_has_to_be_an_online_tool_for_text/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "there has to be an online -----> tool !!!  for text generation right? /!/ friend of mine wants to import a list of names and generate fake names that are similar based on the input training data. he's been having trouble with installing tensorflow on his m1 mac. i was sure there would be an online utility to do something like that without the need to train locally, but i can't seem to find a decent resource for him to use.\n\nany suggestions?", "sortedWord": "None", "removed": "('nan',)", "score": 0, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/o2w4qb/there_has_to_be_an_online_tool_for_text/',)", "identifyer": 5592923, "year": "2021"}, {"autor": "Sonny-Orkidea", "date": 1624003072000, "content": "CDP / XCDP for beginner data scientist /!/ Hi everyone,\n\nDid anyone work as a Data scientist in ecommerce industry? I am currently working with Bloomreach, which can provide full customer experience without really knowing advanced programming.\n\nI am looking for something cheaper alternative tool for data science in ecommerce - tracking, data analysis, predictions, single customer view, omnichannel communication, recommendation etc. What tools/combination of tools are best for you?\n\nI tried almost all CDP demos, but i want to know some experiences from e-commerce segment. For info, we are small agency focused on automation and data.", "link": "https://www.reddit.com/r/datascience/comments/o2jtgd/cdp_xcdp_for_beginner_data_scientist/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "cdp / xcdp for beginner data scientist /!/ hi everyone,\n\ndid anyone work as a data scientist in ecommerce industry? i am currently working with bloomreach, which can provide full customer experience without really knowing advanced programming.\n\ni am looking for something cheaper alternative -----> tool !!!  for data science in ecommerce - tracking, data analysis, predictions, single customer view, omnichannel communication, recommendation etc. what tools/combination of tools are best for you?\n\ni tried almost all cdp demos, but i want to know some experiences from e-commerce segment. for info, we are small agency focused on automation and data.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/o2jtgd/cdp_xcdp_for_beginner_data_scientist/',)", "identifyer": 5592942, "year": "2021"}, {"autor": "artvandalay281", "date": 1613652241000, "content": "Starting point for EV research analyst /!/ Hi Guys,\n\nI am from automotive domain and work for a market research firm. I wanted to learn data science to be more efficient at what I do, and also enable myself for senior roles. So far, our company only uses excel to work on the datasets. \n\nThe question is , what will be the most useful tool for me? And how should I leverage it? People in similar roles might help by sharing insights here.\n\nI have zero exposure Python or other tools but have studied advanced level mathematical courses.", "link": "https://www.reddit.com/r/datascience/comments/lmlygs/starting_point_for_ev_research_analyst/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "starting point for ev research analyst /!/ hi guys,\n\ni am from automotive domain and work for a market research firm. i wanted to learn data science to be more efficient at what i do, and also enable myself for senior roles. so far, our company only uses excel to work on the datasets. \n\nthe question is , what will be the most useful -----> tool !!!  for me? and how should i leverage it? people in similar roles might help by sharing insights here.\n\ni have zero exposure python or other tools but have studied advanced level mathematical courses.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/lmlygs/starting_point_for_ev_research_analyst/',)", "identifyer": 5593069, "year": "2021"}, {"autor": "vasili111", "date": 1613598132000, "content": "I need to capture one json stream, process/transform the data and send it to MySQL database /!/ So, there is one incoming json data stream that I need to capture it. After I need to process/transform that data and store it to MySQL database. Is the Apache Kafka right tool for it?\n\nIf yes, is there need for any additional tools to do that and if yes, which one do you recommend?\n\nIf no, which tools do you recommend for that use case (preferably Python but can be others too)?", "link": "https://www.reddit.com/r/datascience/comments/lm5iee/i_need_to_capture_one_json_stream/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "i need to capture one json stream, process/transform the data and send it to mysql database /!/ so, there is one incoming json data stream that i need to capture it. after i need to process/transform that data and store it to mysql database. is the apache kafka right -----> tool !!!  for it?\n\nif yes, is there need for any additional tools to do that and if yes, which one do you recommend?\n\nif no, which tools do you recommend for that use case (preferably python but can be others too)?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/lm5iee/i_need_to_capture_one_json_stream/',)", "identifyer": 5593094, "year": "2021"}, {"autor": "brahshta4", "date": 1613458892000, "content": "Paid tool available to extract features from twitter data /!/ Regarding my covid 19 research paper, i would not want to waste Time in preprocessing data, extracting features.\nPlease let me know if there are free or paid tools to do this work.", "link": "https://www.reddit.com/r/datascience/comments/lkyb8v/paid_tool_available_to_extract_features_from/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "paid -----> tool !!!  available to extract features from twitter data /!/ regarding my covid 19 research paper, i would not want to waste time in preprocessing data, extracting features.\nplease let me know if there are free or paid tools to do this work.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/lkyb8v/paid_tool_available_to_extract_features_from/',)", "identifyer": 5593118, "year": "2021"}, {"autor": "baldordash", "date": 1613424574000, "content": "Data Science Degree for Current IT Leader/Manager (seeking advice... or reassurance?) /!/ Hi All,\n\nI'm mid-career (34 yo) currently working in a role that might best be called \"IT Business Partner\" at a large pharma company. Basically, I interface with business stakeholders, determine their needs, put together strategic plans to address their needs with technology, and plan projects to launch the technology internally and plan the means of supporting it. This primarily involves evaluating outside products and overseeing consultants/vendors who help us implement the technology (we rarely build our own apps). For the groups I work with, this has so far pretty much evolved into helping build apps with Salesforce, ensuring dashboards are built in Tableau/Power BI and being pretty hands off myself. Occasionally, I'll have other projects that look at more niche needs, including one project to build a tool that relied heavily on machine learning and NLP to help business development folks find opportunities for partnerships/acquisitions.\n\nMy role is pretty high-paying, and I'm good at it, but I've had a lingering feeling for years now that I need to beef up my technical resume , especially around data science-related topics. I continue to see machine learning and artificial intelligence being used (or attempted to be used) by many of the vendors we've worked with, and have come to realize that very few IT leaders in our org have experience or a good understanding of how this stuff works, and a vendor can really sneak poor product offerings by us unless we get 'that-one-person-who-knows-a-lot-about-AI' to attend the meeting and hold a vendor's feet to the fire. Our business counterparts are asking more and more about AI/ML, more vendors are claiming they have expertise on this stuff, and lots of us in IT seem like we're behind the curve. And from the CEO/CIO level on down, there's all this talk about digital this, data pipeline that, AI this, Machine Learning that.\n\nSo all this has led me to think that in order to best position myself for the next 30 years of my career, I NEED some good, hands-on experience in data science! While online courses I've taken were OK, I kinda want the rigor of a project-based academic program, so I've enrolled into a graduate-level program at Worcester Polytechnic Institute in MA (they have a 6-class certification, that I could progress further toward MS if I like it). **I don't necessarily want to change my career to \"Data Science\", but I do want to get that type of hands-on experience. For anyone whose attended a graduate program in data science or related topic, are there students taking it who have little to no intention of actually being a 'data scientist/data engineer/whatever buzzword' by job title?** I would love to get better at programming and understand the nitty-gritty of this stuff, because I think it will come in handy, but I highly doubt I'll be doing it day-in-day-out. Basically, I have this feeling I'm going to be at a great advantage for the rest of my career if I go through this experience.\n\nMy background is econ undergrad/MBA, by the way. I've worked in IT roles for a while, but had some 'data analyst' type roles in past that were pretty SQL/Excel heavy.", "link": "https://www.reddit.com/r/datascience/comments/lkno6k/data_science_degree_for_current_it_leadermanager/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "data science degree for current it leader/manager (seeking advice... or reassurance?) /!/ hi all,\n\ni'm mid-career (34 yo) currently working in a role that might best be called \"it business partner\" at a large pharma company. basically, i interface with business stakeholders, determine their needs, put together strategic plans to address their needs with technology, and plan projects to launch the technology internally and plan the means of supporting it. this primarily involves evaluating outside products and overseeing consultants/vendors who help us implement the technology (we rarely build our own apps). for the groups i work with, this has so far pretty much evolved into helping build apps with salesforce, ensuring dashboards are built in tableau/power bi and being pretty hands off myself. occasionally, i'll have other projects that look at more niche needs, including one project to build a -----> tool !!!  that relied heavily on machine learning and nlp to help business development folks find opportunities for partnerships/acquisitions.\n\nmy role is pretty high-paying, and i'm good at it, but i've had a lingering feeling for years now that i need to beef up my technical resume , especially around data science-related topics. i continue to see machine learning and artificial intelligence being used (or attempted to be used) by many of the vendors we've worked with, and have come to realize that very few it leaders in our org have experience or a good understanding of how this stuff works, and a vendor can really sneak poor product offerings by us unless we get 'that-one-person-who-knows-a-lot-about-ai' to attend the meeting and hold a vendor's feet to the fire. our business counterparts are asking more and more about ai/ml, more vendors are claiming they have expertise on this stuff, and lots of us in it seem like we're behind the curve. and from the ceo/cio level on down, there's all this talk about digital this, data pipeline that, ai this, machine learning that.\n\nso all this has led me to think that in order to best position myself for the next 30 years of my career, i need some good, hands-on experience in data science! while online courses i've taken were ok, i kinda want the rigor of a project-based academic program, so i've enrolled into a graduate-level program at worcester polytechnic institute in ma (they have a 6-class certification, that i could progress further toward ms if i like it). **i don't necessarily want to change my career to \"data science\", but i do want to get that type of hands-on experience. for anyone whose attended a graduate program in data science or related topic, are there students taking it who have little to no intention of actually being a 'data scientist/data engineer/whatever buzzword' by job title?** i would love to get better at programming and understand the nitty-gritty of this stuff, because i think it will come in handy, but i highly doubt i'll be doing it day-in-day-out. basically, i have this feeling i'm going to be at a great advantage for the rest of my career if i go through this experience.\n\nmy background is econ undergrad/mba, by the way. i've worked in it roles for a while, but had some 'data analyst' type roles in past that were pretty sql/excel heavy.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/lkno6k/data_science_degree_for_current_it_leadermanager/',)", "identifyer": 5593138, "year": "2021"}, {"autor": "elisajane", "date": 1613423912000, "content": "Data Integration Brainstorm /!/ I\u2019m working on a data integration project for a part time semester internship. The idea is My team wants to report on various metrics from different initiatives.\n\nThis requires pulling data from different sources, such as exporting old reports, pulling data from systems, etc. The easy by inefficient way is to ask product owners or system owners to give us the data, create dashboard, update data, repeat.\n\nIs there a way to have one tool that houses all the scripts or api calls? I\u2019m still identifying what these data sources are, but it be nice to hear if anyone had success on a similar project.", "link": "https://www.reddit.com/r/datascience/comments/lknful/data_integration_brainstorm/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "data integration brainstorm /!/ i\u2019m working on a data integration project for a part time semester internship. the idea is my team wants to report on various metrics from different initiatives.\n\nthis requires pulling data from different sources, such as exporting old reports, pulling data from systems, etc. the easy by inefficient way is to ask product owners or system owners to give us the data, create dashboard, update data, repeat.\n\nis there a way to have one -----> tool !!!  that houses all the scripts or api calls? i\u2019m still identifying what these data sources are, but it be nice to hear if anyone had success on a similar project.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/lknful/data_integration_brainstorm/',)", "identifyer": 5593139, "year": "2021"}, {"autor": "Arek993", "date": 1613418487000, "content": "Project management tool to manage a data science project /!/ [removed]", "link": "https://www.reddit.com/r/datascience/comments/lklg0k/project_management_tool_to_manage_a_data_science/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "project management -----> tool !!!  to manage a data science project /!/ [removed]", "sortedWord": "None", "removed": "('reddit',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/lklg0k/project_management_tool_to_manage_a_data_science/',)", "identifyer": 5593142, "year": "2021"}, {"autor": "bobcodes247365", "date": 1613410990000, "content": "See how my new tool visualizes Python code - and shows bugs in the code base. /!/ I am sorry if my post doesn't sound like an innovation to you, but would like you to take a look at the tool as it evolved out of a research project! I thought people in this subreddit might be interested :) Oh and yes! Anyone can use it!\n\nThe repository I used is: [https://metabob.com/gh/galt2x/sherlock](https://metabob.com/gh/galt2x/sherlock?utm_source=Reddit%20Post&amp;utm_medium=Reddit&amp;utm_campaign=Reddit%20(r%2Fcoolgithubprojects)%202%2F15%2F21)\n\nThe program works best on Google Chrome, If you would like to check out the website, I linked it [here](https://www.metabob.com/?utm_source=Reddit%20Post&amp;utm_medium=Reddit&amp;utm_campaign=Reddit%20(r%2Fcoolgithubprojects)%202%2F15%2F21).", "link": "https://www.reddit.com/r/datascience/comments/lkipi4/see_how_my_new_tool_visualizes_python_code_and/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "see how my new -----> tool !!!  visualizes python code - and shows bugs in the code base. /!/ i am sorry if my post doesn't sound like an innovation to you, but would like you to take a look at the tool as it evolved out of a research project! i thought people in this subreddit might be interested :) oh and yes! anyone can use it!\n\nthe repository i used is: [https://metabob.com/gh/galt2x/sherlock](https://metabob.com/gh/galt2x/sherlock?utm_source=reddit%20post&amp;utm_medium=reddit&amp;utm_campaign=reddit%20(r%2fcoolgithubprojects)%202%2f15%2f21)\n\nthe program works best on google chrome, if you would like to check out the website, i linked it [here](https://www.metabob.com/?utm_source=reddit%20post&amp;utm_medium=reddit&amp;utm_campaign=reddit%20(r%2fcoolgithubprojects)%202%2f15%2f21).", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/lkipi4/see_how_my_new_tool_visualizes_python_code_and/',)", "identifyer": 5593150, "year": "2021"}, {"autor": "Sonny-Orkidea", "date": 1623412502000, "content": "Beginner - how to apply data science in my job? / how to start freelancing /!/ Hi everyone,\n\nI am starting with data science in DataQuest, everything looks easy for me, i am learning for 3-4 hours a day. I work as a data specialist in marketing automation, but my company dont make any data analysis, any advanced reporting for clients. just marketing automation and some basic KPI reporting from our CDP tool and from Google Analytics. \n\nI want to start implementing new skills to my job, but i really dont know how to start making advanced data analysis for our clients. our CDP tool can do everything - purchase/churn predictions, web tracking with data storage, funnels, recommendation engines with machine learning etc etc.. like google analytics with 360 customer view. There is one thing bad - data expiration, our tracked events has 1-3 months of expiration.\n\nIs it a good thing to start using some data storage to make larger data sets and make analysis outside our tool? Will this be relevation to our clients or just loss of time?.. I am very interested in data science and i found out that this is my dream job, i love it, but i dont know how to implement my knowledge to our company or how to start freelancing for ecommerce clients... \n\nI will be happy for all kinds of opinions and experiences guys. Thanks", "link": "https://www.reddit.com/r/datascience/comments/nxe6nb/beginner_how_to_apply_data_science_in_my_job_how/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "beginner - how to apply data science in my job? / how to start freelancing /!/ hi everyone,\n\ni am starting with data science in dataquest, everything looks easy for me, i am learning for 3-4 hours a day. i work as a data specialist in marketing automation, but my company dont make any data analysis, any advanced reporting for clients. just marketing automation and some basic kpi reporting from our cdp -----> tool !!!  and from google analytics. \n\ni want to start implementing new skills to my job, but i really dont know how to start making advanced data analysis for our clients. our cdp tool can do everything - purchase/churn predictions, web tracking with data storage, funnels, recommendation engines with machine learning etc etc.. like google analytics with 360 customer view. there is one thing bad - data expiration, our tracked events has 1-3 months of expiration.\n\nis it a good thing to start using some data storage to make larger data sets and make analysis outside our tool? will this be relevation to our clients or just loss of time?.. i am very interested in data science and i found out that this is my dream job, i love it, but i dont know how to implement my knowledge to our company or how to start freelancing for ecommerce clients... \n\ni will be happy for all kinds of opinions and experiences guys. thanks", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nxe6nb/beginner_how_to_apply_data_science_in_my_job_how/',)", "identifyer": 5593168, "year": "2021"}, {"autor": "histoire_guy", "date": 1623366159000, "content": "PixLab Annotate - Online Image Annotation, Labeling and Segmentation Tool", "link": "https://www.reddit.com/r/datascience/comments/nx152h/pixlab_annotate_online_image_annotation_labeling/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "pixlab annotate - online image annotation, labeling and segmentation -----> tool !!! ", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('link',)", "medialink": "('https://annotate.pixlab.io/',)", "identifyer": 5593185, "year": "2021"}, {"autor": "wkeber", "date": 1623309094000, "content": "Super Basic Question: Filter &amp; Grouping Tool for Mac? /!/ Hi there, I have a super basic/amateur question and apologies for it being elementary. But I figured that this group would be the right set of professionals to ask. (PS - did I use the right flair?) \n\nI have some basic grouping (sums) and filtering needs for a large CSV data set. (500K plus records). Excel on the Mac is abysmal for such tasks, even though a basic pivot table would accomplish what I need. \n\nIs there any tool out there (could be web-based or local) that fits the bill, and sits between Excel and a full-fledged database? (I\u2019d rather not import to SQL and query it back out.) \n\nAny thoughts/suggestions would be appreciated!", "link": "https://www.reddit.com/r/datascience/comments/nwhghp/super_basic_question_filter_grouping_tool_for_mac/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "super basic question: filter &amp; grouping -----> tool !!!  for mac? /!/ hi there, i have a super basic/amateur question and apologies for it being elementary. but i figured that this group would be the right set of professionals to ask. (ps - did i use the right flair?) \n\ni have some basic grouping (sums) and filtering needs for a large csv data set. (500k plus records). excel on the mac is abysmal for such tasks, even though a basic pivot table would accomplish what i need. \n\nis there any tool out there (could be web-based or local) that fits the bill, and sits between excel and a full-fledged database? (i\u2019d rather not import to sql and query it back out.) \n\nany thoughts/suggestions would be appreciated!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nwhghp/super_basic_question_filter_grouping_tool_for_mac/',)", "identifyer": 5593228, "year": "2021"}, {"autor": "LogicalDocSpock", "date": 1619040487000, "content": "Learning how to map data, best resources to learn /!/ Hello all,\n\nI am job seeking for my first role in data science and recently had a take home test that involved using R, Python, or Tableau. One section was on how to map data. I'm quite familiar with Python and R, although have not used either for mapping data. I don't think I'll move forward with this job because I could not get around to completing the map component in time but after talking to a mentor, using map data was quite common in the field. I'm thinking of taking an online course to improve this skill as I'm sure this isn't too hard to learn (I learned how to web scrape in Python over a weekend). \n\nAny online course recommendations or online resources as this could be something I add to my portfolio.\n\nAlso is there an ideal tool to use to map data in the industry? Are people using Python? The assignment listed for data cleaning and wrangling Airflow, Tableau DM, Kafka whereas for data analytics capabilities, they listed Python, R, or Tableau. I've never heard of Airflow but I don't think it is the primary tool for mapping data.\n\nI went back to university a couple of years ago and used R and had to take a Python course but after this assignment, it made me realize that we weren't taught how to map data but I can see it being useful in many jobs so it's unfortunate it didn't occur to me to learn it sooner. I have brief exposure to ArcGIS but the assignment didn't reference that. What is the industry standard? Thanks in advance for advice on what tools are used in the industry for mapping data and how to best learn it.", "link": "https://www.reddit.com/r/datascience/comments/mvpwql/learning_how_to_map_data_best_resources_to_learn/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "learning how to map data, best resources to learn /!/ hello all,\n\ni am job seeking for my first role in data science and recently had a take home test that involved using r, python, or tableau. one section was on how to map data. i'm quite familiar with python and r, although have not used either for mapping data. i don't think i'll move forward with this job because i could not get around to completing the map component in time but after talking to a mentor, using map data was quite common in the field. i'm thinking of taking an online course to improve this skill as i'm sure this isn't too hard to learn (i learned how to web scrape in python over a weekend). \n\nany online course recommendations or online resources as this could be something i add to my portfolio.\n\nalso is there an ideal -----> tool !!!  to use to map data in the industry? are people using python? the assignment listed for data cleaning and wrangling airflow, tableau dm, kafka whereas for data analytics capabilities, they listed python, r, or tableau. i've never heard of airflow but i don't think it is the primary tool for mapping data.\n\ni went back to university a couple of years ago and used r and had to take a python course but after this assignment, it made me realize that we weren't taught how to map data but i can see it being useful in many jobs so it's unfortunate it didn't occur to me to learn it sooner. i have brief exposure to arcgis but the assignment didn't reference that. what is the industry standard? thanks in advance for advice on what tools are used in the industry for mapping data and how to best learn it.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/mvpwql/learning_how_to_map_data_best_resources_to_learn/',)", "identifyer": 5593271, "year": "2021"}, {"autor": "liveinskin123", "date": 1619013714000, "content": "Free web scraper tool? /!/ Hey guys,\n\nI need help trying to automate part of my job. I need a tool that allows me to upload a list of websites and scrapes information such as numbers, email and addresses if they exist on the website. I would prefer it to be free (which I doubt I\u2019m gonna get a good one with haha)", "link": "https://www.reddit.com/r/datascience/comments/mvgbmn/free_web_scraper_tool/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "free web scraper -----> tool !!! ? /!/ hey guys,\n\ni need help trying to automate part of my job. i need a tool that allows me to upload a list of websites and scrapes information such as numbers, email and addresses if they exist on the website. i would prefer it to be free (which i doubt i\u2019m gonna get a good one with haha)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/mvgbmn/free_web_scraper_tool/',)", "identifyer": 5593290, "year": "2021"}, {"autor": "ReactCereals", "date": 1619001355000, "content": "Data driven Web Frontends....looking at React and beyond for CRUD /!/ Hello fellow community,\n\nSo...While we might love jupyter and all our fancy tools when getting results into the hands of customers Webapps seem to be the deal.\n\nCurrently I am developing a few frontends, calling them \u201cdata driven\u201d for now. Whatever that means, but it\u2019s trendy.\n\nBasically they are CRUD Interfaces with a lot of sugar.\n\nCollapsible lists with tooltips, maybe a summary row, icons, colors, basically presenting data in a way that people will like to pay for.\n\nCurrently I decided to go with a Django backend and a react frontend.\n\nOverall I have to admit I hate frontend dev almost as much as I hate Webapps. Still I thought react was a reasonable choice for a great user experience with a modern toolset.\n\nRight now the frontends authenticate against the backends and fetches data using GraphQL instead of traditional REST. Which sounded like a great idea at the time.\n\nBut actually I feel like this was a terrible approach. When fetching data there needs to be a ton of transformation and looping over arrays done in the frontend to bringt the pieces of fetched data together in a format suitable to render tables.\nWhich in my opinion is a mess; fiddling with arrays in JS while there is a Python backend at my fingertips that could use pandas to do it in the fraction of the time. But that seems just how this works.\n\nI also got fed up with react. It provides a lot of great advantages, but honestly I am not happy having tons of packages for simple stuff that might get compromised with incompatible versions and stuff down the road. Also I feel bad about the packages available to create those tables in general.\nIt just feels extremely inefficient, and that\u2019s coming from someone usually writhing Python ;)\n\nOverall what I like:\n- beautiful frontend\n- great structure\n- single page applications just feel so good\n- easy to use (mainly)\n\nWhat I just can\u2019t stand anymore:\n- way too much logic inside the frontend\n- way too much data transformation inside the frontend (well, all of it)\n- too much packages that don\u2019t feel reliable in the long run\n- sometimes clunky to debug depending on what packages are used\n- I somehow never get the exact visual results rendered that I want\n- I somehow create a memory leak daily that I have to fix then (call me incompetent but I can\u2019t figure out why this always happens to me)\n\n\nSo I have been talking to a few other DS and Devs and...GraphQL and React seem to be really popular and others don\u2019t seem to mind it too much.\n\nWhat are your experiences? Similar problems? Do you use something else?\nI would love to ditch react in favor of something more suitable.\n\nOverall I feel like providing a crud interface with \u201cadvanced\u201d stuff like icons in cells, tool tips, and collapsible rows (tree structure tables) should be a common challenge, I just can\u2019t find the proper tool for the job.\n\nBest regards and would love to hear your thoughts", "link": "https://www.reddit.com/r/datascience/comments/mvcuae/data_driven_web_frontendslooking_at_react_and/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "data driven web frontends....looking at react and beyond for crud /!/ hello fellow community,\n\nso...while we might love jupyter and all our fancy tools when getting results into the hands of customers webapps seem to be the deal.\n\ncurrently i am developing a few frontends, calling them \u201cdata driven\u201d for now. whatever that means, but it\u2019s trendy.\n\nbasically they are crud interfaces with a lot of sugar.\n\ncollapsible lists with tooltips, maybe a summary row, icons, colors, basically presenting data in a way that people will like to pay for.\n\ncurrently i decided to go with a django backend and a react frontend.\n\noverall i have to admit i hate frontend dev almost as much as i hate webapps. still i thought react was a reasonable choice for a great user experience with a modern toolset.\n\nright now the frontends authenticate against the backends and fetches data using graphql instead of traditional rest. which sounded like a great idea at the time.\n\nbut actually i feel like this was a terrible approach. when fetching data there needs to be a ton of transformation and looping over arrays done in the frontend to bringt the pieces of fetched data together in a format suitable to render tables.\nwhich in my opinion is a mess; fiddling with arrays in js while there is a python backend at my fingertips that could use pandas to do it in the fraction of the time. but that seems just how this works.\n\ni also got fed up with react. it provides a lot of great advantages, but honestly i am not happy having tons of packages for simple stuff that might get compromised with incompatible versions and stuff down the road. also i feel bad about the packages available to create those tables in general.\nit just feels extremely inefficient, and that\u2019s coming from someone usually writhing python ;)\n\noverall what i like:\n- beautiful frontend\n- great structure\n- single page applications just feel so good\n- easy to use (mainly)\n\nwhat i just can\u2019t stand anymore:\n- way too much logic inside the frontend\n- way too much data transformation inside the frontend (well, all of it)\n- too much packages that don\u2019t feel reliable in the long run\n- sometimes clunky to debug depending on what packages are used\n- i somehow never get the exact visual results rendered that i want\n- i somehow create a memory leak daily that i have to fix then (call me incompetent but i can\u2019t figure out why this always happens to me)\n\n\nso i have been talking to a few other ds and devs and...graphql and react seem to be really popular and others don\u2019t seem to mind it too much.\n\nwhat are your experiences? similar problems? do you use something else?\ni would love to ditch react in favor of something more suitable.\n\noverall i feel like providing a crud interface with \u201cadvanced\u201d stuff like icons in cells, -----> tool !!!  tips, and collapsible rows (tree structure tables) should be a common challenge, i just can\u2019t find the proper -----> tool !!!  for the job.\n\nbest regards and would love to hear your thoughts", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 49, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/mvcuae/data_driven_web_frontendslooking_at_react_and/',)", "identifyer": 5593297, "year": "2021"}, {"autor": "ales9915", "date": 1634007873000, "content": "Tool recommendation for BI. /!/ I'm working on a project creating text reports for several companies with all the important information a manager needs to know. The idea is to have a sample report with blank spaces and fill them with the information of the current company. For example:\n\nThe\\_\\_\\_\\_\\_\\_\\_ are \\_\\_\\_\\_\\_\\_\\_\\_ by a rate of \\_\\_\\_\\_\\_\\_\n\nGives me the results\n\nThe Sales are Increasing by a rate of 0.5%.\n\nAlthough, I'm kind of stuck in which software can I use for that. It has to read an SQL table and fill the spaces.\n\nI'd appreciate if you guys discuss about it.\n\nThank you", "link": "https://www.reddit.com/r/datascience/comments/q6cmje/tool_recommendation_for_bi/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "-----> tool !!!  recommendation for bi. /!/ i'm working on a project creating text reports for several companies with all the important information a manager needs to know. the idea is to have a sample report with blank spaces and fill them with the information of the current company. for example:\n\nthe\\_\\_\\_\\_\\_\\_\\_ are \\_\\_\\_\\_\\_\\_\\_\\_ by a rate of \\_\\_\\_\\_\\_\\_\n\ngives me the results\n\nthe sales are increasing by a rate of 0.5%.\n\nalthough, i'm kind of stuck in which software can i use for that. it has to read an sql table and fill the spaces.\n\ni'd appreciate if you guys discuss about it.\n\nthank you", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/q6cmje/tool_recommendation_for_bi/',)", "identifyer": 5593404, "year": "2021"}, {"autor": "LexMeat", "date": 1633942928000, "content": "Is there a presentation-like tool that allows the use of Plotly's interactive plots? /!/ I kind of hate PowerPoint but it's also a necessary evil. Either the client is used to it or (worse) the project manager is used to it, but either way, we need to present our progress to the client every once in a while and PowerPoint is the standard tool for that. \n\nOne (more) grief that I have with PowerPoint is that I cannot use these nice interactive plots I make with Plotly. Switching in and out of PowerPoint is a big no especially since my work is remote. \n\nSo my question is simple: Is there a presentation tool (i.e., a PowerPoint replacement) that supports Plotly's interactive plots? \n\nI have discovered a few very nice tools (e.g., Whimsical Docs) but they don't do what I'm looking for.", "link": "https://www.reddit.com/r/datascience/comments/q5rrdn/is_there_a_presentationlike_tool_that_allows_the/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "is there a presentation-like -----> tool !!!  that allows the use of plotly's interactive plots? /!/ i kind of hate powerpoint but it's also a necessary evil. either the client is used to it or (worse) the project manager is used to it, but either way, we need to present our progress to the client every once in a while and powerpoint is the standard tool for that. \n\none (more) grief that i have with powerpoint is that i cannot use these nice interactive plots i make with plotly. switching in and out of powerpoint is a big no especially since my work is remote. \n\nso my question is simple: is there a presentation tool (i.e., a powerpoint replacement) that supports plotly's interactive plots? \n\ni have discovered a few very nice tools (e.g., whimsical docs) but they don't do what i'm looking for.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 11, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/q5rrdn/is_there_a_presentationlike_tool_that_allows_the/',)", "identifyer": 5593437, "year": "2021"}, {"autor": "horriblegirlfriend32", "date": 1633899872000, "content": "Is data science in financial sector a good career path? /!/ Hi,\n\nI've recently have got a job in a financial sector in a DS team. It is not strict DS job apparently. All we do is build statistical models for finance. It takes a long time to develop and document them. \n\nI was told, that the most important thing is to know finance, and use DS more as a tool to deliver results. \n\nIs it a good career path, or is it more of a low paid analyst position?", "link": "https://www.reddit.com/r/datascience/comments/q5gqsp/is_data_science_in_financial_sector_a_good_career/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "is data science in financial sector a good career path? /!/ hi,\n\ni've recently have got a job in a financial sector in a ds team. it is not strict ds job apparently. all we do is build statistical models for finance. it takes a long time to develop and document them. \n\ni was told, that the most important thing is to know finance, and use ds more as a -----> tool !!!  to deliver results. \n\nis it a good career path, or is it more of a low paid analyst position?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/q5gqsp/is_data_science_in_financial_sector_a_good_career/',)", "identifyer": 5593451, "year": "2021"}, {"autor": "moneygotlonger", "date": 1624567546000, "content": "Job offer in process mining, should I take it? /!/ So I got a new job offer to work as a data scientist helping a big company with their process mining using ERP/CRM and enumerous other sources and blend it in uipath processgold tool. \nThis is a bit far from my actual job, I'm currently a data engineer so I don't know if I'll be able to do the work without a lot of effort from my side other than that i want to know what you guys think, this looks too niche and I'm afraid I'll get too specialized and won't be as easily employed as with my current job.\n\nAll answers are appreciated", "link": "https://www.reddit.com/r/datascience/comments/o79n8v/job_offer_in_process_mining_should_i_take_it/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "job offer in process mining, should i take it? /!/ so i got a new job offer to work as a data scientist helping a big company with their process mining using erp/crm and enumerous other sources and blend it in uipath processgold -----> tool !!! . \nthis is a bit far from my actual job, i'm currently a data engineer so i don't know if i'll be able to do the work without a lot of effort from my side other than that i want to know what you guys think, this looks too niche and i'm afraid i'll get too specialized and won't be as easily employed as with my current job.\n\nall answers are appreciated", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/o79n8v/job_offer_in_process_mining_should_i_take_it/',)", "identifyer": 5593476, "year": "2021"}, {"autor": "MattWardVZ", "date": 1624525310000, "content": "Information is Power: Turning Product Hunt Data into Actionable, Crypto-Investable Insights /!/ Hi Reddit,\n\nI don\u2019t know about you guys, but I\u2019ve always been a big fan of Product Hunt. So, in celebration of [**our launch on Product Hunt**](http://veezoo.com/producthunt), we decided to demo[ **Veezoo**](https://www.veezoo.com/) (our simple AI powered natural language tool to ask your data anything) for founders, geeks, and investors everywhere by analyzing Product Hunt's site data ([**demo accessible here**](https://veezoo.com/phdemo)**!**) to provide interesting, actionable insights - especially for those looking to understand current startup and technology trends. As they say, a rising tide lifts all ships\u2026 but it is also hard to make money as an investor or a founder in an overly crowded or hyped market.\n\nData has been dubbed the \u201cnew oil\u201d upon which empires are built. Yet, despite its potential value, most insights remain difficult, if not impossible to unearth without the use of artificial intelligence \u2014 which often brings it outside the realm of your everyday employee.\n\nAt [**Veezoo**](https://veezoo.com), we believe everyone should be empowered to use data-driven insights to improve their work, productivity, and performance without having to call IT or the tech team and then wait days to get an answer. That\u2019s why we created an ultra-simple, chat based AI interface to enable anyone from the insurance salesman or financial analyst to the ecommerce owner or early stage VC to find their \u201cneedle in the haystack\u201d through simple questions, fast answers, and invaluable insights.\n\nTry the Product Hunt demo here: [**https://veezoo.com/PHdemo**](https://veezoo.com/PHdemo)\n\nTo do this (and also show off a bit more what Veezoo can do :), we created an analysis into Product Hunt Trends and Insights including: \n\n\\- Trending Topics on Product Hunt \n\n\\- Top Product Hunter insights \n\n\\- And Tips to Maximize the Success of a Product Hunt Launch\n\nFor the rest of the post with the charts/graphs/videos, visit:\n\n[**https://www.veezoo.com/blog/veezoo-product-hunt-launch/**](https://www.veezoo.com/blog/veezoo-product-hunt-launch/)\n\n\\--\n\n[**And please check out our post on Product Hunt.**](https://veezoo.com/producthunt) We'd really appreciate your support if you find data democratization important!", "link": "https://www.reddit.com/r/datascience/comments/o6x3n3/information_is_power_turning_product_hunt_data/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "information is power: turning product hunt data into actionable, crypto-investable insights /!/ hi reddit,\n\ni don\u2019t know about you guys, but i\u2019ve always been a big fan of product hunt. so, in celebration of [**our launch on product hunt**](http://veezoo.com/producthunt), we decided to demo[ **veezoo**](https://www.veezoo.com/) (our simple ai powered natural language -----> tool !!!  to ask your data anything) for founders, geeks, and investors everywhere by analyzing product hunt's site data ([**demo accessible here**](https://veezoo.com/phdemo)**!**) to provide interesting, actionable insights - especially for those looking to understand current startup and technology trends. as they say, a rising tide lifts all ships\u2026 but it is also hard to make money as an investor or a founder in an overly crowded or hyped market.\n\ndata has been dubbed the \u201cnew oil\u201d upon which empires are built. yet, despite its potential value, most insights remain difficult, if not impossible to unearth without the use of artificial intelligence \u2014 which often brings it outside the realm of your everyday employee.\n\nat [**veezoo**](https://veezoo.com), we believe everyone should be empowered to use data-driven insights to improve their work, productivity, and performance without having to call it or the tech team and then wait days to get an answer. that\u2019s why we created an ultra-simple, chat based ai interface to enable anyone from the insurance salesman or financial analyst to the ecommerce owner or early stage vc to find their \u201cneedle in the haystack\u201d through simple questions, fast answers, and invaluable insights.\n\ntry the product hunt demo here: [**https://veezoo.com/phdemo**](https://veezoo.com/phdemo)\n\nto do this (and also show off a bit more what veezoo can do :), we created an analysis into product hunt trends and insights including: \n\n\\- trending topics on product hunt \n\n\\- top product hunter insights \n\n\\- and tips to maximize the success of a product hunt launch\n\nfor the rest of the post with the charts/graphs/videos, visit:\n\n[**https://www.veezoo.com/blog/veezoo-product-hunt-launch/**](https://www.veezoo.com/blog/veezoo-product-hunt-launch/)\n\n\\--\n\n[**and please check out our post on product hunt.**](https://veezoo.com/producthunt) we'd really appreciate your support if you find data democratization important!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/o6x3n3/information_is_power_turning_product_hunt_data/',)", "identifyer": 5593496, "year": "2021"}, {"autor": "Octoparseideas", "date": 1624505117000, "content": "How to Extract and Monitor Stock Prices from Yahoo! Finance /!/ This article is gonna show you how to extract and monitor stock prices from [Yahoo! Finance](https://finance.yahoo.com/) using a no-coding web scraping tool called Octoparse.\n\n# Auto-detect feature\n\nThe new [Octoparse 8.2](https://www.octoparse.com/product) interface is very intuitive. Once you copy and paste a URL into the address bar, it automatically gets started in parsing the webpage and guesses what content you want to extract. In this case, we are going to enter the most active stocks URL from Yahoo! Finance.\u00a0\n\n# Why do we want to scrape stock prices?\n\nWell, when you constantly extract a stock price and continuously feed the data into your research and data models, you can then train your algorithm with your machine learning code that later gives you more accurate and profitable advice in the investment market.\u00a0\n\nAs you might have known, one of the applications of Octoparse is [price monitoring](https://www.octoparse.com/blog/free-price-monitoring-tools-it-s-fun). Not only does it track and monitor prices on a page, but it also extracts raw data from your competitors and [scrapes real-time data](https://www.octoparse.com/blog/how-to-scrape-real-time-data) within a few mouse clicks.\n\n4 steps to scrape and extract stock prices\n\nWith the [auto-detection](https://helpcenter.octoparse.com/hc/en-us/articles/900000672806-Lesson-1-Extract-data-with-the-brand-new-Auto-detect-algorithm) feature, it takes 4 steps to scrape and extract stock prices.\n\n[**Build your data scraper step by step!**](https://www.octoparse.com/blog/extract-and-monitor-stock-prices-from-yahoo-finance#h8)", "link": "https://www.reddit.com/r/datascience/comments/o6skjm/how_to_extract_and_monitor_stock_prices_from/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "how to extract and monitor stock prices from yahoo! finance /!/ this article is gonna show you how to extract and monitor stock prices from [yahoo! finance](https://finance.yahoo.com/) using a no-coding web scraping -----> tool !!!  called octoparse.\n\n# auto-detect feature\n\nthe new [octoparse 8.2](https://www.octoparse.com/product) interface is very intuitive. once you copy and paste a url into the address bar, it automatically gets started in parsing the webpage and guesses what content you want to extract. in this case, we are going to enter the most active stocks url from yahoo! finance.\u00a0\n\n# why do we want to scrape stock prices?\n\nwell, when you constantly extract a stock price and continuously feed the data into your research and data models, you can then train your algorithm with your machine learning code that later gives you more accurate and profitable advice in the investment market.\u00a0\n\nas you might have known, one of the applications of octoparse is [price monitoring](https://www.octoparse.com/blog/free-price-monitoring-tools-it-s-fun). not only does it track and monitor prices on a page, but it also extracts raw data from your competitors and [scrapes real-time data](https://www.octoparse.com/blog/how-to-scrape-real-time-data) within a few mouse clicks.\n\n4 steps to scrape and extract stock prices\n\nwith the [auto-detection](https://helpcenter.octoparse.com/hc/en-us/articles/900000672806-lesson-1-extract-data-with-the-brand-new-auto-detect-algorithm) feature, it takes 4 steps to scrape and extract stock prices.\n\n[**build your data scraper step by step!**](https://www.octoparse.com/blog/extract-and-monitor-stock-prices-from-yahoo-finance#h8)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/o6skjm/how_to_extract_and_monitor_stock_prices_from/',)", "identifyer": 5593506, "year": "2021"}, {"autor": "carlpaul153", "date": 1624487441000, "content": "Top 5 scraping tools for beginners: Import.io vs Octoparse vs Mozenda vs ParseHub vs Dexi.io. /!/ I have selected the most popular web scraping tools that are friendly for people with little programming skills. Don't be fooled by their simplicity, some of them also support advanced programmable functions. \n\nI have ordered them according to my personal preference (favorites at the end). Of course, this is an opinion and I recommend you do your own research.\n\n* [Import.io](https://www.import.io/): has gained popularity for the way it automatically converts any website into structured data and for its nice interface. Although it can be useful with simple web structures, it is not very good for various types of websites.\n* [Dexi.io](https://www.dexi.io/): similar in usability to Parsehub. Requires more advanced programming skills compared to the following scrapers. Has three types of robots available: extractor, crawler, pipes.\n* [Parsehub](https://www.parsehub.com/): it can deal with complicated scenarios. Although it is intended to offer an easy web scraping experience, a typical user will still need to be a bit technical to fully understand many of its advanced functionalities.\n* [Mozenda](https://www.mozenda.com/): it is one of the \"oldest\" web scraping software on the market. It has an attractive user interface, and very powerful and advanced options. There is not much to criticize it, except..... It's very expensive... and there's no free version :(\n\n[Octoparse](https://www.octoparse.com/): this is my favorite. Like Mozenda it is very simple to use and has powerful advanced options. It guesses the fields surprisingly well, so it saves a lot of time. If this guide is helping you and you are interested in this tool, I ask you to register from [here](https://www.octoparse.com/signup?re=FudAl6IU), so I will get 1 month free of the pro version and you a 30% discount (until June 25).", "link": "https://www.reddit.com/r/datascience/comments/o6nn4k/top_5_scraping_tools_for_beginners_importio_vs/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "top 5 scraping tools for beginners: import.io vs octoparse vs mozenda vs parsehub vs dexi.io. /!/ i have selected the most popular web scraping tools that are friendly for people with little programming skills. don't be fooled by their simplicity, some of them also support advanced programmable functions. \n\ni have ordered them according to my personal preference (favorites at the end). of course, this is an opinion and i recommend you do your own research.\n\n* [import.io](https://www.import.io/): has gained popularity for the way it automatically converts any website into structured data and for its nice interface. although it can be useful with simple web structures, it is not very good for various types of websites.\n* [dexi.io](https://www.dexi.io/): similar in usability to parsehub. requires more advanced programming skills compared to the following scrapers. has three types of robots available: extractor, crawler, pipes.\n* [parsehub](https://www.parsehub.com/): it can deal with complicated scenarios. although it is intended to offer an easy web scraping experience, a typical user will still need to be a bit technical to fully understand many of its advanced functionalities.\n* [mozenda](https://www.mozenda.com/): it is one of the \"oldest\" web scraping software on the market. it has an attractive user interface, and very powerful and advanced options. there is not much to criticize it, except..... it's very expensive... and there's no free version :(\n\n[octoparse](https://www.octoparse.com/): this is my favorite. like mozenda it is very simple to use and has powerful advanced options. it guesses the fields surprisingly well, so it saves a lot of time. if this guide is helping you and you are interested in this -----> tool !!! , i ask you to register from [here](https://www.octoparse.com/signup?re=fudal6iu), so i will get 1 month free of the pro version and you a 30% discount (until june 25).", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/o6nn4k/top_5_scraping_tools_for_beginners_importio_vs/',)", "identifyer": 5593513, "year": "2021"}, {"autor": "nidhaloff", "date": 1624459597000, "content": "I created a machine-learning tool for easy and fast prototyping /!/ Hi everyone, \n\nI wanted to share with you a project that I created along my journey.\n\n[Igel](https://github.com/nidhaloff/igel) is a machine learning tool that makes it very easy to prototype and create/experiment with ML models on the fly. Igel helps you automate many tasks from cleaning your dataset to evaluating the trained model and finally serve it by creating a REST server that is production-ready.\n\nI decided to use FastAPI to build the REST API and unicorn for production.\n\nGithub Repo: [https://github.com/nidhaloff/igel](https://github.com/nidhaloff/igel)", "link": "https://www.reddit.com/r/datascience/comments/o6e9t3/i_created_a_machinelearning_tool_for_easy_and/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "i created a machine-learning -----> tool !!!  for easy and fast prototyping /!/ hi everyone, \n\ni wanted to share with you a project that i created along my journey.\n\n[igel](https://github.com/nidhaloff/igel) is a machine learning tool that makes it very easy to prototype and create/experiment with ml models on the fly. igel helps you automate many tasks from cleaning your dataset to evaluating the trained model and finally serve it by creating a rest server that is production-ready.\n\ni decided to use fastapi to build the rest api and unicorn for production.\n\ngithub repo: [https://github.com/nidhaloff/igel](https://github.com/nidhaloff/igel)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/o6e9t3/i_created_a_machinelearning_tool_for_easy_and/',)", "identifyer": 5593532, "year": "2021"}, {"autor": "grimorg80", "date": 1624441185000, "content": "Hey! This is Al from Gyana, a tech company building a next-gen no-code data science tool /!/ Hey everyone!\n\nI just wanted to introduce myself to the community. I'm grimorg80 on Reddit, but you can call me AL, and I am the Growth Officer at Gyana, a tech company in London. We are working hard to build the next-gen BI tool for everyone, a no-code data analytics (to begin with) cloud tool to remove all the repetitiveness, the prone-to-error tasks, and the time-wasters from daily data science tasks.   \n\n\nWe have ambitious plans for data science, looking at democratising access to tools that help people think critically and find insights based on data. To be more successful, but also to make the world a more practical, data-driven place. Ultimately, we want people to be masters of their own data, whatever the environment. But our focus now is helping data science savvy users to achieve their goals faster and better.\n\nWe have our 2.0 version in beta, and we are obsessed with talking to users. That's the only way we can tell if we are taking the product to the right place. It's a balance between vision and feedback.\n\nWell, that's it for now! I just wanted to make myself known, so that you know where I come from when I will participate to conversations in this group. Take care!!\n\n(Yes, I did post the above on r/nocode, and [r/NoCodeSaaS](https://www.reddit.com/r/NoCodeSaaS/) as well)", "link": "https://www.reddit.com/r/datascience/comments/o69i7k/hey_this_is_al_from_gyana_a_tech_company_building/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "hey! this is al from gyana, a tech company building a next-gen no-code data science -----> tool !!!  /!/ hey everyone!\n\ni just wanted to introduce myself to the community. i'm grimorg80 on reddit, but you can call me al, and i am the growth officer at gyana, a tech company in london. we are working hard to build the next-gen bi tool for everyone, a no-code data analytics (to begin with) cloud tool to remove all the repetitiveness, the prone-to-error tasks, and the time-wasters from daily data science tasks.   \n\n\nwe have ambitious plans for data science, looking at democratising access to tools that help people think critically and find insights based on data. to be more successful, but also to make the world a more practical, data-driven place. ultimately, we want people to be masters of their own data, whatever the environment. but our focus now is helping data science savvy users to achieve their goals faster and better.\n\nwe have our 2.0 version in beta, and we are obsessed with talking to users. that's the only way we can tell if we are taking the product to the right place. it's a balance between vision and feedback.\n\nwell, that's it for now! i just wanted to make myself known, so that you know where i come from when i will participate to conversations in this group. take care!!\n\n(yes, i did post the above on r/nocode, and [r/nocodesaas](https://www.reddit.com/r/nocodesaas/) as well)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/o69i7k/hey_this_is_al_from_gyana_a_tech_company_building/',)", "identifyer": 5593547, "year": "2021"}, {"autor": "vietlinh12hoa", "date": 1627173970000, "content": "What's the future of our work in your opinion? /!/ In 2031, I would believe knowing basic Python or R is being a norm like knowing MS-Office today. We are going to have lots of open source library and data in many sector.\n\nIn DS jobs, We likely won't have any general data scientist but rather than specialists (engineer, banker, marketer...) with expected having a general DS knowledge. \"Offline Notebook\" DS will likely being dead, either being replaced by hard core statistics/mathematician (if the job requires complex math), or DE having DS skill, or ML/cloud engineer having DS skill. \n\nAt school level, DS will be a tool like MS office to be taught for most bachelor programs. Some countries like China can go really far to include them in high school.\n\nData keep growing, people are more smart and aware with their data as a monetary resource. Companies will probably need to pay for the use of client info instead of just asking consent now. There will probably be hot jobs like data security/protector, or data consular/supervisor.\n\nThat's my 2 cents. Would love to know any interesting opinions", "link": "https://www.reddit.com/r/datascience/comments/or1r46/whats_the_future_of_our_work_in_your_opinion/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what's the future of our work in your opinion? /!/ in 2031, i would believe knowing basic python or r is being a norm like knowing ms-office today. we are going to have lots of open source library and data in many sector.\n\nin ds jobs, we likely won't have any general data scientist but rather than specialists (engineer, banker, marketer...) with expected having a general ds knowledge. \"offline notebook\" ds will likely being dead, either being replaced by hard core statistics/mathematician (if the job requires complex math), or de having ds skill, or ml/cloud engineer having ds skill. \n\nat school level, ds will be a -----> tool !!!  like ms office to be taught for most bachelor programs. some countries like china can go really far to include them in high school.\n\ndata keep growing, people are more smart and aware with their data as a monetary resource. companies will probably need to pay for the use of client info instead of just asking consent now. there will probably be hot jobs like data security/protector, or data consular/supervisor.\n\nthat's my 2 cents. would love to know any interesting opinions", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/or1r46/whats_the_future_of_our_work_in_your_opinion/',)", "identifyer": 5593581, "year": "2021"}, {"autor": "analyzeTime", "date": 1627145064000, "content": "Github Discussion: What is your favorite Data Science Repo? /!/ I'm looking to improve my project layout when beginning a new Data Science effort.  For me, the best way to learn is to review other's and see where they were excellent and where their project could use a bit more development.  \n\nIn this vein, I'd like to see what are some favorites for this community and why.  I'd like to keep away from actual tool repos (posting sklearn or keras repos for example) and see projects themselves, but with that said I'm open to see awesome things so if you want to post that anyway then go ahead.", "link": "https://www.reddit.com/r/datascience/comments/oqtknd/github_discussion_what_is_your_favorite_data/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "github discussion: what is your favorite data science repo? /!/ i'm looking to improve my project layout when beginning a new data science effort.  for me, the best way to learn is to review other's and see where they were excellent and where their project could use a bit more development.  \n\nin this vein, i'd like to see what are some favorites for this community and why.  i'd like to keep away from actual -----> tool !!!  repos (posting sklearn or keras repos for example) and see projects themselves, but with that said i'm open to see awesome things so if you want to post that anyway then go ahead.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 23, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/oqtknd/github_discussion_what_is_your_favorite_data/',)", "identifyer": 5593599, "year": "2021"}, {"autor": "daytoniano", "date": 1622766114000, "content": "Is learning web development worth it? /!/ I have seen that sometimes the results obtained from the data science process have to be displayed to the end user in some sort of analytics web tool. Should I add web development to my data science toolbox?", "link": "https://www.reddit.com/r/datascience/comments/nrs4r6/is_learning_web_development_worth_it/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "is learning web development worth it? /!/ i have seen that sometimes the results obtained from the data science process have to be displayed to the end user in some sort of analytics web -----> tool !!! . should i add web development to my data science toolbox?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 5, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nrs4r6/is_learning_web_development_worth_it/',)", "identifyer": 5593831, "year": "2021"}, {"autor": "buttchiquesybobs", "date": 1622756592000, "content": "Best platform for a school data dashboard? /!/ I'm moving into a new role at my school next year--Data Strategist (yay!) and I will be responsible for managing the school's data. At the moment, data is not accessible and we have struggled as a result. \n\nTo my question, what do you think the best tool is for a dashboard that is easy to navigate/filter for many people that are not tech savvy and that will be easy for me to update on a weekly basis. \n\nMy first thought was google data studio since it's pretty straight forward, free, and easily works with sheets. However, I'm also somewhat proficient with python and will be utilizing seaborn (probably) for some of the visualizations. \n\nFurther information.. this is for a high school that has about 600 students and I need dashboards for the entire school, each grade level, and each content area. \n\nThanks!", "link": "https://www.reddit.com/r/datascience/comments/nrou0l/best_platform_for_a_school_data_dashboard/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "best platform for a school data dashboard? /!/ i'm moving into a new role at my school next year--data strategist (yay!) and i will be responsible for managing the school's data. at the moment, data is not accessible and we have struggled as a result. \n\nto my question, what do you think the best -----> tool !!!  is for a dashboard that is easy to navigate/filter for many people that are not tech savvy and that will be easy for me to update on a weekly basis. \n\nmy first thought was google data studio since it's pretty straight forward, free, and easily works with sheets. however, i'm also somewhat proficient with python and will be utilizing seaborn (probably) for some of the visualizations. \n\nfurther information.. this is for a high school that has about 600 students and i need dashboards for the entire school, each grade level, and each content area. \n\nthanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nrou0l/best_platform_for_a_school_data_dashboard/',)", "identifyer": 5593837, "year": "2021"}, {"autor": "0Architectus0", "date": 1635171593000, "content": "Data Science Project Structure Questions /!/ Good day, all! \n\nI'm new to this sub and I thought I'd start with a question I've been struggling with on my data science learning path. I work as a Senior BI analyst at a company who's seeking to engage in data science initatives. I've been very interested in the field for a while as a natural progression of data analysis. Most of the things I've worked on have consisted of scripts/functions running inside of other applications (i.e. Spotfire, Tableau, Power BI). Obviously this doesn't scale very well so I've been exploring using and setting up open source solutions utilizing cloud resources to train and host models. I've been using docker as a virtual environment of sorts after my first attempts at using Conda left me rebuilding venv repeatedly after not properly setting up pip. This has served me well with tasks as I can build up a template image from a base and save off multiple customized images then deploy those images out based on my needs. My latest endeavor has been using an automated project templating tool called kedro. kedro is a python package which prefers to use conda to pass package requirements to other individuals or resources running the project files.  \n\nData pipelines aside my question here is what is the best way to incorporate docker, a virtual environment, and or project management package (i.e. kedro)?\n\nPlease let me know if I'm doing this reddit thing wrong. Have a good one!", "link": "https://www.reddit.com/r/datascience/comments/qfhrzw/data_science_project_structure_questions/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "data science project structure questions /!/ good day, all! \n\ni'm new to this sub and i thought i'd start with a question i've been struggling with on my data science learning path. i work as a senior bi analyst at a company who's seeking to engage in data science initatives. i've been very interested in the field for a while as a natural progression of data analysis. most of the things i've worked on have consisted of scripts/functions running inside of other applications (i.e. spotfire, tableau, power bi). obviously this doesn't scale very well so i've been exploring using and setting up open source solutions utilizing cloud resources to train and host models. i've been using docker as a virtual environment of sorts after my first attempts at using conda left me rebuilding venv repeatedly after not properly setting up pip. this has served me well with tasks as i can build up a template image from a base and save off multiple customized images then deploy those images out based on my needs. my latest endeavor has been using an automated project templating -----> tool !!!  called kedro. kedro is a python package which prefers to use conda to pass package requirements to other individuals or resources running the project files.  \n\ndata pipelines aside my question here is what is the best way to incorporate docker, a virtual environment, and or project management package (i.e. kedro)?\n\nplease let me know if i'm doing this reddit thing wrong. have a good one!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/qfhrzw/data_science_project_structure_questions/',)", "identifyer": 5593937, "year": "2021"}, {"autor": "dokasov", "date": 1615417459000, "content": "How could I go about combining many different formats of tables and data into a single unique format? /!/ Hello, I currently have a job as a data scientist, and in the team I'm in we're working to create a tool that automates the process of reviewing different business files that clients send and turning each of them into a spreadsheet with the same format for each one.\n\nThese files, which are something like promotional PDFs, have the same type of information in all of them, with the same general feel of the tables with data, but the specific column names and headers and general style of the file vary widely from client to client.\n\nCurrently someone reads the file and manually creates the spreadsheet with the regular format. That's what we're aiming to automate. I'm not asking for the software development side of things. I'm asking for the data science part, supposing we already can read the files and get all of the information in each one of them with a manageable format. We need to know how to get the relevant data out of this information, a lot of which is tables, and actually know what belongs to each category in the spreadsheet.\n\nWe have a couple of ideas about how we could go about creating this tool, but I'm a bit new in this and I'm wondering if this type of problem has a name so I can more easily search for it, if there is any literature I can read on this topic, and if you have any ideas about algorithms or something that could point me in the right direction.\n\nAny insight is very appreciated", "link": "https://www.reddit.com/r/datascience/comments/m2bkxl/how_could_i_go_about_combining_many_different/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "how could i go about combining many different formats of tables and data into a single unique format? /!/ hello, i currently have a job as a data scientist, and in the team i'm in we're working to create a -----> tool !!!  that automates the process of reviewing different business files that clients send and turning each of them into a spreadsheet with the same format for each one.\n\nthese files, which are something like promotional pdfs, have the same type of information in all of them, with the same general feel of the tables with data, but the specific column names and headers and general style of the file vary widely from client to client.\n\ncurrently someone reads the file and manually creates the spreadsheet with the regular format. that's what we're aiming to automate. i'm not asking for the software development side of things. i'm asking for the data science part, supposing we already can read the files and get all of the information in each one of them with a manageable format. we need to know how to get the relevant data out of this information, a lot of which is tables, and actually know what belongs to each category in the spreadsheet.\n\nwe have a couple of ideas about how we could go about creating this tool, but i'm a bit new in this and i'm wondering if this type of problem has a name so i can more easily search for it, if there is any literature i can read on this topic, and if you have any ideas about algorithms or something that could point me in the right direction.\n\nany insight is very appreciated", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/m2bkxl/how_could_i_go_about_combining_many_different/',)", "identifyer": 5593990, "year": "2021"}, {"autor": "urban_citrus", "date": 1620794411000, "content": "Excel Hate /!/ What is it with data scientists and being snobs about Excel? It's a great tool if one wants to get the texture of a data, build simple one-off things, or even prototype logic for a workflow. Maybe I'm weird, but I like having stats and using that to flip around data in simple pivots, then digging into the crosstabs.\n\nI understand we like our cutting edge and bespoke tooling, but Excel is as if not more effective to walk a client through parts of their data in a familiar environment. Since I've been in the field I had the idea in the back of my mind that it is a difference between people that tend to think in terms of functions vs tables. \n\nI come from microbiology/mycology research where every record was painstakingly recorded in a table, so I probably put more value on dissecting the data much more. As I moved from that career track in 2013 to where I am now, that focus was on increasingly larger ('big') data and greater distance from the data. This may have just been the different between capturing hundreds of observations in the lab by hand to inferring risk from millions of user permissions.\n\nOr, is it customary to dunk on Excel now?", "link": "https://www.reddit.com/r/datascience/comments/nagk6a/excel_hate/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "excel hate /!/ what is it with data scientists and being snobs about excel? it's a great -----> tool !!!  if one wants to get the texture of a data, build simple one-off things, or even prototype logic for a workflow. maybe i'm weird, but i like having stats and using that to flip around data in simple pivots, then digging into the crosstabs.\n\ni understand we like our cutting edge and bespoke tooling, but excel is as if not more effective to walk a client through parts of their data in a familiar environment. since i've been in the field i had the idea in the back of my mind that it is a difference between people that tend to think in terms of functions vs tables. \n\ni come from microbiology/mycology research where every record was painstakingly recorded in a table, so i probably put more value on dissecting the data much more. as i moved from that career track in 2013 to where i am now, that focus was on increasingly larger ('big') data and greater distance from the data. this may have just been the different between capturing hundreds of observations in the lab by hand to inferring risk from millions of user permissions.\n\nor, is it customary to dunk on excel now?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 187, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nagk6a/excel_hate/',)", "identifyer": 5594061, "year": "2021"}, {"autor": "po-handz", "date": 1620783757000, "content": "What's a good way/metric to test a search engine? /!/ At work we've built a search engine based on big data  and I've got it working great for our inhouse endusers. However, the data is known incomplete, and the missingness is not random.\n\nWhen the leadership team tests the tool they pick a few cases they think they know and gauge results that way. I think this cherry picking will lead to problematic decisions. Are there better ways to test search engine results then large test sets?", "link": "https://www.reddit.com/r/datascience/comments/nadg43/whats_a_good_waymetric_to_test_a_search_engine/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what's a good way/metric to test a search engine? /!/ at work we've built a search engine based on big data  and i've got it working great for our inhouse endusers. however, the data is known incomplete, and the missingness is not random.\n\nwhen the leadership team tests the -----> tool !!!  they pick a few cases they think they know and gauge results that way. i think this cherry picking will lead to problematic decisions. are there better ways to test search engine results then large test sets?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nadg43/whats_a_good_waymetric_to_test_a_search_engine/',)", "identifyer": 5594064, "year": "2021"}, {"autor": "okjnbvgfertyuiojhg", "date": 1620768466000, "content": "Seeking recommendations for a model evaluation tool if one exists? /!/ Our team works on various models and prediction problems. Sometimes we get bogged down in discussion about better approaches and want to test new ideas. In each team members specific individual scripts they have test and evaluation data which makes true side by side comparisons trickier than otherwise.\n\nMy question is, is there a service, open source server or even a paid tool where we could send our predictions to be tested against a single universal out of sample test set for model evaluation? \n\nThe only example I can think of is Kaggle. Years ago I attempted to join a competition and you would submit a csv of predictions on some test data and your score would show on a leader board.\n\nAre there any tools, servers, libraries or platforms out there that work in a similar fashion that would allow our team to compare and compete with each other on some prediction tasks?", "link": "https://www.reddit.com/r/datascience/comments/na88bo/seeking_recommendations_for_a_model_evaluation/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "seeking recommendations for a model evaluation -----> tool !!!  if one exists? /!/ our team works on various models and prediction problems. sometimes we get bogged down in discussion about better approaches and want to test new ideas. in each team members specific individual scripts they have test and evaluation data which makes true side by side comparisons trickier than otherwise.\n\nmy question is, is there a service, open source server or even a paid tool where we could send our predictions to be tested against a single universal out of sample test set for model evaluation? \n\nthe only example i can think of is kaggle. years ago i attempted to join a competition and you would submit a csv of predictions on some test data and your score would show on a leader board.\n\nare there any tools, servers, libraries or platforms out there that work in a similar fashion that would allow our team to compare and compete with each other on some prediction tasks?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/na88bo/seeking_recommendations_for_a_model_evaluation/',)", "identifyer": 5594072, "year": "2021"}, {"autor": "collington99", "date": 1620655379000, "content": "Is It better to start out your career with Excel or SQL for Jr. Data analyst role? /!/ I'm a recent graduate and I've been offer a role as a junior data analyst for a clothing company in their e-commerce department. They use Excel as their analysis tool and perform sales analysis and other types of retail analysis. I've been told that Its better to get a job that uses SQL instead of Excel. Am I setting myself back by accepting this jr. data analyst position that uses Excel?", "link": "https://www.reddit.com/r/datascience/comments/n9564t/is_it_better_to_start_out_your_career_with_excel/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "is it better to start out your career with excel or sql for jr. data analyst role? /!/ i'm a recent graduate and i've been offer a role as a junior data analyst for a clothing company in their e-commerce department. they use excel as their analysis -----> tool !!!  and perform sales analysis and other types of retail analysis. i've been told that its better to get a job that uses sql instead of excel. am i setting myself back by accepting this jr. data analyst position that uses excel?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/n9564t/is_it_better_to_start_out_your_career_with_excel/',)", "identifyer": 5594137, "year": "2021"}, {"autor": "Tenshiroque", "date": 1612387148000, "content": "Data quality assessment tool /!/ Hello everyone,\n\nI am pretty new to data sciences and I have a fairly specific need, even if I am not sure this is the right place for this request :\n\nAs part of my job, I am looking for a tool that could analyze data in a database and generate quality indicators corresponding to management rules.\n\nThe main objective is to enter into a process of improving the quality of the data thanks to these indicators.\n\nDo you know any existing tools allowing this kind of process?", "link": "https://www.reddit.com/r/datascience/comments/lbylhc/data_quality_assessment_tool/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "data quality assessment -----> tool !!!  /!/ hello everyone,\n\ni am pretty new to data sciences and i have a fairly specific need, even if i am not sure this is the right place for this request :\n\nas part of my job, i am looking for a tool that could analyze data in a database and generate quality indicators corresponding to management rules.\n\nthe main objective is to enter into a process of improving the quality of the data thanks to these indicators.\n\ndo you know any existing tools allowing this kind of process?", "sortedWord": "None", "removed": "('nan',)", "score": 0, "comments": 5, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/lbylhc/data_quality_assessment_tool/',)", "identifyer": 5594208, "year": "2021"}, {"autor": "the21st", "date": 1621502478000, "content": "Tableau-like tool for Jupyter? /!/ Hey there! I'm a big fan a drag-and-drop visualization tools like Tableau or Metabase and would love to have something similar inside Jupyter to visualize pandas Dataframes.\n\nSo I did some research and wrote up a [post](https://medium.com/deepnote/alternatives-to-matplotlib-for-pandas-dataframes-f15be61c3cf4) about the options I found. The best ones I know about are D-Tale, bamboolib and Deepnote's chart cells. Full disclosure: I work at Deepnote, and I care a lot about notebooks as a paradigm so I'm trying to help build the best one out there.\n\nLet me know if there are other tools I've missed!", "link": "https://www.reddit.com/r/datascience/comments/ngvhny/tableaulike_tool_for_jupyter/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "tableau-like -----> tool !!!  for jupyter? /!/ hey there! i'm a big fan a drag-and-drop visualization tools like tableau or metabase and would love to have something similar inside jupyter to visualize pandas dataframes.\n\nso i did some research and wrote up a [post](https://medium.com/deepnote/alternatives-to-matplotlib-for-pandas-dataframes-f15be61c3cf4) about the options i found. the best ones i know about are d-tale, bamboolib and deepnote's chart cells. full disclosure: i work at deepnote, and i care a lot about notebooks as a paradigm so i'm trying to help build the best one out there.\n\nlet me know if there are other tools i've missed!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/ngvhny/tableaulike_tool_for_jupyter/',)", "identifyer": 5594276, "year": "2021"}, {"autor": "geldersekifuzuli", "date": 1621447891000, "content": "Paid NLP Tools vs Building Own Model /!/ I have started to work for a small start-up company. We are only 2 data scientists. \n\nI am trying to understand consumer satisfaction by analizing reviews. Sentiment Analysis and NER will be methods I will go for as initial step. \n\nMy company doesn't have an NLP pipeline yet. \n\nI wonder which one is better: using Paid NLP Tools like Google Could NLP, IBM's Watson NLU or a self build NLP model?\n\nI would be happy to hear what you think. Pros and cons.\n\nGoogle's service seems a bit expensive. But, pricing is still confusing. For example, if I have 500 million reviews, how much am I expected to pay for Sentiment Analysis and NER services? (an estimate) \n\nDoes Google charge me again whenever I run sentiment analysis? \n\nIs there any cheaper but still effective cloud computing NLP tool that you can suggest?\n\nI would be happy to hear your insights!", "link": "https://www.reddit.com/r/datascience/comments/ngcufb/paid_nlp_tools_vs_building_own_model/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "paid nlp tools vs building own model /!/ i have started to work for a small start-up company. we are only 2 data scientists. \n\ni am trying to understand consumer satisfaction by analizing reviews. sentiment analysis and ner will be methods i will go for as initial step. \n\nmy company doesn't have an nlp pipeline yet. \n\ni wonder which one is better: using paid nlp tools like google could nlp, ibm's watson nlu or a self build nlp model?\n\ni would be happy to hear what you think. pros and cons.\n\ngoogle's service seems a bit expensive. but, pricing is still confusing. for example, if i have 500 million reviews, how much am i expected to pay for sentiment analysis and ner services? (an estimate) \n\ndoes google charge me again whenever i run sentiment analysis? \n\nis there any cheaper but still effective cloud computing nlp -----> tool !!!  that you can suggest?\n\ni would be happy to hear your insights!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 7, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/ngcufb/paid_nlp_tools_vs_building_own_model/',)", "identifyer": 5594297, "year": "2021"}, {"autor": "quite--average", "date": 1621445386000, "content": "Do data scientists or aspiring data scientists need to grind leetcode for interview prep? /!/ Hello!\n\nThere's a general consensus on apps like Blind that doing leetcode is probably the most important thing to land a job in tech. I understand that Blind is skewed towards SWE. Anyway, this isn't about blind. \nI wanted to know if you guys do leetcode or equivalent questions to prepare for data scientist interviews. (Not talking about SQL)\nI have a Statistics background and I use R at work. Python is not really my strong tool, although I'm working on it. So to prep for Python coding rounds, should I be doing leetcode type questions? I think those are very SWE oriented questions and it will take me a lot of time to even get through Easy section. I used R mostly for data cleaning, Feature Engineering, model training. I don't think I am anywhere close to being able to do SWE type questions.\n\nI wanted to ask from more experienced people in this sub that what's the best way to prep for Python coding rounds? \n\nThanks!", "link": "https://www.reddit.com/r/datascience/comments/ngbu97/do_data_scientists_or_aspiring_data_scientists/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "do data scientists or aspiring data scientists need to grind leetcode for interview prep? /!/ hello!\n\nthere's a general consensus on apps like blind that doing leetcode is probably the most important thing to land a job in tech. i understand that blind is skewed towards swe. anyway, this isn't about blind. \ni wanted to know if you guys do leetcode or equivalent questions to prepare for data scientist interviews. (not talking about sql)\ni have a statistics background and i use r at work. python is not really my strong -----> tool !!! , although i'm working on it. so to prep for python coding rounds, should i be doing leetcode type questions? i think those are very swe oriented questions and it will take me a lot of time to even get through easy section. i used r mostly for data cleaning, feature engineering, model training. i don't think i am anywhere close to being able to do swe type questions.\n\ni wanted to ask from more experienced people in this sub that what's the best way to prep for python coding rounds? \n\nthanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 18, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/ngbu97/do_data_scientists_or_aspiring_data_scientists/',)", "identifyer": 5594298, "year": "2021"}, {"autor": "tanin47", "date": 1625902030000, "content": "Superintendent.app 2.0: a Desktop app for working with large CSV files (tested with 1GB) using SQL /!/ Hi Reddit,\n\nI'm Tanin, and I've built [Superintendent.app](https://Superintendent.app), which is a Desktop that lets you load a bunch of large CSV files and write SQL on them.\n\nI built it because I have hit the 1M row limit on Excel quite often, and loading CSV into a database through command-line (or using full fledge programming language like python) is burdensome. I've been using this tool at work, and I love it... but I'm biased :)\n\nIn the 2.0 version, it now can load 1GB CSV files fairly quickly (\\~20 seconds). It uses C, so I think it is as fast as it can be. Exporting the result back to CSV is also as fast. Previously, this took 10 minutes.\n\nI'm actively looking for beta users, would love for you to try, and give me feedback and thoughts on how it may fit into your tasks.\n\nHere is the website: [https://superintendent.app](https://superintendent.app)\n\nThank you!", "link": "https://www.reddit.com/r/datascience/comments/ohe857/superintendentapp_20_a_desktop_app_for_working/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "superintendent.app 2.0: a desktop app for working with large csv files (tested with 1gb) using sql /!/ hi reddit,\n\ni'm tanin, and i've built [superintendent.app](https://superintendent.app), which is a desktop that lets you load a bunch of large csv files and write sql on them.\n\ni built it because i have hit the 1m row limit on excel quite often, and loading csv into a database through command-line (or using full fledge programming language like python) is burdensome. i've been using this -----> tool !!!  at work, and i love it... but i'm biased :)\n\nin the 2.0 version, it now can load 1gb csv files fairly quickly (\\~20 seconds). it uses c, so i think it is as fast as it can be. exporting the result back to csv is also as fast. previously, this took 10 minutes.\n\ni'm actively looking for beta users, would love for you to try, and give me feedback and thoughts on how it may fit into your tasks.\n\nhere is the website: [https://superintendent.app](https://superintendent.app)\n\nthank you!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/ohe857/superintendentapp_20_a_desktop_app_for_working/',)", "identifyer": 5594497, "year": "2021"}, {"autor": "ThisSoFrustrating", "date": 1626679298000, "content": "What are some tools out there where I can discover growing trends? /!/ I was testing out google trends, and search words like \"cyber security', but is there a way or another tool to check whether that's coming from the general population, rather then say cyber security specialists and engineers that type the word in since there's been a spike in demand. I.e. how would you distinguish who types the words in? And what demographic does that data apply to? Or perhaps it's not meant to be segmented?", "link": "https://www.reddit.com/r/datascience/comments/on93zo/what_are_some_tools_out_there_where_i_can/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what are some tools out there where i can discover growing trends? /!/ i was testing out google trends, and search words like \"cyber security', but is there a way or another -----> tool !!!  to check whether that's coming from the general population, rather then say cyber security specialists and engineers that type the word in since there's been a spike in demand. i.e. how would you distinguish who types the words in? and what demographic does that data apply to? or perhaps it's not meant to be segmented?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/on93zo/what_are_some_tools_out_there_where_i_can/',)", "identifyer": 5594596, "year": "2021"}, {"autor": "Kawaguy90", "date": 1626669352000, "content": "Significance of Lag Features in a Time Series Analysis /!/ I'm a beginner in the DS realm looking to tackle a Kaggle competition to get some experience working on data, and the data follows a time series.\n\nI recently learned about the importance of lag features but I'm having trouble figuring out which lag features are \"best\" to create when preprocessing the data.\n\nI've created autocorrelation and partial autocorrelation plots for my features but that's really as far as I've gotten. I don't fully understand how to interpret these plots.\n\nIs there a tool/method that is able to determine which lag features would be best to create/capture the most information about the data? Also, can someone explain how to properly go about \"choosing\" which lag number is suitable for these plots?", "link": "https://www.reddit.com/r/datascience/comments/on6x26/significance_of_lag_features_in_a_time_series/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "significance of lag features in a time series analysis /!/ i'm a beginner in the ds realm looking to tackle a kaggle competition to get some experience working on data, and the data follows a time series.\n\ni recently learned about the importance of lag features but i'm having trouble figuring out which lag features are \"best\" to create when preprocessing the data.\n\ni've created autocorrelation and partial autocorrelation plots for my features but that's really as far as i've gotten. i don't fully understand how to interpret these plots.\n\nis there a -----> tool !!! /method that is able to determine which lag features would be best to create/capture the most information about the data? also, can someone explain how to properly go about \"choosing\" which lag number is suitable for these plots?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/on6x26/significance_of_lag_features_in_a_time_series/',)", "identifyer": 5594600, "year": "2021"}, {"autor": "vaaalbara", "date": 1612967311000, "content": "Deployable Data Science on the Windows, Azure and Synapse /!/ Hello all, I have two questions about using Windows for deploying ML projects at different scales. They somewhat link in with each other.\n\n1) If I am using a Windows 10 machine, I can imagine a workflow where the first thing I do is install WSL2 and forever pretend I am using Linux. Is this what people usually do? It seems to me that this would be a sensible idea, but maybe I am being a Mac/Ubuntu elitist ha. Then, when I go to deploy my (for example) tensorflow model I could put it in a docker container, and host it on a server somewhere to access it. There would be a git repo with some CI to facilitate working together. \n\n2) However, what if we are working at a much larger scale? The data is being fed in from some Azure Data Warehouse, containing huge amounts of data and there is some PowerBi at the end. In short, the whole pipeline is being handled by Microsoft and Azure. Maybe we want to make live predictions with it. In this scenario, to stay within the already defined ecosystem I now am having to use Azure Synapse (or another one of Microsoft ML pipeline offerings).  This is where my main conceptual problems start. I have always been used to doing everything from the command line, but i am open-minded and want to use the best tool for the problem at hand. I understand Synapse is meant to help do ML in a production pipeline, but I can't reconcile (my maybe poor understanding of) what it does with what I would consider the \"normal\" pipeline in question 1). \n\nIs the new pipeline that I just do the exact same stuff as in question 1), but somewhere within Synapse I call the Docker container? Or, should I now write all my code (including all the time you spend exploring the data and parameter-tuning) within Synapse, and also let it handle version control? Does this remove all need for Docker, as Azure will magically handle hosting and GPUs and everything? Can Synapse do CI? Can I run unit tests and a linter through it? What about debugging problems, or installing a python package from source? I feel old fashioned wanting to stick with what I mentioned in 1) but I think it would be worse to ignore Synapse and unnecessarily be difficult by leaving the Microsoft eco-system when the rest of the pipeline is built in it? \n\nIn this scenario, whatever models I make have to play nice with the pre-established Azure pipeline, but I don't fully understand how to make that happen. What is the best way to deploy a model that has to live within Microsoft ecosystem? Are all my \"problems\" I listed above no really problems when you actually enter Synpase and get to work? Is there anyone who is currently using Synapse, or maybe another one of Microsoft's ML offerings, able to clear this up for me? Links to talks, articles and presentations on the subject would also be hugely helpful. Thanks.", "link": "https://www.reddit.com/r/datascience/comments/lguqxc/deployable_data_science_on_the_windows_azure_and/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "deployable data science on the windows, azure and synapse /!/ hello all, i have two questions about using windows for deploying ml projects at different scales. they somewhat link in with each other.\n\n1) if i am using a windows 10 machine, i can imagine a workflow where the first thing i do is install wsl2 and forever pretend i am using linux. is this what people usually do? it seems to me that this would be a sensible idea, but maybe i am being a mac/ubuntu elitist ha. then, when i go to deploy my (for example) tensorflow model i could put it in a docker container, and host it on a server somewhere to access it. there would be a git repo with some ci to facilitate working together. \n\n2) however, what if we are working at a much larger scale? the data is being fed in from some azure data warehouse, containing huge amounts of data and there is some powerbi at the end. in short, the whole pipeline is being handled by microsoft and azure. maybe we want to make live predictions with it. in this scenario, to stay within the already defined ecosystem i now am having to use azure synapse (or another one of microsoft ml pipeline offerings).  this is where my main conceptual problems start. i have always been used to doing everything from the command line, but i am open-minded and want to use the best -----> tool !!!  for the problem at hand. i understand synapse is meant to help do ml in a production pipeline, but i can't reconcile (my maybe poor understanding of) what it does with what i would consider the \"normal\" pipeline in question 1). \n\nis the new pipeline that i just do the exact same stuff as in question 1), but somewhere within synapse i call the docker container? or, should i now write all my code (including all the time you spend exploring the data and parameter-tuning) within synapse, and also let it handle version control? does this remove all need for docker, as azure will magically handle hosting and gpus and everything? can synapse do ci? can i run unit tests and a linter through it? what about debugging problems, or installing a python package from source? i feel old fashioned wanting to stick with what i mentioned in 1) but i think it would be worse to ignore synapse and unnecessarily be difficult by leaving the microsoft eco-system when the rest of the pipeline is built in it? \n\nin this scenario, whatever models i make have to play nice with the pre-established azure pipeline, but i don't fully understand how to make that happen. what is the best way to deploy a model that has to live within microsoft ecosystem? are all my \"problems\" i listed above no really problems when you actually enter synpase and get to work? is there anyone who is currently using synapse, or maybe another one of microsoft's ml offerings, able to clear this up for me? links to talks, articles and presentations on the subject would also be hugely helpful. thanks.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/lguqxc/deployable_data_science_on_the_windows_azure_and/',)", "identifyer": 5594670, "year": "2021"}, {"autor": "killanight", "date": 1612842542000, "content": "Should I just work as anything? /!/ Hi, hope You're doing well..\n\nI'm (21 yo) looking to be a machine learning engineer someday and so I decided to build up to that by working as a Data Analyst first, gain some experience, and then move on to Data Science and machine learning.\n\nI got the knowledge, the programming skills, the thing is, I don't have experience, and it's Been HELL looking for Jobs, i don't know if it's because of the pandemic or what, but there isnt any junior / trainee position, I applied to about 50+ places (can't find more!) and I don't Even get answers anymore.\n\nOk, so I decided to look out for Jobs related to it, like python developer, that way I can have experience using Python and it'll be useful in the future when looking for data analyst/data science Jobs\n\nAgain, nothing..\n\nI tried to look for SQL analyst, nothing either, I learned Power BI just for the sake of finding a job as a BI analyst (btw how many types of Analyst are out there?), and nothing either\n\nSo... Something happened, i applied as a machine learning engineer (a bit much for now, but who knows), well the recruiter told me that I needed experience for the role, but she has a position as a Java developer Jr in a well known but not faang company in USA (i'm from Argentina) that would be a Nice fit for me.\n\nSo, I accepted cause, I really can't believe i'm having an interview, and because I was curious about the job.\n\nEverything went well, they did a test on me, piece of cake, but i'm having second thoughts now\n\n\nWill this be useful in my path of becoming a data scientist? Is any experience good for recruiters? Should I just use My time instead to learn new tools? \n\nNote that, I love programming, so it wouldnt really be a bother to work there, but I've decided to be a machine learning engineer and I really want to follow that path...\n\nMy skillset is:\n\nProgramming languages: C, Java, Python\nPython libraries: selenium, pandas, matplotlib, spark, \nDatabases: MySQL, postgreSQL\nTools for analysis: PowerBI\nOthers: Github\n\nIt doesn't seen much, so please, any advice about what to do or what tool I should learn in order to get a job in this field is appreciated.\n\nThank You for reading", "link": "https://www.reddit.com/r/datascience/comments/lftvcg/should_i_just_work_as_anything/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "should i just work as anything? /!/ hi, hope you're doing well..\n\ni'm (21 yo) looking to be a machine learning engineer someday and so i decided to build up to that by working as a data analyst first, gain some experience, and then move on to data science and machine learning.\n\ni got the knowledge, the programming skills, the thing is, i don't have experience, and it's been hell looking for jobs, i don't know if it's because of the pandemic or what, but there isnt any junior / trainee position, i applied to about 50+ places (can't find more!) and i don't even get answers anymore.\n\nok, so i decided to look out for jobs related to it, like python developer, that way i can have experience using python and it'll be useful in the future when looking for data analyst/data science jobs\n\nagain, nothing..\n\ni tried to look for sql analyst, nothing either, i learned power bi just for the sake of finding a job as a bi analyst (btw how many types of analyst are out there?), and nothing either\n\nso... something happened, i applied as a machine learning engineer (a bit much for now, but who knows), well the recruiter told me that i needed experience for the role, but she has a position as a java developer jr in a well known but not faang company in usa (i'm from argentina) that would be a nice fit for me.\n\nso, i accepted cause, i really can't believe i'm having an interview, and because i was curious about the job.\n\neverything went well, they did a test on me, piece of cake, but i'm having second thoughts now\n\n\nwill this be useful in my path of becoming a data scientist? is any experience good for recruiters? should i just use my time instead to learn new tools? \n\nnote that, i love programming, so it wouldnt really be a bother to work there, but i've decided to be a machine learning engineer and i really want to follow that path...\n\nmy skillset is:\n\nprogramming languages: c, java, python\npython libraries: selenium, pandas, matplotlib, spark, \ndatabases: mysql, postgresql\ntools for analysis: powerbi\nothers: github\n\nit doesn't seen much, so please, any advice about what to do or what -----> tool !!!  i should learn in order to get a job in this field is appreciated.\n\nthank you for reading", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/lftvcg/should_i_just_work_as_anything/',)", "identifyer": 5594721, "year": "2021"}, {"autor": "narimantos", "date": 1612823547000, "content": "Determine if project is Datascience or just Software Development? /!/ Hi guys, \n\nI'm very new to Datascience. Today I started my graduation Internship. The goal of my assignment is to look into firewall logs and analyze which firewall rule could be made \"better\" or saver. \n\nThe logs are stored into a Elasticsearch DB in order to make the analysis ill have to query or use some other tool like Kibana or Python.\n\nAt the moment I'm trying to decide what project methodology I'm going to use. For software development I know Agile like methodology. But I also read that for Datascience there are methodologies like Crisp-DM or \"Team Data Science Process\" from Microsoft and also combination from CRISP &amp; SCRUM. And a lot more. \n\nMy question how do I decide if the scope of the project is Datascience and not just Software Development? Or maybe it can be little bit of both? Wondering what you guys think.", "link": "https://www.reddit.com/r/datascience/comments/lfnrq1/determine_if_project_is_datascience_or_just/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "determine if project is datascience or just software development? /!/ hi guys, \n\ni'm very new to datascience. today i started my graduation internship. the goal of my assignment is to look into firewall logs and analyze which firewall rule could be made \"better\" or saver. \n\nthe logs are stored into a elasticsearch db in order to make the analysis ill have to query or use some other -----> tool !!!  like kibana or python.\n\nat the moment i'm trying to decide what project methodology i'm going to use. for software development i know agile like methodology. but i also read that for datascience there are methodologies like crisp-dm or \"team data science process\" from microsoft and also combination from crisp &amp; scrum. and a lot more. \n\nmy question how do i decide if the scope of the project is datascience and not just software development? or maybe it can be little bit of both? wondering what you guys think.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/lfnrq1/determine_if_project_is_datascience_or_just/',)", "identifyer": 5594735, "year": "2021"}, {"autor": "tururut_tururut", "date": 1612800844000, "content": "Friendly reminder to myself: sometimes there's a better tool /!/ I'm not a professional data scientist, but data science and analysis are part of what I do in my job. We basically do statistics about cities plus some other consultancy stuff and some of us are trying to raise the bar using DS methods to get better results rather than just hoping the national statistics office will have relevant data.\n\nSo I need to get the total area of urban parks in every municipality of my country, using Open Street Map data (There are a few official land cover tools but their quality is pretty dismal). I do most of my analysis in Python so there I go: import geopandas, import pandas... now, how does one download data from OSM in Python? I've done that using OSMNx (a brilliant library to perform all sorts of network analysis on city street networks using OSM data), besides using JOSM or the QGIS plug-in. So there I write a function to download the data I need for every polygon in a shapefile (Did I mention there's almost 9000 municipalities in my country? Now you know) and set it running. My PC almost grinds to a halt but I manage to still be semi-productive reviewing some work from my colleagues, documenting some stuff, you get it. After twelve hours, Jupyterlab gives up. Never mind, this must be because there are just too many municipalities and, let's face it, OSMNx is slow. So let's try joining as many polygons as possible and try it again. The computer almost grinds to a halt. My laptop is not particularly bad but definitely not up to the task. Back to the drawing board.\n\nThen I think, I already have all the OSM data for my country, I just need libraries to query it... esy-osmfilter! Nope, it didn't work. Timed out. Pyrosm. Neither. I'm loosing the little sanity I had left. Should I try QGIS? It's crashing if I try, been there done that. Another lost day. Finally, I remembered, of course, that one queries with SQL and that as much as I'm the Python guy at work, I need not use it for everything. Now, my SQL knowledge is rather abyssal but I can survive if it's not too tough. So I googled \"how to open pbf file in postgis\" for two minutes. Took twenty minutes to understand how osm2pgsql works, twenty more minutes to set up the database and fifteen minutes to query it, check it worked and export it to a geopackage to do the analysis tomorrow morning. Sometimes you need to remember yourself that you have more than a hammer in your toolbox. Still, the analysis will be long and eat up a lot of memory, but it's in the realm of doable stuff. Thanks for coming to my TED talk, I'll be having a much deserved beer and congratulating myself for having saved my face with the higher-ups. Peace, out.", "link": "https://www.reddit.com/r/datascience/comments/lff5t6/friendly_reminder_to_myself_sometimes_theres_a/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "friendly reminder to myself: sometimes there's a better -----> tool !!!  /!/ i'm not a professional data scientist, but data science and analysis are part of what i do in my job. we basically do statistics about cities plus some other consultancy stuff and some of us are trying to raise the bar using ds methods to get better results rather than just hoping the national statistics office will have relevant data.\n\nso i need to get the total area of urban parks in every municipality of my country, using open street map data (there are a few official land cover tools but their quality is pretty dismal). i do most of my analysis in python so there i go: import geopandas, import pandas... now, how does one download data from osm in python? i've done that using osmnx (a brilliant library to perform all sorts of network analysis on city street networks using osm data), besides using josm or the qgis plug-in. so there i write a function to download the data i need for every polygon in a shapefile (did i mention there's almost 9000 municipalities in my country? now you know) and set it running. my pc almost grinds to a halt but i manage to still be semi-productive reviewing some work from my colleagues, documenting some stuff, you get it. after twelve hours, jupyterlab gives up. never mind, this must be because there are just too many municipalities and, let's face it, osmnx is slow. so let's try joining as many polygons as possible and try it again. the computer almost grinds to a halt. my laptop is not particularly bad but definitely not up to the task. back to the drawing board.\n\nthen i think, i already have all the osm data for my country, i just need libraries to query it... esy-osmfilter! nope, it didn't work. timed out. pyrosm. neither. i'm loosing the little sanity i had left. should i try qgis? it's crashing if i try, been there done that. another lost day. finally, i remembered, of course, that one queries with sql and that as much as i'm the python guy at work, i need not use it for everything. now, my sql knowledge is rather abyssal but i can survive if it's not too tough. so i googled \"how to open pbf file in postgis\" for two minutes. took twenty minutes to understand how osm2pgsql works, twenty more minutes to set up the database and fifteen minutes to query it, check it worked and export it to a geopackage to do the analysis tomorrow morning. sometimes you need to remember yourself that you have more than a hammer in your toolbox. still, the analysis will be long and eat up a lot of memory, but it's in the realm of doable stuff. thanks for coming to my ted talk, i'll be having a much deserved beer and congratulating myself for having saved my face with the higher-ups. peace, out.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/lff5t6/friendly_reminder_to_myself_sometimes_theres_a/',)", "identifyer": 5594753, "year": "2021"}, {"autor": "rwyatt101", "date": 1625389713000, "content": "Where to start? Customer churn /!/ Hello,\n\nA bit of background: I've worked my way through a number of data-based roles over the past 8 years and I've found myself recently as the manager of a small analytics engineering team. The team is currently a mix of technical analysts (BI tool / SQL experts) and junior data engineers. The company I work for had traditionally placed little value in data engineering and, as a result, I've seen a number of data science projects start and fail very quickly for the same reason: poor data management and availability. \n\nI believe that through some great work in my team we now have all the building blocks needed to start a more sophisticated approach to predicting customer churn. However, thanks to previous failed endeavours, it's proving quite difficult to convince any of the analyst teams to spend time on it, and we lack any \"real\" data science functions in the business to pick this up.\n\nIn my view this is a double win for the company if I can get a churn model off the ground: it will firmly embed and prove the value of data engineering as a core function in the business and it will also provide great value on the work already done by my team. In an ideal world I would have had some buy in for this at the start. As a team were having to take little wins in this space in an attempt to win the hearts and minds of senior management. We've had some wins now but a successful churn model changes the game for the company and the direction of the team. \n\nThe specifics\n\nAs a company all of our customers are on monthly subscriptions. There are basic subscriptions that are rolling monthly and some higher tier packages that come with minimum contract lengths. Each subscription has a fixed membership cost which reduces as you spend more on optional extras. In theory you can pay 0 for your membership if you commit to a larger overall spend on optional extras for a fixed period.\n\nThe products are sold B2B so almost all communication is done through sales people. Customers cannot buy or change products without speaking to a sales person and signing a new contract (sometimes with a minimum time period, sometimes without). Customers can cancel via email at any time (assuming not in contract period) with a 30 day notice period.\n\nThe challenge\n\nWe would like to be able to say, with a reasonable degree of confidence, which customers are most likely to submit a contract cancellation in the next 30 days (so to churn in 60 days, after the 30 day notice period). The team have datasets from all over the business cleaned up &amp; piping into our data platform now and we have the ability and freedom to model the data however we need. This data includes 4 years of history, including examples of when customers have churned previously.\n\nMy questions\n\nFirstly:\n\nWe have loads of trusted data now. How do we narrow this down to the key information that drives a churn prediction?\n\nAnd \n\nIs there a \"dip your toe\" approach to this challenge where we might be able to quickly show some value? e.g. prove there is a strong relationship between churn probability and variable X and Y - as a business we should focus on improving variable X and Y in our OKRs. My thinking is this would help me hire for or prioritise more advance data science work.\n\nSecondly:\n\nWhat are the typical approaches to this type of problem that I could read about and investigate viability? Any practical examples people have as case studies would be great to read. We have a good working knowledge of python &amp; SQL so code examples are also great. \n\nLastly:\n\nAny advice??", "link": "https://www.reddit.com/r/datascience/comments/odgqm9/where_to_start_customer_churn/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "where to start? customer churn /!/ hello,\n\na bit of background: i've worked my way through a number of data-based roles over the past 8 years and i've found myself recently as the manager of a small analytics engineering team. the team is currently a mix of technical analysts (bi -----> tool !!!  / sql experts) and junior data engineers. the company i work for had traditionally placed little value in data engineering and, as a result, i've seen a number of data science projects start and fail very quickly for the same reason: poor data management and availability. \n\ni believe that through some great work in my team we now have all the building blocks needed to start a more sophisticated approach to predicting customer churn. however, thanks to previous failed endeavours, it's proving quite difficult to convince any of the analyst teams to spend time on it, and we lack any \"real\" data science functions in the business to pick this up.\n\nin my view this is a double win for the company if i can get a churn model off the ground: it will firmly embed and prove the value of data engineering as a core function in the business and it will also provide great value on the work already done by my team. in an ideal world i would have had some buy in for this at the start. as a team were having to take little wins in this space in an attempt to win the hearts and minds of senior management. we've had some wins now but a successful churn model changes the game for the company and the direction of the team. \n\nthe specifics\n\nas a company all of our customers are on monthly subscriptions. there are basic subscriptions that are rolling monthly and some higher tier packages that come with minimum contract lengths. each subscription has a fixed membership cost which reduces as you spend more on optional extras. in theory you can pay 0 for your membership if you commit to a larger overall spend on optional extras for a fixed period.\n\nthe products are sold b2b so almost all communication is done through sales people. customers cannot buy or change products without speaking to a sales person and signing a new contract (sometimes with a minimum time period, sometimes without). customers can cancel via email at any time (assuming not in contract period) with a 30 day notice period.\n\nthe challenge\n\nwe would like to be able to say, with a reasonable degree of confidence, which customers are most likely to submit a contract cancellation in the next 30 days (so to churn in 60 days, after the 30 day notice period). the team have datasets from all over the business cleaned up &amp; piping into our data platform now and we have the ability and freedom to model the data however we need. this data includes 4 years of history, including examples of when customers have churned previously.\n\nmy questions\n\nfirstly:\n\nwe have loads of trusted data now. how do we narrow this down to the key information that drives a churn prediction?\n\nand \n\nis there a \"dip your toe\" approach to this challenge where we might be able to quickly show some value? e.g. prove there is a strong relationship between churn probability and variable x and y - as a business we should focus on improving variable x and y in our okrs. my thinking is this would help me hire for or prioritise more advance data science work.\n\nsecondly:\n\nwhat are the typical approaches to this type of problem that i could read about and investigate viability? any practical examples people have as case studies would be great to read. we have a good working knowledge of python &amp; sql so code examples are also great. \n\nlastly:\n\nany advice??", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 5, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/odgqm9/where_to_start_customer_churn/',)", "identifyer": 5594820, "year": "2021"}, {"autor": "JB__Quix", "date": 1621322786000, "content": "Does Netflix use Jupyter Notebooks in production? /!/ I love Jupyter Notebooks but never thought of them as a tool to put code into production. \n\nSo I was very surprised by this article [Beyond Interactive: Notebook Innovation at Netflix](https://netflixtechblog.com/notebook-innovation-591ee3221233) (found thanks to [u/yoursdata](https://www.reddit.com/user/yoursdata/)'s [recent post](https://www.reddit.com/r/datascience/comments/neylas/data_science_in_practice/) about what it seems a very interesting [newsletter](https://datascienceinpractice.substack.com/p/data-science-in-practice-post-1)).\n\nThis is a 2018 article, anyone can confirm whether this philosophy continues at Netflix? Any other companies out there doing this?", "link": "https://www.reddit.com/r/datascience/comments/nf47se/does_netflix_use_jupyter_notebooks_in_production/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "does netflix use jupyter notebooks in production? /!/ i love jupyter notebooks but never thought of them as a -----> tool !!!  to put code into production. \n\nso i was very surprised by this article [beyond interactive: notebook innovation at netflix](https://netflixtechblog.com/notebook-innovation-591ee3221233) (found thanks to [u/yoursdata](https://www.reddit.com/user/yoursdata/)'s [recent post](https://www.reddit.com/r/datascience/comments/neylas/data_science_in_practice/) about what it seems a very interesting [newsletter](https://datascienceinpractice.substack.com/p/data-science-in-practice-post-1)).\n\nthis is a 2018 article, anyone can confirm whether this philosophy continues at netflix? any other companies out there doing this?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 51, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nf47se/does_netflix_use_jupyter_notebooks_in_production/',)", "identifyer": 5594875, "year": "2021"}, {"autor": "Turbulent_Inside_256", "date": 1628778167000, "content": "Should an MD learn programming and study data science? /!/ Hello! I just got my degree and I was always fascinated by programming. My interests are in data science, natural language processing, machine learning and bioinformatics. \nMany people dissuade me from programming and tell my it's a waste of time. That I should instead do a masters on drugs, neuroscience or a psychotherapy.\nI want to be a psychiatrist and soon I will begin my  residency. Personally I see programming as a good tool that will help psychiatry to go forward paired with neuroscience and I want to be a part of that. I also believe that the future is on teams with many specialists working together and they should understand each other.\nWhat are your thoughts?", "link": "https://www.reddit.com/r/datascience/comments/p30upk/should_an_md_learn_programming_and_study_data/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "should an md learn programming and study data science? /!/ hello! i just got my degree and i was always fascinated by programming. my interests are in data science, natural language processing, machine learning and bioinformatics. \nmany people dissuade me from programming and tell my it's a waste of time. that i should instead do a masters on drugs, neuroscience or a psychotherapy.\ni want to be a psychiatrist and soon i will begin my  residency. personally i see programming as a good -----> tool !!!  that will help psychiatry to go forward paired with neuroscience and i want to be a part of that. i also believe that the future is on teams with many specialists working together and they should understand each other.\nwhat are your thoughts?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 8, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/p30upk/should_an_md_learn_programming_and_study_data/',)", "identifyer": 5594962, "year": "2021"}, {"autor": "sylwiabr", "date": 1628777532000, "content": "How we should improve Enso so it will become the most powerful ETL and data analytics tool for you?", "link": "https://www.reddit.com/r/datascience/comments/p30ni6/how_we_should_improve_enso_so_it_will_become_the/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "how we should improve enso so it will become the most powerful etl and data analytics -----> tool !!!  for you?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('rich:video',)", "medialink": "('https://youtu.be/77kEES-_wX8',)", "identifyer": 5594965, "year": "2021"}, {"autor": "ThatPhysics2213Kid", "date": 1610263238000, "content": "How do I create a data collection and visual tool? /!/ Just for starters, I\u2019m a beginner at this data but intermediate at programming. To be specific, I want to create a data and visualization tool, preferably a web app that can continually update as users enter survey data. I\u2019m wondering how would I go about this? I want to use python + Wordpress and/or CSS to add these features. I couldn\u2019t quite find any tutorial that has done this. Thanks in advance.", "link": "https://www.reddit.com/r/datascience/comments/ku9wqu/how_do_i_create_a_data_collection_and_visual_tool/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "how do i create a data collection and visual -----> tool !!! ? /!/ just for starters, i\u2019m a beginner at this data but intermediate at programming. to be specific, i want to create a data and visualization tool, preferably a web app that can continually update as users enter survey data. i\u2019m wondering how would i go about this? i want to use python + wordpress and/or css to add these features. i couldn\u2019t quite find any tutorial that has done this. thanks in advance.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/ku9wqu/how_do_i_create_a_data_collection_and_visual_tool/',)", "identifyer": 5595134, "year": "2021"}, {"autor": "Distinct_Scallion850", "date": 1610262100000, "content": "What is your least popular data science opinion? /!/ On the field, a tool, coding practices, career development, etc. Curious to know the stuff they probably won't tell me!", "link": "https://www.reddit.com/r/datascience/comments/ku9nmm/what_is_your_least_popular_data_science_opinion/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what is your least popular data science opinion? /!/ on the field, a -----> tool !!! , coding practices, career development, etc. curious to know the stuff they probably won't tell me!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/ku9nmm/what_is_your_least_popular_data_science_opinion/',)", "identifyer": 5595135, "year": "2021"}, {"autor": "ThrowRAPrisonMike", "date": 1621976582000, "content": "Which tool should I spend time learning? /!/ This might be data analytics instead of data science but, I\u2019ve split time between learning Power BI and Tableau. I\u2019m leaning more towards Tableau, but I keep learning Power BI because I want to keep my options open. I\u2019m learning/relearning SQL and Excel as well and I know I need these definitely. I\u2019m just not sure about the Power BI or Tableau since they\u2019re both visual tools. Should I focus on one vs the other or keep learning both to keep my options open? I worry that learning both might slow me down when looking for a job in the near term or it could open up more jobs with some more time.", "link": "https://www.reddit.com/r/datascience/comments/nkzy9b/which_tool_should_i_spend_time_learning/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "which -----> tool !!!  should i spend time learning? /!/ this might be data analytics instead of data science but, i\u2019ve split time between learning power bi and tableau. i\u2019m leaning more towards tableau, but i keep learning power bi because i want to keep my options open. i\u2019m learning/relearning sql and excel as well and i know i need these definitely. i\u2019m just not sure about the power bi or tableau since they\u2019re both visual tools. should i focus on one vs the other or keep learning both to keep my options open? i worry that learning both might slow me down when looking for a job in the near term or it could open up more jobs with some more time.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nkzy9b/which_tool_should_i_spend_time_learning/',)", "identifyer": 5595161, "year": "2021"}, {"autor": "maxmax007", "date": 1621810174000, "content": "Advice request | Simple tag-based recommender system /!/ Hi everyone. My name is Max and I want to build a simple recommender system for my app. But I'm very new to this field so not sure how to start. Any comments are warmly welcome.\n\n**Context**. I'm part of a nonprofit project to build an app that's aimed to help people living nearby find friends-by-interests and kickoff offline communication. We're in a very early stage so unfortunately I don't have any links/texts/images to share with you except the description below.\n\nHere's what we want to achieve:\n\n* Users can add as many \\*interest tags\\* as they want where interest tag is defined as a short English phrase like \"computer science\", \"psychology\", \"Harry Potter\", \"chatbots\", \"crypto\", \"LoL\", \"Mayakovsky\" etc.\n* The system can calculate \\*similarity\\* between two users and explain why the users are considered as similar.\n\nThe simplest approach would probably be to just provide a hardcoded list of topics. Then we can just count the number of common tags for two users (applying some normalization since users can provide drastically different numbers of tags), then apply some thresholding and if the similarity is higher than a threshold we can provide common tags as the explanation for similarity.\n\nBut this approach has a big drawback: users are forced into tight limits of a small predefined set of tags. So we decided to try another approach: apply some pre-trained word embeddings to convert tags into vectors, then aggregate all the tags into a vector representation of a user and apply cosine similarity. The question is: what's a good procedure for such aggregation?\n\nWe chose fasttext as a tool to get vector representations. And it has a convenient \\`get\\_sentence\\_vector\\` method to get vectors for individual tags even when they are phrases rather than words (\"computer scince\", \"web development\", \"classic music\" etc.) but I'm not sure how to proceed then. Should we just normalize and average the resulting vectors or there are some better approaches that I didn't manage to find on Google? Maybe there is a different vectorization tool that can better handle our task?  \n\n\nThanks in advance.", "link": "https://www.reddit.com/r/datascience/comments/njjf3e/advice_request_simple_tagbased_recommender_system/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "advice request | simple tag-based recommender system /!/ hi everyone. my name is max and i want to build a simple recommender system for my app. but i'm very new to this field so not sure how to start. any comments are warmly welcome.\n\n**context**. i'm part of a nonprofit project to build an app that's aimed to help people living nearby find friends-by-interests and kickoff offline communication. we're in a very early stage so unfortunately i don't have any links/texts/images to share with you except the description below.\n\nhere's what we want to achieve:\n\n* users can add as many \\*interest tags\\* as they want where interest tag is defined as a short english phrase like \"computer science\", \"psychology\", \"harry potter\", \"chatbots\", \"crypto\", \"lol\", \"mayakovsky\" etc.\n* the system can calculate \\*similarity\\* between two users and explain why the users are considered as similar.\n\nthe simplest approach would probably be to just provide a hardcoded list of topics. then we can just count the number of common tags for two users (applying some normalization since users can provide drastically different numbers of tags), then apply some thresholding and if the similarity is higher than a threshold we can provide common tags as the explanation for similarity.\n\nbut this approach has a big drawback: users are forced into tight limits of a small predefined set of tags. so we decided to try another approach: apply some pre-trained word embeddings to convert tags into vectors, then aggregate all the tags into a vector representation of a user and apply cosine similarity. the question is: what's a good procedure for such aggregation?\n\nwe chose fasttext as a -----> tool !!!  to get vector representations. and it has a convenient \\`get\\_sentence\\_vector\\` method to get vectors for individual tags even when they are phrases rather than words (\"computer scince\", \"web development\", \"classic music\" etc.) but i'm not sure how to proceed then. should we just normalize and average the resulting vectors or there are some better approaches that i didn't manage to find on google? maybe there is a different vectorization tool that can better handle our task?  \n\n\nthanks in advance.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/njjf3e/advice_request_simple_tagbased_recommender_system/',)", "identifyer": 5595253, "year": "2021"}, {"autor": "castor-metadata", "date": 1629470451000, "content": "What is the best data quality tool? /!/ Modern organizations are producing, collecting, processing data more than ever before. An [IDG](https://www.matillion.com/resources/blog/matillion-and-idg-survey-data-growth-is-real-and-3-other-key-findings) survey of data professionals reveals that data volumes are growing at an average rate of 63% per month.   \n\n\nAs data proliferates in businesses, the technologies we use to move this data around have become more intricate and complex, to the point that we completely lose visibility on how data is processed. As a result, mistakes accumulate as data is moved around, and we end up with crap, unusable data.   \n\n\nThankfully, **data** **observability** tools have flourished in the past few years, helping companies regain control over data processing. \n\nToday, I wanted to share an analysis on the data quality landscape :  \n[https://towardsdatascience.com/data-monitoring-and-observability-benchmark-for-mid-market-companies-154155f41dcb](https://towardsdatascience.com/data-monitoring-and-observability-benchmark-for-mid-market-companies-154155f41dcb)  \n\n\nAnd get your take on the few tools out there. Tell me more in the comments.\n\n[View Poll](https://www.reddit.com/poll/p86kk7)", "link": "https://www.reddit.com/r/datascience/comments/p86kk7/what_is_the_best_data_quality_tool/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what is the best data quality -----> tool !!! ? /!/ modern organizations are producing, collecting, processing data more than ever before. an [idg](https://www.matillion.com/resources/blog/matillion-and-idg-survey-data-growth-is-real-and-3-other-key-findings) survey of data professionals reveals that data volumes are growing at an average rate of 63% per month.   \n\n\nas data proliferates in businesses, the technologies we use to move this data around have become more intricate and complex, to the point that we completely lose visibility on how data is processed. as a result, mistakes accumulate as data is moved around, and we end up with crap, unusable data.   \n\n\nthankfully, **data** **observability** tools have flourished in the past few years, helping companies regain control over data processing. \n\ntoday, i wanted to share an analysis on the data quality landscape :  \n[https://towardsdatascience.com/data-monitoring-and-observability-benchmark-for-mid-market-companies-154155f41dcb](https://towardsdatascience.com/data-monitoring-and-observability-benchmark-for-mid-market-companies-154155f41dcb)  \n\n\nand get your take on the few tools out there. tell me more in the comments.\n\n[view poll](https://www.reddit.com/poll/p86kk7)", "sortedWord": "None", "removed": "('nan',)", "score": 0, "comments": 5, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/p86kk7/what_is_the_best_data_quality_tool/',)", "identifyer": 5595282, "year": "2021"}, {"autor": "SliceBubbly6877", "date": 1629436561000, "content": "Amazon Data Scraping Tool | Scrape Amazon Product Ratings &amp; Reviews Tool /!/ [removed]", "link": "https://www.reddit.com/r/datascience/comments/p7vp0i/amazon_data_scraping_tool_scrape_amazon_product/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "amazon data scraping -----> tool !!!  | scrape amazon product ratings &amp; reviews -----> tool !!!  /!/ [removed]", "sortedWord": "None", "removed": "('reddit',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/p7vp0i/amazon_data_scraping_tool_scrape_amazon_product/',)", "identifyer": 5595293, "year": "2021"}, {"autor": "techsucker", "date": 1629392962000, "content": "Google Open-Sources Its Data Validation Tool (DVT), A Python CLI Tool That Provides An Automated And Repeatable Solution For Validation Across Different Environments /!/ Machine learning has been possible partly due to the accumulation of data, and within that data, an important step is that of data validation. May it be a data warehouse, database, or data lake migration, all require data validations. It mainly encompasses comparing the structured and the semi-structured data right from the source to the target and subsequently verifying that they match correctly after every step in the process.\n\n**The Data Validation Tool by Google**\n\nLooking at the importance of data validation, Google recently released the [Data Validation Tool](https://github.com/GoogleCloudPlatform/professional-services-data-validator)\u00a0(DVT). This tool will primarily function as an open-sourced Python CLI tool that would provide an automated and repeatable solution for the process of data validation. The researchers have claimed that this tool would work in different environments with brilliant accuracy. The framework that was equipped for this tool is the [Ibis](https://ibis-project.org/docs/tutorial/01-Introduction-to-Ibis.html). This would act as an intermediary link between the numerous data sources like BigQuery, Cloud Spanner, and so forth.\n\n[3 Min Read](https://www.marktechpost.com/2021/08/19/google-open-sources-its-data-validation-tool-dvt-a-python-cli-tool-that-provides-an-automated-and-repeatable-solution-for-validation-across-different-environments/) | [Github](https://github.com/GoogleCloudPlatform/professional-services-data-validator) \n\n&amp;#x200B;\n\n*Processing img mohzuwmakci71...*", "link": "https://www.reddit.com/r/datascience/comments/p7j2h2/google_opensources_its_data_validation_tool_dvt_a/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "google open-sources its data validation -----> tool !!!  (dvt), a python cli -----> tool !!!  that provides an automated and repeatable solution for validation across different environments /!/ machine learning has been possible partly due to the accumulation of data, and within that data, an important step is that of data validation. may it be a data warehouse, database, or data lake migration, all require data validations. it mainly encompasses comparing the structured and the semi-structured data right from the source to the target and subsequently verifying that they match correctly after every step in the process.\n\n**the data validation tool by google**\n\nlooking at the importance of data validation, google recently released the [data validation tool](https://github.com/googlecloudplatform/professional-services-data-validator)\u00a0(dvt). this tool will primarily function as an open-sourced python cli tool that would provide an automated and repeatable solution for the process of data validation. the researchers have claimed that this tool would work in different environments with brilliant accuracy. the framework that was equipped for this tool is the [ibis](https://ibis-project.org/docs/tutorial/01-introduction-to-ibis.html). this would act as an intermediary link between the numerous data sources like bigquery, cloud spanner, and so forth.\n\n[3 min read](https://www.marktechpost.com/2021/08/19/google-open-sources-its-data-validation-tool-dvt-a-python-cli-tool-that-provides-an-automated-and-repeatable-solution-for-validation-across-different-environments/) | [github](https://github.com/googlecloudplatform/professional-services-data-validator) \n\n&amp;#x200b;\n\n*processing img mohzuwmakci71...*", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/p7j2h2/google_opensources_its_data_validation_tool_dvt_a/',)", "identifyer": 5595313, "year": "2021"}, {"autor": "yoi12321", "date": 1629388865000, "content": "The Key Word in Data Science is Science, not Data /!/ I know reddit doesn't represent real life, but just look at the titles of this sub. They're all about tools, code languages/packages, and algorithms. I think to most aspiring data scientists, that's how they see the profession. You're given a tech stack, some data, and your goal is to apply x tool/algorithm to y data. My argument is this is only going to work at super junior levels, and I believe it's the reason why there's a huge oversupply of junior data scientists but teams still can't find competent seniors.  \n\n\nAs another experiment, just head over to r/dataisbeautiful right now. You'll see a ton of different techs used to generate some decent and some awful visualizations. All of those people were able to access, clean, and plot data. There's no shortage of people who can do that. But what you'll notice if you read that sub, is there's a huge lack of people thinking critically about the data they're working with, and that's the science aspect.\n\n&amp;#x200B;\n\nI feel like every week there's a new topic here on how long until data scientists are obsolete. I don't think data scientists are getting less valuable, but people who can just use tool x to leverage data y are. Why would I hire a senior data scientist to create a dashboard when I can teach an intern tableau and get 95% of the same thing? Whether it's recognizing Simpson's paradox, knowing when to keep/stop digging into research questions, figuring out when gathering more data is necessary, knowing how to communicate findings in ways that make an impact, the science part of data science is by far the most valuable. Some people call them soft skills, but I'm not a huge fan of the term. It's science. Unfortunately these are the toughest skills to learn and also the toughest skills to interview for, so I don't suspect you'll see companies steering away from technical questions in interviews any time soon. But mastering the science aspect of data science is I believe the best way to make yourself extremely valuable.", "link": "https://www.reddit.com/r/datascience/comments/p7hpd9/the_key_word_in_data_science_is_science_not_data/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "the key word in data science is science, not data /!/ i know reddit doesn't represent real life, but just look at the titles of this sub. they're all about tools, code languages/packages, and algorithms. i think to most aspiring data scientists, that's how they see the profession. you're given a tech stack, some data, and your goal is to apply x -----> tool !!! /algorithm to y data. my argument is this is only going to work at super junior levels, and i believe it's the reason why there's a huge oversupply of junior data scientists but teams still can't find competent seniors.  \n\n\nas another experiment, just head over to r/dataisbeautiful right now. you'll see a ton of different techs used to generate some decent and some awful visualizations. all of those people were able to access, clean, and plot data. there's no shortage of people who can do that. but what you'll notice if you read that sub, is there's a huge lack of people thinking critically about the data they're working with, and that's the science aspect.\n\n&amp;#x200b;\n\ni feel like every week there's a new topic here on how long until data scientists are obsolete. i don't think data scientists are getting less valuable, but people who can just use tool x to leverage data y are. why would i hire a senior data scientist to create a dashboard when i can teach an intern tableau and get 95% of the same thing? whether it's recognizing simpson's paradox, knowing when to keep/stop digging into research questions, figuring out when gathering more data is necessary, knowing how to communicate findings in ways that make an impact, the science part of data science is by far the most valuable. some people call them soft skills, but i'm not a huge fan of the term. it's science. unfortunately these are the toughest skills to learn and also the toughest skills to interview for, so i don't suspect you'll see companies steering away from technical questions in interviews any time soon. but mastering the science aspect of data science is i believe the best way to make yourself extremely valuable.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 167, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/p7hpd9/the_key_word_in_data_science_is_science_not_data/',)", "identifyer": 5595321, "year": "2021"}, {"autor": "thirtyoneone", "date": 1629371978000, "content": "Hello reddit, what time series forecasting tools are you using? /!/ Hi, \n\nAs the title says I am looking for time series forecasting tool. So far i have used fbProphet and arima with mixed results and was wondering if there is something better out there.  \n\n\nThanks", "link": "https://www.reddit.com/r/datascience/comments/p7cv51/hello_reddit_what_time_series_forecasting_tools/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "hello reddit, what time series forecasting tools are you using? /!/ hi, \n\nas the title says i am looking for time series forecasting -----> tool !!! . so far i have used fbprophet and arima with mixed results and was wondering if there is something better out there.  \n\n\nthanks", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 55, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/p7cv51/hello_reddit_what_time_series_forecasting_tools/',)", "identifyer": 5595330, "year": "2021"}, {"autor": "Stunning_Pace", "date": 1629369020000, "content": "Implementing a tool or use an existing one? /!/ Hello there,\n\n&amp;#x200B;\n\nI have a bunch of JSON data which I want to visualize. The is essentially the security scan results from a scanner and I want to have a nice visualizer. Should I implement it myself or is there any existing one?", "link": "https://www.reddit.com/r/datascience/comments/p7c8f8/implementing_a_tool_or_use_an_existing_one/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "implementing a -----> tool !!!  or use an existing one? /!/ hello there,\n\n&amp;#x200b;\n\ni have a bunch of json data which i want to visualize. the is essentially the security scan results from a scanner and i want to have a nice visualizer. should i implement it myself or is there any existing one?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/p7c8f8/implementing_a_tool_or_use_an_existing_one/',)", "identifyer": 5595331, "year": "2021"}, {"autor": "lschozar", "date": 1628284455000, "content": "python as a data analyst instead of e.g. sql /!/ Hey everybody!\n\nI am working as a data analyst however my work actually consists of:\n- talk/work out requirements of customers to design a data model\n- create an ETL process using databases and/or excel as sources. This is done via the etl tool of my choice &amp; depending on the data low to medium amount of SQL (joins, CTEs, Basic SQL stuff)\n- creating the frontend\n\nSo basically I do everything, however I don't have to work with many different databases so I am quite unskilled in this area? I have a fairly good understanding of SQL and now I tried to dabble with python for data cleaning and NLP. However, what irks me about python is that it seems Soo complicated compared to SQL for data cleaning?\nSo my question is, should a data analyst learn python and if so what would be generic uses cases? Would you say it is advantageous to clean data with python? Or just more versatile?\n\nCheeers\n-", "link": "https://www.reddit.com/r/datascience/comments/ozfqir/python_as_a_data_analyst_instead_of_eg_sql/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "python as a data analyst instead of e.g. sql /!/ hey everybody!\n\ni am working as a data analyst however my work actually consists of:\n- talk/work out requirements of customers to design a data model\n- create an etl process using databases and/or excel as sources. this is done via the etl -----> tool !!!  of my choice &amp; depending on the data low to medium amount of sql (joins, ctes, basic sql stuff)\n- creating the frontend\n\nso basically i do everything, however i don't have to work with many different databases so i am quite unskilled in this area? i have a fairly good understanding of sql and now i tried to dabble with python for data cleaning and nlp. however, what irks me about python is that it seems soo complicated compared to sql for data cleaning?\nso my question is, should a data analyst learn python and if so what would be generic uses cases? would you say it is advantageous to clean data with python? or just more versatile?\n\ncheeers\n-", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/ozfqir/python_as_a_data_analyst_instead_of_eg_sql/',)", "identifyer": 5595364, "year": "2021"}, {"autor": "Key-Desk-1211", "date": 1628189460000, "content": "Mapping job titles to an org structure using ML? /!/ I've created a software tool that attempts to reconstruct org charts based on LinkedIn job title datasets. One of the complexities that I am struggling with the is the very subjective nature of job titles in the English language - e.g., \"Developer\" can mean very different things depending on Real Estate or Software context. Has anyone come across any clever work in this space or seen any libraries that I can repurpose?\n\nHere is the link to the [Reddit org chart](https://listalpha-dev.herokuapp.com/share/p9e4fpXFM5RMx72xlMmDuyfG) for reference.\n\nMuch appreciate any help!", "link": "https://www.reddit.com/r/datascience/comments/oyp0tk/mapping_job_titles_to_an_org_structure_using_ml/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "mapping job titles to an org structure using ml? /!/ i've created a software -----> tool !!!  that attempts to reconstruct org charts based on linkedin job title datasets. one of the complexities that i am struggling with the is the very subjective nature of job titles in the english language - e.g., \"developer\" can mean very different things depending on real estate or software context. has anyone come across any clever work in this space or seen any libraries that i can repurpose?\n\nhere is the link to the [reddit org chart](https://listalpha-dev.herokuapp.com/share/p9e4fpxfm5rmx72xlmmduyfg) for reference.\n\nmuch appreciate any help!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/oyp0tk/mapping_job_titles_to_an_org_structure_using_ml/',)", "identifyer": 5595406, "year": "2021"}, {"autor": "mctavish_", "date": 1628141692000, "content": "Power Automate for ML workflows? /!/ I'm working for a new team that basically has no software engineers. The team is trying hard to build a data set for ML-related work related to their domain knowledge, which is very deep. Generating the data requires repeatedly going through a slow workflow that involves some manualy copy/pasting files across share drive and SharePoint, executing some custom scripts (python, MATLAB, C#) and humans labelling images for supervised learning.\n\nTo give you an idea of size, the team has ~5 people who spend significant time labelling images. It is a decent project.\n\nRight now the team is struggling with a few things:\n- data is poorly organised on a share drive and SharePoint\n- the team lead isn't sure if some cycles of the workflow have only been partially executed...I.e. that some meaningful portion of the work has stalled mid-stream\n- team lead is worried things are being deleted accidentally with manually handling files\n- there is poor tracking on which inputs were used to generate which outputs\n- the time to go thru the workflow is too slow (days not minutes)\n\nI'm thinking about organising and tracking the data using a SharePoint list. That'll add some transparency to workflow progress and which inputs were used to generate results.\n\nI'm also thinking about automating as much of the work as possible using Microsift Power Automate (MSPA). For example, I think I can use MSPA to run a python script. I think it can do all of the copy/pasting required too. That should put an end to the vast majority of the manual file handling, and keep the files in an IT-supported space so we have reliable backups.\n\nI've not used MSPA to run python scripts before. Has anyone here done that? I'm the kind of guy who'd program all of this in Python and use github to share it across the team. But my team doesn't have the software skills to do that kind of thing. And I really don't want to become, essentially, IT support.\n\nHas anyone done something similar? I can't be the first to run into this kind of challenge. Maybe you used a different stack/tool to solve these issues? I'd like to build something that can, ideally, be maintained by a non-developer so I can focus on the actual ML work lol. \n\nI'm all ears!!", "link": "https://www.reddit.com/r/datascience/comments/oyb291/power_automate_for_ml_workflows/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "power automate for ml workflows? /!/ i'm working for a new team that basically has no software engineers. the team is trying hard to build a data set for ml-related work related to their domain knowledge, which is very deep. generating the data requires repeatedly going through a slow workflow that involves some manualy copy/pasting files across share drive and sharepoint, executing some custom scripts (python, matlab, c#) and humans labelling images for supervised learning.\n\nto give you an idea of size, the team has ~5 people who spend significant time labelling images. it is a decent project.\n\nright now the team is struggling with a few things:\n- data is poorly organised on a share drive and sharepoint\n- the team lead isn't sure if some cycles of the workflow have only been partially executed...i.e. that some meaningful portion of the work has stalled mid-stream\n- team lead is worried things are being deleted accidentally with manually handling files\n- there is poor tracking on which inputs were used to generate which outputs\n- the time to go thru the workflow is too slow (days not minutes)\n\ni'm thinking about organising and tracking the data using a sharepoint list. that'll add some transparency to workflow progress and which inputs were used to generate results.\n\ni'm also thinking about automating as much of the work as possible using microsift power automate (mspa). for example, i think i can use mspa to run a python script. i think it can do all of the copy/pasting required too. that should put an end to the vast majority of the manual file handling, and keep the files in an it-supported space so we have reliable backups.\n\ni've not used mspa to run python scripts before. has anyone here done that? i'm the kind of guy who'd program all of this in python and use github to share it across the team. but my team doesn't have the software skills to do that kind of thing. and i really don't want to become, essentially, it support.\n\nhas anyone done something similar? i can't be the first to run into this kind of challenge. maybe you used a different stack/-----> tool !!!  to solve these issues? i'd like to build something that can, ideally, be maintained by a non-developer so i can focus on the actual ml work lol. \n\ni'm all ears!!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/oyb291/power_automate_for_ml_workflows/',)", "identifyer": 5595434, "year": "2021"}, {"autor": "thereisnoiinbryan", "date": 1630691653000, "content": "I built a tool to analyze standup comedy performances. It transcribes jokes, and scores them based on laughs. /!/ Tool:  [https://laughtrack.site](https://laughtrack.site)\n\nHow it works: [https://bgnipp.github.io/LaughTrackNotebook/](https://bgnipp.github.io/LaughTrackNotebook/)\n\nTakes a recording of standup performance (via YouTube link or file upload), transcribes the jokes, and scores the laughs by volume and length.  Input a performance by your favorite comic, and see which jokes performed the best (and worst).", "link": "https://www.reddit.com/r/datascience/comments/phaa5e/i_built_a_tool_to_analyze_standup_comedy/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "i built a -----> tool !!!  to analyze standup comedy performances. it transcribes jokes, and scores them based on laughs. /!/ tool:  [https://laughtrack.site](https://laughtrack.site)\n\nhow it works: [https://bgnipp.github.io/laughtracknotebook/](https://bgnipp.github.io/laughtracknotebook/)\n\ntakes a recording of standup performance (via youtube link or file upload), transcribes the jokes, and scores the laughs by volume and length.  input a performance by your favorite comic, and see which jokes performed the best (and worst).", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/phaa5e/i_built_a_tool_to_analyze_standup_comedy/',)", "identifyer": 5595563, "year": "2021"}, {"autor": "castor-metadata", "date": 1630570951000, "content": "What Extract/Load (EL) tool do you use? /!/ I am working on an EL(T) tool benchmark and deep analysis. I am trying to map out the usage of various tools across data teams.\n\n**What ETL/ELT tool do you use?**\n\nYou can find more modern data stack analysis and benchmark [here](https://notion.castordoc.com/):\n\n\\- [Benchmark for data catalogs](https://notion.castordoc.com/catalog-of-catalogs)\n\n\\- [Benchmark for Reverse ETL](https://notion.castordoc.com/catalog-reverse-etl)\n\n\\- [Benchmark for Data Quality](https://notion.castordoc.com/catalog-of-data-quality)\n\n[View Poll](https://www.reddit.com/poll/pgdn2g)", "link": "https://www.reddit.com/r/datascience/comments/pgdn2g/what_extractload_el_tool_do_you_use/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what extract/load (el) -----> tool !!!  do you use? /!/ i am working on an el(t) tool benchmark and deep analysis. i am trying to map out the usage of various tools across data teams.\n\n**what etl/elt tool do you use?**\n\nyou can find more modern data stack analysis and benchmark [here](https://notion.castordoc.com/):\n\n\\- [benchmark for data catalogs](https://notion.castordoc.com/catalog-of-catalogs)\n\n\\- [benchmark for reverse etl](https://notion.castordoc.com/catalog-reverse-etl)\n\n\\- [benchmark for data quality](https://notion.castordoc.com/catalog-of-data-quality)\n\n[view poll](https://www.reddit.com/poll/pgdn2g)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 17, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pgdn2g/what_extractload_el_tool_do_you_use/',)", "identifyer": 5595631, "year": "2021"}, {"autor": "awkwardlynotcool", "date": 1632425457000, "content": "**Beginner: Need help with my data, could be a basic question ! /!/ Hello there, \n\nSo, I am an applied scientist, I work a lot on collecting data, and information. I want to improve my data processing methods, and help automate a lot my work. For example: I attached a pic of  a graph of  a set of data I collected. **What I want ?** I manually look up the normal part of the curve (R\\^2=1), so I draw a line (shown in red) over and over until my R\\^2 gets closer to 1.  Basically, I want to relocate the part where the derivative becomes the most linear ! is there a tool, tip, info you have for me? .. how would I do it?", "link": "https://www.reddit.com/r/datascience/comments/pu2tu9/beginner_need_help_with_my_data_could_be_a_basic/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "**beginner: need help with my data, could be a basic question ! /!/ hello there, \n\nso, i am an applied scientist, i work a lot on collecting data, and information. i want to improve my data processing methods, and help automate a lot my work. for example: i attached a pic of  a graph of  a set of data i collected. **what i want ?** i manually look up the normal part of the curve (r\\^2=1), so i draw a line (shown in red) over and over until my r\\^2 gets closer to 1.  basically, i want to relocate the part where the derivative becomes the most linear ! is there a -----> tool !!! , tip, info you have for me? .. how would i do it?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pu2tu9/beginner_need_help_with_my_data_could_be_a_basic/',)", "identifyer": 5595713, "year": "2021"}, {"autor": "yunghegemony", "date": 1629981099000, "content": "Patera - a writing tool that lets readers interact with data", "link": "https://www.reddit.com/r/datascience/comments/pbyjri/patera_a_writing_tool_that_lets_readers_interact/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "patera - a writing -----> tool !!!  that lets readers interact with data", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('link',)", "medialink": "('https://patera.io',)", "identifyer": 5595777, "year": "2021"}, {"autor": "FuelYourEpic", "date": 1629975866000, "content": "Best data analyst skill/tool? /!/ Hi \ud83d\udc4b\n\nI am new to the field of data and would like to know which skill/tool would be most beneficial to dedicate my time to as a beginner. I am well aware that as a data analyst, many skills/tools are needed. However, as far as bang for your buck, which would be the most beneficial?\n\n[View Poll](https://www.reddit.com/poll/pbx8pb)", "link": "https://www.reddit.com/r/datascience/comments/pbx8pb/best_data_analyst_skilltool/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "best data analyst skill/-----> tool !!! ? /!/ hi \ud83d\udc4b\n\ni am new to the field of data and would like to know which skill/tool would be most beneficial to dedicate my time to as a beginner. i am well aware that as a data analyst, many skills/tools are needed. however, as far as bang for your buck, which would be the most beneficial?\n\n[view poll](https://www.reddit.com/poll/pbx8pb)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 32, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pbx8pb/best_data_analyst_skilltool/',)", "identifyer": 5595780, "year": "2021"}, {"autor": "adebrijj", "date": 1629973161000, "content": "A tool to help data work /!/ Hi r/datascience,\n\nOver the last year I have been building a work and collaboration tool for data &amp; insight teams.\n\nThink Google Forms, Trello, JIRA and Confluence combined. Its for teams to take in requests, build out requirements, collaborate with stakeholders, manage the project and store deliverables / insights all in one place.\n\nWe have some early adopters using the system but getting valuable feedback hasn't proven as easy as we'd like. So...I'm posting on here to see what you guys think? \n\nYou can access the playground version here to have a play around:  [Playground Entry | Brijj](https://brijj.io/playground-entry/) \n\nOr just check out our website here:  [Brijj | Deliver amazing data &amp; insight](https://brijj.io/) \n\nI'd also be super interested in what tools / methods people use to handle:\n\n\\- Taking in requests\n\n\\- Recording and collaborating on requirements\n\n\\- Managing projects\n\n\\- Storing and delivering outputs\n\n\\- Providing data on their own work\n\nWhat do you get for helping me out? If you like just DM me for a personal discount code which will give you access to any of our tiers for free for 3 months and then at half price for a further 9.", "link": "https://www.reddit.com/r/datascience/comments/pbwmjn/a_tool_to_help_data_work/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "a -----> tool !!!  to help data work /!/ hi r/datascience,\n\nover the last year i have been building a work and collaboration tool for data &amp; insight teams.\n\nthink google forms, trello, jira and confluence combined. its for teams to take in requests, build out requirements, collaborate with stakeholders, manage the project and store deliverables / insights all in one place.\n\nwe have some early adopters using the system but getting valuable feedback hasn't proven as easy as we'd like. so...i'm posting on here to see what you guys think? \n\nyou can access the playground version here to have a play around:  [playground entry | brijj](https://brijj.io/playground-entry/) \n\nor just check out our website here:  [brijj | deliver amazing data &amp; insight](https://brijj.io/) \n\ni'd also be super interested in what tools / methods people use to handle:\n\n\\- taking in requests\n\n\\- recording and collaborating on requirements\n\n\\- managing projects\n\n\\- storing and delivering outputs\n\n\\- providing data on their own work\n\nwhat do you get for helping me out? if you like just dm me for a personal discount code which will give you access to any of our tiers for free for 3 months and then at half price for a further 9.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pbwmjn/a_tool_to_help_data_work/',)", "identifyer": 5595781, "year": "2021"}, {"autor": "SliceBubbly6877", "date": 1629954834000, "content": "Amazon Data Scraping Tool | Scrape Amazon Product Ratings &amp; Reviews Tool /!/ [removed]", "link": "https://www.reddit.com/r/datascience/comments/pbsuls/amazon_data_scraping_tool_scrape_amazon_product/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "amazon data scraping -----> tool !!!  | scrape amazon product ratings &amp; reviews -----> tool !!!  /!/ [removed]", "sortedWord": "None", "removed": "('reddit',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pbsuls/amazon_data_scraping_tool_scrape_amazon_product/',)", "identifyer": 5595788, "year": "2021"}, {"autor": "an_tonova", "date": 1629891505000, "content": "Who is using Y42 (former Datos)? What are the pros besides the all-in-one concept? /!/ My manager asked me to learn as much as possible about Y42 + and -, there is not so much info on the internet about functionality and advantages of the tool. \n\nMaybe somebody has had any experience with it or tried their demo.", "link": "https://www.reddit.com/r/datascience/comments/pb9g2z/who_is_using_y42_former_datos_what_are_the_pros/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "who is using y42 (former datos)? what are the pros besides the all-in-one concept? /!/ my manager asked me to learn as much as possible about y42 + and -, there is not so much info on the internet about functionality and advantages of the -----> tool !!! . \n\nmaybe somebody has had any experience with it or tried their demo.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pb9g2z/who_is_using_y42_former_datos_what_are_the_pros/',)", "identifyer": 5595827, "year": "2021"}, {"autor": "xyzzyrz", "date": 1629870442000, "content": "Slick exploratory data analysis tools similar to honeycomb? /!/ [Honeycomb.io](https://Honeycomb.io) is a devops tool, but it has a nice exploratory data analysis tool that could be quite generalizable. This video does a great job showing how it works: [https://www.youtube.com/watch?v=GuIWQ-EF7YE](https://www.youtube.com/watch?v=GuIWQ-EF7YE)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/saq6uqyrzfj71.png?width=1180&amp;format=png&amp;auto=webp&amp;s=3cabc170822da51fcb7a4083c410dfd0a50217e2\n\nA few observations:\n\n* Time series heavy (but not exclusively time series focused)\n* Fast column store backend\n* \"BubbleUp\" feature automatically surfaces top features discriminating selected region vs unselected - I find myself doing this by hand all the time with data diving tasks (clumsily)\n* Inspired by Facebook Scuba - one of the authors also made [https://snorkel.logv.org/](https://snorkel.logv.org/).\n\nI've used various data science tools, from Jupyter + Pandas + SQL to BI tools (Tableau/Looker/PowerBI). Perhaps I didn't get to know my tools well enough, but I haven't seen anything else similar to this. Has anyone else? It could be super handy beyond devops!", "link": "https://www.reddit.com/r/datascience/comments/pb518l/slick_exploratory_data_analysis_tools_similar_to/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "slick exploratory data analysis tools similar to honeycomb? /!/ [honeycomb.io](https://honeycomb.io) is a devops -----> tool !!! , but it has a nice exploratory data analysis -----> tool !!!  that could be quite generalizable. this video does a great job showing how it works: [https://www.youtube.com/watch?v=guiwq-ef7ye](https://www.youtube.com/watch?v=guiwq-ef7ye)\n\n&amp;#x200b;\n\nhttps://preview.redd.it/saq6uqyrzfj71.png?width=1180&amp;format=png&amp;auto=webp&amp;s=3cabc170822da51fcb7a4083c410dfd0a50217e2\n\na few observations:\n\n* time series heavy (but not exclusively time series focused)\n* fast column store backend\n* \"bubbleup\" feature automatically surfaces top features discriminating selected region vs unselected - i find myself doing this by hand all the time with data diving tasks (clumsily)\n* inspired by facebook scuba - one of the authors also made [https://snorkel.logv.org/](https://snorkel.logv.org/).\n\ni've used various data science tools, from jupyter + pandas + sql to bi tools (tableau/looker/powerbi). perhaps i didn't get to know my tools well enough, but i haven't seen anything else similar to this. has anyone else? it could be super handy beyond devops!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pb518l/slick_exploratory_data_analysis_tools_similar_to/',)", "identifyer": 5595834, "year": "2021"}, {"autor": "sparttann", "date": 1609548901000, "content": "Best Data Visualisation Tool /!/ Which do you think is better for data visualisation-PowerBI, Qlik Sense, Tableau or even Plotly Dash? Personally I have used Power BI, Qlik Sense and Plotly Dash. Qlik Sense seems much easier to use than Power BI, and Plotly Dash really allows me to customise my interface easily. Maybe the good thing about PowerBI is the report server? But it\u2019s the loading time in the report server is so slow:( Other than that, I don\u2019t know why would anyone use PowerBI instead...", "link": "https://www.reddit.com/r/datascience/comments/kon927/best_data_visualisation_tool/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "best data visualisation -----> tool !!!  /!/ which do you think is better for data visualisation-powerbi, qlik sense, tableau or even plotly dash? personally i have used power bi, qlik sense and plotly dash. qlik sense seems much easier to use than power bi, and plotly dash really allows me to customise my interface easily. maybe the good thing about powerbi is the report server? but it\u2019s the loading time in the report server is so slow:( other than that, i don\u2019t know why would anyone use powerbi instead...", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 74, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/kon927/best_data_visualisation_tool/',)", "identifyer": 5595864, "year": "2021"}, {"autor": "load_more_commments", "date": 1613962803000, "content": "Best Big Data Visualization Libraries/Tools? 100M+ rows /!/ I've got a mate who wants to visualize industrial data from equipment that produces logs every 15-60 mins (varies depending on activity)...They've got thousands of this and want a tool to visualize this data for quality assurance and trend analysis.\n\n&amp;#x200B;\n\n**Problem**: 6000+ nodes produce millions of rows of data (like 120M) over a 6 month period.\n\n**Current Infrastructure**: They have all data (2 years+) in a DWH relational SQL DB.\n\n**Currently Method**: Load as much data as they can into PowerBI (I think it tops out at 2-3 weeks) for all 6000 nodes. To look at one node for longer periods they need to create a separate report and pull that data. It ends up being quite tedious and the whole process is cumbersome and slow.\n\n&amp;#x200B;\n\nI'm considering using NVIDIA's RAPIDS (GPU either cloud-hosted or they can buy one) to replicate their current PowerBI charts/tables as a first phase.\n\n&amp;#x200B;\n\nI honestly, haven't looked around for other possible tools and before I get started on this side gig, I wanna know what else you guys might recommend. Thanks!", "link": "https://www.reddit.com/r/datascience/comments/lpdhby/best_big_data_visualization_librariestools_100m/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "best big data visualization libraries/tools? 100m+ rows /!/ i've got a mate who wants to visualize industrial data from equipment that produces logs every 15-60 mins (varies depending on activity)...they've got thousands of this and want a -----> tool !!!  to visualize this data for quality assurance and trend analysis.\n\n&amp;#x200b;\n\n**problem**: 6000+ nodes produce millions of rows of data (like 120m) over a 6 month period.\n\n**current infrastructure**: they have all data (2 years+) in a dwh relational sql db.\n\n**currently method**: load as much data as they can into powerbi (i think it tops out at 2-3 weeks) for all 6000 nodes. to look at one node for longer periods they need to create a separate report and pull that data. it ends up being quite tedious and the whole process is cumbersome and slow.\n\n&amp;#x200b;\n\ni'm considering using nvidia's rapids (gpu either cloud-hosted or they can buy one) to replicate their current powerbi charts/tables as a first phase.\n\n&amp;#x200b;\n\ni honestly, haven't looked around for other possible tools and before i get started on this side gig, i wanna know what else you guys might recommend. thanks!", "sortedWord": "None", "removed": "('nan',)", "score": 3, "comments": 19, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/lpdhby/best_big_data_visualization_librariestools_100m/',)", "identifyer": 5595909, "year": "2021"}, {"autor": "Mooglekunom", "date": 1613844085000, "content": "Tableau CRM's Niche? /!/ Hi, all,\n\nI work for an organization with \\~650 employees, no full time data scientists but I'm on the team which helps with many of the BI/modeling/etc projects. We have one department using Tableau and about 6 departments using Power BI. We're going through the process of rolling out Salesforce as our org-wide CRM, and there's a top-down push to consolidate all of our BI efforts into Tableau CRM (formerly Einstein Analytics), as opposed to Tableau or Power BI.\n\nI have a relatively okay understanding of the pros and cons of Tableau vs. Power BI, but I'm having a hard time unpacking how Tableau CRM (again, formerly Einstein Analytics) is supposed to fit into that picture. We're being pitched that it can meet all of our requirements that Tableau and Power BI can, and that it's the right tool both for reporting against our Salesforce data and against our relational databases.\n\nAnyone have thoughts/opinions on the topic? Is Tableau CRM really a legitimate replacement for Tableau / Power BI? Any articles or whitepapers you can recommend me? Like I noted, I'm having difficulty finding content on the subject.\n\nThanks, all!", "link": "https://www.reddit.com/r/datascience/comments/locotc/tableau_crms_niche/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "tableau crm's niche? /!/ hi, all,\n\ni work for an organization with \\~650 employees, no full time data scientists but i'm on the team which helps with many of the bi/modeling/etc projects. we have one department using tableau and about 6 departments using power bi. we're going through the process of rolling out salesforce as our org-wide crm, and there's a top-down push to consolidate all of our bi efforts into tableau crm (formerly einstein analytics), as opposed to tableau or power bi.\n\ni have a relatively okay understanding of the pros and cons of tableau vs. power bi, but i'm having a hard time unpacking how tableau crm (again, formerly einstein analytics) is supposed to fit into that picture. we're being pitched that it can meet all of our requirements that tableau and power bi can, and that it's the right -----> tool !!!  both for reporting against our salesforce data and against our relational databases.\n\nanyone have thoughts/opinions on the topic? is tableau crm really a legitimate replacement for tableau / power bi? any articles or whitepapers you can recommend me? like i noted, i'm having difficulty finding content on the subject.\n\nthanks, all!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/locotc/tableau_crms_niche/',)", "identifyer": 5595969, "year": "2021"}, {"autor": "bouuciks1", "date": 1611181907000, "content": "data pipelines - explain /!/ Hello,\n\ni work as data analyst, however I am applying to some jobs where they ask for data pipelines understanding. I know that it's a connection between the data source and the data lake. However, I tried google cloud platform courses to build my own pipelines, but they use shell language, which I am not familiar with.\n\n1. is there a free, easy-to-use tool which I can use to build simple data pipelines? (nothing crazy, simple csv and sql connection to data lake\n2. how to get knowledgeable with the whole process?\n3. if I build a data pipeline, do I need something else to orchestrate it? Can it run every hour without another tool?\n4. any good, free tutorial?\n5. does each pipeline involve coding? can't i use something like drag and drop? O\\_O\n\n&amp;#x200B;\n\nthx!!", "link": "https://www.reddit.com/r/datascience/comments/l1kawq/data_pipelines_explain/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "data pipelines - explain /!/ hello,\n\ni work as data analyst, however i am applying to some jobs where they ask for data pipelines understanding. i know that it's a connection between the data source and the data lake. however, i tried google cloud platform courses to build my own pipelines, but they use shell language, which i am not familiar with.\n\n1. is there a free, easy-to-use -----> tool !!!  which i can use to build simple data pipelines? (nothing crazy, simple csv and sql connection to data lake\n2. how to get knowledgeable with the whole process?\n3. if i build a data pipeline, do i need something else to orchestrate it? can it run every hour without another tool?\n4. any good, free tutorial?\n5. does each pipeline involve coding? can't i use something like drag and drop? o\\_o\n\n&amp;#x200b;\n\nthx!!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/l1kawq/data_pipelines_explain/',)", "identifyer": 5596000, "year": "2021"}, {"autor": "damjanv1", "date": 1611180737000, "content": "Text Analytics visualisation portal /!/ I can't remember where I saw it but I'm pretty sure it was on this sub, but someone posted an awesome text analytics tool, where you would just dump the corpus in and it would produce a dashboard and a series of insights. Seemed to be free as well. Had numerous tabs to the dashboard.\n\nIf anyone knows what I may be talking about pls list your suggestions, as I forgot to save the post.", "link": "https://www.reddit.com/r/datascience/comments/l1jx6w/text_analytics_visualisation_portal/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "text analytics visualisation portal /!/ i can't remember where i saw it but i'm pretty sure it was on this sub, but someone posted an awesome text analytics -----> tool !!! , where you would just dump the corpus in and it would produce a dashboard and a series of insights. seemed to be free as well. had numerous tabs to the dashboard.\n\nif anyone knows what i may be talking about pls list your suggestions, as i forgot to save the post.", "sortedWord": "None", "removed": "('nan',)", "score": 2, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/l1jx6w/text_analytics_visualisation_portal/',)", "identifyer": 5596001, "year": "2021"}, {"autor": "IllPoem4426", "date": 1611161455000, "content": "Go-to DS bibles? /!/ Hey everyone, just wondering what DS textbook resources you recommend that you keep coming back to over the years and have aged well. Also any programming/tool references you can think of too.\n\nThanks!", "link": "https://www.reddit.com/r/datascience/comments/l1d07l/goto_ds_bibles/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "go-to ds bibles? /!/ hey everyone, just wondering what ds textbook resources you recommend that you keep coming back to over the years and have aged well. also any programming/-----> tool !!!  references you can think of too.\n\nthanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/l1d07l/goto_ds_bibles/',)", "identifyer": 5596022, "year": "2021"}, {"autor": "elbogotazo", "date": 1617353800000, "content": "Discovering column mappings /!/ I have a nice challenge to work in at work and am trying to figure out the approach. We have an internal system that stores transactional data in a tabular form.\n\nWe receive daily files with data from the same domain (transactions + metadata) but the column names are not standardised, and the data fields are not always the exact same (e.g. The amount field may have 3 digits behind the comma, where our system expects 1 digit  or what our system calls \"amount\" might be called \"quantity1\" in the incoming files etc.. )\n\nWe have a manual mapping and transformation defined for each incoming file, but the volume of different formats and sources is ever increasing. Im looking for a way to take any input file and to train a model that predicts for each column what the most likely corresponding column in the target file is.\n\nI've been looking into a few things : using NLP\\spacy to train a model that recognises patterns in the column data. E.g. Numeric + period + comma is likely to correspond to amount. I've also looked at modeling the data and extracting an RDF representation using a open source tool called Karma to see if I can train a model on a network graph. But really struggling to see how to implement this.\n\nIs anyone aware of the formal name of this type of problem and if there are tried and tested approaches\\implementations out there that I could build upon?", "link": "https://www.reddit.com/r/datascience/comments/migd9s/discovering_column_mappings/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "discovering column mappings /!/ i have a nice challenge to work in at work and am trying to figure out the approach. we have an internal system that stores transactional data in a tabular form.\n\nwe receive daily files with data from the same domain (transactions + metadata) but the column names are not standardised, and the data fields are not always the exact same (e.g. the amount field may have 3 digits behind the comma, where our system expects 1 digit  or what our system calls \"amount\" might be called \"quantity1\" in the incoming files etc.. )\n\nwe have a manual mapping and transformation defined for each incoming file, but the volume of different formats and sources is ever increasing. im looking for a way to take any input file and to train a model that predicts for each column what the most likely corresponding column in the target file is.\n\ni've been looking into a few things : using nlp\\spacy to train a model that recognises patterns in the column data. e.g. numeric + period + comma is likely to correspond to amount. i've also looked at modeling the data and extracting an rdf representation using a open source -----> tool !!!  called karma to see if i can train a model on a network graph. but really struggling to see how to implement this.\n\nis anyone aware of the formal name of this type of problem and if there are tried and tested approaches\\implementations out there that i could build upon?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 5, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/migd9s/discovering_column_mappings/',)", "identifyer": 5596073, "year": "2021"}, {"autor": "ezzeddinabdallah", "date": 1613384428000, "content": "[OC] Get Your FREE Cleaning Data eBook /!/ Hello guys,\n\nAs cleaning data takes most of our time in data science tasks\n\nI've created an ebook to make the command line as easy as possible to do that task.\n\nThe ebook includes code snippets using the terminal dealing with lots of data from the COVID Tracking Project, Reddit users, a scientific paper discussing clickbait and non-clickbait article headlines, and more.\n\nUsed some GNU and BSD commands and command-line utilities like csvkit and the fastest tool: xsv. Some benchmark results included as well.\n\nBe one of the first 10 who gets this ebook for free: [How to Clean Data at the Command Line](https://gumroad.com/l/clean-data-cmd/free)\n\nWould love to see your feedback, Thanks!", "link": "https://www.reddit.com/r/datascience/comments/lkavet/oc_get_your_free_cleaning_data_ebook/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[oc] get your free cleaning data ebook /!/ hello guys,\n\nas cleaning data takes most of our time in data science tasks\n\ni've created an ebook to make the command line as easy as possible to do that task.\n\nthe ebook includes code snippets using the terminal dealing with lots of data from the covid tracking project, reddit users, a scientific paper discussing clickbait and non-clickbait article headlines, and more.\n\nused some gnu and bsd commands and command-line utilities like csvkit and the fastest -----> tool !!! : xsv. some benchmark results included as well.\n\nbe one of the first 10 who gets this ebook for free: [how to clean data at the command line](https://gumroad.com/l/clean-data-cmd/free)\n\nwould love to see your feedback, thanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/lkavet/oc_get_your_free_cleaning_data_ebook/',)", "identifyer": 5596088, "year": "2021"}, {"autor": "dakm1609", "date": 1634414537000, "content": "Survey about Multivariate Testing w/ Live Users /!/ Hey! I'm in Computer Science at the University of Toronto. In the \"Business of Software\" course, my team is looking to build a tool to improve the experience of multivariate testing when experiments go out directly to end users. If anyone's up to fill out a survey, it would be much appreciated :) Thanks!\n\n[https://forms.gle/jgMdo7V6TY2WfWGQ8](https://forms.gle/jgMdo7V6TY2WfWGQ8)", "link": "https://www.reddit.com/r/datascience/comments/q9jfmg/survey_about_multivariate_testing_w_live_users/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "survey about multivariate testing w/ live users /!/ hey! i'm in computer science at the university of toronto. in the \"business of software\" course, my team is looking to build a -----> tool !!!  to improve the experience of multivariate testing when experiments go out directly to end users. if anyone's up to fill out a survey, it would be much appreciated :) thanks!\n\n[https://forms.gle/jgmdo7v6ty2wfwgq8](https://forms.gle/jgmdo7v6ty2wfwgq8)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/q9jfmg/survey_about_multivariate_testing_w_live_users/',)", "identifyer": 5596356, "year": "2021"}, {"autor": "mrcet007", "date": 1634398114000, "content": "Which is the best tool for creating continuous training pipelines for MLOPS ? /!/ One key component of MLOPS is continuous training. Which means the end to end training is put in a pipeline which can be triggered, versioned and metadata of the pipeline can be tracked. Thus enabling  retraining of the model without lots of manual efforts.\n\nWhich tool/package is best suited for creating such a training pipeline. I am looking for simple tool with following criteria\n\n1. Using only python (i.e. docker is not mandatory like kubeflow) \n2. Dev have lot of flexibility and is not created for specific tasks like nlp, regression, classification etc. \n3. Little learning curve/new syntaxes but is simple to understand and use\n4. Platform or cloud agnostic\n\nI came across the below and didn't find it very useful due to the mentioned reason\n\n1. Kubeflow - every component of the pipeline need to be in a separate docker\n2. Kedro - require learning lots of new syntax etc", "link": "https://www.reddit.com/r/datascience/comments/q9e7jg/which_is_the_best_tool_for_creating_continuous/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "which is the best -----> tool !!!  for creating continuous training pipelines for mlops ? /!/ one key component of mlops is continuous training. which means the end to end training is put in a pipeline which can be triggered, versioned and metadata of the pipeline can be tracked. thus enabling  retraining of the model without lots of manual efforts.\n\nwhich tool/package is best suited for creating such a training pipeline. i am looking for simple tool with following criteria\n\n1. using only python (i.e. docker is not mandatory like kubeflow) \n2. dev have lot of flexibility and is not created for specific tasks like nlp, regression, classification etc. \n3. little learning curve/new syntaxes but is simple to understand and use\n4. platform or cloud agnostic\n\ni came across the below and didn't find it very useful due to the mentioned reason\n\n1. kubeflow - every component of the pipeline need to be in a separate docker\n2. kedro - require learning lots of new syntax etc", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 5, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/q9e7jg/which_is_the_best_tool_for_creating_continuous/',)", "identifyer": 5596367, "year": "2021"}, {"autor": "Flewizzle", "date": 1618837299000, "content": "Suggestions on projects I could do to bring value to a digital marketing agency to show my unique worth as the only data analyst/ software developer in their company /!/ I would be coming in as a data analyst / software developer with an interest in marketing through running an ecommerce business in my spare time, I will be the first DA/SD in the agency (about 100 people there, big clients, access to lots of data). My contact at the company who I\u2019ve worked with for 9 months has asked me what I envision myself working on if I was to join, ready for an interview later this week.  \n \n\nits an open ended question and to have the ability to decide is great, I\u2019m aware of the trap of technology first approaches instead of problem first approaches so don\u2019t want to fall into that, but I want to be able to justify my DA/SD skills by bringing something to the table that can\u2019t be done in google analytics,   \n \n\nSo far I have created a causal inference modelling tool and a data driven attribution modelling tool for them(done for the company as a university project). I\u2019m currently looking into cohort analysis but can see that that is possible in google analytics. I\u2019m wondering if any DAs / SDs can offer some inspiration on what you are currently doing in work that I could also suggest.   \n \n\nDon\u2019t worry I\u2019m also doing a ton of research myself, just putting as many rods in the water as possible to yield the best results ready for the interview \ud83d\ude0a Any help would be super appreciated!", "link": "https://www.reddit.com/r/datascience/comments/mu03lf/suggestions_on_projects_i_could_do_to_bring_value/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "suggestions on projects i could do to bring value to a digital marketing agency to show my unique worth as the only data analyst/ software developer in their company /!/ i would be coming in as a data analyst / software developer with an interest in marketing through running an ecommerce business in my spare time, i will be the first da/sd in the agency (about 100 people there, big clients, access to lots of data). my contact at the company who i\u2019ve worked with for 9 months has asked me what i envision myself working on if i was to join, ready for an interview later this week.  \n \n\nits an open ended question and to have the ability to decide is great, i\u2019m aware of the trap of technology first approaches instead of problem first approaches so don\u2019t want to fall into that, but i want to be able to justify my da/sd skills by bringing something to the table that can\u2019t be done in google analytics,   \n \n\nso far i have created a causal inference modelling -----> tool !!!  and a data driven attribution modelling -----> tool !!!  for them(done for the company as a university project). i\u2019m currently looking into cohort analysis but can see that that is possible in google analytics. i\u2019m wondering if any das / sds can offer some inspiration on what you are currently doing in work that i could also suggest.   \n \n\ndon\u2019t worry i\u2019m also doing a ton of research myself, just putting as many rods in the water as possible to yield the best results ready for the interview \ud83d\ude0a any help would be super appreciated!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/mu03lf/suggestions_on_projects_i_could_do_to_bring_value/',)", "identifyer": 5596402, "year": "2021"}, {"autor": "xela-sedinnaoi", "date": 1618815168000, "content": "The benefits of training the simplest model you can think of and deploying it to production, as soon as you can. /!/ I\u2019ve had many successes with this approach. With this in mind, I\u2019ve put together an [example](https://www.bodyworkml.com/posts/scikit-learn-meet-production) of how to make this Agile approach to developing machine learning systems a reality, by demonstrating that it takes under 15 minutes to deploy a Scikit-Learn model, using FastAPI with [Bodywork](https://bodywork.readthedocs.io/en/latest/) (an open-source MLOps tool that I have built).\n\nHow does this compare to your experiences? I\u2019d be interested to get people\u2019s thoughts, as my background is largely with structured data.", "link": "https://www.reddit.com/r/datascience/comments/mtuzlt/the_benefits_of_training_the_simplest_model_you/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "the benefits of training the simplest model you can think of and deploying it to production, as soon as you can. /!/ i\u2019ve had many successes with this approach. with this in mind, i\u2019ve put together an [example](https://www.bodyworkml.com/posts/scikit-learn-meet-production) of how to make this agile approach to developing machine learning systems a reality, by demonstrating that it takes under 15 minutes to deploy a scikit-learn model, using fastapi with [bodywork](https://bodywork.readthedocs.io/en/latest/) (an open-source mlops -----> tool !!!  that i have built).\n\nhow does this compare to your experiences? i\u2019d be interested to get people\u2019s thoughts, as my background is largely with structured data.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/mtuzlt/the_benefits_of_training_the_simplest_model_you/',)", "identifyer": 5596410, "year": "2021"}, {"autor": "designer1one", "date": 1618676826000, "content": "*Semantic* Video Search with OpenAI\u2019s CLIP Neural Network /!/ I made a simple tool that lets you search a video \\*semantically\\* with AI. \ud83c\udf9e\ufe0f\ud83d\udd0d\n\n\u2728 Live web app: http://whichframe.com \u2728\n\nExample: Which video frame has a person with sunglasses and earphones?\n\nThe querying is powered by OpenAI\u2019s CLIP neural network for performing \"zero-shot\" image classification and the interface was built with Streamlit.\n\nTry searching with text, image, or text + image and please share your discoveries!\n\n\ud83d\udc47 More examples https://twitter.com/chuanenlin/status/1383411082853683208", "link": "https://www.reddit.com/r/datascience/comments/mstv0z/semantic_video_search_with_openais_clip_neural/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "*semantic* video search with openai\u2019s clip neural network /!/ i made a simple -----> tool !!!  that lets you search a video \\*semantically\\* with ai. \ud83c\udf9e\ufe0f\ud83d\udd0d\n\n\u2728 live web app: http://whichframe.com \u2728\n\nexample: which video frame has a person with sunglasses and earphones?\n\nthe querying is powered by openai\u2019s clip neural network for performing \"zero-shot\" image classification and the interface was built with streamlit.\n\ntry searching with text, image, or text + image and please share your discoveries!\n\n\ud83d\udc47 more examples https://twitter.com/chuanenlin/status/1383411082853683208", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/mstv0z/semantic_video_search_with_openais_clip_neural/',)", "identifyer": 5596464, "year": "2021"}, {"autor": "8BitSkullDev", "date": 1624390632000, "content": "Game development - how to structure data? /!/ Hi! I'm a game developer, and I have a question about how to best structure my data. As a former hammer (accountant), everything always looks like a nail (spreadsheet) to me. I'm wondering if there's a better way.\n\nHere are a few actual examples of the desired output (in Lua format). Apologies if the formatting isn't right - I did my best. These are a few lines describing maps in an RPG:\n\n    M.map_info[hash(\"forest_tree_h1\")] = {script_require = \"maps.map_codes.forest_tree_h1\", display_name=[[]], biome = \"forest\", max_level=10, min_level=1, store_level_cap=false, player_tint_add = vmath.vector4(0,0,0,0), player_tint_subtract = vmath.vector4(0,0,0,0), cloud =vmath.vector4(0.15, 0.20, 0.10, 0), border = true, map_zoom = true, music = {id=\"\", force=false, ambient=\"indoor_empty\", ambient_multiplier=1, rest_time=120}}    \n\n    M.map_info[hash(\"rusty_fortress_h1\")] = {script_require = \"maps.map_codes.rusty_fortress_h1\", display_name=[[]], biome = \"rusty\", max_level=10, min_level=1, store_level_cap=false, player_tint_add = vmath.vector4(0,0,0,0), player_tint_subtract = vmath.vector4(0,0,0,0), cloud =nil, border = true, map_zoom = true, music = {id=\"\", force=false, ambient=\"indoor_empty\", ambient_multiplier=1, rest_time=120}}    \n\n    M.map_info[hash(\"rusty_huts_h1\")] = {script_require = \"maps.map_codes.rusty_huts_h1\", display_name=[[]], biome = \"rusty\", max_level=10, min_level=1, store_level_cap=false, player_tint_add = vmath.vector4(0,0,0,0), player_tint_subtract = vmath.vector4(0,0,0,0), cloud =nil, border = true, map_zoom = true, music = {id=\"\", force=false, ambient=\"indoor_empty\", ambient_multiplier=1, rest_time=120}}    \n\nHere are a few lines from a space game about alien genetics:\n\n    {skill_name = \"LOG-LOG-4\", skill_id = 1089, race_ids = {4,4}, category_hash = hash(\"4-4-\"), texture = hash(\"skill_icons\"), flipbook = hash(\"4-4-\"), description = \"add 97 crew limit\", is_mutation = false, is_variant = true, can_combine = true, show_in_skillcheck = false, texture_string = \"skill_icons\", flipbook_string = \"4-4-\", default_unlocked = false, },    \n    {skill_name = \"LOG-LOG-5\", skill_id = 1090, race_ids = {4,4}, category_hash = hash(\"4-4-\"), texture = hash(\"skill_icons\"), flipbook = hash(\"4-4-\"), description = \"remove 38 damage\", is_mutation = false, is_variant = true, can_combine = true, show_in_skillcheck = false, texture_string = \"skill_icons\", flipbook_string = \"4-4-\", default_unlocked = false, },    \n    {skill_name = \"POW-POW-POW-1\", skill_id = 1091, race_ids = {1,1,1}, category_hash = hash(\"1-1-1-\"), texture = hash(\"skill_icons\"), flipbook = hash(\"1-1-1-\"), description = \"add 21 damage\", is_mutation = false, is_variant = true, can_combine = false, show_in_skillcheck = false, texture_string = \"skill_icons\", flipbook_string = \"1-1-1-\", default_unlocked = true, },\n    {skill_name = \"POW-POW-POW-2\", skill_id = 1092, race_ids = {1,1,1}, category_hash = hash(\"1-1-1-\"), texture = hash(\"skill_icons\"), flipbook = hash(\"1-1-1-\"), description = \"add 68 crew limit\", is_mutation = false, is_variant = true, can_combine = false, show_in_skillcheck = false, texture_string = \"skill_icons\", flipbook_string = \"1-1-1-\", default_unlocked = false, },    \n\nThis ends up being sometimes hundreds of lines of data, with a lot of stuff (strings, numbers, tables in tables, etc). Since the data is live during development, I end up wanting to change it a lot, add and remove things, etc. It obviously gets unwieldy doing it right in the editor, editing hundreds of lines. Previously I\u2019ve made Google Sheets to handle it. I love spreadsheets but even I will admit this is cumbersome:\n\n    =\"M.map_info[hash(\"\"\"&amp;B3&amp;\"\"\")] = {script_require = \"\"maps.map_codes.\"&amp;B3&amp;\"\"\", display_name=[[\"&amp;E3&amp;\"]], biome = \"\"\"&amp;C3&amp;\"\"\", max_level=\"&amp;P3&amp;\", min_level=\"&amp;Q3&amp;\", store_level_cap=\"&amp;R3&amp;\", player_tint_add = vmath.vector4(\"&amp;S3&amp;\",\"&amp;T3&amp;\",\"&amp;U3&amp;\",\"&amp;V3&amp;\"), player_tint_subtract = vmath.vector4(\"&amp;W3&amp;\",\"&amp;X3&amp;\",\"&amp;Y3&amp;\",\"&amp;Z3&amp;\"), cloud =\"&amp;M3&amp;\", border = \"&amp;N3&amp;\", map_zoom = \"&amp;O3&amp;\", music = {id=\"\"\"&amp;H3&amp;\"\"\", force=\"&amp;I3&amp;\", ambient=\"\"\"&amp;K3&amp;\"\"\", ambient_multiplier=\"&amp;L3&amp;\", rest_time=\"&amp;J3&amp;\"}}\"    \n\nGenerally speaking, tables might look something like this:    \n{a_number = 2, a_string = \"string here\", subtable = {3}},    \n{a_number = 4, a_string = \"another_string\", subtable = {}},    \n{a_number = 8, a_string = \"yet_another_string\", subtable = {3,5,7,11,13,17,19}},    \n\nThe main stumbling block ends up being the subtables, which can have none, one, or multiple values. I haven't figured out a good way to handle this in a spreadsheet. I can either begin entering comma-delineated values manually (this is bad because it\u2019s manual, makes it hard to keep track of the data, and eventually begins resembling the reason I implemented the spreadsheet in the first place), or I have x number of columns for each individual data point in the subtable (this is bad because it is cumbersome to set up and implies a maximum number of entries in a subtable).    \n\nDoes this make sense? Is there a better way to handle this? Some kind of tool (free is great, but I'll pay to make my life easier)?\n\nThank you in advance, and please do let me know if anything needs to be clarified.", "link": "https://www.reddit.com/r/datascience/comments/o5vbsg/game_development_how_to_structure_data/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "game development - how to structure data? /!/ hi! i'm a game developer, and i have a question about how to best structure my data. as a former hammer (accountant), everything always looks like a nail (spreadsheet) to me. i'm wondering if there's a better way.\n\nhere are a few actual examples of the desired output (in lua format). apologies if the formatting isn't right - i did my best. these are a few lines describing maps in an rpg:\n\n    m.map_info[hash(\"forest_tree_h1\")] = {script_require = \"maps.map_codes.forest_tree_h1\", display_name=[[]], biome = \"forest\", max_level=10, min_level=1, store_level_cap=false, player_tint_add = vmath.vector4(0,0,0,0), player_tint_subtract = vmath.vector4(0,0,0,0), cloud =vmath.vector4(0.15, 0.20, 0.10, 0), border = true, map_zoom = true, music = {id=\"\", force=false, ambient=\"indoor_empty\", ambient_multiplier=1, rest_time=120}}    \n\n    m.map_info[hash(\"rusty_fortress_h1\")] = {script_require = \"maps.map_codes.rusty_fortress_h1\", display_name=[[]], biome = \"rusty\", max_level=10, min_level=1, store_level_cap=false, player_tint_add = vmath.vector4(0,0,0,0), player_tint_subtract = vmath.vector4(0,0,0,0), cloud =nil, border = true, map_zoom = true, music = {id=\"\", force=false, ambient=\"indoor_empty\", ambient_multiplier=1, rest_time=120}}    \n\n    m.map_info[hash(\"rusty_huts_h1\")] = {script_require = \"maps.map_codes.rusty_huts_h1\", display_name=[[]], biome = \"rusty\", max_level=10, min_level=1, store_level_cap=false, player_tint_add = vmath.vector4(0,0,0,0), player_tint_subtract = vmath.vector4(0,0,0,0), cloud =nil, border = true, map_zoom = true, music = {id=\"\", force=false, ambient=\"indoor_empty\", ambient_multiplier=1, rest_time=120}}    \n\nhere are a few lines from a space game about alien genetics:\n\n    {skill_name = \"log-log-4\", skill_id = 1089, race_ids = {4,4}, category_hash = hash(\"4-4-\"), texture = hash(\"skill_icons\"), flipbook = hash(\"4-4-\"), description = \"add 97 crew limit\", is_mutation = false, is_variant = true, can_combine = true, show_in_skillcheck = false, texture_string = \"skill_icons\", flipbook_string = \"4-4-\", default_unlocked = false, },    \n    {skill_name = \"log-log-5\", skill_id = 1090, race_ids = {4,4}, category_hash = hash(\"4-4-\"), texture = hash(\"skill_icons\"), flipbook = hash(\"4-4-\"), description = \"remove 38 damage\", is_mutation = false, is_variant = true, can_combine = true, show_in_skillcheck = false, texture_string = \"skill_icons\", flipbook_string = \"4-4-\", default_unlocked = false, },    \n    {skill_name = \"pow-pow-pow-1\", skill_id = 1091, race_ids = {1,1,1}, category_hash = hash(\"1-1-1-\"), texture = hash(\"skill_icons\"), flipbook = hash(\"1-1-1-\"), description = \"add 21 damage\", is_mutation = false, is_variant = true, can_combine = false, show_in_skillcheck = false, texture_string = \"skill_icons\", flipbook_string = \"1-1-1-\", default_unlocked = true, },\n    {skill_name = \"pow-pow-pow-2\", skill_id = 1092, race_ids = {1,1,1}, category_hash = hash(\"1-1-1-\"), texture = hash(\"skill_icons\"), flipbook = hash(\"1-1-1-\"), description = \"add 68 crew limit\", is_mutation = false, is_variant = true, can_combine = false, show_in_skillcheck = false, texture_string = \"skill_icons\", flipbook_string = \"1-1-1-\", default_unlocked = false, },    \n\nthis ends up being sometimes hundreds of lines of data, with a lot of stuff (strings, numbers, tables in tables, etc). since the data is live during development, i end up wanting to change it a lot, add and remove things, etc. it obviously gets unwieldy doing it right in the editor, editing hundreds of lines. previously i\u2019ve made google sheets to handle it. i love spreadsheets but even i will admit this is cumbersome:\n\n    =\"m.map_info[hash(\"\"\"&amp;b3&amp;\"\"\")] = {script_require = \"\"maps.map_codes.\"&amp;b3&amp;\"\"\", display_name=[[\"&amp;e3&amp;\"]], biome = \"\"\"&amp;c3&amp;\"\"\", max_level=\"&amp;p3&amp;\", min_level=\"&amp;q3&amp;\", store_level_cap=\"&amp;r3&amp;\", player_tint_add = vmath.vector4(\"&amp;s3&amp;\",\"&amp;t3&amp;\",\"&amp;u3&amp;\",\"&amp;v3&amp;\"), player_tint_subtract = vmath.vector4(\"&amp;w3&amp;\",\"&amp;x3&amp;\",\"&amp;y3&amp;\",\"&amp;z3&amp;\"), cloud =\"&amp;m3&amp;\", border = \"&amp;n3&amp;\", map_zoom = \"&amp;o3&amp;\", music = {id=\"\"\"&amp;h3&amp;\"\"\", force=\"&amp;i3&amp;\", ambient=\"\"\"&amp;k3&amp;\"\"\", ambient_multiplier=\"&amp;l3&amp;\", rest_time=\"&amp;j3&amp;\"}}\"    \n\ngenerally speaking, tables might look something like this:    \n{a_number = 2, a_string = \"string here\", subtable = {3}},    \n{a_number = 4, a_string = \"another_string\", subtable = {}},    \n{a_number = 8, a_string = \"yet_another_string\", subtable = {3,5,7,11,13,17,19}},    \n\nthe main stumbling block ends up being the subtables, which can have none, one, or multiple values. i haven't figured out a good way to handle this in a spreadsheet. i can either begin entering comma-delineated values manually (this is bad because it\u2019s manual, makes it hard to keep track of the data, and eventually begins resembling the reason i implemented the spreadsheet in the first place), or i have x number of columns for each individual data point in the subtable (this is bad because it is cumbersome to set up and implies a maximum number of entries in a subtable).    \n\ndoes this make sense? is there a better way to handle this? some kind of -----> tool !!!  (free is great, but i'll pay to make my life easier)?\n\nthank you in advance, and please do let me know if anything needs to be clarified.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/o5vbsg/game_development_how_to_structure_data/',)", "identifyer": 5596488, "year": "2021"}, {"autor": "Imaginary_Eagle3751", "date": 1623919461000, "content": "Feedback for open source data engineering tool Cuelake (similar to data bricks) /!/ [removed]", "link": "https://www.reddit.com/r/datascience/comments/o1sfq7/feedback_for_open_source_data_engineering_tool/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "feedback for open source data engineering -----> tool !!!  cuelake (similar to data bricks) /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/o1sfq7/feedback_for_open_source_data_engineering_tool/',)", "identifyer": 5596557, "year": "2021"}, {"autor": "moamen11", "date": 1623879248000, "content": "Applying machine learning model /!/ Sorry if this isn\u2019t the right place to post this. I am doing a project to monitor signals from several sensors and using Amazon AWS to store the data. I want to take the data from the S3 bucket, apply a machine learning model on it and add the prediction result into the bucket. Is Amazon sagemaker the appropriate tool or is it something else? Any links for a tutorial are appreciated.", "link": "https://www.reddit.com/r/datascience/comments/o1g5z4/applying_machine_learning_model/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "applying machine learning model /!/ sorry if this isn\u2019t the right place to post this. i am doing a project to monitor signals from several sensors and using amazon aws to store the data. i want to take the data from the s3 bucket, apply a machine learning model on it and add the prediction result into the bucket. is amazon sagemaker the appropriate -----> tool !!!  or is it something else? any links for a tutorial are appreciated.", "sortedWord": "None", "removed": "('nan',)", "score": 0, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/o1g5z4/applying_machine_learning_model/',)", "identifyer": 5596571, "year": "2021"}, {"autor": "Imaginary_Eagle3751", "date": 1623876221000, "content": "Feedback for open source data engineering tool Cuelake (similar to data bricks)", "link": "https://www.reddit.com/r/datascience/comments/o1ezdr/feedback_for_open_source_data_engineering_tool/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "feedback for open source data engineering -----> tool !!!  cuelake (similar to data bricks)", "sortedWord": "None", "removed": "('reddit',)", "score": 1, "comments": 0, "media": "('link',)", "medialink": "('/r/dataengineering/comments/o0bdox/feedback_for_open_source_data_engineering_tool/',)", "identifyer": 5596575, "year": "2021"}, {"autor": "i_am_exception", "date": 1619259897000, "content": "Predictive Analytics tool for a single user? /!/ Hi guys,\n\nI have been trying to search for a predictive analytics tool that scrapes the web to generate useful future insights that a single user like me can afford. Do you guys know any such tool? the current tools I found are extremely B2B (targetted at big companies) and they provide insights for their businesses specifically. I am looking for something like https\\[://\\]trends\\[.\\]google\\[.\\]com.", "link": "https://www.reddit.com/r/datascience/comments/mxh8sc/predictive_analytics_tool_for_a_single_user/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "predictive analytics -----> tool !!!  for a single user? /!/ hi guys,\n\ni have been trying to search for a predictive analytics tool that scrapes the web to generate useful future insights that a single user like me can afford. do you guys know any such tool? the current tools i found are extremely b2b (targetted at big companies) and they provide insights for their businesses specifically. i am looking for something like https\\[://\\]trends\\[.\\]google\\[.\\]com.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 10, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/mxh8sc/predictive_analytics_tool_for_a_single_user/',)", "identifyer": 5596695, "year": "2021"}, {"autor": "gaga_loo", "date": 1623189997000, "content": "Testing GPT-3 powered survey tool - feedback and thoughts? /!/ Hi!\n\nI'm building a tool that asks personalized follow up questions in order to get detailed survey responses. It's powered in part by GPT-3 (created by OpenAI). If you're open to testing it, it takes a couple of minutes...and I welcome feedback.\n\nIt's imperfect, but that's why we're testing ;)  Please DM me for a link!\n\nI'd also love to hear your experiences building standalone AI products like this.", "link": "https://www.reddit.com/r/datascience/comments/nvfy81/testing_gpt3_powered_survey_tool_feedback_and/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "testing gpt-3 powered survey -----> tool !!!  - feedback and thoughts? /!/ hi!\n\ni'm building a tool that asks personalized follow up questions in order to get detailed survey responses. it's powered in part by gpt-3 (created by openai). if you're open to testing it, it takes a couple of minutes...and i welcome feedback.\n\nit's imperfect, but that's why we're testing ;)  please dm me for a link!\n\ni'd also love to hear your experiences building standalone ai products like this.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nvfy81/testing_gpt3_powered_survey_tool_feedback_and/',)", "identifyer": 5596772, "year": "2021"}, {"autor": "weber_stephen", "date": 1623157120000, "content": "Top Exploratory Data Analysis (EDA) Tool Comparison", "link": "https://www.reddit.com/r/datascience/comments/nv3g48/top_exploratory_data_analysis_eda_tool_comparison/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "top exploratory data analysis (eda) -----> tool !!!  comparison", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 5, "media": "('nan',)", "medialink": "('https://www.bitrook.com/blog/exploratory-data-analysis-comparison',)", "identifyer": 5596800, "year": "2021"}, {"autor": "SliceBubbly6877", "date": 1631854769000, "content": "Amazon Data Scraping Tool | Scrape Amazon Product Ratings &amp; Reviews Tool /!/ [removed]", "link": "https://www.reddit.com/r/datascience/comments/pptpyv/amazon_data_scraping_tool_scrape_amazon_product/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "amazon data scraping -----> tool !!!  | scrape amazon product ratings &amp; reviews -----> tool !!!  /!/ [removed]", "sortedWord": "None", "removed": "('reddit',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pptpyv/amazon_data_scraping_tool_scrape_amazon_product/',)", "identifyer": 5596852, "year": "2021"}, {"autor": "exceln00bie", "date": 1631801979000, "content": "What are some good resources to get data sets on the profit status of an organization? /!/ I hope this is the right sub, if not please point me in the right direction. I want to ask if anyone has any resources that would allow one to check the profit status of an excel sheet of organizations (profit/not-for-profit).I\u2019m aware of the IRS tool, but it is not very good. Thank you!", "link": "https://www.reddit.com/r/datascience/comments/ppe7s1/what_are_some_good_resources_to_get_data_sets_on/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what are some good resources to get data sets on the profit status of an organization? /!/ i hope this is the right sub, if not please point me in the right direction. i want to ask if anyone has any resources that would allow one to check the profit status of an excel sheet of organizations (profit/not-for-profit).i\u2019m aware of the irs -----> tool !!! , but it is not very good. thank you!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/ppe7s1/what_are_some_good_resources_to_get_data_sets_on/',)", "identifyer": 5596889, "year": "2021"}, {"autor": "ChromeCamerata", "date": 1633417194000, "content": "Is low-code data analytics a bad place to start for an aspiring DS? /!/ Hi everyone,\n\nI started my very first data analytics job this month. It's also my very first tech job, before this I was a professional blogger.\n\nI'm liking the work so far and I'm intent on making this a career. There is one thing bothering me, though, which is that I am working in a very low-code environment, and I'm worried that this will leave me unsuited for the broader data science industry.\n\nA typical consulting engagement with a client consists of ETL using Alteryx and then basic EDA in Tableau. I'll sometimes use SQL and occasionally Python when the client doesn't have the above software, but generally speaking, I'm writing only a few dozen lines of code per project.\n\nI'm concerned that this is making me dependent on a handful of commercial software programs. Python seems like it's going to be around and universally popular for the foreseeable future, but I'm less confident about Alteryx and Tableau.\n\nI've also read that Alteryx, while more accessible than Python, is a shallower tool and not as suited to data science as opposed to business analytics.\n\nSo what I'm wondering is, am I starting off my DS career on the wrong foot, and if so, is it worth trying to transition to a more code-heavy skillset asap?", "link": "https://www.reddit.com/r/datascience/comments/q1pf3f/is_lowcode_data_analytics_a_bad_place_to_start/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "is low-code data analytics a bad place to start for an aspiring ds? /!/ hi everyone,\n\ni started my very first data analytics job this month. it's also my very first tech job, before this i was a professional blogger.\n\ni'm liking the work so far and i'm intent on making this a career. there is one thing bothering me, though, which is that i am working in a very low-code environment, and i'm worried that this will leave me unsuited for the broader data science industry.\n\na typical consulting engagement with a client consists of etl using alteryx and then basic eda in tableau. i'll sometimes use sql and occasionally python when the client doesn't have the above software, but generally speaking, i'm writing only a few dozen lines of code per project.\n\ni'm concerned that this is making me dependent on a handful of commercial software programs. python seems like it's going to be around and universally popular for the foreseeable future, but i'm less confident about alteryx and tableau.\n\ni've also read that alteryx, while more accessible than python, is a shallower -----> tool !!!  and not as suited to data science as opposed to business analytics.\n\nso what i'm wondering is, am i starting off my ds career on the wrong foot, and if so, is it worth trying to transition to a more code-heavy skillset asap?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 27, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/q1pf3f/is_lowcode_data_analytics_a_bad_place_to_start/',)", "identifyer": 5596961, "year": "2021"}, {"autor": "CamerataDev", "date": 1633416722000, "content": "Is low-code data analytics a bad place to start? /!/ Hi everyone,\n\nI started my very first data analytics job this month. It's also my very first tech job, before this I was a professional blogger.\n\nI'm liking the work so far and I'm intent on making this a career. There is one thing bothering me, though, which is that I am working in a very low-code environment, and I'm worried that this will leave me unsuited for the broader data science industry.\n\nA typical consulting engagement with a client consists of cleaning, transforming, and preparing the data they give me using Alteryx, and then creating visualizations in Tableau. I'll sometimes use SQL and occasionally Python, but generally speaking, I'm writing only a few dozen lines of code per project.\n\nI'm concerned that this is making me dependent on a handful of commercial software programs. Python seems like it's going to be around and universally popular for the foreseeable future, but I'm less confident about Alteryx and Tableau.\n\nI've also read that Alteryx, while more accessible than Python, is a shallower tool and not as suited to data science as opposed to business analytics.\n\nIn essence, I feel like I'm headed in more of a business analyst direction, when I want to be a data scientist. \n\nSo what I'm wondering is, firstly, am I starting off my DS career on the right foot, and if not, should I try and switch jobs soon? Is it worth trying to transition to a more code-heavy skillset asap, and how should I do that?", "link": "https://www.reddit.com/r/datascience/comments/q1pbp0/is_lowcode_data_analytics_a_bad_place_to_start/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "is low-code data analytics a bad place to start? /!/ hi everyone,\n\ni started my very first data analytics job this month. it's also my very first tech job, before this i was a professional blogger.\n\ni'm liking the work so far and i'm intent on making this a career. there is one thing bothering me, though, which is that i am working in a very low-code environment, and i'm worried that this will leave me unsuited for the broader data science industry.\n\na typical consulting engagement with a client consists of cleaning, transforming, and preparing the data they give me using alteryx, and then creating visualizations in tableau. i'll sometimes use sql and occasionally python, but generally speaking, i'm writing only a few dozen lines of code per project.\n\ni'm concerned that this is making me dependent on a handful of commercial software programs. python seems like it's going to be around and universally popular for the foreseeable future, but i'm less confident about alteryx and tableau.\n\ni've also read that alteryx, while more accessible than python, is a shallower -----> tool !!!  and not as suited to data science as opposed to business analytics.\n\nin essence, i feel like i'm headed in more of a business analyst direction, when i want to be a data scientist. \n\nso what i'm wondering is, firstly, am i starting off my ds career on the right foot, and if not, should i try and switch jobs soon? is it worth trying to transition to a more code-heavy skillset asap, and how should i do that?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/q1pbp0/is_lowcode_data_analytics_a_bad_place_to_start/',)", "identifyer": 5596962, "year": "2021"}, {"autor": "coolio9876", "date": 1617780986000, "content": "Alternatives to Make for data science ? /!/ Hi,\n\nWe curry use Make at work as our \"orchestration\" tool to rebuild projects in order to ensure they are reproducible. It works fine but many of our team complain Make being confusing and frustrating to work with due to its many quirks.\n\nWe had briefly looked into some alternatives such as scon and meson but they seem way too oriented on software development rather than output/report/model development for our use case. So yer was wondering what tools other people use and recommend instead ?", "link": "https://www.reddit.com/r/datascience/comments/mlwwxy/alternatives_to_make_for_data_science/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "alternatives to make for data science ? /!/ hi,\n\nwe curry use make at work as our \"orchestration\" -----> tool !!!  to rebuild projects in order to ensure they are reproducible. it works fine but many of our team complain make being confusing and frustrating to work with due to its many quirks.\n\nwe had briefly looked into some alternatives such as scon and meson but they seem way too oriented on software development rather than output/report/model development for our use case. so yer was wondering what tools other people use and recommend instead ?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 7, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/mlwwxy/alternatives_to_make_for_data_science/',)", "identifyer": 5597063, "year": "2021"}, {"autor": "adamwfletcher", "date": 1617727391000, "content": "What is your DS stack? (and roast mine :) ) /!/ Hi datascience!\n\nI'm curious what everyone's DS stack looks like. What are the tools you use to:\n\n* Ingest data\n* Process/transform/clean data\n* Query data\n* Visualize data\n* Share data\n* Some other tool/process you love\n\nWhat's the good and bad of each of these tools?\n\nMy stack:\n\n* Ingest: Python, typically. It's not the best answer but I can automate it, and there's libraries for whatever source my data is in (CSV, json, a SQL-compatible database, etc)\n* Process: Python for prototyping, then I usually end up doing a bunch of this with Airflow executing each step\n* Query: R Studio, PopSQL, Python+pandas - basically I'm trying to get into a dataframe as fast as possible\n* Visualize: ggplot2\n* Share: I don't have a great answer here; exports\u00a0+ dropbox or s3\n* Love: Jupyter/iPython notebooks (but they're super hard to move into production)\n\nI come from a software engineering background so I'm biased towards programming languages and automation. Feel free to roast my stack in the comments :)\n\nI'll collate the responses into a data set and post it here.", "link": "https://www.reddit.com/r/datascience/comments/mlfy02/what_is_your_ds_stack_and_roast_mine/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what is your ds stack? (and roast mine :) ) /!/ hi datascience!\n\ni'm curious what everyone's ds stack looks like. what are the tools you use to:\n\n* ingest data\n* process/transform/clean data\n* query data\n* visualize data\n* share data\n* some other -----> tool !!! /process you love\n\nwhat's the good and bad of each of these tools?\n\nmy stack:\n\n* ingest: python, typically. it's not the best answer but i can automate it, and there's libraries for whatever source my data is in (csv, json, a sql-compatible database, etc)\n* process: python for prototyping, then i usually end up doing a bunch of this with airflow executing each step\n* query: r studio, popsql, python+pandas - basically i'm trying to get into a dataframe as fast as possible\n* visualize: ggplot2\n* share: i don't have a great answer here; exports\u00a0+ dropbox or s3\n* love: jupyter/ipython notebooks (but they're super hard to move into production)\n\ni come from a software engineering background so i'm biased towards programming languages and automation. feel free to roast my stack in the comments :)\n\ni'll collate the responses into a data set and post it here.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 174, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/mlfy02/what_is_your_ds_stack_and_roast_mine/',)", "identifyer": 5597089, "year": "2021"}, {"autor": "habibTheCoel", "date": 1617726526000, "content": "AI based search engine { Startup idea } /!/  Obviously AI based search engine is the next step . Because of too much data available today on internet . We need this tool . I think it will be more interactive version of google . Where you can search based on size or similarity to other website , article or post . Or specify some part of text and find similar parts from other data sources . To rate the quality of the search after search query . And there could be a lot more features .\n\nIdea is quite simple , it's all about execution . That's why I am searching for cofounders . Feel free to DM me . Potential investors are welcome too . Thank you for reading this ! Stay hungry , stay foolish )", "link": "https://www.reddit.com/r/datascience/comments/mlflvu/ai_based_search_engine_startup_idea/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "ai based search engine { startup idea } /!/  obviously ai based search engine is the next step . because of too much data available today on internet . we need this -----> tool !!!  . i think it will be more interactive version of google . where you can search based on size or similarity to other website , article or post . or specify some part of text and find similar parts from other data sources . to rate the quality of the search after search query . and there could be a lot more features .\n\nidea is quite simple , it's all about execution . that's why i am searching for cofounders . feel free to dm me . potential investors are welcome too . thank you for reading this ! stay hungry , stay foolish )", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/mlflvu/ai_based_search_engine_startup_idea/',)", "identifyer": 5597091, "year": "2021"}, {"autor": "xandalorian", "date": 1617685525000, "content": "Python Tool/api for measuring statistical changes in inference data compared to training data? /!/ [removed]", "link": "https://www.reddit.com/r/datascience/comments/ml43sx/python_toolapi_for_measuring_statistical_changes/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "python -----> tool !!! /api for measuring statistical changes in inference data compared to training data? /!/ [removed]", "sortedWord": "None", "removed": "('reddit',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/ml43sx/python_toolapi_for_measuring_statistical_changes/',)", "identifyer": 5597119, "year": "2021"}, {"autor": "BeH1086", "date": 1617679532000, "content": "Help with personal project /!/ Hi, I am python self Learner and I am concerning about finding a solution for a problem in my actual job (which is not DS related) but I think that I can solve it with an advice from this reddit. \n\nBasically, I have a mechanical system (actually, I have more than 1000) which is measured weekly (some times more than 1 time per week) since 1970. This mechanical system involve fluids, pressures, valves and more variables that are visualized in a blackbox software in a particular way (the software has some features, not more, not less, and can't be changed, it is something old school) ... \n\nThe graph produced when all is working fine is a 4 line graph forming a perfect rectangle. If any of the variables previously mentioned is affected in any way, for example, a valve is loosing capacity of closing, the \"under\" line of the rectangle begin to became concave... When the \"valve\" line in the rectangle became full concave the valve is not working at all. Other variables, like pressure affected the \"right\" line of the rectangle, and the process is the same. The optimized way to work is to know which % of concavity is accepted as \"ok, is working fine\" or in an \"acceptable way\" and maintain the 1000 systems working into those specifications. Of course, waiting till the system broke up to change or repair it is not good because the whole system suffers from that. \n\nSaying that, one of my daily activities is to watch out to those graph, preventing something to collapse.  Minimizing and anticipating problems before they broke up. \n\nI want to program in python an automated or ML solution for reading those graphs. The tool need to understand basically what I know, if the line became concave then an alarm needs to appear, also I want that the tool predict the future problem, I mean, maybe a good estimation on when (how many days) it will reach the % of concavity that I set up as maximum . \n\nHere my questions:\n\nIt is a DS problem or a ML one?\n\nWhich subject is related to this in the DS-ML field? \n\nIf I want to find a course to learn more about this topic in edx or coursera what would be a good idea to enroll?", "link": "https://www.reddit.com/r/datascience/comments/ml2evk/help_with_personal_project/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "help with personal project /!/ hi, i am python self learner and i am concerning about finding a solution for a problem in my actual job (which is not ds related) but i think that i can solve it with an advice from this reddit. \n\nbasically, i have a mechanical system (actually, i have more than 1000) which is measured weekly (some times more than 1 time per week) since 1970. this mechanical system involve fluids, pressures, valves and more variables that are visualized in a blackbox software in a particular way (the software has some features, not more, not less, and can't be changed, it is something old school) ... \n\nthe graph produced when all is working fine is a 4 line graph forming a perfect rectangle. if any of the variables previously mentioned is affected in any way, for example, a valve is loosing capacity of closing, the \"under\" line of the rectangle begin to became concave... when the \"valve\" line in the rectangle became full concave the valve is not working at all. other variables, like pressure affected the \"right\" line of the rectangle, and the process is the same. the optimized way to work is to know which % of concavity is accepted as \"ok, is working fine\" or in an \"acceptable way\" and maintain the 1000 systems working into those specifications. of course, waiting till the system broke up to change or repair it is not good because the whole system suffers from that. \n\nsaying that, one of my daily activities is to watch out to those graph, preventing something to collapse.  minimizing and anticipating problems before they broke up. \n\ni want to program in python an automated or ml solution for reading those graphs. the -----> tool !!!  need to understand basically what i know, if the line became concave then an alarm needs to appear, also i want that the -----> tool !!!  predict the future problem, i mean, maybe a good estimation on when (how many days) it will reach the % of concavity that i set up as maximum . \n\nhere my questions:\n\nit is a ds problem or a ml one?\n\nwhich subject is related to this in the ds-ml field? \n\nif i want to find a course to learn more about this topic in edx or coursera what would be a good idea to enroll?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/ml2evk/help_with_personal_project/',)", "identifyer": 5597122, "year": "2021"}, {"autor": "data-gig", "date": 1622649751000, "content": "I've built my own data analysis platform and learned a lot /!/ Did you know that less than 1% of the population knows how to code? Why not allowing the rest 99%s to make some sense out of their data? Ever heard about the Jupyter Notebook? So, why not having a version for people with no coding skills or desires? So, about four months ago, in a discussion forum I had shared an idea about a browser-based interactive notebook platform to analyze data in one place without writing a single line of code.\n\nEventually, I wrote my own app using Python and Django. It is not launched yet but you can find more info here: [https://magicnote.co](https://magicnote.co)\n\nDo you want to do a career in data science? Then start doing something. Whether it is learning Python, a bi tool or whatever, you have to start from somewhere.", "link": "https://www.reddit.com/r/datascience/comments/nqp82r/ive_built_my_own_data_analysis_platform_and/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "i've built my own data analysis platform and learned a lot /!/ did you know that less than 1% of the population knows how to code? why not allowing the rest 99%s to make some sense out of their data? ever heard about the jupyter notebook? so, why not having a version for people with no coding skills or desires? so, about four months ago, in a discussion forum i had shared an idea about a browser-based interactive notebook platform to analyze data in one place without writing a single line of code.\n\neventually, i wrote my own app using python and django. it is not launched yet but you can find more info here: [https://magicnote.co](https://magicnote.co)\n\ndo you want to do a career in data science? then start doing something. whether it is learning python, a bi -----> tool !!!  or whatever, you have to start from somewhere.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nqp82r/ive_built_my_own_data_analysis_platform_and/',)", "identifyer": 5597238, "year": "2021"}, {"autor": "StrikingButterfly", "date": 1622626320000, "content": "[]Mini-Achievement] My first ever product feature request for a tool got accepted on github. /!/ I know it may sound ridiculous but I am literally giggling like a 5-year-old who has been given their favourite candy!", "link": "https://www.reddit.com/r/datascience/comments/nqhupw/miniachievement_my_first_ever_product_feature/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[]mini-achievement] my first ever product feature request for a -----> tool !!!  got accepted on github. /!/ i know it may sound ridiculous but i am literally giggling like a 5-year-old who has been given their favourite candy!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 9, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nqhupw/miniachievement_my_first_ever_product_feature/',)", "identifyer": 5597251, "year": "2021"}, {"autor": "Dpdr00", "date": 1612212189000, "content": "Has anyone ever felt dumb while being taught something by a senior DS and like they were going to be fired /!/ I am transitioning to the data science team at my company with mostly just knowledge of using pandas and pyspark dataframes. \nI was put on a data engineering project where we are exporting my dataframe and pushing it into an alerting tool using an API. We are setting this up from scratch. \nThe senior data scientist has me share my screen and tells me what to do and half the time im lost and have to ask what he means, i get so frustrated with myself and am afraid they\u2019re going to fire me for not knowing enough. \nI also overthink simple things he\u2019s saying while I\u2019m sharing my screen, like where to put parenthesis after a function because I\u2019m nervous on the spot lmao aahdhjdejbfdnwkdofh", "link": "https://www.reddit.com/r/datascience/comments/ladjw4/has_anyone_ever_felt_dumb_while_being_taught/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "has anyone ever felt dumb while being taught something by a senior ds and like they were going to be fired /!/ i am transitioning to the data science team at my company with mostly just knowledge of using pandas and pyspark dataframes. \ni was put on a data engineering project where we are exporting my dataframe and pushing it into an alerting -----> tool !!!  using an api. we are setting this up from scratch. \nthe senior data scientist has me share my screen and tells me what to do and half the time im lost and have to ask what he means, i get so frustrated with myself and am afraid they\u2019re going to fire me for not knowing enough. \ni also overthink simple things he\u2019s saying while i\u2019m sharing my screen, like where to put parenthesis after a function because i\u2019m nervous on the spot lmao aahdhjdejbfdnwkdofh", "sortedWord": "None", "removed": "('nan',)", "score": 3, "comments": 10, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/ladjw4/has_anyone_ever_felt_dumb_while_being_taught/',)", "identifyer": 5597547, "year": "2021"}, {"autor": "Kobedoggg", "date": 1626441009000, "content": "Which software/tool could you least live without as a data professional?", "link": "https://www.reddit.com/r/datascience/comments/olgk9o/which_softwaretool_could_you_least_live_without/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "which software/-----> tool !!!  could you least live without as a data professional?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('nan',)", "medialink": "('/r/TheAnalystEconomy/comments/olgj3c/which_softwaretool_could_you_least_live_without/',)", "identifyer": 5597618, "year": "2021"}, {"autor": "ifreeski420", "date": 1625264485000, "content": "What did I do wrong in my answer for this Python data science competition? /!/ https://drive.google.com/drive/folders/1fVck0obb8BtmHLFxRzj-lyDbj419AvCd\n\nIn this folder you will find two tables. The Temperature Data table contains daily weather station readings for a sample of cities around the US. The Population Data table contains population estimates for various cities. Imagine your boss asked you to create a population weighted daily temperature timeseries going back to the beginning of 2015 for the US using only this data. Not every weather station was operating every day and some days may be missing altogether. Write a python program that generates the timeseries while handling the missing days and station data. The timeseries must give a reasonable daily value even the data is missing.\n\nPart 2. With the timeseries complete display the data graphically using the graphing tool of your choice. Graph the data in a way that accommodates the following use cases: \n-Someone who wants to know how the temperature compares to the seasonal average as well as high and low cases.\n-Someone who wants to view the monthly average, min, and max.\n-Someone who wants to graphically see what data is missing or projected.\n\nhttps://github.com/verifiedathletics/us_weather_avg\n\nHere is the github link with the code along with weather station csv for mapping. Any tips would be helpful", "link": "https://www.reddit.com/r/datascience/comments/ockyiy/what_did_i_do_wrong_in_my_answer_for_this_python/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what did i do wrong in my answer for this python data science competition? /!/ https://drive.google.com/drive/folders/1fvck0obb8btmhlfxrzj-lydbj419avcd\n\nin this folder you will find two tables. the temperature data table contains daily weather station readings for a sample of cities around the us. the population data table contains population estimates for various cities. imagine your boss asked you to create a population weighted daily temperature timeseries going back to the beginning of 2015 for the us using only this data. not every weather station was operating every day and some days may be missing altogether. write a python program that generates the timeseries while handling the missing days and station data. the timeseries must give a reasonable daily value even the data is missing.\n\npart 2. with the timeseries complete display the data graphically using the graphing -----> tool !!!  of your choice. graph the data in a way that accommodates the following use cases: \n-someone who wants to know how the temperature compares to the seasonal average as well as high and low cases.\n-someone who wants to view the monthly average, min, and max.\n-someone who wants to graphically see what data is missing or projected.\n\nhttps://github.com/verifiedathletics/us_weather_avg\n\nhere is the github link with the code along with weather station csv for mapping. any tips would be helpful", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 5, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/ockyiy/what_did_i_do_wrong_in_my_answer_for_this_python/',)", "identifyer": 5597634, "year": "2021"}, {"autor": "Lostwhispers05", "date": 1628425132000, "content": "Language-agnostic tools similar to Streamlit that are suitable for embedding in production-grade applications, to allow an end-user to fiddle with variables and see the outcome of a model. /!/ Plotly dash comes to mind but because this is Python-based, I figure it might not be readily embeddable in an application that might perhaps be Ruby on Rails based (or am I incorrect in that?).\n\nIf I had a page within a portal where a client could fiddle with variables and see the outcome of a model, what tool (similar to streamlit), would I be able to simply use streamlit itself, or is there a tool similar to streamlit that lends itself readily to being included as part of a much broader application.\n\nThe functionalities of streamlit itself meet our needs nicely, but as far as I know it's much more for quick prototyping rather than production-grade applications.", "link": "https://www.reddit.com/r/datascience/comments/p0dn3f/languageagnostic_tools_similar_to_streamlit_that/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "language-agnostic tools similar to streamlit that are suitable for embedding in production-grade applications, to allow an end-user to fiddle with variables and see the outcome of a model. /!/ plotly dash comes to mind but because this is python-based, i figure it might not be readily embeddable in an application that might perhaps be ruby on rails based (or am i incorrect in that?).\n\nif i had a page within a portal where a client could fiddle with variables and see the outcome of a model, what -----> tool !!!  (similar to streamlit), would i be able to simply use streamlit itself, or is there a -----> tool !!!  similar to streamlit that lends itself readily to being included as part of a much broader application.\n\nthe functionalities of streamlit itself meet our needs nicely, but as far as i know it's much more for quick prototyping rather than production-grade applications.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/p0dn3f/languageagnostic_tools_similar_to_streamlit_that/',)", "identifyer": 5597763, "year": "2021"}, {"autor": "Ecstatic_Tooth_1096", "date": 1621786060000, "content": "Securing a Data Analyst/Data Science job /!/ **Motivation...**\n\nThis post, soon to be Blog, feels like a much needed post here. Since so many people are looking to either start their career in data science or do their career shift. I will explain how I did it, and how many have done it before me and how many will do after.\n\n*This isn't a post on getting accepted at a FAANG or NASA...* \n\n**Essentials**\n\n* know a bit of coding (Python or R preferably). I said a bit, because you can develop your skills along the way\n   * list comprehension, loops, writing small concise functions, regex (i suck at that tbh)...\n   * Pandas (very important for data manipulation) (or its equivalent in R )\n   * Numpy (or its equivalent in R )\n   * matplotlib for visualization (or its equivalent in R)\n\nEveryone uses stackoverflow and google to find a few answers here and there so don't feel scared.\n\n* Statistics \n   * No need to be a god in Stat, stick to the basics that we learn at school. the more advanced concepts are either too much for a regular job or for the seniors (which means you will have time to learn them while at your company)\n   * Your bestfriend is [StatQuest](https://www.youtube.com/user/joshstarmer)\n* Well written CV. One concise page about yourself. Highlight yourself without giving too much details so you can have something to talk about in your interviews.\n   * if you get nervous during interviews, try applying for random positions (that you don't really care about) to receive interviews to improve your skills and confidence when talking to HRs (I have done that, it helped a lot) (*not too ethical but it helps*)\n\n**Bonus**\n\n* A **certificate** (or equivalent), I truly like DataCamp (reason why below and [here](https://dataanalystlife.blogspot.com/2021/05/is-datacamp-worth-it.html)), even though I got rejected when I applied for a position at their firm. Some of their professors teach at top notch universities (in Europe and USA ...)\n* **Dashboarding** tools (PowerBI/Tableau ...).\n* Master's degree. Not for the sake of what you learn, but for the **title**.\n* A very well developed **GitHub** with plenty of projects (if you have this already, you wouldn't need to read the post for sure)\n* Internships! If you're still studying at uni. Try to secure as much internships as possible you will create an attractive and robust CV.\n\n**My Background (skip if you don't care)**\n\nI am a Civil Engineer and a master of AI student at a top university in Belgium. I still have my thesis to defend, if I fail, I am dropping out. I have aced (in my opinion) all my other theoretical courses.\n\nI have taken Machine Learning courses, Deep Learning, Computer Vision and others (only mentioning the trendy ones). In parallel to that, I was doing a certificate on DataCamp called \"Data Scientist Track With Python\" and I truly [believe](https://dataanalystlife.blogspot.com/2021/05/is-datacamp-worth-it.html) that this track was a game changer to me (my blog has a bit of details about that).\n\n**My first Internship at KPMG-BE**\n\nI was one of two interns at KPMG-BE in summer 2020. We got both accepted within the AI team. My topic was about bias and fairness in AI (in other words racism against minorities, sexist models...). I had to make a case study on why reducing the bias of datasets is needed to improve the society and abide to legislations (its a long story).\n\nI used the skills acquired by DataCamp to play with datasets, create visualizations, train models... \n\nThe other soft skills needed were good communication skills and PowerPoint (**communication IS HIGHLY important**).\n\n**My second Internship at PwC-BE**\n\nOn this internship I learned the most valuable lessons of my life. By my two mentors, P. and A. (if they ever read that, they will know themselves). *BTW I was working as a Data Analyst/consultant*.\n\n1. Having common sense and business acumen is a must in many DA/DS jobs\n2. Understanding that seniors don't care about your incredible skills if you can't communicate them.\n3. No-code software like **Alteryx** are GODLY (in my opinion), especially if you're not a programmer. Now, if you are a programmer Alteryx will help you develop a very good intuition for data cleaning and manipulation.\n4. Prototyping/agile work is very useful for your clients\n5. Knowing how to use a dashboarding tool is crazy good to communicate your ideas\n\n**My third Internship/Project with a company**\n\nOn this internship, I was part of a team of 6, where we had to do a market study on everything related to green energies and their infrastructure. Not related to data science but it helps in developing your skills.\n\n**Conclusion**\n\nThe sooner you start working, the faster you'll achieve it. Good luck everyone!", "link": "https://www.reddit.com/r/datascience/comments/njatvc/securing_a_data_analystdata_science_job/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "securing a data analyst/data science job /!/ **motivation...**\n\nthis post, soon to be blog, feels like a much needed post here. since so many people are looking to either start their career in data science or do their career shift. i will explain how i did it, and how many have done it before me and how many will do after.\n\n*this isn't a post on getting accepted at a faang or nasa...* \n\n**essentials**\n\n* know a bit of coding (python or r preferably). i said a bit, because you can develop your skills along the way\n   * list comprehension, loops, writing small concise functions, regex (i suck at that tbh)...\n   * pandas (very important for data manipulation) (or its equivalent in r )\n   * numpy (or its equivalent in r )\n   * matplotlib for visualization (or its equivalent in r)\n\neveryone uses stackoverflow and google to find a few answers here and there so don't feel scared.\n\n* statistics \n   * no need to be a god in stat, stick to the basics that we learn at school. the more advanced concepts are either too much for a regular job or for the seniors (which means you will have time to learn them while at your company)\n   * your bestfriend is [statquest](https://www.youtube.com/user/joshstarmer)\n* well written cv. one concise page about yourself. highlight yourself without giving too much details so you can have something to talk about in your interviews.\n   * if you get nervous during interviews, try applying for random positions (that you don't really care about) to receive interviews to improve your skills and confidence when talking to hrs (i have done that, it helped a lot) (*not too ethical but it helps*)\n\n**bonus**\n\n* a **certificate** (or equivalent), i truly like datacamp (reason why below and [here](https://dataanalystlife.blogspot.com/2021/05/is-datacamp-worth-it.html)), even though i got rejected when i applied for a position at their firm. some of their professors teach at top notch universities (in europe and usa ...)\n* **dashboarding** tools (powerbi/tableau ...).\n* master's degree. not for the sake of what you learn, but for the **title**.\n* a very well developed **github** with plenty of projects (if you have this already, you wouldn't need to read the post for sure)\n* internships! if you're still studying at uni. try to secure as much internships as possible you will create an attractive and robust cv.\n\n**my background (skip if you don't care)**\n\ni am a civil engineer and a master of ai student at a top university in belgium. i still have my thesis to defend, if i fail, i am dropping out. i have aced (in my opinion) all my other theoretical courses.\n\ni have taken machine learning courses, deep learning, computer vision and others (only mentioning the trendy ones). in parallel to that, i was doing a certificate on datacamp called \"data scientist track with python\" and i truly [believe](https://dataanalystlife.blogspot.com/2021/05/is-datacamp-worth-it.html) that this track was a game changer to me (my blog has a bit of details about that).\n\n**my first internship at kpmg-be**\n\ni was one of two interns at kpmg-be in summer 2020. we got both accepted within the ai team. my topic was about bias and fairness in ai (in other words racism against minorities, sexist models...). i had to make a case study on why reducing the bias of datasets is needed to improve the society and abide to legislations (its a long story).\n\ni used the skills acquired by datacamp to play with datasets, create visualizations, train models... \n\nthe other soft skills needed were good communication skills and powerpoint (**communication is highly important**).\n\n**my second internship at pwc-be**\n\non this internship i learned the most valuable lessons of my life. by my two mentors, p. and a. (if they ever read that, they will know themselves). *btw i was working as a data analyst/consultant*.\n\n1. having common sense and business acumen is a must in many da/ds jobs\n2. understanding that seniors don't care about your incredible skills if you can't communicate them.\n3. no-code software like **alteryx** are godly (in my opinion), especially if you're not a programmer. now, if you are a programmer alteryx will help you develop a very good intuition for data cleaning and manipulation.\n4. prototyping/agile work is very useful for your clients\n5. knowing how to use a dashboarding -----> tool !!!  is crazy good to communicate your ideas\n\n**my third internship/project with a company**\n\non this internship, i was part of a team of 6, where we had to do a market study on everything related to green energies and their infrastructure. not related to data science but it helps in developing your skills.\n\n**conclusion**\n\nthe sooner you start working, the faster you'll achieve it. good luck everyone!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/njatvc/securing_a_data_analystdata_science_job/',)", "identifyer": 5597789, "year": "2021"}, {"autor": "NxtGen369", "date": 1629279376000, "content": "Web Scraping tool (free/cheap for mvp) with decent # of data row exports /!/ Hey fellas. I'm in the final phase of a coding bootcamp and working on a aggregator website that scrapes different marketplaces so people don't have to visit all of them. My instructors concern is that with ie scrapestorms free plan we only get 100 rows to export but even one marketplace has like 70k listings. Can anybody recommend a proper free or at least relatively cheap plan so that I can at least for the mvp scrape like two or three marketplaces? Several google search results unfortunately doesn't  even speak about export volume.", "link": "https://www.reddit.com/r/datascience/comments/p6ntwy/web_scraping_tool_freecheap_for_mvp_with_decent/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "web scraping -----> tool !!!  (free/cheap for mvp) with decent # of data row exports /!/ hey fellas. i'm in the final phase of a coding bootcamp and working on a aggregator website that scrapes different marketplaces so people don't have to visit all of them. my instructors concern is that with ie scrapestorms free plan we only get 100 rows to export but even one marketplace has like 70k listings. can anybody recommend a proper free or at least relatively cheap plan so that i can at least for the mvp scrape like two or three marketplaces? several google search results unfortunately doesn't  even speak about export volume.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/p6ntwy/web_scraping_tool_freecheap_for_mvp_with_decent/',)", "identifyer": 5597862, "year": "2021"}, {"autor": "altdataguy", "date": 1629197663000, "content": "Tracking point-in-time data /!/ After having held data analyst positions in two large companies, I've noticed a commonality in that the tracing of historical information is insufficient, e.g. historical value of specific KPIs reported in Tableau over time...\n\nIs there a specific tool to store and analyze historical point-in-time data? Have anyone else experience similar inefficiencies?", "link": "https://www.reddit.com/r/datascience/comments/p61eyq/tracking_pointintime_data/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "tracking point-in-time data /!/ after having held data analyst positions in two large companies, i've noticed a commonality in that the tracing of historical information is insufficient, e.g. historical value of specific kpis reported in tableau over time...\n\nis there a specific -----> tool !!!  to store and analyze historical point-in-time data? have anyone else experience similar inefficiencies?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 10, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/p61eyq/tracking_pointintime_data/',)", "identifyer": 5597924, "year": "2021"}, {"autor": "RMNSNC", "date": 1629182927000, "content": "Visual Scripting Tool for Python /!/ [removed]", "link": "https://www.reddit.com/r/datascience/comments/p5yjb6/visual_scripting_tool_for_python/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "visual scripting -----> tool !!!  for python /!/ [removed]", "sortedWord": "None", "removed": "('reddit',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/p5yjb6/visual_scripting_tool_for_python/',)", "identifyer": 5597930, "year": "2021"}, {"autor": "AbdulWahabAbrar", "date": 1629145791000, "content": "I need help deciding good visual for you the task (please see first comment for the picture) /!/ Hello, I need your suggestions on something I am working on.\n\nI have data collected by Apache Kafka in Kibana. The data file is of an eCommerce website. My task is to analyze the customer behavior (often known as Clickstream Analysis or Customer Behavior Analysis).\n\nThe kibana collects the logs of: when users come on a webpage (Homepage usually) then he clicks buttons and open some stuff (like their profile, shopping cart, checkout page, payment page, etc.)\nMy task is to Visualize when users come on the website then where they go during the time they are on website, in general terms, we have data for time spent by users on website and what path they followed.\n\nI have 3 columns\n1. Time stamp: General time stamp column with date and time\n2. page_urlpath: the page user is present right now (landing page)\n3. refr_urlpath: the page where user was previously and then went to page_urlpath (Previous Page before coming on Landing Page)\n4. Userid: Unique user IDs\n\nI used Python's networkx library to make a network diagram to show how the user flows on the page but it works fine for just one user and doesn't support timestamp.\n\nCan you guys advise me on what tool or library I can use to solve this issue? I am sharing the picture in a link in the first comment of how my current network diagram looks like. (The red node is the starting point)\n\nAppreciate your help!", "link": "https://www.reddit.com/r/datascience/comments/p5oexb/i_need_help_deciding_good_visual_for_you_the_task/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "i need help deciding good visual for you the task (please see first comment for the picture) /!/ hello, i need your suggestions on something i am working on.\n\ni have data collected by apache kafka in kibana. the data file is of an ecommerce website. my task is to analyze the customer behavior (often known as clickstream analysis or customer behavior analysis).\n\nthe kibana collects the logs of: when users come on a webpage (homepage usually) then he clicks buttons and open some stuff (like their profile, shopping cart, checkout page, payment page, etc.)\nmy task is to visualize when users come on the website then where they go during the time they are on website, in general terms, we have data for time spent by users on website and what path they followed.\n\ni have 3 columns\n1. time stamp: general time stamp column with date and time\n2. page_urlpath: the page user is present right now (landing page)\n3. refr_urlpath: the page where user was previously and then went to page_urlpath (previous page before coming on landing page)\n4. userid: unique user ids\n\ni used python's networkx library to make a network diagram to show how the user flows on the page but it works fine for just one user and doesn't support timestamp.\n\ncan you guys advise me on what -----> tool !!!  or library i can use to solve this issue? i am sharing the picture in a link in the first comment of how my current network diagram looks like. (the red node is the starting point)\n\nappreciate your help!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/p5oexb/i_need_help_deciding_good_visual_for_you_the_task/',)", "identifyer": 5597945, "year": "2021"}, {"autor": "belfand", "date": 1634351017000, "content": "Tool that parses excel tables /!/ Hi everyone,\n\nI am working with some excel data that looks like this\n\n&amp;#x200B;\n\nhttps://preview.redd.it/ua548f5r0qt71.png?width=1524&amp;format=png&amp;auto=webp&amp;s=7755f8a2487afe92120463d0c61510ba116a4b9e\n\nThese things are called \"Blocking Charts\", basically they are used in marketing and they show how much money is spent in advertising each week. They don't always come in the exact same format, but in general they follow this format.\n\nIs there any excel data extraction tools (paid or free) that can extract excel files in similar formats? We are trying to automate this process as we process a large amount of similar files a week, and it would be useful to leverage some libraries/APIs.\n\n&amp;#x200B;\n\nThanks!", "link": "https://www.reddit.com/r/datascience/comments/q92xku/tool_that_parses_excel_tables/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "-----> tool !!!  that parses excel tables /!/ hi everyone,\n\ni am working with some excel data that looks like this\n\n&amp;#x200b;\n\nhttps://preview.redd.it/ua548f5r0qt71.png?width=1524&amp;format=png&amp;auto=webp&amp;s=7755f8a2487afe92120463d0c61510ba116a4b9e\n\nthese things are called \"blocking charts\", basically they are used in marketing and they show how much money is spent in advertising each week. they don't always come in the exact same format, but in general they follow this format.\n\nis there any excel data extraction tools (paid or free) that can extract excel files in similar formats? we are trying to automate this process as we process a large amount of similar files a week, and it would be useful to leverage some libraries/apis.\n\n&amp;#x200b;\n\nthanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 15, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/q92xku/tool_that_parses_excel_tables/',)", "identifyer": 5598078, "year": "2021"}, {"autor": "Your_Data_Talking", "date": 1634320867000, "content": "New Project: an OCR image archive classification engine. /!/ So what they want is a tool than can look through an archive of decades of data that is only kinda labeled. The last count of the archive (1st qtr 2020) was 9,300 doc avg per folder and ~1100 folders at 37 TB of data.\n\nNeed help:\nI have experience but it\u2019s been 5 years since I\u2019ve done OCR work, i found nanonets have a tesseract tutorial. Is that of good quality?\n\nIs there a database that is preferred for saving photos or pdf documents? I recall something maybe about saving the path in sql but file goes into mongo? #notsure\n\nI\u2019m using Python3.7 modules \u2018schedule\u2019, a script logger that is solid, and an old LSTM that I should update. What am I missing?", "link": "https://www.reddit.com/r/datascience/comments/q8tvmi/new_project_an_ocr_image_archive_classification/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "new project: an ocr image archive classification engine. /!/ so what they want is a -----> tool !!!  than can look through an archive of decades of data that is only kinda labeled. the last count of the archive (1st qtr 2020) was 9,300 doc avg per folder and ~1100 folders at 37 tb of data.\n\nneed help:\ni have experience but it\u2019s been 5 years since i\u2019ve done ocr work, i found nanonets have a tesseract tutorial. is that of good quality?\n\nis there a database that is preferred for saving photos or pdf documents? i recall something maybe about saving the path in sql but file goes into mongo? #notsure\n\ni\u2019m using python3.7 modules \u2018schedule\u2019, a script logger that is solid, and an old lstm that i should update. what am i missing?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/q8tvmi/new_project_an_ocr_image_archive_classification/',)", "identifyer": 5598096, "year": "2021"}, {"autor": "ohhmichael", "date": 1634318451000, "content": "Free tool for drafting a schema for a relational database? /!/ Does anyone have a recommendation for a tool that will allow me to brainstorm a schema for a relational database I'm trying to create in Airtable? I drafted something on paper, have previously used PPT, and assume there is something more specific that would allow me to mock up and edit tables and relationships. Ultimately, I will use this as an outline to share with a client before building the database.", "link": "https://www.reddit.com/r/datascience/comments/q8t2gq/free_tool_for_drafting_a_schema_for_a_relational/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "free -----> tool !!!  for drafting a schema for a relational database? /!/ does anyone have a recommendation for a tool that will allow me to brainstorm a schema for a relational database i'm trying to create in airtable? i drafted something on paper, have previously used ppt, and assume there is something more specific that would allow me to mock up and edit tables and relationships. ultimately, i will use this as an outline to share with a client before building the database.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/q8t2gq/free_tool_for_drafting_a_schema_for_a_relational/',)", "identifyer": 5598099, "year": "2021"}, {"autor": "reemo141", "date": 1630497978000, "content": "Career Advice - Data Analyst at Big Social Media Company or Stay at Current DS Job /!/ Hello! \n\nI'm currently a DS in a manufacturing company for +2 years, but recently received an offer for 12+ months W2 contract-to-hire for Data Analyst position at a big social media company. \n\nWhy am I even considering this? \n- Significant pay difference (+30% after taking in consideration benefits, PTO, etc.) + remote.\n- Opportunity to get my foot in the door with a big company. \n- Toxic environment and culture at current company. \n- Current position has limited opportunities for cool/interesting projects. This is due to business needs, not because use-cases don't exist. \n\nWhat's the downside? \n- Primary tool used in Data Analyst position is SQL, some Python/R... job sounds a little boring compared to current position. \n- Unclear how this will impact future Data Science job opportunities. \n\nThe timing plays a role for me as well since I really don't like the company I work for (even though some ppl are great) and I will also be finishing up my Master's in Analytics this fall with a NASA-sponsored project under my belt. I have a decent resume with some solid accomplishments, but I haven't been able to land a full-time DS job elsewhere. \n\nOptions: \n1. Take the risk, take the job. If I don't like it, find another job at the end of contract.  \n2. Pass up opportunity and apply again after graduating with MS. \n3. Other.\n\nHas anyone gone through something similar? Can you share your experiences? Any advice and feedback on this is much appreciated!!!", "link": "https://www.reddit.com/r/datascience/comments/pfsid1/career_advice_data_analyst_at_big_social_media/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "career advice - data analyst at big social media company or stay at current ds job /!/ hello! \n\ni'm currently a ds in a manufacturing company for +2 years, but recently received an offer for 12+ months w2 contract-to-hire for data analyst position at a big social media company. \n\nwhy am i even considering this? \n- significant pay difference (+30% after taking in consideration benefits, pto, etc.) + remote.\n- opportunity to get my foot in the door with a big company. \n- toxic environment and culture at current company. \n- current position has limited opportunities for cool/interesting projects. this is due to business needs, not because use-cases don't exist. \n\nwhat's the downside? \n- primary -----> tool !!!  used in data analyst position is sql, some python/r... job sounds a little boring compared to current position. \n- unclear how this will impact future data science job opportunities. \n\nthe timing plays a role for me as well since i really don't like the company i work for (even though some ppl are great) and i will also be finishing up my master's in analytics this fall with a nasa-sponsored project under my belt. i have a decent resume with some solid accomplishments, but i haven't been able to land a full-time ds job elsewhere. \n\noptions: \n1. take the risk, take the job. if i don't like it, find another job at the end of contract.  \n2. pass up opportunity and apply again after graduating with ms. \n3. other.\n\nhas anyone gone through something similar? can you share your experiences? any advice and feedback on this is much appreciated!!!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pfsid1/career_advice_data_analyst_at_big_social_media/',)", "identifyer": 5598142, "year": "2021"}, {"autor": "edugomez28", "date": 1630486887000, "content": "XGBoost use on Forcasting Consumption /!/ Hello,\n\nDuring a meeting today we where discussing the need of having a more precise forecast of consumption of our products, someone suggested XX tool that does this but upon reviewing it I realize that they are using XGboost for this task. Even though I love XGboost I feel like for this particularly problem is not suited, since tree base models are not good with trending data. The data science team said that during testing they found that this was the best model that fitted the data, I feel like I am missing something.\n\nAm I wrong in my assumption that tree base models like XGboost are not suitable? what am I missing?", "link": "https://www.reddit.com/r/datascience/comments/pfpzmw/xgboost_use_on_forcasting_consumption/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "xgboost use on forcasting consumption /!/ hello,\n\nduring a meeting today we where discussing the need of having a more precise forecast of consumption of our products, someone suggested xx -----> tool !!!  that does this but upon reviewing it i realize that they are using xgboost for this task. even though i love xgboost i feel like for this particularly problem is not suited, since tree base models are not good with trending data. the data science team said that during testing they found that this was the best model that fitted the data, i feel like i am missing something.\n\nam i wrong in my assumption that tree base models like xgboost are not suitable? what am i missing?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 15, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pfpzmw/xgboost_use_on_forcasting_consumption/',)", "identifyer": 5598150, "year": "2021"}, {"autor": "jay_ODSC", "date": 1628081849000, "content": "\u201cChicken or the Egg\u201d in MLOPs: The Case of the DIY Feature Store /!/ Date: 11th August 2021 Time: 1 pm EDT (GMT-4)\n\n While developing MLOps in an organization, one often encounters the \u201cchicken or the egg\u201d paradox: \u00a0On one hand, a certain abstraction, workflow, or tool is hypothesized to be a good fit. On the other hand, \u00a0there are often considerable integration efforts even before feasibility testing can commence.\n\n[https://app.aiplus.training/courses/chicken-or-the-egg-in-mlops-the-case-of-the-diy-feature-store](https://app.aiplus.training/courses/chicken-or-the-egg-in-mlops-the-case-of-the-diy-feature-store)", "link": "https://www.reddit.com/r/datascience/comments/oxs4ge/chicken_or_the_egg_in_mlops_the_case_of_the_diy/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "\u201cchicken or the egg\u201d in mlops: the case of the diy feature store /!/ date: 11th august 2021 time: 1 pm edt (gmt-4)\n\n while developing mlops in an organization, one often encounters the \u201cchicken or the egg\u201d paradox: \u00a0on one hand, a certain abstraction, workflow, or -----> tool !!!  is hypothesized to be a good fit. on the other hand, \u00a0there are often considerable integration efforts even before feasibility testing can commence.\n\n[https://app.aiplus.training/courses/chicken-or-the-egg-in-mlops-the-case-of-the-diy-feature-store](https://app.aiplus.training/courses/chicken-or-the-egg-in-mlops-the-case-of-the-diy-feature-store)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/oxs4ge/chicken_or_the_egg_in_mlops_the_case_of_the_diy/',)", "identifyer": 5598219, "year": "2021"}, {"autor": "Spicey_Boii", "date": 1632248301000, "content": "Using Google sheets as the db for a simple searchable page /!/ If I have a sheet filled with some information about some universities and I want to update that but I want that searchable by people is there a good way of building a simple tool to pull and show what is on the sheet and allows people to search it but doesn't give access to the sheet itself?  \n\n\nI want the sheet to still be updatable and then populate to the searchable sheet/page but I'm not sure how to connect the two.", "link": "https://www.reddit.com/r/datascience/comments/psoldl/using_google_sheets_as_the_db_for_a_simple/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "using google sheets as the db for a simple searchable page /!/ if i have a sheet filled with some information about some universities and i want to update that but i want that searchable by people is there a good way of building a simple -----> tool !!!  to pull and show what is on the sheet and allows people to search it but doesn't give access to the sheet itself?  \n\n\ni want the sheet to still be updatable and then populate to the searchable sheet/page but i'm not sure how to connect the two.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 8, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/psoldl/using_google_sheets_as_the_db_for_a_simple/',)", "identifyer": 5598376, "year": "2021"}, {"autor": "Omega037", "date": 1629827823000, "content": "How do you share/track knowledge derived from EDA and/or Modeling at a large company? /!/ As the title says, I was wondering **what process/tools people are using to record and share knowledge discovered while doing data science** with a larger team(s) and even non-technical domain experts.\n\nWe do have tools in place to allow us to track projects and version control our code/data (although they aren't used as often as they should), **this is more a question about how you store, share, and update results/knowledge.**\n\nUnfortunately, the result of **most analytical work at my large company seems to either disappear with time** **as a person leaves a role** or exist in a stale, undiscoverable, high-level Powerpoint that gets passed around.  \n\nAt the moment I have gotten some good formatting/automation mileage out of using the tagging system in JupyterLab along with nbconvert (to HTML/Markdown/PDF), then storing in Github, but **this seems to mostly work only at the individual person/project level.**\n\nThe only tool I have seen try to approach this problem is AirBnB's knowledge-repo and some companies selling archaic Knowledge Base services.\n\nAny ideas?", "link": "https://www.reddit.com/r/datascience/comments/pasj8t/how_do_you_sharetrack_knowledge_derived_from_eda/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "how do you share/track knowledge derived from eda and/or modeling at a large company? /!/ as the title says, i was wondering **what process/tools people are using to record and share knowledge discovered while doing data science** with a larger team(s) and even non-technical domain experts.\n\nwe do have tools in place to allow us to track projects and version control our code/data (although they aren't used as often as they should), **this is more a question about how you store, share, and update results/knowledge.**\n\nunfortunately, the result of **most analytical work at my large company seems to either disappear with time** **as a person leaves a role** or exist in a stale, undiscoverable, high-level powerpoint that gets passed around.  \n\nat the moment i have gotten some good formatting/automation mileage out of using the tagging system in jupyterlab along with nbconvert (to html/markdown/pdf), then storing in github, but **this seems to mostly work only at the individual person/project level.**\n\nthe only -----> tool !!!  i have seen try to approach this problem is airbnb's knowledge-repo and some companies selling archaic knowledge base services.\n\nany ideas?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 12, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pasj8t/how_do_you_sharetrack_knowledge_derived_from_eda/',)", "identifyer": 5598396, "year": "2021"}, {"autor": "guitarnoob98", "date": 1629796655000, "content": "Would you advise learning how to use Power BI? Is this easier to do on a windows laptop? /!/ Hello everyone, \n\nI should hopefully be starting to train as a data analyst in the next month or so and I\u2019m curious whether I will need to purchase a new laptop for this. \n\nAs far as I\u2019m aware the preferred BI tool is Power BI, unfortunately though I have a Mac which Microsoft have explicitly stated they would not be releasing PowerBI on. \n\nHas anyone had any experience implementing Power BI on a Mac, is it worth trying to do this? \n\nAs a data analyst is your best bet having a windows laptop to ensure you have access to as many of the popular and industry standard softwares? \n\nI\u2019m willing to purchase a new Windows laptop if this makes things easier for me, but wanted some advice from people in the industry.", "link": "https://www.reddit.com/r/datascience/comments/pajqxv/would_you_advise_learning_how_to_use_power_bi_is/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "would you advise learning how to use power bi? is this easier to do on a windows laptop? /!/ hello everyone, \n\ni should hopefully be starting to train as a data analyst in the next month or so and i\u2019m curious whether i will need to purchase a new laptop for this. \n\nas far as i\u2019m aware the preferred bi -----> tool !!!  is power bi, unfortunately though i have a mac which microsoft have explicitly stated they would not be releasing powerbi on. \n\nhas anyone had any experience implementing power bi on a mac, is it worth trying to do this? \n\nas a data analyst is your best bet having a windows laptop to ensure you have access to as many of the popular and industry standard softwares? \n\ni\u2019m willing to purchase a new windows laptop if this makes things easier for me, but wanted some advice from people in the industry.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 22, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pajqxv/would_you_advise_learning_how_to_use_power_bi_is/',)", "identifyer": 5598412, "year": "2021"}, {"autor": "Shclopey", "date": 1629757511000, "content": "Hi there! College undergrad looking for some guidance/direction. /!/ Currently browsing for my first internship and I want to make sure I\u2019m applying for roles that fit my likes and skills best.\n\nBasically what I\u2019m asking for is: Am I better off pursuing being a data engineer? A data analyst? A Data scientist? Etc etc. \n\nWhat I like (in general): \n\n1.) I love being able to translate data and the story it tells to people, whether it be through visuals, verbal communication, text, whatever it is.\n\n2.) Tho I am not completely fluent in any particular programming language, I have a basic understanding and have taken courses for a variety of languages: Python, R, C++, SAS. I\u2019d say I have the most experience with R since I just finished a linear regression course  that involved a ton of  R. \n\nI do enjoy coding, but to an extent. I love using it more as a tool rather than it being my sole job to come up with entire programs and such if that makes sense? \n\n3.) Recognizing trends and patterns.\n\n4.) Calculations. I know this is very broad but thought I\u2019d include it just in case. I just get this great satisfaction when I solve a problem using the tools and resources at my disposable.\n\n\nSorry if that is all too broad to work with, but thought I\u2019d give it a chance. There are other things as well as things I don\u2019t like, but I don\u2019t want to drag this post for too long. Thanks!", "link": "https://www.reddit.com/r/datascience/comments/paac2i/hi_there_college_undergrad_looking_for_some/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "hi there! college undergrad looking for some guidance/direction. /!/ currently browsing for my first internship and i want to make sure i\u2019m applying for roles that fit my likes and skills best.\n\nbasically what i\u2019m asking for is: am i better off pursuing being a data engineer? a data analyst? a data scientist? etc etc. \n\nwhat i like (in general): \n\n1.) i love being able to translate data and the story it tells to people, whether it be through visuals, verbal communication, text, whatever it is.\n\n2.) tho i am not completely fluent in any particular programming language, i have a basic understanding and have taken courses for a variety of languages: python, r, c++, sas. i\u2019d say i have the most experience with r since i just finished a linear regression course  that involved a ton of  r. \n\ni do enjoy coding, but to an extent. i love using it more as a -----> tool !!!  rather than it being my sole job to come up with entire programs and such if that makes sense? \n\n3.) recognizing trends and patterns.\n\n4.) calculations. i know this is very broad but thought i\u2019d include it just in case. i just get this great satisfaction when i solve a problem using the tools and resources at my disposable.\n\n\nsorry if that is all too broad to work with, but thought i\u2019d give it a chance. there are other things as well as things i don\u2019t like, but i don\u2019t want to drag this post for too long. thanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/paac2i/hi_there_college_undergrad_looking_for_some/',)", "identifyer": 5598434, "year": "2021"}, {"autor": "MK_statistics", "date": 1631020494000, "content": "Data science applications in medical research (with online tool and video abstract) /!/ [removed]", "link": "https://www.reddit.com/r/datascience/comments/pjmu5k/data_science_applications_in_medical_research/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "data science applications in medical research (with online -----> tool !!!  and video abstract) /!/ [removed]", "sortedWord": "None", "removed": "('reddit',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pjmu5k/data_science_applications_in_medical_research/',)", "identifyer": 5598508, "year": "2021"}, {"autor": "KarlaNour96", "date": 1618580192000, "content": "[R] Ubiai survey /!/ Hi everyone,\n\nWe are looking on a new text annotation tool for Natural Language Processing ( NLP) application and we are conducting a survey to get feedback from NLP practitioners regarding  the text annotation process.\nIf you have a moment, can you please help us answering few questions ( survey time 1 min ) \n\n https://docs.google.com/forms/d/e/1FAIpQLSd61v-ohbQn8QB_cAFQ0tEFFFwircLbwHps6TZnQuWUJQgZSA/viewform?usp=sf_link\n\nYour help will be greatly appreciated ! \n\nUBIAI team\n\nhttps://ubiai.tools", "link": "https://www.reddit.com/r/datascience/comments/ms3al2/r_ubiai_survey/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[r] ubiai survey /!/ hi everyone,\n\nwe are looking on a new text annotation -----> tool !!!  for natural language processing ( nlp) application and we are conducting a survey to get feedback from nlp practitioners regarding  the text annotation process.\nif you have a moment, can you please help us answering few questions ( survey time 1 min ) \n\n https://docs.google.com/forms/d/e/1faipqlsd61v-ohbqn8qb_cafq0tefffwirclbwhps6tznquwujqgzsa/viewform?usp=sf_link\n\nyour help will be greatly appreciated ! \n\nubiai team\n\nhttps://ubiai.tools", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/ms3al2/r_ubiai_survey/',)", "identifyer": 5598553, "year": "2021"}, {"autor": "0bruins", "date": 1631763355000, "content": "When does the job get interesting? /!/ I started a data analyst role 2 months ago. It's my first real job following grad school. The company is fairly technical (relative to its industry and typical DA roles) and flexible (use whatever tool to solve the problem; Excel/Python/R/etc.) which I appreciate; however my team does use Excel a lot and there is a lot of messy Python/R code. \n\nSo far I've been doing small ad-hoc tasks and monitoring automated reports. None of this feels impactful/memorable/challenging/interesting. I understand I have to work my way up to bigger responsibility, but it seems like an incredibly slow process. I'd say I only spend  &lt;50% of my day actually working, and that's sufficient to get what I need done. The other 50% is spent sitting around my computer on Slack, occasionally brainstorming new processes to automate.\n\nShould I be grateful for the slow pace (while it lasts) or should I keep pressing for more responsibility? I value growing a lot at my first job.", "link": "https://www.reddit.com/r/datascience/comments/pp5lbn/when_does_the_job_get_interesting/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "when does the job get interesting? /!/ i started a data analyst role 2 months ago. it's my first real job following grad school. the company is fairly technical (relative to its industry and typical da roles) and flexible (use whatever -----> tool !!!  to solve the problem; excel/python/r/etc.) which i appreciate; however my team does use excel a lot and there is a lot of messy python/r code. \n\nso far i've been doing small ad-hoc tasks and monitoring automated reports. none of this feels impactful/memorable/challenging/interesting. i understand i have to work my way up to bigger responsibility, but it seems like an incredibly slow process. i'd say i only spend  &lt;50% of my day actually working, and that's sufficient to get what i need done. the other 50% is spent sitting around my computer on slack, occasionally brainstorming new processes to automate.\n\nshould i be grateful for the slow pace (while it lasts) or should i keep pressing for more responsibility? i value growing a lot at my first job.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 24, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pp5lbn/when_does_the_job_get_interesting/',)", "identifyer": 5598571, "year": "2021"}, {"autor": "Mysterious_Purple_31", "date": 1631742762000, "content": "How do you decide what tool to use, and how do you learn about new tools? /!/ If you had a new data project to work on, how do you decide what tool to use?\n\nAnd how do you learn about new tools to try?", "link": "https://www.reddit.com/r/datascience/comments/pozvom/how_do_you_decide_what_tool_to_use_and_how_do_you/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "how do you decide what -----> tool !!!  to use, and how do you learn about new tools? /!/ if you had a new data project to work on, how do you decide what tool to use?\n\nand how do you learn about new tools to try?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pozvom/how_do_you_decide_what_tool_to_use_and_how_do_you/',)", "identifyer": 5598581, "year": "2021"}, {"autor": "mfinpi", "date": 1633218830000, "content": "How close is this to a \"real\" Data Science job? /!/ I've recently started a position in marketing science working with MMM and was wondering how close to reality it is to the typical data science job. Here's basically a rundown of what a typical project looks like and what I do:\n\n1. Gather data from clients and our SQL database and format it using my company's in-house ETL tool (basically an Alteryx ripoff). I try to use as much Python as possible out of boredom and preparation for my next role though. Not the most interesting work but surprisingly found it a bit more fun than I thought.\n2. After all the required data are in their proper forms to fit it into the modeling software used, we run it and make sure it looks sensible and then a bunch of different stuff is interpreted like R squared and t-stats to make up some BS which the clients will want to hear. We mostly only do regression analysis and sometimes time series for forecasting so I'm not sure how hard this will lock me out of other jobs which use ML and other statistical techniques.\n3. Finally, prepare Power Point presentations with nice visuals and dashboards and tell them what they want to hear without going too far off from the reality that we see from the results of our models.\n\nThis is my first actual analytics job after graduation so I just want to have a reality check from people who've been in the field. Is this a good role to have to go farther in the data space or a lot different from what to expect going forwards? Will make me competent for other jobs as a data analyst or scientist? Thanks!", "link": "https://www.reddit.com/r/datascience/comments/q06cy2/how_close_is_this_to_a_real_data_science_job/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "how close is this to a \"real\" data science job? /!/ i've recently started a position in marketing science working with mmm and was wondering how close to reality it is to the typical data science job. here's basically a rundown of what a typical project looks like and what i do:\n\n1. gather data from clients and our sql database and format it using my company's in-house etl -----> tool !!!  (basically an alteryx ripoff). i try to use as much python as possible out of boredom and preparation for my next role though. not the most interesting work but surprisingly found it a bit more fun than i thought.\n2. after all the required data are in their proper forms to fit it into the modeling software used, we run it and make sure it looks sensible and then a bunch of different stuff is interpreted like r squared and t-stats to make up some bs which the clients will want to hear. we mostly only do regression analysis and sometimes time series for forecasting so i'm not sure how hard this will lock me out of other jobs which use ml and other statistical techniques.\n3. finally, prepare power point presentations with nice visuals and dashboards and tell them what they want to hear without going too far off from the reality that we see from the results of our models.\n\nthis is my first actual analytics job after graduation so i just want to have a reality check from people who've been in the field. is this a good role to have to go farther in the data space or a lot different from what to expect going forwards? will make me competent for other jobs as a data analyst or scientist? thanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/q06cy2/how_close_is_this_to_a_real_data_science_job/',)", "identifyer": 5598620, "year": "2021"}, {"autor": "bong26", "date": 1633218828000, "content": "Does anyone have any experience implementing an instant valuation model? /!/ https://en.m.wikipedia.org/wiki/Automated_valuation_model\n\nI'm a web developer and I'm building an instant valuation tool for valuaing houses. I presumed there would some sort of formula i could just implement but I can't find anything. Can someone point me in the right direction?", "link": "https://www.reddit.com/r/datascience/comments/q06cxg/does_anyone_have_any_experience_implementing_an/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "does anyone have any experience implementing an instant valuation model? /!/ https://en.m.wikipedia.org/wiki/automated_valuation_model\n\ni'm a web developer and i'm building an instant valuation -----> tool !!!  for valuaing houses. i presumed there would some sort of formula i could just implement but i can't find anything. can someone point me in the right direction?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/q06cxg/does_anyone_have_any_experience_implementing_an/',)", "identifyer": 5598621, "year": "2021"}, {"autor": "huntjb", "date": 1633203838000, "content": "How do I transition from a PhD to a career in data science? /!/ I am 3.5 years into my PhD in neuroscience and I\u2019ve recently decided I don\u2019t want to pursue an academic career. I don\u2019t enjoy the grant writing and data collection, but I really enjoy the data analytics and coding components of my research. Given these interests, I\u2019ve started exploring the possibility of applying for data scientist positions after I complete my PhD. I\u2019m a little overwhelmed and I\u2019m not really sure where to start. I reached out to an alumni of my program and he referred me to a friend of his who works as a data scientist. We met virtually and the gist of what he said is I need to find a position as a junior- or mid-level data scientist with a really supportive team that can \u201cfill in the gaps\u201d in my knowledge. My question is, how do I identify positions like this? What keywords or features of job listings am I looking for, and what platforms should I use to look for jobs? I\u2019ve mostly been using LinkedIn for networking and job searching, but like I said I\u2019m not sure what I\u2019m looking for. Any advice would be much appreciated. I would also like to know what kinds of skills are a prerequisite for an entry- or mid-level data scientist. I have ~5 years experience programming with Python and formal training applying inferential statistics, but is that enough to land a job? Are there skills I have as a researcher that I am overlooking/undervaluing, and are there skills that recruiters/interviewers value that I could learn over the next couple years before I start applying for jobs in earnest? I see SQL appear frequently on job posts. Would it benefit me to dedicate some time to learning how to use SQL or some other relational database tool?", "link": "https://www.reddit.com/r/datascience/comments/q01z4t/how_do_i_transition_from_a_phd_to_a_career_in/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "how do i transition from a phd to a career in data science? /!/ i am 3.5 years into my phd in neuroscience and i\u2019ve recently decided i don\u2019t want to pursue an academic career. i don\u2019t enjoy the grant writing and data collection, but i really enjoy the data analytics and coding components of my research. given these interests, i\u2019ve started exploring the possibility of applying for data scientist positions after i complete my phd. i\u2019m a little overwhelmed and i\u2019m not really sure where to start. i reached out to an alumni of my program and he referred me to a friend of his who works as a data scientist. we met virtually and the gist of what he said is i need to find a position as a junior- or mid-level data scientist with a really supportive team that can \u201cfill in the gaps\u201d in my knowledge. my question is, how do i identify positions like this? what keywords or features of job listings am i looking for, and what platforms should i use to look for jobs? i\u2019ve mostly been using linkedin for networking and job searching, but like i said i\u2019m not sure what i\u2019m looking for. any advice would be much appreciated. i would also like to know what kinds of skills are a prerequisite for an entry- or mid-level data scientist. i have ~5 years experience programming with python and formal training applying inferential statistics, but is that enough to land a job? are there skills i have as a researcher that i am overlooking/undervaluing, and are there skills that recruiters/interviewers value that i could learn over the next couple years before i start applying for jobs in earnest? i see sql appear frequently on job posts. would it benefit me to dedicate some time to learning how to use sql or some other relational database -----> tool !!! ?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 15, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/q01z4t/how_do_i_transition_from_a_phd_to_a_career_in/',)", "identifyer": 5598624, "year": "2021"}, {"autor": "NFerY", "date": 1633195785000, "content": "Tool XYZ has ML: Why do we need data scientists? /!/ You are a data scientist in a large company. Someone approaches you and says:  \n\"*ACME BI Solution Platform now comes with ML capabilities out of the box. Why do we need Data Scientists?!*\"\n\nRespond and elaborate - constructively ;-)", "link": "https://www.reddit.com/r/datascience/comments/pzzj31/tool_xyz_has_ml_why_do_we_need_data_scientists/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "-----> tool !!!  xyz has ml: why do we need data scientists? /!/ you are a data scientist in a large company. someone approaches you and says:  \n\"*acme bi solution platform now comes with ml capabilities out of the box. why do we need data scientists?!*\"\n\nrespond and elaborate - constructively ;-)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pzzj31/tool_xyz_has_ml_why_do_we_need_data_scientists/',)", "identifyer": 5598633, "year": "2021"}, {"autor": "Sam_Oppar", "date": 1634810177000, "content": "BI Tool Recommendations? /!/ I am looking for a free/open source BI tool. Power BI desktop is not an option as it's not available for MacOS. Also don't want to use Tableau public because my data can't be public.\n\nRequirements:\n1. Be able to read data using python script (pandas output). If not at least should be able to query from database. \n\n2. Interactive dashboard/charts.", "link": "https://www.reddit.com/r/datascience/comments/qcoax2/bi_tool_recommendations/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "bi -----> tool !!!  recommendations? /!/ i am looking for a free/open source bi tool. power bi desktop is not an option as it's not available for macos. also don't want to use tableau public because my data can't be public.\n\nrequirements:\n1. be able to read data using python script (pandas output). if not at least should be able to query from database. \n\n2. interactive dashboard/charts.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 16, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/qcoax2/bi_tool_recommendations/',)", "identifyer": 5598712, "year": "2021"}, {"autor": "AbdulWahabAbrar", "date": 1629145392000, "content": "I need help in deciding what visualization tool to use", "link": "https://www.reddit.com/r/datascience/comments/p5oae2/i_need_help_in_deciding_what_visualization_tool/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "i need help in deciding what visualization -----> tool !!!  to use", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('image',)", "medialink": "('https://i.redd.it/wto3fzfq3sh71.jpg',)", "identifyer": 5598807, "year": "2021"}, {"autor": "Cultured_dude", "date": 1627870931000, "content": "DS Project Tracking Tool/App /!/ Does anyone recommend an app/tool to track/manage DS projects? I'm also curious as to how you ensured adoption. \n\nMy team is comprised of 12 data scientists who work on a range of topics, e.g. analytics, experimental design, and ML. We're scattered throughout the U.S.\n\nWe're looking for a simple solution.\n\nThe goal of the solution is twofold:\n\n1. Allow our supervisor to stay informed on our progress.\n2. Serve as a catalog so the team knows what others are working on, so we can leverage each others' work. There is a lot of overlap.\n\nThe company uses Microsoft Teams; therefore, the solution may be a more effective use of the app.\n\nIn my prior career in finance, my team/I used a shared Excel file.\n\nOne problem with Excel is that our project scopes are not consistent. They can be a bit nebulous; consequently, a \"2-D grid approach\" presents challenges.\n\nOne friend has recommended Asana.", "link": "https://www.reddit.com/r/datascience/comments/ow5kts/ds_project_tracking_toolapp/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "ds project tracking -----> tool !!! /app /!/ does anyone recommend an app/tool to track/manage ds projects? i'm also curious as to how you ensured adoption. \n\nmy team is comprised of 12 data scientists who work on a range of topics, e.g. analytics, experimental design, and ml. we're scattered throughout the u.s.\n\nwe're looking for a simple solution.\n\nthe goal of the solution is twofold:\n\n1. allow our supervisor to stay informed on our progress.\n2. serve as a catalog so the team knows what others are working on, so we can leverage each others' work. there is a lot of overlap.\n\nthe company uses microsoft teams; therefore, the solution may be a more effective use of the app.\n\nin my prior career in finance, my team/i used a shared excel file.\n\none problem with excel is that our project scopes are not consistent. they can be a bit nebulous; consequently, a \"2-d grid approach\" presents challenges.\n\none friend has recommended asana.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/ow5kts/ds_project_tracking_toolapp/',)", "identifyer": 5598875, "year": "2021"}, {"autor": "diady11", "date": 1629617546000, "content": "I am excited to share with you a new tool that help me to track of all my marketing activities .It really save my time ! It called", "link": "https://www.reddit.com/r/datascience/comments/p9895z/i_am_excited_to_share_with_you_a_new_tool_that/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "i am excited to share with you a new -----> tool !!!  that help me to track of all my marketing activities .it really save my time ! it called", "sortedWord": "None", "removed": "('reddit',)", "score": 1, "comments": 0, "media": "('link',)", "medialink": "('https://www.gaannotations.com',)", "identifyer": 5598891, "year": "2021"}, {"autor": "Enterthedragon12", "date": 1629577535000, "content": "Calculating time to resolve issues. /!/ Wondering if anyone has any ideas that might work better than my current method.  I\u2019m extracting data from the project management tool Jira. Requests hit our kanban board and are given a Received date/time. Then once the requests are marked Done, they are given a Resolved date.  Our goal is to complete requests within 3 days.   So a resolution time of 3.00 days would be at goal.   I\u2019ve currently been using Excel\u2019s NetWorkday formula to calculate the difference since it pulls out weekends and holidays so easy.  But people complain that the formula counts the day that the request is received as day 1.  So if a request is received on Monday and resolved within 2 hours, it still gives a 1 day resolution time.   If it\u2019s received on Monday at 4pm and completed on Thursday at 8am, it will count it as 4 days even though that\u2019s less than 36 hours (3 days).   \n\nIs there an easier way to calculate the difference between start and finish that doesn\u2019t require me to manually pull out weekends and holidays?   Or should I just simply use the NetWorkday formula and then subtract 1 from the end result to make up for the formula\u2019s flaw?    I\u2019d really like to calculate based on time stamp though so it\u2019s more about the hours it took (and then divide the hours by 24) to be more precise.  But it seems like the holidays and weekends would be tough to pull out.", "link": "https://www.reddit.com/r/datascience/comments/p8ymfh/calculating_time_to_resolve_issues/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "calculating time to resolve issues. /!/ wondering if anyone has any ideas that might work better than my current method.  i\u2019m extracting data from the project management -----> tool !!!  jira. requests hit our kanban board and are given a received date/time. then once the requests are marked done, they are given a resolved date.  our goal is to complete requests within 3 days.   so a resolution time of 3.00 days would be at goal.   i\u2019ve currently been using excel\u2019s networkday formula to calculate the difference since it pulls out weekends and holidays so easy.  but people complain that the formula counts the day that the request is received as day 1.  so if a request is received on monday and resolved within 2 hours, it still gives a 1 day resolution time.   if it\u2019s received on monday at 4pm and completed on thursday at 8am, it will count it as 4 days even though that\u2019s less than 36 hours (3 days).   \n\nis there an easier way to calculate the difference between start and finish that doesn\u2019t require me to manually pull out weekends and holidays?   or should i just simply use the networkday formula and then subtract 1 from the end result to make up for the formula\u2019s flaw?    i\u2019d really like to calculate based on time stamp though so it\u2019s more about the hours it took (and then divide the hours by 24) to be more precise.  but it seems like the holidays and weekends would be tough to pull out.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/p8ymfh/calculating_time_to_resolve_issues/',)", "identifyer": 5598895, "year": "2021"}, {"autor": "botBuilder64", "date": 1619122158000, "content": "Database Analytics Tools /!/ Is there an open souced or free tool for data analytics?  I am looking to have some tool similar to Qlik Sense which I can view my database information in an organized manner for reporting.", "link": "https://www.reddit.com/r/datascience/comments/mwdjpz/database_analytics_tools/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "database analytics tools /!/ is there an open souced or free -----> tool !!!  for data analytics?  i am looking to have some tool similar to qlik sense which i can view my database information in an organized manner for reporting.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/mwdjpz/database_analytics_tools/',)", "identifyer": 5598949, "year": "2021"}, {"autor": "ricvolpe", "date": 1619105281000, "content": "What is the best data analytics product that you have ever used? /!/ What is the product or tool that analysed or summarised your data in a way that provided you with a memorable user experience (eg very useful, pleasant, easy to use)?\n\nWhy was that?\n\nI am interested in *products that analysed your data for you*, such as Google Analytics for data of your website, Apple Health for data about your sleep, Apple Screen Time for data about your device usage, etc.\n\nI am not interested in products or tools that allowed you to analyse your data, like MS Excel.", "link": "https://www.reddit.com/r/datascience/comments/mw79sm/what_is_the_best_data_analytics_product_that_you/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what is the best data analytics product that you have ever used? /!/ what is the product or -----> tool !!!  that analysed or summarised your data in a way that provided you with a memorable user experience (eg very useful, pleasant, easy to use)?\n\nwhy was that?\n\ni am interested in *products that analysed your data for you*, such as google analytics for data of your website, apple health for data about your sleep, apple screen time for data about your device usage, etc.\n\ni am not interested in products or tools that allowed you to analyse your data, like ms excel.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 5, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/mw79sm/what_is_the_best_data_analytics_product_that_you/',)", "identifyer": 5598962, "year": "2021"}], "name": "tooldatascience2021"}
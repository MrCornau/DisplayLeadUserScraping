{"interestingcomments": [{"autor": "ctk_brian", "date": 1625063057000, "content": "How to plot survival curves with Plotly and Altair /!/ [https://www.crosstab.io/articles/survival-plots](https://www.crosstab.io/articles/survival-plots)\n\nLifelines, Convoys, and Scikit-survival do a good job of showing how to draw survival curves in Python, but they focus exclusively on Matplotlib, sometimes to the point of hard-coding Matplotlib-based convenience functions.\n\nThis tutorial shows how to draw survival curves with Altair and Plotly (Python) based on a Lifelines Kaplan-Meier model.\n\nLet me know what you think!", "link": "https://www.reddit.com/r/datascience/comments/oaydto/how_to_plot_survival_curves_with_plotly_and_altair/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "how to -----> plot !!!  survival curves with plotly and altair /!/ [https://www.crosstab.io/articles/survival-plots](https://www.crosstab.io/articles/survival-plots)\n\nlifelines, convoys, and scikit-survival do a good job of showing how to draw survival curves in python, but they focus exclusively on matplotlib, sometimes to the point of hard-coding matplotlib-based convenience functions.\n\nthis tutorial shows how to draw survival curves with altair and plotly (python) based on a lifelines kaplan-meier model.\n\nlet me know what you think!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/oaydto/how_to_plot_survival_curves_with_plotly_and_altair/',)", "identifyer": 5586818, "year": "2021"}, {"autor": "Plus-Rent-8990", "date": 1617136221000, "content": "Is Matplotlib Confusing? /!/ \ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc0d Does anyone else find #Matplotlib's API hard to remember? I have friends who just export their #Pandas data and plot with #Excel. I spend a lot of time googling #Matplotlib help. How do you make your #Python #Data #Plots?", "link": "https://www.reddit.com/r/datascience/comments/mgp9ak/is_matplotlib_confusing/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "is matplotlib confusing? /!/ \ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc0d does anyone else find #matplotlib's api hard to remember? i have friends who just export their #pandas data and -----> plot !!!  with #excel. i spend a lot of time googling #matplotlib help. how do you make your #python #data #plots?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/mgp9ak/is_matplotlib_confusing/',)", "identifyer": 5586936, "year": "2021"}, {"autor": "utkarshshukla2912", "date": 1627819067000, "content": "Outlier Detection in Heavy Tailed Distribution /!/ From time to time I find myself trying to figure out outliers in a heavy tailed distribution. There are some basic methods like adjusted box plot or DBSCAN, and sometimes I have enough domain context to remove outliers but are some the best way to remove/detect these outliers. Have been struggling for a while now", "link": "https://www.reddit.com/r/datascience/comments/ovqtxx/outlier_detection_in_heavy_tailed_distribution/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "outlier detection in heavy tailed distribution /!/ from time to time i find myself trying to figure out outliers in a heavy tailed distribution. there are some basic methods like adjusted box -----> plot !!!  or dbscan, and sometimes i have enough domain context to remove outliers but are some the best way to remove/detect these outliers. have been struggling for a while now", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/ovqtxx/outlier_detection_in_heavy_tailed_distribution/',)", "identifyer": 5587054, "year": "2021"}, {"autor": "TypicalQueryMan", "date": 1632927496000, "content": "How do you go about creating a real-time graph with Plotly? /!/ Hello everyone,\n\nI have a project which requires me to regularly update my line plot, made with Plotly, after each experiment. It's an objective function v Number of experiments graph. \n\nThe project entails an experimental design input(after taking a random number of parameters) and using a separate function to obtain the 'y-value' which is then optimised with a GPEI. The function that presents the 'y-value' is called over and over again till convergence is hit. \n\nI can't wait till the final trial, and I need to update the graph post each trial.\n\nI have absolutely no idea how to update the values after every trial. Can someone help me?", "link": "https://www.reddit.com/r/datascience/comments/pxx4id/how_do_you_go_about_creating_a_realtime_graph/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "how do you go about creating a real-time graph with plotly? /!/ hello everyone,\n\ni have a project which requires me to regularly update my line -----> plot !!! , made with plotly, after each experiment. it's an objective function v number of experiments graph. \n\nthe project entails an experimental design input(after taking a random number of parameters) and using a separate function to obtain the 'y-value' which is then optimised with a gpei. the function that presents the 'y-value' is called over and over again till convergence is hit. \n\ni can't wait till the final trial, and i need to update the graph post each trial.\n\ni have absolutely no idea how to update the values after every trial. can someone help me?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 11, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pxx4id/how_do_you_go_about_creating_a_realtime_graph/',)", "identifyer": 5587234, "year": "2021"}, {"autor": "Gabyto", "date": 1619892872000, "content": "Adding an incremental counter based in two groups on a data frame? (R) /!/ Hello everyone!\n\nBackground:\n\nI'm analyzing the UCI HAR dataset [link here](https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones).\n\nThe dataset consists of multiple x-y-z points of data taken from the accelerometer and gyroscope instruments of a smartphone for 30 different subjects while doing different activities (walking, standing, etc).\n\nMy goal:\n\nI'm looking to extract a tidy dataset from that mess and try to get a tableau dashboard or plot.\n\nObviously, I need to do this in a time series plot since each of these points of data is taken on a set time interval (let's say 1 second).\n\nUnfortunately, tableau won't let me plot this in any way unless I give it a \"time series\" to anchor to the X-axis. \n\nMy idea is to create a new column in this dataset ('time') which increases in value by 1 for each observation on the respective subgroup (Subject ID '1' - Activity 'Walking', etc) \n\nMy current dataset looks something like this:\n\n&amp;#x200B;\n\n|Subject ID|Activity |BodyAcc-X|BodyAcc-Y|BodyAcc-Z|\n|:-|:-|:-|:-|:-|\n|1|Walking|0.2|0.2|0.2|\n|1|Walking|0.3|0.3|0.3|\n|1|Standing|0.5|0.5|0.5|\n|1|Standing|0.6|0.6|0.6|\n|2|Walking|0.3|0.3|0.3|\n|2|Walking|0.2|0.2|0.2|\n|2|Standing|0.1|0.1|0.1|\n|2|Standing|0.4|0.4|0.4|\n\nMy goal is to create something that looks like this so that I can plot it in tableau:\n\n&amp;#x200B;\n\n|Subject ID|Activity |Time|BodyAcc-X|BodyAcc-Y|BodyAcc-Z|\n|:-|:-|:-|:-|:-|:-|\n|1|Walking|1|0.2|0.2|0.2|\n|1|Walking|2|0.3|0.3|0.3|\n|1|Standing|1|0.5|0.5|0.5|\n|1|Standing|2|0.6|0.6|0.6|\n|2|Walking|1|0.3|0.3|0.3|\n|2|Walking|2|0.2|0.2|0.2|\n|2|Walking|3|0.2|0.2|3|\n|2|Standing|1|0.1|0.1|0.1|\n|2|Standing|2|0.4|0.4|0.4|\n|2|Standing|3|0.4|0.4|3|\n|2|Standing|4|0.4|0.4|0.4|\n\n&amp;#x200B;\n\nI grouped SubjectId and activity, and ordered it respectively, so my dataset looks great, I'm just lacking the time stamp for me to be able to plot it.\n\nI was able to do this but only counting towards either the subject Id, or the activity, but not both.\n\nI want the counter to restart for each subject and activity (I'm not sure how to phrase this, I hope it's clear)\n\nThis is my first time posting here and I'm new to R, so if I'm missing anything or if I can improve my question in anyway please let me know,\n\n\nThank you all!", "link": "https://www.reddit.com/r/datascience/comments/n2nx12/adding_an_incremental_counter_based_in_two_groups/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "adding an incremental counter based in two groups on a data frame? (r) /!/ hello everyone!\n\nbackground:\n\ni'm analyzing the uci har dataset [link here](https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones).\n\nthe dataset consists of multiple x-y-z points of data taken from the accelerometer and gyroscope instruments of a smartphone for 30 different subjects while doing different activities (walking, standing, etc).\n\nmy goal:\n\ni'm looking to extract a tidy dataset from that mess and try to get a tableau dashboard or -----> plot !!! .\n\nobviously, i need to do this in a time series plot since each of these points of data is taken on a set time interval (let's say 1 second).\n\nunfortunately, tableau won't let me plot this in any way unless i give it a \"time series\" to anchor to the x-axis. \n\nmy idea is to create a new column in this dataset ('time') which increases in value by 1 for each observation on the respective subgroup (subject id '1' - activity 'walking', etc) \n\nmy current dataset looks something like this:\n\n&amp;#x200b;\n\n|subject id|activity |bodyacc-x|bodyacc-y|bodyacc-z|\n|:-|:-|:-|:-|:-|\n|1|walking|0.2|0.2|0.2|\n|1|walking|0.3|0.3|0.3|\n|1|standing|0.5|0.5|0.5|\n|1|standing|0.6|0.6|0.6|\n|2|walking|0.3|0.3|0.3|\n|2|walking|0.2|0.2|0.2|\n|2|standing|0.1|0.1|0.1|\n|2|standing|0.4|0.4|0.4|\n\nmy goal is to create something that looks like this so that i can plot it in tableau:\n\n&amp;#x200b;\n\n|subject id|activity |time|bodyacc-x|bodyacc-y|bodyacc-z|\n|:-|:-|:-|:-|:-|:-|\n|1|walking|1|0.2|0.2|0.2|\n|1|walking|2|0.3|0.3|0.3|\n|1|standing|1|0.5|0.5|0.5|\n|1|standing|2|0.6|0.6|0.6|\n|2|walking|1|0.3|0.3|0.3|\n|2|walking|2|0.2|0.2|0.2|\n|2|walking|3|0.2|0.2|3|\n|2|standing|1|0.1|0.1|0.1|\n|2|standing|2|0.4|0.4|0.4|\n|2|standing|3|0.4|0.4|3|\n|2|standing|4|0.4|0.4|0.4|\n\n&amp;#x200b;\n\ni grouped subjectid and activity, and ordered it respectively, so my dataset looks great, i'm just lacking the time stamp for me to be able to plot it.\n\ni was able to do this but only counting towards either the subject id, or the activity, but not both.\n\ni want the counter to restart for each subject and activity (i'm not sure how to phrase this, i hope it's clear)\n\nthis is my first time posting here and i'm new to r, so if i'm missing anything or if i can improve my question in anyway please let me know,\n\n\nthank you all!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 5, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/n2nx12/adding_an_incremental_counter_based_in_two_groups/',)", "identifyer": 5587460, "year": "2021"}, {"autor": "In_Robarts_We_Stand", "date": 1619820536000, "content": "Primer for Biological Statistics in RStudio: ANOVA, Regressions, correlations, bar and line graphs /!/ Ecologists, environmental scientists, foresters, horticulturalists, and biologists have two specific requirements for performing statistical analyses. Hereforth is a primer - a simple and hopefully clear distillation of the important components amidst the vastness of the field.\n\n[https://www.youtube.com/watch?v=GDrEwz8r8Xc](https://www.youtube.com/watch?v=GDrEwz8r8Xc)\n\nFirst, a general introduction to probability, assumptions of inference, and hypothesis testing. The second requirement is to perform the analyses, including managing the data using some statistical software.\n\nAt the vanguard of such software is the R statistical software [https://www.r-project.org/;](https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqbWVHOVNHcG95V1ZFcWF5SDdsdXVFUFgwVk9YZ3xBQ3Jtc0ttYUZYVmQzQ1E4MkRaNTFqN2NjNXE4TXNqMW5UZ1BuZGhmZUk3SjUwQ3RFT24tLW5IZkdpczJ6T0t5dWdlU1FaOFJDVngxVUlrYWRnOVREZVhXaHNQaXk0VWN1dHVXWTRRWFBxS1ZKY0RvNTRyTjRlWQ&amp;q=https%3A%2F%2Fwww.r-project.org%2F%3B)\u200b and its integrative environment Rstudio [https://www.rstudio.com/](https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqazB0NjQtTlZvRlp4cDZudWM0dzJfOTQyaEViQXxBQ3Jtc0tsY201bG5pbGltSWhWOVl2YUlvQlNQbk9Delg2bWVHRlluOGNQNUFqUlhqeEVBUEtwcHFNMDRjR2ZsZGV4YzVac0pzV050TzVGbElPdEN3MThyTGFLODFiRlJMOWdESkc5R0NOdW53SGZuMDAtRHBwaw&amp;q=https%3A%2F%2Fwww.rstudio.com%2F)\u200b. Few programs constitute the virtues of open-sources highly integrated and powerful programs as the R statistical software - taught to the modern student in most institutions.\n\nThe following video comprises the time-stamped functions and analyses:\n\n1. Intro to data management, probability, and data presentation ([0:00](https://www.youtube.com/watch?v=GDrEwz8r8Xc&amp;t=0s)\u200b-[13:00](https://www.youtube.com/watch?v=GDrEwz8r8Xc&amp;t=780s)\u200b min)\n2. Intro to producing graphs with R studio ([13:15](https://www.youtube.com/watch?v=GDrEwz8r8Xc&amp;t=795s)\u200b - [17:00](https://www.youtube.com/watch?v=GDrEwz8r8Xc&amp;t=1020s)\u200b min)\n3. Subsetting the data ([23:20](https://www.youtube.com/watch?v=GDrEwz8r8Xc&amp;t=1400s)\u200b)\n4. 'attach' function in R ([23:47](https://www.youtube.com/watch?v=GDrEwz8r8Xc&amp;t=1427s)\u200b min)\n5. 'summarySE' ([24:45](https://www.youtube.com/watch?v=GDrEwz8r8Xc&amp;t=1485s)\u200b min)\n6. Using the plot function ([28:00](https://www.youtube.com/watch?v=GDrEwz8r8Xc&amp;t=1680s)\u200b - [34:00](https://www.youtube.com/watch?v=GDrEwz8r8Xc&amp;t=2040s)\u200b min)\n7. Package 'sciplot' for bar graphs 'bargraph.CI' function ([34:00](https://www.youtube.com/watch?v=GDrEwz8r8Xc&amp;t=2040s)\u200b - [36:00](https://www.youtube.com/watch?v=GDrEwz8r8Xc&amp;t=2160s)\u200b min)\n8. Analysis of Variance including Tukey HSD (ANOVA) ([36:00](https://www.youtube.com/watch?v=GDrEwz8r8Xc&amp;t=2160s)\u200b - [40:00](https://www.youtube.com/watch?v=GDrEwz8r8Xc&amp;t=2400s)\u200b min)\n9. Creating Line Plots and Scatter plot ( [42:00](https://www.youtube.com/watch?v=GDrEwz8r8Xc&amp;t=2520s)\u200b-[45:00](https://www.youtube.com/watch?v=GDrEwz8r8Xc&amp;t=2700s)\u200b min)\n10. Regression analyses via 'lm' function - linear - ([45:00](https://www.youtube.com/watch?v=GDrEwz8r8Xc&amp;t=2700s)\u200b - [48:00](https://www.youtube.com/watch?v=GDrEwz8r8Xc&amp;t=2880s)\u200b mins)\n11. Correlation analysis ([48:00](https://www.youtube.com/watch?v=GDrEwz8r8Xc&amp;t=2880s)\u200b -[51:00](https://www.youtube.com/watch?v=GDrEwz8r8Xc&amp;t=3060s)\u200b mins)\n\nThe video finishes with a summation of what we've learned. If you like this material, then please like and subscribe to this channel.", "link": "https://www.reddit.com/r/datascience/comments/n24oo4/primer_for_biological_statistics_in_rstudio_anova/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "primer for biological statistics in rstudio: anova, regressions, correlations, bar and line graphs /!/ ecologists, environmental scientists, foresters, horticulturalists, and biologists have two specific requirements for performing statistical analyses. hereforth is a primer - a simple and hopefully clear distillation of the important components amidst the vastness of the field.\n\n[https://www.youtube.com/watch?v=gdrewz8r8xc](https://www.youtube.com/watch?v=gdrewz8r8xc)\n\nfirst, a general introduction to probability, assumptions of inference, and hypothesis testing. the second requirement is to perform the analyses, including managing the data using some statistical software.\n\nat the vanguard of such software is the r statistical software [https://www.r-project.org/;](https://www.youtube.com/redirect?event=video_description&amp;redir_token=quffluhqbwvhovnhcg95v1zfcwf5sddsdxvfufgwvk9yz3xbq3jtc0ttyuzyvmqzq1e4mkrantfqn2njnxe4txnqmw5uz1buzghmzuk3sjuwq3rft24tlw5izkdpczj6t0t5dwdlu1faofjdvngxvulrywrnovrezvhxahnqaxk0vwn1dhvxwtrrwfbxs1zky0rvntrytjrlwq&amp;q=https%3a%2f%2fwww.r-project.org%2f%3b)\u200b and its integrative environment rstudio [https://www.rstudio.com/](https://www.youtube.com/redirect?event=video_description&amp;redir_token=quffluhqazb0njqttlzvrlp4cdzudwm0dzjfotqyaeviqxxbq3jtc0tsy201bg5pbgltswhwovl2yulvqlnqbk9delg2bwvhrlluognqnufqulhqeevbuetwchfnmdrjr2zszgv4yzvac0pzv050tzvgbelpden3mthytgflodfirljmowdeskc5r0nodw53sgzumdatrhbwaw&amp;q=https%3a%2f%2fwww.rstudio.com%2f)\u200b. few programs constitute the virtues of open-sources highly integrated and powerful programs as the r statistical software - taught to the modern student in most institutions.\n\nthe following video comprises the time-stamped functions and analyses:\n\n1. intro to data management, probability, and data presentation ([0:00](https://www.youtube.com/watch?v=gdrewz8r8xc&amp;t=0s)\u200b-[13:00](https://www.youtube.com/watch?v=gdrewz8r8xc&amp;t=780s)\u200b min)\n2. intro to producing graphs with r studio ([13:15](https://www.youtube.com/watch?v=gdrewz8r8xc&amp;t=795s)\u200b - [17:00](https://www.youtube.com/watch?v=gdrewz8r8xc&amp;t=1020s)\u200b min)\n3. subsetting the data ([23:20](https://www.youtube.com/watch?v=gdrewz8r8xc&amp;t=1400s)\u200b)\n4. 'attach' function in r ([23:47](https://www.youtube.com/watch?v=gdrewz8r8xc&amp;t=1427s)\u200b min)\n5. 'summaryse' ([24:45](https://www.youtube.com/watch?v=gdrewz8r8xc&amp;t=1485s)\u200b min)\n6. using the -----> plot !!!  function ([28:00](https://www.youtube.com/watch?v=gdrewz8r8xc&amp;t=1680s)\u200b - [34:00](https://www.youtube.com/watch?v=gdrewz8r8xc&amp;t=2040s)\u200b min)\n7. package 'sciplot' for bar graphs 'bargraph.ci' function ([34:00](https://www.youtube.com/watch?v=gdrewz8r8xc&amp;t=2040s)\u200b - [36:00](https://www.youtube.com/watch?v=gdrewz8r8xc&amp;t=2160s)\u200b min)\n8. analysis of variance including tukey hsd (anova) ([36:00](https://www.youtube.com/watch?v=gdrewz8r8xc&amp;t=2160s)\u200b - [40:00](https://www.youtube.com/watch?v=gdrewz8r8xc&amp;t=2400s)\u200b min)\n9. creating line plots and scatter plot ( [42:00](https://www.youtube.com/watch?v=gdrewz8r8xc&amp;t=2520s)\u200b-[45:00](https://www.youtube.com/watch?v=gdrewz8r8xc&amp;t=2700s)\u200b min)\n10. regression analyses via 'lm' function - linear - ([45:00](https://www.youtube.com/watch?v=gdrewz8r8xc&amp;t=2700s)\u200b - [48:00](https://www.youtube.com/watch?v=gdrewz8r8xc&amp;t=2880s)\u200b mins)\n11. correlation analysis ([48:00](https://www.youtube.com/watch?v=gdrewz8r8xc&amp;t=2880s)\u200b -[51:00](https://www.youtube.com/watch?v=gdrewz8r8xc&amp;t=3060s)\u200b mins)\n\nthe video finishes with a summation of what we've learned. if you like this material, then please like and subscribe to this channel.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/n24oo4/primer_for_biological_statistics_in_rstudio_anova/',)", "identifyer": 5587484, "year": "2021"}, {"autor": "EazyStrides", "date": 1619818474000, "content": "What's your approach to developing/iterating on models when you have enormous amount of data? /!/ If you have billions of rows of data sitting on a cluster, and you need to develop a model that will then be used to make billions of predictions. And it's intractable to develop your model (i.e., run experiments, tune hyperparameters, compare models) on all data, because it's too expensive. How would you approach developing a model? \n\nI'm curious if people have principled ways of approaching these kinds of settings. Mine would be:\n\n* Take a stratified random sample of the data. The stratification should respect the distribution of target labels, and any features you consider important. The sample should be small enough so that you can feasibly tune the hyperparameters of models you consider and use cross-validation rather than simple train-test splits.\n* Once you've gone through iterations of feature engineering/model comparison/analysis, and identified your best/candidate model. Re-train on a larger dataset to plot the learning curve and see how important additional data is.\n* Depending on how much your model benefits from additional data, re-train on as large a dataset as possible and use that model in production. \n\nIn terms of tools/libraries, I'd imagine the stratified sampling would be done using Spark. The model development with libraries like scikit-learn and PyTorch. \n\nI realize there are ML frameworks, such as SparkML, that allow you train models on Spark, but I feel like they aren't nearly fleshed out enough to support the iterative workflow described above. However, since these libraries are a lot more efficient, you could train on a lot more data. Thoughts on the tradeoff between more iterations on smaller data vs. fewer iterations on more data?\n\nAnd for inferencing (making predictions), I assume there are ways to deploy scikit-learn/PyTorch models in a Spark environment.", "link": "https://www.reddit.com/r/datascience/comments/n240bf/whats_your_approach_to_developingiterating_on/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "what's your approach to developing/iterating on models when you have enormous amount of data? /!/ if you have billions of rows of data sitting on a cluster, and you need to develop a model that will then be used to make billions of predictions. and it's intractable to develop your model (i.e., run experiments, tune hyperparameters, compare models) on all data, because it's too expensive. how would you approach developing a model? \n\ni'm curious if people have principled ways of approaching these kinds of settings. mine would be:\n\n* take a stratified random sample of the data. the stratification should respect the distribution of target labels, and any features you consider important. the sample should be small enough so that you can feasibly tune the hyperparameters of models you consider and use cross-validation rather than simple train-test splits.\n* once you've gone through iterations of feature engineering/model comparison/analysis, and identified your best/candidate model. re-train on a larger dataset to -----> plot !!!  the learning curve and see how important additional data is.\n* depending on how much your model benefits from additional data, re-train on as large a dataset as possible and use that model in production. \n\nin terms of tools/libraries, i'd imagine the stratified sampling would be done using spark. the model development with libraries like scikit-learn and pytorch. \n\ni realize there are ml frameworks, such as sparkml, that allow you train models on spark, but i feel like they aren't nearly fleshed out enough to support the iterative workflow described above. however, since these libraries are a lot more efficient, you could train on a lot more data. thoughts on the tradeoff between more iterations on smaller data vs. fewer iterations on more data?\n\nand for inferencing (making predictions), i assume there are ways to deploy scikit-learn/pytorch models in a spark environment.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/n240bf/whats_your_approach_to_developingiterating_on/',)", "identifyer": 5587486, "year": "2021"}, {"autor": "theRealDavidDavis", "date": 1621030071000, "content": "I really hate KNIME but my coworkers love it... /!/ I was just recently hired to help a manufacturing company use machine learning for machine maintenance and quality control however no one there knows python / R. Everyone on my team uses KNIME (basically just to plot data as if it were excel or minitab) and there is a chance that they won't allow me to use Python/ R. \n\nI get that it can be cool for some people who are inexperienced or new but really, just fuck Knime. I don't know what makes me more sad, that tools like this are so popular or that someone decided to spend time making these tools which in my experience just makes it harder to do things right because most decision makers fall in love with this stuff...", "link": "https://www.reddit.com/r/datascience/comments/nckbo1/i_really_hate_knime_but_my_coworkers_love_it/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "i really hate knime but my coworkers love it... /!/ i was just recently hired to help a manufacturing company use machine learning for machine maintenance and quality control however no one there knows python / r. everyone on my team uses knime (basically just to -----> plot !!!  data as if it were excel or minitab) and there is a chance that they won't allow me to use python/ r. \n\ni get that it can be cool for some people who are inexperienced or new but really, just fuck knime. i don't know what makes me more sad, that tools like this are so popular or that someone decided to spend time making these tools which in my experience just makes it harder to do things right because most decision makers fall in love with this stuff...", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nckbo1/i_really_hate_knime_but_my_coworkers_love_it/',)", "identifyer": 5587603, "year": "2021"}, {"autor": "screamuchx", "date": 1620974217000, "content": "Why does scaled test data from Scikit-learn's StandardScaler look so different from scaled test data? /!/ I've put my train and test data into the scaler, and decided to plot it to just see what it looks like. That's what happened: [https://imgur.com/FV0TLsT](https://imgur.com/FV0TLsT) (blue is train, orange is test). Is this normal? Original data is continuous timeseries.\n\nIf this isn't normal - how do I fix it? \n\nIf it is - why is so, shouldn't that affect my results? In my understanding, the model will use this train data to predict test data, and since it's all scaled like this, those predictions are going to be off by a long shot.\n\nThe code looks like this, nothing fancy (both X and Y are independent since it's a timeseries, and represent location in 2D space):\n\n    from sklearn.preprocessing import StandardScaler\n    \n    scaler = StandardScaler()\n    scaler = scaler.fit(train[['x', 'y']])\n    \n    train[['x', 'y']] = scaler.transform(train[['x', 'y']])\n    test[['x', 'y']] = scaler.transform(test[['x', 'y']])\n    \n    plt.plot(train.x)\n    plt.plot(test.x)\n\nHopefully it's not a stackoverflow kinda question. I tried googling similar cases, but couldn't find anything - maybe just couldn't figure out the right search query.", "link": "https://www.reddit.com/r/datascience/comments/nc24um/why_does_scaled_test_data_from_scikitlearns/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "why does scaled test data from scikit-learn's standardscaler look so different from scaled test data? /!/ i've put my train and test data into the scaler, and decided to -----> plot !!!  it to just see what it looks like. that's what happened: [https://imgur.com/fv0tlst](https://imgur.com/fv0tlst) (blue is train, orange is test). is this normal? original data is continuous timeseries.\n\nif this isn't normal - how do i fix it? \n\nif it is - why is so, shouldn't that affect my results? in my understanding, the model will use this train data to predict test data, and since it's all scaled like this, those predictions are going to be off by a long shot.\n\nthe code looks like this, nothing fancy (both x and y are independent since it's a timeseries, and represent location in 2d space):\n\n    from sklearn.preprocessing import standardscaler\n    \n    scaler = standardscaler()\n    scaler = scaler.fit(train[['x', 'y']])\n    \n    train[['x', 'y']] = scaler.transform(train[['x', 'y']])\n    test[['x', 'y']] = scaler.transform(test[['x', 'y']])\n    \n    plt.plot(train.x)\n    plt.plot(test.x)\n\nhopefully it's not a stackoverflow kinda question. i tried googling similar cases, but couldn't find anything - maybe just couldn't figure out the right search query.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nc24um/why_does_scaled_test_data_from_scikitlearns/',)", "identifyer": 5587637, "year": "2021"}, {"autor": "Normal_Flan_1269", "date": 1635517895000, "content": "[Q] Correct interpretations and use of PCs? /!/ \n\nHello, im currently working on a data analysis project. I did principal component analysis to reduce the dimension of the data. I noticed that the plot of PCs sometimes shows clusters in the data. How is this different from the clusters of kmeans? I was planning on kmeans clustering with my PCs I got from dimension reduction, does this make sense or is this common?\n\nAlso from an interpretability standpoint, what are negative loadings within a PC mean? Each loading in a PC and it\u2019s value is it\u2019s contribution to the specific PC? And if the value is negative it has a negative correlation with the response?\n\nSo say PC1 has basketball players with loadings such as field goal %, offensive rebounds, and turnovers.\n\nAnd the values of these loadings are, field goal % and offensive rebounds with positive loadings, and turnovers with negative. This means that field goal % and offensive rebounds are each positively correlated with the response, and turnovers are negatively correlated with the response?\n\nAnd interpretation of this PC1 would be, players that capture the most variance in the response are those that have a high fg%, great offensive rebounders, and efficient with the ball as they commit less turnovers?", "link": "https://www.reddit.com/r/datascience/comments/qie50t/q_correct_interpretations_and_use_of_pcs/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "[q] correct interpretations and use of pcs? /!/ \n\nhello, im currently working on a data analysis project. i did principal component analysis to reduce the dimension of the data. i noticed that the -----> plot !!!  of pcs sometimes shows clusters in the data. how is this different from the clusters of kmeans? i was planning on kmeans clustering with my pcs i got from dimension reduction, does this make sense or is this common?\n\nalso from an interpretability standpoint, what are negative loadings within a pc mean? each loading in a pc and it\u2019s value is it\u2019s contribution to the specific pc? and if the value is negative it has a negative correlation with the response?\n\nso say pc1 has basketball players with loadings such as field goal %, offensive rebounds, and turnovers.\n\nand the values of these loadings are, field goal % and offensive rebounds with positive loadings, and turnovers with negative. this means that field goal % and offensive rebounds are each positively correlated with the response, and turnovers are negatively correlated with the response?\n\nand interpretation of this pc1 would be, players that capture the most variance in the response are those that have a high fg%, great offensive rebounders, and efficient with the ball as they commit less turnovers?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/qie50t/q_correct_interpretations_and_use_of_pcs/',)", "identifyer": 5587715, "year": "2021"}, {"autor": "Admiral_Wen", "date": 1610664077000, "content": "Are my interview questions unreasonable? Or are my candidates just bad? /!/ I interview data scientists at a mid-sized firm in the finance/insurance sector. I have seen plenty of resumes, all of them look stellar and hits all of the key buzzwords. But during interviews, I often get the sense that there's a lack of genuine understanding of the concepts beyond the surface level talking points. For example, many candidates get tripped up by one of more of these:\n\n1. If I have a categorical feature (say, state or county), what's the difference between label-encoding and one-hot-encoding it? Why might we *not* want to label-encode? ([Reference](https://towardsdatascience.com/categorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd))\n2. Why might the default feature-importance plot in XGBoost be misleading? What are some other options you have? ([Reference](https://towardsdatascience.com/be-careful-when-interpreting-your-features-importance-in-xgboost-6e16132588e7))\n3. Why do we use logloss as the objective function for classification models? What does it penalize? Why is it \"better\" than just maximizing accuracy? ([Reference](https://stats.stackexchange.com/questions/180116/when-is-log-loss-metric-appropriate-for-evaluating-performance-of-a-classifier))\n4. How can we visualize the performance of two different models on the holdout, beyond just comparing their RMSE or accuracy? ([Reference](https://www.listendata.com/2014/08/excel-template-gain-and-lift-charts.html))\n\nKeep in mind that this is for an experienced position, not an entry level one. I ask these questions because I think if you've truly built ML models before and understood them, then you should be able to answer these no problem. Candidates who can't answer these have a higher chance of falling for common pitfalls or mistakes, either for model building or for interpreting results.\n\nI have had candidates be able to answer all of them easily and concisely. But most of the time, I get either a wrong answer or some long-winded non-answer, especially to 2-4. In fact I just interviewed a candidate whose resume was stacked but couldn't answer any (even slightly easier ones). So this got me wondering, are questions like these unreasonable? Or is it just normal to have to filter out 8 out of 10 candidates it seems? If anyone here does interviews, do you have a similar experience?", "link": "https://www.reddit.com/r/datascience/comments/kxgqbp/are_my_interview_questions_unreasonable_or_are_my/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "are my interview questions unreasonable? or are my candidates just bad? /!/ i interview data scientists at a mid-sized firm in the finance/insurance sector. i have seen plenty of resumes, all of them look stellar and hits all of the key buzzwords. but during interviews, i often get the sense that there's a lack of genuine understanding of the concepts beyond the surface level talking points. for example, many candidates get tripped up by one of more of these:\n\n1. if i have a categorical feature (say, state or county), what's the difference between label-encoding and one-hot-encoding it? why might we *not* want to label-encode? ([reference](https://towardsdatascience.com/categorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd))\n2. why might the default feature-importance -----> plot !!!  in xgboost be misleading? what are some other options you have? ([reference](https://towardsdatascience.com/be-careful-when-interpreting-your-features-importance-in-xgboost-6e16132588e7))\n3. why do we use logloss as the objective function for classification models? what does it penalize? why is it \"better\" than just maximizing accuracy? ([reference](https://stats.stackexchange.com/questions/180116/when-is-log-loss-metric-appropriate-for-evaluating-performance-of-a-classifier))\n4. how can we visualize the performance of two different models on the holdout, beyond just comparing their rmse or accuracy? ([reference](https://www.listendata.com/2014/08/excel-template-gain-and-lift-charts.html))\n\nkeep in mind that this is for an experienced position, not an entry level one. i ask these questions because i think if you've truly built ml models before and understood them, then you should be able to answer these no problem. candidates who can't answer these have a higher chance of falling for common pitfalls or mistakes, either for model building or for interpreting results.\n\ni have had candidates be able to answer all of them easily and concisely. but most of the time, i get either a wrong answer or some long-winded non-answer, especially to 2-4. in fact i just interviewed a candidate whose resume was stacked but couldn't answer any (even slightly easier ones). so this got me wondering, are questions like these unreasonable? or is it just normal to have to filter out 8 out of 10 candidates it seems? if anyone here does interviews, do you have a similar experience?", "sortedWord": "None", "removed": "('nan',)", "score": 2, "comments": 264, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/kxgqbp/are_my_interview_questions_unreasonable_or_are_my/',)", "identifyer": 5588141, "year": "2021"}, {"autor": "ThaRationalist", "date": 1610660807000, "content": "Plots for Classifier Project for Portfolio? /!/ Hey guys,\n\nI recently finished a naive bayes classifier project in R to put on my github portfolio. I was wondering if I should add any additional plots(like a density plot) to go along with the project? Or would this be unnecessary given the project, as a visual doesn't mean much for a classification algorithm?", "link": "https://www.reddit.com/r/datascience/comments/kxfljv/plots_for_classifier_project_for_portfolio/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "plots for classifier project for portfolio? /!/ hey guys,\n\ni recently finished a naive bayes classifier project in r to put on my github portfolio. i was wondering if i should add any additional plots(like a density -----> plot !!! ) to go along with the project? or would this be unnecessary given the project, as a visual doesn't mean much for a classification algorithm?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/kxfljv/plots_for_classifier_project_for_portfolio/',)", "identifyer": 5588142, "year": "2021"}, {"autor": "GreenOwl_0", "date": 1626335269000, "content": "What are some red flags to look out for during data tasks? /!/ I recently applied for a job and was asked to do a \u201cshort unpaid data task\u201d as a part of the application. It involved finding county level data for several variables, across 40 years. I then had to merge all the datasets into a panel and plot trends. The time given to me was 3 days. I couldn\u2019t complete it as there were variables and years for which I couldn\u2019t find the specified datasets, and the datasets that I did have required a lot of work before they could be merged. I also didn\u2019t feel comfortable spending so much time on a task that was supposedly \u201cshort\u201d, and was unpaid. \n\nI\u2019m curious to know what you guys think should be an acceptable time limit for this task, and also what you think are red flags to look out for during coding tasks given by recruiters. My friends keep telling me that the recruiters were just trying to get free work out of the applicants and I cant help but think the same (I should also mention that the post would require the hired person to do the exact same work on the exact same data)\n\nSome info on the post: not a data science job, but a research job that would entail handling datasets. The job post said that knowledge of statistical softwares wasn\u2019t required but would be a plus.", "link": "https://www.reddit.com/r/datascience/comments/oko3es/what_are_some_red_flags_to_look_out_for_during/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "what are some red flags to look out for during data tasks? /!/ i recently applied for a job and was asked to do a \u201cshort unpaid data task\u201d as a part of the application. it involved finding county level data for several variables, across 40 years. i then had to merge all the datasets into a panel and -----> plot !!!  trends. the time given to me was 3 days. i couldn\u2019t complete it as there were variables and years for which i couldn\u2019t find the specified datasets, and the datasets that i did have required a lot of work before they could be merged. i also didn\u2019t feel comfortable spending so much time on a task that was supposedly \u201cshort\u201d, and was unpaid. \n\ni\u2019m curious to know what you guys think should be an acceptable time limit for this task, and also what you think are red flags to look out for during coding tasks given by recruiters. my friends keep telling me that the recruiters were just trying to get free work out of the applicants and i cant help but think the same (i should also mention that the post would require the hired person to do the exact same work on the exact same data)\n\nsome info on the post: not a data science job, but a research job that would entail handling datasets. the job post said that knowledge of statistical softwares wasn\u2019t required but would be a plus.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 16, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/oko3es/what_are_some_red_flags_to_look_out_for_during/',)", "identifyer": 5588481, "year": "2021"}, {"autor": "SQL_beginner", "date": 1626233764000, "content": "Has anyone ever worked on this kind of problem before? /!/  \n\nI am working with R.\n\nSuppose you have the following data:\n\n    #generate data set.seed(123)\n     a1 = rnorm(1000,100,10)\n     b1 = rnorm(1000,100,10) \n    c1 = rnorm(1000,5,1)\n    \n     train_data = data.frame(a1,b1,c1)\n     #view data        \n      a1        b1       c1\n     1 94.39524 90.04201 4.488396 \n    2 97.69823 89.60045 5.236938 \n    3 115.58708 99.82020 4.458411 \n    4 100.70508 98.67825 6.219228 \n    5 101.29288 74.50657 5.174136\n     6 117.15065 110.40573 4.384732 \n\nWe can visualize the data as follows:\n\n    #visualize data par(mfrow=c(2,2))  plot(train_data$a1, train_data$b1, col = train_data$c1, main = \"plot of a1 vs b1, points colored by c1\") \n    hist(train_data$a1) \n    hist(train_data$b1)\n     hist(train_data$c1) \n\n[https://i.stack.imgur.com/t641h.png](https://i.stack.imgur.com/t641h.png)\n\nHere is the **Problem** :\n\n1. From the data, only take variables \"a1\" and \"b1\" : using **only 2 \"logical conditions\"**, split this data into **3 regions** (e.g. Region 1 WHERE 20 &gt; a1 &gt;0 AND 0&lt; b1 &lt; 25)\n2. In each region, you want the \"average value of c1\" within that region to be as small as possible - but each region must have at least some minimum number of data points, e.g. 100 data points (to prevent trivial solutions)\n3. **Goal** : Is it possible to determine the \"boundaries\" of these 3 regions that minimizes :\n\n* the mean value of \"c1\" for region 1\n* the mean value of \"c1\" for region 2\n* the mean value of \"c1\" for region 3\n* the average \"mean value of c1 for all 3 regions\" (i.e. c\\_avg = (region1\\_c1\\_avg + region2\\_c1\\_avg + region3\\_c1\\_avg) / 3  \n)\n\nIn the end, for a given combination, you would find the following, e.g. (made up numbers):\n\n* Region 1 : WHERE 20&gt; a1 &gt;0 AND 0 &lt; b1 &lt; 25 ; region1\\_c1\\_avg = 4\n* Region 2 : WHERE 50&gt; a1 &gt;20 AND 25 &lt; b1 &lt; 60 ; region2\\_c1\\_avg = 2.9\n* Region 3 : WHERE a1&gt;50 AND b1 &gt; 60 ; region3\\_c1\\_avg = 1.9\n* c\\_avg = (4 + 2.9 + 1.9) / 3 = 2.93\n\nAnd hope that (region1\\_c1\\_avg, region2\\_c1\\_avg, region3\\_c1\\_avg and c\\_avg  \n) are minimized\n\n**My Question**:\n\n**Does this kind of problem have an \"exact solution\"?** The only thing I can think of is performing a \"random search\" that considers many different definitions of (Region 1, Region 2 and Region 3  \n) and compares the corresponding values of (region1\\_c1\\_avg, region2\\_c1\\_avg, region3\\_c1\\_avg and c\\_avg  \n), until a minimum value is found. Is this an application of linear programming or multi-objective optimization (e.g. genetic algorithm)? Has anyone worked on something like this before?\n\nI have done a lot of research and haven't found a similar problem like this. I decided to formulate this problem as a \"multi-objective constrained optimization problem\", and figured out how to implement algorithms like \"random search\" and \"genetic algorithm\". **Can someone please tell me if my approach makes sense?**\n\nThanks\n\n**Note 1:** In the context of multi-objective optimization, for a given set of definitions of (Region1, Region2 and Region3  \n): to collectively compare whether a set of values for (region1\\_c1\\_avg, region2\\_c1\\_avg, region3\\_c1\\_avg and c\\_avg  \n) are satisfactory, the concept of \"Pareto Optimality\" ([https://en.wikipedia.org/wiki/Multi-objective\\_optimization#Visualization\\_of\\_the\\_Pareto\\_front](https://en.wikipedia.org/wiki/Multi-objective_optimization#Visualization_of_the_Pareto_front)) is often used to make comparisons between different sets of {(Region1, Region2 and Region3  \n) and (region1\\_c1\\_avg, region2\\_c1\\_avg, region3\\_c1\\_avg and c\\_avg  \n)}\n\n**Note 2** : Ultimately, these 3 Regions can defined by any set of 4 numbers. If each of these 4 numbers can be between \"0 and 100\", and through 0.1 increments (e.g. 12, 12.1, 12.2, 12.3, etc) : this means that there exists 1000 \\^ 4 = 1 e\\^12 possible solutions (roughly 1 trillion) to compare. There are simply far too many solutions to individually verify and compare. I am thinking that a mathematical based search/optimization problem can be used to strategically search for an optimal solution.", "link": "https://www.reddit.com/r/datascience/comments/ojvz08/has_anyone_ever_worked_on_this_kind_of_problem/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "has anyone ever worked on this kind of problem before? /!/  \n\ni am working with r.\n\nsuppose you have the following data:\n\n    #generate data set.seed(123)\n     a1 = rnorm(1000,100,10)\n     b1 = rnorm(1000,100,10) \n    c1 = rnorm(1000,5,1)\n    \n     train_data = data.frame(a1,b1,c1)\n     #view data        \n      a1        b1       c1\n     1 94.39524 90.04201 4.488396 \n    2 97.69823 89.60045 5.236938 \n    3 115.58708 99.82020 4.458411 \n    4 100.70508 98.67825 6.219228 \n    5 101.29288 74.50657 5.174136\n     6 117.15065 110.40573 4.384732 \n\nwe can visualize the data as follows:\n\n    #visualize data par(mfrow=c(2,2))  plot(train_data$a1, train_data$b1, col = train_data$c1, main = \"-----> plot !!!  of a1 vs b1, points colored by c1\") \n    hist(train_data$a1) \n    hist(train_data$b1)\n     hist(train_data$c1) \n\n[https://i.stack.imgur.com/t641h.png](https://i.stack.imgur.com/t641h.png)\n\nhere is the **problem** :\n\n1. from the data, only take variables \"a1\" and \"b1\" : using **only 2 \"logical conditions\"**, split this data into **3 regions** (e.g. region 1 where 20 &gt; a1 &gt;0 and 0&lt; b1 &lt; 25)\n2. in each region, you want the \"average value of c1\" within that region to be as small as possible - but each region must have at least some minimum number of data points, e.g. 100 data points (to prevent trivial solutions)\n3. **goal** : is it possible to determine the \"boundaries\" of these 3 regions that minimizes :\n\n* the mean value of \"c1\" for region 1\n* the mean value of \"c1\" for region 2\n* the mean value of \"c1\" for region 3\n* the average \"mean value of c1 for all 3 regions\" (i.e. c\\_avg = (region1\\_c1\\_avg + region2\\_c1\\_avg + region3\\_c1\\_avg) / 3  \n)\n\nin the end, for a given combination, you would find the following, e.g. (made up numbers):\n\n* region 1 : where 20&gt; a1 &gt;0 and 0 &lt; b1 &lt; 25 ; region1\\_c1\\_avg = 4\n* region 2 : where 50&gt; a1 &gt;20 and 25 &lt; b1 &lt; 60 ; region2\\_c1\\_avg = 2.9\n* region 3 : where a1&gt;50 and b1 &gt; 60 ; region3\\_c1\\_avg = 1.9\n* c\\_avg = (4 + 2.9 + 1.9) / 3 = 2.93\n\nand hope that (region1\\_c1\\_avg, region2\\_c1\\_avg, region3\\_c1\\_avg and c\\_avg  \n) are minimized\n\n**my question**:\n\n**does this kind of problem have an \"exact solution\"?** the only thing i can think of is performing a \"random search\" that considers many different definitions of (region 1, region 2 and region 3  \n) and compares the corresponding values of (region1\\_c1\\_avg, region2\\_c1\\_avg, region3\\_c1\\_avg and c\\_avg  \n), until a minimum value is found. is this an application of linear programming or multi-objective optimization (e.g. genetic algorithm)? has anyone worked on something like this before?\n\ni have done a lot of research and haven't found a similar problem like this. i decided to formulate this problem as a \"multi-objective constrained optimization problem\", and figured out how to implement algorithms like \"random search\" and \"genetic algorithm\". **can someone please tell me if my approach makes sense?**\n\nthanks\n\n**note 1:** in the context of multi-objective optimization, for a given set of definitions of (region1, region2 and region3  \n): to collectively compare whether a set of values for (region1\\_c1\\_avg, region2\\_c1\\_avg, region3\\_c1\\_avg and c\\_avg  \n) are satisfactory, the concept of \"pareto optimality\" ([https://en.wikipedia.org/wiki/multi-objective\\_optimization#visualization\\_of\\_the\\_pareto\\_front](https://en.wikipedia.org/wiki/multi-objective_optimization#visualization_of_the_pareto_front)) is often used to make comparisons between different sets of {(region1, region2 and region3  \n) and (region1\\_c1\\_avg, region2\\_c1\\_avg, region3\\_c1\\_avg and c\\_avg  \n)}\n\n**note 2** : ultimately, these 3 regions can defined by any set of 4 numbers. if each of these 4 numbers can be between \"0 and 100\", and through 0.1 increments (e.g. 12, 12.1, 12.2, 12.3, etc) : this means that there exists 1000 \\^ 4 = 1 e\\^12 possible solutions (roughly 1 trillion) to compare. there are simply far too many solutions to individually verify and compare. i am thinking that a mathematical based search/optimization problem can be used to strategically search for an optimal solution.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/ojvz08/has_anyone_ever_worked_on_this_kind_of_problem/',)", "identifyer": 5588527, "year": "2021"}, {"autor": "getback339", "date": 1627474549000, "content": "Best way to select which modelled data is most representative of observed data? /!/ I\u2019m quite new to data analysis, but I\u2019m working on a problem where I\u2019m looking at around 20 models of weather data and a set of observed data.\n\nI\u2019ve got a time period (of about 25 years) where both models and observations coincide.\n\nI\u2019m looking at annual mean of what and I made a box plot of 25 means for each of the models and the observed.\n\nI can draw a line from the max to the min of observed to see how the models compare, but is there a more rigorous method I could use?\n\nThanks in advance for any suggestions", "link": "https://www.reddit.com/r/datascience/comments/ot8p8n/best_way_to_select_which_modelled_data_is_most/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "best way to select which modelled data is most representative of observed data? /!/ i\u2019m quite new to data analysis, but i\u2019m working on a problem where i\u2019m looking at around 20 models of weather data and a set of observed data.\n\ni\u2019ve got a time period (of about 25 years) where both models and observations coincide.\n\ni\u2019m looking at annual mean of what and i made a box -----> plot !!!  of 25 means for each of the models and the observed.\n\ni can draw a line from the max to the min of observed to see how the models compare, but is there a more rigorous method i could use?\n\nthanks in advance for any suggestions", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/ot8p8n/best_way_to_select_which_modelled_data_is_most/',)", "identifyer": 5588612, "year": "2021"}, {"autor": "rotterdamn8", "date": 1611017262000, "content": "Tired: Python vs. R. Wired: matplotlib vs. ggplot? /!/ I prefer Python but hey, R works too. Whatever works for you. \n\nBut I find matplotlib really frustrating sometimes. I can make plots but in Spyder, the default plot is really small, so I have to change the figure size, and then I have to increase the line width to make it look right. But then the font is too small, so I have to increase that. \n\nYes, I've looked at the documentation, it's good but still leaves me with questions. I read this today in the tutorial. Well that doesn't help. I'm always using pandas dataframes! Sure, I can make it work but it would be easier if there's something that simply works well with dataframes.\n\n&gt;All of plotting functions expect [numpy.array](https://numpy.org/doc/stable/reference/generated/numpy.array.html#numpy.array) or [numpy.ma.masked\\_array](https://numpy.org/doc/stable/reference/generated/numpy.ma.masked_array.html#numpy.ma.masked_array) as input. Classes that are 'array-like' such as [pandas](https://pandas.pydata.org/pandas-docs/stable/index.html#module-pandas) data objects and [numpy.matrix](https://numpy.org/doc/stable/reference/generated/numpy.matrix.html#numpy.matrix) may or may not work as intended. It is best to convert these to [numpy.array](https://numpy.org/doc/stable/reference/generated/numpy.array.html#numpy.array) objects prior to plotting. \n\nDoes anyone have strong feelings about qplot or ggplot? R Studio allows you to save as either image or pdf, and you can just punch in the plot size directly. User friendly!\n\nI get the basics of ggplot but I would still need to get used to all the parameters and how to tweak. Is it worth it?", "link": "https://www.reddit.com/r/datascience/comments/l086ch/tired_python_vs_r_wired_matplotlib_vs_ggplot/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "tired: python vs. r. wired: matplotlib vs. ggplot? /!/ i prefer python but hey, r works too. whatever works for you. \n\nbut i find matplotlib really frustrating sometimes. i can make plots but in spyder, the default -----> plot !!!  is really small, so i have to change the figure size, and then i have to increase the line width to make it look right. but then the font is too small, so i have to increase that. \n\nyes, i've looked at the documentation, it's good but still leaves me with questions. i read this today in the tutorial. well that doesn't help. i'm always using pandas dataframes! sure, i can make it work but it would be easier if there's something that simply works well with dataframes.\n\n&gt;all of plotting functions expect [numpy.array](https://numpy.org/doc/stable/reference/generated/numpy.array.html#numpy.array) or [numpy.ma.masked\\_array](https://numpy.org/doc/stable/reference/generated/numpy.ma.masked_array.html#numpy.ma.masked_array) as input. classes that are 'array-like' such as [pandas](https://pandas.pydata.org/pandas-docs/stable/index.html#module-pandas) data objects and [numpy.matrix](https://numpy.org/doc/stable/reference/generated/numpy.matrix.html#numpy.matrix) may or may not work as intended. it is best to convert these to [numpy.array](https://numpy.org/doc/stable/reference/generated/numpy.array.html#numpy.array) objects prior to plotting. \n\ndoes anyone have strong feelings about qplot or ggplot? r studio allows you to save as either image or pdf, and you can just punch in the plot size directly. user friendly!\n\ni get the basics of ggplot but i would still need to get used to all the parameters and how to tweak. is it worth it?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/l086ch/tired_python_vs_r_wired_matplotlib_vs_ggplot/',)", "identifyer": 5588901, "year": "2021"}, {"autor": "Cill-e-in", "date": 1611012482000, "content": "Hosting output in an accessible, digestible format /!/ A huge part of data science is communicating with stakeholders and helping others make informed decisions. I have been looking into methods of building essentially as slick an app as possible; I've been poking around with some Bokeh, or maybe a script to generate some plots and basic descriptions into a .pptx file, that kind of thing. One thing I'm interested in is after I conduct some analysis, give people a way to essentially \"poke\" around; now, the likes of Power BI are fine (since you just slap a publish button and ta-da), but for fun, I want to whip this up with Python/R. What would people point me towards in order to host say a simple app letting people (for example) choose a predictor and see what the average prediction is for different values (imagine a partial dependency plot with a slicer to choose a variable)? Google's usual torrent of information tends to mention quite a few visualisation libraries, but I'm not well-informed enough to quite narrow down search terms enough to hit the mark. I just want to reach a point where I know I can build something, pop someone a link and have it all run. Currently I'm coming across things such as using Bokeh + GCP. Any simpler solutions &amp; all help is appreciated!  \n\n\nTL;DR: How would people recommend hosting a data-based web app at home for fun?", "link": "https://www.reddit.com/r/datascience/comments/l06okm/hosting_output_in_an_accessible_digestible_format/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "hosting output in an accessible, digestible format /!/ a huge part of data science is communicating with stakeholders and helping others make informed decisions. i have been looking into methods of building essentially as slick an app as possible; i've been poking around with some bokeh, or maybe a script to generate some plots and basic descriptions into a .pptx file, that kind of thing. one thing i'm interested in is after i conduct some analysis, give people a way to essentially \"poke\" around; now, the likes of power bi are fine (since you just slap a publish button and ta-da), but for fun, i want to whip this up with python/r. what would people point me towards in order to host say a simple app letting people (for example) choose a predictor and see what the average prediction is for different values (imagine a partial dependency -----> plot !!!  with a slicer to choose a variable)? google's usual torrent of information tends to mention quite a few visualisation libraries, but i'm not well-informed enough to quite narrow down search terms enough to hit the mark. i just want to reach a point where i know i can build something, pop someone a link and have it all run. currently i'm coming across things such as using bokeh + gcp. any simpler solutions &amp; all help is appreciated!  \n\n\ntl;dr: how would people recommend hosting a data-based web app at home for fun?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/l06okm/hosting_output_in_an_accessible_digestible_format/',)", "identifyer": 5588905, "year": "2021"}, {"autor": "ottawalanguages", "date": 1611423511000, "content": "Multivariate Normal Distributions /!/ I am trying to teach myself about the multivariate normal distribution and I am struggling to understand some basic things about it.\n\nTo show my confusion, I use the famous Iris Flower dataset (I will use the R programming language for some basic scripts). The Iris Flower dataset has 5 columns and 150 rows. Each row contains the measurements for an individual flower (i.e. there are 150 flowers). The columns contain the measurements of the \"Petal Width\", the \"Petal Length\", the \"Sepal Length\" , the \"Sepal Width\" and the \"Type of Flower\" (three types of flowers, categorical variable).\n\nSuppose I Just take the Petal Length variable. I want to see if the Petal Length follows a (univariate) normal distribution. I think this can be easily done using different strategies (R code below):\n\n    #load the iris data and isolate the petal length  data(iris)  \n    var1 = iris$Petal.Length  \n    \n      #visually check if the distribution of the petal length looks like a \"bell curve\" plot(density(var1))  \n    \n      #look at the quantile-quantile  \n    plot qqnorm(var1)   \n    \n     #use statistics (e.g. the shapiro-wilks test) to check for normality \n      shapiro.test(var1) \n    \n     #if the data is normally distributed, we can find out the mean and the variance\n         mean(var1) \n    var(var1) \n\nSimilarly, I can repeat this for the remaining variables in the iris data. However, this task becomes a lot more complicated when you consider the multivariate distribution of the iris data : [https://en.wikipedia.org/wiki/Multivariate\\_normal\\_distribution](https://en.wikipedia.org/wiki/Multivariate_normal_distribution) . When dealing with the multivariate distribution, there is now a \"vector of means\" and a \"variance-covariance matrix\". This means that there are more complex relationships within the data - some parts of the data might have a normal distribution whereas some parts of the data might not be normally distributed.\n\nAfter spending some time researching how to determine if a dataset follows a multivariate distribution , I found out about something called the Mardia test, which apparently uses the \"skewness\" and the \"kurtosis\" to determine if the data is normally distributed (high skewness and high kurtosis means the data is not normally distributed). I tried running the following code in R to perform the Marida test on the iris data:\n\n    library(MVN) \n     data(iris) \n     data = iris[,-5] \n     result = mvn(data)  result \n\nThe results of this are confusing. I am not sure if I am approaching this problem in the right way. My question: How do you determine if the data you are dealing with has a multivariate normal distribution?\n\nMy ultimate goal is to determine if the data follows a multivariate normal distribution for the purpose of outlier detection. I found out about something called the Mahalanobis Distance : [https://en.wikipedia.org/wiki/Mahalanobis\\_distance](https://en.wikipedia.org/wiki/Mahalanobis_distance)\n\nIf my data does follow a multivariate normal distribution, i.e. suppose (given the data) I evaluate the mean vector and the variance-covariance matrix of the data - I can use the Mahalanobis distance to find out which data points are situated further away from the center of the multivariate-normal distribution. The further the point is, the more of an outlier the point is.\n\nAnother idea I had - perhaps the distribution of more complicated data can be approximated by \"combining\" several (multivariate) normal distributions together ([https://scikit-learn.org/stable/modules/mixture.html#:\\~:text=A%20Gaussian%20mixture%20model%20is,Gaussian%20distributions%20with%20unknown%20parameters](https://scikit-learn.org/stable/modules/mixture.html#:~:text=A%20Gaussian%20mixture%20model%20is,Gaussian%20distributions%20with%20unknown%20parameters).) . Therefore, I could try to determine the irregular distribution of my data by combining (i.e. \"mixing\") several different normal distributions together (note: I am not sure if the resulting combination of these mixtures will necessarily have to be normally distributed?). Then, I could use the Mahalanobis distance to measure how far each point is situated from the center of the combined distribution - again, points that are further away from the center of the combined distribution are more likely to be outliers. Is this approach mathematically correct?\n\nCan someone please help me in clarifying these details about finding out the distribution of multivariate data?\n\nThanks", "link": "https://www.reddit.com/r/datascience/comments/l3glu7/multivariate_normal_distributions/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "multivariate normal distributions /!/ i am trying to teach myself about the multivariate normal distribution and i am struggling to understand some basic things about it.\n\nto show my confusion, i use the famous iris flower dataset (i will use the r programming language for some basic scripts). the iris flower dataset has 5 columns and 150 rows. each row contains the measurements for an individual flower (i.e. there are 150 flowers). the columns contain the measurements of the \"petal width\", the \"petal length\", the \"sepal length\" , the \"sepal width\" and the \"type of flower\" (three types of flowers, categorical variable).\n\nsuppose i just take the petal length variable. i want to see if the petal length follows a (univariate) normal distribution. i think this can be easily done using different strategies (r code below):\n\n    #load the iris data and isolate the petal length  data(iris)  \n    var1 = iris$petal.length  \n    \n      #visually check if the distribution of the petal length looks like a \"bell curve\" plot(density(var1))  \n    \n      #look at the quantile-quantile  \n    -----> plot !!!  qqnorm(var1)   \n    \n     #use statistics (e.g. the shapiro-wilks test) to check for normality \n      shapiro.test(var1) \n    \n     #if the data is normally distributed, we can find out the mean and the variance\n         mean(var1) \n    var(var1) \n\nsimilarly, i can repeat this for the remaining variables in the iris data. however, this task becomes a lot more complicated when you consider the multivariate distribution of the iris data : [https://en.wikipedia.org/wiki/multivariate\\_normal\\_distribution](https://en.wikipedia.org/wiki/multivariate_normal_distribution) . when dealing with the multivariate distribution, there is now a \"vector of means\" and a \"variance-covariance matrix\". this means that there are more complex relationships within the data - some parts of the data might have a normal distribution whereas some parts of the data might not be normally distributed.\n\nafter spending some time researching how to determine if a dataset follows a multivariate distribution , i found out about something called the mardia test, which apparently uses the \"skewness\" and the \"kurtosis\" to determine if the data is normally distributed (high skewness and high kurtosis means the data is not normally distributed). i tried running the following code in r to perform the marida test on the iris data:\n\n    library(mvn) \n     data(iris) \n     data = iris[,-5] \n     result = mvn(data)  result \n\nthe results of this are confusing. i am not sure if i am approaching this problem in the right way. my question: how do you determine if the data you are dealing with has a multivariate normal distribution?\n\nmy ultimate goal is to determine if the data follows a multivariate normal distribution for the purpose of outlier detection. i found out about something called the mahalanobis distance : [https://en.wikipedia.org/wiki/mahalanobis\\_distance](https://en.wikipedia.org/wiki/mahalanobis_distance)\n\nif my data does follow a multivariate normal distribution, i.e. suppose (given the data) i evaluate the mean vector and the variance-covariance matrix of the data - i can use the mahalanobis distance to find out which data points are situated further away from the center of the multivariate-normal distribution. the further the point is, the more of an outlier the point is.\n\nanother idea i had - perhaps the distribution of more complicated data can be approximated by \"combining\" several (multivariate) normal distributions together ([https://scikit-learn.org/stable/modules/mixture.html#:\\~:text=a%20gaussian%20mixture%20model%20is,gaussian%20distributions%20with%20unknown%20parameters](https://scikit-learn.org/stable/modules/mixture.html#:~:text=a%20gaussian%20mixture%20model%20is,gaussian%20distributions%20with%20unknown%20parameters).) . therefore, i could try to determine the irregular distribution of my data by combining (i.e. \"mixing\") several different normal distributions together (note: i am not sure if the resulting combination of these mixtures will necessarily have to be normally distributed?). then, i could use the mahalanobis distance to measure how far each point is situated from the center of the combined distribution - again, points that are further away from the center of the combined distribution are more likely to be outliers. is this approach mathematically correct?\n\ncan someone please help me in clarifying these details about finding out the distribution of multivariate data?\n\nthanks", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 5, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/l3glu7/multivariate_normal_distributions/',)", "identifyer": 5589034, "year": "2021"}, {"autor": "Jimbobmij", "date": 1618431838000, "content": "What do you use to plot your multiple regressions? /!/ See title.\n\nI've created a nice and simple multiple regression which works as intended:\n\n    X = multi_df[['Drug Offences', 'Public Order Offences']]\n    y = multi_df['Homicides']\n\n    regr = linear_model.LinearRegression()\n    regr.fit(X, y)\n\n    Y_pred = regr.predict(X) \n\nI had a look online to see what was recommended for plotting multiple regressions, but nothing stood out to me.\n\nWhat do you use to plot your multiple regressions?", "link": "https://www.reddit.com/r/datascience/comments/mqz70b/what_do_you_use_to_plot_your_multiple_regressions/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "what do you use to -----> plot !!!  your multiple regressions? /!/ see title.\n\ni've created a nice and simple multiple regression which works as intended:\n\n    x = multi_df[['drug offences', 'public order offences']]\n    y = multi_df['homicides']\n\n    regr = linear_model.linearregression()\n    regr.fit(x, y)\n\n    y_pred = regr.predict(x) \n\ni had a look online to see what was recommended for plotting multiple regressions, but nothing stood out to me.\n\nwhat do you use to -----> plot !!!  your multiple regressions?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/mqz70b/what_do_you_use_to_plot_your_multiple_regressions/',)", "identifyer": 5589283, "year": "2021"}, {"autor": "wanderingcatto", "date": 1620409703000, "content": "How do you guys remember syntaxes? /!/ I've been working as a new data scientist for 3 months and noticed some people could just jump straight in and type away on jupyter notebook without referring to anything, while I have to maintain a \"cheatsheet\" of syntax\n\nNeed to bin categories? Refer to my cheatsheet\nNeed to plot a few bar charts with annotations? Refer to cheatsheet\nNeed to do a random forest? Err, refer to cheatsheet\n\nUsually, there's nothing wrong with referring to notes or getting some help from Google, of course. But as a professional data scientist, it gets really embarrassing when I'm having a discussion with my colleagues on a project I'm working on, and they're like \"can you create these new features and let me see how their plots look like real quick\", and I'm like...let me refer to my notes on how to do that", "link": "https://www.reddit.com/r/datascience/comments/n74ex9/how_do_you_guys_remember_syntaxes/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "how do you guys remember syntaxes? /!/ i've been working as a new data scientist for 3 months and noticed some people could just jump straight in and type away on jupyter notebook without referring to anything, while i have to maintain a \"cheatsheet\" of syntax\n\nneed to bin categories? refer to my cheatsheet\nneed to -----> plot !!!  a few bar charts with annotations? refer to cheatsheet\nneed to do a random forest? err, refer to cheatsheet\n\nusually, there's nothing wrong with referring to notes or getting some help from google, of course. but as a professional data scientist, it gets really embarrassing when i'm having a discussion with my colleagues on a project i'm working on, and they're like \"can you create these new features and let me see how their plots look like real quick\", and i'm like...let me refer to my notes on how to do that", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 13, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/n74ex9/how_do_you_guys_remember_syntaxes/',)", "identifyer": 5589463, "year": "2021"}, {"autor": "Inferno221", "date": 1624303292000, "content": "Would I be a good data scientist? /!/ I'm a chemical engineer, but I've been unemployed for 6+months. I have been getting interviews for chemE jobs, but I'm still unemployed.\n\nIn my past roles, I had to pull data from our process to look for issues that were happening in real time. For example, if we had a batch solution that wasn't performing well, I would pull data for different process variables (like temperature, pressure, stirring rate, etc), compile it, and get a clear picture of what was going on. \n\nThe data was downloaded with a csv file, but it contained a lot of junk. So I had to use python to cleanup the data, and then look for correlations, sometimes using a correlation matrix, other times I would plot the variables out over time to get a real picture. \n\nThen I would identify an issue (or several), and go from there.\n\nI have experience using python, but no experience with SQL. I have experience forecasting timeseries with python, but for tensorflow machine learning, I don't know much about it.\n\nWould I be a good data scientist? How would I go about getting an entry level role of data science from my experience?", "link": "https://www.reddit.com/r/datascience/comments/o534pz/would_i_be_a_good_data_scientist/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "would i be a good data scientist? /!/ i'm a chemical engineer, but i've been unemployed for 6+months. i have been getting interviews for cheme jobs, but i'm still unemployed.\n\nin my past roles, i had to pull data from our process to look for issues that were happening in real time. for example, if we had a batch solution that wasn't performing well, i would pull data for different process variables (like temperature, pressure, stirring rate, etc), compile it, and get a clear picture of what was going on. \n\nthe data was downloaded with a csv file, but it contained a lot of junk. so i had to use python to cleanup the data, and then look for correlations, sometimes using a correlation matrix, other times i would -----> plot !!!  the variables out over time to get a real picture. \n\nthen i would identify an issue (or several), and go from there.\n\ni have experience using python, but no experience with sql. i have experience forecasting timeseries with python, but for tensorflow machine learning, i don't know much about it.\n\nwould i be a good data scientist? how would i go about getting an entry level role of data science from my experience?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/o534pz/would_i_be_a_good_data_scientist/',)", "identifyer": 5589648, "year": "2021"}, {"autor": "sanketsans", "date": 1624290060000, "content": "Plot Venn diagram with weighted labels. /!/ I want to plot venn diagram. I get weighted diagram, but for labels also I need weighted visualisations. So labels are in a dict, eg. {'A': 5, 'B': 10 ... }. Based on the count / weight, labels should be bolded. \n\nSomething like a word cloud :", "link": "https://www.reddit.com/r/datascience/comments/o4y10h/plot_venn_diagram_with_weighted_labels/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "-----> plot !!!  venn diagram with weighted labels. /!/ i want to plot venn diagram. i get weighted diagram, but for labels also i need weighted visualisations. so labels are in a dict, eg. {'a': 5, 'b': 10 ... }. based on the count / weight, labels should be bolded. \n\nsomething like a word cloud :", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/o4y10h/plot_venn_diagram_with_weighted_labels/',)", "identifyer": 5589661, "year": "2021"}, {"autor": "vietlinh12hoa", "date": 1624718590000, "content": "Preproc function to prepare for take home tests /!/ I'm back to job market and think it'll be more helpful to write a preproc functions tooling to apply into as many take home coding tests as possible to save time. I'm thinking about some common features inside this function like:\n- If categorical variable &gt; 10, take top 10 and group the rest into others\n- If date of birth/created date present, then subtract from today to see aging group.\n- Seaborn correlation plot/matrix among numerical features\n- Option to fillna with zero, or impute (depend)\n- Different method for dealing outlier like quantile clip, or log scale\n- Box plot between each categorical variable with each numerical variable to overview the distribution by groups.\n\nOf course, pandas profiling is the first step. \nAnyone already implemented this idea? Any recommendation you would add? Many thanks.", "link": "https://www.reddit.com/r/datascience/comments/o8c3hg/preproc_function_to_prepare_for_take_home_tests/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "preproc function to prepare for take home tests /!/ i'm back to job market and think it'll be more helpful to write a preproc functions tooling to apply into as many take home coding tests as possible to save time. i'm thinking about some common features inside this function like:\n- if categorical variable &gt; 10, take top 10 and group the rest into others\n- if date of birth/created date present, then subtract from today to see aging group.\n- seaborn correlation -----> plot !!! /matrix among numerical features\n- option to fillna with zero, or impute (depend)\n- different method for dealing outlier like quantile clip, or log scale\n- box -----> plot !!!  between each categorical variable with each numerical variable to overview the distribution by groups.\n\nof course, pandas profiling is the first step. \nanyone already implemented this idea? any recommendation you would add? many thanks.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/o8c3hg/preproc_function_to_prepare_for_take_home_tests/',)", "identifyer": 5589782, "year": "2021"}, {"autor": "nitz_d_blitz", "date": 1635417560000, "content": "Cluster analysis goodness of fit /!/ I'm doing cluster analysis for a retailer spread across multiple countries. This is a gulf retailers so most of the shoppers here are foreigners. With customer data (aggregated transaction metrics and demographics) from 2018 Jan the request was to create customer personas. I'm just worried that this is not possible as the data will not be a good fit for creating clusters as there was covid. Is there anyway to check if my data will good boundaries for the clusters we create. Elbow plot and silhouette scores doesn't make sense I think, correct me if I'm wrong. Is there anyway to control the boundary conditions of clusters", "link": "https://www.reddit.com/r/datascience/comments/qhk4v6/cluster_analysis_goodness_of_fit/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "cluster analysis goodness of fit /!/ i'm doing cluster analysis for a retailer spread across multiple countries. this is a gulf retailers so most of the shoppers here are foreigners. with customer data (aggregated transaction metrics and demographics) from 2018 jan the request was to create customer personas. i'm just worried that this is not possible as the data will not be a good fit for creating clusters as there was covid. is there anyway to check if my data will good boundaries for the clusters we create. elbow -----> plot !!!  and silhouette scores doesn't make sense i think, correct me if i'm wrong. is there anyway to control the boundary conditions of clusters", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 10, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/qhk4v6/cluster_analysis_goodness_of_fit/',)", "identifyer": 5589963, "year": "2021"}, {"autor": "Myzziah", "date": 1623041943000, "content": "Horn\u2019s Parallel Analysis in Python: Am I doing it correctly? /!/ Hi all, been learning Factor Analysis for the first time using datasets from Kaggle. I\u2019ve been using Factor Analysis to break down the dimensionality of the datasets, and want to justify the number of factors to keep with Parallel Analysis (other than Kaiser Criterion and Scree Plot).\n\nThere\u2019s literally nothing I can find on Parallel Analysis (PA) in Python, so I read a paper called: \u2018Parallel Analysis: a method for determining significant principal components\u2019. It suggests generating a random matrix with the same number of variables and samples. After standardising my dataset, I set mean = 0 and dev = 1 for my random matrix, hoping to extract the eigenvalues of the random matrix and perform Parallel Analysis. My end Scree Plot  result of the synthetic data was very lackluster - almost a horizontal line with eigenvalues all close to 1 (basically I would be doing a glorified Kaiser Criterion comparison).\n\nSo have I done something wrong? Are there any resources on PA in Python?", "link": "https://www.reddit.com/r/datascience/comments/nu4deh/horns_parallel_analysis_in_python_am_i_doing_it/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "horn\u2019s parallel analysis in python: am i doing it correctly? /!/ hi all, been learning factor analysis for the first time using datasets from kaggle. i\u2019ve been using factor analysis to break down the dimensionality of the datasets, and want to justify the number of factors to keep with parallel analysis (other than kaiser criterion and scree -----> plot !!! ).\n\nthere\u2019s literally nothing i can find on parallel analysis (pa) in python, so i read a paper called: \u2018parallel analysis: a method for determining significant principal components\u2019. it suggests generating a random matrix with the same number of variables and samples. after standardising my dataset, i set mean = 0 and dev = 1 for my random matrix, hoping to extract the eigenvalues of the random matrix and perform parallel analysis. my end scree plot  result of the synthetic data was very lackluster - almost a horizontal line with eigenvalues all close to 1 (basically i would be doing a glorified kaiser criterion comparison).\n\nso have i done something wrong? are there any resources on pa in python?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nu4deh/horns_parallel_analysis_in_python_am_i_doing_it/',)", "identifyer": 5590087, "year": "2021"}, {"autor": "lucius-verus-fan", "date": 1616949203000, "content": "Simple and elegant software for data science /!/ R based software for basic data analysis that I built with simplicity and speed in mind. Wrangle data, run linear regressions, plot, etc. Give the tutorial a try and let me know what you think! I am curious to hear feedback on whether this software would be useful, what features should be added, etc.", "link": "https://www.reddit.com/r/datascience/comments/mf4ate/simple_and_elegant_software_for_data_science/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "simple and elegant software for data science /!/ r based software for basic data analysis that i built with simplicity and speed in mind. wrangle data, run linear regressions, -----> plot !!! , etc. give the tutorial a try and let me know what you think! i am curious to hear feedback on whether this software would be useful, what features should be added, etc.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/mf4ate/simple_and_elegant_software_for_data_science/',)", "identifyer": 5590141, "year": "2021"}, {"autor": "lucius-verus-fan", "date": 1616880045000, "content": "Simple and elegant software for data science /!/ [www.windts.app](https://www.windts.app/)\n\nR based software for basic data analysis that I built with simplicity and speed in mind. Wrangle data, run linear regressions, plot, etc. Give the tutorial a try and let me know what you think! I am curious to hear feedback on whether this software would be useful, what features should be added, etc.", "link": "https://www.reddit.com/r/datascience/comments/memzaa/simple_and_elegant_software_for_data_science/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "simple and elegant software for data science /!/ [www.windts.app](https://www.windts.app/)\n\nr based software for basic data analysis that i built with simplicity and speed in mind. wrangle data, run linear regressions, -----> plot !!! , etc. give the tutorial a try and let me know what you think! i am curious to hear feedback on whether this software would be useful, what features should be added, etc.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/memzaa/simple_and_elegant_software_for_data_science/',)", "identifyer": 5590164, "year": "2021"}, {"autor": "Lameo23j", "date": 1615983294000, "content": "How do I use sklearn metrics (such as roc_curve) when I am using GPU-enabled arrays and other data structures (cupy data structures)? /!/ I am trying to plot an ROC-AUC curve for my binary classification model which I have created using RAPIDS libraries. Using a KNN model from cuML and while I can get the roc\\_auc\\_score, I have not yet figured out how to plot the ROC-AUC graph.  \nI am trying to use sklearn's roc\\_curve to get the true and false positive rate and upon passing 'y\\_test' and 'y\\_score' (prediction probability) as arguments to roc\\_curve, I get the following error:   \n\" Implicit conversion to a host NumPy array via \\_\\_array\\_\\_ is not allowed, To explicitly construct a GPU array, consider using cupy.asarray(...) To explicitly construct a host array, consider using .to\\_array() \"  \nUpon receiving this error, I tried to convert my y\\_test and y\\_score to cupy.ndarray using 'cupy.asarray()' and passed them again to roc\\_curve and received a new error:  \n\" TypeError: Implicit conversion to a NumPy array is not allowed. Please use \\`.get()\\` to construct a NumPy array explicitly. \"  \nI am stuck here. Any help would be appreciated!  \nThanks", "link": "https://www.reddit.com/r/datascience/comments/m6z2cq/how_do_i_use_sklearn_metrics_such_as_roc_curve/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "how do i use sklearn metrics (such as roc_curve) when i am using gpu-enabled arrays and other data structures (cupy data structures)? /!/ i am trying to -----> plot !!!  an roc-auc curve for my binary classification model which i have created using rapids libraries. using a knn model from cuml and while i can get the roc\\_auc\\_score, i have not yet figured out how to plot the roc-auc graph.  \ni am trying to use sklearn's roc\\_curve to get the true and false positive rate and upon passing 'y\\_test' and 'y\\_score' (prediction probability) as arguments to roc\\_curve, i get the following error:   \n\" implicit conversion to a host numpy array via \\_\\_array\\_\\_ is not allowed, to explicitly construct a gpu array, consider using cupy.asarray(...) to explicitly construct a host array, consider using .to\\_array() \"  \nupon receiving this error, i tried to convert my y\\_test and y\\_score to cupy.ndarray using 'cupy.asarray()' and passed them again to roc\\_curve and received a new error:  \n\" typeerror: implicit conversion to a numpy array is not allowed. please use \\`.get()\\` to construct a numpy array explicitly. \"  \ni am stuck here. any help would be appreciated!  \nthanks", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/m6z2cq/how_do_i_use_sklearn_metrics_such_as_roc_curve/',)", "identifyer": 5590192, "year": "2021"}, {"autor": "ChupitosR", "date": 1620841327000, "content": "C\u00f3mo hacer un radar (spider) plot en R y Rstudio [Chupito de R]", "link": "https://www.reddit.com/r/datascience/comments/naumre/c\u00f3mo_hacer_un_radar_spider_plot_en_r_y_rstudio/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "c\u00f3mo hacer un radar (spider) -----> plot !!!  en r y rstudio [chupito de r]", "sortedWord": "None", "removed": "('reddit',)", "score": 1, "comments": 0, "media": "('rich:video',)", "medialink": "('https://youtube.com/watch?v=tqWSyRMTn5w&amp;feature=share',)", "identifyer": 5590397, "year": "2021"}, {"autor": "fortuitous_monkey", "date": 1621604517000, "content": "I'm fed up of seeing continuous data from tests in the time domain. It offers very little value. /!/ This drives me nuts. \n\nUnless you are looking for a relationship between time and your variable, graphing in time is useless. \n\nI so often see: two parameters plotted on the same graph (x axis time) where people are trying to establish a correlation. \n\nThis isn't limited to the fresh faced grad who's just discovered R. But experienced technical experts in a field, typically engineering data in the fields I work (data logging etc.)\n\nIf you want to visually assess correlation between variables, plot them X vs Y, and then lets have a look.", "link": "https://www.reddit.com/r/datascience/comments/nhstoa/im_fed_up_of_seeing_continuous_data_from_tests_in/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "i'm fed up of seeing continuous data from tests in the time domain. it offers very little value. /!/ this drives me nuts. \n\nunless you are looking for a relationship between time and your variable, graphing in time is useless. \n\ni so often see: two parameters plotted on the same graph (x axis time) where people are trying to establish a correlation. \n\nthis isn't limited to the fresh faced grad who's just discovered r. but experienced technical experts in a field, typically engineering data in the fields i work (data logging etc.)\n\nif you want to visually assess correlation between variables, -----> plot !!!  them x vs y, and then lets have a look.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 51, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nhstoa/im_fed_up_of_seeing_continuous_data_from_tests_in/',)", "identifyer": 5590666, "year": "2021"}, {"autor": "bobskithememe", "date": 1626173905000, "content": "Data Analysis and Visualisation on High-Dimensional Dataset /!/ Hello, all!\n\nI currently have a large dataset consisting of 68,000 rows and 1,550 columns. The dataset is an amalgamation of several datasets I've cleaned and merged together. Before modelling, I would like to carry out EDA and visualisation to see how the features play out and to uncover patterns etc.\n\nI'm not entirely sure what's the best approach given the dataset has 1,550 features. I'm planning to do a lot of PCA and clustering given I cannot really plot individual features from the start. Once, I've found interesting relationships and trends, I can do more conventional plotting with features.\n\nI was thinking of using affinity propagation, mean shift, DBSCAN, birch and a Gaussian mixture model but I'm unsure whether to do it on a group of features or all at once.\n\nI'm interested to hear any ideas on how you would approach this or what would be the best plan of attack?\n\nThanks in advance!  \n(also posted on r/MachineLearning)", "link": "https://www.reddit.com/r/datascience/comments/ojd72l/data_analysis_and_visualisation_on/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "data analysis and visualisation on high-dimensional dataset /!/ hello, all!\n\ni currently have a large dataset consisting of 68,000 rows and 1,550 columns. the dataset is an amalgamation of several datasets i've cleaned and merged together. before modelling, i would like to carry out eda and visualisation to see how the features play out and to uncover patterns etc.\n\ni'm not entirely sure what's the best approach given the dataset has 1,550 features. i'm planning to do a lot of pca and clustering given i cannot really -----> plot !!!  individual features from the start. once, i've found interesting relationships and trends, i can do more conventional plotting with features.\n\ni was thinking of using affinity propagation, mean shift, dbscan, birch and a gaussian mixture model but i'm unsure whether to do it on a group of features or all at once.\n\ni'm interested to hear any ideas on how you would approach this or what would be the best plan of attack?\n\nthanks in advance!  \n(also posted on r/machinelearning)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/ojd72l/data_analysis_and_visualisation_on/',)", "identifyer": 5590862, "year": "2021"}, {"autor": "Honest_Lingonberry59", "date": 1626155867000, "content": "help needed to understand KDE plot /!/ &amp;#x200B;\n\nhttps://preview.redd.it/i5miehvp6xa71.png?width=2160&amp;format=png&amp;auto=webp&amp;s=628a5325d4150b3bf1472b2ea3c4a1c2ca9e9b77\n\nI plotted logarithmic datas as KDE plot about the total confirmed, total deaths, total recovered cases of worldwide covid19data . What insights can be draw from the above plot??????", "link": "https://www.reddit.com/r/datascience/comments/oj9fi5/help_needed_to_understand_kde_plot/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "help needed to understand kde -----> plot !!!  /!/ &amp;#x200b;\n\nhttps://preview.redd.it/i5miehvp6xa71.png?width=2160&amp;format=png&amp;auto=webp&amp;s=628a5325d4150b3bf1472b2ea3c4a1c2ca9e9b77\n\ni plotted logarithmic datas as kde -----> plot !!!  about the total confirmed, total deaths, total recovered cases of worldwide covid19data . what insights can be draw from the above plot??????", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/oj9fi5/help_needed_to_understand_kde_plot/',)", "identifyer": 5590868, "year": "2021"}, {"autor": "Honest_Lingonberry59", "date": 1626155369000, "content": "help needed to understand KDE plot. /!/ I plotted logarithmic datas as KDE plot about the total confirmed, total deaths, total recovered cases of worldwide covid19data . What insights can be draw from the above plot??????", "link": "https://www.reddit.com/r/datascience/comments/oj9bij/help_needed_to_understand_kde_plot/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "help needed to understand kde -----> plot !!! . /!/ i plotted logarithmic datas as kde plot about the total confirmed, total deaths, total recovered cases of worldwide covid19data . what insights can be draw from the above plot??????", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/oj9bij/help_needed_to_understand_kde_plot/',)", "identifyer": 5590869, "year": "2021"}, {"autor": "National-Bread3799", "date": 1626101369000, "content": "What are the drawbacks of box plot method and convex hull method for outlier detection? /!/ Original question I have is-: what are the drawbacks of graphical approach for anomaly detection?", "link": "https://www.reddit.com/r/datascience/comments/oissar/what_are_the_drawbacks_of_box_plot_method_and/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "what are the drawbacks of box -----> plot !!!  method and convex hull method for outlier detection? /!/ original question i have is-: what are the drawbacks of graphical approach for anomaly detection?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/oissar/what_are_the_drawbacks_of_box_plot_method_and/',)", "identifyer": 5590892, "year": "2021"}, {"autor": "HadenMoore-", "date": 1626100485000, "content": "Suggestions for Data Visualizations in Python for Cost of Doing Business Report /!/ Looking for suggestions on what kind of Plot, Graph, or other things I should include in this report for my companies CFO. I'm currently using Python with Pandas. \n\nI appreciate any ideas and thoughts.", "link": "https://www.reddit.com/r/datascience/comments/oisht6/suggestions_for_data_visualizations_in_python_for/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "suggestions for data visualizations in python for cost of doing business report /!/ looking for suggestions on what kind of -----> plot !!! , graph, or other things i should include in this report for my companies cfo. i'm currently using python with pandas. \n\ni appreciate any ideas and thoughts.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/oisht6/suggestions_for_data_visualizations_in_python_for/',)", "identifyer": 5590895, "year": "2021"}, {"autor": "brunocborges", "date": 1610491498000, "content": "[Video] Why and How to Plot 1001k scientific data points @ 25 Hz", "link": "https://www.reddit.com/r/datascience/comments/kw2qzq/video_why_and_how_to_plot_1001k_scientific_data/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "[video] why and how to -----> plot !!!  1001k scientific data points @ 25 hz", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('rich:video',)", "medialink": "('https://www.youtube.com/watch?v=NK4pgRF9XWk',)", "identifyer": 5590966, "year": "2021"}, {"autor": "n3ur0n3rd", "date": 1622053842000, "content": "Multiple line plot from multiple csv files /!/ [removed]", "link": "https://www.reddit.com/r/datascience/comments/nlnyxw/multiple_line_plot_from_multiple_csv_files/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "multiple line -----> plot !!!  from multiple csv files /!/ [removed]", "sortedWord": "None", "removed": "('reddit',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nlnyxw/multiple_line_plot_from_multiple_csv_files/',)", "identifyer": 5591098, "year": "2021"}, {"autor": "albions-angel", "date": 1631469226000, "content": "Where to find region boundary data to match Google Covid Mobility Data? /!/ Hi all, new here. \n\nI wanted to play around with the Google Covid-19 mobility data that is publicly available. Just for fun, and because I wanted to try out some data vis stuff I haven't done before. I have immediately hit a road block. \n\nHaving downloaded the Google Covid-19 Mobility Report csv for Great Britain (for 2020), I find that the documentation seems to give no indication of how Google defines its sub-regions. I thought maybe they follow the Ordinance Survey Boundries datasets, which I can access as .shp files and thus plot in Python, but they dont match up - Google is short by several hundred districts, but has far too many regions for counties. \n\nDoes anyone know how Google define their sub-regions? And if those are also available for download (as .shp files) somewhere?", "link": "https://www.reddit.com/r/datascience/comments/pmxj6o/where_to_find_region_boundary_data_to_match/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "where to find region boundary data to match google covid mobility data? /!/ hi all, new here. \n\ni wanted to play around with the google covid-19 mobility data that is publicly available. just for fun, and because i wanted to try out some data vis stuff i haven't done before. i have immediately hit a road block. \n\nhaving downloaded the google covid-19 mobility report csv for great britain (for 2020), i find that the documentation seems to give no indication of how google defines its sub-regions. i thought maybe they follow the ordinance survey boundries datasets, which i can access as .shp files and thus -----> plot !!!  in python, but they dont match up - google is short by several hundred districts, but has far too many regions for counties. \n\ndoes anyone know how google define their sub-regions? and if those are also available for download (as .shp files) somewhere?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pmxj6o/where_to_find_region_boundary_data_to_match/',)", "identifyer": 5591341, "year": "2021"}, {"autor": "Limp-Ad-7289", "date": 1614713871000, "content": "A tale in Neural Network Plots....(Keras in R, need feedback!) /!/  So I've been messing around with a synthetic dataset. Here's what it looks like, I'm trying to predict attrition based on 3 potential outcomes.\n\nThis is only my training set which includes 5800 observations, I'm trying to revise my training methods before predicting.\n\n**3 class categorical response:** 0,1,2  | Highly imbalanced, 0 = 85% of dataset, 1 = \\~14.5%, 2 = 0.5%\n\n**15 predictors:** Majority are categorical, I tried to reduce cardinality through grouping and transformations. Predictors are a mixed bag of title, role, dept. etc. etc. However, some have 50 different classes (*factors* in R)\n\n**NN Details:**\n\n* Layer 1: 15 inputs, 100 nodes, relu\n* Layer 2: 50 nodes, relu\n* Layer 3: 20 nodes, relu\n* Layer 4: 3 nodes, softmax (classification)\n* Lossfunction: Categorical crossentropy\n* Optimizer: Adam with LR = 0.00001\n* 1000 Epochs, 0.1 Validation Split, classweights included (see below code) \n\nI know in these cases it is best to use a random forest for categorical classifiers, but I really wanted to try neural networks so i attempted all this in R using the Keras library.\n\nFor a youtube reference to show a modelling approach I followed, you can see [here](https://www.youtube.com/watch?v=SrQw_fWo4lw)\n\nSo here's where I'm at and what I've encountered, along with some thoughts as to why...appreciate your feedback on these plots (all generated from Keras, during the training stage) and questions. Thanks!\n\n[Plot #1:](https://ibb.co/Nm0C24G)What happened here? Why did my CV suddenly dip? Any thoughts? I'm stumped here...\n\n[Plot #2:](https://ibb.co/ChWNQzS) I included dropouts after every layer, which led to this plot, my CV is yet again higher than my accuracy, what's happening at the elbow near epoch 600? Did the model calculate/find a new minima for the loss function? \n\n[Plot #3:](https://ibb.co/MkqVPsn) I think this is the best, removed dropouts....but yet again my CV loss is lower than actual loss. My suspicion is that when I add weights to help with the class imbalances, it inflates my loss as a result...is that correct? So not an issue?\n\nBeyond that, anything else that is good for me to know here? I'm pretty happy and proud that I put this all together.....i did a ton more programming around this, feature engineering, and lots of side reading too, and R isn't as popular as Python, but I'm doing a bunch in R anyway these days so it was more intuitive to work with.....thanks in advance!\n\n**Here is my code (R)**\n\nkerasModel2 &lt;- keras\\_model\\_sequential()\n\nkerasModel2 %&gt;% \n\nlayer\\_dense(units = 100, activation = 'relu', input\\_shape = c(15)) %&gt;%    #15 input parameters                             \nlayer\\_dense(units = 50, activation = 'relu') %&gt;%                                                         \nlayer\\_dense(units = 20, activation = 'relu') %&gt;%                                                                                                                              \nlayer\\_dense(units = 3, activation = 'softmax') #3 class responses\n\nkerasModel2 %&gt;% compile(loss = 'categorical\\_crossentropy',                     #multinomial loss                                         \noptimizer = optimizer\\_adamax(lr = 0.00001)                                                                                                          \nmetrics = c('accuracy'))\n\nxMatrix &lt;- data.matrix(x) #data formatting for Keras\n\nyMatrix &lt;- data.matrix(y) #data formatting for Keras\n\nyMatrix &lt;- yMatrix - 1 #my classes were actually 1/2/3, so i run this to change to 0/1/2 mapping\n\nlabels &lt;- to\\_categorical(yMatrix, num\\_classes = 3) #data formatting for Keras\n\nkerasModel2Fit &lt;- kerasModel2 %&gt;%  fit(xMatrix, labels, epochs = 1000, validation\\_split = 0.1, batch\\_size = 128, class\\_weight = list(\"0\" = 1, \"1\" = 2, \"2\" = 4)) #added in class weights, but some plots do not use them", "link": "https://www.reddit.com/r/datascience/comments/lwakrv/a_tale_in_neural_network_plotskeras_in_r_need/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "a tale in neural network plots....(keras in r, need feedback!) /!/  so i've been messing around with a synthetic dataset. here's what it looks like, i'm trying to predict attrition based on 3 potential outcomes.\n\nthis is only my training set which includes 5800 observations, i'm trying to revise my training methods before predicting.\n\n**3 class categorical response:** 0,1,2  | highly imbalanced, 0 = 85% of dataset, 1 = \\~14.5%, 2 = 0.5%\n\n**15 predictors:** majority are categorical, i tried to reduce cardinality through grouping and transformations. predictors are a mixed bag of title, role, dept. etc. etc. however, some have 50 different classes (*factors* in r)\n\n**nn details:**\n\n* layer 1: 15 inputs, 100 nodes, relu\n* layer 2: 50 nodes, relu\n* layer 3: 20 nodes, relu\n* layer 4: 3 nodes, softmax (classification)\n* lossfunction: categorical crossentropy\n* optimizer: adam with lr = 0.00001\n* 1000 epochs, 0.1 validation split, classweights included (see below code) \n\ni know in these cases it is best to use a random forest for categorical classifiers, but i really wanted to try neural networks so i attempted all this in r using the keras library.\n\nfor a youtube reference to show a modelling approach i followed, you can see [here](https://www.youtube.com/watch?v=srqw_fwo4lw)\n\nso here's where i'm at and what i've encountered, along with some thoughts as to why...appreciate your feedback on these plots (all generated from keras, during the training stage) and questions. thanks!\n\n[-----> plot !!!  #1:](https://ibb.co/nm0c24g)what happened here? why did my cv suddenly dip? any thoughts? i'm stumped here...\n\n[plot #2:](https://ibb.co/chwnqzs) i included dropouts after every layer, which led to this plot, my cv is yet again higher than my accuracy, what's happening at the elbow near epoch 600? did the model calculate/find a new minima for the loss function? \n\n[plot #3:](https://ibb.co/mkqvpsn) i think this is the best, removed dropouts....but yet again my cv loss is lower than actual loss. my suspicion is that when i add weights to help with the class imbalances, it inflates my loss as a result...is that correct? so not an issue?\n\nbeyond that, anything else that is good for me to know here? i'm pretty happy and proud that i put this all together.....i did a ton more programming around this, feature engineering, and lots of side reading too, and r isn't as popular as python, but i'm doing a bunch in r anyway these days so it was more intuitive to work with.....thanks in advance!\n\n**here is my code (r)**\n\nkerasmodel2 &lt;- keras\\_model\\_sequential()\n\nkerasmodel2 %&gt;% \n\nlayer\\_dense(units = 100, activation = 'relu', input\\_shape = c(15)) %&gt;%    #15 input parameters                             \nlayer\\_dense(units = 50, activation = 'relu') %&gt;%                                                         \nlayer\\_dense(units = 20, activation = 'relu') %&gt;%                                                                                                                              \nlayer\\_dense(units = 3, activation = 'softmax') #3 class responses\n\nkerasmodel2 %&gt;% compile(loss = 'categorical\\_crossentropy',                     #multinomial loss                                         \noptimizer = optimizer\\_adamax(lr = 0.00001)                                                                                                          \nmetrics = c('accuracy'))\n\nxmatrix &lt;- data.matrix(x) #data formatting for keras\n\nymatrix &lt;- data.matrix(y) #data formatting for keras\n\nymatrix &lt;- ymatrix - 1 #my classes were actually 1/2/3, so i run this to change to 0/1/2 mapping\n\nlabels &lt;- to\\_categorical(ymatrix, num\\_classes = 3) #data formatting for keras\n\nkerasmodel2fit &lt;- kerasmodel2 %&gt;%  fit(xmatrix, labels, epochs = 1000, validation\\_split = 0.1, batch\\_size = 128, class\\_weight = list(\"0\" = 1, \"1\" = 2, \"2\" = 4)) #added in class weights, but some plots do not use them", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/lwakrv/a_tale_in_neural_network_plotskeras_in_r_need/',)", "identifyer": 5591555, "year": "2021"}, {"autor": "veeeerain", "date": 1629038852000, "content": "Kinda wish there were better data viz libraries in python for deep learning /!/ Yes I know, there is the plotly, the matplotlib, and seaborn etc. but it gets really hard to plot with matplotlib when you are doing deep learning. Especially unsupervised deep learning. I\u2019ve been trying to do some research on generative deep learning because I thought it was interesting, and looking through code examples, and man, the amount of ugly matplotlib code is just horrible. People are using for loops!  I just wish there was better visualizing libraries for deep learning. I like R for ggplot2 but if I\u2019m using pytorch and need to visualize things matplotlib is what everyone uses and it\u2019s just tough.", "link": "https://www.reddit.com/r/datascience/comments/p4utdw/kinda_wish_there_were_better_data_viz_libraries/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "kinda wish there were better data viz libraries in python for deep learning /!/ yes i know, there is the plotly, the matplotlib, and seaborn etc. but it gets really hard to -----> plot !!!  with matplotlib when you are doing deep learning. especially unsupervised deep learning. i\u2019ve been trying to do some research on generative deep learning because i thought it was interesting, and looking through code examples, and man, the amount of ugly matplotlib code is just horrible. people are using for loops!  i just wish there was better visualizing libraries for deep learning. i like r for ggplot2 but if i\u2019m using pytorch and need to visualize things matplotlib is what everyone uses and it\u2019s just tough.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 50, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/p4utdw/kinda_wish_there_were_better_data_viz_libraries/',)", "identifyer": 5591663, "year": "2021"}, {"autor": "verdict0224", "date": 1628852361000, "content": "What kind of information can you use violin plots on? (Powerbi) /!/ Power BI has an option to use violin plots, but it's very confusing because it's not like the way it is in Python where you literally just put your values and your categories. You have to have Three different things! Values and sampling, or values sampling and categories. I have not found a way to be able to use them yet in a way that makes sense like you can in Python...\n\nSo for example in Python, I can just model product name by sales amount and now I have five violins. If I try to do this in power bi with their own shitty and heavily confusing violin plot, you will have one violin with a huge amount of data. It makes no sense to me", "link": "https://www.reddit.com/r/datascience/comments/p3l3r8/what_kind_of_information_can_you_use_violin_plots/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "what kind of information can you use violin plots on? (powerbi) /!/ power bi has an option to use violin plots, but it's very confusing because it's not like the way it is in python where you literally just put your values and your categories. you have to have three different things! values and sampling, or values sampling and categories. i have not found a way to be able to use them yet in a way that makes sense like you can in python...\n\nso for example in python, i can just model product name by sales amount and now i have five violins. if i try to do this in power bi with their own shitty and heavily confusing violin -----> plot !!! , you will have one violin with a huge amount of data. it makes no sense to me", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 11, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/p3l3r8/what_kind_of_information_can_you_use_violin_plots/',)", "identifyer": 5591735, "year": "2021"}, {"autor": "FranticToaster", "date": 1617495486000, "content": "Plotting in R's ggplot2 vs Python's Matplotlib: Is it just me or is ggplot2 WAY smoother of an experience than Matplotlib? /!/ I came up in the space using R for ad hoc plotting and EDA, and I'd like to check to see if it's my home base bias warping my perception or if Matplotlib really is a more cumbersome experience for plotting.\n\nIn my experience, ggplot2's chains make plots easy to manage in the code. Functions corresponding to plot elements are simple and take care of all of the customization I could want. Matplotlib, on the other hand, makes me feel like I need to write whole separate programs to build and style my plots.\n\nAm I missing something in Matplotlib that makes it especially powerful for plotting?", "link": "https://www.reddit.com/r/datascience/comments/mjkv5y/plotting_in_rs_ggplot2_vs_pythons_matplotlib_is/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "plotting in r's ggplot2 vs python's matplotlib: is it just me or is ggplot2 way smoother of an experience than matplotlib? /!/ i came up in the space using r for ad hoc plotting and eda, and i'd like to check to see if it's my home base bias warping my perception or if matplotlib really is a more cumbersome experience for plotting.\n\nin my experience, ggplot2's chains make plots easy to manage in the code. functions corresponding to -----> plot !!!  elements are simple and take care of all of the customization i could want. matplotlib, on the other hand, makes me feel like i need to write whole separate programs to build and style my plots.\n\nam i missing something in matplotlib that makes it especially powerful for plotting?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 203, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/mjkv5y/plotting_in_rs_ggplot2_vs_pythons_matplotlib_is/',)", "identifyer": 5591939, "year": "2021"}, {"autor": "RedAndy_", "date": 1617913447000, "content": "Pareto plot /!/ [removed]", "link": "https://www.reddit.com/r/datascience/comments/mn0nc4/pareto_plot/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "pareto -----> plot !!!  /!/ [removed]", "sortedWord": "None", "removed": "('reddit',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/mn0nc4/pareto_plot/',)", "identifyer": 5592191, "year": "2021"}, {"autor": "march-2020", "date": 1610950236000, "content": "matplotlib.pyplot help: Why is my curve smooth, I need the rough curve... /!/ I copied the exact code from the answer key which shows [this](https://i.imgur.com/ZipWnjO.png) plot. However, when I run the exact same code, I get [this](https://i.imgur.com/gctpEv1.png) instead. Any ideas why?", "link": "https://www.reddit.com/r/datascience/comments/kzoql2/matplotlibpyplot_help_why_is_my_curve_smooth_i/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "matplotlib.pyplot help: why is my curve smooth, i need the rough curve... /!/ i copied the exact code from the answer key which shows [this](https://i.imgur.com/zipwnjo.png) -----> plot !!! . however, when i run the exact same code, i get [this](https://i.imgur.com/gctpev1.png) instead. any ideas why?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/kzoql2/matplotlibpyplot_help_why_is_my_curve_smooth_i/',)", "identifyer": 5592271, "year": "2021"}, {"autor": "2435191", "date": 1610941142000, "content": "I made a quick bot to track the size of the New York Times headline! /!/ I'll probably have made a plot by the end of the year, so stay tuned.\n\ngithub: [https://github.com/2435191/GetNYTHeadlineWidth](https://github.com/2435191/GetNYTHeadlineWidth)", "link": "https://www.reddit.com/r/datascience/comments/kzm9ia/i_made_a_quick_bot_to_track_the_size_of_the_new/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "i made a quick bot to track the size of the new york times headline! /!/ i'll probably have made a -----> plot !!!  by the end of the year, so stay tuned.\n\ngithub: [https://github.com/2435191/getnytheadlinewidth](https://github.com/2435191/getnytheadlinewidth)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/kzm9ia/i_made_a_quick_bot_to_track_the_size_of_the_new/',)", "identifyer": 5592274, "year": "2021"}, {"autor": "imm_uol1819", "date": 1610884572000, "content": "Getting mixed results in the output of a linear regression of a faceted scatter plot in R /!/ Hello,\n\nthis is the output of a faceted scatter plot with a linear regression in each graph, aimed at studying the potential relationship between Unemployment Rate and Crime Rate for 3 different types of crimes: Anti-Social Behaviour, Theft, and Violence &amp; Sexual Offences.\n\nHere's the output, and my interpretation follows:\n\n`Call:`\n\n`lm(formula = Crime_occurrences ~ Unemployment_rate + Crime, data = df)`\n\n`Residuals:`\n\n`Min 1Q Median 3Q Max`\n\n`-20871 -6755 362 4597 32818`\n\n`Coefficients:`\n\n`Estimate Std. Error t value Pr(&gt;|t|)`\n\n`(Intercept) 71252 21686 3.286 0.00168 **`\n\n`Unemployment_rate -3508 5327 -0.658 0.51267`\n\n`CrimeTheft -6613 3180 -2.080 0.04169 *`\n\n`CrimeViolence and sexual offences 5606 3180 1.763 0.08287 .`\n\n`---`\n\n`Signif. codes: 0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1`\n\n`Residual standard error: 10550 on 62 degrees of freedom`\n\n`Multiple R-squared: 0.1972, Adjusted R-squared: 0.1584`\n\n`F-statistic: 5.077 on 3 and 62 DF, p-value: 0.003301`\n\n**My interpretation:**\n\n\\- t value is extremely low, suggesting no relationship between the two variables\n\n\\- `Pr(&gt;|t|)` is &lt;0.05 for THEFT, suggesting a relationship between that and unemployment rate\n\n\\- R-squared is very low, suggesting that the linear regression line wasn't that succesful at capturing the values in the scatter plot (i.e. the residuals are all over the place, which is true; another sign of no relationship)\n\n\\- F is fairly low, suggesting no relationship\n\n\\- p value is &lt;0.05, suggesting a strong relationship\n\n**My questions:**\n\n\\- Where is \"Anti-Social Behaviour\"? It's in my data frame but not in this output\n\n\\- What does the p value refer to? Unemployment rate and WHICH type of Crime?\n\n\\- How should I interpret the discrepancy between Theft `Pr(&gt;|t|)` being &lt;0.05 but t value being not only low, but even negative?\n\nAny suggestions would be highly appreciated!", "link": "https://www.reddit.com/r/datascience/comments/kz56qj/getting_mixed_results_in_the_output_of_a_linear/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "getting mixed results in the output of a linear regression of a faceted scatter -----> plot !!!  in r /!/ hello,\n\nthis is the output of a faceted scatter plot with a linear regression in each graph, aimed at studying the potential relationship between unemployment rate and crime rate for 3 different types of crimes: anti-social behaviour, theft, and violence &amp; sexual offences.\n\nhere's the output, and my interpretation follows:\n\n`call:`\n\n`lm(formula = crime_occurrences ~ unemployment_rate + crime, data = df)`\n\n`residuals:`\n\n`min 1q median 3q max`\n\n`-20871 -6755 362 4597 32818`\n\n`coefficients:`\n\n`estimate std. error t value pr(&gt;|t|)`\n\n`(intercept) 71252 21686 3.286 0.00168 **`\n\n`unemployment_rate -3508 5327 -0.658 0.51267`\n\n`crimetheft -6613 3180 -2.080 0.04169 *`\n\n`crimeviolence and sexual offences 5606 3180 1.763 0.08287 .`\n\n`---`\n\n`signif. codes: 0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1`\n\n`residual standard error: 10550 on 62 degrees of freedom`\n\n`multiple r-squared: 0.1972, adjusted r-squared: 0.1584`\n\n`f-statistic: 5.077 on 3 and 62 df, p-value: 0.003301`\n\n**my interpretation:**\n\n\\- t value is extremely low, suggesting no relationship between the two variables\n\n\\- `pr(&gt;|t|)` is &lt;0.05 for theft, suggesting a relationship between that and unemployment rate\n\n\\- r-squared is very low, suggesting that the linear regression line wasn't that succesful at capturing the values in the scatter plot (i.e. the residuals are all over the place, which is true; another sign of no relationship)\n\n\\- f is fairly low, suggesting no relationship\n\n\\- p value is &lt;0.05, suggesting a strong relationship\n\n**my questions:**\n\n\\- where is \"anti-social behaviour\"? it's in my data frame but not in this output\n\n\\- what does the p value refer to? unemployment rate and which type of crime?\n\n\\- how should i interpret the discrepancy between theft `pr(&gt;|t|)` being &lt;0.05 but t value being not only low, but even negative?\n\nany suggestions would be highly appreciated!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/kz56qj/getting_mixed_results_in_the_output_of_a_linear/',)", "identifyer": 5592304, "year": "2021"}, {"autor": "WadeEffingWilson", "date": 1609709592000, "content": "Why are there points that exist above the max line and below the min line in a Box &amp; Whisker plot? Are the min/max lines not really min/max? /!/ I sat down last night and worked through plotting out a Box &amp; Whisker plot to visualize the distribution of points in one of my datasets. It was fairly simple and direct but I noticed that I have a point that exists above the max line for one of my days. I only used a single week out of a much larger dataset but I don't think this is unique. Looking online, I see numerous examples of outliers in Box &amp; Whisker plots, which doesn't make sense to me. The whiskers are supposed to terminate at the min and max values, so the min/max isn't correct if something exists beyond it, right?\n\n&amp;#x200B;\n\nI think that I'm wrong in assuming that the min/max are true min/max, but everywhere I've looked, it states the bottom whisker is for min -&gt; Q1 and the top whisker is for Q3 -&gt; max, but the evidence is contradictory.\n\n&amp;#x200B;\n\nSorry, I'm completely new to DS and I'm fumbling my way through. Figured I would come here and ask.", "link": "https://www.reddit.com/r/datascience/comments/kptlcy/why_are_there_points_that_exist_above_the_max/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "why are there points that exist above the max line and below the min line in a box &amp; whisker -----> plot !!! ? are the min/max lines not really min/max? /!/ i sat down last night and worked through plotting out a box &amp; whisker plot to visualize the distribution of points in one of my datasets. it was fairly simple and direct but i noticed that i have a point that exists above the max line for one of my days. i only used a single week out of a much larger dataset but i don't think this is unique. looking online, i see numerous examples of outliers in box &amp; whisker plots, which doesn't make sense to me. the whiskers are supposed to terminate at the min and max values, so the min/max isn't correct if something exists beyond it, right?\n\n&amp;#x200b;\n\ni think that i'm wrong in assuming that the min/max are true min/max, but everywhere i've looked, it states the bottom whisker is for min -&gt; q1 and the top whisker is for q3 -&gt; max, but the evidence is contradictory.\n\n&amp;#x200b;\n\nsorry, i'm completely new to ds and i'm fumbling my way through. figured i would come here and ask.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 9, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/kptlcy/why_are_there_points_that_exist_above_the_max/',)", "identifyer": 5592409, "year": "2021"}, {"autor": "failureforeverr", "date": 1620063283000, "content": "For those studying on your own, do you often feel that your pace is too slow? /!/ Even though I do have a background in CS with a minor in math and I'm trying to study almost every day, it seems that I'm making slow progress. \n\nI have no problems regarding the programming part, but I have such a hard time grasping math &amp; statistics concepts and it's been a month since I started studying. Today I spent 2 hours trying to understand how to plot a figure and what linear regression is. I'm still not sure what's the purpose of linear regression or what train/test datasets are.\n\nIt's so discouraging because I like studying Data science so much, but does it really have to take me 2 hours to understand how to build a scatterplot and linear regression? I would gladly dedicate all of my time to these, but I have 9 classes this semester and I don't really have much time to work on my personal projects. \n\nIs it normal? Are my expectations too high? I'm not cut out for this and I should just quit?", "link": "https://www.reddit.com/r/datascience/comments/n423um/for_those_studying_on_your_own_do_you_often_feel/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "for those studying on your own, do you often feel that your pace is too slow? /!/ even though i do have a background in cs with a minor in math and i'm trying to study almost every day, it seems that i'm making slow progress. \n\ni have no problems regarding the programming part, but i have such a hard time grasping math &amp; statistics concepts and it's been a month since i started studying. today i spent 2 hours trying to understand how to -----> plot !!!  a figure and what linear regression is. i'm still not sure what's the purpose of linear regression or what train/test datasets are.\n\nit's so discouraging because i like studying data science so much, but does it really have to take me 2 hours to understand how to build a scatterplot and linear regression? i would gladly dedicate all of my time to these, but i have 9 classes this semester and i don't really have much time to work on my personal projects. \n\nis it normal? are my expectations too high? i'm not cut out for this and i should just quit?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/n423um/for_those_studying_on_your_own_do_you_often_feel/',)", "identifyer": 5592494, "year": "2021"}, {"autor": "jj4646", "date": 1620167182000, "content": "Basic questions about linear regression /!/ 1) often we are told that linear regression models with many beta coefficients are \"undesirable\" (e.g. 100 beta coefficients, 100 predictors) - in general, these models are said to be unstable, high variance and likely to perform poorly. Does anyone know why this is?\n\n2) this is a common sense question: suppose you have 3 variables (1 response, 2 predictors). You make a plot of these 3 variables and you notice that the plot of this data is clearly NOT linear. Therefore, you would decide NOT to use a linear regression model. Is there any mathematical logic that shows why a linear regression model is unable to well represent non-linear data? For 3 variables, i guess you could show this visually - but for higher dimensional data, what is the mathematical justification used to understand why a linear model can not capture non linear data?\n\n3) in the previous question, i asked if linear models are \"too rigid\" to capture non linear patterns. But what about the opposite? Suppose you take the same example with the 3 variables : 2 predictors and 1 response. This time you have new data and make a plot, and the data appears to have a strong linear patterns. In this example, if you had still chosen to use a non-linear model : has their been any mathematical research that examines the ability of a non-linear model to capture linear patterns? In this example, would a linear model have some advantage at recognizing and capturing linear patterns compared to a non-linear model? Or in general, are linear patterns completely within the domain of non-linear patterns and as such, non-linear models are not expected to have any disadvantages at recognizing linear patterns (compared to linear models)?\n\n4) Are non-linear patterns more likely to occur in bigger datasets (more columns and more rows)? Could we not say that if there are more data points, there exist more geometric configurations that these data points can be arranged in -  making non-linear arrangements more probable?", "link": "https://www.reddit.com/r/datascience/comments/n505et/basic_questions_about_linear_regression/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "basic questions about linear regression /!/ 1) often we are told that linear regression models with many beta coefficients are \"undesirable\" (e.g. 100 beta coefficients, 100 predictors) - in general, these models are said to be unstable, high variance and likely to perform poorly. does anyone know why this is?\n\n2) this is a common sense question: suppose you have 3 variables (1 response, 2 predictors). you make a -----> plot !!!  of these 3 variables and you notice that the -----> plot !!!  of this data is clearly not linear. therefore, you would decide not to use a linear regression model. is there any mathematical logic that shows why a linear regression model is unable to well represent non-linear data? for 3 variables, i guess you could show this visually - but for higher dimensional data, what is the mathematical justification used to understand why a linear model can not capture non linear data?\n\n3) in the previous question, i asked if linear models are \"too rigid\" to capture non linear patterns. but what about the opposite? suppose you take the same example with the 3 variables : 2 predictors and 1 response. this time you have new data and make a plot, and the data appears to have a strong linear patterns. in this example, if you had still chosen to use a non-linear model : has their been any mathematical research that examines the ability of a non-linear model to capture linear patterns? in this example, would a linear model have some advantage at recognizing and capturing linear patterns compared to a non-linear model? or in general, are linear patterns completely within the domain of non-linear patterns and as such, non-linear models are not expected to have any disadvantages at recognizing linear patterns (compared to linear models)?\n\n4) are non-linear patterns more likely to occur in bigger datasets (more columns and more rows)? could we not say that if there are more data points, there exist more geometric configurations that these data points can be arranged in -  making non-linear arrangements more probable?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/n505et/basic_questions_about_linear_regression/',)", "identifyer": 5592841, "year": "2021"}, {"autor": "ice_shadow", "date": 1634611220000, "content": "Anyone else find data visualization to be a huge pain? /!/ I think its def the most tedious part. Also happens to be the part which requires some more artsy skills and evaluating how a visualization impacts people\u2019s\n\nIts so tedious going back and forth to modify plots constantly. Often times stakeholders want plots in very very specific ways and some stuff just isn\u2019t easily possible in ggplot2. Often times its like literally 50+ plots to get nice and ready. Some are automatable but not always especially when they want annotations or between different groups of plots.\n\nAnd then all the ad-hoc stuff. Like you may have removed a column during data wrangling since it wasn\u2019t relevant earlier but then suddenly they want that column for a plot and you have to go trace it all the way back.\n\nI find data viz often times more cumbersome than just data cleaning.", "link": "https://www.reddit.com/r/datascience/comments/qb24ar/anyone_else_find_data_visualization_to_be_a_huge/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "anyone else find data visualization to be a huge pain? /!/ i think its def the most tedious part. also happens to be the part which requires some more artsy skills and evaluating how a visualization impacts people\u2019s\n\nits so tedious going back and forth to modify plots constantly. often times stakeholders want plots in very very specific ways and some stuff just isn\u2019t easily possible in ggplot2. often times its like literally 50+ plots to get nice and ready. some are automatable but not always especially when they want annotations or between different groups of plots.\n\nand then all the ad-hoc stuff. like you may have removed a column during data wrangling since it wasn\u2019t relevant earlier but then suddenly they want that column for a -----> plot !!!  and you have to go trace it all the way back.\n\ni find data viz often times more cumbersome than just data cleaning.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 99, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/qb24ar/anyone_else_find_data_visualization_to_be_a_huge/',)", "identifyer": 5593043, "year": "2021"}, {"autor": "justdarkblue", "date": 1618974616000, "content": "Basic ANN questions /!/ Hi everyone! Can anyone confirm it is ok to use mixed data (continuous, discrete, and categorical) in a basic ANN? Also, is it ok to use discrete numeric data in a Garson's algorithm plot in R? I am using the basic nnet function in R.", "link": "https://www.reddit.com/r/datascience/comments/mv70xb/basic_ann_questions/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "basic ann questions /!/ hi everyone! can anyone confirm it is ok to use mixed data (continuous, discrete, and categorical) in a basic ann? also, is it ok to use discrete numeric data in a garson's algorithm -----> plot !!!  in r? i am using the basic nnet function in r.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/mv70xb/basic_ann_questions/',)", "identifyer": 5593303, "year": "2021"}, {"autor": "vietlinh12hoa", "date": 1624575650000, "content": "Steps to answer the question customer understanding /!/ I've assigned a task to analyze *customer understanding*. Here's what available in sale data\n\nID, Age, Invoice, Value, Country, Cluster (already segmented), lifetime value (from model), promo tier (from model), purchased product, and revenue.\n\nThe steps I have in my mind are to do:\n\n1. Basic analytics break down into country, age group (or generation), and cluster (like number of invoices, number of client, and revenue)\n2. RFM analysis to identify characteristic of top clients. Then again, break down the analytics of those top clients into country, age, and each cluster.\n3. Product view, identify which products are often purchased for client base and for top clients. (This can be done by checking word frequency in product description). Then again, break down product view into into country, age, and each cluster.\n4. Plot a time series of revenue, number active clients by month to see they respond (multiple line chart series for age, country, and, cluster, so 3 subplots).\n5. Predicting their sales next month, and their next purchase days. (This is optional imo)\n6. Also, an important step is how to transfer those insights into more making money action?\n\nWould love to hear your opinion on this and anyone have further recommendation?  Many thanks.", "link": "https://www.reddit.com/r/datascience/comments/o7b1il/steps_to_answer_the_question_customer/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "steps to answer the question customer understanding /!/ i've assigned a task to analyze *customer understanding*. here's what available in sale data\n\nid, age, invoice, value, country, cluster (already segmented), lifetime value (from model), promo tier (from model), purchased product, and revenue.\n\nthe steps i have in my mind are to do:\n\n1. basic analytics break down into country, age group (or generation), and cluster (like number of invoices, number of client, and revenue)\n2. rfm analysis to identify characteristic of top clients. then again, break down the analytics of those top clients into country, age, and each cluster.\n3. product view, identify which products are often purchased for client base and for top clients. (this can be done by checking word frequency in product description). then again, break down product view into into country, age, and each cluster.\n4. -----> plot !!!  a time series of revenue, number active clients by month to see they respond (multiple line chart series for age, country, and, cluster, so 3 subplots).\n5. predicting their sales next month, and their next purchase days. (this is optional imo)\n6. also, an important step is how to transfer those insights into more making money action?\n\nwould love to hear your opinion on this and anyone have further recommendation?  many thanks.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/o7b1il/steps_to_answer_the_question_customer/',)", "identifyer": 5593473, "year": "2021"}, {"autor": "baldymcbadlstein", "date": 1624480053000, "content": "Question on control/variant testing /!/ Non-data scientist struggling to find the \u201cright\u201d way to set up populations for control vs variant testing, and am wondering if my methodology holds any ground. \n\nWe\u2019re targeting our best customers with a campaign and want to see how much incremental revenue it generates.\n\nTo find the control and variant populations I\u2019m simply ranking the customers by their recency, AOV, order count and other order economics to get to a weighted rank. I\u2019m doing this in SQL, which is the primary language I know.\n\nThen once I have every customer assigned a unique rank I choose only the odd numbered ranks and keep the even ranked customers as the control that is excluded from the campaign.\n\nMy thought is that this would provide an even distribution of our best customers in the control and variant groups.\n\nWhen I plot the distributions for the different kpis the control and variant do look aligned but am not sure how confident I can be in the results.\n\nDoes this methodology for selecting control/variant groups seem ok?", "link": "https://www.reddit.com/r/datascience/comments/o6l9jz/question_on_controlvariant_testing/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "question on control/variant testing /!/ non-data scientist struggling to find the \u201cright\u201d way to set up populations for control vs variant testing, and am wondering if my methodology holds any ground. \n\nwe\u2019re targeting our best customers with a campaign and want to see how much incremental revenue it generates.\n\nto find the control and variant populations i\u2019m simply ranking the customers by their recency, aov, order count and other order economics to get to a weighted rank. i\u2019m doing this in sql, which is the primary language i know.\n\nthen once i have every customer assigned a unique rank i choose only the odd numbered ranks and keep the even ranked customers as the control that is excluded from the campaign.\n\nmy thought is that this would provide an even distribution of our best customers in the control and variant groups.\n\nwhen i -----> plot !!!  the distributions for the different kpis the control and variant do look aligned but am not sure how confident i can be in the results.\n\ndoes this methodology for selecting control/variant groups seem ok?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/o6l9jz/question_on_controlvariant_testing/',)", "identifyer": 5593519, "year": "2021"}, {"autor": "geekcuisine", "date": 1624454458000, "content": "Tidying time-series &amp; behavioral experimental data [Question] /!/ Hi all,\n\nI recently came across the original [Tidy Data paper](https://www.jstatsoft.org/article/view/v059i10) (if you haven't read this, check it out- I wish I had a long time ago) and have been thinking a lot about how to best reorganize data from my experiments to make analyses and visualization easier. However, my datasets seem kind of unconventional and I haven't found great examples for how to best tidy them.\n\nBasically, I continuously collect rodent behavioral data in sessions that span 60 trials. I collect **timestamps** of events of interest (e.g. lever press, trial start). So my data looks something like this currently (I've been playing with both Matlab structs and Python pandas.dataframes). For simplicity I shortened this example to just 3 trials. \n\n&amp;#x200B;\n\n|subject|group|date|trialType|trialStart|behaviorA|\n|:-|:-|:-|:-|:-|:-|\n|1|0|20210601|\\[0,0,1\\]|\\[10,40,60\\]|\\[8,12,42,65\\]|\n|2|1|20210601|\\[0,0,1\\]|\\[10,40,60\\]|\\[15,22,26,48,53\\]|\n|3|0|20210601|\\[0,0,1\\]|\\[10,40,60\\]|\\[12,42,80\\]|\n|1|0|20210602|\\[0,0,1\\]|\\[10,40,60\\]|\\[4,22,45,63\\]|\n|2|1|20210602|\\[0,0,1\\]|\\[10,40,60\\]|\\[30,44,68\\]|\n|3|0|20210602|\\[0,0,1\\]|\\[10,40,60\\]|\\[12,29,43,63,68\\]|\n\n&amp;#x200B;\n\nNotice that the events contain arrays of multiple values, which makes it \"untidy\". Currently I have one row per subject per date (this corresponds to my raw data files). While this organization is intuitive to me, it seems to make it harder to conveniently plot and analyze the data using readily available packages.\n\nIn order to have a single value, I was thinking of reorganizing it so that each row corresponds to a time bin in every file. Instead of event timestamps, the events would be binary coded so that a 1 would be placed at the corresponding timestamp and would be 0 otherwise. It would look like:\n\n&amp;#x200B;\n\n|subject|group|date|timeBin|trialStart|trialType|behaviorA|\n|:-|:-|:-|:-|:-|:-|:-|\n|1|0|20210601|1|0|0|0|\n|1|0|20210601|2|0|0|0|\n|1|0|20210601|3|1|2|0|\n|1|0|20210601|4|0|0|0|\n|1|0|20210601|5|0|0|1|\n|1|0|20210601|6|0|0|0|\n\nMy event timestamps are sampled at 10ms resolution and I have a few hundred files so this could get huge but I guess I could downsample by rounding the timestamps. \n\nThis dataset is pretty much the \"template\" of most of my datasets but I also have more complex datasets where I am continuously tracking a variable over time (e.g. I have video recordings I can use to track position &amp; velocity) so I think organizing things by timestamp may effectively translate to those in the long run?\n\nAn alternative would be to have one row for each trial, but this wouldn't be effective if I am also collecting a continuous variable. \n\nI'd love any feedback &amp; ideas about how to best organize these data! That includes coding resources and packages if you have recommendations.", "link": "https://www.reddit.com/r/datascience/comments/o6cp1u/tidying_timeseries_behavioral_experimental_data/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "tidying time-series &amp; behavioral experimental data [question] /!/ hi all,\n\ni recently came across the original [tidy data paper](https://www.jstatsoft.org/article/view/v059i10) (if you haven't read this, check it out- i wish i had a long time ago) and have been thinking a lot about how to best reorganize data from my experiments to make analyses and visualization easier. however, my datasets seem kind of unconventional and i haven't found great examples for how to best tidy them.\n\nbasically, i continuously collect rodent behavioral data in sessions that span 60 trials. i collect **timestamps** of events of interest (e.g. lever press, trial start). so my data looks something like this currently (i've been playing with both matlab structs and python pandas.dataframes). for simplicity i shortened this example to just 3 trials. \n\n&amp;#x200b;\n\n|subject|group|date|trialtype|trialstart|behaviora|\n|:-|:-|:-|:-|:-|:-|\n|1|0|20210601|\\[0,0,1\\]|\\[10,40,60\\]|\\[8,12,42,65\\]|\n|2|1|20210601|\\[0,0,1\\]|\\[10,40,60\\]|\\[15,22,26,48,53\\]|\n|3|0|20210601|\\[0,0,1\\]|\\[10,40,60\\]|\\[12,42,80\\]|\n|1|0|20210602|\\[0,0,1\\]|\\[10,40,60\\]|\\[4,22,45,63\\]|\n|2|1|20210602|\\[0,0,1\\]|\\[10,40,60\\]|\\[30,44,68\\]|\n|3|0|20210602|\\[0,0,1\\]|\\[10,40,60\\]|\\[12,29,43,63,68\\]|\n\n&amp;#x200b;\n\nnotice that the events contain arrays of multiple values, which makes it \"untidy\". currently i have one row per subject per date (this corresponds to my raw data files). while this organization is intuitive to me, it seems to make it harder to conveniently -----> plot !!!  and analyze the data using readily available packages.\n\nin order to have a single value, i was thinking of reorganizing it so that each row corresponds to a time bin in every file. instead of event timestamps, the events would be binary coded so that a 1 would be placed at the corresponding timestamp and would be 0 otherwise. it would look like:\n\n&amp;#x200b;\n\n|subject|group|date|timebin|trialstart|trialtype|behaviora|\n|:-|:-|:-|:-|:-|:-|:-|\n|1|0|20210601|1|0|0|0|\n|1|0|20210601|2|0|0|0|\n|1|0|20210601|3|1|2|0|\n|1|0|20210601|4|0|0|0|\n|1|0|20210601|5|0|0|1|\n|1|0|20210601|6|0|0|0|\n\nmy event timestamps are sampled at 10ms resolution and i have a few hundred files so this could get huge but i guess i could downsample by rounding the timestamps. \n\nthis dataset is pretty much the \"template\" of most of my datasets but i also have more complex datasets where i am continuously tracking a variable over time (e.g. i have video recordings i can use to track position &amp; velocity) so i think organizing things by timestamp may effectively translate to those in the long run?\n\nan alternative would be to have one row for each trial, but this wouldn't be effective if i am also collecting a continuous variable. \n\ni'd love any feedback &amp; ideas about how to best organize these data! that includes coding resources and packages if you have recommendations.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/o6cp1u/tidying_timeseries_behavioral_experimental_data/',)", "identifyer": 5593538, "year": "2021"}, {"autor": "blueest", "date": 1627179324000, "content": "Is this a good strategy for \"exploratory data analytics\"? /!/ Suppose I have the following data - I created this example using the R programming language. This a standard \"supervised binary classification task\". There are 4 observed variables (var\\_1, var\\_2, var\\_3 ,var\\_4), and the goal is to predict the \"response\" variable:\n\n    library(ggplot2)\n    library(gridExtra)\n    set.seed(123)\n    #create data\n    var_1 = rnorm(1000,10,10) \n    \n    var_2 = rnorm(1000,10,15) \n    \n    var_3 = rnorm(1000,20,5)\n    \n    var_4 = rnorm(1000,10,1)\n    \n    my_data = data.frame(var_1, var_2, var_3, var_4)\n    \n    \n    my_data$response = ifelse(var_1 &gt;20 + rnorm(1,1,1), \"1\", \"0\")\n    \n    my_data$response = as.factor(my_data$response)\n    \n    \n\nSuppose I am given this data, I decide make some quick graphs to look at the distribution of each of these variables with respect to the response variable:\n\n    #first plot\n    p1 = ggplot(my_data) +\n        geom_histogram(aes(x=var_1, fill=response), \n                       colour=\"grey50\", alpha=0.5, position=\"identity\")+ ggtitle(\"var_1 vs response\")\n    \n    #second plot\n    \n    p2 = ggplot(my_data) +\n        geom_histogram(aes(x=var_2, fill=response), \n                       colour=\"grey50\", alpha=0.5, position=\"identity\")+ ggtitle(\"var_2 vs response\")\n    \n    #third plot\n    p3 = ggplot(my_data) +\n        geom_histogram(aes(x=var_3, fill=response), \n                       colour=\"grey50\", alpha=0.5, position=\"identity\")+ ggtitle(\"var_3 vs response\")\n    \n    #fourth plot\n    p4 = ggplot(my_data) +\n        geom_histogram(aes(x=var_4, fill=response), \n                       colour=\"grey50\", alpha=0.5, position=\"identity\")+ ggtitle(\"var_4 vs response\")\n    \n    #display all together\n    grid.arrange(p1, p2, p3, p4, ncol=2) \n\n&amp;#x200B;\n\n[All 4 Graphs](https://preview.redd.it/hp6ppge6p9d71.png?width=1091&amp;format=png&amp;auto=webp&amp;s=c081717b872aba58d25995ebc264715578cd8147)\n\nBased on these graphs: it would appear that \"higher values of var\\_1\" are more associated with a \"response = 1\" , but all other variables do not appear to have this pattern. One could make the premature argument that \"var\\_1\" might be an important variable for discriminating between the response variable.\n\nHowever, the thing which concerns me the most is that \"var\\_1\" **on its own** might be a good indicator for discriminating between the response variable, BUT \"var\\_1\" might not be a good indicator for discriminating between the response variable **along with the other variables.**  In statistics, I think this is called \"Simpson's Paradox\": [https://en.wikipedia.org/wiki/Simpson%27s\\_paradox#/media/File:Simpsons\\_paradox\\_-\\_animation.gif](https://en.wikipedia.org/wiki/Simpson%27s_paradox#/media/File:Simpsons_paradox_-_animation.gif)\n\n**My Question:** When faced with these graphs, can one make a harmful conclusion by believing that \"higher values of \"var\\_1\" are good indicators of the \"response\" variable?\n\nThanks", "link": "https://www.reddit.com/r/datascience/comments/or32v5/is_this_a_good_strategy_for_exploratory_data/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "is this a good strategy for \"exploratory data analytics\"? /!/ suppose i have the following data - i created this example using the r programming language. this a standard \"supervised binary classification task\". there are 4 observed variables (var\\_1, var\\_2, var\\_3 ,var\\_4), and the goal is to predict the \"response\" variable:\n\n    library(ggplot2)\n    library(gridextra)\n    set.seed(123)\n    #create data\n    var_1 = rnorm(1000,10,10) \n    \n    var_2 = rnorm(1000,10,15) \n    \n    var_3 = rnorm(1000,20,5)\n    \n    var_4 = rnorm(1000,10,1)\n    \n    my_data = data.frame(var_1, var_2, var_3, var_4)\n    \n    \n    my_data$response = ifelse(var_1 &gt;20 + rnorm(1,1,1), \"1\", \"0\")\n    \n    my_data$response = as.factor(my_data$response)\n    \n    \n\nsuppose i am given this data, i decide make some quick graphs to look at the distribution of each of these variables with respect to the response variable:\n\n    #first -----> plot !!! \n    p1 = ggplot(my_data) +\n        geom_histogram(aes(x=var_1, fill=response), \n                       colour=\"grey50\", alpha=0.5, position=\"identity\")+ ggtitle(\"var_1 vs response\")\n    \n    #second -----> plot !!! \n    \n    p2 = ggplot(my_data) +\n        geom_histogram(aes(x=var_2, fill=response), \n                       colour=\"grey50\", alpha=0.5, position=\"identity\")+ ggtitle(\"var_2 vs response\")\n    \n    #third -----> plot !!! \n    p3 = ggplot(my_data) +\n        geom_histogram(aes(x=var_3, fill=response), \n                       colour=\"grey50\", alpha=0.5, position=\"identity\")+ ggtitle(\"var_3 vs response\")\n    \n    #fourth plot\n    p4 = ggplot(my_data) +\n        geom_histogram(aes(x=var_4, fill=response), \n                       colour=\"grey50\", alpha=0.5, position=\"identity\")+ ggtitle(\"var_4 vs response\")\n    \n    #display all together\n    grid.arrange(p1, p2, p3, p4, ncol=2) \n\n&amp;#x200b;\n\n[all 4 graphs](https://preview.redd.it/hp6ppge6p9d71.png?width=1091&amp;format=png&amp;auto=webp&amp;s=c081717b872aba58d25995ebc264715578cd8147)\n\nbased on these graphs: it would appear that \"higher values of var\\_1\" are more associated with a \"response = 1\" , but all other variables do not appear to have this pattern. one could make the premature argument that \"var\\_1\" might be an important variable for discriminating between the response variable.\n\nhowever, the thing which concerns me the most is that \"var\\_1\" **on its own** might be a good indicator for discriminating between the response variable, but \"var\\_1\" might not be a good indicator for discriminating between the response variable **along with the other variables.**  in statistics, i think this is called \"simpson's paradox\": [https://en.wikipedia.org/wiki/simpson%27s\\_paradox#/media/file:simpsons\\_paradox\\_-\\_animation.gif](https://en.wikipedia.org/wiki/simpson%27s_paradox#/media/file:simpsons_paradox_-_animation.gif)\n\n**my question:** when faced with these graphs, can one make a harmful conclusion by believing that \"higher values of \"var\\_1\" are good indicators of the \"response\" variable?\n\nthanks", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/or32v5/is_this_a_good_strategy_for_exploratory_data/',)", "identifyer": 5593580, "year": "2021"}, {"autor": "sikdertahsin", "date": 1627056048000, "content": "Help post: Model for Time Series data prediction /!/ I have a dataset whose output (y) looks like this plot. X-axis is the number of days and Y-axis is the output column. I have only 400 days of data. Any idea which model will be a good prediction model? Currently, I am trying with an LSTM network with 16 layers, but the loss is very high (\\~1600). Any tips/suggestions will be very helpful.", "link": "https://www.reddit.com/r/datascience/comments/oq5f01/help_post_model_for_time_series_data_prediction/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "help post: model for time series data prediction /!/ i have a dataset whose output (y) looks like this -----> plot !!! . x-axis is the number of days and y-axis is the output column. i have only 400 days of data. any idea which model will be a good prediction model? currently, i am trying with an lstm network with 16 layers, but the loss is very high (\\~1600). any tips/suggestions will be very helpful.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 8, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/oq5f01/help_post_model_for_time_series_data_prediction/',)", "identifyer": 5593640, "year": "2021"}, {"autor": "earthinversion01", "date": 1622767018000, "content": "How to plot the earthquake data on a topographic map", "link": "https://www.reddit.com/r/datascience/comments/nrseph/how_to_plot_the_earthquake_data_on_a_topographic/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "how to -----> plot !!!  the earthquake data on a topographic map", "sortedWord": "None", "removed": "('reddit',)", "score": 1, "comments": 0, "media": "('link',)", "medialink": "('https://earthinversion.com/utilities/how-to-plot-the-earthquake-data-on-a-topographic-map/',)", "identifyer": 5593830, "year": "2021"}, {"autor": "3l3ktr4", "date": 1612355926000, "content": "Run your MySQL, PostgreSQL, SQLite3 (and other!) queries and plot their graphs in the Jupyter notebook", "link": "https://www.reddit.com/r/datascience/comments/lbmnef/run_your_mysql_postgresql_sqlite3_and_other/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "run your mysql, postgresql, sqlite3 (and other!) queries and -----> plot !!!  their graphs in the jupyter notebook", "sortedWord": "None", "removed": "('moderator',)", "score": 0, "comments": 0, "media": "('nan',)", "medialink": "('https://blog.jupyter.org/an-sql-solution-for-jupyter-ef4a00a0d925',)", "identifyer": 5594239, "year": "2021"}, {"autor": "blueest", "date": 1610084847000, "content": "in this picture, what does the color \"grey\" represent? /!/ https://www.rpubs.com/loveb/som\n\nTheres a picture on this page called \"count plot\". There is a color gradient scale that shows the size of each circle. Grey is not on the gradient scale. Does that mean grey is empty?\n\nThanks", "link": "https://www.reddit.com/r/datascience/comments/kswb47/in_this_picture_what_does_the_color_grey_represent/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "in this picture, what does the color \"grey\" represent? /!/ https://www.rpubs.com/loveb/som\n\ntheres a picture on this page called \"count -----> plot !!! \". there is a color gradient scale that shows the size of each circle. grey is not on the gradient scale. does that mean grey is empty?\n\nthanks", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/kswb47/in_this_picture_what_does_the_color_grey_represent/',)", "identifyer": 5594416, "year": "2021"}, {"autor": "RealisticMost", "date": 1625853580000, "content": "(Pandas) How to import mutliple csv with different time frequencies? /!/ Hello, \n\nI am currently learning Pyrhon and Pandas. My goal is to plot data from a huge database. There are multiple csv files with different time frequencies. For example One file has a value which is recorded in a 5 min frequency and one is in 30 min another one maybe 10 min. In Excel I import them with the Power Query editor and plot it with the help of Power Pivot and it shows me the day average or one hour average. \n\nNow my question is. Is this also possible with pandas? Can I import the different csv files into different dataframes for example df1 and df2 with different frequency and then compare both with the one hour average? Thanks in advance.:)", "link": "https://www.reddit.com/r/datascience/comments/oh10rx/pandas_how_to_import_mutliple_csv_with_different/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "(pandas) how to import mutliple csv with different time frequencies? /!/ hello, \n\ni am currently learning pyrhon and pandas. my goal is to -----> plot !!!  data from a huge database. there are multiple csv files with different time frequencies. for example one file has a value which is recorded in a 5 min frequency and one is in 30 min another one maybe 10 min. in excel i import them with the power query editor and plot it with the help of power pivot and it shows me the day average or one hour average. \n\nnow my question is. is this also possible with pandas? can i import the different csv files into different dataframes for example df1 and df2 with different frequency and then compare both with the one hour average? thanks in advance.:)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/oh10rx/pandas_how_to_import_mutliple_csv_with_different/',)", "identifyer": 5594525, "year": "2021"}, {"autor": "jdk4876", "date": 1612892098000, "content": "Should I learn power bi? /!/ My boss yesterday came to me all excited about Power BI and wanted me to look into it. I generally spend my days in RStudio and build reports using rMarkdown.  I am pretty comfortable in R (mainly tidyverse tools), know enough Python to be dangerous, and routinely query against the company's sql server database.\n\nPower bi looks like a drag and drop plot and dashboard builder, but I'm not sure if there are other features that I should think about in it.", "link": "https://www.reddit.com/r/datascience/comments/lg8koq/should_i_learn_power_bi/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "should i learn power bi? /!/ my boss yesterday came to me all excited about power bi and wanted me to look into it. i generally spend my days in rstudio and build reports using rmarkdown.  i am pretty comfortable in r (mainly tidyverse tools), know enough python to be dangerous, and routinely query against the company's sql server database.\n\npower bi looks like a drag and drop -----> plot !!!  and dashboard builder, but i'm not sure if there are other features that i should think about in it.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 13, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/lg8koq/should_i_learn_power_bi/',)", "identifyer": 5594691, "year": "2021"}, {"autor": "blueest", "date": 1612820127000, "content": "Has anyone ever worked with \"siding\" graphs before? (R) /!/ Recently, I have started to learn how to make interactive time series plots in R - there are additional options which allow you to \"drag and slide\" the plot. \n\nI posted a question here on stackoverflow: https://stackoverflow.com/questions/66109378/r-adding-multiple-sliders-to-the-same-graph-error-aesthetics-must-be-eithe\n\nCould someone please take a look at it?\nThanks!", "link": "https://www.reddit.com/r/datascience/comments/lfmisc/has_anyone_ever_worked_with_siding_graphs_before_r/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "has anyone ever worked with \"siding\" graphs before? (r) /!/ recently, i have started to learn how to make interactive time series plots in r - there are additional options which allow you to \"drag and slide\" the -----> plot !!! . \n\ni posted a question here on stackoverflow: https://stackoverflow.com/questions/66109378/r-adding-multiple-sliders-to-the-same-graph-error-aesthetics-must-be-eithe\n\ncould someone please take a look at it?\nthanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/lfmisc/has_anyone_ever_worked_with_siding_graphs_before_r/',)", "identifyer": 5594739, "year": "2021"}, {"autor": "SQL_beginner", "date": 1625519376000, "content": "Customizing Interactive Graphics /!/   I am using the R programming language. In particular, I am learning how to customize and how to make interactive plots in R using plotly : \n\n    library(plotly)\n    \n    iris %&gt;% plot_ly(type = 'parcoords', line = list(color = ~as.integer(Species), \n             colorscale = list(c(0,'red'),c(0.5,'green'),c(1,'blue'))), \n             dimensions = list( list(range = c(2,4.5), label = 'Sepal Width', values = ~Sepal.Width), \n                          list(range = c(4,8), constraintrange = c(5,6), label = 'Sepal Length', values = ~Sepal.Length), \n                          list(range = c(0,2.5), label = 'Petal Width', values = ~Petal.Width), \n                          list(range = c(1,7), label = 'Petal Length', values = ~Petal.Length) ) )  \n\n \n\nSuppose if I was to add an \"id\" column to the data set, e.g.\n\n    library(dplyr)\n     df &lt;- iris %&gt;% mutate(id = row_number()) \n\nIs it possible so that when you \"click\" on any of the \"lines\" on this plot, information from the dataset (i.e. \"df\") corresponding to row of that line appears?\n\n[https://i.stack.imgur.com/sGxLg.jpg](https://i.stack.imgur.com/sGxLg.jpg)\n\nDoes anyone know how to do this?\n\nThanks!\n\n&amp;#x200B;\n\n[Desired Result](https://preview.redd.it/d6hc5vm2mg971.jpg?width=2184&amp;format=pjpg&amp;auto=webp&amp;s=f4a08c1cfaf447ef3d6d61b45c7b5bafaa639e01)", "link": "https://www.reddit.com/r/datascience/comments/oegfd6/customizing_interactive_graphics/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "customizing interactive graphics /!/   i am using the r programming language. in particular, i am learning how to customize and how to make interactive plots in r using plotly : \n\n    library(plotly)\n    \n    iris %&gt;% plot_ly(type = 'parcoords', line = list(color = ~as.integer(species), \n             colorscale = list(c(0,'red'),c(0.5,'green'),c(1,'blue'))), \n             dimensions = list( list(range = c(2,4.5), label = 'sepal width', values = ~sepal.width), \n                          list(range = c(4,8), constraintrange = c(5,6), label = 'sepal length', values = ~sepal.length), \n                          list(range = c(0,2.5), label = 'petal width', values = ~petal.width), \n                          list(range = c(1,7), label = 'petal length', values = ~petal.length) ) )  \n\n \n\nsuppose if i was to add an \"id\" column to the data set, e.g.\n\n    library(dplyr)\n     df &lt;- iris %&gt;% mutate(id = row_number()) \n\nis it possible so that when you \"click\" on any of the \"lines\" on this -----> plot !!! , information from the dataset (i.e. \"df\") corresponding to row of that line appears?\n\n[https://i.stack.imgur.com/sgxlg.jpg](https://i.stack.imgur.com/sgxlg.jpg)\n\ndoes anyone know how to do this?\n\nthanks!\n\n&amp;#x200b;\n\n[desired result](https://preview.redd.it/d6hc5vm2mg971.jpg?width=2184&amp;format=pjpg&amp;auto=webp&amp;s=f4a08c1cfaf447ef3d6d61b45c7b5bafaa639e01)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/oegfd6/customizing_interactive_graphics/',)", "identifyer": 5594769, "year": "2021"}, {"autor": "coolSnipesMore", "date": 1628770235000, "content": "Super beginner problem that's so basic I don't know how to explain it /!/ Hi everyone, \n\nThis is probably something that's more suited to a beginner excel sub, but I don't know any or what this problem really is, so I thought I'd post here. Apologies if it's in the wrong place.\n\nI essentially have an excel worksheet with a dozen columns. I only want the data from one column, and I want that data to be pushed to another stock sheet that I have. \n\nI do not know how to do this properly, I assume Python + Panda (?)\n\nAs an example, to help my awful explanation of my problem:\n\n&amp;#x200B;\n\n|*Date*|*Customer*|*Product*|\n|:-|:-|:-|\n|1/1/20|Test Ltd|Item1|\n\n&amp;#x200B;\n\nI have a table with this data, I only need to extract the Product column, and need to push it to another worksheet. However, in the other work sheet, I have a row for each *Product,* and the Columns are *Dispatch centres.*\n\n&amp;#x200B;\n\n||Dispatch 1|*Dispatch 2*|*Dispatch 3*|\n|:-|:-|:-|:-|\n|*Item1*|4|1|12|\n|*Item2*|*7*|3|1|\n|*Item3*|3|6|3|\n\nSo, for every customer I have that orders an Item, I want that to be added to the second sheet as a +1.\n\nEssentially, I think, I need to extract all information from the Product column in workbook 1, and then add a +1 to workbook 2, depending on what Item is in the Product column.\n\nI do not know how to do this.. Help?\n\n(Additionally, I will have to grab the Dispatch column too, and plot each one more accurately, but I think once I've found out how to actually grab one column, convert it to a number depending on what is in it, and then push it ot the second, I will be able to work out the Dispatch problem too)/\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nOR, if it is more convenient, just push me in the right direction to the official docs or a Youtube video.", "link": "https://www.reddit.com/r/datascience/comments/p2yjax/super_beginner_problem_thats_so_basic_i_dont_know/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "super beginner problem that's so basic i don't know how to explain it /!/ hi everyone, \n\nthis is probably something that's more suited to a beginner excel sub, but i don't know any or what this problem really is, so i thought i'd post here. apologies if it's in the wrong place.\n\ni essentially have an excel worksheet with a dozen columns. i only want the data from one column, and i want that data to be pushed to another stock sheet that i have. \n\ni do not know how to do this properly, i assume python + panda (?)\n\nas an example, to help my awful explanation of my problem:\n\n&amp;#x200b;\n\n|*date*|*customer*|*product*|\n|:-|:-|:-|\n|1/1/20|test ltd|item1|\n\n&amp;#x200b;\n\ni have a table with this data, i only need to extract the product column, and need to push it to another worksheet. however, in the other work sheet, i have a row for each *product,* and the columns are *dispatch centres.*\n\n&amp;#x200b;\n\n||dispatch 1|*dispatch 2*|*dispatch 3*|\n|:-|:-|:-|:-|\n|*item1*|4|1|12|\n|*item2*|*7*|3|1|\n|*item3*|3|6|3|\n\nso, for every customer i have that orders an item, i want that to be added to the second sheet as a +1.\n\nessentially, i think, i need to extract all information from the product column in workbook 1, and then add a +1 to workbook 2, depending on what item is in the product column.\n\ni do not know how to do this.. help?\n\n(additionally, i will have to grab the dispatch column too, and -----> plot !!!  each one more accurately, but i think once i've found out how to actually grab one column, convert it to a number depending on what is in it, and then push it ot the second, i will be able to work out the dispatch problem too)/\n\n&amp;#x200b;\n\n&amp;#x200b;\n\nor, if it is more convenient, just push me in the right direction to the official docs or a youtube video.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 8, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/p2yjax/super_beginner_problem_thats_so_basic_i_dont_know/',)", "identifyer": 5594967, "year": "2021"}, {"autor": "mr_chanandler_bong_1", "date": 1610387533000, "content": "Help with multiple time series forecasting /!/ So I was working on a project which consists of around 400+ products, each having their own time series.\n\nI also have an additional feature revenue, but since we're forecasting in the future, I don't know if it makes sense to use it for training, because let's say we're trying to forecast for the next month, we cannot know the revenue. \n\nNow I wanted to know what would be my best approach for this problem.\n\nSo far I've applied regression techniques, by splitting the date feature into day, month, year, day of the week. But this makes training/forecasts for one product at a time, I wanted to know if I can do it for all the products together. Like make a model that can learn from individual time series collectively.\n\nI made a function that takes in the data, product name and returns predictions and the plot for actual vs predictions plot.\n\nData is stationary.\n\nI'm using lgbm, xgboost, fbprophet. But fbprophet doesn't seem to perform that well.\n\nNote - I used vanilla fbprophet model, without specifying any parameters.\n\nWhile I did hyperparameter tuning on lgbm, xgboost.\n\nAny suggestions that could help me, are really appreciated.\n\nThank you!\n\nEdit - Both lgb and xgb work really well on some data, and not well on other.", "link": "https://www.reddit.com/r/datascience/comments/kv7v8g/help_with_multiple_time_series_forecasting/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "help with multiple time series forecasting /!/ so i was working on a project which consists of around 400+ products, each having their own time series.\n\ni also have an additional feature revenue, but since we're forecasting in the future, i don't know if it makes sense to use it for training, because let's say we're trying to forecast for the next month, we cannot know the revenue. \n\nnow i wanted to know what would be my best approach for this problem.\n\nso far i've applied regression techniques, by splitting the date feature into day, month, year, day of the week. but this makes training/forecasts for one product at a time, i wanted to know if i can do it for all the products together. like make a model that can learn from individual time series collectively.\n\ni made a function that takes in the data, product name and returns predictions and the -----> plot !!!  for actual vs predictions -----> plot !!! .\n\ndata is stationary.\n\ni'm using lgbm, xgboost, fbprophet. but fbprophet doesn't seem to perform that well.\n\nnote - i used vanilla fbprophet model, without specifying any parameters.\n\nwhile i did hyperparameter tuning on lgbm, xgboost.\n\nany suggestions that could help me, are really appreciated.\n\nthank you!\n\nedit - both lgb and xgb work really well on some data, and not well on other.", "sortedWord": "None", "removed": "('nan',)", "score": 0, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/kv7v8g/help_with_multiple_time_series_forecasting/',)", "identifyer": 5595071, "year": "2021"}, {"autor": "jnwang", "date": 1621883425000, "content": "DataPrep V0.3 has been released! /!/ [DataPrep](https://dataprep.ai/) is the easiest way to prepare data in Python. We\u2019re an open-source data preparation library, and we just dropped a very big, very exciting release.\n\nAll 3 of DataPrep\u2019s current components have received some notable updates to make DataPrep closer than ever to a full-fledged, off-the-shelf data preparation solution. Here\u2019s a rundown of some changes:\n\n[EDA](https://www.youtube.com/watch?v=RCZ8c8o6HH0&amp;t=8s)**:**\n\n* Added plot\\_diff(): Compare dataframes\n* plot() now sports geographical heatmaps\n* Customize plots by changing display parameters\n\n[Clean](https://www.youtube.com/watch?v=EHArIpp-DDw&amp;t=7s)**:**\n\n* Added clean\\_duplication: Creates an interface for users to select how to cluster detected duplicate values\n* Added clean\\_address(): Clean US addresses\n* Added clean\\_headers(): Clean column headers\n* Added clean\\_date(): Clean dates\n* Added clean\\_df(): Clean a dataframe\n\n[Connector](https://www.youtube.com/watch?v=kZCi0b4cu1c)**:**\n\n* Added support for more APIs - we now support over 30 APIs on Connector\n* The Connector team is currently working on expanding Connector with the addition of [ConnectorX](https://github.com/sfu-db/connector-x)\n\nThese updates allow users to use DataPrep for all their data preparation needs. We are very excited to be taking this step forward and becoming closer to an end-to-end data preparation tool.\n\nMore information\n\n* DataPrep v0.3 Release Blog Post: [https://towardsdatascience.com/dataprep-v0-3-0-has-been-released-be49b1be0e72](https://towardsdatascience.com/dataprep-v0-3-0-has-been-released-be49b1be0e72)\n* Youtube Channel: [https://www.youtube.com/channel/UC7OpZsQwWcmuD0SUaOjGBMA](https://www.youtube.com/channel/UC7OpZsQwWcmuD0SUaOjGBMA)\n* Codebase: [https://github.com/sfu-db/dataprep](https://github.com/sfu-db/dataprep)", "link": "https://www.reddit.com/r/datascience/comments/nk5oj2/dataprep_v03_has_been_released/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "dataprep v0.3 has been released! /!/ [dataprep](https://dataprep.ai/) is the easiest way to prepare data in python. we\u2019re an open-source data preparation library, and we just dropped a very big, very exciting release.\n\nall 3 of dataprep\u2019s current components have received some notable updates to make dataprep closer than ever to a full-fledged, off-the-shelf data preparation solution. here\u2019s a rundown of some changes:\n\n[eda](https://www.youtube.com/watch?v=rcz8c8o6hh0&amp;t=8s)**:**\n\n* added plot\\_diff(): compare dataframes\n* -----> plot !!! () now sports geographical heatmaps\n* customize plots by changing display parameters\n\n[clean](https://www.youtube.com/watch?v=eharipp-ddw&amp;t=7s)**:**\n\n* added clean\\_duplication: creates an interface for users to select how to cluster detected duplicate values\n* added clean\\_address(): clean us addresses\n* added clean\\_headers(): clean column headers\n* added clean\\_date(): clean dates\n* added clean\\_df(): clean a dataframe\n\n[connector](https://www.youtube.com/watch?v=kzci0b4cu1c)**:**\n\n* added support for more apis - we now support over 30 apis on connector\n* the connector team is currently working on expanding connector with the addition of [connectorx](https://github.com/sfu-db/connector-x)\n\nthese updates allow users to use dataprep for all their data preparation needs. we are very excited to be taking this step forward and becoming closer to an end-to-end data preparation tool.\n\nmore information\n\n* dataprep v0.3 release blog post: [https://towardsdatascience.com/dataprep-v0-3-0-has-been-released-be49b1be0e72](https://towardsdatascience.com/dataprep-v0-3-0-has-been-released-be49b1be0e72)\n* youtube channel: [https://www.youtube.com/channel/uc7opzsqwwcmud0suaojgbma](https://www.youtube.com/channel/uc7opzsqwwcmud0suaojgbma)\n* codebase: [https://github.com/sfu-db/dataprep](https://github.com/sfu-db/dataprep)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nk5oj2/dataprep_v03_has_been_released/',)", "identifyer": 5595210, "year": "2021"}, {"autor": "dothingsright_", "date": 1621882657000, "content": "UPDATE: Just had my interview, was it an \"air raid\" style of interview that I was afraid it was going to be? /!/ Prior Post for context: https://www.reddit.com/r/datascience/comments/nhi11p/has_anyone_here_had_the_experience_of_what_i_call/\n\nSadly I think it was an air raid for the most part... and I don't think I did very well.\n\nI didn't really get many rapid fire trivia questions, although I got some, and I had to answer a couple with \"I don't know.\" For example I was asked what is the mathematical equation to calculate p-values... I answered with \"I don't remember? I would have to consult my college notes from 11 years ago\". In my head I was thinking, why does it matter when scikit learn or R does that all under the hood? There was another trivia question I got which I answered I don't know but will not say what it was for the sake of not outing myself incase the interviewers are reading this.\n\nI got some data engineering questions(not my strong suit) and I don't believe I gave good answers. Everywhere I have been I have worked along side with data engineering teams but have never actually done the data engineering work. When I got to the point of the interview where I could ask questions I asked if this role was more of a data science role or data engineering role and they said definitely data science, they have separate data engineering teams, so I have no idea why I was even asked these questions.\n\nI think my strong suit was using my past work experiences to outline how I got things done, but even then I am not sure if my responses were good enough because I felt like I was on defense from the get go. I do better in interviews when I am \"on offense\", so to speak. For example I was asked for an example of how I segmented customers in the past and used that to drive revenue growth. I said I would look at various segmentation techniques and choose the best one based on model performance against test data, and how I determined that to be K-Means clustering after using PCA to reduce the number of features and using a scree plot to determine the number of segments. And how this segmentation approach allowed us to grow our revenue from 4th place in the market to 2nd place. I don't think I was precise or deep enough in this answer and was too vague and I didn't project enough confidence. But, in an hour long verbal interview, how deep am I supposed to go? I struggled with finding the right balance of depth and respecting the time constraints.\n\nI'll be surprised if I don't get rejected... but I feel like this format was tough to succeed in. I believe I am a strong data scientist, but I am at my best when I know exactly what scenario I am facing. Questions like \"Here is what we want to accomplish, how would you do it?\" are what I do best, I didn't get any of those questions. It was more open ended broad questions such as \"What is your go to machine learning model\" and I literally could have given anything. It depends on the task at hand. I said Random Forest to go with the question and how I like the built in OOB functionality in R. Although if I could answer the question again I'd go with Gradient Boosting. \n\nAnother question I hated was \"How do you stay on top of data science industry trends\". I didn't really know how to answer that one. I said I stay engaged with publications and gave a personal example of what I do in my free time(that I won't say since I don't want to out myself incase those who interviewed lurk here) but I don't like this because I believe you stick with what works and you don't always need to be jumping to the new and shiny toy out there unless there is a good reason, and if there was you'd probably know about it without having to be proactive to learn about it.\n\nAnother question was \"Do you follow up on your own models performance against real world data?\". I didn't have a problem with this question because of course I do. Who doesn't? I said its one thing for a model to have high accuracy on train and test data, its another thing for it to actually perform on real world data that you collect after the training and testing process. Therefore you want to make sure its actually hitting the marks that you calibrated during the model building phase otherwise go back to the drawing board. Then they followed up with \"What if you gained more responsibilities on your plate and could no longer follow up on all of your models?\", which I think was a silly follow up, because I feel that is putting the cart before the horse. I said in a hypothetical scenario where my bandwidth was overextended to where I could not do such a thing, as I hope it would not get to that point, I would have to resort to being a leader and calibrating the bandwidth of my team and allocating resources to ensure it does get done.\n\nI got asked \"How do you perform feature selection\" and I simply explained various ways to do it based on the model at hand(i.e. p-Values for linear regression, feature score for Random Forest, etc.). I didn't like this question because its something you would ask an entry level candidate, not someone interviewing for director. I answered it anyway but with only an hour, I feel like better questions could have been asked.\n\nThe interview was solely to gauge my technical abilities, not my leadership or management abilities, so I understand that the interview was aimed to do that. With that being said I don't think I answered the questions given to me with enough confidence or precision. I am good at gauging body language, facial expressions and tone of voice of those interviewing me and I don't think they were impressed. If its up to these 2 guys I don't have a chance. The only way I have a chance is if all I needed to do was show some basic technical competency and the hiring manager(who was on the call but was just there to listen) values my management/leadership attributes and track record more than my hour long call with these 2 guys. \n\nComparing this interview to the 3 bad ones I had before I think this one was definitely the worst of the bunch in terms of how I gauged my performance. I am starting to lose confidence in my interviewing skills, as I believe I have a hard time translating my actual 10 year track record into verbal competency. I know how to do things, I just have a hard time coherently explaining how those things are done.\n\nFortunately... I have another interview starting in 10 minutes... and 5 others lined up this week. But this was the job I really wanted out of all of them.", "link": "https://www.reddit.com/r/datascience/comments/nk5dny/update_just_had_my_interview_was_it_an_air_raid/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "update: just had my interview, was it an \"air raid\" style of interview that i was afraid it was going to be? /!/ prior post for context: https://www.reddit.com/r/datascience/comments/nhi11p/has_anyone_here_had_the_experience_of_what_i_call/\n\nsadly i think it was an air raid for the most part... and i don't think i did very well.\n\ni didn't really get many rapid fire trivia questions, although i got some, and i had to answer a couple with \"i don't know.\" for example i was asked what is the mathematical equation to calculate p-values... i answered with \"i don't remember? i would have to consult my college notes from 11 years ago\". in my head i was thinking, why does it matter when scikit learn or r does that all under the hood? there was another trivia question i got which i answered i don't know but will not say what it was for the sake of not outing myself incase the interviewers are reading this.\n\ni got some data engineering questions(not my strong suit) and i don't believe i gave good answers. everywhere i have been i have worked along side with data engineering teams but have never actually done the data engineering work. when i got to the point of the interview where i could ask questions i asked if this role was more of a data science role or data engineering role and they said definitely data science, they have separate data engineering teams, so i have no idea why i was even asked these questions.\n\ni think my strong suit was using my past work experiences to outline how i got things done, but even then i am not sure if my responses were good enough because i felt like i was on defense from the get go. i do better in interviews when i am \"on offense\", so to speak. for example i was asked for an example of how i segmented customers in the past and used that to drive revenue growth. i said i would look at various segmentation techniques and choose the best one based on model performance against test data, and how i determined that to be k-means clustering after using pca to reduce the number of features and using a scree -----> plot !!!  to determine the number of segments. and how this segmentation approach allowed us to grow our revenue from 4th place in the market to 2nd place. i don't think i was precise or deep enough in this answer and was too vague and i didn't project enough confidence. but, in an hour long verbal interview, how deep am i supposed to go? i struggled with finding the right balance of depth and respecting the time constraints.\n\ni'll be surprised if i don't get rejected... but i feel like this format was tough to succeed in. i believe i am a strong data scientist, but i am at my best when i know exactly what scenario i am facing. questions like \"here is what we want to accomplish, how would you do it?\" are what i do best, i didn't get any of those questions. it was more open ended broad questions such as \"what is your go to machine learning model\" and i literally could have given anything. it depends on the task at hand. i said random forest to go with the question and how i like the built in oob functionality in r. although if i could answer the question again i'd go with gradient boosting. \n\nanother question i hated was \"how do you stay on top of data science industry trends\". i didn't really know how to answer that one. i said i stay engaged with publications and gave a personal example of what i do in my free time(that i won't say since i don't want to out myself incase those who interviewed lurk here) but i don't like this because i believe you stick with what works and you don't always need to be jumping to the new and shiny toy out there unless there is a good reason, and if there was you'd probably know about it without having to be proactive to learn about it.\n\nanother question was \"do you follow up on your own models performance against real world data?\". i didn't have a problem with this question because of course i do. who doesn't? i said its one thing for a model to have high accuracy on train and test data, its another thing for it to actually perform on real world data that you collect after the training and testing process. therefore you want to make sure its actually hitting the marks that you calibrated during the model building phase otherwise go back to the drawing board. then they followed up with \"what if you gained more responsibilities on your plate and could no longer follow up on all of your models?\", which i think was a silly follow up, because i feel that is putting the cart before the horse. i said in a hypothetical scenario where my bandwidth was overextended to where i could not do such a thing, as i hope it would not get to that point, i would have to resort to being a leader and calibrating the bandwidth of my team and allocating resources to ensure it does get done.\n\ni got asked \"how do you perform feature selection\" and i simply explained various ways to do it based on the model at hand(i.e. p-values for linear regression, feature score for random forest, etc.). i didn't like this question because its something you would ask an entry level candidate, not someone interviewing for director. i answered it anyway but with only an hour, i feel like better questions could have been asked.\n\nthe interview was solely to gauge my technical abilities, not my leadership or management abilities, so i understand that the interview was aimed to do that. with that being said i don't think i answered the questions given to me with enough confidence or precision. i am good at gauging body language, facial expressions and tone of voice of those interviewing me and i don't think they were impressed. if its up to these 2 guys i don't have a chance. the only way i have a chance is if all i needed to do was show some basic technical competency and the hiring manager(who was on the call but was just there to listen) values my management/leadership attributes and track record more than my hour long call with these 2 guys. \n\ncomparing this interview to the 3 bad ones i had before i think this one was definitely the worst of the bunch in terms of how i gauged my performance. i am starting to lose confidence in my interviewing skills, as i believe i have a hard time translating my actual 10 year track record into verbal competency. i know how to do things, i just have a hard time coherently explaining how those things are done.\n\nfortunately... i have another interview starting in 10 minutes... and 5 others lined up this week. but this was the job i really wanted out of all of them.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 36, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nk5dny/update_just_had_my_interview_was_it_an_air_raid/',)", "identifyer": 5595211, "year": "2021"}, {"autor": "SnooStories7725", "date": 1621881259000, "content": "DataPrep V0.3 has been released! /!/ [Here](https://towardsdatascience.com/dataprep-v0-3-0-has-been-released-be49b1be0e72) is a link to the full TowardsDataScience article!\n\n[DataPrep](https://dataprep.ai/) is the easiest way to prepare data in Python. We\u2019re an open-source data preparation library, and we just dropped a very big, very exciting release.\n\nAll 3 of DataPrep\u2019s current components have received some notable updates to make DataPrep closer than ever to a full-fledged, off-the-shelf data preparation solution. Here\u2019s a rundown of some changes:\n\n[EDA](https://www.youtube.com/watch?v=RCZ8c8o6HH0)**:**\n\n* Added plot\\_diff(): Compare dataframes\n* plot() now sports geographical heatmaps\n* Customize plots by changing display parameters\n\n[Clean](https://www.youtube.com/watch?v=EHArIpp-DDw)**:**\n\n* Added clean\\_duplication: Creates an interface for users to select how to cluster detected duplicate values\n* Added clean\\_address(): Clean US addresses\n* Added clean\\_headers(): Clean column headers\n* Added clean\\_date(): Clean dates\n* Added clean\\_df(): Clean a dataframe\n\n[Connector](https://www.youtube.com/watch?v=kZCi0b4cu1c)**:**\n\n* Added support for more APIs - we now support over 30 APIs on Connector\n* The Connector team is currently working on expanding Connector with the addition of [ConnectorX](https://github.com/sfu-db/connector-x)\n\nThese updates allow users to use DataPrep for all their data preparation needs. We are very excited to be taking this step forward and becoming closer to an end-to-end data preparation tool.", "link": "https://www.reddit.com/r/datascience/comments/nk4uj6/dataprep_v03_has_been_released/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "dataprep v0.3 has been released! /!/ [here](https://towardsdatascience.com/dataprep-v0-3-0-has-been-released-be49b1be0e72) is a link to the full towardsdatascience article!\n\n[dataprep](https://dataprep.ai/) is the easiest way to prepare data in python. we\u2019re an open-source data preparation library, and we just dropped a very big, very exciting release.\n\nall 3 of dataprep\u2019s current components have received some notable updates to make dataprep closer than ever to a full-fledged, off-the-shelf data preparation solution. here\u2019s a rundown of some changes:\n\n[eda](https://www.youtube.com/watch?v=rcz8c8o6hh0)**:**\n\n* added plot\\_diff(): compare dataframes\n* -----> plot !!! () now sports geographical heatmaps\n* customize plots by changing display parameters\n\n[clean](https://www.youtube.com/watch?v=eharipp-ddw)**:**\n\n* added clean\\_duplication: creates an interface for users to select how to cluster detected duplicate values\n* added clean\\_address(): clean us addresses\n* added clean\\_headers(): clean column headers\n* added clean\\_date(): clean dates\n* added clean\\_df(): clean a dataframe\n\n[connector](https://www.youtube.com/watch?v=kzci0b4cu1c)**:**\n\n* added support for more apis - we now support over 30 apis on connector\n* the connector team is currently working on expanding connector with the addition of [connectorx](https://github.com/sfu-db/connector-x)\n\nthese updates allow users to use dataprep for all their data preparation needs. we are very excited to be taking this step forward and becoming closer to an end-to-end data preparation tool.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nk4uj6/dataprep_v03_has_been_released/',)", "identifyer": 5595213, "year": "2021"}, {"autor": "leddleschnitzel", "date": 1629509517000, "content": "Input wanted on land buying project for critique and sources. /!/ Hi, I am a chemist switching to data science and want to use this project to help build my portfolio while making a good investment. I am looking for input on my approach and also direction to good sources for the data I am seeking. I am utilizing state government websites mostly.\n\nI am working on a personal data project to better determine land I should pursue buying. I have been wanting to secure a plot of undeveloped land \\~50-100acres large in North Carolina, Tennessee, or Northern Georgia. I am wanting to use historical data on land value in these areas to help determine what part of the area mentioned might appreciate the most over a few decades. The following are things I want to try and consider in my analysis and use to narrow down my search areas.\n\n&amp;#x200B;\n\n1. General land value per acre in different counties of these states.\n2. what the land is zoned for to join to the value per acre.\n3. if the land is in any floodzones, wetlands, or other hazard areas and protected areas to join to 1 and 2.\n4. historical land appreciation of these states to join as well.\n5. I intend to plug this data into BigQuery, tableau, or kaggle to play with.\n\nMy intent with this is to be able to filter out areas I can totally ignore and to see if there are any areas with notable appreciation or stagnant value over time indicating a poor area to invest in. I also want to access the difference in appreciation of lands that are zoned for various purposes in these areas so that I can get an idea of if rezoning or excluding certain types of zoned areas would also be prudent.\n\nAny input or help is appreciated as I am new to data science and will surely waste plenty of time that could be better spent with a nudge in the right direction here and there. Data sources for what I am looking for are super appreciated.", "link": "https://www.reddit.com/r/datascience/comments/p8iee8/input_wanted_on_land_buying_project_for_critique/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "input wanted on land buying project for critique and sources. /!/ hi, i am a chemist switching to data science and want to use this project to help build my portfolio while making a good investment. i am looking for input on my approach and also direction to good sources for the data i am seeking. i am utilizing state government websites mostly.\n\ni am working on a personal data project to better determine land i should pursue buying. i have been wanting to secure a -----> plot !!!  of undeveloped land \\~50-100acres large in north carolina, tennessee, or northern georgia. i am wanting to use historical data on land value in these areas to help determine what part of the area mentioned might appreciate the most over a few decades. the following are things i want to try and consider in my analysis and use to narrow down my search areas.\n\n&amp;#x200b;\n\n1. general land value per acre in different counties of these states.\n2. what the land is zoned for to join to the value per acre.\n3. if the land is in any floodzones, wetlands, or other hazard areas and protected areas to join to 1 and 2.\n4. historical land appreciation of these states to join as well.\n5. i intend to plug this data into bigquery, tableau, or kaggle to play with.\n\nmy intent with this is to be able to filter out areas i can totally ignore and to see if there are any areas with notable appreciation or stagnant value over time indicating a poor area to invest in. i also want to access the difference in appreciation of lands that are zoned for various purposes in these areas so that i can get an idea of if rezoning or excluding certain types of zoned areas would also be prudent.\n\nany input or help is appreciated as i am new to data science and will surely waste plenty of time that could be better spent with a nudge in the right direction here and there. data sources for what i am looking for are super appreciated.", "sortedWord": "None", "removed": "('nan',)", "score": 3, "comments": 13, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/p8iee8/input_wanted_on_land_buying_project_for_critique/',)", "identifyer": 5595260, "year": "2021"}, {"autor": "northwestredditor", "date": 1631225122000, "content": "Hal9: Data Science with JavaScript /!/ Hi there, this last year I've been exploring how far we can take Data Science with JavaScript and as part of this started [hal9.ai](https://hal9.ai), an integrated environment to help us be more productive when analyzing data with JavaScript.\n\nI want to ask for your feedback, but more importantly, I want to use this post to share what we've learned so far:\n\n1. **Visualizations**: JavaScript is great at visualizing interactive data, this is probably obvious but worth mentioning nonetheless. Some of the highlights here, [D3.js](http://d3.org/) is still a great library to perform visualizations; however, D3.js is really low level -- Kinda like TensorFlow, not Keras. We actually went to create our own charting library to combine the flexibility of D3 with the ease-of-use of other libraries like Plotly; just to find out later on that [Plot.js](https://observablehq.com/@observablehq/plot) got launched as an amazing library that builds on top of D3. So we ended up integrating Plot.js.\n2. **Transformations**: We found out that JavaScript in combination with D3.js has a pretty decent set of data transformation functions; however, it comes nowhere near to Pandas or dplyr. We found out about [Tidy.js](https://pbeshai.github.io/tidy/) quite early, loved it, and adopted it. The combination of Tidy.js and D3.js and Plot.js is absolutely amazing for visualizations and data wrangling with small datasets, say 10-100K rows. We were very happy with this for a while; however, once you move away from visualizations into real-world data analysis, we found out 100K rows restrictive, which gets worse when having 100 or 1K columns. So we switched gears and started using [Arquero.js](https://uwdata.github.io/arquero/), which happens to be columnar and enabled us to process +1M rows in the browser, descent size for real-world data analysis.\n3. **Modeling**: We are currently exploring this space so our findings are not final yet but let me share what we've found so far. [TensorFlow.js](https://www.tensorflow.org/js) is absolutely amazing, it provides a native port from TensorFlow to JavaScript with CPU, WebGL, WebAssembly and NodeJS backends. We were able to write an LSTM to do time series prediction, so far so good. However, we started hitting issues when we started to do simpler models, like a linear regression. We tried [Regression.js](https://github.com/Tom-Alexander/regression-js) but we found it falls short, so we wrote our own script to handle single-variable regressions using TF.js and gradient decent. It actually worked quite well but exposed a flaw in this approach; basically, there is a lot of work to be done to bring many models into the web. We also found out Arquero.js does not play nicely with TF.js since well, Arquero.js does not support tensors; so we went on to explore [Danfo.js](https://danfo.jsdata.org/), which integrates great with TF.js but we found out it's hard to scale it's transformations to +1M rows and found other rough edges. Since then, well, we started exploring [Pyodide](https://pyodide.org/en/stable/) and perhaps contributing to Danfo.js, or perhaps involve more server-side compute, but that will be a story for another time.\n\nSo net-net, we are still super excited about exploring Data Science, Data Engineering, Visualization and Artificial Intelligence with JavaScript; but realistically, this is going to take a few years to pick up.\n\nIn the meantime, we think Data Science with JavaScript shines with small datasets and interactive visualizations; which we believe Hal9 can help you be productive at. That said, we do believe there must be motivated JavaScript fans out there to unblock themselves with new functionality and contributing back to our open source project, please do reach out in [Hal9's GitHub](https://github.com/hal9ai/hal9ai) repo if you wanna lend a hand!\n\nAlright, so call to action? Please head to [hal9.ai](https://hal9.ai) and give it a shot! We would love to hear where you think this could be useful, what features we are missing, and in general, any feedback you may have.\n\nTo keep in touch, please subscribe to our weekly email at [news.hal9.ai](https://news.hal9.ai/), or reach out to me at javier at hal9.ai, or follow us on Twitter as [@hal9.ai](https://twitter.com/hal9ai)\n\nIf you want to share this post, we ended up publishing here: [https://news.hal9.ai/posts/data-science-with-javascript](https://news.hal9.ai/posts/data-science-with-javascript)\n\nThanks for reading along!", "link": "https://www.reddit.com/r/datascience/comments/pl7ruf/hal9_data_science_with_javascript/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "hal9: data science with javascript /!/ hi there, this last year i've been exploring how far we can take data science with javascript and as part of this started [hal9.ai](https://hal9.ai), an integrated environment to help us be more productive when analyzing data with javascript.\n\ni want to ask for your feedback, but more importantly, i want to use this post to share what we've learned so far:\n\n1. **visualizations**: javascript is great at visualizing interactive data, this is probably obvious but worth mentioning nonetheless. some of the highlights here, [d3.js](http://d3.org/) is still a great library to perform visualizations; however, d3.js is really low level -- kinda like tensorflow, not keras. we actually went to create our own charting library to combine the flexibility of d3 with the ease-of-use of other libraries like plotly; just to find out later on that [plot.js](https://observablehq.com/@observablehq/-----> plot !!! ) got launched as an amazing library that builds on top of d3. so we ended up integrating plot.js.\n2. **transformations**: we found out that javascript in combination with d3.js has a pretty decent set of data transformation functions; however, it comes nowhere near to pandas or dplyr. we found out about [tidy.js](https://pbeshai.github.io/tidy/) quite early, loved it, and adopted it. the combination of tidy.js and d3.js and plot.js is absolutely amazing for visualizations and data wrangling with small datasets, say 10-100k rows. we were very happy with this for a while; however, once you move away from visualizations into real-world data analysis, we found out 100k rows restrictive, which gets worse when having 100 or 1k columns. so we switched gears and started using [arquero.js](https://uwdata.github.io/arquero/), which happens to be columnar and enabled us to process +1m rows in the browser, descent size for real-world data analysis.\n3. **modeling**: we are currently exploring this space so our findings are not final yet but let me share what we've found so far. [tensorflow.js](https://www.tensorflow.org/js) is absolutely amazing, it provides a native port from tensorflow to javascript with cpu, webgl, webassembly and nodejs backends. we were able to write an lstm to do time series prediction, so far so good. however, we started hitting issues when we started to do simpler models, like a linear regression. we tried [regression.js](https://github.com/tom-alexander/regression-js) but we found it falls short, so we wrote our own script to handle single-variable regressions using tf.js and gradient decent. it actually worked quite well but exposed a flaw in this approach; basically, there is a lot of work to be done to bring many models into the web. we also found out arquero.js does not play nicely with tf.js since well, arquero.js does not support tensors; so we went on to explore [danfo.js](https://danfo.jsdata.org/), which integrates great with tf.js but we found out it's hard to scale it's transformations to +1m rows and found other rough edges. since then, well, we started exploring [pyodide](https://pyodide.org/en/stable/) and perhaps contributing to danfo.js, or perhaps involve more server-side compute, but that will be a story for another time.\n\nso net-net, we are still super excited about exploring data science, data engineering, visualization and artificial intelligence with javascript; but realistically, this is going to take a few years to pick up.\n\nin the meantime, we think data science with javascript shines with small datasets and interactive visualizations; which we believe hal9 can help you be productive at. that said, we do believe there must be motivated javascript fans out there to unblock themselves with new functionality and contributing back to our open source project, please do reach out in [hal9's github](https://github.com/hal9ai/hal9ai) repo if you wanna lend a hand!\n\nalright, so call to action? please head to [hal9.ai](https://hal9.ai) and give it a shot! we would love to hear where you think this could be useful, what features we are missing, and in general, any feedback you may have.\n\nto keep in touch, please subscribe to our weekly email at [news.hal9.ai](https://news.hal9.ai/), or reach out to me at javier at hal9.ai, or follow us on twitter as [@hal9.ai](https://twitter.com/hal9ai)\n\nif you want to share this post, we ended up publishing here: [https://news.hal9.ai/posts/data-science-with-javascript](https://news.hal9.ai/posts/data-science-with-javascript)\n\nthanks for reading along!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pl7ruf/hal9_data_science_with_javascript/',)", "identifyer": 5595477, "year": "2021"}, {"autor": "mrtac96", "date": 1630670998000, "content": "How to represent multiple lines in same graph effectively. /!/ I have 3 DL models and 5 augmentation techniques, and i want to compare them. If i put them all in one, its very hard to see individuals one. If i plot separate models, then there will be three plots and i cant compare the techniques in between.\n\nThis is how it look. if i plot all in same graph. So what is the best way to represent\n\nhttps://preview.redd.it/3j3f1xjf4al71.png?width=1584&amp;format=png&amp;auto=webp&amp;s=e1fea8be34d8b2bbd9fc2acff266e65b8a87b1a4", "link": "https://www.reddit.com/r/datascience/comments/ph3wje/how_to_represent_multiple_lines_in_same_graph/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "how to represent multiple lines in same graph effectively. /!/ i have 3 dl models and 5 augmentation techniques, and i want to compare them. if i put them all in one, its very hard to see individuals one. if i -----> plot !!!  separate models, then there will be three plots and i cant compare the techniques in between.\n\nthis is how it look. if i plot all in same graph. so what is the best way to represent\n\nhttps://preview.redd.it/3j3f1xjf4al71.png?width=1584&amp;format=png&amp;auto=webp&amp;s=e1fea8be34d8b2bbd9fc2acff266e65b8a87b1a4", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 20, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/ph3wje/how_to_represent_multiple_lines_in_same_graph/',)", "identifyer": 5595582, "year": "2021"}, {"autor": "RealisticMost", "date": 1629835198000, "content": "How to use regression results for correction of data? /!/ Hello, \n\n\ncurrently I am working on a timeseries of two sensor readings. One sensor is a reference sensor and the second one is our sensor and both are outside. Our sensor has an offset which varies with the humidity. I did a plot of our sensor against the reference, simple scatter plot where x is the reference sensor and y is our sensor. \n\nI got the regression function y=0.37x+4.673.\n\nMy question is, how can I use this to correct my data? I often read something about correction factors in publications and wonder if someone can show me a direction regarding my question.:) \n\nThanks in advance.", "link": "https://www.reddit.com/r/datascience/comments/pauzx7/how_to_use_regression_results_for_correction_of/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "how to use regression results for correction of data? /!/ hello, \n\n\ncurrently i am working on a timeseries of two sensor readings. one sensor is a reference sensor and the second one is our sensor and both are outside. our sensor has an offset which varies with the humidity. i did a -----> plot !!!  of our sensor against the reference, simple scatter -----> plot !!!  where x is the reference sensor and y is our sensor. \n\ni got the regression function y=0.37x+4.673.\n\nmy question is, how can i use this to correct my data? i often read something about correction factors in publications and wonder if someone can show me a direction regarding my question.:) \n\nthanks in advance.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 14, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pauzx7/how_to_use_regression_results_for_correction_of/',)", "identifyer": 5595849, "year": "2021"}, {"autor": "Miel998", "date": 1613335140000, "content": "Analyzing a categorical feature against a continuous target variable ? /!/ Imagine using a boxplot to plot the distribution of a target variable across all categories of a feature, what are the points that you're looking for? that you are eventually taking action on (dropping or keeping the feature, etc), example, you are checking :\n\n\\- Category imbalance, where most of the instances have one category.\n\n\\- Difference in IQR, etc.\n\nThanks.", "link": "https://www.reddit.com/r/datascience/comments/ljwttw/analyzing_a_categorical_feature_against_a/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "analyzing a categorical feature against a continuous target variable ? /!/ imagine using a boxplot to -----> plot !!!  the distribution of a target variable across all categories of a feature, what are the points that you're looking for? that you are eventually taking action on (dropping or keeping the feature, etc), example, you are checking :\n\n\\- category imbalance, where most of the instances have one category.\n\n\\- difference in iqr, etc.\n\nthanks.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/ljwttw/analyzing_a_categorical_feature_against_a/',)", "identifyer": 5596106, "year": "2021"}, {"autor": "shivang__", "date": 1633687447000, "content": "I am having trouble plotting train, forecast and validation using ggplot in R where everything is in the same dataframe. Can anyone help me? /!/ My data looks something like this :\n\nhttps://preview.redd.it/h8kbbx8q87s71.png?width=201&amp;format=png&amp;auto=webp&amp;s=0667f18f320ecde66b7662c38876eaca4c07e919\n\nThe plot I created looks something like this:\n\nWhen I am using geom\\_line:\n\nhttps://preview.redd.it/qj53x1xt87s71.png?width=664&amp;format=png&amp;auto=webp&amp;s=0bf77c79a7b0b88e071a09f41f4057f9c3b83d5f\n\nwhen I am using geom\\_point:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/1h9x69f697s71.png?width=649&amp;format=png&amp;auto=webp&amp;s=1880f83c3c209be75552814d02046d81aa9efc21\n\nSince the dates of forecast and test are same, I think it is causing the issue in line plot. But how do I resolve this?\n\nThe code I am writing is this:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/71xrf23s97s71.png?width=1203&amp;format=png&amp;auto=webp&amp;s=ac77e00e1f6682eb8f9f997e70f29cb9cd88a712", "link": "https://www.reddit.com/r/datascience/comments/q3uoto/i_am_having_trouble_plotting_train_forecast_and/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "i am having trouble plotting train, forecast and validation using ggplot in r where everything is in the same dataframe. can anyone help me? /!/ my data looks something like this :\n\nhttps://preview.redd.it/h8kbbx8q87s71.png?width=201&amp;format=png&amp;auto=webp&amp;s=0667f18f320ecde66b7662c38876eaca4c07e919\n\nthe -----> plot !!!  i created looks something like this:\n\nwhen i am using geom\\_line:\n\nhttps://preview.redd.it/qj53x1xt87s71.png?width=664&amp;format=png&amp;auto=webp&amp;s=0bf77c79a7b0b88e071a09f41f4057f9c3b83d5f\n\nwhen i am using geom\\_point:\n\n&amp;#x200b;\n\nhttps://preview.redd.it/1h9x69f697s71.png?width=649&amp;format=png&amp;auto=webp&amp;s=1880f83c3c209be75552814d02046d81aa9efc21\n\nsince the dates of forecast and test are same, i think it is causing the issue in line -----> plot !!! . but how do i resolve this?\n\nthe code i am writing is this:\n\n&amp;#x200b;\n\nhttps://preview.redd.it/71xrf23s97s71.png?width=1203&amp;format=png&amp;auto=webp&amp;s=ac77e00e1f6682eb8f9f997e70f29cb9cd88a712", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/q3uoto/i_am_having_trouble_plotting_train_forecast_and/',)", "identifyer": 5596265, "year": "2021"}, {"autor": "drake10k", "date": 1633674067000, "content": "Advice Request: 'impactful' plot to show income and expenses /!/ Hey everyone - need your advice on how to present some data.\n\nOne of my company's clients requested us to do EDA on some data it provided. The data includes a lot of stuff, but part of it refers to income and expenses for multiple departments and categories within does departments. It looks something like this:\n\n&amp;#x200B;\n\n[Real data is in the hundreds of millions for 7 departments with 16 categories each. Categories are the same for each department.](https://preview.redd.it/rc0d41on46s71.png?width=641&amp;format=png&amp;auto=webp&amp;s=034b61622fa4064b590baf3b4c3f5d96eb882a1f)\n\nNormally I would use a categorical vertical bar graph (with positives and negatives) to display this, but  my manager asked for 'something impactful' and specifically asked to not use standard bar charts. The graph needs to be static (it will go in a PowerPoint).\n\nWe're mostly doing visualizations using matplotlib/seaborn and plotly/dash.\n\nI don't want the code or anything, but I would appreciate if you could recommend some graphs that are suited for this data and are 'impactful'.", "link": "https://www.reddit.com/r/datascience/comments/q3ryf2/advice_request_impactful_plot_to_show_income_and/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "advice request: 'impactful' -----> plot !!!  to show income and expenses /!/ hey everyone - need your advice on how to present some data.\n\none of my company's clients requested us to do eda on some data it provided. the data includes a lot of stuff, but part of it refers to income and expenses for multiple departments and categories within does departments. it looks something like this:\n\n&amp;#x200b;\n\n[real data is in the hundreds of millions for 7 departments with 16 categories each. categories are the same for each department.](https://preview.redd.it/rc0d41on46s71.png?width=641&amp;format=png&amp;auto=webp&amp;s=034b61622fa4064b590baf3b4c3f5d96eb882a1f)\n\nnormally i would use a categorical vertical bar graph (with positives and negatives) to display this, but  my manager asked for 'something impactful' and specifically asked to not use standard bar charts. the graph needs to be static (it will go in a powerpoint).\n\nwe're mostly doing visualizations using matplotlib/seaborn and plotly/dash.\n\ni don't want the code or anything, but i would appreciate if you could recommend some graphs that are suited for this data and are 'impactful'.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/q3ryf2/advice_request_impactful_plot_to_show_income_and/',)", "identifyer": 5596273, "year": "2021"}, {"autor": "lukewhite01", "date": 1618664170000, "content": "What kind of statistical analysis is a scatter plot?", "link": "https://www.reddit.com/r/datascience/comments/msq7vu/what_kind_of_statistical_analysis_is_a_scatter/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "what kind of statistical analysis is a scatter -----> plot !!! ?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 10, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/msq7vu/what_kind_of_statistical_analysis_is_a_scatter/',)", "identifyer": 5596469, "year": "2021"}, {"autor": "vietlinh12hoa", "date": 1623861902000, "content": "Make the mistake and how to check if sale data makes sense at step 1 /!/ So, I've switched from energy to sale e-commerce analytics for 4 months. As a consequence of lacking domain knowledge, today I made the mistake and my colleagues are not really pleased. The problem is that I have no idea whether the number is OK from beginning. So far I'm doing some SQL and basic calculation for reporting. Usually I will need the feedback from senior ones to double check, but only after the final output is ready, which is not really efficient.\n\n*Anyone here with sale analytics experience can share the idea how to check if data/table is OK since the first SQL querying?*\n\nFrom my experience with energy, I usually do some simple time series plot kW to make sure the step and data are going well. But for sale, it's a different story. ( I'm the first data scientist in the company, so there is no other technical guy to confirm, but I have to submit the final output to business guys for feedback. Before that, the company hired external consultants for analytics. And, the best I can have from them it's only Excel file, code, without any official technical documentation. So the only way to find out the logic calculation is to look at the code)", "link": "https://www.reddit.com/r/datascience/comments/o19982/make_the_mistake_and_how_to_check_if_sale_data/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "make the mistake and how to check if sale data makes sense at step 1 /!/ so, i've switched from energy to sale e-commerce analytics for 4 months. as a consequence of lacking domain knowledge, today i made the mistake and my colleagues are not really pleased. the problem is that i have no idea whether the number is ok from beginning. so far i'm doing some sql and basic calculation for reporting. usually i will need the feedback from senior ones to double check, but only after the final output is ready, which is not really efficient.\n\n*anyone here with sale analytics experience can share the idea how to check if data/table is ok since the first sql querying?*\n\nfrom my experience with energy, i usually do some simple time series -----> plot !!!  kw to make sure the step and data are going well. but for sale, it's a different story. ( i'm the first data scientist in the company, so there is no other technical guy to confirm, but i have to submit the final output to business guys for feedback. before that, the company hired external consultants for analytics. and, the best i can have from them it's only excel file, code, without any official technical documentation. so the only way to find out the logic calculation is to look at the code)", "sortedWord": "None", "removed": "('nan',)", "score": 0, "comments": 6, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/o19982/make_the_mistake_and_how_to_check_if_sale_data/',)", "identifyer": 5596589, "year": "2021"}, {"autor": "woshichidoufu", "date": 1623838149000, "content": "How to plot seconds in jupyter with timestamp - Pandas or matplotlib /!/ I need to create a plot that will show the movement of a device and to be more specific it is like a log file", "link": "https://www.reddit.com/r/datascience/comments/o11fr4/how_to_plot_seconds_in_jupyter_with_timestamp/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "how to -----> plot !!!  seconds in jupyter with timestamp - pandas or matplotlib /!/ i need to create a plot that will show the movement of a device and to be more specific it is like a log file", "sortedWord": "None", "removed": "('nan',)", "score": 0, "comments": 6, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/o11fr4/how_to_plot_seconds_in_jupyter_with_timestamp/',)", "identifyer": 5596605, "year": "2021"}, {"autor": "Hmmmmmmmmmmmmm11111", "date": 1619410297000, "content": "Need Help with Fantasy Football Dashboard /!/  \n\nHi Everyone,\n\nIm building a fantasy football dashboard and im using dash and plotly. i want my bar plot to show the teams on the x axis and the total number of points from each team on the y axis. Im doing:\n\n`fig =` [`px.bar`](https://px.bar/)`(df, x = \"Teams\", y = \"Total_Points\")`\n\nBut for some reason the plot is broken into segments for each player on that teams total points. How do i work around this? Total\\_Points is a float and Teams is a string value in the df if that matters.\n\nThanks!", "link": "https://www.reddit.com/r/datascience/comments/myp9ic/need_help_with_fantasy_football_dashboard/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "need help with fantasy football dashboard /!/  \n\nhi everyone,\n\nim building a fantasy football dashboard and im using dash and plotly. i want my bar -----> plot !!!  to show the teams on the x axis and the total number of points from each team on the y axis. im doing:\n\n`fig =` [`px.bar`](https://px.bar/)`(df, x = \"teams\", y = \"total_points\")`\n\nbut for some reason the plot is broken into segments for each player on that teams total points. how do i work around this? total\\_points is a float and teams is a string value in the df if that matters.\n\nthanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/myp9ic/need_help_with_fantasy_football_dashboard/',)", "identifyer": 5596636, "year": "2021"}, {"autor": "NormieInTheMaking", "date": 1623241753000, "content": "How do you do statistical modeling when analyzing data? /!/ I'm a Data Analyst/Scientist wannabe and I'm about to start my second project ever to further demonstrate my data analysis skills using Python and SQL. I've already done a project before, which was based on an NBA dataset, where I used SQLite windowing functions in my queries and all but what exactly is \"statistical modeling\"? I don't have a stats background(chem.engineer)so it's completely foreign to me. They talk about calculating coefficients to showcase how much each variable affects the target, is that what it's all about? Currently all I know and do when I do data analysis on a jupyter notebook is ask logical questions and write queries where I group by a column(s) to calculate running totals, averages, rankings and plot my findings. Any help is appreciated guys!!", "link": "https://www.reddit.com/r/datascience/comments/nvuqz9/how_do_you_do_statistical_modeling_when_analyzing/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "how do you do statistical modeling when analyzing data? /!/ i'm a data analyst/scientist wannabe and i'm about to start my second project ever to further demonstrate my data analysis skills using python and sql. i've already done a project before, which was based on an nba dataset, where i used sqlite windowing functions in my queries and all but what exactly is \"statistical modeling\"? i don't have a stats background(chem.engineer)so it's completely foreign to me. they talk about calculating coefficients to showcase how much each variable affects the target, is that what it's all about? currently all i know and do when i do data analysis on a jupyter notebook is ask logical questions and write queries where i group by a column(s) to calculate running totals, averages, rankings and -----> plot !!!  my findings. any help is appreciated guys!!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nvuqz9/how_do_you_do_statistical_modeling_when_analyzing/',)", "identifyer": 5596738, "year": "2021"}, {"autor": "minato5972", "date": 1633353397000, "content": "Plotting seaborn plot /!/ Plotted this bar graph from stackoverflow 2020 datset. for average coding experience per continent.\n\nThere are  so much difference in respondents\n\nMale      45189 \n\nFemale     3697 \n\nOther       677 \n\nbut because of average the graph looks bad, is there any way i can make it better? using some other conditions?", "link": "https://www.reddit.com/r/datascience/comments/q158kg/plotting_seaborn_plot/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "plotting seaborn -----> plot !!!  /!/ plotted this bar graph from stackoverflow 2020 datset. for average coding experience per continent.\n\nthere are  so much difference in respondents\n\nmale      45189 \n\nfemale     3697 \n\nother       677 \n\nbut because of average the graph looks bad, is there any way i can make it better? using some other conditions?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/q158kg/plotting_seaborn_plot/',)", "identifyer": 5597002, "year": "2021"}, {"autor": "n3pst3r_007", "date": 1622730975000, "content": "Is there a function in python that can easily plot such a subplot? /!/ I'm sorry if i flaired this post wrong.\n\nI tried writing a code, but couldn't get it working. What I'm trying to come up with is a subplot that is able to plot 2 graphs for each car (mileage vs age and sell price vs age\n\n`&lt;code&gt;`\n\n`fig, ax = plt.subplots()`  \n    \n`# plot.subplot(nrows, ncols, index of figure, **kwargs)`  \n`plt.subplot(1, 2, 1)`  \n`x = df1.Age(yrs)`  \n`y = df1[['']]`  \n`plt.plot(kind = 'line', x = 'Age(yrs)', y  = 'Sell Price($)', color = \"black\")`  \n`plt.subplot(1, 2, 2)`  \n`plt.plot(kind = 'line', x = 'Age(yrs)', y = 'Mileage', color = \"green\")`  \n`plt.show()`  \n`plt.subplot(1, 3, 1)`  \n`plt.title(\"BMW X5\")`  \n`plt.plot(x,y, color = \"black\")`  \n`# plot 2:`  \n`plt.subplot(1, 3, 2)`  \n`plt.title(\"Audi A5\")`  \n`plt.plot(x, y, color = \"green\")`  \n`# plot 3:`  \n`plt.subplot(1, 3, 3)`  \n`plt.title(\"Mercedez Benz C class\")`  \n`plt.plot(x, y, color = 'red')`  \n`plt.show()`\n\n`&lt;/code&gt;`", "link": "https://www.reddit.com/r/datascience/comments/nrezvo/is_there_a_function_in_python_that_can_easily/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "is there a function in python that can easily -----> plot !!!  such a subplot? /!/ i'm sorry if i flaired this post wrong.\n\ni tried writing a code, but couldn't get it working. what i'm trying to come up with is a subplot that is able to plot 2 graphs for each car (mileage vs age and sell price vs age\n\n`&lt;code&gt;`\n\n`fig, ax = plt.subplots()`  \n    \n`# plot.subplot(nrows, ncols, index of figure, **kwargs)`  \n`plt.subplot(1, 2, 1)`  \n`x = df1.age(yrs)`  \n`y = df1[['']]`  \n`plt.plot(kind = 'line', x = 'age(yrs)', y  = 'sell price($)', color = \"black\")`  \n`plt.subplot(1, 2, 2)`  \n`plt.plot(kind = 'line', x = 'age(yrs)', y = 'mileage', color = \"green\")`  \n`plt.show()`  \n`plt.subplot(1, 3, 1)`  \n`plt.title(\"bmw x5\")`  \n`plt.plot(x,y, color = \"black\")`  \n`# plot 2:`  \n`plt.subplot(1, 3, 2)`  \n`plt.title(\"audi a5\")`  \n`plt.plot(x, y, color = \"green\")`  \n`# plot 3:`  \n`plt.subplot(1, 3, 3)`  \n`plt.title(\"mercedez benz c class\")`  \n`plt.plot(x, y, color = 'red')`  \n`plt.show()`\n\n`&lt;/code&gt;`", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nrezvo/is_there_a_function_in_python_that_can_easily/',)", "identifyer": 5597185, "year": "2021"}, {"autor": "ksdio", "date": 1635066633000, "content": "UK Census data download and analysis in Python /!/ &amp;#x200B;\n\n[https://github.com/kulbinderdio/UKCensusDataAnalysis](https://github.com/kulbinderdio/UKCensusDataAnalysis)\n\nI've just uploaded a new project that uses Jupyter Notebook to download UK 2011 census data and save it to a Postgres database. Additional notebooks then use GeoPandas, Matplotlib and other Python packages to query and plot this data.\n\nThe download off the data takes a while.\n\nExample\n\nhttps://preview.redd.it/yudnp8s07dv71.png?width=1162&amp;format=png&amp;auto=webp&amp;s=e77611184985ebc2424667e68d35c0b81e566cbe", "link": "https://www.reddit.com/r/datascience/comments/qeokru/uk_census_data_download_and_analysis_in_python/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "uk census data download and analysis in python /!/ &amp;#x200b;\n\n[https://github.com/kulbinderdio/ukcensusdataanalysis](https://github.com/kulbinderdio/ukcensusdataanalysis)\n\ni've just uploaded a new project that uses jupyter notebook to download uk 2011 census data and save it to a postgres database. additional notebooks then use geopandas, matplotlib and other python packages to query and -----> plot !!!  this data.\n\nthe download off the data takes a while.\n\nexample\n\nhttps://preview.redd.it/yudnp8s07dv71.png?width=1162&amp;format=png&amp;auto=webp&amp;s=e77611184985ebc2424667e68d35c0b81e566cbe", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/qeokru/uk_census_data_download_and_analysis_in_python/',)", "identifyer": 5597294, "year": "2021"}, {"autor": "Ordinary_Zombie_2345", "date": 1626446612000, "content": "Does this logistic regression \u201cscoring\u201d methodology make sense? /!/ I am working with consultants and they are building a churn model. I am trying to understand if their process makes sense from a statistical perspective. \n\nThey are trying to test if individual covariates are statistically significantly related to churn. They have 3 levels of aggregation that they are looking at. The first being the individual covariates and their relationship with churn. The second level is what they are calling the \u201chypothesis level\u201d, where they aggregate significant results from covariates that are all related to a specific idea or hypothesis about why people are churning. There are 3 hypotheses, each with their own set of covariates that they are looking at. Finally, the last level of aggregation is what they are calling a \u201ctheme level\u201d, where each of the 3 hypothesis level results are again aggregated together. I am not clear on how this aggregation is occurring because I haven\u2019t been able to get a good answer from them about how they are aggregating their results. \n\nTheir methodology is for each of the covariates in a hypothesis level that are significant, they get the predicted probability of churn for each observation by fitting a logit model using only that covariate, decile the predicted probabilities, plot those deciles 1-10 on the x axis, and then plot the churn rate associated with that decile. They have stressed however that the predicted probabilities are not probabilities, but rather that they are scores. \n\nFrom there, they then aggregate these \u201cscores\u201d for each significant covariate together to the hypothesis level, and then they aggregate each of the hypotheses together to the theme level. \n\nI have never seen this kind of analysis done, and I am wondering if this makes sense to any of you and if you think it is a statistically sound way to go about solving the problem. Thanks in advance!", "link": "https://www.reddit.com/r/datascience/comments/oli8bm/does_this_logistic_regression_scoring_methodology/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "does this logistic regression \u201cscoring\u201d methodology make sense? /!/ i am working with consultants and they are building a churn model. i am trying to understand if their process makes sense from a statistical perspective. \n\nthey are trying to test if individual covariates are statistically significantly related to churn. they have 3 levels of aggregation that they are looking at. the first being the individual covariates and their relationship with churn. the second level is what they are calling the \u201chypothesis level\u201d, where they aggregate significant results from covariates that are all related to a specific idea or hypothesis about why people are churning. there are 3 hypotheses, each with their own set of covariates that they are looking at. finally, the last level of aggregation is what they are calling a \u201ctheme level\u201d, where each of the 3 hypothesis level results are again aggregated together. i am not clear on how this aggregation is occurring because i haven\u2019t been able to get a good answer from them about how they are aggregating their results. \n\ntheir methodology is for each of the covariates in a hypothesis level that are significant, they get the predicted probability of churn for each observation by fitting a logit model using only that covariate, decile the predicted probabilities, -----> plot !!!  those deciles 1-10 on the x axis, and then -----> plot !!!  the churn rate associated with that decile. they have stressed however that the predicted probabilities are not probabilities, but rather that they are scores. \n\nfrom there, they then aggregate these \u201cscores\u201d for each significant covariate together to the hypothesis level, and then they aggregate each of the hypotheses together to the theme level. \n\ni have never seen this kind of analysis done, and i am wondering if this makes sense to any of you and if you think it is a statistically sound way to go about solving the problem. thanks in advance!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 8, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/oli8bm/does_this_logistic_regression_scoring_methodology/',)", "identifyer": 5597610, "year": "2021"}, {"autor": "redwat3r", "date": 1627965120000, "content": "Machine learning App: Python frameworks /!/ I am trying to build a GUI housing an ML algorithm. Basically, the user would input some hard parameters and upload a csv, advance to next screen, where i want a drag and drop GUI for general model structure. Advance to next screen, click \"train\", then I want to live plot the fit procedure (like live plot the loss per epoch).\n\nI have looked into a few frameworks, namely: Django, Dash, and Kivy. Having all 3 functionalities described above (uploading a csv file of parameters into DB, hard entering other parameters, live plotting training) seems possible, but I'm not sure which framework will work best. I have worked in all 3, but am by no means an expert. Any recommendations?\n\nI have used Django to do something similar, but it output static plots after training, and didnt have a drag and drop interface.", "link": "https://www.reddit.com/r/datascience/comments/owvxuq/machine_learning_app_python_frameworks/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "machine learning app: python frameworks /!/ i am trying to build a gui housing an ml algorithm. basically, the user would input some hard parameters and upload a csv, advance to next screen, where i want a drag and drop gui for general model structure. advance to next screen, click \"train\", then i want to live -----> plot !!!  the fit procedure (like live -----> plot !!!  the loss per epoch).\n\ni have looked into a few frameworks, namely: django, dash, and kivy. having all 3 functionalities described above (uploading a csv file of parameters into db, hard entering other parameters, live plotting training) seems possible, but i'm not sure which framework will work best. i have worked in all 3, but am by no means an expert. any recommendations?\n\ni have used django to do something similar, but it output static plots after training, and didnt have a drag and drop interface.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 11, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/owvxuq/machine_learning_app_python_frameworks/',)", "identifyer": 5598286, "year": "2021"}], "name": "plotdatascience2021"}
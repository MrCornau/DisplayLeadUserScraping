{"interestingcomments": [{"autor": "blueest", "date": 1612068899000, "content": "feature engineering a response variable /!/ Suppose i have the height, weight and salary (response variable) of a group of people. I am interested in predicting salary based on height and weight. \n\nSuppose i want to transform this problem into a classification: e.g. salary &gt; $1000 or salary &lt; $1000.\n\nDoes this make sense? I dont know if $1000 is the best value to make the split. Therefore, could i try a CART decision tree and transfrom the response variable into different categories(e.g. salary &gt; $1000 or salary &lt; $1000 ....vs salary &gt; $5000 or salary &lt; $5000.... vs salary &gt; $10000 or salary &lt; $10000, etc.)\n\nThen, i see which salary range produces the best decision tree.\n\nDoes this make sense?", "link": "https://www.reddit.com/r/datascience/comments/l94y9k/feature_engineering_a_response_variable/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "feature engineering a response variable /!/ suppose i have the height, weight and salary (response variable) of a group of people. i am interested in predicting salary based on height and weight. \n\nsuppose i want to transform this problem into a classification: e.g. salary &gt; $1000 or salary &lt; $1000.\n\ndoes this make sense? i dont know if $1000 is the best value to make the split. therefore, could i try a cart decision -----> tree !!!  and transfrom the response variable into different categories(e.g. salary &gt; $1000 or salary &lt; $1000 ....vs salary &gt; $5000 or salary &lt; $5000.... vs salary &gt; $10000 or salary &lt; $10000, etc.)\n\nthen, i see which salary range produces the best decision tree.\n\ndoes this make sense?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/l94y9k/feature_engineering_a_response_variable/',)", "identifyer": 5586643, "year": "2021"}, {"autor": "blueest", "date": 1612067923000, "content": "Importance of the gini index in decision trees? /!/ Can anyone please explain why \"minimizing the gini index\" in a decision tree is important? Why does minimizing the gini index result in a \"better decision tree\" (i.e. high accuracy)?\n\nThanks", "link": "https://www.reddit.com/r/datascience/comments/l94o9q/importance_of_the_gini_index_in_decision_trees/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "importance of the gini index in decision trees? /!/ can anyone please explain why \"minimizing the gini index\" in a decision -----> tree !!!  is important? why does minimizing the gini index result in a \"better decision tree\" (i.e. high accuracy)?\n\nthanks", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/l94o9q/importance_of_the_gini_index_in_decision_trees/',)", "identifyer": 5586645, "year": "2021"}, {"autor": "memture", "date": 1617180825000, "content": "Can't make sense of \"A high order of disorder means a low level of impurity\". /!/ Hi, I am learning ML/AI and today I am going though the Decision Tree algorithm.I am having little difficulty understanding the Entropy. I am reading this article, [Understanding the Gini Index and Information Gain in Decision Trees](https://medium.com/analytics-steps/understanding-the-gini-index-and-information-gain-in-decision-trees-ab4720518ba8). So it defines the Entropy like this,\n\n&gt; \u201cWhat is entropy?\u201d In the Lyman words, it is nothing just the measure of disorder, or measure of purity. Basically, ***it is the measurement of the impurity or randomness in the data points.*** \n\nAn on the next line it says this,\n\n&gt; A high order of disorder means a low level of impurity.\n\nI am having difficulty to understand the above line.Does it says that \"more disorder or randomness leads to low impurity\"?Should it not be reverse like more randomness means more impurity? Can't get the intuition behind this.", "link": "https://www.reddit.com/r/datascience/comments/mh1vdc/cant_make_sense_of_a_high_order_of_disorder_means/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "can't make sense of \"a high order of disorder means a low level of impurity\". /!/ hi, i am learning ml/ai and today i am going though the decision -----> tree !!!  algorithm.i am having little difficulty understanding the entropy. i am reading this article, [understanding the gini index and information gain in decision trees](https://medium.com/analytics-steps/understanding-the-gini-index-and-information-gain-in-decision-trees-ab4720518ba8). so it defines the entropy like this,\n\n&gt; \u201cwhat is entropy?\u201d in the lyman words, it is nothing just the measure of disorder, or measure of purity. basically, ***it is the measurement of the impurity or randomness in the data points.*** \n\nan on the next line it says this,\n\n&gt; a high order of disorder means a low level of impurity.\n\ni am having difficulty to understand the above line.does it says that \"more disorder or randomness leads to low impurity\"?should it not be reverse like more randomness means more impurity? can't get the intuition behind this.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/mh1vdc/cant_make_sense_of_a_high_order_of_disorder_means/',)", "identifyer": 5586920, "year": "2021"}, {"autor": "davideganna", "date": 1627838611000, "content": "Developed an open source ML software which predicts results from NBA games. Looking for contributors :) /!/  \n\nHi everyone :)\n\nAs the title says, I've been working on an open source project which aims at predicting the outcome of NBA matches. In its current version, the software (100% written in python):\n\n* Scrapes a website ([https://www.basketball-reference.com/](https://www.basketball-reference.com/)) for new data at fixed times;\n* Features 4 models:  \n\n   * Random Forest\n   * Decision Tree\n   * Ada Boost\n   * Elo\n* Provides a way to backtest each model;\n* Allows Telegram integration (you get a notification on profitable matches directly on Telegram);\n* (*in progress*) Contacts the bookmaker of choice to get the odds for a particular match.\n\n# Does it work?\n\nBy testing the models on the past NBA season, the accuracy of the them are close to 64%.\n\n# What I am looking for\n\nI am looking for passionate people to contribute to the software, most importantly by improving the models. In my deepest dreams, I would like to reach an accuracy of \\~70%.\n\nPlease note that I am not a software engineer, and despite the code works, that might not be the best code you have ever seen. For this reason, any comments/suggestions are very much appreciated.\n\n# I am interested, how can I contribute?\n\nThe software is available at: [https://github.com/davideganna/NBA\\_Bet](https://github.com/davideganna/NBA_Bet)\n\n*Side note:* besides [r/Python](https://www.reddit.com/r/Python/), r/MachineLearning and r/datascience, which other subreddits might be suitable for promoting the software?", "link": "https://www.reddit.com/r/datascience/comments/ovw9zm/developed_an_open_source_ml_software_which/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "developed an open source ml software which predicts results from nba games. looking for contributors :) /!/  \n\nhi everyone :)\n\nas the title says, i've been working on an open source project which aims at predicting the outcome of nba matches. in its current version, the software (100% written in python):\n\n* scrapes a website ([https://www.basketball-reference.com/](https://www.basketball-reference.com/)) for new data at fixed times;\n* features 4 models:  \n\n   * random forest\n   * decision -----> tree !!! \n   * ada boost\n   * elo\n* provides a way to backtest each model;\n* allows telegram integration (you get a notification on profitable matches directly on telegram);\n* (*in progress*) contacts the bookmaker of choice to get the odds for a particular match.\n\n# does it work?\n\nby testing the models on the past nba season, the accuracy of the them are close to 64%.\n\n# what i am looking for\n\ni am looking for passionate people to contribute to the software, most importantly by improving the models. in my deepest dreams, i would like to reach an accuracy of \\~70%.\n\nplease note that i am not a software engineer, and despite the code works, that might not be the best code you have ever seen. for this reason, any comments/suggestions are very much appreciated.\n\n# i am interested, how can i contribute?\n\nthe software is available at: [https://github.com/davideganna/nba\\_bet](https://github.com/davideganna/nba_bet)\n\n*side note:* besides [r/python](https://www.reddit.com/r/python/), r/machinelearning and r/datascience, which other subreddits might be suitable for promoting the software?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 11, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/ovw9zm/developed_an_open_source_ml_software_which/',)", "identifyer": 5587043, "year": "2021"}, {"autor": "HastagAB", "date": 1633004603000, "content": "Is there any open source implementation of \"multiway split decision tree on multiple output\" ? /!/ Is there any tool or package that can build trees on multiple output and can split each node or more than 2 nodes?\nSklearn uses CART, which is limited to binary splits.\nchefboost uses CHAID but are built only on single output.", "link": "https://www.reddit.com/r/datascience/comments/pyinyn/is_there_any_open_source_implementation_of/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "is there any open source implementation of \"multiway split decision -----> tree !!!  on multiple output\" ? /!/ is there any tool or package that can build trees on multiple output and can split each node or more than 2 nodes?\nsklearn uses cart, which is limited to binary splits.\nchefboost uses chaid but are built only on single output.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pyinyn/is_there_any_open_source_implementation_of/',)", "identifyer": 5587190, "year": "2021"}, {"autor": "hall_monitor_666", "date": 1632953479000, "content": "Basic ML Question /!/ Hi,\n\nI am new to data science and machine learning. I am dabbling with fitting some sklearn models to college football data I scraped and preprocessed on my own. I am trying to predict total game points using the offensive and defensive statistics of the two teams in a single game.\n\nLinear models end with a mean squared error of ~300 and an R2 of ~14% on the test data.\n\nA decision tree regression ends with a mean squared error of ~600 but an R2 of ~85%.\n\nHow is this possible? Wouldn't I expect R2 to move inversely to mean squared error? What resources can I check out to improve my model selection?", "link": "https://www.reddit.com/r/datascience/comments/py5zrn/basic_ml_question/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "basic ml question /!/ hi,\n\ni am new to data science and machine learning. i am dabbling with fitting some sklearn models to college football data i scraped and preprocessed on my own. i am trying to predict total game points using the offensive and defensive statistics of the two teams in a single game.\n\nlinear models end with a mean squared error of ~300 and an r2 of ~14% on the test data.\n\na decision -----> tree !!!  regression ends with a mean squared error of ~600 but an r2 of ~85%.\n\nhow is this possible? wouldn't i expect r2 to move inversely to mean squared error? what resources can i check out to improve my model selection?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/py5zrn/basic_ml_question/',)", "identifyer": 5587210, "year": "2021"}, {"autor": "blueest", "date": 1611764094000, "content": "\"Decision tree rules\" vs the \"actual decision tree model\" /!/ Suppose you have a binary classification problem (trying to predict if someone has a certain medical condition based on age, gender, weight, etc.). Now, you make a classic CART decision tree. The CART decision tree outputs a set of rules.\n\nMy question: \n\n\nA) Can you use these \"set of rules\" to directly classify new observations (e.g. I print out the rules on a piece of paper, give them to the boss - and to predict new observations, the boss just reads off the sheet of paper, e.g. for a new observation: if \"gender = male, weight = 81 kg and age &gt;30, then no medical condition ?\") \n\nB) Or do you NEED to feed new data into the \"decision tree model itself\" to make these predictions?\n\nIs there any difference between A) and B)?\n\nThanks", "link": "https://www.reddit.com/r/datascience/comments/l67vjw/decision_tree_rules_vs_the_actual_decision_tree/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "\"decision -----> tree !!!  rules\" vs the \"actual decision -----> tree !!!  model\" /!/ suppose you have a binary classification problem (trying to predict if someone has a certain medical condition based on age, gender, weight, etc.). now, you make a classic cart decision tree. the cart decision tree outputs a set of rules.\n\nmy question: \n\n\na) can you use these \"set of rules\" to directly classify new observations (e.g. i print out the rules on a piece of paper, give them to the boss - and to predict new observations, the boss just reads off the sheet of paper, e.g. for a new observation: if \"gender = male, weight = 81 kg and age &gt;30, then no medical condition ?\") \n\nb) or do you need to feed new data into the \"decision tree model itself\" to make these predictions?\n\nis there any difference between a) and b)?\n\nthanks", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/l67vjw/decision_tree_rules_vs_the_actual_decision_tree/',)", "identifyer": 5587307, "year": "2021"}, {"autor": "redneckhippynerd", "date": 1619801004000, "content": "Am I an idiot, or is there some lingo I wasn't taught in school? /!/ I had an interview for a DS position last week and the interviewer asked me a couple of questions that threw me for a loop. First, he asked me to compare a *rectilinear decision tree* to a random forest, and second, he asked me about *scale-free distributions*. I guess that he wanted me to speak of the advantages of using an ensemble of trees (along with randomly selected features and bagging) over a single tree. As for the distribution question, no clue. I can't find anything on Google. \n\n&amp;#x200B;\n\nAny thoughts?", "link": "https://www.reddit.com/r/datascience/comments/n1xvpz/am_i_an_idiot_or_is_there_some_lingo_i_wasnt/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "am i an idiot, or is there some lingo i wasn't taught in school? /!/ i had an interview for a ds position last week and the interviewer asked me a couple of questions that threw me for a loop. first, he asked me to compare a *rectilinear decision -----> tree !!! * to a random forest, and second, he asked me about *scale-free distributions*. i guess that he wanted me to speak of the advantages of using an ensemble of trees (along with randomly selected features and bagging) over a single tree. as for the distribution question, no clue. i can't find anything on google. \n\n&amp;#x200b;\n\nany thoughts?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 12, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/n1xvpz/am_i_an_idiot_or_is_there_some_lingo_i_wasnt/',)", "identifyer": 5587500, "year": "2021"}, {"autor": "hawktrojan", "date": 1619794476000, "content": "Advice on breaking into tech industry /!/ I've been working as a data scientist in the insurance industry (2 companies) for almost 5 years and I really need some advice on getting in the tech industry. I've submitted over 100 applications across various industries and I have gotten 10 phone screens from the insurance industry and 0 from any other industry, let alone tech. I graduated 5 years ago with a computer engineering degree, no masters, and have a good amount of experience building regression, boosting, and tree models. I am proficient in SQL, Python and R and I feel like I have no problem handling responsibilities stated in tech job postings regarding predictive modeling, data manipulation and machine learning. \n\n&amp;#x200B;\n\nI just don't see myself being a data scientist in an industry that I don't enjoy, but I don't really know how to improve my skillset/resume to get even get a phone interview in a different industry. Has anyone also experienced this kind of barrier to entry and I would love to hear some stories of people who have successfully transitioned to tech as a data scientist.", "link": "https://www.reddit.com/r/datascience/comments/n1vk7v/advice_on_breaking_into_tech_industry/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "advice on breaking into tech industry /!/ i've been working as a data scientist in the insurance industry (2 companies) for almost 5 years and i really need some advice on getting in the tech industry. i've submitted over 100 applications across various industries and i have gotten 10 phone screens from the insurance industry and 0 from any other industry, let alone tech. i graduated 5 years ago with a computer engineering degree, no masters, and have a good amount of experience building regression, boosting, and -----> tree !!!  models. i am proficient in sql, python and r and i feel like i have no problem handling responsibilities stated in tech job postings regarding predictive modeling, data manipulation and machine learning. \n\n&amp;#x200b;\n\ni just don't see myself being a data scientist in an industry that i don't enjoy, but i don't really know how to improve my skillset/resume to get even get a phone interview in a different industry. has anyone also experienced this kind of barrier to entry and i would love to hear some stories of people who have successfully transitioned to tech as a data scientist.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 15, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/n1vk7v/advice_on_breaking_into_tech_industry/',)", "identifyer": 5587514, "year": "2021"}, {"autor": "MrAstroThomas", "date": 1621173539000, "content": "Data + Space Science (Side) Project /!/ Hey everyone,\n\nIn the last couple of weeks I worked on a small side project combining Space Science and Machine Learning.\n\nBackground: As a former Solar System scientist I still like to work on astronomy topics in my free time. My main focus: Asteroids, Near-Earth Objects, Comets, Cosmic Dust and Meteors. So basically everything that is rocky or icy.\n\nAsteroids appear like dots in a telescope. If they approach our planet quite close one can use radars to determine their size and shape quite well. The rest of the time scientists can \"only\" determine their brightness, indirectly their size, and orbit around the Sun (and other intrinsic parameters). That's it!\n\nBut one can also determine the spectrum of an asteroid: E.g., by determining the reflectance of the surface for different wavelengths. These reflectance spectra allow scientists to determine the composition of the object. In total, there are 4 \"main groups\":\n\n* C: Carbonaceous Asteroid, most common in the Solar System with... well you guess it, carbon-like compositions\n* S: ... like \"stony\" asteroids, silicates, minerals, etc. dominate the surface\n* X: -class asteroids are mostly classified as \"iron\" asteroids (no these are not pure \"iron rocks\" but show a high abundance of iron, nickel, etc.\n* Other: Outliers with rare or unique compositions\n\nThese main groups are the most common ones. Other classification systems by Bus et al., Tholen et al. and so on (I spare you with the details) introduce more classes and differentiate them to up to 20 classes and more.\n\nWhy am I telling you this? Well I was thinking: Bus et al. classified over 1200 spectra by hand (20 years ago). Why not testing Machine Learning algorithms on these data for classification purposes? And that's what I did. In the GitHub repo that is shown below, several Jupyter Notebooks are stored starting from fetching and parsing the data, to trainings of multi class Support Vector Machines (SVMs) using scikit-learn.\n\nFurther, I programmed a small Feed-Forward and 1D Convolutional Neural Network using Keras to compress the spectra to a 2-dimensional space, to see whether \"spectra clusters\" appear that can be associated with these arbitrary classification schemas. Small videos of these clusters can be seen on my [Twitter post](https://twitter.com/MrAstroThomas/status/1393191707710271489); however these interactive plotting routines are all available in the corresponding scripts.\n\n[The GitHub Repository](https://github.com/ThomasAlbin/sandbox/tree/main/asteroid_taxonomy)\n\n*What is the long-term goal of this project and the \"sandbox projects\" in general?* In June I would like to show the results on a space science conference. Further, I am developing a Solar System Python library that focuses on Small Bodies (Asteroids, etc.) that could be used by students and also by myself for my Medium tutorials (and also future YT tutorials). I think that enthusiastic Python developers and free-time data scientists and citizen scientists could improve, support and boost the scientific community with great ideas and Open Source solutions.\n\nThomas", "link": "https://www.reddit.com/r/datascience/comments/ndowqg/data_space_science_side_project/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "data + space science (side) project /!/ hey everyone,\n\nin the last couple of weeks i worked on a small side project combining space science and machine learning.\n\nbackground: as a former solar system scientist i still like to work on astronomy topics in my free time. my main focus: asteroids, near-earth objects, comets, cosmic dust and meteors. so basically everything that is rocky or icy.\n\nasteroids appear like dots in a telescope. if they approach our planet quite close one can use radars to determine their size and shape quite well. the rest of the time scientists can \"only\" determine their brightness, indirectly their size, and orbit around the sun (and other intrinsic parameters). that's it!\n\nbut one can also determine the spectrum of an asteroid: e.g., by determining the reflectance of the surface for different wavelengths. these reflectance spectra allow scientists to determine the composition of the object. in total, there are 4 \"main groups\":\n\n* c: carbonaceous asteroid, most common in the solar system with... well you guess it, carbon-like compositions\n* s: ... like \"stony\" asteroids, silicates, minerals, etc. dominate the surface\n* x: -class asteroids are mostly classified as \"iron\" asteroids (no these are not pure \"iron rocks\" but show a high abundance of iron, nickel, etc.\n* other: outliers with rare or unique compositions\n\nthese main groups are the most common ones. other classification systems by bus et al., tholen et al. and so on (i spare you with the details) introduce more classes and differentiate them to up to 20 classes and more.\n\nwhy am i telling you this? well i was thinking: bus et al. classified over 1200 spectra by hand (20 years ago). why not testing machine learning algorithms on these data for classification purposes? and that's what i did. in the github repo that is shown below, several jupyter notebooks are stored starting from fetching and parsing the data, to trainings of multi class support vector machines (svms) using scikit-learn.\n\nfurther, i programmed a small feed-forward and 1d convolutional neural network using keras to compress the spectra to a 2-dimensional space, to see whether \"spectra clusters\" appear that can be associated with these arbitrary classification schemas. small videos of these clusters can be seen on my [twitter post](https://twitter.com/mrastrothomas/status/1393191707710271489); however these interactive plotting routines are all available in the corresponding scripts.\n\n[the github repository](https://github.com/thomasalbin/sandbox/-----> tree !!! /main/asteroid_taxonomy)\n\n*what is the long-term goal of this project and the \"sandbox projects\" in general?* in june i would like to show the results on a space science conference. further, i am developing a solar system python library that focuses on small bodies (asteroids, etc.) that could be used by students and also by myself for my medium tutorials (and also future yt tutorials). i think that enthusiastic python developers and free-time data scientists and citizen scientists could improve, support and boost the scientific community with great ideas and open source solutions.\n\nthomas", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/ndowqg/data_space_science_side_project/',)", "identifyer": 5587544, "year": "2021"}, {"autor": "nangaparbat1", "date": 1620989571000, "content": "DAE find that the 'Imbalanced Class Problem' is rarely an actual problem? /!/ Just curious as to peoples' experience with training classification models on highly imbalanced data. You often hear about the imbalanced class problem and the various approaches to 'tackling' it \u2014 SMOTE, ADASYN etc \u2014 but in my experience working with highly imbalanced datasets, I find that these techniques rarely lead to any significant improvement in model performance, and most of the time an out-of-the-box XGBoost or LGBM model will perform just as well.\n\nPerhaps this is just because I basically always use GLMs and tree-based methods, but in my experience the vast majority of models will do just fine on highly imbalanced data. Does anyone else find that to be the case too?\n\nFor context, I'm at a tech company and most of the problems we try to model have some form of extreme class imbalance (e.g. we might have X million users in our training set, with a \\~thousand users in the positive class).", "link": "https://www.reddit.com/r/datascience/comments/nc5sbv/dae_find_that_the_imbalanced_class_problem_is/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "dae find that the 'imbalanced class problem' is rarely an actual problem? /!/ just curious as to peoples' experience with training classification models on highly imbalanced data. you often hear about the imbalanced class problem and the various approaches to 'tackling' it \u2014 smote, adasyn etc \u2014 but in my experience working with highly imbalanced datasets, i find that these techniques rarely lead to any significant improvement in model performance, and most of the time an out-of-the-box xgboost or lgbm model will perform just as well.\n\nperhaps this is just because i basically always use glms and -----> tree !!! -based methods, but in my experience the vast majority of models will do just fine on highly imbalanced data. does anyone else find that to be the case too?\n\nfor context, i'm at a tech company and most of the problems we try to model have some form of extreme class imbalance (e.g. we might have x million users in our training set, with a \\~thousand users in the positive class).", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nc5sbv/dae_find_that_the_imbalanced_class_problem_is/',)", "identifyer": 5587631, "year": "2021"}, {"autor": "SQL_beginner", "date": 1610633454000, "content": "Is this a legitimate statistical modelling approach? /!/ Suppose i have a binary classification task with 10 predictor variables. Suppose i make a decision tree (cart) and it turns out the decision tree (high precision and accuracy) mainly uses 3 of those variables are used by the model (another example: the variable importance plot for an xgboost model).\n\nShould you refit the model with only those 3 variables? Is it likely you might get better classification results? Or is this a bad idea, as eventually you might lose all the variables?\n\nThanks", "link": "https://www.reddit.com/r/datascience/comments/kx6c13/is_this_a_legitimate_statistical_modelling/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "is this a legitimate statistical modelling approach? /!/ suppose i have a binary classification task with 10 predictor variables. suppose i make a decision -----> tree !!!  (cart) and it turns out the decision -----> tree !!!  (high precision and accuracy) mainly uses 3 of those variables are used by the model (another example: the variable importance plot for an xgboost model).\n\nshould you refit the model with only those 3 variables? is it likely you might get better classification results? or is this a bad idea, as eventually you might lose all the variables?\n\nthanks", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/kx6c13/is_this_a_legitimate_statistical_modelling/',)", "identifyer": 5588172, "year": "2021"}, {"autor": "Sheensta", "date": 1610587483000, "content": "ML Feature selection for statistical inference? /!/ I'm given a fairly high-dimensional dataset (150 columns x 1000 observations) to attempt to model a particular problem. The variables are a combination of continuous and categorical variables. Thus, my task is to select features and create a statistical model (something like general linear model or general linear mixed model).\n\nAs I'm in a fairly regulated industry, interpretability is important. Additionally, most of my colleagues are trained in classical statistics but not ML so they'd be more comfortable with an inferential model rather than a predictive model.\n\nI was wondering how feature selection in ML could apply. A simple example would just be to run a decision tree and select the most important features based on GINI. Another could be something like wrapper or embedded methods.\n\nAre these 'acceptable' methods for feature selection in the statistical space? \nThanks!", "link": "https://www.reddit.com/r/datascience/comments/kwv330/ml_feature_selection_for_statistical_inference/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "ml feature selection for statistical inference? /!/ i'm given a fairly high-dimensional dataset (150 columns x 1000 observations) to attempt to model a particular problem. the variables are a combination of continuous and categorical variables. thus, my task is to select features and create a statistical model (something like general linear model or general linear mixed model).\n\nas i'm in a fairly regulated industry, interpretability is important. additionally, most of my colleagues are trained in classical statistics but not ml so they'd be more comfortable with an inferential model rather than a predictive model.\n\ni was wondering how feature selection in ml could apply. a simple example would just be to run a decision -----> tree !!!  and select the most important features based on gini. another could be something like wrapper or embedded methods.\n\nare these 'acceptable' methods for feature selection in the statistical space? \nthanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/kwv330/ml_feature_selection_for_statistical_inference/',)", "identifyer": 5588201, "year": "2021"}, {"autor": "sarvesh2", "date": 1626362350000, "content": "How to approach this problem ? /!/ I am trying to built a model to help our engineering team to estimate the remaining life of the batteries in our product. We use lithium batteries and Since they batteries have a fairly flat discharge curve it's hard to\nestimate the remaining life. If the battery drains out the product won't work so we have to notify them before that happen so that they can change it.  \nCurrently we have a rule based system based on temperature, voltage drop and some other technical factors, where they have defined the thresholds and check the status of the battery every \ntime someone uses our product based on those threshold. If the battery exceed those thresholds, we send them an email to change the batteries.\nNow this method is not very accurate(the thresholds are just best guesses they have found after testing on the lab and feedbacks from customers).\nAll the data is being streamed into our system every day.\nI want to approach this problem through a data science perspective and also learn some new things along the way. I am not sure if it can beat the exisitng sytem we have in place but will be a fun \nexercise. I have the data of old products and it's events/usages(like how many times it is being used, what are the functions performed thoughout the day, temp at that time etc) which the current system don\u2019t take into account. \nWhat would the best way to approach this model. I want to become pro active and inform them way before the battery drains out completely based on their product usage. So that they can change it before time. \nI was thinking about doing a survival analysis.  Or build a tree based or regression based but the temp is an important factor I need \nto encorporate with every event. So I am not sure if this way it can be done.\nWhat do you guys think about approaching this problem ?", "link": "https://www.reddit.com/r/datascience/comments/okuyxw/how_to_approach_this_problem/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "how to approach this problem ? /!/ i am trying to built a model to help our engineering team to estimate the remaining life of the batteries in our product. we use lithium batteries and since they batteries have a fairly flat discharge curve it's hard to\nestimate the remaining life. if the battery drains out the product won't work so we have to notify them before that happen so that they can change it.  \ncurrently we have a rule based system based on temperature, voltage drop and some other technical factors, where they have defined the thresholds and check the status of the battery every \ntime someone uses our product based on those threshold. if the battery exceed those thresholds, we send them an email to change the batteries.\nnow this method is not very accurate(the thresholds are just best guesses they have found after testing on the lab and feedbacks from customers).\nall the data is being streamed into our system every day.\ni want to approach this problem through a data science perspective and also learn some new things along the way. i am not sure if it can beat the exisitng sytem we have in place but will be a fun \nexercise. i have the data of old products and it's events/usages(like how many times it is being used, what are the functions performed thoughout the day, temp at that time etc) which the current system don\u2019t take into account. \nwhat would the best way to approach this model. i want to become pro active and inform them way before the battery drains out completely based on their product usage. so that they can change it before time. \ni was thinking about doing a survival analysis.  or build a -----> tree !!!  based or regression based but the temp is an important factor i need \nto encorporate with every event. so i am not sure if this way it can be done.\nwhat do you guys think about approaching this problem ?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/okuyxw/how_to_approach_this_problem/',)", "identifyer": 5588468, "year": "2021"}, {"autor": "Love_Tech", "date": 1626361756000, "content": "How to approach this problem? /!/ I am trying to built a model to help our engineering team to estimate the remaining life of the batteries in our product. We use lithium batteries and Since they batteries have a fairly flat discharge curve it's hard to\nestimate the remaining life. If the battery drains out the product won't work so we have to notify them before that happen so that they can change it.  \nCurrently we have a rule based system based on temperature, voltage drop and some other technical factors, where they have defined the thresholds and check the status of the battery every \ntime someone uses our product based on those threshold. If the battery exceed those thresholds, we send them an email to change the batteries.\nNow this method is not very accurate(the thresholds are just best guesses they have found after testing on the lab and feedbacks from customers). \nAll the data is being streamed into our system every day.\nI want to approach this problem through a data science perspective and also learn some new things along the way. I am not sure if it can beat the exisitng sytem we have in place but will be a fun \nexercise. I have the data of old products and it's events/usages(like how many times it is being used, what are the functions performed thoughout the day, temp at that time etc). \nWhat would the best way to approach this model. I was thinking about doing a survival analysis.  Or build a tree based or regression based but the temp is an important factor I need \nto encorporate with every event. So I am not sure if this way it can be done.\nWhat do you guys think about approaching this problem ?", "link": "https://www.reddit.com/r/datascience/comments/okuro4/how_to_approach_this_problem/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "how to approach this problem? /!/ i am trying to built a model to help our engineering team to estimate the remaining life of the batteries in our product. we use lithium batteries and since they batteries have a fairly flat discharge curve it's hard to\nestimate the remaining life. if the battery drains out the product won't work so we have to notify them before that happen so that they can change it.  \ncurrently we have a rule based system based on temperature, voltage drop and some other technical factors, where they have defined the thresholds and check the status of the battery every \ntime someone uses our product based on those threshold. if the battery exceed those thresholds, we send them an email to change the batteries.\nnow this method is not very accurate(the thresholds are just best guesses they have found after testing on the lab and feedbacks from customers). \nall the data is being streamed into our system every day.\ni want to approach this problem through a data science perspective and also learn some new things along the way. i am not sure if it can beat the exisitng sytem we have in place but will be a fun \nexercise. i have the data of old products and it's events/usages(like how many times it is being used, what are the functions performed thoughout the day, temp at that time etc). \nwhat would the best way to approach this model. i was thinking about doing a survival analysis.  or build a -----> tree !!!  based or regression based but the temp is an important factor i need \nto encorporate with every event. so i am not sure if this way it can be done.\nwhat do you guys think about approaching this problem ?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/okuro4/how_to_approach_this_problem/',)", "identifyer": 5588469, "year": "2021"}, {"autor": "bonjarno65", "date": 1631582669000, "content": "Sanity check: Any pitfalls in normalizing the target variable by a feature for any decision tree regression model? /!/ Hello folks - the title says it all. Are there any pitfalls (i.e. overfitting or bias) in training a regression ML model like:\n\nmethod1: feature1, feature2, ... , featureN -&gt; target\\_Y/feature1\n\ninstead of:\n\nmethod2: feature1, feature2, ..., featureN -&gt; target\\_Y?\n\nOriginally for method1 above that in using feature1 on both input and output it would lead to data leakage, but apparently it does not meet the definition of data leakage, since feature1 will be available during ML model deployment as well:  \n[https://en.wikipedia.org/wiki/Leakage\\_(machine\\_learning)](https://en.wikipedia.org/wiki/Leakage_(machine_learning))\n\nThe other piece of relevant info here is that feature1 is correlated with target\\_Y to within 90%+ but not the other features nearly as much.  \n\n\nI am pretty sure it's likely OK - but I just to be 100% sure there are no edge cases", "link": "https://www.reddit.com/r/datascience/comments/pnswqs/sanity_check_any_pitfalls_in_normalizing_the/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "sanity check: any pitfalls in normalizing the target variable by a feature for any decision -----> tree !!!  regression model? /!/ hello folks - the title says it all. are there any pitfalls (i.e. overfitting or bias) in training a regression ml model like:\n\nmethod1: feature1, feature2, ... , featuren -&gt; target\\_y/feature1\n\ninstead of:\n\nmethod2: feature1, feature2, ..., featuren -&gt; target\\_y?\n\noriginally for method1 above that in using feature1 on both input and output it would lead to data leakage, but apparently it does not meet the definition of data leakage, since feature1 will be available during ml model deployment as well:  \n[https://en.wikipedia.org/wiki/leakage\\_(machine\\_learning)](https://en.wikipedia.org/wiki/leakage_(machine_learning))\n\nthe other piece of relevant info here is that feature1 is correlated with target\\_y to within 90%+ but not the other features nearly as much.  \n\n\ni am pretty sure it's likely ok - but i just to be 100% sure there are no edge cases", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 8, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pnswqs/sanity_check_any_pitfalls_in_normalizing_the/',)", "identifyer": 5588689, "year": "2021"}, {"autor": "Nexus2011t", "date": 1632841587000, "content": "How to visualize decision tree", "link": "https://www.reddit.com/r/datascience/comments/px8dlm/how_to_visualize_decision_tree/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "how to visualize decision -----> tree !!! ", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 12, "media": "('link',)", "medialink": "('https://explained.ai/decision-tree-viz/index.html',)", "identifyer": 5588774, "year": "2021"}, {"autor": "7Seas_ofRyhme", "date": 1632811815000, "content": "How do I select the 'best tree' to show from a Random Forest model in R ? ( /!/ I couldn't seem to find any solution to this, I was thinking of just showing a decision tree from the rpart function, however, I'm not sure on this approach", "link": "https://www.reddit.com/r/datascience/comments/px0ly1/how_do_i_select_the_best_tree_to_show_from_a/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "how do i select the 'best -----> tree !!! ' to show from a random forest model in r ? ( /!/ i couldn't seem to find any solution to this, i was thinking of just showing a decision tree from the rpart function, however, i'm not sure on this approach", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/px0ly1/how_do_i_select_the_best_tree_to_show_from_a/',)", "identifyer": 5588798, "year": "2021"}, {"autor": "lalopark", "date": 1618517328000, "content": "Feature Importance varies for each model (churn prediction) /!/ I'm using Logistic Regression and Decision Tree to predict churn and for some reason, the Decision Tree has lower accuracy and the two models' evaluations of feature importance are different. Does anyone know why/how this may be happening?", "link": "https://www.reddit.com/r/datascience/comments/mrmyh3/feature_importance_varies_for_each_model_churn/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "feature importance varies for each model (churn prediction) /!/ i'm using logistic regression and decision -----> tree !!!  to predict churn and for some reason, the decision -----> tree !!!  has lower accuracy and the two models' evaluations of feature importance are different. does anyone know why/how this may be happening?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/mrmyh3/feature_importance_varies_for_each_model_churn/',)", "identifyer": 5589244, "year": "2021"}, {"autor": "Bnf91", "date": 1618418694000, "content": "Visualizing non-numerical data; dictionary to mindmap? /!/  Hello everybody!\n\nI am looking for a way to get a quick overview of the data contained in a dataframe. Plotting the data doesnt really do the trick because the non-numerical data is what interest me in the dataframes. I mostly use the uniqe values and value count functions in pandas to get a high level overview of the contained data but I find it a bit clumsy. I would like to display all the dataframes, their columns and respective uniqe values all at once, and that use this as a reference for further work.\n\nI was thinking of converting unique values data in somekind of a mind map or a branching tree.\n\nHave someone done something similair?\n\n&amp;#x200B;\n\n[Something like this maybe](https://i.stack.imgur.com/58dk2.png)", "link": "https://www.reddit.com/r/datascience/comments/mqukj4/visualizing_nonnumerical_data_dictionary_to/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "visualizing non-numerical data; dictionary to mindmap? /!/  hello everybody!\n\ni am looking for a way to get a quick overview of the data contained in a dataframe. plotting the data doesnt really do the trick because the non-numerical data is what interest me in the dataframes. i mostly use the uniqe values and value count functions in pandas to get a high level overview of the contained data but i find it a bit clumsy. i would like to display all the dataframes, their columns and respective uniqe values all at once, and that use this as a reference for further work.\n\ni was thinking of converting unique values data in somekind of a mind map or a branching -----> tree !!! .\n\nhave someone done something similair?\n\n&amp;#x200b;\n\n[something like this maybe](https://i.stack.imgur.com/58dk2.png)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/mqukj4/visualizing_nonnumerical_data_dictionary_to/',)", "identifyer": 5589296, "year": "2021"}, {"autor": "veeeerain", "date": 1618407381000, "content": "Agent based modeling vs Statistical Learning Approaches /!/ Hello, I\u2019m currently a sophomore at my university whose in a undergraduate data science club. We had a speaker come talk about the use of \u201cagent based\u201d models, network models, feedback models, spatial models etc. which the way he described it as was simulation based approaches. \n\nAs a statistics Major and only being familiar with the statistical learning approach to modeling, this was very different from the usual tree based models/clustering models that I\u2019m used to hearing about.\n\n\nCan anyone go into a bit more depth of what those types of models are? Where are they used? Are agent based models part of reinforcement learning?", "link": "https://www.reddit.com/r/datascience/comments/mqqpw4/agent_based_modeling_vs_statistical_learning/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "agent based modeling vs statistical learning approaches /!/ hello, i\u2019m currently a sophomore at my university whose in a undergraduate data science club. we had a speaker come talk about the use of \u201cagent based\u201d models, network models, feedback models, spatial models etc. which the way he described it as was simulation based approaches. \n\nas a statistics major and only being familiar with the statistical learning approach to modeling, this was very different from the usual -----> tree !!!  based models/clustering models that i\u2019m used to hearing about.\n\n\ncan anyone go into a bit more depth of what those types of models are? where are they used? are agent based models part of reinforcement learning?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 12, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/mqqpw4/agent_based_modeling_vs_statistical_learning/',)", "identifyer": 5589302, "year": "2021"}, {"autor": "pp314159", "date": 1609861044000, "content": "MLJAR Automated Machine Learning for Tabular Data (Stacking, Golden Features, Explanations, and AutoDoc) /!/ I'm working on AutoML since 2016. I think that the latest release (0.7.15) of MLJAR AutoML is amazing. It has a ton of fantastic features that I always want to have in AutoML:\n\n- Operates in three modes: Explain, Perform, Compete.\n\n- `Explain` is for data exploratory and checking the default performance (without HP tuning). It has Automatic Exploratory Data Analysis.\n\n- `Perform` is for building production-ready models (HP tuning + ensembling).\n\n- `Compete` is for solving ML competitions in limited time amount (HP tuning + ensembling + stacking).\n\n- All ML experiments have automatic documentation that creates Markdown reports ready to commit to the repo ([example1](https://github.com/mljar/mljar-examples/blob/master/media/decision_tree_summary.gif), [example2](https://github.com/mljar/mljar-examples/tree/master/Income_classification/AutoML_1#automl-leaderboard)).\n\n- The package produces extensive explanations: decision tree visualization, feature importance, SHAP explanations, advanced metrics values.\n\n- It has advanced feature engineering, like Golden Features, Features Selection, Time and Text Transformations, Categoricals handling with the target, label, or one-hot encodings.\n\nLink to the source code: https://github.com/mljar/mljar-supervised (MIT License)", "link": "https://www.reddit.com/r/datascience/comments/kr0pcy/mljar_automated_machine_learning_for_tabular_data/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "mljar automated machine learning for tabular data (stacking, golden features, explanations, and autodoc) /!/ i'm working on automl since 2016. i think that the latest release (0.7.15) of mljar automl is amazing. it has a ton of fantastic features that i always want to have in automl:\n\n- operates in three modes: explain, perform, compete.\n\n- `explain` is for data exploratory and checking the default performance (without hp tuning). it has automatic exploratory data analysis.\n\n- `perform` is for building production-ready models (hp tuning + ensembling).\n\n- `compete` is for solving ml competitions in limited time amount (hp tuning + ensembling + stacking).\n\n- all ml experiments have automatic documentation that creates markdown reports ready to commit to the repo ([example1](https://github.com/mljar/mljar-examples/blob/master/media/decision_tree_summary.gif), [example2](https://github.com/mljar/mljar-examples/-----> tree !!! /master/income_classification/automl_1#automl-leaderboard)).\n\n- the package produces extensive explanations: decision tree visualization, feature importance, shap explanations, advanced metrics values.\n\n- it has advanced feature engineering, like golden features, features selection, time and text transformations, categoricals handling with the target, label, or one-hot encodings.\n\nlink to the source code: https://github.com/mljar/mljar-supervised (mit license)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/kr0pcy/mljar_automated_machine_learning_for_tabular_data/',)", "identifyer": 5589910, "year": "2021"}, {"autor": "NaN_Loss", "date": 1635351556000, "content": "The MLVU course is an awesome and rigorous introduction to machine learning /!/ For  those who need a beginner friendly but still complete introduction to  machine learning, I found the mlvu course (by VU University Amsterdam )  to be excellent!\n\n[https://tutobase.com/post/436](https://tutobase.com/post/436)\n\nHere's an overview of the content:\n\n* Linear models for regression/classification, gradient descent\n* Model evaluation and metrics\n* Data pre-processing: normalization, missing values, PCA, ...\n* Introduction to probability, Naive Bayes, information theory\n* SVMs, Kernel trick\n* Neural nets, backpropagation,convolutions\n* Density estimation\n* GANs, VAEs, Autoencoders\n* Tree models (decision trees, random forests, gradient boosting)\n* Sequence models (markov chains, RNNs, LSTMs, ...)\n* Embeddings (Recommender systems, graph models)\n* Reinforcement learning\n\nCheckout also DLVU, the deep learning focused course by the same lecturers: [https://tutobase.com/post/137](https://tutobase.com/post/137). Their lectures on transformers are the best I've seen yet.", "link": "https://www.reddit.com/r/datascience/comments/qh0czp/the_mlvu_course_is_an_awesome_and_rigorous/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "the mlvu course is an awesome and rigorous introduction to machine learning /!/ for  those who need a beginner friendly but still complete introduction to  machine learning, i found the mlvu course (by vu university amsterdam )  to be excellent!\n\n[https://tutobase.com/post/436](https://tutobase.com/post/436)\n\nhere's an overview of the content:\n\n* linear models for regression/classification, gradient descent\n* model evaluation and metrics\n* data pre-processing: normalization, missing values, pca, ...\n* introduction to probability, naive bayes, information theory\n* svms, kernel trick\n* neural nets, backpropagation,convolutions\n* density estimation\n* gans, vaes, autoencoders\n* -----> tree !!!  models (decision trees, random forests, gradient boosting)\n* sequence models (markov chains, rnns, lstms, ...)\n* embeddings (recommender systems, graph models)\n* reinforcement learning\n\ncheckout also dlvu, the deep learning focused course by the same lecturers: [https://tutobase.com/post/137](https://tutobase.com/post/137). their lectures on transformers are the best i've seen yet.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/qh0czp/the_mlvu_course_is_an_awesome_and_rigorous/',)", "identifyer": 5589996, "year": "2021"}, {"autor": "suspicious_gardener", "date": 1623070408000, "content": "Best alternatives to 'shap' package? /!/ The shap package has been great when it works, but I would like an alternative package that has similar functionality. I mostly use gradient boosting, so any package that can use the tree-path methods (interventional is nice too, but not as important) would be a life saver.", "link": "https://www.reddit.com/r/datascience/comments/nubt22/best_alternatives_to_shap_package/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "best alternatives to 'shap' package? /!/ the shap package has been great when it works, but i would like an alternative package that has similar functionality. i mostly use gradient boosting, so any package that can use the -----> tree !!! -path methods (interventional is nice too, but not as important) would be a life saver.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 9, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nubt22/best_alternatives_to_shap_package/',)", "identifyer": 5590075, "year": "2021"}, {"autor": "D4ZZL3", "date": 1616888211000, "content": "Does it make sense to perform best subset selection before a gradient boosting tree? /!/ Why/why not?", "link": "https://www.reddit.com/r/datascience/comments/mepl07/does_it_make_sense_to_perform_best_subset/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "does it make sense to perform best subset selection before a gradient boosting -----> tree !!! ? /!/ why/why not?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/mepl07/does_it_make_sense_to_perform_best_subset/',)", "identifyer": 5590161, "year": "2021"}, {"autor": "bingingwithdata", "date": 1627300123000, "content": "Why not AutoML every tabular data? /!/ Hi reddit, \n\nI did some internships, had some interviews with companies and feel that out that the use of AutoML is not that prevalent for tabular\\* data. **Can anybody provide me with some insights on why?**  \n\n\nHere is why I feel that that AutoML should be used more often:\n\n1. **Efficiency**  \nIgnoring the feature engineering aspects aside, a typical data scientist workflow involves trying out the different models. Some of the AutoML modules like [H2O AutoML](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html),  [AutoSklearn](https://automl.github.io/auto-sklearn/) does this for you, and  allow you to interpret your models. All these save so much time experimenting with the standard models.  \n\n2. **Reliability**  \nSince these are packages developed by more experienced people, you do not need to worry much handling splitting data into training, testing and splitting, worry about introducing bugs into your modelling code (For example, if you are a beginner, you do not have to worry about how to structure your data for ensembling)  \n\n3. **Better results**  \nSince you are using  a pipeline created by an experienced person, it is equivalent to having a data science expert create a data science model for you.  \n\n\n\\* I only mentioned tabular data because to my understanding, we still need deep learning for unstructured data, and traditional ML models that are easily automated like tree-based models typically perform well for tabular data\n\n&amp;#x200B;\n\n(This is my first time making any reddit post, so please let me know if I make any mistakes as well!)", "link": "https://www.reddit.com/r/datascience/comments/orx2ih/why_not_automl_every_tabular_data/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "why not automl every tabular data? /!/ hi reddit, \n\ni did some internships, had some interviews with companies and feel that out that the use of automl is not that prevalent for tabular\\* data. **can anybody provide me with some insights on why?**  \n\n\nhere is why i feel that that automl should be used more often:\n\n1. **efficiency**  \nignoring the feature engineering aspects aside, a typical data scientist workflow involves trying out the different models. some of the automl modules like [h2o automl](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html),  [autosklearn](https://automl.github.io/auto-sklearn/) does this for you, and  allow you to interpret your models. all these save so much time experimenting with the standard models.  \n\n2. **reliability**  \nsince these are packages developed by more experienced people, you do not need to worry much handling splitting data into training, testing and splitting, worry about introducing bugs into your modelling code (for example, if you are a beginner, you do not have to worry about how to structure your data for ensembling)  \n\n3. **better results**  \nsince you are using  a pipeline created by an experienced person, it is equivalent to having a data science expert create a data science model for you.  \n\n\n\\* i only mentioned tabular data because to my understanding, we still need deep learning for unstructured data, and traditional ml models that are easily automated like -----> tree !!! -based models typically perform well for tabular data\n\n&amp;#x200b;\n\n(this is my first time making any reddit post, so please let me know if i make any mistakes as well!)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/orx2ih/why_not_automl_every_tabular_data/',)", "identifyer": 5590610, "year": "2021"}, {"autor": "90375", "date": 1621573432000, "content": "What are some good fundamentals of data mining course available online? /!/ And also which book is the good one for learning fundamentals of data mining?\n\nBased on this syllabus?\n\n \n\n* Introduction (2 hours)\n\n1. Data Mining Origin\n2. Data Mining &amp; Data Warehousing basics\n\n* Data Pre-Processing (6 hours )\n\n1. Data Types and Attributes\n2. Data Pre-processing\n3. OLAP &amp; Multidimensional Data Analysis\n4. Various Similarity Measures\n\n* Classification (12 hours)\n\n1. Basics and Algorithms\n2. Decision Tree Classifier\n3. Rule Based Classifier\n4. Nearest Neighbor Classifier\n5. Bayesian Classifier\n6. Artificial Neural Network Classifier\n7. Issues : Overfitting, Validation, Model Comparison\n\n* Association Analysis (10 hours)\n\n1. Basics and Algorithms\n2. Frequent Itemset  Pattern &amp; *Apriori* Principle\n3. FP-Growth, FP-Tree\n4. Handling Categorical Attributes\n5. Sequential, Subgraph, and Infrequent  Patterns\n\n* Cluster Analysis (9 hours)\n\n1. Basics and Algorithms\n2. K-means Clustering \n3. Hierarchical Clustering\n4. DBSCAN Clustering\n5. Issues : Evaluation, Scalability, Comparison\n\n* Anomaly / Fraud Detection (3 hours)\n* Advanced Applications (3 hours)\n\n1. Mining Object and Multimedia\n2. Web-mining\n3. Time-series data mining", "link": "https://www.reddit.com/r/datascience/comments/nhkk3h/what_are_some_good_fundamentals_of_data_mining/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "what are some good fundamentals of data mining course available online? /!/ and also which book is the good one for learning fundamentals of data mining?\n\nbased on this syllabus?\n\n \n\n* introduction (2 hours)\n\n1. data mining origin\n2. data mining &amp; data warehousing basics\n\n* data pre-processing (6 hours )\n\n1. data types and attributes\n2. data pre-processing\n3. olap &amp; multidimensional data analysis\n4. various similarity measures\n\n* classification (12 hours)\n\n1. basics and algorithms\n2. decision -----> tree !!!  classifier\n3. rule based classifier\n4. nearest neighbor classifier\n5. bayesian classifier\n6. artificial neural network classifier\n7. issues : overfitting, validation, model comparison\n\n* association analysis (10 hours)\n\n1. basics and algorithms\n2. frequent itemset  pattern &amp; *apriori* principle\n3. fp-growth, fp-tree\n4. handling categorical attributes\n5. sequential, subgraph, and infrequent  patterns\n\n* cluster analysis (9 hours)\n\n1. basics and algorithms\n2. k-means clustering \n3. hierarchical clustering\n4. dbscan clustering\n5. issues : evaluation, scalability, comparison\n\n* anomaly / fraud detection (3 hours)\n* advanced applications (3 hours)\n\n1. mining object and multimedia\n2. web-mining\n3. time-series data mining", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nhkk3h/what_are_some_good_fundamentals_of_data_mining/',)", "identifyer": 5590683, "year": "2021"}, {"autor": "CKL-IT", "date": 1621528239000, "content": "1 line to visualizations for dependency trees, entity relationships, resolution, assertion, NER and new models for Afrikaans, Welsh, Maltese, Tamil, and Vietnamese - John Snow Labs NLU 3.0.1 for Python /!/ # NLU 3.0.1 Release Notes\nWe are very excited to announce NLU 3.0.1 has been released!\nThis is one of the most visually appealing releases, with the integration of the [Spark-NLP-Display](https://nlp.johnsnowlabs.com/docs/en/display) library and visualizations for `dependency trees`, `entity resolution`, `entity assertion`, `relationship between entities` and `named\nentity recognition`. In addition to this, the schema of how columns are named by NLU has been reworked and all 140+ tutorial notebooks have been updated to reflect the latest changes in NLU 3.0.0+\nFinally, new multilingual models for `Afrikaans`, `Welsh`, `Maltese`, `Tamil`, and`Vietnamese` are now available.\n\n\n\n\n# New Features and Enhancements\n- 1 line to visualization for `NER`, `Dependency`, `Resolution`, `Assertion` and `Relation` via [Spark-NLP-Display](https://nlp.johnsnowlabs.com/docs/en/display) integration\n- Improved column naming schema\n- [Over 140 + NLU tutorial Notebooks updated](https://github.com/JohnSnowLabs/nlu/tree/master/examples) and improved to reflect latest changes in NLU 3.0.0 +\n- New multilingual models for `Afrikaans`, `Welsh`, `Maltese`, `Tamil`, and`Vietnamese`\n- Enhanced offline loading\n\n\n## NLU visualization\nThe latest NLU release integrated the beautiful Spark-NLP-Display package visualizations. You do not need to worry about installing it, when you try to visualize something, NLU will check if\nSpark-NLP-Display is installed, if it is missing it will be dynamically installed into your python executable environment, so you don't need to worry about anything!\n\nSee the [visualization tutorial notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/visualization/NLU_visualizations_tutorial.ipynb)  and [visualization docs](https://nlu.johnsnowlabs.com/docs/en/viz_examples) for more info.\n\n![Cheat Sheet visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/cheat_sheet.png)\n\n## NER visualization\nApplicable to any of the [100+ NER models! See here for an overview](https://nlp.johnsnowlabs.com/models?task=Named+Entity+Recognition)\n```python\nnlu.load('ner').viz(\"Donald Trump from America and Angela Merkel from Germany don't share many oppinions.\")\n```\n![NER visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/NER.png)\n\n## Dependency tree visualization\nVisualizes the structure of the labeled dependency tree and part of speech tags\n```python\nnlu.load('dep.typed').viz(\"Billy went to the mall\")\n```\n\n![Dependency Tree visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/DEP.png)\n\n```python\n#Bigger Example\nnlu.load('dep.typed').viz(\"Donald Trump from America and Angela Merkel from Germany don't share many oppinions but they both love John Snow Labs software\")\n```\n![Dependency Tree visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/DEP_big.png)\n\n## Assertion status visualization\nVisualizes asserted statuses and entities.        \nApplicable to any of the [10 + Assertion models! See here for an overview](https://nlp.johnsnowlabs.com/models?task=Assertion+Status)\n```python\nnlu.load('med_ner.clinical assert').viz(\"The MRI scan showed no signs of cancer in the left lung\")\n```\n\n\n![Assert visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/assertion.png)\n\n```python\n#bigger example\ndata ='This is the case of a very pleasant 46-year-old Caucasian female, seen in clinic on 12/11/07 during which time MRI of the left shoulder showed no evidence of rotator cuff tear. She did have a previous MRI of the cervical spine that did show an osteophyte on the left C6-C7 level. Based on this, negative MRI of the shoulder, the patient was recommended to have anterior cervical discectomy with anterior interbody fusion at C6-C7 level. Operation, expected outcome, risks, and benefits were discussed with her. Risks include, but not exclusive of bleeding and infection, bleeding could be soft tissue bleeding, which may compromise airway and may result in return to the operating room emergently for evacuation of said hematoma. There is also the possibility of bleeding into the epidural space, which can compress the spinal cord and result in weakness and numbness of all four extremities as well as impairment of bowel and bladder function. However, the patient may develop deeper-seated infection, which may require return to the operating room. Should the infection be in the area of the spinal instrumentation, this will cause a dilemma since there might be a need to remove the spinal instrumentation and/or allograft. There is also the possibility of potential injury to the esophageus, the trachea, and the carotid artery. There is also the risks of stroke on the right cerebral circulation should an undiagnosed plaque be propelled from the right carotid. She understood all of these risks and agreed to have the procedure performed.'\nnlu.load('med_ner.clinical assert').viz(data)\n```\n![Assert visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/assertion_big.png)\n\n\n## Relationship between entities visualization\nVisualizes the extracted entities between relationship.    \nApplicable to any of the [20 + Relation Extractor models See here for an overview](https://nlp.johnsnowlabs.com/models?task=Relation+Extraction)\n```python\nnlu.load('med_ner.jsl.wip.clinical relation.temporal_events').viz('The patient developed cancer after a mercury poisoning in 1999 ')\n```\n![Entity Relation visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/relation.png)\n\n```python\n# bigger example\ndata = 'This is the case of a very pleasant 46-year-old Caucasian female, seen in clinic on 12/11/07 during which time MRI of the left shoulder showed no evidence of rotator cuff tear. She did have a previous MRI of the cervical spine that did show an osteophyte on the left C6-C7 level. Based on this, negative MRI of the shoulder, the patient was recommended to have anterior cervical discectomy with anterior interbody fusion at C6-C7 level. Operation, expected outcome, risks, and benefits were discussed with her. Risks include, but not exclusive of bleeding and infection, bleeding could be soft tissue bleeding, which may compromise airway and may result in return to the operating room emergently for evacuation of said hematoma. There is also the possibility of bleeding into the epidural space, which can compress the spinal cord and result in weakness and numbness of all four extremities as well as impairment of bowel and bladder function. However, the patient may develop deeper-seated infection, which may require return to the operating room. Should the infection be in the area of the spinal instrumentation, this will cause a dilemma since there might be a need to remove the spinal instrumentation and/or allograft. There is also the possibility of potential injury to the esophageus, the trachea, and the carotid artery. There is also the risks of stroke on the right cerebral circulation should an undiagnosed plaque be propelled from the right carotid. She understood all of these risks and agreed to have the procedure performed'\npipe = nlu.load('med_ner.jsl.wip.clinical relation.clinical').viz(data)\n```\n![Entity Relation visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/relation_big.png)\n\n\n## Entity Resolution visualization for chunks\nVisualizes resolutions of entities\nApplicable to any of the [100+ Resolver models See here for an overview](https://nlp.johnsnowlabs.com/models?task=Entity+Resolution)\n```python\nnlu.load('med_ner.jsl.wip.clinical resolve_chunk.rxnorm.in').viz(\"He took Prevacid 30 mg  daily\")\n```\n![Chunk Resolution visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/resolve_chunk.png)\n\n```python\n# bigger example\ndata = \"This is an 82 - year-old male with a history of prior tobacco use , hypertension , chronic renal insufficiency , COPD , gastritis , and TIA who initially presented to Braintree with a non-ST elevation MI and Guaiac positive stools , transferred to St . Margaret\\'s Center for Women &amp; Infants for cardiac catheterization with PTCA to mid LAD lesion complicated by hypotension and bradycardia requiring Atropine , IV fluids and transient dopamine possibly secondary to vagal reaction , subsequently transferred to CCU for close monitoring , hemodynamically stable at the time of admission to the CCU .\"\nnlu.load('med_ner.jsl.wip.clinical resolve_chunk.rxnorm.in').viz(data)\n```\n\n![Chunk Resolution visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/resolve_chunk_big.png)\n\n\n## Entity Resolution visualization for sentences\nVisualizes resolutions of entities in sentences\nApplicable to any of the [100+ Resolver models See here for an overview](https://nlp.johnsnowlabs.com/models?task=Entity+Resolution)\n```python\nnlu.load('med_ner.jsl.wip.clinical resolve.icd10cm').viz('She was diagnosed with a respiratory congestion')\n```\n![Sentence Resolution visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/resolve_sentence.png)\n\n```python\n# bigger example\ndata = 'The patient is a 5-month-old infant who presented initially on Monday with a cold, cough, and runny nose for 2 days. Mom states she had no fever. Her appetite was good but she was spitting up a lot. She had no difficulty breathing and her cough was described as dry and hacky. At that time, physical exam showed a right TM, which was red. Left TM was okay. She was fairly congested but looked happy and playful. She was started on Amoxil and Aldex and we told to recheck in 2 weeks to recheck her ear. Mom returned to clinic again today because she got much worse overnight. She was having difficulty breathing. She was much more congested and her appetite had decreased significantly today. She also spiked a temperature yesterday of 102.6 and always having trouble sleeping secondary to congestion'\nnlu.load('med_ner.jsl.wip.clinical resolve.icd10cm').viz(data)\n```\n![Sentence Resolution visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/resolve_sentence_big.png)\n\n## Configure visualizations\n### Define custom colors for labels\nSome entity and relation labels will be highlighted with a pre-defined color, which you [can find here](https://github.com/JohnSnowLabs/spark-nlp-display/tree/main/sparknlp_display/label_colors).    \nFor labels that have no color defined, a random color will be generated.     \nYou can define colors for labels manually, by specifying via the `viz_colors` parameter\nand defining `hex color codes` in a dictionary that maps `labels` to `colors` .\n```python\ndata = 'Dr. John Snow suggested that Fritz takes 5mg penicilin for his cough'\n# Define custom colors for labels\nviz_colors={'STRENGTH':'#800080', 'DRUG_BRANDNAME':'#77b5fe', 'GENDER':'#77ffe'}\nnlu.load('med_ner.jsl.wip.clinical').viz(data,viz_colors =viz_colors)\n```\n![define colors labels](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/define_colors.png)\n\n\n### Filter entities that get highlighted\nBy default every entity class will be visualized.    \nThe `labels_to_viz` can be used to define a set of labels to highlight.       \nApplicable for ner, resolution and assert.\n```python\ndata = 'Dr. John Snow suggested that Fritz takes 5mg penicilin for his cough'\n# Filter wich NER label to viz\nlabels_to_viz=['SYMPTOM']\nnlu.load('med_ner.jsl.wip.clinical').viz(data,labels_to_viz=labels_to_viz)\n```\n![filter labels](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/filter_labels.png)\n\n\n## New models\nNew multilingual models for `Afrikaans`, `Welsh`, `Maltese`, `Tamil`, and`Vietnamese`\n\n| nlu.load() Refrence                                          | Spark NLP Refrence                                           |\n| ------------------------------------------------------------ | ------------------------------------------------------------ |\n| [vi.lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_vi.html) | [lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_vi.html) |\n| [mt.lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_mt.html) | [lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_mt.html) |\n| [ta.lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_ta.html) | [lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_ta.html) |\n| [af.lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_af.html) | [lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_af.html) |\n| [af.pos](https://nlp.johnsnowlabs.com/2021/04/06/pos_afribooms_af.html) | [pos_afribooms](https://nlp.johnsnowlabs.com/2021/04/06/pos_afribooms_af.html) |\n| [cy.lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_cy.html) | [lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_cy.html) |\n\n## Reworked and updated NLU tutorial notebooks\n\nAll of the [140+ NLU tutorial Notebooks](https://github.com/JohnSnowLabs/nlu/tree/master/examples) have been updated and reworked to reflect the latest changes in NLU 3.0.0+\n\n\n## Improved Column Name generation\n- NLU categorized each internal component now with boolean labels for `name_deductable` and `always_name_deductable` .\n- Before generating column names, NLU checks wether each component is of unique in the pipeline or not. If a component is not unique in the\n  pipe and there are multiple components of same type, i.e. multiple `NER` models, NLU will deduct a base name for the final output columns from the\n  NLU reference each NER model is pointing to.\n- If on the other hand, there is only one `NER` model in the pipeline, only the default `ner` column prefixed will be generated.\n- For some components, like `embeddings` and `classifiers` are now defined as `always_name_deductable`, for those NLU will always try to infer a meaningful base name for the output columns.\n- Newly trained component output columns will now be prefixed with `trained_&lt;type&gt;` , for types `pos` , `ner`, `cLassifier`, `sentiment` and `multi_classifier`\n\n## Enhanced offline mode\n- You can still load a model from a path as usual with `nlu.load(path=model_path)` and output columns will be suffixed with `from_disk`\n- You can now optionally also specify `request` parameter during  load a model from HDD, it will be used to deduct more meaningful column name suffixes, instead of `from_disk`, i.e. by calling `nlu.load(request ='en.embed_sentence.biobert.pubmed_pmc_base_cased', path=model_path)`\n\n\n### Bugfixes\n- Fixed a bug that caused  resolution algorithms output level to be inferred incorrectly\n- Fixed a bug that caused stranger cols got dropped\n- Fixed a bug that caused endings to miss when  .predict(position=True) was specified\n- Fixed a bug that caused pd.Series to be converted incorrectly internally\n- Fixed a bug that caused output level transformations to crash\n- Fixed a bug that caused verbose mode not to turn of properly after turning it on.\n- fixed a bug that caused some models to crash when loaded for HDD\n\n# Additional NLU resources\n* [140+ updates tutorials](https://github.com/JohnSnowLabs/nlu/tree/master/examples)\n* [Updated visualization docs](https://nlu.johnsnowlabs.com/docs/en/viz_examples)\n* [Models Hub](https://nlp.johnsnowlabs.com/models) with new models\n* [Spark NLP publications](https://medium.com/spark-nlp)\n* [NLU in Action](https://nlp.johnsnowlabs.com/demo)\n* [NLU documentation](https://nlu.johnsnowlabs.com/docs/en/install)\n* [Discussions](https://github.com/JohnSnowLabs/spark-nlp/discussions) Engage with other community members, share ideas, and show off how you use Spark NLP and NLU!\n\n# 1 line Install NLU on Google Colab\n```!wget https://setup.johnsnowlabs.com/nlu/colab.sh  -O - | bash```\n# 1 line Install NLU on Kaggle\n```!wget https://setup.johnsnowlabs.com/nlu/kaggle.sh  -O - | bash```\n# Install via PIP\n```! pip install nlu pyspark==3.0.1```", "link": "https://www.reddit.com/r/datascience/comments/nh4qv7/1_line_to_visualizations_for_dependency_trees/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "1 line to visualizations for dependency trees, entity relationships, resolution, assertion, ner and new models for afrikaans, welsh, maltese, tamil, and vietnamese - john snow labs nlu 3.0.1 for python /!/ # nlu 3.0.1 release notes\nwe are very excited to announce nlu 3.0.1 has been released!\nthis is one of the most visually appealing releases, with the integration of the [spark-nlp-display](https://nlp.johnsnowlabs.com/docs/en/display) library and visualizations for `dependency trees`, `entity resolution`, `entity assertion`, `relationship between entities` and `named\nentity recognition`. in addition to this, the schema of how columns are named by nlu has been reworked and all 140+ tutorial notebooks have been updated to reflect the latest changes in nlu 3.0.0+\nfinally, new multilingual models for `afrikaans`, `welsh`, `maltese`, `tamil`, and`vietnamese` are now available.\n\n\n\n\n# new features and enhancements\n- 1 line to visualization for `ner`, `dependency`, `resolution`, `assertion` and `relation` via [spark-nlp-display](https://nlp.johnsnowlabs.com/docs/en/display) integration\n- improved column naming schema\n- [over 140 + nlu tutorial notebooks updated](https://github.com/johnsnowlabs/nlu/-----> tree !!! /master/examples) and improved to reflect latest changes in nlu 3.0.0 +\n- new multilingual models for `afrikaans`, `welsh`, `maltese`, `tamil`, and`vietnamese`\n- enhanced offline loading\n\n\n## nlu visualization\nthe latest nlu release integrated the beautiful spark-nlp-display package visualizations. you do not need to worry about installing it, when you try to visualize something, nlu will check if\nspark-nlp-display is installed, if it is missing it will be dynamically installed into your python executable environment, so you don't need to worry about anything!\n\nsee the [visualization tutorial notebook](https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/visualization/nlu_visualizations_tutorial.ipynb)  and [visualization docs](https://nlu.johnsnowlabs.com/docs/en/viz_examples) for more info.\n\n![cheat sheet visualization](https://raw.githubusercontent.com/johnsnowlabs/nlu/master/docs/assets/images/nlu/vizexamples/viz_module/cheat_sheet.png)\n\n## ner visualization\napplicable to any of the [100+ ner models! see here for an overview](https://nlp.johnsnowlabs.com/models?task=named+entity+recognition)\n```python\nnlu.load('ner').viz(\"donald trump from america and angela merkel from germany don't share many oppinions.\")\n```\n![ner visualization](https://raw.githubusercontent.com/johnsnowlabs/nlu/master/docs/assets/images/nlu/vizexamples/viz_module/ner.png)\n\n## dependency tree visualization\nvisualizes the structure of the labeled dependency tree and part of speech tags\n```python\nnlu.load('dep.typed').viz(\"billy went to the mall\")\n```\n\n![dependency tree visualization](https://raw.githubusercontent.com/johnsnowlabs/nlu/master/docs/assets/images/nlu/vizexamples/viz_module/dep.png)\n\n```python\n#bigger example\nnlu.load('dep.typed').viz(\"donald trump from america and angela merkel from germany don't share many oppinions but they both love john snow labs software\")\n```\n![dependency tree visualization](https://raw.githubusercontent.com/johnsnowlabs/nlu/master/docs/assets/images/nlu/vizexamples/viz_module/dep_big.png)\n\n## assertion status visualization\nvisualizes asserted statuses and entities.        \napplicable to any of the [10 + assertion models! see here for an overview](https://nlp.johnsnowlabs.com/models?task=assertion+status)\n```python\nnlu.load('med_ner.clinical assert').viz(\"the mri scan showed no signs of cancer in the left lung\")\n```\n\n\n![assert visualization](https://raw.githubusercontent.com/johnsnowlabs/nlu/master/docs/assets/images/nlu/vizexamples/viz_module/assertion.png)\n\n```python\n#bigger example\ndata ='this is the case of a very pleasant 46-year-old caucasian female, seen in clinic on 12/11/07 during which time mri of the left shoulder showed no evidence of rotator cuff tear. she did have a previous mri of the cervical spine that did show an osteophyte on the left c6-c7 level. based on this, negative mri of the shoulder, the patient was recommended to have anterior cervical discectomy with anterior interbody fusion at c6-c7 level. operation, expected outcome, risks, and benefits were discussed with her. risks include, but not exclusive of bleeding and infection, bleeding could be soft tissue bleeding, which may compromise airway and may result in return to the operating room emergently for evacuation of said hematoma. there is also the possibility of bleeding into the epidural space, which can compress the spinal cord and result in weakness and numbness of all four extremities as well as impairment of bowel and bladder function. however, the patient may develop deeper-seated infection, which may require return to the operating room. should the infection be in the area of the spinal instrumentation, this will cause a dilemma since there might be a need to remove the spinal instrumentation and/or allograft. there is also the possibility of potential injury to the esophageus, the trachea, and the carotid artery. there is also the risks of stroke on the right cerebral circulation should an undiagnosed plaque be propelled from the right carotid. she understood all of these risks and agreed to have the procedure performed.'\nnlu.load('med_ner.clinical assert').viz(data)\n```\n![assert visualization](https://raw.githubusercontent.com/johnsnowlabs/nlu/master/docs/assets/images/nlu/vizexamples/viz_module/assertion_big.png)\n\n\n## relationship between entities visualization\nvisualizes the extracted entities between relationship.    \napplicable to any of the [20 + relation extractor models see here for an overview](https://nlp.johnsnowlabs.com/models?task=relation+extraction)\n```python\nnlu.load('med_ner.jsl.wip.clinical relation.temporal_events').viz('the patient developed cancer after a mercury poisoning in 1999 ')\n```\n![entity relation visualization](https://raw.githubusercontent.com/johnsnowlabs/nlu/master/docs/assets/images/nlu/vizexamples/viz_module/relation.png)\n\n```python\n# bigger example\ndata = 'this is the case of a very pleasant 46-year-old caucasian female, seen in clinic on 12/11/07 during which time mri of the left shoulder showed no evidence of rotator cuff tear. she did have a previous mri of the cervical spine that did show an osteophyte on the left c6-c7 level. based on this, negative mri of the shoulder, the patient was recommended to have anterior cervical discectomy with anterior interbody fusion at c6-c7 level. operation, expected outcome, risks, and benefits were discussed with her. risks include, but not exclusive of bleeding and infection, bleeding could be soft tissue bleeding, which may compromise airway and may result in return to the operating room emergently for evacuation of said hematoma. there is also the possibility of bleeding into the epidural space, which can compress the spinal cord and result in weakness and numbness of all four extremities as well as impairment of bowel and bladder function. however, the patient may develop deeper-seated infection, which may require return to the operating room. should the infection be in the area of the spinal instrumentation, this will cause a dilemma since there might be a need to remove the spinal instrumentation and/or allograft. there is also the possibility of potential injury to the esophageus, the trachea, and the carotid artery. there is also the risks of stroke on the right cerebral circulation should an undiagnosed plaque be propelled from the right carotid. she understood all of these risks and agreed to have the procedure performed'\npipe = nlu.load('med_ner.jsl.wip.clinical relation.clinical').viz(data)\n```\n![entity relation visualization](https://raw.githubusercontent.com/johnsnowlabs/nlu/master/docs/assets/images/nlu/vizexamples/viz_module/relation_big.png)\n\n\n## entity resolution visualization for chunks\nvisualizes resolutions of entities\napplicable to any of the [100+ resolver models see here for an overview](https://nlp.johnsnowlabs.com/models?task=entity+resolution)\n```python\nnlu.load('med_ner.jsl.wip.clinical resolve_chunk.rxnorm.in').viz(\"he took prevacid 30 mg  daily\")\n```\n![chunk resolution visualization](https://raw.githubusercontent.com/johnsnowlabs/nlu/master/docs/assets/images/nlu/vizexamples/viz_module/resolve_chunk.png)\n\n```python\n# bigger example\ndata = \"this is an 82 - year-old male with a history of prior tobacco use , hypertension , chronic renal insufficiency , copd , gastritis , and tia who initially presented to braintree with a non-st elevation mi and guaiac positive stools , transferred to st . margaret\\'s center for women &amp; infants for cardiac catheterization with ptca to mid lad lesion complicated by hypotension and bradycardia requiring atropine , iv fluids and transient dopamine possibly secondary to vagal reaction , subsequently transferred to ccu for close monitoring , hemodynamically stable at the time of admission to the ccu .\"\nnlu.load('med_ner.jsl.wip.clinical resolve_chunk.rxnorm.in').viz(data)\n```\n\n![chunk resolution visualization](https://raw.githubusercontent.com/johnsnowlabs/nlu/master/docs/assets/images/nlu/vizexamples/viz_module/resolve_chunk_big.png)\n\n\n## entity resolution visualization for sentences\nvisualizes resolutions of entities in sentences\napplicable to any of the [100+ resolver models see here for an overview](https://nlp.johnsnowlabs.com/models?task=entity+resolution)\n```python\nnlu.load('med_ner.jsl.wip.clinical resolve.icd10cm').viz('she was diagnosed with a respiratory congestion')\n```\n![sentence resolution visualization](https://raw.githubusercontent.com/johnsnowlabs/nlu/master/docs/assets/images/nlu/vizexamples/viz_module/resolve_sentence.png)\n\n```python\n# bigger example\ndata = 'the patient is a 5-month-old infant who presented initially on monday with a cold, cough, and runny nose for 2 days. mom states she had no fever. her appetite was good but she was spitting up a lot. she had no difficulty breathing and her cough was described as dry and hacky. at that time, physical exam showed a right tm, which was red. left tm was okay. she was fairly congested but looked happy and playful. she was started on amoxil and aldex and we told to recheck in 2 weeks to recheck her ear. mom returned to clinic again today because she got much worse overnight. she was having difficulty breathing. she was much more congested and her appetite had decreased significantly today. she also spiked a temperature yesterday of 102.6 and always having trouble sleeping secondary to congestion'\nnlu.load('med_ner.jsl.wip.clinical resolve.icd10cm').viz(data)\n```\n![sentence resolution visualization](https://raw.githubusercontent.com/johnsnowlabs/nlu/master/docs/assets/images/nlu/vizexamples/viz_module/resolve_sentence_big.png)\n\n## configure visualizations\n### define custom colors for labels\nsome entity and relation labels will be highlighted with a pre-defined color, which you [can find here](https://github.com/johnsnowlabs/spark-nlp-display/tree/main/sparknlp_display/label_colors).    \nfor labels that have no color defined, a random color will be generated.     \nyou can define colors for labels manually, by specifying via the `viz_colors` parameter\nand defining `hex color codes` in a dictionary that maps `labels` to `colors` .\n```python\ndata = 'dr. john snow suggested that fritz takes 5mg penicilin for his cough'\n# define custom colors for labels\nviz_colors={'strength':'#800080', 'drug_brandname':'#77b5fe', 'gender':'#77ffe'}\nnlu.load('med_ner.jsl.wip.clinical').viz(data,viz_colors =viz_colors)\n```\n![define colors labels](https://raw.githubusercontent.com/johnsnowlabs/nlu/master/docs/assets/images/nlu/vizexamples/viz_module/define_colors.png)\n\n\n### filter entities that get highlighted\nby default every entity class will be visualized.    \nthe `labels_to_viz` can be used to define a set of labels to highlight.       \napplicable for ner, resolution and assert.\n```python\ndata = 'dr. john snow suggested that fritz takes 5mg penicilin for his cough'\n# filter wich ner label to viz\nlabels_to_viz=['symptom']\nnlu.load('med_ner.jsl.wip.clinical').viz(data,labels_to_viz=labels_to_viz)\n```\n![filter labels](https://raw.githubusercontent.com/johnsnowlabs/nlu/master/docs/assets/images/nlu/vizexamples/viz_module/filter_labels.png)\n\n\n## new models\nnew multilingual models for `afrikaans`, `welsh`, `maltese`, `tamil`, and`vietnamese`\n\n| nlu.load() refrence                                          | spark nlp refrence                                           |\n| ------------------------------------------------------------ | ------------------------------------------------------------ |\n| [vi.lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_vi.html) | [lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_vi.html) |\n| [mt.lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_mt.html) | [lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_mt.html) |\n| [ta.lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_ta.html) | [lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_ta.html) |\n| [af.lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_af.html) | [lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_af.html) |\n| [af.pos](https://nlp.johnsnowlabs.com/2021/04/06/pos_afribooms_af.html) | [pos_afribooms](https://nlp.johnsnowlabs.com/2021/04/06/pos_afribooms_af.html) |\n| [cy.lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_cy.html) | [lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_cy.html) |\n\n## reworked and updated nlu tutorial notebooks\n\nall of the [140+ nlu tutorial notebooks](https://github.com/johnsnowlabs/nlu/tree/master/examples) have been updated and reworked to reflect the latest changes in nlu 3.0.0+\n\n\n## improved column name generation\n- nlu categorized each internal component now with boolean labels for `name_deductable` and `always_name_deductable` .\n- before generating column names, nlu checks wether each component is of unique in the pipeline or not. if a component is not unique in the\n  pipe and there are multiple components of same type, i.e. multiple `ner` models, nlu will deduct a base name for the final output columns from the\n  nlu reference each ner model is pointing to.\n- if on the other hand, there is only one `ner` model in the pipeline, only the default `ner` column prefixed will be generated.\n- for some components, like `embeddings` and `classifiers` are now defined as `always_name_deductable`, for those nlu will always try to infer a meaningful base name for the output columns.\n- newly trained component output columns will now be prefixed with `trained_&lt;type&gt;` , for types `pos` , `ner`, `classifier`, `sentiment` and `multi_classifier`\n\n## enhanced offline mode\n- you can still load a model from a path as usual with `nlu.load(path=model_path)` and output columns will be suffixed with `from_disk`\n- you can now optionally also specify `request` parameter during  load a model from hdd, it will be used to deduct more meaningful column name suffixes, instead of `from_disk`, i.e. by calling `nlu.load(request ='en.embed_sentence.biobert.pubmed_pmc_base_cased', path=model_path)`\n\n\n### bugfixes\n- fixed a bug that caused  resolution algorithms output level to be inferred incorrectly\n- fixed a bug that caused stranger cols got dropped\n- fixed a bug that caused endings to miss when  .predict(position=true) was specified\n- fixed a bug that caused pd.series to be converted incorrectly internally\n- fixed a bug that caused output level transformations to crash\n- fixed a bug that caused verbose mode not to turn of properly after turning it on.\n- fixed a bug that caused some models to crash when loaded for hdd\n\n# additional nlu resources\n* [140+ updates tutorials](https://github.com/johnsnowlabs/nlu/tree/master/examples)\n* [updated visualization docs](https://nlu.johnsnowlabs.com/docs/en/viz_examples)\n* [models hub](https://nlp.johnsnowlabs.com/models) with new models\n* [spark nlp publications](https://medium.com/spark-nlp)\n* [nlu in action](https://nlp.johnsnowlabs.com/demo)\n* [nlu documentation](https://nlu.johnsnowlabs.com/docs/en/install)\n* [discussions](https://github.com/johnsnowlabs/spark-nlp/discussions) engage with other community members, share ideas, and show off how you use spark nlp and nlu!\n\n# 1 line install nlu on google colab\n```!wget https://setup.johnsnowlabs.com/nlu/colab.sh  -o - | bash```\n# 1 line install nlu on kaggle\n```!wget https://setup.johnsnowlabs.com/nlu/kaggle.sh  -o - | bash```\n# install via pip\n```! pip install nlu pyspark==3.0.1```", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nh4qv7/1_line_to_visualizations_for_dependency_trees/',)", "identifyer": 5590721, "year": "2021"}, {"autor": "blueest", "date": 1610470958000, "content": "Manually Extracting \"Rules\" from a Decision Tree /!/  \n\nI am using the R programming language. I used the \"rpart\" library and fit a decision tree using some data:\n\n    #from a previous question : https://stackoverflow.com/questions/65678552/r-changing-plot-sizes        \n    library(rpart)   \n       car.test.frame$Reliability = as.factor(car.test.frame$Reliability)   \n            z.auto &lt;- rpart(Reliability ~ ., car.test.frame)      \n    plot(z.auto)    \n      text(z.auto, use.n=TRUE, xpd=TRUE, cex=.8) \n\nThis is good, but I am looking for an easier way to summarize the results of this tree in case the tree becomes too big, complicated and cluttered (and impossible to visualize). I found another stackoverflow post over here that shows how to obtain a listing of rules: [Extracting Information from the Decision Rules in rpart package](https://stackoverflow.com/questions/36401411/extracting-information-from-the-decision-rules-in-rpart-package/51680932)\n\n    library(party)\n      library(partykit) \n    \n      party_obj &lt;- as.party.rpart(z.auto, data = TRUE) \n     decisions &lt;- partykit:::.list.rules.party(party_obj)  \n    cat(paste(decisions, collapse = \"\\n\")) \n\nThis returns the following list of rules (each line is a rule corresponding to the plot of \"z.auto\"):\n\n    Country %in% c(\"NA\", \"Germany\", \"Korea\", \"Mexico\", \"Sweden\", \"USA\") &amp; Weight &gt;= 3167.5 Country %in% c(\"NA\", \"Germany\", \"Korea\", \"Mexico\", \"Sweden\", \"USA\") &amp; Weight &lt; 3167.5 Country %in% c(\"NA\", \"Japan\", \"Japan/USA\")&gt;  \n\nHowever, from this list, it is not possible to know which rule results in which value of \"Reliability\". For the time being, I am manually interpreting the tree and manually tracing each rule to the result, but is there a way to add to each line \"the corresponding value of reliability\"?\n\ne.g. Is it possible to produce something like this?\n\n    Country %in% c(\"NA\", \"Germany\", \"Korea\", \"Mexico\", \"Sweden\", \"USA\") &amp; Weight &gt;= 3167.5 then reliability = 3,7,4,0 \n\n(note: I am also not sure why the countries are appearing as \"befgh\" instead of their actual names. )\n\nThanks", "link": "https://www.reddit.com/r/datascience/comments/kvvmn6/manually_extracting_rules_from_a_decision_tree/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "manually extracting \"rules\" from a decision -----> tree !!!  /!/  \n\ni am using the r programming language. i used the \"rpart\" library and fit a decision tree using some data:\n\n    #from a previous question : https://stackoverflow.com/questions/65678552/r-changing-plot-sizes        \n    library(rpart)   \n       car.test.frame$reliability = as.factor(car.test.frame$reliability)   \n            z.auto &lt;- rpart(reliability ~ ., car.test.frame)      \n    plot(z.auto)    \n      text(z.auto, use.n=true, xpd=true, cex=.8) \n\nthis is good, but i am looking for an easier way to summarize the results of this tree in case the tree becomes too big, complicated and cluttered (and impossible to visualize). i found another stackoverflow post over here that shows how to obtain a listing of rules: [extracting information from the decision rules in rpart package](https://stackoverflow.com/questions/36401411/extracting-information-from-the-decision-rules-in-rpart-package/51680932)\n\n    library(party)\n      library(partykit) \n    \n      party_obj &lt;- as.party.rpart(z.auto, data = true) \n     decisions &lt;- partykit:::.list.rules.party(party_obj)  \n    cat(paste(decisions, collapse = \"\\n\")) \n\nthis returns the following list of rules (each line is a rule corresponding to the plot of \"z.auto\"):\n\n    country %in% c(\"na\", \"germany\", \"korea\", \"mexico\", \"sweden\", \"usa\") &amp; weight &gt;= 3167.5 country %in% c(\"na\", \"germany\", \"korea\", \"mexico\", \"sweden\", \"usa\") &amp; weight &lt; 3167.5 country %in% c(\"na\", \"japan\", \"japan/usa\")&gt;  \n\nhowever, from this list, it is not possible to know which rule results in which value of \"reliability\". for the time being, i am manually interpreting the tree and manually tracing each rule to the result, but is there a way to add to each line \"the corresponding value of reliability\"?\n\ne.g. is it possible to produce something like this?\n\n    country %in% c(\"na\", \"germany\", \"korea\", \"mexico\", \"sweden\", \"usa\") &amp; weight &gt;= 3167.5 then reliability = 3,7,4,0 \n\n(note: i am also not sure why the countries are appearing as \"befgh\" instead of their actual names. )\n\nthanks", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/kvvmn6/manually_extracting_rules_from_a_decision_tree/',)", "identifyer": 5590977, "year": "2021"}, {"autor": "SQL_beginner", "date": 1610416856000, "content": "Using Decision Trees to Find Business Rules - Will this work? /!/ Suppose there is a bank manager that wants to find out if a customer will be approved for a loan or not (based on variables associated with each customer such as age, gender, credit, etc). A lot of historical information is available for different customers on whether they were approved or not approved. The bank managers knows that a predictive model such as a neural network can be made that could accurately predict whether or not a customer will be approved for a loan or not. But, the bank manager is more interested in determining a set of \"business rules\" that are easily understandable for both his staff and the customers, e.g. older people will university degrees are more likely to be approved.\n\nWould this idea work? Take the historical data and fit a regular decision tree (e.g. c 4.5) and see how well this decision tree can predict whether or not customers will be approved. If this decision tree is able to predict this data well - then, visually inspect the branch-tree structure of the tree, and see if there any clear \"paths\" in the tree ... and then try to \"reverse engineer\" business rules from this tree.\n\nIs this idea logical?\n\nThanks!", "link": "https://www.reddit.com/r/datascience/comments/kvhrfb/using_decision_trees_to_find_business_rules_will/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "using decision trees to find business rules - will this work? /!/ suppose there is a bank manager that wants to find out if a customer will be approved for a loan or not (based on variables associated with each customer such as age, gender, credit, etc). a lot of historical information is available for different customers on whether they were approved or not approved. the bank managers knows that a predictive model such as a neural network can be made that could accurately predict whether or not a customer will be approved for a loan or not. but, the bank manager is more interested in determining a set of \"business rules\" that are easily understandable for both his staff and the customers, e.g. older people will university degrees are more likely to be approved.\n\nwould this idea work? take the historical data and fit a regular decision -----> tree !!!  (e.g. c 4.5) and see how well this decision -----> tree !!!  can predict whether or not customers will be approved. if this decision tree is able to predict this data well - then, visually inspect the branch-tree structure of the tree, and see if there any clear \"paths\" in the tree ... and then try to \"reverse engineer\" business rules from this tree.\n\nis this idea logical?\n\nthanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/kvhrfb/using_decision_trees_to_find_business_rules_will/',)", "identifyer": 5591009, "year": "2021"}, {"autor": "ath_leaps", "date": 1626932735000, "content": "Decision Tree", "link": "https://www.reddit.com/r/datascience/comments/op7q30/decision_tree/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "decision -----> tree !!! ", "sortedWord": "None", "removed": "('reddit',)", "score": 1, "comments": 0, "media": "('link',)", "medialink": "('https://t.me/leapsprograms',)", "identifyer": 5591133, "year": "2021"}, {"autor": "Jbor941197", "date": 1613056735000, "content": "Decision Tree Best Split for Entropy Optimization /!/ Hello all, \n\nMy company is pretty bad with allowing us to get more packages and I don't have decisions trees. I've written the code for it following one of the decision trees from scratch videos on youtube, but now Im handling a lot of data and my code is not optimized. Right now the logic is like this  \nfor col in descriptive\\_columns:  \npotential splits = distinct values of col  \nfor split of potential splits:  \nabove, below = df\\[col&gt;split\\],df\\[col&lt;=split\\]  \ncalcTotal Entropy from above and below  \nif it's less the prior entropy store the split  \n\n\n  \n\n\nDoes anyone know how to make this faster?\n\nI was thinking maybe if the entropy starts decreasing after a certain split value there's no need to calculate the rest cause it will only decrease but I'm not sure if that's true", "link": "https://www.reddit.com/r/datascience/comments/lhmtp8/decision_tree_best_split_for_entropy_optimization/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "decision -----> tree !!!  best split for entropy optimization /!/ hello all, \n\nmy company is pretty bad with allowing us to get more packages and i don't have decisions trees. i've written the code for it following one of the decision trees from scratch videos on youtube, but now im handling a lot of data and my code is not optimized. right now the logic is like this  \nfor col in descriptive\\_columns:  \npotential splits = distinct values of col  \nfor split of potential splits:  \nabove, below = df\\[col&gt;split\\],df\\[col&lt;=split\\]  \ncalctotal entropy from above and below  \nif it's less the prior entropy store the split  \n\n\n  \n\n\ndoes anyone know how to make this faster?\n\ni was thinking maybe if the entropy starts decreasing after a certain split value there's no need to calculate the rest cause it will only decrease but i'm not sure if that's true", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 7, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/lhmtp8/decision_tree_best_split_for_entropy_optimization/',)", "identifyer": 5591299, "year": "2021"}, {"autor": "donttalktomeorson", "date": 1625599538000, "content": "Regression/Decision Tree model building tutor /!/ Hi all! I\u2019m currently in a data science program and need a few hours of tutoring. PM me if you\u2019re interested and we can figure something out!", "link": "https://www.reddit.com/r/datascience/comments/of23qs/regressiondecision_tree_model_building_tutor/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "regression/decision -----> tree !!!  model building tutor /!/ hi all! i\u2019m currently in a data science program and need a few hours of tutoring. pm me if you\u2019re interested and we can figure something out!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/of23qs/regressiondecision_tree_model_building_tutor/',)", "identifyer": 5591498, "year": "2021"}, {"autor": "Solar1xxx", "date": 1632579257000, "content": "Tabular data and decision tree - how to improve acc? /!/ Hello all,\nI'm now working on a tabular dataset that contain information about customers and I need to classify them using decision tree.. that is to visualize the tree to explain the model.\n\nThe data is 800 samples with 170 features and 30 classes.\nSo far I tried to focus on the preprocessing to improve but got stuck without any new ideas..\n\nWhat I did so far - missing information we filled with unknown (to avoid Nan), encoded all the strings in the data to be numbers, also the labels (label encoder), then ran the model few times.\nAfter running the model with checked what features are not useful at all or very little and removed them.. then ran the model again .\n\nSo far 42% acc.. but we wish to get higher.. hopping to cross the 50% mark\n\nAny ideas?", "link": "https://www.reddit.com/r/datascience/comments/pv7jte/tabular_data_and_decision_tree_how_to_improve_acc/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "tabular data and decision -----> tree !!!  - how to improve acc? /!/ hello all,\ni'm now working on a tabular dataset that contain information about customers and i need to classify them using decision tree.. that is to visualize the tree to explain the model.\n\nthe data is 800 samples with 170 features and 30 classes.\nso far i tried to focus on the preprocessing to improve but got stuck without any new ideas..\n\nwhat i did so far - missing information we filled with unknown (to avoid nan), encoded all the strings in the data to be numbers, also the labels (label encoder), then ran the model few times.\nafter running the model with checked what features are not useful at all or very little and removed them.. then ran the model again .\n\nso far 42% acc.. but we wish to get higher.. hopping to cross the 50% mark\n\nany ideas?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pv7jte/tabular_data_and_decision_tree_how_to_improve_acc/',)", "identifyer": 5591644, "year": "2021"}, {"autor": "Laurence-Lin", "date": 1628907379000, "content": "I want to build my own data science blog, what is the best recommendation? /!/ I've now used Medium, Github to record my learned projects and stuffs.\n\nHowever, I would like to build an data science blog that is structured in separate topics and others could view it more easily.\n\n&amp;#x200B;\n\nFor example, in the medium page: [https://lawrence123.medium.com/](https://lawrence123.medium.com/)\n\n&amp;#x200B;\n\nI could see all the post stories for an author, but I could not build tree structured posts like: \n\n&amp;#x200B;\n\n`Visualization (2):`\n\n`-- Andrew plot`\n\n`-- Correlation plot`\n\n&amp;#x200B;\n\n`Probabilistic (1):`\n\n`-- Monte Carlo`\n\n&amp;#x200B;\n\nThat I could find the bigger topic, and group different posts into subgroup.\n\n&amp;#x200B;\n\nIs there any recommended website to build these type of blogs?\n\n&amp;#x200B;\n\nThank you!", "link": "https://www.reddit.com/r/datascience/comments/p3zfub/i_want_to_build_my_own_data_science_blog_what_is/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "i want to build my own data science blog, what is the best recommendation? /!/ i've now used medium, github to record my learned projects and stuffs.\n\nhowever, i would like to build an data science blog that is structured in separate topics and others could view it more easily.\n\n&amp;#x200b;\n\nfor example, in the medium page: [https://lawrence123.medium.com/](https://lawrence123.medium.com/)\n\n&amp;#x200b;\n\ni could see all the post stories for an author, but i could not build -----> tree !!!  structured posts like: \n\n&amp;#x200b;\n\n`visualization (2):`\n\n`-- andrew plot`\n\n`-- correlation plot`\n\n&amp;#x200b;\n\n`probabilistic (1):`\n\n`-- monte carlo`\n\n&amp;#x200b;\n\nthat i could find the bigger topic, and group different posts into subgroup.\n\n&amp;#x200b;\n\nis there any recommended website to build these type of blogs?\n\n&amp;#x200b;\n\nthank you!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 18, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/p3zfub/i_want_to_build_my_own_data_science_blog_what_is/',)", "identifyer": 5591710, "year": "2021"}, {"autor": "senor_shoes", "date": 1628791980000, "content": "My information dump for people trying to break into data science/interview notes /!/ **TLRD**: People in my personal life have asked for insight on breaking into the data science field/the interview loop. The following is a poorly formatted/continually updated list of my thoughts that I continually send out to people who've asked for them. I've decided to share it with the wider community. Apologizes for the poor formatting, I [did not have the time to get the markup pretty](https://www.goodreads.com/quotes/21422-i-didn-t-have-time-to-write-a-short-letter-so).\n\n\n**Audience**: People who are trying to break into data science and need help with the interview/job search. Early-mid career people might find some nuggets useful. \n\n\n**About me**: Did my PhD doing experimental stuff with semiconductors. I'm comfortable with math and reading research papers, I'm a shit programmer. After grad school, I spent 2 years working for a no-name ML startup doing basic ML (mostly cleaning data, pipelines, feature engr experiments). I'm now a DS at FAANG-MULA for about a year. Opinions are my own, please feel free to disagree in the comments. \n\n===================== CONTENT ===================== \n\n0. If you can code, consider looking into positions as a software engr. They make more money and there are about 10x more jobs than data scientists. The interviews at the lower levels are basically optimizing code that you can cram for via leetcode.com.\n\n1. Look up leetcode for programming problems. You should be able to solve most of the easy ones in ~3 minutes (warm up) and discuss big O, etc. Medium ones in ~7 minutes.\n\n2. Know SQL (joins, aggregations, and window functions) down cold. Keep in mind that SQL/pipelines often power plots in dashboards. This means all the business logic/transformations are done in SQL and the dash just visualizes it. You should be able to take raw data and format it into common figures (line chart, bart chart, histogram, etc). The most annoying part, for me, was remembering the different date functions (e.g. convert XYZ date format to quarterly date for aggregation). These tend to vary among different SQL dialects. Good companies won't get that you get the exact syntax of the function right. Also, look up fct and dim tables. \n\nSide note: I hate subqueries and I **love** CTEs. The easier you make it for your interviewer to read what you are doing, the better.\n\n\n3. Youtube [lectures on ML](https://www.youtube.com/watch?v=MrLPzBxG95I&amp;list=PLl8OlHZGYOQ7bkVbuRthEsaLr7bONzbXS) I enjoyed. He also has course notes and what not somewhere on the internet. You may find other lecture series better and the curriculum is pretty standard at this level so don't feel attached to this one because I liked it.\n3.1 For DS roles that blend into MLE roles, you'll probably be asked to code some basic ML model. Linear regression, KNN, K-means, decision tree(s), etc. I've found engrs with more traditional CS backgrounds have some belief that their question digs at the heart of ML and that it's an effective screen. All will say that hiring is a noisy process. Maybe 1/3 will actually take steps to counter it. I've never seen anyone ask about SVMs though. I've even seen one company that asked people to code a Markov Chain in the 45 minute interview section. You'll almost certainly be asked how to make these methods scalable; you may or may not be asked to code the scalable method up in the short time frame. \n\n\n4. Some company tech blogs that could be useful:\n- Instacart, in particular this [one](https://tech.instacart.com/it-all-depends-4bb7b22e854b) is a very good discussion on how to do a proper test. You won't be expected to be a master statistician, but you need to be able to show that your model/decision is better than the prior setting.\n- The above blog referenced by Instacart is called a switchback experiment. DoorDash has some very detailed posts about it [[1]](https://doordash.engineering/2018/02/13/switchback-tests-and-randomized-experimentation-under-network-effects-at-doordash/), [[2]](https://doordash.engineering/2019/02/20/experiment-rigor-for-switchback-experiment-analysis/), [[3]](https://doordash.engineering/2019/09/11/cluster-robust-standard-error-in-switchback-experiments/). The details are not relevant for the interview, and I generally wouldn't expect a new DS to be familiar with this type of experiment in detail, but the general idea is worth digesting and it is interesting to see what a multi-year experimentation project could look like. Any company that has to deal with time AND location sensitive confounders will probably implement some version of this experiment.\n- Lyft is also very good. In particular, [this post](https://doordash.engineering/2019/09/11/cluster-robust-standard-error-in-switchback-experiments/) (which focuses more on software engineering, but still very relevant) will give you a lot of insight on the other side of the table and what the interviewer is looking for.\n- something to keep in mind in terms of having empathy for the hiring team: it likely costs ~1/2 million dollars/year to employ you. Your salary is ~200K. But once you factor in healthcare, payroll taxes, infrastructure (SV real estate ain't cheap), etc you've effectively doubled the cost to the company. That means you need to bring in ~1 million dollars/year in value. Also consider that new hires take 2-6 months to ramp, so that value delivery is backloaded. At the end of all your projects (and interview problems), you should be asking \"Have I delivered enough value to justify my disgusting compensation package?\"\n- Also consider this Lyft [post](https://eng.lyft.com/what-is-data-science-at-lyft-4101a69be028) (contrast the decisions vs. algorithms data scientists) and this Airbnb [post](https://medium.com/airbnb-engineering/at-airbnb-data-science-belongs-everywhere-917250c6beba) to see how data science often fits into the bigger picture. This airbnb [post](https://www.linkedin.com/pulse/one-data-science-job-doesnt-fit-all-elena-grewal/) also talks about the different DS tracks.\n- This [post](https://doordash.engineering/2021/01/14/data-scientists-technical-skill-business-impact/) from DoorDash talks a little bit about their interviews and wanted business/communication sense. It is worth looking into combining [MECE](https://en.wikipedia.org/wiki/MECE_principle) and [funnel analysis](https://clevertap.com/blog/funnel-analysis/) to really structure your thoughts. Again, the point of interviews is not to answer the question, it is to show you approach the problem in a systematic way. If you can combine the two principles above, you can realistically list \"all\" the possible solutions. After that, the question is just how to prioritize which likely areas to investigate.\n- DoorDash has a pretty heavy duty engr focus interview [prep post](https://doordash.engineering/2021/04/12/technical-interview-preparation/), that likely isn't relevant to people pursuing a DS role but would be fair game for people looking to be an ML engr.\n- Last point about the tracks, consider this [post](https://medium.com/airbnb-engineering/designing-experimentation-guardrails-ed6a976ec669) on metrics at Airbnb. It's a pretty stats heavy subject (even if the post is not super deep) - look at the author. She was a professor in statistics prior to Airbnb. Keep in mind what the competition looks like. It is worth noting my information applies to all the tracks. Some tracks may not ask you certain types of problems. For example, there may be tons of product/statistics types DS positions that would never ask you to write engr quality code. \n- Another point about companies. It is worth realizing that many of the companies in tech (and the ones in this section) are marketplace companies.That means they create value by connecting buyers &lt;=&gt; sellers (and maybe shoppers and/or advertisers). That means these marketplace all deal with the same kinds of problems on both the business and technical side. An example of a market place [post](https://eng.lyft.com/dynamic-pricing-to-sustain-marketplace-balance-1d23a8d1be90) from Lyft.\n- I really enjoyed the book [Lean Analytics](https://www.amazon.com/Lean-Analytics-Better-Startup-Faster/dp/1449335675) for a comparison of different tech company types and the metrics they should care about. And it has a good discussion about metrics in general. You should be able to find a pdf copy on library genesis. \n\n\n5. Taking all of the above, you really should expect a few types of product questions in your interview loop:\n\n(a) Metric XX is going down. How would you investigate it? I always think about these problems from MECE + funnel analysis perspective as noted above.\n\n(b) After expt AA, metric XX is going up but metric YY is going down. How would you think about it? This is a common problem where you're trying to understand tradeoffs/ambiguity and communication with managers/top line goals. If you EVER find yourself saying something definitive to this kind of problem, you're doing something wrong. Look up [Pareto Frontier](https://medium.com/civis-analytics/the-best-mario-kart-character-according-to-data-science-7dfb65d4c18e), but don't force it in.\n\n(c) Team XX wants to implement some solution to solve this issue (identify XX type of customer, roll out new product, etc), how would you go about it? This is an ML problem in disguise. That being said, the first question is always in the business context - how will the business use this information to make money/reduce costs? How will you know you are successful? Then you talk about how you would frame the problem and make it tractable for ML (regression/classification? What is a label? what are you optimizing the model to predict?). What features do you think would be predictive/would use in the model?  Where would you get the labels to train a model? How would you train the model/set up the cross validation [a]? How would you interpret the results of the model; e.g. for a classification model, interpret the confusion matrix - with an emphasis on biasing false positives and false negatives. It's very easy to have tons of technical side-bars here (how would you control for overfitting? How does a linear model differ from a tree based model? how to handle outliers + imbalanced data set? how to deal with a small data set?) [b]. \n\n\nAt the lower levels, the focus on these interview problems are typically very technical. As you get more experienced/start applying for more senior roles, you'll be asked more questions around project management. How will you integrate with XYZ services? How will you set up a project roadmap that ensures a steady drip of deliverables over{review_cycle_length}? How can you design a risk ladder so that if the super-awesome deep learning project doesn't work out, you can still deliver something of value (simpler/narrower scooped model or analytic insights)? \n\n\n[a] please think very carefully before you blurt out 80%train/10%validation/10% eval or whatever ratio - there is almost always some kind of leakage between the sets that means you have to think about it. For example, if you're predicting time series data, you don't want to train on 2018 and 2020 data and then predict on 2019 data.[b] for whatever reason, these interview problems are always binary classification problems. But not always. \n(d) How do you measure the effectiveness of XX (maybe test the effectiveness of the ML solution in (c))? AKA how do you run an AB test? Can you turn the problem into a testable hypothesis? How do you structure the experiment? What metric will you test on [c]? What unit would you test on (session_id vs user_id vs. account_id? e.g In the switchback expt above, you randomize on spatial-temporal units). Who is the defined population? How do you do a power analysis to calculate the needed sample size? --&gt; if you can get the sample size in 30 minutes, how long should you actually run the experiment? How do you calculate if this feature is worth shipping/what is the worthwhile minimum detectable effect (I typically compare the number of engr hours to complete to expected lift in dollars)? \nFor more junior positions, the focus of these questions are always focused around an AB test. Switchback experiments (for marketplace companies) and network effects (for social media companies) are table stakes because these problems are so core to the product. Pseudo experiments (difference-in-difference, propensity matching, etc) are typically not expected for new hires/generalist roles. \n\nFor both all of the above, it's typically fair game for the interviewer to ask you to explain some technical concept (ROC curve, p-value) as if you were talking to a non-technical audience member (e.g. a product manager). It is also completely reasonable (and should be mandatory IMO) to ask you for a decision/recommendation of some kind. I believe your job is to effect change and make recommendations that are backed up by data; no [two-handed economists](https://www.goodreads.com/quotes/7887683-give-me-a-one-handed-economist-all-my-economists-say-on). See my bullet above about justifying your paycheck. \n\n[c] I thought the lean analytics book I recommended above has a lot of good discussion on metrics. A short disc can be found on this Airbnb [post](https://medium.com/airbnb-engineering/from-data-to-action-with-airbnb-plus-54f4d8f80361) written by an intern. \n\n6. I forgot one more! A friend of mine wrote a few articles on interviews. I generally agree.\n[Coding points](https://towardsdatascience.com/the-ultimate-guide-to-acing-coding-interviews-for-data-scientists-d45c99d6bddc) and [business points](https://towardsdatascience.com/the-ultimate-guide-to-cracking-business-case-interviews-for-data-scientists-part-1-cb768c37edf4). In particular, Emma [wrote about her experience](https://towardsdatascience.com/how-i-got-4-data-science-offers-and-doubled-my-income-2-months-after-being-laid-off-b3b6d2de6938#64a7) getting a job. Of relevance, see section 2 of her post where she talks about figuring out which data science jobs are relevant to her, given her skillset.\n\n\n7. https://github.com/eugeneyan/applied-ml You may find some of his links interesting. I would avoid anything that refers to scaling up a platform as these are more backend engr focus. The more relevant posts to you are probably on the scale of blog posts that are product oriented like the ones I listed in section 4 (e.g. we wanted to solve X for our users and this is how we scoped and defined it). The technical aspects should come backseat to the business aspects. There's def a lot of companies/blog posts that he missed, but the internet is huge.\n\n\n8. Random note: Always keep in mind the STAR method for communication. Situation (context), task, action, result (impact). It is really helpful in the soft skills questions (tell me about a time you had conflict/tight deadline/unclear requirements/etc). I've found lots of academics struggle with contextualizing their work in a quick manner (the details of your 2nd order perturbation term or the type of spectrometer are often irrelevant). I think everyone struggles with articulating their impact. Focusing too much on tasks/action sections just reads like a to-do list. Situation tells us why that task/action was important/difficult and the result tells us why you're awesome and justified that paycheck.\n\n\n9. Resumes still matter. Yes, you may be able to get an interview via a friend connection (and you should!) but that referral won't carry you all the way through the interview process. Most companies actively avoid discussing your application/performance among other interviewers to avoid bias (at large companies, this is explicit due to legal reasons). This means for every other person on the interview loop, all they will know about you is a pdf copy of your resume - only one first impression, yeah? Additionally, some companies have the interview panel (people who interview you) and the hiring committee (people who actually make the decision) as separate groups. All the hiring committee gets is your resume and interview feedback; the only direct voice you have in that disc is your resume. This is where they will make a hiring decision and potentially, what teams/groups you'd be a fit for and what level/compensation you will get.", "link": "https://www.reddit.com/r/datascience/comments/p35hhh/my_information_dump_for_people_trying_to_break/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "my information dump for people trying to break into data science/interview notes /!/ **tlrd**: people in my personal life have asked for insight on breaking into the data science field/the interview loop. the following is a poorly formatted/continually updated list of my thoughts that i continually send out to people who've asked for them. i've decided to share it with the wider community. apologizes for the poor formatting, i [did not have the time to get the markup pretty](https://www.goodreads.com/quotes/21422-i-didn-t-have-time-to-write-a-short-letter-so).\n\n\n**audience**: people who are trying to break into data science and need help with the interview/job search. early-mid career people might find some nuggets useful. \n\n\n**about me**: did my phd doing experimental stuff with semiconductors. i'm comfortable with math and reading research papers, i'm a shit programmer. after grad school, i spent 2 years working for a no-name ml startup doing basic ml (mostly cleaning data, pipelines, feature engr experiments). i'm now a ds at faang-mula for about a year. opinions are my own, please feel free to disagree in the comments. \n\n===================== content ===================== \n\n0. if you can code, consider looking into positions as a software engr. they make more money and there are about 10x more jobs than data scientists. the interviews at the lower levels are basically optimizing code that you can cram for via leetcode.com.\n\n1. look up leetcode for programming problems. you should be able to solve most of the easy ones in ~3 minutes (warm up) and discuss big o, etc. medium ones in ~7 minutes.\n\n2. know sql (joins, aggregations, and window functions) down cold. keep in mind that sql/pipelines often power plots in dashboards. this means all the business logic/transformations are done in sql and the dash just visualizes it. you should be able to take raw data and format it into common figures (line chart, bart chart, histogram, etc). the most annoying part, for me, was remembering the different date functions (e.g. convert xyz date format to quarterly date for aggregation). these tend to vary among different sql dialects. good companies won't get that you get the exact syntax of the function right. also, look up fct and dim tables. \n\nside note: i hate subqueries and i **love** ctes. the easier you make it for your interviewer to read what you are doing, the better.\n\n\n3. youtube [lectures on ml](https://www.youtube.com/watch?v=mrlpzbxg95i&amp;list=pll8olhzgyoq7bkvburthesalr7bonzbxs) i enjoyed. he also has course notes and what not somewhere on the internet. you may find other lecture series better and the curriculum is pretty standard at this level so don't feel attached to this one because i liked it.\n3.1 for ds roles that blend into mle roles, you'll probably be asked to code some basic ml model. linear regression, knn, k-means, decision tree(s), etc. i've found engrs with more traditional cs backgrounds have some belief that their question digs at the heart of ml and that it's an effective screen. all will say that hiring is a noisy process. maybe 1/3 will actually take steps to counter it. i've never seen anyone ask about svms though. i've even seen one company that asked people to code a markov chain in the 45 minute interview section. you'll almost certainly be asked how to make these methods scalable; you may or may not be asked to code the scalable method up in the short time frame. \n\n\n4. some company tech blogs that could be useful:\n- instacart, in particular this [one](https://tech.instacart.com/it-all-depends-4bb7b22e854b) is a very good discussion on how to do a proper test. you won't be expected to be a master statistician, but you need to be able to show that your model/decision is better than the prior setting.\n- the above blog referenced by instacart is called a switchback experiment. doordash has some very detailed posts about it [[1]](https://doordash.engineering/2018/02/13/switchback-tests-and-randomized-experimentation-under-network-effects-at-doordash/), [[2]](https://doordash.engineering/2019/02/20/experiment-rigor-for-switchback-experiment-analysis/), [[3]](https://doordash.engineering/2019/09/11/cluster-robust-standard-error-in-switchback-experiments/). the details are not relevant for the interview, and i generally wouldn't expect a new ds to be familiar with this type of experiment in detail, but the general idea is worth digesting and it is interesting to see what a multi-year experimentation project could look like. any company that has to deal with time and location sensitive confounders will probably implement some version of this experiment.\n- lyft is also very good. in particular, [this post](https://doordash.engineering/2019/09/11/cluster-robust-standard-error-in-switchback-experiments/) (which focuses more on software engineering, but still very relevant) will give you a lot of insight on the other side of the table and what the interviewer is looking for.\n- something to keep in mind in terms of having empathy for the hiring team: it likely costs ~1/2 million dollars/year to employ you. your salary is ~200k. but once you factor in healthcare, payroll taxes, infrastructure (sv real estate ain't cheap), etc you've effectively doubled the cost to the company. that means you need to bring in ~1 million dollars/year in value. also consider that new hires take 2-6 months to ramp, so that value delivery is backloaded. at the end of all your projects (and interview problems), you should be asking \"have i delivered enough value to justify my disgusting compensation package?\"\n- also consider this lyft [post](https://eng.lyft.com/what-is-data-science-at-lyft-4101a69be028) (contrast the decisions vs. algorithms data scientists) and this airbnb [post](https://medium.com/airbnb-engineering/at-airbnb-data-science-belongs-everywhere-917250c6beba) to see how data science often fits into the bigger picture. this airbnb [post](https://www.linkedin.com/pulse/one-data-science-job-doesnt-fit-all-elena-grewal/) also talks about the different ds tracks.\n- this [post](https://doordash.engineering/2021/01/14/data-scientists-technical-skill-business-impact/) from doordash talks a little bit about their interviews and wanted business/communication sense. it is worth looking into combining [mece](https://en.wikipedia.org/wiki/mece_principle) and [funnel analysis](https://clevertap.com/blog/funnel-analysis/) to really structure your thoughts. again, the point of interviews is not to answer the question, it is to show you approach the problem in a systematic way. if you can combine the two principles above, you can realistically list \"all\" the possible solutions. after that, the question is just how to prioritize which likely areas to investigate.\n- doordash has a pretty heavy duty engr focus interview [prep post](https://doordash.engineering/2021/04/12/technical-interview-preparation/), that likely isn't relevant to people pursuing a ds role but would be fair game for people looking to be an ml engr.\n- last point about the tracks, consider this [post](https://medium.com/airbnb-engineering/designing-experimentation-guardrails-ed6a976ec669) on metrics at airbnb. it's a pretty stats heavy subject (even if the post is not super deep) - look at the author. she was a professor in statistics prior to airbnb. keep in mind what the competition looks like. it is worth noting my information applies to all the tracks. some tracks may not ask you certain types of problems. for example, there may be tons of product/statistics types ds positions that would never ask you to write engr quality code. \n- another point about companies. it is worth realizing that many of the companies in tech (and the ones in this section) are marketplace companies.that means they create value by connecting buyers &lt;=&gt; sellers (and maybe shoppers and/or advertisers). that means these marketplace all deal with the same kinds of problems on both the business and technical side. an example of a market place [post](https://eng.lyft.com/dynamic-pricing-to-sustain-marketplace-balance-1d23a8d1be90) from lyft.\n- i really enjoyed the book [lean analytics](https://www.amazon.com/lean-analytics-better-startup-faster/dp/1449335675) for a comparison of different tech company types and the metrics they should care about. and it has a good discussion about metrics in general. you should be able to find a pdf copy on library genesis. \n\n\n5. taking all of the above, you really should expect a few types of product questions in your interview loop:\n\n(a) metric xx is going down. how would you investigate it? i always think about these problems from mece + funnel analysis perspective as noted above.\n\n(b) after expt aa, metric xx is going up but metric yy is going down. how would you think about it? this is a common problem where you're trying to understand tradeoffs/ambiguity and communication with managers/top line goals. if you ever find yourself saying something definitive to this kind of problem, you're doing something wrong. look up [pareto frontier](https://medium.com/civis-analytics/the-best-mario-kart-character-according-to-data-science-7dfb65d4c18e), but don't force it in.\n\n(c) team xx wants to implement some solution to solve this issue (identify xx type of customer, roll out new product, etc), how would you go about it? this is an ml problem in disguise. that being said, the first question is always in the business context - how will the business use this information to make money/reduce costs? how will you know you are successful? then you talk about how you would frame the problem and make it tractable for ml (regression/classification? what is a label? what are you optimizing the model to predict?). what features do you think would be predictive/would use in the model?  where would you get the labels to train a model? how would you train the model/set up the cross validation [a]? how would you interpret the results of the model; e.g. for a classification model, interpret the confusion matrix - with an emphasis on biasing false positives and false negatives. it's very easy to have tons of technical side-bars here (how would you control for overfitting? how does a linear model differ from a -----> tree !!!  based model? how to handle outliers + imbalanced data set? how to deal with a small data set?) [b]. \n\n\nat the lower levels, the focus on these interview problems are typically very technical. as you get more experienced/start applying for more senior roles, you'll be asked more questions around project management. how will you integrate with xyz services? how will you set up a project roadmap that ensures a steady drip of deliverables over{review_cycle_length}? how can you design a risk ladder so that if the super-awesome deep learning project doesn't work out, you can still deliver something of value (simpler/narrower scooped model or analytic insights)? \n\n\n[a] please think very carefully before you blurt out 80%train/10%validation/10% eval or whatever ratio - there is almost always some kind of leakage between the sets that means you have to think about it. for example, if you're predicting time series data, you don't want to train on 2018 and 2020 data and then predict on 2019 data.[b] for whatever reason, these interview problems are always binary classification problems. but not always. \n(d) how do you measure the effectiveness of xx (maybe test the effectiveness of the ml solution in (c))? aka how do you run an ab test? can you turn the problem into a testable hypothesis? how do you structure the experiment? what metric will you test on [c]? what unit would you test on (session_id vs user_id vs. account_id? e.g in the switchback expt above, you randomize on spatial-temporal units). who is the defined population? how do you do a power analysis to calculate the needed sample size? --&gt; if you can get the sample size in 30 minutes, how long should you actually run the experiment? how do you calculate if this feature is worth shipping/what is the worthwhile minimum detectable effect (i typically compare the number of engr hours to complete to expected lift in dollars)? \nfor more junior positions, the focus of these questions are always focused around an ab test. switchback experiments (for marketplace companies) and network effects (for social media companies) are table stakes because these problems are so core to the product. pseudo experiments (difference-in-difference, propensity matching, etc) are typically not expected for new hires/generalist roles. \n\nfor both all of the above, it's typically fair game for the interviewer to ask you to explain some technical concept (roc curve, p-value) as if you were talking to a non-technical audience member (e.g. a product manager). it is also completely reasonable (and should be mandatory imo) to ask you for a decision/recommendation of some kind. i believe your job is to effect change and make recommendations that are backed up by data; no [two-handed economists](https://www.goodreads.com/quotes/7887683-give-me-a-one-handed-economist-all-my-economists-say-on). see my bullet above about justifying your paycheck. \n\n[c] i thought the lean analytics book i recommended above has a lot of good discussion on metrics. a short disc can be found on this airbnb [post](https://medium.com/airbnb-engineering/from-data-to-action-with-airbnb-plus-54f4d8f80361) written by an intern. \n\n6. i forgot one more! a friend of mine wrote a few articles on interviews. i generally agree.\n[coding points](https://towardsdatascience.com/the-ultimate-guide-to-acing-coding-interviews-for-data-scientists-d45c99d6bddc) and [business points](https://towardsdatascience.com/the-ultimate-guide-to-cracking-business-case-interviews-for-data-scientists-part-1-cb768c37edf4). in particular, emma [wrote about her experience](https://towardsdatascience.com/how-i-got-4-data-science-offers-and-doubled-my-income-2-months-after-being-laid-off-b3b6d2de6938#64a7) getting a job. of relevance, see section 2 of her post where she talks about figuring out which data science jobs are relevant to her, given her skillset.\n\n\n7. https://github.com/eugeneyan/applied-ml you may find some of his links interesting. i would avoid anything that refers to scaling up a platform as these are more backend engr focus. the more relevant posts to you are probably on the scale of blog posts that are product oriented like the ones i listed in section 4 (e.g. we wanted to solve x for our users and this is how we scoped and defined it). the technical aspects should come backseat to the business aspects. there's def a lot of companies/blog posts that he missed, but the internet is huge.\n\n\n8. random note: always keep in mind the star method for communication. situation (context), task, action, result (impact). it is really helpful in the soft skills questions (tell me about a time you had conflict/tight deadline/unclear requirements/etc). i've found lots of academics struggle with contextualizing their work in a quick manner (the details of your 2nd order perturbation term or the type of spectrometer are often irrelevant). i think everyone struggles with articulating their impact. focusing too much on tasks/action sections just reads like a to-do list. situation tells us why that task/action was important/difficult and the result tells us why you're awesome and justified that paycheck.\n\n\n9. resumes still matter. yes, you may be able to get an interview via a friend connection (and you should!) but that referral won't carry you all the way through the interview process. most companies actively avoid discussing your application/performance among other interviewers to avoid bias (at large companies, this is explicit due to legal reasons). this means for every other person on the interview loop, all they will know about you is a pdf copy of your resume - only one first impression, yeah? additionally, some companies have the interview panel (people who interview you) and the hiring committee (people who actually make the decision) as separate groups. all the hiring committee gets is your resume and interview feedback; the only direct voice you have in that disc is your resume. this is where they will make a hiring decision and potentially, what teams/groups you'd be a fit for and what level/compensation you will get.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/p35hhh/my_information_dump_for_people_trying_to_break/',)", "identifyer": 5591754, "year": "2021"}, {"autor": "7Seas_ofRyhme", "date": 1632066958000, "content": "Examples of Tree based machine learning algorithms? /!/ Aside from Decision Tree and Random Forest, what are the other options I can consider to make a comparison ?", "link": "https://www.reddit.com/r/datascience/comments/prakeb/examples_of_tree_based_machine_learning_algorithms/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "examples of -----> tree !!!  based machine learning algorithms? /!/ aside from decision tree and random forest, what are the other options i can consider to make a comparison ?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 5, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/prakeb/examples_of_tree_based_machine_learning_algorithms/',)", "identifyer": 5592017, "year": "2021"}, {"autor": "ottawalanguages", "date": 1620111263000, "content": "Inevitable Manual Work Required in Data Science Projects /!/ I have feeling that not many people are willing to admit - but ultimately, is a significant part of many data mining projects (e.g. checking data quality, parsing through data, etc.) still done manually? \n\nFor example here is an example I just made up relating to Supervised NLP (Natural Language Processing) Classification : Suppose I have 1000 medical reports of patients, containing unstructured text made by a doctor during a hospital visit. For a given patient, each report contains all the text notes that the doctor made for that patient, for visits between 2010 and 2020. These reports make mention of the patients bio data (e.g. age, gender, medical history, etc.) and the details of the symptoms that the patient is experiencing over a long period of time (e.g. let's say that these reports are 2000 words on average). The problem is, different doctors have different styles of writing - each of these 1000 reports is different from another. If a human were to read the report, the human could figure out what happened to the patient - did the patient have a serious condition (let's call this \"class 1\") or a non-serious condition (let's call this \"class 0\"). This is what we are interested in predicting for future patients based on the limited medical notes made by doctors for these future patients. \n\nThe problem is - there is no clear and fast way (not that I know of) to take the 1000 medical reports that are available, and label each report as \"class 1\" or \"class 0\". For example, for \"class 0\" : one of the doctors could clearly write at the end of a report \"all medical tests were conducted and the results and were all negative\", and another doctor could end the report by saying \"the patient should seriously consider changing their lifestyle and eat healthier food. benign.\" . \n\nIn this example, how would someone assign labels to all these 1000 cases, without manually reading them and deciding if the information in the report corresponds to a \"serious condition\" or a \"non-serious condition\"? I was thinking of using something like \"sentiment analysis\" to capture the \"mood\" of these reports, and use sentiment analysis a method to informally gauge if the tone of the report is \"dark\" (serious condition) or \"light\" (non serious condition). But I am not sure if this is the best way to approach this problem. Is there a way to do this without reading all the reports and manually deciding labels?\n\nIn the end - this is what I am interested in doing : suppose a new patient comes in and on the first visit, the doctor makes some quick notes (e.g. patient is male, 30 years old, 180 cm, 100 kg, non-smoker, frequently complains of chest pains, no high blood pressure, works a construction worker and takes daily medicine for acid reflex). Just based on these quick notes and the 1000 reports available (NOTE: I am trying to illustrate a point here, that the medical notes for the new patient and the 1000 reports DO NOT have the same format), can a researcher predict (supervised classification, e.g. decision tree) if this patient will have a \"serious\" or a \"non-serious\" condition?\n\nPS: suppose the doctors have a very detailed medical encyclopedia on their computers - can this medical encyclopedia be used alongside the 1000 medical reports to improve the prediction results?", "link": "https://www.reddit.com/r/datascience/comments/n4i0e9/inevitable_manual_work_required_in_data_science/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inevitable manual work required in data science projects /!/ i have feeling that not many people are willing to admit - but ultimately, is a significant part of many data mining projects (e.g. checking data quality, parsing through data, etc.) still done manually? \n\nfor example here is an example i just made up relating to supervised nlp (natural language processing) classification : suppose i have 1000 medical reports of patients, containing unstructured text made by a doctor during a hospital visit. for a given patient, each report contains all the text notes that the doctor made for that patient, for visits between 2010 and 2020. these reports make mention of the patients bio data (e.g. age, gender, medical history, etc.) and the details of the symptoms that the patient is experiencing over a long period of time (e.g. let's say that these reports are 2000 words on average). the problem is, different doctors have different styles of writing - each of these 1000 reports is different from another. if a human were to read the report, the human could figure out what happened to the patient - did the patient have a serious condition (let's call this \"class 1\") or a non-serious condition (let's call this \"class 0\"). this is what we are interested in predicting for future patients based on the limited medical notes made by doctors for these future patients. \n\nthe problem is - there is no clear and fast way (not that i know of) to take the 1000 medical reports that are available, and label each report as \"class 1\" or \"class 0\". for example, for \"class 0\" : one of the doctors could clearly write at the end of a report \"all medical tests were conducted and the results and were all negative\", and another doctor could end the report by saying \"the patient should seriously consider changing their lifestyle and eat healthier food. benign.\" . \n\nin this example, how would someone assign labels to all these 1000 cases, without manually reading them and deciding if the information in the report corresponds to a \"serious condition\" or a \"non-serious condition\"? i was thinking of using something like \"sentiment analysis\" to capture the \"mood\" of these reports, and use sentiment analysis a method to informally gauge if the tone of the report is \"dark\" (serious condition) or \"light\" (non serious condition). but i am not sure if this is the best way to approach this problem. is there a way to do this without reading all the reports and manually deciding labels?\n\nin the end - this is what i am interested in doing : suppose a new patient comes in and on the first visit, the doctor makes some quick notes (e.g. patient is male, 30 years old, 180 cm, 100 kg, non-smoker, frequently complains of chest pains, no high blood pressure, works a construction worker and takes daily medicine for acid reflex). just based on these quick notes and the 1000 reports available (note: i am trying to illustrate a point here, that the medical notes for the new patient and the 1000 reports do not have the same format), can a researcher predict (supervised classification, e.g. decision -----> tree !!! ) if this patient will have a \"serious\" or a \"non-serious\" condition?\n\nps: suppose the doctors have a very detailed medical encyclopedia on their computers - can this medical encyclopedia be used alongside the 1000 medical reports to improve the prediction results?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/n4i0e9/inevitable_manual_work_required_in_data_science/',)", "identifyer": 5592466, "year": "2021"}, {"autor": "Professional_Crazy49", "date": 1614104086000, "content": "My first technical interview experience(22+ interview questions) /!/ Today, I had a 45mins technical interview with a media based company and I thought I'd share the questions with you all since so many people on this subreddit are looking for jobs. I hope it helps someone! :)\n\n**Background:**\n\nI currently work as a DS and I have 1.5 years of work ex in the data and analytics field. I was initially hired as a DA so my interview was based on SQL which was quite easy (i'm a CS undergrad). I later got promoted to a DS position so I hadn't faced any serious technical DS interviews until today.\n\n**Technical Questions asked:**\n\n1. How would you go about predicting hotel prices for a company like [Booking.com](https://Booking.com)? - I previously worked at a similar company as a business analyst and hence the question. I was able to answer this based on the work I had done there.\n2. Let's say you have a categorical column with 500 categories. How would you tackle this? - I answered that we can use Catboost as it uses the catboost target encoder which would help convert the categorical values into numerical values rather than going for one hot encoding. He then mentioned that he wants to use linear regression so I said that we can use target encoding methods like James Stein encoder or Catboost encoder(preferred as it tackles target leakage). Was my answer right or is there some other way because he didn't seem 100% convinced with it?\n3. How would you check the weight of each feature in a decision tree? - I said that we can look at the feature importance of each feature. He then asked if a feature importance of 100 means the feature's influence on the target is 100? To which I replied that you can see the SHAP values to understand the influence of a feature on the target but honestly I haven't researched enough on it to comment further.\n4. Can I use K Means with categorical data? - You can use one hot encoding to convert categorical data to numerical but using K Means with Euclidian distance on binary columns does not make sense so I would use K Modes rather than K Means for categorical data\n5. How do I choose the number of clusters for K Means? - use elbow method or silhouette score and I explained both the methods\n6. Let's say I use silhouette analysis on a customer segmentation exercise and get K=30 as optimal number of clusters. I can't show 30 clusters to the business so what do I do now? - I said that generally for customer segmentation we would need business input as well so what is a practical number of segments according to the business? He replied 5-10 so I said that well out of the 5-10 clusters whichever has the highest silhouette score should be chosen. But I don't know if this is the right answer?\n7. Difference b/w K Means and K modes? - I just said that for categorical data we use K Modes because finding the mode of a particular category is more accurate and makes more sense rather than converting the category to binary values and using a distance algo like K Means.\n8. How would you perform customer segmentation on OTT platforms? - I panicked on this one honestly and said age, gender, nationality and probably genre of shows, do they watch shows completely, how long have they been a member on the OTT platform (Yes ik some of these don't make sense but like i said i PANCIKED)\n9. Do you think the above mentioned factors are a good representative of the customer lifetime value? - Uhh no idea what customer life time value means so I just winged this one\n10. Can you have more than one independent variable in ARIMA? - I answered yes cause I do vaguely remember coming across this but I am not 100% sure.\n11. What is the difference b/w ARIMA and ARIMAX? - ARIMAX is ARIMA but also has exogenous variables which help identify surges like holidays.\n12. Would you use ARIMA or Prophet for time series? - I read an article that says a properly tuned SARIMA would outperform Prophet so i answered the same\n13. How would you tune ARIMA? - by finding the best parameter values for p,d,q\n14. What are p,d,q in ARIMA? - (I forgot what they represent but I tried to answer from whatever I could recall ) p=no. of previous lags to consider, q= i forgot, d = difference(?)\n15. What exactly is \"d\"? - I said that it represents the seasonality pattern but I now realize that seasonality is in SARIMA and not ARIMA. (ugh)\n16. Can you pass non - stationary data to ARIMA? - No, because the assumption of TS is that data is stationary with constant mean and variance as it will assume the same patterns for future values as well\n17. How do we check if data is stationary? - By plotting it first but more accurate way is to use Dickey Fuller test to confirm it\n18. How do I choose which 10 new hotels to onboard on [Booking.com](https://Booking.com)? - I said that we can look at the number of bookings, location, accessibility( metro, bus), is it near a tourist spot, reviews, stars.\n19. What if my model has recommended that all the 10 new hotels that we should onboard should be from the same area X? How do I add a constraint to fix this? - I don't even know what topic this question is from but I said maybe you can modify the cost function by adding a variable which will penalize the cost function based on the number of hotels it suggests that belong to the same area or maybe we can add constraints to the cost function\n20. If I add constraints to the cost function then it becomes a non linear optimization problem so how would you use linear programming to solve it? - I had no idea lol\n21. What is the difference b/w segmentation and clustering? - I answered that segmentation is a use case of clustering but apparently the interviewer said that clustering is an unsupervised learning algorithm while segmentation is a supervised learning algorithm.\n22. Have you created a data pipeline before? - Nope\n\n&amp;#x200B;\n\n**Edit:**Thank you so much for the comments, upvotes and awards! I really appreciate the feedback as well! I am honestly relieved to hear that such interviews aren't the norm since it was really intense given I am not really that experienced.\n\nSince I got a few questions around the job requirements, I have put the technical requirements below but I did NOT have ALL of these so I really don't know on what basis they shortlisted my cv.\n\n\u00b7 Experience with Amazon Web Services Big data platform (ie. S3, RS)\n\n\u00b7 Solid experience with digital measurement and analytics platforms (ie. Google analytics, Big query, Return path data)\n\n\u00b7 Strong knowledge and experience in data modelling and wrangling techniques\n\n\u00b7 Strong knowledge and experience using Big Data programming languages (mainly R and Python)\n\n\u00b7 Strong knowledge of machine learning algorithms like Random Forrest, Decision trees, Matrix forecasting, Time series, Bayesian networks, Clustering, Regression, classification, and enable look\u2013a-like modelling, propensity to churn, propensity to buy, CLV, clustering, collaborative filtering, RFM, data fusion techniques, predictive modelling and audience profiling.\n\n\u00b7 Experienced in using SPARK, Pentaho, HIVE, SQL. FLUME, NoSQL, Javascript. Big query, Hadoop, Map reduce, HDFS, Hive, Pig, Lambda, Kinesis\n\n\u00b7 Knowledge and experience in Data Visualization", "link": "https://www.reddit.com/r/datascience/comments/lqozmp/my_first_technical_interview_experience22/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "my first technical interview experience(22+ interview questions) /!/ today, i had a 45mins technical interview with a media based company and i thought i'd share the questions with you all since so many people on this subreddit are looking for jobs. i hope it helps someone! :)\n\n**background:**\n\ni currently work as a ds and i have 1.5 years of work ex in the data and analytics field. i was initially hired as a da so my interview was based on sql which was quite easy (i'm a cs undergrad). i later got promoted to a ds position so i hadn't faced any serious technical ds interviews until today.\n\n**technical questions asked:**\n\n1. how would you go about predicting hotel prices for a company like [booking.com](https://booking.com)? - i previously worked at a similar company as a business analyst and hence the question. i was able to answer this based on the work i had done there.\n2. let's say you have a categorical column with 500 categories. how would you tackle this? - i answered that we can use catboost as it uses the catboost target encoder which would help convert the categorical values into numerical values rather than going for one hot encoding. he then mentioned that he wants to use linear regression so i said that we can use target encoding methods like james stein encoder or catboost encoder(preferred as it tackles target leakage). was my answer right or is there some other way because he didn't seem 100% convinced with it?\n3. how would you check the weight of each feature in a decision -----> tree !!! ? - i said that we can look at the feature importance of each feature. he then asked if a feature importance of 100 means the feature's influence on the target is 100? to which i replied that you can see the shap values to understand the influence of a feature on the target but honestly i haven't researched enough on it to comment further.\n4. can i use k means with categorical data? - you can use one hot encoding to convert categorical data to numerical but using k means with euclidian distance on binary columns does not make sense so i would use k modes rather than k means for categorical data\n5. how do i choose the number of clusters for k means? - use elbow method or silhouette score and i explained both the methods\n6. let's say i use silhouette analysis on a customer segmentation exercise and get k=30 as optimal number of clusters. i can't show 30 clusters to the business so what do i do now? - i said that generally for customer segmentation we would need business input as well so what is a practical number of segments according to the business? he replied 5-10 so i said that well out of the 5-10 clusters whichever has the highest silhouette score should be chosen. but i don't know if this is the right answer?\n7. difference b/w k means and k modes? - i just said that for categorical data we use k modes because finding the mode of a particular category is more accurate and makes more sense rather than converting the category to binary values and using a distance algo like k means.\n8. how would you perform customer segmentation on ott platforms? - i panicked on this one honestly and said age, gender, nationality and probably genre of shows, do they watch shows completely, how long have they been a member on the ott platform (yes ik some of these don't make sense but like i said i panciked)\n9. do you think the above mentioned factors are a good representative of the customer lifetime value? - uhh no idea what customer life time value means so i just winged this one\n10. can you have more than one independent variable in arima? - i answered yes cause i do vaguely remember coming across this but i am not 100% sure.\n11. what is the difference b/w arima and arimax? - arimax is arima but also has exogenous variables which help identify surges like holidays.\n12. would you use arima or prophet for time series? - i read an article that says a properly tuned sarima would outperform prophet so i answered the same\n13. how would you tune arima? - by finding the best parameter values for p,d,q\n14. what are p,d,q in arima? - (i forgot what they represent but i tried to answer from whatever i could recall ) p=no. of previous lags to consider, q= i forgot, d = difference(?)\n15. what exactly is \"d\"? - i said that it represents the seasonality pattern but i now realize that seasonality is in sarima and not arima. (ugh)\n16. can you pass non - stationary data to arima? - no, because the assumption of ts is that data is stationary with constant mean and variance as it will assume the same patterns for future values as well\n17. how do we check if data is stationary? - by plotting it first but more accurate way is to use dickey fuller test to confirm it\n18. how do i choose which 10 new hotels to onboard on [booking.com](https://booking.com)? - i said that we can look at the number of bookings, location, accessibility( metro, bus), is it near a tourist spot, reviews, stars.\n19. what if my model has recommended that all the 10 new hotels that we should onboard should be from the same area x? how do i add a constraint to fix this? - i don't even know what topic this question is from but i said maybe you can modify the cost function by adding a variable which will penalize the cost function based on the number of hotels it suggests that belong to the same area or maybe we can add constraints to the cost function\n20. if i add constraints to the cost function then it becomes a non linear optimization problem so how would you use linear programming to solve it? - i had no idea lol\n21. what is the difference b/w segmentation and clustering? - i answered that segmentation is a use case of clustering but apparently the interviewer said that clustering is an unsupervised learning algorithm while segmentation is a supervised learning algorithm.\n22. have you created a data pipeline before? - nope\n\n&amp;#x200b;\n\n**edit:**thank you so much for the comments, upvotes and awards! i really appreciate the feedback as well! i am honestly relieved to hear that such interviews aren't the norm since it was really intense given i am not really that experienced.\n\nsince i got a few questions around the job requirements, i have put the technical requirements below but i did not have all of these so i really don't know on what basis they shortlisted my cv.\n\n\u00b7 experience with amazon web services big data platform (ie. s3, rs)\n\n\u00b7 solid experience with digital measurement and analytics platforms (ie. google analytics, big query, return path data)\n\n\u00b7 strong knowledge and experience in data modelling and wrangling techniques\n\n\u00b7 strong knowledge and experience using big data programming languages (mainly r and python)\n\n\u00b7 strong knowledge of machine learning algorithms like random forrest, decision trees, matrix forecasting, time series, bayesian networks, clustering, regression, classification, and enable look\u2013a-like modelling, propensity to churn, propensity to buy, clv, clustering, collaborative filtering, rfm, data fusion techniques, predictive modelling and audience profiling.\n\n\u00b7 experienced in using spark, pentaho, hive, sql. flume, nosql, javascript. big query, hadoop, map reduce, hdfs, hive, pig, lambda, kinesis\n\n\u00b7 knowledge and experience in data visualization", "sortedWord": "None", "removed": "('nan',)", "score": 727, "comments": 244, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/lqozmp/my_first_technical_interview_experience22/',)", "identifyer": 5592585, "year": "2021"}, {"autor": "grid_world", "date": 1619545529000, "content": "Tree bark cross-section - CNN /!/ I have a use case where I am supposed to use different tree bark cross section images. An example image can be seen [here](https://treegrowthstructure.weebly.com/uploads/1/9/2/6/19264931/_8443258.jpeg).\n\nThe goal is to have a CNN model to recognize the tree bark and then predict the tree from to which it belongs to? Do you have any such dataset/similar problem into which I can look into?\n\n&amp;#x200B;\n\nThanks!", "link": "https://www.reddit.com/r/datascience/comments/mzu5ei/tree_bark_crosssection_cnn/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "-----> tree !!!  bark cross-section - cnn /!/ i have a use case where i am supposed to use different tree bark cross section images. an example image can be seen [here](https://treegrowthstructure.weebly.com/uploads/1/9/2/6/19264931/_8443258.jpeg).\n\nthe goal is to have a cnn model to recognize the tree bark and then predict the tree from to which it belongs to? do you have any such dataset/similar problem into which i can look into?\n\n&amp;#x200b;\n\nthanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/mzu5ei/tree_bark_crosssection_cnn/',)", "identifyer": 5592683, "year": "2021"}, {"autor": "CKL-IT", "date": 1620223481000, "content": "1 line to visualizations for dependency trees, entity relationships, resolution, assertion, NER and new models for Afrikaans, Welsh, Maltese, Tamil, and Vietnamese - John Snow Labs NLU 3.0.1 for Python /!/ # NLU 3.0.1 Release Notes\nWe are very excited to announce NLU 3.0.1 has been released!\nThis is one of the most visually appealing releases, with the integration of the [Spark-NLP-Display](https://nlp.johnsnowlabs.com/docs/en/display) library and visualizations for `dependency trees`, `entity resolution`, `entity assertion`, `relationship between entities` and `named\nentity recognition`. In addition to this, the schema of how columns are named by NLU has been reworked and all 140+ tutorial notebooks have been updated to reflect the latest changes in NLU 3.0.0+\nFinally, new multilingual models for `Afrikaans`, `Welsh`, `Maltese`, `Tamil`, and`Vietnamese` are now available.\n\n\n\n\n# New Features and Enhancements\n- 1 line to visualization for `NER`, `Dependency`, `Resolution`, `Assertion` and `Relation` via [Spark-NLP-Display](https://nlp.johnsnowlabs.com/docs/en/display) integration\n- Improved column naming schema\n- [Over 140 + NLU tutorial Notebooks updated](https://github.com/JohnSnowLabs/nlu/tree/master/examples) and improved to reflect latest changes in NLU 3.0.0 +\n- New multilingual models for `Afrikaans`, `Welsh`, `Maltese`, `Tamil`, and`Vietnamese`\n- Enhanced offline loading\n\n\n## NLU visualization\nThe latest NLU release integrated the beautiful Spark-NLP-Display package visualizations. You do not need to worry about installing it, when you try to visualize something, NLU will check if\nSpark-NLP-Display is installed, if it is missing it will be dynamically installed into your python executable environment, so you don't need to worry about anything!\n\nSee the [visualization tutorial notebook](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/visualization/NLU_visualizations_tutorial.ipynb)  and [visualization docs](https://nlu.johnsnowlabs.com/docs/en/viz_examples) for more info.\n\n![Cheat Sheet visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/cheat_sheet.png)\n\n## NER visualization\nApplicable to any of the [100+ NER models! See here for an overview](https://nlp.johnsnowlabs.com/models?task=Named+Entity+Recognition)\n```python\nnlu.load('ner').viz(\"Donald Trump from America and Angela Merkel from Germany don't share many oppinions.\")\n```\n![NER visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/NER.png)\n\n## Dependency tree visualization\nVisualizes the structure of the labeled dependency tree and part of speech tags\n```python\nnlu.load('dep.typed').viz(\"Billy went to the mall\")\n```\n\n![Dependency Tree visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/DEP.png)\n\n```python\n#Bigger Example\nnlu.load('dep.typed').viz(\"Donald Trump from America and Angela Merkel from Germany don't share many oppinions but they both love John Snow Labs software\")\n```\n![Dependency Tree visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/DEP_big.png)\n\n## Assertion status visualization\nVisualizes asserted statuses and entities.        \nApplicable to any of the [10 + Assertion models! See here for an overview](https://nlp.johnsnowlabs.com/models?task=Assertion+Status)\n```python\nnlu.load('med_ner.clinical assert').viz(\"The MRI scan showed no signs of cancer in the left lung\")\n```\n\n\n![Assert visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/assertion.png)\n\n```python\n#bigger example\ndata ='This is the case of a very pleasant 46-year-old Caucasian female, seen in clinic on 12/11/07 during which time MRI of the left shoulder showed no evidence of rotator cuff tear. She did have a previous MRI of the cervical spine that did show an osteophyte on the left C6-C7 level. Based on this, negative MRI of the shoulder, the patient was recommended to have anterior cervical discectomy with anterior interbody fusion at C6-C7 level. Operation, expected outcome, risks, and benefits were discussed with her. Risks include, but not exclusive of bleeding and infection, bleeding could be soft tissue bleeding, which may compromise airway and may result in return to the operating room emergently for evacuation of said hematoma. There is also the possibility of bleeding into the epidural space, which can compress the spinal cord and result in weakness and numbness of all four extremities as well as impairment of bowel and bladder function. However, the patient may develop deeper-seated infection, which may require return to the operating room. Should the infection be in the area of the spinal instrumentation, this will cause a dilemma since there might be a need to remove the spinal instrumentation and/or allograft. There is also the possibility of potential injury to the esophageus, the trachea, and the carotid artery. There is also the risks of stroke on the right cerebral circulation should an undiagnosed plaque be propelled from the right carotid. She understood all of these risks and agreed to have the procedure performed.'\nnlu.load('med_ner.clinical assert').viz(data)\n```\n![Assert visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/assertion_big.png)\n\n\n## Relationship between entities visualization\nVisualizes the extracted entities between relationship.    \nApplicable to any of the [20 + Relation Extractor models See here for an overview](https://nlp.johnsnowlabs.com/models?task=Relation+Extraction)\n```python\nnlu.load('med_ner.jsl.wip.clinical relation.temporal_events').viz('The patient developed cancer after a mercury poisoning in 1999 ')\n```\n![Entity Relation visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/relation.png)\n\n```python\n# bigger example\ndata = 'This is the case of a very pleasant 46-year-old Caucasian female, seen in clinic on 12/11/07 during which time MRI of the left shoulder showed no evidence of rotator cuff tear. She did have a previous MRI of the cervical spine that did show an osteophyte on the left C6-C7 level. Based on this, negative MRI of the shoulder, the patient was recommended to have anterior cervical discectomy with anterior interbody fusion at C6-C7 level. Operation, expected outcome, risks, and benefits were discussed with her. Risks include, but not exclusive of bleeding and infection, bleeding could be soft tissue bleeding, which may compromise airway and may result in return to the operating room emergently for evacuation of said hematoma. There is also the possibility of bleeding into the epidural space, which can compress the spinal cord and result in weakness and numbness of all four extremities as well as impairment of bowel and bladder function. However, the patient may develop deeper-seated infection, which may require return to the operating room. Should the infection be in the area of the spinal instrumentation, this will cause a dilemma since there might be a need to remove the spinal instrumentation and/or allograft. There is also the possibility of potential injury to the esophageus, the trachea, and the carotid artery. There is also the risks of stroke on the right cerebral circulation should an undiagnosed plaque be propelled from the right carotid. She understood all of these risks and agreed to have the procedure performed'\npipe = nlu.load('med_ner.jsl.wip.clinical relation.clinical').viz(data)\n```\n![Entity Relation visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/relation_big.png)\n\n\n## Entity Resolution visualization for chunks\nVisualizes resolutions of entities\nApplicable to any of the [100+ Resolver models See here for an overview](https://nlp.johnsnowlabs.com/models?task=Entity+Resolution)\n```python\nnlu.load('med_ner.jsl.wip.clinical resolve_chunk.rxnorm.in').viz(\"He took Prevacid 30 mg  daily\")\n```\n![Chunk Resolution visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/resolve_chunk.png)\n\n```python\n# bigger example\ndata = \"This is an 82 - year-old male with a history of prior tobacco use , hypertension , chronic renal insufficiency , COPD , gastritis , and TIA who initially presented to Braintree with a non-ST elevation MI and Guaiac positive stools , transferred to St . Margaret\\'s Center for Women &amp; Infants for cardiac catheterization with PTCA to mid LAD lesion complicated by hypotension and bradycardia requiring Atropine , IV fluids and transient dopamine possibly secondary to vagal reaction , subsequently transferred to CCU for close monitoring , hemodynamically stable at the time of admission to the CCU .\"\nnlu.load('med_ner.jsl.wip.clinical resolve_chunk.rxnorm.in').viz(data)\n```\n\n![Chunk Resolution visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/resolve_chunk_big.png)\n\n\n## Entity Resolution visualization for sentences\nVisualizes resolutions of entities in sentences\nApplicable to any of the [100+ Resolver models See here for an overview](https://nlp.johnsnowlabs.com/models?task=Entity+Resolution)\n```python\nnlu.load('med_ner.jsl.wip.clinical resolve.icd10cm').viz('She was diagnosed with a respiratory congestion')\n```\n![Sentence Resolution visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/resolve_sentence.png)\n\n```python\n# bigger example\ndata = 'The patient is a 5-month-old infant who presented initially on Monday with a cold, cough, and runny nose for 2 days. Mom states she had no fever. Her appetite was good but she was spitting up a lot. She had no difficulty breathing and her cough was described as dry and hacky. At that time, physical exam showed a right TM, which was red. Left TM was okay. She was fairly congested but looked happy and playful. She was started on Amoxil and Aldex and we told to recheck in 2 weeks to recheck her ear. Mom returned to clinic again today because she got much worse overnight. She was having difficulty breathing. She was much more congested and her appetite had decreased significantly today. She also spiked a temperature yesterday of 102.6 and always having trouble sleeping secondary to congestion'\nnlu.load('med_ner.jsl.wip.clinical resolve.icd10cm').viz(data)\n```\n![Sentence Resolution visualization](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/resolve_sentence_big.png)\n\n## Configure visualizations\n### Define custom colors for labels\nSome entity and relation labels will be highlighted with a pre-defined color, which you [can find here](https://github.com/JohnSnowLabs/spark-nlp-display/tree/main/sparknlp_display/label_colors).    \nFor labels that have no color defined, a random color will be generated.     \nYou can define colors for labels manually, by specifying via the `viz_colors` parameter\nand defining `hex color codes` in a dictionary that maps `labels` to `colors` .\n```python\ndata = 'Dr. John Snow suggested that Fritz takes 5mg penicilin for his cough'\n# Define custom colors for labels\nviz_colors={'STRENGTH':'#800080', 'DRUG_BRANDNAME':'#77b5fe', 'GENDER':'#77ffe'}\nnlu.load('med_ner.jsl.wip.clinical').viz(data,viz_colors =viz_colors)\n```\n![define colors labels](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/define_colors.png)\n\n\n### Filter entities that get highlighted\nBy default every entity class will be visualized.    \nThe `labels_to_viz` can be used to define a set of labels to highlight.       \nApplicable for ner, resolution and assert.\n```python\ndata = 'Dr. John Snow suggested that Fritz takes 5mg penicilin for his cough'\n# Filter wich NER label to viz\nlabels_to_viz=['SYMPTOM']\nnlu.load('med_ner.jsl.wip.clinical').viz(data,labels_to_viz=labels_to_viz)\n```\n![filter labels](https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/docs/assets/images/nlu/VizExamples/viz_module/filter_labels.png)\n\n\n## New models\nNew multilingual models for `Afrikaans`, `Welsh`, `Maltese`, `Tamil`, and`Vietnamese`\n\n| nlu.load() Refrence                                          | Spark NLP Refrence                                           |\n| ------------------------------------------------------------ | ------------------------------------------------------------ |\n| [vi.lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_vi.html) | [lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_vi.html) |\n| [mt.lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_mt.html) | [lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_mt.html) |\n| [ta.lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_ta.html) | [lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_ta.html) |\n| [af.lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_af.html) | [lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_af.html) |\n| [af.pos](https://nlp.johnsnowlabs.com/2021/04/06/pos_afribooms_af.html) | [pos_afribooms](https://nlp.johnsnowlabs.com/2021/04/06/pos_afribooms_af.html) |\n| [cy.lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_cy.html) | [lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_cy.html) |\n\n## Reworked and updated NLU tutorial notebooks\n\nAll of the [140+ NLU tutorial Notebooks](https://github.com/JohnSnowLabs/nlu/tree/master/examples) have been updated and reworked to reflect the latest changes in NLU 3.0.0+\n\n\n## Improved Column Name generation\n- NLU categorized each internal component now with boolean labels for `name_deductable` and `always_name_deductable` .\n- Before generating column names, NLU checks wether each component is of unique in the pipeline or not. If a component is not unique in the\n  pipe and there are multiple components of same type, i.e. multiple `NER` models, NLU will deduct a base name for the final output columns from the\n  NLU reference each NER model is pointing to.\n- If on the other hand, there is only one `NER` model in the pipeline, only the default `ner` column prefixed will be generated.\n- For some components, like `embeddings` and `classifiers` are now defined as `always_name_deductable`, for those NLU will always try to infer a meaningful base name for the output columns.\n- Newly trained component output columns will now be prefixed with `trained_&lt;type&gt;` , for types `pos` , `ner`, `cLassifier`, `sentiment` and `multi_classifier`\n\n## Enhanced offline mode\n- You can still load a model from a path as usual with `nlu.load(path=model_path)` and output columns will be suffixed with `from_disk`\n- You can now optionally also specify `request` parameter during  load a model from HDD, it will be used to deduct more meaningful column name suffixes, instead of `from_disk`, i.e. by calling `nlu.load(request ='en.embed_sentence.biobert.pubmed_pmc_base_cased', path=model_path)`\n\n\n### Bugfixes\n- Fixed a bug that caused  resolution algorithms output level to be inferred incorrectly\n- Fixed a bug that caused stranger cols got dropped\n- Fixed a bug that caused endings to miss when  .predict(position=True) was specified\n- Fixed a bug that caused pd.Series to be converted incorrectly internally\n- Fixed a bug that caused output level transformations to crash\n- Fixed a bug that caused verbose mode not to turn of properly after turning it on.\n- fixed a bug that caused some models to crash when loaded for HDD\n\n# Additional NLU resources\n* [140+ updates tutorials](https://github.com/JohnSnowLabs/nlu/tree/master/examples)\n* [Updated visualization docs](https://nlu.johnsnowlabs.com/docs/en/viz_examples)\n* [Models Hub](https://nlp.johnsnowlabs.com/models) with new models\n* [Spark NLP publications](https://medium.com/spark-nlp)\n* [NLU in Action](https://nlp.johnsnowlabs.com/demo)\n* [NLU documentation](https://nlu.johnsnowlabs.com/docs/en/install)\n* [Discussions](https://github.com/JohnSnowLabs/spark-nlp/discussions) Engage with other community members, share ideas, and show off how you use Spark NLP and NLU!\n\n# 1 line Install NLU on Google Colab\n```!wget https://setup.johnsnowlabs.com/nlu/colab.sh  -O - | bash```\n# 1 line Install NLU on Kaggle\n```!wget https://setup.johnsnowlabs.com/nlu/kaggle.sh  -O - | bash```\n# Install via PIP\n```! pip install nlu pyspark==3.0.1```", "link": "https://www.reddit.com/r/datascience/comments/n5goqe/1_line_to_visualizations_for_dependency_trees/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "1 line to visualizations for dependency trees, entity relationships, resolution, assertion, ner and new models for afrikaans, welsh, maltese, tamil, and vietnamese - john snow labs nlu 3.0.1 for python /!/ # nlu 3.0.1 release notes\nwe are very excited to announce nlu 3.0.1 has been released!\nthis is one of the most visually appealing releases, with the integration of the [spark-nlp-display](https://nlp.johnsnowlabs.com/docs/en/display) library and visualizations for `dependency trees`, `entity resolution`, `entity assertion`, `relationship between entities` and `named\nentity recognition`. in addition to this, the schema of how columns are named by nlu has been reworked and all 140+ tutorial notebooks have been updated to reflect the latest changes in nlu 3.0.0+\nfinally, new multilingual models for `afrikaans`, `welsh`, `maltese`, `tamil`, and`vietnamese` are now available.\n\n\n\n\n# new features and enhancements\n- 1 line to visualization for `ner`, `dependency`, `resolution`, `assertion` and `relation` via [spark-nlp-display](https://nlp.johnsnowlabs.com/docs/en/display) integration\n- improved column naming schema\n- [over 140 + nlu tutorial notebooks updated](https://github.com/johnsnowlabs/nlu/-----> tree !!! /master/examples) and improved to reflect latest changes in nlu 3.0.0 +\n- new multilingual models for `afrikaans`, `welsh`, `maltese`, `tamil`, and`vietnamese`\n- enhanced offline loading\n\n\n## nlu visualization\nthe latest nlu release integrated the beautiful spark-nlp-display package visualizations. you do not need to worry about installing it, when you try to visualize something, nlu will check if\nspark-nlp-display is installed, if it is missing it will be dynamically installed into your python executable environment, so you don't need to worry about anything!\n\nsee the [visualization tutorial notebook](https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/visualization/nlu_visualizations_tutorial.ipynb)  and [visualization docs](https://nlu.johnsnowlabs.com/docs/en/viz_examples) for more info.\n\n![cheat sheet visualization](https://raw.githubusercontent.com/johnsnowlabs/nlu/master/docs/assets/images/nlu/vizexamples/viz_module/cheat_sheet.png)\n\n## ner visualization\napplicable to any of the [100+ ner models! see here for an overview](https://nlp.johnsnowlabs.com/models?task=named+entity+recognition)\n```python\nnlu.load('ner').viz(\"donald trump from america and angela merkel from germany don't share many oppinions.\")\n```\n![ner visualization](https://raw.githubusercontent.com/johnsnowlabs/nlu/master/docs/assets/images/nlu/vizexamples/viz_module/ner.png)\n\n## dependency tree visualization\nvisualizes the structure of the labeled dependency tree and part of speech tags\n```python\nnlu.load('dep.typed').viz(\"billy went to the mall\")\n```\n\n![dependency tree visualization](https://raw.githubusercontent.com/johnsnowlabs/nlu/master/docs/assets/images/nlu/vizexamples/viz_module/dep.png)\n\n```python\n#bigger example\nnlu.load('dep.typed').viz(\"donald trump from america and angela merkel from germany don't share many oppinions but they both love john snow labs software\")\n```\n![dependency tree visualization](https://raw.githubusercontent.com/johnsnowlabs/nlu/master/docs/assets/images/nlu/vizexamples/viz_module/dep_big.png)\n\n## assertion status visualization\nvisualizes asserted statuses and entities.        \napplicable to any of the [10 + assertion models! see here for an overview](https://nlp.johnsnowlabs.com/models?task=assertion+status)\n```python\nnlu.load('med_ner.clinical assert').viz(\"the mri scan showed no signs of cancer in the left lung\")\n```\n\n\n![assert visualization](https://raw.githubusercontent.com/johnsnowlabs/nlu/master/docs/assets/images/nlu/vizexamples/viz_module/assertion.png)\n\n```python\n#bigger example\ndata ='this is the case of a very pleasant 46-year-old caucasian female, seen in clinic on 12/11/07 during which time mri of the left shoulder showed no evidence of rotator cuff tear. she did have a previous mri of the cervical spine that did show an osteophyte on the left c6-c7 level. based on this, negative mri of the shoulder, the patient was recommended to have anterior cervical discectomy with anterior interbody fusion at c6-c7 level. operation, expected outcome, risks, and benefits were discussed with her. risks include, but not exclusive of bleeding and infection, bleeding could be soft tissue bleeding, which may compromise airway and may result in return to the operating room emergently for evacuation of said hematoma. there is also the possibility of bleeding into the epidural space, which can compress the spinal cord and result in weakness and numbness of all four extremities as well as impairment of bowel and bladder function. however, the patient may develop deeper-seated infection, which may require return to the operating room. should the infection be in the area of the spinal instrumentation, this will cause a dilemma since there might be a need to remove the spinal instrumentation and/or allograft. there is also the possibility of potential injury to the esophageus, the trachea, and the carotid artery. there is also the risks of stroke on the right cerebral circulation should an undiagnosed plaque be propelled from the right carotid. she understood all of these risks and agreed to have the procedure performed.'\nnlu.load('med_ner.clinical assert').viz(data)\n```\n![assert visualization](https://raw.githubusercontent.com/johnsnowlabs/nlu/master/docs/assets/images/nlu/vizexamples/viz_module/assertion_big.png)\n\n\n## relationship between entities visualization\nvisualizes the extracted entities between relationship.    \napplicable to any of the [20 + relation extractor models see here for an overview](https://nlp.johnsnowlabs.com/models?task=relation+extraction)\n```python\nnlu.load('med_ner.jsl.wip.clinical relation.temporal_events').viz('the patient developed cancer after a mercury poisoning in 1999 ')\n```\n![entity relation visualization](https://raw.githubusercontent.com/johnsnowlabs/nlu/master/docs/assets/images/nlu/vizexamples/viz_module/relation.png)\n\n```python\n# bigger example\ndata = 'this is the case of a very pleasant 46-year-old caucasian female, seen in clinic on 12/11/07 during which time mri of the left shoulder showed no evidence of rotator cuff tear. she did have a previous mri of the cervical spine that did show an osteophyte on the left c6-c7 level. based on this, negative mri of the shoulder, the patient was recommended to have anterior cervical discectomy with anterior interbody fusion at c6-c7 level. operation, expected outcome, risks, and benefits were discussed with her. risks include, but not exclusive of bleeding and infection, bleeding could be soft tissue bleeding, which may compromise airway and may result in return to the operating room emergently for evacuation of said hematoma. there is also the possibility of bleeding into the epidural space, which can compress the spinal cord and result in weakness and numbness of all four extremities as well as impairment of bowel and bladder function. however, the patient may develop deeper-seated infection, which may require return to the operating room. should the infection be in the area of the spinal instrumentation, this will cause a dilemma since there might be a need to remove the spinal instrumentation and/or allograft. there is also the possibility of potential injury to the esophageus, the trachea, and the carotid artery. there is also the risks of stroke on the right cerebral circulation should an undiagnosed plaque be propelled from the right carotid. she understood all of these risks and agreed to have the procedure performed'\npipe = nlu.load('med_ner.jsl.wip.clinical relation.clinical').viz(data)\n```\n![entity relation visualization](https://raw.githubusercontent.com/johnsnowlabs/nlu/master/docs/assets/images/nlu/vizexamples/viz_module/relation_big.png)\n\n\n## entity resolution visualization for chunks\nvisualizes resolutions of entities\napplicable to any of the [100+ resolver models see here for an overview](https://nlp.johnsnowlabs.com/models?task=entity+resolution)\n```python\nnlu.load('med_ner.jsl.wip.clinical resolve_chunk.rxnorm.in').viz(\"he took prevacid 30 mg  daily\")\n```\n![chunk resolution visualization](https://raw.githubusercontent.com/johnsnowlabs/nlu/master/docs/assets/images/nlu/vizexamples/viz_module/resolve_chunk.png)\n\n```python\n# bigger example\ndata = \"this is an 82 - year-old male with a history of prior tobacco use , hypertension , chronic renal insufficiency , copd , gastritis , and tia who initially presented to braintree with a non-st elevation mi and guaiac positive stools , transferred to st . margaret\\'s center for women &amp; infants for cardiac catheterization with ptca to mid lad lesion complicated by hypotension and bradycardia requiring atropine , iv fluids and transient dopamine possibly secondary to vagal reaction , subsequently transferred to ccu for close monitoring , hemodynamically stable at the time of admission to the ccu .\"\nnlu.load('med_ner.jsl.wip.clinical resolve_chunk.rxnorm.in').viz(data)\n```\n\n![chunk resolution visualization](https://raw.githubusercontent.com/johnsnowlabs/nlu/master/docs/assets/images/nlu/vizexamples/viz_module/resolve_chunk_big.png)\n\n\n## entity resolution visualization for sentences\nvisualizes resolutions of entities in sentences\napplicable to any of the [100+ resolver models see here for an overview](https://nlp.johnsnowlabs.com/models?task=entity+resolution)\n```python\nnlu.load('med_ner.jsl.wip.clinical resolve.icd10cm').viz('she was diagnosed with a respiratory congestion')\n```\n![sentence resolution visualization](https://raw.githubusercontent.com/johnsnowlabs/nlu/master/docs/assets/images/nlu/vizexamples/viz_module/resolve_sentence.png)\n\n```python\n# bigger example\ndata = 'the patient is a 5-month-old infant who presented initially on monday with a cold, cough, and runny nose for 2 days. mom states she had no fever. her appetite was good but she was spitting up a lot. she had no difficulty breathing and her cough was described as dry and hacky. at that time, physical exam showed a right tm, which was red. left tm was okay. she was fairly congested but looked happy and playful. she was started on amoxil and aldex and we told to recheck in 2 weeks to recheck her ear. mom returned to clinic again today because she got much worse overnight. she was having difficulty breathing. she was much more congested and her appetite had decreased significantly today. she also spiked a temperature yesterday of 102.6 and always having trouble sleeping secondary to congestion'\nnlu.load('med_ner.jsl.wip.clinical resolve.icd10cm').viz(data)\n```\n![sentence resolution visualization](https://raw.githubusercontent.com/johnsnowlabs/nlu/master/docs/assets/images/nlu/vizexamples/viz_module/resolve_sentence_big.png)\n\n## configure visualizations\n### define custom colors for labels\nsome entity and relation labels will be highlighted with a pre-defined color, which you [can find here](https://github.com/johnsnowlabs/spark-nlp-display/tree/main/sparknlp_display/label_colors).    \nfor labels that have no color defined, a random color will be generated.     \nyou can define colors for labels manually, by specifying via the `viz_colors` parameter\nand defining `hex color codes` in a dictionary that maps `labels` to `colors` .\n```python\ndata = 'dr. john snow suggested that fritz takes 5mg penicilin for his cough'\n# define custom colors for labels\nviz_colors={'strength':'#800080', 'drug_brandname':'#77b5fe', 'gender':'#77ffe'}\nnlu.load('med_ner.jsl.wip.clinical').viz(data,viz_colors =viz_colors)\n```\n![define colors labels](https://raw.githubusercontent.com/johnsnowlabs/nlu/master/docs/assets/images/nlu/vizexamples/viz_module/define_colors.png)\n\n\n### filter entities that get highlighted\nby default every entity class will be visualized.    \nthe `labels_to_viz` can be used to define a set of labels to highlight.       \napplicable for ner, resolution and assert.\n```python\ndata = 'dr. john snow suggested that fritz takes 5mg penicilin for his cough'\n# filter wich ner label to viz\nlabels_to_viz=['symptom']\nnlu.load('med_ner.jsl.wip.clinical').viz(data,labels_to_viz=labels_to_viz)\n```\n![filter labels](https://raw.githubusercontent.com/johnsnowlabs/nlu/master/docs/assets/images/nlu/vizexamples/viz_module/filter_labels.png)\n\n\n## new models\nnew multilingual models for `afrikaans`, `welsh`, `maltese`, `tamil`, and`vietnamese`\n\n| nlu.load() refrence                                          | spark nlp refrence                                           |\n| ------------------------------------------------------------ | ------------------------------------------------------------ |\n| [vi.lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_vi.html) | [lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_vi.html) |\n| [mt.lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_mt.html) | [lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_mt.html) |\n| [ta.lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_ta.html) | [lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_ta.html) |\n| [af.lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_af.html) | [lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_af.html) |\n| [af.pos](https://nlp.johnsnowlabs.com/2021/04/06/pos_afribooms_af.html) | [pos_afribooms](https://nlp.johnsnowlabs.com/2021/04/06/pos_afribooms_af.html) |\n| [cy.lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_cy.html) | [lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_cy.html) |\n\n## reworked and updated nlu tutorial notebooks\n\nall of the [140+ nlu tutorial notebooks](https://github.com/johnsnowlabs/nlu/tree/master/examples) have been updated and reworked to reflect the latest changes in nlu 3.0.0+\n\n\n## improved column name generation\n- nlu categorized each internal component now with boolean labels for `name_deductable` and `always_name_deductable` .\n- before generating column names, nlu checks wether each component is of unique in the pipeline or not. if a component is not unique in the\n  pipe and there are multiple components of same type, i.e. multiple `ner` models, nlu will deduct a base name for the final output columns from the\n  nlu reference each ner model is pointing to.\n- if on the other hand, there is only one `ner` model in the pipeline, only the default `ner` column prefixed will be generated.\n- for some components, like `embeddings` and `classifiers` are now defined as `always_name_deductable`, for those nlu will always try to infer a meaningful base name for the output columns.\n- newly trained component output columns will now be prefixed with `trained_&lt;type&gt;` , for types `pos` , `ner`, `classifier`, `sentiment` and `multi_classifier`\n\n## enhanced offline mode\n- you can still load a model from a path as usual with `nlu.load(path=model_path)` and output columns will be suffixed with `from_disk`\n- you can now optionally also specify `request` parameter during  load a model from hdd, it will be used to deduct more meaningful column name suffixes, instead of `from_disk`, i.e. by calling `nlu.load(request ='en.embed_sentence.biobert.pubmed_pmc_base_cased', path=model_path)`\n\n\n### bugfixes\n- fixed a bug that caused  resolution algorithms output level to be inferred incorrectly\n- fixed a bug that caused stranger cols got dropped\n- fixed a bug that caused endings to miss when  .predict(position=true) was specified\n- fixed a bug that caused pd.series to be converted incorrectly internally\n- fixed a bug that caused output level transformations to crash\n- fixed a bug that caused verbose mode not to turn of properly after turning it on.\n- fixed a bug that caused some models to crash when loaded for hdd\n\n# additional nlu resources\n* [140+ updates tutorials](https://github.com/johnsnowlabs/nlu/tree/master/examples)\n* [updated visualization docs](https://nlu.johnsnowlabs.com/docs/en/viz_examples)\n* [models hub](https://nlp.johnsnowlabs.com/models) with new models\n* [spark nlp publications](https://medium.com/spark-nlp)\n* [nlu in action](https://nlp.johnsnowlabs.com/demo)\n* [nlu documentation](https://nlu.johnsnowlabs.com/docs/en/install)\n* [discussions](https://github.com/johnsnowlabs/spark-nlp/discussions) engage with other community members, share ideas, and show off how you use spark nlp and nlu!\n\n# 1 line install nlu on google colab\n```!wget https://setup.johnsnowlabs.com/nlu/colab.sh  -o - | bash```\n# 1 line install nlu on kaggle\n```!wget https://setup.johnsnowlabs.com/nlu/kaggle.sh  -o - | bash```\n# install via pip\n```! pip install nlu pyspark==3.0.1```", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/n5goqe/1_line_to_visualizations_for_dependency_trees/',)", "identifyer": 5592809, "year": "2021"}, {"autor": "afrojacksparrow", "date": 1618934541000, "content": "Decision Tree Classification Percentage with Incomplete Data /!/ I have a large data set that I want to fit a decision tree to. I am wondering if it is possible to fit a tree and then find the classification percentage at any given node. \n\nFor example: Lets say I have a game where a person makes 3 consecutive binary decisions. Depending on the decisions they make they can either win or lose. So the first node is decision 1, second node would be decision 2, third node would be decision 3.  So there are 8 combinations total. I want to find the probability that they win after the first or second decision. I know it is as simple as relationship between the leafs that are associated with that specific node. My question is if it is possible to find that specific percentage in Python or R. \n\nAs a side question, is it possible to set the variable order that the tree splits on? \n\nIf you think there is a better way for me to accomplish this or if my question is unclear please let me know.", "link": "https://www.reddit.com/r/datascience/comments/mutnem/decision_tree_classification_percentage_with/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "decision -----> tree !!!  classification percentage with incomplete data /!/ i have a large data set that i want to fit a decision tree to. i am wondering if it is possible to fit a tree and then find the classification percentage at any given node. \n\nfor example: lets say i have a game where a person makes 3 consecutive binary decisions. depending on the decisions they make they can either win or lose. so the first node is decision 1, second node would be decision 2, third node would be decision 3.  so there are 8 combinations total. i want to find the probability that they win after the first or second decision. i know it is as simple as relationship between the leafs that are associated with that specific node. my question is if it is possible to find that specific percentage in python or r. \n\nas a side question, is it possible to set the variable order that the tree splits on? \n\nif you think there is a better way for me to accomplish this or if my question is unclear please let me know.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/mutnem/decision_tree_classification_percentage_with/',)", "identifyer": 5593330, "year": "2021"}, {"autor": "NaN_Loss", "date": 1633537913000, "content": "DeepMind Reinforcement Learning Lecture Series 2021 /!/ [https://www.youtube.com/watch?v=TCCjZe0y4Qc&amp;list=PLqYmG7hTraZDVH599EItlEWsUOsJbAodm](https://www.youtube.com/watch?v=TCCjZe0y4Qc&amp;list=PLqYmG7hTraZDVH599EItlEWsUOsJbAodm)\n\nThis is a theoretical course (expect lots of math and probabilities)\n\n* Lecture 1: Introduction to Reinforcement Learning\n* Lecture 2: Exploration &amp; Control\n* Lecture 3: MDPs &amp; Dynamic Programming\n* Lecture 4: Theoretical Fundamentals of Dynamic Programming Algorithms\n* Lecture 5: Model-free prediction, Monte Carlo and temporal difference algorithms.\n* Lecture 6: Model-free Control\n* Lecture 7: Function Approximation: Deep Reinforcement Learning\n* Lecture 8: Planning &amp; models: Dyna and Monte-Carlo tree search (MCTS).\n* Lecture 9: Policy-Gradient &amp; Actor-Critic methods\n* Lecture 10: Approximate Dynamic Programming\n* Lecture 11: Multi-step &amp; Off Policy\n* Lecture 12: Deep Reinforcement Learning #1: Practical considerations, Auto-differentiation with Jax\n* Lecture 13: Deep Reinforcement Learning #2: General value functions\n\nMore Machine Learning courses on [https://tutobase.com/t/MachineLearning?tag=course](https://tutobase.com/t/MachineLearning?tag=course)", "link": "https://www.reddit.com/r/datascience/comments/q2o2en/deepmind_reinforcement_learning_lecture_series/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "deepmind reinforcement learning lecture series 2021 /!/ [https://www.youtube.com/watch?v=tccjze0y4qc&amp;list=plqymg7htrazdvh599eitlewsuosjbaodm](https://www.youtube.com/watch?v=tccjze0y4qc&amp;list=plqymg7htrazdvh599eitlewsuosjbaodm)\n\nthis is a theoretical course (expect lots of math and probabilities)\n\n* lecture 1: introduction to reinforcement learning\n* lecture 2: exploration &amp; control\n* lecture 3: mdps &amp; dynamic programming\n* lecture 4: theoretical fundamentals of dynamic programming algorithms\n* lecture 5: model-free prediction, monte carlo and temporal difference algorithms.\n* lecture 6: model-free control\n* lecture 7: function approximation: deep reinforcement learning\n* lecture 8: planning &amp; models: dyna and monte-carlo -----> tree !!!  search (mcts).\n* lecture 9: policy-gradient &amp; actor-critic methods\n* lecture 10: approximate dynamic programming\n* lecture 11: multi-step &amp; off policy\n* lecture 12: deep reinforcement learning #1: practical considerations, auto-differentiation with jax\n* lecture 13: deep reinforcement learning #2: general value functions\n\nmore machine learning courses on [https://tutobase.com/t/machinelearning?tag=course](https://tutobase.com/t/machinelearning?tag=course)", "sortedWord": "None", "removed": "('nan',)", "score": 20, "comments": 3, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/q2o2en/deepmind_reinforcement_learning_lecture_series/',)", "identifyer": 5593696, "year": "2021"}, {"autor": "fanhui3", "date": 1635230904000, "content": "R to Python conversion /!/ Hi, I finished a post grad in data science last year and I finally starting a job in the related field next month! \n\nHowever, my employer uses python while I did my course in R. I am currently picking up python from scratch and has since completed lessons on numpy, pandas and matplotlib. I think my progress is still too slow and would like some guidance from the community on what to focus on next to get to close up my proficiency gap with R. \n\nWhat i have done in R: \n\n* \\-Unguided  ML \n   * Clustering \n* Guided ML \n   * Decision Tree\n   * RandomForest\n   * CHAID\n* Predictive Modeling \n   * Linear, MLR, Logisitc Regression\n   * KNN\n   * Bagging/Boosting\n   * SMOTE\n\n&amp;#x200B;\n\nAny advise would be appreciated and treasured. Thank you!", "link": "https://www.reddit.com/r/datascience/comments/qg0ldp/r_to_python_conversion/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "r to python conversion /!/ hi, i finished a post grad in data science last year and i finally starting a job in the related field next month! \n\nhowever, my employer uses python while i did my course in r. i am currently picking up python from scratch and has since completed lessons on numpy, pandas and matplotlib. i think my progress is still too slow and would like some guidance from the community on what to focus on next to get to close up my proficiency gap with r. \n\nwhat i have done in r: \n\n* \\-unguided  ml \n   * clustering \n* guided ml \n   * decision -----> tree !!! \n   * randomforest\n   * chaid\n* predictive modeling \n   * linear, mlr, logisitc regression\n   * knn\n   * bagging/boosting\n   * smote\n\n&amp;#x200b;\n\nany advise would be appreciated and treasured. thank you!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/qg0ldp/r_to_python_conversion/',)", "identifyer": 5593892, "year": "2021"}, {"autor": "yiriwer", "date": 1625817440000, "content": "Decision Tree Algorithm, Explained", "link": "https://www.reddit.com/r/datascience/comments/ogqwy8/decision_tree_algorithm_explained/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "decision -----> tree !!!  algorithm, explained", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.thinkdataanalytics.com/decision-tree-algorithm/',)", "identifyer": 5594540, "year": "2021"}, {"autor": "Vivid_Perception_143", "date": 1612934386000, "content": "SeaLion - machine learning/data science library by a freshman /!/ I've posted a very similar post on r/Python and r/MachineLearning, so there's a chance you may have seen this before. But anyways, here it is. \n\nRecently during the first semester of my 9th grade I've worked on a library called [SeaLion](https://pypi.org/project/sealion/) (from scratch.) It is for machine learning algorithms, from the basic linear regression up to neural networks.\n\nRecently SeaLion has gained some popularity (177 star + 14 forks), so I thought I should share it with this subreddit. Some information : SeaLion uses Python and also Cython in the back, and took 3 months to build and is about 5k lines. \n\nYou can install this library with pip. It's on PyPI : [https://pypi.org/project/sealion/](https://pypi.org/project/sealion/) and GitHub : [SeaLion Repo](https://github.com/anish-lakkapragada/SeaLion)\n\nOkay enough talk, so what is this thing?\n\n**SeaLion is a data science library that's extremely comprehensive. Rather than assuming you know the theory behind the algorithms it guides you every step of the way. It mostly deals with regression, unsupervised clustering, bayesian models, dimensionality reduction, neural networks, etc. I think those who are new to data science and machine learning will greatly enjoy it. There are code examples that explain each function in SeaLion, how to use it, and most importantly why and when to use it.** \n\nThe code examples are a set of 12 jupyter notebooks. They tackle real world datasets/problems like breast cancer, iris, moons, titanic, spam classification, MNIST, etc. with SeaLion's algorithms. You can find the code examples here : [SeaLion Example Code](https://github.com/anish-lakkapragada/SeaLion/tree/main/examples)\n\nSeaLion is also incredibly easy to use. *The naming of the functions and classes was deliberately made similar to most other ML frameworks so those already with experience can easily pick up SeaLion to give me feedback or use it. None of the actual source code uses any ML frameworks of course.*\n\nI think not just beginners, but even those who are familiar with machine learning algorithms would  benefit from using it and going through any of the example notebooks. And on a side note, please give the repo a star!\n\n**If you have any feedback, suggestions, or questions please put them in the comments below. I'm really hoping to get feedback from this community. This could be on code practices, how to make the code go faster (I got a really good issue on GitHub for that), more algorithms I could build, etc.**\n\nAs usual thank you for your time! I really hope this is helpful for others. Feel free to ask any questions below.", "link": "https://www.reddit.com/r/datascience/comments/lgmoxp/sealion_machine_learningdata_science_library_by_a/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "sealion - machine learning/data science library by a freshman /!/ i've posted a very similar post on r/python and r/machinelearning, so there's a chance you may have seen this before. but anyways, here it is. \n\nrecently during the first semester of my 9th grade i've worked on a library called [sealion](https://pypi.org/project/sealion/) (from scratch.) it is for machine learning algorithms, from the basic linear regression up to neural networks.\n\nrecently sealion has gained some popularity (177 star + 14 forks), so i thought i should share it with this subreddit. some information : sealion uses python and also cython in the back, and took 3 months to build and is about 5k lines. \n\nyou can install this library with pip. it's on pypi : [https://pypi.org/project/sealion/](https://pypi.org/project/sealion/) and github : [sealion repo](https://github.com/anish-lakkapragada/sealion)\n\nokay enough talk, so what is this thing?\n\n**sealion is a data science library that's extremely comprehensive. rather than assuming you know the theory behind the algorithms it guides you every step of the way. it mostly deals with regression, unsupervised clustering, bayesian models, dimensionality reduction, neural networks, etc. i think those who are new to data science and machine learning will greatly enjoy it. there are code examples that explain each function in sealion, how to use it, and most importantly why and when to use it.** \n\nthe code examples are a set of 12 jupyter notebooks. they tackle real world datasets/problems like breast cancer, iris, moons, titanic, spam classification, mnist, etc. with sealion's algorithms. you can find the code examples here : [sealion example code](https://github.com/anish-lakkapragada/sealion/-----> tree !!! /main/examples)\n\nsealion is also incredibly easy to use. *the naming of the functions and classes was deliberately made similar to most other ml frameworks so those already with experience can easily pick up sealion to give me feedback or use it. none of the actual source code uses any ml frameworks of course.*\n\ni think not just beginners, but even those who are familiar with machine learning algorithms would  benefit from using it and going through any of the example notebooks. and on a side note, please give the repo a star!\n\n**if you have any feedback, suggestions, or questions please put them in the comments below. i'm really hoping to get feedback from this community. this could be on code practices, how to make the code go faster (i got a really good issue on github for that), more algorithms i could build, etc.**\n\nas usual thank you for your time! i really hope this is helpful for others. feel free to ask any questions below.", "sortedWord": "None", "removed": "('nan',)", "score": 0, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/lgmoxp/sealion_machine_learningdata_science_library_by_a/',)", "identifyer": 5594685, "year": "2021"}, {"autor": "opensourcecolumbus", "date": 1625447122000, "content": "How I transformed my open-source project, a neural search framework to add AI capabilities to your search system /!/ 3 months back, I gathered a lot of feedback  on my project [Jina](https://github.com/jina-ai/jina/) 1.0 version, many people were keen to use Jina for multimedia search because that's where use of Neural Networks makes significant difference. So I focused on that part and I was able to transform it from 1.0 to 2.0 within 3 months.\n\nActually, I should say - \"'we' made this\", because there were more than 155 contributors who did it, not just me. Here's what we did\n\n* We saw MachineLearning beginners struggle in using Jina 1.0, so we separated the codebase where ML expertise is required([jina-hub](https://github.com/jina-ai/jina-hub)) and the one which MachineLearning beginners can use(the [jina](https://github.com/jina-ai/jina/) core). Now ML beginners don't need to worry about jina-hub and can use jina hub packages directly to implement ML specific tasks without the need to understand advanced ML concepts. While advanced ML users can create their own jina-hub packages.\n* We cut down a lots of abstractions to make it easy to use for beginners\n* Made python APIs more intuitive to use\n* Improved performance(3.6x faster on startup) \n\nHere's [Jina 2.0](https://github.com/jina-ai/jina/) and here's [Jina 1.0](https://github.com/jina-ai/jina/tree/v1.0.0).  I seek feedback from people who are looking at this project for the  first time, as well as people who have tried their hands before but had some challenges in using it. Few questions, I'm seeking answers to\n\nHow easy it is to use for a beginner now? Are we going in the right direction?", "link": "https://www.reddit.com/r/datascience/comments/odwwma/how_i_transformed_my_opensource_project_a_neural/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "how i transformed my open-source project, a neural search framework to add ai capabilities to your search system /!/ 3 months back, i gathered a lot of feedback  on my project [jina](https://github.com/jina-ai/jina/) 1.0 version, many people were keen to use jina for multimedia search because that's where use of neural networks makes significant difference. so i focused on that part and i was able to transform it from 1.0 to 2.0 within 3 months.\n\nactually, i should say - \"'we' made this\", because there were more than 155 contributors who did it, not just me. here's what we did\n\n* we saw machinelearning beginners struggle in using jina 1.0, so we separated the codebase where ml expertise is required([jina-hub](https://github.com/jina-ai/jina-hub)) and the one which machinelearning beginners can use(the [jina](https://github.com/jina-ai/jina/) core). now ml beginners don't need to worry about jina-hub and can use jina hub packages directly to implement ml specific tasks without the need to understand advanced ml concepts. while advanced ml users can create their own jina-hub packages.\n* we cut down a lots of abstractions to make it easy to use for beginners\n* made python apis more intuitive to use\n* improved performance(3.6x faster on startup) \n\nhere's [jina 2.0](https://github.com/jina-ai/jina/) and here's [jina 1.0](https://github.com/jina-ai/jina/-----> tree !!! /v1.0.0).  i seek feedback from people who are looking at this project for the  first time, as well as people who have tried their hands before but had some challenges in using it. few questions, i'm seeking answers to\n\nhow easy it is to use for a beginner now? are we going in the right direction?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/odwwma/how_i_transformed_my_opensource_project_a_neural/',)", "identifyer": 5594801, "year": "2021"}, {"autor": "opensourcecolumbus", "date": 1621219794000, "content": "PDF search - Another project I built using Jina(AI Search framework) /!/ [Source Code on Github](https://github.com/jina-ai/examples/tree/master/multimodal-search-pdf)\n\nIn this project I am using [Jina](https://github.com/jina-ai/jina) to search a repository of PDF files. The project allows a user to query the data by providing text, or an image, or both simultaneously.\n\n**How to use it?**\n\nClone the project and run following commands\n\n    # Install requirements\n    pip install -r requirements.txt\n    \n    # Start the server\n    python app.py -t query_restful\n    \n    # Query via REST API\n    curl --request POST -d '{\"top_k\": 10, \"mode\": \"search\",  \"data\": [\"jina hello multimodal\"]}' -H 'Content-Type: application/json' 'http://0.0.0.0:45670/api/search'\n\nWhat's included in this example:\n\n* Search text, image, PDF all in one Flow or in separate Flows\n* Speed up indexing time with parallel Peas\n* Use customized executors to better fit your needs\n* Provide detailed docstrings for YAML files to help you understand the example  \n\n\nLet me know your feedback and what would you use this project for. I'd love to help", "link": "https://www.reddit.com/r/datascience/comments/ne4tco/pdf_search_another_project_i_built_using_jinaai/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "pdf search - another project i built using jina(ai search framework) /!/ [source code on github](https://github.com/jina-ai/examples/-----> tree !!! /master/multimodal-search-pdf)\n\nin this project i am using [jina](https://github.com/jina-ai/jina) to search a repository of pdf files. the project allows a user to query the data by providing text, or an image, or both simultaneously.\n\n**how to use it?**\n\nclone the project and run following commands\n\n    # install requirements\n    pip install -r requirements.txt\n    \n    # start the server\n    python app.py -t query_restful\n    \n    # query via rest api\n    curl --request post -d '{\"top_k\": 10, \"mode\": \"search\",  \"data\": [\"jina hello multimodal\"]}' -h 'content-type: application/json' 'http://0.0.0.0:45670/api/search'\n\nwhat's included in this example:\n\n* search text, image, pdf all in one flow or in separate flows\n* speed up indexing time with parallel peas\n* use customized executors to better fit your needs\n* provide detailed docstrings for yaml files to help you understand the example  \n\n\nlet me know your feedback and what would you use this project for. i'd love to help", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/ne4tco/pdf_search_another_project_i_built_using_jinaai/',)", "identifyer": 5594929, "year": "2021"}, {"autor": "ez613", "date": 1621948544000, "content": "Efficient Decision Tree Pruning in Python /!/ Hi,\n\nI'm working on a project where I need to automatically create a large number of decision tree, and to prune them.\n\nThe only way to do that is a function like that :\n\n    def do_best_tree(Xtrain, ytrain, Xtest, ytest):\n        clf = DecisionTreeClassifier()\n        clf.fit(Xtrain, ytrain)\n        path = clf.cost_complexity_pruning_path(Xtrain, ytrain)\n        ccp_alphas = path.ccp_alphas\n        clfs = []\n        if len(ccp_alphas) &gt; 100:\n            nb = 2\n        else:\n            nb=1\n        for ccp_alpha in tqdm(ccp_alphas[::nb]):\n            clf = DecisionTreeClassifier(ccp_alpha=ccp_alpha)\n            clf.fit(Xtrain, ytrain)\n            clfs.append(clf)\n        return max(clfs, key=lambda x:x.score(Xtest, ytest))\n    \n\nSo it take a huge amount of time, as it fit a lot of trees. See the\n\n       if len(ccp_alphas) &gt; 100:\n            nb = 2\n\nI added in order to divide by two the number of trees, but it still very slow.\n\nIsn't there another way to do that ? For example by really pruning a single tree ? I'm not limited to scikit-learn, but didn't find anything that is doing that....\n\nThank you in advance !", "link": "https://www.reddit.com/r/datascience/comments/nkpidl/efficient_decision_tree_pruning_in_python/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "efficient decision -----> tree !!!  pruning in python /!/ hi,\n\ni'm working on a project where i need to automatically create a large number of decision tree, and to prune them.\n\nthe only way to do that is a function like that :\n\n    def do_best_tree(xtrain, ytrain, xtest, ytest):\n        clf = decisiontreeclassifier()\n        clf.fit(xtrain, ytrain)\n        path = clf.cost_complexity_pruning_path(xtrain, ytrain)\n        ccp_alphas = path.ccp_alphas\n        clfs = []\n        if len(ccp_alphas) &gt; 100:\n            nb = 2\n        else:\n            nb=1\n        for ccp_alpha in tqdm(ccp_alphas[::nb]):\n            clf = decisiontreeclassifier(ccp_alpha=ccp_alpha)\n            clf.fit(xtrain, ytrain)\n            clfs.append(clf)\n        return max(clfs, key=lambda x:x.score(xtest, ytest))\n    \n\nso it take a huge amount of time, as it fit a lot of trees. see the\n\n       if len(ccp_alphas) &gt; 100:\n            nb = 2\n\ni added in order to divide by two the number of trees, but it still very slow.\n\nisn't there another way to do that ? for example by really pruning a single tree ? i'm not limited to scikit-learn, but didn't find anything that is doing that....\n\nthank you in advance !", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/nkpidl/efficient_decision_tree_pruning_in_python/',)", "identifyer": 5595183, "year": "2021"}, {"autor": "bngproduct", "date": 1631258420000, "content": "I made a curriculum that helps data science job seekers who is actively interviewing be better prepared for interview. I'd love your feedback /!/ # Approach: \n\nBusiness Problem solving Skill using data  + Hands-on Coding Skill Interview focussed + People Skill/ Leadership Skill + Mentorship = Interview Success Formula  \n\n\n**Part 1.) Practical Problem Solving Interview prep: (10- 12 Minute Videos)**\n\n**Practice Framework for answering:** Problem Statement, Metrics, Architectural Components, Feature Engineering, Training Data Generation, Modeling\n\n* Design a machine learning platform\n* Design Facebook photo tagging\n* Design Amazon Alexa\n* Design a fake news detector\n* Design youtube\u2019s recommendation system\n* Fraud detection engine for a bank\n* Customer churn prevention for Food Aggregator\n* Flight Price optimization model for Makemytrip\n* Search Ranking\n* Feed Based System\n* Recommendation System\n* Self-Driving Car: Image Segmentation\n* Entity Linking System\n* Ad Prediction System\n* School Budgeting with Machine Learning in Python  \nTask for students: Prepare Google Sheet in Framework format for 10 different Solution  \n**Part 2.) Hands-on Coding Simulation Practice:** \n\nMath Concept: \n\n* Sparse Matrix Multiplier\n* Get statistic\n\nModel Concepts\n\n* K-nearest neighbour\n* K-means\n* Multinomial Naive Bayes\n* Regression Tree\n* Neuron\n\nModel Application\n\n* Classify Text from Image\n* Predict Cancellation\n\nPython \n\n* Exploratory data analysis using python\n* Apply Naive Bayes on data\n* Applying Bag of words on data\n* Apply ARIMA Algorithm on data  \n\n\n**Part 3.) Behavioural Interview Prep (6-10 Minute Videos)**\n\nThrough our behavioural matrix, we'll teach you how to nail your elevator pitch and draft answers to questions we predict with data.\n\nPrepare for Amazon Principles: Customer Obsession, Ownership, Invent and Simplify, Learn and Be Curious, Hire and Develop the Best, Insist on the Highest Standards, Think Big, Bias for Action, Frugality, Earn Trust, Dive Deep, Disagree and Commit, Deliver Results\n\n* General Tips\n* Frameworks to answer question\n* Imagine you had a low performer on your team. How would you handle the situation? What would you do to help them?\n* Describe a time when there was a conflict within your team. How did you help resolve the conflict? Did you do anything to prevent it in the future?\n* Why do you want to work at ?\n* Describe a time when you strongly disagreed with a coworker about an engineering decision. How did you go about making the final decision? What did you do after the decision was made?\n* Imagine you and your team are in the middle of a major project at work, with many moving parts, complicated context, a lot of work, etc.. A new software engineer joins your team, and you're tasked with onboarding them; what do you do?\n* How would you go about distributing work for a project across a team of software engineers? If you've led a project in the past, describe what you did.\n* Describe a time when you made a mistake. How did you deal with the repercussions of the mistake? What lessons did you learn from the mistake?\n* Describe a challenging project that you worked on. Why was it challenging? What was your role in the project? How did you deal with the various difficulties of the project?\n* Describe a time when you had to deal with an outage at work. How did you handle the situation? What steps did you take after the issue was resolved?\n* What do you think about receiving and giving feedback? Describe a time when you received tough feedback and/or a time when you give tough feedback. How did you react to it? How did you give it?\n* What aspects of data science do you think you're very good at? What about areas where you'd like to improve? How do you plan on improving?\n* Describe a time when you went out of your comfort zone. Why did you do it? What lessons did you learn from the experience?  \n**Task: Prepare Google Sheet in Framework format for 10 different Solution**\n\n**Part 4.) Community Answers: (At least 5-different perceptive answer for each question)**\n\n* Answer to 100 top Interview Questions in Text format from Experts, Mentor, Alumni, Professor, and Fellow Student.", "link": "https://www.reddit.com/r/datascience/comments/plg6w9/i_made_a_curriculum_that_helps_data_science_job/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "i made a curriculum that helps data science job seekers who is actively interviewing be better prepared for interview. i'd love your feedback /!/ # approach: \n\nbusiness problem solving skill using data  + hands-on coding skill interview focussed + people skill/ leadership skill + mentorship = interview success formula  \n\n\n**part 1.) practical problem solving interview prep: (10- 12 minute videos)**\n\n**practice framework for answering:** problem statement, metrics, architectural components, feature engineering, training data generation, modeling\n\n* design a machine learning platform\n* design facebook photo tagging\n* design amazon alexa\n* design a fake news detector\n* design youtube\u2019s recommendation system\n* fraud detection engine for a bank\n* customer churn prevention for food aggregator\n* flight price optimization model for makemytrip\n* search ranking\n* feed based system\n* recommendation system\n* self-driving car: image segmentation\n* entity linking system\n* ad prediction system\n* school budgeting with machine learning in python  \ntask for students: prepare google sheet in framework format for 10 different solution  \n**part 2.) hands-on coding simulation practice:** \n\nmath concept: \n\n* sparse matrix multiplier\n* get statistic\n\nmodel concepts\n\n* k-nearest neighbour\n* k-means\n* multinomial naive bayes\n* regression -----> tree !!! \n* neuron\n\nmodel application\n\n* classify text from image\n* predict cancellation\n\npython \n\n* exploratory data analysis using python\n* apply naive bayes on data\n* applying bag of words on data\n* apply arima algorithm on data  \n\n\n**part 3.) behavioural interview prep (6-10 minute videos)**\n\nthrough our behavioural matrix, we'll teach you how to nail your elevator pitch and draft answers to questions we predict with data.\n\nprepare for amazon principles: customer obsession, ownership, invent and simplify, learn and be curious, hire and develop the best, insist on the highest standards, think big, bias for action, frugality, earn trust, dive deep, disagree and commit, deliver results\n\n* general tips\n* frameworks to answer question\n* imagine you had a low performer on your team. how would you handle the situation? what would you do to help them?\n* describe a time when there was a conflict within your team. how did you help resolve the conflict? did you do anything to prevent it in the future?\n* why do you want to work at ?\n* describe a time when you strongly disagreed with a coworker about an engineering decision. how did you go about making the final decision? what did you do after the decision was made?\n* imagine you and your team are in the middle of a major project at work, with many moving parts, complicated context, a lot of work, etc.. a new software engineer joins your team, and you're tasked with onboarding them; what do you do?\n* how would you go about distributing work for a project across a team of software engineers? if you've led a project in the past, describe what you did.\n* describe a time when you made a mistake. how did you deal with the repercussions of the mistake? what lessons did you learn from the mistake?\n* describe a challenging project that you worked on. why was it challenging? what was your role in the project? how did you deal with the various difficulties of the project?\n* describe a time when you had to deal with an outage at work. how did you handle the situation? what steps did you take after the issue was resolved?\n* what do you think about receiving and giving feedback? describe a time when you received tough feedback and/or a time when you give tough feedback. how did you react to it? how did you give it?\n* what aspects of data science do you think you're very good at? what about areas where you'd like to improve? how do you plan on improving?\n* describe a time when you went out of your comfort zone. why did you do it? what lessons did you learn from the experience?  \n**task: prepare google sheet in framework format for 10 different solution**\n\n**part 4.) community answers: (at least 5-different perceptive answer for each question)**\n\n* answer to 100 top interview questions in text format from experts, mentor, alumni, professor, and fellow student.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/plg6w9/i_made_a_curriculum_that_helps_data_science_job/',)", "identifyer": 5595463, "year": "2021"}, {"autor": "Krypton_Rimsdim", "date": 1632371074000, "content": "[Resource Hunting] Data Mining using Python Data Analytics Libraries /!/  \n\nThese topics are in my data mining class this semester and I must learn and master all of'em.\n\nUsing Python Data Analytics Libraries:\n\n1. Linear Regression\n2. Model Selection &amp; regularization\n3. K-Nearest Neighbors\n4. Cross-Validation\n5. Logistic Regression\n6. Tree-Based Models\n7. Principal Component Analysis\n8. Time Series Analysis\n\nMy professor is kinda not good at teaching so what I gotta do is find online resources that are easy to understand and concise for all these topics since I don't have a very good background in stats.\n\nI would appreciate it if you could guide me and share some top-notch resources with me.", "link": "https://www.reddit.com/r/datascience/comments/ptnh94/resource_hunting_data_mining_using_python_data/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "[resource hunting] data mining using python data analytics libraries /!/  \n\nthese topics are in my data mining class this semester and i must learn and master all of'em.\n\nusing python data analytics libraries:\n\n1. linear regression\n2. model selection &amp; regularization\n3. k-nearest neighbors\n4. cross-validation\n5. logistic regression\n6. -----> tree !!! -based models\n7. principal component analysis\n8. time series analysis\n\nmy professor is kinda not good at teaching so what i gotta do is find online resources that are easy to understand and concise for all these topics since i don't have a very good background in stats.\n\ni would appreciate it if you could guide me and share some top-notch resources with me.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 13, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/ptnh94/resource_hunting_data_mining_using_python_data/',)", "identifyer": 5595754, "year": "2021"}, {"autor": "asianyo", "date": 1629993639000, "content": "Help me understand what I\u2019m doing wrong /!/ I\u2019m at the end of my line here. For years I\u2019ve been trying to understand and learn data science to no avail. I\u2019ve ignored the haters telling me I\u2019m doing it all wrong but I can only take so much before they start to get to me. Please help. \n\nI drove 3 hours to a random forrest and not a single tree gave me a decision. Every time a hit a server with a pickaxe it breaks. I\u2019ve scraped so many webpages my knife dulled and now my screen is busted. I\u2019ve read every book on dangerous snakes and still don\u2019t understand how the python is in any way related to DS. I was kicked out of the Pirates of the Caribbean filming set because i demanded to know where the pacman machine was. I have 3 restraining orders by woman named Julia. And how tf is CNN related to nets? Is it because they have a website? I broke my third screen trying to scrape it. I read bed time stories to my samsung smart fridge but it won\u2019t learn. \n\nHas anyone else ran into similar problems?  Would love any advice.", "link": "https://www.reddit.com/r/datascience/comments/pc2g4c/help_me_understand_what_im_doing_wrong/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "help me understand what i\u2019m doing wrong /!/ i\u2019m at the end of my line here. for years i\u2019ve been trying to understand and learn data science to no avail. i\u2019ve ignored the haters telling me i\u2019m doing it all wrong but i can only take so much before they start to get to me. please help. \n\ni drove 3 hours to a random forrest and not a single -----> tree !!!  gave me a decision. every time a hit a server with a pickaxe it breaks. i\u2019ve scraped so many webpages my knife dulled and now my screen is busted. i\u2019ve read every book on dangerous snakes and still don\u2019t understand how the python is in any way related to ds. i was kicked out of the pirates of the caribbean filming set because i demanded to know where the pacman machine was. i have 3 restraining orders by woman named julia. and how tf is cnn related to nets? is it because they have a website? i broke my third screen trying to scrape it. i read bed time stories to my samsung smart fridge but it won\u2019t learn. \n\nhas anyone else ran into similar problems?  would love any advice.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 115, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/pc2g4c/help_me_understand_what_im_doing_wrong/',)", "identifyer": 5595770, "year": "2021"}, {"autor": "pp314159", "date": 1613375532000, "content": "Compare AutoML frameworks on 10 Tabular Kaggle competitions /!/ I'm working on an AutoML system for tabular datasets. It is called MLJAR and is available as open-source with code on GitHub: https://github.com/mljar/mljar-supervised\n\n\nI've compared my AutoML with other systems on 10 tabular datasets from Kaggle. The final result is the Percentile Rank in the Private Leaderboard (evaluated internally by Kaggle). The results other than MLJAR systems are from AutoGluon paper. Below is the summary of the comparison presented:\n\n\n| Dataset      | Auto-WEKA | auto-sklearn | TPOT  | H2O AutoML | GCP-Tables | AutoGluon | MLJAR |\n|--------------|-----------|--------------|-------|------------|------------|-----------|-------|\n| ieee-fraud   | 0.119     | 0.349        |       |            | 0.119      | 0.322     | 0.172 |\n| value        | 0.114     | 0.319        | 0.325 | 0.377      |            | 0.415     | 0.445 |\n| walmart      |           | 0.39         | 0.379 |            | 0.398      | 0.384     | 0.423 |\n| transaction  | 0.131     | 0.329        | 0.326 |            | 0.404      | 0.406     | 0.463 |\n| porto        | 0.158     | 0.331        | 0.315 | 0.406      | 0.434      | 0.462     | 0.54  |\n| allstate     | 0.124     | 0.31         | 0.237 | 0.352      | 0.74       | 0.706     | 0.764 |\n| mercedes     | 0.16      | 0.444        | 0.547 | 0.363      | 0.658      | 0.169     | 0.879 |\n| otto         | 0.145     | 0.717        | 0.597 | 0.729      | 0.821      | 0.988     | 0.924 |\n| satisfaction | 0.235     | 0.408        | 0.495 | 0.74       | 0.763      | 0.823     | 0.975 |\n| bnp-paribas  | 0.193     | 0.412        | 0.46  | 0.417      | 0.44       | 0.986     | 0.986 |\n\nThe higher the value, the better. The 1st place solution in the Kaggle competition will get Percentile Rank equal 1.0. The last place in the solution in the competition will give 0 value. You can see that some AutoML frameworks jump into the Top-10% of the competition (without any human help)!\n\n\nI think that my AutoML system is quite advanced compared to other AutoML systems:\n\n- it can generate new features with [K-Means](https://mljar.com/automated-machine-learning/k-means-features/) or [Golden Features Search](https://mljar.com/automated-machine-learning/golden-features/)\n\n- it has many ML algorithms available, can tune them and train (with early stopping if applicable), in selected time regime,\n\n- can stack models in complex ensembles\n\n- creates interpretations for ML models: SHAP plots, permutation-based importance, decision tree visualizations ...\n\n- automatically generates documentation to Markdown or HTML (works like a dream in Jupyter notebook)\n\n\nI hope that many data scientists will benefit from my AutoML system. I put a lot of effort into it. I'm very interested in your opinion and feedback about my AutoML, and AutoML in general.", "link": "https://www.reddit.com/r/datascience/comments/lk8tne/compare_automl_frameworks_on_10_tabular_kaggle/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "compare automl frameworks on 10 tabular kaggle competitions /!/ i'm working on an automl system for tabular datasets. it is called mljar and is available as open-source with code on github: https://github.com/mljar/mljar-supervised\n\n\ni've compared my automl with other systems on 10 tabular datasets from kaggle. the final result is the percentile rank in the private leaderboard (evaluated internally by kaggle). the results other than mljar systems are from autogluon paper. below is the summary of the comparison presented:\n\n\n| dataset      | auto-weka | auto-sklearn | tpot  | h2o automl | gcp-tables | autogluon | mljar |\n|--------------|-----------|--------------|-------|------------|------------|-----------|-------|\n| ieee-fraud   | 0.119     | 0.349        |       |            | 0.119      | 0.322     | 0.172 |\n| value        | 0.114     | 0.319        | 0.325 | 0.377      |            | 0.415     | 0.445 |\n| walmart      |           | 0.39         | 0.379 |            | 0.398      | 0.384     | 0.423 |\n| transaction  | 0.131     | 0.329        | 0.326 |            | 0.404      | 0.406     | 0.463 |\n| porto        | 0.158     | 0.331        | 0.315 | 0.406      | 0.434      | 0.462     | 0.54  |\n| allstate     | 0.124     | 0.31         | 0.237 | 0.352      | 0.74       | 0.706     | 0.764 |\n| mercedes     | 0.16      | 0.444        | 0.547 | 0.363      | 0.658      | 0.169     | 0.879 |\n| otto         | 0.145     | 0.717        | 0.597 | 0.729      | 0.821      | 0.988     | 0.924 |\n| satisfaction | 0.235     | 0.408        | 0.495 | 0.74       | 0.763      | 0.823     | 0.975 |\n| bnp-paribas  | 0.193     | 0.412        | 0.46  | 0.417      | 0.44       | 0.986     | 0.986 |\n\nthe higher the value, the better. the 1st place solution in the kaggle competition will get percentile rank equal 1.0. the last place in the solution in the competition will give 0 value. you can see that some automl frameworks jump into the top-10% of the competition (without any human help)!\n\n\ni think that my automl system is quite advanced compared to other automl systems:\n\n- it can generate new features with [k-means](https://mljar.com/automated-machine-learning/k-means-features/) or [golden features search](https://mljar.com/automated-machine-learning/golden-features/)\n\n- it has many ml algorithms available, can tune them and train (with early stopping if applicable), in selected time regime,\n\n- can stack models in complex ensembles\n\n- creates interpretations for ml models: shap plots, permutation-based importance, decision -----> tree !!!  visualizations ...\n\n- automatically generates documentation to markdown or html (works like a dream in jupyter notebook)\n\n\ni hope that many data scientists will benefit from my automl system. i put a lot of effort into it. i'm very interested in your opinion and feedback about my automl, and automl in general.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 34, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/lk8tne/compare_automl_frameworks_on_10_tabular_kaggle/',)", "identifyer": 5596094, "year": "2021"}, {"autor": "st_pallella", "date": 1618683719000, "content": "Module Dependency Finder /!/ I recently inherited a big mess of a code base from a defunct team in my company. I cherrypicked a small portion of the process flow for an initial release. Now I want to remove all the unused libraries (both standard and custom) to make the code base lean.\n\nIs there any tools that will look at my main.py crawl through the code base and list all the packages that are used.\n\nI am look for a deep tree, for example:\nmain.py\n   \u2014library.load1func\n             \u2014func2\n                   \u2014someotherfunc\nAnd so on.\n\nIs there any easy tools which will create this map for me? Too lazy to code this myself for a onetime use.\n\nPS: just to clarify, say that \u2018library.\u2019 in the above example  has two function but my project uses only load1func, so I want only that name returned.\n\n\nThanks a lot!!!", "link": "https://www.reddit.com/r/datascience/comments/msw0vw/module_dependency_finder/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "module dependency finder /!/ i recently inherited a big mess of a code base from a defunct team in my company. i cherrypicked a small portion of the process flow for an initial release. now i want to remove all the unused libraries (both standard and custom) to make the code base lean.\n\nis there any tools that will look at my main.py crawl through the code base and list all the packages that are used.\n\ni am look for a deep -----> tree !!! , for example:\nmain.py\n   \u2014library.load1func\n             \u2014func2\n                   \u2014someotherfunc\nand so on.\n\nis there any easy tools which will create this map for me? too lazy to code this myself for a onetime use.\n\nps: just to clarify, say that \u2018library.\u2019 in the above example  has two function but my project uses only load1func, so i want only that name returned.\n\n\nthanks a lot!!!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/msw0vw/module_dependency_finder/',)", "identifyer": 5596461, "year": "2021"}, {"autor": "__compactsupport__", "date": 1623847263000, "content": "Euro 2020 Predictions Update /!/ EDIT:  /u/pedrosorio Makes good points.  I've changed the prior rankings to reflect the team ELOs from the start of qualifying, and have also included UEFA nations games.  What I really should do is model the team ability as a random walk in time, or add some sort of competition/tournament effect.  The model is good enough for me for now.  I appreciate all your comments though, so please share.\n\nLast week, I posted some predictions for the 2020 Euro.  Now that the first round is over, we can examine some of my performance.\n\nMy predictions and results for the first round are shown in [this](https://i.imgur.com/qkY6SQc.png) table (sorry it isn't prettier).  I achieve an average log loss of 0.92, where assigning all outcomes as equiprobable yields an average loss of 1.1.  My multiclass ROC for predicting the outcome is 0.77.  In short, in the first 12 games I perform slightly better than random guessing (which is honestly fine for me).  However, most people who have watched international football wouldn't assign all match events as equally likely (is Italy drawing Turkey really as probable as Italy losing to Turkey?  No).  Its hard for me to measure against a \"reasonable guesser\".  My work pool records all our guesses, and so at the end of the group stage I can use that as a sort of ensemble method to compare against.  We'll see.\n\n[Here](https://i.imgur.com/mZaivJY.png) are match predictions for the remaining group stage games conditioned on the results of the first games.  The model is not perfect, and still makes some weird predictions.  For example, Portugal is given higher probability to beat France than they are to beat Germany even though France beat Germany in the first round.  If you subscribe to some sort of sports law of transitivity, this may sound weird.\n\nMy predictions for the second round and the results of the first can be found [here](https://github.com/Dpananos/Euro2021Predictions/tree/main/predictions).", "link": "https://www.reddit.com/r/datascience/comments/o13xow/euro_2020_predictions_update/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "euro 2020 predictions update /!/ edit:  /u/pedrosorio makes good points.  i've changed the prior rankings to reflect the team elos from the start of qualifying, and have also included uefa nations games.  what i really should do is model the team ability as a random walk in time, or add some sort of competition/tournament effect.  the model is good enough for me for now.  i appreciate all your comments though, so please share.\n\nlast week, i posted some predictions for the 2020 euro.  now that the first round is over, we can examine some of my performance.\n\nmy predictions and results for the first round are shown in [this](https://i.imgur.com/qky6sqc.png) table (sorry it isn't prettier).  i achieve an average log loss of 0.92, where assigning all outcomes as equiprobable yields an average loss of 1.1.  my multiclass roc for predicting the outcome is 0.77.  in short, in the first 12 games i perform slightly better than random guessing (which is honestly fine for me).  however, most people who have watched international football wouldn't assign all match events as equally likely (is italy drawing turkey really as probable as italy losing to turkey?  no).  its hard for me to measure against a \"reasonable guesser\".  my work pool records all our guesses, and so at the end of the group stage i can use that as a sort of ensemble method to compare against.  we'll see.\n\n[here](https://i.imgur.com/mzaivjy.png) are match predictions for the remaining group stage games conditioned on the results of the first games.  the model is not perfect, and still makes some weird predictions.  for example, portugal is given higher probability to beat france than they are to beat germany even though france beat germany in the first round.  if you subscribe to some sort of sports law of transitivity, this may sound weird.\n\nmy predictions for the second round and the results of the first can be found [here](https://github.com/dpananos/euro2021predictions/-----> tree !!! /main/predictions).", "sortedWord": "None", "removed": "('nan',)", "score": 13, "comments": 15, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/o13xow/euro_2020_predictions_update/',)", "identifyer": 5596598, "year": "2021"}, {"autor": "HellzinhoSj", "date": 1635027290000, "content": "Most efficient type of school prediction based on exam scores /!/ I have this exercise, based on data taken from Harvard, which asks for what is the kind of school type (mixed, only for girls or only for boys) that is most efficient in the training of its students based on the results of the normalized exam score.\n\nThe datasets were taken from and old address that is not online anymore, but the url was:   [tutorials.iq.harvard.edu](http://tutorials.iq.harvard.edu/R/Rstatistics/Rstatistics.html) \n\nThe data description is shown in the image below.\n\nI have 4 datasets, two for schools and two for students, and what tehy have in commom are the school's Id.\n\nI am new to DS and I am stucked in this exercise. It seems simple, but I need some help/hints on how to start it. I mean, I thought on building a decision tree algorithm, but I want to be sure the best way to proceed, because: what will be my imput? My target, if I understood it correctly, is the school type (or school gender), and my imput is the normalized exam score of the students. My aproach was to take the mean of all the students' normalized exam score within school x, and y, and z etc and then construct a table with only two columns: the mean of the normalized exam score and the school's gender. So a classification algorithm with three possible outputs would do the job.\n\nBut I want to know if I am being naive or in a completely wrong path to start this exercise.", "link": "https://www.reddit.com/r/datascience/comments/qeet7q/most_efficient_type_of_school_prediction_based_on/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "most efficient type of school prediction based on exam scores /!/ i have this exercise, based on data taken from harvard, which asks for what is the kind of school type (mixed, only for girls or only for boys) that is most efficient in the training of its students based on the results of the normalized exam score.\n\nthe datasets were taken from and old address that is not online anymore, but the url was:   [tutorials.iq.harvard.edu](http://tutorials.iq.harvard.edu/r/rstatistics/rstatistics.html) \n\nthe data description is shown in the image below.\n\ni have 4 datasets, two for schools and two for students, and what tehy have in commom are the school's id.\n\ni am new to ds and i am stucked in this exercise. it seems simple, but i need some help/hints on how to start it. i mean, i thought on building a decision -----> tree !!!  algorithm, but i want to be sure the best way to proceed, because: what will be my imput? my target, if i understood it correctly, is the school type (or school gender), and my imput is the normalized exam score of the students. my aproach was to take the mean of all the students' normalized exam score within school x, and y, and z etc and then construct a table with only two columns: the mean of the normalized exam score and the school's gender. so a classification algorithm with three possible outputs would do the job.\n\nbut i want to know if i am being naive or in a completely wrong path to start this exercise.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/qeet7q/most_efficient_type_of_school_prediction_based_on/',)", "identifyer": 5597306, "year": "2021"}, {"autor": "anthrax0987", "date": 1634991175000, "content": "Working with numerical range in Decision Tree Classification /!/  Does Decision Tree Classifier works with Numerical Range?\n\nIn this example set of dataset.\n\nI am planning to test the data regarding the age range of :\n\n* 18-25\n* 25-30\n* 30-40\n* 40 and above\n\nUsing the Performance Score of these Employees, can the decision tree be able to make the decision by using age ranges? The objective of the program is to identity what affects this employees age ranges by using the numerical data under : Education, and # Of years Exp.\n\nSo the output would be(Using Gini Index here)\n\n* **Age** | **Factor that affect based on age range.**\n* 18-25 Education 25%\n* 25-30 #Of Years Exp 9.50%\n\nIs it possible to train the algorithm by basing the Age Range and Performance Score. I'm still a beginner in data science. I haven't heard of using numerical ranges.\n\nKuddos!", "link": "https://www.reddit.com/r/datascience/comments/qe3r3n/working_with_numerical_range_in_decision_tree/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "working with numerical range in decision -----> tree !!!  classification /!/  does decision -----> tree !!!  classifier works with numerical range?\n\nin this example set of dataset.\n\ni am planning to test the data regarding the age range of :\n\n* 18-25\n* 25-30\n* 30-40\n* 40 and above\n\nusing the performance score of these employees, can the decision tree be able to make the decision by using age ranges? the objective of the program is to identity what affects this employees age ranges by using the numerical data under : education, and # of years exp.\n\nso the output would be(using gini index here)\n\n* **age** | **factor that affect based on age range.**\n* 18-25 education 25%\n* 25-30 #of years exp 9.50%\n\nis it possible to train the algorithm by basing the age range and performance score. i'm still a beginner in data science. i haven't heard of using numerical ranges.\n\nkuddos!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 5, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/qe3r3n/working_with_numerical_range_in_decision_tree/',)", "identifyer": 5597320, "year": "2021"}, {"autor": "sadgaygirl98", "date": 1625238907000, "content": "The colors in rpart decision tree? /!/ I know it shows two different colors in a classification tree but why is it sometimes a darker green vs a lighter green", "link": "https://www.reddit.com/r/datascience/comments/occ6i5/the_colors_in_rpart_decision_tree/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "the colors in rpart decision -----> tree !!! ? /!/ i know it shows two different colors in a classification tree but why is it sometimes a darker green vs a lighter green", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/occ6i5/the_colors_in_rpart_decision_tree/',)", "identifyer": 5597651, "year": "2021"}, {"autor": "7Seas_ofRyhme", "date": 1628521076000, "content": "Base algorithm to use for wrapper methods in feature selection (binary classification) ? /!/ Hey guys, I'm wondering which algorithm should I use for feature selection in a binary classification problem ? (Wrapper methods)\n\nI plan to perform a RFE, forward/backward selection, and I'm unsure of which algo to use as a base (eg random forest, decision tree) \n\nIs it possible to use SVM or logistic regression here as well? \n\n\nThanks :)", "link": "https://www.reddit.com/r/datascience/comments/p133uk/base_algorithm_to_use_for_wrapper_methods_in/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "base algorithm to use for wrapper methods in feature selection (binary classification) ? /!/ hey guys, i'm wondering which algorithm should i use for feature selection in a binary classification problem ? (wrapper methods)\n\ni plan to perform a rfe, forward/backward selection, and i'm unsure of which algo to use as a base (eg random forest, decision -----> tree !!! ) \n\nis it possible to use svm or logistic regression here as well? \n\n\nthanks :)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/p133uk/base_algorithm_to_use_for_wrapper_methods_in/',)", "identifyer": 5597716, "year": "2021"}, {"autor": "Xayllernste", "date": 1610198579000, "content": "Which course will be more beneficial for an aspiring Data Scientist? Derivative Markets and Discrete Time Finance, or Ordinary Differential Equations? /!/ For my 2nd semester of my current uni year I was given these 2 as an option for an elective course and I want to hear more perspectives or experiences from others in data science.   \n\n\nFrom what I could see regarding course information:\n\n* Derivative markets and Discrete Time Finance syllabus introduces derivative securities, forward and option contracts in risk management, discusses use of derivatives in investment strategies, concept of arbitrage-free pricing, the fundamental theorem of asset pricing in discrete time, pricing on the binomial tree. Following up from the course I did in semester 1: Portfolio Theory \n* Whereas Ordinary Differential Equations introduces Linear systems of ODEs, Laplace transforms, discusses Boundary value problem, as well as Phase Planes. Following up from the course I did last year: Linear Algebra. \n\n&amp;#x200B;\n\nDerivative Markets also gets a followup next year for Continuous Time Finance which strictly requires me to take Derivative Markets this semester.   \n\n\nFrom what I roughly gathered through a bit of research, ODEs seem to be in this middle ground for application, some have said that they don't really come into application that much, some have said that the more advanced followup of Partial Derivatives Equations are very useful when it came to optimization. I also do find it odd that ODEs is only exclusive to the program I've selected: Statistical Data Science, and that Actuarial Science students cannot select this topic and are forced to select Derivative Markets.   \n\n\nIs the choice as simple as \"Do I want a finance course or not\" or is there more to decide when it comes to making a decision between the two of these?", "link": "https://www.reddit.com/r/datascience/comments/ktqu1v/which_course_will_be_more_beneficial_for_an/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "which course will be more beneficial for an aspiring data scientist? derivative markets and discrete time finance, or ordinary differential equations? /!/ for my 2nd semester of my current uni year i was given these 2 as an option for an elective course and i want to hear more perspectives or experiences from others in data science.   \n\n\nfrom what i could see regarding course information:\n\n* derivative markets and discrete time finance syllabus introduces derivative securities, forward and option contracts in risk management, discusses use of derivatives in investment strategies, concept of arbitrage-free pricing, the fundamental theorem of asset pricing in discrete time, pricing on the binomial -----> tree !!! . following up from the course i did in semester 1: portfolio theory \n* whereas ordinary differential equations introduces linear systems of odes, laplace transforms, discusses boundary value problem, as well as phase planes. following up from the course i did last year: linear algebra. \n\n&amp;#x200b;\n\nderivative markets also gets a followup next year for continuous time finance which strictly requires me to take derivative markets this semester.   \n\n\nfrom what i roughly gathered through a bit of research, odes seem to be in this middle ground for application, some have said that they don't really come into application that much, some have said that the more advanced followup of partial derivatives equations are very useful when it came to optimization. i also do find it odd that odes is only exclusive to the program i've selected: statistical data science, and that actuarial science students cannot select this topic and are forced to select derivative markets.   \n\n\nis the choice as simple as \"do i want a finance course or not\" or is there more to decide when it comes to making a decision between the two of these?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/ktqu1v/which_course_will_be_more_beneficial_for_an/',)", "identifyer": 5597778, "year": "2021"}, {"autor": "sonicking12", "date": 1621792294000, "content": "What is a good technical interview machine learning question? /!/ I am giving a technical interview later this week.  My background is in statistics and I will be a question with coding on it.  But I also want to ask a machine learning question. \n\nOur company usually uses machine learning algorithms for prediction as an alternative to logistic regression.  My own understanding is just that decision tree has a decent interpretability but less accurate than random forest or Xgboost.  I don\u2019t think it is fair for me to ask the candidate their algorithms in details since I am not an expert myself.  \n\nDoes anyone have any suggestions?  Thanks.", "link": "https://www.reddit.com/r/datascience/comments/njd0ft/what_is_a_good_technical_interview_machine/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "what is a good technical interview machine learning question? /!/ i am giving a technical interview later this week.  my background is in statistics and i will be a question with coding on it.  but i also want to ask a machine learning question. \n\nour company usually uses machine learning algorithms for prediction as an alternative to logistic regression.  my own understanding is just that decision -----> tree !!!  has a decent interpretability but less accurate than random forest or xgboost.  i don\u2019t think it is fair for me to ask the candidate their algorithms in details since i am not an expert myself.  \n\ndoes anyone have any suggestions?  thanks.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/njd0ft/what_is_a_good_technical_interview_machine/',)", "identifyer": 5597784, "year": "2021"}, {"autor": "DataGeek0", "date": 1629302788000, "content": "Webinar: The Macroscope Initiative - Building Planetary geoML with OmniSci with Q&amp;A /!/ Hey everyone! I'm planning on attending this webinar 'The Macroscope Initiative - Building Planetary geoML with OmniSci with Q&amp;A'. \n\nLearn how the #macroscope concept can be applied to the monitoring of tree health, the exposure of static and moving assets to weather, and the analysis of ship movement patterns. Featured speakers Dr. Mike Flaxman, Spatial Data Science Practice Lead and Abhishek Damera, Data Scientist at OmiSci will discuss:\n\n*  LIDAR, GPS cell phones and \u2018cube\u2019 sats\n* Machine Learning methods applied to #geodata which can classify patterns of movement or pixels or #geoML.\n* Modern computing architectures which move algorithms to data and stream highly-distilled information\n\nInfo: [https://events.cognilytica.com/CLNTY0NHwyNA](https://events.cognilytica.com/CLNTY0NHwyNA)\n\nWhat do you think? I think a macroscope perspective sounds interesting.", "link": "https://www.reddit.com/r/datascience/comments/p6u8na/webinar_the_macroscope_initiative_building/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "webinar: the macroscope initiative - building planetary geoml with omnisci with q&amp;a /!/ hey everyone! i'm planning on attending this webinar 'the macroscope initiative - building planetary geoml with omnisci with q&amp;a'. \n\nlearn how the #macroscope concept can be applied to the monitoring of -----> tree !!!  health, the exposure of static and moving assets to weather, and the analysis of ship movement patterns. featured speakers dr. mike flaxman, spatial data science practice lead and abhishek damera, data scientist at omisci will discuss:\n\n*  lidar, gps cell phones and \u2018cube\u2019 sats\n* machine learning methods applied to #geodata which can classify patterns of movement or pixels or #geoml.\n* modern computing architectures which move algorithms to data and stream highly-distilled information\n\ninfo: [https://events.cognilytica.com/clnty0nhwyna](https://events.cognilytica.com/clnty0nhwyna)\n\nwhat do you think? i think a macroscope perspective sounds interesting.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('self',)", "medialink": "('https://www.reddit.com/r/datascience/comments/p6u8na/webinar_the_macroscope_initiative_building/',)", "identifyer": 5597850, "year": "2021"}, {"autor": "Hipocampus777", "date": 1629302301000, "content": "1-hour live technical task in Python for a DS assessment day /!/ Hi. Tomorrow I have my first graduate DS interview(virtually) and there will also be a 1-hour live technical assessment in a jupyter notebook in Python. Given this time limit and the fact that it's a graduate role, what task should I expect? I guess it's not going to be ML model building, as this would normally require more? Anyways, I'm sure I should be expecting anything, but what is your guess?\n\nThe role is for an insurance company and some person specifications details were \"experience of programming in Python or R, Knowledge of machine learning techniques including linear models, tree based methods and Deep Learning is beneficial\"\n\nThey will also have a group activity. What is a typical example of this type of assessment for a DS position? Thank you.", "link": "https://www.reddit.com/r/datascience/comments/p6u269/1hour_live_technical_task_in_python_for_a_ds/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "1-hour live technical task in python for a ds assessment day /!/ hi. tomorrow i have my first graduate ds interview(virtually) and there will also be a 1-hour live technical assessment in a jupyter notebook in python. given this time limit and the fact that it's a graduate role, what task should i expect? i guess it's not going to be ml model building, as this would normally require more? anyways, i'm sure i should be expecting anything, but what is your guess?\n\nthe role is for an insurance company and some person specifications details were \"experience of programming in python or r, knowledge of machine learning techniques including linear models, -----> tree !!!  based methods and deep learning is beneficial\"\n\nthey will also have a group activity. what is a typical example of this type of assessment for a ds position? thank you.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/p6u269/1hour_live_technical_task_in_python_for_a_ds/',)", "identifyer": 5597851, "year": "2021"}, {"autor": "vietlinh12hoa", "date": 1628005596000, "content": "Would it make sense to have random forest based-xgboost? /!/ Random forest is the aggregation of random decision trees with random feature selection. Can we just replace decision tree by xgboost. E.g.: Build a random xgboost with random feature selection. Then aggregation those boosted trees to have a forest.", "link": "https://www.reddit.com/r/datascience/comments/ox66sz/would_it_make_sense_to_have_random_forest/", "origin": "Reddit", "suborigin": "datascience", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "would it make sense to have random forest based-xgboost? /!/ random forest is the aggregation of random decision trees with random feature selection. can we just replace decision -----> tree !!!  by xgboost. e.g.: build a random xgboost with random feature selection. then aggregation those boosted trees to have a forest.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/datascience/comments/ox66sz/would_it_make_sense_to_have_random_forest/',)", "identifyer": 5598260, "year": "2021"}], "name": "treedatascience2021"}
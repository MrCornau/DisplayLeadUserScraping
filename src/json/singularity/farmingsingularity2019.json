{"interestingcomments": [{"autor": "-hillbean", "date": 1567034960000, "content": "Existential obsessions /!/ Hi, I\u2019m somewhat new to reddit so I hope some people see this. I\u2019m going through probably the worst mental health patch I\u2019ve had in my 28 years. It started when I got out of a bad relationship where I was basically completely financially supporting someone. It was so much pressure and this person would get kicked out of where they were living so I\u2019d have to find them somewhere else to live, and they couldn\u2019t find a job and the few times they did they got fired. This went on for a year, and I was afraid to leave this person because they\u2019d end up homeless, threaten to kill them selves, etc. \n\nSo I finally made a decision that enough was enough and gave this person what money I had left to give and broke off contact. \n\nI didn\u2019t have much time to get acclimated to just being responsible for myself and enjoy it though. Not long after I started getting panic attacks about existential questions like the odd nature of our reality if you really think about it. I\u2019ve always been a deep thinker, but I couldn\u2019t shake the thoughts, or the panic they were now suddenly causing, and I started having derealization as well. The two paired together were the hardest thing I had went through. But my doctor prescribed me lexapro 5 mg but it was too much because of the initial side effects, so I had to ween down to 2.5 for a week or so, then 5, and I just went up to 7 mg a few days ago. I also do see a therapist about once a month. Things started to get better. \n\nRecently though,  I watched the Joe Rogan podcast episode with Elon Musk and it freaked me the hell out. I started thinking about simulation theory, and being in a simulation itself isn\u2019t what scares me, it\u2019s the thought that maybe the being that is behind it is perhaps not as benevolent as I had always believed a creator would be. Then I thought if this world is simulated, then what if there\u2019s a simulated heaven, or a simulated hell? And the way my mind works I just automatically assumed I\u2019m definitely going to that hell. I was raised in a religious household and have always been afraid of hell. So now the possibility of it has surfaced again via simulation hypothesis. I started evaluating humanity and thinking of all the fucked up things we do. Like why wouldn\u2019t we be punished is how I felt. Look at how we treat animals? Why would a higher being treat us better? In radically evaluating my morals I stopped eating meat the last few days because I figured factory farming is abhorrent, and it is about the only suffering I contribute to that I can think of. I\u2019m am really not eating much of anything. \n\nSo in that line of thought of what higher beings would think of us, I started thinking about the inevitable rise of AI and the myriad of unimaginable ways that it could not only destroy us, but make us suffer, and because it can perhaps interfere with our mortality, maybe this suffering again, would never end. \n\nI\u2019m not scared of dying, if it\u2019s either a peaceful place we go or just nothing. But I seem to be very preoccupied with these terrifying concepts. I work from home, and I literally can only bring myself to work and shower. That\u2019s about it. And I try not to tell my mom because she has anxiety too, and she\u2019s a sweet middle aged lady who doesn\u2019t really understand these things, angel that she is. I have a rational side that knows there\u2019s literally nothing I can do about these things even if they were to be a reality, but anxiety and obsessive thoughts can\u2019t be reasoned with much. And I really don\u2019t talk to anyone on a daily basis, so I feel very alone. I\u2019m suffering quite a bit and I\u2019ve had some suicidal thoughts but I\u2019d never, ever do that to my mom. I just wish I could find some peace. That\u2019s why I\u2019m reaching out to anyone who might take the time to read this, if you\u2019ve been through something similar, any advice, etc. \n\nI\u2019m having very real intrusive thoughts about the future having such horrific possibilities of suffering, and that suffering continuing after death. And in turn, I\u2019m suffering now. \n\nI sincerely appreciate any responses I get, thank you.", "link": "https://www.reddit.com/r/singularity/comments/cwspdk/existential_obsessions/", "origin": "Reddit", "suborigin": "singularity", "result": true, "Selector": "farming", "selectorShort": "farming", "MarkedSent": "existential obsessions /!/ hi, i\u2019m somewhat new to reddit so i hope some people see this. i\u2019m going through probably the worst mental health patch i\u2019ve had in my 28 years. it started when i got out of a bad relationship where i was basically completely financially supporting someone. it was so much pressure and this person would get kicked out of where they were living so i\u2019d have to find them somewhere else to live, and they couldn\u2019t find a job and the few times they did they got fired. this went on for a year, and i was afraid to leave this person because they\u2019d end up homeless, threaten to kill them selves, etc. \n\nso i finally made a decision that enough was enough and gave this person what money i had left to give and broke off contact. \n\ni didn\u2019t have much time to get acclimated to just being responsible for myself and enjoy it though. not long after i started getting panic attacks about existential questions like the odd nature of our reality if you really think about it. i\u2019ve always been a deep thinker, but i couldn\u2019t shake the thoughts, or the panic they were now suddenly causing, and i started having derealization as well. the two paired together were the hardest thing i had went through. but my doctor prescribed me lexapro 5 mg but it was too much because of the initial side effects, so i had to ween down to 2.5 for a week or so, then 5, and i just went up to 7 mg a few days ago. i also do see a therapist about once a month. things started to get better. \n\nrecently though,  i watched the joe rogan podcast episode with elon musk and it freaked me the hell out. i started thinking about simulation theory, and being in a simulation itself isn\u2019t what scares me, it\u2019s the thought that maybe the being that is behind it is perhaps not as benevolent as i had always believed a creator would be. then i thought if this world is simulated, then what if there\u2019s a simulated heaven, or a simulated hell? and the way my mind works i just automatically assumed i\u2019m definitely going to that hell. i was raised in a religious household and have always been afraid of hell. so now the possibility of it has surfaced again via simulation hypothesis. i started evaluating humanity and thinking of all the fucked up things we do. like why wouldn\u2019t we be punished is how i felt. look at how we treat animals? why would a higher being treat us better? in radically evaluating my morals i stopped eating meat the last few days because i figured factory -----> farming !!!  is abhorrent, and it is about the only suffering i contribute to that i can think of. i\u2019m am really not eating much of anything. \n\nso in that line of thought of what higher beings would think of us, i started thinking about the inevitable rise of ai and the myriad of unimaginable ways that it could not only destroy us, but make us suffer, and because it can perhaps interfere with our mortality, maybe this suffering again, would never end. \n\ni\u2019m not scared of dying, if it\u2019s either a peaceful place we go or just nothing. but i seem to be very preoccupied with these terrifying concepts. i work from home, and i literally can only bring myself to work and shower. that\u2019s about it. and i try not to tell my mom because she has anxiety too, and she\u2019s a sweet middle aged lady who doesn\u2019t really understand these things, angel that she is. i have a rational side that knows there\u2019s literally nothing i can do about these things even if they were to be a reality, but anxiety and obsessive thoughts can\u2019t be reasoned with much. and i really don\u2019t talk to anyone on a daily basis, so i feel very alone. i\u2019m suffering quite a bit and i\u2019ve had some suicidal thoughts but i\u2019d never, ever do that to my mom. i just wish i could find some peace. that\u2019s why i\u2019m reaching out to anyone who might take the time to read this, if you\u2019ve been through something similar, any advice, etc. \n\ni\u2019m having very real intrusive thoughts about the future having such horrific possibilities of suffering, and that suffering continuing after death. and in turn, i\u2019m suffering now. \n\ni sincerely appreciate any responses i get, thank you.", "sortedWord": "None", "removed": "('nan',)", "score": 3, "comments": 13, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/singularity/comments/cwspdk/existential_obsessions/',)", "identifyer": 5585158, "year": "2019"}, {"autor": "adam_ford", "date": 1551360474000, "content": "Robin Hanson on AI Takeoff Scenarios - AI Go Foom? /!/ We discuss the feasibility &amp; desirability of two AI takeoff scenarios (soft/hard), consider the historical and current degree of lumpiness of services in order to help gauge the likelihood of a soft or hard takeoff and discuss how to think about them.  \n\n# Interview link here : [https://www.youtube.com/watch?v=qk3bQrSfUzs](https://www.youtube.com/watch?v=qk3bQrSfUzs)\n\nOther stuff discussed:\n\n&amp;#x200B;\n\nRobin Hanson\u2019s idea of \u2018constant elasticity of substitution (CES)\u2019 - 3 exponential growth modes: Hunting.. substituted by Farming.. substituted by  Industry - each growing world product by a few hundred, and sprouted a hundred times faster than its predecessor - weakly suggesting within a century another a new mode might appear with a doubling time measured in days (not years). - [https://mason.gmu.edu/\\~rhanson/longgrow.pdf](https://mason.gmu.edu/~rhanson/longgrow.pdf)\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nEric Drexler\u2019s : Reframing Superintelligence: Comprehensive AI Services as General Intelligence [https://www.fhi.ox.ac.uk/reframing/](https://www.fhi.ox.ac.uk/reframing/)\n\n&amp;#x200B;\n\nAnders Sandberg/Carl Shulman (2010) believe that if a singularity happens early, it's more likely to be a soft takeoff - bottlenecks in AI like algorithmic improvements / inventions require research &amp; work efforts that don\u2019t scale well just by adding more researchers in limited time (10k people for 1 year won\u2019t beat 1k people working for 10 yrs) - so should expect slow, intermittent and possibly unpredictable progress. - [http://www.aleph.se/andart/archives/2010/10/why\\_early\\_singularities\\_are\\_softer.html](http://www.aleph.se/andart/archives/2010/10/why_early_singularities_are_softer.html)\n\n&amp;#x200B;\n\nThe AI Foom debate between Eliezer Yudkowski &amp; Robin Hanson\n\nIn late 2008, economist Robin Hanson and AI theorist Eliezer Yudkowsky conducted an online debate about the future of artificial intelligence, and in particular about whether generally intelligent AIs will be able to improve their own capabilities very quickly (a.k.a. \u201cfoom\u201d). James Miller and Carl Shulman also contributed guest posts to the debate.\n\n \n\nThe original debate took place in a long series of blog posts, which are collected here. This book also includes a transcript of a 2011 in-person debate between Hanson and Yudkowsky on this subject, a summary of the debate written by Kaj Sotala, and a 2013 technical report on AI takeoff dynamics (\u201cintelligence explosion microeconomics\u201d) written by Yudkowsky.\n\nThere is an AI Foom Debate ebook here: [https://intelligence.org/ai-foom-debate/](https://intelligence.org/ai-foom-debate/)\n\n&amp;#x200B;\n\n&amp;#x200B;", "link": "https://www.reddit.com/r/singularity/comments/avr4kw/robin_hanson_on_ai_takeoff_scenarios_ai_go_foom/", "origin": "Reddit", "suborigin": "singularity", "result": true, "Selector": "farming", "selectorShort": "farming", "MarkedSent": "robin hanson on ai takeoff scenarios - ai go foom? /!/ we discuss the feasibility &amp; desirability of two ai takeoff scenarios (soft/hard), consider the historical and current degree of lumpiness of services in order to help gauge the likelihood of a soft or hard takeoff and discuss how to think about them.  \n\n# interview link here : [https://www.youtube.com/watch?v=qk3bqrsfuzs](https://www.youtube.com/watch?v=qk3bqrsfuzs)\n\nother stuff discussed:\n\n&amp;#x200b;\n\nrobin hanson\u2019s idea of \u2018constant elasticity of substitution (ces)\u2019 - 3 exponential growth modes: hunting.. substituted by -----> farming !!! .. substituted by  industry - each growing world product by a few hundred, and sprouted a hundred times faster than its predecessor - weakly suggesting within a century another a new mode might appear with a doubling time measured in days (not years). - [https://mason.gmu.edu/\\~rhanson/longgrow.pdf](https://mason.gmu.edu/~rhanson/longgrow.pdf)\n\n&amp;#x200b;\n\n&amp;#x200b;\n\neric drexler\u2019s : reframing superintelligence: comprehensive ai services as general intelligence [https://www.fhi.ox.ac.uk/reframing/](https://www.fhi.ox.ac.uk/reframing/)\n\n&amp;#x200b;\n\nanders sandberg/carl shulman (2010) believe that if a singularity happens early, it's more likely to be a soft takeoff - bottlenecks in ai like algorithmic improvements / inventions require research &amp; work efforts that don\u2019t scale well just by adding more researchers in limited time (10k people for 1 year won\u2019t beat 1k people working for 10 yrs) - so should expect slow, intermittent and possibly unpredictable progress. - [http://www.aleph.se/andart/archives/2010/10/why\\_early\\_singularities\\_are\\_softer.html](http://www.aleph.se/andart/archives/2010/10/why_early_singularities_are_softer.html)\n\n&amp;#x200b;\n\nthe ai foom debate between eliezer yudkowski &amp; robin hanson\n\nin late 2008, economist robin hanson and ai theorist eliezer yudkowsky conducted an online debate about the future of artificial intelligence, and in particular about whether generally intelligent ais will be able to improve their own capabilities very quickly (a.k.a. \u201cfoom\u201d). james miller and carl shulman also contributed guest posts to the debate.\n\n \n\nthe original debate took place in a long series of blog posts, which are collected here. this book also includes a transcript of a 2011 in-person debate between hanson and yudkowsky on this subject, a summary of the debate written by kaj sotala, and a 2013 technical report on ai takeoff dynamics (\u201cintelligence explosion microeconomics\u201d) written by yudkowsky.\n\nthere is an ai foom debate ebook here: [https://intelligence.org/ai-foom-debate/](https://intelligence.org/ai-foom-debate/)\n\n&amp;#x200b;\n\n&amp;#x200b;", "sortedWord": "None", "removed": "('nan',)", "score": 2, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/singularity/comments/avr4kw/robin_hanson_on_ai_takeoff_scenarios_ai_go_foom/',)", "identifyer": 5585657, "year": "2019"}], "name": "farmingsingularity2019"}
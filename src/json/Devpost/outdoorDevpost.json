{"interestingcomments": [{"Unnamed: 0": 473, "autor": "Unremarkable project IXO wallet", "date": null, "content": "Inspiration\nWe want to take the power back from Web2 monopolies and hand it back to users by empowering them with sovereign tools. To achieve this, we've built a Web3-native operating system and we're designing hardware to deeply integrate with the software to provide the first true alternative to the Apple/Google duopoly. We wanted to integrated the ixo project to enable to our users to participate in the Internet of Impact.\nWhat it does\nSee for yourself in the photos/videos :) But in summary, the device itself is a kindle-ipad hybrid. Think of an ipad with a kindle screen that refreshes at the same speed as an ipad screen. Think of an ipad with the long battery life of a kindle. Think of an ipad with an open operating system that integrates Web3 primitives and enables self-sovereignty and exit from the centralized internet.\nCrucially the novel screen technology allows use in the field - visible in direct sunlight & 1-2 week battery life even in the bright sun.\nThe combination of long battery life, offline first architecture, durability to drops, and visibility in direct sunlight makes it hardware suited for outdoor classrooms and the developing world.\nHow we built it\nWe forked the ixo Keysafe extension and made it compatible with our device/OS.\nChallenges we ran into\nThe desktop \"pop-up\" nature of the Keysafe extension presented a challenge. We also had rendering issues that made it challenging to use & work on mobile.\nAccomplishments that we're proud of\nKeysafe installed, wallet created, running, and signing transactions on the app-uat.ixo.world on a mobile device. Mobile wallet extension integrations are rare, novel, and mostly lacking:\n.. we feel proud to bring the ability of extensible wallets to mobile devices! Crypto has mostly been stuck to the desktop.\nWhat we learned\nWe learnt about integrating Cosmos-based wallets into our OS, and translating desktop technologies to mobile, and figuring out the primitives and flows of the IXO SDK & webapps.\nWhat's next for Unremarkable project\nThis is just a proof of concept and we have many ideas on how to evolve from here. We will iterate on this integration, polish off the UI/UX, make changes to our fork to optimize the rendering size, and eventually include the IXO Keysafe wallet as one of the default wallets on our device. Deeper integrations can include smoother UX for micropayments, DID authentication & signing, claims etc.\nUser Story: Rahul in Kashmir, India is 6 years old and is learning to read using digital books on the Unremarkable tablet while in his outdoor classroom, and uses the inbuilt IXO wallet with his DID to sign claims.", "link": "https://devpost.com/software/unremarkable-project", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "outdoor", "selectorShort": "outdoor", "MarkedSent": "inspiration\nwe want to take the power back from web2 monopolies and hand it back to users by empowering them with sovereign tools. to achieve this, we've built a web3-native operating system and we're designing hardware to deeply integrate with the software to provide the first true alternative to the apple/google duopoly. we wanted to integrated the ixo project to enable to our users to participate in the internet of impact.\nwhat it does\nsee for yourself in the photos/videos :) but in summary, the device itself is a kindle-ipad hybrid. think of an ipad with a kindle screen that refreshes at the same speed as an ipad screen. think of an ipad with the long battery life of a kindle. think of an ipad with an open operating system that integrates web3 primitives and enables self-sovereignty and exit from the centralized internet.\ncrucially the novel screen technology allows use in the field - visible in direct sunlight & 1-2 week battery life even in the bright sun.\nthe combination of long battery life, offline first architecture, durability to drops, and visibility in direct sunlight makes it hardware suited for -----> outdoor !!!  classrooms and the developing world.\nhow we built it\nwe forked the ixo keysafe extension and made it compatible with our device/os.\nchallenges we ran into\nthe desktop \"pop-up\" nature of the keysafe extension presented a challenge. we also had rendering issues that made it challenging to use & work on mobile.\naccomplishments that we're proud of\nkeysafe installed, wallet created, running, and signing transactions on the app-uat.ixo.world on a mobile device. mobile wallet extension integrations are rare, novel, and mostly lacking:\n.. we feel proud to bring the ability of extensible wallets to mobile devices! crypto has mostly been stuck to the desktop.\nwhat we learned\nwe learnt about integrating cosmos-based wallets into our os, and translating desktop technologies to mobile, and figuring out the primitives and flows of the ixo sdk & webapps.\nwhat's next for unremarkable project\nthis is just a proof of concept and we have many ideas on how to evolve from here. we will iterate on this integration, polish off the ui/ux, make changes to our fork to optimize the rendering size, and eventually include the ixo keysafe wallet as one of the default wallets on our device. deeper integrations can include smoother ux for micropayments, did authentication & signing, claims etc.\nuser story: rahul in kashmir, india is 6 years old and is learning to read using digital books on the unremarkable tablet while in his outdoor classroom, and uses the inbuilt ixo wallet with his did to sign claims.", "sortedWord": "None", "removed": "Nan", "score": 7, "comments": 0, "media": null, "medialink": null, "identifyer": 59500473}, {"Unnamed: 0": 588, "autor": "AutoFarmer", "date": null, "content": "Inspiration\nTaking care of other living things is hard [citation needed]. In the case of plants, a consistent routine is necessary to give the plant what it needs, these needs differ by plant, and long term planning is required to keep it alive even when away. As such, there exists a market for taking care of plants automatically. There are many current products to meet this need, but they all fall short in different ways. One possibility is slow-drip mechanisms that slowly dispense water into a plant\u2019s pot. However, this cannot be changed to account for the plant\u2019s individual needs, and the amount of water it can store is not high enough to be able to keep a plant alive for an extended vacation. Alternatively, there exist timed plant watering devices that water a plant every day. However, limited adjustability in timing and access to data means that this strategy is restricted in other ways. Our product exceeds the utility of all these other devices, while also providing invaluable logged data and water tracking to the user. Given the utility of our device, and the fact that on the order of 33 million households participate in indoor gardening/houseplant ownership, paired with the fact that our product likely could be sold for somewhere on the order of $20 (and likely much less were we to manufacture this at scale), the expected number of users could very well be in the millions of households. All these factors considered, we believe this project can fill a definitive niche in the market as it currently stands.\nWhat it does\nPut simply, our project is an automatic plant caretaker: it intelligently and effectively cares for a single plant by watering it according to a combination of the plant\u2019s needs and environmental effects. The Auto-Farmer dispenses water by means of a peristaltic pump, while rotating the plant to give an even watering. The amount of water dispensed is a function of the plant\u2019s individual needs as specified by the owner, the environment (temperature, humidity), and the weather forecast (rain chance, foreseen temperature, etc.) The watering is either done automatically every t seconds (where t is defined by the user, generally once per day is sufficient), or when the user presses a button on an app. Lastly, continuously during operation, the device sends information to the user to help them keep track of watering trends, environmental patterns, and other long-term effects on their plants. With this system of sensors and effectors, combined with the ability to keep the user informed, the Auto-Farmer makes the home gardening experience much more easy and enjoyable.\nHow To Use It\nUser Interface Mechanical Setup:\nPlace the Auto-Farmer where the plant is intended to be kept (taking care to provide appropriately decide on lighting, indoor/outdoor, etc according to the specific plant)\nPut the potted plant on the turntable. There is a wall around the table to help better center the pot on the table and allow for even watering centered on the stem.\nFill the water reservoir, then place the long end of the peristaltic pump tubing inside it (with its end touching the bottom of the reservoir to minimize refilling).\nElectrical/Software Setup:\nIn the source code for the Auto-Farmer, input the desired parameters for the plant\u2019s individual water need (cactuses need less than ferns, for instance), the watering period (default should be 1 day between watering), sensor VS weather report weight (weight the sensor higher if inside, the weather report more if outside), data report period (amount of time between logging a data point), and calibrated \u201cseconds-per-mL\u201d pump constant (optional, but allows to precisely tune the device to the specific pump hardware on hand)\nIn the source code, input the latitude and longitude of the place in which you live in the lat and lon sections (Note: In a consumer product, we\u2019d likely build a database or user interface to make this selection easier; for demonstration purposes, we have pre-loaded latitude/longitude data for Berkeley).\nConnect the computer on which the source code is being run to the ESP32, use shell49 to interface with the ESP32, and connect it to the internet\nThe source code can be copied to the ESP32 using the cp command, or if preferred it can simply be run while connected to the computer (running while connected to the computer will allow for status printouts, but those can also be seen using the MQTT protocol). The commands to set up the ESP32 and the Auto-Farmer program are as follows:\n>>>shell49\n>>>run boot.py\n>>>run source_code.py\nIf you would like to remotely monitor your plant, run the following command to open the data plotter once the other scripts are running (note you can do this on any Internet-connected computer where the data plotter script exists; it gets data through the MQTT protocol from the Auto-Farmer):\n>>>run data_plotter.py\nIf you would like to remotely water your plant manually, use the provided IFTTT application on any device to do so (press the button).\nActuation\nTurntable DC Motor:\nIn order to evenly water the plant, we considered many ideas, including sprinkler heads, a moving water dispenser, or letting water seep in through the bottom of a pot. The design we decided on, however, uses a turntable under the plant to allow for the water to be dispensed in a circular pattern around the plant\u2019s stem. To actuate this turntable, we used the Pololu Micro Metal Gearmotor (lab kit standard-issue) with its corresponding H-bridge motor controller. The motor controller--despite being an H-bridge--is simply used as a voltage regulator, as we do not need bidirectionality in our application. We also experimented with PWM signals to regulate speed of the turntable motor, but eventually we realized that given the gearing, we were unlikely to need anything less than full power at any given time. As such, all following calculations will be using the motor run at maximum power, as in practice this is how we run our device. When unloaded, the motor we used runs at 410 RPM with a current of 100 mA. However, this was significantly too fast for our purposes, so we decided to gear it down with a 25:1 gear ratio for a new speed of 16.4 RPM (0.27 rev/sec). With a 1.3 Kg*cm max torque with the ungeared motor, we now could get a 1.3 * 25 = 32.5 Kg*cm max torque which, given the radius of the platform\u2019s 6.3 cm radius, could allow for a 5.16 Kg lateral force on the rotating platform (much less than the plant weighs, even if the force were directed completely sideways and off-center). In total, for each day of operation--assuming the plant is watered once a day for 5 minutes (far more than most plants will ever need)--the motor draws 300s*0.1A*12V = 360 Watt*seconds = 360 J per day of operation, only drawing 1.2 Watts at any given time [Pololu]. This is a negligible amount of power compared to the amount households use (~30 KWH per day), which easily fits our design criteria [US Energy Information Administration].\nPeristaltic Pump:\nTo transport water from the reservoir to the plant, we initially considered two options: a solenoid+siphon system, and a pump. Through experimentation, we quickly found that the solenoid+siphon system would require a sizable build envelope to build up sufficient pressure head for our desired flow rate through the solenoid valve, and so we opted for a pump instead. Specifically, we were looking for a compact and robust pump, which could reliably and precisely deliver relatively small quantities of water at a time. For that purpose, the Adafruit 12V peristaltic pump was perfect. Essentially, the pump converts rotary motion from the motor into liquid flow by progressively squeezing an interior hose, pushing small \u201cpackets\u201d of water through the system and out the other side. In this way, no moving components actually ever come into contact with the liquid itself, resulting in uncontaminated water, and zero mineral buildup/wear from the liquid-pump interface. Requiring 200-300 mA at 12V (thus requiring between 2.4 and 3.6 Watts during operation), the pump is capable of pumping up to 100 mL/minute [Adafruit], though that value is somewhat variable from pump to pump, and required calibration on our part. Taken together, this actuator checks off all the boxes: adequate and reliable flow rate, enough pressure head to pull water out of the reservoir and into the potted plant, small size for a compact device, and minimal wear/need for repair.\nSensing\nThe sensor our project uses is the DHT11 Humidity and Temperature sensor from Adafruit. This sensor detects both humidity and temperature and sends them down a single serial line. According to the datasheet, the sensor outputs a temperature between 0 and 50 degrees Celsius with an accuracy of \u00b12 degrees, and a humidity between 20% and 90% with an accuracy of \u00b15%. The resolution of the sensor is 1, so an integer value is returned for both temperature and humidity, while the repeatability of the sensor is \u00b11%. While this is definitely on the lower end in performance, our device does not rely on precise measurements, only general tendencies of the outdoor weather/indoor microclimate, so this sensor was deemed adequate.\nThe more important benefit of the sensor is its low cost (around $4) and low power consumption. Before a reading is needed, the sensor\u2019s pull up resistor gives it an output equal to its 5V VDD input. However, the residual current flowing through in this standby-state is ~100 uA, requiring negligible input power. When a reading is needed, the sensor outputs a 40 bit signal: 16 bit temperature reading, 16 bit humidity reading, and 8 bit validation (checksum). Note however that only 8 bits of each of the temperature and humidity readings are integral (as opposed to another 8 bits of decimal data, which are always zero due to hardware limitations), thus explaining the low resolution despite having many more available bits in each serial data package to work with. After this data is sent, the sensor resumes lower power mode, only having been in high power (active) mode for 160 \u03bcs (the start of the signal is a short low-followed-by-high voltage signal to indicate that the data transmission is starting) + 40 * 50 \u03bcs (each of the bits has a 50 \u03bcs low-voltage signal directly preceding it) for a total of 2.16 ms [D-Robotics]. With this low power consumption and low cost, combined with the ability to read both temperature and humidity , this sensor was the perfect one for our needs.\nSoftware\nIn designing the software to control the vehicle, care was taken to appropriately divide the processing between onboard and offboard computing. Of course, as a general principle, onboard computing can operate with far less latency, but has very limited computing power; offboard computers are the exact opposite. For our purposes, we have very limited computation (a few simple arithmetic operations every time it waters a plant), with the longest-latency operation being an Internet access (which would be necessary to interface with MQTT anyways). What\u2019s more, in a real deployment of this device we\u2019d only realistically need to do computations once every couple minutes (and likely much longer than that). In that case, speed is not a huge concern, especially compared with the latency and potential failure point of relying on an external host computer. As such, we deemed that offboard computing would be unnecessary, and that it made more sense to do the computation on board.\nThe software codebase for the device consists of five main components: one that makes sensor readings/Internet accesses (onboard), one that calculates quantity of water and waters the plant (onboard), one that logs data periodically and sends it to the cloud (onboard), one that interfaces with an IFTTT manual control option (onboard), and one that receives data from the cloud and plots it in a user-friendly interface to allow for monitoring of environmental data and watering trends (offboard). These are outlined in more detail below:\nData Collection\nTo access the data from the DHT sensor, we simply used the built-in DHT class in ESP32 micropython to interpret the serial data into temperature and humidity readings. To access Internet weather data, on the other hand, is a far more involved task. Because of the difficulties reading IFTTT weather data, we opted to use OpenWeatherMap, an open source weather data API. Essentially, we made accounts, received uniquely identifying \u201ckeys\u201d, and then used these to grab data from OpenWeatherMap\u2019s sources given our input latitude and longitude. The site then returned a massive JSON file containing weather data for the next seven days which we subsequently turned into an equally massive multi-layer hashmap. Finally, we indexed into the hashmap using commands like data[\u201cdaily\u201d][\u201c0\u201d][\u201cpop\u201d] where \u201cdata\u201d is the name of the outer map, \u201cdaily\u201d means we search for daily data as opposed to weekly or hourly data, \u201c0\u201d for the index of the day, and \u201dpop\u201d for probability of precipitation. Notably, because we only pull this data when the data-tracker requires, power consumption/computation is not much of a concern (we use interrupts instead of constant polling).\nWatering Logic\nIn order to properly leverage all the data that our device is collecting, both from the sensors and from the Internet, an intelligently-designed watering logic was developed. First, we enumerate the inputs for our function:\nW: Nominal Amount of water per period*\nK: Sensor-Internet Bias parameter*\nsensor_temp: Sensor Temperature\nhumidity: Sensor Humidity\ncurr_temp: Today\u2019s Weather Report Temperature\ncurr_prec: Today\u2019s Weather Report Probability of Precipitation (POP)\ntom_temp: Tomorrow\u2019s Weather Report Temperature\ntom_prec: Tomorrow\u2019s Weather Report Probability of Precipitation (POP)\nUser-specified parameter, see the \u201cUSER INTERFACE\u201d section for more details.\nEssentially, the function is a product of linear functions of each of these parameters, which is used to weight the nominal amount of water W. The whole function takes this form (comments inserted below each line for design justification):\navg_temp = K*sensor_temp + (1-K)*(curr_temp+tom_temp)/2\nFirst, we take a simple average of today\u2019s and tomorrow\u2019s weather forecast temperature\nNext, we take a weighted (specified by K) average of the sensor-read temperature and that average weather forecast term\navg_prec = 1 - (1-curr_prec)*(1-tom_prec)\nCalculate the probability that there will be precipitation either today or tomorrow.\nweight = (avg_temp/30 + 1/3) * (1-avg_prec) * (1.5-0.01*humidity)\nNote: When weights = 1, all conditions are nominal (20\u00b0C, 0% POP, 50% humidity)\nWater more when it\u2019s hotter: For the temperature weight factor T, we want T = 1 at 20\u00b0C and T = 1.5 at T = 35\u00b0C, linearly interpolate between those two points\nWater less when it\u2019s more likely to precipitate: For the POP weight factor P, we want P = 1 at 0% POP and P = 0 at 100% POP, linearly interpolate between those two points\nWater less when it\u2019s more humid: For the humidity weight factor H, we want H = 1 at 50% humidity and H = 1.5 at 0% humidity, linearly interpolate between those two points\nwater_amount = weight * W\nWeight the nominal amount of water by the product of the weights calculated in the previous line.\nThis water_amount quantity we calculate is then converted into a time to run the peristaltic pump at full power, which has been calibrated with volumetric flow rate testing. Thus, the plant has received a dose of water, based on plant-specific factors (ie how much water a plant should nominally receive, as well as how frequently it should be watered), real-time environmental factors (eg temperature, humidity), and forecasted weather (ie next-day temperature and precipitation). With the automatic timer, this quantity will be delivered periodically for as long as the device is on.\nData Logging\nBesides watering the plant, the device also packages the data it collects from sensing hardware and the Internet weather report, and sends it into the cloud via its data logging functionality. Specifically, the device tracks and logs sensor-read temperature/humidity, current Internet-provided temperature/POP, and future Internet-provided temperature/POP. The ESP32 code is equipped with a timer, which makes a sensor reading and accesses Internet weather data every datalogging period (a user-defined amount of time), and sends it to an MQTT channel to be processed. Additionally, at each watering stage, a sensor reading/Internet request is made, and that data, along with the amount watered, is also packaged and sent to the same MQTT data channel.\nIFTTT-Powered Manual Control\nTo implement a manual plant-watering functionality, we simply use a separate MQTT command channel to issue commands to water. Every time a command is received, an interrupt is triggered, which immediately delivers the recommended dose of water (as determined by the same logic as laid out in the \u201cWatering Logic\u201d section above), This event also overrides the next automatic watering event, before resuming with the automatic watering cycle after another period. For a user-friendly interface, sending this manual-watering MQTT signal is simply triggered by the push of a button on the IFTTT app.\nExternal Data Plotting\nOnce that data is sent to the MQTT feed, a separate script, untethered from the ESP32, processes that data and plots it in a user-friendly interface. Through these real-time updating plots, a user can track the conditions the plant is experiencing, as well as the amount of water the plant has received over time. This enables intelligent tweaking of the user-defined parameters, allowing for maximized customizability of the care one can provide for one\u2019s plant.\nOur design relies quite heavily on the Adafruit MQTT broker (using it for both manual commands and for data-logging/status reports) and Internet access for weather reports. As such, latency is a primary concern. Empirically, based on our network configuration setup, we experienced delays of around 5-7 seconds for a MQTT message to propagate from the IFTTT app, through to the MQTT feed, and then trigger an ESP32 interrupt. The Internet access to weather data takes 2-3 seconds on top of that. Overall, given that it is not critical to water absolutely instantaneously in this application, these delays are perfectly serviceable for our purposes. The device\u2019s utility lies mainly in its long-term planning and data records, as opposed to instant response to external stimuli, making it the perfect application for these intuitive and user-friendly MQTT/IFTTT software tools.\nWhat's next for AutoFarmer\nOverall, the part of the code that could be improved the most is the watering logic algorithm. In its current state, many of the parameters must be input by the user, and adjustment factors are based mostly on an overall average of so-called \u201cnominal conditions\u201d. As such, this algorithm would need to undergo serious review and user testing to ensure that the prescribed watering dose is appropriate and sensible, given the input data. For better results, it may likely be worthwhile to compile a database of plant preferred watering cycles/quantities (which could be accessed from the cloud upon setup), to make the user experience as straightforward and intuitive as possible. Other future extensions include refining the watering logic, utilizing more future weather data points, and adding additional sensor functionality (e.g. soil moisture sensor).\nConclusion\nWhile taking care of plants is often a fun and relaxing hobby, it can also be equal parts drudgery and frustration, especially when it comes to meeting many varied plants\u2019 needs. With its economical design and intelligent software, the Auto-Farmer takes the work out of home gardening and allows the user to focus on the more interesting parts of gardening. The actuators allows for the plants to be watered effectively even when the owner is not attentive or present, the sensor/weather data allows for a highly tailored watering for the plants, and the data logging gives the plant owner a much more informed perspective on keeping their plants healthy and happy, both in the short- and long-term. From the avid plant connoisseur to the simple tasteful desk-plant owner, we sincerely hope you consider the Auto-Farmer for your plant caretaking needs.", "link": "https://devpost.com/software/autofarmer", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "outdoor", "selectorShort": "outdoor", "MarkedSent": "inspiration\ntaking care of other living things is hard [citation needed]. in the case of plants, a consistent routine is necessary to give the plant what it needs, these needs differ by plant, and long term planning is required to keep it alive even when away. as such, there exists a market for taking care of plants automatically. there are many current products to meet this need, but they all fall short in different ways. one possibility is slow-drip mechanisms that slowly dispense water into a plant\u2019s pot. however, this cannot be changed to account for the plant\u2019s individual needs, and the amount of water it can store is not high enough to be able to keep a plant alive for an extended vacation. alternatively, there exist timed plant watering devices that water a plant every day. however, limited adjustability in timing and access to data means that this strategy is restricted in other ways. our product exceeds the utility of all these other devices, while also providing invaluable logged data and water tracking to the user. given the utility of our device, and the fact that on the order of 33 million households participate in indoor gardening/houseplant ownership, paired with the fact that our product likely could be sold for somewhere on the order of $20 (and likely much less were we to manufacture this at scale), the expected number of users could very well be in the millions of households. all these factors considered, we believe this project can fill a definitive niche in the market as it currently stands.\nwhat it does\nput simply, our project is an automatic plant caretaker: it intelligently and effectively cares for a single plant by watering it according to a combination of the plant\u2019s needs and environmental effects. the auto-farmer dispenses water by means of a peristaltic pump, while rotating the plant to give an even watering. the amount of water dispensed is a function of the plant\u2019s individual needs as specified by the owner, the environment (temperature, humidity), and the weather forecast (rain chance, foreseen temperature, etc.) the watering is either done automatically every t seconds (where t is defined by the user, generally once per day is sufficient), or when the user presses a button on an app. lastly, continuously during operation, the device sends information to the user to help them keep track of watering trends, environmental patterns, and other long-term effects on their plants. with this system of sensors and effectors, combined with the ability to keep the user informed, the auto-farmer makes the home gardening experience much more easy and enjoyable.\nhow to use it\nuser interface mechanical setup:\nplace the auto-farmer where the plant is intended to be kept (taking care to provide appropriately decide on lighting, indoor/-----> outdoor !!! , etc according to the specific plant)\nput the potted plant on the turntable. there is a wall around the table to help better center the pot on the table and allow for even watering centered on the stem.\nfill the water reservoir, then place the long end of the peristaltic pump tubing inside it (with its end touching the bottom of the reservoir to minimize refilling).\nelectrical/software setup:\nin the source code for the auto-farmer, input the desired parameters for the plant\u2019s individual water need (cactuses need less than ferns, for instance), the watering period (default should be 1 day between watering), sensor vs weather report weight (weight the sensor higher if inside, the weather report more if outside), data report period (amount of time between logging a data point), and calibrated \u201cseconds-per-ml\u201d pump constant (optional, but allows to precisely tune the device to the specific pump hardware on hand)\nin the source code, input the latitude and longitude of the place in which you live in the lat and lon sections (note: in a consumer product, we\u2019d likely build a database or user interface to make this selection easier; for demonstration purposes, we have pre-loaded latitude/longitude data for berkeley).\nconnect the computer on which the source code is being run to the esp32, use shell49 to interface with the esp32, and connect it to the internet\nthe source code can be copied to the esp32 using the cp command, or if preferred it can simply be run while connected to the computer (running while connected to the computer will allow for status printouts, but those can also be seen using the mqtt protocol). the commands to set up the esp32 and the auto-farmer program are as follows:\n>>>shell49\n>>>run boot.py\n>>>run source_code.py\nif you would like to remotely monitor your plant, run the following command to open the data plotter once the other scripts are running (note you can do this on any internet-connected computer where the data plotter script exists; it gets data through the mqtt protocol from the auto-farmer):\n>>>run data_plotter.py\nif you would like to remotely water your plant manually, use the provided ifttt application on any device to do so (press the button).\nactuation\nturntable dc motor:\nin order to evenly water the plant, we considered many ideas, including sprinkler heads, a moving water dispenser, or letting water seep in through the bottom of a pot. the design we decided on, however, uses a turntable under the plant to allow for the water to be dispensed in a circular pattern around the plant\u2019s stem. to actuate this turntable, we used the pololu micro metal gearmotor (lab kit standard-issue) with its corresponding h-bridge motor controller. the motor controller--despite being an h-bridge--is simply used as a voltage regulator, as we do not need bidirectionality in our application. we also experimented with pwm signals to regulate speed of the turntable motor, but eventually we realized that given the gearing, we were unlikely to need anything less than full power at any given time. as such, all following calculations will be using the motor run at maximum power, as in practice this is how we run our device. when unloaded, the motor we used runs at 410 rpm with a current of 100 ma. however, this was significantly too fast for our purposes, so we decided to gear it down with a 25:1 gear ratio for a new speed of 16.4 rpm (0.27 rev/sec). with a 1.3 kg*cm max torque with the ungeared motor, we now could get a 1.3 * 25 = 32.5 kg*cm max torque which, given the radius of the platform\u2019s 6.3 cm radius, could allow for a 5.16 kg lateral force on the rotating platform (much less than the plant weighs, even if the force were directed completely sideways and off-center). in total, for each day of operation--assuming the plant is watered once a day for 5 minutes (far more than most plants will ever need)--the motor draws 300s*0.1a*12v = 360 watt*seconds = 360 j per day of operation, only drawing 1.2 watts at any given time [pololu]. this is a negligible amount of power compared to the amount households use (~30 kwh per day), which easily fits our design criteria [us energy information administration].\nperistaltic pump:\nto transport water from the reservoir to the plant, we initially considered two options: a solenoid+siphon system, and a pump. through experimentation, we quickly found that the solenoid+siphon system would require a sizable build envelope to build up sufficient pressure head for our desired flow rate through the solenoid valve, and so we opted for a pump instead. specifically, we were looking for a compact and robust pump, which could reliably and precisely deliver relatively small quantities of water at a time. for that purpose, the adafruit 12v peristaltic pump was perfect. essentially, the pump converts rotary motion from the motor into liquid flow by progressively squeezing an interior hose, pushing small \u201cpackets\u201d of water through the system and out the other side. in this way, no moving components actually ever come into contact with the liquid itself, resulting in uncontaminated water, and zero mineral buildup/wear from the liquid-pump interface. requiring 200-300 ma at 12v (thus requiring between 2.4 and 3.6 watts during operation), the pump is capable of pumping up to 100 ml/minute [adafruit], though that value is somewhat variable from pump to pump, and required calibration on our part. taken together, this actuator checks off all the boxes: adequate and reliable flow rate, enough pressure head to pull water out of the reservoir and into the potted plant, small size for a compact device, and minimal wear/need for repair.\nsensing\nthe sensor our project uses is the dht11 humidity and temperature sensor from adafruit. this sensor detects both humidity and temperature and sends them down a single serial line. according to the datasheet, the sensor outputs a temperature between 0 and 50 degrees celsius with an accuracy of \u00b12 degrees, and a humidity between 20% and 90% with an accuracy of \u00b15%. the resolution of the sensor is 1, so an integer value is returned for both temperature and humidity, while the repeatability of the sensor is \u00b11%. while this is definitely on the lower end in performance, our device does not rely on precise measurements, only general tendencies of the outdoor weather/indoor microclimate, so this sensor was deemed adequate.\nthe more important benefit of the sensor is its low cost (around $4) and low power consumption. before a reading is needed, the sensor\u2019s pull up resistor gives it an output equal to its 5v vdd input. however, the residual current flowing through in this standby-state is ~100 ua, requiring negligible input power. when a reading is needed, the sensor outputs a 40 bit signal: 16 bit temperature reading, 16 bit humidity reading, and 8 bit validation (checksum). note however that only 8 bits of each of the temperature and humidity readings are integral (as opposed to another 8 bits of decimal data, which are always zero due to hardware limitations), thus explaining the low resolution despite having many more available bits in each serial data package to work with. after this data is sent, the sensor resumes lower power mode, only having been in high power (active) mode for 160 \u03bcs (the start of the signal is a short low-followed-by-high voltage signal to indicate that the data transmission is starting) + 40 * 50 \u03bcs (each of the bits has a 50 \u03bcs low-voltage signal directly preceding it) for a total of 2.16 ms [d-robotics]. with this low power consumption and low cost, combined with the ability to read both temperature and humidity , this sensor was the perfect one for our needs.\nsoftware\nin designing the software to control the vehicle, care was taken to appropriately divide the processing between onboard and offboard computing. of course, as a general principle, onboard computing can operate with far less latency, but has very limited computing power; offboard computers are the exact opposite. for our purposes, we have very limited computation (a few simple arithmetic operations every time it waters a plant), with the longest-latency operation being an internet access (which would be necessary to interface with mqtt anyways). what\u2019s more, in a real deployment of this device we\u2019d only realistically need to do computations once every couple minutes (and likely much longer than that). in that case, speed is not a huge concern, especially compared with the latency and potential failure point of relying on an external host computer. as such, we deemed that offboard computing would be unnecessary, and that it made more sense to do the computation on board.\nthe software codebase for the device consists of five main components: one that makes sensor readings/internet accesses (onboard), one that calculates quantity of water and waters the plant (onboard), one that logs data periodically and sends it to the cloud (onboard), one that interfaces with an ifttt manual control option (onboard), and one that receives data from the cloud and plots it in a user-friendly interface to allow for monitoring of environmental data and watering trends (offboard). these are outlined in more detail below:\ndata collection\nto access the data from the dht sensor, we simply used the built-in dht class in esp32 micropython to interpret the serial data into temperature and humidity readings. to access internet weather data, on the other hand, is a far more involved task. because of the difficulties reading ifttt weather data, we opted to use openweathermap, an open source weather data api. essentially, we made accounts, received uniquely identifying \u201ckeys\u201d, and then used these to grab data from openweathermap\u2019s sources given our input latitude and longitude. the site then returned a massive json file containing weather data for the next seven days which we subsequently turned into an equally massive multi-layer hashmap. finally, we indexed into the hashmap using commands like data[\u201cdaily\u201d][\u201c0\u201d][\u201cpop\u201d] where \u201cdata\u201d is the name of the outer map, \u201cdaily\u201d means we search for daily data as opposed to weekly or hourly data, \u201c0\u201d for the index of the day, and \u201dpop\u201d for probability of precipitation. notably, because we only pull this data when the data-tracker requires, power consumption/computation is not much of a concern (we use interrupts instead of constant polling).\nwatering logic\nin order to properly leverage all the data that our device is collecting, both from the sensors and from the internet, an intelligently-designed watering logic was developed. first, we enumerate the inputs for our function:\nw: nominal amount of water per period*\nk: sensor-internet bias parameter*\nsensor_temp: sensor temperature\nhumidity: sensor humidity\ncurr_temp: today\u2019s weather report temperature\ncurr_prec: today\u2019s weather report probability of precipitation (pop)\ntom_temp: tomorrow\u2019s weather report temperature\ntom_prec: tomorrow\u2019s weather report probability of precipitation (pop)\nuser-specified parameter, see the \u201cuser interface\u201d section for more details.\nessentially, the function is a product of linear functions of each of these parameters, which is used to weight the nominal amount of water w. the whole function takes this form (comments inserted below each line for design justification):\navg_temp = k*sensor_temp + (1-k)*(curr_temp+tom_temp)/2\nfirst, we take a simple average of today\u2019s and tomorrow\u2019s weather forecast temperature\nnext, we take a weighted (specified by k) average of the sensor-read temperature and that average weather forecast term\navg_prec = 1 - (1-curr_prec)*(1-tom_prec)\ncalculate the probability that there will be precipitation either today or tomorrow.\nweight = (avg_temp/30 + 1/3) * (1-avg_prec) * (1.5-0.01*humidity)\nnote: when weights = 1, all conditions are nominal (20\u00b0c, 0% pop, 50% humidity)\nwater more when it\u2019s hotter: for the temperature weight factor t, we want t = 1 at 20\u00b0c and t = 1.5 at t = 35\u00b0c, linearly interpolate between those two points\nwater less when it\u2019s more likely to precipitate: for the pop weight factor p, we want p = 1 at 0% pop and p = 0 at 100% pop, linearly interpolate between those two points\nwater less when it\u2019s more humid: for the humidity weight factor h, we want h = 1 at 50% humidity and h = 1.5 at 0% humidity, linearly interpolate between those two points\nwater_amount = weight * w\nweight the nominal amount of water by the product of the weights calculated in the previous line.\nthis water_amount quantity we calculate is then converted into a time to run the peristaltic pump at full power, which has been calibrated with volumetric flow rate testing. thus, the plant has received a dose of water, based on plant-specific factors (ie how much water a plant should nominally receive, as well as how frequently it should be watered), real-time environmental factors (eg temperature, humidity), and forecasted weather (ie next-day temperature and precipitation). with the automatic timer, this quantity will be delivered periodically for as long as the device is on.\ndata logging\nbesides watering the plant, the device also packages the data it collects from sensing hardware and the internet weather report, and sends it into the cloud via its data logging functionality. specifically, the device tracks and logs sensor-read temperature/humidity, current internet-provided temperature/pop, and future internet-provided temperature/pop. the esp32 code is equipped with a timer, which makes a sensor reading and accesses internet weather data every datalogging period (a user-defined amount of time), and sends it to an mqtt channel to be processed. additionally, at each watering stage, a sensor reading/internet request is made, and that data, along with the amount watered, is also packaged and sent to the same mqtt data channel.\nifttt-powered manual control\nto implement a manual plant-watering functionality, we simply use a separate mqtt command channel to issue commands to water. every time a command is received, an interrupt is triggered, which immediately delivers the recommended dose of water (as determined by the same logic as laid out in the \u201cwatering logic\u201d section above), this event also overrides the next automatic watering event, before resuming with the automatic watering cycle after another period. for a user-friendly interface, sending this manual-watering mqtt signal is simply triggered by the push of a button on the ifttt app.\nexternal data plotting\nonce that data is sent to the mqtt feed, a separate script, untethered from the esp32, processes that data and plots it in a user-friendly interface. through these real-time updating plots, a user can track the conditions the plant is experiencing, as well as the amount of water the plant has received over time. this enables intelligent tweaking of the user-defined parameters, allowing for maximized customizability of the care one can provide for one\u2019s plant.\nour design relies quite heavily on the adafruit mqtt broker (using it for both manual commands and for data-logging/status reports) and internet access for weather reports. as such, latency is a primary concern. empirically, based on our network configuration setup, we experienced delays of around 5-7 seconds for a mqtt message to propagate from the ifttt app, through to the mqtt feed, and then trigger an esp32 interrupt. the internet access to weather data takes 2-3 seconds on top of that. overall, given that it is not critical to water absolutely instantaneously in this application, these delays are perfectly serviceable for our purposes. the device\u2019s utility lies mainly in its long-term planning and data records, as opposed to instant response to external stimuli, making it the perfect application for these intuitive and user-friendly mqtt/ifttt software tools.\nwhat's next for autofarmer\noverall, the part of the code that could be improved the most is the watering logic algorithm. in its current state, many of the parameters must be input by the user, and adjustment factors are based mostly on an overall average of so-called \u201cnominal conditions\u201d. as such, this algorithm would need to undergo serious review and user testing to ensure that the prescribed watering dose is appropriate and sensible, given the input data. for better results, it may likely be worthwhile to compile a database of plant preferred watering cycles/quantities (which could be accessed from the cloud upon setup), to make the user experience as straightforward and intuitive as possible. other future extensions include refining the watering logic, utilizing more future weather data points, and adding additional sensor functionality (e.g. soil moisture sensor).\nconclusion\nwhile taking care of plants is often a fun and relaxing hobby, it can also be equal parts drudgery and frustration, especially when it comes to meeting many varied plants\u2019 needs. with its economical design and intelligent software, the auto-farmer takes the work out of home gardening and allows the user to focus on the more interesting parts of gardening. the actuators allows for the plants to be watered effectively even when the owner is not attentive or present, the sensor/weather data allows for a highly tailored watering for the plants, and the data logging gives the plant owner a much more informed perspective on keeping their plants healthy and happy, both in the short- and long-term. from the avid plant connoisseur to the simple tasteful desk-plant owner, we sincerely hope you consider the auto-farmer for your plant caretaking needs.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59500588}, {"Unnamed: 0": 1149, "autor": "Air Purifier Vs Humidifier | Difference & Comparison", "date": null, "content": "Air pollution has become a matter of grave concern in the 21st century. Besides the outdoor air pollution, indoor air pollution has become extremely dangerous. Various studies have shown that indoor air quality could be 2-5 times worse than outdoor air quality.\nAlong with the various initiatives taken by the government and private bodies to tackle the outdoor air pollution, the indoor air pollution can be reduced by simple measures. Installing the right air purifier can be a positive step to improve the indoor air quality(IAQ).\nModern advanced air purifiers equipped with both active and passive air purification technologies can get rid of most of the pollutants like PM 2,5, 10, dander, pollen, mould, fungus, VOCs (Volatile Organic Compounds), bacteria and viruses (including the SARS-CoV-2 coronavirus).\nAlthough air purifiers do restore the freshness of the inside air, it is not the only deciding factor. A low or high humidity can also affect the comfort level indoors. To maintain an optimal humidity level inside the room especially during the dry winter season, the use of a humidifier is a must. The comparison between an air purifier and Humidifier can be seen below:\nWhat it does? Air Purifier : Eliminates all kinds of pollutants. Humidifier : Adds moisture thus increasing the relative humidity.\nHow it works? Air Purifier : Uses a combination of filter technologies to remove the pollutants inside the room. HEPA filter, Activated Carbon, UV and ionizers are some of the common ones.\nHumidifier :Moisture is produced in multiple ways. Ultrasonic, Evaporation and Hot Steam are the most common ones.\nWho needs it? Air Purifier : People with respiratory problems like asthma and pneumonia. Also advised for people suffering from any kind of allergies.\nHumidifier :People having dry skins and sleeping problems. It makes the air comfortable and cosy with aromatherapy.\nWhen to use it? Air Purifier : It can be used all the year-long, especially during the winter and spring seasons.\nHumidifier :Ideal for use during the dry winter season.\nLimitations - Air Purifier : Awareness of the right product in the market.\nHumidifier :Condense mist humidifiers can dampen the floors.\nPrice - Air Purifier : Available at various prices as per the size of the room.\nHumidifier : Cost is less but the operational cost is much higher for some types such as warm steam humidifier.\nBesides the above comparison, there are other differences also like the types of air purifiers and humidifiers present in the market. A broad classification has been listed below:\nTypes of Air Purifiers\nPassive Air Purifiers: Air purifiers that contain a combination Passive filter technologies. \u2022 HEPA Filtration: It is a highly dense filter that can trap airborne contaminants. Searching for a True HEPA filter is the right way to go that can eliminate and is certified to remove 99.97% of the particles (as small as 0.3 microns) from the enclosed space. Air purifiers equipped with HEPA filters are good for removing pollutants like particulate matter, dust, allergens, visible smoke and pet dander.\n\u2022 Carbon Filtration: This filter is equipped with a special form of activated carbon that can trap odours, gases, chemicals and volatile organic compounds. It also helps in retaining the freshness of the room by reducing toxic substances that are in the air.\n\u2022 Ionization: The air purifier with this technology emits charged ions into the air that attaches to airborne contaminants. The weight of the particles increases and is easily captured by the internal air filters. The process can remove ultra-fine particles down to 0.01 microns in size.\n\u2022 Ultraviolet Light: It is a special kind of light that kills airborne microbial including bacteria and viruses by destroying their molecular DNA/RNA structure. While the air passes through the air purifier, the UV light traps the microbial to ensure that the air flowing back in the room is sterilized and clean.\nActive Air Purifiers: Air purifiers that work with active air purification technologies:\n\u2022 PHI Cell: The PHI-Cell produces atmospheric hydrogen peroxides molecules in the indoor spaces imitating the action of early morning sun. It is the most advanced active air purification technology that neutralizes microbes in the air (responsible for cross infection & respiratory diseases) by altering their DNA/RNA and rendering them inactive. Hence allowing easy & safe breathing. This technology has been successfully tested and approved by an internationally accredited virology lab in USA and India over SARS-COV-2 (COVID-19) virus with 99% reduction from the air and surfaces.\n\u2022 REME-LED: REME LED uses the UV Led of optimum wavelength directed on a hybrid catalyst that combines to produce friendly oxidizers like Hydrogen Peroxide molecules in the indoor space. These friendly oxidizers neutralize allergens, odor, viruses, mold, bacteria, VOCs, and microbial in the air. In addition to REME-LED\u2122 Bipolar Ionization (BPI) releases positive and negative ions, which reduce PM2.5 levels in the indoor space. It is also mercury free and zero ozone compliant. It is safe for kids, pregnant women and old people. .\nTypes of Humidifiers-\nHumidifiers come in 3 forms commonly. \u2022 Warm Mist: These kinds of humidifiers heat up the water with a gentle boiling process for producing a warm moist that can be seen and felt in the air.\n\u2022 Cool Mist: These types of humidifiers are equipped with filters that trap minerals, sediments and other impurities while delivering a cool invisible mist that evaporates into the air.\n\u2022 Ultrasonic: These kinds of humidifiers make use of a diaphragm that vibrates at an ultrasonic frequency. Water droplets are created in the process that are pushed into the air with a fan to create a cool lighter mist which quickly disperses humidity into the room.", "link": "https://devpost.com/software/air-purifier-vs-humidifier-difference-comparison", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "outdoor", "selectorShort": "outdoor", "MarkedSent": "air pollution has become a matter of grave concern in the 21st century. besides the -----> outdoor !!!  air pollution, indoor air pollution has become extremely dangerous. various studies have shown that indoor air quality could be 2-5 times worse than outdoor air quality.\nalong with the various initiatives taken by the government and private bodies to tackle the outdoor air pollution, the indoor air pollution can be reduced by simple measures. installing the right air purifier can be a positive step to improve the indoor air quality(iaq).\nmodern advanced air purifiers equipped with both active and passive air purification technologies can get rid of most of the pollutants like pm 2,5, 10, dander, pollen, mould, fungus, vocs (volatile organic compounds), bacteria and viruses (including the sars-cov-2 coronavirus).\nalthough air purifiers do restore the freshness of the inside air, it is not the only deciding factor. a low or high humidity can also affect the comfort level indoors. to maintain an optimal humidity level inside the room especially during the dry winter season, the use of a humidifier is a must. the comparison between an air purifier and humidifier can be seen below:\nwhat it does? air purifier : eliminates all kinds of pollutants. humidifier : adds moisture thus increasing the relative humidity.\nhow it works? air purifier : uses a combination of filter technologies to remove the pollutants inside the room. hepa filter, activated carbon, uv and ionizers are some of the common ones.\nhumidifier :moisture is produced in multiple ways. ultrasonic, evaporation and hot steam are the most common ones.\nwho needs it? air purifier : people with respiratory problems like asthma and pneumonia. also advised for people suffering from any kind of allergies.\nhumidifier :people having dry skins and sleeping problems. it makes the air comfortable and cosy with aromatherapy.\nwhen to use it? air purifier : it can be used all the year-long, especially during the winter and spring seasons.\nhumidifier :ideal for use during the dry winter season.\nlimitations - air purifier : awareness of the right product in the market.\nhumidifier :condense mist humidifiers can dampen the floors.\nprice - air purifier : available at various prices as per the size of the room.\nhumidifier : cost is less but the operational cost is much higher for some types such as warm steam humidifier.\nbesides the above comparison, there are other differences also like the types of air purifiers and humidifiers present in the market. a broad classification has been listed below:\ntypes of air purifiers\npassive air purifiers: air purifiers that contain a combination passive filter technologies. \u2022 hepa filtration: it is a highly dense filter that can trap airborne contaminants. searching for a true hepa filter is the right way to go that can eliminate and is certified to remove 99.97% of the particles (as small as 0.3 microns) from the enclosed space. air purifiers equipped with hepa filters are good for removing pollutants like particulate matter, dust, allergens, visible smoke and pet dander.\n\u2022 carbon filtration: this filter is equipped with a special form of activated carbon that can trap odours, gases, chemicals and volatile organic compounds. it also helps in retaining the freshness of the room by reducing toxic substances that are in the air.\n\u2022 ionization: the air purifier with this technology emits charged ions into the air that attaches to airborne contaminants. the weight of the particles increases and is easily captured by the internal air filters. the process can remove ultra-fine particles down to 0.01 microns in size.\n\u2022 ultraviolet light: it is a special kind of light that kills airborne microbial including bacteria and viruses by destroying their molecular dna/rna structure. while the air passes through the air purifier, the uv light traps the microbial to ensure that the air flowing back in the room is sterilized and clean.\nactive air purifiers: air purifiers that work with active air purification technologies:\n\u2022 phi cell: the phi-cell produces atmospheric hydrogen peroxides molecules in the indoor spaces imitating the action of early morning sun. it is the most advanced active air purification technology that neutralizes microbes in the air (responsible for cross infection & respiratory diseases) by altering their dna/rna and rendering them inactive. hence allowing easy & safe breathing. this technology has been successfully tested and approved by an internationally accredited virology lab in usa and india over sars-cov-2 (covid-19) virus with 99% reduction from the air and surfaces.\n\u2022 reme-led: reme led uses the uv led of optimum wavelength directed on a hybrid catalyst that combines to produce friendly oxidizers like hydrogen peroxide molecules in the indoor space. these friendly oxidizers neutralize allergens, odor, viruses, mold, bacteria, vocs, and microbial in the air. in addition to reme-led\u2122 bipolar ionization (bpi) releases positive and negative ions, which reduce pm2.5 levels in the indoor space. it is also mercury free and zero ozone compliant. it is safe for kids, pregnant women and old people. .\ntypes of humidifiers-\nhumidifiers come in 3 forms commonly. \u2022 warm mist: these kinds of humidifiers heat up the water with a gentle boiling process for producing a warm moist that can be seen and felt in the air.\n\u2022 cool mist: these types of humidifiers are equipped with filters that trap minerals, sediments and other impurities while delivering a cool invisible mist that evaporates into the air.\n\u2022 ultrasonic: these kinds of humidifiers make use of a diaphragm that vibrates at an ultrasonic frequency. water droplets are created in the process that are pushed into the air with a fan to create a cool lighter mist which quickly disperses humidity into the room.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59501149}, {"Unnamed: 0": 1500, "autor": "WhatsThis", "date": null, "content": "Inspiration\nPeople started to enjoy outdoor activities and gatherings recently as more people are getting vaccinated. As much as we are getting used to the online, non-face-to-face environment, this painfully long period of various restrictions have triggered pandemic fatigue on countless people.\nThis is a time to relieve the stress; we get to enjoy the nature that's around us, like trees, flowers, animals, etc. The increasing frequency of exposure to the outdoor environments means the increase of curiosity of things we have not yet explored. Of course, we may have seen them online, but they do not appear 100% the same as on the internet.\nWe wanted to make a better outdoors experience, so we came up with the simple, yet powerful idea - object recognition app. With WhatsThis app, you can find out what the object you are looking at is, even if it's the very first time seeing the object, by simply turning on your camera. This way, you can skip all the tedious steps of going onto the internet, giving the description of its appearance, and searching all the results.\nWhat it does\nWhatsThis is an iOS application that identifies objects with an image classification technique, featuring ResNet-50 convolutional neural network.\nWhile you are scanning with your camera, this app captures objects that are within the camera view, and returns its analysis on what those objects are at the bottom of the screen, along with the accuracy of its classifications. This operation is done on a real-time basis, so whenever you turn your camera i.e. the camera viewport changes, the classifications returned at the bottom will change accordingly.\nYou also have an option to take an instant picture or to select an image from the gallery. The core functionality is the same where the app returns the classification of an object, this time with the static image instead of a real-time camera view.\nHow we built it\nAs this is an iOS application, the development was done under Xcode environment, using Swift as a primary language. The app was built using Model-View-Controller design pattern, with the use of CoreML and Vision for ResNet-50 image classification integration.\nChallenges we ran into\nWe were new to the iOS development and Swift language itself, so learning a new language and platform came to us as a great challenge.\nXcode not only required updates on the app itself, but it also required the newest macOS update in order for Xcode to be compatible - this update took a very long time, which has significantly delayed our development procedure. The testing environment was quite limited as well, as some of our members did not have any iOS or macOS devices at all.\nAccomplishments that we're proud of\nWe have decided to challenge ourselves by trying a different type of development that we have not yet done previously. Both iOS and macOS are commonly used platforms, so we thought it would be worthwhile to try developing on those platforms.\nAs explained in \"Challenges we ran into\" section, picking up a new language was indeed very difficult, but at the same time it was very accomplishing, in a sense that we have built a fully functioning app after all the hardships.\nResNet-50 integration was also an accomplishment that we are proud of, since machine learning is a very widely known field, yet very difficult to grasp on.\nWhat we learned\nAlthough we are not experts on machine learning, this project has definitely enlightened us on how to utilize machine learning libraries. Additionally, as much as it was a challenge, we have learned a lot about iOS development using Xcode and Swift language itself.\nWhat's next for whatsThis\nShow classification analysis on multiple objects within the camera view", "link": "https://devpost.com/software/whatsthis", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "outdoor", "selectorShort": "outdoor", "MarkedSent": "inspiration\npeople started to enjoy -----> outdoor !!!  activities and gatherings recently as more people are getting vaccinated. as much as we are getting used to the online, non-face-to-face environment, this painfully long period of various restrictions have triggered pandemic fatigue on countless people.\nthis is a time to relieve the stress; we get to enjoy the nature that's around us, like trees, flowers, animals, etc. the increasing frequency of exposure to the outdoor environments means the increase of curiosity of things we have not yet explored. of course, we may have seen them online, but they do not appear 100% the same as on the internet.\nwe wanted to make a better outdoors experience, so we came up with the simple, yet powerful idea - object recognition app. with whatsthis app, you can find out what the object you are looking at is, even if it's the very first time seeing the object, by simply turning on your camera. this way, you can skip all the tedious steps of going onto the internet, giving the description of its appearance, and searching all the results.\nwhat it does\nwhatsthis is an ios application that identifies objects with an image classification technique, featuring resnet-50 convolutional neural network.\nwhile you are scanning with your camera, this app captures objects that are within the camera view, and returns its analysis on what those objects are at the bottom of the screen, along with the accuracy of its classifications. this operation is done on a real-time basis, so whenever you turn your camera i.e. the camera viewport changes, the classifications returned at the bottom will change accordingly.\nyou also have an option to take an instant picture or to select an image from the gallery. the core functionality is the same where the app returns the classification of an object, this time with the static image instead of a real-time camera view.\nhow we built it\nas this is an ios application, the development was done under xcode environment, using swift as a primary language. the app was built using model-view-controller design pattern, with the use of coreml and vision for resnet-50 image classification integration.\nchallenges we ran into\nwe were new to the ios development and swift language itself, so learning a new language and platform came to us as a great challenge.\nxcode not only required updates on the app itself, but it also required the newest macos update in order for xcode to be compatible - this update took a very long time, which has significantly delayed our development procedure. the testing environment was quite limited as well, as some of our members did not have any ios or macos devices at all.\naccomplishments that we're proud of\nwe have decided to challenge ourselves by trying a different type of development that we have not yet done previously. both ios and macos are commonly used platforms, so we thought it would be worthwhile to try developing on those platforms.\nas explained in \"challenges we ran into\" section, picking up a new language was indeed very difficult, but at the same time it was very accomplishing, in a sense that we have built a fully functioning app after all the hardships.\nresnet-50 integration was also an accomplishment that we are proud of, since machine learning is a very widely known field, yet very difficult to grasp on.\nwhat we learned\nalthough we are not experts on machine learning, this project has definitely enlightened us on how to utilize machine learning libraries. additionally, as much as it was a challenge, we have learned a lot about ios development using xcode and swift language itself.\nwhat's next for whatsthis\nshow classification analysis on multiple objects within the camera view", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59501500}, {"Unnamed: 0": 2700, "autor": "The Juke Brick House", "date": null, "content": "Inspiration\nThere is currently a struggle between the desire to return to a pre-covid normal and the need to stay safe and healthy. Restaurants and bars where people could connect over shared interests were one of the most missed social experiences during the quarantine. To reopen safely, these establishments have had to lower seating capacity and minimize the use of frequently touched objects. In a world where we are forced apart, we wanted to find a way to bring people back together. That way was through music.\nWhat it does\nWhile feeling more divided than ever because of the new covid restrictions, late-night partiers, couples on their first dates, and girl\u2019s nights out can regain their lost sense of connection to others by helping pick the next song playing in their bar, restaurant, or outdoor gathering. They get to help curate the night with the rest of the attendees so that they feel more involved and engaged in their setting, similar to how they would have felt pre-covid while using a jukebox.\nHow we built it\nUsing Android Studio and Java, we brought our Adobe XD designs to life and uploaded them to our personal phones for testing. The physical jukebox is a raspberry pi and a python program that displays the current song playing and the songs to vote on next. The users' voting information is processed in cockroachDB and sends that data to the python program to pull the song from the Spotify API. Juke Brick uses Dolby media processing technology to enhance the audio quality. Dolby media processing also gives the AI information including BPM and genera of the music currently playing to use Google Prediction to suggest songs to be voted for. This allows the \"vibe\" in the room to keep going.\nChallenges we ran into\nWe ran into issues regarding dolby.io. Once we watched the workshop Dolby gave, we had a better understanding of how to use this technology in our program. We ran into issues with the computational power of the raspberry pi and just decided to run a Google Cloud server for machine learning algorithms.\nAccomplishments that we're proud of\nWe are proud to connect our mobile app with the raspberry pi using web authentication. This project will help create a great social experience for people in bars and outdoor experiences. Juke Brick helps create an analog and digital experience that sparks connections over new songs at happy hour and bars.\nWhat we learned\nwe learned a lot about APIs and Dolby Media Processing. We were so excited to learn new technologies, we implemented everything we learned into one project. We also learned how to make an AI hosted on Google Cloud and cockroachDB for machine learning programs.", "link": "https://devpost.com/software/the-juke-brick-house", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "outdoor", "selectorShort": "outdoor", "MarkedSent": "inspiration\nthere is currently a struggle between the desire to return to a pre-covid normal and the need to stay safe and healthy. restaurants and bars where people could connect over shared interests were one of the most missed social experiences during the quarantine. to reopen safely, these establishments have had to lower seating capacity and minimize the use of frequently touched objects. in a world where we are forced apart, we wanted to find a way to bring people back together. that way was through music.\nwhat it does\nwhile feeling more divided than ever because of the new covid restrictions, late-night partiers, couples on their first dates, and girl\u2019s nights out can regain their lost sense of connection to others by helping pick the next song playing in their bar, restaurant, or -----> outdoor !!!  gathering. they get to help curate the night with the rest of the attendees so that they feel more involved and engaged in their setting, similar to how they would have felt pre-covid while using a jukebox.\nhow we built it\nusing android studio and java, we brought our adobe xd designs to life and uploaded them to our personal phones for testing. the physical jukebox is a raspberry pi and a python program that displays the current song playing and the songs to vote on next. the users' voting information is processed in cockroachdb and sends that data to the python program to pull the song from the spotify api. juke brick uses dolby media processing technology to enhance the audio quality. dolby media processing also gives the ai information including bpm and genera of the music currently playing to use google prediction to suggest songs to be voted for. this allows the \"vibe\" in the room to keep going.\nchallenges we ran into\nwe ran into issues regarding dolby.io. once we watched the workshop dolby gave, we had a better understanding of how to use this technology in our program. we ran into issues with the computational power of the raspberry pi and just decided to run a google cloud server for machine learning algorithms.\naccomplishments that we're proud of\nwe are proud to connect our mobile app with the raspberry pi using web authentication. this project will help create a great social experience for people in bars and outdoor experiences. juke brick helps create an analog and digital experience that sparks connections over new songs at happy hour and bars.\nwhat we learned\nwe learned a lot about apis and dolby media processing. we were so excited to learn new technologies, we implemented everything we learned into one project. we also learned how to make an ai hosted on google cloud and cockroachdb for machine learning programs.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59502700}, {"Unnamed: 0": 3388, "autor": "Strong Incentive", "date": null, "content": "Inspiration\nThe inspiration came from the problem I face since earlier this year. I need reliable people to host and maintain outdoor air quality sensors. This is for my non-profit project. It aims to bring attention to the air quality problem in Yerevan, the capital of Armenia. More details can be found at Frankyants.org. The most reliable hosts so far have been my relatives. However, for the project to help cover the city with enough sensors, I need more hosts than just my relatives. I feel I will be able to find enough hosts if there is a reliable reward system for people hosting and maintaining the sensors. The hosting requires providing electricity and internet. The maintenance requires replacing the internal fan and filter when they wear out.\nWhat it does\nThe goal is to automatically and reliably reward the hosts for each IoT device they host and maintain. I use the PurpleAir air quality sensor in this hackathon project to demonstrate the use case based on my experience. I believe many similar use cases are possible and would be popular.\nHow I built it\nTo demonstrate how it could work I developed an external adapter and 2 smart contracts.\nThe external adapter connects to sensors and gathers parameters via the API. These parameters help understand if the sensor has been offline and for how long, confirm that it is placed outside, confirm that it does not need maintenance etc. Based on the data the external adapter responds with the proportion of the reward payout earned for the sensor, e.g. 100 meaning 100% of the payout.\nThe first smart contract (APIConsumer.sol) checks if the host is to be paid for the hosted sensors. To do that it makes requests to the external adapter via ChainlinkClient based HTTP Get (for now). The adapter is hosted on Google Cloud for the demo purposes. I've already started a discussion on discord with a Chainlink node operator about hosting the adapter.\nThe second smart contract (Trigger.sol) is a Chainlink Keeper compatible contract that triggers the first at the specified update interval. I set the interval to 1 minute for test and demo purposes. The interval should be increased in production.\nTo manage the Keeper contract I registered the Chainlink UpKeep Strong Incentive Chainlink Demo. As long as the UpKeep and corresponding APIConsumer are funded, the upkeep is performed every minute and the transactions can be seen on Etherscan.\nChallenges I ran into\nThis was my first solidity code, first blockchain work, and so on. You can imagine. Pretty much everything blockchain related seemed like a challenge. The teaching materials and my software development and architecture experience helped me grasp things relatively quickly and build something working. Big thanks to Patrick and others who helped produce the documentation, examples, and videos. Otherwise, I would be just an observer.\nAccomplishments that I'm proud of\nAt first implementing my idea on Chainlink and blockchain seemed overwhelming, because the tools and processes I am used to are not applicable or available there. I learned new ways of doing things, e.g. using Keepers instead of Cron. In the end I made everything necessary to prove my concept using Chainlink and blockchain and learned a lot along the way. I feel proud and super pumped to continue to the next stage of my project.\nWhat I learned\nBefore the hackathon I knew some general theory about blockchains and some concepts about specific blockchain projects like Chainlink. The hackathon helped me start practicing and in the process learn much more about Ethereum blockchain, oracles, smart contracts, side-chains, tokens, wallets, blockchain transactions, state, and so on.\nWhat's next for Strong Incentive\nI need to figure out what is necessary to make Strong Incentive work in real life. It must be cost effective, reliable, and gather enough interest to cover the development costs. I believe there are others who have a need similar to mine and not necessarily with air sensors, but with all kinds of sensors and IoT devices. I will try to find those people and get their feedback. In the meantime, I would like to continue building my solution to learn more about smart contracts, oracles, and blockchains in general. I plan to partner with a node-operator to make my external-adapter available on Chainlink. I would like to experiment with Open Zeppelin and other libraries that help make smart contracts reliable and secure. I am glad that the Chainlink development community is supportive. I am sure I will have plenty of questions along the way. At some point soon, I hope to organize an experienced team to bring Strong Incentive to fruition.", "link": "https://devpost.com/software/iot-tbd", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "outdoor", "selectorShort": "outdoor", "MarkedSent": "inspiration\nthe inspiration came from the problem i face since earlier this year. i need reliable people to host and maintain -----> outdoor !!!  air quality sensors. this is for my non-profit project. it aims to bring attention to the air quality problem in yerevan, the capital of armenia. more details can be found at frankyants.org. the most reliable hosts so far have been my relatives. however, for the project to help cover the city with enough sensors, i need more hosts than just my relatives. i feel i will be able to find enough hosts if there is a reliable reward system for people hosting and maintaining the sensors. the hosting requires providing electricity and internet. the maintenance requires replacing the internal fan and filter when they wear out.\nwhat it does\nthe goal is to automatically and reliably reward the hosts for each iot device they host and maintain. i use the purpleair air quality sensor in this hackathon project to demonstrate the use case based on my experience. i believe many similar use cases are possible and would be popular.\nhow i built it\nto demonstrate how it could work i developed an external adapter and 2 smart contracts.\nthe external adapter connects to sensors and gathers parameters via the api. these parameters help understand if the sensor has been offline and for how long, confirm that it is placed outside, confirm that it does not need maintenance etc. based on the data the external adapter responds with the proportion of the reward payout earned for the sensor, e.g. 100 meaning 100% of the payout.\nthe first smart contract (apiconsumer.sol) checks if the host is to be paid for the hosted sensors. to do that it makes requests to the external adapter via chainlinkclient based http get (for now). the adapter is hosted on google cloud for the demo purposes. i've already started a discussion on discord with a chainlink node operator about hosting the adapter.\nthe second smart contract (trigger.sol) is a chainlink keeper compatible contract that triggers the first at the specified update interval. i set the interval to 1 minute for test and demo purposes. the interval should be increased in production.\nto manage the keeper contract i registered the chainlink upkeep strong incentive chainlink demo. as long as the upkeep and corresponding apiconsumer are funded, the upkeep is performed every minute and the transactions can be seen on etherscan.\nchallenges i ran into\nthis was my first solidity code, first blockchain work, and so on. you can imagine. pretty much everything blockchain related seemed like a challenge. the teaching materials and my software development and architecture experience helped me grasp things relatively quickly and build something working. big thanks to patrick and others who helped produce the documentation, examples, and videos. otherwise, i would be just an observer.\naccomplishments that i'm proud of\nat first implementing my idea on chainlink and blockchain seemed overwhelming, because the tools and processes i am used to are not applicable or available there. i learned new ways of doing things, e.g. using keepers instead of cron. in the end i made everything necessary to prove my concept using chainlink and blockchain and learned a lot along the way. i feel proud and super pumped to continue to the next stage of my project.\nwhat i learned\nbefore the hackathon i knew some general theory about blockchains and some concepts about specific blockchain projects like chainlink. the hackathon helped me start practicing and in the process learn much more about ethereum blockchain, oracles, smart contracts, side-chains, tokens, wallets, blockchain transactions, state, and so on.\nwhat's next for strong incentive\ni need to figure out what is necessary to make strong incentive work in real life. it must be cost effective, reliable, and gather enough interest to cover the development costs. i believe there are others who have a need similar to mine and not necessarily with air sensors, but with all kinds of sensors and iot devices. i will try to find those people and get their feedback. in the meantime, i would like to continue building my solution to learn more about smart contracts, oracles, and blockchains in general. i plan to partner with a node-operator to make my external-adapter available on chainlink. i would like to experiment with open zeppelin and other libraries that help make smart contracts reliable and secure. i am glad that the chainlink development community is supportive. i am sure i will have plenty of questions along the way. at some point soon, i hope to organize an experienced team to bring strong incentive to fruition.", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59503388}, {"Unnamed: 0": 4384, "autor": "Breathe", "date": null, "content": "BREATHE\ud83c\udf2a\ufe0f\u2764\ufe0f\ud83e\ude7a\ud83e\udec1\nOften people say Breathe and Relax but does anyone understands the problems that can be caused due to breathing the impure air. Air pollution is contamination of the indoor or outdoor environment by any chemical, physical or biological agent that modifies the natural characteristics of the atmosphere. Household combustion devices, motor vehicles, industrial facilities and forest fires are common sources of air pollution. Pollutants of major public health concern include particulate matter, carbon monoxide, ozone, nitrogen dioxide and sulfur dioxide. Outdoor and indoor air pollution cause respiratory and other diseases and is an important source of morbidity and mortality.\nBreathing in the same stale air everyday:\u2601\ufe0f\ud83d\udca8\nWeakens the immune system\nDepresses the mood\nIncreases risk of infection and allergies\nReduces elimination of toxins\nIncreases headaches\nIncreases joint pain\nReduces overall health\nRole of Technology\ud83d\udcdd\ud83c\udf87\ud83d\udea9\ud83d\udd79\ud83d\udcbb\nWe all dread the heat and often find ways to manage ourselves in the scorching sun. We install heavy machines that consume too much power to breathe purified air. We then dread the bills we need to pay to manage the pollution in India. This year we can consider the newest invention - a portable ventilator and air purifier with an oxygen concentrator. To help people tackle this concern, portable mini ventilator-cum-air purifiers have been developed by the industry.\nHow can Pure Air help us?\u2764\ufe0f\ud83e\ude7a\ud83e\udec1\ud83c\udf2a\ufe0f\nPure fresh air\nCalms the nerves\nRelaxes the body\nRelieves anxiety\nImproves digestion\nBoosts the immune system\nImproves blood circulation\nImproves mood\nPromotes sound sleep\nReduces stress\nTry out this website to help people get aware about the causes and cures of Air Pollution.\nThe project is Hosted on Replit \u2764\ufe0f", "link": "https://devpost.com/software/breathe-37wof2", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "outdoor", "selectorShort": "outdoor", "MarkedSent": "breathe\ud83c\udf2a\ufe0f\u2764\ufe0f\ud83e\ude7a\ud83e\udec1\noften people say breathe and relax but does anyone understands the problems that can be caused due to breathing the impure air. air pollution is contamination of the indoor or -----> outdoor !!!  environment by any chemical, physical or biological agent that modifies the natural characteristics of the atmosphere. household combustion devices, motor vehicles, industrial facilities and forest fires are common sources of air pollution. pollutants of major public health concern include particulate matter, carbon monoxide, ozone, nitrogen dioxide and sulfur dioxide. outdoor and indoor air pollution cause respiratory and other diseases and is an important source of morbidity and mortality.\nbreathing in the same stale air everyday:\u2601\ufe0f\ud83d\udca8\nweakens the immune system\ndepresses the mood\nincreases risk of infection and allergies\nreduces elimination of toxins\nincreases headaches\nincreases joint pain\nreduces overall health\nrole of technology\ud83d\udcdd\ud83c\udf87\ud83d\udea9\ud83d\udd79\ud83d\udcbb\nwe all dread the heat and often find ways to manage ourselves in the scorching sun. we install heavy machines that consume too much power to breathe purified air. we then dread the bills we need to pay to manage the pollution in india. this year we can consider the newest invention - a portable ventilator and air purifier with an oxygen concentrator. to help people tackle this concern, portable mini ventilator-cum-air purifiers have been developed by the industry.\nhow can pure air help us?\u2764\ufe0f\ud83e\ude7a\ud83e\udec1\ud83c\udf2a\ufe0f\npure fresh air\ncalms the nerves\nrelaxes the body\nrelieves anxiety\nimproves digestion\nboosts the immune system\nimproves blood circulation\nimproves mood\npromotes sound sleep\nreduces stress\ntry out this website to help people get aware about the causes and cures of air pollution.\nthe project is hosted on replit \u2764\ufe0f", "sortedWord": "None", "removed": "Nan", "score": 31, "comments": 0, "media": null, "medialink": null, "identifyer": 59504384}, {"Unnamed: 0": 4540, "autor": "Zeus", "date": null, "content": "Inspiration\nAccording to the World Health Organization, 4.2 million deaths every year occur due to outdoor air pollution. Exposure to air pollution can result in significant health problems such as decreased lung function, aggravated asthma, and increased respiratory symptoms.\nAdditionally, UV radiation exposure is a risk factor for skin cancer, cataracts, and other ill-ness. The incidence of skin cancer, including melanoma, has increased due to excess exposure to UV radiation. To alert individuals of both air quality and UV index outdoors, along with several other weather conditions, we created Zeus the weather bot.\nWhat it does\nZeus alerts individuals via call or text about weather conditions. After the user messages the number with \u2018start\u2019 and enters their location, they have 3 options for interacting with Zeus. The first option allows them to receive current weather condition data which includes temperature, the chance of precipitation, cloud coverage, wind speed, UV index, visibility, humidity, and air quality.\nThe second option allows them to receive daily reminders - at a time of their choosing - of any selection of the weather conditions outlined above. A user might, for example, decide to receive a UV index reminder every morning to determine how much sunlight exposure is safe.\nThe third option allows users to set up alerts that will continue for the following 24 hours. These alerts will notify users of certain weather conditions, such as when the air quality increases to possibly hazardous levels. A user may set up an alert for the UV index on a day they go outside in order to warn them from experiencing UV radiation.\nThe user is always able to access the help menu with the command \u201chelp me\u201d. From here, the user can access the menu, delete their reminders, and change their location. The service is currently functional for the entire U.S. subcontinent.\nHow we built it\nWe developed the backend for the chatbot using Twilio API and Python Flask to deliver automated SMS messages and phone calls. The Twilio API uses a webhook to redirect SMS messages to the Flask backend which houses Zeus\u2019s implementation. This Flask backend is hosted on an AWS EC2 instance. After the user initially messages the Zeus phone number, they are prompted to enter their location. This phone number and location data is then sent to our Google Cloud Firestore database.\nWhen the user requests for weather data pertaining to their current location, our Flask backend calls the WeatherBit API using the stored location data. When the user wants to create a reminder or alert, the Flask backend creates and stores reminder objects in an SQLite database. This database is accessed by the reminder system which ensures that reminders and alerts are being sent out periodically as necessary.\nChallenges we ran into\nWe were having trouble handling phone numbers who wanted multiple reminders at different times. So if someone wanted an alert at both 9 am and 9 pm, the program had difficulty determining when to send the alert. Our solution was accessing the reminders SQLite database using the hour parameter in order to be able to distinguish between different reminders created by the same phone number. Additionally, using the 24-hour time method made it much easier to store and manipulate time values.\nThere was initial difficulty in figuring out how to get the weather API to work for our uses. We had some difficulty accessing the API with a key and figuring out what the API could offer for our project ideas. In the end, we became more familiar with the methods and made our project idea more flexible to create our product.\nGiven our unfamiliarity with Twilio, we had some trouble deciding how to work with contacting and receiving input from users. This meant experimenting with the API and figuring out which best fit the purposes of our project. In the end, we had a mix of texting and calling users for weather alerts and updating preferences, with a heavier emphasis on texting.\nAccomplishments that we're proud of\nOne of our main accomplishments was being able to efficiently retrieve data from the WeatherBit API in order to ensure there was not a very large delay between the user\u2019s texts and our Zeus bot\u2019s responses.\nFor the majority of our team members, this was their first time working with the Twilio API so we are proud that we were able to use Webhooks in order to receive messages from the Twilio API to the Flask backend.\nWhat we learned\nPrivacy and security are of great concern to maintain user trust in our app. We must ensure compliance and other industry best practices to protect our users\u2019 data while offering convenient services. In order to attain this, we ensured that user\u2019s could only affect create and delete reminders from the individual\u2019s own phone number.\nThe SMS messaging and phone call notification system we developed with the Twilio API is extremely powerful and has the potential to improve the quality of life for millions of people across the globe.\nWhat's next for Weather Bot\nCurrently, Zeus only supports locations within the U.S. The next step would be expanding this to include users across the globe and help millions of users.\nAnother feature implementation would be adding custom thresholds that users can adjust for reminders and alerts.", "link": "https://devpost.com/software/weather-bot-fbvh0r", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "outdoor", "selectorShort": "outdoor", "MarkedSent": "inspiration\naccording to the world health organization, 4.2 million deaths every year occur due to -----> outdoor !!!  air pollution. exposure to air pollution can result in significant health problems such as decreased lung function, aggravated asthma, and increased respiratory symptoms.\nadditionally, uv radiation exposure is a risk factor for skin cancer, cataracts, and other ill-ness. the incidence of skin cancer, including melanoma, has increased due to excess exposure to uv radiation. to alert individuals of both air quality and uv index outdoors, along with several other weather conditions, we created zeus the weather bot.\nwhat it does\nzeus alerts individuals via call or text about weather conditions. after the user messages the number with \u2018start\u2019 and enters their location, they have 3 options for interacting with zeus. the first option allows them to receive current weather condition data which includes temperature, the chance of precipitation, cloud coverage, wind speed, uv index, visibility, humidity, and air quality.\nthe second option allows them to receive daily reminders - at a time of their choosing - of any selection of the weather conditions outlined above. a user might, for example, decide to receive a uv index reminder every morning to determine how much sunlight exposure is safe.\nthe third option allows users to set up alerts that will continue for the following 24 hours. these alerts will notify users of certain weather conditions, such as when the air quality increases to possibly hazardous levels. a user may set up an alert for the uv index on a day they go outside in order to warn them from experiencing uv radiation.\nthe user is always able to access the help menu with the command \u201chelp me\u201d. from here, the user can access the menu, delete their reminders, and change their location. the service is currently functional for the entire u.s. subcontinent.\nhow we built it\nwe developed the backend for the chatbot using twilio api and python flask to deliver automated sms messages and phone calls. the twilio api uses a webhook to redirect sms messages to the flask backend which houses zeus\u2019s implementation. this flask backend is hosted on an aws ec2 instance. after the user initially messages the zeus phone number, they are prompted to enter their location. this phone number and location data is then sent to our google cloud firestore database.\nwhen the user requests for weather data pertaining to their current location, our flask backend calls the weatherbit api using the stored location data. when the user wants to create a reminder or alert, the flask backend creates and stores reminder objects in an sqlite database. this database is accessed by the reminder system which ensures that reminders and alerts are being sent out periodically as necessary.\nchallenges we ran into\nwe were having trouble handling phone numbers who wanted multiple reminders at different times. so if someone wanted an alert at both 9 am and 9 pm, the program had difficulty determining when to send the alert. our solution was accessing the reminders sqlite database using the hour parameter in order to be able to distinguish between different reminders created by the same phone number. additionally, using the 24-hour time method made it much easier to store and manipulate time values.\nthere was initial difficulty in figuring out how to get the weather api to work for our uses. we had some difficulty accessing the api with a key and figuring out what the api could offer for our project ideas. in the end, we became more familiar with the methods and made our project idea more flexible to create our product.\ngiven our unfamiliarity with twilio, we had some trouble deciding how to work with contacting and receiving input from users. this meant experimenting with the api and figuring out which best fit the purposes of our project. in the end, we had a mix of texting and calling users for weather alerts and updating preferences, with a heavier emphasis on texting.\naccomplishments that we're proud of\none of our main accomplishments was being able to efficiently retrieve data from the weatherbit api in order to ensure there was not a very large delay between the user\u2019s texts and our zeus bot\u2019s responses.\nfor the majority of our team members, this was their first time working with the twilio api so we are proud that we were able to use webhooks in order to receive messages from the twilio api to the flask backend.\nwhat we learned\nprivacy and security are of great concern to maintain user trust in our app. we must ensure compliance and other industry best practices to protect our users\u2019 data while offering convenient services. in order to attain this, we ensured that user\u2019s could only affect create and delete reminders from the individual\u2019s own phone number.\nthe sms messaging and phone call notification system we developed with the twilio api is extremely powerful and has the potential to improve the quality of life for millions of people across the globe.\nwhat's next for weather bot\ncurrently, zeus only supports locations within the u.s. the next step would be expanding this to include users across the globe and help millions of users.\nanother feature implementation would be adding custom thresholds that users can adjust for reminders and alerts.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59504540}, {"Unnamed: 0": 4943, "autor": "Skylight Yoga", "date": null, "content": "Skylight Yoga offers yoga training, yoga classes and corporate yoga instruction for anybody who wants to learn more about the practice of yoga. Our goal is to provide a diverse selection of high-quality yoga classes in Miami with top instructors that are accessible anytime, anywhere. We are dedicated to empowering people through movement, breath, and community.\n#yoga #yogaschool #yogaclass #yogateachercertification #corporateyoga #yogateacher #yogatraining #yogastudio #outdooryoga #yogaonthebeach #MiamiBeach\nSkylight Yoga - Location\nAddress :- 7900 Tatum Waterway Dr. #303 Miami Beach FL 33141\nPhone :- (786) 505-8591\nServices We Offer\nyoga classes\nmeditation\noutdoor yoga classes\nOur Other Links\nyoga teacher certification Miami - https://www.skylightyoga.com/ttc.html\ncorporate yoga retreat Miami - https://www.skylightyoga.com/corporate.html\ncertified yoga teacher training miami beach - https://www.skylightyoga.com/teachertraining.html\nyoga ashram Florida - https://www.skylightyoga.com/ashram.html\nSkylight Yoga - Social Profile\nFacebook - https://www.facebook.com/skylightyoga/\nYoutube - https://www.youtube.com/channel/UCtfXEavN18tzBBLI6iRzooQ/about\nPinterest - https://www.pinterest.com/SkylightYoga/\nTwitter - https://twitter.com/SkylightYoga\ninstagram.com - https://www.instagram.com/skylightyoga/", "link": "https://devpost.com/software/skylight-yoga", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "outdoor", "selectorShort": "outdoor", "MarkedSent": "skylight yoga offers yoga training, yoga classes and corporate yoga instruction for anybody who wants to learn more about the practice of yoga. our goal is to provide a diverse selection of high-quality yoga classes in miami with top instructors that are accessible anytime, anywhere. we are dedicated to empowering people through movement, breath, and community.\n#yoga #yogaschool #yogaclass #yogateachercertification #corporateyoga #yogateacher #yogatraining #yogastudio #outdooryoga #yogaonthebeach #miamibeach\nskylight yoga - location\naddress :- 7900 tatum waterway dr. #303 miami beach fl 33141\nphone :- (786) 505-8591\nservices we offer\nyoga classes\nmeditation\n-----> outdoor !!!  yoga classes\nour other links\nyoga teacher certification miami - https://www.skylightyoga.com/ttc.html\ncorporate yoga retreat miami - https://www.skylightyoga.com/corporate.html\ncertified yoga teacher training miami beach - https://www.skylightyoga.com/teachertraining.html\nyoga ashram florida - https://www.skylightyoga.com/ashram.html\nskylight yoga - social profile\nfacebook - https://www.facebook.com/skylightyoga/\nyoutube - https://www.youtube.com/channel/uctfxeavn18tzbbli6irzooq/about\npinterest - https://www.pinterest.com/skylightyoga/\ntwitter - https://twitter.com/skylightyoga\ninstagram.com - https://www.instagram.com/skylightyoga/", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59504943}, {"Unnamed: 0": 6102, "autor": "Grizzle Guide", "date": null, "content": "Overview\nWe love the great outdoors but we also like to be prepared for what\u2019s coming our way. That's why we created an App that gives nearby hiking locations with info on the local weather such as sky condition, temperature, and humidity. You can start by entering your home address or an address near where you\u2019d like to hike! The addresses are converted to precise latitude and longitude coordinates through google\u2019s geocoding API. Those coordinates are sent to Google\u2019s places API to search for popular hiking, camping, or other outdoor recreational locations. Each of these potential locations has its coordinates sent to a weather API to get the current climate conditions. The recreation sites are then arranged in order of proximity to the initial address and displayed in a list with all of its relevant data. Merging these data sources gives the user the knowledge and power to safely prepare for a trip into the great wilderness so they can live healthier, more sustainable lives.\nImpact & Sustainability\nWith Grizzle Guide, the team hopes that users will become more excited about the possibilities of the outdoors, leading to a more active and sustainable lifestyle. Additionally, by becoming more experienced in the outdoors, users may become more aware of how the environment is being threatened today. By giving the users easily accessible parks and hiking locations, it is more likely the users will visit the location, increasing the chance for a true interest in the environment.\nChallenges\nThis was the first time the group worked with Android Studio to create apps, or even worked on developing apps in general. The learning process for app development and Android Studio was eased due to the resources available at HackGT 8, such as the Android App Development workshop in the Emerging track. The other main issue the group faced revolved around the use of APIs to power the processes the app executed, such as GPS retrieval and finding nearby destinations. However, once this issue was resolved, it became easier to execute the operations that relied on the APIs.\nWhat We Learned\nThe team learned many useful tools for developing apps in the future. As mentioned above, one of the major accomplishments involved the use of the Google APIs, which allows the team a more solid understanding of the Google Cloud Platform in order to build more complex and powerful applications. Another tool, also mentioned previously, was Android Studio, which would streamline app development in any future endeavors. Finally, the team was able to cultivate skills learned from class through this experience, and the emerging workshops added additional skills to aid future aspirations.\nFuture Goals\n- Map interface that allows for users to visually comprehend data\n- More accurate ranking system based on weather conditions in different areas\n- User preferences to rank location by user interests", "link": "https://devpost.com/software/grizzle-guide", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "outdoor", "selectorShort": "outdoor", "MarkedSent": "overview\nwe love the great outdoors but we also like to be prepared for what\u2019s coming our way. that's why we created an app that gives nearby hiking locations with info on the local weather such as sky condition, temperature, and humidity. you can start by entering your home address or an address near where you\u2019d like to hike! the addresses are converted to precise latitude and longitude coordinates through google\u2019s geocoding api. those coordinates are sent to google\u2019s places api to search for popular hiking, camping, or other -----> outdoor !!!  recreational locations. each of these potential locations has its coordinates sent to a weather api to get the current climate conditions. the recreation sites are then arranged in order of proximity to the initial address and displayed in a list with all of its relevant data. merging these data sources gives the user the knowledge and power to safely prepare for a trip into the great wilderness so they can live healthier, more sustainable lives.\nimpact & sustainability\nwith grizzle guide, the team hopes that users will become more excited about the possibilities of the outdoors, leading to a more active and sustainable lifestyle. additionally, by becoming more experienced in the outdoors, users may become more aware of how the environment is being threatened today. by giving the users easily accessible parks and hiking locations, it is more likely the users will visit the location, increasing the chance for a true interest in the environment.\nchallenges\nthis was the first time the group worked with android studio to create apps, or even worked on developing apps in general. the learning process for app development and android studio was eased due to the resources available at hackgt 8, such as the android app development workshop in the emerging track. the other main issue the group faced revolved around the use of apis to power the processes the app executed, such as gps retrieval and finding nearby destinations. however, once this issue was resolved, it became easier to execute the operations that relied on the apis.\nwhat we learned\nthe team learned many useful tools for developing apps in the future. as mentioned above, one of the major accomplishments involved the use of the google apis, which allows the team a more solid understanding of the google cloud platform in order to build more complex and powerful applications. another tool, also mentioned previously, was android studio, which would streamline app development in any future endeavors. finally, the team was able to cultivate skills learned from class through this experience, and the emerging workshops added additional skills to aid future aspirations.\nfuture goals\n- map interface that allows for users to visually comprehend data\n- more accurate ranking system based on weather conditions in different areas\n- user preferences to rank location by user interests", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59506102}, {"Unnamed: 0": 6535, "autor": "Weather Watcher", "date": null, "content": "Weather Watcher\nWe envision a future where students from around the world can access high quality education through Web3.0, augmented reality, and metaverse environments. Dissatisfaction with public education, coupled with the coronavirus pandemic, has driven new interest in virtual learning. As our world continues to rapidly change, Chainlink\u2019s ability to bridge immediate, definitive, real-world truth into a digital environment, while also providing the provenance of that information, is critical to evolving better, more adaptive educational systems.\nWhat does Weather Watcher do?\nWeather Watcher is a whimsical weather predictor. Using Chainlink VRF, you can generate a truly random weather prediction in the form of an on-chain SVG NFT.\ncontract WeatherWatcher is RainCatcher\nOur original project, RainCatcher, is a simple, non-competitive, educational game intended to teach players about blockchain and meteorology. Players would mint an NFT \u201cbucket\u201d and stake it on one of multiple locations from around the world. Chainlink Keepers would monitor the weather conditions for each location, and whenever it started raining somewhere, RAIN tokens would be gradually minted to all addresses that staked a bucket at that location. Gathered RAIN could then be used to water and grow NFT plants. Players would learn about weather and climate science, and use data to make predictions about weather patterns, while also learning the basics of blockchain navigation.\nBlockchain Navigation + Game Design > Engineering Skills\nWe began the hackathon with lot of blockchain navigation experience but no real blockchain development experience. We had never created a Web3 front-end, or studied React, or even deployed a real Solidity contract (just the learning contracts from last summer\u2019s Chainlink Bootcamp).\nWhen researching Chainlink Keepers for our use case, we found data parameters offered by the Accuweather node that would probably work well, but we couldn\u2019t figure out how to access the node, let alone determine the right code syntax and order of operations to bring its data to our contract.\nThe biggest lesson for us during the hackathon was that we understand how Solidity and Chainlink work in concept, but the actual practice of building has a steep learning curve. After several weeks of working on RainCatcher, we decided to pivot toward a facet of our project that better aligns with our level of experience and skill.\nHow did we build Weather Watcher?\nWe used Visual Studio Code with Hardhat + OpenZeppelin and Chainlink contracts. We finally figured out how to make the code work to mint SVG NFTs and connected our smart contracts to the frontend at a Buildspace workshop during the weekend of November 20.\nWe went to almost all of the workshops offered during the hackathon, rewatched more than a few, found other YouTube tutorials to watch, explored Moralis, went back to our notes from last summer\u2019s Chainlink Bootcamp, repeatedly returned to the Chainlink tutorials and documents, and spent a lot of time studying contracts on GitHub.\nEventually we found a contract on GitHub (credit:SimRunBot) that mints SVG NFTs composed of randomized circles. We modified the contract\u2019s functions to create our randomized weather predictions, and deployed it with our Buildspace / Replit frontend.\nBiggest Accomplishment\nIt was a struggle to piece together all the skills needed to deploy a working dApp, but we persevered, and finally launched a working project on Rinkeby at 4:30am on November 28. To be honest, after all the time spent working on this and so many failures and near-misses, it feels surreal to have a working project!\nWhat's next for Weather Watcher / RainCatcher?\nSleep will be good, and some exercise, and some outdoor time. We will be returning to CryptoZombies and a React tutorial in a \u2018recovery run\u2019 kind of way. We have two Buildspace workshops coming up.\nWe\u2019ll use Weather Watcher to upgrade RainCatcher\u2019s bucket minting contract. After that, we need to write contracts that will allow the NFT buckets to be staked on different locations, then register for Chainlink Keepers to check the weather in those locations and toggle RAIN minting on and off.\nEventually we will add the Garden contract where players can use their RAIN to grow dynamic plant NFTs. Finished plants could be placed in \u201cThe Field\u201d, an amazing metaverse space we\u2019re imagining where players and visitors can explore.\nRainCatcher would be just one of a suite of educational dApps attached to a metaverse, pulling real-world data in real-time to give students immediate access to learning environments far removed from wherever they may be.\nThank you to everyone who made the Chainlink Hackathon happen, who presented, who answered endless questions, and who is cheering on developers of all skill levels in the Web3 space. The work done by educators in this space has been tremendous, and is something we truly appreciate. Looking forward to seeing the winning projects in December!", "link": "https://devpost.com/software/raincatcher", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "outdoor", "selectorShort": "outdoor", "MarkedSent": "weather watcher\nwe envision a future where students from around the world can access high quality education through web3.0, augmented reality, and metaverse environments. dissatisfaction with public education, coupled with the coronavirus pandemic, has driven new interest in virtual learning. as our world continues to rapidly change, chainlink\u2019s ability to bridge immediate, definitive, real-world truth into a digital environment, while also providing the provenance of that information, is critical to evolving better, more adaptive educational systems.\nwhat does weather watcher do?\nweather watcher is a whimsical weather predictor. using chainlink vrf, you can generate a truly random weather prediction in the form of an on-chain svg nft.\ncontract weatherwatcher is raincatcher\nour original project, raincatcher, is a simple, non-competitive, educational game intended to teach players about blockchain and meteorology. players would mint an nft \u201cbucket\u201d and stake it on one of multiple locations from around the world. chainlink keepers would monitor the weather conditions for each location, and whenever it started raining somewhere, rain tokens would be gradually minted to all addresses that staked a bucket at that location. gathered rain could then be used to water and grow nft plants. players would learn about weather and climate science, and use data to make predictions about weather patterns, while also learning the basics of blockchain navigation.\nblockchain navigation + game design > engineering skills\nwe began the hackathon with lot of blockchain navigation experience but no real blockchain development experience. we had never created a web3 front-end, or studied react, or even deployed a real solidity contract (just the learning contracts from last summer\u2019s chainlink bootcamp).\nwhen researching chainlink keepers for our use case, we found data parameters offered by the accuweather node that would probably work well, but we couldn\u2019t figure out how to access the node, let alone determine the right code syntax and order of operations to bring its data to our contract.\nthe biggest lesson for us during the hackathon was that we understand how solidity and chainlink work in concept, but the actual practice of building has a steep learning curve. after several weeks of working on raincatcher, we decided to pivot toward a facet of our project that better aligns with our level of experience and skill.\nhow did we build weather watcher?\nwe used visual studio code with hardhat + openzeppelin and chainlink contracts. we finally figured out how to make the code work to mint svg nfts and connected our smart contracts to the frontend at a buildspace workshop during the weekend of november 20.\nwe went to almost all of the workshops offered during the hackathon, rewatched more than a few, found other youtube tutorials to watch, explored moralis, went back to our notes from last summer\u2019s chainlink bootcamp, repeatedly returned to the chainlink tutorials and documents, and spent a lot of time studying contracts on github.\neventually we found a contract on github (credit:simrunbot) that mints svg nfts composed of randomized circles. we modified the contract\u2019s functions to create our randomized weather predictions, and deployed it with our buildspace / replit frontend.\nbiggest accomplishment\nit was a struggle to piece together all the skills needed to deploy a working dapp, but we persevered, and finally launched a working project on rinkeby at 4:30am on november 28. to be honest, after all the time spent working on this and so many failures and near-misses, it feels surreal to have a working project!\nwhat's next for weather watcher / raincatcher?\nsleep will be good, and some exercise, and some -----> outdoor !!!  time. we will be returning to cryptozombies and a react tutorial in a \u2018recovery run\u2019 kind of way. we have two buildspace workshops coming up.\nwe\u2019ll use weather watcher to upgrade raincatcher\u2019s bucket minting contract. after that, we need to write contracts that will allow the nft buckets to be staked on different locations, then register for chainlink keepers to check the weather in those locations and toggle rain minting on and off.\neventually we will add the garden contract where players can use their rain to grow dynamic plant nfts. finished plants could be placed in \u201cthe field\u201d, an amazing metaverse space we\u2019re imagining where players and visitors can explore.\nraincatcher would be just one of a suite of educational dapps attached to a metaverse, pulling real-world data in real-time to give students immediate access to learning environments far removed from wherever they may be.\nthank you to everyone who made the chainlink hackathon happen, who presented, who answered endless questions, and who is cheering on developers of all skill levels in the web3 space. the work done by educators in this space has been tremendous, and is something we truly appreciate. looking forward to seeing the winning projects in december!", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59506535}, {"Unnamed: 0": 6920, "autor": "Parc Instrumental Spatial", "date": null, "content": "Inspiration\nDuring the pandemic, we encountered the VRC rave scene. When we attended THE VRC club \"Ghost Club\", we were shocked by the production quality, energy and the creator community's consistent contribution to the space. For the first time we saw the potential of virtual worlds and the positive connection between people established in such space. It is an alive, supportive community of creators and audience not some zombie product made to impress VC. So we started trying to build our own world and host a \"portal event\" where we had a simultaneous live performance in an underground club offline in Shanghai and also in VRC.\nThe world is consisted of indoor live stages and outdoor acoustic architecture park. For this exhibition because there is no live performances we are exhibiting the outdoor area.\nWhat it does\nIt's an acoustic architecture park, consisted of 3 parts:\nPart 1 Tunnel: This piece was inspired by the text \"Der Tunnel\" by Friedrich D\u00fcrrenmatt. The text describes a young man, 24 years old, who took the train to the university, but the train entered an endless tunnel and fell into the center of the earth.\nPart 2 Waterphone: This instrument has a ghost like sound. Every steel bar here bring an interesting pitch to the mixture of sound.\nPart 3 Spherical Network: It is constructed with thousands of thin beams forming a shape of hemisphere. You will see the trajectory of the three comets (les trois m\u00e9t\u00e9ores). They are all directly along the beam of the spheric networks. These trajectories are used as the trajectory of the sound elements. There are three kind of sound materials: noise, granular sounds like pulse-train, and language. The sounds choose the direction freely, and pass very quickly, like a meteor in the sky.\nThe architecture is inspired by the text of Borg\u00e8s, le jardin aux sentiers qui bifurquent, with the idea of parallel possibility of life events, and the three kind of sound and three trajectory are metaphor of the parallel possibility.\nHow we built it\nThis is the first VRC world we built. We started by reading vrc documentation, and we go from there.\nModel: Unreal Texture, lighting, UI, Interation: Unity + VRC SDK Sound: 64 channel audio is recorded in Modalys\nChallenges we ran into\nDynamic lighting that changes with video texture\nNetwork streaming issue within VRC\nWhat we learned\nLighting, sound and post processing in Unity\nVRC SDK -Discord marketing and online community \"management\".\nWhat's next for Parc Instrumental Spatial\nWe are constantly renovating the world, we are hoping to have the next portal event. We will update in this discord: https://discord.gg/j4NjQaXXJt", "link": "https://devpost.com/software/parc-instrumental-spatial", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "outdoor", "selectorShort": "outdoor", "MarkedSent": "inspiration\nduring the pandemic, we encountered the vrc rave scene. when we attended the vrc club \"ghost club\", we were shocked by the production quality, energy and the creator community's consistent contribution to the space. for the first time we saw the potential of virtual worlds and the positive connection between people established in such space. it is an alive, supportive community of creators and audience not some zombie product made to impress vc. so we started trying to build our own world and host a \"portal event\" where we had a simultaneous live performance in an underground club offline in shanghai and also in vrc.\nthe world is consisted of indoor live stages and -----> outdoor !!!  acoustic architecture park. for this exhibition because there is no live performances we are exhibiting the outdoor area.\nwhat it does\nit's an acoustic architecture park, consisted of 3 parts:\npart 1 tunnel: this piece was inspired by the text \"der tunnel\" by friedrich d\u00fcrrenmatt. the text describes a young man, 24 years old, who took the train to the university, but the train entered an endless tunnel and fell into the center of the earth.\npart 2 waterphone: this instrument has a ghost like sound. every steel bar here bring an interesting pitch to the mixture of sound.\npart 3 spherical network: it is constructed with thousands of thin beams forming a shape of hemisphere. you will see the trajectory of the three comets (les trois m\u00e9t\u00e9ores). they are all directly along the beam of the spheric networks. these trajectories are used as the trajectory of the sound elements. there are three kind of sound materials: noise, granular sounds like pulse-train, and language. the sounds choose the direction freely, and pass very quickly, like a meteor in the sky.\nthe architecture is inspired by the text of borg\u00e8s, le jardin aux sentiers qui bifurquent, with the idea of parallel possibility of life events, and the three kind of sound and three trajectory are metaphor of the parallel possibility.\nhow we built it\nthis is the first vrc world we built. we started by reading vrc documentation, and we go from there.\nmodel: unreal texture, lighting, ui, interation: unity + vrc sdk sound: 64 channel audio is recorded in modalys\nchallenges we ran into\ndynamic lighting that changes with video texture\nnetwork streaming issue within vrc\nwhat we learned\nlighting, sound and post processing in unity\nvrc sdk -discord marketing and online community \"management\".\nwhat's next for parc instrumental spatial\nwe are constantly renovating the world, we are hoping to have the next portal event. we will update in this discord: https://discord.gg/j4njqaxxjt", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59506920}, {"Unnamed: 0": 7036, "autor": "youraibuddy", "date": null, "content": "Inspiration\nIt started as an experience. Our friend Manu Prasad is a visually impaired person. We had seen him struggling in day-to-day life. Sometimes he can't navigate well with his stick and he can't use any AI app that helps to read or identify objects due to low light, since he doesn't know whether there is light or not. We just looked every possible android app available on the play store, but none has this feature. He said \"If there is no light how can my already installed app can help to identify an object or read text??\". During the hackathon, we started working on every possible problem he is facing. We connected with Kerala Blind Association. Mr. Sudheesh, Mr. Umeshan T, Mr. Sathyan, and hackathon mentors who guided us to understand the actual problem the community is facing since three of them are visually impaired and they are members of the blind community and Sathyan sir is a blind school headmaster.\nSpecial Thanks to Mr. Sudheesh for extensive testing of the app\nWhat it does\nThere is 4 mode in the android app.\nIndoor and outdoor navigation with a state-of-the-art hybrid AI depth estimation and object detection model. Help you to hear the object, how far from you. and light detection using the depth estimation model.\nFace attribute recognition including age, gender, mustache, beard, sunglasses, eyeglasses, open eyes, and open mouth.\nText recognition (US-English)\nAI buddy - chatbot using Facebook AI Research BlenderBot API\nHow we built it\nWe build it using python, Kotlin, OpenCV, TensorFlow, etc.\nNavigation - We used SSD Mobilenet V2 object detection model for detecting objects, Then Crop the bounding box and apply a Monocular Depth Estimation model on the cropped bitmap for depth estimation and we used a mathematical formula to get the absolute distance.\nLight detection- We get the lowest pixel value from the depth inference and applied a threshold to detect light.\nFace attributes recognition - Used AWS Rekognition with AWS Lambda and AWS S3, the file gets deleted after processing.\nText recognition - Used AWS Textract with AWS Lambda and AWS S3, the file gets deleted after processing.\nBuddy AI chatbot - Used Facebook AI Research team ParlAI language model, we build a custom API for blenderbot 90M model. which is both empathetic and knowledge retrieval model.\nChallenges we ran into\nIntegrating all the solutions as one, was harder than we thought. Introducing the voice response, building and adding the blender bot API to Kotlin, adding the optimized light detection logic, combining the depth estimation model with the object detection model, all pushed our ability to the limit.\nAccomplishments that we're proud of\nSuccessfully built an app that has a login page(uses firebase), and all the 4 functionalities\nWhat we learned\nWe learned how to build a great application for the wonderful community\nWhat's next for your buddy\nBetter UI, adding more features and optimizations", "link": "https://devpost.com/software/blind-ai-companion", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "outdoor", "selectorShort": "outdoor", "MarkedSent": "inspiration\nit started as an experience. our friend manu prasad is a visually impaired person. we had seen him struggling in day-to-day life. sometimes he can't navigate well with his stick and he can't use any ai app that helps to read or identify objects due to low light, since he doesn't know whether there is light or not. we just looked every possible android app available on the play store, but none has this feature. he said \"if there is no light how can my already installed app can help to identify an object or read text??\". during the hackathon, we started working on every possible problem he is facing. we connected with kerala blind association. mr. sudheesh, mr. umeshan t, mr. sathyan, and hackathon mentors who guided us to understand the actual problem the community is facing since three of them are visually impaired and they are members of the blind community and sathyan sir is a blind school headmaster.\nspecial thanks to mr. sudheesh for extensive testing of the app\nwhat it does\nthere is 4 mode in the android app.\nindoor and -----> outdoor !!!  navigation with a state-of-the-art hybrid ai depth estimation and object detection model. help you to hear the object, how far from you. and light detection using the depth estimation model.\nface attribute recognition including age, gender, mustache, beard, sunglasses, eyeglasses, open eyes, and open mouth.\ntext recognition (us-english)\nai buddy - chatbot using facebook ai research blenderbot api\nhow we built it\nwe build it using python, kotlin, opencv, tensorflow, etc.\nnavigation - we used ssd mobilenet v2 object detection model for detecting objects, then crop the bounding box and apply a monocular depth estimation model on the cropped bitmap for depth estimation and we used a mathematical formula to get the absolute distance.\nlight detection- we get the lowest pixel value from the depth inference and applied a threshold to detect light.\nface attributes recognition - used aws rekognition with aws lambda and aws s3, the file gets deleted after processing.\ntext recognition - used aws textract with aws lambda and aws s3, the file gets deleted after processing.\nbuddy ai chatbot - used facebook ai research team parlai language model, we build a custom api for blenderbot 90m model. which is both empathetic and knowledge retrieval model.\nchallenges we ran into\nintegrating all the solutions as one, was harder than we thought. introducing the voice response, building and adding the blender bot api to kotlin, adding the optimized light detection logic, combining the depth estimation model with the object detection model, all pushed our ability to the limit.\naccomplishments that we're proud of\nsuccessfully built an app that has a login page(uses firebase), and all the 4 functionalities\nwhat we learned\nwe learned how to build a great application for the wonderful community\nwhat's next for your buddy\nbetter ui, adding more features and optimizations", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59507036}, {"Unnamed: 0": 7391, "autor": "FREAK", "date": null, "content": "Inspiration\nWe all are breathing under the covid pandemic situation, and we have a lot to worry about our health. One must stay miles away from getting sick, which the healthily working immune system promises us to allow ourselves to experience a productive version of ourselves. Together we can see the homecoming of a stronger system by working physically and letting the mind relax. It is essential for controlling diabetes and high blood pressure, maintaining bone strength and muscle tone through regular exercise is important especially after ordinary outdoor activities are curtailed during the COVID -19 pandemic.\nSedentary behavior and low levels of physical activity can have negative effects on the health, well-being, and quality of life of individuals. Self-quarantine can also cause additional stress and challenge the mental health of citizens. Physical inactivity is common among people with intellectual impairments, as are avoidable health problems such as obesity and cardiovascular disease. Increased exercise and wellness involvement can help them enhance their overall health and well-being.\nPhysical activity and relaxation techniques can be valuable tools to help you remain calm and continue to protect your health during this time. WHO recommends 150 minutes of moderate-intensity or 75 minutes of vigorous-intensity physical activity per week, or a combination of both. ThereFore Freak App is the Best Solution to stay mentally and physically fit during these tough times.\nWhat it does\n\"FREAK\" is predominantly designed to strengthen a person's immune system, which plays a vital role in the pandemic situation of Covid 19, which the world is struggling with. The app strives to improve the physical and mental health of the population to revive the immune system and wellbeing. The best thing which can happen to health is fitness. Physical activity reduces the risk of chronic disease, improves balance and coordination, helps to lose weight, and promotes strong muscles and bones. It boosts respiratory, cardiovascular health, and overall health at the same time. You may reduce body weight along with the risk of fatal diseases like diabetes, heart disease, and some cancers and even expand your sleeping habits and self-esteem.\nFREAK fitness coach provides its users with specific exercise plans and guides them through training to achieve their fitness goals. It uses the front camera of your smartphone to monitor your performance and then make changes to the workout plan. It also delivers audio feedback in real-time. It offers personalized fitness and lifestyle plans. Aaptiv needs to be fed with a few details like eating habits with diet plans, fitness goals, and current fitness level. Freak AI fitness coach's main goal is to get its users active, fit, and healthy. It does this by providing customized fitness training that can be done anywhere, at any time. Freak turns your smartphone into a personal fitness trainer that tracks your workouts using computer vision. Freak, like the other well-known personal trainers, provides real-time audio feedback.\nHow We built it\nMachine Learning Postnet Model by Tensorflow. It is a Predefined Model by Tensorflow that is a train to detect the body key points of a person\u2019s body, by detecting body parts such as elbows, hips, wrists, knees, and ankles. It uses the joints of these body parts to determine body postures. Postnet Model will help to get key points on the real-time data and help to detect a person's Body.\nPostnet Classifier to train the Custom Pose Detection. With the help of a classifier, different poses of exercises are classified which can be trained on different Dataset of exercises resulting in good accuracy.\nGoogle Cloud/ Firebase. Google Cloud to create and access data and for creating chatbots and other ML Models.\nChatBot using Google\u2019s DialogueFlow. Dialogflow is a natural language understanding platform used to design and integrate a conversational user interface into mobile apps bots, interactive voice response systems. The chatbot will help the user to interact and to solve their queries.\nImmune System Checker. It will help to generate the health report of a person and to recommend activities.\nBMI Calculator. To check Body Mass Index.\nChallenges we ran into\nFreak uses the neural network, and classic computer vision algorithms, with a 95% accuracy of recognition of asanas, which includes a data set of more than a thousand images. This dataset acts as training for the neural network and therefore is the most important aspect of the app. These representative datasets are hard to create, particularly when it comes to finding several images of people in a particular pose. These images should contain different angles, and because different people in their ordinary lives have performed different yoga poses, it becomes difficult to sync them with the existing systems. Along with the representative dataset, another problem faced by the app is the visibility of the body. Now, if a person is standing near a window or bright light, it might be difficult for the camera to catch certain parts of the body or it might blend the body\u2019s outline with a large object behind. So, while using Freak, it is not recommended to keep the camera against bright light or close to a window.\nWhat's next for FREAK - YOUR AI FITNESS COACH\nThe current App is presently accessible in English, but I intend to expand to include other languages in the future. The model is presently trained on a limited number of datasets; however, the dataset may be increased by including additional yoga postures done by humans in both indoor and outdoor settings to improve the model's performance.\nWith the aid of sensors, I plan to include additional health-related analyzers in the future, such as tracking footsteps, heart rate, and blood pressure. Also, a reminder system that notifies the user when it's time to drink, take medicine, or exercise.", "link": "https://devpost.com/software/freak-ai-fitness-coach-t2haw9", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "outdoor", "selectorShort": "outdoor", "MarkedSent": "inspiration\nwe all are breathing under the covid pandemic situation, and we have a lot to worry about our health. one must stay miles away from getting sick, which the healthily working immune system promises us to allow ourselves to experience a productive version of ourselves. together we can see the homecoming of a stronger system by working physically and letting the mind relax. it is essential for controlling diabetes and high blood pressure, maintaining bone strength and muscle tone through regular exercise is important especially after ordinary -----> outdoor !!!  activities are curtailed during the covid -19 pandemic.\nsedentary behavior and low levels of physical activity can have negative effects on the health, well-being, and quality of life of individuals. self-quarantine can also cause additional stress and challenge the mental health of citizens. physical inactivity is common among people with intellectual impairments, as are avoidable health problems such as obesity and cardiovascular disease. increased exercise and wellness involvement can help them enhance their overall health and well-being.\nphysical activity and relaxation techniques can be valuable tools to help you remain calm and continue to protect your health during this time. who recommends 150 minutes of moderate-intensity or 75 minutes of vigorous-intensity physical activity per week, or a combination of both. therefore freak app is the best solution to stay mentally and physically fit during these tough times.\nwhat it does\n\"freak\" is predominantly designed to strengthen a person's immune system, which plays a vital role in the pandemic situation of covid 19, which the world is struggling with. the app strives to improve the physical and mental health of the population to revive the immune system and wellbeing. the best thing which can happen to health is fitness. physical activity reduces the risk of chronic disease, improves balance and coordination, helps to lose weight, and promotes strong muscles and bones. it boosts respiratory, cardiovascular health, and overall health at the same time. you may reduce body weight along with the risk of fatal diseases like diabetes, heart disease, and some cancers and even expand your sleeping habits and self-esteem.\nfreak fitness coach provides its users with specific exercise plans and guides them through training to achieve their fitness goals. it uses the front camera of your smartphone to monitor your performance and then make changes to the workout plan. it also delivers audio feedback in real-time. it offers personalized fitness and lifestyle plans. aaptiv needs to be fed with a few details like eating habits with diet plans, fitness goals, and current fitness level. freak ai fitness coach's main goal is to get its users active, fit, and healthy. it does this by providing customized fitness training that can be done anywhere, at any time. freak turns your smartphone into a personal fitness trainer that tracks your workouts using computer vision. freak, like the other well-known personal trainers, provides real-time audio feedback.\nhow we built it\nmachine learning postnet model by tensorflow. it is a predefined model by tensorflow that is a train to detect the body key points of a person\u2019s body, by detecting body parts such as elbows, hips, wrists, knees, and ankles. it uses the joints of these body parts to determine body postures. postnet model will help to get key points on the real-time data and help to detect a person's body.\npostnet classifier to train the custom pose detection. with the help of a classifier, different poses of exercises are classified which can be trained on different dataset of exercises resulting in good accuracy.\ngoogle cloud/ firebase. google cloud to create and access data and for creating chatbots and other ml models.\nchatbot using google\u2019s dialogueflow. dialogflow is a natural language understanding platform used to design and integrate a conversational user interface into mobile apps bots, interactive voice response systems. the chatbot will help the user to interact and to solve their queries.\nimmune system checker. it will help to generate the health report of a person and to recommend activities.\nbmi calculator. to check body mass index.\nchallenges we ran into\nfreak uses the neural network, and classic computer vision algorithms, with a 95% accuracy of recognition of asanas, which includes a data set of more than a thousand images. this dataset acts as training for the neural network and therefore is the most important aspect of the app. these representative datasets are hard to create, particularly when it comes to finding several images of people in a particular pose. these images should contain different angles, and because different people in their ordinary lives have performed different yoga poses, it becomes difficult to sync them with the existing systems. along with the representative dataset, another problem faced by the app is the visibility of the body. now, if a person is standing near a window or bright light, it might be difficult for the camera to catch certain parts of the body or it might blend the body\u2019s outline with a large object behind. so, while using freak, it is not recommended to keep the camera against bright light or close to a window.\nwhat's next for freak - your ai fitness coach\nthe current app is presently accessible in english, but i intend to expand to include other languages in the future. the model is presently trained on a limited number of datasets; however, the dataset may be increased by including additional yoga postures done by humans in both indoor and outdoor settings to improve the model's performance.\nwith the aid of sensors, i plan to include additional health-related analyzers in the future, such as tracking footsteps, heart rate, and blood pressure. also, a reminder system that notifies the user when it's time to drink, take medicine, or exercise.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59507391}, {"Unnamed: 0": 7730, "autor": "SparkWalk", "date": null, "content": "Inspiration\nThis project was inspired by my love of walking. We all need more outdoor time, but people often feel like walking is pointless unless they have somewhere to go. I have fond memories of spending hours walking around just to play Pokemon Go, so I wanted to create something that would give people a reason to go somewhere new. I envision friends and family sending mystery locations to their loved ones with a secret message, picture, or video that will be revealed when they arrive. You could send them to a historical landmark, a beautiful park, or just like a neat rock you saw somewhere. The possibilities are endless!\nWhat it does\nYou want to go out for a walk, but where to? SparkWalk offers users their choice of exciting \"mystery walks\". Given a secret location, the app tells you which direction to go and roughly how long it will take. When you get close to your destination, the app welcomes you with a message. For now, SparkWalk has just a few preset messages and locations, but the ability for users to add their own and share them with others is coming soon.\nHow we built it\nSparkWalk was created using Expo for React Native. The map and location functionalities were implemented using the react-native-maps, expo-location, and geolib libraries.\nChallenges we ran into\nStyling components for different devices is always tricky! Unfortunately, I didn't have time to ensure the styling works on every device, but it works well on at least one iOS and one Android device that I tested it on.\nAccomplishments that we're proud of\nThis is my first time using geolocation and integrating a map, so I'm proud that I was able to make it work.\nWhat we learned\nI've learned a lot more about how to work with React Native, especially using state and effect hooks.\nWhat's next for SparkWalk\nNext, I plan to add user authentication and the ability to add friends and send locations to each other. Users will be able to store messages for their friends that are tied to specific locations. I'll add a backend server and a database to host saved locations and messages. I also want to add reward cards for visiting locations that can be saved to the user's profile and reviewed later. Eventually, I'll publish the app so anyone can use it!", "link": "https://devpost.com/software/sparkwalk", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "outdoor", "selectorShort": "outdoor", "MarkedSent": "inspiration\nthis project was inspired by my love of walking. we all need more -----> outdoor !!!  time, but people often feel like walking is pointless unless they have somewhere to go. i have fond memories of spending hours walking around just to play pokemon go, so i wanted to create something that would give people a reason to go somewhere new. i envision friends and family sending mystery locations to their loved ones with a secret message, picture, or video that will be revealed when they arrive. you could send them to a historical landmark, a beautiful park, or just like a neat rock you saw somewhere. the possibilities are endless!\nwhat it does\nyou want to go out for a walk, but where to? sparkwalk offers users their choice of exciting \"mystery walks\". given a secret location, the app tells you which direction to go and roughly how long it will take. when you get close to your destination, the app welcomes you with a message. for now, sparkwalk has just a few preset messages and locations, but the ability for users to add their own and share them with others is coming soon.\nhow we built it\nsparkwalk was created using expo for react native. the map and location functionalities were implemented using the react-native-maps, expo-location, and geolib libraries.\nchallenges we ran into\nstyling components for different devices is always tricky! unfortunately, i didn't have time to ensure the styling works on every device, but it works well on at least one ios and one android device that i tested it on.\naccomplishments that we're proud of\nthis is my first time using geolocation and integrating a map, so i'm proud that i was able to make it work.\nwhat we learned\ni've learned a lot more about how to work with react native, especially using state and effect hooks.\nwhat's next for sparkwalk\nnext, i plan to add user authentication and the ability to add friends and send locations to each other. users will be able to store messages for their friends that are tied to specific locations. i'll add a backend server and a database to host saved locations and messages. i also want to add reward cards for visiting locations that can be saved to the user's profile and reviewed later. eventually, i'll publish the app so anyone can use it!", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 1, "media": null, "medialink": null, "identifyer": 59507730}, {"Unnamed: 0": 8416, "autor": "TILLY", "date": null, "content": "What it is\nTILLY is a head-worn lightweight and wireless pair of glasses, which can be controlled either through a wireless handheld controller or a set of voice commands. It is designed for visually impaired and legally blind to help reduce barriers and make the world more accessible for people with disabilities, giving them more access to information, communication, and independence.\nWhy Tilly?\nThe device is named after Matilda (Tilly) Aston \u2013 writer and teacher who had spent many years promoting for the vision-impaired people\u2019s rights.\nWhat it does\nThe device evaluates the environment with RGB cameras, thermal infrared camera and ultrasound sensors, provides data processing with Computer Vision and Machine Learning on separate mobile computer connected to the glasses with a cable or wirelessly and with sound signal and vibration informs the user about objects. Voice commands are handled by stereo microphones. Self-adjusting ergonomic arms spread pressure evenly across the temple. The features of the device are as follows: pinpoint and indoor navigation; outdoor GPS navigation; object and on-demand wiki; hazard alerts; area discovery; onboard LTE; \u201cHey, Tilly\u201d? \u2013 the voice assistant; emergency dial (automated).", "link": "https://devpost.com/software/tilly", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "outdoor", "selectorShort": "outdoor", "MarkedSent": "what it is\ntilly is a head-worn lightweight and wireless pair of glasses, which can be controlled either through a wireless handheld controller or a set of voice commands. it is designed for visually impaired and legally blind to help reduce barriers and make the world more accessible for people with disabilities, giving them more access to information, communication, and independence.\nwhy tilly?\nthe device is named after matilda (tilly) aston \u2013 writer and teacher who had spent many years promoting for the vision-impaired people\u2019s rights.\nwhat it does\nthe device evaluates the environment with rgb cameras, thermal infrared camera and ultrasound sensors, provides data processing with computer vision and machine learning on separate mobile computer connected to the glasses with a cable or wirelessly and with sound signal and vibration informs the user about objects. voice commands are handled by stereo microphones. self-adjusting ergonomic arms spread pressure evenly across the temple. the features of the device are as follows: pinpoint and indoor navigation; -----> outdoor !!!  gps navigation; object and on-demand wiki; hazard alerts; area discovery; onboard lte; \u201chey, tilly\u201d? \u2013 the voice assistant; emergency dial (automated).", "sortedWord": "None", "removed": "Nan", "score": 69, "comments": 0, "media": null, "medialink": null, "identifyer": 59508416}], "name": "outdoorDevpost"}
{"interestingcomments": [{"Unnamed: 0": 1582, "autor": "Hobbits Of Polygon", "date": null, "content": "Hobbits Of Polygon\nHobbits of polygon is web3 based RPG game with an algorithmic engine made using chainlink VRF on Polygon.\nAbout Project\nHelp the Dwarf king to rebuild his homeland all made up of NFT.\nThe game is built using phaser and solidity deployed on Polygon blockchain with help of truffle.\nThere are multiple maps of the game each map sets different attributes of players and characters.\nThe game engine and attributes such as Health, enemy damage, etc are set by an algorithmic engine using chainlink VRF exhibiting the powerful use case of VRF not just for randomness but as to set up a game's meta.\nMeta can be described as an advantage, the disadvantage of characters on a certain map, or against particular characters. (Example:- 1) water type pokemon are stronger against fire type but weeker against the grass-type pokemon and this even implies on the map which the pokemon are fighting), with respect to this scenario, the meta and backend logic is created with chainlink VRF on the Polygon POS network.\nDemo Video\n*https://www.youtube.com/watch?v=eqVygMZc9GY\nWhat it does\n\ud83c\udfae User connects the Metamask to the game.\n\ud83c\udfae User pays a certain amount of entry fees to the game. (During test phases the test gold tokens can be obtained by faucet).\n\ud83c\udfae User triggers the chainlink VRF logic setup by entering the game.\n\ud83c\udfae The VRF sets the characters attributes and a random map from the multiple options is chosen.\n\ud83c\udfae If users win the game by overcoming all the enemies and collecting all the chest of gold user is rewarded with a 2x bounty and NFT reward.\n\ud83c\udfae If users lose their hp lesser than zero he/she losses.\nGlimpse of the Game\nConnect MetaMask Wallet\nDungeon Map\nForest Map\nIce Map\nHow we built it\nWe used phaser.js as a game engine.\nEthers library to interact with the blockchain.\ntruffle to deploy contracts on polygon chain.\nIPFS to store metadata of NFT.\ntiled to create game maps and vs code editor.\nChallenges we ran into\nIt was difficult to set dynamic in-game attributes which we overcame by using VRF.\nRinkeby chain was kinda slow which was disturbing the game flow which we overcame using polygon layer2 chain for faster transaction.\nproperly parse the bigNumber obtained as a random number from Chainlink VRF to integer as a game object.\nAccomplishments that we're proud of\nWe are proud that we were able to make an MVP for the project. We plan to continue to build on this and make a big project.\nMetaverse\nGameFi\nPlay2Earn Game\nRewards and off-chain storage\nWhat we learned\nChainlink VRF implementation\nMinting and distribution on NFTs,\nSwitching between multiple maps in the phaser game.\nWhat's next for Hobbits Of Polygon\nWe are working on an escrow system so players can challenge one another.\nWe are adding additional characters to the game which can be traded on the NFT marketplace.\nWe have implemented the lobby using socket.io so people can feel it as a metaverse.", "link": "https://devpost.com/software/hobbits-of-polygon", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "grass", "selectorShort": "grass", "MarkedSent": "hobbits of polygon\nhobbits of polygon is web3 based rpg game with an algorithmic engine made using chainlink vrf on polygon.\nabout project\nhelp the dwarf king to rebuild his homeland all made up of nft.\nthe game is built using phaser and solidity deployed on polygon blockchain with help of truffle.\nthere are multiple maps of the game each map sets different attributes of players and characters.\nthe game engine and attributes such as health, enemy damage, etc are set by an algorithmic engine using chainlink vrf exhibiting the powerful use case of vrf not just for randomness but as to set up a game's meta.\nmeta can be described as an advantage, the disadvantage of characters on a certain map, or against particular characters. (example:- 1) water type pokemon are stronger against fire type but weeker against the -----> grass !!! -type pokemon and this even implies on the map which the pokemon are fighting), with respect to this scenario, the meta and backend logic is created with chainlink vrf on the polygon pos network.\ndemo video\n*https://www.youtube.com/watch?v=eqvygmzc9gy\nwhat it does\n\ud83c\udfae user connects the metamask to the game.\n\ud83c\udfae user pays a certain amount of entry fees to the game. (during test phases the test gold tokens can be obtained by faucet).\n\ud83c\udfae user triggers the chainlink vrf logic setup by entering the game.\n\ud83c\udfae the vrf sets the characters attributes and a random map from the multiple options is chosen.\n\ud83c\udfae if users win the game by overcoming all the enemies and collecting all the chest of gold user is rewarded with a 2x bounty and nft reward.\n\ud83c\udfae if users lose their hp lesser than zero he/she losses.\nglimpse of the game\nconnect metamask wallet\ndungeon map\nforest map\nice map\nhow we built it\nwe used phaser.js as a game engine.\nethers library to interact with the blockchain.\ntruffle to deploy contracts on polygon chain.\nipfs to store metadata of nft.\ntiled to create game maps and vs code editor.\nchallenges we ran into\nit was difficult to set dynamic in-game attributes which we overcame by using vrf.\nrinkeby chain was kinda slow which was disturbing the game flow which we overcame using polygon layer2 chain for faster transaction.\nproperly parse the bignumber obtained as a random number from chainlink vrf to integer as a game object.\naccomplishments that we're proud of\nwe are proud that we were able to make an mvp for the project. we plan to continue to build on this and make a big project.\nmetaverse\ngamefi\nplay2earn game\nrewards and off-chain storage\nwhat we learned\nchainlink vrf implementation\nminting and distribution on nfts,\nswitching between multiple maps in the phaser game.\nwhat's next for hobbits of polygon\nwe are working on an escrow system so players can challenge one another.\nwe are adding additional characters to the game which can be traded on the nft marketplace.\nwe have implemented the lobby using socket.io so people can feel it as a metaverse.", "sortedWord": "None", "removed": "Nan", "score": 13, "comments": 0, "media": null, "medialink": null, "identifyer": 59501582}, {"Unnamed: 0": 3733, "autor": "Generating Image Descriptions", "date": null, "content": "Generating Image Descriptions\nFinal Writeup\nWriteup here!\nGithub\nGithub Repo\nCheck Point 2\nIntroduction\nWe are implementing an existing paper, and the objective of this paper is to generate text descriptions of images. Contrary to previous papers that have focused on retrieving image descriptions from GPS metadata to access relevant text documents, the method used in this paper first detects objects, modifiers (adjectives), and spatial relationships (prepositions) in the image and then uses either an n-gram language model or a simple template based approach to generate the sentences/captions. This method puts more emphasis on the objects and their positions/spatial orientation and attributes. This problem is both a classification problem and a natural language processing problem.\nWe chose this paper because it combined many of our interests: computer vision, classification, and natural language processing. Generating image descriptions that closely resemble human speech while also providing positional orientation and descriptors for the objects found in the picture effectively mimic what it\u2019s like to see the image. For people who are visually impaired, having effective text descriptions of images will allow them to better understand what is going on in images.\nChallenges\nOne challenge was storing and moving large files, specifically, features.pkl, which was produced in preprocessing. Since features.pkl was over the size limit for pushing to git, we learned how to use git-lfs (large file storage) to move the file into the remote repository.\nAnother challenge we have encountered is dealing with differences in the original paper and our project. We chose to make our base and target goals more simple than the paper, because the paper deals with certain architectures that we have not yet learned. We made our target goal to have our model recognize primary objects in an image with modifiers. Our caption would then just consist of the objects and their modifiers. Because of this simplification, we have been drawing on other related works as our primary sources for the architecture of this project. This is due to the fact that parsing/dividing the original research paper into implementation-specific details relevant to our current goals was nearly impossible. The integration of different sources makes designing our model more difficult, because we need to continuously analyze the differences in their implementations and purposes.\nInsights\nAccording to the paper \u201cWhere to put the Image in an Image Caption Generator\u201d, these are the ranges of \u201cgood\u201d BLEU scores at each stage:\nBLEU-1: 0.401 to 0.578\nBLEU-2: 0.176 to 0.390\nBLEU-3: 0.099 to 0.260\nBLEU-4: 0.059 to 0.170\nAlthough we are aiming for these scores, at the moment, we do not have BLEU scores since we are debugging our testing code. However, we have a concrete plan as for how to debug and should be able to see the scores soon. As for other concrete results that we can show, we have produced 1) descriptions.txt, a file storing the dictionary of image identifiers mapped to descriptions and 2) features.pkl, a file storing the dictionary of the extracted features for each image.\nPlan\nFurther Work\nWe are on track so far. We need to dedicate more time to researching other similar research papers and projects that will help us bridge the difference between our current implementation and the one used in the original research paper we referenced. We also need to dedicate more time to trying potential ways to bridge this gap.\nPotential Changes\nWe are thinking of changing the way our captions are generated. Our current implementation uses a dataset of pregenerated descriptions, and based on the image the model pulls from this dataset the model matches it to one of the pre generated descriptions. However, we want our model to be able to generate sentences using an N-gram, drawing from a dictionary of individual words rather than pre-generated sentences.\nCheck Point 1\nOverview\nOur project will generate image descriptions by analyzing the objects in each input image and recording its attributes and orientation. It will then generate a description of the image scene based on the objects and attributes recorded.\ntldr; take in images as input and generate sentences.\nWho\nRia Rajesh [rrajesh], Mandy He [mhe26], and Sophia Liu [sliu176]\nIntroduction\nWe are implementing an existing paper, and the objective of this paper is to generate text descriptions of images. Contrary to previous papers that have focused on retrieving image descriptions from GPS metadata to access relevant text documents, the method used in this paper first detects objects, modifiers (adjectives), and spatial relationships (prepositions) in the image and then uses either an n-gram language model or a simple template based approach to generate the sentences/captions. This method puts more emphasis on the objects and their positions/spatial orientation and attributes. This problem is both a classification problem and a natural language processing problem. We chose this paper because it combined many of our interests: computer vision, classification, and natural language processing. Generating image descriptions that closely resemble human speech while also providing positional orientation and descriptors for the objects found in the picture effectively mimic what it\u2019s like to see the image. For people who are visually impaired, having effective text descriptions of images will allow them to better understand what is going on in images.\nRelated Work\nWe found no public implementations of the paper. However, in a web article called \u201cHow to Develop a Deep Learning Photo Caption Generator from Scratch\u201d, we found code related to our topic written in Tensorflow. This article goes through how to obtain the photo and caption dataset, how to prepare the photo data with the Oxford Visual Geometry Group and pre-compute photo features, how to prepare the text data through data cleaning, and how to develop the deep learning model. The paper also explains how to evaluate the model and how to generate new captions.\nData\nWe plan to use training images from Flickr, Google, the attribute dataset provided by Farhadi et al, and ImageNet, as used in the paper. As for image descriptions, we will collect them by querying the Flickr API with the objects\u2019 categories. We aim to gather upwards of 50,000 descriptions and parse them with the Stanford dependency parser.\nMethodology\nArchitecture\nPrimary objects (person, bus, etc.) and surrounding objects (grass, sky, etc.) detectors find candidate objects. Each candidate region is processed by a set of attribute classifiers. Each pair of candidate regions is processed by prepositional relationship functions. A CRF (conditional random field) is constructed that incorporates the unary image potentials computed in the steps above and higher order text based potentials computed from large document corpora. A labeling of the graph is predicted. Sentences are generated using an N-gram model based on the labeling.\nHow are we training the model (CRF) -> with 100 hand-labeled images. (section 4.2 of paper)\n4) CRF is used to predict best labeling for an object: Nodes correspond to object things or stuff, attributes of appearance of object, prepositions relating to spatial relationship between objects.\nSince preposition nodes describe the relationship between a preposition label and two object labels (obj-prep-obj), they are most often modeled through trinary potential functions. However we must convert this to a set of unary and pairwise potentials through a z-node.\nBackup Ideas If we run into issues, we plan to simplify the project and remove the spatial relationship of objects and just generate captions describing objects and their attributes. We can also use the methods described in the papers of related projects (see related work section).\nMetrics\nWe plan to perform experiments where we input an image into the model and manually examine the outputted caption. We then determine whether the caption was generated correctly given the image contents. The notion of accuracy does apply to our project, but perhaps not as strictly as accuracy may apply to a project that just classifies objects in images. Although classifying images is a part of our project, our project\u2019s goal is to generate captions in which there are many potentially accurate captions possible for a given image. In the existing paper, the authors manually scored the generated image captions in three categories (quality of image parsing, language model-based, and template-based generation), hoping to find high average scores. The scores were on a scale of 1 to 4, where 4 was perfect without error, 3 was good with some errors, 2 was many errors, and 1 was failure. The results were an average score of 3.49 for the quality of generated sentences (very high score!), and a relatively high score for predicting image content: 2.85.\nGoal of the authors of this paper\nThe authors of this paper sought to generate captions rather than just retrieve and summarize captions from textual documents related to the images like previous similar projects/papers have done. Many previous projects leading up to this paper put little emphasis on the spatial relationships between objects. The authors of this paper also wanted spatial relationships between objects to be an important part of their project.\nBase, target, and stretch goals\nBase goal = what you think you definitely can achieve by the final due date.\nRecognize primary objects in an image. Our caption would then just consist of the objects.\nTarget goal = what you think you should be able to achieve by the due date. Recognize primary objects in an image with modifiers.\nOur caption would then just consist of the objects and their modifiers\nStretch goal = what you want to do if you exceed your target goal.\nInclude spatial relationships in captions. This would mean our results fully match those described in the paper.\nEthics\nWhy is Deep Learning a good approach to this problem?\nDeep Learning is a good approach to this problem because it is unfeasible for text descriptions to be generated by a human for every image. Developing a successful technique for developing these descriptions using deep learning will be much easier and more practical in the long run to be used by a screen reader for people who are visually impaired, for example.\nHow are you planning to quantify or measure error or success? What implications does your quantification have?\nWe are planning to measure error and success based on if the sentences outputted are an accurate representation of the input image. For example, if we input a picture of a black dog we would expect the output sentence to describe the image as including a black dog. Because our algorithm has to categorize objects in an image, it could make certain assumptions about these objects in the image that may increase specificity. We would want to limit assumptions like this as much as possible. For example, given an image of a person, we would not want the algorithm to describe them as a \u201cman\u201d or a \u201cwoman\u201d to avoid potentially harmful stereotypes. We would choose rather to describe the image by saying a \u201cperson\u201d. We also noticed that in the paper one of the images described included the description of a \u201cblack person\u201d, which was a person wearing black clothing. We would want to avoid categorizing this as a \u201cblack person\u201d.\nDivision of labor\nThis project is quite the challenge for each of us individually. Moreover, this project divides naturally into two heavy parts (classification and caption generation). Our group agreed to meet and tackle each part together during group meetings to bounce ideas off each other and work as a group through discussion and pair-programming. We plan to swap who is the driver each meeting.", "link": "https://devpost.com/software/generating-image-descriptions", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "grass", "selectorShort": "grass", "MarkedSent": "generating image descriptions\nfinal writeup\nwriteup here!\ngithub\ngithub repo\ncheck point 2\nintroduction\nwe are implementing an existing paper, and the objective of this paper is to generate text descriptions of images. contrary to previous papers that have focused on retrieving image descriptions from gps metadata to access relevant text documents, the method used in this paper first detects objects, modifiers (adjectives), and spatial relationships (prepositions) in the image and then uses either an n-gram language model or a simple template based approach to generate the sentences/captions. this method puts more emphasis on the objects and their positions/spatial orientation and attributes. this problem is both a classification problem and a natural language processing problem.\nwe chose this paper because it combined many of our interests: computer vision, classification, and natural language processing. generating image descriptions that closely resemble human speech while also providing positional orientation and descriptors for the objects found in the picture effectively mimic what it\u2019s like to see the image. for people who are visually impaired, having effective text descriptions of images will allow them to better understand what is going on in images.\nchallenges\none challenge was storing and moving large files, specifically, features.pkl, which was produced in preprocessing. since features.pkl was over the size limit for pushing to git, we learned how to use git-lfs (large file storage) to move the file into the remote repository.\nanother challenge we have encountered is dealing with differences in the original paper and our project. we chose to make our base and target goals more simple than the paper, because the paper deals with certain architectures that we have not yet learned. we made our target goal to have our model recognize primary objects in an image with modifiers. our caption would then just consist of the objects and their modifiers. because of this simplification, we have been drawing on other related works as our primary sources for the architecture of this project. this is due to the fact that parsing/dividing the original research paper into implementation-specific details relevant to our current goals was nearly impossible. the integration of different sources makes designing our model more difficult, because we need to continuously analyze the differences in their implementations and purposes.\ninsights\naccording to the paper \u201cwhere to put the image in an image caption generator\u201d, these are the ranges of \u201cgood\u201d bleu scores at each stage:\nbleu-1: 0.401 to 0.578\nbleu-2: 0.176 to 0.390\nbleu-3: 0.099 to 0.260\nbleu-4: 0.059 to 0.170\nalthough we are aiming for these scores, at the moment, we do not have bleu scores since we are debugging our testing code. however, we have a concrete plan as for how to debug and should be able to see the scores soon. as for other concrete results that we can show, we have produced 1) descriptions.txt, a file storing the dictionary of image identifiers mapped to descriptions and 2) features.pkl, a file storing the dictionary of the extracted features for each image.\nplan\nfurther work\nwe are on track so far. we need to dedicate more time to researching other similar research papers and projects that will help us bridge the difference between our current implementation and the one used in the original research paper we referenced. we also need to dedicate more time to trying potential ways to bridge this gap.\npotential changes\nwe are thinking of changing the way our captions are generated. our current implementation uses a dataset of pregenerated descriptions, and based on the image the model pulls from this dataset the model matches it to one of the pre generated descriptions. however, we want our model to be able to generate sentences using an n-gram, drawing from a dictionary of individual words rather than pre-generated sentences.\ncheck point 1\noverview\nour project will generate image descriptions by analyzing the objects in each input image and recording its attributes and orientation. it will then generate a description of the image scene based on the objects and attributes recorded.\ntldr; take in images as input and generate sentences.\nwho\nria rajesh [rrajesh], mandy he [mhe26], and sophia liu [sliu176]\nintroduction\nwe are implementing an existing paper, and the objective of this paper is to generate text descriptions of images. contrary to previous papers that have focused on retrieving image descriptions from gps metadata to access relevant text documents, the method used in this paper first detects objects, modifiers (adjectives), and spatial relationships (prepositions) in the image and then uses either an n-gram language model or a simple template based approach to generate the sentences/captions. this method puts more emphasis on the objects and their positions/spatial orientation and attributes. this problem is both a classification problem and a natural language processing problem. we chose this paper because it combined many of our interests: computer vision, classification, and natural language processing. generating image descriptions that closely resemble human speech while also providing positional orientation and descriptors for the objects found in the picture effectively mimic what it\u2019s like to see the image. for people who are visually impaired, having effective text descriptions of images will allow them to better understand what is going on in images.\nrelated work\nwe found no public implementations of the paper. however, in a web article called \u201chow to develop a deep learning photo caption generator from scratch\u201d, we found code related to our topic written in tensorflow. this article goes through how to obtain the photo and caption dataset, how to prepare the photo data with the oxford visual geometry group and pre-compute photo features, how to prepare the text data through data cleaning, and how to develop the deep learning model. the paper also explains how to evaluate the model and how to generate new captions.\ndata\nwe plan to use training images from flickr, google, the attribute dataset provided by farhadi et al, and imagenet, as used in the paper. as for image descriptions, we will collect them by querying the flickr api with the objects\u2019 categories. we aim to gather upwards of 50,000 descriptions and parse them with the stanford dependency parser.\nmethodology\narchitecture\nprimary objects (person, bus, etc.) and surrounding objects (-----> grass !!! , sky, etc.) detectors find candidate objects. each candidate region is processed by a set of attribute classifiers. each pair of candidate regions is processed by prepositional relationship functions. a crf (conditional random field) is constructed that incorporates the unary image potentials computed in the steps above and higher order text based potentials computed from large document corpora. a labeling of the graph is predicted. sentences are generated using an n-gram model based on the labeling.\nhow are we training the model (crf) -> with 100 hand-labeled images. (section 4.2 of paper)\n4) crf is used to predict best labeling for an object: nodes correspond to object things or stuff, attributes of appearance of object, prepositions relating to spatial relationship between objects.\nsince preposition nodes describe the relationship between a preposition label and two object labels (obj-prep-obj), they are most often modeled through trinary potential functions. however we must convert this to a set of unary and pairwise potentials through a z-node.\nbackup ideas if we run into issues, we plan to simplify the project and remove the spatial relationship of objects and just generate captions describing objects and their attributes. we can also use the methods described in the papers of related projects (see related work section).\nmetrics\nwe plan to perform experiments where we input an image into the model and manually examine the outputted caption. we then determine whether the caption was generated correctly given the image contents. the notion of accuracy does apply to our project, but perhaps not as strictly as accuracy may apply to a project that just classifies objects in images. although classifying images is a part of our project, our project\u2019s goal is to generate captions in which there are many potentially accurate captions possible for a given image. in the existing paper, the authors manually scored the generated image captions in three categories (quality of image parsing, language model-based, and template-based generation), hoping to find high average scores. the scores were on a scale of 1 to 4, where 4 was perfect without error, 3 was good with some errors, 2 was many errors, and 1 was failure. the results were an average score of 3.49 for the quality of generated sentences (very high score!), and a relatively high score for predicting image content: 2.85.\ngoal of the authors of this paper\nthe authors of this paper sought to generate captions rather than just retrieve and summarize captions from textual documents related to the images like previous similar projects/papers have done. many previous projects leading up to this paper put little emphasis on the spatial relationships between objects. the authors of this paper also wanted spatial relationships between objects to be an important part of their project.\nbase, target, and stretch goals\nbase goal = what you think you definitely can achieve by the final due date.\nrecognize primary objects in an image. our caption would then just consist of the objects.\ntarget goal = what you think you should be able to achieve by the due date. recognize primary objects in an image with modifiers.\nour caption would then just consist of the objects and their modifiers\nstretch goal = what you want to do if you exceed your target goal.\ninclude spatial relationships in captions. this would mean our results fully match those described in the paper.\nethics\nwhy is deep learning a good approach to this problem?\ndeep learning is a good approach to this problem because it is unfeasible for text descriptions to be generated by a human for every image. developing a successful technique for developing these descriptions using deep learning will be much easier and more practical in the long run to be used by a screen reader for people who are visually impaired, for example.\nhow are you planning to quantify or measure error or success? what implications does your quantification have?\nwe are planning to measure error and success based on if the sentences outputted are an accurate representation of the input image. for example, if we input a picture of a black dog we would expect the output sentence to describe the image as including a black dog. because our algorithm has to categorize objects in an image, it could make certain assumptions about these objects in the image that may increase specificity. we would want to limit assumptions like this as much as possible. for example, given an image of a person, we would not want the algorithm to describe them as a \u201cman\u201d or a \u201cwoman\u201d to avoid potentially harmful stereotypes. we would choose rather to describe the image by saying a \u201cperson\u201d. we also noticed that in the paper one of the images described included the description of a \u201cblack person\u201d, which was a person wearing black clothing. we would want to avoid categorizing this as a \u201cblack person\u201d.\ndivision of labor\nthis project is quite the challenge for each of us individually. moreover, this project divides naturally into two heavy parts (classification and caption generation). our group agreed to meet and tackle each part together during group meetings to bounce ideas off each other and work as a group through discussion and pair-programming. we plan to swap who is the driver each meeting.", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 23, "media": null, "medialink": null, "identifyer": 59503733}, {"Unnamed: 0": 6108, "autor": "GrassRoots", "date": null, "content": "Inspiration\nFor some time, corporations and governments have been redirecting the responsibility of addressing climate change onto individuals. For example, the idea of a person's \"carbon footprint\" was actually popularized by British Petroleum 16 years ago to divert the attention of climate responsibility from the company onto the individual. This strategy has expanded to other fossil fuel and plastic companies as well, which is meant to make us believe that only individual actions will save us from climate change. While it is true that we need all the help we can get, the vast majority of our ability to address climate change lies in the hands of a select few people in power who refuse to do what is necessary. A recent study found that just 100 companies are responsible for 71% of global emissions. Systemic change\u2014as opposed to individual change\u2014is far more valuable, yet we see so little of it. It\u2019s easy to lose hope and think there\u2019s nothing one person can do to help. However, we believe there is. One way that people can make a difference in this fight is by leading a grassroots movement and joining together to call on these companies, governments, and organizations to change.\nWhat it does\nGrassRoots is a mobile app for climate activism. You may have been told by activists to contact your local representatives about policies that matter to you. We were inspired to expand upon this idea and create an ecosystem where people can pressure those in power to make advances in stopping climate change. In GrassRoots, users can create posts that explain how a certain person or organization is in a position to cause change, and then attach these people\u2019s contact info and encourage other users to reach out and express the need to cause this change. For example, let\u2019s say I knew that the Governor of Florida was expected to veto an important climate bill. I could create a post that explains this situation and attach the governor\u2019s twitter handle and email address to the post. Other users on the app could see my post and upvote it so that more people can see it too. Then, in seconds, they can tap on the contact info that I posted and automatically be taken to the governor\u2019s twitter page or automatically open an email draft addressed to him. Ideally, the governor would receive a flood of tweets and emails showing how much people care about this issue, and hopefully choose to not veto the bill. Altogether, this creates a unique community that strengthens each other through a grassroots movement engaged in stopping climate change.\nHow we built it\nGrass Roots is an iOS and Android mobile application built using React Native, JavaScript, NativeBase, Redux, Firebase, and Expo. The frontend and backend are both built in JavaScript using the React Native framework. We used NativeBase as a UI library for the frontend, Redux for state management, Firebase for user authentication and cloud databasing, and expo for testing and simulating. We simultaneously worked on different parts of the applications and pushed our changes to different branches in GitHub.\nChallenges we ran into\nWe had several problems with git merges and errors that popped up because someone had made changes in the backend that the frontend was not ready to handle yet. Also, we ran into difficulties with simulating the app on Android Emulator because of incompatible versions of the expo-cli package.\nAccomplishments that we're proud of\nWe\u2019re very proud of our app\u2019s functionality and close resemblance to what we had envisioned. We believe our UI is streamlined, clean, and intuitive to use.\nWhat we learned\nWe learned how to make a mobile application using React Native as the framework for the front end. Half of our team was working with React Native for the first time, and the project was definitely a learning experience. Working on the app required us to pick up concepts as we went; limited familiarity with using VS Code and Expo offered a lot of opportunities to learn. We also gained experience using a cloud system such as Firebase to store data and for programming the back end of a mobile application.\nWhat's next for GrassRoots\nIn the future, we hope to patch bugs and expand the functionalities of our app. We plan on letting users tag specific topics to discuss, and also expand the number of contact platforms that our app supports. We currently have integrated support for Twitter, Facebook, Email, Instagram, and Phone . We also hope to expand GrassRoots to cover other areas of activism besides climate change, such as gender equity, racial justice, and LGBTQ+ rights.", "link": "https://devpost.com/software/grass-roots-kej3cm", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "grass", "selectorShort": "grass", "MarkedSent": "inspiration\nfor some time, corporations and governments have been redirecting the responsibility of addressing climate change onto individuals. for example, the idea of a person's \"carbon footprint\" was actually popularized by british petroleum 16 years ago to divert the attention of climate responsibility from the company onto the individual. this strategy has expanded to other fossil fuel and plastic companies as well, which is meant to make us believe that only individual actions will save us from climate change. while it is true that we need all the help we can get, the vast majority of our ability to address climate change lies in the hands of a select few people in power who refuse to do what is necessary. a recent study found that just 100 companies are responsible for 71% of global emissions. systemic change\u2014as opposed to individual change\u2014is far more valuable, yet we see so little of it. it\u2019s easy to lose hope and think there\u2019s nothing one person can do to help. however, we believe there is. one way that people can make a difference in this fight is by leading a grassroots movement and joining together to call on these companies, governments, and organizations to change.\nwhat it does\ngrassroots is a mobile app for climate activism. you may have been told by activists to contact your local representatives about policies that matter to you. we were inspired to expand upon this idea and create an ecosystem where people can pressure those in power to make advances in stopping climate change. in grassroots, users can create posts that explain how a certain person or organization is in a position to cause change, and then attach these people\u2019s contact info and encourage other users to reach out and express the need to cause this change. for example, let\u2019s say i knew that the governor of florida was expected to veto an important climate bill. i could create a post that explains this situation and attach the governor\u2019s twitter handle and email address to the post. other users on the app could see my post and upvote it so that more people can see it too. then, in seconds, they can tap on the contact info that i posted and automatically be taken to the governor\u2019s twitter page or automatically open an email draft addressed to him. ideally, the governor would receive a flood of tweets and emails showing how much people care about this issue, and hopefully choose to not veto the bill. altogether, this creates a unique community that strengthens each other through a grassroots movement engaged in stopping climate change.\nhow we built it\n-----> grass !!!  roots is an ios and android mobile application built using react native, javascript, nativebase, redux, firebase, and expo. the frontend and backend are both built in javascript using the react native framework. we used nativebase as a ui library for the frontend, redux for state management, firebase for user authentication and cloud databasing, and expo for testing and simulating. we simultaneously worked on different parts of the applications and pushed our changes to different branches in github.\nchallenges we ran into\nwe had several problems with git merges and errors that popped up because someone had made changes in the backend that the frontend was not ready to handle yet. also, we ran into difficulties with simulating the app on android emulator because of incompatible versions of the expo-cli package.\naccomplishments that we're proud of\nwe\u2019re very proud of our app\u2019s functionality and close resemblance to what we had envisioned. we believe our ui is streamlined, clean, and intuitive to use.\nwhat we learned\nwe learned how to make a mobile application using react native as the framework for the front end. half of our team was working with react native for the first time, and the project was definitely a learning experience. working on the app required us to pick up concepts as we went; limited familiarity with using vs code and expo offered a lot of opportunities to learn. we also gained experience using a cloud system such as firebase to store data and for programming the back end of a mobile application.\nwhat's next for grassroots\nin the future, we hope to patch bugs and expand the functionalities of our app. we plan on letting users tag specific topics to discuss, and also expand the number of contact platforms that our app supports. we currently have integrated support for twitter, facebook, email, instagram, and phone . we also hope to expand grassroots to cover other areas of activism besides climate change, such as gender equity, racial justice, and lgbtq+ rights.", "sortedWord": "None", "removed": "Nan", "score": 27, "comments": 1, "media": null, "medialink": null, "identifyer": 59506108}], "name": "grassDevpost"}
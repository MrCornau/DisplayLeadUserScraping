{"interestingcomments": [{"Unnamed: 0": 111, "autor": "QB Mastermind", "date": null, "content": "Inspiration\nPresenting our solution for Quickbase Appathon 2021 Q B Mastermind . With key focus on this Appathon on modernizing productivity bottlenecks and business inefficiencies that are holding back the team, we are presenting a solution which can solve the problem faced by application operations teams from more than 2 years.\nWhat it does\nReporting is a very important task for any team and more important for the operations team to give the insights of IT Applications health to business stakeholders. For multiple application in production there are many best in class tools available to assist different processes, but, the restriction for custom reporting automation is affecting teams productivity in reporting. 12 to 15 percent of team's bandwidth is used for manually preparing excel based reports by extracting data from ITSM tools, and then manually adding the data and logic in excel reports. Similar set of tasks are done by more than 5 teams for their respective tracks. In the past there were multiple initiatives to automate portions of this manual reporting process but No End To End Automation achieved yet. Multiple pain points including recurring mundane tasks, error in manual report, high bandwidth utilization, data duplication, manual emails and no option for data validation for the stake holders.\nSolution Features -\n100% ROI in < 1 Week: Solution built in less than 24 hours and the effort the solution save will ensure that the return of investment is less than a week.\nKPI Based Reports: The KPI Based reports will give meaningful insights to the business stakeholders regarding application health and team performance.\nAutomated Reports: Realtime Integration with tools and automated report generation\nSignificant Cost Savings: 450 hours/year savings for 1 Team, *2000 hours/year savings for 5 Teams\nEnd to End Automation: Complete automation for the reporting workflows, no manual inputs needed.\nReal Time Analytics: Application and team related insights available at real time.\nCollaboration with Team: Realtime collaboration with the team.\nSaving Team Bandwidth: Saved bandwidth can be re-invested for more innovation and this could yield compounded benefits.\nHow we built it\nThe QB Mastermind solution comprises of multiple entities, namely Incidents, Activities, Service Level Agreement, Knowledge Base, Communication, Teams, Groups and more. There are multiple user personas available in the solution\nWe will first look the solution from Managers perspective. Here in the home page we see various reports and charts which will give him the insights of incident management by the team, workload spread across teams, types of incident in the queue, team performance and team bandwidth utilization. In addition to that, a summarized report of incident is available for review which has informative indicator for Service level agreement mapping, and effort spent by the teams on the incidents. The same home page also has a report, which draws attention towards the incidents which have age of more than 5 days. Many times an incident moves between multiple teams, and tracking the incident assignment and effort spent on the incident can be really challenging if done manually, our solution automates this process. The manager can click on the reports to see the drill down details of the report. For any queries on the incident, the manager can add new communication which will go the team working the incident. On the left hand menu there is option of adding communication, search any individual incident.\nThe Effort spent by the team on particular incident is calculated by including only working hours and excluding after office hours and weekend. This task was done manually earlier and was the most tedious of all the tasks. The SLA Indicator uses Quickbase formulas to smartly calculate the SLA adherence and takes into account the effort spent and the SLA based on the priority of the incident.\nOne more striking feature of the platform is, the way to track the effort spent by the team. The reports will give early insights to the manager to understand if the team is underutilized or overloaded, this will help him to mobilize the team members to reduce the burden on overloaded teams, and efficiently utilize the underutilized team . For example, application development team can help our application operations team if they have bandwidth available and vice versa.\nThe QB Mastermind solution efficiently uses the pipelines to automate the process collating data from state of art of ITSM tool using channel. The pipelines is automatically triggered every time a new incident is added or existing incident is updated in the ITSM Tool. Workflow automation including monitoring of the fields changed and taking action accordingly.\nFrom Team Member's perspective the solution acts as single source of truth for the reports. Reports and the dashboard will be generated automatically based on the data received. Additionally, there is Knowledge Repository will help team long run to create summarized version of knowledge insights for similar or frequently occurring similar incidents.\nChallenges we ran into\nIntegration challenges between low code platform and capabilities/APIs provides by the ITSM Platforms. This enforced us to think differently - developed additional layer of abstraction exposing course grained API end points that matches the workflow requirements.\nIn addition to that, there were we challenges with the way data was received. Incomplete metadata received from the upstream system. We built our own automated data transformation workflow to standardize the data,\nAccomplishments that we're proud of\nFirst and foremost we are solving a problem which has been hampering the team productivity of for than 2 years. To be able to do that in such a short duration is sometime which we are really proud of. Faster transformation will bring along change the mindset of the team to look at solving the problem at hand rather than spending tireless effort on the workaround. In a hyper-personalized world, making new solutions quickly that work the same across an omnichannel ecosystem is a game changer.\nWith this solution there are far fewer bugs and integration problems to deal with now that the parts are standard, pretested, and ready to use than there were in the past, because the parts are already made. By spending less time on maintenance, developers can work on new ideas that add more value to the business.\nWhat we learned\nPeople came up with ideas for how we could get to the right problem to solve. Design thinking was used to write down the problems that were most important to deal with after the pandemic. How could we try out different tech stacks to get more speed and flexibility?\nWhat's next for QB Mastermind\nThere are so many thing completed in just 24 hours and as we progress there so many other ideas which to mind which can add more value to the solution. 24 hours of Innovation using Quickbase can save close to 450 hours for one team and more than 2000 hours for 5 or more teams. If these 500 or 2000 hours are used for more innovation then the possibilities are endless.\nLooking forward to add more features to QB Mastermind:\n- Training the Machine learning model on the data and integrating QB mastermind with Machine Learning Models for Predictive Analysis\n- Custom Integrations: Planning to add more custom integrators to broaden the scope of data collection.\n- Reinvesting saved effort to fuel more innovation", "link": "https://devpost.com/software/qb-mastermind", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\npresenting our solution for quickbase appathon 2021 q b mastermind . with key focus on this appathon on modernizing productivity bottlenecks and business inefficiencies that are holding back the team, we are presenting a solution which can solve the problem faced by application operations teams from more than 2 years.\nwhat it does\nreporting is a very important task for any team and more important for the operations team to give the insights of it applications health to business stakeholders. for multiple application in production there are many best in class tools available to assist different processes, but, the restriction for custom reporting automation is affecting teams productivity in reporting. 12 to 15 percent of team's bandwidth is used for manually preparing excel based reports by extracting data from itsm tools, and then manually adding the data and logic in excel reports. similar set of tasks are done by more than 5 teams for their respective tracks. in the past there were multiple initiatives to automate portions of this manual reporting process but no end to end automation achieved yet. multiple pain points including recurring mundane tasks, error in manual report, high bandwidth utilization, data duplication, manual emails and no option for data validation for the stake holders.\nsolution features -\n100% roi in < 1 week: solution built in less than 24 hours and the effort the solution save will ensure that the return of investment is less than a week.\nkpi based reports: the kpi based reports will give meaningful insights to the business stakeholders regarding application health and team performance.\nautomated reports: realtime integration with tools and automated report generation\nsignificant cost savings: 450 hours/year savings for 1 team, *2000 hours/year savings for 5 teams\nend to end automation: complete automation for the reporting workflows, no manual inputs needed.\nreal time analytics: application and team related insights available at real time.\ncollaboration with team: realtime collaboration with the team.\nsaving team bandwidth: saved bandwidth can be re-invested for more innovation and this could yield compounded benefits.\nhow we built it\nthe qb mastermind solution comprises of multiple entities, namely incidents, activities, service level agreement, knowledge base, communication, teams, groups and more. there are multiple user personas available in the solution\nwe will first look the solution from managers perspective. here in the home page we see various reports and charts which will give him the insights of incident management by the team, workload spread across teams, types of incident in the queue, team performance and team bandwidth utilization. in addition to that, a summarized report of incident is available for review which has informative indicator for service level agreement mapping, and effort spent by the teams on the incidents. the same home page also has a report, which draws attention towards the incidents which have age of more than 5 days. many times an incident moves between multiple teams, and tracking the incident assignment and effort spent on the incident can be really challenging if done manually, our solution automates this process. the manager can click on the reports to see the drill down details of the report. for any queries on the incident, the manager can add new communication which will go the team working the incident. on the left hand menu there is option of adding communication, search any individual incident.\nthe effort spent by the team on particular incident is calculated by including only working hours and excluding after office hours and weekend. this task was done manually earlier and was the most tedious of all the tasks. the sla indicator uses quickbase formulas to smartly calculate the sla adherence and takes into account the effort spent and the sla based on the priority of the incident.\none more striking feature of the platform is, the way to track the effort spent by the team. the reports will give early insights to the manager to understand if the team is underutilized or overloaded, this will help him to mobilize the team members to reduce the burden on overloaded teams, and efficiently utilize the underutilized team . for example, application development team can help our application operations team if they have bandwidth available and vice versa.\nthe qb mastermind solution efficiently uses the pipelines to automate the process collating data from state of art of itsm -----> tool !!!  using channel. the pipelines is automatically triggered every time a new incident is added or existing incident is updated in the itsm tool. workflow automation including monitoring of the fields changed and taking action accordingly.\nfrom team member's perspective the solution acts as single source of truth for the reports. reports and the dashboard will be generated automatically based on the data received. additionally, there is knowledge repository will help team long run to create summarized version of knowledge insights for similar or frequently occurring similar incidents.\nchallenges we ran into\nintegration challenges between low code platform and capabilities/apis provides by the itsm platforms. this enforced us to think differently - developed additional layer of abstraction exposing course grained api end points that matches the workflow requirements.\nin addition to that, there were we challenges with the way data was received. incomplete metadata received from the upstream system. we built our own automated data transformation workflow to standardize the data,\naccomplishments that we're proud of\nfirst and foremost we are solving a problem which has been hampering the team productivity of for than 2 years. to be able to do that in such a short duration is sometime which we are really proud of. faster transformation will bring along change the mindset of the team to look at solving the problem at hand rather than spending tireless effort on the workaround. in a hyper-personalized world, making new solutions quickly that work the same across an omnichannel ecosystem is a game changer.\nwith this solution there are far fewer bugs and integration problems to deal with now that the parts are standard, pretested, and ready to use than there were in the past, because the parts are already made. by spending less time on maintenance, developers can work on new ideas that add more value to the business.\nwhat we learned\npeople came up with ideas for how we could get to the right problem to solve. design thinking was used to write down the problems that were most important to deal with after the pandemic. how could we try out different tech stacks to get more speed and flexibility?\nwhat's next for qb mastermind\nthere are so many thing completed in just 24 hours and as we progress there so many other ideas which to mind which can add more value to the solution. 24 hours of innovation using quickbase can save close to 450 hours for one team and more than 2000 hours for 5 or more teams. if these 500 or 2000 hours are used for more innovation then the possibilities are endless.\nlooking forward to add more features to qb mastermind:\n- training the machine learning model on the data and integrating qb mastermind with machine learning models for predictive analysis\n- custom integrations: planning to add more custom integrators to broaden the scope of data collection.\n- reinvesting saved effort to fuel more innovation", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59500111}, {"Unnamed: 0": 141, "autor": "Feedback Emotion Miner", "date": null, "content": "Inspiration\nSurveys are the powerful tool proven all over the world to get the feedback about a product or a service from the end customers or clients. This helps to strategize the business or to make changes to the business model or the product or service accordingly. Along with assessing the rating-based feedback, there is a trend now to try to assess the customer mood as much possible through many ways. One of the efficient methods identified is to analyze and derive the customer mood or emotions based on the text feedback provided as part of survey responses. With Quickbase tool being used in many different business verticals and for tons of use cases, it becomes now important to see how the user survey creations can be managed and automated to seek the responses and analyze them more effectively within Quickbase\nWith Pipelines capability, the customer satisfaction surveys can be made configurable and automated to send them periodically to clients/customers and seek responses with the capability of sending multiple reminders automatically after periodic intervals. Also, with the emergence of sophisticated text analysis methods using machine learning models, it becomes easy now to derive the sentiment of the customer by providing the feedback text as input. By integrating it with Quickbase, it will open myriad of possibilities to automate many of the analyses of Quickbase data that will add great value to the business.\nWhat it does\nFeedback Emotion Miner Powered by QuickBase & Machine Learning is a productivity tool that helps organizations derive and aggregate insightful information using Sentiment Analysis such as customer buying trends, areas of improvement, customer relationships etc.\nThe user surveys automatically created in periodic intervals and the follow-up email notifications are sent to seek the responses from client stakeholders automatically. The survey submissions are analyzed to arrive at average rating and sentiment scores that help to gauge the sentiment of our clients and accordingly take proactive actions.\nHow we built it\nSeparate Quickbase tables are created to manage the clients, client surveys for different products/services and questionnaire.\nPipelines are used to create the user surveys based on configured time intervals to seek feedback from client stakeholders/end users.\nEmail Notifications are configured to periodically remind and seek the responses from end users automatically\nPython is used for coding the sentiment model code which is then deployed in Azure Functions which provides server less web service setup\nIn python Script, QuickBase API is used to query the feedback data entered by the User in survey form.\nAfter preprocessing on that data, VADER Sentiment Analyzer model of Natural Language Processing is used to derive the sentiment based on textual feedback.\nPost Sentiment Analysis, Pipeline \u201cIncoming JSON\u201d webhook channel action is used to update the sentiment score back to Quickbase.\nQuickbase Executive Dashboard is used to visualize the sentiment scores derived using the ML model and the numeric rating entered by the client stakeholders.\nChallenges we ran into\n1) Auto-creation of survey questions based on master questionnaire and to show them in a generic survey format 2) Hosting the python code that performs the sentiment analysis 3) Posting back the sentiment score back to Quickbase after the variable time taken by the ML model to run the sentiment analysis\nAccomplishments that we're proud of\nWe showcased how to trigger a python web service that runs a ML model to get the analysis/prediction outcome back to Quickbase\nEfficiently used Pipelines to create surveys along with Questions and capturing the sentiment score back to the QB table\nWhat we learned\nInnovatively use Pipelines to do various internal operations within Quickbase and to send/receive data to/from external web services\nWhat's next for Feedback Emotion Miner\nThis framework can be used in future to develop many other use cases with Quickbase as a backend as well as front-end solution with Machine Learning Analysis and Artificial Intelligence functionality added.", "link": "https://devpost.com/software/feedback-emotion-miner", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nsurveys are the powerful -----> tool !!!  proven all over the world to get the feedback about a product or a service from the end customers or clients. this helps to strategize the business or to make changes to the business model or the product or service accordingly. along with assessing the rating-based feedback, there is a trend now to try to assess the customer mood as much possible through many ways. one of the efficient methods identified is to analyze and derive the customer mood or emotions based on the text feedback provided as part of survey responses. with quickbase tool being used in many different business verticals and for tons of use cases, it becomes now important to see how the user survey creations can be managed and automated to seek the responses and analyze them more effectively within quickbase\nwith pipelines capability, the customer satisfaction surveys can be made configurable and automated to send them periodically to clients/customers and seek responses with the capability of sending multiple reminders automatically after periodic intervals. also, with the emergence of sophisticated text analysis methods using machine learning models, it becomes easy now to derive the sentiment of the customer by providing the feedback text as input. by integrating it with quickbase, it will open myriad of possibilities to automate many of the analyses of quickbase data that will add great value to the business.\nwhat it does\nfeedback emotion miner powered by quickbase & machine learning is a productivity tool that helps organizations derive and aggregate insightful information using sentiment analysis such as customer buying trends, areas of improvement, customer relationships etc.\nthe user surveys automatically created in periodic intervals and the follow-up email notifications are sent to seek the responses from client stakeholders automatically. the survey submissions are analyzed to arrive at average rating and sentiment scores that help to gauge the sentiment of our clients and accordingly take proactive actions.\nhow we built it\nseparate quickbase tables are created to manage the clients, client surveys for different products/services and questionnaire.\npipelines are used to create the user surveys based on configured time intervals to seek feedback from client stakeholders/end users.\nemail notifications are configured to periodically remind and seek the responses from end users automatically\npython is used for coding the sentiment model code which is then deployed in azure functions which provides server less web service setup\nin python script, quickbase api is used to query the feedback data entered by the user in survey form.\nafter preprocessing on that data, vader sentiment analyzer model of natural language processing is used to derive the sentiment based on textual feedback.\npost sentiment analysis, pipeline \u201cincoming json\u201d webhook channel action is used to update the sentiment score back to quickbase.\nquickbase executive dashboard is used to visualize the sentiment scores derived using the ml model and the numeric rating entered by the client stakeholders.\nchallenges we ran into\n1) auto-creation of survey questions based on master questionnaire and to show them in a generic survey format 2) hosting the python code that performs the sentiment analysis 3) posting back the sentiment score back to quickbase after the variable time taken by the ml model to run the sentiment analysis\naccomplishments that we're proud of\nwe showcased how to trigger a python web service that runs a ml model to get the analysis/prediction outcome back to quickbase\nefficiently used pipelines to create surveys along with questions and capturing the sentiment score back to the qb table\nwhat we learned\ninnovatively use pipelines to do various internal operations within quickbase and to send/receive data to/from external web services\nwhat's next for feedback emotion miner\nthis framework can be used in future to develop many other use cases with quickbase as a backend as well as front-end solution with machine learning analysis and artificial intelligence functionality added.", "sortedWord": "None", "removed": "Nan", "score": 6, "comments": 0, "media": null, "medialink": null, "identifyer": 59500141}, {"Unnamed: 0": 154, "autor": "HR Hiring & Interview Coordinator", "date": null, "content": "Inspiration\nHR Hiring Process is a complex and tedious process in terms of collecting the resumes for open demands and align the interviews. Also, there is no single place to sort list the candidates based on the skills, schedule the interview and collect feedback to further make decisions on hiring.\nWhat it does\nHR Hiring Process is a completely automated process of Hiring from uploading a resume to schedule interviews, conduct Video/audio call interviews, collect feedback from multiple interviewers, select and onboard the candidate, and also collect the vital documents required for Onboarding process.\nThere are many tools that tried to solve this problem, however, to have a customized, low-cost application which can be used by users across the globe, and make the tool available to market in short span of time is a real challenge.\nQuick Base is one of such low code platforms which can be easily customized to such needs. In 2-3 weeks of time, this kind of application can be created and hosted users globally.\nHow we built it\nWe established API connection with prompt API which receives the uploaded document from Quick Base and parse the details of resume to JSON response. The response is received and saved to Quick Base via RESTful API.\nBased on the skillset the candidate is aligned to the demand automatically using pipelines\nOnce the interviewer schedules the interview, the system generates a Teams video conference link, and a pipeline executes to send the link to outlook channel. This also uses Quick Base notifications to send emails.\nAfter the candidate is selected, HR initiates the offer process by providing the required details. System generates the Offer letter from a MS Word template, which is pre defined with all required formatting.\nQuick Base Executive dashboard feature is used to the fullest to show case and visualize the data on dashboard.\nChallenges we ran into\nWe want to show case with a few weeks of time, having a complete use case implemented end to end and be usable by teams globally.\nThe challenges we faced in finding out the best fit framework/api for the current need with less cost and feature rich.\nFormatted word document generation\nAccomplishments that we're proud of\nResume parsing API integration with Quick Base.\nBest use of Pipelines wherever required.\nGenerating Teams call for Video conferencing calls and send a calendar invite to candidate.\nGeneration of Formatted Offer letter from a predefined word template.\nQuick Base Executive dashboard to visualize the details at it's best.\nWhat we learned\nAPI Integrations\nTeams Integration\nPipelines that are used for auto align the candidate\nQuick Base code pages with Javascript\nWhat's next for HR Hiring Process\nThis solution can be leveraged by adding Machine learning to its Resume shortlisting, and aligning interviews based on the AI algorithms.\nCandidate documents can be scrutinized while uploading at the time of Virtual Onboarding, and display the warnings or errors, this would save huge amount of time to go through each document by HR.\nUpstream we can have integrations with Resume stores or Linked In to get the candidate details to the system directly.", "link": "https://devpost.com/software/hr-hiring-process", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nhr hiring process is a complex and tedious process in terms of collecting the resumes for open demands and align the interviews. also, there is no single place to sort list the candidates based on the skills, schedule the interview and collect feedback to further make decisions on hiring.\nwhat it does\nhr hiring process is a completely automated process of hiring from uploading a resume to schedule interviews, conduct video/audio call interviews, collect feedback from multiple interviewers, select and onboard the candidate, and also collect the vital documents required for onboarding process.\nthere are many tools that tried to solve this problem, however, to have a customized, low-cost application which can be used by users across the globe, and make the -----> tool !!!  available to market in short span of time is a real challenge.\nquick base is one of such low code platforms which can be easily customized to such needs. in 2-3 weeks of time, this kind of application can be created and hosted users globally.\nhow we built it\nwe established api connection with prompt api which receives the uploaded document from quick base and parse the details of resume to json response. the response is received and saved to quick base via restful api.\nbased on the skillset the candidate is aligned to the demand automatically using pipelines\nonce the interviewer schedules the interview, the system generates a teams video conference link, and a pipeline executes to send the link to outlook channel. this also uses quick base notifications to send emails.\nafter the candidate is selected, hr initiates the offer process by providing the required details. system generates the offer letter from a ms word template, which is pre defined with all required formatting.\nquick base executive dashboard feature is used to the fullest to show case and visualize the data on dashboard.\nchallenges we ran into\nwe want to show case with a few weeks of time, having a complete use case implemented end to end and be usable by teams globally.\nthe challenges we faced in finding out the best fit framework/api for the current need with less cost and feature rich.\nformatted word document generation\naccomplishments that we're proud of\nresume parsing api integration with quick base.\nbest use of pipelines wherever required.\ngenerating teams call for video conferencing calls and send a calendar invite to candidate.\ngeneration of formatted offer letter from a predefined word template.\nquick base executive dashboard to visualize the details at it's best.\nwhat we learned\napi integrations\nteams integration\npipelines that are used for auto align the candidate\nquick base code pages with javascript\nwhat's next for hr hiring process\nthis solution can be leveraged by adding machine learning to its resume shortlisting, and aligning interviews based on the ai algorithms.\ncandidate documents can be scrutinized while uploading at the time of virtual onboarding, and display the warnings or errors, this would save huge amount of time to go through each document by hr.\nupstream we can have integrations with resume stores or linked in to get the candidate details to the system directly.", "sortedWord": "None", "removed": "Nan", "score": 7, "comments": 0, "media": null, "medialink": null, "identifyer": 59500154}, {"Unnamed: 0": 162, "autor": "Figma2Android", "date": null, "content": "Inspiration\nVery often designs contain many different properties for elements of distance, color, proportion, as well as their layout. Developing such projects can be very tedious, and inconsistencies in design and development can negatively affect the overall design view after development.\nWhat it does\nEasy convert from Figma designs to Android Jetpack Compose code\nHow I built it\nIn begin, we saw the plugin as a simple internal tool for converting the properties of a selected element into code, but with a deeper study of the Figma API, ideas began to appear that it is possible to export completely blocks and pages, including their properties and child elements. Then, of course, the fantasy played out and I wanted to immediately export a ready-made functional element, for example, it is not easy to Text, and also a TextField, but then the plugin would become more difficult to use and there would be more design requirements, which is a waste of time for us at the moment, in the future it is possible we will implement this one of the ideas for the development of the plug-in, about the ideas for the development of the plug-in will be described in more detail below.\nWhat I learned\nWhen creating the plugin, we used several different technologies, below is a list and a small description of what, where and how it was used. Jetpack Compose (Android) \u2014 well, of course, the first item is Jetpack Compose itself, now there is a lot of controversy about the pros and cons of this technology, but for me who develops and supports projects simultaneously on SwiftUI and Android Layout, the release of the stable version was a holiday, I think this is the future, if you understand how to use it correctly, you can do incredible things. Figma API + TypeScript + Visual Studio Code \u2014 Figma\u2019s Plugins API is well documented, plus, of course, I had to use console output in order to better understand the content of each Figma element and its properties. Also, the Figma API documentation explains quite well how to deploy development tools in this case, Visual Studio Code and TypeScript as the development language of the plugin itself. Gumroad + Gumroad API + PHP \u2014 we used these tools for the future monetization of our product, plus see how difficult it will be to integrate a subscription into the plugin. More details on monetization will be given below. SVG Path Data \u2014 I had to study and add almost at the very end, since today icons are used a lot in any interface, and without them the converted result looked frankly bad, of course, as a developer, I recommend exporting icons separately and adding them to the project resources, but what can you do for the sake of a beautiful picture :)\nWhat's next for Figma2Android\nIn future plans for the development of the Figma2Android plugin: Add the ability to select export with children or just the selected item. More complete support for Shapes Different types of export for example: export of the selected Instance element immediately into a separate component in the code. Ability to export ready-made functional elements such as TextField, CheckBox, etc. Release a similar plugin for SwiftUI and much more :)", "link": "https://devpost.com/software/figma2android", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nvery often designs contain many different properties for elements of distance, color, proportion, as well as their layout. developing such projects can be very tedious, and inconsistencies in design and development can negatively affect the overall design view after development.\nwhat it does\neasy convert from figma designs to android jetpack compose code\nhow i built it\nin begin, we saw the plugin as a simple internal -----> tool !!!  for converting the properties of a selected element into code, but with a deeper study of the figma api, ideas began to appear that it is possible to export completely blocks and pages, including their properties and child elements. then, of course, the fantasy played out and i wanted to immediately export a ready-made functional element, for example, it is not easy to text, and also a textfield, but then the plugin would become more difficult to use and there would be more design requirements, which is a waste of time for us at the moment, in the future it is possible we will implement this one of the ideas for the development of the plug-in, about the ideas for the development of the plug-in will be described in more detail below.\nwhat i learned\nwhen creating the plugin, we used several different technologies, below is a list and a small description of what, where and how it was used. jetpack compose (android) \u2014 well, of course, the first item is jetpack compose itself, now there is a lot of controversy about the pros and cons of this technology, but for me who develops and supports projects simultaneously on swiftui and android layout, the release of the stable version was a holiday, i think this is the future, if you understand how to use it correctly, you can do incredible things. figma api + typescript + visual studio code \u2014 figma\u2019s plugins api is well documented, plus, of course, i had to use console output in order to better understand the content of each figma element and its properties. also, the figma api documentation explains quite well how to deploy development tools in this case, visual studio code and typescript as the development language of the plugin itself. gumroad + gumroad api + php \u2014 we used these tools for the future monetization of our product, plus see how difficult it will be to integrate a subscription into the plugin. more details on monetization will be given below. svg path data \u2014 i had to study and add almost at the very end, since today icons are used a lot in any interface, and without them the converted result looked frankly bad, of course, as a developer, i recommend exporting icons separately and adding them to the project resources, but what can you do for the sake of a beautiful picture :)\nwhat's next for figma2android\nin future plans for the development of the figma2android plugin: add the ability to select export with children or just the selected item. more complete support for shapes different types of export for example: export of the selected instance element immediately into a separate component in the code. ability to export ready-made functional elements such as textfield, checkbox, etc. release a similar plugin for swiftui and much more :)", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59500162}, {"Unnamed: 0": 166, "autor": "Work Management Tool", "date": null, "content": "Inspiration\nWe were inspired to create this app based on the real life struggle of manual tracking and allocation of daily activities in a workplace and consolidating together in a single platform.\nWhat it does\nWork Management Tool is a Task Allocation and Tracking application which consists of multiple features like Auto allocation and integration with Teams, Slack, Jira and Alexa. Auto allocation feature enables allocation of tickets automatically to users based on their capacity and leave tracker. Teams and Slack integration enables users to check the status of different tasks and workload for a specific user. CI CD software such as Jira is integrated so that users can update the user stories from QuickBase and update the QuickBase from Jira if any user story or tasks are created. Alexa integration enables users to understand a task status.\nHow we built it\nWe have built this application on QuickBase platform and integrated to different services through pipelines. \u2022 Jira Integration \u2013 We have a Jira integration for the task creation pushing data through pipelines to QuickBase. \u2022 Use of QuickBase pipelines \u2013 After the task creation, it will be auto allocated to a resource based on capacity via best of QuickBase channel pipelines. \u2022 Slack Integration \u2013 Task has a feature to enable a chat functionality with colleagues over the Slack channel through pipelines. It provides a feature to initiate a conversation from QuickBase, will post the same message in the Slack channel and response can be provided. Slack also has a feature to get the task details and status through the channel. \u2022 Outlook Integration - This application also has an added feature to set up Outlook meetings by clicking a button available in the task form and integrate with the online Microsoft Team meetings. We have achieved this vis best use of QuickBase random variables/formula fields along with the outlook pipeline channels \u2022 QuickBase Native Features (Formula Fields & Pipelines) \u2013 In this application we have built a feature for Leave Requests and Request Approval based on the historical data analysis. This also has a feature for SLA mechanism for each task and to calculate the Overall Productivity. \u2022 Voicify QuickBase with Alexa Integration - Helps the users to check the task status emerging with IOT technologies. \u2022 Quick Base Executive Dashboard \u2013 We have used the QuickBase latest executive dashboard for reporting the team building activities and their productivity along with the best use of roles (Supervisor & Processor)\nChallenges we ran into\n\u2022 Jira Integration \u2013 Syncing the data on both the directions, but it has been resolved using QuickBase pipeline channels. \u2022 Slack Integration \u2013 Handling the Slack channel is a tedious task, we have accomplished using QuickBase pipeline channels \u2022 Voicify QuickBase with Alexa Integration \u2013 IOT Technology doesn\u2019t have a direct integration with QuickBase, we achieved this via AWS Alexa platform. \u2022 Auto assignment of Tasks based on the capacity and the availability of the resources \u2013 Solved this problem via using Jinja expressions within QuickBase pipelines\nAccomplishments that we're proud of\nWe are proud that we have built an app successfully which would help different users to easily track their tasks with different technologies such as Slack, Jira, Alexa, Outlook, QuickBase formulas, Pipeline Integrations, Jinja expressions, Roles and Executive Dashboards.\nWhat we learned\n\u2022 Lot of Integrations with technologies \u2022 Best use of QuickBase native features \u2022 Best use of Pipeline/API channels \u2022 Team Effort\nWhat's next for Work Management Tool\nIn future, this application can be integrated to more technology stack to expand in terms of processing and reporting.", "link": "https://devpost.com/software/work-management-tool", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe were inspired to create this app based on the real life struggle of manual tracking and allocation of daily activities in a workplace and consolidating together in a single platform.\nwhat it does\nwork management -----> tool !!!  is a task allocation and tracking application which consists of multiple features like auto allocation and integration with teams, slack, jira and alexa. auto allocation feature enables allocation of tickets automatically to users based on their capacity and leave tracker. teams and slack integration enables users to check the status of different tasks and workload for a specific user. ci cd software such as jira is integrated so that users can update the user stories from quickbase and update the quickbase from jira if any user story or tasks are created. alexa integration enables users to understand a task status.\nhow we built it\nwe have built this application on quickbase platform and integrated to different services through pipelines. \u2022 jira integration \u2013 we have a jira integration for the task creation pushing data through pipelines to quickbase. \u2022 use of quickbase pipelines \u2013 after the task creation, it will be auto allocated to a resource based on capacity via best of quickbase channel pipelines. \u2022 slack integration \u2013 task has a feature to enable a chat functionality with colleagues over the slack channel through pipelines. it provides a feature to initiate a conversation from quickbase, will post the same message in the slack channel and response can be provided. slack also has a feature to get the task details and status through the channel. \u2022 outlook integration - this application also has an added feature to set up outlook meetings by clicking a button available in the task form and integrate with the online microsoft team meetings. we have achieved this vis best use of quickbase random variables/formula fields along with the outlook pipeline channels \u2022 quickbase native features (formula fields & pipelines) \u2013 in this application we have built a feature for leave requests and request approval based on the historical data analysis. this also has a feature for sla mechanism for each task and to calculate the overall productivity. \u2022 voicify quickbase with alexa integration - helps the users to check the task status emerging with iot technologies. \u2022 quick base executive dashboard \u2013 we have used the quickbase latest executive dashboard for reporting the team building activities and their productivity along with the best use of roles (supervisor & processor)\nchallenges we ran into\n\u2022 jira integration \u2013 syncing the data on both the directions, but it has been resolved using quickbase pipeline channels. \u2022 slack integration \u2013 handling the slack channel is a tedious task, we have accomplished using quickbase pipeline channels \u2022 voicify quickbase with alexa integration \u2013 iot technology doesn\u2019t have a direct integration with quickbase, we achieved this via aws alexa platform. \u2022 auto assignment of tasks based on the capacity and the availability of the resources \u2013 solved this problem via using jinja expressions within quickbase pipelines\naccomplishments that we're proud of\nwe are proud that we have built an app successfully which would help different users to easily track their tasks with different technologies such as slack, jira, alexa, outlook, quickbase formulas, pipeline integrations, jinja expressions, roles and executive dashboards.\nwhat we learned\n\u2022 lot of integrations with technologies \u2022 best use of quickbase native features \u2022 best use of pipeline/api channels \u2022 team effort\nwhat's next for work management tool\nin future, this application can be integrated to more technology stack to expand in terms of processing and reporting.", "sortedWord": "None", "removed": "Nan", "score": 8, "comments": 1, "media": null, "medialink": null, "identifyer": 59500166}, {"Unnamed: 0": 186, "autor": "Disease Database", "date": null, "content": "Inspiration\nVery few mobile apps dealt with disease databases so we wanted to expand the mobile market of this type of application.\nWhat it does\nUsers can scroll through a list of diseases and expand them to view information about that disease, such as a summary, symptoms, treatment, and related diseases.\nHow we built it\nWe used React Native to build our mobile app so both Android and iOS users can enjoy our app and shorten our development time to do so. We used Expo CLI to develop our app.\nChallenges we ran into\nThe backend struggled with the development process and therefore most of our app is client-side. None of our group members built a mobile app before so the general process of making a mobile app, and the tasks associated with it, was difficult.\nAccomplishments that we're proud of\nThe frontend looks good and looks accurate to the Figma we built early on in the development process. We used some tricks and frameworks to make it look nice but a lot of it is pure javascript styling.\nWhat we learned\nWe learnt a lot more about the differences between web development and mobile development but at the same time, the similarities as well. This was the first time we used React Native and it's always fun to use a new language for the first time. The KnightHacks team taught us about Figma which was a very useful tool in our planning and design process. We know the struggle of trying to design an application with no prior design planning, but with Figma, it helped us create a clear image for our mobile app that the team could agree on. It was cool to see how the frontend and backend connects and how to use node.js to make API calls. Some members also learned how to use Git for the first time, which is a much needed skill in today's world.\nWhat's next for Disease Database\nUser login/signup functionality\nSingle sign-on\nTaking notes on diseases\nTeleconference features", "link": "https://devpost.com/software/disease-database", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nvery few mobile apps dealt with disease databases so we wanted to expand the mobile market of this type of application.\nwhat it does\nusers can scroll through a list of diseases and expand them to view information about that disease, such as a summary, symptoms, treatment, and related diseases.\nhow we built it\nwe used react native to build our mobile app so both android and ios users can enjoy our app and shorten our development time to do so. we used expo cli to develop our app.\nchallenges we ran into\nthe backend struggled with the development process and therefore most of our app is client-side. none of our group members built a mobile app before so the general process of making a mobile app, and the tasks associated with it, was difficult.\naccomplishments that we're proud of\nthe frontend looks good and looks accurate to the figma we built early on in the development process. we used some tricks and frameworks to make it look nice but a lot of it is pure javascript styling.\nwhat we learned\nwe learnt a lot more about the differences between web development and mobile development but at the same time, the similarities as well. this was the first time we used react native and it's always fun to use a new language for the first time. the knighthacks team taught us about figma which was a very useful -----> tool !!!  in our planning and design process. we know the struggle of trying to design an application with no prior design planning, but with figma, it helped us create a clear image for our mobile app that the team could agree on. it was cool to see how the frontend and backend connects and how to use node.js to make api calls. some members also learned how to use git for the first time, which is a much needed skill in today's world.\nwhat's next for disease database\nuser login/signup functionality\nsingle sign-on\ntaking notes on diseases\nteleconference features", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59500186}, {"Unnamed: 0": 194, "autor": "Tutoro", "date": null, "content": "Inspiration\nIn the past, we had all thought of getting community service hours by tutoring kids. However, none of us had ever gotten around to actually doing that. We thought that a tool to manage it would be helpful.\nWhat it does\nTutoro lets tutors list themselves on the site, and lets students search for tutors by subject.\nHow we built it\nWe used Python, along with Flask, a python library for creating websites.\nChallenges we ran into\nWe had trouble getting all of the forms to work and information to save.\nAccomplishments that we're proud of\nWe're proud of the backend. Even though it's not very glamorous, it works, and it was tough to get it that way.\nWhat we learned\nAll of us got better at HTML/CSS/JS, Python, and also at project management.\nWhat's next for Tutoro\nIn the future, we could expand on it, finish all of the pages that we wanted to, and even buy a domain to host it on a server.", "link": "https://devpost.com/software/tutoro", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nin the past, we had all thought of getting community service hours by tutoring kids. however, none of us had ever gotten around to actually doing that. we thought that a -----> tool !!!  to manage it would be helpful.\nwhat it does\ntutoro lets tutors list themselves on the site, and lets students search for tutors by subject.\nhow we built it\nwe used python, along with flask, a python library for creating websites.\nchallenges we ran into\nwe had trouble getting all of the forms to work and information to save.\naccomplishments that we're proud of\nwe're proud of the backend. even though it's not very glamorous, it works, and it was tough to get it that way.\nwhat we learned\nall of us got better at html/css/js, python, and also at project management.\nwhat's next for tutoro\nin the future, we could expand on it, finish all of the pages that we wanted to, and even buy a domain to host it on a server.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59500194}, {"Unnamed: 0": 198, "autor": "Waves - Virtual Classroom Environment", "date": null, "content": "Introduction:\nMany things have changed over the past couple of years, from a global pandemic and major changes in our school systems to staying in our homes. In the case of online learning, many teachers have seen an uptick in disengagement in their students. Students have found it hard to stay engaged in class. Many blame it on the monotony of online learning, while others feel it may be because of the lack of social interaction. Our product aspires to redefine the online learning experience, cultivating a more immersive and engaging platform where students and teachers alike can connect at a deeper level.\nProblem:\nThe problem we will be tackling in regards to virtual education is the lack of engagement and interest that is seen in students. This problem has become more and more prevalent in the last few years due to the pandemic and the switch to online school. Recent studies have shown that a lack of social interaction has been a major factor in lack of interest and engagement during class. \u201cSome say it is not fun, because they feel that something is missing, such as being together with their friends both in learning and in play\u201d (Sutarto et al., 2020). Many things have been missing ever since the switch to online learning and none are more apparent than the social interactions we no longer have. The platforms we use for online school no longer feel like classroom settings, rather they almost feel like a work meeting.\nWhy It Matters:\nSocial interactions are a very important aspect of learning and they have a positive impact on the level of learning that occurs in a classroom. According to a study done in 2013, students learned better from other students, which [leads] to higher comprehension levels of the material taught. Additionally, social interaction created a more inviting and positive work environment and interaction provided students with improved critical thinking skills. (Wallace & Nixon, 2013)\u200b\u200b. In many cases students are able to relate to other students better than teachers are able to relate to their students. Asking the students for help can help build stronger connections with others and be more convenient and helpful than raising your hand and asking the teacher. In our research, we also found evidence that student and teacher engagement does have a positive impact on perceived student learning and student satisfaction (Gray & Diloreto, 2016). Additionally, social-emotional experiences have been shown to be positively correlated to a student\u2019s motivation, learning strategies, and achievement, along with student immersement in learning activities (Noteborn et al., 2012).\nSolution:\nTo help promote student engagement and interest in virtual learning, our solution is to develop an application that fosters social interaction between peers as well as between students and teachers. Our solution is a product that will supplement students with the engagement and social interactions that virtual learning has lacked in the past. The product that we designed is an expansion of existing virtual classrooms that allows its users to actively interact with their virtual environments. However, the novelty of our product lies within its features that the human interaction aspect of an in-person classroom to a virtual, collaborative space that teachers and students alike, may flourish in. As shown in our concept art, we have developed a virtual classroom setting where students and teachers are able to interact as if they were in an in-person classroom. We accomplished this through the use of a proximity chat function and an interactive 2D classroom projection, all while maintaining the function of an online meeting tool, such as Zoom or Google Meets. Our main feature is our virtual classroom environment system, allowing students to feel as if they were actually in a classroom. Within this feature, we have many subfeatures that are key to making the learning experience as engaging and immersive as possible. First, to make the students feel as though they are in a physical classroom with actual autonomy, we implemented a movement feature where students can move their avatars around in a 2D classroom. Of course, the teacher can always lock movement in order to prevent distraction, but this feature allows for more interactive class activities that involve more \u201cmovement\u201d. Second, in order to combat the intimidating feeling of unmuting one\u2019s microphone and turning on one\u2019s camera in front of the entire class, we decided to include a feature that gives students the option to only unmute and turn on their cameras to their tablemates. This way, we hope a sense of virtual proximity can be established, leading to the construction of vital peer connections in the classroom. In that same vein, our third feature is a chat system in which students can message the other students at their table without fear of having to bother the teacher or the entire class in the public chat. Our fourth feature is a table-based whiteboard system where each table has a virtual workspace similar to a physical table on which students can write text, sketch ideas, and collaborate. Finally, our fifth feature is a variety of commands and settings the teacher can configure to tailor the classroom experience to their own needs. Teachers can customize their classroom layout, drag students from table to table, and lock movement while being able to manage their online class with the same features one might see in Zoom or Google Meets: enabling screen sharing, screen recording, enabling direct messaging, muting all participants, etc. With all these features, we hope to bring the feeling of an actual classroom to the online student.\nTarget Audience:\nOur target audience for our product will be teachers and school administrations, since we will be needing to convince them of its merits and why it appeals to students.\nApplications/Implications:\nAlthough we didn\u2019t fully complete our design, we believe that our product can be very successful in its function of engaging students and creating an immersive learning environment that mimics the traditional in-person classroom online. Using our product, teachers would be able to more effectively manage student-to-student interactions and facilitate more group work time, while students are able to retain the social interactions they would normally have in in-person school and still be engaged in class while online. We believe that our product will be able to engage students more effectively than the meeting applications schools have been using which would, in turn, greatly enhance the online learning experience for students and teachers alike.\nDiscussion/Conclusion:\nEven though the pandemic seems to be getting better, policies that pertain to online learning models cannot be ignored. In our modern digital world, education is gradually shifting towards being delivered via technological mediums. Thus, it is becoming increasingly important for students to be able to effectively learn in a digital environment. The Coronavirus pandemic taught us a lot of things about our society and education, highlighting flaws and weaknesses in our policies. In order to be better prepared for future circumstances and changing job markets, we must continue to investigate potential solutions to problems associated with online education, even if our pandemic is coming to an end.", "link": "https://devpost.com/software/waves-virtual-classroom-environment", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "introduction:\nmany things have changed over the past couple of years, from a global pandemic and major changes in our school systems to staying in our homes. in the case of online learning, many teachers have seen an uptick in disengagement in their students. students have found it hard to stay engaged in class. many blame it on the monotony of online learning, while others feel it may be because of the lack of social interaction. our product aspires to redefine the online learning experience, cultivating a more immersive and engaging platform where students and teachers alike can connect at a deeper level.\nproblem:\nthe problem we will be tackling in regards to virtual education is the lack of engagement and interest that is seen in students. this problem has become more and more prevalent in the last few years due to the pandemic and the switch to online school. recent studies have shown that a lack of social interaction has been a major factor in lack of interest and engagement during class. \u201csome say it is not fun, because they feel that something is missing, such as being together with their friends both in learning and in play\u201d (sutarto et al., 2020). many things have been missing ever since the switch to online learning and none are more apparent than the social interactions we no longer have. the platforms we use for online school no longer feel like classroom settings, rather they almost feel like a work meeting.\nwhy it matters:\nsocial interactions are a very important aspect of learning and they have a positive impact on the level of learning that occurs in a classroom. according to a study done in 2013, students learned better from other students, which [leads] to higher comprehension levels of the material taught. additionally, social interaction created a more inviting and positive work environment and interaction provided students with improved critical thinking skills. (wallace & nixon, 2013)\u200b\u200b. in many cases students are able to relate to other students better than teachers are able to relate to their students. asking the students for help can help build stronger connections with others and be more convenient and helpful than raising your hand and asking the teacher. in our research, we also found evidence that student and teacher engagement does have a positive impact on perceived student learning and student satisfaction (gray & diloreto, 2016). additionally, social-emotional experiences have been shown to be positively correlated to a student\u2019s motivation, learning strategies, and achievement, along with student immersement in learning activities (noteborn et al., 2012).\nsolution:\nto help promote student engagement and interest in virtual learning, our solution is to develop an application that fosters social interaction between peers as well as between students and teachers. our solution is a product that will supplement students with the engagement and social interactions that virtual learning has lacked in the past. the product that we designed is an expansion of existing virtual classrooms that allows its users to actively interact with their virtual environments. however, the novelty of our product lies within its features that the human interaction aspect of an in-person classroom to a virtual, collaborative space that teachers and students alike, may flourish in. as shown in our concept art, we have developed a virtual classroom setting where students and teachers are able to interact as if they were in an in-person classroom. we accomplished this through the use of a proximity chat function and an interactive 2d classroom projection, all while maintaining the function of an online meeting -----> tool !!! , such as zoom or google meets. our main feature is our virtual classroom environment system, allowing students to feel as if they were actually in a classroom. within this feature, we have many subfeatures that are key to making the learning experience as engaging and immersive as possible. first, to make the students feel as though they are in a physical classroom with actual autonomy, we implemented a movement feature where students can move their avatars around in a 2d classroom. of course, the teacher can always lock movement in order to prevent distraction, but this feature allows for more interactive class activities that involve more \u201cmovement\u201d. second, in order to combat the intimidating feeling of unmuting one\u2019s microphone and turning on one\u2019s camera in front of the entire class, we decided to include a feature that gives students the option to only unmute and turn on their cameras to their tablemates. this way, we hope a sense of virtual proximity can be established, leading to the construction of vital peer connections in the classroom. in that same vein, our third feature is a chat system in which students can message the other students at their table without fear of having to bother the teacher or the entire class in the public chat. our fourth feature is a table-based whiteboard system where each table has a virtual workspace similar to a physical table on which students can write text, sketch ideas, and collaborate. finally, our fifth feature is a variety of commands and settings the teacher can configure to tailor the classroom experience to their own needs. teachers can customize their classroom layout, drag students from table to table, and lock movement while being able to manage their online class with the same features one might see in zoom or google meets: enabling screen sharing, screen recording, enabling direct messaging, muting all participants, etc. with all these features, we hope to bring the feeling of an actual classroom to the online student.\ntarget audience:\nour target audience for our product will be teachers and school administrations, since we will be needing to convince them of its merits and why it appeals to students.\napplications/implications:\nalthough we didn\u2019t fully complete our design, we believe that our product can be very successful in its function of engaging students and creating an immersive learning environment that mimics the traditional in-person classroom online. using our product, teachers would be able to more effectively manage student-to-student interactions and facilitate more group work time, while students are able to retain the social interactions they would normally have in in-person school and still be engaged in class while online. we believe that our product will be able to engage students more effectively than the meeting applications schools have been using which would, in turn, greatly enhance the online learning experience for students and teachers alike.\ndiscussion/conclusion:\neven though the pandemic seems to be getting better, policies that pertain to online learning models cannot be ignored. in our modern digital world, education is gradually shifting towards being delivered via technological mediums. thus, it is becoming increasingly important for students to be able to effectively learn in a digital environment. the coronavirus pandemic taught us a lot of things about our society and education, highlighting flaws and weaknesses in our policies. in order to be better prepared for future circumstances and changing job markets, we must continue to investigate potential solutions to problems associated with online education, even if our pandemic is coming to an end.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59500198}, {"Unnamed: 0": 205, "autor": "Aut With Us", "date": null, "content": "Inspiration\nWith the new normal and digitalization of every sector we came to realize that there are mostly blogs available on the internet on Autism and felt that the Autism community was somewhere sidelined with having barely any resources or softwares which could help the parents and therapists continue their therapies virtually and still had to depend on traditional means of crafts etc to develop puzzles for children to teach life skills . On more surfing around the internet, we realized the gravity of the situation when there was only one website which had educational games teaching autistic kids eye contact etc. which had also discontinued because of technical glitches. The severity of the situation alongside the fact that 1 in 54 children have autism inspired us to use technology to create tools catering to the needs of the children, parents and people associated with autism. Spreading awareness about Autism through our project has been the major driving force.\nWhat it does\nAut With Us is a working prototype aimed at incorporating technology into the traditional tools used in teaching people with autism basic life skills. The project is coded in color blue and has the puzzle logo which are symbolic to autism and have been incorporated with the aim to spread awareness.\nThe project has three distinct highlights aimed at trying to cover people who are on different levels of the autism spectrum.\nSpecial Educators use traditional means of making adaptive books to teach life skills like brushing teeth etc and to reduce the cumbersome work load in making work system using hand crafts, we decided to design prototype centralizing to the people with autism by keeping them simple so that with one click of mouse, the child can easily navigate through the steps and learn new skills making the work easier for both the educators and children.\nThe website in itself is a compact platform providing knowledge of Autism and spreading awareness about it through blogs ,videos etc as well as linking the features together including the game prototype and bot.\nThe discord bot linked with the website is a tool for non verbal autistic people (approx 30-40% people on spectrum are non verbal) who can use the text input to interact with the bot. The bot provides positive reinforcements to the user which in return helps in uplifting the mood of the user as well as increasing confidence and preaching self worth through phrases like \u201cYou can do it.\u201d ,\u201cYou are the best.\u201d etc. It also has quotations from personalities like Rumi etc.\nHow we built it\nOur project is an amalgamation of the following things-\nDiscord Bot- Python\nFront End -HTML/CSS\n- Bootstrap\n- Java script\nBack End - PhP\nEducational Game Prototype -Google Slides\nChallenges we ran into\n1)We faced a lot of system and softwares problems while the frontend of the website was being created due to which we had to redo the whole front end.\n2)We had one of our teammate leave us just after the start of the Hack due to some unforeseen circumstances, which made our schedule more scrambled but we managed to finish our project .\n3) It took us some time and help from the mentors to figure out a way to present the idea of the game in form of a prototype but we managed to make the best use of google slides in prototyping our idea.\nAccomplishments that we're proud of\nWe now know the use of HTML/CSS,using sandbox,bootstrap,template editing and shuffling in an intermediate level as well as learning how to create games/short animations using google slides.\nWe are proud of how we were able to come together as a team since we had never met each other prior but were still able to work together in a collaborative and healthy environment.\nWhat we learned\nThe research phase of the hack was an eye opening experience for us all as a team especially learning about Autism and how an autistic person spends his/her daily routine especially the tools and techniques used in teaching an autistic child life skills. We also realised that apart from numerous blogs on autism, there isn't much out there in terms of softwares or educational games etc. catering to the needs of autistic people etc which motivated us to take this project in the hackathon.\nOn the technical forefront, working on the project has taught us how to manage our time while simultaneously collaborating with each other and making quick decisions as well as learning new technical hacks like extensions etc which enables us to code in real time.\nWhat's next for Aut With Us\nPost the hackathon, converting the prototype for the educational games into actual games using platforms like unity is on our to-do list. Considering the lack of educational games built catering to autistic people, we are planning to improve our prototype and turn it into educational games teaching people with autism basic life skills helping them with their journey into being independent.\nWe also plan on improving the discord bot as well as text to speech functionality keeping in mind the non verbal children on the autism spectrum and helping them communicate through technology.We plan on making the security of our website as well as the discord so as to make sure there is no breach or unparliamentary action in our environment and to ensure the environment runs as a safe,civil and learning platform.\nAlongside this we look forward to adding more functionality to the website and making it dynamic with good quality content and incorporating a better tech stack like react into it. We also plan on improving the content of the website in terms of resources etc like sharing the journey of people who are on the spectrum, sharing books written by people having autism ,sessions and conferences catering to autism etc.\nWe hope to even develop an app version of the website , incorporate an inbuilt chat feature for a peer to peer interaction or peer to admin interaction as well as develop more advanced,secure admin records with the database.\n2) WCAG\nWe plan to integrate and follow the WCAG 2.0(WEB ACCESSIBILITY GUIDELINES) which are :\n1)Provides content not prone to have seizures\n2)Content should be substituted with pictures for better understanding\n3)Sentence should not be cluttered\n4)Font needs to be large and legible\n5)Content present needs to be verified such that there is no violation of moral conduct. Overall we have a lot to look forward to in terms of our Hack.\n-->CATEGORIES APPLIED FOR: 1st place PWE 2nd Place PWE 3rd place PWE Best Overall Idea", "link": "https://devpost.com/software/aut-with-us", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwith the new normal and digitalization of every sector we came to realize that there are mostly blogs available on the internet on autism and felt that the autism community was somewhere sidelined with having barely any resources or softwares which could help the parents and therapists continue their therapies virtually and still had to depend on traditional means of crafts etc to develop puzzles for children to teach life skills . on more surfing around the internet, we realized the gravity of the situation when there was only one website which had educational games teaching autistic kids eye contact etc. which had also discontinued because of technical glitches. the severity of the situation alongside the fact that 1 in 54 children have autism inspired us to use technology to create tools catering to the needs of the children, parents and people associated with autism. spreading awareness about autism through our project has been the major driving force.\nwhat it does\naut with us is a working prototype aimed at incorporating technology into the traditional tools used in teaching people with autism basic life skills. the project is coded in color blue and has the puzzle logo which are symbolic to autism and have been incorporated with the aim to spread awareness.\nthe project has three distinct highlights aimed at trying to cover people who are on different levels of the autism spectrum.\nspecial educators use traditional means of making adaptive books to teach life skills like brushing teeth etc and to reduce the cumbersome work load in making work system using hand crafts, we decided to design prototype centralizing to the people with autism by keeping them simple so that with one click of mouse, the child can easily navigate through the steps and learn new skills making the work easier for both the educators and children.\nthe website in itself is a compact platform providing knowledge of autism and spreading awareness about it through blogs ,videos etc as well as linking the features together including the game prototype and bot.\nthe discord bot linked with the website is a -----> tool !!!  for non verbal autistic people (approx 30-40% people on spectrum are non verbal) who can use the text input to interact with the bot. the bot provides positive reinforcements to the user which in return helps in uplifting the mood of the user as well as increasing confidence and preaching self worth through phrases like \u201cyou can do it.\u201d ,\u201cyou are the best.\u201d etc. it also has quotations from personalities like rumi etc.\nhow we built it\nour project is an amalgamation of the following things-\ndiscord bot- python\nfront end -html/css\n- bootstrap\n- java script\nback end - php\neducational game prototype -google slides\nchallenges we ran into\n1)we faced a lot of system and softwares problems while the frontend of the website was being created due to which we had to redo the whole front end.\n2)we had one of our teammate leave us just after the start of the hack due to some unforeseen circumstances, which made our schedule more scrambled but we managed to finish our project .\n3) it took us some time and help from the mentors to figure out a way to present the idea of the game in form of a prototype but we managed to make the best use of google slides in prototyping our idea.\naccomplishments that we're proud of\nwe now know the use of html/css,using sandbox,bootstrap,template editing and shuffling in an intermediate level as well as learning how to create games/short animations using google slides.\nwe are proud of how we were able to come together as a team since we had never met each other prior but were still able to work together in a collaborative and healthy environment.\nwhat we learned\nthe research phase of the hack was an eye opening experience for us all as a team especially learning about autism and how an autistic person spends his/her daily routine especially the tools and techniques used in teaching an autistic child life skills. we also realised that apart from numerous blogs on autism, there isn't much out there in terms of softwares or educational games etc. catering to the needs of autistic people etc which motivated us to take this project in the hackathon.\non the technical forefront, working on the project has taught us how to manage our time while simultaneously collaborating with each other and making quick decisions as well as learning new technical hacks like extensions etc which enables us to code in real time.\nwhat's next for aut with us\npost the hackathon, converting the prototype for the educational games into actual games using platforms like unity is on our to-do list. considering the lack of educational games built catering to autistic people, we are planning to improve our prototype and turn it into educational games teaching people with autism basic life skills helping them with their journey into being independent.\nwe also plan on improving the discord bot as well as text to speech functionality keeping in mind the non verbal children on the autism spectrum and helping them communicate through technology.we plan on making the security of our website as well as the discord so as to make sure there is no breach or unparliamentary action in our environment and to ensure the environment runs as a safe,civil and learning platform.\nalongside this we look forward to adding more functionality to the website and making it dynamic with good quality content and incorporating a better tech stack like react into it. we also plan on improving the content of the website in terms of resources etc like sharing the journey of people who are on the spectrum, sharing books written by people having autism ,sessions and conferences catering to autism etc.\nwe hope to even develop an app version of the website , incorporate an inbuilt chat feature for a peer to peer interaction or peer to admin interaction as well as develop more advanced,secure admin records with the database.\n2) wcag\nwe plan to integrate and follow the wcag 2.0(web accessibility guidelines) which are :\n1)provides content not prone to have seizures\n2)content should be substituted with pictures for better understanding\n3)sentence should not be cluttered\n4)font needs to be large and legible\n5)content present needs to be verified such that there is no violation of moral conduct. overall we have a lot to look forward to in terms of our hack.\n-->categories applied for: 1st place pwe 2nd place pwe 3rd place pwe best overall idea", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59500205}, {"Unnamed: 0": 218, "autor": "Flapped", "date": null, "content": "Inspiration\nImposter Syndrome Therapeutic App Focus Group: A method of tackling an issue that is becoming widely prevalent among professionals in all fields, imposter syndrome. Imposter Syndrome involves feelings of self-doubt and personal incompetence that persist despite your education, experience, and accomplishments. To counter these feelings, you might end up working harder and holding yourself to ever higher standards. The current idea for the app is to gauge the student's state of mind (via questions or a happy meter, etc.) and act accordingly depending on their input. The app will give the students challenges ranging from simple to complex coding tasks based on the b.tech curriculum or any other. This will helpfully boost their confidence by being able to accomplish small tasks that they are comfortable with. The app will also provide features for sending push notifications of uplifting quotes and also a discussion forum for CS students to help each other out.\nWhat it does\nHelp students to gain confidence, socialize, communicate, colaborate\nHow we built it\nWe used MERN stack and alan sdk, symbol.ai\nChallenges we ran into\nmaking tool provided by sponsors to communicate with each other.\nAccomplishments that we're proud of\nmaking the best use of the tools and making it work and making sure each feature is useful for the user.\nWhat we learned\nlatest tools and technologies provided by sponsors , github best practices, and making best use of documentation\nWhat's next for Flapped\nadding more features for users and improving user experience. We'll add more of getstreamio features and improve alan sdk with the app", "link": "https://devpost.com/software/flapped", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nimposter syndrome therapeutic app focus group: a method of tackling an issue that is becoming widely prevalent among professionals in all fields, imposter syndrome. imposter syndrome involves feelings of self-doubt and personal incompetence that persist despite your education, experience, and accomplishments. to counter these feelings, you might end up working harder and holding yourself to ever higher standards. the current idea for the app is to gauge the student's state of mind (via questions or a happy meter, etc.) and act accordingly depending on their input. the app will give the students challenges ranging from simple to complex coding tasks based on the b.tech curriculum or any other. this will helpfully boost their confidence by being able to accomplish small tasks that they are comfortable with. the app will also provide features for sending push notifications of uplifting quotes and also a discussion forum for cs students to help each other out.\nwhat it does\nhelp students to gain confidence, socialize, communicate, colaborate\nhow we built it\nwe used mern stack and alan sdk, symbol.ai\nchallenges we ran into\nmaking -----> tool !!!  provided by sponsors to communicate with each other.\naccomplishments that we're proud of\nmaking the best use of the tools and making it work and making sure each feature is useful for the user.\nwhat we learned\nlatest tools and technologies provided by sponsors , github best practices, and making best use of documentation\nwhat's next for flapped\nadding more features for users and improving user experience. we'll add more of getstreamio features and improve alan sdk with the app", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 6, "media": null, "medialink": null, "identifyer": 59500218}, {"Unnamed: 0": 225, "autor": "Custom Policies using Datree", "date": null, "content": "Inspiration\nDebugging in k8s environment is complex. In most cases the issues are related to configuration of manifests, therefore validating the inputs in manifest before hand should be a good practice. Datree is one such tool that allows designers to write policies to validate the manifest. Our idea is to use this Policy as code to extend it to follow the deployment best practices like terminationPolicies to avoid production surprises.\nWhat it does\nIt uses the Datree tool to validate certain k8s attributes which are must have to debug the cluster in minimum time and cost.\nHow we built it\nWe have build it using the datree json and yaml schema , where we used the underlying Datree functionality to validate our k8s before deployments.\nChallenges we ran into\nInitially, since the tool is very new to us, we struggled alot to write complex policies but with the help of datree engineers demo and minimal understanding of go we were able to make it.\nAccomplishments that we're proud of\nMajor accomplishments are:\nBeing able to validate the manifest against custom policies,\nUnderstanding the underlying Datree architecture and how it works,\nAt the end of the day, the courage we accomplish to complete the planned work , it feels great. ## What we learned Learned to debug the k8s manifest. Prior manifest validation can reduce the change of errors.\n## What's next for Custom Policies using Datree I would like to see Datree integration with open-source k8s workflows to catch required custom policies before the code is merged.", "link": "https://devpost.com/software/custom-policies-using-datree", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ndebugging in k8s environment is complex. in most cases the issues are related to configuration of manifests, therefore validating the inputs in manifest before hand should be a good practice. datree is one such -----> tool !!!  that allows designers to write policies to validate the manifest. our idea is to use this policy as code to extend it to follow the deployment best practices like terminationpolicies to avoid production surprises.\nwhat it does\nit uses the datree tool to validate certain k8s attributes which are must have to debug the cluster in minimum time and cost.\nhow we built it\nwe have build it using the datree json and yaml schema , where we used the underlying datree functionality to validate our k8s before deployments.\nchallenges we ran into\ninitially, since the tool is very new to us, we struggled alot to write complex policies but with the help of datree engineers demo and minimal understanding of go we were able to make it.\naccomplishments that we're proud of\nmajor accomplishments are:\nbeing able to validate the manifest against custom policies,\nunderstanding the underlying datree architecture and how it works,\nat the end of the day, the courage we accomplish to complete the planned work , it feels great. ## what we learned learned to debug the k8s manifest. prior manifest validation can reduce the change of errors.\n## what's next for custom policies using datree i would like to see datree integration with open-source k8s workflows to catch required custom policies before the code is merged.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59500225}, {"Unnamed: 0": 227, "autor": "Karen - The Summarizer Whatsapp Bot using Twilio", "date": null, "content": "Inspiration\nEvery needs are integrated with WhatsApp nowadays. Starting from sending messages to money we can do with a single tick. This is the inspiration that i should build something which will be accessible more easily. With the help of Twilio WhatsApp Sandbox i decided to create a WhatsApp Bot. Now , What to do in Bot ? Then I came to know about Summarization using NLP algorithm and Google Pegasus Summarization Model . So, the main goal of the idea inspired from this . Too reduce much time in a webpage for a information we have to undergo with many unnecessary information ads, etc. So Karen will make it simple with WhatsApp Bot.\nWhat it does\nWe have WhatsApp Bot and Web Application in where users will provide the URL of a Blog , News or any article and you will get Summarized paragraph of that Web Page.\nHow we built it\nWhatsApp Bot was built with the help of Twilio's WhatsApp Sandbox And Web Application were built with python's flask framework. Finally NLP Algorithm will be performed with nltk open source package.\nChallenges we ran into\nChallenges were quite a lot . Main challenge is that what if user provided Invalid URL to the Bot, and what should the bot replay when the link is invalid . If the user didn't gets a replay then Karen will get some bad feedback. So we made try and exception and if algorithm didn't give us correct output then Bot will tell the user that the link is Invalid.\nAccomplishments that we're proud of\nFinally I have made what i dreamt of with the help of Twilio Tool We made a WhatsApp Bot which will summarize a Web Page.\nWhat we learned\nDuring this Hackathon I have learned lot of things Flask framework , How API works , How the Bots Works , Natural Language Processing, How to deploy a Web Application. And finally about Git and Github.\nWhat's next for Karen - The Summarizer Whatsapp Bot using Twilio\nNext for Karen is to make many more useful features and can be accessible as a Bot in WhatsApp. I have add more features to the web Application . Also to Perform CI/CD which i tried for this hackathon but couldn't able to perform it . I will surely develop in next time.\nI also wrote a blog in Google Dev Library which is under verification. Links were provided in the Try Out Section.", "link": "https://devpost.com/software/karen-the-summarizer-whatsapp-bot-using-twilio", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nevery needs are integrated with whatsapp nowadays. starting from sending messages to money we can do with a single tick. this is the inspiration that i should build something which will be accessible more easily. with the help of twilio whatsapp sandbox i decided to create a whatsapp bot. now , what to do in bot ? then i came to know about summarization using nlp algorithm and google pegasus summarization model . so, the main goal of the idea inspired from this . too reduce much time in a webpage for a information we have to undergo with many unnecessary information ads, etc. so karen will make it simple with whatsapp bot.\nwhat it does\nwe have whatsapp bot and web application in where users will provide the url of a blog , news or any article and you will get summarized paragraph of that web page.\nhow we built it\nwhatsapp bot was built with the help of twilio's whatsapp sandbox and web application were built with python's flask framework. finally nlp algorithm will be performed with nltk open source package.\nchallenges we ran into\nchallenges were quite a lot . main challenge is that what if user provided invalid url to the bot, and what should the bot replay when the link is invalid . if the user didn't gets a replay then karen will get some bad feedback. so we made try and exception and if algorithm didn't give us correct output then bot will tell the user that the link is invalid.\naccomplishments that we're proud of\nfinally i have made what i dreamt of with the help of twilio -----> tool !!!  we made a whatsapp bot which will summarize a web page.\nwhat we learned\nduring this hackathon i have learned lot of things flask framework , how api works , how the bots works , natural language processing, how to deploy a web application. and finally about git and github.\nwhat's next for karen - the summarizer whatsapp bot using twilio\nnext for karen is to make many more useful features and can be accessible as a bot in whatsapp. i have add more features to the web application . also to perform ci/cd which i tried for this hackathon but couldn't able to perform it . i will surely develop in next time.\ni also wrote a blog in google dev library which is under verification. links were provided in the try out section.", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59500227}, {"Unnamed: 0": 229, "autor": "Custom policies to ensure Stability, Security and Governance", "date": null, "content": "Inspiration\nAs an infrastructure administrator, l spend lot of time manually reviewing Kubernetes Manifests created by my team members before apply them to live clusters. I have to manually check and inform them about security best practices that they should follow in their manifest files.\nTo solve this problem, Datree provides a CLI tool that can be plugged into our existing CI/CD systems. Datree comes with number of useful defaults that a team can use. But organizations have a number of different use cases that cannot be all addressed by a fixed number of default policies. Kubernetes Administrators will definitely have to create custom policy checks customized to their organization.\nWhat it does\nAs mentioned above, there is an important requirement of creating custom policies for any policy enforcement tool. Datree uses JSON schema to create custom policies. But it doesn't show examples that would cover most of the use cases. This project solves that.\nIn this project, I have included 3 custom policies containing 8 different rules. This sample policy can help the admins to get quickly started with Datree and adopt it for their organization. Easy onboarding to a new tool can be a competitive advantage for the tool company and help in increasing conversion rates.\nHow we built it\nI studied JSON Schema in depth and spoke to 3 kubernetes administrators to come up with list of interesting policy use cases that are missing in the default list.\nI then built 3 policies and 8 rules and created sample kubernetes manifest files to test all those rules. I ensured that pass.yaml passes all the rules and fail.yaml actually fails in all 8 rules. This is very crucial for developers looking for sample custom policies to implement in their organization.\nChallenges we ran into\nThere were two rules on which I spent more than 4 hours but still couldn't implement for Datree platform. I did try to discuss that on their slack channel, but wasn't able to resolve the problems during the hackathon period. The two rules were:\nEqual memory in requests and limit rule (Stability Policy): Kubernetes can terminate Pods requesting extra memory if memory is needed on the node. This rule promotes stability by ensuring that all containers requested memory limit is exactly equal to the memory limit. To implement this rule, I wanted to extract value for requests and compare it with value defined in limit.\nMaximum ratio for container resource limits to requests (Governance Policy): This rule can be useful for organization that want to ensure that the ratio of limits and requests is under a certain value for containers. Keeping very high limit compared to request isn't a good practice and should be avoided. To implement this rule, I had to perform division operation on two values.\nAccomplishments that we're proud of\nCovering most of the use cases that an administrator might want to implement using custom policies. I am sure that my project would be helpful for all such admins who want to quickly get started with Datree platform and adopt it in their organizations.\nWhat's next\nSolve problem in implementing the two rules mentioned above. I'll discuss them with Datree core maintainers and try to come up with a possible solution\nI am certain that there are few use cases which I would have missed covering in my policy. I would love to speak to more Kubernetes Administrators to find their policy challenges and implement them in the sample policy.", "link": "https://devpost.com/software/useful-best-practices-custom-policies-for-datree", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nas an infrastructure administrator, l spend lot of time manually reviewing kubernetes manifests created by my team members before apply them to live clusters. i have to manually check and inform them about security best practices that they should follow in their manifest files.\nto solve this problem, datree provides a cli -----> tool !!!  that can be plugged into our existing ci/cd systems. datree comes with number of useful defaults that a team can use. but organizations have a number of different use cases that cannot be all addressed by a fixed number of default policies. kubernetes administrators will definitely have to create custom policy checks customized to their organization.\nwhat it does\nas mentioned above, there is an important requirement of creating custom policies for any policy enforcement tool. datree uses json schema to create custom policies. but it doesn't show examples that would cover most of the use cases. this project solves that.\nin this project, i have included 3 custom policies containing 8 different rules. this sample policy can help the admins to get quickly started with datree and adopt it for their organization. easy onboarding to a new tool can be a competitive advantage for the tool company and help in increasing conversion rates.\nhow we built it\ni studied json schema in depth and spoke to 3 kubernetes administrators to come up with list of interesting policy use cases that are missing in the default list.\ni then built 3 policies and 8 rules and created sample kubernetes manifest files to test all those rules. i ensured that pass.yaml passes all the rules and fail.yaml actually fails in all 8 rules. this is very crucial for developers looking for sample custom policies to implement in their organization.\nchallenges we ran into\nthere were two rules on which i spent more than 4 hours but still couldn't implement for datree platform. i did try to discuss that on their slack channel, but wasn't able to resolve the problems during the hackathon period. the two rules were:\nequal memory in requests and limit rule (stability policy): kubernetes can terminate pods requesting extra memory if memory is needed on the node. this rule promotes stability by ensuring that all containers requested memory limit is exactly equal to the memory limit. to implement this rule, i wanted to extract value for requests and compare it with value defined in limit.\nmaximum ratio for container resource limits to requests (governance policy): this rule can be useful for organization that want to ensure that the ratio of limits and requests is under a certain value for containers. keeping very high limit compared to request isn't a good practice and should be avoided. to implement this rule, i had to perform division operation on two values.\naccomplishments that we're proud of\ncovering most of the use cases that an administrator might want to implement using custom policies. i am sure that my project would be helpful for all such admins who want to quickly get started with datree platform and adopt it in their organizations.\nwhat's next\nsolve problem in implementing the two rules mentioned above. i'll discuss them with datree core maintainers and try to come up with a possible solution\ni am certain that there are few use cases which i would have missed covering in my policy. i would love to speak to more kubernetes administrators to find their policy challenges and implement them in the sample policy.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59500229}, {"Unnamed: 0": 236, "autor": "Pizza Cook Party", "date": null, "content": "Inspiration\nMy love for pizza <3\nWhat it does\nyou can schedule a pizza cook party and what the wesite does is send an invitation to each guest with the information of the party a a acontribution that the guests needs to bring to the party, for example, it could be an ingredient or an tool that the host dosen't have.\nHow we built it\nNode\nChallenges we ran into\nI have never used Auth0 or sendgrid before and I strugled a liitle with the 5 version of marko.\nAccomplishments that we're proud of\nI tried new tools like auth0 and twillo!!!\nWhat we learned\nHow to send emails", "link": "https://devpost.com/software/pizza-cook-party", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nmy love for pizza <3\nwhat it does\nyou can schedule a pizza cook party and what the wesite does is send an invitation to each guest with the information of the party a a acontribution that the guests needs to bring to the party, for example, it could be an ingredient or an -----> tool !!!  that the host dosen't have.\nhow we built it\nnode\nchallenges we ran into\ni have never used auth0 or sendgrid before and i strugled a liitle with the 5 version of marko.\naccomplishments that we're proud of\ni tried new tools like auth0 and twillo!!!\nwhat we learned\nhow to send emails", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59500236}, {"Unnamed: 0": 273, "autor": "Apolo M'estimo M'animo", "date": null, "content": "Inspiration\nWhen we are feeling low, sometimes we want someone that stays and makes us feel better, but not always we will dispose of this person. Ourselves are the only ones that always will be there.\nWhat it does\nWe have created an app that acts as a companion for anyone that is going through a rough time. The main idea of the app is to provide a safe space where the user can translate his/her thoughts into videos/images/audios/text, creating a sort of emotional and memories diary.\nWe first ask the user for a picture and perform sentiment analysis to roughly determine their emotional status. Given the emotional status, we decide if the user is happy enough to save thoughts/memories for future times or sad enough to receive good memories or past thoughts. We also allow the user to choose if he/she is feeling in a different mood than predicted and would like to perform a different action. With the texts the user fills, we perform sentiment analysis on those texts through deep learning to get historical data of the user\u2019s moods. This latter data could be forwarded to a psychologist if the case were extreme\nThe ultimate goal of the app is to, in the end, make the user smile (through reliving a good memory, seeing something funny, getting reassured by their own written thoughts in the past, etc).\nHow we built it\nWe used Streamlit in order to develop the app and involve all the models used: a facial recognitions sentiment classification and a text sentiment multi-class detector\nAccomplishments that we're proud of\nWe feel that we have created a tool that, at least, can give the user a smile. We thought that a first goal as simple as that could already be meaningful for a user being in a bad state. On top of that, we have created a safe state where thoughts, memories and content can be saved and remembered.\nWhat's next for Apolo M'estimo M'animo\nLink the app with some experts in order to be a link between people with the need for specialists and these specialists.", "link": "https://devpost.com/software/apolo-m-estimo-m-animo", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwhen we are feeling low, sometimes we want someone that stays and makes us feel better, but not always we will dispose of this person. ourselves are the only ones that always will be there.\nwhat it does\nwe have created an app that acts as a companion for anyone that is going through a rough time. the main idea of the app is to provide a safe space where the user can translate his/her thoughts into videos/images/audios/text, creating a sort of emotional and memories diary.\nwe first ask the user for a picture and perform sentiment analysis to roughly determine their emotional status. given the emotional status, we decide if the user is happy enough to save thoughts/memories for future times or sad enough to receive good memories or past thoughts. we also allow the user to choose if he/she is feeling in a different mood than predicted and would like to perform a different action. with the texts the user fills, we perform sentiment analysis on those texts through deep learning to get historical data of the user\u2019s moods. this latter data could be forwarded to a psychologist if the case were extreme\nthe ultimate goal of the app is to, in the end, make the user smile (through reliving a good memory, seeing something funny, getting reassured by their own written thoughts in the past, etc).\nhow we built it\nwe used streamlit in order to develop the app and involve all the models used: a facial recognitions sentiment classification and a text sentiment multi-class detector\naccomplishments that we're proud of\nwe feel that we have created a -----> tool !!!  that, at least, can give the user a smile. we thought that a first goal as simple as that could already be meaningful for a user being in a bad state. on top of that, we have created a safe state where thoughts, memories and content can be saved and remembered.\nwhat's next for apolo m'estimo m'animo\nlink the app with some experts in order to be a link between people with the need for specialists and these specialists.", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59500273}, {"Unnamed: 0": 291, "autor": "How much is 2 much", "date": null, "content": "Inspiration\nMany university students (especially first year students) can sometimes go overboard when drinking alcohol and we aim to just make them realise how much should be a sensible amount to consume.\nWhat it does\nThe user inputs their information such as age, weight, height and the program creates a rough estimate of your \"alcohol allowance\", that if you go over you are likely to not be able to handle it. Afterwards, the user can select what drinks they are consuming and also select the amount (in ml), and we show them what percentage of their alcohol allowance they have \"used up\".\nHow we built it\nWe built it in Java, using eclipse and a tool called window-builder.\nChallenges we ran into\nThe hardest challenge was definetly trying to come up with an estimate that is relatively close to something that would apply to real life. We made a generalised prototype of this app because in order to be more precise with this type of app you would need more information about the person's health and the drinks they consume. This type of app would be easier to implement with information provided by smart watches such as apple watch that have a good record of you health info.\nAccomplishments that we're proud of\nWe actually have a program that runs.\nWhat we learned\nBook a hotel for the next hackathon. Do not chug 2 Monster cans in less than an hour.\nWhat's next for How much is 2 much\nDeveloping the prototype furthe to be compatible with different platforms, such as phones, smart watches, etc.", "link": "https://devpost.com/software/how-much-is-2-much", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nmany university students (especially first year students) can sometimes go overboard when drinking alcohol and we aim to just make them realise how much should be a sensible amount to consume.\nwhat it does\nthe user inputs their information such as age, weight, height and the program creates a rough estimate of your \"alcohol allowance\", that if you go over you are likely to not be able to handle it. afterwards, the user can select what drinks they are consuming and also select the amount (in ml), and we show them what percentage of their alcohol allowance they have \"used up\".\nhow we built it\nwe built it in java, using eclipse and a -----> tool !!!  called window-builder.\nchallenges we ran into\nthe hardest challenge was definetly trying to come up with an estimate that is relatively close to something that would apply to real life. we made a generalised prototype of this app because in order to be more precise with this type of app you would need more information about the person's health and the drinks they consume. this type of app would be easier to implement with information provided by smart watches such as apple watch that have a good record of you health info.\naccomplishments that we're proud of\nwe actually have a program that runs.\nwhat we learned\nbook a hotel for the next hackathon. do not chug 2 monster cans in less than an hour.\nwhat's next for how much is 2 much\ndeveloping the prototype furthe to be compatible with different platforms, such as phones, smart watches, etc.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59500291}, {"Unnamed: 0": 298, "autor": "Scale to the moon!", "date": null, "content": "Inspiration\nI have built a lot of SaaS products based on Docker and Kubernetes, and many a times, scaling them is a hassle due to manual configuration and continous testing of the cluster, and the containers within it. Hence, for this hackathon, I had the idea of creating a policy for datree which would enable Kubernetes engineers to scale their services hassle-free and quickly. I believe it can make quite the impact on the way we use Kubernetes in our daily workflow.\nWhat it does\nThis policy checks and validates the following fields: targetCPUUtilizationPercentage, minReplicas, maxReplicas, scaleTargetRef and containers[].resource.limits and containers[].resource.requests. It validates these fields for the resource, HorizontalPodAutoscaler. The aim is to reduce the time and effort is takes to scale Kubernetes clusters and enhance developer productivity and decrease costs. HPA is an amazing feature of Kubernetes, thus it only makes sense to use it right.\nHow we built it\nI built it using the following technologies:\nYAML: It was used as the language of choice to write out the schema logic as well as the policy itself. Using Yaml was a big decision as I had no prior experience with Yaml, so I learnt a great deal about it.\nDatree: It was used to develop my policy for, and to test my policy. Datree was undoubtedly the most important technology since this policy is built for it.\nChallenges we ran into\nOne challenge I ran into was the time constraint as I joined the hackathon a bit late, so I had to cover up by watching the video and exploring the CLI. Then I had to write the policy with no prior experience pertaining Yaml, so all in all, it was a superb learning experience for me.\nAccomplishments that we're proud of\nThe main accomplishment for me was that I was able to build this entire policy on my own with no prior knowledge of Yaml and Datree. Moreover, I didn't even have any knowledge of json schema which was the schema language for writing our Kubernetes schemas in the policies, so I would say that I really challenges myself and explored these new technologies.\nWhat we learned\nI learnt about Yaml, since it was my first time working and writing code in Yaml. I also learnt about Datree because before this hackathon, I hadn't even heard the name of Datree yet here I am developing policies for it, so I had to learn about using the CLI, writing custom policies and even making pull requests to the GitHub repository for Datree.\nWhat's next for Scale to the moon!\nIn the future, I would like to add more rules to the policy as it can really automate and help Kubernetes engineers save time and money. Plus, I would probably expand this policy to a standalone CLI tool as well, if time permits, making this a full-fledged project.", "link": "https://devpost.com/software/horizontal-pod-autoscaling-policy-datree", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ni have built a lot of saas products based on docker and kubernetes, and many a times, scaling them is a hassle due to manual configuration and continous testing of the cluster, and the containers within it. hence, for this hackathon, i had the idea of creating a policy for datree which would enable kubernetes engineers to scale their services hassle-free and quickly. i believe it can make quite the impact on the way we use kubernetes in our daily workflow.\nwhat it does\nthis policy checks and validates the following fields: targetcpuutilizationpercentage, minreplicas, maxreplicas, scaletargetref and containers[].resource.limits and containers[].resource.requests. it validates these fields for the resource, horizontalpodautoscaler. the aim is to reduce the time and effort is takes to scale kubernetes clusters and enhance developer productivity and decrease costs. hpa is an amazing feature of kubernetes, thus it only makes sense to use it right.\nhow we built it\ni built it using the following technologies:\nyaml: it was used as the language of choice to write out the schema logic as well as the policy itself. using yaml was a big decision as i had no prior experience with yaml, so i learnt a great deal about it.\ndatree: it was used to develop my policy for, and to test my policy. datree was undoubtedly the most important technology since this policy is built for it.\nchallenges we ran into\none challenge i ran into was the time constraint as i joined the hackathon a bit late, so i had to cover up by watching the video and exploring the cli. then i had to write the policy with no prior experience pertaining yaml, so all in all, it was a superb learning experience for me.\naccomplishments that we're proud of\nthe main accomplishment for me was that i was able to build this entire policy on my own with no prior knowledge of yaml and datree. moreover, i didn't even have any knowledge of json schema which was the schema language for writing our kubernetes schemas in the policies, so i would say that i really challenges myself and explored these new technologies.\nwhat we learned\ni learnt about yaml, since it was my first time working and writing code in yaml. i also learnt about datree because before this hackathon, i hadn't even heard the name of datree yet here i am developing policies for it, so i had to learn about using the cli, writing custom policies and even making pull requests to the github repository for datree.\nwhat's next for scale to the moon!\nin the future, i would like to add more rules to the policy as it can really automate and help kubernetes engineers save time and money. plus, i would probably expand this policy to a standalone cli -----> tool !!!  as well, if time permits, making this a full-fledged project.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59500298}, {"Unnamed: 0": 303, "autor": "Pandbot", "date": null, "content": "Inspiration\nSometimes mental health problems emerge from daily tasks and routines. That's why we found out it would be helpful to create a tool to prevent this kind of issues. As a result, we decided to do a Telegram bot because of its accessibility and simplicity.\nWhat it does\nPandbot's main function is to go along with the user during daily life, to make the user smile and overcome his/her/their situation. User can send media from themselves to the bot and when they need it the bot will resend these good moments. Moreover, the bot sends daily challenges related to self-esteem such as walking or meditating. Finally we implemented a message and media system to resend messages to their future themselves.\nHow we built it\nWe used python language to program the bot from Telegram, moreover we used Inkscape to graphically design the Pandbot.\nChallenges we ran into\nProgramming the bot itself is a big challenge for us. We also had difficulties debugging.\nAccomplishments that we're proud of\nMostly, we are proud of programming the bot.\nWhat we learned\nWe learn that mental health is a really big issue in our society that anyone can suffer. We really need to work on new technologies to help solving these problems.\nWhat's next for Pandbot\nWe thought about adding a voice recogniser feature to detect whether the user feels better or not.", "link": "https://devpost.com/software/pandbot", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nsometimes mental health problems emerge from daily tasks and routines. that's why we found out it would be helpful to create a -----> tool !!!  to prevent this kind of issues. as a result, we decided to do a telegram bot because of its accessibility and simplicity.\nwhat it does\npandbot's main function is to go along with the user during daily life, to make the user smile and overcome his/her/their situation. user can send media from themselves to the bot and when they need it the bot will resend these good moments. moreover, the bot sends daily challenges related to self-esteem such as walking or meditating. finally we implemented a message and media system to resend messages to their future themselves.\nhow we built it\nwe used python language to program the bot from telegram, moreover we used inkscape to graphically design the pandbot.\nchallenges we ran into\nprogramming the bot itself is a big challenge for us. we also had difficulties debugging.\naccomplishments that we're proud of\nmostly, we are proud of programming the bot.\nwhat we learned\nwe learn that mental health is a really big issue in our society that anyone can suffer. we really need to work on new technologies to help solving these problems.\nwhat's next for pandbot\nwe thought about adding a voice recogniser feature to detect whether the user feels better or not.", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59500303}, {"Unnamed: 0": 312, "autor": "Resc-Mgmt-APIServiceMadeWithCIVO_K3s_Cluster&DevtronTools", "date": null, "content": "Inspiration\nInspired by CIVO kubernetes and CI/CD pipelines using Devtron tool\nWhat it does\nOur API enables you to make use of managing and accessing your resources for storing PDF, TXT files and images [Blogs]\nHow we built it\nBuilt around CIVO Kubernetes cluster and Devtron tools\nChallenges we ran into\nHow to configure our cluster for our Networking How to setup CI/CD pipelines How to setup deployment charts\nAccomplishments that we're proud of\nCIVO kubernetes configuration and CI/CD pipelines setup\nWhat we learned\nKubernetes networking, CI/CD configuration with Devtron tools.\nWhat's next for Resource Management APP\nTo integrate with UI and setup multi repos for our APP using Devtron tools", "link": "https://devpost.com/software/resource-management-app", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ninspired by civo kubernetes and ci/cd pipelines using devtron -----> tool !!! \nwhat it does\nour api enables you to make use of managing and accessing your resources for storing pdf, txt files and images [blogs]\nhow we built it\nbuilt around civo kubernetes cluster and devtron tools\nchallenges we ran into\nhow to configure our cluster for our networking how to setup ci/cd pipelines how to setup deployment charts\naccomplishments that we're proud of\ncivo kubernetes configuration and ci/cd pipelines setup\nwhat we learned\nkubernetes networking, ci/cd configuration with devtron tools.\nwhat's next for resource management app\nto integrate with ui and setup multi repos for our app using devtron tools", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59500312}, {"Unnamed: 0": 338, "autor": "Open Source Buddy", "date": null, "content": "\ud83d\udd25 Inspiration !\nThe need to rise of awareness of Open Source Contribution made us 4 newbies Aman, Atif, Aditya and myself(Paavani) to bring together some of the best resources at a single platform for a beginner in order to get started with their Open Source Journey. Our main inspiration came from the amazing and most welcoming opensource communities i.e Community classroom , Eddie hub , CNCF, DOK community and others , and their pure motive to help and support the community , without demanding anything from the individual.\n\ud83e\udd29What it does ?\nOpen Source Buddy will help every individual to explore Open Source World and guide them to kick-start their journey. This project provides all the necessary materials and resources and proper roadmap with the correct guidance. It also connects the users to other welcoming Open Source communities and popular Open Source programs.\n\ud83e\udd14How we built it?\nOpen Source Buddy couldn't have been possible without Docker, Civo, CNCF, Kubernates and lens IDE. HTML, CSS, Javascript and Bootstrap helped us to make this super fast responsive website with all required resources and attractive features!\n\ud83d\ude09Challenges we ran into?\nWe faced many errors after deploying the website like the 502 error and also Bad Gateway error, then we later on realized that there were minor mistakes in our deployment yaml and ingress yaml files. We were successful in resolving them after seeking help from the community and twitter. After we successfully deployed the demo project, then we got an error while deploying the main project. We realised that there was a bug in github, which we then resolved.\n\ud83d\ude0eAccomplishments that we're proud of !\nThis being the first hackathon for some of us, implementing our idea in such a short span of time through strong dedication is what we are most proud of. We are proud of the fact that we could successfully complete our project without any glitches and make it super fast and responsive.\n\u2699\ufe0fWhat we learned?\nFirst, we learned how to create a Docker images, then we learned how to make cluster using civo kubernetes. After installing Lens IDE we learnt to generate YAML files from the IDE by adding clusters and also were able to monitor our cluster's performance by built in promenthius tool inside the IDE. On the non-technical front, we learnt a lot of important things like leadership, team coordination and patience.\n\ud83d\ude00What's next for Open Source Buddy?\nAs contributing to the Open Source is little harder for Beginners, we will try to make Open Source Buddy more user-friendly and include more resources in an organised way for Open Source Contribution. Open source buddy \ud83d\udd17 link:- final-web.c69ae80b-b35e-4cab-b30c-f9e7942b3df2.k8s.civo.com", "link": "https://devpost.com/software/open-source-buddy", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "\ud83d\udd25 inspiration !\nthe need to rise of awareness of open source contribution made us 4 newbies aman, atif, aditya and myself(paavani) to bring together some of the best resources at a single platform for a beginner in order to get started with their open source journey. our main inspiration came from the amazing and most welcoming opensource communities i.e community classroom , eddie hub , cncf, dok community and others , and their pure motive to help and support the community , without demanding anything from the individual.\n\ud83e\udd29what it does ?\nopen source buddy will help every individual to explore open source world and guide them to kick-start their journey. this project provides all the necessary materials and resources and proper roadmap with the correct guidance. it also connects the users to other welcoming open source communities and popular open source programs.\n\ud83e\udd14how we built it?\nopen source buddy couldn't have been possible without docker, civo, cncf, kubernates and lens ide. html, css, javascript and bootstrap helped us to make this super fast responsive website with all required resources and attractive features!\n\ud83d\ude09challenges we ran into?\nwe faced many errors after deploying the website like the 502 error and also bad gateway error, then we later on realized that there were minor mistakes in our deployment yaml and ingress yaml files. we were successful in resolving them after seeking help from the community and twitter. after we successfully deployed the demo project, then we got an error while deploying the main project. we realised that there was a bug in github, which we then resolved.\n\ud83d\ude0eaccomplishments that we're proud of !\nthis being the first hackathon for some of us, implementing our idea in such a short span of time through strong dedication is what we are most proud of. we are proud of the fact that we could successfully complete our project without any glitches and make it super fast and responsive.\n\u2699\ufe0fwhat we learned?\nfirst, we learned how to create a docker images, then we learned how to make cluster using civo kubernetes. after installing lens ide we learnt to generate yaml files from the ide by adding clusters and also were able to monitor our cluster's performance by built in promenthius -----> tool !!!  inside the ide. on the non-technical front, we learnt a lot of important things like leadership, team coordination and patience.\n\ud83d\ude00what's next for open source buddy?\nas contributing to the open source is little harder for beginners, we will try to make open source buddy more user-friendly and include more resources in an organised way for open source contribution. open source buddy \ud83d\udd17 link:- final-web.c69ae80b-b35e-4cab-b30c-f9e7942b3df2.k8s.civo.com", "sortedWord": "None", "removed": "Nan", "score": 6, "comments": 2, "media": null, "medialink": null, "identifyer": 59500338}, {"Unnamed: 0": 363, "autor": "Gun Emu", "date": null, "content": "Inspiration\nThe dire condition in Ethopian Civil war reminded us of the tragedy of armed conflict. Thinking bout this, we wanted to lear more about how governments in conflict are armed. We became inspired to learn more, and help educate others too.\nWhat it does\nUsing data from the uppsala conflict data program and the stockholm peace research institute, gun emu provides an intuitive visualization of the global arms trade between nations, and its relation to the geographies of global violence. It is our hope that our gun emu will inform citizens about the connection between world conflict, their nations, and themselves.\nHow we built it\nWe obtained and processed the data from the uppsala conflict data program and the stockholm peace research institute using pandas and made use of the pydeck module to provide an intuitive visualization of the obtained data.\nAccomplishments that we're proud of\nIncreased our understanding of python, pandas, and pydeck. Developed a tool that can help citizens understand global arms trade and its impact on peace.\nWhat we learned\nGlobal arms trade has significant impacts on global peace and prosperity, and arms trade data can be carefully followed to identify locations under risk of incoming/ongoing violence.\nWhat's next for Gun Emu\nWe noticed patterns between global arms trade and major state-based violent events in almost all countries that have had such events in the past. We hope to understand such patterns to try and predict possible risks to global peace.", "link": "https://devpost.com/software/gun-emu", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nthe dire condition in ethopian civil war reminded us of the tragedy of armed conflict. thinking bout this, we wanted to lear more about how governments in conflict are armed. we became inspired to learn more, and help educate others too.\nwhat it does\nusing data from the uppsala conflict data program and the stockholm peace research institute, gun emu provides an intuitive visualization of the global arms trade between nations, and its relation to the geographies of global violence. it is our hope that our gun emu will inform citizens about the connection between world conflict, their nations, and themselves.\nhow we built it\nwe obtained and processed the data from the uppsala conflict data program and the stockholm peace research institute using pandas and made use of the pydeck module to provide an intuitive visualization of the obtained data.\naccomplishments that we're proud of\nincreased our understanding of python, pandas, and pydeck. developed a -----> tool !!!  that can help citizens understand global arms trade and its impact on peace.\nwhat we learned\nglobal arms trade has significant impacts on global peace and prosperity, and arms trade data can be carefully followed to identify locations under risk of incoming/ongoing violence.\nwhat's next for gun emu\nwe noticed patterns between global arms trade and major state-based violent events in almost all countries that have had such events in the past. we hope to understand such patterns to try and predict possible risks to global peace.", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59500363}, {"Unnamed: 0": 374, "autor": "Todogenix", "date": null, "content": "Inspiration\nCloud Native Hackathon was our first hackathon. We want to learn and explore more about open source and we saw this as a great opportunity.\nWhat it does\nTodogenix is a personal assistant app powered by Alan AI that helps store and manage our daily tasks in three categories Explorer, Flexible and Strict. We can sort our tasks into these three categories based upon our priorities and interests. The best part is that we can relax and navigate the entire application just by voice after logging in. Alan AI does everything possible for us. This might be very helpful for physically challenged people.\nThe app also comes with a Pomodoro timer that helps us manage time between our work and breaks. It follows a simple rule, work for 25 minutes, take a short 5 minute break and repeat. We can adjust the break and work time based upon our capabilities.\nHow we built it\nThere are two versions of the project, the android version and the web version. The android version was built using the android framework and the web was built using react. Both the apps run on same database, i.e. Firebase and are integrated to the same Alan project. So , all our data stays synced across devices. The UI of the application was built first, and basic functionality was implemented and then the voice capability was added.\nChallenges we ran into\nThe primary challenge was to build two versions of the project in 3 days time. We had planned a lot on the project but were not able to implement everything in such a short span of time. Other challenges were to synchronize the method names in Alan Studio, so that we make sure the application works the same way on both the platforms. We also faced a challenge in designing the proper schema of the database as to what data goes where. Overall, the major challenges we faced were to maintain the same functionality of the app on both the platforms and synchronize all the data. We somehow managed to solve many of these challenges and build a working model.\nAccomplishments that we're proud of\nWe got so much exposure to contributing to open source and working with a third party tool. We also had frequent discussions where we found solutions to many of the challenges we faced. We gained communication and time management skills throughout the span of project that we believe are very essential for a developer.\nWhat we learned\nWe learned how to use git and github and also how to script the commands in Alan Studio and learned how to implement the features from the documentation of firebase and Alan Studio.\nWhat's next for Todogenix\nThere will definitely be higher versions of the application, with more and more features in the future. The application does not completely run with voice as of present. Our final goal is to make a hands free application.", "link": "https://devpost.com/software/todogenix", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ncloud native hackathon was our first hackathon. we want to learn and explore more about open source and we saw this as a great opportunity.\nwhat it does\ntodogenix is a personal assistant app powered by alan ai that helps store and manage our daily tasks in three categories explorer, flexible and strict. we can sort our tasks into these three categories based upon our priorities and interests. the best part is that we can relax and navigate the entire application just by voice after logging in. alan ai does everything possible for us. this might be very helpful for physically challenged people.\nthe app also comes with a pomodoro timer that helps us manage time between our work and breaks. it follows a simple rule, work for 25 minutes, take a short 5 minute break and repeat. we can adjust the break and work time based upon our capabilities.\nhow we built it\nthere are two versions of the project, the android version and the web version. the android version was built using the android framework and the web was built using react. both the apps run on same database, i.e. firebase and are integrated to the same alan project. so , all our data stays synced across devices. the ui of the application was built first, and basic functionality was implemented and then the voice capability was added.\nchallenges we ran into\nthe primary challenge was to build two versions of the project in 3 days time. we had planned a lot on the project but were not able to implement everything in such a short span of time. other challenges were to synchronize the method names in alan studio, so that we make sure the application works the same way on both the platforms. we also faced a challenge in designing the proper schema of the database as to what data goes where. overall, the major challenges we faced were to maintain the same functionality of the app on both the platforms and synchronize all the data. we somehow managed to solve many of these challenges and build a working model.\naccomplishments that we're proud of\nwe got so much exposure to contributing to open source and working with a third party -----> tool !!! . we also had frequent discussions where we found solutions to many of the challenges we faced. we gained communication and time management skills throughout the span of project that we believe are very essential for a developer.\nwhat we learned\nwe learned how to use git and github and also how to script the commands in alan studio and learned how to implement the features from the documentation of firebase and alan studio.\nwhat's next for todogenix\nthere will definitely be higher versions of the application, with more and more features in the future. the application does not completely run with voice as of present. our final goal is to make a hands free application.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59500374}, {"Unnamed: 0": 389, "autor": "Datree: Containers_Best_Practices", "date": null, "content": "Inspiration\nLet's take an example of a some developers that made some rectification to K8s configurations files, to deploy their objects, and they push their configuration file on git repository. Now assume that what happens if there is a failure in production.? This will be difficult to handle the all such errors. To answer all such questions , a Datree enters.\nWhat it does?\nA Datree is a CLI tool that supports Kubernetes and help the developers to prevent from making errors. Or we can say that It is a prevention tool that prevents Kubernetes Misconfigurations before to their Production. It contains different Categories of Policies like Containers, Workload, CronJob, Network, and Custom to check different policies.\nHow we get installed in our System?\nTo get Started with datree.io, firstly we need to Install Datree's CLI integration. To do that we have to open Command Line Interface(Windows Poer Shell) and type a simple command to install it: \"iwr -useb https://get.datree.io/windows_install.ps1 | iex\"\nThen Run Datree to test it against a pre-compiled demo file - k8s-demo.yaml\n\"datree.exe test $env:USERPROFILE.datree\\k8s-demo.yaml\"\nThen Sign in to configure your policy .\nChallenges that we faced during creation of own Policy.\nDatree also performs schema validation checks for your files before running the policy checks, If your have build your Policy, make sure to check that it will pass the Schema Validation check. For example If I make an error in the structure of my YAML file and run a Datree check on it, it's going to give an error into it.\nAccomplishments that we're proud of\nFinally, Containers_best_practices Policy check. These Container state is reported by Kubernetes to check containers image state label during of the process of running the container .\nIt helps to easily identify the Containers state label and verify the correct state and have a valid state.\nWhat we learned\nI had learnt so about i) Reading and writing JSON and YAML. ii) Schema and JSON Validation checks iii) Create our own Policy. iv) How Datree helps to prevents Kubernetes Misconfigurations before to their Production using their Custom Policy checks.", "link": "https://devpost.com/software/datree-containers_best_practices", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nlet's take an example of a some developers that made some rectification to k8s configurations files, to deploy their objects, and they push their configuration file on git repository. now assume that what happens if there is a failure in production.? this will be difficult to handle the all such errors. to answer all such questions , a datree enters.\nwhat it does?\na datree is a cli -----> tool !!!  that supports kubernetes and help the developers to prevent from making errors. or we can say that it is a prevention tool that prevents kubernetes misconfigurations before to their production. it contains different categories of policies like containers, workload, cronjob, network, and custom to check different policies.\nhow we get installed in our system?\nto get started with datree.io, firstly we need to install datree's cli integration. to do that we have to open command line interface(windows poer shell) and type a simple command to install it: \"iwr -useb https://get.datree.io/windows_install.ps1 | iex\"\nthen run datree to test it against a pre-compiled demo file - k8s-demo.yaml\n\"datree.exe test $env:userprofile.datree\\k8s-demo.yaml\"\nthen sign in to configure your policy .\nchallenges that we faced during creation of own policy.\ndatree also performs schema validation checks for your files before running the policy checks, if your have build your policy, make sure to check that it will pass the schema validation check. for example if i make an error in the structure of my yaml file and run a datree check on it, it's going to give an error into it.\naccomplishments that we're proud of\nfinally, containers_best_practices policy check. these container state is reported by kubernetes to check containers image state label during of the process of running the container .\nit helps to easily identify the containers state label and verify the correct state and have a valid state.\nwhat we learned\ni had learnt so about i) reading and writing json and yaml. ii) schema and json validation checks iii) create our own policy. iv) how datree helps to prevents kubernetes misconfigurations before to their production using their custom policy checks.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59500389}, {"Unnamed: 0": 390, "autor": "Athena", "date": null, "content": "Inspiration\nI and my friends have been planning a vacation for a while now. It was going to be a non-English speaking country. One of our major concerns was the obvious language barrier. I thought it would be great if there was a tool we could use that could help us translate our words in English to the common language of the country.\nWhen I found out about the 5G Edge Computing Challenge, I thought this would be the perfect opportunity to leverage the ultra-low latency that 5G provides to test out the capability of the solutions and possibly provide a great user experience for travelers to new countries\nWhat it does\nThe solution is a web app that will convert English to a target language e.g Spanish in both text and sound.\nThe language translator web app captures audio input and streams it to a Speech to Text service hosted locally on the server. As the input speech is transcribed, it is sent to a local Python Language Translator service to be translated into the language selected. The transcribed and translated text are both displayed by the app in real-time. Each completed phrase is sent to a Text to Speech service to be spoken in the language earlier selected.\nHow we built it\nThe initial plan was to make use of the AI services made available by AWS. Going through this route would have defeated the purpose of reducing latency as much as possible by calling services not available in AWS Wavelength zones.\nTo solve this I searched online for open source software that could be installed locally on a 5G Edge instance. They included:\nVosk - Speech-to-Text Software\nMachine Translation Service - A Language Translator Service\npyttsx3 - Text-to-Speech\nAll software was installed on a t3.xlarge instance running ubuntu 20. The front-end application was made with angular and is served by a node.js instance. Nginx is used as a proxy server to serve the angular app and to send the audio to be transcribed from the browser to the speech recognition ML software. All these services are on the same server.\nThe server is hosted in the London Wavelength Zone, eu-west-2-wl1-lon-wlz-1.\nChallenges we ran into\nFound out that web workers work only with SSL. The web worker in this application handles sending the audio to the speech recognition software using WebSockets. Due to this, I had to install SSL certificates on the Nginx server\nSSH-ing into the wavelength server. Solved this by creating a bastion host in a public subnet in the same VPC as directed by an Wavelength Developer guide\nAccomplishments that we're proud of\nHacking together open source software related to speech-to-text, language-translator ml services and text-to-voice\nDeploying an application to leverage the ultra low latencies provided by 5G and computing at the edge\nWhat we learned\nGaining more exposure to VPCs and learning about internet gateways, carrier gateways, and CIDR blocks. Basically improving my networking knowledge a lot.\nLearned a enough of python and flask (python web framework) to hack together the open source ML software\nImproved my understanding of angular\nTry It Links\nhttps://mageweave.xyz/ (Vodafone 5G Wavelength Zone)", "link": "https://devpost.com/software/athena-j2f4g8", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ni and my friends have been planning a vacation for a while now. it was going to be a non-english speaking country. one of our major concerns was the obvious language barrier. i thought it would be great if there was a -----> tool !!!  we could use that could help us translate our words in english to the common language of the country.\nwhen i found out about the 5g edge computing challenge, i thought this would be the perfect opportunity to leverage the ultra-low latency that 5g provides to test out the capability of the solutions and possibly provide a great user experience for travelers to new countries\nwhat it does\nthe solution is a web app that will convert english to a target language e.g spanish in both text and sound.\nthe language translator web app captures audio input and streams it to a speech to text service hosted locally on the server. as the input speech is transcribed, it is sent to a local python language translator service to be translated into the language selected. the transcribed and translated text are both displayed by the app in real-time. each completed phrase is sent to a text to speech service to be spoken in the language earlier selected.\nhow we built it\nthe initial plan was to make use of the ai services made available by aws. going through this route would have defeated the purpose of reducing latency as much as possible by calling services not available in aws wavelength zones.\nto solve this i searched online for open source software that could be installed locally on a 5g edge instance. they included:\nvosk - speech-to-text software\nmachine translation service - a language translator service\npyttsx3 - text-to-speech\nall software was installed on a t3.xlarge instance running ubuntu 20. the front-end application was made with angular and is served by a node.js instance. nginx is used as a proxy server to serve the angular app and to send the audio to be transcribed from the browser to the speech recognition ml software. all these services are on the same server.\nthe server is hosted in the london wavelength zone, eu-west-2-wl1-lon-wlz-1.\nchallenges we ran into\nfound out that web workers work only with ssl. the web worker in this application handles sending the audio to the speech recognition software using websockets. due to this, i had to install ssl certificates on the nginx server\nssh-ing into the wavelength server. solved this by creating a bastion host in a public subnet in the same vpc as directed by an wavelength developer guide\naccomplishments that we're proud of\nhacking together open source software related to speech-to-text, language-translator ml services and text-to-voice\ndeploying an application to leverage the ultra low latencies provided by 5g and computing at the edge\nwhat we learned\ngaining more exposure to vpcs and learning about internet gateways, carrier gateways, and cidr blocks. basically improving my networking knowledge a lot.\nlearned a enough of python and flask (python web framework) to hack together the open source ml software\nimproved my understanding of angular\ntry it links\nhttps://mageweave.xyz/ (vodafone 5g wavelength zone)", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59500390}, {"Unnamed: 0": 391, "autor": "Datree Automated", "date": null, "content": "Datree Automated \ud83d\udd28\n\ud83d\udca1 Inspiration\nWhile working towards Datree's sponsor track we realized that building new policies for each use case is redundant and why aren't we trying to just automate it. And we did just that by pivoting to build this tool on the last day of the hack!\n\u2699 What it does\nOur CLI tool takes in a Kubernetes config file, parses it using our custom Datree policy generative algorithm to produce a policy.yml file that can be published to test configurations using Datree.\nFeatures\nParsing YAML config properties\nSupport for Resource Limits. Ex: maximum: 25\nSupports enums, string and limit values\n\ud83d\udd27 How we built it\nThe tool is written in Typescript and we've used Node to run the script.\n\ud83e\udd14 Challenges we ran into\nWe ran into a bunch of challenges while building this, mostly while trying to figure out where we can find the Kubernetes schemas to use (the swagger file on the K8s repository is very difficult to parse).\nWe tried using Kubeconform but ultimately decided to use Kubernetes JSON Schema for our use case.\nBecause of the decision we made to pivot on the last day of the hackathon, our biggest constraint became time. However, we believe our hack is original and innovative enough for us to take this risk and showcase all the possibilities of our idea!\n\ud83c\udfc6 Accomplishments that we're proud of\nGetting it to work\nMaking it stable enough to be used for generating policies\n\ud83d\udcda What we learned\nIncreased our understanding and interest in Kubernetes\nThe problem that Datree is solving and a bit about how it works under the hood\n\ud83e\udd20 What's next for Datree Automated\nWe want to integrate our tool into the existing Datree CLI package and provide an automated solution for every user who wants to build a custom policy. Our tool can also be built into a web app that can generate a Datree policy from a user's Kubernetes configuration file.\nThanks for checking out our hack! \ud83d\ude80 We would love to talk all about it or if you run into any issues trying to run it", "link": "https://devpost.com/software/shhhh", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "datree automated \ud83d\udd28\n\ud83d\udca1 inspiration\nwhile working towards datree's sponsor track we realized that building new policies for each use case is redundant and why aren't we trying to just automate it. and we did just that by pivoting to build this -----> tool !!!  on the last day of the hack!\n\u2699 what it does\nour cli tool takes in a kubernetes config file, parses it using our custom datree policy generative algorithm to produce a policy.yml file that can be published to test configurations using datree.\nfeatures\nparsing yaml config properties\nsupport for resource limits. ex: maximum: 25\nsupports enums, string and limit values\n\ud83d\udd27 how we built it\nthe tool is written in typescript and we've used node to run the script.\n\ud83e\udd14 challenges we ran into\nwe ran into a bunch of challenges while building this, mostly while trying to figure out where we can find the kubernetes schemas to use (the swagger file on the k8s repository is very difficult to parse).\nwe tried using kubeconform but ultimately decided to use kubernetes json schema for our use case.\nbecause of the decision we made to pivot on the last day of the hackathon, our biggest constraint became time. however, we believe our hack is original and innovative enough for us to take this risk and showcase all the possibilities of our idea!\n\ud83c\udfc6 accomplishments that we're proud of\ngetting it to work\nmaking it stable enough to be used for generating policies\n\ud83d\udcda what we learned\nincreased our understanding and interest in kubernetes\nthe problem that datree is solving and a bit about how it works under the hood\n\ud83e\udd20 what's next for datree automated\nwe want to integrate our tool into the existing datree cli package and provide an automated solution for every user who wants to build a custom policy. our tool can also be built into a web app that can generate a datree policy from a user's kubernetes configuration file.\nthanks for checking out our hack! \ud83d\ude80 we would love to talk all about it or if you run into any issues trying to run it", "sortedWord": "None", "removed": "Nan", "score": 8, "comments": 0, "media": null, "medialink": null, "identifyer": 59500391}, {"Unnamed: 0": 410, "autor": "VSCode Datree", "date": null, "content": "Inspiration\nDatree is a CLI tool which allows rectification of K8s configuration files with ease and zero dependency. While researching the possible use case for the hackathon I came across an idea about a VSCode extension that can show errors directly in the editor and to my surprise there was no existing solution to this!! , I always wanted to make a VSCode extension and now it was the perfect opportunity.\nWhat it does\nA VSCode extension that can run Datree tests and show errors and suggestions directly in the editor to assist developers writing error-free and up to the mark Kubernetes configurations to achieve optimum results\nHow we built it\nFirst, I started with parsing the CLI\u2019s output with it\u2019s JSON output format, faced some issues with the structure and contacted datree support for assistance, I came to know that there was already a issue which addressed this and Eyar Zilberman provided me with some ideas and a video that link that suggests such VSCode extension. I was very much pumped at this point and worked with full force to make it into a reality.\nVSCode has a very extensive API when it comes to developing extensions. I got the opportunity to learn and understand the logic that goes behind building such extensions. I worked with webviews, child processes, providers and typescript while building this extension and React.js for building the webviews.\nI periodically updated datree team with my progress and they were kind enough to solve my doubts and suggest improvements and features. Shimon Tolts suggested adding Helm support and Dima Brusilovsky suggested having an interface where the user can provide custom configuration for the tests, thanks to them I was able to complete the first stable release of the extension.\nWhile working on this project I interacted with many hackers, solved their doubts, got some of mine cleared, thanks to Community Classroom for making this happen.\nChallenges we ran into\nParsing data from Datree was challenging at first as Datree is a command line based tool, parsing the output requires creating a child process which runs the commands, we then listen for events and collect the Buffer data through the std streams, Datree allows passing the output flag which makes it easy to interpret the data as JSON. The challenge that came up was to render the errors, at the moment Datree\u2019s JSON output doesn\u2019t have a defined structure that made me add sanity checks over the JSON objects. As of now Datree\u2019s output lacks error context which makes it hard to show errors on specific line numbers, I built an algorithm that takes in Datree\u2019s output and YAML file content and tries to map the errors with the use of regex and label values. With upcoming versions of Datree it would surely be easy to implement this. VSCode has a comprehensive guide for building extensions but was daunting at first and required some trial and error to arrive at functioning results.\nAccomplishments that we're proud of\nThis open-source extension extends Datree\u2019s functionality by allowing users to use custom policies made with Datree and use them directly through VSCode.\nWhat we learned\nWhile understanding the Datree policy system I learned various misconfigurations that people can unknowingly write in their K8s config files. I learned how to parse data from the command line and incorporate it in our own applications. Before adding Helm support to the extension I read about the working of Helm Charts, how it functions and makes writing K8s configuration easy. This allowed me to write detection functions that can detect the Helm workspace and act accordingly. I learned about VSCode developer API and how we can contribute to open-sources by creating extensions that make developer lives easier.\nWhat's next for VSCode Datree\nWork with Datree team and keep the extension up-to-date with the CLI for best developer experience.", "link": "https://devpost.com/software/vscode-datree", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ndatree is a cli -----> tool !!!  which allows rectification of k8s configuration files with ease and zero dependency. while researching the possible use case for the hackathon i came across an idea about a vscode extension that can show errors directly in the editor and to my surprise there was no existing solution to this!! , i always wanted to make a vscode extension and now it was the perfect opportunity.\nwhat it does\na vscode extension that can run datree tests and show errors and suggestions directly in the editor to assist developers writing error-free and up to the mark kubernetes configurations to achieve optimum results\nhow we built it\nfirst, i started with parsing the cli\u2019s output with it\u2019s json output format, faced some issues with the structure and contacted datree support for assistance, i came to know that there was already a issue which addressed this and eyar zilberman provided me with some ideas and a video that link that suggests such vscode extension. i was very much pumped at this point and worked with full force to make it into a reality.\nvscode has a very extensive api when it comes to developing extensions. i got the opportunity to learn and understand the logic that goes behind building such extensions. i worked with webviews, child processes, providers and typescript while building this extension and react.js for building the webviews.\ni periodically updated datree team with my progress and they were kind enough to solve my doubts and suggest improvements and features. shimon tolts suggested adding helm support and dima brusilovsky suggested having an interface where the user can provide custom configuration for the tests, thanks to them i was able to complete the first stable release of the extension.\nwhile working on this project i interacted with many hackers, solved their doubts, got some of mine cleared, thanks to community classroom for making this happen.\nchallenges we ran into\nparsing data from datree was challenging at first as datree is a command line based tool, parsing the output requires creating a child process which runs the commands, we then listen for events and collect the buffer data through the std streams, datree allows passing the output flag which makes it easy to interpret the data as json. the challenge that came up was to render the errors, at the moment datree\u2019s json output doesn\u2019t have a defined structure that made me add sanity checks over the json objects. as of now datree\u2019s output lacks error context which makes it hard to show errors on specific line numbers, i built an algorithm that takes in datree\u2019s output and yaml file content and tries to map the errors with the use of regex and label values. with upcoming versions of datree it would surely be easy to implement this. vscode has a comprehensive guide for building extensions but was daunting at first and required some trial and error to arrive at functioning results.\naccomplishments that we're proud of\nthis open-source extension extends datree\u2019s functionality by allowing users to use custom policies made with datree and use them directly through vscode.\nwhat we learned\nwhile understanding the datree policy system i learned various misconfigurations that people can unknowingly write in their k8s config files. i learned how to parse data from the command line and incorporate it in our own applications. before adding helm support to the extension i read about the working of helm charts, how it functions and makes writing k8s configuration easy. this allowed me to write detection functions that can detect the helm workspace and act accordingly. i learned about vscode developer api and how we can contribute to open-sources by creating extensions that make developer lives easier.\nwhat's next for vscode datree\nwork with datree team and keep the extension up-to-date with the cli for best developer experience.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59500410}, {"Unnamed: 0": 412, "autor": "netra", "date": null, "content": "Inspirationfront End Development Tool is a product application that assists engineers with building alluring site designs and applications easily. Those apparatuses help to speed up the web advancement process by giving intuitive components and different implicit elements to make a more alluring website architecture format.\nThere are many front-end web advancement programming that helps you quicker your improvement work. Here is an organized rundown of top front-end advancement instruments with their famous highlights and site joins. The rundown contains both open source (free) and business (paid) programming.\nWhat it does\nHow I built it\nChallenges I ran into\nAccomplishments that I'm proud of\nWhat I learned\nWhat's next for netra", "link": "https://devpost.com/software/netra-q9ecbm", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspirationfront end development -----> tool !!!  is a product application that assists engineers with building alluring site designs and applications easily. those apparatuses help to speed up the web advancement process by giving intuitive components and different implicit elements to make a more alluring website architecture format.\nthere are many front-end web advancement programming that helps you quicker your improvement work. here is an organized rundown of top front-end advancement instruments with their famous highlights and site joins. the rundown contains both open source (free) and business (paid) programming.\nwhat it does\nhow i built it\nchallenges i ran into\naccomplishments that i'm proud of\nwhat i learned\nwhat's next for netra", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59500412}, {"Unnamed: 0": 458, "autor": "Music Mastermind", "date": null, "content": "Inspiration\nMusic Creators come up with amazing ideas, but some of them are already out there as songs. We don't want them to waste precious hours of their effort just to get demonetized for \"copying\" someone else's music.\nWhat it does\nWhich is why we made music mastermind. Users submit the chorus of their song, we go over a database of other choruses of other songs and see how much it matches based on the pitch pattern.\nHow we built it\nWe built it using a tool called librosa. It allows developers to get data about audio and we used it to compare pitches of different songs.\nChallenges we ran into\nObviously nothing comes easy, and we were learning how to work with this module with time not on our side.\nAccomplishments that we're proud of\nBut overall, we are proud of how we handled pressure under time, and created a rough model for an actual application in the music industry.\nWhat's next for Music Mastermind\nWe plan to use whole songs as comparison rather than choruses and incorporate a variety of factors into deciding how similar a song is to another. And we would obviously continue expanding our database of songs but we weren't able to get that many mp3 files because of time and budgetary restrictions", "link": "https://devpost.com/software/music-mastermind", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nmusic creators come up with amazing ideas, but some of them are already out there as songs. we don't want them to waste precious hours of their effort just to get demonetized for \"copying\" someone else's music.\nwhat it does\nwhich is why we made music mastermind. users submit the chorus of their song, we go over a database of other choruses of other songs and see how much it matches based on the pitch pattern.\nhow we built it\nwe built it using a -----> tool !!!  called librosa. it allows developers to get data about audio and we used it to compare pitches of different songs.\nchallenges we ran into\nobviously nothing comes easy, and we were learning how to work with this module with time not on our side.\naccomplishments that we're proud of\nbut overall, we are proud of how we handled pressure under time, and created a rough model for an actual application in the music industry.\nwhat's next for music mastermind\nwe plan to use whole songs as comparison rather than choruses and incorporate a variety of factors into deciding how similar a song is to another. and we would obviously continue expanding our database of songs but we weren't able to get that many mp3 files because of time and budgetary restrictions", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59500458}, {"Unnamed: 0": 476, "autor": "Shelm", "date": null, "content": "Inspiration\nFrom my experience as a SRE I have observed that when using helm charts developers and certain administrators often neglect the security best practices that are essential, negligence of security best practices can result in severe vulnerabilities which can be exploited by an attacker to either take control of your Kubernetes cluster or to destroy/damage your application data. And it is always good to enforce security best practices from the beginning. After all prevention is better than cure.\nWhat it does\nShelm is a command line utility written in python 3 which lets users download secure by design helm charts. All our helm charts are tested strictly, using Datree. The CLI tool is connected to a backend API written using flask framework of python which lets users to list, search and download our helm charts.\nInstallation curl https://raw.githubusercontent.com/viploveb/shelm/master/install.sh | /bin/bash\nUsage - \ud83d\udc49 shelm list This will list all the available shelm charts. \ud83d\udc49 shelm search Example - \"shelm search haproxy\" This will tell the user if the specified chart is available or not. \ud83d\udc49 shelm install This will download the specified chart.\nHow we built it\nI used python argparse module of python to create a CLI utility and the for backend API flask is used. Our API is deployed on AWS. Users can easily download it using one single command that runs an installation script which is written in bash.\nChallenges we ran into\nCreating the bash script and make it work successfully was quite a challenge for me. But in the end it worked \ud83d\ude4c Also yaml schema was a new thing for me so faced some trouble with but community came to the rescue \ud83d\udcaa\nAccomplishments that we're proud of\nI am proud that I am finally able to make this project a reality and complete it in such a short period of time single handedly. \ud83d\udd7a\nWhat we learned\nI always wanted to get into open source and understand its nuts and bolts but never got a chance like this before. This hackathon helped me learn a lot about how open source works! Learned a lot about Datree and how to create custom policies using yaml schema. Will definitely work more on it in future and contribute useful stuff. Also learnt how to secure helm charts and kubernetes security best practices.\nWhat's next for Shelm\nTo take shelm to the next level continous improvement is required. Create more secure helm charts and add them to our repo and improve our CLI and API with industry best practices and user experience.\nCustom Datree policy\nI have also created a policy which checks for security parameters. More about it can be read here", "link": "https://devpost.com/software/shelm-iauf71", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nfrom my experience as a sre i have observed that when using helm charts developers and certain administrators often neglect the security best practices that are essential, negligence of security best practices can result in severe vulnerabilities which can be exploited by an attacker to either take control of your kubernetes cluster or to destroy/damage your application data. and it is always good to enforce security best practices from the beginning. after all prevention is better than cure.\nwhat it does\nshelm is a command line utility written in python 3 which lets users download secure by design helm charts. all our helm charts are tested strictly, using datree. the cli -----> tool !!!  is connected to a backend api written using flask framework of python which lets users to list, search and download our helm charts.\ninstallation curl https://raw.githubusercontent.com/viploveb/shelm/master/install.sh | /bin/bash\nusage - \ud83d\udc49 shelm list this will list all the available shelm charts. \ud83d\udc49 shelm search example - \"shelm search haproxy\" this will tell the user if the specified chart is available or not. \ud83d\udc49 shelm install this will download the specified chart.\nhow we built it\ni used python argparse module of python to create a cli utility and the for backend api flask is used. our api is deployed on aws. users can easily download it using one single command that runs an installation script which is written in bash.\nchallenges we ran into\ncreating the bash script and make it work successfully was quite a challenge for me. but in the end it worked \ud83d\ude4c also yaml schema was a new thing for me so faced some trouble with but community came to the rescue \ud83d\udcaa\naccomplishments that we're proud of\ni am proud that i am finally able to make this project a reality and complete it in such a short period of time single handedly. \ud83d\udd7a\nwhat we learned\ni always wanted to get into open source and understand its nuts and bolts but never got a chance like this before. this hackathon helped me learn a lot about how open source works! learned a lot about datree and how to create custom policies using yaml schema. will definitely work more on it in future and contribute useful stuff. also learnt how to secure helm charts and kubernetes security best practices.\nwhat's next for shelm\nto take shelm to the next level continous improvement is required. create more secure helm charts and add them to our repo and improve our cli and api with industry best practices and user experience.\ncustom datree policy\ni have also created a policy which checks for security parameters. more about it can be read here", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59500476}, {"Unnamed: 0": 478, "autor": "Image Segmentation for Prostate MRI", "date": null, "content": "FinalProject2470\nRuofan Bie and Ruya Kang\nIntroduction\nIn this project, we intend to perform image segmentation with prostate Magnetic Resonance Imaging (MRI) data.\nProstate cancer is the second most frequent cancer diagnosis made in men and the fifth leading cause of death worldwide. [1] A few techniques are used for early detection of prostate cancer, including blood tests, biopsy and imaging tests. The Magnetic Resonance Imaging (MRI) scans create detailed images of soft tissues in the body using radio waves and strong magnets. MRI scans can give doctors a very clear picture of the prostate and nearby areas. [2]\nMRI of prostate cancer usually consists of two non-overlapping adjacent regions: the peripheral zone (PZ) and the transition zone (TZ). An example of prostate MRI with labelled zones is shown in Figure 1. Identifying prostate zones is important for diagnostic and therapies. However, the identification work requires substantial expertise in reading MRI scans. Therefore, automatic segmentation of prostate zones is instrumental for prostate lesion detection.\nThe problem of prostate zone segmentation is challenging because of the lack of a clear prostate boundary, prostate tissue heterogeneity, and the wide inter-individual variety of prostate shapes. [3] In this project, we will be implementing some existing CNN and RNN models for image segmentation using prostate MRI data. We will use a survey for image segmentation using deep learning [4] as a guide, implement selected models and compare their performance.\nRelated Work\nPrior Work\nThere are some works done for segmenting prostate MRI images using deep learning.\n[5] focus on the work done using fully convolutional neural networks (FCNN). They suggested eight different FCNNs-based deep 2D network structures for automatic MRI prostate segmentation by analysing various structures of shortcut connections together with the size of a deep network using the PROMISE12 dataset [6].\n[7] mentions that 3D neural networks have strong potential for prostate MRI segmentation. However, substantial computational resources are required due to the large number of trainable parameters. They proposed a network architecture called V-net Light (VnL), which is based on an efficient 3D Module called 3D Light, that minimises the number of network parameters while maintaining state-of-art segmentation results. The proposed architecture replaces regular 3D convolutions of the V-net architecture [8] with novel 3D Light modules. Figure 2 shows the architecture of VnL. The original V-net model consists of encoder and decoder paths with convolutional layers. To reduce the number of parameters, [7] inserts pooling layers between the encoder stages. The novel 3D-Light Module is used in each stage of the encoder and decoder. The 3D-Light Module is a parameter-efficient 3D convolutional block consisting of parallel convolution blocks, blocks composed of regular convolutions, followed by a group convolution. It reduces the number of parameters by 88%\u221292% in comparison to V-net. The VnL achieves comparable results to V-Net on the PROMISE12 dataset [6] while requiring 90% fewer learning parameters, 90% less hard-disk storage and just 3.3% of the FLOPs.\n[9] proposed a transfer learning method based on deep neural networks for prostate MRI segmentation. They also designed a multi-level edge attention module using wavelet decomposition to overcome the difficulty of ambiguous boundaries in the task.\nPublic Implementation\nSome of the model architectures we would like to implement have publicly available implementation.\nFCN-8s: https://github.com/MarvinTeichmann/tensorflow-fcn/blob/master/fcn8_vgg.py\nDeconvNet: https://github.com/HyeonwooNoh/DeconvNet/blob/master/model/DeconvNet/DeconvNet_inference_deploy.prototxt (Caffe)\nU-Net: https://github.com/milesial/Pytorch-UNet/tree/master/unet (Pytorch).\nData\nWe use a set of prostate MRI data from The Medical Segmentation Decathlon -- a biomedical image analysis challenge. The Decathlon challenge made ten data sets available online. All data sets have been released with a permissive copyright license (CC-BY-SA 4.0), thus allowing for data sharing, redistribution, and commercial usage. [10]\nAccording to [10], all images were de-identified and reformatted to the Neuroimaging Informatics Technology Initiative (NIfTI) format (https://nifti.nimh.nih.gov). All images were transposed (without resampling) to the most approximate right-anterior-superior coordinate frame, ensuring the data matrix x\u2212y\u2212z direction was consistent. Lastly, non-quantitative modalities (e.g., MRI) were robust min-max scaled to the same range. For each segmentation task, a pixel-level label annotation was provided.\nThe Decathlon challenge provides users with training sets (images and labels) and test sets (images without labels). To evaluate the performance with true labels, we only use the training set provided and randomly select a third of the data to be our own test set.\nThe prostate data set was acquired at Radboud University Medical Center, Nijmegen Medical Centre, Nijmegen, The Netherlands. It consists of 48 prostate multiparametric MRI (mpMRI) studies, 32 of them have corresponding region-of-interest (ROI) targets (background= 0, TZ= 1 and PZ= 2). Each study contains approximately 15 to 20 slices of MRI images, resulting in 602 images in total. Figure 3 shows the 20 slices from one study. The first and last few MRI slices contain little segmentation information. Therefore, to simplify the problem, we will discard the first and last 5 MRI slices from each study. We will use 10 studies (95 images) as the test set and the remaining 22 studies (187 images) as the training set.\nMethodology\nFully Convolutional Networks (FCNs)\nThe FCNs [11] is constructed by very deep convolutional layers with deconvolutional layers as decoders and 1x1 convolutional layer for pixel-wise prediction. (Figure 4). As shown in the structure, the output of max-pooling layer goes through a deconvolutional layer and then fuse with the previous max-pooling layer output to make a prediction. This technique is called as skip connection and can combine down-sampling features with up-sampling features for more accurate prediction. As shown in Figure 2, we implement the FCN-8s model, where there are 2 skip connections and the final convolutional layer uses strides 8 to recover the original image size.\nEncoder-Decoder Based Models\nMost of the popular DL-based segmentation models use some kind of encoder-decoder architecture. A basic encoder-decoder model to implement image segmentation is to use convolutional layers as encoders and then use deconvolutional or convolution-transpose layers for decoders. We will implement two of them: DeConvNet for general image segmentation and U-Net for medical image segmentation.\nDeConvNet\nThe DeConvNet [13] is designed on top of the convolutional layers adopted from the VGG 16-layer net [12]. As shown in Figure 5, DeConvNet is composed of convolution and deconvolution networks, where the convolution network acts as the feature extractor and the deconvolution network is a shape generator. The proposed architecture aims to overcome two limitations of FCNs. First, using FCN models, label prediction is done with only local information for large objects. Also, FCNs often ignore small objects and classify them as background. Second, in FCN, the input to the deconvolutional layer is too coarse and the deconvolution procedure is overly simple.\nWe simplify our DeConvNet model by reducing the number of filters per convolutional layer and incorporating less (de)convolutional blocks. There are no unpooling layers defined in TensorFlow. We make use of the implementation from https://github.com/aizawan/segnet/blob/master/ops.py.\nU-Net\nThe U-Net model [14] is built upon [11]. It is designed specifically for biomedical data, where there is very little training data available. Different from [11], a large number of feature channels is used in the upsampling part. The modification allows the network to propagate context information to higher resolution layers. The network does not have any fully connected layers and only uses the valid part of each convolution. We will implement the simplified version of U-Net model shown in Figure 6 with a three-channel output corresponding to the three segmentation areas. The model is simplified in terms of the number of filters per convolutional layer and the number of blocks.\nDilated Convolutional Models\nDue to the translation-invariant property of the convolutional layer, the FCN model is reliable in predicting the presence and roughly the position of objects in an image. However, as a trade-off between classification accuracy and localization accuracy, the FCN model might not be able to sketch the exact outline of the object. Instead of using standard convolutional layers, we consider using dilated convolutional layers [15] in the above FCN architecture. Unlike the standard convolutional layers, which apply filters on kernel-size blocks with adjacent pixels, the dilated convolutional layers apply filters on kernel-size blocks with pixels in distance l-1, where l is the dilated rate (Figure 7).\nSince we only have 187 images, we firstly use data augmentation to add rotated or flipped images into the original training set to increase the training sample size and also increase the robustness of our model. All four models are trained with 50 epochs through the whole training set with batch size 1. The trained models are then applied to the test set and compute the accuracy metrics describes below.\nMetrics\nThe model performance for image segmentation is measured differently from for classification. We will evaluate the model using a few new metrics. [4]\nPixel Accuracy\nPixel accuracy (PA) measures the proportion of correctly classified pixels. For K+1 classes, the pixel accuracy is defined as\n,\nwhere is the number of pixels of class predicted as belonging to class j.\nMean pixel accuracy (MPA) extends PA to the proportion of correctly predicted pixels in a per-class manner, and then average over the total number of classes.\nIntersection over Union\nPixel accuracy has limitations such that it has a bias in the presence of very imbalanced classes, while mean pixel accuracy is not suitable for data with a strong background class. Another segmentation evaluation metric is the intersection over union (IoU). It is defined as the area of intersection between the predicted segmentation map A and the ground truth map B, divided by the area of the union between the two maps:\n.\nThe mean intersection over union (Mean-IoU) is defined as the average IoU over all classes.\nIn this project, we would expect an accuracy of 50% for all models using the intersection over union metric as a baseline. Our goal is to implement 70-75% of accuracy for the four models. If these accuracies are easily achieved, we would consider adjusting the model to achieve around 90% accuracy.\nResults\nFor each mode, we perform four experiments with different data augmentation methods: (a) no augmentation at all; (b) randomly flip each image vertically or horizontally; (c) randomly rotate reach image by any angle; (d) both image flip and rotation.\nModel Pixel Accuracy (PA) Mean Pixel Accuracy (MPA) Intersection over Union (IoU)\nFCN (no aug) 0.962719 0.620694 0.459717\nDilated FCN (no aug) 0.974142 0.581018 0.481629\nDeConvNet (no aug) 0.974025 0.619032 0.509451\nU-Net (no aug) 0.976238 0.718744 0.597541\n--- --- --- ---\nFCN (flip) 0.962796 0.601290 0.451951\nDilated FCN (flip) 0.977338 0.606064 0.481629\nDeConvNet (flip) 0.979059 0.705706 0.515348\nU-Net (flip) 0.978152 0.741071 0.616579\n--- --- --- ---\nFCN (rotation) 0.969019 0.597827 0.468806\nDilated FCN (rotation) 0.976503 0.578570 0.504504\nDeConvNet (rotation) 0.977549 0.637209 0.554754\nU-Net (rotation) 0.979722 0.719528 0.613457\n--- --- --- ---\nFCN (both) 0.962796 0.601290 0.451951\nDilated FCN (both) 0.972596 0.644752 0.538985\nDeConvNet (both) 0.980162 0.698144 0.601530\nU-Net (both) 0.973854 0.740341 0.601107\nIt can be seen that for our task, where the class labels are highly unbalanced, the pixel accuracy is not representative. Therefore, we compare the performance using the mean pixel accuracy over classes and the IoU.\nFCN is outperformed by the two encoder-decoder models due to the thin deconvolutional layer as decoder and coarse output in the final deconvolutional layer. The dilated FCN managed to improve performance of FCN but is still outperformed by the encoder-decoder models. For the two encoder-decoder based models, U-Net outperforms DeConvNet in general. This is not surprising since U-Net is specifically designed for medical images and is supposed to achieve higher segmentation accuracy with fewer data. Moreover, randomly flipping input images seems to help improve performance for both of the models. Figure 8 and Figure 9 are the true (top) and predicted (bottom) segmentation of 10 test images from FCN and dilated FCN trained with data augmented by flipping and rotation. Figure 10 and Figure 11 are the true (top) and predicted (bottom) segmentation of 10 test images from DeConvNet and U-Net trained with data augmented by flipping and rotation. It can be seen that U-Net gives a smoother edge than FCN, dilated FCN and DeConvNet.\nEthics\nMagnetic resonance imaging (MRI) is a medical imaging technique that uses a magnetic field and computer-generated radio waves to create detailed images of the organs and tissues in a patients' body. It's also an important tool for doctors to detect any abnormalities of the tissue or organ. Developing an image segmentation neural network that can reach high accuracy of detecting prostate can help to relieve doctors' burden in manually checking the MRI and can increase efficiency in the medical system. However, developing such neural networks doesn't necessarily mean physicians never have to look at MRI. The neural network results would justify the physician's diagnosis to secure the diagnosis process.\nChallenges\nOne challenge of MRI segmentation is the imbalance between labels. Since the cross-entropy loss function is based on pixel-wise accuracy, it\u2019s easy for our models to produce all-background predictions. To solve this problem, we down-sampled all-background images in the training set and used weighted cross-entropy as loss function. Specifically, we assign weight ratio 1:3:3 to label 0, 1 and 2. After this adjustment, it becomes easier for the DeConvNet and U-Net to learn the position and shape of the prostate (IoU accuracies are over 60%). Note that down-sampling results in less training data, so augmentation is necessary for a better performace.\nAnother challenge is that the original FCN-8s model still failed to learn any feature and kept producing all-background prediction. After comparing the structure of FCN and U-Net, we realize the importance of the comparable depth of encoder and decoder in image segmentation. We also noticed that by using stride 8 in the last deconvolutional layer in the FCN, the output of FCN can be very coarse. After increasing the number of filters and reducing strides in the deconvolutional layers of FCN, we managed to improve the performance of FCN and reached 48.227% accuracy in IoU. Furthermore, we considered using dilated convolutional layers in the FCN model. The dilated convolutional layers apply filters on kernel-size blocks with pixels in l-1 distance, where l is the dilated rate. Since dilated convolutional network might be able to extract information from larger region, the dilated FCN managed to improve the performance (5% improvement in IoU for none augmentation, 14% improvement in IoU for flip augmnetation, 8% improvement in IoU for rotation augmentation and 12% improvement in IoU for both augmentation).\nThe third challenge is that the two encoder-decoder based models do not learn well in its originally proposed architecture. One possible reason is that the models may be too complicated for this task so they overfitted to the noise. Moreover, the original DeConvNet and U-Net model are too large for a 16GB GPU to train. To solve the problems, we simplified the encoder-decoder models by reducing the number of encoder and decoder blocks as well as reducing the number of filters per block.\nReflection\nIn this project, we managed to implement our basic goals and part of the target goals: implementing 4 image segmentation model and reach IoU and mean pixel accuracy over 50%. We planned to reproduce the proposed model architectures from the original papers. However, it turned out that simpler models of same structure performed well. Also, we performed more data pre-processing than proposed due to the high imbalance in segmentation labels. Using weighted loss helped training as well.\nDuring the project process, we found the performance of models are poorer than our expectation. There are two possible reasons for this: 1. The training set is relatively small. After down-sampling the all-background images, there are only 187 original images in the training set and 561 images after both flip and rotation augmentation. 2. Unlike ordinary figures where there are three color layers and the area of subjects and background is usually balanced, the MRIs only have one color layer and the position of the prostate is usually at the center of the image and the area of prostate is usually much smaller than the background. This increase the difficulty for neural networks to learn features.\nOur project shows that comparable depth of encoder and decoder is crucial to segmentation of such kind of images. Deep convolutional layers are used to extract features from the inputs while deconvolution, upsampling or unpooling layers should be incorporated to reconstruct the inputs moderately (in contrast to FCN, where only a coarse deconvolution layer is applied). Moreover, for many image segmentation tasks where the label classes are highly unbalanced, adjusting the model with appropriate class weights is also very effective. By comparing the FCN and dilated FCN model, we also found that dilated convolutional layers might be better at learning features in this problem.\nWe also noticed that the training of dilated FCN model can be unstable: the same code can be stuck with all-background predictions sometimes and can learn the features in other times. Our insight for this problem is that initialization can be important to neural network training and the all-background prediction might be a local minimization for the model to be easily stuck with. In our future work, we can explore the reason why dilated FCN failed to learn any feature occasionally and try to modify the structure of DeConvNet and U-Net, such as using dilated convolutional layers, to see whether the performance of the two models can be improved. Existing models with pretrained weights could also be involved in the comparison.\nDivision of labour\nRuofan is responsible for running the FCN and dilated convolutional models. Ruya is responsible for running the encoder-decoder models. The report and the poster are works of collaboration.\nReferences\n[1] Prashanth Rawla. Epidemiology of prostate cancer. World journal of oncology, 10(2):63, 2019. pages 3\n[2] American Cancer Society. Cancer Statistics Center. Tests to diagnose and stage prostate cancer. URL http://cancerstatisticscenter.cancer.org [Accessed8thNovember2021]. pages 3\n[3] Nader Aldoj, Federico Biavati, Florian Michallek, Sebastian Stober, and Marc Dewey. Automatic prostate and prostate zones segmentation of magnetic resonance images using densenet-like u-net. Scientific reports, 10(1):1\u201317, 2020. pages 3\n[4] Shervin Minaee, Yuri Y Boykov, Fatih Porikli, Antonio J Plaza, Nasser Kehtarnavaz, and Demetri Terzopoulos. Image segmentation using deep learning: A survey. IEEE Transactions on pattern analysis and Machine Intelligence, 2021. pages 3, 8\n[5] Tahereh Hassanzadeh, Leonard GC Hamey, and Kevin Ho-Shon. Convolutional neural networks for prostate magnetic resonance image segmentation. IEEE Access, 7:36748\u201336760, 2019. pages 3\n[6] Geert Litjens, Robert Toth, Wendy van de Ven, Caroline Hoeks, Sjoerd Kerkstra, Bram van Gin-neken, Graham Vincent, Gwenael Guillard, Neil Birbeck, Jindang Zhang, Robin Strand, FilipMalmberg, Yangming Ou, Christos Davatzikos, Matthias Kirschner, Florian Jung, Jing Yuan, Wu Qiu, Qinquan Gao, Philip aEddiea Edwards, Bianca Maan, Ferdinand van der Heijden, Soumya Ghose, Jhimli Mitra, Jason Dowling, Dean Barratt, Henkjan Huisman, and Anant Madabhushi. Evaluation of prostate segmentation algorithms for mri: The promise12 challenge. Medical Image Analysis, 18(2):359\u2013373, 2014. ISSN 1361-8415. doi: https://doi.org/10.1016/j.media.2013.12.002. URL https://www.sciencedirect.com/science/article/pii/S1361841513001734. pages 3, 4\n[7] Ophir Yaniv, Orith Portnoy, Amit Talmon, Nahum Kiryati, Eli Konen, and Arnaldo Mayer. V-netlight-parameter-efficient 3-d convolutional neural network for prostate mri segmentation. In 2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI), pages 442\u2013445. IEEE, 2020. pages 3\n[8] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In 2016 fourth international conference on 3D vision (3DV), pages 565\u2013571. IEEE, 2016. pages 3\n[9] Xiangxiang Qin. Transfer learning with edge attention for prostate mri segmentation. arXivpreprint arXiv:1912.09847, 2019. pages 4\n[10] Michela Antonelli, Annika Reinke, Spyridon Bakas, Keyvan Farahani, Bennett A Landman, GeertLitjens, Bjoern Menze, Olaf Ronneberger, Ronald M Summers, Bram van Ginneken, et al. The medical segmentation decathlon. arXiv preprint arXiv:2106.05735, 2021. pages 4\n[11] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3431\u20133440, 2015. pages 5\n[12] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. pages 6\n[13] Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han. Learning deconvolution network for semantic segmentation. In Proceedings of the IEEE international conference on computer vision, pages 1520\u20131528, 2015. pages 6\n[14] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234\u2013241. Springer, 2015. pages 6\n[15] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Semantic image segmentation with deep convolutional nets and fully connected crfs. arXivpreprint arXiv:1412.7062, 2014. pages 7", "link": "https://devpost.com/software/image-segmentation-for-prostate-mri-71cowt", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "finalproject2470\nruofan bie and ruya kang\nintroduction\nin this project, we intend to perform image segmentation with prostate magnetic resonance imaging (mri) data.\nprostate cancer is the second most frequent cancer diagnosis made in men and the fifth leading cause of death worldwide. [1] a few techniques are used for early detection of prostate cancer, including blood tests, biopsy and imaging tests. the magnetic resonance imaging (mri) scans create detailed images of soft tissues in the body using radio waves and strong magnets. mri scans can give doctors a very clear picture of the prostate and nearby areas. [2]\nmri of prostate cancer usually consists of two non-overlapping adjacent regions: the peripheral zone (pz) and the transition zone (tz). an example of prostate mri with labelled zones is shown in figure 1. identifying prostate zones is important for diagnostic and therapies. however, the identification work requires substantial expertise in reading mri scans. therefore, automatic segmentation of prostate zones is instrumental for prostate lesion detection.\nthe problem of prostate zone segmentation is challenging because of the lack of a clear prostate boundary, prostate tissue heterogeneity, and the wide inter-individual variety of prostate shapes. [3] in this project, we will be implementing some existing cnn and rnn models for image segmentation using prostate mri data. we will use a survey for image segmentation using deep learning [4] as a guide, implement selected models and compare their performance.\nrelated work\nprior work\nthere are some works done for segmenting prostate mri images using deep learning.\n[5] focus on the work done using fully convolutional neural networks (fcnn). they suggested eight different fcnns-based deep 2d network structures for automatic mri prostate segmentation by analysing various structures of shortcut connections together with the size of a deep network using the promise12 dataset [6].\n[7] mentions that 3d neural networks have strong potential for prostate mri segmentation. however, substantial computational resources are required due to the large number of trainable parameters. they proposed a network architecture called v-net light (vnl), which is based on an efficient 3d module called 3d light, that minimises the number of network parameters while maintaining state-of-art segmentation results. the proposed architecture replaces regular 3d convolutions of the v-net architecture [8] with novel 3d light modules. figure 2 shows the architecture of vnl. the original v-net model consists of encoder and decoder paths with convolutional layers. to reduce the number of parameters, [7] inserts pooling layers between the encoder stages. the novel 3d-light module is used in each stage of the encoder and decoder. the 3d-light module is a parameter-efficient 3d convolutional block consisting of parallel convolution blocks, blocks composed of regular convolutions, followed by a group convolution. it reduces the number of parameters by 88%\u221292% in comparison to v-net. the vnl achieves comparable results to v-net on the promise12 dataset [6] while requiring 90% fewer learning parameters, 90% less hard-disk storage and just 3.3% of the flops.\n[9] proposed a transfer learning method based on deep neural networks for prostate mri segmentation. they also designed a multi-level edge attention module using wavelet decomposition to overcome the difficulty of ambiguous boundaries in the task.\npublic implementation\nsome of the model architectures we would like to implement have publicly available implementation.\nfcn-8s: https://github.com/marvinteichmann/tensorflow-fcn/blob/master/fcn8_vgg.py\ndeconvnet: https://github.com/hyeonwoonoh/deconvnet/blob/master/model/deconvnet/deconvnet_inference_deploy.prototxt (caffe)\nu-net: https://github.com/milesial/pytorch-unet/tree/master/unet (pytorch).\ndata\nwe use a set of prostate mri data from the medical segmentation decathlon -- a biomedical image analysis challenge. the decathlon challenge made ten data sets available online. all data sets have been released with a permissive copyright license (cc-by-sa 4.0), thus allowing for data sharing, redistribution, and commercial usage. [10]\naccording to [10], all images were de-identified and reformatted to the neuroimaging informatics technology initiative (nifti) format (https://nifti.nimh.nih.gov). all images were transposed (without resampling) to the most approximate right-anterior-superior coordinate frame, ensuring the data matrix x\u2212y\u2212z direction was consistent. lastly, non-quantitative modalities (e.g., mri) were robust min-max scaled to the same range. for each segmentation task, a pixel-level label annotation was provided.\nthe decathlon challenge provides users with training sets (images and labels) and test sets (images without labels). to evaluate the performance with true labels, we only use the training set provided and randomly select a third of the data to be our own test set.\nthe prostate data set was acquired at radboud university medical center, nijmegen medical centre, nijmegen, the netherlands. it consists of 48 prostate multiparametric mri (mpmri) studies, 32 of them have corresponding region-of-interest (roi) targets (background= 0, tz= 1 and pz= 2). each study contains approximately 15 to 20 slices of mri images, resulting in 602 images in total. figure 3 shows the 20 slices from one study. the first and last few mri slices contain little segmentation information. therefore, to simplify the problem, we will discard the first and last 5 mri slices from each study. we will use 10 studies (95 images) as the test set and the remaining 22 studies (187 images) as the training set.\nmethodology\nfully convolutional networks (fcns)\nthe fcns [11] is constructed by very deep convolutional layers with deconvolutional layers as decoders and 1x1 convolutional layer for pixel-wise prediction. (figure 4). as shown in the structure, the output of max-pooling layer goes through a deconvolutional layer and then fuse with the previous max-pooling layer output to make a prediction. this technique is called as skip connection and can combine down-sampling features with up-sampling features for more accurate prediction. as shown in figure 2, we implement the fcn-8s model, where there are 2 skip connections and the final convolutional layer uses strides 8 to recover the original image size.\nencoder-decoder based models\nmost of the popular dl-based segmentation models use some kind of encoder-decoder architecture. a basic encoder-decoder model to implement image segmentation is to use convolutional layers as encoders and then use deconvolutional or convolution-transpose layers for decoders. we will implement two of them: deconvnet for general image segmentation and u-net for medical image segmentation.\ndeconvnet\nthe deconvnet [13] is designed on top of the convolutional layers adopted from the vgg 16-layer net [12]. as shown in figure 5, deconvnet is composed of convolution and deconvolution networks, where the convolution network acts as the feature extractor and the deconvolution network is a shape generator. the proposed architecture aims to overcome two limitations of fcns. first, using fcn models, label prediction is done with only local information for large objects. also, fcns often ignore small objects and classify them as background. second, in fcn, the input to the deconvolutional layer is too coarse and the deconvolution procedure is overly simple.\nwe simplify our deconvnet model by reducing the number of filters per convolutional layer and incorporating less (de)convolutional blocks. there are no unpooling layers defined in tensorflow. we make use of the implementation from https://github.com/aizawan/segnet/blob/master/ops.py.\nu-net\nthe u-net model [14] is built upon [11]. it is designed specifically for biomedical data, where there is very little training data available. different from [11], a large number of feature channels is used in the upsampling part. the modification allows the network to propagate context information to higher resolution layers. the network does not have any fully connected layers and only uses the valid part of each convolution. we will implement the simplified version of u-net model shown in figure 6 with a three-channel output corresponding to the three segmentation areas. the model is simplified in terms of the number of filters per convolutional layer and the number of blocks.\ndilated convolutional models\ndue to the translation-invariant property of the convolutional layer, the fcn model is reliable in predicting the presence and roughly the position of objects in an image. however, as a trade-off between classification accuracy and localization accuracy, the fcn model might not be able to sketch the exact outline of the object. instead of using standard convolutional layers, we consider using dilated convolutional layers [15] in the above fcn architecture. unlike the standard convolutional layers, which apply filters on kernel-size blocks with adjacent pixels, the dilated convolutional layers apply filters on kernel-size blocks with pixels in distance l-1, where l is the dilated rate (figure 7).\nsince we only have 187 images, we firstly use data augmentation to add rotated or flipped images into the original training set to increase the training sample size and also increase the robustness of our model. all four models are trained with 50 epochs through the whole training set with batch size 1. the trained models are then applied to the test set and compute the accuracy metrics describes below.\nmetrics\nthe model performance for image segmentation is measured differently from for classification. we will evaluate the model using a few new metrics. [4]\npixel accuracy\npixel accuracy (pa) measures the proportion of correctly classified pixels. for k+1 classes, the pixel accuracy is defined as\n,\nwhere is the number of pixels of class predicted as belonging to class j.\nmean pixel accuracy (mpa) extends pa to the proportion of correctly predicted pixels in a per-class manner, and then average over the total number of classes.\nintersection over union\npixel accuracy has limitations such that it has a bias in the presence of very imbalanced classes, while mean pixel accuracy is not suitable for data with a strong background class. another segmentation evaluation metric is the intersection over union (iou). it is defined as the area of intersection between the predicted segmentation map a and the ground truth map b, divided by the area of the union between the two maps:\n.\nthe mean intersection over union (mean-iou) is defined as the average iou over all classes.\nin this project, we would expect an accuracy of 50% for all models using the intersection over union metric as a baseline. our goal is to implement 70-75% of accuracy for the four models. if these accuracies are easily achieved, we would consider adjusting the model to achieve around 90% accuracy.\nresults\nfor each mode, we perform four experiments with different data augmentation methods: (a) no augmentation at all; (b) randomly flip each image vertically or horizontally; (c) randomly rotate reach image by any angle; (d) both image flip and rotation.\nmodel pixel accuracy (pa) mean pixel accuracy (mpa) intersection over union (iou)\nfcn (no aug) 0.962719 0.620694 0.459717\ndilated fcn (no aug) 0.974142 0.581018 0.481629\ndeconvnet (no aug) 0.974025 0.619032 0.509451\nu-net (no aug) 0.976238 0.718744 0.597541\n--- --- --- ---\nfcn (flip) 0.962796 0.601290 0.451951\ndilated fcn (flip) 0.977338 0.606064 0.481629\ndeconvnet (flip) 0.979059 0.705706 0.515348\nu-net (flip) 0.978152 0.741071 0.616579\n--- --- --- ---\nfcn (rotation) 0.969019 0.597827 0.468806\ndilated fcn (rotation) 0.976503 0.578570 0.504504\ndeconvnet (rotation) 0.977549 0.637209 0.554754\nu-net (rotation) 0.979722 0.719528 0.613457\n--- --- --- ---\nfcn (both) 0.962796 0.601290 0.451951\ndilated fcn (both) 0.972596 0.644752 0.538985\ndeconvnet (both) 0.980162 0.698144 0.601530\nu-net (both) 0.973854 0.740341 0.601107\nit can be seen that for our task, where the class labels are highly unbalanced, the pixel accuracy is not representative. therefore, we compare the performance using the mean pixel accuracy over classes and the iou.\nfcn is outperformed by the two encoder-decoder models due to the thin deconvolutional layer as decoder and coarse output in the final deconvolutional layer. the dilated fcn managed to improve performance of fcn but is still outperformed by the encoder-decoder models. for the two encoder-decoder based models, u-net outperforms deconvnet in general. this is not surprising since u-net is specifically designed for medical images and is supposed to achieve higher segmentation accuracy with fewer data. moreover, randomly flipping input images seems to help improve performance for both of the models. figure 8 and figure 9 are the true (top) and predicted (bottom) segmentation of 10 test images from fcn and dilated fcn trained with data augmented by flipping and rotation. figure 10 and figure 11 are the true (top) and predicted (bottom) segmentation of 10 test images from deconvnet and u-net trained with data augmented by flipping and rotation. it can be seen that u-net gives a smoother edge than fcn, dilated fcn and deconvnet.\nethics\nmagnetic resonance imaging (mri) is a medical imaging technique that uses a magnetic field and computer-generated radio waves to create detailed images of the organs and tissues in a patients' body. it's also an important -----> tool !!!  for doctors to detect any abnormalities of the tissue or organ. developing an image segmentation neural network that can reach high accuracy of detecting prostate can help to relieve doctors' burden in manually checking the mri and can increase efficiency in the medical system. however, developing such neural networks doesn't necessarily mean physicians never have to look at mri. the neural network results would justify the physician's diagnosis to secure the diagnosis process.\nchallenges\none challenge of mri segmentation is the imbalance between labels. since the cross-entropy loss function is based on pixel-wise accuracy, it\u2019s easy for our models to produce all-background predictions. to solve this problem, we down-sampled all-background images in the training set and used weighted cross-entropy as loss function. specifically, we assign weight ratio 1:3:3 to label 0, 1 and 2. after this adjustment, it becomes easier for the deconvnet and u-net to learn the position and shape of the prostate (iou accuracies are over 60%). note that down-sampling results in less training data, so augmentation is necessary for a better performace.\nanother challenge is that the original fcn-8s model still failed to learn any feature and kept producing all-background prediction. after comparing the structure of fcn and u-net, we realize the importance of the comparable depth of encoder and decoder in image segmentation. we also noticed that by using stride 8 in the last deconvolutional layer in the fcn, the output of fcn can be very coarse. after increasing the number of filters and reducing strides in the deconvolutional layers of fcn, we managed to improve the performance of fcn and reached 48.227% accuracy in iou. furthermore, we considered using dilated convolutional layers in the fcn model. the dilated convolutional layers apply filters on kernel-size blocks with pixels in l-1 distance, where l is the dilated rate. since dilated convolutional network might be able to extract information from larger region, the dilated fcn managed to improve the performance (5% improvement in iou for none augmentation, 14% improvement in iou for flip augmnetation, 8% improvement in iou for rotation augmentation and 12% improvement in iou for both augmentation).\nthe third challenge is that the two encoder-decoder based models do not learn well in its originally proposed architecture. one possible reason is that the models may be too complicated for this task so they overfitted to the noise. moreover, the original deconvnet and u-net model are too large for a 16gb gpu to train. to solve the problems, we simplified the encoder-decoder models by reducing the number of encoder and decoder blocks as well as reducing the number of filters per block.\nreflection\nin this project, we managed to implement our basic goals and part of the target goals: implementing 4 image segmentation model and reach iou and mean pixel accuracy over 50%. we planned to reproduce the proposed model architectures from the original papers. however, it turned out that simpler models of same structure performed well. also, we performed more data pre-processing than proposed due to the high imbalance in segmentation labels. using weighted loss helped training as well.\nduring the project process, we found the performance of models are poorer than our expectation. there are two possible reasons for this: 1. the training set is relatively small. after down-sampling the all-background images, there are only 187 original images in the training set and 561 images after both flip and rotation augmentation. 2. unlike ordinary figures where there are three color layers and the area of subjects and background is usually balanced, the mris only have one color layer and the position of the prostate is usually at the center of the image and the area of prostate is usually much smaller than the background. this increase the difficulty for neural networks to learn features.\nour project shows that comparable depth of encoder and decoder is crucial to segmentation of such kind of images. deep convolutional layers are used to extract features from the inputs while deconvolution, upsampling or unpooling layers should be incorporated to reconstruct the inputs moderately (in contrast to fcn, where only a coarse deconvolution layer is applied). moreover, for many image segmentation tasks where the label classes are highly unbalanced, adjusting the model with appropriate class weights is also very effective. by comparing the fcn and dilated fcn model, we also found that dilated convolutional layers might be better at learning features in this problem.\nwe also noticed that the training of dilated fcn model can be unstable: the same code can be stuck with all-background predictions sometimes and can learn the features in other times. our insight for this problem is that initialization can be important to neural network training and the all-background prediction might be a local minimization for the model to be easily stuck with. in our future work, we can explore the reason why dilated fcn failed to learn any feature occasionally and try to modify the structure of deconvnet and u-net, such as using dilated convolutional layers, to see whether the performance of the two models can be improved. existing models with pretrained weights could also be involved in the comparison.\ndivision of labour\nruofan is responsible for running the fcn and dilated convolutional models. ruya is responsible for running the encoder-decoder models. the report and the poster are works of collaboration.\nreferences\n[1] prashanth rawla. epidemiology of prostate cancer. world journal of oncology, 10(2):63, 2019. pages 3\n[2] american cancer society. cancer statistics center. tests to diagnose and stage prostate cancer. url http://cancerstatisticscenter.cancer.org [accessed8thnovember2021]. pages 3\n[3] nader aldoj, federico biavati, florian michallek, sebastian stober, and marc dewey. automatic prostate and prostate zones segmentation of magnetic resonance images using densenet-like u-net. scientific reports, 10(1):1\u201317, 2020. pages 3\n[4] shervin minaee, yuri y boykov, fatih porikli, antonio j plaza, nasser kehtarnavaz, and demetri terzopoulos. image segmentation using deep learning: a survey. ieee transactions on pattern analysis and machine intelligence, 2021. pages 3, 8\n[5] tahereh hassanzadeh, leonard gc hamey, and kevin ho-shon. convolutional neural networks for prostate magnetic resonance image segmentation. ieee access, 7:36748\u201336760, 2019. pages 3\n[6] geert litjens, robert toth, wendy van de ven, caroline hoeks, sjoerd kerkstra, bram van gin-neken, graham vincent, gwenael guillard, neil birbeck, jindang zhang, robin strand, filipmalmberg, yangming ou, christos davatzikos, matthias kirschner, florian jung, jing yuan, wu qiu, qinquan gao, philip aeddiea edwards, bianca maan, ferdinand van der heijden, soumya ghose, jhimli mitra, jason dowling, dean barratt, henkjan huisman, and anant madabhushi. evaluation of prostate segmentation algorithms for mri: the promise12 challenge. medical image analysis, 18(2):359\u2013373, 2014. issn 1361-8415. doi: https://doi.org/10.1016/j.media.2013.12.002. url https://www.sciencedirect.com/science/article/pii/s1361841513001734. pages 3, 4\n[7] ophir yaniv, orith portnoy, amit talmon, nahum kiryati, eli konen, and arnaldo mayer. v-netlight-parameter-efficient 3-d convolutional neural network for prostate mri segmentation. in 2020 ieee 17th international symposium on biomedical imaging (isbi), pages 442\u2013445. ieee, 2020. pages 3\n[8] fausto milletari, nassir navab, and seyed-ahmad ahmadi. v-net: fully convolutional neural networks for volumetric medical image segmentation. in 2016 fourth international conference on 3d vision (3dv), pages 565\u2013571. ieee, 2016. pages 3\n[9] xiangxiang qin. transfer learning with edge attention for prostate mri segmentation. arxivpreprint arxiv:1912.09847, 2019. pages 4\n[10] michela antonelli, annika reinke, spyridon bakas, keyvan farahani, bennett a landman, geertlitjens, bjoern menze, olaf ronneberger, ronald m summers, bram van ginneken, et al. the medical segmentation decathlon. arxiv preprint arxiv:2106.05735, 2021. pages 4\n[11] jonathan long, evan shelhamer, and trevor darrell. fully convolutional networks for semantic segmentation. in proceedings of the ieee conference on computer vision and pattern recognition, pages 3431\u20133440, 2015. pages 5\n[12] karen simonyan and andrew zisserman. very deep convolutional networks for large-scale image recognition. arxiv preprint arxiv:1409.1556, 2014. pages 6\n[13] hyeonwoo noh, seunghoon hong, and bohyung han. learning deconvolution network for semantic segmentation. in proceedings of the ieee international conference on computer vision, pages 1520\u20131528, 2015. pages 6\n[14] olaf ronneberger, philipp fischer, and thomas brox. u-net: convolutional networks for biomedical image segmentation. in international conference on medical image computing and computer-assisted intervention, pages 234\u2013241. springer, 2015. pages 6\n[15] liang-chieh chen, george papandreou, iasonas kokkinos, kevin murphy, and alan l yuille. semantic image segmentation with deep convolutional nets and fully connected crfs. arxivpreprint arxiv:1412.7062, 2014. pages 7", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 6, "media": null, "medialink": null, "identifyer": 59500478}, {"Unnamed: 0": 482, "autor": "FoodSy", "date": null, "content": "Inspiration\nUnited States Environmental Protection Agency estimated that in 2018, about 63 million tons of wasted food were generated in the United States. By donating food, we can feed people, support local communities, and prevent food from going to waste.\nThus, there is a need for a food bank that can gather donated food and distribute food to people in need. Further, the site should be the place for people who want to help others volunteer and do good deeds. This is the reason why we built FoodSy. A site that offers donated food to people in need and let others volunteer. We also have senior box program to deliver food to elders at their doors.\nWhat it does\nWelcome to FoodSy. First you need to log in to your account, putting in your email and password. You can go back to the homepage after signing in.\nWe have a mission statement is to distributes food and other needed essentials to people in need. We have services such as \"Donate funds or food for homeless people and the poor\", \"volunteer in group or individual and join multiple food charity events\", \"Food delivery program for senior elders\", and \"help for people in need\".\nWe have a scan food bar code service to help people scan food and donate food faster. People can contact us by text this phone number for immediate response.\nWe have a chatbot called Foodsy that was built by Google Cloud. Foodsy likes spaghetti and has a spaghetti cooking playlist that she can share in the chat. Foodsy can also guide people who want to donate food or volunteer.\nFor the backend, we use Linode to host our site.\nHow we built it\nBackend: Linode\nBackend: Auth0\nTwilio\nGoogle Cloud\nFrontend: HTML\nFrontend: CSS\nFrontend: JS\nPython\nUse of Linode\nWe used Linode for the backend of the page. After using Linode, we discovered that Linode has powerful resources for data storage and processing, Linode is easy to use. We want to explore more functions of Linode in our future projects and discover more benefits that Linode can bring to us.\nUse of Auth0\nAuth0 is a great and secure tool to protect users and the authorization. Auth0 is helpful and is a great tool that we like using.\nUse of Twilio\nWe use Twilio to set up the automated phone message which is good to assist anyone who is in need of our assistance and want to contact us.\nChallenges we ran into\nWe had challenges to work with auth0 for the first time and connected everything. At the end, we managed to finish our project which is great.\nAccomplishments that we're proud of\nWe were able to set up the authorization process through Auth0 and use Linode as backend.\nWhat we learned\nWe learnt to work with Auth0. We were able to connect the components.\nWhat's next for FoodSy~\nCollaborating with NGOs worldwide to reduce hunger and reach more people", "link": "https://devpost.com/software/food-charity-hip-hop", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nunited states environmental protection agency estimated that in 2018, about 63 million tons of wasted food were generated in the united states. by donating food, we can feed people, support local communities, and prevent food from going to waste.\nthus, there is a need for a food bank that can gather donated food and distribute food to people in need. further, the site should be the place for people who want to help others volunteer and do good deeds. this is the reason why we built foodsy. a site that offers donated food to people in need and let others volunteer. we also have senior box program to deliver food to elders at their doors.\nwhat it does\nwelcome to foodsy. first you need to log in to your account, putting in your email and password. you can go back to the homepage after signing in.\nwe have a mission statement is to distributes food and other needed essentials to people in need. we have services such as \"donate funds or food for homeless people and the poor\", \"volunteer in group or individual and join multiple food charity events\", \"food delivery program for senior elders\", and \"help for people in need\".\nwe have a scan food bar code service to help people scan food and donate food faster. people can contact us by text this phone number for immediate response.\nwe have a chatbot called foodsy that was built by google cloud. foodsy likes spaghetti and has a spaghetti cooking playlist that she can share in the chat. foodsy can also guide people who want to donate food or volunteer.\nfor the backend, we use linode to host our site.\nhow we built it\nbackend: linode\nbackend: auth0\ntwilio\ngoogle cloud\nfrontend: html\nfrontend: css\nfrontend: js\npython\nuse of linode\nwe used linode for the backend of the page. after using linode, we discovered that linode has powerful resources for data storage and processing, linode is easy to use. we want to explore more functions of linode in our future projects and discover more benefits that linode can bring to us.\nuse of auth0\nauth0 is a great and secure -----> tool !!!  to protect users and the authorization. auth0 is helpful and is a great tool that we like using.\nuse of twilio\nwe use twilio to set up the automated phone message which is good to assist anyone who is in need of our assistance and want to contact us.\nchallenges we ran into\nwe had challenges to work with auth0 for the first time and connected everything. at the end, we managed to finish our project which is great.\naccomplishments that we're proud of\nwe were able to set up the authorization process through auth0 and use linode as backend.\nwhat we learned\nwe learnt to work with auth0. we were able to connect the components.\nwhat's next for foodsy~\ncollaborating with ngos worldwide to reduce hunger and reach more people", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59500482}, {"Unnamed: 0": 487, "autor": "DevTool API", "date": null, "content": "Inspiration\nDevelopers go through a lot of problems and suffering for deployment Kubernetes manifest files into production and so we thought why no make an API service to make the work for the developers easier and also be able to get to know about the misconfigurations in the manifest files and be alerted to them via notification system.\nWhat it does\nUsing Kubernetes production best policies to run the k8s manifest files through Datree.io to prevent Kubernetes misconfigurations from ever reaching production.\nUsage of the project\nThis simplifies the job of the DevOps developers, who are working with misconfiguration and security of the k8s. And it can be used as a JSON datasource in Grafana. Inside the API we can populate the info of the datasources. There we can use the Datree CLI to analyze the k8s manifests and POST the report into the API to see the result in Grafana natively.\nHow we built it\nClusters are created using Kubernetes in Civo\nDevOps(Configurations, etc)\nFlask API integration\nPython runs Datree commands and POST request to API\nMetrics and logs of the configurations are checked using Grafana (Devtron)\nChallenges we ran into\nHad to get the fail.yaml file from the customized policies of the Datree using our API POST request argument.\nIntegration of the Kubernetes Python Client with the local k8s clusters.\nDocker Image Build issue regarding the predefined ports.\nAccomplishments that we're proud of\nBeing able to create custom policies to PR into the Datree.io.\nBuild the metrics into Devtron using Grafana.\nWhat we learned\nDevtron being a good devops tool which can boost the development process. Also it contains not only CI/CD stuff but also allow to understand the performance of deployed services and make changes in Kubernetes cluster using Devtron UI. Datree is a tool which can help in analyzing Kubernetes manifests, not only before applying it but also for existing manifests as well. In Datree, you can use custom policy which allow you to cover any possible use-cases. Civo helped us to spin-up the k8s clusters faster and had predictable prices and we were able to spin-up the main and all needed service which we need and make our development make it all centralized in development instances for all the members.\nWhat's next for DevTool API\nAlert the developers to the misconfigurations that have happened against the custom policy through Twilio alert.\nIncrease more options for Datree usage.\nMore monitoring system.\nSecurity system enhancement.\nWorkflow of the project", "link": "https://devpost.com/software/devtool-api", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ndevelopers go through a lot of problems and suffering for deployment kubernetes manifest files into production and so we thought why no make an api service to make the work for the developers easier and also be able to get to know about the misconfigurations in the manifest files and be alerted to them via notification system.\nwhat it does\nusing kubernetes production best policies to run the k8s manifest files through datree.io to prevent kubernetes misconfigurations from ever reaching production.\nusage of the project\nthis simplifies the job of the devops developers, who are working with misconfiguration and security of the k8s. and it can be used as a json datasource in grafana. inside the api we can populate the info of the datasources. there we can use the datree cli to analyze the k8s manifests and post the report into the api to see the result in grafana natively.\nhow we built it\nclusters are created using kubernetes in civo\ndevops(configurations, etc)\nflask api integration\npython runs datree commands and post request to api\nmetrics and logs of the configurations are checked using grafana (devtron)\nchallenges we ran into\nhad to get the fail.yaml file from the customized policies of the datree using our api post request argument.\nintegration of the kubernetes python client with the local k8s clusters.\ndocker image build issue regarding the predefined ports.\naccomplishments that we're proud of\nbeing able to create custom policies to pr into the datree.io.\nbuild the metrics into devtron using grafana.\nwhat we learned\ndevtron being a good devops -----> tool !!!  which can boost the development process. also it contains not only ci/cd stuff but also allow to understand the performance of deployed services and make changes in kubernetes cluster using devtron ui. datree is a tool which can help in analyzing kubernetes manifests, not only before applying it but also for existing manifests as well. in datree, you can use custom policy which allow you to cover any possible use-cases. civo helped us to spin-up the k8s clusters faster and had predictable prices and we were able to spin-up the main and all needed service which we need and make our development make it all centralized in development instances for all the members.\nwhat's next for devtool api\nalert the developers to the misconfigurations that have happened against the custom policy through twilio alert.\nincrease more options for datree usage.\nmore monitoring system.\nsecurity system enhancement.\nworkflow of the project", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59500487}, {"Unnamed: 0": 505, "autor": "LearnScape", "date": null, "content": "\ud83d\udcc8 LearnScape \ud83d\udcc8\n\ud83d\udca1 Inspiration -\nGoing through the CNCF landscape and getting started with contributions gets overwhelming. We wanted to simplify the existing landscape for beginners and added a roadmap too for folks who want to learn more in the field of DevOps.\nThis platform is developed such that it can guide new folks and learn concepts such as Docker and Kubernetes even if they contribute to this project because of the tools used in this project such as Docker, Datree, Alan AI, Firebase, getstream.io and Twilio.\n\u2699 What it does -\nSimplified CNCF LandScape where you can filter, search projects to contribute according to your tech stack\nGet your questions answered about DevOps by Alan AI\nGet your own personalised roadmap to reach your goal of DevOps\nTwilio will alert the project admins if there occurs any error in the server and server is about to shut down\nGet your brains working and join, network with other contributors of CNCF in the chat channel powered by getstream.io\n\ud83d\udd27 How we built it-\nDeveloped an API to get all the projects in the database and specific project from large project data. The project id is given by the database. This API will also give you number of project provided in limit field and you can increment the page no to get the next limit frame data along all names of the projects. Check out the documentation of API here\nThe fronted/client consumes this api in order to fetch the details of the several cncf projects and their information which can be needed by the beginner.\nWe used Alan AI to develop a DevOps FAQ and roadmap section which could guide beginners on how to get started. Check out the Alan assistant implemented here\nFor folks to get more involved and carry the conversations on the platform itself we used getstream.io to created a messaging channel where you could ask anything related to CNCF.\nWant to escape the chaos caused by server failure? Yes we can! by the use of Twilio. We used it to get notifications before a server goes down.\n//Twillio function here\nclient.messages\n.create({\nbody: 'App crashing!!',\nfrom: '+15017122661',\nto: process.env.DEST_NUMBER\n})\n.then(message => console.log(message.sid));\n}\nUsed Datree While starting the configuration it's essential to make sure that there is name specified for the deployment.Not having this might not raise errors by the native development kits (kubectl etc) but naming the deployment configurations while working with several clusters in a team is essential and cosidered a good practice. Check it here\nUsed Google Dev Library's Firebase to host the react app.\n\ud83d\udcaa Challenges we ran into -\nWe could not properly implement the API to frontend because of time constrictions and we tried to make use of each and every tool in a creative and unique way possible. This project has still lot of potential and improvements to make.\n\ud83d\udccc Accomplishments that we're proud of -\nAlthough we were not familiar with most of the tools and tech we used to build it, we tried and learned on the go to make it possible.\n\ud83d\udcda What we learned -\nWe learnt about react and how to make use of all the tools according to the use case of our project even if they were different and sometimes caused a lot of trouble.\n\u23ed What's next for -\nProper implementation of frontend to get all the projects from the API.", "link": "https://devpost.com/software/learnscape", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "\ud83d\udcc8 learnscape \ud83d\udcc8\n\ud83d\udca1 inspiration -\ngoing through the cncf landscape and getting started with contributions gets overwhelming. we wanted to simplify the existing landscape for beginners and added a roadmap too for folks who want to learn more in the field of devops.\nthis platform is developed such that it can guide new folks and learn concepts such as docker and kubernetes even if they contribute to this project because of the tools used in this project such as docker, datree, alan ai, firebase, getstream.io and twilio.\n\u2699 what it does -\nsimplified cncf landscape where you can filter, search projects to contribute according to your tech stack\nget your questions answered about devops by alan ai\nget your own personalised roadmap to reach your goal of devops\ntwilio will alert the project admins if there occurs any error in the server and server is about to shut down\nget your brains working and join, network with other contributors of cncf in the chat channel powered by getstream.io\n\ud83d\udd27 how we built it-\ndeveloped an api to get all the projects in the database and specific project from large project data. the project id is given by the database. this api will also give you number of project provided in limit field and you can increment the page no to get the next limit frame data along all names of the projects. check out the documentation of api here\nthe fronted/client consumes this api in order to fetch the details of the several cncf projects and their information which can be needed by the beginner.\nwe used alan ai to develop a devops faq and roadmap section which could guide beginners on how to get started. check out the alan assistant implemented here\nfor folks to get more involved and carry the conversations on the platform itself we used getstream.io to created a messaging channel where you could ask anything related to cncf.\nwant to escape the chaos caused by server failure? yes we can! by the use of twilio. we used it to get notifications before a server goes down.\n//twillio function here\nclient.messages\n.create({\nbody: 'app crashing!!',\nfrom: '+15017122661',\nto: process.env.dest_number\n})\n.then(message => console.log(message.sid));\n}\nused datree while starting the configuration it's essential to make sure that there is name specified for the deployment.not having this might not raise errors by the native development kits (kubectl etc) but naming the deployment configurations while working with several clusters in a team is essential and cosidered a good practice. check it here\nused google dev library's firebase to host the react app.\n\ud83d\udcaa challenges we ran into -\nwe could not properly implement the api to frontend because of time constrictions and we tried to make use of each and every -----> tool !!!  in a creative and unique way possible. this project has still lot of potential and improvements to make.\n\ud83d\udccc accomplishments that we're proud of -\nalthough we were not familiar with most of the tools and tech we used to build it, we tried and learned on the go to make it possible.\n\ud83d\udcda what we learned -\nwe learnt about react and how to make use of all the tools according to the use case of our project even if they were different and sometimes caused a lot of trouble.\n\u23ed what's next for -\nproper implementation of frontend to get all the projects from the api.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59500505}, {"Unnamed: 0": 506, "autor": "Best use of Datree in kubernetes configurations", "date": null, "content": "Inspiration\nAs a cloud-computing enthusiastic and learner, i'm very happy to participate in this hackathon . The tools and resources provided by the hackathon team are very impressive ,useful, professional and easy-to-learn content. As a learner this makes me inspired to do this hackathon\nWhat it does\nDatree CLI tool used in this project is user-friendly. To modify and validate kubernetes configuations mainfests it is very useful and moreover it has customized policy checks to validate yaml files using schema rules\nHow we built it\nI have used datree to create four custom policies to validate various kubernetes API configuration fields. Designed using VS code, yaml, json-schema and datree CLI tool. Small datree comands had played key role in this project because, just by using few commands we can validate the files\nChallenges we ran into\nNo one can learn in a single-shot. Due to enormous kubernetes API configurations, we need basic knowledge on API configurations . Eventhough having easy interface tool, it took some basic kubernetes, and yaml-schema knowledge to conquer this\nAccomplishments that we're proud of\nMany lessons takes place here . I have strarted from 0, but now i'm confident to accomplish this tool use cases accurately.\nWhat we learned\n-In the span of these few days i have learnt many things just because of this hackathon. This helps me to move forward to learn many things regarding cloud-native\nWhat's next for Best use of Datree in kubernetes configurations\nMany Kubernetes admins and developers are using Datree to configure their files. By increasing dificulty in policy rules and compressing small scale rules into single policy will benifit. It is a open source tool so not only a industrial purpose but also students, learners, anyone can easily use this tool", "link": "https://devpost.com/software/best-use-of-datree-in-kubernetes-configurations", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nas a cloud-computing enthusiastic and learner, i'm very happy to participate in this hackathon . the tools and resources provided by the hackathon team are very impressive ,useful, professional and easy-to-learn content. as a learner this makes me inspired to do this hackathon\nwhat it does\ndatree cli -----> tool !!!  used in this project is user-friendly. to modify and validate kubernetes configuations mainfests it is very useful and moreover it has customized policy checks to validate yaml files using schema rules\nhow we built it\ni have used datree to create four custom policies to validate various kubernetes api configuration fields. designed using vs code, yaml, json-schema and datree cli tool. small datree comands had played key role in this project because, just by using few commands we can validate the files\nchallenges we ran into\nno one can learn in a single-shot. due to enormous kubernetes api configurations, we need basic knowledge on api configurations . eventhough having easy interface tool, it took some basic kubernetes, and yaml-schema knowledge to conquer this\naccomplishments that we're proud of\nmany lessons takes place here . i have strarted from 0, but now i'm confident to accomplish this tool use cases accurately.\nwhat we learned\n-in the span of these few days i have learnt many things just because of this hackathon. this helps me to move forward to learn many things regarding cloud-native\nwhat's next for best use of datree in kubernetes configurations\nmany kubernetes admins and developers are using datree to configure their files. by increasing dificulty in policy rules and compressing small scale rules into single policy will benifit. it is a open source tool so not only a industrial purpose but also students, learners, anyone can easily use this tool", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59500506}, {"Unnamed: 0": 515, "autor": "E-Medico: AI for health care", "date": null, "content": "About the project\nA large number of people across our country, due to improper medical check up and lack of timely diagnosis are suffering from incurable medical conditions. People rely on doctors that charge a huge amount of money as consultancy fees and in a few places there are lower number of specialist doctors that could provide people with proper consultancy and treatment. While it is cheaper to go on Google to self-diagnose rather than to visit a doctor, this often leads to inaccurate diagnosis and can be extremely dangerous as they might follow a wrong treatment plan or may not realize the severity of their condition.\nThat is where E-Medico comes in, it can help people get a diagnosis of multiple diseases within a matter of seconds, thus helping with early detection of life threatening diseases. As we have seen during the pandemic there is a huge shortage of doctors in the industry. The healthcare industry is ripe for some major changes. From quick consultation to risk prediction, there are endless possibilities to leverage technology to deploy more precise and efficient care for patients. AI and machine learning can make the work of physicians, nurses and the other healthcare workers easier and save the lives of patients.\nE-Medico addresses the opportunity of using AI and machine learning in risk prediction and consultation of diseases for improving efficiency. E-Medico functions as a virtual AI medical assistant. It is deployed as a web application using azure web apps( app service) leveraging the server less computing technology it provides. It uses azure health bot to rapidly and systematically process patient symptoms and help them identify the correct diagnosis. It also uses machine learning models to predict if a patient has high or low risk of getting a particular disease as ML models can quickly identify diseases that are hard to diagnose. This will help the patient to get a quicker and a better treatment. Using AI in healthcare to predict risk can help the physicians detect diseases in their early stages before they become life threatening. Such a project can tackle several problems in the existing healthcare system and can enhance healthcare delivery and improve efficiency by physicians, nurse practitioners and healthcare workers.\nAn added bonus: Patients and doctors can use E-Medico as an extra tool to get an instantaneous second opinion and avoid any false negative/positive results, further reducing the load of the healthcare system, making this web application socially sustainable. It is also a safe and low-carbon health system, protecting the environment.\nWhat's next for E-Medico: AI for health care\nMy aim is to create an application that could one day completely automate the consultation process in healthcare and will be able to detect any disease so that patients can get early treatment which could save their lives.", "link": "https://devpost.com/software/e-medico-ai-for-health-care-g09p6r", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "about the project\na large number of people across our country, due to improper medical check up and lack of timely diagnosis are suffering from incurable medical conditions. people rely on doctors that charge a huge amount of money as consultancy fees and in a few places there are lower number of specialist doctors that could provide people with proper consultancy and treatment. while it is cheaper to go on google to self-diagnose rather than to visit a doctor, this often leads to inaccurate diagnosis and can be extremely dangerous as they might follow a wrong treatment plan or may not realize the severity of their condition.\nthat is where e-medico comes in, it can help people get a diagnosis of multiple diseases within a matter of seconds, thus helping with early detection of life threatening diseases. as we have seen during the pandemic there is a huge shortage of doctors in the industry. the healthcare industry is ripe for some major changes. from quick consultation to risk prediction, there are endless possibilities to leverage technology to deploy more precise and efficient care for patients. ai and machine learning can make the work of physicians, nurses and the other healthcare workers easier and save the lives of patients.\ne-medico addresses the opportunity of using ai and machine learning in risk prediction and consultation of diseases for improving efficiency. e-medico functions as a virtual ai medical assistant. it is deployed as a web application using azure web apps( app service) leveraging the server less computing technology it provides. it uses azure health bot to rapidly and systematically process patient symptoms and help them identify the correct diagnosis. it also uses machine learning models to predict if a patient has high or low risk of getting a particular disease as ml models can quickly identify diseases that are hard to diagnose. this will help the patient to get a quicker and a better treatment. using ai in healthcare to predict risk can help the physicians detect diseases in their early stages before they become life threatening. such a project can tackle several problems in the existing healthcare system and can enhance healthcare delivery and improve efficiency by physicians, nurse practitioners and healthcare workers.\nan added bonus: patients and doctors can use e-medico as an extra -----> tool !!!  to get an instantaneous second opinion and avoid any false negative/positive results, further reducing the load of the healthcare system, making this web application socially sustainable. it is also a safe and low-carbon health system, protecting the environment.\nwhat's next for e-medico: ai for health care\nmy aim is to create an application that could one day completely automate the consultation process in healthcare and will be able to detect any disease so that patients can get early treatment which could save their lives.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59500515}, {"Unnamed: 0": 520, "autor": "E-Medico: AI for health care", "date": null, "content": "About the Project\nA large number of people across our country, due to improper medical check up and lack of timely diagnosis are suffering from incurable medical conditions. People rely on doctors that charge a huge amount of money as consultancy fees and in a few places there are lower number of specialist doctors that could provide people with proper consultancy and treatment. While it is cheaper to go on Google to self-diagnose rather than to visit a doctor, this often leads to inaccurate diagnosis and can be extremely dangerous as they might follow a wrong treatment plan or may not realize the severity of their condition.\nThat is where E-Medico comes in, it can help people get a diagnosis of multiple diseases within a matter of seconds, thus helping with early detection of life threatening diseases. As we have seen during the pandemic there is a huge shortage of doctors in the industry. The healthcare industry is ripe for some major changes. From quick consultation to risk prediction, there are endless possibilities to leverage technology to deploy more precise and efficient care for patients. AI and machine learning can make the work of physicians, nurses and the other healthcare workers easier and save the lives of patients.\nE-Medico addresses the opportunity of using AI and machine learning in risk prediction and consultation of diseases for improving efficiency. E-Medico functions as a virtual AI medical assistant. It is deployed as a web application using azure web apps( app service) leveraging the server less computing technology it provides. It uses azure health bot to rapidly and systematically process patient symptoms and help them identify the correct diagnosis. It also uses machine learning models to predict if a patient has high or low risk of getting a particular disease as ML models can quickly identify diseases that are hard to diagnose. This will help the patient to get a quicker and a better treatment. Using AI in healthcare to predict risk can help the physicians detect diseases in their early stages before they become life threatening. Such a project can tackle several problems in the existing healthcare system and can enhance healthcare delivery and improve efficiency by physicians, nurse practitioners and healthcare workers.\nAn added bonus: Patients and doctors can use E-Medico as an extra tool to get an instantaneous second opinion and avoid any false negative/positive results, further reducing the load of the healthcare system, making this web application socially sustainable. It is also a safe and low-carbon health system, protecting the environment.\nWhat's next for E-Medico: AI for health care\nMy aim is to create an application that could one day completely automate the consultation process in healthcare and will be able to detect any disease so that patients can get early treatment which could save their lives.", "link": "https://devpost.com/software/e-medico-ai-for-health-care", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "about the project\na large number of people across our country, due to improper medical check up and lack of timely diagnosis are suffering from incurable medical conditions. people rely on doctors that charge a huge amount of money as consultancy fees and in a few places there are lower number of specialist doctors that could provide people with proper consultancy and treatment. while it is cheaper to go on google to self-diagnose rather than to visit a doctor, this often leads to inaccurate diagnosis and can be extremely dangerous as they might follow a wrong treatment plan or may not realize the severity of their condition.\nthat is where e-medico comes in, it can help people get a diagnosis of multiple diseases within a matter of seconds, thus helping with early detection of life threatening diseases. as we have seen during the pandemic there is a huge shortage of doctors in the industry. the healthcare industry is ripe for some major changes. from quick consultation to risk prediction, there are endless possibilities to leverage technology to deploy more precise and efficient care for patients. ai and machine learning can make the work of physicians, nurses and the other healthcare workers easier and save the lives of patients.\ne-medico addresses the opportunity of using ai and machine learning in risk prediction and consultation of diseases for improving efficiency. e-medico functions as a virtual ai medical assistant. it is deployed as a web application using azure web apps( app service) leveraging the server less computing technology it provides. it uses azure health bot to rapidly and systematically process patient symptoms and help them identify the correct diagnosis. it also uses machine learning models to predict if a patient has high or low risk of getting a particular disease as ml models can quickly identify diseases that are hard to diagnose. this will help the patient to get a quicker and a better treatment. using ai in healthcare to predict risk can help the physicians detect diseases in their early stages before they become life threatening. such a project can tackle several problems in the existing healthcare system and can enhance healthcare delivery and improve efficiency by physicians, nurse practitioners and healthcare workers.\nan added bonus: patients and doctors can use e-medico as an extra -----> tool !!!  to get an instantaneous second opinion and avoid any false negative/positive results, further reducing the load of the healthcare system, making this web application socially sustainable. it is also a safe and low-carbon health system, protecting the environment.\nwhat's next for e-medico: ai for health care\nmy aim is to create an application that could one day completely automate the consultation process in healthcare and will be able to detect any disease so that patients can get early treatment which could save their lives.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59500520}, {"Unnamed: 0": 578, "autor": "Resume", "date": null, "content": "Inspiration\nA lot industry leading visualization tools are used for creating CV. This shows how flexible the tool is and how well versed the user is. I want to create the same type of CV in the Quickbase's new dashboards, to highlight how flexible the new dashbaord setup is.\nWhat it does\nIt creates a resume, just by entering your details in few tables.\nHow we built it\nI have multiple tables for different informations storage. The information is reflected in a charts and graphs, which is organised in the dashboard.\nChallenges we ran into\nSo use the area chart and the quarterly chart. I had to mold the data so that the chart will reflect the expected result.\nAccomplishments that we're proud of\nAn Curriculum Vitae which the user can use. Just share the Quickbase application or take a screen grab. To take the Resume. It is keep the Curriculum Vitae updated, just a new row of data to add experience or any other information.\nWhat we learned\nI learnt to setup the dashboard.\nWhat's next for Resume\nTo create printable Curriculum Vitae from Quickbase, save it as a record in quickbase and send it to any Recruiter from quickbase via Pipelines", "link": "https://devpost.com/software/resume-nvzu7o", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\na lot industry leading visualization tools are used for creating cv. this shows how flexible the -----> tool !!!  is and how well versed the user is. i want to create the same type of cv in the quickbase's new dashboards, to highlight how flexible the new dashbaord setup is.\nwhat it does\nit creates a resume, just by entering your details in few tables.\nhow we built it\ni have multiple tables for different informations storage. the information is reflected in a charts and graphs, which is organised in the dashboard.\nchallenges we ran into\nso use the area chart and the quarterly chart. i had to mold the data so that the chart will reflect the expected result.\naccomplishments that we're proud of\nan curriculum vitae which the user can use. just share the quickbase application or take a screen grab. to take the resume. it is keep the curriculum vitae updated, just a new row of data to add experience or any other information.\nwhat we learned\ni learnt to setup the dashboard.\nwhat's next for resume\nto create printable curriculum vitae from quickbase, save it as a record in quickbase and send it to any recruiter from quickbase via pipelines", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59500578}, {"Unnamed: 0": 654, "autor": "dokodemo", "date": null, "content": "LOOM VIDEO DEMO: https://www.loom.com/share/fa654b27f6f745ab933559d2c0a6dd2c\nInspiration\n\"dokodemo\" means \"anywhere\" in Japanese. No matter the team size, this app aims to help groups spread across anywhere in any timezone come together for demo days and interact asynchronously.\nThe company I work for is entirely remote with team members located around the world. The most effective way we bond and have serendipitous encounters is through our bi-weekly demo days on Zoom. Anyone can volunteer for a slot to \"show and tell\" an accomplishment they made individually or with a team, whether it be a serious win or a fun hacky project. It's how we get to know people and projects in other teams and find cross-functional opportunities.\nPain point\nThe only problem with our Zoom demo days is that they are split into two regions to accommodate opposite time zones. While the demo days are recorded, it's a hassle to download a chunky hour long video from the cloud and skim through them all - ergo half of the company doesn't know the other half very well.\nWhat it does\nThis app is a not-so-real-time watering hole for groups to show allows demo day organizers to create \"Spaces\" where company or team members can upload their demos. Spaces can be shared with the group so everyone can get up to speed on their own time while still getting to connect with one another with kudos, jokes and questions right inside the demos.\nLoom is the PERFECT tool for asynchronous video demos that involve screensharing. Viewers can react in-video with emojis, comments and video replies, even if they don't have a Loom account.\nIdeally the organizer would be able to see which members showed up and track company engagement... didn't quite finish that part in time :)\nHow I built it\nSlowly.\nRedwoodJS (Typescript/React/GraphQL/Prisma)\nNetlify frontend/functions hosting\nRailway for Postgres DB\nChallenges I ran into\nInitially I set out to make a realtime virtual expo space in Phaser where you can view Loom demos while walking around and chatting with other users, to make people really feel connected. I learned a lot about Phaser and building tilemaps... but decided realtime chat wasn't much value-add for asynchronous teams in wildly different timezones.\nI pivoted to an app that would allow organizers of demo days to have engagement insights for their asynchronous teams. I spent too long learning a new framework and didn't implement all the features I had in mind.\nAlso, I wanted to use the Loom record SDK to keep all the activity in the app, but the current limitation of only being able to see the face bubble in the current window wasn't ideal for doing demos (the face is important!). That's OK, instead I redirect users to Loom so they can make a real account and start uploading :P\nAccomplishments that we're proud of\nThis is the deepest I've ever ventured into building and deploying a full-stack app with RedwoodJS. I shared some of the learnings I had along the way (ex.). I'm super proud to have gotten database auth working with password resets and emails, even if the feature I had in mind for it wasn't implemented in time.\nWhat we learned\nSpend less time in the weeds with your curiosities and maybe you'll actually finish a meaningful app MVP :)\nWhat's next for dokodemo\nI fell short of all the features I wanted. I'd like to add these:\nPrivate spaces by org (SSO) or groups\nMark watched demos for authed user so they know which are left to see\nAnalytics view for space admin (How many employees attended a given demo day; Engagement trends over time;)\nSchedule a cut off time for submissions\nView archived demo days in your org\nUsers can add tags to demos for searching later\nExplore public demo days\nAdd a cute logo, of a buffalo or something", "link": "https://devpost.com/software/dokodemo-qap71u", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "loom video demo: https://www.loom.com/share/fa654b27f6f745ab933559d2c0a6dd2c\ninspiration\n\"dokodemo\" means \"anywhere\" in japanese. no matter the team size, this app aims to help groups spread across anywhere in any timezone come together for demo days and interact asynchronously.\nthe company i work for is entirely remote with team members located around the world. the most effective way we bond and have serendipitous encounters is through our bi-weekly demo days on zoom. anyone can volunteer for a slot to \"show and tell\" an accomplishment they made individually or with a team, whether it be a serious win or a fun hacky project. it's how we get to know people and projects in other teams and find cross-functional opportunities.\npain point\nthe only problem with our zoom demo days is that they are split into two regions to accommodate opposite time zones. while the demo days are recorded, it's a hassle to download a chunky hour long video from the cloud and skim through them all - ergo half of the company doesn't know the other half very well.\nwhat it does\nthis app is a not-so-real-time watering hole for groups to show allows demo day organizers to create \"spaces\" where company or team members can upload their demos. spaces can be shared with the group so everyone can get up to speed on their own time while still getting to connect with one another with kudos, jokes and questions right inside the demos.\nloom is the perfect -----> tool !!!  for asynchronous video demos that involve screensharing. viewers can react in-video with emojis, comments and video replies, even if they don't have a loom account.\nideally the organizer would be able to see which members showed up and track company engagement... didn't quite finish that part in time :)\nhow i built it\nslowly.\nredwoodjs (typescript/react/graphql/prisma)\nnetlify frontend/functions hosting\nrailway for postgres db\nchallenges i ran into\ninitially i set out to make a realtime virtual expo space in phaser where you can view loom demos while walking around and chatting with other users, to make people really feel connected. i learned a lot about phaser and building tilemaps... but decided realtime chat wasn't much value-add for asynchronous teams in wildly different timezones.\ni pivoted to an app that would allow organizers of demo days to have engagement insights for their asynchronous teams. i spent too long learning a new framework and didn't implement all the features i had in mind.\nalso, i wanted to use the loom record sdk to keep all the activity in the app, but the current limitation of only being able to see the face bubble in the current window wasn't ideal for doing demos (the face is important!). that's ok, instead i redirect users to loom so they can make a real account and start uploading :p\naccomplishments that we're proud of\nthis is the deepest i've ever ventured into building and deploying a full-stack app with redwoodjs. i shared some of the learnings i had along the way (ex.). i'm super proud to have gotten database auth working with password resets and emails, even if the feature i had in mind for it wasn't implemented in time.\nwhat we learned\nspend less time in the weeds with your curiosities and maybe you'll actually finish a meaningful app mvp :)\nwhat's next for dokodemo\ni fell short of all the features i wanted. i'd like to add these:\nprivate spaces by org (sso) or groups\nmark watched demos for authed user so they know which are left to see\nanalytics view for space admin (how many employees attended a given demo day; engagement trends over time;)\nschedule a cut off time for submissions\nview archived demo days in your org\nusers can add tags to demos for searching later\nexplore public demo days\nadd a cute logo, of a buffalo or something", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59500654}, {"Unnamed: 0": 689, "autor": "Pulse Project", "date": null, "content": "Pulse\nComplete analysis of audio conversations.Best tool for analysis of customer support calls for insights,sentiments, ranking, prioritizing accordingly.Taking customer service to the next level with Pulse.\nLive URL : https://pulse-data-app.netlify.app/ For testing please upload audio files less than 1 min in wav format.Currently our server is not able to handle large files.(Please make sure your audio file is in wav format and is less than 1 min in duration)\nFeatures\nUpload Audio conversations\nGet topics, key words and ideas related to the conversation\nGet sentiment analysis of the conversation\nGet Summary of the conversation\nApplications\nCustomer care can use this data to prioritize customers\nImprove customer service\nGain insights\nEasy analysis\nHow we built it\nBackend - Node.js,express.js\nFrontend - React.js\nCloud - Heroku and Netlify\nModzy AI Models\nList of Modzy AI Models used\nText Summarization (https://app.modzy.com/models/rs2qqwbjwb/0.0.2)\nSentiment Analysis (https://app.modzy.com/models/ed542963de/1.0.1)\nText topic modelling (https://app.modzy.com/models/m8z2mwe3pt/0.0.1)\nWorkflow\nTech Stack\nHow to use it\nUse it on the Live website\nGo to https://pulse-data-app.netlify.app/\nSignup/Login and upload a sample audio conversation to get insights and more.(Sample audio -- Link)\nupload the audio on our website and wait to get the results. Voila that's it.\nTo run it locally\nDownload the source code from the GitHub link provided\nIn the root folder npm start to run the backend server\nIn the client folder npm start to run the frontend.\nNow you can view the Application running on localhost:3000.\nYou will need to provide your own MongoDB and Modzy API keys and credentials to make it run locally\nNext steps for Pulse\nProcess large audio files\nimprove efficiency and accuracy\nCreated by\nAbhay R Patel Rishav Raj Jain", "link": "https://devpost.com/software/pulse-project", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "pulse\ncomplete analysis of audio conversations.best -----> tool !!!  for analysis of customer support calls for insights,sentiments, ranking, prioritizing accordingly.taking customer service to the next level with pulse.\nlive url : https://pulse-data-app.netlify.app/ for testing please upload audio files less than 1 min in wav format.currently our server is not able to handle large files.(please make sure your audio file is in wav format and is less than 1 min in duration)\nfeatures\nupload audio conversations\nget topics, key words and ideas related to the conversation\nget sentiment analysis of the conversation\nget summary of the conversation\napplications\ncustomer care can use this data to prioritize customers\nimprove customer service\ngain insights\neasy analysis\nhow we built it\nbackend - node.js,express.js\nfrontend - react.js\ncloud - heroku and netlify\nmodzy ai models\nlist of modzy ai models used\ntext summarization (https://app.modzy.com/models/rs2qqwbjwb/0.0.2)\nsentiment analysis (https://app.modzy.com/models/ed542963de/1.0.1)\ntext topic modelling (https://app.modzy.com/models/m8z2mwe3pt/0.0.1)\nworkflow\ntech stack\nhow to use it\nuse it on the live website\ngo to https://pulse-data-app.netlify.app/\nsignup/login and upload a sample audio conversation to get insights and more.(sample audio -- link)\nupload the audio on our website and wait to get the results. voila that's it.\nto run it locally\ndownload the source code from the github link provided\nin the root folder npm start to run the backend server\nin the client folder npm start to run the frontend.\nnow you can view the application running on localhost:3000.\nyou will need to provide your own mongodb and modzy api keys and credentials to make it run locally\nnext steps for pulse\nprocess large audio files\nimprove efficiency and accuracy\ncreated by\nabhay r patel rishav raj jain", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59500689}, {"Unnamed: 0": 695, "autor": "IPFS Navigator", "date": null, "content": "Inspiration\nIn the world of Web 3.0, the use of decentralised storage is pushed and IPFS is used instead of http/https. However, by doing so, the user's file also becomes a string of Qm codes, which is not easy to remember.\nWhat it does\nWith this tool, users can enter the Qm code into the address bar to get a preview of the file, which is convenient for users and also effective in preventing the spread of virus files.\nHow we built it\nWe first built a large number of IPFS clusters to use to speed up the retrieval of the platform across the IPFS network. Secondly we have integrated smart contracts into the platform that can bind the user's data to the retrieved content.\nChallenges we ran into\nWhen deploying the IPFS cluster again we had problems with instability and slow retrieval, as well as sometimes problems getting tokens from the tap. We needed tokens to deploy the contract.\nAccomplishments that we're proud of\nWe finally solved the problem of unstable IPFS clusters.\nWhat we learned\nWe learned how to deploy smart contracts.\nWhat's next for IPFS Navigator\nNext, we will continue to optimize the whole experience of using the platform, especially the smart contract part.", "link": "https://devpost.com/software/ipfs-navigator", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nin the world of web 3.0, the use of decentralised storage is pushed and ipfs is used instead of http/https. however, by doing so, the user's file also becomes a string of qm codes, which is not easy to remember.\nwhat it does\nwith this -----> tool !!! , users can enter the qm code into the address bar to get a preview of the file, which is convenient for users and also effective in preventing the spread of virus files.\nhow we built it\nwe first built a large number of ipfs clusters to use to speed up the retrieval of the platform across the ipfs network. secondly we have integrated smart contracts into the platform that can bind the user's data to the retrieved content.\nchallenges we ran into\nwhen deploying the ipfs cluster again we had problems with instability and slow retrieval, as well as sometimes problems getting tokens from the tap. we needed tokens to deploy the contract.\naccomplishments that we're proud of\nwe finally solved the problem of unstable ipfs clusters.\nwhat we learned\nwe learned how to deploy smart contracts.\nwhat's next for ipfs navigator\nnext, we will continue to optimize the whole experience of using the platform, especially the smart contract part.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59500695}, {"Unnamed: 0": 700, "autor": "Pi6 Finance", "date": null, "content": "Inspiration\nThe Ethereum ecosystem has dashboards like Zapper, Zerion etc to manage all things related to DeFi. But the Cosmos ecosystem lacks any such tool. There has been a demand for a specific tool to keep track of all the airdrops, to get notified about up and coming proposals. Hence we wanted to bridge the gap by providing such tool to the community. We also want to make it such that newcomers won't feel it as a barricade for their entry to the Cosmos ecosystem.\nWhat it does\nThe user will be able to manage their portfolio, check up and coming airdrops, check proposals and be notified whenever a new proposal is up. To do all these, the user just has to enter his wallet address.\nHow we built it\nWe used React for the web frontend and Flutter for Mobile frontend. To store the airdrops details, we have used Firebase Firestore and to get blockchain specific data, we used APIs provided by certain validators.\nWhat's next for Pi6 Finance\nThis is just an MVP, we are planning to further improve it so that it can be used by everyone. We are also planning to include the ability of connecting with Keplr extension and also the ability of creating wallets and transacting through the app itself.", "link": "https://devpost.com/software/pi6-finance", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nthe ethereum ecosystem has dashboards like zapper, zerion etc to manage all things related to defi. but the cosmos ecosystem lacks any such -----> tool !!! . there has been a demand for a specific tool to keep track of all the airdrops, to get notified about up and coming proposals. hence we wanted to bridge the gap by providing such tool to the community. we also want to make it such that newcomers won't feel it as a barricade for their entry to the cosmos ecosystem.\nwhat it does\nthe user will be able to manage their portfolio, check up and coming airdrops, check proposals and be notified whenever a new proposal is up. to do all these, the user just has to enter his wallet address.\nhow we built it\nwe used react for the web frontend and flutter for mobile frontend. to store the airdrops details, we have used firebase firestore and to get blockchain specific data, we used apis provided by certain validators.\nwhat's next for pi6 finance\nthis is just an mvp, we are planning to further improve it so that it can be used by everyone. we are also planning to include the ability of connecting with keplr extension and also the ability of creating wallets and transacting through the app itself.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59500700}, {"Unnamed: 0": 713, "autor": "News Without Ads", "date": null, "content": "Inspiration\nAs many people went online this year, they needed a proper way to get the news. Ads filled news websites with many controversies were what they were left with. Me, being a developer, decided to fix that issue, creating an amazing tool to get the latest news.\nWhat It Does\nThis cool tool (rhyming) fetches the latest news and returns it. It shows the user the article title, image, description, and link.\nHow I Built It\nI used a news API, and returned information to the user.\nChallenges I Ran Into\nSetting this whole thing up was a big hassle. I had to navigate the server code and all the complicated things. I finally finished the project just in time.\nWhat I Learned\nI learned a lot of things in this project. The styling of the webpage and the content was difficult to create. In general, I learned MANY things form this project!\nWhat's next for News Without Ads\nI have already completed next steps. This is part of a bigger website to help everyone. The news website is linked below, here is the main website https://random-yet-useful-website.epiccodewizard.repl.co/. I have the goal of bringing the tools and other things people need to succeed during this tough time. This project is HUGE, and will be a momentous step in the right direction.", "link": "https://devpost.com/software/news-without-ads", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nas many people went online this year, they needed a proper way to get the news. ads filled news websites with many controversies were what they were left with. me, being a developer, decided to fix that issue, creating an amazing -----> tool !!!  to get the latest news.\nwhat it does\nthis cool tool (rhyming) fetches the latest news and returns it. it shows the user the article title, image, description, and link.\nhow i built it\ni used a news api, and returned information to the user.\nchallenges i ran into\nsetting this whole thing up was a big hassle. i had to navigate the server code and all the complicated things. i finally finished the project just in time.\nwhat i learned\ni learned a lot of things in this project. the styling of the webpage and the content was difficult to create. in general, i learned many things form this project!\nwhat's next for news without ads\ni have already completed next steps. this is part of a bigger website to help everyone. the news website is linked below, here is the main website https://random-yet-useful-website.epiccodewizard.repl.co/. i have the goal of bringing the tools and other things people need to succeed during this tough time. this project is huge, and will be a momentous step in the right direction.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59500713}, {"Unnamed: 0": 728, "autor": "CG Quiz", "date": null, "content": "Inspiration\nWant to take a Hard challege for Fully Functional app Which Defines and Promote Our Culture *_\u091b\u0924\u094d\u0924\u0940\u0917\u0921\u093c\u093f\u092f\u093e \u0938\u092c \u0932\u0947\u092c\u0922\u093c\u093f\u092f\u093e _ *\nWhat it does\nSimply asks a question with 4 Options Correct Answer gives a point and wrong have no minus marking\nHow we built it\nWith Some 3rd Party tool from Anywheres, JAVA, CSS, VB\nChallenges we ran into\nSince its Fully functional It took intence hardwork and dedication and also other team members are not very good in programing but they helped in any way possible. Tons and tons of errors but we have an infinite amout of dedication\nAccomplishments that we're proud of\nApp is almost Errorless with the fact was made in intense preassure\nBuild from Total Scratch without using any templete\nSince It's an Native Android app but not an Web App It was mutch harder to design and code\nWhat we learned\nNothing is impossible if you have a true vison to see the possible path to our goal. Also Mentioning the fact App can store user data Flawlessly like Name and Last Score", "link": "https://devpost.com/software/cg-quiz", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwant to take a hard challege for fully functional app which defines and promote our culture *_\u091b\u0924\u094d\u0924\u0940\u0917\u0921\u093c\u093f\u092f\u093e \u0938\u092c \u0932\u0947\u092c\u0922\u093c\u093f\u092f\u093e _ *\nwhat it does\nsimply asks a question with 4 options correct answer gives a point and wrong have no minus marking\nhow we built it\nwith some 3rd party -----> tool !!!  from anywheres, java, css, vb\nchallenges we ran into\nsince its fully functional it took intence hardwork and dedication and also other team members are not very good in programing but they helped in any way possible. tons and tons of errors but we have an infinite amout of dedication\naccomplishments that we're proud of\napp is almost errorless with the fact was made in intense preassure\nbuild from total scratch without using any templete\nsince it's an native android app but not an web app it was mutch harder to design and code\nwhat we learned\nnothing is impossible if you have a true vison to see the possible path to our goal. also mentioning the fact app can store user data flawlessly like name and last score", "sortedWord": "None", "removed": "Nan", "score": 5, "comments": 0, "media": null, "medialink": null, "identifyer": 59500728}, {"Unnamed: 0": 751, "autor": "matiz.io", "date": null, "content": "Inspiration\nWe strongly believe in the emotional power of music and the effect that has on our lives. Precisely this inspires us to translate a hering centric music world into a visual one.\nWhat it does\nmatiz.io, is a tool that enables interactive user experiences by Latin Artists to make music visible for everyone.\nHow we built it\nWe use javascript to connect to the WAX Blockchain to be able to get the blocks of information that can be found in the user WAX wallet. Once we get the San Blu NFT collection from the user matiz.io kicks off the interactive NFT by blending music with art. We use the p5js library to get the sound frequency of the song and pair it with the visuals that change in the NFT.\nChallenges we ran into\nThe biggest challenge has been the learning curve to get familiar with sound frequency and how we can blend it with visual art that is understood not only by the hearing music fanatic but also by the hearing impaired.\nAccomplishments that we're proud of\nWe have built a strong relationship with the artist Sad Blu. We are looking forward to collaborating and releasing NFT drops that people will love to be part of.\nWhat we learned\nWe got more immersed in the NFT world and its capabilities and validated our mission to enable user experiences through music and art.\nWhat's next for matiz\nmatiz will be partnering with more Latin music and visual art artists to enable more unique experiences for NFT collectors. These experiences range from live events, physical goods, virtual reality, and metaverse.", "link": "https://devpost.com/software/matiz", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe strongly believe in the emotional power of music and the effect that has on our lives. precisely this inspires us to translate a hering centric music world into a visual one.\nwhat it does\nmatiz.io, is a -----> tool !!!  that enables interactive user experiences by latin artists to make music visible for everyone.\nhow we built it\nwe use javascript to connect to the wax blockchain to be able to get the blocks of information that can be found in the user wax wallet. once we get the san blu nft collection from the user matiz.io kicks off the interactive nft by blending music with art. we use the p5js library to get the sound frequency of the song and pair it with the visuals that change in the nft.\nchallenges we ran into\nthe biggest challenge has been the learning curve to get familiar with sound frequency and how we can blend it with visual art that is understood not only by the hearing music fanatic but also by the hearing impaired.\naccomplishments that we're proud of\nwe have built a strong relationship with the artist sad blu. we are looking forward to collaborating and releasing nft drops that people will love to be part of.\nwhat we learned\nwe got more immersed in the nft world and its capabilities and validated our mission to enable user experiences through music and art.\nwhat's next for matiz\nmatiz will be partnering with more latin music and visual art artists to enable more unique experiences for nft collectors. these experiences range from live events, physical goods, virtual reality, and metaverse.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59500751}, {"Unnamed: 0": 764, "autor": "Rexchange", "date": null, "content": "What is Rexchange?\nRexchange is a website for people to ask questions related to research papers.\nIt's interface is similar to the currently existing stackexchange. What makes Rexchange different is that people are able to ask questions related to particular research papers.\nInspiration and why it is useful\nImagine you're reading a research paper related to Machine Learning. There is one particular concept in the paper that you don't understand. If you want to ask the community for an answer, you would have to ask it on a website like stackexchange (of that particular subject) or a website like researchgate.\nSo what's the issue in that?\nThere is low chance that someone who has actually read that particular paper will come across your question. Even though websites like stackexchange classify questions based on topics, even then one topics will have so many questions, not necessarily related to the paper you're looking for.\nCurrently there is no platform where you can ask questions aimed at people who have read a particular research paper.\nThis is where Rexchange comes in\nOn Rexchange, you can ask questions related to a particular research paper. Now, a person who has read that particular paper can just come to the relevant paper's section on the website and can see all the questions asked for that particular paper.\nStackexchange vs Rexchange\nRexchange is not meant to compete with stackexchange as a platform for asking questions. The two platforms differ in motivation.\nStackexchange classifies questions by topics and welcomes anyone to answer the questions as long as they have knowledge about that topic.\nRexchange however, is meant only for those who have read a certain paper to ask each other doubts.\nWho does this benefit?\nThis platform is meant to benefit both parties, ie, the one asking the doubt and the one who is going to answer it.\nThe person who asks the question\nPeople who have doubts related to a research article would post their questions here. They are benefitted because there is a much higher chance that people who have actually read the paper answer their questions.\nThis also eliminates the need to give entire context and background while asking a question since it is assumed that the person answering will have read the paper already.\nThe person answering\nUsually after reading a paper, even though you think you have understood everything, there are nitty-gritty details that need to be thought about. These details can often be pointed out by quality questions/doubts from the community.\nHence, after reading a paper, Rexchange would be a go-to platform for a reader to look at other people's doubts. He/she might come across a doubt that heightens their understanding of the topic.\nAdditionally, this is the time when he/she would naturally answer other people's doubts.\nHow it works\nEvery research paper is idenified by a string known as DOI. This DOI string can be found easily for those reading a paper.\nOn Rexchange, you simple enter the DOI and you will be taken to the questions section.\nHere, you can view the questions asked by others and their answers. You can also ask a question yourself.\nIf you know the answer to a question, you can answer it.\nAn authentication system has also been implemented to keep track of who asked/answered questions.\nRelated Questions\nThe website also gives the option of showing questions related to the paper they are currently on. This is based on the references (citations) of the current paper.\nUsually references come under the same topic as the original paper. Hence, showing such related questions can benefit readers.\nHow we built it\nRexchange was built completely in python using a tool called Anvil.\nWhat's next for Rexchange\nDue to the duration of the hackathon, only the features mentioned previously have been implemented.\nHowever, there is much more scope for the platform to grow in terms of features. They include:\nFeatures that Stackexchange has\nStackexchange has almost become a standard for asking and answering questions. The following features that it has are a must for Rexchange:\nImage support in questions and answers\nAllow editing of answers/questions\nLatex support in questions/answers\nEnabling voting for answers and questions\nFlagging/reporting inappropriate questions/answers\nAdding tags to questions\nA ranking system for users who post quality answers\nMajor Potential\nComing to features other that stackexchange's features that I will try to implement in Rexchange in the future\nA Notification System\nUser can sign up for notification (like emails) whenever someone answers their question or add a new question on a particular paper.\nPublic Notes\nA section for creating notes in latex/markdown for each paper.\nThe user can write/upload his/her notes for a paper. He/She will be given the choice to make it public so the community can benefit from it.\nAuthor's Explanations\nUsually research papers are written very consisely. Hence, often there is a lack of sufficient explanation in them.\nRexchange will confirm the author's identity and provide him priviledged accounts.\nThis account will enable him/her to provide special explanations on the papers written by him/her on Rexchange. This will benefit the understanding of the readers and can provide authors a central platform to post such easy-to-understand explanations.", "link": "https://devpost.com/software/rexchange", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what is rexchange?\nrexchange is a website for people to ask questions related to research papers.\nit's interface is similar to the currently existing stackexchange. what makes rexchange different is that people are able to ask questions related to particular research papers.\ninspiration and why it is useful\nimagine you're reading a research paper related to machine learning. there is one particular concept in the paper that you don't understand. if you want to ask the community for an answer, you would have to ask it on a website like stackexchange (of that particular subject) or a website like researchgate.\nso what's the issue in that?\nthere is low chance that someone who has actually read that particular paper will come across your question. even though websites like stackexchange classify questions based on topics, even then one topics will have so many questions, not necessarily related to the paper you're looking for.\ncurrently there is no platform where you can ask questions aimed at people who have read a particular research paper.\nthis is where rexchange comes in\non rexchange, you can ask questions related to a particular research paper. now, a person who has read that particular paper can just come to the relevant paper's section on the website and can see all the questions asked for that particular paper.\nstackexchange vs rexchange\nrexchange is not meant to compete with stackexchange as a platform for asking questions. the two platforms differ in motivation.\nstackexchange classifies questions by topics and welcomes anyone to answer the questions as long as they have knowledge about that topic.\nrexchange however, is meant only for those who have read a certain paper to ask each other doubts.\nwho does this benefit?\nthis platform is meant to benefit both parties, ie, the one asking the doubt and the one who is going to answer it.\nthe person who asks the question\npeople who have doubts related to a research article would post their questions here. they are benefitted because there is a much higher chance that people who have actually read the paper answer their questions.\nthis also eliminates the need to give entire context and background while asking a question since it is assumed that the person answering will have read the paper already.\nthe person answering\nusually after reading a paper, even though you think you have understood everything, there are nitty-gritty details that need to be thought about. these details can often be pointed out by quality questions/doubts from the community.\nhence, after reading a paper, rexchange would be a go-to platform for a reader to look at other people's doubts. he/she might come across a doubt that heightens their understanding of the topic.\nadditionally, this is the time when he/she would naturally answer other people's doubts.\nhow it works\nevery research paper is idenified by a string known as doi. this doi string can be found easily for those reading a paper.\non rexchange, you simple enter the doi and you will be taken to the questions section.\nhere, you can view the questions asked by others and their answers. you can also ask a question yourself.\nif you know the answer to a question, you can answer it.\nan authentication system has also been implemented to keep track of who asked/answered questions.\nrelated questions\nthe website also gives the option of showing questions related to the paper they are currently on. this is based on the references (citations) of the current paper.\nusually references come under the same topic as the original paper. hence, showing such related questions can benefit readers.\nhow we built it\nrexchange was built completely in python using a -----> tool !!!  called anvil.\nwhat's next for rexchange\ndue to the duration of the hackathon, only the features mentioned previously have been implemented.\nhowever, there is much more scope for the platform to grow in terms of features. they include:\nfeatures that stackexchange has\nstackexchange has almost become a standard for asking and answering questions. the following features that it has are a must for rexchange:\nimage support in questions and answers\nallow editing of answers/questions\nlatex support in questions/answers\nenabling voting for answers and questions\nflagging/reporting inappropriate questions/answers\nadding tags to questions\na ranking system for users who post quality answers\nmajor potential\ncoming to features other that stackexchange's features that i will try to implement in rexchange in the future\na notification system\nuser can sign up for notification (like emails) whenever someone answers their question or add a new question on a particular paper.\npublic notes\na section for creating notes in latex/markdown for each paper.\nthe user can write/upload his/her notes for a paper. he/she will be given the choice to make it public so the community can benefit from it.\nauthor's explanations\nusually research papers are written very consisely. hence, often there is a lack of sufficient explanation in them.\nrexchange will confirm the author's identity and provide him priviledged accounts.\nthis account will enable him/her to provide special explanations on the papers written by him/her on rexchange. this will benefit the understanding of the readers and can provide authors a central platform to post such easy-to-understand explanations.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 1, "media": null, "medialink": null, "identifyer": 59500764}, {"Unnamed: 0": 811, "autor": "Unisight", "date": null, "content": "Inspiration \ud83d\udca1\nE-Learning has become the new normal in this Pandemic era. Besides, for students, video conferencing tools have become the part & parcel of daily activities. But in general, this leads to feeble face-to-face traditional interaction between lecturers and students degrading the interest in online education extensively in the last few months, globally! We aim to innovate a revolutionary tool that helps teachers and students improve the online learning experience for the better.\nWe believe that with the power of AI, this can be solved if proceeded creatively. Thus we made UniSight! \u2728\nSo what\u2019s the app about? \ud83e\udd14\nUnisight captures the realtime expressions of the student throughout the duration of the class which will help the person on the other side of the screen understand their flow turning online learning into a fun, interesting & productive experience. We plan to use different technologies & cutting edge AI methods to extract the data & generate instant analytics.\nSay goodbye to boring lectures, switch to Unisight! \ud83d\ude80\nTech Stack \ud83c\udfd7\nFirst and foremost, it is Crafted with \ud83d\udc99. For the front-end, we\u2019ve used React.js & Tailwind as CSS framework. The Authentication (OAuth) has been done by Firebase & we\u2019re also using the Cloudstore database for storing user logs.\nTo create this, we trained a custom sentiment analysis model which is used to analyze & predict the user emotions directly from the input video. The model used Tensorflow.js under the hood to process the data directly on the client-side & hence no data is being sent to the server which also preserves privacy.\nFor heartbeat detection, we have taken the advantage of Mayer waves \u2014 oscillations of arterial pressure that occur in conscious subjects. Using these, we determine your heart rate by monitoring the tiny fluctuations in the color of the forehead. This is done by taking the average pixel values of the forehead region and performing a Fourier Transform to convert this signal to a sum of frequencies, the most prominent of which will correspond to the user's heart rate.\nThe net data is then averaged and passed into a chart library where all of these calculations are made in real-time to populate the average analytics infront of us.\nChallenges We ran into \ud83e\uddf1\nThere were a lot of challenges on our way. First, because we are all online and spread around the globe, it was somewhat difficult for us to be communicating during the process. We also spent a great deal of time discussing ideas for the project. As we forwarded, we had to fix critical bugs & obviously, compiling chunks of modules into one was the most difficult part!\nDesign \ud83c\udfa8\nWe were heavily inspired by the revised version of Iterative design process, which not only includes visual design, but a full-fledged research cycle in which you must discover and define your problem before tackling your solution & then finally deploy it.\nDiscover: a deep dive into the problem we are trying to solve.\nDefine: synthesizing the information from the discovery phase into a problem definition.\nDevelop: think up solutions to the problem.\nDeliver: pick the best solution and build that.\nThis time went for the minimalist Material UI design. We utilized design tools like Figma, Photoshop & Illustrator to prototype our designs before doing any coding. Through this, we are able to get iterative feedback so that we spend less time re-writing code.\nResearch \ud83d\udcda\nResearch is the key to empathizing with users: we found our specific user group early and that paves the way for our whole project. Here are a few of the resources that were helpful to us \u2014\nA multi-task approach using deep learning (ArXiv, Oct 2021) \u2014 https://arxiv.org/pdf/2110.15028.pdf\nReal Time Heart Rate Monitoring from Facial RGB Color Video Using Webcam \u2014 bit.ly/3pq9syt\nTFjs docs : https://js.tensorflow.org/api/latest\nDistractions via new Technologies ft. students \u2014 bit.ly/3DjOtlO\n84% of teachers facing challenges during online classes \u2014 bit.ly/3pq9W7L\nCREDITS\nDesign Resources : Freepik\nIcons : Icons8, fontawesome\nFont : Roboto / Raleway / RF Dewi / Muller\nTakeaways\nAccomplishments that we're proud of \ud83d\ude4c\nA fully working prototype! This has been intense yet insightful. We are proud of the final product we designed and engineered within such a short timeframe of 24hrs. The fluidity of the user's experience is something that we take pride in as we strive to return a sense of serenity to the oftentimes stressful and difficult process of studying and learning. This could not have been achieved without synergizing modular components to enhance the learning process.\nWhat we learned \ud83d\ude4c\nStaying hydrated was our motto for completing this impactful and complicated project on time. We have learned how great wins are accomplished by working together. Apart from the above, together we had a lot of fun participating in KHE'21 & these are the few things we learned \u2014\nLearned about Design Centric approaches & Figma Prototyping\nExplored different kinds of open-sourced chart libraries\nImproved Time management skills\nOptimizing Machine Learning Models\nLeveraging Tailwind CSS to quickly style React Components\nWhat's next for Unisight \ud83d\udcc3\nWe believe that our App has great potential. Besides, we plan to expand Unisight's capabilities by incorporating other languages, for right now it can natively process English transcripts only. This will not only permit a greater audience to transform their education of more people but also facilitate the exchange of diverse information and technical expertise globally. Additionally, we intend to continue improving the accuracy and speed of our Machine learning Model to provide better accuracy & low-spec support.\nNote \u2014 API credentials have been revoked. If you want to run the same on your local, use your own credentials.", "link": "https://devpost.com/software/unisight", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration \ud83d\udca1\ne-learning has become the new normal in this pandemic era. besides, for students, video conferencing tools have become the part & parcel of daily activities. but in general, this leads to feeble face-to-face traditional interaction between lecturers and students degrading the interest in online education extensively in the last few months, globally! we aim to innovate a revolutionary -----> tool !!!  that helps teachers and students improve the online learning experience for the better.\nwe believe that with the power of ai, this can be solved if proceeded creatively. thus we made unisight! \u2728\nso what\u2019s the app about? \ud83e\udd14\nunisight captures the realtime expressions of the student throughout the duration of the class which will help the person on the other side of the screen understand their flow turning online learning into a fun, interesting & productive experience. we plan to use different technologies & cutting edge ai methods to extract the data & generate instant analytics.\nsay goodbye to boring lectures, switch to unisight! \ud83d\ude80\ntech stack \ud83c\udfd7\nfirst and foremost, it is crafted with \ud83d\udc99. for the front-end, we\u2019ve used react.js & tailwind as css framework. the authentication (oauth) has been done by firebase & we\u2019re also using the cloudstore database for storing user logs.\nto create this, we trained a custom sentiment analysis model which is used to analyze & predict the user emotions directly from the input video. the model used tensorflow.js under the hood to process the data directly on the client-side & hence no data is being sent to the server which also preserves privacy.\nfor heartbeat detection, we have taken the advantage of mayer waves \u2014 oscillations of arterial pressure that occur in conscious subjects. using these, we determine your heart rate by monitoring the tiny fluctuations in the color of the forehead. this is done by taking the average pixel values of the forehead region and performing a fourier transform to convert this signal to a sum of frequencies, the most prominent of which will correspond to the user's heart rate.\nthe net data is then averaged and passed into a chart library where all of these calculations are made in real-time to populate the average analytics infront of us.\nchallenges we ran into \ud83e\uddf1\nthere were a lot of challenges on our way. first, because we are all online and spread around the globe, it was somewhat difficult for us to be communicating during the process. we also spent a great deal of time discussing ideas for the project. as we forwarded, we had to fix critical bugs & obviously, compiling chunks of modules into one was the most difficult part!\ndesign \ud83c\udfa8\nwe were heavily inspired by the revised version of iterative design process, which not only includes visual design, but a full-fledged research cycle in which you must discover and define your problem before tackling your solution & then finally deploy it.\ndiscover: a deep dive into the problem we are trying to solve.\ndefine: synthesizing the information from the discovery phase into a problem definition.\ndevelop: think up solutions to the problem.\ndeliver: pick the best solution and build that.\nthis time went for the minimalist material ui design. we utilized design tools like figma, photoshop & illustrator to prototype our designs before doing any coding. through this, we are able to get iterative feedback so that we spend less time re-writing code.\nresearch \ud83d\udcda\nresearch is the key to empathizing with users: we found our specific user group early and that paves the way for our whole project. here are a few of the resources that were helpful to us \u2014\na multi-task approach using deep learning (arxiv, oct 2021) \u2014 https://arxiv.org/pdf/2110.15028.pdf\nreal time heart rate monitoring from facial rgb color video using webcam \u2014 bit.ly/3pq9syt\ntfjs docs : https://js.tensorflow.org/api/latest\ndistractions via new technologies ft. students \u2014 bit.ly/3djotlo\n84% of teachers facing challenges during online classes \u2014 bit.ly/3pq9w7l\ncredits\ndesign resources : freepik\nicons : icons8, fontawesome\nfont : roboto / raleway / rf dewi / muller\ntakeaways\naccomplishments that we're proud of \ud83d\ude4c\na fully working prototype! this has been intense yet insightful. we are proud of the final product we designed and engineered within such a short timeframe of 24hrs. the fluidity of the user's experience is something that we take pride in as we strive to return a sense of serenity to the oftentimes stressful and difficult process of studying and learning. this could not have been achieved without synergizing modular components to enhance the learning process.\nwhat we learned \ud83d\ude4c\nstaying hydrated was our motto for completing this impactful and complicated project on time. we have learned how great wins are accomplished by working together. apart from the above, together we had a lot of fun participating in khe'21 & these are the few things we learned \u2014\nlearned about design centric approaches & figma prototyping\nexplored different kinds of open-sourced chart libraries\nimproved time management skills\noptimizing machine learning models\nleveraging tailwind css to quickly style react components\nwhat's next for unisight \ud83d\udcc3\nwe believe that our app has great potential. besides, we plan to expand unisight's capabilities by incorporating other languages, for right now it can natively process english transcripts only. this will not only permit a greater audience to transform their education of more people but also facilitate the exchange of diverse information and technical expertise globally. additionally, we intend to continue improving the accuracy and speed of our machine learning model to provide better accuracy & low-spec support.\nnote \u2014 api credentials have been revoked. if you want to run the same on your local, use your own credentials.", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 4, "media": null, "medialink": null, "identifyer": 59500811}, {"Unnamed: 0": 895, "autor": "Quiksave Savings Platform", "date": null, "content": "Inspiration\nIn an increasingly turbulent economy, individuals are focusing more on saving for the future. Standard bank applications and websites allow users to track their savings goals, but most do not employ a separate allocation for charitable giving. We wanted to create a tool to amplify the spirit of giving. In response, we created Quiksave to provide people with a giving-focused digital platform to help people achieve their saving and charitable goals.\nWhat it does\nQuiksave takes inspiration from financial dashboards. Users are encouraged to devote some of their income to future purchases, with their savings progress tracked our on website. It was built so the user can easily allocate a portion of their savings to a charitable cause. We also included machine learning tools to help our users find the best price for their purchases.\nHow we built it\nWe started by creating mockups in Figma. Once we decided on a color palette and design that we wanted to use we created our website using Bootstrap, CSS, and HTML. We used machine learning by using Google Teachable Machine to identify various items and wrote the code using HTML and JavaScript. We also curated a Search Engine to search for items in major shopping website by using Google Programmable Search and integrating the Javascript code with our existing framework.\nFor the backend we used PHP and MySQL to create a server-side database. The backend was created with security in mind. SQL queries are a common attack vector for cybercriminals. We limited the possible user inputs to prevent SQL injections, which ultimately will prevent our database from being exposed. Our PHP code authenticates user login and password information which is stored in our MySQL database. Another security feature that we added was having activation codes sent to the user's email address if they just signed up for an account. This confirms that they are in fact the actual owner of the email account.\nChallenges we ran into\nOur team was also working remotely across 3 different timezones, so it was challenging to sync up while each member was in a different location in the world. It was also difficult to incorporate all the different features that we wanted our website to be able to have. Our ambitions were a bit too much for the amount of time that we had.\nAccomplishments that we're proud of\nI think we did a really great job communicating with each other, even though we were in 3 different time zones. We decided each of our roles in the beginning and gave each other support throughout the hackathon whenever we got stuck on something.\nWhat we learned\nWe learned a lot about how important it is to connected the front end and back end, in order to have a dynamic website. I personally learned PHP and MySQL over the weekend, in order to create a database to store information for the website. Taking example from the previous Tech Together Workshops (New York), I used Teachable Machine and tried implementing it in my project. It is a very easy tool to create your own machine learning project and integrate with a broader idea.\nWhat's next for Quiksave Savings Platform\nAs a next step, we would like to integrate Quiksave with major shopping websites like Amazon, Walmart, Target and BestBuy and curate a personal wishlist for every user from each of the websites and save them in our database. Once this is done, we will create a financial plan for them for each month so that they can get gifts for themselves and/or their family during the holiday seasons. We would also like to partner with charitable organizations so that it becomes easier for users to donate and at the same time, keep a check on their personal finances. The appropriate budget plans will be made by linking the app to their respective banking apps, which is also part of our future plans.", "link": "https://devpost.com/software/quiksave-savings-platform", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nin an increasingly turbulent economy, individuals are focusing more on saving for the future. standard bank applications and websites allow users to track their savings goals, but most do not employ a separate allocation for charitable giving. we wanted to create a -----> tool !!!  to amplify the spirit of giving. in response, we created quiksave to provide people with a giving-focused digital platform to help people achieve their saving and charitable goals.\nwhat it does\nquiksave takes inspiration from financial dashboards. users are encouraged to devote some of their income to future purchases, with their savings progress tracked our on website. it was built so the user can easily allocate a portion of their savings to a charitable cause. we also included machine learning tools to help our users find the best price for their purchases.\nhow we built it\nwe started by creating mockups in figma. once we decided on a color palette and design that we wanted to use we created our website using bootstrap, css, and html. we used machine learning by using google teachable machine to identify various items and wrote the code using html and javascript. we also curated a search engine to search for items in major shopping website by using google programmable search and integrating the javascript code with our existing framework.\nfor the backend we used php and mysql to create a server-side database. the backend was created with security in mind. sql queries are a common attack vector for cybercriminals. we limited the possible user inputs to prevent sql injections, which ultimately will prevent our database from being exposed. our php code authenticates user login and password information which is stored in our mysql database. another security feature that we added was having activation codes sent to the user's email address if they just signed up for an account. this confirms that they are in fact the actual owner of the email account.\nchallenges we ran into\nour team was also working remotely across 3 different timezones, so it was challenging to sync up while each member was in a different location in the world. it was also difficult to incorporate all the different features that we wanted our website to be able to have. our ambitions were a bit too much for the amount of time that we had.\naccomplishments that we're proud of\ni think we did a really great job communicating with each other, even though we were in 3 different time zones. we decided each of our roles in the beginning and gave each other support throughout the hackathon whenever we got stuck on something.\nwhat we learned\nwe learned a lot about how important it is to connected the front end and back end, in order to have a dynamic website. i personally learned php and mysql over the weekend, in order to create a database to store information for the website. taking example from the previous tech together workshops (new york), i used teachable machine and tried implementing it in my project. it is a very easy tool to create your own machine learning project and integrate with a broader idea.\nwhat's next for quiksave savings platform\nas a next step, we would like to integrate quiksave with major shopping websites like amazon, walmart, target and bestbuy and curate a personal wishlist for every user from each of the websites and save them in our database. once this is done, we will create a financial plan for them for each month so that they can get gifts for themselves and/or their family during the holiday seasons. we would also like to partner with charitable organizations so that it becomes easier for users to donate and at the same time, keep a check on their personal finances. the appropriate budget plans will be made by linking the app to their respective banking apps, which is also part of our future plans.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59500895}, {"Unnamed: 0": 900, "autor": "Sentiment Analysis of Customer Reviews", "date": null, "content": "Inspiration\nInspired by the configuration of the brain, sentiment analysis algorithms imitate how the human brain processes data through an artificial network of neurons.\nWhat it does\nSentiment analysis is a machine learning tool that analyzes texts for polarity, from positive to negative. By training machine learning tools with examples of emotions in text, machines automatically learn how to detect sentiment without human input.\nHow we built it\nWe used 8000 customer reviews of different smart phones and later we cleaned the data and performed EDA and used libraries to detect the reviews as positive or negative depending on score of sentiment like if the score is above 0.6 it is considered to be positive and lesser than that will be negative. We used Logistic regression ML algorithm and achieved an accuracy score of 93%.\nChallenges we ran into\nThe challenges we faced are we scraped our own data from amazon and used library Beautiful soup also it's not a easy task to scrape 8000 reviews from online website.\nAccomplishments that we're proud of\nWe achieved an accuracy score of 93%.\nWhat we learned\nWe learned how can we use real world data to achieve so that brings great insights to service providers to improve their products or whatever they're providing to the market.\nWhat's next for Sentiment Analysis of Customer Reviews\nImproving accuracy score to 100% and building an web application for it.", "link": "https://devpost.com/software/sentiment-analysis-fmsh8e", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ninspired by the configuration of the brain, sentiment analysis algorithms imitate how the human brain processes data through an artificial network of neurons.\nwhat it does\nsentiment analysis is a machine learning -----> tool !!!  that analyzes texts for polarity, from positive to negative. by training machine learning tools with examples of emotions in text, machines automatically learn how to detect sentiment without human input.\nhow we built it\nwe used 8000 customer reviews of different smart phones and later we cleaned the data and performed eda and used libraries to detect the reviews as positive or negative depending on score of sentiment like if the score is above 0.6 it is considered to be positive and lesser than that will be negative. we used logistic regression ml algorithm and achieved an accuracy score of 93%.\nchallenges we ran into\nthe challenges we faced are we scraped our own data from amazon and used library beautiful soup also it's not a easy task to scrape 8000 reviews from online website.\naccomplishments that we're proud of\nwe achieved an accuracy score of 93%.\nwhat we learned\nwe learned how can we use real world data to achieve so that brings great insights to service providers to improve their products or whatever they're providing to the market.\nwhat's next for sentiment analysis of customer reviews\nimproving accuracy score to 100% and building an web application for it.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59500900}, {"Unnamed: 0": 960, "autor": "DeadBit", "date": null, "content": "Inspiration\nWe gathered our inspiration from the opportunity of being a part of this growing crypto gaming industry.\nWhat it does\nIt's a game where you explore a small scale of a city, based on Bayamon Puerto Rico. The concept is simple, you need to capture the monsters, which are NFT's. You collect them using a camera as the main tool. Once you capture the NFT monster you get his avatar on your wallet.\nHow we built it\nTo develop the city of Bayamon we use Godot a cross-platform, free and open-source game engine released under the MIT license.\nWe created the characters with Forger, a digital sculpting application for professionals and aspiring amateurs alike. This app allows the artists to feel like artists, it is versatile and easy to use.\nThe characters were animated in blender a free and open-source 3D computer graphics software toolset used for creating animated films, visual effects, art, 3D printed models, motion graphics, interactive 3D applications, virtual reality, and computer games.\nThe smart contract was developed with visual studio code using solidity language.\nAlso we use project Nebula as a reference to create the crypto Wallet in the game.\nChallenges we ran into\nDuring the process of creating this game we faced the challenge of innovating on a new level. The purpose of reaching our goal was to prove there is alway space for new ideas on crypto gaming development.\nAccomplishments that we're proud of\nDevelop a new function on Godot to implement a wallet into the game. Create and design a 3D Crypto Game in Puerto Rico. Growth of knowledge on this new initiative called the MetaVerse.\nWhat we learned\nWe all learned something new during this process and one of our members had the opportunity of participating on her first hackathon by designing different assets and concepts as part of the team. Other members learned how to use Godot and Forger. We learned how to create a smart contract and many more tools used to built this project.\nWhat's next for DeadBit\nWe desire to develop more monsters, capture gadgets and improve our design skill in order to have better graphics for the future of the collection of NFT's.", "link": "https://devpost.com/software/deadbit", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe gathered our inspiration from the opportunity of being a part of this growing crypto gaming industry.\nwhat it does\nit's a game where you explore a small scale of a city, based on bayamon puerto rico. the concept is simple, you need to capture the monsters, which are nft's. you collect them using a camera as the main -----> tool !!! . once you capture the nft monster you get his avatar on your wallet.\nhow we built it\nto develop the city of bayamon we use godot a cross-platform, free and open-source game engine released under the mit license.\nwe created the characters with forger, a digital sculpting application for professionals and aspiring amateurs alike. this app allows the artists to feel like artists, it is versatile and easy to use.\nthe characters were animated in blender a free and open-source 3d computer graphics software toolset used for creating animated films, visual effects, art, 3d printed models, motion graphics, interactive 3d applications, virtual reality, and computer games.\nthe smart contract was developed with visual studio code using solidity language.\nalso we use project nebula as a reference to create the crypto wallet in the game.\nchallenges we ran into\nduring the process of creating this game we faced the challenge of innovating on a new level. the purpose of reaching our goal was to prove there is alway space for new ideas on crypto gaming development.\naccomplishments that we're proud of\ndevelop a new function on godot to implement a wallet into the game. create and design a 3d crypto game in puerto rico. growth of knowledge on this new initiative called the metaverse.\nwhat we learned\nwe all learned something new during this process and one of our members had the opportunity of participating on her first hackathon by designing different assets and concepts as part of the team. other members learned how to use godot and forger. we learned how to create a smart contract and many more tools used to built this project.\nwhat's next for deadbit\nwe desire to develop more monsters, capture gadgets and improve our design skill in order to have better graphics for the future of the collection of nft's.", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59500960}, {"Unnamed: 0": 982, "autor": "Taggle", "date": null, "content": "Inspiration\nWe love working on whiteboards because it gives us creative freedom. We used to have an analog whiteboard in our office where we planned our work just the way we wanted to. But then the pandemic came and we mostly adapted to working remotely which meant that we couldn't work on the local whiteboard anymore.\nWhat it does\nTaggle is basically what would happen if a project management tool and a whiteboard had a child. We carefully chose the best features of both and combined them into a digital tool that feels like a whiteboard but helps you manage your projects with metrics, WIP limits, etc. So it basically removes the boring work from project management and allows creative freedom, because every process is just as unique as the team is.\nHow we built it\nOur core team is a team of three. Two full-stack devs and a UI/UX designer. We chose the lean approach from the book the lean startup and tried to iterate as fast as possible. We kept close contact with project managers, users, and coaches to build a product that people actually want. From a technical perspective, we used Next.js in conjunction with Konva (HTML Canvas simplified). The state is managed with redux because we chose an optimistic approach with our event-driven architecture. As soon as you drag an element, this action is applied locally and simultaneously sent to the server (Node.js, fastify, MongoDB), which sends this event to all other connected users on that board. The live communication is done with websockets using socket.io\nChallenges we ran into\nMultiplayer / Live collaboration is much harder than we thought. Especially when there are cases like unexpected client disconnects (bad connection, tab up for too long) it's not obvious what you want to do. I think there is a good market for a SaaS which handles this for you.\nAccomplishments that we're proud of\nOur event-driven architecture. We built it from scratch and it was the first time for the devs to do that. We actually have a hidden feature to test this event-driven approach which allows you to scroll through your board's events and rewatch how it was built. I'm really proud of this and I learned a lot in the process.\nWhat we learned\nA lot! On a technical side, it was the first project for us that featured live collaboration. We are also new to event-driven architectures, which leads to reading and research regarding those topics. On the product side of things, we learned how to build a product from the ground up including all make-or-buy decisions, building a community, doing market research, and marketing. Regarding marketing, we learned that it's much harder than we thought because we tend to overthink campaigns: should we do this? This isn't perfect. We learned that sometimes just giving it a try can lead to unexpected results and sometimes even positive ones \ud83d\ude09\nWhat's next for Taggle\nMarketing and collecting more feedback from end-users. We've had some spikes in users and also had some churn and we want to find out why. Therefore we added a live chat, but also want to ask more for feedback and give the users even better chances to give feedback without any effort.", "link": "https://devpost.com/software/taggle", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe love working on whiteboards because it gives us creative freedom. we used to have an analog whiteboard in our office where we planned our work just the way we wanted to. but then the pandemic came and we mostly adapted to working remotely which meant that we couldn't work on the local whiteboard anymore.\nwhat it does\ntaggle is basically what would happen if a project management -----> tool !!!  and a whiteboard had a child. we carefully chose the best features of both and combined them into a digital tool that feels like a whiteboard but helps you manage your projects with metrics, wip limits, etc. so it basically removes the boring work from project management and allows creative freedom, because every process is just as unique as the team is.\nhow we built it\nour core team is a team of three. two full-stack devs and a ui/ux designer. we chose the lean approach from the book the lean startup and tried to iterate as fast as possible. we kept close contact with project managers, users, and coaches to build a product that people actually want. from a technical perspective, we used next.js in conjunction with konva (html canvas simplified). the state is managed with redux because we chose an optimistic approach with our event-driven architecture. as soon as you drag an element, this action is applied locally and simultaneously sent to the server (node.js, fastify, mongodb), which sends this event to all other connected users on that board. the live communication is done with websockets using socket.io\nchallenges we ran into\nmultiplayer / live collaboration is much harder than we thought. especially when there are cases like unexpected client disconnects (bad connection, tab up for too long) it's not obvious what you want to do. i think there is a good market for a saas which handles this for you.\naccomplishments that we're proud of\nour event-driven architecture. we built it from scratch and it was the first time for the devs to do that. we actually have a hidden feature to test this event-driven approach which allows you to scroll through your board's events and rewatch how it was built. i'm really proud of this and i learned a lot in the process.\nwhat we learned\na lot! on a technical side, it was the first project for us that featured live collaboration. we are also new to event-driven architectures, which leads to reading and research regarding those topics. on the product side of things, we learned how to build a product from the ground up including all make-or-buy decisions, building a community, doing market research, and marketing. regarding marketing, we learned that it's much harder than we thought because we tend to overthink campaigns: should we do this? this isn't perfect. we learned that sometimes just giving it a try can lead to unexpected results and sometimes even positive ones \ud83d\ude09\nwhat's next for taggle\nmarketing and collecting more feedback from end-users. we've had some spikes in users and also had some churn and we want to find out why. therefore we added a live chat, but also want to ask more for feedback and give the users even better chances to give feedback without any effort.", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59500982}, {"Unnamed: 0": 984, "autor": "Import OST Files into MS Outlook", "date": null, "content": "OST file, when disconnected from server either due to downtime or due to any other reason, becomes orphan and thus unusable to send-receive any emails at that time. If you import OST files into PST, then you can send/receive email instantly using Outlook application.\nOrphan OST\nBeing orphan is a state where an OST file is disconnected in some way or the other from the parent machine or the email server system. This happens mostly when the server is down or there is no internet connection to sync the file with the server.\nLet\u2019s Explicate!\nActually what happens with this client-server co-relation in an Exchange email network is that OST file is formed at the client end, which is an offline record of his mailbox over the server! Now, when the user makes any changes in this file like say he updates his calendar or adds some more contact details then these changes get synched with the server when there is complete connectivity or when the connection with the server is established. However, in cases where server fails to mount or is disconnected due to improper internet connectivity, then OST file cannot be synchronized with the server and is thus in orphan state of existence where it cannot be used to send/receive any emails or the changes made to it cannot get in sync with the server.\nThe Real Problem and the Real Solution!\nDuring downtime, until when the server is down or there is no proper net connection, the user cannot receive his urgent emails or cannot send any crucial emails too. For this, he either needs to establish a proper connection back again, which is dependant on the state of the system \u2013 or he can otherwise import OST files data into such format that he can readily use with any available email application. In most companies, Outlook is conjugated with Exchange to run such client-server collaborated type of network. So, if such is your case, then converting data from OST into PST is a good way to address the problem as there would be no additional costs involved in running the PST, which is run by Outlook application only. No additional costs will be incurred by the company to help you undertake your email processes. Only a small investment would be there so as to convert OST to PST, which is quite a logical investment on the part of your employer as he is going to benefit from it greatly now and in the future as well, because downtimes are a regular affair in any company.\nOST Converter\nThough recommendations do not work in such cases as most people trust their own conscience and they have their own reasons to choose or not to choose a particular solution for this purpose of importing OST files. But, still in order to help you make a good decision, we want to recommend the SysKare OST Converter tool to you, which is a great tool to function this problem in a way better than others.", "link": "https://devpost.com/software/import-ost-files-into-ms-outlook", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "ost file, when disconnected from server either due to downtime or due to any other reason, becomes orphan and thus unusable to send-receive any emails at that time. if you import ost files into pst, then you can send/receive email instantly using outlook application.\norphan ost\nbeing orphan is a state where an ost file is disconnected in some way or the other from the parent machine or the email server system. this happens mostly when the server is down or there is no internet connection to sync the file with the server.\nlet\u2019s explicate!\nactually what happens with this client-server co-relation in an exchange email network is that ost file is formed at the client end, which is an offline record of his mailbox over the server! now, when the user makes any changes in this file like say he updates his calendar or adds some more contact details then these changes get synched with the server when there is complete connectivity or when the connection with the server is established. however, in cases where server fails to mount or is disconnected due to improper internet connectivity, then ost file cannot be synchronized with the server and is thus in orphan state of existence where it cannot be used to send/receive any emails or the changes made to it cannot get in sync with the server.\nthe real problem and the real solution!\nduring downtime, until when the server is down or there is no proper net connection, the user cannot receive his urgent emails or cannot send any crucial emails too. for this, he either needs to establish a proper connection back again, which is dependant on the state of the system \u2013 or he can otherwise import ost files data into such format that he can readily use with any available email application. in most companies, outlook is conjugated with exchange to run such client-server collaborated type of network. so, if such is your case, then converting data from ost into pst is a good way to address the problem as there would be no additional costs involved in running the pst, which is run by outlook application only. no additional costs will be incurred by the company to help you undertake your email processes. only a small investment would be there so as to convert ost to pst, which is quite a logical investment on the part of your employer as he is going to benefit from it greatly now and in the future as well, because downtimes are a regular affair in any company.\nost converter\nthough recommendations do not work in such cases as most people trust their own conscience and they have their own reasons to choose or not to choose a particular solution for this purpose of importing ost files. but, still in order to help you make a good decision, we want to recommend the syskare ost converter -----> tool !!!  to you, which is a great -----> tool !!!  to function this problem in a way better than others.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59500984}, {"Unnamed: 0": 1009, "autor": "Kenni Velez Records AI INC", "date": null, "content": "Inspiration\nI have been enjoying the Music Business since late 1990's and so on I kept the idea of help other artist create their own independent record label so they can maximize the options for collecting royalties.\nWhat it does\nBasically the app creates a user profile so the user can interact with it. It takes all Artist's Music Album and autogenerate a Karaoke Album to be monetized as NFT on the OpenSea market. Also offer a free NFT creator platform using AI to auto replicate a minimum of 10,000 copies. It will be the first NFT collection an artist will promote on the NFT Market.\nHow we built it\nOpenAI has been my tool to create all this code into 1 platform willing to become an APP\nChallenges we ran into\nMonetizacion and Administration\nAccomplishments that we're proud of\nTo create new art from previos artistic creations using AI to enhance the experience.\nWhat we learned\nWe learn how to interact with experts in every single topic to give the best solution for our customers creating reddit comunity r/KenniVelez so we can share an collaborate with other developers and get the right path.\nWhat's next for Kenni Velez Records AI INC\nPromote the first Latin America AI Artist and hit the charts.", "link": "https://devpost.com/software/kenni-velez-records-ai-inc", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ni have been enjoying the music business since late 1990's and so on i kept the idea of help other artist create their own independent record label so they can maximize the options for collecting royalties.\nwhat it does\nbasically the app creates a user profile so the user can interact with it. it takes all artist's music album and autogenerate a karaoke album to be monetized as nft on the opensea market. also offer a free nft creator platform using ai to auto replicate a minimum of 10,000 copies. it will be the first nft collection an artist will promote on the nft market.\nhow we built it\nopenai has been my -----> tool !!!  to create all this code into 1 platform willing to become an app\nchallenges we ran into\nmonetizacion and administration\naccomplishments that we're proud of\nto create new art from previos artistic creations using ai to enhance the experience.\nwhat we learned\nwe learn how to interact with experts in every single topic to give the best solution for our customers creating reddit comunity r/kennivelez so we can share an collaborate with other developers and get the right path.\nwhat's next for kenni velez records ai inc\npromote the first latin america ai artist and hit the charts.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59501009}, {"Unnamed: 0": 1012, "autor": "Banco Satoshi", "date": null, "content": "Inspiration\nInvestment Banking shouldn't be limited to only accredited investors.\nAccess to capital is a human right and with Bitcoin, and bancosatoshi.com, investment can now come from anywhere in the world.\nBanco Satoshi won't ask for private keys, ever!\nWhat it does\nbancosatoshi.com is an open-source responsive multilingual web application that provides a seamless checkout experience for Bitcoin and Bitcoin Lightning payments that allows anyone in the world to INVEST IN BUSINESSES STARTING FROM 1 SAT.\nHow we built it\nbancosatoshi.com is cutting edge in every detail. It features:\nA Next JS web application (responsive, multilingual)\nA CMS for dynamic business funding campaigns data\nA BTC Pay Server instance for Bitcoin and Lightning payments (funds go to a Business Owner wallet via the xpub key)\nInvestment Protocol (Protocolo de Inversi\u00f3n) is a process where business owners can apply for funding\nChallenges we ran into\nIntegrating all the aforementioned systems was challenging, but working as a team, we were able to merge everything together within the Bankathon period.\nWe tried to use Qredo, but deriving the Bitcoin addresses for each checkout experience was not possible because Qredo won't display the xpub key. We tried though, and created a network, a fund and policies.\nAccomplishments that we're proud of\nbancosatoshi.com is a revolutionary and powerful tool. Only intelligent governments will allow this tool to run in their country.\nWhat we learned\nWe learned that integrating with Bitcoin and Bitcoin LN makes payment experiences a breeze. It only takes a few seconds and you can buy great things! Like business ownership.\nWhat's next for Banco Satoshi\nWe are distributed and very well organized:\nPlease look at this Deck to see the details of the use of proceeds and other important information for investors and judges: https://docs.google.com/presentation/d/1Bk-lvG-lvH7MO08pZdFPUKMsOp7P7YHAfl9p1PoKArQ/edit?usp=sharing\nPlease also look at the intended use of funds and our current costs (with the 100k USD we can keep developing for ~5 months): https://docs.google.com/spreadsheets/d/1kkk1KDcSubN84c2MqH4jFlgQRnzoQbr72AOWzspYAsM/edit?usp=sharing", "link": "https://devpost.com/software/banco-satoshi", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ninvestment banking shouldn't be limited to only accredited investors.\naccess to capital is a human right and with bitcoin, and bancosatoshi.com, investment can now come from anywhere in the world.\nbanco satoshi won't ask for private keys, ever!\nwhat it does\nbancosatoshi.com is an open-source responsive multilingual web application that provides a seamless checkout experience for bitcoin and bitcoin lightning payments that allows anyone in the world to invest in businesses starting from 1 sat.\nhow we built it\nbancosatoshi.com is cutting edge in every detail. it features:\na next js web application (responsive, multilingual)\na cms for dynamic business funding campaigns data\na btc pay server instance for bitcoin and lightning payments (funds go to a business owner wallet via the xpub key)\ninvestment protocol (protocolo de inversi\u00f3n) is a process where business owners can apply for funding\nchallenges we ran into\nintegrating all the aforementioned systems was challenging, but working as a team, we were able to merge everything together within the bankathon period.\nwe tried to use qredo, but deriving the bitcoin addresses for each checkout experience was not possible because qredo won't display the xpub key. we tried though, and created a network, a fund and policies.\naccomplishments that we're proud of\nbancosatoshi.com is a revolutionary and powerful -----> tool !!! . only intelligent governments will allow this tool to run in their country.\nwhat we learned\nwe learned that integrating with bitcoin and bitcoin ln makes payment experiences a breeze. it only takes a few seconds and you can buy great things! like business ownership.\nwhat's next for banco satoshi\nwe are distributed and very well organized:\nplease look at this deck to see the details of the use of proceeds and other important information for investors and judges: https://docs.google.com/presentation/d/1bk-lvg-lvh7mo08pzdfpukmsop7p7yhafl9p1pokarq/edit?usp=sharing\nplease also look at the intended use of funds and our current costs (with the 100k usd we can keep developing for ~5 months): https://docs.google.com/spreadsheets/d/1kkk1kdcsubn84c2mqh4jflgqrnzoqbr72aowzspyasm/edit?usp=sharing", "sortedWord": "None", "removed": "Nan", "score": 5, "comments": 0, "media": null, "medialink": null, "identifyer": 59501012}, {"Unnamed: 0": 1036, "autor": "Construe", "date": null, "content": "The data one uses for training and deploying machine learning models can determine success or failure for one's AI projects.\nCreating medical datasets requires uncommon file support, and a series of features required to maintain accountability of annotations unlike most annotation tools on the market.\nConstrue plays an important role in whether you can create a high-performing model that powers a disruptive solution or solves a painful, expensive problem - or end up investing time and resources on a failed experiment.\nThe Challenge: Medical Imaging Annotation vs Normal Annotation\nIf your ultimate goal is to train machine learning models, there are few differences between annotating a medical image versus a regular PNG or JPEG. Here are a few things to consider about medical imaging that do not apply in other vision data,\nMedical Imaging contains transparencies\nThis means occlusions must be treated differently. Objects in front of one another may appear behind one another. It's no secret that AI handles occlusion poorly due to a lack of presence of mind, and transparent objects can be even worse. Luckily though, an organ, cell, or bone appearing transparent is far more obvious to an AI than a pane of glass.\nSee the chest x-ray below: are the lungs behind or in front of the diaphragm? The answer is both. The occluded portion of the lungs cannot be perceived by traditional computer vision methods, however a deep neural network can easily learn to spot it.\nA chest x-ray displaying the lower portion of the lungs, extending in front of the diaphragm posteriorly and behind it anteriorly.\nFormats may vary, for better or for worse\nMost medical imaging will be in DICOM format. A DICOM file represents a case, which may contain one or more images.\nFrom a machine learning perspective, the DICOM file will be converted to another lossless image format during training, therefore it's not a necessity to use DICOM files for AI research. However preserving the integrity of the DICOM image can be useful for the annotation phase, particularly as radiologists are familiar with how DICOM viewers work after operating them for years.\nDiffering views and differing volumes A case may contain 2D or 3D imaging. In both examples, often more than one view is necessary to assess what's happening. For example, the x-ray of a hand may only reveal a fracture when the hand is in certain pose or angle. Nonetheless it is standard to capture a frontal view of the hand anyways:\nA small fracture at the base of the 3rd and 4th middle phalanx is mostly only visible on the right image.\nIt's important not to include un-usable data in your machine learning dataset. If a view such as the one above is useful for reference purposes, but cannot be labelled and turned into training data, it's best to discard it.\nFeatures\nViewer tools are provided to adjust windowing (brightness/contrast), zoom, invert, and panning of the image.\nAnnotation tools are provided for marking images with either rectangles or length measurements. These annotations are defined by their bounding (x,y) coordinates as well as some other metadata.\nAll annotation metadata can be exported in JSON format to be used for a variety of purposes, such as training input for deep learning models using bounding box algorithms. The downloaded JSON files can also be later used to restore any annotation state stored in them by simply dragging and dropping the relevant JSON file onto the viewer while the images are loaded or can be shared to other team members and organizations for collaboration.\nInspiration\nPeople are shaped by their childhood experiences and by their families. For me, both of my parents are doctors, so several aspects of my life have been affected by being surrounded by people who practice medicine.\nI know that radiologists annotate (or markup) medical images on a daily basis. This can be done in DICOM viewers, which contain basic annotation capabilities such as bounding boxes, arrows, and sometimes polygons. Machine learning (ML) may sometimes leverage these labels, however their format is often inconsistent with the needs of ML research, such as lack of instance IDs, attributes, a labeling queue, or the correct formats for deep learning frameworks like Pytorch or TensorFlow.\nConstrue tries to solve that. It's the first step in that direction. I want to empower all the people like my parents using computer science.\nWhat I learned\nWhile many services exist that provide annotation tools for common file formats like PDF and JPEG, the healthcare sector presents a unique challenge in the form of DICOM files. This industry-specific format contains both images and important metadata identifiers that provide information about the image itself and the patient in question. While there are ways to extract images from DICOM files and convert them into a more manageable format, doing so could endanger compliance status or permanently degrade the image quality.\nAccomplishments that I'm proud of\nCreating this web-based tool in somewhat short period of time and taking the first step toward my dreams.\nWhat's next for Construe\nIf it's recommended, I would love to work more on Construe to get it out and open it to the public, allowing medical organizations to test out Construe.", "link": "https://devpost.com/software/construe", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "the data one uses for training and deploying machine learning models can determine success or failure for one's ai projects.\ncreating medical datasets requires uncommon file support, and a series of features required to maintain accountability of annotations unlike most annotation tools on the market.\nconstrue plays an important role in whether you can create a high-performing model that powers a disruptive solution or solves a painful, expensive problem - or end up investing time and resources on a failed experiment.\nthe challenge: medical imaging annotation vs normal annotation\nif your ultimate goal is to train machine learning models, there are few differences between annotating a medical image versus a regular png or jpeg. here are a few things to consider about medical imaging that do not apply in other vision data,\nmedical imaging contains transparencies\nthis means occlusions must be treated differently. objects in front of one another may appear behind one another. it's no secret that ai handles occlusion poorly due to a lack of presence of mind, and transparent objects can be even worse. luckily though, an organ, cell, or bone appearing transparent is far more obvious to an ai than a pane of glass.\nsee the chest x-ray below: are the lungs behind or in front of the diaphragm? the answer is both. the occluded portion of the lungs cannot be perceived by traditional computer vision methods, however a deep neural network can easily learn to spot it.\na chest x-ray displaying the lower portion of the lungs, extending in front of the diaphragm posteriorly and behind it anteriorly.\nformats may vary, for better or for worse\nmost medical imaging will be in dicom format. a dicom file represents a case, which may contain one or more images.\nfrom a machine learning perspective, the dicom file will be converted to another lossless image format during training, therefore it's not a necessity to use dicom files for ai research. however preserving the integrity of the dicom image can be useful for the annotation phase, particularly as radiologists are familiar with how dicom viewers work after operating them for years.\ndiffering views and differing volumes a case may contain 2d or 3d imaging. in both examples, often more than one view is necessary to assess what's happening. for example, the x-ray of a hand may only reveal a fracture when the hand is in certain pose or angle. nonetheless it is standard to capture a frontal view of the hand anyways:\na small fracture at the base of the 3rd and 4th middle phalanx is mostly only visible on the right image.\nit's important not to include un-usable data in your machine learning dataset. if a view such as the one above is useful for reference purposes, but cannot be labelled and turned into training data, it's best to discard it.\nfeatures\nviewer tools are provided to adjust windowing (brightness/contrast), zoom, invert, and panning of the image.\nannotation tools are provided for marking images with either rectangles or length measurements. these annotations are defined by their bounding (x,y) coordinates as well as some other metadata.\nall annotation metadata can be exported in json format to be used for a variety of purposes, such as training input for deep learning models using bounding box algorithms. the downloaded json files can also be later used to restore any annotation state stored in them by simply dragging and dropping the relevant json file onto the viewer while the images are loaded or can be shared to other team members and organizations for collaboration.\ninspiration\npeople are shaped by their childhood experiences and by their families. for me, both of my parents are doctors, so several aspects of my life have been affected by being surrounded by people who practice medicine.\ni know that radiologists annotate (or markup) medical images on a daily basis. this can be done in dicom viewers, which contain basic annotation capabilities such as bounding boxes, arrows, and sometimes polygons. machine learning (ml) may sometimes leverage these labels, however their format is often inconsistent with the needs of ml research, such as lack of instance ids, attributes, a labeling queue, or the correct formats for deep learning frameworks like pytorch or tensorflow.\nconstrue tries to solve that. it's the first step in that direction. i want to empower all the people like my parents using computer science.\nwhat i learned\nwhile many services exist that provide annotation tools for common file formats like pdf and jpeg, the healthcare sector presents a unique challenge in the form of dicom files. this industry-specific format contains both images and important metadata identifiers that provide information about the image itself and the patient in question. while there are ways to extract images from dicom files and convert them into a more manageable format, doing so could endanger compliance status or permanently degrade the image quality.\naccomplishments that i'm proud of\ncreating this web-based -----> tool !!!  in somewhat short period of time and taking the first step toward my dreams.\nwhat's next for construe\nif it's recommended, i would love to work more on construe to get it out and open it to the public, allowing medical organizations to test out construe.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59501036}, {"Unnamed: 0": 1048, "autor": "ThetaWatch 2.0", "date": null, "content": "Inspiration\nThe inspiration for ThetaWatch was a combination of by backgrounds as a tv producer, web developer, and journalist. My experience in media fields made me aware of the value of Theta early on, but my education as a journalist made it very clear that in order to grow, Theta Network will need community tools and resources that put things into an easy to understand format.\nAbout 1.0\nThe idea started as creating a place that's just a collection of resources that are curated in a way that makes sense to the average person. From there the idea grew to include custom reporting features utilizing my background as a developer.\nWhen I started looking in to the other explorers out there, I realized there was no easy way to save reports, make historical comparisons, download data, print charts, or request new statistics. So why not create one myself?\nChallenges we ran into\nThe biggest challenge creating Version 2.0 was coming up with a UX that was easy to use, standardized, and still capable of growing with the needs of the platform.\nWhat it does\nThetaWatch integrates with the Theta Explorer api to pull node data into an off-chain database. This allows the application to run a number of custom reports that users can search and filter for their own purposes. By standardizing the common search/print/download/charting features, it also creates a hub where community members can request custom charts to be added for the whole community to utilize in their own investing, staking, or other research needs.\nIt is meant to be a tool that grows both as the service offerings grow from Theta, and as the community needs grow for deeper analysis of those services.\nHow we built it\nThe app is built on a codeigniter framework, bootstrap for ui, chart.js for displaying data. It integrates with Theta via the Explorer API.\nAccomplishments that we're proud of\nI'm really happy with the UX. I think it will be clean and easy to navigate for a lot of people. And that's a big hurdle to get over when trying to bring new audiences into a complex project.\nWhat's next for ThetaWatch 2.0\nMy local copy is capable of integrating with the R programming language to help create more advanced stats and reports.", "link": "https://devpost.com/software/thetawatch-2-0", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nthe inspiration for thetawatch was a combination of by backgrounds as a tv producer, web developer, and journalist. my experience in media fields made me aware of the value of theta early on, but my education as a journalist made it very clear that in order to grow, theta network will need community tools and resources that put things into an easy to understand format.\nabout 1.0\nthe idea started as creating a place that's just a collection of resources that are curated in a way that makes sense to the average person. from there the idea grew to include custom reporting features utilizing my background as a developer.\nwhen i started looking in to the other explorers out there, i realized there was no easy way to save reports, make historical comparisons, download data, print charts, or request new statistics. so why not create one myself?\nchallenges we ran into\nthe biggest challenge creating version 2.0 was coming up with a ux that was easy to use, standardized, and still capable of growing with the needs of the platform.\nwhat it does\nthetawatch integrates with the theta explorer api to pull node data into an off-chain database. this allows the application to run a number of custom reports that users can search and filter for their own purposes. by standardizing the common search/print/download/charting features, it also creates a hub where community members can request custom charts to be added for the whole community to utilize in their own investing, staking, or other research needs.\nit is meant to be a -----> tool !!!  that grows both as the service offerings grow from theta, and as the community needs grow for deeper analysis of those services.\nhow we built it\nthe app is built on a codeigniter framework, bootstrap for ui, chart.js for displaying data. it integrates with theta via the explorer api.\naccomplishments that we're proud of\ni'm really happy with the ux. i think it will be clean and easy to navigate for a lot of people. and that's a big hurdle to get over when trying to bring new audiences into a complex project.\nwhat's next for thetawatch 2.0\nmy local copy is capable of integrating with the r programming language to help create more advanced stats and reports.", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59501048}, {"Unnamed: 0": 1075, "autor": "Text Simplify", "date": null, "content": "What it does\nOur project is a chrome extension that uses Modzy services to analyze text and get insights like sentiment, keywords and summary for anything on the internet. In real-time, you may look into YouTube videos, news articles, tweets, and even subreddits to gain useful information. Our project Text Simplify offers a simple way to Increase your research productivity with real-time summarization and get insights from text and Instantly summarize long YouTube videos.\nLeveraging Modzy's sentiment analysis capabilities we can find the item's perception, whether it is negative or positive.\nUsing the snipping tool you can screenshot & select specific parts of the web pages across multiple sites to get a detailed report on a certain topic.\nEx: You can use it to understand public sentiment after a government implements a new law in the country across different platforms like YouTube, Reddit, Twitter, blogs, and news articles.\nImagine you are watching the tutorial video which is an hour-long and do not know whether it is useful. Using Text simplify you can understand the insights of the video without watching the entire video.\nBy just opening the video in a tab and by clicking \"video caption\" you will get all the insights in the video. And by clicking the \"video summary\" button you will get the text summary of the subtitles of the entire 1-hour long video in a few sentences.\nHow we built it\nWe build the chrome extension using HTML, CSS, and JavaScript. The charts have been added using the chart.js library.\nWe built our backend rest API using flask. We use Modzy OCR to get text from the screenshots using optical character recognition. Once the text is extracted then it is analyzed by following Modzy API's.\nsentiment analysis\nsummary\nTopic modeling\nYouTube transcript python package is used to get the transcripts for the video. Once the text is extracted then it is analyzed by following Modzy API's.\nsummary\nvideo caption\nWhat's next for Text Simplify\nDeploy backend api and publish chrome extension in chrome web store .", "link": "https://devpost.com/software/text-simplify", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what it does\nour project is a chrome extension that uses modzy services to analyze text and get insights like sentiment, keywords and summary for anything on the internet. in real-time, you may look into youtube videos, news articles, tweets, and even subreddits to gain useful information. our project text simplify offers a simple way to increase your research productivity with real-time summarization and get insights from text and instantly summarize long youtube videos.\nleveraging modzy's sentiment analysis capabilities we can find the item's perception, whether it is negative or positive.\nusing the snipping -----> tool !!!  you can screenshot & select specific parts of the web pages across multiple sites to get a detailed report on a certain topic.\nex: you can use it to understand public sentiment after a government implements a new law in the country across different platforms like youtube, reddit, twitter, blogs, and news articles.\nimagine you are watching the tutorial video which is an hour-long and do not know whether it is useful. using text simplify you can understand the insights of the video without watching the entire video.\nby just opening the video in a tab and by clicking \"video caption\" you will get all the insights in the video. and by clicking the \"video summary\" button you will get the text summary of the subtitles of the entire 1-hour long video in a few sentences.\nhow we built it\nwe build the chrome extension using html, css, and javascript. the charts have been added using the chart.js library.\nwe built our backend rest api using flask. we use modzy ocr to get text from the screenshots using optical character recognition. once the text is extracted then it is analyzed by following modzy api's.\nsentiment analysis\nsummary\ntopic modeling\nyoutube transcript python package is used to get the transcripts for the video. once the text is extracted then it is analyzed by following modzy api's.\nsummary\nvideo caption\nwhat's next for text simplify\ndeploy backend api and publish chrome extension in chrome web store .", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59501075}, {"Unnamed: 0": 1105, "autor": "Starport Plugin Service", "date": null, "content": "Inspiration\nI was inspired to take place in the Starport challenge, and specifically in the Plugin System Challenge, because I had used Starport before and was interested in expanding its functionality. The opportunity to expand on this tool which had helped me \"scaffold\" my journey into blockchain was too attractive to pass!\nWhat it does\nMy Plugin Service is able to list, download, build, extract, and inject configured plugins into the running Starport process. It supports both command and hook plugins. Command plugins can add custom commands under a specified parent command. Hooks can add pre/post run behavior to a specified parent command.\nHow we built it\nI broke the process into the steps the plugin would go through.\nFirst, the user would have to download the plugins. This posed the first question: where can we put them? I looked into where Starport stores its local configuration. I saw that in the .starport directory, individual files of each known chain were stored, under the local-chains directory. I decided to put plugins in a plugins directory under the configured plugin ID.\nAs a second step, the user would want to build the plugins. In correspondence with the go-plugin docs, building plugins meant compiling them to a plain binary. I decided to store these in the output directory under the configured plugin ID.\nNext, the user would want to use them alongside their project. To properly add their functionality to the Starport binary, an extraction process would have to occur. In this process, which is run before every suitable command, the plugin listens on RPC, and gives the proper information regarding the data of the plugin. The data is stored in an application-friendly data structure which allowed future processes to easily access the functionalities of the plugin. Specifically, the service uses a pattern which allows the implemented function to be called after the extraction client is killed. This involved creating a new client inside the running function with configuration from the initial client. This made the injection process much easier.\nTo inject the plugins, the service uses functions provided by the cobra package, which made a breeze to mount hooks and add a custom command.\nChallenges we ran into\nImplementing the wrong tech at first\nInitially, I set off on the goal of making a plugin system that would use the native plugin module of Go. There were some signs of progress, such as the completion of the download and build phase. Specifically, it was able to run plugins, but had extreme limitations when it came to dependency versioning. Articles such as Plugins In Go by Eli Bendersky further enforced the drawbacks I was seeing when testing this solution.\nI had seen other potential solutions, such as go-plugin by Hashicorp but had not fully understood it the first time around. At a roadblock, I decided to take a look in the Discord, and sure enough, others had the same issues, and \u0130lker made the suggestion to look into go-plugin again. With no other choice, I decided to look at it again, this time fully understanding what solutions it provided to the obvious limitations of native Go plugins. This was the first step in fixing the problem that I had assumed would present a problem.\nThinking through how to properly execute functions later\nAfter extracting the data and functions from a plugin, it's information would have to be stored somewhere, to use later. First of all, it would need a way to get custom named plugins. This required a Mapper type to be implemented in the plugin, which gets the names of the custom plugins. This would be killed, and a new plugin, with knowledge of these custom plugins, starts. It then extracts all information about each of these plugins, including the information required to run them later. This required a lot of thinking about design, specifically because of the introduction of another map to the go-plugin map of plugins seemed over the top.\nAccomplishments that we're proud of\nI am extremely proud of:\nDiligence when researching go-plugin, a project that revolved around a communication protocol I had little experience with.\nCritical thinking when debugging unexpected behaviors\nBreaking down plugin process into understandable components\nWhat we learned\nThrough the challenges I faced in this project, I have learned more about RPC's and how to use them in Go. It has given me more of an idea of how exactly they play a part in scaled systems, and especially distributed systems. RPC's are widely used in the blockchain for efficient information retrieval, and practically applying a subset of the very widely used communication protocol.\nWhat's next for Starport Plugin Service\nI hope to get this feature implemented in the next Starport release, so many can use it to expand to their individual needs with its modular nature. I plan to internalize feedback from the community to continue making improvements to the system. Future expansions that include a marketplace for plugins, that can be sold, could be possible. Expansions to this system could be possible, enabling a developer community around Starport, and ultimately aiding developers of all experience in their Starport workflow.\nGo check it out!\nhttps://github.com/lukerhoads/starport https://github.com/lukerhoads/testplugin https://github.com/lukerhoads/starport/blob/develop/starport/services/pluginsrpc/readme.md", "link": "https://devpost.com/software/starport-plugin-service", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ni was inspired to take place in the starport challenge, and specifically in the plugin system challenge, because i had used starport before and was interested in expanding its functionality. the opportunity to expand on this -----> tool !!!  which had helped me \"scaffold\" my journey into blockchain was too attractive to pass!\nwhat it does\nmy plugin service is able to list, download, build, extract, and inject configured plugins into the running starport process. it supports both command and hook plugins. command plugins can add custom commands under a specified parent command. hooks can add pre/post run behavior to a specified parent command.\nhow we built it\ni broke the process into the steps the plugin would go through.\nfirst, the user would have to download the plugins. this posed the first question: where can we put them? i looked into where starport stores its local configuration. i saw that in the .starport directory, individual files of each known chain were stored, under the local-chains directory. i decided to put plugins in a plugins directory under the configured plugin id.\nas a second step, the user would want to build the plugins. in correspondence with the go-plugin docs, building plugins meant compiling them to a plain binary. i decided to store these in the output directory under the configured plugin id.\nnext, the user would want to use them alongside their project. to properly add their functionality to the starport binary, an extraction process would have to occur. in this process, which is run before every suitable command, the plugin listens on rpc, and gives the proper information regarding the data of the plugin. the data is stored in an application-friendly data structure which allowed future processes to easily access the functionalities of the plugin. specifically, the service uses a pattern which allows the implemented function to be called after the extraction client is killed. this involved creating a new client inside the running function with configuration from the initial client. this made the injection process much easier.\nto inject the plugins, the service uses functions provided by the cobra package, which made a breeze to mount hooks and add a custom command.\nchallenges we ran into\nimplementing the wrong tech at first\ninitially, i set off on the goal of making a plugin system that would use the native plugin module of go. there were some signs of progress, such as the completion of the download and build phase. specifically, it was able to run plugins, but had extreme limitations when it came to dependency versioning. articles such as plugins in go by eli bendersky further enforced the drawbacks i was seeing when testing this solution.\ni had seen other potential solutions, such as go-plugin by hashicorp but had not fully understood it the first time around. at a roadblock, i decided to take a look in the discord, and sure enough, others had the same issues, and i\u0307lker made the suggestion to look into go-plugin again. with no other choice, i decided to look at it again, this time fully understanding what solutions it provided to the obvious limitations of native go plugins. this was the first step in fixing the problem that i had assumed would present a problem.\nthinking through how to properly execute functions later\nafter extracting the data and functions from a plugin, it's information would have to be stored somewhere, to use later. first of all, it would need a way to get custom named plugins. this required a mapper type to be implemented in the plugin, which gets the names of the custom plugins. this would be killed, and a new plugin, with knowledge of these custom plugins, starts. it then extracts all information about each of these plugins, including the information required to run them later. this required a lot of thinking about design, specifically because of the introduction of another map to the go-plugin map of plugins seemed over the top.\naccomplishments that we're proud of\ni am extremely proud of:\ndiligence when researching go-plugin, a project that revolved around a communication protocol i had little experience with.\ncritical thinking when debugging unexpected behaviors\nbreaking down plugin process into understandable components\nwhat we learned\nthrough the challenges i faced in this project, i have learned more about rpc's and how to use them in go. it has given me more of an idea of how exactly they play a part in scaled systems, and especially distributed systems. rpc's are widely used in the blockchain for efficient information retrieval, and practically applying a subset of the very widely used communication protocol.\nwhat's next for starport plugin service\ni hope to get this feature implemented in the next starport release, so many can use it to expand to their individual needs with its modular nature. i plan to internalize feedback from the community to continue making improvements to the system. future expansions that include a marketplace for plugins, that can be sold, could be possible. expansions to this system could be possible, enabling a developer community around starport, and ultimately aiding developers of all experience in their starport workflow.\ngo check it out!\nhttps://github.com/lukerhoads/starport https://github.com/lukerhoads/testplugin https://github.com/lukerhoads/starport/blob/develop/starport/services/pluginsrpc/readme.md", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59501105}, {"Unnamed: 0": 1109, "autor": "IntelliTags", "date": null, "content": "\u2728Introduction:\nIn today's world, everyone posts something online, whether it's an article, a photo, or a video, but it's difficult for your work to reach the right audience. Hashtags have evolved into a powerful tool for increasing engagement, organizing content, and connecting users across multiple social media platforms.\nDid you know, posting with correct hashtags can increase engagement up to 100% (2x) for individuals and 50% (1.5x) for brands. Our Goal Through IntelliTags is to make your work more Visible and Approachable\n\ud83d\udca1Inspiration:\nIt's Hard to Always find the best possible Hashtags for our Content, whether it is relevant or not, too specific or too vague, and sometimes one's just too lazy to research on it.\nWe came up with the idea of IntelliTags as an all-in-one solution for the top 3 kind of media i.e Text, Images and Videos.\n\ud83c\udfaeWhat it does:\nThe app is designed to intelligently analyse your content and create relevant Hashtags for sharing on Social media platforms.\nEven Segregate Data on an industry level to efficiently create tags for easy searching and sorting of Text, Images and Videos.\n\ud83d\udee0How we built it\nModzy, Kotlin, Figma, XML, AWS, Symbl.ai\n\ud83d\udea9Challenges we ran into:\nOne of the first things was the new technology i.e. Modzy, with so many new models it was a difficult learning curve.\nOne Place we find ourselves stuck was while storing our images on a remote server and pipeline that information through the Modzy models to fetch data quickly.\nA Good UI is always a little challenging, but having something which is visually aesthetic while helping the user understand what to do, can go a long way into make the app more user-friendly.\n\ud83c\udf89Accomplishments that we're proud of:\n1) We were able to benefit from multiple Modzy Models, and integrate them into an all-in-one solution for the Most Frequently used form of data, i.e. Text, Images and Videos. 2) Even as Beginners, creating a Project with Modzy help us Explore the AWS domain as well, which helped in Deploying the S3 Containers for cloud storage of our images and videos.\n\u2728What we learned\nModzy - MLOps & ModelOps\nAWS\n\u2753 What's next for IntelliTags\nOur First aim is to create a band new modzy model to enable full video transcription, that can allow the user to upload their locally saved videos on the platform and get relevant tags for that.\nNext, we aim to expand the usability on even more types of images, which can intelligently analyse what\u2019s in the picture without the need for text to generate tags!", "link": "https://devpost.com/software/intellitags", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "\u2728introduction:\nin today's world, everyone posts something online, whether it's an article, a photo, or a video, but it's difficult for your work to reach the right audience. hashtags have evolved into a powerful -----> tool !!!  for increasing engagement, organizing content, and connecting users across multiple social media platforms.\ndid you know, posting with correct hashtags can increase engagement up to 100% (2x) for individuals and 50% (1.5x) for brands. our goal through intellitags is to make your work more visible and approachable\n\ud83d\udca1inspiration:\nit's hard to always find the best possible hashtags for our content, whether it is relevant or not, too specific or too vague, and sometimes one's just too lazy to research on it.\nwe came up with the idea of intellitags as an all-in-one solution for the top 3 kind of media i.e text, images and videos.\n\ud83c\udfaewhat it does:\nthe app is designed to intelligently analyse your content and create relevant hashtags for sharing on social media platforms.\neven segregate data on an industry level to efficiently create tags for easy searching and sorting of text, images and videos.\n\ud83d\udee0how we built it\nmodzy, kotlin, figma, xml, aws, symbl.ai\n\ud83d\udea9challenges we ran into:\none of the first things was the new technology i.e. modzy, with so many new models it was a difficult learning curve.\none place we find ourselves stuck was while storing our images on a remote server and pipeline that information through the modzy models to fetch data quickly.\na good ui is always a little challenging, but having something which is visually aesthetic while helping the user understand what to do, can go a long way into make the app more user-friendly.\n\ud83c\udf89accomplishments that we're proud of:\n1) we were able to benefit from multiple modzy models, and integrate them into an all-in-one solution for the most frequently used form of data, i.e. text, images and videos. 2) even as beginners, creating a project with modzy help us explore the aws domain as well, which helped in deploying the s3 containers for cloud storage of our images and videos.\n\u2728what we learned\nmodzy - mlops & modelops\naws\n\u2753 what's next for intellitags\nour first aim is to create a band new modzy model to enable full video transcription, that can allow the user to upload their locally saved videos on the platform and get relevant tags for that.\nnext, we aim to expand the usability on even more types of images, which can intelligently analyse what\u2019s in the picture without the need for text to generate tags!", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59501109}, {"Unnamed: 0": 1110, "autor": "EdgeStats", "date": null, "content": "Inspiration\nSimple. The Theta community is awesome!! On a side note, however, one day my Theta Edge Node was effectively down - no votes/ broadcasts and no peers - without my knowledge, and the most I could do was search my wallet in the explorer and hope that randomness was responsible for no rewards over an extended period. As the reward-less period extended further I sat at the node hoping to see if I was overlooking anything. Finally I realized the node was 'submitting proof of uptime' but there were no peers and eventually I restarted it. There was also an instance where a node I could not check on as easily was down for many days without my knowing. I ultimately decided to work on a tool that would allow me to view current and past stats on my edge node's uptime. At last, I thought the community may perhaps appreciate this tool as well, so this hackathon entry is primarily so that the community knows it exists. I am also very interested in IoT, and the time series nature of the edge node uptime logs served as added motivation.\nWhat it does\nAt its core EdgeStats provides near real-time statistics on Theta Edge Node uptime votes/ broadcasts and peers. An edge node must be connected to EdgeStats to enable the real time stats, however, as the EdgeStats server gets its data directly from the EdgeStats clients.\nHow we built it\nEdgeStats comprises three components - client, server, and web ui - which work together to provide edge node uptime statistics. In particular, the EdgeStats client while on, simply watches an edge node's log file; scans the logs as they are created; filters the logs relevant to uptime and peers; sends relevant uptime data to the EdgeStats server/ database; and finally the web interface displays the edge node uptime stats.\nChallenges we ran into\nThere were a few challenges. The first was attempting to understanding how the edge node really works and the edge node log generating process. Overcoming this challenge involved sampling multiple 'log.old' log files, categorizing the generated logs, and deciding which were relevant to an edge node's uptime. The second challenge involved mapping the various filters and sub-filters of each of the generated log categories to the data objects relevant to uptime. With the logs filtered to data objects, all that remained was to send to a server/ database and build a web interface. The server was for the most part straightforward, however, it involved more work than expected as I opted to go for an embedded database so as to have as few components as possible, which will hopefully make it simpler to run a private EdgeStats setup, i.e. no database config, etc. As a logical person the interface is always a challenge, but I believe the submitted version serves its purpose well.\nAccomplishments that we're proud of\nI am generally proud that I was able to complete this project as it is something that I use personally and I hope that the community will benefit from it as well. I view my edge nodes as important to both myself and the Theta Edge Network, and this tool allows me to check the stats/ status of my edges node remotely. There was once a time where I would sit at my edge node or view the explorer and attempt to infer whether the node missed any blocks. Now I just check EdgeStats.\nWhat we learned\nI learned a lot concerning how the edge node and edge network works. Also, as this is my first hackathon and community project, I learned a great deal about packaging software and structuring code so that it is somewhat presentable. Finally, I learned more about time series databases and IoT related things - think of the edge node as an IoT sensor device \ud83e\udd14...\nWhat's next for EdgeStats\nFollowing the launch of EdgeStats, I hope to work with community artists/ designers to create Proof-of-Uptime NFTs (monthly, annually, etc.) to be awarded to edge nodes connected to EdgeStats that achieve commendable uptime statistics. I also intend to keep working on EdgeStats as the edge node and edge network expand their features and use cases - there will be more relevant edge node stats to collect that EdgeStats intends to make available to the community.", "link": "https://devpost.com/software/edgestats", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nsimple. the theta community is awesome!! on a side note, however, one day my theta edge node was effectively down - no votes/ broadcasts and no peers - without my knowledge, and the most i could do was search my wallet in the explorer and hope that randomness was responsible for no rewards over an extended period. as the reward-less period extended further i sat at the node hoping to see if i was overlooking anything. finally i realized the node was 'submitting proof of uptime' but there were no peers and eventually i restarted it. there was also an instance where a node i could not check on as easily was down for many days without my knowing. i ultimately decided to work on a -----> tool !!!  that would allow me to view current and past stats on my edge node's uptime. at last, i thought the community may perhaps appreciate this tool as well, so this hackathon entry is primarily so that the community knows it exists. i am also very interested in iot, and the time series nature of the edge node uptime logs served as added motivation.\nwhat it does\nat its core edgestats provides near real-time statistics on theta edge node uptime votes/ broadcasts and peers. an edge node must be connected to edgestats to enable the real time stats, however, as the edgestats server gets its data directly from the edgestats clients.\nhow we built it\nedgestats comprises three components - client, server, and web ui - which work together to provide edge node uptime statistics. in particular, the edgestats client while on, simply watches an edge node's log file; scans the logs as they are created; filters the logs relevant to uptime and peers; sends relevant uptime data to the edgestats server/ database; and finally the web interface displays the edge node uptime stats.\nchallenges we ran into\nthere were a few challenges. the first was attempting to understanding how the edge node really works and the edge node log generating process. overcoming this challenge involved sampling multiple 'log.old' log files, categorizing the generated logs, and deciding which were relevant to an edge node's uptime. the second challenge involved mapping the various filters and sub-filters of each of the generated log categories to the data objects relevant to uptime. with the logs filtered to data objects, all that remained was to send to a server/ database and build a web interface. the server was for the most part straightforward, however, it involved more work than expected as i opted to go for an embedded database so as to have as few components as possible, which will hopefully make it simpler to run a private edgestats setup, i.e. no database config, etc. as a logical person the interface is always a challenge, but i believe the submitted version serves its purpose well.\naccomplishments that we're proud of\ni am generally proud that i was able to complete this project as it is something that i use personally and i hope that the community will benefit from it as well. i view my edge nodes as important to both myself and the theta edge network, and this tool allows me to check the stats/ status of my edges node remotely. there was once a time where i would sit at my edge node or view the explorer and attempt to infer whether the node missed any blocks. now i just check edgestats.\nwhat we learned\ni learned a lot concerning how the edge node and edge network works. also, as this is my first hackathon and community project, i learned a great deal about packaging software and structuring code so that it is somewhat presentable. finally, i learned more about time series databases and iot related things - think of the edge node as an iot sensor device \ud83e\udd14...\nwhat's next for edgestats\nfollowing the launch of edgestats, i hope to work with community artists/ designers to create proof-of-uptime nfts (monthly, annually, etc.) to be awarded to edge nodes connected to edgestats that achieve commendable uptime statistics. i also intend to keep working on edgestats as the edge node and edge network expand their features and use cases - there will be more relevant edge node stats to collect that edgestats intends to make available to the community.", "sortedWord": "None", "removed": "Nan", "score": 14, "comments": 1, "media": null, "medialink": null, "identifyer": 59501110}, {"Unnamed: 0": 1122, "autor": "asdf-starport", "date": null, "content": "Inspiration\nI'm very new to Cosmos SDK and Starport, and I experimented with different tutorials. Every tutorial used a different Starport version, and also, for the hackatom, I was using the latest version of Starport, which was different from the previous ones.\nI was struggling to keep up with the different versions!\nIn the past, I have used asdf to solve these problems, and I have decided to create a plugin for asdf for Starport to simplify my life.\nWhat is asdf?\nasdf is a CLI tool that can manage multiple language runtime versions on a per-project basis. It is like gvm, nvm, rbenv & pyenv (and more) all in one! Simply install your language's plugin!\n\ud83c\udfad Read the ballad of asdf here\nWhat it does\nThe plugin I have developed for asdf allows to install and switch between different Starport versions quickly.\nTo install asdf, please refer to this guide\n# Add the plugin to asdf\nasdf plugin add starport\n# Show all installable versions\nasdf list-all starport\n# Install the latest version\nasdf install starport latest\nSet a version\nasdf performs a version lookup of a tool in all .tool-versions files from the current working directory up to the $HOME directory. The lookup occurs just in time when you execute a tool that asdf manages.\nTL;DR keep a .tool-version in your folder with starport <VERSION> to quickly switch to that specific version of Starport\nGlobal\nGlobal defaults are managed in $HOME/.tool-versions. Set a global version with:\nasdf global starport latest\n$HOME/.tool-versions will then look like:\nstarport 0.18.6\nLocal\nLocal versions are defined in the $PWD/.tool-versions file (your current working directory). Usually, this will be the Git repository for a project. When in your desired directory, execute:\nasdf local starport 0.18.0\n$PWD/.tool-versions will then look like:\nstarport 0.18.0\nHow we built it\nasdf comes with an extensible plugin system.\nI have created a plugin for asdf following the requested process in this repository and merged it in the official asdf-plugin repository.\nAccomplishments that we're proud of\nThe PR I made to the asdf-plugin repository it's already merged, and anyone can use this plugin immediately!\nGo asdf! :tada:\nWhat we learned\nCosmos SDK and Starport are amazing, and they are actively maintained. It could be challenging to keep up with the pace.\nConfiguration of Cosmos SDK and Starport shouldn't be intimidating (especially to beginners).\nI hope that this contribution will save some headaches for everyone using it.\nWhat's next for asdf-starport\nI will keep the plugin updated to ensure that it's usable by the community.", "link": "https://devpost.com/software/asdf-starport", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ni'm very new to cosmos sdk and starport, and i experimented with different tutorials. every tutorial used a different starport version, and also, for the hackatom, i was using the latest version of starport, which was different from the previous ones.\ni was struggling to keep up with the different versions!\nin the past, i have used asdf to solve these problems, and i have decided to create a plugin for asdf for starport to simplify my life.\nwhat is asdf?\nasdf is a cli -----> tool !!!  that can manage multiple language runtime versions on a per-project basis. it is like gvm, nvm, rbenv & pyenv (and more) all in one! simply install your language's plugin!\n\ud83c\udfad read the ballad of asdf here\nwhat it does\nthe plugin i have developed for asdf allows to install and switch between different starport versions quickly.\nto install asdf, please refer to this guide\n# add the plugin to asdf\nasdf plugin add starport\n# show all installable versions\nasdf list-all starport\n# install the latest version\nasdf install starport latest\nset a version\nasdf performs a version lookup of a tool in all .tool-versions files from the current working directory up to the $home directory. the lookup occurs just in time when you execute a tool that asdf manages.\ntl;dr keep a .tool-version in your folder with starport <version> to quickly switch to that specific version of starport\nglobal\nglobal defaults are managed in $home/.tool-versions. set a global version with:\nasdf global starport latest\n$home/.tool-versions will then look like:\nstarport 0.18.6\nlocal\nlocal versions are defined in the $pwd/.tool-versions file (your current working directory). usually, this will be the git repository for a project. when in your desired directory, execute:\nasdf local starport 0.18.0\n$pwd/.tool-versions will then look like:\nstarport 0.18.0\nhow we built it\nasdf comes with an extensible plugin system.\ni have created a plugin for asdf following the requested process in this repository and merged it in the official asdf-plugin repository.\naccomplishments that we're proud of\nthe pr i made to the asdf-plugin repository it's already merged, and anyone can use this plugin immediately!\ngo asdf! :tada:\nwhat we learned\ncosmos sdk and starport are amazing, and they are actively maintained. it could be challenging to keep up with the pace.\nconfiguration of cosmos sdk and starport shouldn't be intimidating (especially to beginners).\ni hope that this contribution will save some headaches for everyone using it.\nwhat's next for asdf-starport\ni will keep the plugin updated to ensure that it's usable by the community.", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59501122}, {"Unnamed: 0": 1135, "autor": "StarryBot", "date": null, "content": "Inspiration\nThe Cosmos ecosystem needs a bot! We thought that it would be fun to make a helpful bot that detects Cosmos ecosystem tokens and NFTs, then promotes users with those tokens or NFTs in their wallets to different roles in Discord. We've already heard feedback from various communities and DAOs building with Cosmos that this would be an extremely helpful (and currently nonexistent) tool!\nStarrybot is a project created by me (Amber Case), Anselm Hook and Mike Purvis as a way of getting into the Cosmos ecosystem. We're all part of a little dinner group called Dinner DAO, and after one of our group dinners, we got together to start building it!\nI\u2019m interested in DAOs, regenerative ecosystems (currently working on a Juno validator that donates profit to Amber Initiative). I\u2019ve built a number of large-scale projects, including a developer platform and tools eventually used by over 5 million people.\nAnselm Hook has created lots of small conferences, like WhereCamp, and has been a hacker on many art projects, community projects and super large projects since he grew up working out of a tiny computer shop run by his dad in Alberta.\nMike Purvis currently works as a dev evangelist at NEAR, but got hooked on the Cosmos ecosystem when I showed him what was being built. He has great energy and enthusiasm for making tools that help emerging ecosystems and people.\nYou can find out more about the project by visiting the Discord and saying hello: https://discord.gg/MCrnh73EDn\nWhat it does\nThe project currently has a test website, support for cw20 tokens, and a running bot that can be added to a Discord Server. Next on our roadmap is cw721 support, testing of the user interface, and website updates. After that, we\u2019ll write some basic documentation before testing the bot with the DAO/DAO server. Then we\u2019ll go through a few revisions and bug fixes before unleashing the bot more widely.\nDAOs and DAO tooling groups like DAO DAO would like to accept members into their Discords that have specific governance tokens, but there is no solution for the Cosmos ecosystem at this time.\nToken-gated bots are helpful for DAOs because they allow Discords to verify DAO members by membership NFTs or tokens\nToken gates can help verify trusted members of the community via checking wallets. Discord admins can create levels of membership depending on how specific tokens or number of tokens, giving them more flexibility in their communities.\nCollabland is the closest to what\u2019s out there for token-gating, but it does not support the Cosmos ecosystem or follow along with the theme of the Cosmos ecosystem.\nHow we built it\nWe have four parts to StarryBot, all deployed to Render:\nStatic welcome website, inviting folks to add StarryBot to their Discord server.\nStatic verification subdomain website, where folks use Keplr to sign a message that goes to a backend. (https://github.com/starryzone/cosmos-webapp)\nDiscord bot + ExpressJS backend. The backend receives the offline signature, verifies it, and uses its access to the Discord token to add roles where needed. (https://github.com/starryzone/starrybot-discord)\nDatabase \u2014 postgresql database with two tables tracking the conditions to add roles to users and members who need to sign custom Carl Sagan jibberish, matching them to their unique session token during verification.\nLibraries used from the Cosmos ecosystem:\n@cosmjs/amino\n@cosmjs/crypto\n@cosmjs/encoding\n@cosmjs/stargate\nThey were great.\nOverview\nHere is an overview of Starrybot\u2019s two main components: the Keplr offline signing workflow and the Discord bot.\nKepler Signing\nThe Keplr signing used the browser window\u2019s keplr object and the offline signer available with it.\nUnlike similar Discord gating bots, instead of signing a message like \u201csalt0312219\u201d we decided to have each user sign a unique Carl Sagan ipsum phrase.\nOnce signed, the signature and other items are sent to a backend where the signature is confirmed. After signature confirmation, two RPC endpoints (Juno and Osmosis) are called, checking for native token balances of juno and uosmo. For the purposes of the demonstration of this app, we check for these two native tokens, although cw20 and cw721 are a trivial update.\nWe also use Bech32 to decode and encode the address so the user can see both balances from their one signed message, which abides by ADR36 to the best of its ability.\nDiscord bot\nThe Discord bot utilizes a fairly new system: application commands. These behave like \u201cslash commands\u201d folks may be familiar with in Slack. This feature was announced in March of this year, and changes the way the UX can work for Discord bots. The Discord bot uses the discordjs package to accomplish its objectives. It creates two roles by default:\nosmo-hodler\njuno-hodler\nWhen the Discord bot is first added, an event is fired, creating two rows in a database table. This database table allows StarryBot to scale and be far more customizable than a quick demo might show. Having a database allows StarryBot to behave in an \u201cif-this-than-that\u201d manner, storing token conditions and what role to assign a user. By default, if a user holds JUNO or OSMO, those roles will be applied respectively.\nFinally, the Discord bot also ushers the admin through the onboarding experience, showing how to get slash commands working properly. Once set up, the slash command for users is /starry-join. This will reply to their slash command in a private message in that same channel, only visible to them.\nChallenges we ran into\nAfter discovering the ADR36 standard, we ran into the issue where at least 1 gas is required to sign using Keplr. (Again, an issue was created in that repo, including a couple of other minor fixes.)\nWhile we were building the Discord bot, a timely thread appeared on Twitter revealing that there are security concerns to a common approach. When a user executed a traditional command (like !join) we would send them a direct message. Note these tweets:\nSecurity concerns: https://twitter.com/nix_eth/status/1465741803185971201\nResponse from CollabLand, a similar Discord bot: https://twitter.com/Collab_Land_/status/1465743609571934209\nGeneral fatigue from Ric Burton about hunting down a direct message on Discord: https://twitter.com/ricburton/status/1465767692342759425\nThe solution: we decided we needed to pivot to avoid using direct messages. Without getting too much into it, the feature of having a \u201cprivate message\u201d in a public channel only works with interactions, and it made the most sense to move to using slash commands and abandon traditional commands.\nLastly, while it\u2019s common to add a Discord bot with one authorization step, to use application commands (like /starry-join) there is an additional step needed once the bot is added. It seems that Discord wants to give server admins more granular permissions, even if the bot is granted administrator privileges. After a couple hours of confirming this need, the team had to come up with a workflow and set of instructions when the bot joins. And also a system where the user can confirm they\u2019ve completed the instructions, and if they haven\u2019t, kindly remind them again with a dash of sass. :)\nThe last challenge was figuring out how to sign an offline message. Simon Warta from Confio had a great draft pull request for signing ADR36 messages, but in the end there was some code using Amino signing and verification that was gleaned from unit test code.\nAccomplishments that we're proud of\nWe used the new Slash commands for Discord, which are much more secure!\nWe learned a LOT about the Cosmos ecosystem, as well as emerging projects like Stargaze and Juno!\nWhat we learned\nWe learned that even silly-sounding DAOs like DinnerDAO can lead to amazing hackathon friends.\nWe learned that given the new updates to Discord, there is an opportunity to strengthen security and invent a brand new workflow that hasn\u2019t been done yet.\nWe learned that folks from Stargate and Juno are incredibly responsive to questions. We also received replies in GitHub to questions and saw tickets move along. This ecosystem is attentive and roaring. We\u2019re excited to be a part of it!\nWhat's next for Starrybot\nFor the next step to use cw20 and cw721, we\u2019ll be using logic straight from an example repo Jake Hartnell graciously created. The ability to use cw20 and cw721 tokens for token gating is within arm\u2019s reach.\nOn the Discord front, we\u2019re learning how to reinvent the UX for onboarding with their new features. We\u2019ll want to make sure the app is able to clean up after itself, and help admins of Discord servers if they run into edges cases. (For instance, if you kick the bot and read it in a certain way, the StarryBot role needs to be dragged up in the settings. See this, this, this, this, and this.)\nTo truly make an \u201cif-this-than-that\u201d system of token ownership and the roles that should be applied, we\u2019ll want to make it easy for admins to customize these rules. We\u2019ll explore options for more slash commands, as well as an OAuth2 approach. An amazing feature might be to have a settings website that allows Discord admins to log in and modify their database settings using simple modern form submissions.\nWe already have a few beta testers from a couple of DAO Discords lined up for our token support handling, and we plan to apply for several ecosystem grants to keep this project going. Our vision is to build Starrybot into a core tool for the entire Cosmos ecosystem!", "link": "https://devpost.com/software/starrybot", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nthe cosmos ecosystem needs a bot! we thought that it would be fun to make a helpful bot that detects cosmos ecosystem tokens and nfts, then promotes users with those tokens or nfts in their wallets to different roles in discord. we've already heard feedback from various communities and daos building with cosmos that this would be an extremely helpful (and currently nonexistent) -----> tool !!! !\nstarrybot is a project created by me (amber case), anselm hook and mike purvis as a way of getting into the cosmos ecosystem. we're all part of a little dinner group called dinner dao, and after one of our group dinners, we got together to start building it!\ni\u2019m interested in daos, regenerative ecosystems (currently working on a juno validator that donates profit to amber initiative). i\u2019ve built a number of large-scale projects, including a developer platform and tools eventually used by over 5 million people.\nanselm hook has created lots of small conferences, like wherecamp, and has been a hacker on many art projects, community projects and super large projects since he grew up working out of a tiny computer shop run by his dad in alberta.\nmike purvis currently works as a dev evangelist at near, but got hooked on the cosmos ecosystem when i showed him what was being built. he has great energy and enthusiasm for making tools that help emerging ecosystems and people.\nyou can find out more about the project by visiting the discord and saying hello: https://discord.gg/mcrnh73edn\nwhat it does\nthe project currently has a test website, support for cw20 tokens, and a running bot that can be added to a discord server. next on our roadmap is cw721 support, testing of the user interface, and website updates. after that, we\u2019ll write some basic documentation before testing the bot with the dao/dao server. then we\u2019ll go through a few revisions and bug fixes before unleashing the bot more widely.\ndaos and dao tooling groups like dao dao would like to accept members into their discords that have specific governance tokens, but there is no solution for the cosmos ecosystem at this time.\ntoken-gated bots are helpful for daos because they allow discords to verify dao members by membership nfts or tokens\ntoken gates can help verify trusted members of the community via checking wallets. discord admins can create levels of membership depending on how specific tokens or number of tokens, giving them more flexibility in their communities.\ncollabland is the closest to what\u2019s out there for token-gating, but it does not support the cosmos ecosystem or follow along with the theme of the cosmos ecosystem.\nhow we built it\nwe have four parts to starrybot, all deployed to render:\nstatic welcome website, inviting folks to add starrybot to their discord server.\nstatic verification subdomain website, where folks use keplr to sign a message that goes to a backend. (https://github.com/starryzone/cosmos-webapp)\ndiscord bot + expressjs backend. the backend receives the offline signature, verifies it, and uses its access to the discord token to add roles where needed. (https://github.com/starryzone/starrybot-discord)\ndatabase \u2014 postgresql database with two tables tracking the conditions to add roles to users and members who need to sign custom carl sagan jibberish, matching them to their unique session token during verification.\nlibraries used from the cosmos ecosystem:\n@cosmjs/amino\n@cosmjs/crypto\n@cosmjs/encoding\n@cosmjs/stargate\nthey were great.\noverview\nhere is an overview of starrybot\u2019s two main components: the keplr offline signing workflow and the discord bot.\nkepler signing\nthe keplr signing used the browser window\u2019s keplr object and the offline signer available with it.\nunlike similar discord gating bots, instead of signing a message like \u201csalt0312219\u201d we decided to have each user sign a unique carl sagan ipsum phrase.\nonce signed, the signature and other items are sent to a backend where the signature is confirmed. after signature confirmation, two rpc endpoints (juno and osmosis) are called, checking for native token balances of juno and uosmo. for the purposes of the demonstration of this app, we check for these two native tokens, although cw20 and cw721 are a trivial update.\nwe also use bech32 to decode and encode the address so the user can see both balances from their one signed message, which abides by adr36 to the best of its ability.\ndiscord bot\nthe discord bot utilizes a fairly new system: application commands. these behave like \u201cslash commands\u201d folks may be familiar with in slack. this feature was announced in march of this year, and changes the way the ux can work for discord bots. the discord bot uses the discordjs package to accomplish its objectives. it creates two roles by default:\nosmo-hodler\njuno-hodler\nwhen the discord bot is first added, an event is fired, creating two rows in a database table. this database table allows starrybot to scale and be far more customizable than a quick demo might show. having a database allows starrybot to behave in an \u201cif-this-than-that\u201d manner, storing token conditions and what role to assign a user. by default, if a user holds juno or osmo, those roles will be applied respectively.\nfinally, the discord bot also ushers the admin through the onboarding experience, showing how to get slash commands working properly. once set up, the slash command for users is /starry-join. this will reply to their slash command in a private message in that same channel, only visible to them.\nchallenges we ran into\nafter discovering the adr36 standard, we ran into the issue where at least 1 gas is required to sign using keplr. (again, an issue was created in that repo, including a couple of other minor fixes.)\nwhile we were building the discord bot, a timely thread appeared on twitter revealing that there are security concerns to a common approach. when a user executed a traditional command (like !join) we would send them a direct message. note these tweets:\nsecurity concerns: https://twitter.com/nix_eth/status/1465741803185971201\nresponse from collabland, a similar discord bot: https://twitter.com/collab_land_/status/1465743609571934209\ngeneral fatigue from ric burton about hunting down a direct message on discord: https://twitter.com/ricburton/status/1465767692342759425\nthe solution: we decided we needed to pivot to avoid using direct messages. without getting too much into it, the feature of having a \u201cprivate message\u201d in a public channel only works with interactions, and it made the most sense to move to using slash commands and abandon traditional commands.\nlastly, while it\u2019s common to add a discord bot with one authorization step, to use application commands (like /starry-join) there is an additional step needed once the bot is added. it seems that discord wants to give server admins more granular permissions, even if the bot is granted administrator privileges. after a couple hours of confirming this need, the team had to come up with a workflow and set of instructions when the bot joins. and also a system where the user can confirm they\u2019ve completed the instructions, and if they haven\u2019t, kindly remind them again with a dash of sass. :)\nthe last challenge was figuring out how to sign an offline message. simon warta from confio had a great draft pull request for signing adr36 messages, but in the end there was some code using amino signing and verification that was gleaned from unit test code.\naccomplishments that we're proud of\nwe used the new slash commands for discord, which are much more secure!\nwe learned a lot about the cosmos ecosystem, as well as emerging projects like stargaze and juno!\nwhat we learned\nwe learned that even silly-sounding daos like dinnerdao can lead to amazing hackathon friends.\nwe learned that given the new updates to discord, there is an opportunity to strengthen security and invent a brand new workflow that hasn\u2019t been done yet.\nwe learned that folks from stargate and juno are incredibly responsive to questions. we also received replies in github to questions and saw tickets move along. this ecosystem is attentive and roaring. we\u2019re excited to be a part of it!\nwhat's next for starrybot\nfor the next step to use cw20 and cw721, we\u2019ll be using logic straight from an example repo jake hartnell graciously created. the ability to use cw20 and cw721 tokens for token gating is within arm\u2019s reach.\non the discord front, we\u2019re learning how to reinvent the ux for onboarding with their new features. we\u2019ll want to make sure the app is able to clean up after itself, and help admins of discord servers if they run into edges cases. (for instance, if you kick the bot and read it in a certain way, the starrybot role needs to be dragged up in the settings. see this, this, this, this, and this.)\nto truly make an \u201cif-this-than-that\u201d system of token ownership and the roles that should be applied, we\u2019ll want to make it easy for admins to customize these rules. we\u2019ll explore options for more slash commands, as well as an oauth2 approach. an amazing feature might be to have a settings website that allows discord admins to log in and modify their database settings using simple modern form submissions.\nwe already have a few beta testers from a couple of dao discords lined up for our token support handling, and we plan to apply for several ecosystem grants to keep this project going. our vision is to build starrybot into a core tool for the entire cosmos ecosystem!", "sortedWord": "None", "removed": "Nan", "score": 11, "comments": 1, "media": null, "medialink": null, "identifyer": 59501135}, {"Unnamed: 0": 1166, "autor": "[Tools and Apps] Base Code OG", "date": null, "content": "Inspiration:\nWhile creating the BetaBirds for RMRK2 we thought it would be useful to have a tool to help you create the Base code. One that would make it less prone to errors and faster than typing the entire code yourself since RMRK2 doesn't have a UI (yet).\nWhat it does:\nIt takes the user input and converts it into the RMRK2 format which can then be saved as json or url-encoded into a .txt file. A list of Base parts can be saved too in preparation for the Resadd. Since the program only checks whether there is input but does not check if this input is correct a basic understanding of how minting works for RMRK2 is required to make sure the input format is correct so that the resulting Base code is functional.\nHow we built it:\nWith blood, sweat and tears\nChallenges we ran into:\nBuilding something useable\nAccomplishments that we're proud of:\nEven though it was built a bit too late for our Beta Bird project it should make future projects a bit easier\nWhat we learned:\nCreating a RMRK2 Base is less error prone with a program but still a challenge\nWhat's next for Base Code OG:\nWe might add Themes and a list where you can see the names of the parts that have already been added to the Base in the future", "link": "https://devpost.com/software/base-code-og", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration:\nwhile creating the betabirds for rmrk2 we thought it would be useful to have a -----> tool !!!  to help you create the base code. one that would make it less prone to errors and faster than typing the entire code yourself since rmrk2 doesn't have a ui (yet).\nwhat it does:\nit takes the user input and converts it into the rmrk2 format which can then be saved as json or url-encoded into a .txt file. a list of base parts can be saved too in preparation for the resadd. since the program only checks whether there is input but does not check if this input is correct a basic understanding of how minting works for rmrk2 is required to make sure the input format is correct so that the resulting base code is functional.\nhow we built it:\nwith blood, sweat and tears\nchallenges we ran into:\nbuilding something useable\naccomplishments that we're proud of:\neven though it was built a bit too late for our beta bird project it should make future projects a bit easier\nwhat we learned:\ncreating a rmrk2 base is less error prone with a program but still a challenge\nwhat's next for base code og:\nwe might add themes and a list where you can see the names of the parts that have already been added to the base in the future", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59501166}, {"Unnamed: 0": 1179, "autor": "Clover Analytics", "date": null, "content": "Inspiration\nI like any kinds of monitoring and analytical tools, because they give the ability to see the state of the observed object (blockchain node in our case) and make predictions about its behaviour stability and health in the future. I know that geth provides interface to get a bunch of metrics to store them in database and/or visualize on dashboards . And I was wondered that cloverchain node hasn't implement such functional yet.\nWhat it does\nCurrently I've made just a simple prototype which demonstrate how the monitoring tool might look like. It doesn't use real datasets but uses dummy ones.\nHow we built it\nIt wasn't very difficult. But I had to look through a lot of examples how the Dash framework could be used to draw charts, tables, etc.\nAccomplishments that we're proud of\nI hope this idea will be start point to introduce the monitoring tools into cloverchain infrastructure.\nWhat's next for Clover Analytics\nIt isn't standalone project. At first the node should get the facility to collect various metrics and provide them via RPC or websocket for example or save into database. Then the UI have to be globally reworked or might be better to use Netdata or Prometheus or something else. And another one but not last idea is to use collected datasets to predict average time of processing transactions, average time of the waiting transaction in the queue or something else.", "link": "https://devpost.com/software/clover-analytics", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ni like any kinds of monitoring and analytical tools, because they give the ability to see the state of the observed object (blockchain node in our case) and make predictions about its behaviour stability and health in the future. i know that geth provides interface to get a bunch of metrics to store them in database and/or visualize on dashboards . and i was wondered that cloverchain node hasn't implement such functional yet.\nwhat it does\ncurrently i've made just a simple prototype which demonstrate how the monitoring -----> tool !!!  might look like. it doesn't use real datasets but uses dummy ones.\nhow we built it\nit wasn't very difficult. but i had to look through a lot of examples how the dash framework could be used to draw charts, tables, etc.\naccomplishments that we're proud of\ni hope this idea will be start point to introduce the monitoring tools into cloverchain infrastructure.\nwhat's next for clover analytics\nit isn't standalone project. at first the node should get the facility to collect various metrics and provide them via rpc or websocket for example or save into database. then the ui have to be globally reworked or might be better to use netdata or prometheus or something else. and another one but not last idea is to use collected datasets to predict average time of processing transactions, average time of the waiting transaction in the queue or something else.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59501179}, {"Unnamed: 0": 1208, "autor": "Train Smart", "date": null, "content": "Inspiration\nWe know that many people suffer from health problems during the pandemic due to lack of exercise. We wanted to create a tool for them, that would make it easier to interact with coaches and create their daily exercise routine. It is really hard for couches too, because they can not do their job. Our app is giving a chance for couches to continue training with their customers online.\nWhat it does\nThe main reason why we decided to choose this topic is the lack of opportunities for coaches, clubs and sport associations to show their courses and training sessions. We haven't forgotten about their clients either, to whom we provide opportunities to sign up for their courses. Especially now, during the pandemic is our application beneficial.\nHow we built it\nWe used the most modern technologies and frameworks to create Single Page Application for our customers to enjoy. Our two good friends, Vue and Bootstrap, decided to help us with building front-end of our application and Firebase let us use her space for storing our data.\nChallenges we ran into\nThe most challenging thing for us was to think out of the box and bring the best solution we could offer.\nAccomplishments that we're proud of\nWe wanted to create an idea that would help a lot of people in the time of the pandemic. We can only say one thing, mission accomplished!\nWhat we learned\nWith this project, we learned a valuable lesson. That sometimes less is more. AND ..... Teamwork makes dreams work!\nWhat's next for Train Smart\nWe would like to implement more features for our couches and users to give them the best experience with our application. Features like an interactive map with couches and sport centers to show our users or implementing video meetings for courses would be a great addition.", "link": "https://devpost.com/software/smart-med", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe know that many people suffer from health problems during the pandemic due to lack of exercise. we wanted to create a -----> tool !!!  for them, that would make it easier to interact with coaches and create their daily exercise routine. it is really hard for couches too, because they can not do their job. our app is giving a chance for couches to continue training with their customers online.\nwhat it does\nthe main reason why we decided to choose this topic is the lack of opportunities for coaches, clubs and sport associations to show their courses and training sessions. we haven't forgotten about their clients either, to whom we provide opportunities to sign up for their courses. especially now, during the pandemic is our application beneficial.\nhow we built it\nwe used the most modern technologies and frameworks to create single page application for our customers to enjoy. our two good friends, vue and bootstrap, decided to help us with building front-end of our application and firebase let us use her space for storing our data.\nchallenges we ran into\nthe most challenging thing for us was to think out of the box and bring the best solution we could offer.\naccomplishments that we're proud of\nwe wanted to create an idea that would help a lot of people in the time of the pandemic. we can only say one thing, mission accomplished!\nwhat we learned\nwith this project, we learned a valuable lesson. that sometimes less is more. and ..... teamwork makes dreams work!\nwhat's next for train smart\nwe would like to implement more features for our couches and users to give them the best experience with our application. features like an interactive map with couches and sport centers to show our users or implementing video meetings for courses would be a great addition.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59501208}, {"Unnamed: 0": 1228, "autor": "Feelings", "date": null, "content": "Inspiration\nWe know there are about 465 Million people diagnosed with diabetes in the World. We want to help them understand how to apply their insulin shots. We believe, Feelings, our VR app technology can help them understand their first steps.\nWhat it does\nSeeing is feeling. Users will see how to inject themselves with insulin showing that is not so traumatized as thought.\nHow we built it\nUsing stereo kit, we built a POC with intuitive instructions and a experience that let track your hands and interact with the space.\nWe added an intuitive UI with hand menus.\nWe imlemented Animations with collisions.\nWe implemented interactions with models.\nChallenges we ran into\nMaking the actual action of holding a needle. Implementing Animations and Shaders From the perspective of a unity developer is a little bit hard to adapt.\nAccomplishments that we're proud of\nWe created a POC that will help patients and doctors in its diagnoses.\nWhat we learned\nWe found in stereokit a different tool with great capabilities and Open Source that let developers model and create products in minutes, a cross platform framework that runs with or without device that let the business iterate and test faster.\nWhat's next for Feelings\nWe believe that this kind of experience will break the barrier patient and diabetes diagnostic and help patients lose their fear and assume its diagnostic with no fear with the certancy they gather all the information for their health care in a more practical and intuitive way", "link": "https://devpost.com/software/feelings-c30jtf", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe know there are about 465 million people diagnosed with diabetes in the world. we want to help them understand how to apply their insulin shots. we believe, feelings, our vr app technology can help them understand their first steps.\nwhat it does\nseeing is feeling. users will see how to inject themselves with insulin showing that is not so traumatized as thought.\nhow we built it\nusing stereo kit, we built a poc with intuitive instructions and a experience that let track your hands and interact with the space.\nwe added an intuitive ui with hand menus.\nwe imlemented animations with collisions.\nwe implemented interactions with models.\nchallenges we ran into\nmaking the actual action of holding a needle. implementing animations and shaders from the perspective of a unity developer is a little bit hard to adapt.\naccomplishments that we're proud of\nwe created a poc that will help patients and doctors in its diagnoses.\nwhat we learned\nwe found in stereokit a different -----> tool !!!  with great capabilities and open source that let developers model and create products in minutes, a cross platform framework that runs with or without device that let the business iterate and test faster.\nwhat's next for feelings\nwe believe that this kind of experience will break the barrier patient and diabetes diagnostic and help patients lose their fear and assume its diagnostic with no fear with the certancy they gather all the information for their health care in a more practical and intuitive way", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 1, "media": null, "medialink": null, "identifyer": 59501228}, {"Unnamed: 0": 1236, "autor": "Chromatograph", "date": null, "content": "Inspiration\nTo build a tool that enables DAOs to easily diversify their treasury with eachother.\nWhat it does\nIt exchanges superfluid streams and opens a stream back to participant.\nHow we built it\nIt is built with chainlink keepers and superfluid\nChallenges we ran into\nWe had issues of managing outgoing streams from our smart contract because of how superfluid works\nAccomplishments that we're proud of\nWhat we learned\nWe learnt a lot on how chainklink keepers and superfluid streams work\nWhat's next for Chromatograph", "link": "https://devpost.com/software/chromatograph", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nto build a -----> tool !!!  that enables daos to easily diversify their treasury with eachother.\nwhat it does\nit exchanges superfluid streams and opens a stream back to participant.\nhow we built it\nit is built with chainlink keepers and superfluid\nchallenges we ran into\nwe had issues of managing outgoing streams from our smart contract because of how superfluid works\naccomplishments that we're proud of\nwhat we learned\nwe learnt a lot on how chainklink keepers and superfluid streams work\nwhat's next for chromatograph", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59501236}, {"Unnamed: 0": 1256, "autor": "Apprecio.Finance", "date": null, "content": "Inspiration\nInspiration for Apprecio.Finance (AF) came from August 5, 2021, 1729 Newsletter, sponsored by Bajali Srinivasan (@balajis):\n\u201cInflation is a monetary phenomenon, a function of money printing. But it is also in part a social phenomenon, a function of mass psychology. If enough of the right people believe that inflation is going to happen, it will. As such, when inflation is happening, there is often a push to censor discussion of inflation itself, under the grounds that discussing the problem actually causes it in the first place. That is exactly what happened in Argentina and Venezuela over the last decade [\u2026] And that is why the world needs a global, decentralized, censorship-resistant inflation dashboard.\nWhy do we need an inflation dashboard? Because trillions of dollars have been printed over the last year \u2013 and it's not just dollars. Many countries besides the US have been printing fiat like mad to fund the response to COVID. Figures that shocked and stunned ten years ago ($787B for the bailout!) no longer warrant a headline today, though they may well result in headlines tomorrow.\nIf inflation is a government-caused problem, we can't necessarily rely on government statistics like the CPI to diagnose it or remediate it. Indeed, in places with high inflation, censorship and denial is the rule rather than the exception.\u201d\nWhat it does\nAF is a decentralized inflation dashboard: it estimates inflation from various price sources and makes it available through a front-end and on-chain trough Chainlink.\nIts main objective is to resist censorship and provide an alternative to centralized measures such as those that come from Central Banks and national statistical agencies. AF implements decentralization at three different times in its design. (1) The price data are obtained from more than 1,000 different sources (in the Mexican case), independent of the government, which prevents the failure or manipulation of any source from significantly affecting the inflation calculation. (2) Data is automatically stored off-chain using the Interplanetary File System (IPFS), for data to be distributed across multiple off-chain nodes, and Filecoin, for data permanence and availability at any moment. (3) Aggregate prices and inflation rate are stored on-chain, Kovan testnet for AF v0.2, using Chainlink. The last two steps would also allow the data to be cryptographically signed twice.\nHow we built it\nAF Design\nTo achieve the highest possible resistance to censorship, AF implements decentralization at three different times in its design. The following figure captures them.\nData Recollection\nThis step is crucial to maintain decentralization. If manipulation of the data source by a third party can significantly alter AF's inflation rate measurement, then the objective of censorship resistance would be compromised.\nThus, for the Mexican case, our first measure, the data are obtained from two different types of sources.\nSmall sellers. Through the Apprecio web and mobile applications.\nMedium and large sellers. Through web scrapping of online stores.\nData Recollection: Small vendors (tienditas, in Spanish)\nAF is unique in that it takes advantage of a data source such as that from small vendors.\nIn Latin America, these sellers are frequent. Around 700 thousand tienditas exist in Mexico and generate 1% of GDP. Not taking this data source into account would greatly skew the inflation estimate. However, obtaining data from these sources is a grassroots activity. There is no database that tracks prices in these relevant units.\nWe take advantage of the work of the Mexican Apprecio fintech with more than 8 thousand small vendors as clients throughout the Mexican Republic. Among other financial and strategic services, Apprecio helps small suppliers to calculate the ideal prices for their products. To do this, they require units to charge the prices at which they offer their products. AF then uses these data to calculate inflation.\nThis source is decentralized. The data, for November 2021, comes from at least 1,000 different units and the number is growing. The fact that a unit does not report accurate data would not affect the general median price of the good and therefore any calculations made with the prices.\nData Recollection: Medium to Big Vendors\nPrices from Walmart, La Comer, and Soriana are included to further decentralize data sources. AF scrapes their online stores. For Mexico, it is planned to include at least two additional sources: Costco and Sam's Club.\nThese sources provide the assurance of the constant presence of at least one price during any period. The main challenge comes from maintaining scrappers that can repeatedly beat the web pages. The 1729 Newsletter mentions several issues related to reliance on scrappers for pricing data, including dealing with automated countermeasures, maintaining the web crawler, and analyzing hard-to-determine units. AF met these problems successfully.\nDecentralized storage off-chain\nFor AF to be a successful tool for openly measuring inflation, it must be available for audit (1) at any time and (2) as cheaply as possible.\nStorage has at least two requirements for AF:\nRaw price data must be auditable. Currently, raw data storage is done off-chain due to gas prices. Possibly, the next step would be for AF to move to an Ethereum L2 and be able to store raw data on the chain.\nThe data must be stored in a decentralized way. A central server means the consolidation of control over the data. Even off the chain, due to point 1, this should be achieved.\nAF achieves both requirements by storing raw price data off-chain using IFPS. The entire price data set is divided into smaller chunks, cryptographic hashed, and assigned a unique fingerprint called a Content Identifier (CID). The dataset would be stored on at least one node, a local server anywhere in the world. By using the CID, other nodes can request the data and will cache the data themselves locally. This would increase the decentralization of the data set off the chain.\nThe data must always be online and available for direct request.\n\u201cWhile IPFS guarantees that any content on the network is discoverable, it doesn't guarantee that any content is persistently available. This is where Filecoin comes in [\u2026] a decentralized storage network in which storage providers rent their storage space to clients.\u201d (IPFS documentation).\nAF uses Filecoin so that price data is always available while maintaining its decentralization. The dataset would be anchored in at least one node.\nThe Web3.Storage solution enables AF (1) to automatically store the dataset in IPFS and back it up with Filecoin; and (2) replicate pricing data across a network of storage providers and verify its integrity.\nThis solution allows data to be persistent, available, and decentralized even off-chain. The next layer of decentralization will come from making the data available on-chain.\nOn-Chain Storage and Availability\nRaw off-chain data of available sources are aggregated to obtain the median value of the prices for a determined period of time, leading to a single price per item that must be taken into account in the inflation calculation. AF calculates inflation off-chain using the CalculateInflation.py function in AF's GitHub repository. The calculation is done off-chain to save gas. However, the next step is to allow any user to request an on-chain calculation.\nAF v0.2 uses Chainlink to bring off-chain data to the Kovan testnet. The next step would be to run a node directly on the Ethereum Mainnet to add an additional layer of decentralization. Chainlink is a decentralized Oracle network that cryptographically signs and delivers data on-chain. Currently, any user can request (1) the current inflation rate; and (2) the aggregate prices of each of the items in the inflation calculation.\nChallenges we ran into\nDebugging a Chainlink node was not trivial at first.\nObtaining price data in a decentralized way.\nCalculate the inflation as close as possible to how the official calculation in Mexico does.\nAccomplishments that we're proud of\nWe gave the first step in solving an incredibly relevant problem. The 1729 Newsletter, sponsored by Bajali Srinivasan (@balajis), let us see this clearly.\nAF is a project that, given support for an amazing team, could really help decentralize economic indicators such as inflation.\nWhat we learned\nHow to create our own Chainlink node and operate it.\nHow to use web3.storage for decentrally store files in IPFS.\nHow Chainlink works and the incredibly relevant role of aggregation. We even wrote a blog post about it.\nWhat's next for Apprecio.Finance\nThree main points appear on the AF roadmap:\nCoverage extended to new countries. The next would be the United States and Brazil. Requests from other economies are appreciated.\nPossible migration to full on-chain storage. With the development of new Ethereum L2 rollups like Starknet, it's cheaper than ever to chain data. This would allow users to request and integrate AF data into their smart contracts inexpensively.\nAdding new data sources. The more good quality data sources AF uses, the more decentralized and secure the inflation rate will be. In Mexico, AF is already implementing data from small suppliers (tienditas) and retailers.", "link": "https://devpost.com/software/apprecio-finance", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ninspiration for apprecio.finance (af) came from august 5, 2021, 1729 newsletter, sponsored by bajali srinivasan (@balajis):\n\u201cinflation is a monetary phenomenon, a function of money printing. but it is also in part a social phenomenon, a function of mass psychology. if enough of the right people believe that inflation is going to happen, it will. as such, when inflation is happening, there is often a push to censor discussion of inflation itself, under the grounds that discussing the problem actually causes it in the first place. that is exactly what happened in argentina and venezuela over the last decade [\u2026] and that is why the world needs a global, decentralized, censorship-resistant inflation dashboard.\nwhy do we need an inflation dashboard? because trillions of dollars have been printed over the last year \u2013 and it's not just dollars. many countries besides the us have been printing fiat like mad to fund the response to covid. figures that shocked and stunned ten years ago ($787b for the bailout!) no longer warrant a headline today, though they may well result in headlines tomorrow.\nif inflation is a government-caused problem, we can't necessarily rely on government statistics like the cpi to diagnose it or remediate it. indeed, in places with high inflation, censorship and denial is the rule rather than the exception.\u201d\nwhat it does\naf is a decentralized inflation dashboard: it estimates inflation from various price sources and makes it available through a front-end and on-chain trough chainlink.\nits main objective is to resist censorship and provide an alternative to centralized measures such as those that come from central banks and national statistical agencies. af implements decentralization at three different times in its design. (1) the price data are obtained from more than 1,000 different sources (in the mexican case), independent of the government, which prevents the failure or manipulation of any source from significantly affecting the inflation calculation. (2) data is automatically stored off-chain using the interplanetary file system (ipfs), for data to be distributed across multiple off-chain nodes, and filecoin, for data permanence and availability at any moment. (3) aggregate prices and inflation rate are stored on-chain, kovan testnet for af v0.2, using chainlink. the last two steps would also allow the data to be cryptographically signed twice.\nhow we built it\naf design\nto achieve the highest possible resistance to censorship, af implements decentralization at three different times in its design. the following figure captures them.\ndata recollection\nthis step is crucial to maintain decentralization. if manipulation of the data source by a third party can significantly alter af's inflation rate measurement, then the objective of censorship resistance would be compromised.\nthus, for the mexican case, our first measure, the data are obtained from two different types of sources.\nsmall sellers. through the apprecio web and mobile applications.\nmedium and large sellers. through web scrapping of online stores.\ndata recollection: small vendors (tienditas, in spanish)\naf is unique in that it takes advantage of a data source such as that from small vendors.\nin latin america, these sellers are frequent. around 700 thousand tienditas exist in mexico and generate 1% of gdp. not taking this data source into account would greatly skew the inflation estimate. however, obtaining data from these sources is a grassroots activity. there is no database that tracks prices in these relevant units.\nwe take advantage of the work of the mexican apprecio fintech with more than 8 thousand small vendors as clients throughout the mexican republic. among other financial and strategic services, apprecio helps small suppliers to calculate the ideal prices for their products. to do this, they require units to charge the prices at which they offer their products. af then uses these data to calculate inflation.\nthis source is decentralized. the data, for november 2021, comes from at least 1,000 different units and the number is growing. the fact that a unit does not report accurate data would not affect the general median price of the good and therefore any calculations made with the prices.\ndata recollection: medium to big vendors\nprices from walmart, la comer, and soriana are included to further decentralize data sources. af scrapes their online stores. for mexico, it is planned to include at least two additional sources: costco and sam's club.\nthese sources provide the assurance of the constant presence of at least one price during any period. the main challenge comes from maintaining scrappers that can repeatedly beat the web pages. the 1729 newsletter mentions several issues related to reliance on scrappers for pricing data, including dealing with automated countermeasures, maintaining the web crawler, and analyzing hard-to-determine units. af met these problems successfully.\ndecentralized storage off-chain\nfor af to be a successful -----> tool !!!  for openly measuring inflation, it must be available for audit (1) at any time and (2) as cheaply as possible.\nstorage has at least two requirements for af:\nraw price data must be auditable. currently, raw data storage is done off-chain due to gas prices. possibly, the next step would be for af to move to an ethereum l2 and be able to store raw data on the chain.\nthe data must be stored in a decentralized way. a central server means the consolidation of control over the data. even off the chain, due to point 1, this should be achieved.\naf achieves both requirements by storing raw price data off-chain using ifps. the entire price data set is divided into smaller chunks, cryptographic hashed, and assigned a unique fingerprint called a content identifier (cid). the dataset would be stored on at least one node, a local server anywhere in the world. by using the cid, other nodes can request the data and will cache the data themselves locally. this would increase the decentralization of the data set off the chain.\nthe data must always be online and available for direct request.\n\u201cwhile ipfs guarantees that any content on the network is discoverable, it doesn't guarantee that any content is persistently available. this is where filecoin comes in [\u2026] a decentralized storage network in which storage providers rent their storage space to clients.\u201d (ipfs documentation).\naf uses filecoin so that price data is always available while maintaining its decentralization. the dataset would be anchored in at least one node.\nthe web3.storage solution enables af (1) to automatically store the dataset in ipfs and back it up with filecoin; and (2) replicate pricing data across a network of storage providers and verify its integrity.\nthis solution allows data to be persistent, available, and decentralized even off-chain. the next layer of decentralization will come from making the data available on-chain.\non-chain storage and availability\nraw off-chain data of available sources are aggregated to obtain the median value of the prices for a determined period of time, leading to a single price per item that must be taken into account in the inflation calculation. af calculates inflation off-chain using the calculateinflation.py function in af's github repository. the calculation is done off-chain to save gas. however, the next step is to allow any user to request an on-chain calculation.\naf v0.2 uses chainlink to bring off-chain data to the kovan testnet. the next step would be to run a node directly on the ethereum mainnet to add an additional layer of decentralization. chainlink is a decentralized oracle network that cryptographically signs and delivers data on-chain. currently, any user can request (1) the current inflation rate; and (2) the aggregate prices of each of the items in the inflation calculation.\nchallenges we ran into\ndebugging a chainlink node was not trivial at first.\nobtaining price data in a decentralized way.\ncalculate the inflation as close as possible to how the official calculation in mexico does.\naccomplishments that we're proud of\nwe gave the first step in solving an incredibly relevant problem. the 1729 newsletter, sponsored by bajali srinivasan (@balajis), let us see this clearly.\naf is a project that, given support for an amazing team, could really help decentralize economic indicators such as inflation.\nwhat we learned\nhow to create our own chainlink node and operate it.\nhow to use web3.storage for decentrally store files in ipfs.\nhow chainlink works and the incredibly relevant role of aggregation. we even wrote a blog post about it.\nwhat's next for apprecio.finance\nthree main points appear on the af roadmap:\ncoverage extended to new countries. the next would be the united states and brazil. requests from other economies are appreciated.\npossible migration to full on-chain storage. with the development of new ethereum l2 rollups like starknet, it's cheaper than ever to chain data. this would allow users to request and integrate af data into their smart contracts inexpensively.\nadding new data sources. the more good quality data sources af uses, the more decentralized and secure the inflation rate will be. in mexico, af is already implementing data from small suppliers (tienditas) and retailers.", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59501256}, {"Unnamed: 0": 1276, "autor": "City4People", "date": null, "content": "Inspiration\nOur inspiration came in the means of creating a centralized system for reporting and solving issues in the city, thus making it simpler and faster for these issues to be found, reported, acknowledged by the city and finally resolved.\nWhat it does\nThe app is designed to be simple to use. People can share, report and participate in fixing the many types of problems that exist in your city. All you have to do is take a photo of the issue, post it and wait for other people to upvote it. Subsequently city representatives will decide how the given issue will be handled. In some instances a volunteer event will be created. If you feel that simply reporting these problems is insufficient and you're looking for a more active role in helping your city, then you might consider participating in such an event with other volunteers and do your part in making a better home for you and everyone else.\nHow we built it\nFirst we started with the design. Figma was deemed the best choice for making a prototype as it was by far the best prototyping tool we knew of and all of us already had some experience using it. Then the prototype was put to life using react-native in Visual Studio Code and Webstorm.\nChallenges we ran into\nThere were a few obstacles to get trough, like implementing the navigation between screens. Sometimes even the merging of branches on git wasn't a simple task.\nAccomplishments that we're proud of\nWe're honestly proud of what we've accomplished in such a short amount of time.\nWhat we learned\nWe have all gained new skills in designing and coding but most importantly we now have the experience of what it's like to be working on a team project. What surprised us the most was how much faster the making of a project is when you're working in a team. With proper task management we've managed to do more work in a single day, than what would take one of us a week.\nWhat's next for city4people\nOur next steps include :\nA reputation and badge system: this feature would allow a user to earn reputation points for reporting and participating in tasks, the user would also be rewarded with special badges upon completing certain tasks or sets of tasks. These badges would be displayed next to the username of the user.\nA report spam feature which would allow users to report inappropriate content so it can be removed.\nA comment feature which would make it possible for a user to comment and view comments on issues.\nA city dashboard which would show all high priority news on the homepage (e.g. a hazardous weather warning or an electricity outage).", "link": "https://devpost.com/software/city4people", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nour inspiration came in the means of creating a centralized system for reporting and solving issues in the city, thus making it simpler and faster for these issues to be found, reported, acknowledged by the city and finally resolved.\nwhat it does\nthe app is designed to be simple to use. people can share, report and participate in fixing the many types of problems that exist in your city. all you have to do is take a photo of the issue, post it and wait for other people to upvote it. subsequently city representatives will decide how the given issue will be handled. in some instances a volunteer event will be created. if you feel that simply reporting these problems is insufficient and you're looking for a more active role in helping your city, then you might consider participating in such an event with other volunteers and do your part in making a better home for you and everyone else.\nhow we built it\nfirst we started with the design. figma was deemed the best choice for making a prototype as it was by far the best prototyping -----> tool !!!  we knew of and all of us already had some experience using it. then the prototype was put to life using react-native in visual studio code and webstorm.\nchallenges we ran into\nthere were a few obstacles to get trough, like implementing the navigation between screens. sometimes even the merging of branches on git wasn't a simple task.\naccomplishments that we're proud of\nwe're honestly proud of what we've accomplished in such a short amount of time.\nwhat we learned\nwe have all gained new skills in designing and coding but most importantly we now have the experience of what it's like to be working on a team project. what surprised us the most was how much faster the making of a project is when you're working in a team. with proper task management we've managed to do more work in a single day, than what would take one of us a week.\nwhat's next for city4people\nour next steps include :\na reputation and badge system: this feature would allow a user to earn reputation points for reporting and participating in tasks, the user would also be rewarded with special badges upon completing certain tasks or sets of tasks. these badges would be displayed next to the username of the user.\na report spam feature which would allow users to report inappropriate content so it can be removed.\na comment feature which would make it possible for a user to comment and view comments on issues.\na city dashboard which would show all high priority news on the homepage (e.g. a hazardous weather warning or an electricity outage).", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59501276}, {"Unnamed: 0": 1323, "autor": "OGSwap TVT: cross-chain transaction validation tool", "date": null, "content": "Inspiration\nSolana is shooting up the charts to become the fourth-largest cryptocurrency due to its strong tech foundation and infrastructure. The whole crypto space is showing interest in Solana, with a multitude of cross-chain projects striving to integrate it into their platforms. However, integrating Solana into an existing cross-chain project is difficult in a number of ways, e.g. securely and persistently storing data. We wanted to bridge this gap and enable the building of cross-chain applications that can easily integrate Solana.\nFrom a technical perspective, we aimed to create a cross-chain solution helping to prevent double-spending or serving inaccurate data during cross-chain interactions between EVM blockchains and Solana.\nWhat it does\nOGSwap TVT is a solution that allows for securely transferring events from EVM chains to Solana. TVT can be used by various cross-chain dApps to validate transactions that happen on EVM, such as token locks, and publish those on Solana. Currently, this solution is being tested with OGSwap cross-chain transfer events.\nAt its core, OGSwap TVT is an extended Chainlink oracle node capable of monitoring events on EVM chains, sending corresponding transactions to Solana, and achieving a consensus with other nodes about the validity of events.\nThe process is as follows:\n-An event occurs on an EVM chain, e.g. locking of a token on the OGSwap smart contact (SC1)\n-A Chainlink node extended with TVT identifies the event and sends a transaction to Solana\n-A smart contract on Solana (SC2) verifies the validity of the data being transferred and makes sure that the destination account can be sent to\n-A data account on Solana stores verifications of the event made by a network of Chainlink oracles and validates the event upon achieving a predefined limit of verifications, marking it as \u201cconfirmed\u201d.\n-Thus, any smart contract on Solana can verify that the event took place on the EVM chain.\nThe contract itself contains governance mechanics:\n-voting in the validators\u2019 network for event validation by the oracle nodes\n-voting on the smart contract to add or remove validators\n-max amount of oracles in the network set at the moment - 20\nThis tool can support the creation of EVM-to-Solana cross-chain applications.\nChallenges we ran into\n-In Solana, data accounts disappear with time (appr. 2 years; Rent). We solved this issue by setting a term of limitation for each event.\n-We faced issues with safely storing keys.\n-Finding an optimal approach for safe data storage was also non-trivial.\nAccomplishments that we're proud of\nWe developed a working tool that ensures the safety of cross-chain transactions between EVM blockchains and Solana.\nWhat we learned\n-How to deal with \u201cexpiring\u201d data accounts on Solana\n-How to securely store data on Solana\n-How to expand any EVM-based application onto Solana\nWhat's next for OGSwap TVT: cross-chain transaction validation tool\nWe plan to expand into a platform with a helpful toolset that will allow for the easy creation of cross-chain dApps on Solana and EVM blockchains.", "link": "https://devpost.com/software/ogswap-tvt-cross-chain-transaction-validation-tool", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nsolana is shooting up the charts to become the fourth-largest cryptocurrency due to its strong tech foundation and infrastructure. the whole crypto space is showing interest in solana, with a multitude of cross-chain projects striving to integrate it into their platforms. however, integrating solana into an existing cross-chain project is difficult in a number of ways, e.g. securely and persistently storing data. we wanted to bridge this gap and enable the building of cross-chain applications that can easily integrate solana.\nfrom a technical perspective, we aimed to create a cross-chain solution helping to prevent double-spending or serving inaccurate data during cross-chain interactions between evm blockchains and solana.\nwhat it does\nogswap tvt is a solution that allows for securely transferring events from evm chains to solana. tvt can be used by various cross-chain dapps to validate transactions that happen on evm, such as token locks, and publish those on solana. currently, this solution is being tested with ogswap cross-chain transfer events.\nat its core, ogswap tvt is an extended chainlink oracle node capable of monitoring events on evm chains, sending corresponding transactions to solana, and achieving a consensus with other nodes about the validity of events.\nthe process is as follows:\n-an event occurs on an evm chain, e.g. locking of a token on the ogswap smart contact (sc1)\n-a chainlink node extended with tvt identifies the event and sends a transaction to solana\n-a smart contract on solana (sc2) verifies the validity of the data being transferred and makes sure that the destination account can be sent to\n-a data account on solana stores verifications of the event made by a network of chainlink oracles and validates the event upon achieving a predefined limit of verifications, marking it as \u201cconfirmed\u201d.\n-thus, any smart contract on solana can verify that the event took place on the evm chain.\nthe contract itself contains governance mechanics:\n-voting in the validators\u2019 network for event validation by the oracle nodes\n-voting on the smart contract to add or remove validators\n-max amount of oracles in the network set at the moment - 20\nthis -----> tool !!!  can support the creation of evm-to-solana cross-chain applications.\nchallenges we ran into\n-in solana, data accounts disappear with time (appr. 2 years; rent). we solved this issue by setting a term of limitation for each event.\n-we faced issues with safely storing keys.\n-finding an optimal approach for safe data storage was also non-trivial.\naccomplishments that we're proud of\nwe developed a working tool that ensures the safety of cross-chain transactions between evm blockchains and solana.\nwhat we learned\n-how to deal with \u201cexpiring\u201d data accounts on solana\n-how to securely store data on solana\n-how to expand any evm-based application onto solana\nwhat's next for ogswap tvt: cross-chain transaction validation tool\nwe plan to expand into a platform with a helpful toolset that will allow for the easy creation of cross-chain dapps on solana and evm blockchains.", "sortedWord": "None", "removed": "Nan", "score": 92, "comments": 0, "media": null, "medialink": null, "identifyer": 59501323}, {"Unnamed: 0": 1365, "autor": "Walk With Me", "date": null, "content": "THE PROJECT WAS SHOWN LIVE.\nInspiration\nWith the new normal and digitalization of every sector we came to realise that there are mostly blogs available on the internet on Autism and felt that the Autism community was somewhere sidelined with having barely any resources or softwares which could help the parents and therapists continue their therapies virtually and still had to depend on traditional means of crafts etc to develop puzzles for children to teach life skills . On more surfing around the internet, we realised the gravity of the situation when there was only one website which had educational games teaching autistic kids eye contact etc. which had also discontinued because of technical glitches.The severity of the situation alongside the fact that 1 in 54 children have autism inspired us to use technology to create tools catering to the needs of the children, parents and people associated with autism. Spreading awareness about Autism through our project has been the major driving force.\nWhat it does\nWalk with me is a working prototype aimed at incorporating technology into the traditional tools used in teaching people with autism basic life skills. The project is coded in colour blue and has the puzzle logo which are symbolic to autism and have been incorporated with the aim to spread awareness. The project has three distinct highlights aimed at trying to cover people who are on different levels of the autism spectrum. Special Educators use traditional means of making adaptive books to teach life skills like brushing teeth etc and to reduce the cumbersome work load in making work system using hand crafts, we decided to design prototype centralizing to the people with autism by keeping them simple so that with one click of mouse, the child can easily navigate through the steps and learn new skills making the work easier for both the educators and children.\nThe website in itself is a compact platform providing knowledge of Autism and spreading awareness about it through blogs ,videos etc as well as linking the features together including the game prototype and bot.\nThe discord bot linked with the website is a tool for non verbal autistic people (approx 30-40% people on spectrum are non verbal) who can use the text input to interact with the bot. The bot provides positive reinforcements to the user which in return helps in uplifting the mood of the user as well as increasing confidence and preaching self worth through phrases like \u201cYou can do it.\u201d ,\u201cYou are the best.\u201d etc. It also has quotations from personalities like Rumi etc.\nHow we built it\nOur project is an amalgamation of the following things-\nDiscord Bot- Python Front End -HTML/CSS - Bootstrap - Java script Back End - php Educational Game Prototype -Google Slides\nChallenges we ran into\n1)We faced a lot of system and softwares problems while the frontend of the website was being created due to which we had to redo the whole front end. 2)We had one of our teammate leave us just after the start of the Hack due to some unforeseen circumstances, which made our schedule more scrambled but we managed to finish our project . 3) It took us some time and help from the mentors to figure out a way to present the idea of the game in form of a prototype but we managed to make the best use of google slides in prototyping our idea.\nAccomplishments that we're proud of\nWe now know the use of HTML/CSS,using sandbox,bootstrap,template editing and shuffling in an intermediate level as well as learning how to create games/short animations using google slides. We are proud of how we were able to come together as a team since we had never met each other prior but were still able to work together in a collaborative and healthy environment.\nWhat we learned\nThe research phase of the hack was an eye opening experience for us all as a team especially learning about Autism and how an autistic person spends his/her daily routine especially the tools and techniques used in teaching an autistic child life skills. We also realised that apart from numerous blogs on autism, there isn't much out there in terms of softwares or educational games etc. catering to the needs of autistic people etc which motivated us to take this project in the hackathon. On the technical forefront, working on the project has taught us how to manage our time while simultaneously collaborating with each other and making quick decisions as well as learning new technical hacks like extensions etc which enables us to code in real time.\nWhat's next for Walk With Me\nPost the hackathon, converting the prototype for the educational games into actual games using platforms like unity is on our to-do list. Considering the lack of educational games built catering to autistic people, we are planning to improve our prototype and turn it into educational games teaching people with autism basic life skills helping them with their journey into being independent. We also plan on improving the discord bot as well as text to speech functionality keeping in mind the non verbal children on the autism spectrum and helping them communicate through technology.We plan on making the security of our website as well as the discord so as to make sure there is no breach or unparliamentary action in our environment and to ensure the environment runs as a safe,civil and learning platform. Alongside this we look forward to adding more functionality to the website and making it dynamic with good quality content and incorporating a better tech stack like react into it. We also plan on improving the content of the website in terms of resources etc like sharing the journey of people who are on the spectrum, sharing books written by people having autism ,sessions and conferences catering to autism etc. We hope to even develop an app version of the website , incorporate an inbuilt chat feature for a peer to peer interaction or peer to admin interaction as well as develop more advanced,secure admin records with the database.\n2) WCAG We plan to integrate and follow the WCAG 2.0(WEB ACCESSIBILITY GUIDELINES) which are : 1)Provides content not prone to have seizures 2)Content should be substituted with pictures for better understanding 3)Sentence should not be cluttered 4)Font needs to be large and legible 5)Content present needs to be verified such that it is not prone to seizures.", "link": "https://devpost.com/software/walk-with-me-b0l68q", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "the project was shown live.\ninspiration\nwith the new normal and digitalization of every sector we came to realise that there are mostly blogs available on the internet on autism and felt that the autism community was somewhere sidelined with having barely any resources or softwares which could help the parents and therapists continue their therapies virtually and still had to depend on traditional means of crafts etc to develop puzzles for children to teach life skills . on more surfing around the internet, we realised the gravity of the situation when there was only one website which had educational games teaching autistic kids eye contact etc. which had also discontinued because of technical glitches.the severity of the situation alongside the fact that 1 in 54 children have autism inspired us to use technology to create tools catering to the needs of the children, parents and people associated with autism. spreading awareness about autism through our project has been the major driving force.\nwhat it does\nwalk with me is a working prototype aimed at incorporating technology into the traditional tools used in teaching people with autism basic life skills. the project is coded in colour blue and has the puzzle logo which are symbolic to autism and have been incorporated with the aim to spread awareness. the project has three distinct highlights aimed at trying to cover people who are on different levels of the autism spectrum. special educators use traditional means of making adaptive books to teach life skills like brushing teeth etc and to reduce the cumbersome work load in making work system using hand crafts, we decided to design prototype centralizing to the people with autism by keeping them simple so that with one click of mouse, the child can easily navigate through the steps and learn new skills making the work easier for both the educators and children.\nthe website in itself is a compact platform providing knowledge of autism and spreading awareness about it through blogs ,videos etc as well as linking the features together including the game prototype and bot.\nthe discord bot linked with the website is a -----> tool !!!  for non verbal autistic people (approx 30-40% people on spectrum are non verbal) who can use the text input to interact with the bot. the bot provides positive reinforcements to the user which in return helps in uplifting the mood of the user as well as increasing confidence and preaching self worth through phrases like \u201cyou can do it.\u201d ,\u201cyou are the best.\u201d etc. it also has quotations from personalities like rumi etc.\nhow we built it\nour project is an amalgamation of the following things-\ndiscord bot- python front end -html/css - bootstrap - java script back end - php educational game prototype -google slides\nchallenges we ran into\n1)we faced a lot of system and softwares problems while the frontend of the website was being created due to which we had to redo the whole front end. 2)we had one of our teammate leave us just after the start of the hack due to some unforeseen circumstances, which made our schedule more scrambled but we managed to finish our project . 3) it took us some time and help from the mentors to figure out a way to present the idea of the game in form of a prototype but we managed to make the best use of google slides in prototyping our idea.\naccomplishments that we're proud of\nwe now know the use of html/css,using sandbox,bootstrap,template editing and shuffling in an intermediate level as well as learning how to create games/short animations using google slides. we are proud of how we were able to come together as a team since we had never met each other prior but were still able to work together in a collaborative and healthy environment.\nwhat we learned\nthe research phase of the hack was an eye opening experience for us all as a team especially learning about autism and how an autistic person spends his/her daily routine especially the tools and techniques used in teaching an autistic child life skills. we also realised that apart from numerous blogs on autism, there isn't much out there in terms of softwares or educational games etc. catering to the needs of autistic people etc which motivated us to take this project in the hackathon. on the technical forefront, working on the project has taught us how to manage our time while simultaneously collaborating with each other and making quick decisions as well as learning new technical hacks like extensions etc which enables us to code in real time.\nwhat's next for walk with me\npost the hackathon, converting the prototype for the educational games into actual games using platforms like unity is on our to-do list. considering the lack of educational games built catering to autistic people, we are planning to improve our prototype and turn it into educational games teaching people with autism basic life skills helping them with their journey into being independent. we also plan on improving the discord bot as well as text to speech functionality keeping in mind the non verbal children on the autism spectrum and helping them communicate through technology.we plan on making the security of our website as well as the discord so as to make sure there is no breach or unparliamentary action in our environment and to ensure the environment runs as a safe,civil and learning platform. alongside this we look forward to adding more functionality to the website and making it dynamic with good quality content and incorporating a better tech stack like react into it. we also plan on improving the content of the website in terms of resources etc like sharing the journey of people who are on the spectrum, sharing books written by people having autism ,sessions and conferences catering to autism etc. we hope to even develop an app version of the website , incorporate an inbuilt chat feature for a peer to peer interaction or peer to admin interaction as well as develop more advanced,secure admin records with the database.\n2) wcag we plan to integrate and follow the wcag 2.0(web accessibility guidelines) which are : 1)provides content not prone to have seizures 2)content should be substituted with pictures for better understanding 3)sentence should not be cluttered 4)font needs to be large and legible 5)content present needs to be verified such that it is not prone to seizures.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59501365}, {"Unnamed: 0": 1414, "autor": "Parahelp: gives the power of speech to everyone", "date": null, "content": "Inspiration\nParalysis and other motor neurone diseases such as ALS can cripple peoples lives and livelihoods in a devastatingly short time.\nAs the neurodegenerative disease spreads across the brain and spinal cord, the victims diagnosed with it will suddenly realize that most of their muscular activities come to a halt.\nAll most people are left with is the ability to move their eye-balls freely. A good example of a person who suffered from ALS is Stephen Hawkings.\nHence, we decided to help those suffering with this issue to communicate in plain English by developing an app that tracks the motion of their eyes and navigate the simple 4 panel UI of our app to speak again.\nWhat it does\nThe app basically tracks the direction in while the eye looks at and this data is used by the app to convert this into an English sentence which can also be converted to speech.\nEssentially, The eye-tracker tool we built using Open CV looks for 6 functions:\nTop_left\nTop_right\nBottom_left\nBottom_right\nBlink\nEye Closed\nThe app screen we built using Android Studio and FireBase is divided into 4 panels:\nTop_left\nTop_right\nBottom_left\nBottom_right\nHow we built it\nFront-end\nWe decided to use android studio to run the app natively\nThrough trial and error we came to an UI that is both simple and easy to understand.\nThis intuitive design was the main core to our entire app design.\nWe used colour palettes that beautifully match the background and give the app a fluid frictionless look.\nBack-end\nWe decided to use Firebase for both authentication and storage of data.\nAll the required words and sub-words and stored online in a real-time database which can be fetched - at all times to display it.\nWe used firebase for authentication of emails and passwords due to its excellent tools for forgetting password, etc.\nEye-tracking\nWe used Open CV and Dlib along with its libraries to do eye-tracking.\nWe have decided to divide the open CV screen into 4 quadrants and the position of the eye in a quadrant will return a position to firebase.\nThis data is then used by the app to choose the relevant panel.\nChallenges we ran into\nMaking Open CV work. It was our first time.\nConnecting Firebase and making a completely new UI\nWhat's next for Parahelp: gives the power of speech to everyone\nUse more research and better our understanding of the English language to better use the 4 pane layout.\nUse machine learning to essentially collect user data and make accurate per person recommendations for common sentences or phrases\nBetter the use of Open CV and eliminate use of uploading/downloading of positions from firebase and make it happen natively on the mobile.", "link": "https://devpost.com/software/parahelp-gives-the-power-of-speech-to-everyone-cu8v9l", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nparalysis and other motor neurone diseases such as als can cripple peoples lives and livelihoods in a devastatingly short time.\nas the neurodegenerative disease spreads across the brain and spinal cord, the victims diagnosed with it will suddenly realize that most of their muscular activities come to a halt.\nall most people are left with is the ability to move their eye-balls freely. a good example of a person who suffered from als is stephen hawkings.\nhence, we decided to help those suffering with this issue to communicate in plain english by developing an app that tracks the motion of their eyes and navigate the simple 4 panel ui of our app to speak again.\nwhat it does\nthe app basically tracks the direction in while the eye looks at and this data is used by the app to convert this into an english sentence which can also be converted to speech.\nessentially, the eye-tracker -----> tool !!!  we built using open cv looks for 6 functions:\ntop_left\ntop_right\nbottom_left\nbottom_right\nblink\neye closed\nthe app screen we built using android studio and firebase is divided into 4 panels:\ntop_left\ntop_right\nbottom_left\nbottom_right\nhow we built it\nfront-end\nwe decided to use android studio to run the app natively\nthrough trial and error we came to an ui that is both simple and easy to understand.\nthis intuitive design was the main core to our entire app design.\nwe used colour palettes that beautifully match the background and give the app a fluid frictionless look.\nback-end\nwe decided to use firebase for both authentication and storage of data.\nall the required words and sub-words and stored online in a real-time database which can be fetched - at all times to display it.\nwe used firebase for authentication of emails and passwords due to its excellent tools for forgetting password, etc.\neye-tracking\nwe used open cv and dlib along with its libraries to do eye-tracking.\nwe have decided to divide the open cv screen into 4 quadrants and the position of the eye in a quadrant will return a position to firebase.\nthis data is then used by the app to choose the relevant panel.\nchallenges we ran into\nmaking open cv work. it was our first time.\nconnecting firebase and making a completely new ui\nwhat's next for parahelp: gives the power of speech to everyone\nuse more research and better our understanding of the english language to better use the 4 pane layout.\nuse machine learning to essentially collect user data and make accurate per person recommendations for common sentences or phrases\nbetter the use of open cv and eliminate use of uploading/downloading of positions from firebase and make it happen natively on the mobile.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59501414}, {"Unnamed: 0": 1440, "autor": "Retro Study", "date": null, "content": "Track: Advanced Category\nTheme: Quality Education\n\u2728 Inspiration\nRetro Study is a website aimed to assisting students and lecturers in online classes. The COVID-19 has caused schools all across the world to close. Over 1.2 billion youngsters are out of school worldwide. In the survey, 54-58 percent of students said they had severe physical strain, vision problems, backache and headaches from posture problems, lethargy, exhaustion, irritation, and obesity. Even in all-remote contexts, teachers reported higher student absenteeism and less interaction among student than in face-to-face classes.\n\ud83c\udf41 What it does\nRetro Study has various features which target and provide a solution for the different problems faced during online for both teachers and students.\nEmotion camera : It's tough for lecturers to assess students' understanding after lectures in an online class. This tool assists the teacher in determining whether the students are glad, unhappy, confused, or distracted. As a result, they may tailor classes to the needs of the students.\nPosture Detector : The use of computers can lead to them having an unhealthy posture which, being kids, they may not be able to correct as they are not aware of it. This tool helps the children to sit in better position.\nSpeech to text notes: Most of us find it challenging to take notes and learn in an online session. You can speak up your notes and save them here. You'll be able to retrieve it later, and you'll be able to hear the note once more. It also helps children who have difficulty writing. Increased efficiency and less paperwork which helps to save time.\nAR Learning: AR in education enables the visual depiction of the learning material and enables teachers to impart interactive learning in lessons while it allows learners to view information visually (layered on the top of their real-world surroundings). AR can greatly help students become more involved in their classes. It provides much more interactive learning along with opportunities to implement hands-on learning approaches that can increase engagement and enhance the learning experience for the students.\nDoubt Chat: Here students can ask doubt to teachers regarding the subjects. Reason that students don't speak up in class could be that they don't understand the material and might feel afraid that their questions will make them appear unintelligent to their peers. So, by using doubt chat student can clarify their doubts with their teacher.\n\ud83d\udd27 How we built it\nFor our frontend, We used the templating language EJS in combination with CSS. When we were using EJS, We made sure to use stuff like partials. We also used NodeJS to put up a small web server, and everything was constructed using Repl.it, an in-browser IDE that was great for collaborating and giving feedback in real time. Then we used tensorflow.js and machine learning to train the model for the emotion camera and posture detector, and then we put it to our website using JavaScript! We also used google cloud for speech to text feature.\n\ud83c\udfc3\u200d\u2640\ufe0f Challenges we ran into\nWhile training machine learning models, We found it difficult. This is my first time utilizing machine learning. I'm proud of what I've accomplished in just a few hours. Managing my time while working on this project was a struggle for me.Furthermore, determining which features to include was a difficult task. We had so many various ideas, but we realized we wouldn't be able to apply them all, so we had to pick a few.\n\ud83d\udcab Accomplishments that we're proud of\nWe're proud that in just a few hours, we was able to create a code a fully functional website, build a slide deck, and record & edit my pitch video. We was also able to learn a lot from this hackathon, using technologies that I had barely touched previously, which I'm proud that I was able to do!\n\u2604\ufe0f What we learned\nDoing research for this website definitely opened my eyes to how widespread the impact of online classes is--and how important it is to work towards a solution to the problem. On the more technical side, this was my first real project that I\u2019ve used machine learning. I learned a lot about how to use format elements on the webpage. Overall though, since I\u2019m still fairly new to web development, with every project I create, We learn something new; whether it be creating a certain element or just designing the overall page.\n\u26a1 What's next for Retro Study\nIn the future, We would like to fully implement some of the features that we didn't have time to do today, such as user authentication for teachers and students, so that each student has their own unique platform to work and learn on. Graph the data of student emotions , so that they can adjust their lessons and materials accordingly.", "link": "https://devpost.com/software/retro-study-2f091b", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "track: advanced category\ntheme: quality education\n\u2728 inspiration\nretro study is a website aimed to assisting students and lecturers in online classes. the covid-19 has caused schools all across the world to close. over 1.2 billion youngsters are out of school worldwide. in the survey, 54-58 percent of students said they had severe physical strain, vision problems, backache and headaches from posture problems, lethargy, exhaustion, irritation, and obesity. even in all-remote contexts, teachers reported higher student absenteeism and less interaction among student than in face-to-face classes.\n\ud83c\udf41 what it does\nretro study has various features which target and provide a solution for the different problems faced during online for both teachers and students.\nemotion camera : it's tough for lecturers to assess students' understanding after lectures in an online class. this -----> tool !!!  assists the teacher in determining whether the students are glad, unhappy, confused, or distracted. as a result, they may tailor classes to the needs of the students.\nposture detector : the use of computers can lead to them having an unhealthy posture which, being kids, they may not be able to correct as they are not aware of it. this tool helps the children to sit in better position.\nspeech to text notes: most of us find it challenging to take notes and learn in an online session. you can speak up your notes and save them here. you'll be able to retrieve it later, and you'll be able to hear the note once more. it also helps children who have difficulty writing. increased efficiency and less paperwork which helps to save time.\nar learning: ar in education enables the visual depiction of the learning material and enables teachers to impart interactive learning in lessons while it allows learners to view information visually (layered on the top of their real-world surroundings). ar can greatly help students become more involved in their classes. it provides much more interactive learning along with opportunities to implement hands-on learning approaches that can increase engagement and enhance the learning experience for the students.\ndoubt chat: here students can ask doubt to teachers regarding the subjects. reason that students don't speak up in class could be that they don't understand the material and might feel afraid that their questions will make them appear unintelligent to their peers. so, by using doubt chat student can clarify their doubts with their teacher.\n\ud83d\udd27 how we built it\nfor our frontend, we used the templating language ejs in combination with css. when we were using ejs, we made sure to use stuff like partials. we also used nodejs to put up a small web server, and everything was constructed using repl.it, an in-browser ide that was great for collaborating and giving feedback in real time. then we used tensorflow.js and machine learning to train the model for the emotion camera and posture detector, and then we put it to our website using javascript! we also used google cloud for speech to text feature.\n\ud83c\udfc3\u200d\u2640\ufe0f challenges we ran into\nwhile training machine learning models, we found it difficult. this is my first time utilizing machine learning. i'm proud of what i've accomplished in just a few hours. managing my time while working on this project was a struggle for me.furthermore, determining which features to include was a difficult task. we had so many various ideas, but we realized we wouldn't be able to apply them all, so we had to pick a few.\n\ud83d\udcab accomplishments that we're proud of\nwe're proud that in just a few hours, we was able to create a code a fully functional website, build a slide deck, and record & edit my pitch video. we was also able to learn a lot from this hackathon, using technologies that i had barely touched previously, which i'm proud that i was able to do!\n\u2604\ufe0f what we learned\ndoing research for this website definitely opened my eyes to how widespread the impact of online classes is--and how important it is to work towards a solution to the problem. on the more technical side, this was my first real project that i\u2019ve used machine learning. i learned a lot about how to use format elements on the webpage. overall though, since i\u2019m still fairly new to web development, with every project i create, we learn something new; whether it be creating a certain element or just designing the overall page.\n\u26a1 what's next for retro study\nin the future, we would like to fully implement some of the features that we didn't have time to do today, such as user authentication for teachers and students, so that each student has their own unique platform to work and learn on. graph the data of student emotions , so that they can adjust their lessons and materials accordingly.", "sortedWord": "None", "removed": "Nan", "score": 6, "comments": 2, "media": null, "medialink": null, "identifyer": 59501440}, {"Unnamed: 0": 1456, "autor": "ISCN wallet", "date": null, "content": "Inspiration\nCurrently, there's no online tool for users who want to update ISCN or batch create ISCN.\nWhat it does\nUsers can (batch) create, update ISCN, transfer ISCN, list ISCN by owner address and find ISCN by ID with the tool.\nHow we built it\nVue.js + cosmjs + iscn-js\nChallenges we ran into\nDuring the development, we found out that including multiple \"create ISCN\" messages in one transaction will cause an error. After some investigation, we figured out the module will generate the same ISCN ID for every ISCN in the same transaction and cause the error (ID should not be duplicated for newly created ISCN), so users need to sign the transaction one by one currently.\nWe also reported the bug to the chain development team.\nAccomplishments that we're proud of\nReport a module bug about the \"create ISCN\" message.\nWhat we learned\nAnalyzation of the module source code is also an important part for front-end developers.\nWhat's next for ISCN wallet\nImprove the UI design, add more descriptions about what's ISCN and how to create ISCN.", "link": "https://devpost.com/software/iscn-wallet", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ncurrently, there's no online -----> tool !!!  for users who want to update iscn or batch create iscn.\nwhat it does\nusers can (batch) create, update iscn, transfer iscn, list iscn by owner address and find iscn by id with the tool.\nhow we built it\nvue.js + cosmjs + iscn-js\nchallenges we ran into\nduring the development, we found out that including multiple \"create iscn\" messages in one transaction will cause an error. after some investigation, we figured out the module will generate the same iscn id for every iscn in the same transaction and cause the error (id should not be duplicated for newly created iscn), so users need to sign the transaction one by one currently.\nwe also reported the bug to the chain development team.\naccomplishments that we're proud of\nreport a module bug about the \"create iscn\" message.\nwhat we learned\nanalyzation of the module source code is also an important part for front-end developers.\nwhat's next for iscn wallet\nimprove the ui design, add more descriptions about what's iscn and how to create iscn.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59501456}, {"Unnamed: 0": 1462, "autor": "Identificador de impressions 3D per imatge", "date": null, "content": "Inspiration\nWe think that Invelon proposed an interesting challenge. We especially liked the 3D aspect of it and its difficulty. So, we wanted to challenge ourselves by being as ambitious as possible to see how far we could get!\nWhat it does\nA worker in front of a 3D printer with a print finished can take and upload a picture of it from their phone into our web app Then, our server finds the .stl file (from the ones we were provided) that best matches the picture.\nHow does it internally find the match?\nThe image is cropped to be square and a fixed resolution.\nThe image background is removed with an open-source library written in python and based on the deep learning u2net architecture. (We initially tried to code our own, but we gave up due to the difficulty and that it was not the main focus).\nThe processed image is compared to a set of renders of the .stls (more details on this latter). It uses two algorithms in combination to compare how similar the photo is with a render: the average hash and the perceptual hash. We return the .stl that scored higher in the similarity comparison.\nFinally, we gather the related information of that .stl (e.g. customer info) from our database. This complete information is returned to the frontend.\nWhat about the \u201crenders\u201d\nTo compare 3d models with photos, we evaluated 2 approaches:\nUse deep learning to find matches.\nCreate renders of the 3d models from many angles and do simple image comparison. We discarded a. because we were worried that we wouldn\u2019t be able to train the neural net properly and get satisfying results. Also, b. seemed more achievable because we could split the problem into smaller ones, split work better, and increase our chances of success.\nThe renderer internally uses Three.js running in Node.js (using TypeScript). It loops through the .stls and for each one it renders 6 images (top, bottom, left, right, front and back). Then the images are saved in the server\u2019s local storage, to then be accessed by other processes, in real life this would be a bucket in some cloud.\nHow we built it\nFor the FrontEnd part we used Vue.js with typescript to develop all the platforms and also, three.js as the library to show the models. For the backend part we used python (w/ Flask) to develop all the backend with opencv and also everything integrated with docker and sql for the database.\nIf you take a look in our GitHub, we have a monorepo with all the little projects on it, each one is in one different folder, you can have more information in the Readme for each folder but basically we have:\nmatcher-client that contains the front-end where you can upload the 3D model\nmatcher-server that matches the image with models we have in the renderer.\nrenderer-server that stores the Models and Renders of them to be used in the matcher.\nYou can see a picture of the planned architecture in the project media shown on the top.\nChallenges we ran into\nOn a technical level:\nColorless stl: it\u2019s difficult to match pictures of different colors\nBackground removal to improve accuracy: we could not do this properly until we used a deep-learning-based approach\nRendering stl in headless browser. Three.js is a wonderful library to do this kind of rendering but it counts on having a browser window to display the scene. To programmatically create the renders, we had to use puppeteer as a headless browser automation tool.\nWe wanted to create a scalable solution, so creating a DL classification model was not the optimal solution. This is why we ended using and trying old school ML solutions as hashing or SIFT.\nOn top of that, we\u2019d add lack of time and tiredness, the usual suspects! Also, one of our team members is participating online and it's harder to communicate and develop the project in less time if we are not together.\nAccomplishments that we're proud of\nWe are proud of the project we developed, we are proud of getting it to work. Also we are proud of following good code practices in our development, despite finally not being able to do all the testing we wanted at first because of the lack of time.\nWhat we learned\nDuring these 24 hours (a bit less because we slept) we learnt how to render STLs and also how to compute approximate similarity on images. It was a big challenge because we learnt that, because it's a 3D model, it can be in a lot of different angles so it's really difficult to do a match and if the figures are similar, it's difficult to differentiate between them.\nWhat's next for Identificador de impressions 3D per imatge\nMaybe add more models and also if there is enough data maybe train a machine learning model to be more effective in the render so you can upload more pictures, nowadays there are just a few models.", "link": "https://devpost.com/software/identificador-de-impressions-3d-per-imatge", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe think that invelon proposed an interesting challenge. we especially liked the 3d aspect of it and its difficulty. so, we wanted to challenge ourselves by being as ambitious as possible to see how far we could get!\nwhat it does\na worker in front of a 3d printer with a print finished can take and upload a picture of it from their phone into our web app then, our server finds the .stl file (from the ones we were provided) that best matches the picture.\nhow does it internally find the match?\nthe image is cropped to be square and a fixed resolution.\nthe image background is removed with an open-source library written in python and based on the deep learning u2net architecture. (we initially tried to code our own, but we gave up due to the difficulty and that it was not the main focus).\nthe processed image is compared to a set of renders of the .stls (more details on this latter). it uses two algorithms in combination to compare how similar the photo is with a render: the average hash and the perceptual hash. we return the .stl that scored higher in the similarity comparison.\nfinally, we gather the related information of that .stl (e.g. customer info) from our database. this complete information is returned to the frontend.\nwhat about the \u201crenders\u201d\nto compare 3d models with photos, we evaluated 2 approaches:\nuse deep learning to find matches.\ncreate renders of the 3d models from many angles and do simple image comparison. we discarded a. because we were worried that we wouldn\u2019t be able to train the neural net properly and get satisfying results. also, b. seemed more achievable because we could split the problem into smaller ones, split work better, and increase our chances of success.\nthe renderer internally uses three.js running in node.js (using typescript). it loops through the .stls and for each one it renders 6 images (top, bottom, left, right, front and back). then the images are saved in the server\u2019s local storage, to then be accessed by other processes, in real life this would be a bucket in some cloud.\nhow we built it\nfor the frontend part we used vue.js with typescript to develop all the platforms and also, three.js as the library to show the models. for the backend part we used python (w/ flask) to develop all the backend with opencv and also everything integrated with docker and sql for the database.\nif you take a look in our github, we have a monorepo with all the little projects on it, each one is in one different folder, you can have more information in the readme for each folder but basically we have:\nmatcher-client that contains the front-end where you can upload the 3d model\nmatcher-server that matches the image with models we have in the renderer.\nrenderer-server that stores the models and renders of them to be used in the matcher.\nyou can see a picture of the planned architecture in the project media shown on the top.\nchallenges we ran into\non a technical level:\ncolorless stl: it\u2019s difficult to match pictures of different colors\nbackground removal to improve accuracy: we could not do this properly until we used a deep-learning-based approach\nrendering stl in headless browser. three.js is a wonderful library to do this kind of rendering but it counts on having a browser window to display the scene. to programmatically create the renders, we had to use puppeteer as a headless browser automation -----> tool !!! .\nwe wanted to create a scalable solution, so creating a dl classification model was not the optimal solution. this is why we ended using and trying old school ml solutions as hashing or sift.\non top of that, we\u2019d add lack of time and tiredness, the usual suspects! also, one of our team members is participating online and it's harder to communicate and develop the project in less time if we are not together.\naccomplishments that we're proud of\nwe are proud of the project we developed, we are proud of getting it to work. also we are proud of following good code practices in our development, despite finally not being able to do all the testing we wanted at first because of the lack of time.\nwhat we learned\nduring these 24 hours (a bit less because we slept) we learnt how to render stls and also how to compute approximate similarity on images. it was a big challenge because we learnt that, because it's a 3d model, it can be in a lot of different angles so it's really difficult to do a match and if the figures are similar, it's difficult to differentiate between them.\nwhat's next for identificador de impressions 3d per imatge\nmaybe add more models and also if there is enough data maybe train a machine learning model to be more effective in the render so you can upload more pictures, nowadays there are just a few models.", "sortedWord": "None", "removed": "Nan", "score": 5, "comments": 0, "media": null, "medialink": null, "identifyer": 59501462}, {"Unnamed: 0": 1468, "autor": "Insect Counting with Deep Learning", "date": null, "content": "Inspiration\nAutomatizing hard and tedious processes is a challenging task, and nowadays everything is possible with AI and Data Science technologies. We wanted to use state-of-the-art technologies in the field AI to overcome the problem of automatizing object detection and counting.\nWhat it does\nWe developed a web application in React that allows users to upload photos of yellow sticky insect traps in order to automatically count the number of insects that appear in the image. The tool displays the processed image showing the retections and counting performed (distinguishing between small and big insects).\nHow we built it\nWe used a deep learning object recognition model pre-trained with thousands of images, called YOLOR (a recent and improved version of the YOLO model). We tweaked the model to detect insects given the slight similarity that they have with birds. The web application has been developed with React for the front-end and Django for the backend.\nChallenges we ran into\nWe wanted to improve the detections and we found out the model precision strongly depends on the image resolution used for inference. We used super-resolution tools to upsample the photos to have greater resolution and it turned out to be a significant improvement for the detection, concluding that it is important to provide HQ images to the model if we want to get good results. Also, giving an accuracy metric for our problem is quite hard since we don't exactly know the number of insects in each photo (it's also hard to determine for experts), so we had to show qualitative results of the detections to evaluate the model performance.\nAccomplishments that we're proud of\nThe model gives a really good estimation in most of the provided images. We managed to implement a complete deep learning-based solution with a user interface that is able to predict the number of insects for any given image.\nWhat we learned\nWe are astonished by the versatility of Deep Learning models and the potential they have to adapt to different tasks.\nWhat's next for Insect Counting with Deep Learning\nRe-train the network with annotated data, to improve the model detection.", "link": "https://devpost.com/software/insect-counting-with-deep-learning", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nautomatizing hard and tedious processes is a challenging task, and nowadays everything is possible with ai and data science technologies. we wanted to use state-of-the-art technologies in the field ai to overcome the problem of automatizing object detection and counting.\nwhat it does\nwe developed a web application in react that allows users to upload photos of yellow sticky insect traps in order to automatically count the number of insects that appear in the image. the -----> tool !!!  displays the processed image showing the retections and counting performed (distinguishing between small and big insects).\nhow we built it\nwe used a deep learning object recognition model pre-trained with thousands of images, called yolor (a recent and improved version of the yolo model). we tweaked the model to detect insects given the slight similarity that they have with birds. the web application has been developed with react for the front-end and django for the backend.\nchallenges we ran into\nwe wanted to improve the detections and we found out the model precision strongly depends on the image resolution used for inference. we used super-resolution tools to upsample the photos to have greater resolution and it turned out to be a significant improvement for the detection, concluding that it is important to provide hq images to the model if we want to get good results. also, giving an accuracy metric for our problem is quite hard since we don't exactly know the number of insects in each photo (it's also hard to determine for experts), so we had to show qualitative results of the detections to evaluate the model performance.\naccomplishments that we're proud of\nthe model gives a really good estimation in most of the provided images. we managed to implement a complete deep learning-based solution with a user interface that is able to predict the number of insects for any given image.\nwhat we learned\nwe are astonished by the versatility of deep learning models and the potential they have to adapt to different tasks.\nwhat's next for insect counting with deep learning\nre-train the network with annotated data, to improve the model detection.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 1, "media": null, "medialink": null, "identifyer": 59501468}, {"Unnamed: 0": 1498, "autor": "Ultimate Division", "date": null, "content": "Inspiration\nWe all love football and have been in game development for 4 years. We had a football game project, but the right time never came to start working on it. That is, until NFT adoption made this game project a perfect one to build as an NFT game. The team is excited to working on it and to bring innovations as we go.\nWhat it does\nUltimate Division (UD) is a Play to Earn football simulator game with card collecting mechanics based on blockchain. Players can join the game for free and start their own football clubs, grind the game to get in-game items. To play with others, they are invited to compete in weekly tournaments that keep a competition ledger. The events and results of the game are fully transparent and are stored on blockchain. For players, who are ranked high in the competition divisions, in-game rewards are more lucrative to the point where playing well can be a successful Play to Earn strategy for some. Unlike other football simulators, UD does not control player-owned assets and doesn't limit real- money trades. We aim to bridge the gap between a sport simulator game and dedicated fans, who want to build their clubs for years to come.\nHow we built it\nWe used Golang for back-end and React for front-end. Our game uses metamask integration and the smart contracts are built on Solidity.\nChallenges we ran into\nMaking a dynamic game where assets are stored on blockchain required us to come up with a way to manage NFT minting authorisation and build custom listeners for ownership changes outside of the game.\nAccomplishments that we're proud of\nWe've made a great tool of P2E Gaming for UD. For games like Axie, most players need to rent NFTs to be able to play. In Ultimate Division, we have created a manager role, like in real football. Players don't need to own expensive team to be able to play.\nWhat we learned\nWe've learned a lot of tricks in managing NFTs within a game and handling private keys of the system in a secure manner.\nWhat's next for Ultimate Division\nGetting licenses from football clubs is our next big milestone.", "link": "https://devpost.com/software/ultimate-division", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe all love football and have been in game development for 4 years. we had a football game project, but the right time never came to start working on it. that is, until nft adoption made this game project a perfect one to build as an nft game. the team is excited to working on it and to bring innovations as we go.\nwhat it does\nultimate division (ud) is a play to earn football simulator game with card collecting mechanics based on blockchain. players can join the game for free and start their own football clubs, grind the game to get in-game items. to play with others, they are invited to compete in weekly tournaments that keep a competition ledger. the events and results of the game are fully transparent and are stored on blockchain. for players, who are ranked high in the competition divisions, in-game rewards are more lucrative to the point where playing well can be a successful play to earn strategy for some. unlike other football simulators, ud does not control player-owned assets and doesn't limit real- money trades. we aim to bridge the gap between a sport simulator game and dedicated fans, who want to build their clubs for years to come.\nhow we built it\nwe used golang for back-end and react for front-end. our game uses metamask integration and the smart contracts are built on solidity.\nchallenges we ran into\nmaking a dynamic game where assets are stored on blockchain required us to come up with a way to manage nft minting authorisation and build custom listeners for ownership changes outside of the game.\naccomplishments that we're proud of\nwe've made a great -----> tool !!!  of p2e gaming for ud. for games like axie, most players need to rent nfts to be able to play. in ultimate division, we have created a manager role, like in real football. players don't need to own expensive team to be able to play.\nwhat we learned\nwe've learned a lot of tricks in managing nfts within a game and handling private keys of the system in a secure manner.\nwhat's next for ultimate division\ngetting licenses from football clubs is our next big milestone.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59501498}, {"Unnamed: 0": 1508, "autor": "KwickQuick", "date": null, "content": "Kwickquick-Client\nYou can view the kwickquick-API\nand the Scanner App kwickquick-scanner\nThe organization dashboard\nThe Scanner\nInspiration\nWho enjoys being stuck in a queue, waiting for longer time just to buy something? I guest no one. Let's be honest, Queues are bad, annoying for customers and a great loss for businesses. How, you may ask??.To elaborate, imaging you going to get just a can of milk for $5 and you end up in a long queue waiting up to 45 min to to pay for the item. As a customer, that is a frustrating, unsatisfactory, and a big waist of time. Typically, you won't like buy from that mall whenever you notice a queue there. The business loses you. Fortunately, there's an actionable tool to dissmiss this problem. Devcamp a small group of young developers plans to deploy kwickquick, an enterprise Fintech application focused primarily on MVP pattern.\nWhat it does\nLet's take a closer look at how this works. The application is made up of two different apps, one for organizations (businesses) and the other one for consumers (users). Organizations have to sign-up to make use of our services, and then they can now add their products and generate a unique QR code for each products which would contain the products info and also prints this QR code and then attached it to their physical product.\nThe second app is essentially a scanner with which customers can scan the above-mentioned QR code. After the QR code scan, customers can see all the product details, including information like nutrition, components or ingredients and care instructions, a dialog box for quantity, size, colour, etc... and then automatically adds them to cart for you to pay, still within the app with your card info. Basically, kwickquick introduces a customer to self-serving himself in the mall while limiting the need for man to man contact. With just a few clicks, from whereever your desired product is stacked in the mall, you can scan the QR code of the product and pay your bills at the spot.\nHow we built it\nThe whole application is built using Javascript HTML CSS for the frontend and Nodejs/Express.js for the backend api and postgresql for database.\nChallenges we ran into\nDeveloping all complex functionalities in a short period of time and connecting the two applications was one of the challenges we faced.\nAccomplishments that we're proud of\nAl though it not yet a full fledge app yet, but we were proud to bring this two systems into reality and also solving real world case problems\nWhat we learned\nWe learnt about how users flow through the app.\nWhat's next for KwickQuick\nAs the team lead software developer weve decided in taking kwickquick to the next place and also funding it", "link": "https://devpost.com/software/kwickquick", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "kwickquick-client\nyou can view the kwickquick-api\nand the scanner app kwickquick-scanner\nthe organization dashboard\nthe scanner\ninspiration\nwho enjoys being stuck in a queue, waiting for longer time just to buy something? i guest no one. let's be honest, queues are bad, annoying for customers and a great loss for businesses. how, you may ask??.to elaborate, imaging you going to get just a can of milk for $5 and you end up in a long queue waiting up to 45 min to to pay for the item. as a customer, that is a frustrating, unsatisfactory, and a big waist of time. typically, you won't like buy from that mall whenever you notice a queue there. the business loses you. fortunately, there's an actionable -----> tool !!!  to dissmiss this problem. devcamp a small group of young developers plans to deploy kwickquick, an enterprise fintech application focused primarily on mvp pattern.\nwhat it does\nlet's take a closer look at how this works. the application is made up of two different apps, one for organizations (businesses) and the other one for consumers (users). organizations have to sign-up to make use of our services, and then they can now add their products and generate a unique qr code for each products which would contain the products info and also prints this qr code and then attached it to their physical product.\nthe second app is essentially a scanner with which customers can scan the above-mentioned qr code. after the qr code scan, customers can see all the product details, including information like nutrition, components or ingredients and care instructions, a dialog box for quantity, size, colour, etc... and then automatically adds them to cart for you to pay, still within the app with your card info. basically, kwickquick introduces a customer to self-serving himself in the mall while limiting the need for man to man contact. with just a few clicks, from whereever your desired product is stacked in the mall, you can scan the qr code of the product and pay your bills at the spot.\nhow we built it\nthe whole application is built using javascript html css for the frontend and nodejs/express.js for the backend api and postgresql for database.\nchallenges we ran into\ndeveloping all complex functionalities in a short period of time and connecting the two applications was one of the challenges we faced.\naccomplishments that we're proud of\nal though it not yet a full fledge app yet, but we were proud to bring this two systems into reality and also solving real world case problems\nwhat we learned\nwe learnt about how users flow through the app.\nwhat's next for kwickquick\nas the team lead software developer weve decided in taking kwickquick to the next place and also funding it", "sortedWord": "None", "removed": "Nan", "score": 5, "comments": 2, "media": null, "medialink": null, "identifyer": 59501508}, {"Unnamed: 0": 1540, "autor": "ClimbBro", "date": null, "content": "Inspiration\nWe are both enthusiastic rock climbers and noticed a gap in the market for apps to assist in training plans and suggesting suitable climbs. We made this to help fellow climbers.\nWhat it does\nIt uses a reinforcement leaning algorithm to suggest climbs, predict how hard the user will find them, then when the user logs the climb, compare the prediction to the actual value and update the weightings.\nClimbing centres can add routes to our system using the route setter tool, which are then shown to the users of the app, with the most suitable climbs recommended to the user, and then used to train our model.\nHow we built it\nWe used NodeJs for our backend, along with MySql for the main database and MongoDB to store data used for the visualisation. For the front end we used jQuery and the Bulma framework.\nFor our route setting component, we extended a framework called KonvaJS to suit our use case. We did this by adding vectors to a stack each time a user clicked on a point on the wall, and then drawing lines through that vectors in the stack to show the route. We then extended this to multiple routes, so that route setters can switch between highlighted routes on the wall by selecting the new route.\nOur reinforcement learning algorithm works by a matrix of weightings for metric combinations, and then predicting different responses from a climber based on their previous responses to routes. We then compare our prediction to the climbers actual response, and update our matrix of weightings accordingly, using the formula we created (shown in the video).\nChallenges we ran into\nThe main challenge was coming up with our own reinforcement leaning system, as we both had little experience in it. We initially thought we could use a deep reinforcement learning system using a neural network, but we then discovered this may not be suitable so opted for a alternative reinforcement learning algorithm that we designed ourselves.\nAdditionally, using both MySql and MongoDB at the same time was difficult to set up but was eventually worth it.\nAccomplishments that we're proud of\nWe're proud to have built and deployed a full stack web app in under 24 hours, and the general user interface/design of the system.\nWhat we learned\nWe've learned a lot about deep learning (even though in the end we realised it wasn't suitable for our use case), and also general reinforcement learning and the mathematics behind it.\nWhat's next for ClimbBro\nWe would like to take some time to restructure some of the project to make it more maintainable and we would like to further develop our AI system by including more metrics and potentially using collaborative filtering as well as our current system to provide better suggestions.", "link": "https://devpost.com/software/climbbro", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe are both enthusiastic rock climbers and noticed a gap in the market for apps to assist in training plans and suggesting suitable climbs. we made this to help fellow climbers.\nwhat it does\nit uses a reinforcement leaning algorithm to suggest climbs, predict how hard the user will find them, then when the user logs the climb, compare the prediction to the actual value and update the weightings.\nclimbing centres can add routes to our system using the route setter -----> tool !!! , which are then shown to the users of the app, with the most suitable climbs recommended to the user, and then used to train our model.\nhow we built it\nwe used nodejs for our backend, along with mysql for the main database and mongodb to store data used for the visualisation. for the front end we used jquery and the bulma framework.\nfor our route setting component, we extended a framework called konvajs to suit our use case. we did this by adding vectors to a stack each time a user clicked on a point on the wall, and then drawing lines through that vectors in the stack to show the route. we then extended this to multiple routes, so that route setters can switch between highlighted routes on the wall by selecting the new route.\nour reinforcement learning algorithm works by a matrix of weightings for metric combinations, and then predicting different responses from a climber based on their previous responses to routes. we then compare our prediction to the climbers actual response, and update our matrix of weightings accordingly, using the formula we created (shown in the video).\nchallenges we ran into\nthe main challenge was coming up with our own reinforcement leaning system, as we both had little experience in it. we initially thought we could use a deep reinforcement learning system using a neural network, but we then discovered this may not be suitable so opted for a alternative reinforcement learning algorithm that we designed ourselves.\nadditionally, using both mysql and mongodb at the same time was difficult to set up but was eventually worth it.\naccomplishments that we're proud of\nwe're proud to have built and deployed a full stack web app in under 24 hours, and the general user interface/design of the system.\nwhat we learned\nwe've learned a lot about deep learning (even though in the end we realised it wasn't suitable for our use case), and also general reinforcement learning and the mathematics behind it.\nwhat's next for climbbro\nwe would like to take some time to restructure some of the project to make it more maintainable and we would like to further develop our ai system by including more metrics and potentially using collaborative filtering as well as our current system to provide better suggestions.", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59501540}, {"Unnamed: 0": 1581, "autor": "Theta Infinity Studio", "date": null, "content": "First and foremost\nThanks for this opportunity, I added some extra information to the following blog post.\nWhole project info at Theta Infinity Studio;\nInspiration\nStreaming is the future of media, no matter if you are watching tv shows, live streams, podcasts, or playing video games online.\nBUT, from my perspective, we are not taking advantage of the whole potential this technology offers.\nMore than 10 years ago I had a job at a streaming company. I had a lot of ideas that I could never implement and Theta looks like the perfect project and technology to become motivated again.\nTheta Infinity Studio is a collection of small proof of concepts that hints how multiple decentralized platforms could integrate to bring media streaming to the next level\nWhat it does\nThe Project consists of two main products:\nInfinity Studio\nIs a Decentralized CMS for media management, with Real-time Analytics, Advertisements, eCommerce, and Real-time events management.\nAnalytics\nReal-time analytics for both videos on demand and live stream using GunJS rendered on the browser using D3.js Library\nIn order to get the [COUNTRY] I am relying on the service provided by DB IP In order to get the [LANGUAGE] I am relying on the information provided by the browser\nVoD\nstats/[PROJECT-ID]/media/views/total\nstats/[PROJECT-ID]/media/views/date/[YEAR]/[MONTH]/[DATE]\nstats/[PROJECT-ID]/media/views/country/[COUNTRY]/[YEAR]/[MONTH]/[DATE]\nstats/[PROJECT-ID]/media/views/language/[LANGUAGE]/[YEAR]/[MONTH]/[DATE]\nThe order of the elements above is important because it could allow filtering by language/country, year, month, and date. This feature has not been added to the UI but the information is there to be used in the future.\nVoD analytics also display the most viewed moments on the video which could be useful for better product placement.\nLivestream\nstats/[PROJECT-ID]/media/views/live/[CURRENT-MINUTE]/total\nstats/[PROJECT-ID]/media/views/live/[CURRENT-MINUTE]/country/[COUNTRY]\n[CURRENT-MINUTE]is the number of minutes since 1970. This allows me to group values per minute, having a historic per minute of viewers and country.\nLivestream analytics also display a real-time user counter at the bottom\nCaveats\nGunJS information is decentralized but could also be manipulated by users who would know how to access the network. This could render this feature useless when using it as a production-ready solution.\nVoD Overlays\nThe overlays section allows users to add interactive layers above the media either served via streaming or on-demand.\nVoD Overlays can trigger 3 types of overlays: Products, Advertisements, and Events.\nProduct Placement\nIn order to create a product placement, the editor must click on the record button and then track the element on the media. Pressing the mouse down will start recording the product and releasing the button will stop recording the position. Those values can be overridden by replaying the media and recording during the same time as previously recorded.\nProduct placement integrates with DShop Network to provide content to those ads. Although it would be possible to implement a shop inside Theta Infinity the idea was to show the capabilities of integration with Theta Network with other platforms. That said, DShop provides not only a Product CMS but also a payment system out of the box.\nAdvertisements\nAdvertisements can be placed anywhere on the video and it will display an ad (320x250 is the only standard size implemented for this PoC).\nAdvertisements are provided by AdEx Network and its content might depend on the configuration settings on their platform. Similarly to DShop, it could be possible to implement an advertisement section inside Theta Infinity but the idea is to integrate with as many decentralized platforms as possible.\nThe only limitation is that AdEx requires Ad Slots to be displayed only on a single certified domain, this makes it difficult to test but nonetheless, the PoC has been successful.\nEvents\nEvents are messages sent to the player at certain moments on time. For example, you might want to change the player website background at a given point in time.\nLivestream Overlays\nLivestream Overlays are designed more like an extendable library than like a limited number of features. Basically, the editor can send events to all Livestream viewers.\nIn order to achieve this, I used Streamr Network to send real-time events that will be interpreted by the media players.\nFor this proof of concept I have implemented 3 real-time events:\nPoll\nThe poll event allows editors to notify the player that must display an interactive poll where users can vote an option (note that the voting system has not been implemented)\nRumble\nRumble is a simple event that makes the player interface shake\nAnimation\nReal-time animation events allow editors to notify players to display given animations. These animations are JSON files provided by Lottie. I chose Lottie because it could also be used on mobile devices such as iOS or Android\nIn the image, you can see a Celebration Animation from Lottie> Current media players have limited options regarding advertisements or notifications, but this can be changed by adding interaction increasing the engagement\nInfinity Player\nA VideoJS wrapper provides an extra layer for extra features explained above\nHow we built it\nInfinity Studio\nInfinity Studio is a React-based project written in Javascript and hosted as a static website on AWS S3. Considering the platform does not use any server and all integrations are decentralized this means this tool costs almost 0 cents to maintain.\nBelow there is a list of the platforms this project is integrated with (at different levels)\nStreamr Network: Used for Real-time events, just touching the surface of what would be possible. On this demo, I am just sending simple events but those events could not only be triggered manually by the CMS admin but also automatically or from other sources such as IoT devices.\nDshop: Used for dynamic product placement on video. For example, companies like Netflix could use this technology to sell anything on their shows, from mugs to T-shirts. For the demo, this is done manually by the CMS editor but soon enough this will be done automatically using AI.\nGunJS: I could not find a decentralized analytics platform I could integrate with so I decided to implement one of my own. It's a very simple platform and it does not intend to be a professional solution.\nAdEx: I used AdEx to display advertisements at certain points of the video.\nIPFS: Used as a decentralized CDN to store media files and OrbitDB. For example, when creating a thumbnail strip, this is stored on IPFS.\nOrbitDB: Used as a decentralized database to store information about the projects, similar to what would you do with MongoDB\nMetamask: Used as a decentralized Identity provider but it could be used for far more applications, for example, integrating revenue from the streamed media.\nLottieFiles: The Infinity player also includes the Lottie library which allows the possibility of displaying any animation on top of the video, this is used when launching real-time animations on live streams.\nInfinity Player\nInfinity Player is written in vanilla Javascript to avoid incompatibilities with any possible future integrations (although it would normally be embedded as an iframe)\nChallenges we ran into\nAt the time of this writing, I couldn't achieve to integrate the Infinity Player with a real media stream coming from the Theta Edge Node. Although I did find some repositories explaining possible solutions none did work\nAdEx AdEx network requires the project to be hosted on a certified domain and each Ad Slot allows only one. This makes it difficult to test it locally or in different environments.\nDShop\nDuring the implementation of this project, Dshop origin removed the image from the AWS Marketplace making it really difficult to finish the feature I tested a few weeks ago. This reduced the scope of the features I wanted to deliver.\nAccomplishments that we're proud of\nWell, I achieved to integrate most of the platforms/technologies I thought of: Advertisements, Real-time, eCommerce, and so on\nWhat we learned\nBlockchain/ Decentralized networks are still at a very early stage and there is an important lack of platforms for dedicated markets such as Analytics, Online Bets, Gaming, IoT, ...\nSome of those platforms do exist, but there is no easy integration or they are just starting\nWhat's next for Theta Infinity Studio\nWell, first I'd like to learn about this project feedback and see what people think about it. What I did right and what I did wrong.\nIn my head, those ideas can be interesting but to the rest of the world might be not :D\nI hope you like the project as much as I did implementing it\nX", "link": "https://devpost.com/software/theta-infinity-studio", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "first and foremost\nthanks for this opportunity, i added some extra information to the following blog post.\nwhole project info at theta infinity studio;\ninspiration\nstreaming is the future of media, no matter if you are watching tv shows, live streams, podcasts, or playing video games online.\nbut, from my perspective, we are not taking advantage of the whole potential this technology offers.\nmore than 10 years ago i had a job at a streaming company. i had a lot of ideas that i could never implement and theta looks like the perfect project and technology to become motivated again.\ntheta infinity studio is a collection of small proof of concepts that hints how multiple decentralized platforms could integrate to bring media streaming to the next level\nwhat it does\nthe project consists of two main products:\ninfinity studio\nis a decentralized cms for media management, with real-time analytics, advertisements, ecommerce, and real-time events management.\nanalytics\nreal-time analytics for both videos on demand and live stream using gunjs rendered on the browser using d3.js library\nin order to get the [country] i am relying on the service provided by db ip in order to get the [language] i am relying on the information provided by the browser\nvod\nstats/[project-id]/media/views/total\nstats/[project-id]/media/views/date/[year]/[month]/[date]\nstats/[project-id]/media/views/country/[country]/[year]/[month]/[date]\nstats/[project-id]/media/views/language/[language]/[year]/[month]/[date]\nthe order of the elements above is important because it could allow filtering by language/country, year, month, and date. this feature has not been added to the ui but the information is there to be used in the future.\nvod analytics also display the most viewed moments on the video which could be useful for better product placement.\nlivestream\nstats/[project-id]/media/views/live/[current-minute]/total\nstats/[project-id]/media/views/live/[current-minute]/country/[country]\n[current-minute]is the number of minutes since 1970. this allows me to group values per minute, having a historic per minute of viewers and country.\nlivestream analytics also display a real-time user counter at the bottom\ncaveats\ngunjs information is decentralized but could also be manipulated by users who would know how to access the network. this could render this feature useless when using it as a production-ready solution.\nvod overlays\nthe overlays section allows users to add interactive layers above the media either served via streaming or on-demand.\nvod overlays can trigger 3 types of overlays: products, advertisements, and events.\nproduct placement\nin order to create a product placement, the editor must click on the record button and then track the element on the media. pressing the mouse down will start recording the product and releasing the button will stop recording the position. those values can be overridden by replaying the media and recording during the same time as previously recorded.\nproduct placement integrates with dshop network to provide content to those ads. although it would be possible to implement a shop inside theta infinity the idea was to show the capabilities of integration with theta network with other platforms. that said, dshop provides not only a product cms but also a payment system out of the box.\nadvertisements\nadvertisements can be placed anywhere on the video and it will display an ad (320x250 is the only standard size implemented for this poc).\nadvertisements are provided by adex network and its content might depend on the configuration settings on their platform. similarly to dshop, it could be possible to implement an advertisement section inside theta infinity but the idea is to integrate with as many decentralized platforms as possible.\nthe only limitation is that adex requires ad slots to be displayed only on a single certified domain, this makes it difficult to test but nonetheless, the poc has been successful.\nevents\nevents are messages sent to the player at certain moments on time. for example, you might want to change the player website background at a given point in time.\nlivestream overlays\nlivestream overlays are designed more like an extendable library than like a limited number of features. basically, the editor can send events to all livestream viewers.\nin order to achieve this, i used streamr network to send real-time events that will be interpreted by the media players.\nfor this proof of concept i have implemented 3 real-time events:\npoll\nthe poll event allows editors to notify the player that must display an interactive poll where users can vote an option (note that the voting system has not been implemented)\nrumble\nrumble is a simple event that makes the player interface shake\nanimation\nreal-time animation events allow editors to notify players to display given animations. these animations are json files provided by lottie. i chose lottie because it could also be used on mobile devices such as ios or android\nin the image, you can see a celebration animation from lottie> current media players have limited options regarding advertisements or notifications, but this can be changed by adding interaction increasing the engagement\ninfinity player\na videojs wrapper provides an extra layer for extra features explained above\nhow we built it\ninfinity studio\ninfinity studio is a react-based project written in javascript and hosted as a static website on aws s3. considering the platform does not use any server and all integrations are decentralized this means this -----> tool !!!  costs almost 0 cents to maintain.\nbelow there is a list of the platforms this project is integrated with (at different levels)\nstreamr network: used for real-time events, just touching the surface of what would be possible. on this demo, i am just sending simple events but those events could not only be triggered manually by the cms admin but also automatically or from other sources such as iot devices.\ndshop: used for dynamic product placement on video. for example, companies like netflix could use this technology to sell anything on their shows, from mugs to t-shirts. for the demo, this is done manually by the cms editor but soon enough this will be done automatically using ai.\ngunjs: i could not find a decentralized analytics platform i could integrate with so i decided to implement one of my own. it's a very simple platform and it does not intend to be a professional solution.\nadex: i used adex to display advertisements at certain points of the video.\nipfs: used as a decentralized cdn to store media files and orbitdb. for example, when creating a thumbnail strip, this is stored on ipfs.\norbitdb: used as a decentralized database to store information about the projects, similar to what would you do with mongodb\nmetamask: used as a decentralized identity provider but it could be used for far more applications, for example, integrating revenue from the streamed media.\nlottiefiles: the infinity player also includes the lottie library which allows the possibility of displaying any animation on top of the video, this is used when launching real-time animations on live streams.\ninfinity player\ninfinity player is written in vanilla javascript to avoid incompatibilities with any possible future integrations (although it would normally be embedded as an iframe)\nchallenges we ran into\nat the time of this writing, i couldn't achieve to integrate the infinity player with a real media stream coming from the theta edge node. although i did find some repositories explaining possible solutions none did work\nadex adex network requires the project to be hosted on a certified domain and each ad slot allows only one. this makes it difficult to test it locally or in different environments.\ndshop\nduring the implementation of this project, dshop origin removed the image from the aws marketplace making it really difficult to finish the feature i tested a few weeks ago. this reduced the scope of the features i wanted to deliver.\naccomplishments that we're proud of\nwell, i achieved to integrate most of the platforms/technologies i thought of: advertisements, real-time, ecommerce, and so on\nwhat we learned\nblockchain/ decentralized networks are still at a very early stage and there is an important lack of platforms for dedicated markets such as analytics, online bets, gaming, iot, ...\nsome of those platforms do exist, but there is no easy integration or they are just starting\nwhat's next for theta infinity studio\nwell, first i'd like to learn about this project feedback and see what people think about it. what i did right and what i did wrong.\nin my head, those ideas can be interesting but to the rest of the world might be not :d\ni hope you like the project as much as i did implementing it\nx", "sortedWord": "None", "removed": "Nan", "score": 42, "comments": 0, "media": null, "medialink": null, "identifyer": 59501581}, {"Unnamed: 0": 1603, "autor": "OnChain Foundation", "date": null, "content": "Inspiration\nWe are inspired by the selfless work of individuals in the charitable giving & blockchain industry that are helping to make the world a better place everyday.\nWhat it does/who we are\nOn-Chain Foundation(OCF) is a C-Corp based in Wyoming that aims to automate and decentralize charitable giving and fund development within the digital asset ecosystem. We are building a system top to bottom that will serve as a foundation for executing individualized philanthropic objectives within the digital asset ecosystem, while providing a simplified & affordable path to incorporating digital assets as funding sources for non profit organizations regardless of size or budget.\nHow we built it\nWe built this proof of concept with Solidity and Java Script using Remix and Truffle development framework. We utilized SushiSwap, Chainlink Keepers and Moralis as our decentralized integrations for this Hackathon. We used SushiSwap decentalized liquidity for token swaps because of their relationship with Chainlink Price Feeds. We have Chainlink Keepers integrated to automate the swaps and token transfers. Moralis is our framework for web3 development.\nChallenges we ran into\nOur teams biggest challenge to date was establishing how we wanted to execute the development of our smart-contacts while remaining Keepers compatible.\nAccomplishments that we're proud of\nWe are proud of our legal implementation, our innovative way to access decentralized liquidity via SushiSwap for charitable giving, and our use case of Chainlink Keepers to make trust-less on-chain transfers of charitable donations directly to a secure multi-signature wallet.\nWhat we learned\nWe learned that there is a need to integrate digital assets into the multi-billion dollar charitable giving industry, and that we can significantly impact the barrier of entry for small to medium size NPO's looking to accept digital assets. Our understanding grew for the community demand for tax-planning tools for investors and corporations will only increase with the adoption of digital assets, as well as worldwide demand to provide open source, on-chain information on the impact of users donations.\nWhat's next for OnChainFoundation\nWe are excited to implement in the future; Onboarding existing small to medium size non-profits to the digital asset ecosystem, creating our impact fund to provide donors with a innovative tax planning tool that will increase the amount of revenue for NPO's, quantifying our donors impact with on-chain data, reducing workforce cost for NPO's by generating automated tax receipts and thank you communications, and creating our own NFT Marketplace for listing charitable NFT projects.", "link": "https://devpost.com/software/onchainfoundation-sdemr9", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe are inspired by the selfless work of individuals in the charitable giving & blockchain industry that are helping to make the world a better place everyday.\nwhat it does/who we are\non-chain foundation(ocf) is a c-corp based in wyoming that aims to automate and decentralize charitable giving and fund development within the digital asset ecosystem. we are building a system top to bottom that will serve as a foundation for executing individualized philanthropic objectives within the digital asset ecosystem, while providing a simplified & affordable path to incorporating digital assets as funding sources for non profit organizations regardless of size or budget.\nhow we built it\nwe built this proof of concept with solidity and java script using remix and truffle development framework. we utilized sushiswap, chainlink keepers and moralis as our decentralized integrations for this hackathon. we used sushiswap decentalized liquidity for token swaps because of their relationship with chainlink price feeds. we have chainlink keepers integrated to automate the swaps and token transfers. moralis is our framework for web3 development.\nchallenges we ran into\nour teams biggest challenge to date was establishing how we wanted to execute the development of our smart-contacts while remaining keepers compatible.\naccomplishments that we're proud of\nwe are proud of our legal implementation, our innovative way to access decentralized liquidity via sushiswap for charitable giving, and our use case of chainlink keepers to make trust-less on-chain transfers of charitable donations directly to a secure multi-signature wallet.\nwhat we learned\nwe learned that there is a need to integrate digital assets into the multi-billion dollar charitable giving industry, and that we can significantly impact the barrier of entry for small to medium size npo's looking to accept digital assets. our understanding grew for the community demand for tax-planning tools for investors and corporations will only increase with the adoption of digital assets, as well as worldwide demand to provide open source, on-chain information on the impact of users donations.\nwhat's next for onchainfoundation\nwe are excited to implement in the future; onboarding existing small to medium size non-profits to the digital asset ecosystem, creating our impact fund to provide donors with a innovative tax planning -----> tool !!!  that will increase the amount of revenue for npo's, quantifying our donors impact with on-chain data, reducing workforce cost for npo's by generating automated tax receipts and thank you communications, and creating our own nft marketplace for listing charitable nft projects.", "sortedWord": "None", "removed": "Nan", "score": 6, "comments": 0, "media": null, "medialink": null, "identifyer": 59501603}, {"Unnamed: 0": 1611, "autor": "NFTest", "date": null, "content": "Inspiration\nNFTs are a very interesting new data structure. Developers are just beginning to explore the new possibilities that NFTs present. I wanted to create an application that might extend these possibilities. And I wanted to create a tool that might help others.\nWhat it does\nNFTest allows users to create tests and award custom NFTs based on results. It may be useful for educators and could help onboard new users to our space.\nHow I built it\nNFTest was built using Web3.js as well as JavaScript, PHP, and MySQL.\nChallenges I ran into\nI have never written a smartcontract or made a Chainlink request before so those were new things for me. But I had a vision and just set out building the project. I figured that I could hack my way through the new languages, and I did.\nAccomplishments that I'm proud of\nI am pleased that I deployed my application on Avalanche, Polygon, and Ethereum Mainnet. These are all chains that I actually use, so it was cool, almost surreal, to get to interact with them as a developer.\nWhat I learned\nI learned that building Web 3 apps with Web3.js is fun and easy. I wrote some Solidity and learned how to create user interfaces that interact with smartcontracts. I learned how to connect to Chainlink oracles and make requests. I am excited to help build web 3.0 and may develop a game next. That could be fun.\nWhat's next for NFTest\nI see NFTest heading in several possible directions.\nIf it attracts users and they create a rich collection of tests and assets, I could see spinning it into some kind of DAO or tokenized community platform.\nIf it fails to attract a considerable user base, it could be pivoted into a secure online testing platform. I believe in the future that the demand for such a platform could exist and blockchains, specifically with the use of Chainlink oracles, could faciliate a more secure at-home test.\nAs a developer, I can add additional features and tools, like different visual styles for the testing interface, more question types, as well as open-ended test types, like personality tests. Some of these features can be given away or perhaps sold at a premium if such a model were to make sense.", "link": "https://devpost.com/software/nftest", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nnfts are a very interesting new data structure. developers are just beginning to explore the new possibilities that nfts present. i wanted to create an application that might extend these possibilities. and i wanted to create a -----> tool !!!  that might help others.\nwhat it does\nnftest allows users to create tests and award custom nfts based on results. it may be useful for educators and could help onboard new users to our space.\nhow i built it\nnftest was built using web3.js as well as javascript, php, and mysql.\nchallenges i ran into\ni have never written a smartcontract or made a chainlink request before so those were new things for me. but i had a vision and just set out building the project. i figured that i could hack my way through the new languages, and i did.\naccomplishments that i'm proud of\ni am pleased that i deployed my application on avalanche, polygon, and ethereum mainnet. these are all chains that i actually use, so it was cool, almost surreal, to get to interact with them as a developer.\nwhat i learned\ni learned that building web 3 apps with web3.js is fun and easy. i wrote some solidity and learned how to create user interfaces that interact with smartcontracts. i learned how to connect to chainlink oracles and make requests. i am excited to help build web 3.0 and may develop a game next. that could be fun.\nwhat's next for nftest\ni see nftest heading in several possible directions.\nif it attracts users and they create a rich collection of tests and assets, i could see spinning it into some kind of dao or tokenized community platform.\nif it fails to attract a considerable user base, it could be pivoted into a secure online testing platform. i believe in the future that the demand for such a platform could exist and blockchains, specifically with the use of chainlink oracles, could faciliate a more secure at-home test.\nas a developer, i can add additional features and tools, like different visual styles for the testing interface, more question types, as well as open-ended test types, like personality tests. some of these features can be given away or perhaps sold at a premium if such a model were to make sense.", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 2, "media": null, "medialink": null, "identifyer": 59501611}, {"Unnamed: 0": 1612, "autor": "Gratreat", "date": null, "content": "Purpose & Motivation\nThe major purpose of our project is to create a dynamic tool that allows college students to take care of their mental health on busy days with short but meaningful \u201chappy thoughts\u201d. With a simple process to keep the daily mood and gratitude logs on our app, students become aware of their wellbeing by taking action and gratitude for positive things/objects/events around them. The app also aims to encourage practicing gratitude and mindfulness practice for those who are suffering from anxiety and depression. By writing and keeping the gratitude logs through our app, users will improve their mental health and change their perception of life. The app intends to teach you how to be more thankful every day and treat yourself through the power of gratefulness.\nHow does the application work?\nThis application works on the web and it is adapted to mobile by running npm as indicated in the instructions in the Git repo.\nHow was the application developed?\nThis application was developed using React.js to build the user interfaces in a framework which was easy to follow by the involved developers. REsponsive web design was implemented to allow the application content to respond to the different screen and window sizes. During the application development, prototyping tools such as CodeSandBox were used to test algorithms online before directly implementing them to Visual Studio Editor. This allowed less-experienced developers to rapidly test their algorithms and feel free to play with modifications without breaking compiled algorithms. Other libraries such as Bootstrap and Material Icons from Google were used to speed up the styling and design. Developers contribute to the public repositories named \u201cGratreat\u201d and \u201cGratreat-Api\u201d of the GIT-MHAH-GratTreat Organization.\nHow to use the application\nUser will first have to sign in to the web app in order to access the tools Once the user has logged in they are given the options of creating a new entry, looking through their records or checking how many points they have to get a treat from a vendor When they pick a mood they will be redirected to a new page where they can select prompts or skip the prompts selection If they pick the prompts then the user would go to a section in the site where they use that prompt as a topic of their journal entry Each prompt has a save button so when the user has completed a specific log they can click on the that button in order for it to be in their records section If they chose to skip the prompts then the user would go to a blank journal entry section. After they write something down they would click on the submit button which will save their entry in the records section When user click on the submit button they will shown a pop up which show them how many points they earned with their entry as well as give them the options to either go the their records page or rewards Every time the user saves a log they will be given points which they can use for getting treats from vendors Users can access the treats page by clicking on the link on the footer nav They can select a treat using points that they earned\nDifficulties & Challenges faced during the design and/or development process.\nSeveral challenges were faced in various aspects of the application development. For this, difficulties and challenges have been divided into sections. Challenges faced during:\nDevelopment:\nLack of experience in building functions in React to render essential elements of the application. For instance, the code to build the final calendar (page number 6 in wireframe) required more programming experience/skills from the developers than the team thought. Swapping components was another feature that was impossible to implement for the same reason and was considered for page number 5 in wireframe.\nThe documentation to install dependencies using npm could be complicated when developers look for a specific open-resource up to date.\nThe addition of icons was difficult because the documentation was hard to find and the icons names were mixed up. We also found the names were repeating for more than one icon and the class name would not work to call the desired icon.\nCommunication: 1. The continuous communication between team members was affected due to the limited time that Google Meet or Zoom provides. Slack was useful for continuous texting, however it does not allow free video chats.\nGeneral organization: No major challenges were faced with the general organization of the project. Each group member had a role to play and were able to deliver items in a timely manner.\nTime Management: Some components were not completed due to the time limit. Including features such as the tracker page.\nGo-to-Market (How will the application be available to the public, and is it scalable?)\nThe project is scalable since other features are needed to be added such as user authentication which will help keep track users logs and also gives indications of how many people are continuously using the app. Second, we can make the website available as a mobile app on the app store which would make the website more convenient to use. Third, adding a swipe feature to the app", "link": "https://devpost.com/software/gratreat", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "purpose & motivation\nthe major purpose of our project is to create a dynamic -----> tool !!!  that allows college students to take care of their mental health on busy days with short but meaningful \u201chappy thoughts\u201d. with a simple process to keep the daily mood and gratitude logs on our app, students become aware of their wellbeing by taking action and gratitude for positive things/objects/events around them. the app also aims to encourage practicing gratitude and mindfulness practice for those who are suffering from anxiety and depression. by writing and keeping the gratitude logs through our app, users will improve their mental health and change their perception of life. the app intends to teach you how to be more thankful every day and treat yourself through the power of gratefulness.\nhow does the application work?\nthis application works on the web and it is adapted to mobile by running npm as indicated in the instructions in the git repo.\nhow was the application developed?\nthis application was developed using react.js to build the user interfaces in a framework which was easy to follow by the involved developers. responsive web design was implemented to allow the application content to respond to the different screen and window sizes. during the application development, prototyping tools such as codesandbox were used to test algorithms online before directly implementing them to visual studio editor. this allowed less-experienced developers to rapidly test their algorithms and feel free to play with modifications without breaking compiled algorithms. other libraries such as bootstrap and material icons from google were used to speed up the styling and design. developers contribute to the public repositories named \u201cgratreat\u201d and \u201cgratreat-api\u201d of the git-mhah-grattreat organization.\nhow to use the application\nuser will first have to sign in to the web app in order to access the tools once the user has logged in they are given the options of creating a new entry, looking through their records or checking how many points they have to get a treat from a vendor when they pick a mood they will be redirected to a new page where they can select prompts or skip the prompts selection if they pick the prompts then the user would go to a section in the site where they use that prompt as a topic of their journal entry each prompt has a save button so when the user has completed a specific log they can click on the that button in order for it to be in their records section if they chose to skip the prompts then the user would go to a blank journal entry section. after they write something down they would click on the submit button which will save their entry in the records section when user click on the submit button they will shown a pop up which show them how many points they earned with their entry as well as give them the options to either go the their records page or rewards every time the user saves a log they will be given points which they can use for getting treats from vendors users can access the treats page by clicking on the link on the footer nav they can select a treat using points that they earned\ndifficulties & challenges faced during the design and/or development process.\nseveral challenges were faced in various aspects of the application development. for this, difficulties and challenges have been divided into sections. challenges faced during:\ndevelopment:\nlack of experience in building functions in react to render essential elements of the application. for instance, the code to build the final calendar (page number 6 in wireframe) required more programming experience/skills from the developers than the team thought. swapping components was another feature that was impossible to implement for the same reason and was considered for page number 5 in wireframe.\nthe documentation to install dependencies using npm could be complicated when developers look for a specific open-resource up to date.\nthe addition of icons was difficult because the documentation was hard to find and the icons names were mixed up. we also found the names were repeating for more than one icon and the class name would not work to call the desired icon.\ncommunication: 1. the continuous communication between team members was affected due to the limited time that google meet or zoom provides. slack was useful for continuous texting, however it does not allow free video chats.\ngeneral organization: no major challenges were faced with the general organization of the project. each group member had a role to play and were able to deliver items in a timely manner.\ntime management: some components were not completed due to the time limit. including features such as the tracker page.\ngo-to-market (how will the application be available to the public, and is it scalable?)\nthe project is scalable since other features are needed to be added such as user authentication which will help keep track users logs and also gives indications of how many people are continuously using the app. second, we can make the website available as a mobile app on the app store which would make the website more convenient to use. third, adding a swipe feature to the app", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59501612}, {"Unnamed: 0": 1620, "autor": "Celo Tax", "date": null, "content": "Inspiration\nI love trying out the new projects on Celo and have built up 1,000 transactions. Now, doing taxes is a pain because Celo isn't well supported.\nWhat it does\nOn Celo.Tax you can download a ready-formatted csv file that can quickly be uploaded into tax software.\nHow we built it\nUsing Vercel and a python back end.\nChallenges we ran into\nHaving a smooth user login was tricky, but now we just let people sign in with their wallet and then they can do a one-click Pay & Download.\nAccomplishments that we're proud of\nWe've been quick in getting the tool up and are already generating revenue on our site.\nWhat's next for Celo.Tax\nWe're excited to move from a single payment to a subscription product so users can come back to re-download the file any time. We're also looking forward to providing users with direct calculations of their income and capital gains.", "link": "https://devpost.com/software/celo-tax", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ni love trying out the new projects on celo and have built up 1,000 transactions. now, doing taxes is a pain because celo isn't well supported.\nwhat it does\non celo.tax you can download a ready-formatted csv file that can quickly be uploaded into tax software.\nhow we built it\nusing vercel and a python back end.\nchallenges we ran into\nhaving a smooth user login was tricky, but now we just let people sign in with their wallet and then they can do a one-click pay & download.\naccomplishments that we're proud of\nwe've been quick in getting the -----> tool !!!  up and are already generating revenue on our site.\nwhat's next for celo.tax\nwe're excited to move from a single payment to a subscription product so users can come back to re-download the file any time. we're also looking forward to providing users with direct calculations of their income and capital gains.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59501620}, {"Unnamed: 0": 1645, "autor": "Euler Tools", "date": null, "content": "Euler Tools, founded 5 April 2021, follows a well-traveled entrepreneurial path. Jorge de los Santos, a gifted software engineer and new cryptocurrency investor, became frustrated with switching between tools to view current prices and liquidity, checking the status of his wallet, and performing research. So, Jorge set out to build a better and more inclusive experience.\nBy quickly building a beta application that addressed his initial frustrations, Jorge achieved his goal, but he didn\u2019t stop there. Not only did his tool provide a one-stop shop for prices and liquidity, wallet analytics, and crypto research, but he also built a worker-based architecture that reads blocks on the chain the moment they're created. The real-time nature of this new product exposes accurate, fast-changing cryptocurrency prices while competitors\u2019 pricing is sometimes hours old.\nThe platform bests the competitors who rely upon third-party data by extracting, processing, and storing the data in-house.\nNamed Euler Tools in homage to the 18th-century Swiss mathematician who founded graph theory and topology, it has enviable and controllable reliability and uptime because it has no reliance on other data providers.\nRealizing he was onto a winning product, Jorge bootstrapped Euler Tools, to benefit blockchain consumers. He built a small but mighty team that accomplished audacious goals in record time.\nEuler Tools\u2019 initial mission was to provide an all-in-one, unified, complete platform for blockchain data consumers, replacing a landscape of slow and inaccurate point solutions.\nThe company intends to introduce blockchain technology that solves the problems associated with the lack of transparency, participation, and control of the current financial system. Creating an available, verifiable, reliable, and scalable platform brings us one step closer to this vision.\nEuler Tools is committed to transparency, bidirectional communication, stakeholder inclusion, and aggressive innovation. A Single Platform Euler Tools is the first platform aggregating data from multiple blockchain sources, spanning Smart Contracts to the end-user application in a single place. The platform offers safe technology and data for governments, corporations, and individuals who want to build solutions around it.\nEuler Tools created its first utility token product, the $EULER, one month after envisioning the application. The Token fuels the ecosystem that the company generates. By allowing the owners to access discounts, services, voting, and benefits, we encourage users to buy and hold $EULER, which raises demand and generates revenue for the company.\nInfrastructure as Service (IaaS) Owned Nodes Euler Tools owns the infrastructure used to access the networks. This ownership ensures the availability and security of the data. These nodes are also offered as a service to consumers and corporations wanting to interact safely with them.\nEuler Tools nodes are currently syncing: \u25cf Ethereum \u25cf Binance Smart Chain \u25cf HecoChain\nPlatform As Service (PaaS) The Euler Tools platform extracts information from blockchain nodes and processes it in real-time before depositing it into a data lake. This data lake combines cold and hot storage, allowing consumers to perform myriad analytics, reports, searches, and queries against the data. The Euler PaaS operations perform decryption, enrichment, mapping, and normalization prior to exposing the data.\nPrice Feeds The nature of the Euler Tools architecture allows data streams to calculate real-time prices for assets with accuracy, and make the prices available immediately across multiple channels. WebSockets, Streams, and APIs are offered as part of this solution for all off-chain and on-chain prices.\nTax Reports The ownership of all historical prices for any token allows Euler Tools to calculate the exact price at which a token was traded. This precise price calculation allows taxes to be incorporated into all trades, enabling users to see the exact profitability of each trade. Software as a Service (SaaS) Applications Euler Explorer (WebApp) The current product provides a rich toolset for blockchain data consumers. The attractive and data-rich user interface includes a blockchain explorer for tokens, addresses, transactions, and pairs. The explorer provides drill-down charts. Investors can monitor their personal and business wallets and portfolios using Euler Tools visual interface.\nEuler Tools\u2019 live data feed provides real-time prices, liquidity, and transfers.\nEuler Tools provides customizable alerts and notifications. Users may select which devices receive alerts and notifications. In addition, users are provided with a rich interface to specify the conditions that trigger alerts and notifications.", "link": "https://devpost.com/software/euler-tools", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "euler tools, founded 5 april 2021, follows a well-traveled entrepreneurial path. jorge de los santos, a gifted software engineer and new cryptocurrency investor, became frustrated with switching between tools to view current prices and liquidity, checking the status of his wallet, and performing research. so, jorge set out to build a better and more inclusive experience.\nby quickly building a beta application that addressed his initial frustrations, jorge achieved his goal, but he didn\u2019t stop there. not only did his -----> tool !!!  provide a one-stop shop for prices and liquidity, wallet analytics, and crypto research, but he also built a worker-based architecture that reads blocks on the chain the moment they're created. the real-time nature of this new product exposes accurate, fast-changing cryptocurrency prices while competitors\u2019 pricing is sometimes hours old.\nthe platform bests the competitors who rely upon third-party data by extracting, processing, and storing the data in-house.\nnamed euler tools in homage to the 18th-century swiss mathematician who founded graph theory and topology, it has enviable and controllable reliability and uptime because it has no reliance on other data providers.\nrealizing he was onto a winning product, jorge bootstrapped euler tools, to benefit blockchain consumers. he built a small but mighty team that accomplished audacious goals in record time.\neuler tools\u2019 initial mission was to provide an all-in-one, unified, complete platform for blockchain data consumers, replacing a landscape of slow and inaccurate point solutions.\nthe company intends to introduce blockchain technology that solves the problems associated with the lack of transparency, participation, and control of the current financial system. creating an available, verifiable, reliable, and scalable platform brings us one step closer to this vision.\neuler tools is committed to transparency, bidirectional communication, stakeholder inclusion, and aggressive innovation. a single platform euler tools is the first platform aggregating data from multiple blockchain sources, spanning smart contracts to the end-user application in a single place. the platform offers safe technology and data for governments, corporations, and individuals who want to build solutions around it.\neuler tools created its first utility token product, the $euler, one month after envisioning the application. the token fuels the ecosystem that the company generates. by allowing the owners to access discounts, services, voting, and benefits, we encourage users to buy and hold $euler, which raises demand and generates revenue for the company.\ninfrastructure as service (iaas) owned nodes euler tools owns the infrastructure used to access the networks. this ownership ensures the availability and security of the data. these nodes are also offered as a service to consumers and corporations wanting to interact safely with them.\neuler tools nodes are currently syncing: \u25cf ethereum \u25cf binance smart chain \u25cf hecochain\nplatform as service (paas) the euler tools platform extracts information from blockchain nodes and processes it in real-time before depositing it into a data lake. this data lake combines cold and hot storage, allowing consumers to perform myriad analytics, reports, searches, and queries against the data. the euler paas operations perform decryption, enrichment, mapping, and normalization prior to exposing the data.\nprice feeds the nature of the euler tools architecture allows data streams to calculate real-time prices for assets with accuracy, and make the prices available immediately across multiple channels. websockets, streams, and apis are offered as part of this solution for all off-chain and on-chain prices.\ntax reports the ownership of all historical prices for any token allows euler tools to calculate the exact price at which a token was traded. this precise price calculation allows taxes to be incorporated into all trades, enabling users to see the exact profitability of each trade. software as a service (saas) applications euler explorer (webapp) the current product provides a rich toolset for blockchain data consumers. the attractive and data-rich user interface includes a blockchain explorer for tokens, addresses, transactions, and pairs. the explorer provides drill-down charts. investors can monitor their personal and business wallets and portfolios using euler tools visual interface.\neuler tools\u2019 live data feed provides real-time prices, liquidity, and transfers.\neuler tools provides customizable alerts and notifications. users may select which devices receive alerts and notifications. in addition, users are provided with a rich interface to specify the conditions that trigger alerts and notifications.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59501645}, {"Unnamed: 0": 1697, "autor": "Solaris Margin", "date": null, "content": "Inspiration\nTrading tools are an important part of every DeFi ecosystem as it allows traders to remove imbalances and inefficiencies between and in different protocols. We saw that there is a lack of such tools on Celo and decided to build a simple margin trading tool, as such instrument is on the most popular for traders together with futures, options and perpetuals\nWhat it does and how it works\nLet's say a user wants to open 3x long position cUSD against CELO and has $1000 cUSD in his wallet.\nUser approves $1000 cUSD to Solaris Margin Contract and delegates a credit line of $2000 in CELO (via Moola v2 credit delegation feature)\nUser asks our smart contract to create 3x long position. The contract does the following actions on behalf of a user to create a long position:\nTakes $2000 cUSD flashloan on Moola\nPulls $1000 cUSD from user's wallet and swaps $3000 cUSD to $3000 CELO on Ubeswap\nDeposits $3k CELO as collateral to Moola and borrows $2000 cUSD + .35% flashloan fee\nReturns $2000 cUSD flashloan + .35% flashloan fee\nAs a result the user has $2000 cUSD debt and $3k CELO as collateral - 3x long leveraged cUSD -> CELO position. Than, for example he/she can create a limit order for it which when executed will close this position. Adding limit orders is the next step for us\nHow we built it\nWe worked on this idea in Celo Camp. As the first step we interviewed traders if they need tools like this and why. Then we defined a minimum set of features that should be in MVP and built the app.\nChallenges we ran into\nWe ran into three challenges: 1) A problem with debugging contracts on Celo (we've found a workaround finally) 2) Price difference between Moola oracle on testnet and Ubeswap price (we did a kind of arbitrage bot to make these prices almost equal) 3) It was hard to track user's positions in a smart contract without limit orders.\nAccomplishments that we're proud of\nWe built it finally!\nWhat we learned\nWe learned how to build on Celo, met a lot of people from Celo community. It was great experience.\nWhat's next for Solaris Margin\nGet 100 users that are interested in using our app Add limit orders to create seamless experience for traders Add isolated margin trading based on Kashi", "link": "https://devpost.com/software/solaris-margin", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ntrading tools are an important part of every defi ecosystem as it allows traders to remove imbalances and inefficiencies between and in different protocols. we saw that there is a lack of such tools on celo and decided to build a simple margin trading -----> tool !!! , as such instrument is on the most popular for traders together with futures, options and perpetuals\nwhat it does and how it works\nlet's say a user wants to open 3x long position cusd against celo and has $1000 cusd in his wallet.\nuser approves $1000 cusd to solaris margin contract and delegates a credit line of $2000 in celo (via moola v2 credit delegation feature)\nuser asks our smart contract to create 3x long position. the contract does the following actions on behalf of a user to create a long position:\ntakes $2000 cusd flashloan on moola\npulls $1000 cusd from user's wallet and swaps $3000 cusd to $3000 celo on ubeswap\ndeposits $3k celo as collateral to moola and borrows $2000 cusd + .35% flashloan fee\nreturns $2000 cusd flashloan + .35% flashloan fee\nas a result the user has $2000 cusd debt and $3k celo as collateral - 3x long leveraged cusd -> celo position. than, for example he/she can create a limit order for it which when executed will close this position. adding limit orders is the next step for us\nhow we built it\nwe worked on this idea in celo camp. as the first step we interviewed traders if they need tools like this and why. then we defined a minimum set of features that should be in mvp and built the app.\nchallenges we ran into\nwe ran into three challenges: 1) a problem with debugging contracts on celo (we've found a workaround finally) 2) price difference between moola oracle on testnet and ubeswap price (we did a kind of arbitrage bot to make these prices almost equal) 3) it was hard to track user's positions in a smart contract without limit orders.\naccomplishments that we're proud of\nwe built it finally!\nwhat we learned\nwe learned how to build on celo, met a lot of people from celo community. it was great experience.\nwhat's next for solaris margin\nget 100 users that are interested in using our app add limit orders to create seamless experience for traders add isolated margin trading based on kashi", "sortedWord": "None", "removed": "Nan", "score": 5, "comments": 0, "media": null, "medialink": null, "identifyer": 59501697}, {"Unnamed: 0": 1708, "autor": "Promo.com", "date": null, "content": "Promo is an easy-to-use, subscription-based video creation platform. Promo includes a video library of over two million clips indexed by subject matter and industry, along with fully licensed music and a flexible, customizable text editor. With Promo, anyone can create and share engaging, professional-quality marketing videos.\nFeatures: Video footage library contains over two million clips. Fully-licensed music library, or upload your own music. Customize your video with logo, text and more. Easily share on web, mobile and social channels.\nFunction: Promo is the first and only tool that offers you pre-filtered high quality footage, full-licensing, music, and editing to create a beautiful and effective video. Simply put, Promo is easy, instant, and produces awesome results and higher engagement at an affordable price.\nPricing: One Time Fee (Project, Sponsorship or Outright Purchase) Monthly Subscription", "link": "https://devpost.com/software/promo-com", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "promo is an easy-to-use, subscription-based video creation platform. promo includes a video library of over two million clips indexed by subject matter and industry, along with fully licensed music and a flexible, customizable text editor. with promo, anyone can create and share engaging, professional-quality marketing videos.\nfeatures: video footage library contains over two million clips. fully-licensed music library, or upload your own music. customize your video with logo, text and more. easily share on web, mobile and social channels.\nfunction: promo is the first and only -----> tool !!!  that offers you pre-filtered high quality footage, full-licensing, music, and editing to create a beautiful and effective video. simply put, promo is easy, instant, and produces awesome results and higher engagement at an affordable price.\npricing: one time fee (project, sponsorship or outright purchase) monthly subscription", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59501708}, {"Unnamed: 0": 1715, "autor": "uhub", "date": null, "content": "Inspiration\nThe basic motivation for this project was to create a tool that would simplify the interview process and automate candidate selection for a job by removing human biases through a more objective framework AI and Machine Learning.\nWhat it does\nough the project is in its very initial phase. The basic premise of this project is to take a prerecorded interview video or audio and upload it to Assembly AI through which it will return a text transcript. Then it will be further analyzed through other NLP AIs which will evaluate the candidates skills, experiences, \"ranking\" in comparison to other candidates (also based on custom criteria given by the company) and finally yield a qualitative and visual answer on a candidates employability (ie. whether someone is a good fit for the company or not) Please see the chart below for a better visualization of the process.\nHow we built it\nThe technology used creating this project is both simple and complex. First the transcription was done through Assembly AI. The webapp was created using Streamlit (Python tool). The text analysis was done through Parallel dots API.\nChallenges we ran into\nI ran into some challenges with the Assembly AI. However, the documentation helped me resolved the issue.\nAccomplishments that we're proud ofHaving a working prototype to showcase for hackathon.\nWhat we learned\nWorking with this project I learned how to work with Assembly AI. I also learned how to use Streamlit to make quick web APPs. Additionally, I gained a deeper understanding of NLP and AI\nWhat's next for uhub\nAdding more complex machine learning, NLP and Custom Algorithm for better text analysis Improving the UI for professional use. Adding youtube_dl functionality so video can be directly inputted through YT And many more", "link": "https://devpost.com/software/uhub", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nthe basic motivation for this project was to create a -----> tool !!!  that would simplify the interview process and automate candidate selection for a job by removing human biases through a more objective framework ai and machine learning.\nwhat it does\nough the project is in its very initial phase. the basic premise of this project is to take a prerecorded interview video or audio and upload it to assembly ai through which it will return a text transcript. then it will be further analyzed through other nlp ais which will evaluate the candidates skills, experiences, \"ranking\" in comparison to other candidates (also based on custom criteria given by the company) and finally yield a qualitative and visual answer on a candidates employability (ie. whether someone is a good fit for the company or not) please see the chart below for a better visualization of the process.\nhow we built it\nthe technology used creating this project is both simple and complex. first the transcription was done through assembly ai. the webapp was created using streamlit (python tool). the text analysis was done through parallel dots api.\nchallenges we ran into\ni ran into some challenges with the assembly ai. however, the documentation helped me resolved the issue.\naccomplishments that we're proud ofhaving a working prototype to showcase for hackathon.\nwhat we learned\nworking with this project i learned how to work with assembly ai. i also learned how to use streamlit to make quick web apps. additionally, i gained a deeper understanding of nlp and ai\nwhat's next for uhub\nadding more complex machine learning, nlp and custom algorithm for better text analysis improving the ui for professional use. adding youtube_dl functionality so video can be directly inputted through yt and many more", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59501715}, {"Unnamed: 0": 1752, "autor": "Kit", "date": null, "content": "Inspiration\nThe pandemic has transformed how people communicate forever. Most in-person meetings are now just hour-long video calls. But at its core, the art of conversation is still the same. We believe that the key to making a great product is to build something you need. So, we started by listing all the problems we face daily while communicating.\nWe tend to hesitate while speaking.\nWe lack confidence.\nWe overuse common words while speaking.\nGrammatical Errors while speaking.\nKit attempts to solve all the above issues effectively.\nWhat it does\nSimply put, Kit is a multipurpose video calling platform, which helps people improve their communication skills. How do we do it? Let me explain with an example.\nVideo Conferencing Platform\nSuppose you have a debate competition coming up in the next week.\nHow do you prepare for it? Enter, Kit.\nVisit our website and create a new meeting. While doing so, select your interest area. You can choose from a wide range of options like Environment, Politics, etc.\nAfter picking something, send the invite link to one of your friends.\nWait for them to join. Kit presents a question to both of you based on your interest. You can choose to speak in favour or against the motion.\nThe debate begins.\nBoth of you take turns and say what you have to say.\nThe meeting ends when both of you have nothing left to say.\nAWS Transcribe transcribes the meeting audio for both the participants.\nAI-powered Reporting\nThe transcribed text from both the participants' audio files is fed into multiple state-of-the-art Natural Langauge Processing algorithms to get meaningful insights. These insights are captured in a report and are mailed to the users after the meeting ends. They can view these reports on the web application. Every report has -\nSentiment Analysis - What was the tone behind what you said? Positive or Negative?\nBehavioural Analysis - What was your state of mind during the meeting\nEmotional Analysis - How did the other person react to what you said?\nWord Cloud - What words did you use?\nMost Used Words - What are the words you use the most? Are there any alternate words?\nTranscript - What did you say during the meeting?\nGrammar Checker Tool - What are the common mistakes you make while speaking?\nSentiment Analysis is a basic NLP technique that gives you a more human-like understanding of the text. Emotional & Behavioral traits provide insights into your personality type by capturing the specific emotions you project. This can help individuals answer questions like how to be more likeable.Word Cloud is a data visualization technique used for representing text data in which the size of each word indicates its frequency or importance. The Most Used Words feature helps you expand your vocabulary. Studies have shown that the lack of proper communication is one of the reasons why most candidates get rejected in a job interview. While speaking, using poor grammar can lead to ambiguity and misinterpretation. So, the question that now arises is how do we solve this problem? Kit transcribes every word you speak in the meeting and stores it for inspection. The built-in AI-powered Grammar Checker Tool not only points out all the mistakes you make while speaking but also finds a suitable substitute for all the errors.\nHow we built it\nThe front end uses React & Redux.\nNodejs & PostgreSQL to store the data.\nPython & Flask for Natural Langauge Processing & Artificial Intelligence.\nVideo calling using the Dolby.io Communications APIs\nAWS transcribe to transcribe recordings\nFor the Grammar Checker, we used the Hugging Face Transformers pipeline.\nThe NLTK python library to generate the synonyms\nThe word cloud python library for making word clouds\nThe Sentiments, Emotional traits & Behavioral traits that we get are from Expert.ai's NLP APIs.\nChallenges we ran into\nLearning about Dolby's Communications APIs\nMaking the demo video\nSetting up Dolby API to work in tandem with AWS Transcribe\nDesigning the overall architecture of the application\nAccomplishments that we're proud of\nWhat makes us proud is that we were able to come up with something truly novel and revolutionary. The use-cases for Kit are infinite. Kit works for everyone, whether you are a working professional or a student. Our application will help you become a better communicator. In the future, we look forward to rolling out new releases and delivering a better product.\nWhat we learned\nWhere do I start? We are a small team of college students from India. Every member was working from home due to the current global situation. Despite the difficulties, we managed to work together just fine. This hackathon has strengthened our bond as a team and made us better at collaboration. On the tech side of things, learning Dolby's Communication API's was fun. Building the Grammar Checker Tool required us to have a thorough understanding of Natural Language Processing. And before I forget to mention, we also learned the basics of Premiere Pro & Adobe Spark for making our demo video. We hope you like it!\nWhat's next for Kit\nCurrently, we focus only on the audio. In the future, we plan to integrate computer vision into the application. Tracking the facial expressions and emotions of the participants will help improve reporting drastically. It will also help us analyze all the gestures made during the call. The reports are only available via the online interface. However, in the next release, the users will have the ability to export them as pdf and use them on the go. Lastly, We would like to thank Dolby for organizing this hackathon.", "link": "https://devpost.com/software/kit-h54t9m", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nthe pandemic has transformed how people communicate forever. most in-person meetings are now just hour-long video calls. but at its core, the art of conversation is still the same. we believe that the key to making a great product is to build something you need. so, we started by listing all the problems we face daily while communicating.\nwe tend to hesitate while speaking.\nwe lack confidence.\nwe overuse common words while speaking.\ngrammatical errors while speaking.\nkit attempts to solve all the above issues effectively.\nwhat it does\nsimply put, kit is a multipurpose video calling platform, which helps people improve their communication skills. how do we do it? let me explain with an example.\nvideo conferencing platform\nsuppose you have a debate competition coming up in the next week.\nhow do you prepare for it? enter, kit.\nvisit our website and create a new meeting. while doing so, select your interest area. you can choose from a wide range of options like environment, politics, etc.\nafter picking something, send the invite link to one of your friends.\nwait for them to join. kit presents a question to both of you based on your interest. you can choose to speak in favour or against the motion.\nthe debate begins.\nboth of you take turns and say what you have to say.\nthe meeting ends when both of you have nothing left to say.\naws transcribe transcribes the meeting audio for both the participants.\nai-powered reporting\nthe transcribed text from both the participants' audio files is fed into multiple state-of-the-art natural langauge processing algorithms to get meaningful insights. these insights are captured in a report and are mailed to the users after the meeting ends. they can view these reports on the web application. every report has -\nsentiment analysis - what was the tone behind what you said? positive or negative?\nbehavioural analysis - what was your state of mind during the meeting\nemotional analysis - how did the other person react to what you said?\nword cloud - what words did you use?\nmost used words - what are the words you use the most? are there any alternate words?\ntranscript - what did you say during the meeting?\ngrammar checker -----> tool !!!  - what are the common mistakes you make while speaking?\nsentiment analysis is a basic nlp technique that gives you a more human-like understanding of the text. emotional & behavioral traits provide insights into your personality type by capturing the specific emotions you project. this can help individuals answer questions like how to be more likeable.word cloud is a data visualization technique used for representing text data in which the size of each word indicates its frequency or importance. the most used words feature helps you expand your vocabulary. studies have shown that the lack of proper communication is one of the reasons why most candidates get rejected in a job interview. while speaking, using poor grammar can lead to ambiguity and misinterpretation. so, the question that now arises is how do we solve this problem? kit transcribes every word you speak in the meeting and stores it for inspection. the built-in ai-powered grammar checker tool not only points out all the mistakes you make while speaking but also finds a suitable substitute for all the errors.\nhow we built it\nthe front end uses react & redux.\nnodejs & postgresql to store the data.\npython & flask for natural langauge processing & artificial intelligence.\nvideo calling using the dolby.io communications apis\naws transcribe to transcribe recordings\nfor the grammar checker, we used the hugging face transformers pipeline.\nthe nltk python library to generate the synonyms\nthe word cloud python library for making word clouds\nthe sentiments, emotional traits & behavioral traits that we get are from expert.ai's nlp apis.\nchallenges we ran into\nlearning about dolby's communications apis\nmaking the demo video\nsetting up dolby api to work in tandem with aws transcribe\ndesigning the overall architecture of the application\naccomplishments that we're proud of\nwhat makes us proud is that we were able to come up with something truly novel and revolutionary. the use-cases for kit are infinite. kit works for everyone, whether you are a working professional or a student. our application will help you become a better communicator. in the future, we look forward to rolling out new releases and delivering a better product.\nwhat we learned\nwhere do i start? we are a small team of college students from india. every member was working from home due to the current global situation. despite the difficulties, we managed to work together just fine. this hackathon has strengthened our bond as a team and made us better at collaboration. on the tech side of things, learning dolby's communication api's was fun. building the grammar checker tool required us to have a thorough understanding of natural language processing. and before i forget to mention, we also learned the basics of premiere pro & adobe spark for making our demo video. we hope you like it!\nwhat's next for kit\ncurrently, we focus only on the audio. in the future, we plan to integrate computer vision into the application. tracking the facial expressions and emotions of the participants will help improve reporting drastically. it will also help us analyze all the gestures made during the call. the reports are only available via the online interface. however, in the next release, the users will have the ability to export them as pdf and use them on the go. lastly, we would like to thank dolby for organizing this hackathon.", "sortedWord": "None", "removed": "Nan", "score": 6, "comments": 0, "media": null, "medialink": null, "identifyer": 59501752}, {"Unnamed: 0": 1759, "autor": "IoT-Chainlink-MQTT-Client", "date": null, "content": "Inspiration\nThe Fall 2020 Chainlink Hackathon Best Open Project winner The Open Library Project really got my gears turning. There happens to be a \"Little Free Library\" location right by a park my wife and I walk our dog weekly, and I'm reminded of Aram's project every time we pass it. His use of an IoT device, capturing its output through an external adapter, and executing a Smart Contract on that data, was the primary inspiration for this project.\nWhat it does\nThis project allows a Smart Contract to connect to an MQTT Broker, through a Chainlink External Adapter, and publish or subscribe to IoT devices operating on this MQTT Broker.\nWith this project, IoT devices can retain existing MQTT communication infrasctructure when integrating into decentralized systems.\nHow we built it\nPython Python Python. I used python to write the CL-EA-MQTT-Client external adapter. This adapter was heavinly influenced by (and actually directly forked from) Tom Hodges CL-EA-python-template. I also used python to test the smart contracts in Brownie.\nThe Smart Contract was written in Solidity, of course, and the IoT device featured in this project was written in c++ as a PlatformIO project.\nChallenges we ran into\nI struggled to get Brownie really working for me. Still not exactly sure where I was going wrong. I was excited to have a python framework for interacting with Smart Contracts, but wish I had spent more time learning this tool earlier on to better utilize it in this project.\nAccomplishments that we're proud of\nI was pretty excited to see data flow from my Smart Contract, to my Oracle, to my Node, across my external adapter, onto the MQTT Broker, and then all the way back. There were a lot of steps where things could (and did) go wrong, but finally getting an output I was expecting got me pretty pumped.\nWhat we learned\nCreated my first smart contract\nRan up my first Chainlink node and connected it to the Kovan Testnet\nWrote and executed jobs on a Chainlink node\nConnected and called external adapter through bridge task\nWhat's next for IoT-Chainlink-MQTT-Client\nI'd really like to:\nget a keeper contract auto-executing on IoT data\ncreate an application for both managing IoT devices and generating Smart Contracts that execute on the data they produce.", "link": "https://devpost.com/software/iot-chainlink-mqtt-client", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nthe fall 2020 chainlink hackathon best open project winner the open library project really got my gears turning. there happens to be a \"little free library\" location right by a park my wife and i walk our dog weekly, and i'm reminded of aram's project every time we pass it. his use of an iot device, capturing its output through an external adapter, and executing a smart contract on that data, was the primary inspiration for this project.\nwhat it does\nthis project allows a smart contract to connect to an mqtt broker, through a chainlink external adapter, and publish or subscribe to iot devices operating on this mqtt broker.\nwith this project, iot devices can retain existing mqtt communication infrasctructure when integrating into decentralized systems.\nhow we built it\npython python python. i used python to write the cl-ea-mqtt-client external adapter. this adapter was heavinly influenced by (and actually directly forked from) tom hodges cl-ea-python-template. i also used python to test the smart contracts in brownie.\nthe smart contract was written in solidity, of course, and the iot device featured in this project was written in c++ as a platformio project.\nchallenges we ran into\ni struggled to get brownie really working for me. still not exactly sure where i was going wrong. i was excited to have a python framework for interacting with smart contracts, but wish i had spent more time learning this -----> tool !!!  earlier on to better utilize it in this project.\naccomplishments that we're proud of\ni was pretty excited to see data flow from my smart contract, to my oracle, to my node, across my external adapter, onto the mqtt broker, and then all the way back. there were a lot of steps where things could (and did) go wrong, but finally getting an output i was expecting got me pretty pumped.\nwhat we learned\ncreated my first smart contract\nran up my first chainlink node and connected it to the kovan testnet\nwrote and executed jobs on a chainlink node\nconnected and called external adapter through bridge task\nwhat's next for iot-chainlink-mqtt-client\ni'd really like to:\nget a keeper contract auto-executing on iot data\ncreate an application for both managing iot devices and generating smart contracts that execute on the data they produce.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59501759}, {"Unnamed: 0": 1776, "autor": "MindBucket", "date": null, "content": "After studying the brief our team decided to design an application which would focus on providing remote care while educating people about mental health.\nThe idea is to create an application which can help Urban Indian Teenagers in introducing their (reluctant) parents or family members to the idea of therapy. The application can be used as a tool to bring awareness and additionally, if the family feels that they are in need of help it can also connect them to a therapist according to their preference. We\u2019re hoping that this idea can help in breaking the general taboo that Indian parents seem to have towards mental health and to make therapy more accessible \u2013 if they want to!\nWhat inpired us were the brainstorming team sessions which made us share our own opinions on therapy and how we collectively and almost simulatneously agreed on the negative talk that exists around therapy in our country, India. It\u2019s ingrained in our value systems that asking for help is considered weak. The upcoming generation is very well breaking that stigma but people belonging to the older demographic, specifically our parents within the age group of 41-60 are most averse to the whole concept of mental health.\nWe started researching on our idea and you can look up the findings here : link\nThe pain points we found were how therapy according to the older demographic was unnecessary , expensive , embarassing and how most of the time they are not able to find the right provider.\nTherefore, the goals of our product is to provide an application that doesn\u2019t feel like a crutch but more like a support system. We want to ease our audience into the idea of therapy instead of just throwing them in the deep end.\nIntroducing MindBucket\n**MindBucket is an application that aims to create awareness about mental illnesses by curating a catered list of podcasts based on user preferences. These podcasts are conducted by people who are going or have been through similar experiences. We\u2019ve tried to accomodate two types of users \u2013 users that are ready to take the lead and directly connect to a professional, and the conservative users that need a little push to cross the bridge by listening to familiar experiences and stories.\nThe app also offers free trial sessions with Therapists so that the users can find a Therapist that fits them well.\nAll the podcasts will revolve around the topics of mental health and wellness.**\nThe features of MindBucket are :\nIntroducing Min! Our friendly neighbourhood bot assistant to help our audience tackle the new terrain of mental wellness.\nStraightforward user flow for easy navigation.\nClean and minimalistic user interface.\nConsiders users present emotion to test the waters by frequent check-in prompts.\nHassle-free booking of therapists.\nA varied list of podcasts catering to all kinds of preferences.\nOffering two free trial sessions for our prudent users.\nPlease have a look at our wireframes uploaded as PNGs and try the application prototype too!\nWe really enjoyed making MindBucket. Our team's personal ideologies strongly align with the essence of our application. The biggest challenge that we faced was during primary research when we had to reach out to people to collect data. When talked about mental health and therapy, they were willing to open up about themselves, however most of our interviewees closed off when asked about their parents perspective of it. This further showed us how the topic of mental wellness is still a taboo in India.", "link": "https://devpost.com/software/ayo-portfolio", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "after studying the brief our team decided to design an application which would focus on providing remote care while educating people about mental health.\nthe idea is to create an application which can help urban indian teenagers in introducing their (reluctant) parents or family members to the idea of therapy. the application can be used as a -----> tool !!!  to bring awareness and additionally, if the family feels that they are in need of help it can also connect them to a therapist according to their preference. we\u2019re hoping that this idea can help in breaking the general taboo that indian parents seem to have towards mental health and to make therapy more accessible \u2013 if they want to!\nwhat inpired us were the brainstorming team sessions which made us share our own opinions on therapy and how we collectively and almost simulatneously agreed on the negative talk that exists around therapy in our country, india. it\u2019s ingrained in our value systems that asking for help is considered weak. the upcoming generation is very well breaking that stigma but people belonging to the older demographic, specifically our parents within the age group of 41-60 are most averse to the whole concept of mental health.\nwe started researching on our idea and you can look up the findings here : link\nthe pain points we found were how therapy according to the older demographic was unnecessary , expensive , embarassing and how most of the time they are not able to find the right provider.\ntherefore, the goals of our product is to provide an application that doesn\u2019t feel like a crutch but more like a support system. we want to ease our audience into the idea of therapy instead of just throwing them in the deep end.\nintroducing mindbucket\n**mindbucket is an application that aims to create awareness about mental illnesses by curating a catered list of podcasts based on user preferences. these podcasts are conducted by people who are going or have been through similar experiences. we\u2019ve tried to accomodate two types of users \u2013 users that are ready to take the lead and directly connect to a professional, and the conservative users that need a little push to cross the bridge by listening to familiar experiences and stories.\nthe app also offers free trial sessions with therapists so that the users can find a therapist that fits them well.\nall the podcasts will revolve around the topics of mental health and wellness.**\nthe features of mindbucket are :\nintroducing min! our friendly neighbourhood bot assistant to help our audience tackle the new terrain of mental wellness.\nstraightforward user flow for easy navigation.\nclean and minimalistic user interface.\nconsiders users present emotion to test the waters by frequent check-in prompts.\nhassle-free booking of therapists.\na varied list of podcasts catering to all kinds of preferences.\noffering two free trial sessions for our prudent users.\nplease have a look at our wireframes uploaded as pngs and try the application prototype too!\nwe really enjoyed making mindbucket. our team's personal ideologies strongly align with the essence of our application. the biggest challenge that we faced was during primary research when we had to reach out to people to collect data. when talked about mental health and therapy, they were willing to open up about themselves, however most of our interviewees closed off when asked about their parents perspective of it. this further showed us how the topic of mental wellness is still a taboo in india.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59501776}, {"Unnamed: 0": 1777, "autor": "Shamba", "date": null, "content": "Inspiration\nDeFi solutions based on real-world data, such as parametric insurance, have the potential to impact a huge part of the global population. From making financial products affordable to those who need them, to helping protect the environment and fight climate change. We want to spur the development of these solutions by making available the data and tools required to easily build and deploy them.\nWhat it does\nOur solution helps developers integrate satellite data from various global providers into their decentralized applications. It does this by providing them with boilerplate code that meets their user requirements for satellite data, and can be further customized to fulfill any function.\nHow we built it\nWe build our solution on the Google Cloud Platform. Leveraging a variety of Google cloud services to build the geospatial analytics API, external adapters for the API, and a UI tool for integrating the data into smart contracts. As part of the development we also set up an oracle node to serve the data requests.\nChallenges we ran into\nA few things here and there.\nAccomplishments that we're proud of\nSetting up the first geospatial analytics data oracle on the Chainlink network, and innovating new workflows for leveraging this data in a new class of products.\nWhat we learned\nTakes time to get everything running just right.\nWhat's next for Shamba\nExtending our parametric DeFi toolkit from developers to enterprises and creating a no-code API for building and deploying enterprise parametric DeFi solutions.", "link": "https://devpost.com/software/shamba", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ndefi solutions based on real-world data, such as parametric insurance, have the potential to impact a huge part of the global population. from making financial products affordable to those who need them, to helping protect the environment and fight climate change. we want to spur the development of these solutions by making available the data and tools required to easily build and deploy them.\nwhat it does\nour solution helps developers integrate satellite data from various global providers into their decentralized applications. it does this by providing them with boilerplate code that meets their user requirements for satellite data, and can be further customized to fulfill any function.\nhow we built it\nwe build our solution on the google cloud platform. leveraging a variety of google cloud services to build the geospatial analytics api, external adapters for the api, and a ui -----> tool !!!  for integrating the data into smart contracts. as part of the development we also set up an oracle node to serve the data requests.\nchallenges we ran into\na few things here and there.\naccomplishments that we're proud of\nsetting up the first geospatial analytics data oracle on the chainlink network, and innovating new workflows for leveraging this data in a new class of products.\nwhat we learned\ntakes time to get everything running just right.\nwhat's next for shamba\nextending our parametric defi toolkit from developers to enterprises and creating a no-code api for building and deploying enterprise parametric defi solutions.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59501777}, {"Unnamed: 0": 1798, "autor": "GWAP", "date": null, "content": "Inspiration\nGrep! An awesome tool.\nWhat it does\nLike grep, gwap is a command line application that will search through a given audio file, or recursively for all audio files in folder, for a word or words input and print out the timestamp of where the words are said in an audio.\nHow we built it\nThis application was built in python using the cli library click and the Modzy python sdk. For the gui component a python library called eel was\nChallenges we ran into\nCurrently showing a progress bar in the command line as the inference job is taking place.\nAccomplishments that we're proud of\nI'm proud of getting the base application working with the Modzy integration and a gui working.\nWhat we learned\nI learned how to use the Modzy ai models and python integeration.\nWhat's next for GWAP\nAdding more audio filetypes than just mp3\nrefining the gui design\nAdding more search features as well as playing the found words directly from the app", "link": "https://devpost.com/software/gwap", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ngrep! an awesome -----> tool !!! .\nwhat it does\nlike grep, gwap is a command line application that will search through a given audio file, or recursively for all audio files in folder, for a word or words input and print out the timestamp of where the words are said in an audio.\nhow we built it\nthis application was built in python using the cli library click and the modzy python sdk. for the gui component a python library called eel was\nchallenges we ran into\ncurrently showing a progress bar in the command line as the inference job is taking place.\naccomplishments that we're proud of\ni'm proud of getting the base application working with the modzy integration and a gui working.\nwhat we learned\ni learned how to use the modzy ai models and python integeration.\nwhat's next for gwap\nadding more audio filetypes than just mp3\nrefining the gui design\nadding more search features as well as playing the found words directly from the app", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59501798}, {"Unnamed: 0": 1810, "autor": "RedStone Oracle on Celo", "date": null, "content": "Inspiration\nThe origins of the idea come straight from developers' need - we took part in various blockchain hackathons, where we saw many interesting ideas being killed early stage, due to a lack of available specific type of data on chain. That's what made us start working on RedStone!\nWhat it does\nRedStone delivers customisable data feeds by using on-demand data providing model and on-chain signature verification. We support long-tail tokens, real world data, soon NFT related information and we're here to serve diverse needs in regards to Oracles datafeeds. You need data about Interest rates, indexes or stocks? With our scalable infrastructure we're able to bring it on chain.\nHow we built it\nWe source data feeds from our providers and then store it cost-efficiently on Arweave blockchain (Arweave blockchain has been designed to store large sets of data). When a user in a Celo dApp wants to use our data, it is automatically fetched from the arweave cache and appended to the transaction data. Then our smart contract verifies if the data signature is valid and created by trusted data provider.\nThis approach is much more scalable and cost-efficient than the current oracles' approach of recurring data pushing to blockchains.\nThis cost-efficient approach allows us to bring different kinds of data to Celo, to enable new ideas targeted for undeserved markets, e.g. using real world data about crops that is relevant for developing markets.\nChallenges we ran into\nFetching data from different API with different interfaces. Luckily we've found ccxt library, which unifies interface for more than 100 of the most popular cryptocurrency exchange APIs\nFetching data from DEXes. We've integrated with theGraph to obtain the data from DEXes, such as Uniswap and Sushiswap\nIntegration with Celo. We've already had an oracle tool for ethers.js library. But the ethers.js was not supported on Celo right away. We've managed to connect it using @celo-tools/celo-ethers-wrapper. More info here\nAccomplishments that we're proud of\nWe've managed to connect more than 50 data sources. Link to all sources\nWe managed to get pre-seed round in April 2021, now we're closing the Seed round\nWe managed to integrate price feeds for over 1000 asset from crypto and real world, divided into 10 categories. Link to all price feeds\nTo show possibilities of RedStone Oracles, we built the first implementation working on Celo - Synthetic commodities platform. Users can use CELO and cUSD as collateral to min synthetic Corn / Wheat / commodities tokens. Link to the app, link to the github repo\nWhat we learned\nCollaboration with other projects\nOur approach from the very beginning was deliver what users would use and benefit from. At the hackathon we learned to talk to other projects, frequently update with them on progress and if any new requirements from community side raised.\nNew technical knowledge\nWe've learnt how to integrate with Celo blockchain, how to connect ethers.js library and we've also learn the basic security considerations regarding providing entropy data on-chain. We will probably integrate with https://twitter.com/cybertime_eth (NFT marketplace on Celo) and help them to improve pseudo-random generation in smart contracts.\nWhat's next for RedStone Oracle on Celo\nOur goal is to become the first choice Oracle for all projects based on Celo. To achieve that we'll keep adding new data types that builders will request. At the same time, we want to increase exposure and presence on Celo market significantly.", "link": "https://devpost.com/software/redstone-oracle-on-celo", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nthe origins of the idea come straight from developers' need - we took part in various blockchain hackathons, where we saw many interesting ideas being killed early stage, due to a lack of available specific type of data on chain. that's what made us start working on redstone!\nwhat it does\nredstone delivers customisable data feeds by using on-demand data providing model and on-chain signature verification. we support long-tail tokens, real world data, soon nft related information and we're here to serve diverse needs in regards to oracles datafeeds. you need data about interest rates, indexes or stocks? with our scalable infrastructure we're able to bring it on chain.\nhow we built it\nwe source data feeds from our providers and then store it cost-efficiently on arweave blockchain (arweave blockchain has been designed to store large sets of data). when a user in a celo dapp wants to use our data, it is automatically fetched from the arweave cache and appended to the transaction data. then our smart contract verifies if the data signature is valid and created by trusted data provider.\nthis approach is much more scalable and cost-efficient than the current oracles' approach of recurring data pushing to blockchains.\nthis cost-efficient approach allows us to bring different kinds of data to celo, to enable new ideas targeted for undeserved markets, e.g. using real world data about crops that is relevant for developing markets.\nchallenges we ran into\nfetching data from different api with different interfaces. luckily we've found ccxt library, which unifies interface for more than 100 of the most popular cryptocurrency exchange apis\nfetching data from dexes. we've integrated with thegraph to obtain the data from dexes, such as uniswap and sushiswap\nintegration with celo. we've already had an oracle -----> tool !!!  for ethers.js library. but the ethers.js was not supported on celo right away. we've managed to connect it using @celo-tools/celo-ethers-wrapper. more info here\naccomplishments that we're proud of\nwe've managed to connect more than 50 data sources. link to all sources\nwe managed to get pre-seed round in april 2021, now we're closing the seed round\nwe managed to integrate price feeds for over 1000 asset from crypto and real world, divided into 10 categories. link to all price feeds\nto show possibilities of redstone oracles, we built the first implementation working on celo - synthetic commodities platform. users can use celo and cusd as collateral to min synthetic corn / wheat / commodities tokens. link to the app, link to the github repo\nwhat we learned\ncollaboration with other projects\nour approach from the very beginning was deliver what users would use and benefit from. at the hackathon we learned to talk to other projects, frequently update with them on progress and if any new requirements from community side raised.\nnew technical knowledge\nwe've learnt how to integrate with celo blockchain, how to connect ethers.js library and we've also learn the basic security considerations regarding providing entropy data on-chain. we will probably integrate with https://twitter.com/cybertime_eth (nft marketplace on celo) and help them to improve pseudo-random generation in smart contracts.\nwhat's next for redstone oracle on celo\nour goal is to become the first choice oracle for all projects based on celo. to achieve that we'll keep adding new data types that builders will request. at the same time, we want to increase exposure and presence on celo market significantly.", "sortedWord": "None", "removed": "Nan", "score": 6, "comments": 0, "media": null, "medialink": null, "identifyer": 59501810}, {"Unnamed: 0": 1883, "autor": "ABSTRACT", "date": null, "content": "Inspiration\nWhat it does\nABSTRACT is a document automation tool for scientific manuscript preparation and submission for peer review.\nHow we built it\nChallenges we ran into\nAccomplishments that we're proud of\nWhat we learned\nWhat's next for ABSTRACT", "link": "https://devpost.com/software/abstract-8n64lv", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwhat it does\nabstract is a document automation -----> tool !!!  for scientific manuscript preparation and submission for peer review.\nhow we built it\nchallenges we ran into\naccomplishments that we're proud of\nwhat we learned\nwhat's next for abstract", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59501883}, {"Unnamed: 0": 1957, "autor": "Mushrooms Finance", "date": null, "content": "Inspiration\nMushrooms Finance is yet another crypto earning vault with focus on seeking sustainable profit in DeFi universe. The first version of the Mushrooms Finance smart contracts were inspired by other popular DeFi projects like Yearn Finance (YFI) and Sushiswap among many others.\nWhat it does\nMushrooms Finance is now alive in Ethereum, Binance Smart Chain, Fantom and Polygon with thousands of users and tens of auto-compounding earning strategies.\nHow we built it\nOur core team started working in Blockchain/Ethereum/DeFi in the second half of 2018 and worked together for the last three years. We are a small but cohesive team. We have started Mushrooms Finance as a yield aggregator project aiming for sustainable yield mining since Oct 2020.\nAccomplishments that we're proud of\nWe have developed innovative product auto portfolio-rebalancing tool MushMon. MushMon is a Yield-Farming Boosted Portfolio Tool, which innovatively combines the traits of both DeFi yield-aggregator and grid trading algorithms. I would like to introduce MushMon as a resolution to the following four questions that EVERYONE would always be asked about: 1) When should I sell the rip when the market is rocketing? 2) When should I buy the dip when the market is slumping heavily? 3) What if I miss the margin-call when good trade timing is there? 4) How can I make my money work for me all the time?\nMushMon will work at your configured risk preference to constantly monitor the market and let the trading bot work for you. It always tries to keep two assets in the portfolio of equal value, i.e., 1:1 valuation for stablecoin and Bitcoin for example. If the price of Bitcoin goes up (triggered by your target increase), the bot will execute \u201csell\u201d to secure some profit in stablecoin and if the price of Bitcoin goes down (triggered by your target decrease), the bot will execute \u201cbuy\u201d to lower the average cost of existing position.\nIn essence, MushMon could make money out of the fluctuations of the asset\u2019s price and reduce the volatility of expected returns by portfolio diversification.\nAdditionally, MushMon allows you to earn yield-farming with underlying Mushrooms earning vaults without any idle time for your deposited assets.\nWe believe MushMon is the first of its kind in the current DeFi landscape across different blockchains which combines yield-farming and portfolio rebalancing in a creative way for the benefit of its users. However, it is of course not a silver-bullet for all scenarios and may incur loss if the market falls deep in one direction without bouncing back (similar to Impermanent Loss in AMM).\nLastly, please bear the risk in mind and use it with care also DYOR before putting a serious amount of crypto assets. More details about MushMon could be found in our blog below.\nhttps://blog.mushrooms.finance/introduction-to-mushmon-3a3518ebd3fd\nHow do you differentiate yourself from other yield aggregators?\n1) The first and most important is around asset safeguard. Security is always priority \u21161 in our mind. We have conducted three independent security code audits with leading audit partners including PeckShield, Dedaub and Coin Fabrik. And We are among the earliest projects having up to a $50K security bounty program within Immunefi. More details about our security code audits please refer to our Github link below.\nhttps://github.com/mushroomsforest/deployment/tree/main/audits\n2) We have well-designed tokenomics starting day one by applying a buyback-and-make mechanism to provide reasonable incentives to token hodler and every participant in Mushrooms ecosystem.\n3) We would like to focus on sustainable yield farming in the long term and innovation is carved into the spirit of core team by continually build and deliver in DeFi. In the last two months, we have upgraded nearly half of our strategies on Ethereum/Fantom/BSC to keep our earning APR competitive in the market.\n4) We will continue to build on multi-chains with active support on existing deployments on Ethereum, Binance Smart Chain, Fantom Opera and Polypon(Matic). We are very open-minded and react swiftly to the community feedback and be ready at any time for more integrations with DeFi legos.\nWhat's next for Mushrooms Finance\nWe will continue to build on multi-chains with active support on existing deployments on Ethereum, Binance Smart Chain, Fantom Opera and Polypon(Matic). We are very open-minded and react swiftly to the community feedback and be ready at any time for more integrations with DeFi legos.", "link": "https://devpost.com/software/mushrooms-finance", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nmushrooms finance is yet another crypto earning vault with focus on seeking sustainable profit in defi universe. the first version of the mushrooms finance smart contracts were inspired by other popular defi projects like yearn finance (yfi) and sushiswap among many others.\nwhat it does\nmushrooms finance is now alive in ethereum, binance smart chain, fantom and polygon with thousands of users and tens of auto-compounding earning strategies.\nhow we built it\nour core team started working in blockchain/ethereum/defi in the second half of 2018 and worked together for the last three years. we are a small but cohesive team. we have started mushrooms finance as a yield aggregator project aiming for sustainable yield mining since oct 2020.\naccomplishments that we're proud of\nwe have developed innovative product auto portfolio-rebalancing -----> tool !!!  mushmon. mushmon is a yield-farming boosted portfolio tool, which innovatively combines the traits of both defi yield-aggregator and grid trading algorithms. i would like to introduce mushmon as a resolution to the following four questions that everyone would always be asked about: 1) when should i sell the rip when the market is rocketing? 2) when should i buy the dip when the market is slumping heavily? 3) what if i miss the margin-call when good trade timing is there? 4) how can i make my money work for me all the time?\nmushmon will work at your configured risk preference to constantly monitor the market and let the trading bot work for you. it always tries to keep two assets in the portfolio of equal value, i.e., 1:1 valuation for stablecoin and bitcoin for example. if the price of bitcoin goes up (triggered by your target increase), the bot will execute \u201csell\u201d to secure some profit in stablecoin and if the price of bitcoin goes down (triggered by your target decrease), the bot will execute \u201cbuy\u201d to lower the average cost of existing position.\nin essence, mushmon could make money out of the fluctuations of the asset\u2019s price and reduce the volatility of expected returns by portfolio diversification.\nadditionally, mushmon allows you to earn yield-farming with underlying mushrooms earning vaults without any idle time for your deposited assets.\nwe believe mushmon is the first of its kind in the current defi landscape across different blockchains which combines yield-farming and portfolio rebalancing in a creative way for the benefit of its users. however, it is of course not a silver-bullet for all scenarios and may incur loss if the market falls deep in one direction without bouncing back (similar to impermanent loss in amm).\nlastly, please bear the risk in mind and use it with care also dyor before putting a serious amount of crypto assets. more details about mushmon could be found in our blog below.\nhttps://blog.mushrooms.finance/introduction-to-mushmon-3a3518ebd3fd\nhow do you differentiate yourself from other yield aggregators?\n1) the first and most important is around asset safeguard. security is always priority \u21161 in our mind. we have conducted three independent security code audits with leading audit partners including peckshield, dedaub and coin fabrik. and we are among the earliest projects having up to a $50k security bounty program within immunefi. more details about our security code audits please refer to our github link below.\nhttps://github.com/mushroomsforest/deployment/tree/main/audits\n2) we have well-designed tokenomics starting day one by applying a buyback-and-make mechanism to provide reasonable incentives to token hodler and every participant in mushrooms ecosystem.\n3) we would like to focus on sustainable yield farming in the long term and innovation is carved into the spirit of core team by continually build and deliver in defi. in the last two months, we have upgraded nearly half of our strategies on ethereum/fantom/bsc to keep our earning apr competitive in the market.\n4) we will continue to build on multi-chains with active support on existing deployments on ethereum, binance smart chain, fantom opera and polypon(matic). we are very open-minded and react swiftly to the community feedback and be ready at any time for more integrations with defi legos.\nwhat's next for mushrooms finance\nwe will continue to build on multi-chains with active support on existing deployments on ethereum, binance smart chain, fantom opera and polypon(matic). we are very open-minded and react swiftly to the community feedback and be ready at any time for more integrations with defi legos.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59501957}, {"Unnamed: 0": 1976, "autor": "Interview-Mate", "date": null, "content": "Inspiration\nThe basic motivation for this project was to create a tool that would simplify the interview process and automate candidate selection for a job by removing human biases through a more objective framework AI and Machine Learning.\nWhat it does\nThough the project is in its very initial phase. The basic premise of this project is to take a prerecorded interview video or audio and upload it to Assembly AI through which it will return a text transcript. Then it will be further analyzed through other NLP AIs which will evaluate the candidates skills, experiences, \"ranking\" in comparison to other candidates (also based on custom criteria given by the company) and finally yield a qualitative and visual answer on a candidates employability (ie. whether someone is a good fit for the company or not) Please see the chart below for a better visualization of the process.\nHow we built it\nThe technology used creating this project is both simple and complex. First the transcription was done through Assembly AI. The webapp was created using Streamlit (Python tool). The text analysis was done through Parallel dots API.\nChallenges we ran into\nI ran into some challenges with the Assembly AI. However, the documentation helped me resolved the issue.\nAccomplishments that we're proud of\nHaving a working prototype to showcase for hackathon.\nWhat we learned\nWorking with this project I learned how to work with Assembly AI. I also learned how to use Streamlit to make quick web APPs. Additionally, I gained a deeper understanding of NLP and AI\nWhat's next for Interview-Mate\nAdding more complex machine learning, NLP and Custom Algorithm for better text analysis\nImproving the UI for professional use.\nAdding youtube_dl functionality so video can be directly inputted through YT\nAnd many more\nHacker Age: 18+", "link": "https://devpost.com/software/interview-mate", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nthe basic motivation for this project was to create a -----> tool !!!  that would simplify the interview process and automate candidate selection for a job by removing human biases through a more objective framework ai and machine learning.\nwhat it does\nthough the project is in its very initial phase. the basic premise of this project is to take a prerecorded interview video or audio and upload it to assembly ai through which it will return a text transcript. then it will be further analyzed through other nlp ais which will evaluate the candidates skills, experiences, \"ranking\" in comparison to other candidates (also based on custom criteria given by the company) and finally yield a qualitative and visual answer on a candidates employability (ie. whether someone is a good fit for the company or not) please see the chart below for a better visualization of the process.\nhow we built it\nthe technology used creating this project is both simple and complex. first the transcription was done through assembly ai. the webapp was created using streamlit (python tool). the text analysis was done through parallel dots api.\nchallenges we ran into\ni ran into some challenges with the assembly ai. however, the documentation helped me resolved the issue.\naccomplishments that we're proud of\nhaving a working prototype to showcase for hackathon.\nwhat we learned\nworking with this project i learned how to work with assembly ai. i also learned how to use streamlit to make quick web apps. additionally, i gained a deeper understanding of nlp and ai\nwhat's next for interview-mate\nadding more complex machine learning, nlp and custom algorithm for better text analysis\nimproving the ui for professional use.\nadding youtube_dl functionality so video can be directly inputted through yt\nand many more\nhacker age: 18+", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59501976}, {"Unnamed: 0": 1997, "autor": "PhotoWound", "date": null, "content": "Inspiration\nWe were inspired to create this project when we saw the inadequacies in the current wound detection and measurement process. With one of the members of our team being involved in a home-health care company, we had first hand experience with these shortcomings and knew the impacts that these mistakes can have on the treatments process.\nWhat it does\nOur algorithm is a pixel-wise semantic segmentation of the initial image that points out the exact shape of the \"objects of interest\", in this case, wounds.\nHow we built it\nWe trained a UNET based structure built on the Keras framework on the MICCAI Ulcer Detection Database, which included thousands of annotated images of wounds and their proper masks. We used a 80%/20% split between our training and testing databases. No major data preprocessing was involved (other than size standardization) as the images were already mostly processed in the database.\nChallenges we ran into\nAlthough we were able to successfully develop our code, we were unable to connect our backend to our front end interface. We hope to do this in the next coming days so that we can truly have a fully functional application that is capable of revolutionizing the way in which we diagnose and measure wounds.\nAccomplishments that we're proud of\nWe are proud of the fact that, despite being a 3-person team consisting of members that (prior to Metrohacks) did not know each other, we were able to unite our talents and work towards the common vision of creating PhotoWound. Beyond this, we were also pleasantly surprised by our teams ability to rapidly prototype a solution to our problem and create an effective backend UNET model.\nWhat we learned\nThrough this project, we learned quite a bit about the wound detection and measurements process, as well as rapid prototyping in Figma and other such softwares, as well as the development of more advanced machine learning structures.\nWhat's next for PhotoWound\nWe hope to soon add a depth measurement tool using a bi-camera and color/edge detection system. We also hope to create a singular server less architecture in which hospitals, providers, nurses, and patients can all easily access the wound scans recorded by our app.", "link": "https://devpost.com/software/photowound", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe were inspired to create this project when we saw the inadequacies in the current wound detection and measurement process. with one of the members of our team being involved in a home-health care company, we had first hand experience with these shortcomings and knew the impacts that these mistakes can have on the treatments process.\nwhat it does\nour algorithm is a pixel-wise semantic segmentation of the initial image that points out the exact shape of the \"objects of interest\", in this case, wounds.\nhow we built it\nwe trained a unet based structure built on the keras framework on the miccai ulcer detection database, which included thousands of annotated images of wounds and their proper masks. we used a 80%/20% split between our training and testing databases. no major data preprocessing was involved (other than size standardization) as the images were already mostly processed in the database.\nchallenges we ran into\nalthough we were able to successfully develop our code, we were unable to connect our backend to our front end interface. we hope to do this in the next coming days so that we can truly have a fully functional application that is capable of revolutionizing the way in which we diagnose and measure wounds.\naccomplishments that we're proud of\nwe are proud of the fact that, despite being a 3-person team consisting of members that (prior to metrohacks) did not know each other, we were able to unite our talents and work towards the common vision of creating photowound. beyond this, we were also pleasantly surprised by our teams ability to rapidly prototype a solution to our problem and create an effective backend unet model.\nwhat we learned\nthrough this project, we learned quite a bit about the wound detection and measurements process, as well as rapid prototyping in figma and other such softwares, as well as the development of more advanced machine learning structures.\nwhat's next for photowound\nwe hope to soon add a depth measurement -----> tool !!!  using a bi-camera and color/edge detection system. we also hope to create a singular server less architecture in which hospitals, providers, nurses, and patients can all easily access the wound scans recorded by our app.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59501997}, {"Unnamed: 0": 2113, "autor": "HearAssist 2.0", "date": null, "content": "Inspiration\nCOVID-19 has impacted the lives globally especially the ones with disabilities hence, we have thought of a tool focusing on helping auditory-impaired individuals by assisting them. In the current scenario, there are special schools for them, so we decided to develop a tool that will bridge the gap and will enable them to study anywhere, interact with anyone around the world and even translate into any language.\nWhat it does\nWe built HearAssist 1.0 at BostonHacks, but we felt that it lacked the translation feature. Added Translation to this tool can act as a superpower for the users. It is even possible to view the transcriptions in mobile view for ease of access.\nHow we built it\nAPI Used: AssemblyAI and Google Cloud Translate\nBack-end: Flask, Replit\nFront-end: HTML, CSS\nChallenges we ran into\nTranslating the language was initially difficult but we found a solution using the Google Cloud Translate API.\nAccomplishments that we're proud of\nWe're proud of adding translations and enhancing the experience of our users.\nWhat we learned\nWe learned about google cloud translate API and making the transcriptions available in mobile view.\nWhat's next for HearAssist 2.0\nWe're planning to launch it on ProductHunt this week!", "link": "https://devpost.com/software/hearassist-2-0", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ncovid-19 has impacted the lives globally especially the ones with disabilities hence, we have thought of a -----> tool !!!  focusing on helping auditory-impaired individuals by assisting them. in the current scenario, there are special schools for them, so we decided to develop a tool that will bridge the gap and will enable them to study anywhere, interact with anyone around the world and even translate into any language.\nwhat it does\nwe built hearassist 1.0 at bostonhacks, but we felt that it lacked the translation feature. added translation to this tool can act as a superpower for the users. it is even possible to view the transcriptions in mobile view for ease of access.\nhow we built it\napi used: assemblyai and google cloud translate\nback-end: flask, replit\nfront-end: html, css\nchallenges we ran into\ntranslating the language was initially difficult but we found a solution using the google cloud translate api.\naccomplishments that we're proud of\nwe're proud of adding translations and enhancing the experience of our users.\nwhat we learned\nwe learned about google cloud translate api and making the transcriptions available in mobile view.\nwhat's next for hearassist 2.0\nwe're planning to launch it on producthunt this week!", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59502113}, {"Unnamed: 0": 2146, "autor": "AutoWhen2Meet", "date": null, "content": "Imagination / Vision\nOur idea was born from scheduling a team meeting for this weekend and for pretty much every event we have ever tried to organize in the past. It\u2019s always hard to find a common time, and so many collaborative ventures or just fun events do not end up happening because of it. Instead of building a whole scheduling tool from scratch in just 2 days, we decided to see if we can leverage an existing tool and make people want to use it more! We love using When2meet, so we tried to automate the process of syncing a user\u2019s Microsoft Teams\u2019 calendar and populating their schedule on to When2meet.\nCollaboration\nThe only people on our team we could reach were developers, so we split up the work by having one person focus on using Teams\u2019 API and the other person focus on using selenium to automate getting that data into When2meet.\nInnovation\nAlmost all the tech was new to us! We spent the weekend learning to use API\u2019s and selenium and building stuff while we were at it :)\nFail forward\nScope!! We had a way more ambitious plan in the beginning (create our own version of When2meet that cut down the time to plan a meeting by half!) but very quickly realized we could not do that in a weekend. We had to re-define scope twice more throughout our working period.\nDesign\nBy shaving down the time it takes to schedule meetings, there would be a lot more buy-in from participants to actually put it into their own schedules and show up.\nFunction\nAlthough we were unable to finish this project, we have a working proof-of-concept that could be refined into a working MVP!", "link": "https://devpost.com/software/autowhen2meet", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "imagination / vision\nour idea was born from scheduling a team meeting for this weekend and for pretty much every event we have ever tried to organize in the past. it\u2019s always hard to find a common time, and so many collaborative ventures or just fun events do not end up happening because of it. instead of building a whole scheduling -----> tool !!!  from scratch in just 2 days, we decided to see if we can leverage an existing -----> tool !!!  and make people want to use it more! we love using when2meet, so we tried to automate the process of syncing a user\u2019s microsoft teams\u2019 calendar and populating their schedule on to when2meet.\ncollaboration\nthe only people on our team we could reach were developers, so we split up the work by having one person focus on using teams\u2019 api and the other person focus on using selenium to automate getting that data into when2meet.\ninnovation\nalmost all the tech was new to us! we spent the weekend learning to use api\u2019s and selenium and building stuff while we were at it :)\nfail forward\nscope!! we had a way more ambitious plan in the beginning (create our own version of when2meet that cut down the time to plan a meeting by half!) but very quickly realized we could not do that in a weekend. we had to re-define scope twice more throughout our working period.\ndesign\nby shaving down the time it takes to schedule meetings, there would be a lot more buy-in from participants to actually put it into their own schedules and show up.\nfunction\nalthough we were unable to finish this project, we have a working proof-of-concept that could be refined into a working mvp!", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59502146}, {"Unnamed: 0": 2152, "autor": "Sampah.id", "date": null, "content": "Inspiration\nIn 2020, Indonesia produced 68 million tons of waste, with less than 10% of it being recycled. This waste includes recyclables, which were left piling up in landfills instead of being recycled into useful products such as recycled bottles and packaging. Our team endeavors to provide a tool to help educate the general public about the different types of waste, and even a waste detection system that is able to categorise waste based on a picture uploaded to the site, as well as where to drop the sorted recyclables.\nOur mission for this project is to equip people with the knowledge to recycle and be sustainable in their own households, and our vision is to increase the recycling rate from household waste and make Indonesia a cleaner place.\nWhat it does\nSampah.id is an all-in-one tool for people to learn about recycling. In the website, users would be able to access different features including:\nInformation: Comprehensive information about the categories of waste and which types of waste are recyclable, as well as how to recycle them\nItem detector: Allows user to upload or snapshot a picture of an object, in which an AI would detect the type of waste, as well as redirect users to the relevant recycling method\nTrash bank locator: Provides the nearest trash bank locations relative to the user\u2019s current location. Also contains basic information regarding trash banks and how they work in general\nHow we built it\nWe used Figma for design, and built the website through Node.js, html, and css for both frontend and backend. For the item detection we used Google Vision API (https://cloud.google.com/vision). This API takes a image file and able to detect their labels (the object name, label, etc), and for the trash bank location detector we used Google Maps API (https://developers.google.com/maps).\nChallenges we ran into\nWe ran into both technical and non-technical challenges during the course of this hackathon. Our technical issues include our inability to incorporate the APIs into our prototype webpage due to the lack of knowledge as we did not have enough time to properly learn about the framework. In addition, some of us were inexperienced and unfamiliar with the languages used in this hack.\nAs for non-technical issues, it was the first hackathon for all of us. We have little knowledge on app development and we lacked the organisational skills and preparation for the event, leading to a lackluster team management and work distribution, with each member of the team doing whatever they want out of the tasks that need to be done. Furthermore, we overestimated how much time we actually had to do the project, and so we were very pressed for time with this project.\nAccomplishments that we're proud of\nAs a team, we were able to complete our first hackathon and create our first website from scratch. In addition, we were able to learn and troubleshoot issues regarding node.js. Moreover, for some of us it was the first time doing website development and also the tools such as Git, Visual Studio, and node.js.\nWhat we learned\nThis project was a new experience for us, equipping the team with better understanding and knowledge as to the preparation before the hack starts. For example, with a strict time constraint, proper scheduling and planning as well as realistic MVP goals should be set. Furthermore, we learned how to develop a website and use different tools to do it.\nWhat's next for Sampah.id\nWe plan to make the API work, or if time allows we may build and teach an AI to detect the waste instead of using an API. If the base website is complete, we have thoughts of providing the incentive for people to use our object detection system, in which using it once generates an amount of money, possibly from ads, in which the proceeds can be donated to a non-profit organization geared towards sustainability or waste management in Indonesia.", "link": "https://devpost.com/software/sampah-id", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nin 2020, indonesia produced 68 million tons of waste, with less than 10% of it being recycled. this waste includes recyclables, which were left piling up in landfills instead of being recycled into useful products such as recycled bottles and packaging. our team endeavors to provide a -----> tool !!!  to help educate the general public about the different types of waste, and even a waste detection system that is able to categorise waste based on a picture uploaded to the site, as well as where to drop the sorted recyclables.\nour mission for this project is to equip people with the knowledge to recycle and be sustainable in their own households, and our vision is to increase the recycling rate from household waste and make indonesia a cleaner place.\nwhat it does\nsampah.id is an all-in-one tool for people to learn about recycling. in the website, users would be able to access different features including:\ninformation: comprehensive information about the categories of waste and which types of waste are recyclable, as well as how to recycle them\nitem detector: allows user to upload or snapshot a picture of an object, in which an ai would detect the type of waste, as well as redirect users to the relevant recycling method\ntrash bank locator: provides the nearest trash bank locations relative to the user\u2019s current location. also contains basic information regarding trash banks and how they work in general\nhow we built it\nwe used figma for design, and built the website through node.js, html, and css for both frontend and backend. for the item detection we used google vision api (https://cloud.google.com/vision). this api takes a image file and able to detect their labels (the object name, label, etc), and for the trash bank location detector we used google maps api (https://developers.google.com/maps).\nchallenges we ran into\nwe ran into both technical and non-technical challenges during the course of this hackathon. our technical issues include our inability to incorporate the apis into our prototype webpage due to the lack of knowledge as we did not have enough time to properly learn about the framework. in addition, some of us were inexperienced and unfamiliar with the languages used in this hack.\nas for non-technical issues, it was the first hackathon for all of us. we have little knowledge on app development and we lacked the organisational skills and preparation for the event, leading to a lackluster team management and work distribution, with each member of the team doing whatever they want out of the tasks that need to be done. furthermore, we overestimated how much time we actually had to do the project, and so we were very pressed for time with this project.\naccomplishments that we're proud of\nas a team, we were able to complete our first hackathon and create our first website from scratch. in addition, we were able to learn and troubleshoot issues regarding node.js. moreover, for some of us it was the first time doing website development and also the tools such as git, visual studio, and node.js.\nwhat we learned\nthis project was a new experience for us, equipping the team with better understanding and knowledge as to the preparation before the hack starts. for example, with a strict time constraint, proper scheduling and planning as well as realistic mvp goals should be set. furthermore, we learned how to develop a website and use different tools to do it.\nwhat's next for sampah.id\nwe plan to make the api work, or if time allows we may build and teach an ai to detect the waste instead of using an api. if the base website is complete, we have thoughts of providing the incentive for people to use our object detection system, in which using it once generates an amount of money, possibly from ads, in which the proceeds can be donated to a non-profit organization geared towards sustainability or waste management in indonesia.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59502152}, {"Unnamed: 0": 2179, "autor": "Code Share", "date": null, "content": "Inspiration \ud83d\udca1\ud83d\ude00\nThere are a lot of learning platforms available online and in our experience, the best ones always have an amazing community. Be it a YouTube Channel, a MOOC or a website dedicated to learning altogether. So, we decided to create a platform where community is where the tutorials come from! If students and even people from non coding platforms have a platform where they can share their knowledge with their own perspective, people who are from similar back grounds will be able to connect with the tutorial in a better manner. And thus we created an online community driven learning and sharing platform.\nWhat it does \u2753\u2754\nCode Share is an online community driven learning and sharing platform which allows users to write \"tutorials\" in a blog like fashion. They can create a rich blog with code snippets, images and videos and then add a practice question to go with the tutorial upon getting it published.\nThe Platform also has an online python IDE, where users can practice and solve the questions which accompany the tutorials they are interested in. They can keep a track of their profile which shows how many tutorials they have authored in total and how many did they complete.\nHow we built it \ud83c\udfc6\ud83e\udd47\nWe used bootstrap to create the layout of the website, made the necessary changes with CSS and HTML and we had the front end with us. We then used Django to create the back-end and hooked it up with cockroach-db for our database. For the IDE, we used the sys library available in python to help execute the input code written by the user, and printed the output via HttpResponse.\nChallenges we ran into \ud83e\uddff\ud83e\udd14\ud83e\udd28\nCreating an online IDE was a big deal! We wanted to create an IDE which will support C/C++, Java and python, plus other major programming languages. We didn't feel like simply embedding a third party tool on our website like replit (where's the fun in that ;) ).\nSo for the sake of a prototype, we defined our own \"input\" function which took the user input over a POST request, split it on '\\n' (endline) characters and than split it again on spaces, basically giving us a dictionary of dictionaries. Not Ideal, but we got it working. This is where we faced most tricky bugs\nAccomplishments that we're proud of \ud83e\udd73\ud83d\udcaa\nThinking an off the beat project and actually managing to implement a working prototype! We learnt that it is very difficult to code an online IDE ourselves as there are many security threats involved with running scripts on an Http server. So for now, we just went ahead with the python server(because sys is comparatively safe and we did not make any database queries through the IDE).\nWhat we learned \ud83d\ude0d\ud83d\ude01\nAlthough we didn't make an ideal online compiler, we still learnt a lot about how IDEs actually work. Also, we learnt some \"best practices\" to follow while dealing with system requests.\nWhat's next for Code Share \ud83e\uddff\nMaking it compatible with other languages than python, make some corrections on the front end and deploy the project with a proper domain name. This project could have some potential!", "link": "https://devpost.com/software/code-share", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration \ud83d\udca1\ud83d\ude00\nthere are a lot of learning platforms available online and in our experience, the best ones always have an amazing community. be it a youtube channel, a mooc or a website dedicated to learning altogether. so, we decided to create a platform where community is where the tutorials come from! if students and even people from non coding platforms have a platform where they can share their knowledge with their own perspective, people who are from similar back grounds will be able to connect with the tutorial in a better manner. and thus we created an online community driven learning and sharing platform.\nwhat it does \u2753\u2754\ncode share is an online community driven learning and sharing platform which allows users to write \"tutorials\" in a blog like fashion. they can create a rich blog with code snippets, images and videos and then add a practice question to go with the tutorial upon getting it published.\nthe platform also has an online python ide, where users can practice and solve the questions which accompany the tutorials they are interested in. they can keep a track of their profile which shows how many tutorials they have authored in total and how many did they complete.\nhow we built it \ud83c\udfc6\ud83e\udd47\nwe used bootstrap to create the layout of the website, made the necessary changes with css and html and we had the front end with us. we then used django to create the back-end and hooked it up with cockroach-db for our database. for the ide, we used the sys library available in python to help execute the input code written by the user, and printed the output via httpresponse.\nchallenges we ran into \ud83e\uddff\ud83e\udd14\ud83e\udd28\ncreating an online ide was a big deal! we wanted to create an ide which will support c/c++, java and python, plus other major programming languages. we didn't feel like simply embedding a third party -----> tool !!!  on our website like replit (where's the fun in that ;) ).\nso for the sake of a prototype, we defined our own \"input\" function which took the user input over a post request, split it on '\\n' (endline) characters and than split it again on spaces, basically giving us a dictionary of dictionaries. not ideal, but we got it working. this is where we faced most tricky bugs\naccomplishments that we're proud of \ud83e\udd73\ud83d\udcaa\nthinking an off the beat project and actually managing to implement a working prototype! we learnt that it is very difficult to code an online ide ourselves as there are many security threats involved with running scripts on an http server. so for now, we just went ahead with the python server(because sys is comparatively safe and we did not make any database queries through the ide).\nwhat we learned \ud83d\ude0d\ud83d\ude01\nalthough we didn't make an ideal online compiler, we still learnt a lot about how ides actually work. also, we learnt some \"best practices\" to follow while dealing with system requests.\nwhat's next for code share \ud83e\uddff\nmaking it compatible with other languages than python, make some corrections on the front end and deploy the project with a proper domain name. this project could have some potential!", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59502179}, {"Unnamed: 0": 2192, "autor": "Twenty Four Hours", "date": null, "content": "Inspiration\nEverybody wants to be productive, but we don't always get what we want done if we don't have that extra motivation. We built Twenty Four Hours in order to have that extra boost of motivation and confidence when working on a goal. Not only are there soft deadlines that encourage you to work on whatever you need to work on, but it always feels good to broadcast your goals and successes to others.\nWhat it does\nTwenty Four Hours allows you to make goals and set deadlines of, believe it or not, 24 hours. It's a very easy to navigate system that allows you to focus on setting goals for yourself rather than learning a new technology.\nFeatures we have added so far include: Email Sign-Up/Login Goal Setting Separate Goals Per Account\nHow we built it\nWe built Twenty Four Hours using a code-free website maker called Bubble (https://bubble.io/). Everything that we made was done in this website.\nChallenges we ran into\nWhile Bubble is a great tool for making simple websites, making anything more complicated than a blog becomes a challenge. Making a presentable UI was one of the many challenges we had to conquer. We were limited to only the tools that it provided to us, and this often wasn't enough. Adding functionality and good interactivity to the website was also a challenge because, again, it isn't really made for anything more than simple websites.\nAccomplishments that we're proud of\nLearning to use this technology during the contest was a challenge, but we were able to overcome it. Both of us have only ever worked with backend systems so being able to produce a good looking website is something we never would have thought we would be able to accomplish. Overall we are very proud with how the final product looks.\nWhat we learned\nThe most obvious thing we learned was how to use Bubble, but as I mentioned earlier, this is our first time working on the front-end of products. We had to learn how to properly use colors in order to make a presentable product as well as common design structures. This was a great learning experience and I believe, at least looking at the final product, that we were able to gather a lot from this project.\nWhat's next for Twenty Four Hours\nWe weren't able to add it in the time span provided, but originally we were also going to add the ability to donate to charity with this app. We would make it so that if you failed to complete a goal in the allotted 24 hours, you could make it so you automatically donated a set amount of money to a charity of your choosing. More failure cases could be added, but we thought this was a pretty cool idea.", "link": "https://devpost.com/software/twenty-four-hours", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\neverybody wants to be productive, but we don't always get what we want done if we don't have that extra motivation. we built twenty four hours in order to have that extra boost of motivation and confidence when working on a goal. not only are there soft deadlines that encourage you to work on whatever you need to work on, but it always feels good to broadcast your goals and successes to others.\nwhat it does\ntwenty four hours allows you to make goals and set deadlines of, believe it or not, 24 hours. it's a very easy to navigate system that allows you to focus on setting goals for yourself rather than learning a new technology.\nfeatures we have added so far include: email sign-up/login goal setting separate goals per account\nhow we built it\nwe built twenty four hours using a code-free website maker called bubble (https://bubble.io/). everything that we made was done in this website.\nchallenges we ran into\nwhile bubble is a great -----> tool !!!  for making simple websites, making anything more complicated than a blog becomes a challenge. making a presentable ui was one of the many challenges we had to conquer. we were limited to only the tools that it provided to us, and this often wasn't enough. adding functionality and good interactivity to the website was also a challenge because, again, it isn't really made for anything more than simple websites.\naccomplishments that we're proud of\nlearning to use this technology during the contest was a challenge, but we were able to overcome it. both of us have only ever worked with backend systems so being able to produce a good looking website is something we never would have thought we would be able to accomplish. overall we are very proud with how the final product looks.\nwhat we learned\nthe most obvious thing we learned was how to use bubble, but as i mentioned earlier, this is our first time working on the front-end of products. we had to learn how to properly use colors in order to make a presentable product as well as common design structures. this was a great learning experience and i believe, at least looking at the final product, that we were able to gather a lot from this project.\nwhat's next for twenty four hours\nwe weren't able to add it in the time span provided, but originally we were also going to add the ability to donate to charity with this app. we would make it so that if you failed to complete a goal in the allotted 24 hours, you could make it so you automatically donated a set amount of money to a charity of your choosing. more failure cases could be added, but we thought this was a pretty cool idea.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59502192}, {"Unnamed: 0": 2207, "autor": "Stockify", "date": null, "content": "Inspiration\nDo you encounter a situation where you have an item that you really really desire, and you go to the website, navigate to that product, and then\u2026 ops, you find that product is out of stock. So you keep checking the website from time to time. That happens to me often time. So I really wish I could have a tool that reminds me when the product is back in stock. That is why we decide to develop Stockify - a web application that checks the availability of products automatically.\nWhat it does\nUsers will receive notifications once their targets product are back in stock. It will save people a large amount of time on tedious, repetitive manual work on clicking on the website. All you need to do is just input an URL of the product and then Stockify will check the stock for you on a frequent time basis. If you have more than one product on your wish list? Don\u2019t worry. Stockify allows you to send several requests for different products on different websites at the same time. You can also create a list of products you wish to check their stock for, and make adding or delete any time.\nHow we built it & Challenges we ran into\nWe initially run the server on Node.js which is a lot harder than flask which we use afterwords. The website building cost us most of the hours. Due to our unfamiliarity with html5, we spent a lot of time determining the error in our webpage. Struggling on simple issues like GET and POST functionality. In the end, we have completed a website that we did our best to create.\nAccomplishments that we're proud of\nCheck the availability of products\nReceive notification in no time\nMultiple products monitoring possible\nEasy to manage a list of products\nLabor and time saving\nWhat we learned\nPython automation, Javascript, HTML ( GET and POST)\nWhat's next for Stockify\nEnable checking stock for more websites (we only use two websites for now) Enable vague search for products\u2019 names Price comparison with multiple websites Set up stock checking automatically at a certain frequency.", "link": "https://devpost.com/software/stockify-zfqo5m", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ndo you encounter a situation where you have an item that you really really desire, and you go to the website, navigate to that product, and then\u2026 ops, you find that product is out of stock. so you keep checking the website from time to time. that happens to me often time. so i really wish i could have a -----> tool !!!  that reminds me when the product is back in stock. that is why we decide to develop stockify - a web application that checks the availability of products automatically.\nwhat it does\nusers will receive notifications once their targets product are back in stock. it will save people a large amount of time on tedious, repetitive manual work on clicking on the website. all you need to do is just input an url of the product and then stockify will check the stock for you on a frequent time basis. if you have more than one product on your wish list? don\u2019t worry. stockify allows you to send several requests for different products on different websites at the same time. you can also create a list of products you wish to check their stock for, and make adding or delete any time.\nhow we built it & challenges we ran into\nwe initially run the server on node.js which is a lot harder than flask which we use afterwords. the website building cost us most of the hours. due to our unfamiliarity with html5, we spent a lot of time determining the error in our webpage. struggling on simple issues like get and post functionality. in the end, we have completed a website that we did our best to create.\naccomplishments that we're proud of\ncheck the availability of products\nreceive notification in no time\nmultiple products monitoring possible\neasy to manage a list of products\nlabor and time saving\nwhat we learned\npython automation, javascript, html ( get and post)\nwhat's next for stockify\nenable checking stock for more websites (we only use two websites for now) enable vague search for products\u2019 names price comparison with multiple websites set up stock checking automatically at a certain frequency.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59502207}, {"Unnamed: 0": 2230, "autor": "Fusion", "date": null, "content": "VIDEO LINK: https://drive.google.com/file/d/1eBhTndCY_TOc1Exnojuell04R3JYgWQA/view?usp=sharing\nInspiration\nEver been a panelist at a virtual event speaking into the dark abyss of cameras off?\nWhat about running a virtual event? Setting up your Google Slide, a Slido for audience Q&A, a Spotify playlist to share during breaks, and a feedback form to share with all participants... only to realize you forgot to open one of those tabs and you end up screen sharing your entire desktop with your spicy dms to your significant other while trying to find your other tab.\nThese are pain points we face every day as execs on student clubs like the Women in Tech Society, where we plan virtual events for 40-70 students every month. For our virtual events, we consistently have to screen-share 3-4 different platforms for a successful event. We also have to get creative with how we engage with event attendees, which just adds more frustration to our event planning.\nWe needed a solution that could:\nCombine the features of our favourite event planning platforms to increase event planning efficiency\nCreatively increase engagement with event attendees who have cameras off\nWe call this \"Fusion\"\nWhat it does\nFusion is an all in one event planning platform, that combines features of our favourite event planning platforms into one simple webpage that can be screen shared during an online event. For event moderators, Fusion is a webpage that:\nIntegrates with your Google Slides presentation\nIntegrates with Spotify for welcome & break music\nIntegrates with Slido for easy audience Q&A\nSend out and pin important announcements, which normally get lost in chats\nHas the ability to email all participants after an event (thank you notes and feedback forms)\nFor event attendees, Fusion is a simple webpage that:\nAllows them to pick a customized zoom background for the event to encourage cameras on\nIntegrates with Slido for easy Q&A without visiting another platform\nAllows them to see event details, speakers & announcements\nFusion is made to be simple. Event attendees don\u2019t have to register or create an account, and can access Fusion in all its glory through the event specific URL, which is easy to share.\nHow we built it\nTech Stack: MERN (Mongodb, Express.js, React.js, Node.js), Twilio Sendgrid for automated emails Design: Figma\nEvent settings are sent in through the Admin Portal (client side) and uploaded to our database by accessing the Node backend endpoints using Express (server)\nDatabase is queried for a specific event ID and the appropriate content is displayed for admins and attendees\nAttendees join an event using the unique event ID and their info is added to our database\nWith the click of a button, the Twilio Sendgrid integration sends a customized email to all attendees\nCheck it out in action here! https://drive.google.com/file/d/1c_XGoqwbMdeX1HgR-qO9FQRSfVnyyhMD/view?usp=sharing\nChallenges we ran into\nElaine: This is my first time working with Node and Express and had challenges getting accustomed to JavaScript. (If you see pythonic-style JavaScript, it was probably committed by me!) Because of this learning curve, I had difficulties making requests to our backend using Axios. It was also my first time setting up a Mongodb database so there was a bit of learning here as well.\nEng: I think the most challenging part about this Hackathon was collaborating in a team setting and learning to resolve merge conflicts with Git!\nAccomplishments that we're proud of\nAs event organizers ourselves, we\u2019re so proud of creating a tool that will help us solve many of our pain points running events in the future. We hope to continue to work on Fusion so that we can launch it and use Fusion in some of our own events!\nWhat we learned\nFor some of us, we learned a lot about new languages and tools such as Node.js, Mongodb, and Twilio. It was also fun to learn more about the virtual event space during our preliminary research and better understand some of the most common pain points experienced by both attendees and organizers. We were also able to look into other tools and APIs and get a good sense of next steps we want to take with this app!\nWhat's next for Fusion\nWe hope to continue work on this after the hackathon and hope to turn into a mini Startup. We are hoping to integrate with the Zoom API to leverage some of its features such as chat and event creation abilities.", "link": "https://devpost.com/software/fusion-ibt7hz", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "video link: https://drive.google.com/file/d/1ebhtndcy_toc1exnojuell04r3jygwqa/view?usp=sharing\ninspiration\never been a panelist at a virtual event speaking into the dark abyss of cameras off?\nwhat about running a virtual event? setting up your google slide, a slido for audience q&a, a spotify playlist to share during breaks, and a feedback form to share with all participants... only to realize you forgot to open one of those tabs and you end up screen sharing your entire desktop with your spicy dms to your significant other while trying to find your other tab.\nthese are pain points we face every day as execs on student clubs like the women in tech society, where we plan virtual events for 40-70 students every month. for our virtual events, we consistently have to screen-share 3-4 different platforms for a successful event. we also have to get creative with how we engage with event attendees, which just adds more frustration to our event planning.\nwe needed a solution that could:\ncombine the features of our favourite event planning platforms to increase event planning efficiency\ncreatively increase engagement with event attendees who have cameras off\nwe call this \"fusion\"\nwhat it does\nfusion is an all in one event planning platform, that combines features of our favourite event planning platforms into one simple webpage that can be screen shared during an online event. for event moderators, fusion is a webpage that:\nintegrates with your google slides presentation\nintegrates with spotify for welcome & break music\nintegrates with slido for easy audience q&a\nsend out and pin important announcements, which normally get lost in chats\nhas the ability to email all participants after an event (thank you notes and feedback forms)\nfor event attendees, fusion is a simple webpage that:\nallows them to pick a customized zoom background for the event to encourage cameras on\nintegrates with slido for easy q&a without visiting another platform\nallows them to see event details, speakers & announcements\nfusion is made to be simple. event attendees don\u2019t have to register or create an account, and can access fusion in all its glory through the event specific url, which is easy to share.\nhow we built it\ntech stack: mern (mongodb, express.js, react.js, node.js), twilio sendgrid for automated emails design: figma\nevent settings are sent in through the admin portal (client side) and uploaded to our database by accessing the node backend endpoints using express (server)\ndatabase is queried for a specific event id and the appropriate content is displayed for admins and attendees\nattendees join an event using the unique event id and their info is added to our database\nwith the click of a button, the twilio sendgrid integration sends a customized email to all attendees\ncheck it out in action here! https://drive.google.com/file/d/1c_xgoqwbmdex1hgr-qo9fqrsfvnyyhmd/view?usp=sharing\nchallenges we ran into\nelaine: this is my first time working with node and express and had challenges getting accustomed to javascript. (if you see pythonic-style javascript, it was probably committed by me!) because of this learning curve, i had difficulties making requests to our backend using axios. it was also my first time setting up a mongodb database so there was a bit of learning here as well.\neng: i think the most challenging part about this hackathon was collaborating in a team setting and learning to resolve merge conflicts with git!\naccomplishments that we're proud of\nas event organizers ourselves, we\u2019re so proud of creating a -----> tool !!!  that will help us solve many of our pain points running events in the future. we hope to continue to work on fusion so that we can launch it and use fusion in some of our own events!\nwhat we learned\nfor some of us, we learned a lot about new languages and tools such as node.js, mongodb, and twilio. it was also fun to learn more about the virtual event space during our preliminary research and better understand some of the most common pain points experienced by both attendees and organizers. we were also able to look into other tools and apis and get a good sense of next steps we want to take with this app!\nwhat's next for fusion\nwe hope to continue work on this after the hackathon and hope to turn into a mini startup. we are hoping to integrate with the zoom api to leverage some of its features such as chat and event creation abilities.", "sortedWord": "None", "removed": "Nan", "score": 7, "comments": 1, "media": null, "medialink": null, "identifyer": 59502230}, {"Unnamed: 0": 2252, "autor": "WestERnScheduling", "date": null, "content": "Inspiration\nRecently I burned my hand quite badly and went to the ER to have it treated. It took 4 hours for a doctor to see me, for less than 15 minutes. A 4 hour wait time is unfortunately considered to be \"not bad\" in much of Ontario.\nWhat it does\nOur program uses a combination of weather, population density, and proximity data, as well as a calendar of community events to predict the number of patients expected in an ER on any given evening. This tool will help hospitals to better estimate their need for doctors, nurses, and support staff, and hopefully reduce wait times.\nHow we built it\nIn a whirlwind of caffeine and Papa Johns... Additionally, we built our project by utilizing a series of python scripts. Our python scripts make use of a weather api, google maps api, and a date time api to provide relevant data for our calculations. Additional data were sourced from several studies as well as London census data.\nChallenges we ran into\nThe first large issue we ran into was the lack of historical data on the number of ER admission to London hospitals. We addressed this by building our program to take the 5 year average for that date as a parameter. As this is a prototype, we would expect that any real-world implementations would be accompanied by better data from the hospital, and so we could compute averages ourselves. Another challenge we ran into was being able to make our program accessible to admin through the use of a website. Our design initially involved using WinSCP to host an HTML frontend serviced by a php file which would run and interact with our python script. Unfortunately, we were unable to overcome this challenge but will work to improve this in future development. And a lack of Papa Johns...\nAccomplishments that we're proud of\nOne of the things we are proud of is our concept, as it addresses a real-world challenge, and we believe our project if fully implemented could have a significant impact on improving the current scheduling system in ER's. An accomplishment we are proud of is our ability to create a functional program in a 36 hours.\nWhat we learned\nForm this process we have learned that good programs take time and cannot be rushed. This is why we plan on continuing the development of this application after Hack Western ends. Also Papa Johns is good pizza but King Richies is just better value.\nWhat's next for WestERnScheduling\nKing Richies.", "link": "https://devpost.com/software/westernscheduling", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nrecently i burned my hand quite badly and went to the er to have it treated. it took 4 hours for a doctor to see me, for less than 15 minutes. a 4 hour wait time is unfortunately considered to be \"not bad\" in much of ontario.\nwhat it does\nour program uses a combination of weather, population density, and proximity data, as well as a calendar of community events to predict the number of patients expected in an er on any given evening. this -----> tool !!!  will help hospitals to better estimate their need for doctors, nurses, and support staff, and hopefully reduce wait times.\nhow we built it\nin a whirlwind of caffeine and papa johns... additionally, we built our project by utilizing a series of python scripts. our python scripts make use of a weather api, google maps api, and a date time api to provide relevant data for our calculations. additional data were sourced from several studies as well as london census data.\nchallenges we ran into\nthe first large issue we ran into was the lack of historical data on the number of er admission to london hospitals. we addressed this by building our program to take the 5 year average for that date as a parameter. as this is a prototype, we would expect that any real-world implementations would be accompanied by better data from the hospital, and so we could compute averages ourselves. another challenge we ran into was being able to make our program accessible to admin through the use of a website. our design initially involved using winscp to host an html frontend serviced by a php file which would run and interact with our python script. unfortunately, we were unable to overcome this challenge but will work to improve this in future development. and a lack of papa johns...\naccomplishments that we're proud of\none of the things we are proud of is our concept, as it addresses a real-world challenge, and we believe our project if fully implemented could have a significant impact on improving the current scheduling system in er's. an accomplishment we are proud of is our ability to create a functional program in a 36 hours.\nwhat we learned\nform this process we have learned that good programs take time and cannot be rushed. this is why we plan on continuing the development of this application after hack western ends. also papa johns is good pizza but king richies is just better value.\nwhat's next for westernscheduling\nking richies.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59502252}, {"Unnamed: 0": 2258, "autor": "NotifyMe", "date": null, "content": "Inspiration\nIt is often very difficult to enrol in courses with limited lecture sections. In cases like this, students need to constantly keep an eye on the University of Waterloo course offerings website in case a spot opens up in the course. On top of this, they must also rush against time to enrol into the course before other students can take up that spot.\nHaving experienced this struggle, we decided to develop a program to notify users when a spot in their desired courses opens up, so that they can be the first to enrol in it.\nWhat it does\nNotifyMe uses web scraping to find certain information about University of Waterloo courses. This includes which courses are being offered that semester, along with the professors teaching it. The tool makes course selection easier for students, as they no longer need to search the university website each time they want information regarding their courses.\nHow we built it\nNotifyMe was built entirely using Python. We use the library, Beautiful Soup, to extract data from the University of Waterloo course website and integrate it into our app. The tool creates a parse tree for parsed pages, which can then be used to obtain data from HTML.\nChallenges we ran into\nA challenge we ran into was when we had difficulties connecting the web scraped data to the notifier portion of the program. We collected the web scraped data in a dictionary-like parse tree; however, we could not determine how best to use that dictionary to send notifications in real time. Fortunately, we realized we could treat the web scraped data as an object, and used Object Oriented Programming (OOP) principles to simplify the connection between the files.\nAccomplishments that we're proud of\nWe are proud of designing and creating an app which is extremely beneficial to the students of the University of Waterloo. Especially during course selection period, students can benefit from the technology we created. We are also proud of learning a new technology, Figma, and creating visuals with it during a time constraint. Although none of us had thorough experience with the tool, we were able to use it both efficiently and effectively.\nWhat we learned\nThis was the first time we approached web scraping. Therefore, web scraping was a major proportion of the skills we learnt during this hackathon. Furthermore, we also learnt how to use python as an Object Oriented language, which was something we had also never done before. Lastly, we also learnt how to use Figma as a tool to design prototypes for our program, which was also something that we had never done before.\nWhat's next for notify me\nWe hope to expand our application so that it features information regarding courses from other universities in the surrounding area. This would widen our target audience to other students. We also hope to integrate more features into our app, such as a countdown until course selection begins.", "link": "https://devpost.com/software/notify-me-8l3sc4", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nit is often very difficult to enrol in courses with limited lecture sections. in cases like this, students need to constantly keep an eye on the university of waterloo course offerings website in case a spot opens up in the course. on top of this, they must also rush against time to enrol into the course before other students can take up that spot.\nhaving experienced this struggle, we decided to develop a program to notify users when a spot in their desired courses opens up, so that they can be the first to enrol in it.\nwhat it does\nnotifyme uses web scraping to find certain information about university of waterloo courses. this includes which courses are being offered that semester, along with the professors teaching it. the -----> tool !!!  makes course selection easier for students, as they no longer need to search the university website each time they want information regarding their courses.\nhow we built it\nnotifyme was built entirely using python. we use the library, beautiful soup, to extract data from the university of waterloo course website and integrate it into our app. the tool creates a parse tree for parsed pages, which can then be used to obtain data from html.\nchallenges we ran into\na challenge we ran into was when we had difficulties connecting the web scraped data to the notifier portion of the program. we collected the web scraped data in a dictionary-like parse tree; however, we could not determine how best to use that dictionary to send notifications in real time. fortunately, we realized we could treat the web scraped data as an object, and used object oriented programming (oop) principles to simplify the connection between the files.\naccomplishments that we're proud of\nwe are proud of designing and creating an app which is extremely beneficial to the students of the university of waterloo. especially during course selection period, students can benefit from the technology we created. we are also proud of learning a new technology, figma, and creating visuals with it during a time constraint. although none of us had thorough experience with the tool, we were able to use it both efficiently and effectively.\nwhat we learned\nthis was the first time we approached web scraping. therefore, web scraping was a major proportion of the skills we learnt during this hackathon. furthermore, we also learnt how to use python as an object oriented language, which was something we had also never done before. lastly, we also learnt how to use figma as a tool to design prototypes for our program, which was also something that we had never done before.\nwhat's next for notify me\nwe hope to expand our application so that it features information regarding courses from other universities in the surrounding area. this would widen our target audience to other students. we also hope to integrate more features into our app, such as a countdown until course selection begins.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59502258}, {"Unnamed: 0": 2272, "autor": "Wellbe", "date": null, "content": "Inspiration\n1 in 4 people globally will experience a mental health condition in their lifetime. But 54% of people with a mental health illness do not access any treatment.\nEven for those who seek help, a shortage of mental health practitioners means they may not get the support they need straight away. In Australia, wait times for psychologists have ballooned, to months since the start of the COVID-19 pandemic. Some patients are even being turned away.\nThere is a growing gap between those that need professional care and those that can access it.\nSo now, more than ever, friends and family have an important role to play in providing immediate support to loved ones who are struggling, before they can access professional help.\nHowever 63% of people are not confident they know the signs that someone might be struggling with life, or how they should approach them and offer assistance.\nWe\u2019ve recognised that there is an immediate need to upskill more everyday people so that they are knowledgeable enough to help loved ones struggling with mental health.\nWhat our app does\nWellbe is the app that enables conversations about mental health.\nIt educates users about various mental health conditions, signs and symptoms to look for, and how to offer and provide support in real life situations.\nAddressing a gap in the market\nFrom a business perspective, there are already many mental health apps in the market, most are focused on self-care for the individual. There is currently a gap in the market for apps that encourage genuine human connection within the mental health community.\nWellbe\u2019s purpose however is to encourage human connection by arming people with the right information and tools to reach out to someone they know who is in need.\nOur Target Audience\nOur App has a broad appeal to all demographics. Nearly a quarter of people globally (23%) would like to see education features on social media platforms in the future for learning. For our bullseye audience, in the next 5 years Millennials in AU, UK and the US are more likely to take an online course to expand their knowledge for personal development.\nHow the app works\nTo ensure the most relevant content is being surfaced, users will be asked to complete a quiz when they first use the app which will tailor the content that appears first in the feed. The quiz will determine what their motivation for using the app is, who they are looking to help, and if there is a particular mental health illness or topic they would like to know more about. The search function featured on every page, will also enable users to seek out specific content, whilst the favourite feature will allow them to save content they\u2019d like to refer back to later.\nAt its core, the app is an educational resource that is easily accessible and intuitive to navigate. Users will have access to a content library feed of bite sized articles and visual aids teaching awareness and understanding of various mental health issues. All content is to be created in house to ensure content is customised to fit the brand's tone of voice.\nA key skill that Wellbe seeks to promote is the ability to start a conversation with a loved one about a suspected mental health issue. Our \u2018practice conversations\u2019 tool will enable users to practice having the conversation with an AI bot and gain confidence before doing it in real life.\nHow we built it\nUsing Figma and a library of free images, we put our UX skills to practice. We followed the standard dev ops process - starting off with a wireframe and then moved onto hi-fi prototypes.\nChallenges we ran into\nWhen we first came up with this idea, we had to ensure we weren't creating an app that replaces professional mental health practitioners. We think it's very important to refer users to seek professional help.\nWe overcame this challenge by ensuring our product offering was clear, and we aim to include shortcuts to professionals and other organisations in the future to make it as accessible for our users.\nAccomplishments that we're proud of\nCreating a chatbot for the first time with a unique tone of voice was something new for all of us, but we are happy with how it's turned out!\nWe also overcame different time zones. Jane was country hopping in Europe, so we needed to coordinate accordingly and ensure we were communicating concisely and clearly.\nWhat's next for Wellbe\nNext on the horizon for Wellbe is to turn our prototype concept into a functioning app.\nPartnerships\nThere is also potential for future partnerships with the likes of other mental health organisations like Mental Health First Aid. We can include partner's referral links to ensure Wellbe is the go to place for resources for the mental health community.\nThe ability to link our app to social media will also enable users to identify friends who are using Wellbe who may be open to having conversations about mental health so that they can reach out and connect.\nGlobal Expansion\nThis demo has been created with the Australian market in mind. We would look to expand to other markets and adapt the tone of voice and content to match cultural nuances or trends\nThank you for taking the time to read about Wellbe: the app that enables conversations about mental health.", "link": "https://devpost.com/software/wellbe-jrox10", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\n1 in 4 people globally will experience a mental health condition in their lifetime. but 54% of people with a mental health illness do not access any treatment.\neven for those who seek help, a shortage of mental health practitioners means they may not get the support they need straight away. in australia, wait times for psychologists have ballooned, to months since the start of the covid-19 pandemic. some patients are even being turned away.\nthere is a growing gap between those that need professional care and those that can access it.\nso now, more than ever, friends and family have an important role to play in providing immediate support to loved ones who are struggling, before they can access professional help.\nhowever 63% of people are not confident they know the signs that someone might be struggling with life, or how they should approach them and offer assistance.\nwe\u2019ve recognised that there is an immediate need to upskill more everyday people so that they are knowledgeable enough to help loved ones struggling with mental health.\nwhat our app does\nwellbe is the app that enables conversations about mental health.\nit educates users about various mental health conditions, signs and symptoms to look for, and how to offer and provide support in real life situations.\naddressing a gap in the market\nfrom a business perspective, there are already many mental health apps in the market, most are focused on self-care for the individual. there is currently a gap in the market for apps that encourage genuine human connection within the mental health community.\nwellbe\u2019s purpose however is to encourage human connection by arming people with the right information and tools to reach out to someone they know who is in need.\nour target audience\nour app has a broad appeal to all demographics. nearly a quarter of people globally (23%) would like to see education features on social media platforms in the future for learning. for our bullseye audience, in the next 5 years millennials in au, uk and the us are more likely to take an online course to expand their knowledge for personal development.\nhow the app works\nto ensure the most relevant content is being surfaced, users will be asked to complete a quiz when they first use the app which will tailor the content that appears first in the feed. the quiz will determine what their motivation for using the app is, who they are looking to help, and if there is a particular mental health illness or topic they would like to know more about. the search function featured on every page, will also enable users to seek out specific content, whilst the favourite feature will allow them to save content they\u2019d like to refer back to later.\nat its core, the app is an educational resource that is easily accessible and intuitive to navigate. users will have access to a content library feed of bite sized articles and visual aids teaching awareness and understanding of various mental health issues. all content is to be created in house to ensure content is customised to fit the brand's tone of voice.\na key skill that wellbe seeks to promote is the ability to start a conversation with a loved one about a suspected mental health issue. our \u2018practice conversations\u2019 -----> tool !!!  will enable users to practice having the conversation with an ai bot and gain confidence before doing it in real life.\nhow we built it\nusing figma and a library of free images, we put our ux skills to practice. we followed the standard dev ops process - starting off with a wireframe and then moved onto hi-fi prototypes.\nchallenges we ran into\nwhen we first came up with this idea, we had to ensure we weren't creating an app that replaces professional mental health practitioners. we think it's very important to refer users to seek professional help.\nwe overcame this challenge by ensuring our product offering was clear, and we aim to include shortcuts to professionals and other organisations in the future to make it as accessible for our users.\naccomplishments that we're proud of\ncreating a chatbot for the first time with a unique tone of voice was something new for all of us, but we are happy with how it's turned out!\nwe also overcame different time zones. jane was country hopping in europe, so we needed to coordinate accordingly and ensure we were communicating concisely and clearly.\nwhat's next for wellbe\nnext on the horizon for wellbe is to turn our prototype concept into a functioning app.\npartnerships\nthere is also potential for future partnerships with the likes of other mental health organisations like mental health first aid. we can include partner's referral links to ensure wellbe is the go to place for resources for the mental health community.\nthe ability to link our app to social media will also enable users to identify friends who are using wellbe who may be open to having conversations about mental health so that they can reach out and connect.\nglobal expansion\nthis demo has been created with the australian market in mind. we would look to expand to other markets and adapt the tone of voice and content to match cultural nuances or trends\nthank you for taking the time to read about wellbe: the app that enables conversations about mental health.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59502272}, {"Unnamed: 0": 2275, "autor": "Shelter Alert", "date": null, "content": "Inspiration\nWe have seen the huge number of homeless people on our city streets. We have spoken to shelters and members of the community about the biggest problems facing homeless people in Australia and their answer was overwhelmingly: that it is hard to keep track of their guests. People just vanish off the street and shelters don't know what happened to them. People can and do go missing without anyone noticing.\nWhat it does\nSHELTER ALERT is an easy-to-use, super light-weight and secure tool for shelter workers to ensure that their housing insecure guests are free from harm.\nShelter workers can download the app from the app store then log-in with their verified and secure SHELTER-ID, personal username and password.\nWhen guests arrive at the shelter, they can use the search bar to find their name, and use the attached to identify them.\nThey can then view that guest's profile, see the currently active safety checks, and add new checks on request from guests.\nThey can also update the guests preferred emergency contact details, who will be notified in the event of an alert.\nIf a guest is not checked-in to the shelter within an hour of their set safety check, an alert will be triggered that sends a text message to all workers at the shelter, the shelter manager and the guest\u2019s emergency contact - informing them that the guest didn\u2019t get in and may be in danger.\nThese parties are encouraged to contact the police and file a missing persons report, and speak to the people that may know the guest to determine any potential dangers.\nHow we built it\nAfter building the Android mobile application files in the Android Studio IDE we perform this demo on the Pixel 4 in-built emulator. The splash screen automatically appears and has a fading animation to navigate to the landing screen which is the login. We enter the shelter manager or co-ordinator\u2019s credentials (which will be pre-assigned by the supervisor). After the id, username and password are verified from our online/cloud database- the firebase real-time database by Google developers- we reach the check-in screen. Here we select from the drop-down menu the guest\u2019s name and their picture appears. We then view the safety check list, deleting the expired or irrelevant ones alongside the guest\u2019s profile details. From here one can enter a new emergency contact or even schedule a new safety check alert. The app is programming in Kotlin and its resource assets include layout files to strings, colors, styles, dimensions, drawable icons and media. Kotlin good practices are followed like the MVC software design pattern, concurrency and recycler view to improve performance (validated by Profiler recordings) and sound UI/UX principles like colour button affordances and visibility of system status by the progress bar and popup message after the login is successful. Dependencies for the firebase remote database storage, UI widgets and live data using android lifecycles and viewmodels are implemented in the gradle and network use and smooth navigation from the manifest files. Google material design, styles, icons and documentation has been diligently followed as well.\nChallenges we ran into\nThis is our first hack-a-thon so we had no experience.\nWe ran really tight on time.\nWe spent 7 hours stuck on ideation because we struggled to find a problem that other people weren't solving. ## Accomplishments that we're proud of\nWe worked very well together as a team. We all played our part and pooled our strengths to get the project done.\nThe UI/UX is sleek and highly-usable.\nWe are proud of the highly-creative pitch that took a lot of filming, practicing and editing. ## What we learned\nOur ideation skills have improved ten-fold.\nWe learnt how fun it is to participate in a hack-a-thon.\nWe learnt how quickly we can work when time is not on our side.\nWe learnt resilience. We didn't give up when we didn't have a solid idea after 7 hours. ## What's next for Shelter Alert\nWith proper funding and time, we want to add more features to the system such as a conflict manager to help shelter workers avoid conflicts between guests and a flag-system that allows staff to have a record of guest's health risks, medical problems and allergies when they check in.", "link": "https://devpost.com/software/shelter-alert", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe have seen the huge number of homeless people on our city streets. we have spoken to shelters and members of the community about the biggest problems facing homeless people in australia and their answer was overwhelmingly: that it is hard to keep track of their guests. people just vanish off the street and shelters don't know what happened to them. people can and do go missing without anyone noticing.\nwhat it does\nshelter alert is an easy-to-use, super light-weight and secure -----> tool !!!  for shelter workers to ensure that their housing insecure guests are free from harm.\nshelter workers can download the app from the app store then log-in with their verified and secure shelter-id, personal username and password.\nwhen guests arrive at the shelter, they can use the search bar to find their name, and use the attached to identify them.\nthey can then view that guest's profile, see the currently active safety checks, and add new checks on request from guests.\nthey can also update the guests preferred emergency contact details, who will be notified in the event of an alert.\nif a guest is not checked-in to the shelter within an hour of their set safety check, an alert will be triggered that sends a text message to all workers at the shelter, the shelter manager and the guest\u2019s emergency contact - informing them that the guest didn\u2019t get in and may be in danger.\nthese parties are encouraged to contact the police and file a missing persons report, and speak to the people that may know the guest to determine any potential dangers.\nhow we built it\nafter building the android mobile application files in the android studio ide we perform this demo on the pixel 4 in-built emulator. the splash screen automatically appears and has a fading animation to navigate to the landing screen which is the login. we enter the shelter manager or co-ordinator\u2019s credentials (which will be pre-assigned by the supervisor). after the id, username and password are verified from our online/cloud database- the firebase real-time database by google developers- we reach the check-in screen. here we select from the drop-down menu the guest\u2019s name and their picture appears. we then view the safety check list, deleting the expired or irrelevant ones alongside the guest\u2019s profile details. from here one can enter a new emergency contact or even schedule a new safety check alert. the app is programming in kotlin and its resource assets include layout files to strings, colors, styles, dimensions, drawable icons and media. kotlin good practices are followed like the mvc software design pattern, concurrency and recycler view to improve performance (validated by profiler recordings) and sound ui/ux principles like colour button affordances and visibility of system status by the progress bar and popup message after the login is successful. dependencies for the firebase remote database storage, ui widgets and live data using android lifecycles and viewmodels are implemented in the gradle and network use and smooth navigation from the manifest files. google material design, styles, icons and documentation has been diligently followed as well.\nchallenges we ran into\nthis is our first hack-a-thon so we had no experience.\nwe ran really tight on time.\nwe spent 7 hours stuck on ideation because we struggled to find a problem that other people weren't solving. ## accomplishments that we're proud of\nwe worked very well together as a team. we all played our part and pooled our strengths to get the project done.\nthe ui/ux is sleek and highly-usable.\nwe are proud of the highly-creative pitch that took a lot of filming, practicing and editing. ## what we learned\nour ideation skills have improved ten-fold.\nwe learnt how fun it is to participate in a hack-a-thon.\nwe learnt how quickly we can work when time is not on our side.\nwe learnt resilience. we didn't give up when we didn't have a solid idea after 7 hours. ## what's next for shelter alert\nwith proper funding and time, we want to add more features to the system such as a conflict manager to help shelter workers avoid conflicts between guests and a flag-system that allows staff to have a record of guest's health risks, medical problems and allergies when they check in.", "sortedWord": "None", "removed": "Nan", "score": 10, "comments": 0, "media": null, "medialink": null, "identifyer": 59502275}, {"Unnamed: 0": 2283, "autor": "Doctorize.AI", "date": null, "content": "Inspiration\n\"A correct diagnosis is three-fourths the remedy.\" - Mahatma Gandhi\nIn this fast-paced world where everything seems to be conveniently accessed in a matter of seconds at our fingertips with our smartphones and laptops, some parts of our lives can not be replaced or compromised. Let's not kid ourselves, we are all guilty of getting a scare when we see something suspicious on our skin or if we feel funny, we fall into the black hole of googling the symptoms, believing everything we read and scaring ourselves to an unnecessary extent.\nEven forty-four percent of Americans prefer to self-diagnose their illness online rather than see a medical professional, according to a survey conducted by The Tinker Law Firm. That is an alarmingly large amount of people for just a country.\nWhile it is cheaper to go to Google to self-diagnose rather than to visit a doctor, this often leads to inaccurate diagnosis and can be extremely dangerous as they might follow a wrong treatment plan or may not realize the severity of their condition.\nThrough our personal experiences in Asian countries, it was common to get an X-Ray scan at one place, and then another appointment with a doctor had to be booked the next day to receive an opinion. We also wanted to create a way to avoid inconvenience for some people and make it socially sustainable this way. Especially with the exponentially rising cases of damaging effects on the environment, we wanted to create a means of a sustainable health care system while reducing the negative impacts.\nWhat it does\nDoctorize.AI is an easy-to-use web application that uses Machine Learning to scan the images or audio clip uploaded, and with a simple click of a button, it is processed, and the results inform if there are any concerning medical issues recognized or if the x-ray is clear. It also lets you know if you must seek immediate medical attention and connects you to a matching specialist to help you out. Worried about something in general? Use the \u201cRequest A Doctor\u201d to connect and talk all your worries out.\nAn added bonus: Patients and doctors can use Doctorize.AI as an extra tool to get an instantaneous second opinion and avoid any false negative/positive results, further reducing the load of the healthcare system, making this web application socially sustainable. It is also a safe and low-carbon health system, protecting the environment.\nOur models are able to recognize and respond to cases by classifying:\n- skin cancer (Malignant or Benign)\n- brain tumor (Glioma_Tumor, Meningioma_Tumor, Pituitary_Tumor, No_Tumor)\n- X-ray (Tuberculosis, Pneumonia, COVID-19 induced Pneumonia, or Normal)\nHow we built it\nThe frontend was built using:\n- Next.js\n- HTML\n- CSS\n- JavaScript\nThe backend was built using:\n- Flask\n- Python\n- TensorFlow/Keras for the Deep learning models to classify images/audio\n- AWS S3 for storage of large data set\nChallenges we ran into\nAs four individuals came together, we were bursting with uncountable ideas, so it took a long discussion or two to settle and choose what we could realistically achieve in a span of 36 hours.\nHere are a few challenges we ran into:\n- Lack of dataset availability\n- Different time-zones\n- Mix between first time hacker, new hacker(s), and experienced hacker in the team\n- AWS S3 - Simple Storage Service\n- Storage of large data\n- AWS Sagemaker\n- Computational power - deep learning takes time\nAccomplishments that we're proud of\n- Being able to tackle and develop the Machine Learning Models with the supportive team we had.\n- Creating a successful clean and polished look to the design\n- Models with over 80% accuracy across the board\n- Figuring out how to implement Flask\n- Experimenting with AWS (S3, and Sagemaker (not as successful))\nWhat we learned\n- Together as a team, we learnt how to use and apply CSS in an efficient way and how different CSS tools helped to achieve certain looks we were aiming for.\n- We also learned how to use Flask to connect ML models to our web application.\n- Further, we learned how to use AWS (S3, and Sagemaker (not as successful)).\nWhat's next for Doctorize.AI\n- Allow patients and doctors to interact smoothly on the platform\n- Expand our collection of medical cases that can be scanned and recognized such as more types of bacteria/viruses and rashes\n- Bring in new helpful features such as advanced search of specialists and general doctors in the area of your own choice\n- Record the patient\u2019s history and information for future references\n- QR codes on patient\u2019s profile for smoother connectivity\n- Voice Memo AI to summarize what the patient is talking about into targeted key topics", "link": "https://devpost.com/software/doctorize-ai", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\n\"a correct diagnosis is three-fourths the remedy.\" - mahatma gandhi\nin this fast-paced world where everything seems to be conveniently accessed in a matter of seconds at our fingertips with our smartphones and laptops, some parts of our lives can not be replaced or compromised. let's not kid ourselves, we are all guilty of getting a scare when we see something suspicious on our skin or if we feel funny, we fall into the black hole of googling the symptoms, believing everything we read and scaring ourselves to an unnecessary extent.\neven forty-four percent of americans prefer to self-diagnose their illness online rather than see a medical professional, according to a survey conducted by the tinker law firm. that is an alarmingly large amount of people for just a country.\nwhile it is cheaper to go to google to self-diagnose rather than to visit a doctor, this often leads to inaccurate diagnosis and can be extremely dangerous as they might follow a wrong treatment plan or may not realize the severity of their condition.\nthrough our personal experiences in asian countries, it was common to get an x-ray scan at one place, and then another appointment with a doctor had to be booked the next day to receive an opinion. we also wanted to create a way to avoid inconvenience for some people and make it socially sustainable this way. especially with the exponentially rising cases of damaging effects on the environment, we wanted to create a means of a sustainable health care system while reducing the negative impacts.\nwhat it does\ndoctorize.ai is an easy-to-use web application that uses machine learning to scan the images or audio clip uploaded, and with a simple click of a button, it is processed, and the results inform if there are any concerning medical issues recognized or if the x-ray is clear. it also lets you know if you must seek immediate medical attention and connects you to a matching specialist to help you out. worried about something in general? use the \u201crequest a doctor\u201d to connect and talk all your worries out.\nan added bonus: patients and doctors can use doctorize.ai as an extra -----> tool !!!  to get an instantaneous second opinion and avoid any false negative/positive results, further reducing the load of the healthcare system, making this web application socially sustainable. it is also a safe and low-carbon health system, protecting the environment.\nour models are able to recognize and respond to cases by classifying:\n- skin cancer (malignant or benign)\n- brain tumor (glioma_tumor, meningioma_tumor, pituitary_tumor, no_tumor)\n- x-ray (tuberculosis, pneumonia, covid-19 induced pneumonia, or normal)\nhow we built it\nthe frontend was built using:\n- next.js\n- html\n- css\n- javascript\nthe backend was built using:\n- flask\n- python\n- tensorflow/keras for the deep learning models to classify images/audio\n- aws s3 for storage of large data set\nchallenges we ran into\nas four individuals came together, we were bursting with uncountable ideas, so it took a long discussion or two to settle and choose what we could realistically achieve in a span of 36 hours.\nhere are a few challenges we ran into:\n- lack of dataset availability\n- different time-zones\n- mix between first time hacker, new hacker(s), and experienced hacker in the team\n- aws s3 - simple storage service\n- storage of large data\n- aws sagemaker\n- computational power - deep learning takes time\naccomplishments that we're proud of\n- being able to tackle and develop the machine learning models with the supportive team we had.\n- creating a successful clean and polished look to the design\n- models with over 80% accuracy across the board\n- figuring out how to implement flask\n- experimenting with aws (s3, and sagemaker (not as successful))\nwhat we learned\n- together as a team, we learnt how to use and apply css in an efficient way and how different css tools helped to achieve certain looks we were aiming for.\n- we also learned how to use flask to connect ml models to our web application.\n- further, we learned how to use aws (s3, and sagemaker (not as successful)).\nwhat's next for doctorize.ai\n- allow patients and doctors to interact smoothly on the platform\n- expand our collection of medical cases that can be scanned and recognized such as more types of bacteria/viruses and rashes\n- bring in new helpful features such as advanced search of specialists and general doctors in the area of your own choice\n- record the patient\u2019s history and information for future references\n- qr codes on patient\u2019s profile for smoother connectivity\n- voice memo ai to summarize what the patient is talking about into targeted key topics", "sortedWord": "None", "removed": "Nan", "score": 6, "comments": 0, "media": null, "medialink": null, "identifyer": 59502283}, {"Unnamed: 0": 2285, "autor": "Gatherer", "date": null, "content": "Inspiration:\nThe inspiration for this product came from finding it difficult to quickly find emails for different companies. This started with another project some of us made to get free merch from different companies by emailing them. It took a lot of time to gather these emails, so we wished there was an affordable tool we could use. Some of us have also had jobs where we look for leads for a company, and this involves a lot of searching online for emails.\nWhat it does:\nOur website makes it easy to find emails for different companies and organizations. Simply input the company's domain name (e.g. google -> google.com) and our website finds all the email addresses associated with that domain from across the web.\nHow we built it:\nThis project can be broken down into three components, a web-scraper, a database and server less backend, and a statically hosted website.\nThe foundation for the web-scraper was the scrapy python library. This allowed us to scrape with super high concurrency and some other nice features. The scraper had ~20 target domains where we scraped most pages on the website and parsed out emails. This ran on our team's local server for the duration of the hackathon.\nThese emails (as well as some other information) was then stored in a MongoDB Atlas database. MongoDB was chosen because it's NoSQL and we didn't have time to rewrite schemas. Another reason was because it had a simple cloud hosting service, so we didn't need to configure a cloud hosting provider.\nThe front-end was initially planned out using Figma (which we learned at the hackathon workshop). We then implemented our designs using Bootstrap. The static web pages were hosted on Netlify, because it avoids unnecessary complexity.\nThe front-end interfaces with the database via Netlify functions, which are a easier way to use AWS Lambda server less endpoints. We created some endpoints in Node.js that we could use to query the database. In our functions, we also validate some emails.\nChallenges we ran into:\nWe got stuck in a few places when it came to the website. We thought that Figma was a tool to make websites, and we would be able to export the project when we were done. We went to the Figma workshop, and spent a lot of time using Figma when that time probably could have been better spent actually coding the website. CSS is also a tricky thing to wrap your head around when you are not used to it.\nAccomplishments that we're proud of:\nWe're proud that we were able to complete this project on time, and have an end product that is semi-polished. We're also proud that we created a plan that stripped away unnecessary complications, and we stuck with it. Also that our product works!\nNo only are we happy with the turnout of our technical project, but out marketing content as well. Our video is pretty eye-catching, which is something we should be proud of!\nWhat we learned:\nOur team had two members that didn't have much experience with programming, but wanted to learn more. They were able to learn some of the basics of designing web pages. They also picked up some general knowledge on other languages.\nThe more technically experienced team members got some new experience working with serverless and NoSQL databases. Also, we learned more about how to market an idea.\nWhat's next for Gatherer:\nGatherer is a basic product now with a huge amount of potential to grow. Whether we end up targeting business users, with features to help generate effective leads, or we target end users that just need a tool to find some emails for a project. Helping users access all sorts of data in one place is something our team could envision. Besides emails, our team could scrape phone numbers, social media profiles, and much more.\nWe think this product has lots of potential to become monetizable in some way.", "link": "https://devpost.com/software/gatherer", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration:\nthe inspiration for this product came from finding it difficult to quickly find emails for different companies. this started with another project some of us made to get free merch from different companies by emailing them. it took a lot of time to gather these emails, so we wished there was an affordable -----> tool !!!  we could use. some of us have also had jobs where we look for leads for a company, and this involves a lot of searching online for emails.\nwhat it does:\nour website makes it easy to find emails for different companies and organizations. simply input the company's domain name (e.g. google -> google.com) and our website finds all the email addresses associated with that domain from across the web.\nhow we built it:\nthis project can be broken down into three components, a web-scraper, a database and server less backend, and a statically hosted website.\nthe foundation for the web-scraper was the scrapy python library. this allowed us to scrape with super high concurrency and some other nice features. the scraper had ~20 target domains where we scraped most pages on the website and parsed out emails. this ran on our team's local server for the duration of the hackathon.\nthese emails (as well as some other information) was then stored in a mongodb atlas database. mongodb was chosen because it's nosql and we didn't have time to rewrite schemas. another reason was because it had a simple cloud hosting service, so we didn't need to configure a cloud hosting provider.\nthe front-end was initially planned out using figma (which we learned at the hackathon workshop). we then implemented our designs using bootstrap. the static web pages were hosted on netlify, because it avoids unnecessary complexity.\nthe front-end interfaces with the database via netlify functions, which are a easier way to use aws lambda server less endpoints. we created some endpoints in node.js that we could use to query the database. in our functions, we also validate some emails.\nchallenges we ran into:\nwe got stuck in a few places when it came to the website. we thought that figma was a tool to make websites, and we would be able to export the project when we were done. we went to the figma workshop, and spent a lot of time using figma when that time probably could have been better spent actually coding the website. css is also a tricky thing to wrap your head around when you are not used to it.\naccomplishments that we're proud of:\nwe're proud that we were able to complete this project on time, and have an end product that is semi-polished. we're also proud that we created a plan that stripped away unnecessary complications, and we stuck with it. also that our product works!\nno only are we happy with the turnout of our technical project, but out marketing content as well. our video is pretty eye-catching, which is something we should be proud of!\nwhat we learned:\nour team had two members that didn't have much experience with programming, but wanted to learn more. they were able to learn some of the basics of designing web pages. they also picked up some general knowledge on other languages.\nthe more technically experienced team members got some new experience working with serverless and nosql databases. also, we learned more about how to market an idea.\nwhat's next for gatherer:\ngatherer is a basic product now with a huge amount of potential to grow. whether we end up targeting business users, with features to help generate effective leads, or we target end users that just need a tool to find some emails for a project. helping users access all sorts of data in one place is something our team could envision. besides emails, our team could scrape phone numbers, social media profiles, and much more.\nwe think this product has lots of potential to become monetizable in some way.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59502285}, {"Unnamed: 0": 2286, "autor": "beNFT", "date": null, "content": "Inspiration\nOur team has been involved with social endeavors before, so we brought together our drive for social change and our drive to develop for Celo's blockchain to create beNFT, a platform that connects NGOs and philanthropic organizations with people that want to contribute.\nWhat it does\nIn beNFT, contributors can buy NFTs on our marketplace from organizations' fundraising campaigns. In turn, organizations can hire our NFT production service, through which we'll design a complete and personalized NFT collection for their campaign. On top of that, organizations will receive profit over future resells, making the collections interesting as a means of investment. The organizations have a dedicated dashboard to keep up with their campaigns and funds raised. For those looking to contribute, the NFT sales happen in a quick and uncomplicated fashion through Celo's blockchain.\nHow we built it\nRaised our certainties, suppositions and doubts about the challenge.\nConducted desk research (using Notion as our organization tool)\nListed the pains we wanted to solve for ONGs and philanthropic organizations\nDefined our features\nStructured our use cases\nConstructed the high fidelity prototype (in Figma)\nPlanned the business model\nImplemented the app (front-end + Celo integration)\nChallenges we ran into\nUnderstanding some blockchain concepts\nFirst time developing a mobile-first web app\nAccomplishments that we're proud of\nConnecting an important theme to the team (social change) to blockchain and NFTs\nIdeating an entire product in a short time\nComplete Celo integration\nWhat we learned\nA lot about blockchain, crypto and NFTs\nCelo's purpose as a company/blockchain\nHow to integrate our app with Celo's services\nWhat's next for beNFT\nPartnerships with NGOs and philanthropic organizations to bring them to the platform\nMKT campaigns to bring contributors and NFT collectors to the platform\nContinue dApp implementation\nImplement the web dashboard for organizations\nUsability and UX testing\nImplement direct purchase (credit card, PIX, Apple Pay)", "link": "https://devpost.com/software/benft", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nour team has been involved with social endeavors before, so we brought together our drive for social change and our drive to develop for celo's blockchain to create benft, a platform that connects ngos and philanthropic organizations with people that want to contribute.\nwhat it does\nin benft, contributors can buy nfts on our marketplace from organizations' fundraising campaigns. in turn, organizations can hire our nft production service, through which we'll design a complete and personalized nft collection for their campaign. on top of that, organizations will receive profit over future resells, making the collections interesting as a means of investment. the organizations have a dedicated dashboard to keep up with their campaigns and funds raised. for those looking to contribute, the nft sales happen in a quick and uncomplicated fashion through celo's blockchain.\nhow we built it\nraised our certainties, suppositions and doubts about the challenge.\nconducted desk research (using notion as our organization -----> tool !!! )\nlisted the pains we wanted to solve for ongs and philanthropic organizations\ndefined our features\nstructured our use cases\nconstructed the high fidelity prototype (in figma)\nplanned the business model\nimplemented the app (front-end + celo integration)\nchallenges we ran into\nunderstanding some blockchain concepts\nfirst time developing a mobile-first web app\naccomplishments that we're proud of\nconnecting an important theme to the team (social change) to blockchain and nfts\nideating an entire product in a short time\ncomplete celo integration\nwhat we learned\na lot about blockchain, crypto and nfts\ncelo's purpose as a company/blockchain\nhow to integrate our app with celo's services\nwhat's next for benft\npartnerships with ngos and philanthropic organizations to bring them to the platform\nmkt campaigns to bring contributors and nft collectors to the platform\ncontinue dapp implementation\nimplement the web dashboard for organizations\nusability and ux testing\nimplement direct purchase (credit card, pix, apple pay)", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59502286}, {"Unnamed: 0": 2308, "autor": "Communi-Gesture", "date": null, "content": "Inspiration\n\u201cAbled does not mean enabled. Disabled does not mean less abled.\u201d \u2015 Khang Kijarro Nguyen\nToday, around 15 per cent of the world\u2019s population, or estimated 1 billion people, live with disabilities. They are the world\u2019s largest minority. For deaf people, however, the major barriers are Lack of recognition, Acceptance, and use of sign language in all areas of life, and Lack of respect for their cultural and linguistic identity. Acceptance too sign language is one of the major ways how we can better communicate and understand them. Moreover, around 1 in 5 of the world's children and adolescents have a mental disorder. Disabled people have reported having frequent mental distress as well. we need to have an inclusive and safe space to solve this together!\nOur Goal\nTo promote awareness about disabled individuals through interactive techniques and to disseminate information about them through various interactive channels more about sign language communication and help everyone effectively communicate in sign language! Moreover, helping people with visual and mental disabilities as well!\nWhat it does\nIt's a portal where we've various features to help create a safe space for disabled people!\nKey Features\nDiscussion Room: Using discussion rooms usually entails mustering the confidence to tell your tale. People with sensitive stories or who are socially anxious frequently avoid employing this kind of catharsis. Here they can either be public or anonymously share their views on some specific topic or create some new threads and share their heart out! It's an open space to discuss things freely. Users can create threads and continue their discussion either openly or anonymously.\nSolace Zone: We've added a personal diary feature, where the users can add in their notes, day-to-day activities. We also have added a calm room, a place where user can log into and listen to mind soothing beats of the ocean. Because at times all we need is a place to rest, a place to find solace!\nASL Translator (speech and text to asl) Our application has a speech to sign language translation tool where a user can simply speak, and the message will be translated to sign language. It also has a text to ASL translation support, users can simply write/paste their message and the message is translated to ASL.\nColorblind extension Its goal is to help visually impaired people see text easily. User can directly activate the extension and select various of the options to get started!\nHow we built it\nWe built it using Angular and Firebase.\nChallenges we ran into\nImplementing all the features in the given time constraint was one of the major challenges for us! Managing our time, the right way helped us a lot in successfully building the product. It was an amazing learning experience for us overall!\nAccomplishments that we're proud of\nWe are proud of overcoming our challenges and building the application.\nWhat we learned\nA lot and lot of things, it was such a fun experience! It was amazing to see it working finally!\nWhat's next for Communi-Gesture\nAdding more features for so everyone can better connect, adding more support for accessibility, and so much more!\nSo let's Communi-Gesture!", "link": "https://devpost.com/software/communi-gesture-gbepsc", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\n\u201cabled does not mean enabled. disabled does not mean less abled.\u201d \u2015 khang kijarro nguyen\ntoday, around 15 per cent of the world\u2019s population, or estimated 1 billion people, live with disabilities. they are the world\u2019s largest minority. for deaf people, however, the major barriers are lack of recognition, acceptance, and use of sign language in all areas of life, and lack of respect for their cultural and linguistic identity. acceptance too sign language is one of the major ways how we can better communicate and understand them. moreover, around 1 in 5 of the world's children and adolescents have a mental disorder. disabled people have reported having frequent mental distress as well. we need to have an inclusive and safe space to solve this together!\nour goal\nto promote awareness about disabled individuals through interactive techniques and to disseminate information about them through various interactive channels more about sign language communication and help everyone effectively communicate in sign language! moreover, helping people with visual and mental disabilities as well!\nwhat it does\nit's a portal where we've various features to help create a safe space for disabled people!\nkey features\ndiscussion room: using discussion rooms usually entails mustering the confidence to tell your tale. people with sensitive stories or who are socially anxious frequently avoid employing this kind of catharsis. here they can either be public or anonymously share their views on some specific topic or create some new threads and share their heart out! it's an open space to discuss things freely. users can create threads and continue their discussion either openly or anonymously.\nsolace zone: we've added a personal diary feature, where the users can add in their notes, day-to-day activities. we also have added a calm room, a place where user can log into and listen to mind soothing beats of the ocean. because at times all we need is a place to rest, a place to find solace!\nasl translator (speech and text to asl) our application has a speech to sign language translation -----> tool !!!  where a user can simply speak, and the message will be translated to sign language. it also has a text to asl translation support, users can simply write/paste their message and the message is translated to asl.\ncolorblind extension its goal is to help visually impaired people see text easily. user can directly activate the extension and select various of the options to get started!\nhow we built it\nwe built it using angular and firebase.\nchallenges we ran into\nimplementing all the features in the given time constraint was one of the major challenges for us! managing our time, the right way helped us a lot in successfully building the product. it was an amazing learning experience for us overall!\naccomplishments that we're proud of\nwe are proud of overcoming our challenges and building the application.\nwhat we learned\na lot and lot of things, it was such a fun experience! it was amazing to see it working finally!\nwhat's next for communi-gesture\nadding more features for so everyone can better connect, adding more support for accessibility, and so much more!\nso let's communi-gesture!", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59502308}, {"Unnamed: 0": 2322, "autor": "Skrip", "date": null, "content": "Inspiration\nIn Indonesia, creating thesis is the most important prerequisite to graduate from college. This type of thesis is commonly termed as Skripsi. The term Skripsi is used in Indonesia to describe a scientific effort that addresses an issue in a specific field of study. Writing a \"Skripsi\" is the biggest challenge undergraduate students in Indonesia have to confront, starting from finding ideas, not knowing the proper writing format, find it difficult to gather references, or even arranging a timetable for consultation with the lecturers and many more.\nThis project is inspired by the difficulties faced by all Indonesian college students we've mentioned earlier. When a student has reach the final year of college, hearing the term \"Skripsi\" really can drive them crazy. It is not unusual to see this issue becomes a big obstacle to graduate from college on time. Therefore, we intended to make an app that can assist them throughout their journey.\nWhat it does\nIndividuals who are having trouble writing their thesis can use our mobile app for assistance. Skrip has unique characteristics that combine education and business, including: a. Mentoring service Where students can find mentors according to the field of science from which the thesis will be prepared b. Chat and discussion service Users can have conversation with other users to discuss anything through this service c. Article search service Looking for related articles as a reference to complete the thesis d. Reminder service Set to remind on important activities e. Auto citation service User can find out how to write the correct citation.\nHow we built it\nWe were using a no-code programming tool to create this app.\nChallenges we ran into\nApparently, this is our first hackthon that we compete for, indeed there were various challenges we faced. However, with enthusiasm and good teamwork, we can get through it all. The first challenge that we faced was determining what application we will make, how we can relate the existing problems to the solutions that will be provided. Seeing the problems that are often faced by students in compiling their final assignments (thesis), we want to create an application that can help them to overcome this, with the aim that they can complete their education without feeling depressed.\nThe second challenge was that we are having difficulty determining what application we will use in prototyping. Since we are not very good at coding, we decided to use a no-code programming application. But what application is suitable and easy to use?. finally we found the right application, named figma.\nThe third challenge appeared in the process of making the prototype. How is the application logo that we made?, what features are contained in the application?, and what kind of design is in accordance with user interface standards?. We were having many questions that have to be addressed.\nIn making the logo, we were inspired by the \"Toga\" hat which is a symbol of students who have completed their bachelor's degree. In creating the features, we decided to keep it simple, yet very useful. And in making the design, we tried to determine the fonts, colors, elements, and content that are suitable for the purpose of the application. So that users not only feel helped by the application that we made, but are also comfortable in using it.\nAccomplishments that we're proud of\nAlthough this competition is just a beginning for our team, yet we feel proud of ourselves, because it succeeded in making an application prototype that is useful for many people. Even though we've gone through many challenges, we still made it at the end.\nWhat we learned\nThe valuable thing that we get from this hackathon is not only the experience of making an application prototype that we hope could help many college students, but also the good time we've spent together as a team where we can build teamwork. In teamwork, it required responsibility and team unity. Differences will always exist, but we learned to overcome them by respecting and helping each other every time we were facing difficulties. Each of us has a different role, it's just a matter of how we are responsible for carrying out that role to achieve our goals.\nWhat's next for Skrip\nAlthough the prototype we made is far from perfect, we really hope that in the future, our work can be realized. Indeed, we want all college students not to think of the final project (thesis) as a nightmare anymore. They can use our application as a guidance to complete their Education. We also hope that, with this application, we can reduce the number of unemployed in Indonesia by creating job opportunities for mentors . If our application is successfully realized, we will endeavor to develop this application further.", "link": "https://devpost.com/software/skrip", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nin indonesia, creating thesis is the most important prerequisite to graduate from college. this type of thesis is commonly termed as skripsi. the term skripsi is used in indonesia to describe a scientific effort that addresses an issue in a specific field of study. writing a \"skripsi\" is the biggest challenge undergraduate students in indonesia have to confront, starting from finding ideas, not knowing the proper writing format, find it difficult to gather references, or even arranging a timetable for consultation with the lecturers and many more.\nthis project is inspired by the difficulties faced by all indonesian college students we've mentioned earlier. when a student has reach the final year of college, hearing the term \"skripsi\" really can drive them crazy. it is not unusual to see this issue becomes a big obstacle to graduate from college on time. therefore, we intended to make an app that can assist them throughout their journey.\nwhat it does\nindividuals who are having trouble writing their thesis can use our mobile app for assistance. skrip has unique characteristics that combine education and business, including: a. mentoring service where students can find mentors according to the field of science from which the thesis will be prepared b. chat and discussion service users can have conversation with other users to discuss anything through this service c. article search service looking for related articles as a reference to complete the thesis d. reminder service set to remind on important activities e. auto citation service user can find out how to write the correct citation.\nhow we built it\nwe were using a no-code programming -----> tool !!!  to create this app.\nchallenges we ran into\napparently, this is our first hackthon that we compete for, indeed there were various challenges we faced. however, with enthusiasm and good teamwork, we can get through it all. the first challenge that we faced was determining what application we will make, how we can relate the existing problems to the solutions that will be provided. seeing the problems that are often faced by students in compiling their final assignments (thesis), we want to create an application that can help them to overcome this, with the aim that they can complete their education without feeling depressed.\nthe second challenge was that we are having difficulty determining what application we will use in prototyping. since we are not very good at coding, we decided to use a no-code programming application. but what application is suitable and easy to use?. finally we found the right application, named figma.\nthe third challenge appeared in the process of making the prototype. how is the application logo that we made?, what features are contained in the application?, and what kind of design is in accordance with user interface standards?. we were having many questions that have to be addressed.\nin making the logo, we were inspired by the \"toga\" hat which is a symbol of students who have completed their bachelor's degree. in creating the features, we decided to keep it simple, yet very useful. and in making the design, we tried to determine the fonts, colors, elements, and content that are suitable for the purpose of the application. so that users not only feel helped by the application that we made, but are also comfortable in using it.\naccomplishments that we're proud of\nalthough this competition is just a beginning for our team, yet we feel proud of ourselves, because it succeeded in making an application prototype that is useful for many people. even though we've gone through many challenges, we still made it at the end.\nwhat we learned\nthe valuable thing that we get from this hackathon is not only the experience of making an application prototype that we hope could help many college students, but also the good time we've spent together as a team where we can build teamwork. in teamwork, it required responsibility and team unity. differences will always exist, but we learned to overcome them by respecting and helping each other every time we were facing difficulties. each of us has a different role, it's just a matter of how we are responsible for carrying out that role to achieve our goals.\nwhat's next for skrip\nalthough the prototype we made is far from perfect, we really hope that in the future, our work can be realized. indeed, we want all college students not to think of the final project (thesis) as a nightmare anymore. they can use our application as a guidance to complete their education. we also hope that, with this application, we can reduce the number of unemployed in indonesia by creating job opportunities for mentors . if our application is successfully realized, we will endeavor to develop this application further.", "sortedWord": "None", "removed": "Nan", "score": 6, "comments": 0, "media": null, "medialink": null, "identifyer": 59502322}, {"Unnamed: 0": 2341, "autor": "Parenthesis Validator", "date": null, "content": "Inspiration\nWe all know what it's like to see our code not working just because we forgot to close a parenthesis. Besides, if that's a huge project, or an assignment to submit for our exam, facing the problem of not having balanced parenthesis and proper nesting creates a lot of difficulties. Moreover, reading through all the codes to find the missing parenthesis is an exhausting process. This problem can arrive for other educational purposes too. Thus, we, Three Flaskateers, came up with this project to solve this issue effectively.\nWhat it does\nParenthesis Validator is a web application that checks the validity of balanced parenthesis for all types of text files. First, it asks the user to either upload a file or a string. When they choose to upload a file, we'll redirect them to a new page where they can upload one file at a time. Then, we'll return the file name and if it's balanced or not. If it's balanced, we'll return True; if not, we'll return false and will give the index error. If the user uploads a not supported text file, we'll ask them to upload a different file. If the user chooses to upload a string, we'll redirect them to another new page. The user can type out their string and press 'check'. Then, we'll return the results similar to how we did it on the text file page, validating their parenthesis usage.\nHow we built it\nWe built the web application using Flask, a web framework tool using Python. The algorithm and scripts to run back-end functions were made with python. We utilized sqlalchemy, a database tool for python to store data from the app. Finally, we made it aesthetically pleasing with HTML and CSS!\nChallenges we ran into\nLearning flask was an incredibly challenging part. Setting up the environment and working with the database proved tricky when trying to access data. Also, linking from several different .html to the .py files also proved difficult to track. We had some troubles using git as well when contributing to the same files. We decided to branch out and allow one member to focus on the front end while still working on the back end which smoothed things out.\nAccomplishments that we're proud of\nWe are proud of each other for working diligently and with a good attitude all the time despite being from different countries(and time zones) and just recently meeting each other on Friday. We are also proud of our mostly functional application as it satisfied almost all the goals we initially set out for it to do.\nWhat we learned\nI think we all improved our git skills and collaboration skills mainly. We had to figure out how to plan with each other about our varying schedules and backgrounds to make an effective end product. We also personally learned some new skills through the use of flask, python and front end respectively.\nWhat's next for Parenthesis Validator\nFuture Idea: Take in multiple files to check at once\nCurrently, the program is limited to a single file at a time. If we wanted to read multiple files we would have used a variable arguments method, so the user is not just limited to a single file, but the user can upload as many files as he/she wants. The updated algorithm will scan each file for errors and print the results. For files, our algorithm opens the files in Read Mode and then create a stack, and starts reading one character at a time from the input file until the EOF is encountered, and what it does is: it push uses the same string-based algorithm but on each character of the file, maintains the stack the same way, and print the results.", "link": "https://devpost.com/software/parenthesis-validator", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe all know what it's like to see our code not working just because we forgot to close a parenthesis. besides, if that's a huge project, or an assignment to submit for our exam, facing the problem of not having balanced parenthesis and proper nesting creates a lot of difficulties. moreover, reading through all the codes to find the missing parenthesis is an exhausting process. this problem can arrive for other educational purposes too. thus, we, three flaskateers, came up with this project to solve this issue effectively.\nwhat it does\nparenthesis validator is a web application that checks the validity of balanced parenthesis for all types of text files. first, it asks the user to either upload a file or a string. when they choose to upload a file, we'll redirect them to a new page where they can upload one file at a time. then, we'll return the file name and if it's balanced or not. if it's balanced, we'll return true; if not, we'll return false and will give the index error. if the user uploads a not supported text file, we'll ask them to upload a different file. if the user chooses to upload a string, we'll redirect them to another new page. the user can type out their string and press 'check'. then, we'll return the results similar to how we did it on the text file page, validating their parenthesis usage.\nhow we built it\nwe built the web application using flask, a web framework -----> tool !!!  using python. the algorithm and scripts to run back-end functions were made with python. we utilized sqlalchemy, a database tool for python to store data from the app. finally, we made it aesthetically pleasing with html and css!\nchallenges we ran into\nlearning flask was an incredibly challenging part. setting up the environment and working with the database proved tricky when trying to access data. also, linking from several different .html to the .py files also proved difficult to track. we had some troubles using git as well when contributing to the same files. we decided to branch out and allow one member to focus on the front end while still working on the back end which smoothed things out.\naccomplishments that we're proud of\nwe are proud of each other for working diligently and with a good attitude all the time despite being from different countries(and time zones) and just recently meeting each other on friday. we are also proud of our mostly functional application as it satisfied almost all the goals we initially set out for it to do.\nwhat we learned\ni think we all improved our git skills and collaboration skills mainly. we had to figure out how to plan with each other about our varying schedules and backgrounds to make an effective end product. we also personally learned some new skills through the use of flask, python and front end respectively.\nwhat's next for parenthesis validator\nfuture idea: take in multiple files to check at once\ncurrently, the program is limited to a single file at a time. if we wanted to read multiple files we would have used a variable arguments method, so the user is not just limited to a single file, but the user can upload as many files as he/she wants. the updated algorithm will scan each file for errors and print the results. for files, our algorithm opens the files in read mode and then create a stack, and starts reading one character at a time from the input file until the eof is encountered, and what it does is: it push uses the same string-based algorithm but on each character of the file, maintains the stack the same way, and print the results.", "sortedWord": "None", "removed": "Nan", "score": 5, "comments": 0, "media": null, "medialink": null, "identifyer": 59502341}, {"Unnamed: 0": 2343, "autor": "Fruitful", "date": null, "content": "Inspiration\nWe were inspired to make Fruitful based on our own experience. We found that while we might know that it is important to recycle, we still lack a lot of awareness on how we can be more efficient. We also notice that while education is also important, many people lack the motivation to maintain sustainable habits. To counter both the issue of education and laziness, we created Fruitful to fulfill both aspects. Fruitful promotes knowledge through communities and having daily tips. The app also includes weekly challenges, something to track the number of goals people have completed, and have some friendly fun with others through leaderboards. We personally love to compete, so we thought it would be a good idea to include this in the app as other people may feel the same.\nWhat it does\nUpon opening Fruitful, users will be prompted to either sign in or sign up. If this is their first time making an account, they will be given a brief survey asking them what they are most interested in changing in their lives. These are all sustainable related, and will be taken into account when designing specific goals for users to complete. Users will be able to track the number of goals they have completed day-by-day, allowing them to view their progress. If it is the first time they have opened the app, then they will be shown a daily tip that they can return to from the homepage at any given time. They will also be given a weekly challenge to complete so that they can add something new to their lives. Users will also have a map localized to them to include any nearby events, recycling centers, or clothing donation centers. These pages will have details on them and the opportunity for users to bookmark them for future use. For users that have friends added, they will also be able to see if their friends have declared that they are going to any events. There will also be community and friend activity pages to further promote social awareness. Community pages will have some highlights with the most viewed or liked pages, and threads offer people the opportunity to socialize, ask questions, and seek advice. Friend activity pages offer a smaller, more trusted community, as users can openly ask each other if they want to hang out, come over and take any leftovers, or post any pictures related to sustainability.\nHow we built it\nWe built Fruitful with Figma. We first discussed what interfaces to include based on what we thought would be most helpful. We each then took a stab at creating our own black and white sketch of what we imagined the app to look like. We used both of our sketches to combine certain ideas that we thought would best suit the app. We also agreed on a color palette to keep the app consistent and stylized. The name and logo is something we naturally came to agree on as we kept bouncing ideas off of each other. The logo itself was designed by Jan through Adobe Illustrator. We also used free svg icons from Google Fonts.\nChallenges we ran into\nWe didn't discuss too much about our challenges, as we spent most of the time independently working out with Figma. The both of us either relied on figuring things out ourselves, or using YouTube as reference to further understand Figma.\nAccomplishments that we're proud of\nSince this is our first hackathon and our first time using Figma, we're both really proud that we were able to finish our project. We were both worried before the hackathon even begun because we had a stressful week of school before the weekend, so we weren't too sure how ready we were to tackle the hackathon. However, we seemed to be really compatible as we were able to quickly come up with ideas for the app on the first night. We also didn't find Figma to be too challenging, and we are super grateful that we were able to learn such an amazing tool through this project.\nWhat we learned\nSo much! We learned how to brainstorm ideas, use Figma, and how to design apps to be pleasing and fitting.\nWhat's next for Fruitful\nWe hope that we can implement features and code Fruitful so that it can actually be a useable app!", "link": "https://devpost.com/software/fruitful", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe were inspired to make fruitful based on our own experience. we found that while we might know that it is important to recycle, we still lack a lot of awareness on how we can be more efficient. we also notice that while education is also important, many people lack the motivation to maintain sustainable habits. to counter both the issue of education and laziness, we created fruitful to fulfill both aspects. fruitful promotes knowledge through communities and having daily tips. the app also includes weekly challenges, something to track the number of goals people have completed, and have some friendly fun with others through leaderboards. we personally love to compete, so we thought it would be a good idea to include this in the app as other people may feel the same.\nwhat it does\nupon opening fruitful, users will be prompted to either sign in or sign up. if this is their first time making an account, they will be given a brief survey asking them what they are most interested in changing in their lives. these are all sustainable related, and will be taken into account when designing specific goals for users to complete. users will be able to track the number of goals they have completed day-by-day, allowing them to view their progress. if it is the first time they have opened the app, then they will be shown a daily tip that they can return to from the homepage at any given time. they will also be given a weekly challenge to complete so that they can add something new to their lives. users will also have a map localized to them to include any nearby events, recycling centers, or clothing donation centers. these pages will have details on them and the opportunity for users to bookmark them for future use. for users that have friends added, they will also be able to see if their friends have declared that they are going to any events. there will also be community and friend activity pages to further promote social awareness. community pages will have some highlights with the most viewed or liked pages, and threads offer people the opportunity to socialize, ask questions, and seek advice. friend activity pages offer a smaller, more trusted community, as users can openly ask each other if they want to hang out, come over and take any leftovers, or post any pictures related to sustainability.\nhow we built it\nwe built fruitful with figma. we first discussed what interfaces to include based on what we thought would be most helpful. we each then took a stab at creating our own black and white sketch of what we imagined the app to look like. we used both of our sketches to combine certain ideas that we thought would best suit the app. we also agreed on a color palette to keep the app consistent and stylized. the name and logo is something we naturally came to agree on as we kept bouncing ideas off of each other. the logo itself was designed by jan through adobe illustrator. we also used free svg icons from google fonts.\nchallenges we ran into\nwe didn't discuss too much about our challenges, as we spent most of the time independently working out with figma. the both of us either relied on figuring things out ourselves, or using youtube as reference to further understand figma.\naccomplishments that we're proud of\nsince this is our first hackathon and our first time using figma, we're both really proud that we were able to finish our project. we were both worried before the hackathon even begun because we had a stressful week of school before the weekend, so we weren't too sure how ready we were to tackle the hackathon. however, we seemed to be really compatible as we were able to quickly come up with ideas for the app on the first night. we also didn't find figma to be too challenging, and we are super grateful that we were able to learn such an amazing -----> tool !!!  through this project.\nwhat we learned\nso much! we learned how to brainstorm ideas, use figma, and how to design apps to be pleasing and fitting.\nwhat's next for fruitful\nwe hope that we can implement features and code fruitful so that it can actually be a useable app!", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59502343}, {"Unnamed: 0": 2348, "autor": "GIVE-A", "date": null, "content": "Inspiration\nWe observe there\u2019s a lot of people that have come to this state where they have so much stuff or have no space left and want to give it away to people who need it for free. We as enablers see an opportunity to make a listing site so people could explore around and connect those who want to give their stuff and those who want free stuff. Our name was \u201cGIVE-A\u201d or naturalized to \u201cGIVA\u201d do to the Indonesia accent, our name have two definition which is \u201cGiveaway\u201d, and \u201cGive Anywhere\u201d\nWhat it does\nGIVE-A is a listing site that connects people who want to give their free stuff such as shirts, dresses, caps, or even television, etc. We\u2019re as enablers achieve this goal by making a website that listing a free stuff that contain information about those who willing to give it for free\nHow we built it\nFirst of all we research the problem which is how or where people give their free stuff, we\u2019ve notice those people who doesn't have relation around their neighbour getting hard to get rid away their stuff or just because their stuff are to much. We\u2019re implement tech stack such as nextjs, chakra-ui, mongoDB, vercel, and auth0 to handle our website mechanisme also using github as collaborative tool and discord as our communication tool\nChallenges we ran into\nOur team was constructed with 3 fullstack engineers and 1 ui/ux designer, first problem that we ran into was we doesn't have product guy that his job was to brainstorm and come of with idea so we talk a lot with our mentor to discuss our idea, till we come to this idea about making listing site for free stuff. Second, our team joined a hackathon for the first time so we have little knowledge about the workflow as well as what things to do at first. Then the third we\u2019re thigh scheduled on weekend so we\u2019re not having full of our time\nWhat we learned\nWe learn new tech such as auth0 and we also learn how to do public speaking for the pitching, and we learn from mistake we\u2019ve make hopefully our next hackathon will be better with the knowledge we have right now\nWhat's next for GIVE-A\nWe will polish the UI/UX so the interaction with user will be a lot smoother\nWe will seed the proofread data to our database\nFix some bugs found\nAdding features", "link": "https://devpost.com/software/give-a", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe observe there\u2019s a lot of people that have come to this state where they have so much stuff or have no space left and want to give it away to people who need it for free. we as enablers see an opportunity to make a listing site so people could explore around and connect those who want to give their stuff and those who want free stuff. our name was \u201cgive-a\u201d or naturalized to \u201cgiva\u201d do to the indonesia accent, our name have two definition which is \u201cgiveaway\u201d, and \u201cgive anywhere\u201d\nwhat it does\ngive-a is a listing site that connects people who want to give their free stuff such as shirts, dresses, caps, or even television, etc. we\u2019re as enablers achieve this goal by making a website that listing a free stuff that contain information about those who willing to give it for free\nhow we built it\nfirst of all we research the problem which is how or where people give their free stuff, we\u2019ve notice those people who doesn't have relation around their neighbour getting hard to get rid away their stuff or just because their stuff are to much. we\u2019re implement tech stack such as nextjs, chakra-ui, mongodb, vercel, and auth0 to handle our website mechanisme also using github as collaborative -----> tool !!!  and discord as our communication -----> tool !!! \nchallenges we ran into\nour team was constructed with 3 fullstack engineers and 1 ui/ux designer, first problem that we ran into was we doesn't have product guy that his job was to brainstorm and come of with idea so we talk a lot with our mentor to discuss our idea, till we come to this idea about making listing site for free stuff. second, our team joined a hackathon for the first time so we have little knowledge about the workflow as well as what things to do at first. then the third we\u2019re thigh scheduled on weekend so we\u2019re not having full of our time\nwhat we learned\nwe learn new tech such as auth0 and we also learn how to do public speaking for the pitching, and we learn from mistake we\u2019ve make hopefully our next hackathon will be better with the knowledge we have right now\nwhat's next for give-a\nwe will polish the ui/ux so the interaction with user will be a lot smoother\nwe will seed the proofread data to our database\nfix some bugs found\nadding features", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59502348}, {"Unnamed: 0": 2349, "autor": "Sustainability Express", "date": null, "content": "Inspiration\nThere are three pressing problems in the freight industry that result from the over usage of trucks as the method of transportation.\nFirstly, there are severe environmental impacts that demonstrate the need for a change to reduce our carbon footprint. Global water levels, average temperature and average CO2 levels are all projected to rise in the coming decades, so we need to do our part to in reducing these alarming statistics before it is too late. Reducing the amount of transport trucks on the road, which are much less fuel efficient than rail, will help reduce global emission rates, hopefully putting us in the right direction towards a more sustainable future.\nSecondly, these environmental impacts, paired with the time and cost inefficiencies for trucking for long distances, show that there is clearly need for changes towards a more sustainable future in the freight industry.\nAs mentioned earlier, transport trucks are less fuel efficient than rail. They are also much more limited in their freight capacity. Additional investments into the railway industry will create not only a more environmentally friendly transportation industry, but also could lead to a more efficient supply chain for these businesses.\nLastly, the 400 series highways of Ontario grow more congested everyday due to increased demands for products, and building new highways/ expanding the old ones will simple lead to induced congestion (https://environmentaldefence.ca/2020/09/15/why-building-more-highways-wont-make-your-commute-any-better/).\nThat's where rail comes in. By increasing the amount of freight shipped on railways, we should see a proportional decrease in the amount of transport trucks on the 400 series highways, leading to reduced congestion in Ontario's most densely populated areas.\nThe magnitudes of these multi layered issues is why we decided to tackle this problem. We hoped that a change could be made by a applying an already existing model to help create a sustainable future for the industry of freight.\nWhat it does\nIt's an application of an optimization model that implements an \"ant-colony\"-esque solution to the traveling salesman problem. We in turn further modified this model to allow it to take in coordinates for specific locations, allowing the model to return the near-optimal path, which would be used as a guiding tool for future railway construction.\nChallenges we ran into\nUnderstanding how different parts of the optimization algorithm were implemented were definitely hard. A lot of us didn't know much ML coming into the hackathon so trying to understand the developed algorithm as well as simultaneously learning ML was a massive hurdle.\nAdditionally, the model itself wasn't the ideal algorithm to solve this particular solution. While it does create a near optimal straight line path for the railways, it struggles to account for barriers such as water/ towns that aren't inputted into its coordinates.\nThe model also doesn't allow for backtracking, which in this case means that we wouldn't be able to find an optimal railway system that allows for trains to run in multiple directions, or allow for multiple railway paths to be made (i.e a Northern path, Eastern path and Western path from Toronto are not all possible at the same time in the current iteration of the project). We will learn from this experience and find a more applicable algorithm for our problems in the future.\nAccomplishments that we're proud of\nWe're very proud of the fact that we understood a large chunk of the complex algorithm and implemented it to solve a real issue in the freight industry. We're also happy that we knew the constraints/ limitations of the model after working with it, such as it's inability to detect water. Knowing this will allow us to build upon this experience for future endeavors, creating more robust projects.", "link": "https://devpost.com/software/western-ai-hacks-team-1", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nthere are three pressing problems in the freight industry that result from the over usage of trucks as the method of transportation.\nfirstly, there are severe environmental impacts that demonstrate the need for a change to reduce our carbon footprint. global water levels, average temperature and average co2 levels are all projected to rise in the coming decades, so we need to do our part to in reducing these alarming statistics before it is too late. reducing the amount of transport trucks on the road, which are much less fuel efficient than rail, will help reduce global emission rates, hopefully putting us in the right direction towards a more sustainable future.\nsecondly, these environmental impacts, paired with the time and cost inefficiencies for trucking for long distances, show that there is clearly need for changes towards a more sustainable future in the freight industry.\nas mentioned earlier, transport trucks are less fuel efficient than rail. they are also much more limited in their freight capacity. additional investments into the railway industry will create not only a more environmentally friendly transportation industry, but also could lead to a more efficient supply chain for these businesses.\nlastly, the 400 series highways of ontario grow more congested everyday due to increased demands for products, and building new highways/ expanding the old ones will simple lead to induced congestion (https://environmentaldefence.ca/2020/09/15/why-building-more-highways-wont-make-your-commute-any-better/).\nthat's where rail comes in. by increasing the amount of freight shipped on railways, we should see a proportional decrease in the amount of transport trucks on the 400 series highways, leading to reduced congestion in ontario's most densely populated areas.\nthe magnitudes of these multi layered issues is why we decided to tackle this problem. we hoped that a change could be made by a applying an already existing model to help create a sustainable future for the industry of freight.\nwhat it does\nit's an application of an optimization model that implements an \"ant-colony\"-esque solution to the traveling salesman problem. we in turn further modified this model to allow it to take in coordinates for specific locations, allowing the model to return the near-optimal path, which would be used as a guiding -----> tool !!!  for future railway construction.\nchallenges we ran into\nunderstanding how different parts of the optimization algorithm were implemented were definitely hard. a lot of us didn't know much ml coming into the hackathon so trying to understand the developed algorithm as well as simultaneously learning ml was a massive hurdle.\nadditionally, the model itself wasn't the ideal algorithm to solve this particular solution. while it does create a near optimal straight line path for the railways, it struggles to account for barriers such as water/ towns that aren't inputted into its coordinates.\nthe model also doesn't allow for backtracking, which in this case means that we wouldn't be able to find an optimal railway system that allows for trains to run in multiple directions, or allow for multiple railway paths to be made (i.e a northern path, eastern path and western path from toronto are not all possible at the same time in the current iteration of the project). we will learn from this experience and find a more applicable algorithm for our problems in the future.\naccomplishments that we're proud of\nwe're very proud of the fact that we understood a large chunk of the complex algorithm and implemented it to solve a real issue in the freight industry. we're also happy that we knew the constraints/ limitations of the model after working with it, such as it's inability to detect water. knowing this will allow us to build upon this experience for future endeavors, creating more robust projects.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59502349}, {"Unnamed: 0": 2351, "autor": "Lesson", "date": null, "content": "Inspiration\nWe were inspired by the fact that many of our friends, students, or even ourselves that are still confused about what field we want to choose in the future and how we can achieve that dream of ours. A lot of students are stressed or depressed because they don\u2019t know what career they want to choose and had no one to teach us about what we need to learn to be able to get a career that we want. And so we want to make a way that helps people learn about the careers that they want and how to prepare to get that career.\nWhat it does\nOur project aims to help students that haven\u2019t chosen a career or field choose what they want to become in the future and guides the students that have already chosen what career or field that they want.\nHow we built it\nThe three of us decided to use the no code programming tool, Wordpress to create our website.\nChallenges we ran into\nThe short time period that we were given.\nWe had to balance our collage work along with programming our web.\nWe only know the basics of making a website. ## Accomplishments that we're proud of\nWe were able to build a prototype website even with our limited set of skills.\nWe were able to work together even from a long distance.\nWe were able to exchange our ideas and find one idea that works. ## What we learned We thought of an idea one day before the competition started so we wasted a lot of time. This happened because this was our very first competition that we competed in. And so we learned that we should plan things more earlier to save more time and be more productive in the long run. ## What's next for Lesson Next, we would like to use actual coding rather than using a no-code programming tool to create a website. We would like to develop our website even further by enabling students to be able to register themselves.", "link": "https://devpost.com/software/lesson", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe were inspired by the fact that many of our friends, students, or even ourselves that are still confused about what field we want to choose in the future and how we can achieve that dream of ours. a lot of students are stressed or depressed because they don\u2019t know what career they want to choose and had no one to teach us about what we need to learn to be able to get a career that we want. and so we want to make a way that helps people learn about the careers that they want and how to prepare to get that career.\nwhat it does\nour project aims to help students that haven\u2019t chosen a career or field choose what they want to become in the future and guides the students that have already chosen what career or field that they want.\nhow we built it\nthe three of us decided to use the no code programming -----> tool !!! , wordpress to create our website.\nchallenges we ran into\nthe short time period that we were given.\nwe had to balance our collage work along with programming our web.\nwe only know the basics of making a website. ## accomplishments that we're proud of\nwe were able to build a prototype website even with our limited set of skills.\nwe were able to work together even from a long distance.\nwe were able to exchange our ideas and find one idea that works. ## what we learned we thought of an idea one day before the competition started so we wasted a lot of time. this happened because this was our very first competition that we competed in. and so we learned that we should plan things more earlier to save more time and be more productive in the long run. ## what's next for lesson next, we would like to use actual coding rather than using a no-code programming tool to create a website. we would like to develop our website even further by enabling students to be able to register themselves.", "sortedWord": "None", "removed": "Nan", "score": 5, "comments": 0, "media": null, "medialink": null, "identifyer": 59502351}, {"Unnamed: 0": 2355, "autor": "AvaAssist", "date": null, "content": "\ud83d\udca1 INSPIRATION \ud83d\udca1\nMany students have poor spending habits and losing track of one's finances may cause unnecessary stress. As university students ourselves, we're often plagued with financial struggles. As young adults down on our luck, we often look to open up a credit card or take out student loans to help support ourselves. However, we're deterred from loans because they normally involve phoning automatic call centers which are robotic and impersonal. We also don't know why or what to do when we've been rejected from loans. Many of us weren't taught how to plan our finances properly and we frequently find it difficult to keep track of our spending habits. To address this problem troubling our generation, we decided to create AvaAssist! The goal of the app is to provide a welcoming place where you can seek financial advice and plan for your future.\n\u2699\ufe0f WHAT IT DOES \u2699\ufe0f\nAvaAssist is a financial advisor built to support young adults and students. Ava can provide loan evaluation, financial planning, and monthly spending breakdowns. If you ever need banking advice, Ava's got your back!\n\ud83d\udd0eRESEARCH\ud83d\udd0d\n\ud83e\udde0UX Research\ud83e\udde0\nTo discover the pain points of existing banking practices, we interviewed 2 and surveyed 7 participants on their current customer experience and behaviors. The results guided us in defining a major problem area and the insights collected contributed to discovering our final solution.\n\ud83d\udcb8Loan Research\ud83d\udcb8\nTo properly predict whether a loan would be approved or not, we researched what goes into the loan approval process. The resulting research guided us towards ensuring that each loan was profitable and didn't take on too much risk for the bank.\n\ud83d\udee0\ufe0f HOW WE BUILT IT\ud83d\udee0\ufe0f\n\u270f\ufe0fUI/UX Design\u270f\ufe0f\nFigma was used to create a design prototype. The prototype was designed in accordance with Voice UI (VUI) design principles & Material design as a base. This expedited us to the next stage of development because the programmers had visual guidance in developing the app. With the use of Dasha.AI, we were able to create an intuitive user experience in supporting customers through natural dialog via the chatbot, and a friendly interface with the use of an AR avatar.\nCheck out our figma here\nCheck out our presentation here\n\ud83d\udcc8Predictive Modeling\ud83d\udcc8\nThe final iteration of each model has a test prediction accuracy of +85%!\nWe only got to this point because of our due diligence, preprocessing, and feature engineering. After coming up with our project, we began thinking about and researching HOW banks evaluate loans. Loan evaluation at banks is extremely complex and we tried to capture some aspects of it in our model. We came up with one major aspect to focus on during preprocessing and while searching for our datasets, profitability. There would be no point for banks to take on a loan if it weren't profitable. We found a couple of databases with credit card and loan data on Kaggle. The datasets were smaller than desired. We had to be very careful during preprocessing when deciding what data to remove and how to fill NULL values to preserve as much data as possible. Feature engineering was certainly the most painstaking part of building the prediction model. One of the most important features we added was the Risk Free Rate (CORRA). The Risk Free Rate is the rate of return of an investment with no risk of loss. It helped with the engineering process of another feature, min_loan, which is the minimum amount of money that the bank can make with no risk of loss. Min_loan would ultimately help our model understand which loans are profitable and which aren't. As a result, the model learned to decline unprofitable loans.\nWe also did market research on the average interest rate of specific types of loans to make assumptions about certain features to supplement our lack of data. For example, we used the average credit card loan interest rate of 22%. The culmination of newly engineered features and the already existing data resulted in our complex, high accuracy models. We have a model for Conventional Loans, Credit Card Loans, and Student Loans. The model we used was RandomForests from sklearn because of its wide variety of hyperparameters and robustness. It was fine-tuned using gridsearchCV to find its best hyperparameters. We designed a pipeline for each model using Pipeline, OneHotEncoder, StandardScaler, FunctionTransformer, GradientBoostingClassifier, and RandomForestClassifier from sklearn. Finally, the models were saved as pickle files for front-end deployment.\n\ud83d\ude80Frontend Deployment\ud83d\ude80\nWorking on the frontend was a very big challenge. Since we didn't have a dedicated or experienced frontend developer, there was a lot of work and learning to be done. Additionally, a lot of ideas had to be cut from our final product as well. First, we had to design the frontend with React Native, using our UI/UX Designer's layout. For this we decided to use Figma, and we were able to dynamically update our design to keep up with any changes that were made. Next, we decided to tackle hooking up the machine learning models to React with Flask. Having Typescript communicate with Python was difficult. Thanks to these libraries and a lot of work, we were able to route requests from the frontend to the backend, and vice versa. This way, we could send the values that our user inputs on the frontend to be processed by the ML models, and have them give an accurate result. Finally, we took on the challenge of learning how to use Dasha.AI and integrating it with the frontend. Learning how to use DashaScript (Dasha.AI's custom programming language) took time, but eventually, we started getting the hang of it, and everything was looking good!\n\ud83d\ude23 CHALLENGES WE RAN INTO \ud83d\ude23\nOur teammate, Abdullah, who is no longer on our team, had family issues come up and was no longer able to attend HackWestern unfortunately. This forced us to get creative when deciding a plan of action to execute our ambitious project. We needed to redistribute roles, change schedules, look for a new teammate, but most importantly, learn EVEN MORE NEW SKILLS and adapt our project to our changing team. As a team, we had to go through our ideation phase again to decide what would and wouldn't be viable for our project. We ultimately decided to not use Dialogflow for our project. However, this was a blessing in disguise because it allowed us to hone in on other aspects of our project such as finding good data to enhance user experience and designing a user interface for our target market.\nThe programmers had to learn DashaScript on the fly which was a challenge as we normally code with OOP\u2019s. But, with help from mentors and workshops, we were able to understand the language and implement it into our project\nCombining the frontend and backend processes proved to be very troublesome because the chatbot needed to get user data and relay it to the model. We eventually used react-native to store the inputs across instances/files.\nThe entire team has very little experience and understanding of the finance world, it was both difficult and fun to research different financial models that banks use to evaluate loans.\nWe had initial problems designing a UI centered around a chatbot/machine learning model because we couldn't figure out a user flow that incorporated all of our desired UX aspects.\nFinding good data to train the prediction models off of was very tedious, even though there are some Kaggle datasets there were few to none that were large enough for our purposes. The majority of the datasets were missing information and good datasets were hidden behind paywalls. It was for this reason that couldn't make a predictive model for mortgages. To overcome this, I had to combine datasets/feature engineer to get a useable dataset.\n\ud83c\udf89 ACCOMPLISHMENTS WE ARE PROUD OF \ud83c\udf89\nOur time management was impeccable, we are all very proud of ourselves since we were able to build an entire app with a chat bot and prediction system within 36 hours\nOrganization within the team was perfect, we were all able to contribute and help each other when needed; ex. the UX/UI design in figma paved the way for our front end developer\nSuper proud of how we were able to overcome missing a teammate and build an amazing project!\nWe are happy to empower people during their financial journey and provide them with a welcoming source to gain new financial skills and knowledge\nLearning and implementing DashaAi was a BLAST and we're proud that we could learn this new and very useful technology. We couldn't have done it without mentor help, \ud83d\udce3shout out to Arthur and Sreekaran\ud83d\udce3 for providing us with such great support.\nThis was a SUPER amazing project! We're all proud to have done it in such a short period of time, everyone is new to the hackathon scene and are still eager to learn new technologies\n\ud83d\udcda WHAT WE LEARNED \ud83d\udcda\nDashaAi is a brand new technology we learned from the DashaAi workshop. We wanted to try and implement it in our project. We needed a handful of mentor sessions to figure out how to respond to inputs properly, but we're happy we learned it!\nReact-native is a framework our team utilized to its fullest, but it had its learning curve. We learned how to make asynchronous calls to integrate our backend with our frontend.\nUnderstanding how to take the work of the UX/UI designer and apply it dynamically was important because of the numerous design changes we had throughout the weekend.\nHow to use REST APIs to predict an output with flask using the models we designed was an amazing skill that we learned\nWe were super happy that we took the time to learn Expo-cli because of how efficient it is, we could check how our mobile app would look on our phones immediately.\nFirst time using AR models in Animaze, it took some time to understand, but it ultimately proved to be a great tool!\n\u23ed\ufe0fWHAT'S NEXT FOR AvaAssist\u23ed\ufe0f\nAvaAssist has a lot to do before it can be deployed as a genuine app. It will only be successful if the customer is satisfied and benefits from using it, otherwise, it will be a failure. Our next steps are to implement more features for the user experience.\nFor starters, we want to implement Dialogflow back into our idea. Dialogflow would be able to understand the intent behind conversations and the messages it exchanges with the user. The long-term prospect of this would be that we could implement more functions for Ava. In the future Ava could be making investments for the user, moving money between personal bank accounts, setting up automatic/making payments, and much more.\nFinally, we also hope to create more tabs within the AvaAssist app where the user can see their bank account history and its breakdown, user spending over time, and a financial planner where users can set intervals to put aside/invest their money.\n\ud83c\udf81 ABOUT THE TEAM\ud83c\udf81\nYifan is a 3rd year interactive design student at Sheridan College, currently interning at SAP. With experience in designing for social startups and B2B software, she is interested in expanding her repertoire in designing for emerging technologies and healthcare. You can connect with her at her LinkedIn or view her Portfolio\nAlan is a 2nd year computer science student at the University of Calgary. He's has a wide variety of technical skills in frontend and backend development! Moreover, he has a strong passion for both data science and app development. You can reach out to him at his LinkedIn\nMatthew is a 2nd year student at Simon Fraser University studying computer science. He has formal training in data science. He's interested in learning new and honing his current frontend skills/technologies. Moreover, he has a deep understanding of machine learning, AI and neural networks. He's always willing to have a chat about games, school, data science and more! You can reach out to him at his LinkedIn\n\ud83d\udce3\ud83d\udce3 SHOUT OUT TO ABDULLAH FOR HELPING US THROUGH IDEATION\ud83d\udce3\ud83d\udce3 You can still connect with Abdullah at his LinkedIn He's super passionate about reactJS and wants to learn more about machine learning and AI!\n\ud83e\udd73\ud83c\udf89 THANK YOU UW FOR HOSTING HACKWESTERN\ud83e\udd73\ud83c\udf89", "link": "https://devpost.com/software/scotiabank-chatbot", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "\ud83d\udca1 inspiration \ud83d\udca1\nmany students have poor spending habits and losing track of one's finances may cause unnecessary stress. as university students ourselves, we're often plagued with financial struggles. as young adults down on our luck, we often look to open up a credit card or take out student loans to help support ourselves. however, we're deterred from loans because they normally involve phoning automatic call centers which are robotic and impersonal. we also don't know why or what to do when we've been rejected from loans. many of us weren't taught how to plan our finances properly and we frequently find it difficult to keep track of our spending habits. to address this problem troubling our generation, we decided to create avaassist! the goal of the app is to provide a welcoming place where you can seek financial advice and plan for your future.\n\u2699\ufe0f what it does \u2699\ufe0f\navaassist is a financial advisor built to support young adults and students. ava can provide loan evaluation, financial planning, and monthly spending breakdowns. if you ever need banking advice, ava's got your back!\n\ud83d\udd0eresearch\ud83d\udd0d\n\ud83e\udde0ux research\ud83e\udde0\nto discover the pain points of existing banking practices, we interviewed 2 and surveyed 7 participants on their current customer experience and behaviors. the results guided us in defining a major problem area and the insights collected contributed to discovering our final solution.\n\ud83d\udcb8loan research\ud83d\udcb8\nto properly predict whether a loan would be approved or not, we researched what goes into the loan approval process. the resulting research guided us towards ensuring that each loan was profitable and didn't take on too much risk for the bank.\n\ud83d\udee0\ufe0f how we built it\ud83d\udee0\ufe0f\n\u270f\ufe0fui/ux design\u270f\ufe0f\nfigma was used to create a design prototype. the prototype was designed in accordance with voice ui (vui) design principles & material design as a base. this expedited us to the next stage of development because the programmers had visual guidance in developing the app. with the use of dasha.ai, we were able to create an intuitive user experience in supporting customers through natural dialog via the chatbot, and a friendly interface with the use of an ar avatar.\ncheck out our figma here\ncheck out our presentation here\n\ud83d\udcc8predictive modeling\ud83d\udcc8\nthe final iteration of each model has a test prediction accuracy of +85%!\nwe only got to this point because of our due diligence, preprocessing, and feature engineering. after coming up with our project, we began thinking about and researching how banks evaluate loans. loan evaluation at banks is extremely complex and we tried to capture some aspects of it in our model. we came up with one major aspect to focus on during preprocessing and while searching for our datasets, profitability. there would be no point for banks to take on a loan if it weren't profitable. we found a couple of databases with credit card and loan data on kaggle. the datasets were smaller than desired. we had to be very careful during preprocessing when deciding what data to remove and how to fill null values to preserve as much data as possible. feature engineering was certainly the most painstaking part of building the prediction model. one of the most important features we added was the risk free rate (corra). the risk free rate is the rate of return of an investment with no risk of loss. it helped with the engineering process of another feature, min_loan, which is the minimum amount of money that the bank can make with no risk of loss. min_loan would ultimately help our model understand which loans are profitable and which aren't. as a result, the model learned to decline unprofitable loans.\nwe also did market research on the average interest rate of specific types of loans to make assumptions about certain features to supplement our lack of data. for example, we used the average credit card loan interest rate of 22%. the culmination of newly engineered features and the already existing data resulted in our complex, high accuracy models. we have a model for conventional loans, credit card loans, and student loans. the model we used was randomforests from sklearn because of its wide variety of hyperparameters and robustness. it was fine-tuned using gridsearchcv to find its best hyperparameters. we designed a pipeline for each model using pipeline, onehotencoder, standardscaler, functiontransformer, gradientboostingclassifier, and randomforestclassifier from sklearn. finally, the models were saved as pickle files for front-end deployment.\n\ud83d\ude80frontend deployment\ud83d\ude80\nworking on the frontend was a very big challenge. since we didn't have a dedicated or experienced frontend developer, there was a lot of work and learning to be done. additionally, a lot of ideas had to be cut from our final product as well. first, we had to design the frontend with react native, using our ui/ux designer's layout. for this we decided to use figma, and we were able to dynamically update our design to keep up with any changes that were made. next, we decided to tackle hooking up the machine learning models to react with flask. having typescript communicate with python was difficult. thanks to these libraries and a lot of work, we were able to route requests from the frontend to the backend, and vice versa. this way, we could send the values that our user inputs on the frontend to be processed by the ml models, and have them give an accurate result. finally, we took on the challenge of learning how to use dasha.ai and integrating it with the frontend. learning how to use dashascript (dasha.ai's custom programming language) took time, but eventually, we started getting the hang of it, and everything was looking good!\n\ud83d\ude23 challenges we ran into \ud83d\ude23\nour teammate, abdullah, who is no longer on our team, had family issues come up and was no longer able to attend hackwestern unfortunately. this forced us to get creative when deciding a plan of action to execute our ambitious project. we needed to redistribute roles, change schedules, look for a new teammate, but most importantly, learn even more new skills and adapt our project to our changing team. as a team, we had to go through our ideation phase again to decide what would and wouldn't be viable for our project. we ultimately decided to not use dialogflow for our project. however, this was a blessing in disguise because it allowed us to hone in on other aspects of our project such as finding good data to enhance user experience and designing a user interface for our target market.\nthe programmers had to learn dashascript on the fly which was a challenge as we normally code with oop\u2019s. but, with help from mentors and workshops, we were able to understand the language and implement it into our project\ncombining the frontend and backend processes proved to be very troublesome because the chatbot needed to get user data and relay it to the model. we eventually used react-native to store the inputs across instances/files.\nthe entire team has very little experience and understanding of the finance world, it was both difficult and fun to research different financial models that banks use to evaluate loans.\nwe had initial problems designing a ui centered around a chatbot/machine learning model because we couldn't figure out a user flow that incorporated all of our desired ux aspects.\nfinding good data to train the prediction models off of was very tedious, even though there are some kaggle datasets there were few to none that were large enough for our purposes. the majority of the datasets were missing information and good datasets were hidden behind paywalls. it was for this reason that couldn't make a predictive model for mortgages. to overcome this, i had to combine datasets/feature engineer to get a useable dataset.\n\ud83c\udf89 accomplishments we are proud of \ud83c\udf89\nour time management was impeccable, we are all very proud of ourselves since we were able to build an entire app with a chat bot and prediction system within 36 hours\norganization within the team was perfect, we were all able to contribute and help each other when needed; ex. the ux/ui design in figma paved the way for our front end developer\nsuper proud of how we were able to overcome missing a teammate and build an amazing project!\nwe are happy to empower people during their financial journey and provide them with a welcoming source to gain new financial skills and knowledge\nlearning and implementing dashaai was a blast and we're proud that we could learn this new and very useful technology. we couldn't have done it without mentor help, \ud83d\udce3shout out to arthur and sreekaran\ud83d\udce3 for providing us with such great support.\nthis was a super amazing project! we're all proud to have done it in such a short period of time, everyone is new to the hackathon scene and are still eager to learn new technologies\n\ud83d\udcda what we learned \ud83d\udcda\ndashaai is a brand new technology we learned from the dashaai workshop. we wanted to try and implement it in our project. we needed a handful of mentor sessions to figure out how to respond to inputs properly, but we're happy we learned it!\nreact-native is a framework our team utilized to its fullest, but it had its learning curve. we learned how to make asynchronous calls to integrate our backend with our frontend.\nunderstanding how to take the work of the ux/ui designer and apply it dynamically was important because of the numerous design changes we had throughout the weekend.\nhow to use rest apis to predict an output with flask using the models we designed was an amazing skill that we learned\nwe were super happy that we took the time to learn expo-cli because of how efficient it is, we could check how our mobile app would look on our phones immediately.\nfirst time using ar models in animaze, it took some time to understand, but it ultimately proved to be a great -----> tool !!! !\n\u23ed\ufe0fwhat's next for avaassist\u23ed\ufe0f\navaassist has a lot to do before it can be deployed as a genuine app. it will only be successful if the customer is satisfied and benefits from using it, otherwise, it will be a failure. our next steps are to implement more features for the user experience.\nfor starters, we want to implement dialogflow back into our idea. dialogflow would be able to understand the intent behind conversations and the messages it exchanges with the user. the long-term prospect of this would be that we could implement more functions for ava. in the future ava could be making investments for the user, moving money between personal bank accounts, setting up automatic/making payments, and much more.\nfinally, we also hope to create more tabs within the avaassist app where the user can see their bank account history and its breakdown, user spending over time, and a financial planner where users can set intervals to put aside/invest their money.\n\ud83c\udf81 about the team\ud83c\udf81\nyifan is a 3rd year interactive design student at sheridan college, currently interning at sap. with experience in designing for social startups and b2b software, she is interested in expanding her repertoire in designing for emerging technologies and healthcare. you can connect with her at her linkedin or view her portfolio\nalan is a 2nd year computer science student at the university of calgary. he's has a wide variety of technical skills in frontend and backend development! moreover, he has a strong passion for both data science and app development. you can reach out to him at his linkedin\nmatthew is a 2nd year student at simon fraser university studying computer science. he has formal training in data science. he's interested in learning new and honing his current frontend skills/technologies. moreover, he has a deep understanding of machine learning, ai and neural networks. he's always willing to have a chat about games, school, data science and more! you can reach out to him at his linkedin\n\ud83d\udce3\ud83d\udce3 shout out to abdullah for helping us through ideation\ud83d\udce3\ud83d\udce3 you can still connect with abdullah at his linkedin he's super passionate about reactjs and wants to learn more about machine learning and ai!\n\ud83e\udd73\ud83c\udf89 thank you uw for hosting hackwestern\ud83e\udd73\ud83c\udf89", "sortedWord": "None", "removed": "Nan", "score": 9, "comments": 0, "media": null, "medialink": null, "identifyer": 59502355}, {"Unnamed: 0": 2365, "autor": "LearnOne", "date": null, "content": "LearnOne\nLearnOne is a platform to help online learning be easier. LearnOne is used by students, teachers, and schools to do online school.\nInspiration\nSince the COVID-19 pandemic, the demand for online learning platforms skyrocketed. My other inspiration is that my high school's learning platform is hard-to-use and looks sad in color. That's why I'm trying to create a nice in look platform and make it easy to use.\nWhat it does\nLearnOne has several features that are:\nJoin/Create Classes\nClassroom page\nCreating material and homework\nClass code for easier class join\nHow I built it\nI built it on replit and connected it with the GoDaddy Registry for easier website edit. Then I used Python flask to connect python with HTML. I also used the Bootstrap framework for responsive UI. Using SQLite3 for the database.\nChallenges I ran into\nThis is my first time connecting SQLite manually to python because I usually used a tool to simplify SQLite connection. I had a lot of problems with the SQL queries. I'm also bad at designing so I can't make a good UI.\nWhat I learned\nThe web programming world is REALLY broad. We can't underestimate anything, really. I still have a lot to learn and discover about the programming world.\nWhat's next for LearnOne\nMuch MUCH better UI\nAuthorization with Auth0\nAndroid and IOS app ...and more!", "link": "https://devpost.com/software/learnone", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "learnone\nlearnone is a platform to help online learning be easier. learnone is used by students, teachers, and schools to do online school.\ninspiration\nsince the covid-19 pandemic, the demand for online learning platforms skyrocketed. my other inspiration is that my high school's learning platform is hard-to-use and looks sad in color. that's why i'm trying to create a nice in look platform and make it easy to use.\nwhat it does\nlearnone has several features that are:\njoin/create classes\nclassroom page\ncreating material and homework\nclass code for easier class join\nhow i built it\ni built it on replit and connected it with the godaddy registry for easier website edit. then i used python flask to connect python with html. i also used the bootstrap framework for responsive ui. using sqlite3 for the database.\nchallenges i ran into\nthis is my first time connecting sqlite manually to python because i usually used a -----> tool !!!  to simplify sqlite connection. i had a lot of problems with the sql queries. i'm also bad at designing so i can't make a good ui.\nwhat i learned\nthe web programming world is really broad. we can't underestimate anything, really. i still have a lot to learn and discover about the programming world.\nwhat's next for learnone\nmuch much better ui\nauthorization with auth0\nandroid and ios app ...and more!", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59502365}, {"Unnamed: 0": 2366, "autor": "Engage Kit", "date": null, "content": "Inspiration\nFirst step during the brainstorming and research phase was to review what information was available from the Square APIs and what existing functionality was available via the seller dashboard. After completing the brainstorming and research phase, the idea of a digital marketing tool was chosen for the hackathon.\nWhat it does\nEngage Kit allows sellers to engage with their customers via email and social media channels. Sellers can customise their campaigns using the built-in design editor and schedule when their campaigns start.\nFor email campaigns, the seller can select recipients from a list of customer groups and customer segments populated using Square APIs.\nHow we built it\nEngage Kit is built as a React web application using the Next.js framework and uses UI components from Ant Design. Data is persisted in a MongoDB database and made available to the frontend application via server-side rendering and API routes.\nEngage Kit is deployed on Azure using App Service and CosmosDB.\nChallenges we ran into\nThe biggest challenge was learning to use the Next.js React framework and Ant Design UI components for the first time in a production application - it required dedication and persistence that paid off in the end!\nAccomplishments that we're proud of\nGetting to the finish line and being able to deploy the final version of Engage Kit to production on Azure.\nWhat we learned\nThinking of an idea is actually much more difficult then implementing the idea!\nWhat's next for Engage Kit\nImproving design customisations and templates available in Engage Kit - the design editor is a bit elementary due to time constraints.\nLinking marketing assets like photos and videos with the seller's products.", "link": "https://devpost.com/software/engage-kit", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nfirst step during the brainstorming and research phase was to review what information was available from the square apis and what existing functionality was available via the seller dashboard. after completing the brainstorming and research phase, the idea of a digital marketing -----> tool !!!  was chosen for the hackathon.\nwhat it does\nengage kit allows sellers to engage with their customers via email and social media channels. sellers can customise their campaigns using the built-in design editor and schedule when their campaigns start.\nfor email campaigns, the seller can select recipients from a list of customer groups and customer segments populated using square apis.\nhow we built it\nengage kit is built as a react web application using the next.js framework and uses ui components from ant design. data is persisted in a mongodb database and made available to the frontend application via server-side rendering and api routes.\nengage kit is deployed on azure using app service and cosmosdb.\nchallenges we ran into\nthe biggest challenge was learning to use the next.js react framework and ant design ui components for the first time in a production application - it required dedication and persistence that paid off in the end!\naccomplishments that we're proud of\ngetting to the finish line and being able to deploy the final version of engage kit to production on azure.\nwhat we learned\nthinking of an idea is actually much more difficult then implementing the idea!\nwhat's next for engage kit\nimproving design customisations and templates available in engage kit - the design editor is a bit elementary due to time constraints.\nlinking marketing assets like photos and videos with the seller's products.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59502366}, {"Unnamed: 0": 2406, "autor": "Sixt Taxi Distributer Optimizer", "date": null, "content": "Inspiration\nSixt new taxi service! It creates interesting problems about how to distribute your resources effectively We sought to create a fleet management system, that would optimally use the fleet's resources. We wanted to take into account the possibility of carsharing, the current batterystatus of any given vehicle. to help with understanding of the current traffic, we created a visualization tool, by translating points of interest into a graph. With the given graph, we could then implement Dijkstra's algorithm to calculate the shortest path. This system could be used in combination with the api to simulate orders and vehicles as objects.\nWhat it does\nIt simulates a town with people that book sixt taxis and calculates optimal paths to reduce pollution and costs\nHow we built it\nWe used normal c# to create a model, which then can be visualized with the unity3d\nChallenges we ran into\nTSP, Dijkstra and formulating the problem\nAccomplishments that we're proud of\nWe've created a city that automatically creates and assigned bookings to simulate the pick up process of taxis and customers. We've visualized the process\nWhat we learned\nA lot about the TSP complexity, Implementing efficient dijkstra =>.net 5 doesn't have a priority queue => thanks for that... And getting better at problem-solving\nWhat's next for Sixt Taxi Distributer Optimizer\nCreating more complex environments to see if our algorithms can withstand that and creating better visuals Also taking the following things into account! => Charge status => Charge stations => Traffic => Traveling Sales Person Problem Problem => Comparison between different optimization alogrithms", "link": "https://devpost.com/software/sixt-taxi-distributer-optimizer", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nsixt new taxi service! it creates interesting problems about how to distribute your resources effectively we sought to create a fleet management system, that would optimally use the fleet's resources. we wanted to take into account the possibility of carsharing, the current batterystatus of any given vehicle. to help with understanding of the current traffic, we created a visualization -----> tool !!! , by translating points of interest into a graph. with the given graph, we could then implement dijkstra's algorithm to calculate the shortest path. this system could be used in combination with the api to simulate orders and vehicles as objects.\nwhat it does\nit simulates a town with people that book sixt taxis and calculates optimal paths to reduce pollution and costs\nhow we built it\nwe used normal c# to create a model, which then can be visualized with the unity3d\nchallenges we ran into\ntsp, dijkstra and formulating the problem\naccomplishments that we're proud of\nwe've created a city that automatically creates and assigned bookings to simulate the pick up process of taxis and customers. we've visualized the process\nwhat we learned\na lot about the tsp complexity, implementing efficient dijkstra =>.net 5 doesn't have a priority queue => thanks for that... and getting better at problem-solving\nwhat's next for sixt taxi distributer optimizer\ncreating more complex environments to see if our algorithms can withstand that and creating better visuals also taking the following things into account! => charge status => charge stations => traffic => traveling sales person problem problem => comparison between different optimization alogrithms", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59502406}, {"Unnamed: 0": 2435, "autor": "PropoMan", "date": null, "content": "Inspiration\nProposal Manager was born out of the chaos that contracting companies encounter while searching for federal, state gov. and educational entities' proposals, figuring out who can take the lead to be the proposal manager/assigning to the right person/lead for competency and finally meeting the deadline to complete the submissions on time. This is an everyday effort for Federal contractor companies and is time consuming and expensive.\nWhat it does\nProposal Manager scours the SAM.gov website, looking for contracts with NAICS code 5415 (Computer Systems Design and Related Services). Each of the contract detail page is scraped for the details and stored in a CSV file. This CSV file is then passed to the UiPath AI Fabric's out of the box package for Named Entity recognition. The idea is to figure out the department from which the Contract opportunity originated, so that the right Proposal Manager with the expertise/knowledge of the department could be assigned the task for the proposal writing.\nHow we built it\nWe built it using the UiPath RPA tool. The UiPath project scours the SAM.gov website for contracts with NAICS code 5415 (Computer Systems Design and Related Services).\nChallenges we ran into\nThe SAM.gov website is still a beta site and has lots of html issues. The DIV tags were not being identified.\nAccomplishments that we're proud of\nI am proud of learning UiPath and it's ease to create RPA projects.\nWhat we learned\nThis is my first RPA project. I am planning to do a switch in my career and was looking at RPA as an option. The Hackathon was an inspiration to learn UiPath.\nWhat's next for Proposal Manager\nProposal Manager was an idea that I had while working for a beltway contractor in DC area. I want Proposal Manager to utilize another ML package that I am working on, which would look at past win rates for proposals submitted and figure out if it makes sense to explore the new contract opportunity. This would save companies a lot of time from avoiding pre-proposal meetings that determines whether to go after the proposal or not.", "link": "https://devpost.com/software/proposal-manager", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nproposal manager was born out of the chaos that contracting companies encounter while searching for federal, state gov. and educational entities' proposals, figuring out who can take the lead to be the proposal manager/assigning to the right person/lead for competency and finally meeting the deadline to complete the submissions on time. this is an everyday effort for federal contractor companies and is time consuming and expensive.\nwhat it does\nproposal manager scours the sam.gov website, looking for contracts with naics code 5415 (computer systems design and related services). each of the contract detail page is scraped for the details and stored in a csv file. this csv file is then passed to the uipath ai fabric's out of the box package for named entity recognition. the idea is to figure out the department from which the contract opportunity originated, so that the right proposal manager with the expertise/knowledge of the department could be assigned the task for the proposal writing.\nhow we built it\nwe built it using the uipath rpa -----> tool !!! . the uipath project scours the sam.gov website for contracts with naics code 5415 (computer systems design and related services).\nchallenges we ran into\nthe sam.gov website is still a beta site and has lots of html issues. the div tags were not being identified.\naccomplishments that we're proud of\ni am proud of learning uipath and it's ease to create rpa projects.\nwhat we learned\nthis is my first rpa project. i am planning to do a switch in my career and was looking at rpa as an option. the hackathon was an inspiration to learn uipath.\nwhat's next for proposal manager\nproposal manager was an idea that i had while working for a beltway contractor in dc area. i want proposal manager to utilize another ml package that i am working on, which would look at past win rates for proposals submitted and figure out if it makes sense to explore the new contract opportunity. this would save companies a lot of time from avoiding pre-proposal meetings that determines whether to go after the proposal or not.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59502435}, {"Unnamed: 0": 2454, "autor": "Social Media web report", "date": null, "content": "Inspiration\nthe pandemic made alot of business go online, and its hard. so we made a tool to analyze each social media insight and made it easy to understand.\nHow we built it\nPHP next.js a lot of hard works\nChallenges we ran into\nteam member missing not understanding the language\nAccomplishments that we're proud of\nimplementing calendar and auth0\nWhat we learned\nneeds to be faster\nWhat's next for Social Media web report\nprobably implement web crawler using automation robot", "link": "https://devpost.com/software/social-media-web-report", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nthe pandemic made alot of business go online, and its hard. so we made a -----> tool !!!  to analyze each social media insight and made it easy to understand.\nhow we built it\nphp next.js a lot of hard works\nchallenges we ran into\nteam member missing not understanding the language\naccomplishments that we're proud of\nimplementing calendar and auth0\nwhat we learned\nneeds to be faster\nwhat's next for social media web report\nprobably implement web crawler using automation robot", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59502454}, {"Unnamed: 0": 2489, "autor": "GELARTIKAR", "date": null, "content": "Inspiration\nOne day we were walking on the street in the Jakarta area, it should be noted that Jakarta is one of the big cities in Indonesia. At that time we saw an unpleasant sight, we saw a lot of traders (MSME entrepreneur) whose stalls were forced to be closed or even evicted and confiscated by the authorities because there is no permit to trade in that area. That moment really made us feel so sad, so it made us think how to provide a place/land for them to continue to sell and trade legally so that they can continue their life.\nWhat it does\nWe give opportunity to those who have a MSME business to run their business through event to event, which is would help them a lot to gain more profits for them to scale up their business. besides it also would help them to be recognize by many people out there. it can be a milestone for them to keep evolving their business.\nHow we built it\nWe build it as a mobile application using lumen + flutter framework that will work on multiplatform like android and ios. for database we use MySQL to store the database. For UI/UX Design we use Figma as a main tool for making the whole UI App and Prototyping. For UML Class Diagram and Use Case Diagram we simply use Microsoft Word. We doing a team work to make this application, We devide the role into only 2 Roles :\nUI/UX Designer : Me (Rosyad) the one who make UI/UX Design and also making the UML Class Diagram and Use Case Diagram for the developer.\nFull-Stack Developer : My Partner (Naufal) the one who worked hard to implement the design into a mobile application using Lumen + Flutter Framework and he also handle the database for the application.\nChallenges we ran into\nActually we do never make a mobile application before, so this is our first time making a mobile app project. Beside, this hackathon event is also our first hackathon we joined in our life. We both used to be a web app developer, so this is the real deal for us to make a mobile application in only 36 hour. Fun Facts : -This is the first time naufal write a code using flutter Framework that's a real big big challage for him. -And This is also my first time using Figma and become an UI/UX designer so this is also become a real challange for me.\nAccomplishments that we're proud of\nFor naufal, i really proud of him, because he finally can use the flutter framework and make his code run pretty well. i know that working on the new framework that he never touched before is really that hard. but, He is really good partner, he Finished his JOB at the end.\nFor me, i proud of my self because i can finish my work as a UI/UX designer, this is new for me but i think i can do this.\nWhat we learned\nOf Course we learned a lot from this hackathon, we learn to making a great team work, we learn some hardskill as i mentioned above, we learn how to manage a team, time, and we also learned to work under pressure.\nWhat's next for GELARTIKAR\nWe will continue this project, and we gonna work hard to focus on helping MSME entrepreneur, because the problem that we saw is something that we both want to solve. we make our business model Agile so that we are ready to solve another problem for MSME entrepreneur. and we gonna make GELARTIKAR as One Stop Application for every MSME entrepreneur needs. As our project name \"GELARTIKAR\" which is the antonym from \"GULUNGTIKAR\". we want to avoid MSME entrepreneur from GULUNGTIKAR ( ending/stopping/quitting their business ).", "link": "https://devpost.com/software/gelartikar", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\none day we were walking on the street in the jakarta area, it should be noted that jakarta is one of the big cities in indonesia. at that time we saw an unpleasant sight, we saw a lot of traders (msme entrepreneur) whose stalls were forced to be closed or even evicted and confiscated by the authorities because there is no permit to trade in that area. that moment really made us feel so sad, so it made us think how to provide a place/land for them to continue to sell and trade legally so that they can continue their life.\nwhat it does\nwe give opportunity to those who have a msme business to run their business through event to event, which is would help them a lot to gain more profits for them to scale up their business. besides it also would help them to be recognize by many people out there. it can be a milestone for them to keep evolving their business.\nhow we built it\nwe build it as a mobile application using lumen + flutter framework that will work on multiplatform like android and ios. for database we use mysql to store the database. for ui/ux design we use figma as a main -----> tool !!!  for making the whole ui app and prototyping. for uml class diagram and use case diagram we simply use microsoft word. we doing a team work to make this application, we devide the role into only 2 roles :\nui/ux designer : me (rosyad) the one who make ui/ux design and also making the uml class diagram and use case diagram for the developer.\nfull-stack developer : my partner (naufal) the one who worked hard to implement the design into a mobile application using lumen + flutter framework and he also handle the database for the application.\nchallenges we ran into\nactually we do never make a mobile application before, so this is our first time making a mobile app project. beside, this hackathon event is also our first hackathon we joined in our life. we both used to be a web app developer, so this is the real deal for us to make a mobile application in only 36 hour. fun facts : -this is the first time naufal write a code using flutter framework that's a real big big challage for him. -and this is also my first time using figma and become an ui/ux designer so this is also become a real challange for me.\naccomplishments that we're proud of\nfor naufal, i really proud of him, because he finally can use the flutter framework and make his code run pretty well. i know that working on the new framework that he never touched before is really that hard. but, he is really good partner, he finished his job at the end.\nfor me, i proud of my self because i can finish my work as a ui/ux designer, this is new for me but i think i can do this.\nwhat we learned\nof course we learned a lot from this hackathon, we learn to making a great team work, we learn some hardskill as i mentioned above, we learn how to manage a team, time, and we also learned to work under pressure.\nwhat's next for gelartikar\nwe will continue this project, and we gonna work hard to focus on helping msme entrepreneur, because the problem that we saw is something that we both want to solve. we make our business model agile so that we are ready to solve another problem for msme entrepreneur. and we gonna make gelartikar as one stop application for every msme entrepreneur needs. as our project name \"gelartikar\" which is the antonym from \"gulungtikar\". we want to avoid msme entrepreneur from gulungtikar ( ending/stopping/quitting their business ).", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59502489}, {"Unnamed: 0": 2500, "autor": "LaunchPitch (LaP)", "date": null, "content": "Inspiration\nWe were inspired by a lot of people that have a great idea that can be a profitable business in the future, but having problems to make their ideas come true, how to make a business from their idea, or to find an investor to help their project's idea come true. So we thought that we should make an app to help them build their raw idea into working business in the future called LaunchPitch.\nWhat it does\nLaunchPitch can create an opportunity for those who\u2019s only have an idea but don't know how to objectify it. They can register and submit their idea, and we will choose some of the best ideas. Then we can teach them how to start the business from their idea, help them to make a prototype, help them to make a proper pitch, and finally help them to get their investor.\nHow we built it\nWe built this prototype using non-code tool, Figma .\nChallenges we ran into\nThis is our first hackathon, so we don't have any experience with something like this. We're ran into problems like:\nMaking a prototype with a very limited time\nManaging time with our college's assignments\nLack of designing skill\nBad connection\nAccomplishments that we're proud of\nFinishing our prototype with very limited time\nAble to objectify our idea into a prototype\nBrainstorming an idea that can help others\nWhat we learned\nFrom this hackathon we've learned how to manage our time, and gained some experience to compete with a lot of talented competitors.\nWhat's next for LaunchPitch (LaP)\nAs you can see LaunchPitch (LaP) is still a prototype. The next thing we want to apply to this prototype is to make this prototype into a functional application, and complete all incomplete features from the prototype such as transaction between the investor and the project's founder.", "link": "https://devpost.com/software/launchpitch-lap", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe were inspired by a lot of people that have a great idea that can be a profitable business in the future, but having problems to make their ideas come true, how to make a business from their idea, or to find an investor to help their project's idea come true. so we thought that we should make an app to help them build their raw idea into working business in the future called launchpitch.\nwhat it does\nlaunchpitch can create an opportunity for those who\u2019s only have an idea but don't know how to objectify it. they can register and submit their idea, and we will choose some of the best ideas. then we can teach them how to start the business from their idea, help them to make a prototype, help them to make a proper pitch, and finally help them to get their investor.\nhow we built it\nwe built this prototype using non-code -----> tool !!! , figma .\nchallenges we ran into\nthis is our first hackathon, so we don't have any experience with something like this. we're ran into problems like:\nmaking a prototype with a very limited time\nmanaging time with our college's assignments\nlack of designing skill\nbad connection\naccomplishments that we're proud of\nfinishing our prototype with very limited time\nable to objectify our idea into a prototype\nbrainstorming an idea that can help others\nwhat we learned\nfrom this hackathon we've learned how to manage our time, and gained some experience to compete with a lot of talented competitors.\nwhat's next for launchpitch (lap)\nas you can see launchpitch (lap) is still a prototype. the next thing we want to apply to this prototype is to make this prototype into a functional application, and complete all incomplete features from the prototype such as transaction between the investor and the project's founder.", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59502500}, {"Unnamed: 0": 2512, "autor": "SONUX", "date": null, "content": "Inspiration\nHave you ever wondered or experienced a world without colour? What is life like being unable to see the surroundings, the people and do entertainment? In this case, hearing becomes an essential sense, which provides people with vision disabilities a unique way to experience the world. In addition, with the growing smartphone penetration rate and the forced stay-at-home caused by the pandemic, more people have gradually become isolated from society. As a team, we genuinely believe that social interaction is a crucial factor in achieving better health. Sonux is here to help build both online and offline connectivity, especially for those experiencing blindness and/or visual impairment. Through playing this game, players will not only gain a satisfying acoustic sensory experience but also encourage more people to stay away from the screen and appreciate the scenery of different communities, of diverse cultures and mother nature, leading to better health outcomes, mentally and physically.\nWhat it does\nSonux is a mobile application designed for anyone looking to relax through screen breaks while gaining a whole experience of auditory sensation. The application is user-friendly, in which people with blindness and/or visual impairment will achieve the same experience as other players. Once players launch the game, the phone\u2019s screen will be in a darkened mode, and players can barely see any displays. The only way to navigate and select game options is through swiping, knocking, or simply waving their phones. As players explore around, Sonux will sense their surrounding environment and search for sounds. At distinct locations, Sonux is able to capture different soundtracks; it can be a lyric of a song, a piece of classical music, the sound of nature or a message from someone unknown. Players are, therefore, able to store all the sounds that they encountered during their journey. As users explore more places, travel to further locations, they will discover diverse sounds. The game enables players to expand their maps to other parts of the world and unlock more soundtracks. Furthermore, players are encouraged to leave warm and positive messages that other users will collect. With multiple sounds collected, users could always return to their sound archive, re-listen to all the sounds, and make various combinations of harmonies.\nHow we built it\nUnity Engine Sonux is developed based on the Unity engine to enable the best 3-D acoustic experience. We started by creating a 3-D space with natural objects modelled inside. Each element has its unique sound and lighting effects, guiding the player's journey. Unlike traditional games where navigation is achieved through screen touches, Sonux utilizes hand gestures, device movement and haptic feedback. By creating a 3-D environment for players to explore, Doppler Effects is achieved virtually by varying sound channels and volume levels, maximizing Sonux's acoustic experience.\nNatural Language Processing We employed Unity's voice recognition API with a sentiment analysis model to analyze players' appropriateness of message input. After translating the player's voice message into texts, we process the text message using the textBlob API, a pre-trained NLP model, to determine a sentiment score of the message. To factor in the potential inconsistency of the model, messages indicating harmful contents will be filtered and further reviewed by our staff.\nChallenges we ran into\nOne of the biggest challenges we ran into was in our application of Unity. Unity is the most suitable real-time game development platform in our case to build the audio game for mobile devices. However, our group has encountered several debugging problems as we lack previous experience and skill set with this tool. We have gone through multiple tutorials, navigated the platform thoroughly, and self-taught the fundamental skills needed to develop Sonux. Nevertheless, when coding for the game to be able to listen and react to voice instructions, we failed the first few trials. It resulted from Mac laptops not supporting certain sound features. Thus, we switched to a PC laptop and adjusted our code to successfully make the voice recognition function. Recognizing this bug was one of the big moments for the project.\nAccomplishments that we're proud of\nThe biggest accomplishment we believe is that as a team, we have worked well together. With some members from a tech background and another from a media information major, we had very thoughtful and insightful discussions in the project brainstorming stage. In which all members have identified and agreed upon the need for mobile games to be more inclusive to the special player group and be more considerate towards players' health. Through working cohesively, we have turned our idea from sketch to an actual presentable model that is able to include all the features we have drafted.\nWhat we learned\nWe learned how to use Unity, how the platform works, and the foundation to make mobile games. Members with a tech background have gained a better understanding of the coding process for game development and how to navigate game engines. For the member who does not have previous tech/science knowledge, it was a fascinating experience to see how the games played throughout daily lives come into being. In addition, all members have gained well-rounded perspectives by seeing how different people think and approach problem-solving differently.\nWhat's next for SONUX\nSonux comes alive after evaluating both the practical components and considering the ethical and social responsibility that a mobile game company should take on to achieve long-term sustainability. Sonux hopes to raise more attention and care to the visual disability population while also creating a more healthy lifestyle for the ever-increasing mobile game players. With these goals in mind, Sonux will continue to build a more connected, long-lasting and inclusive player community.", "link": "https://devpost.com/software/ebitda", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nhave you ever wondered or experienced a world without colour? what is life like being unable to see the surroundings, the people and do entertainment? in this case, hearing becomes an essential sense, which provides people with vision disabilities a unique way to experience the world. in addition, with the growing smartphone penetration rate and the forced stay-at-home caused by the pandemic, more people have gradually become isolated from society. as a team, we genuinely believe that social interaction is a crucial factor in achieving better health. sonux is here to help build both online and offline connectivity, especially for those experiencing blindness and/or visual impairment. through playing this game, players will not only gain a satisfying acoustic sensory experience but also encourage more people to stay away from the screen and appreciate the scenery of different communities, of diverse cultures and mother nature, leading to better health outcomes, mentally and physically.\nwhat it does\nsonux is a mobile application designed for anyone looking to relax through screen breaks while gaining a whole experience of auditory sensation. the application is user-friendly, in which people with blindness and/or visual impairment will achieve the same experience as other players. once players launch the game, the phone\u2019s screen will be in a darkened mode, and players can barely see any displays. the only way to navigate and select game options is through swiping, knocking, or simply waving their phones. as players explore around, sonux will sense their surrounding environment and search for sounds. at distinct locations, sonux is able to capture different soundtracks; it can be a lyric of a song, a piece of classical music, the sound of nature or a message from someone unknown. players are, therefore, able to store all the sounds that they encountered during their journey. as users explore more places, travel to further locations, they will discover diverse sounds. the game enables players to expand their maps to other parts of the world and unlock more soundtracks. furthermore, players are encouraged to leave warm and positive messages that other users will collect. with multiple sounds collected, users could always return to their sound archive, re-listen to all the sounds, and make various combinations of harmonies.\nhow we built it\nunity engine sonux is developed based on the unity engine to enable the best 3-d acoustic experience. we started by creating a 3-d space with natural objects modelled inside. each element has its unique sound and lighting effects, guiding the player's journey. unlike traditional games where navigation is achieved through screen touches, sonux utilizes hand gestures, device movement and haptic feedback. by creating a 3-d environment for players to explore, doppler effects is achieved virtually by varying sound channels and volume levels, maximizing sonux's acoustic experience.\nnatural language processing we employed unity's voice recognition api with a sentiment analysis model to analyze players' appropriateness of message input. after translating the player's voice message into texts, we process the text message using the textblob api, a pre-trained nlp model, to determine a sentiment score of the message. to factor in the potential inconsistency of the model, messages indicating harmful contents will be filtered and further reviewed by our staff.\nchallenges we ran into\none of the biggest challenges we ran into was in our application of unity. unity is the most suitable real-time game development platform in our case to build the audio game for mobile devices. however, our group has encountered several debugging problems as we lack previous experience and skill set with this -----> tool !!! . we have gone through multiple tutorials, navigated the platform thoroughly, and self-taught the fundamental skills needed to develop sonux. nevertheless, when coding for the game to be able to listen and react to voice instructions, we failed the first few trials. it resulted from mac laptops not supporting certain sound features. thus, we switched to a pc laptop and adjusted our code to successfully make the voice recognition function. recognizing this bug was one of the big moments for the project.\naccomplishments that we're proud of\nthe biggest accomplishment we believe is that as a team, we have worked well together. with some members from a tech background and another from a media information major, we had very thoughtful and insightful discussions in the project brainstorming stage. in which all members have identified and agreed upon the need for mobile games to be more inclusive to the special player group and be more considerate towards players' health. through working cohesively, we have turned our idea from sketch to an actual presentable model that is able to include all the features we have drafted.\nwhat we learned\nwe learned how to use unity, how the platform works, and the foundation to make mobile games. members with a tech background have gained a better understanding of the coding process for game development and how to navigate game engines. for the member who does not have previous tech/science knowledge, it was a fascinating experience to see how the games played throughout daily lives come into being. in addition, all members have gained well-rounded perspectives by seeing how different people think and approach problem-solving differently.\nwhat's next for sonux\nsonux comes alive after evaluating both the practical components and considering the ethical and social responsibility that a mobile game company should take on to achieve long-term sustainability. sonux hopes to raise more attention and care to the visual disability population while also creating a more healthy lifestyle for the ever-increasing mobile game players. with these goals in mind, sonux will continue to build a more connected, long-lasting and inclusive player community.", "sortedWord": "None", "removed": "Nan", "score": 6, "comments": 0, "media": null, "medialink": null, "identifyer": 59502512}, {"Unnamed: 0": 2548, "autor": "Morning Byte. - Your daily dose of data-driven insights.", "date": null, "content": "Inspiration\nWe believe that data analytics can at times be too ponderous and that more bite-sized information needs to be made available to decision-makers. A highly-relevant influx of no more than two or three key data insights in the morning could vastly enhance an entire day's work...\nWhat it does\nMorningByte and the architecture behind it are a next-generation, automated morning briefing for managers in video-format.\nThe idea is to equip managers with tight schedules with a small pre-selection of the most relevant anomalies or newly discovered trends of the day that warrant their attention. In order to maintain said attention, these trends and anomalies are explained in the format of short, yet insightful videos.\nUnder the hood, the following happens:\nAfter end of business and once all sales units (in our example stores) have reported their data for the day, our pipeline comes to life. An analytics engine receives the newest data and automatically runs a series of models to look for anomalies or newly discovered trends. For every insight discovered this way, a descriptive text and action recommendation are generated. Each action item is also marked with a priority score based on severity and type. The results are then sent to a video-generation tool, which creates a short video for each detected insight. Those videos are then uploaded to a mobile-friendly app, putting the most relevant insights at the manager's fingertips.\nHow we built it\nWe first built a python script that loads the CSV files and creates dataframes out of each. We then run a series of analyses to detect and create insight items. The insight items contain information on the type of graph required, the descriptive text, as well as the action suggestion text and a relevance-score.\nThe insight items resulting from this first part are then passed over to a second python script that is responsible for creating mp4 video files from the given information. The text is processed using Google Text-to-Speech to be converted into sound. In parallel, we use Matplotlib to visualize the data passed over by the first script. We then use MoviePy to generate an MP4 file for each insight. These are then sent to the webapp for display, which is hosted using Heroku.\nChallenges we ran into\nThe first challenge we had was trying to limit the scope of the app. The provided data-set offered near limitless opportunities for data-analytics and it would have been very easy to get lost. We debated whether to use complicated forecasting models, or more advanced time-series techniques, but decided to settle on simple methods, in order to deliver a fully functioning product.\nIt was also initially unclear who our target audience was going to be. We spent a lot of time carefully thinking about our value proposition and found that a morning briefing format for management provided the right mixture of plausibility and feasibility for a 36h hackathon, whilst still offering enough potential down the line to get us super excited.\nThe largest technical challenge we faced were various formatting issues with Moviepy. Seeing as we had to juggle various issues at the same the videos were often displaying differently on different devices. This took forever to fix.\nAccomplishments that we're proud of\nWe're most proud of the way we harmonised as a team. Every one of us contributed to the project in vastly different and unique ways. We used our strengths where we could and helped cover for each other's weaknesses. The result is a multi-faceted project that none of us could have ever dreamt up by ourselves.\nWhat we learned\nWe spent a lot of time working in pairs and were able to learn direct technical skills from each other. We also learned that teamwork is absolutely crucial and that a diverse team of ambitious and creative people with varied skill-sets can outgrow our individual imaginations. The biggest thing we learned may be that hackathons are a ton of fun and we'll be throwing our hat in the ring again in the future...\nWhat's next for Morning-Byte?\nWe believe Morning-Byte has enormous potential and we're genuinely convinced that millions of people will incorporate similar apps into their morning routines in the months and years to come. Because this was born out of a caffeine-fuelled hackathon, the next steps will initially be to iron out any minor bugs and expand the depth of analytics catalogue.\nIn the long-run there are many potential applications:\nIn a future scenario, we could well imagine a subscription-based model for the tool, where employees in different company roles could subscribe to various elements of the data to receive personalised video feeds. In the example of the Turkish retailer, one could conceive a model where store managers get versions of the app that only prioritise videos concerning their specific stores. Similar custom priorities could be introduced for employees only interested in one product categories.\nThe app could also introduce interactive elements. One obvious example would be a button that connects to the company's Outlook and automatically schedules a high-priority meeting with employees involved in a detected anomaly. A regional account manager could use the app to immediately summon underperforming store managers when poor revenue data is detected or data is missing altogether, for example.", "link": "https://devpost.com/software/animated-data-stories", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe believe that data analytics can at times be too ponderous and that more bite-sized information needs to be made available to decision-makers. a highly-relevant influx of no more than two or three key data insights in the morning could vastly enhance an entire day's work...\nwhat it does\nmorningbyte and the architecture behind it are a next-generation, automated morning briefing for managers in video-format.\nthe idea is to equip managers with tight schedules with a small pre-selection of the most relevant anomalies or newly discovered trends of the day that warrant their attention. in order to maintain said attention, these trends and anomalies are explained in the format of short, yet insightful videos.\nunder the hood, the following happens:\nafter end of business and once all sales units (in our example stores) have reported their data for the day, our pipeline comes to life. an analytics engine receives the newest data and automatically runs a series of models to look for anomalies or newly discovered trends. for every insight discovered this way, a descriptive text and action recommendation are generated. each action item is also marked with a priority score based on severity and type. the results are then sent to a video-generation -----> tool !!! , which creates a short video for each detected insight. those videos are then uploaded to a mobile-friendly app, putting the most relevant insights at the manager's fingertips.\nhow we built it\nwe first built a python script that loads the csv files and creates dataframes out of each. we then run a series of analyses to detect and create insight items. the insight items contain information on the type of graph required, the descriptive text, as well as the action suggestion text and a relevance-score.\nthe insight items resulting from this first part are then passed over to a second python script that is responsible for creating mp4 video files from the given information. the text is processed using google text-to-speech to be converted into sound. in parallel, we use matplotlib to visualize the data passed over by the first script. we then use moviepy to generate an mp4 file for each insight. these are then sent to the webapp for display, which is hosted using heroku.\nchallenges we ran into\nthe first challenge we had was trying to limit the scope of the app. the provided data-set offered near limitless opportunities for data-analytics and it would have been very easy to get lost. we debated whether to use complicated forecasting models, or more advanced time-series techniques, but decided to settle on simple methods, in order to deliver a fully functioning product.\nit was also initially unclear who our target audience was going to be. we spent a lot of time carefully thinking about our value proposition and found that a morning briefing format for management provided the right mixture of plausibility and feasibility for a 36h hackathon, whilst still offering enough potential down the line to get us super excited.\nthe largest technical challenge we faced were various formatting issues with moviepy. seeing as we had to juggle various issues at the same the videos were often displaying differently on different devices. this took forever to fix.\naccomplishments that we're proud of\nwe're most proud of the way we harmonised as a team. every one of us contributed to the project in vastly different and unique ways. we used our strengths where we could and helped cover for each other's weaknesses. the result is a multi-faceted project that none of us could have ever dreamt up by ourselves.\nwhat we learned\nwe spent a lot of time working in pairs and were able to learn direct technical skills from each other. we also learned that teamwork is absolutely crucial and that a diverse team of ambitious and creative people with varied skill-sets can outgrow our individual imaginations. the biggest thing we learned may be that hackathons are a ton of fun and we'll be throwing our hat in the ring again in the future...\nwhat's next for morning-byte?\nwe believe morning-byte has enormous potential and we're genuinely convinced that millions of people will incorporate similar apps into their morning routines in the months and years to come. because this was born out of a caffeine-fuelled hackathon, the next steps will initially be to iron out any minor bugs and expand the depth of analytics catalogue.\nin the long-run there are many potential applications:\nin a future scenario, we could well imagine a subscription-based model for the tool, where employees in different company roles could subscribe to various elements of the data to receive personalised video feeds. in the example of the turkish retailer, one could conceive a model where store managers get versions of the app that only prioritise videos concerning their specific stores. similar custom priorities could be introduced for employees only interested in one product categories.\nthe app could also introduce interactive elements. one obvious example would be a button that connects to the company's outlook and automatically schedules a high-priority meeting with employees involved in a detected anomaly. a regional account manager could use the app to immediately summon underperforming store managers when poor revenue data is detected or data is missing altogether, for example.", "sortedWord": "None", "removed": "Nan", "score": 5, "comments": 1, "media": null, "medialink": null, "identifyer": 59502548}, {"Unnamed: 0": 2559, "autor": "Chimera", "date": null, "content": "Inspiration\nBring our team's best effort to develop the best solution to improve financial inclusion and diminish the digital gap in our country.\nWhat it does\nmicroBank, allows you to create financial products for the sector, based on the information processed and stored in Reddito, an ERP on the web or mobile, which keeps track of sales, inventories, purchases, and additional information related to the footprint of carbon. With remittances and the Chivo Wallet, the opportunity opens for this sector to access these opportunities, for which we offer PayMart, a payment gateway that accepts Bitcoins, QR payments from any bank and credit and debit cards with the greatest circulation.\nHow we built it\nUsing Reddito and Paymart, our ERP and Payment Gateway, develop with Node.Js, Golang and Flutter, integrated with OpenBank, API3, Qredo tools, BCTPay, Chivo Wallet## Challenges we ran into To know how to wrap Chivo wallet or any bitcoin wallet with off-chain data\nAccomplishments that we're proud of\nWe already have a solid idea of what we would like to accomplish\nWhat we learned\nHow to use Bitcoin new tool's set to develop sound and safe solutions\nWhat's next for Chimera\nWe have to make sure we deploy our project in the next coming year", "link": "https://devpost.com/software/chimera-7dim2e", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nbring our team's best effort to develop the best solution to improve financial inclusion and diminish the digital gap in our country.\nwhat it does\nmicrobank, allows you to create financial products for the sector, based on the information processed and stored in reddito, an erp on the web or mobile, which keeps track of sales, inventories, purchases, and additional information related to the footprint of carbon. with remittances and the chivo wallet, the opportunity opens for this sector to access these opportunities, for which we offer paymart, a payment gateway that accepts bitcoins, qr payments from any bank and credit and debit cards with the greatest circulation.\nhow we built it\nusing reddito and paymart, our erp and payment gateway, develop with node.js, golang and flutter, integrated with openbank, api3, qredo tools, bctpay, chivo wallet## challenges we ran into to know how to wrap chivo wallet or any bitcoin wallet with off-chain data\naccomplishments that we're proud of\nwe already have a solid idea of what we would like to accomplish\nwhat we learned\nhow to use bitcoin new -----> tool !!! 's set to develop sound and safe solutions\nwhat's next for chimera\nwe have to make sure we deploy our project in the next coming year", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59502559}, {"Unnamed: 0": 2616, "autor": "Megaloverse", "date": null, "content": "Inspiration\nWe have enough meeting apps free and paid, but they are all boring because they stick to what they r meant for, i.e, meetings.. Its not a bad thing but it makes it very old school and boring.\nWhat it does\nWhat I have created in the Megoloverse prototype is a virtual space for hackathons, expos, art galleries, and even fun meetups. The virtual space has a customizable map with support for interactions and storing the state of activities in database. It uses spatial audio from dolby.IO for more dynamic experience.\nHow I built it\nDolby IO for communications and spatial audio\nAngular front end hosted on github-pages\nExpress, NodeJS backend hosted on netlify\nMongoDB database\nAccomplishments that Iam proud of\nEasiest tool to start a meeting. (just give the name of your space in url ... thats it !!!)\nhttps://solid-droid.github.io/Megaloverse/<your_space_name>\nInteractive custom map (map is pan - zoom capable and mobile friendly)\nWhat's next for Megaloverse\nGUI for admins to configure custom maps\nIsolated spots for private chat\nInteraction on the map can be saved, which gives possibilities to create new widgets, games , physics simulations which can be saved to database..(what you do in Megaloverse .. stay in Megaloverse )", "link": "https://devpost.com/software/megaloverse", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe have enough meeting apps free and paid, but they are all boring because they stick to what they r meant for, i.e, meetings.. its not a bad thing but it makes it very old school and boring.\nwhat it does\nwhat i have created in the megoloverse prototype is a virtual space for hackathons, expos, art galleries, and even fun meetups. the virtual space has a customizable map with support for interactions and storing the state of activities in database. it uses spatial audio from dolby.io for more dynamic experience.\nhow i built it\ndolby io for communications and spatial audio\nangular front end hosted on github-pages\nexpress, nodejs backend hosted on netlify\nmongodb database\naccomplishments that iam proud of\neasiest -----> tool !!!  to start a meeting. (just give the name of your space in url ... thats it !!!)\nhttps://solid-droid.github.io/megaloverse/<your_space_name>\ninteractive custom map (map is pan - zoom capable and mobile friendly)\nwhat's next for megaloverse\ngui for admins to configure custom maps\nisolated spots for private chat\ninteraction on the map can be saved, which gives possibilities to create new widgets, games , physics simulations which can be saved to database..(what you do in megaloverse .. stay in megaloverse )", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59502616}, {"Unnamed: 0": 2623, "autor": "MagicBox", "date": null, "content": "Inspiration\nManufacturers face the challenge that no real-time information on the current product quality is available to react short term on up stream and down stream changes. We want to address this need and provide transparency on, for example, cost impact of waste, delays, losses and errors in production lines. We help to ensure consistent quality in every product (=screens) in a highly dynamic manufacturing environment.\nWith the help of Accenture and the provided Azure Account we're ready to hack!\nWhat it does\nOur solution provides real-time information about the quality outcome of a product. In this case we take a look at defect detection on surfaces of screens. In this manner we propose a cascading low-level image preprocessing strategy to get the best results out of a lightweight Deep Learning algorithm, which can provide fast and accurate scratch detection performance. Most importantly we will show you all our achievements in a real-life demonstration of our implemented dashboard.\nHow we built it\nThe Data provided from Accenture was labeled with the tool \"LabelMe\" (see given credits in our Slides). We used Azure services to train and validate our DL-Model. For preprocessing and image augmentation we used the Visual Studio IDE and the python language. The dashboard was created with the frontend framework React. Javascript has been used to create the clean UI.\nChallenges we ran into\nWe also address the challenge of a small dataset in our solution by giving insights in our Augmented Data Concept and Image Preprocessing Strategy to overcome the challenge and train an even more efficient DL-Model.\nAccomplishments that we're proud of\nMeeting in person to get to know each other and enjoing every minute of the hackathon. We started as a collection of strangers and got out of the HackaTUM as friends.\nWhat we learned\nIf your in extreme coding flow you wont get any sleep.\nWhat's next for We Wont Byte\nFor the future the possibility of generating more data additional to the result of the screen quality should be considered. This can help to detect which parameters are most important to get high quality products and which are critical. All in all this would provide a digital twin of the manufacturing process and create the possibility to understand the process in more depth as well as react to changes before the product loses in quality and prevent defect products rather than just detecting this cases.\nWith this information the Dashboard will be improved and additional features provided.", "link": "https://devpost.com/software/our-prohooojeeekt", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nmanufacturers face the challenge that no real-time information on the current product quality is available to react short term on up stream and down stream changes. we want to address this need and provide transparency on, for example, cost impact of waste, delays, losses and errors in production lines. we help to ensure consistent quality in every product (=screens) in a highly dynamic manufacturing environment.\nwith the help of accenture and the provided azure account we're ready to hack!\nwhat it does\nour solution provides real-time information about the quality outcome of a product. in this case we take a look at defect detection on surfaces of screens. in this manner we propose a cascading low-level image preprocessing strategy to get the best results out of a lightweight deep learning algorithm, which can provide fast and accurate scratch detection performance. most importantly we will show you all our achievements in a real-life demonstration of our implemented dashboard.\nhow we built it\nthe data provided from accenture was labeled with the -----> tool !!!  \"labelme\" (see given credits in our slides). we used azure services to train and validate our dl-model. for preprocessing and image augmentation we used the visual studio ide and the python language. the dashboard was created with the frontend framework react. javascript has been used to create the clean ui.\nchallenges we ran into\nwe also address the challenge of a small dataset in our solution by giving insights in our augmented data concept and image preprocessing strategy to overcome the challenge and train an even more efficient dl-model.\naccomplishments that we're proud of\nmeeting in person to get to know each other and enjoing every minute of the hackathon. we started as a collection of strangers and got out of the hackatum as friends.\nwhat we learned\nif your in extreme coding flow you wont get any sleep.\nwhat's next for we wont byte\nfor the future the possibility of generating more data additional to the result of the screen quality should be considered. this can help to detect which parameters are most important to get high quality products and which are critical. all in all this would provide a digital twin of the manufacturing process and create the possibility to understand the process in more depth as well as react to changes before the product loses in quality and prevent defect products rather than just detecting this cases.\nwith this information the dashboard will be improved and additional features provided.", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 2, "media": null, "medialink": null, "identifyer": 59502623}, {"Unnamed: 0": 2624, "autor": "PhilOpt", "date": null, "content": "Inspiration\nHelping others in need is an importabt, yet not an easy task, well, where shall one start? How can one discover projects she / he is passionate about? Often people would like to contribute, but find it hard, because of the effort thats needs to be put into researching about projects. We provide a simple solution for you and help you to help others.\nWhat it does\nWe deliver a public tool for you to discover highly relevant and certified charitable projects you personally, or as a whole company feel passionate about and can support with your specific talents, skills and resources.\nHow we built it\nWe built our responsive Web Application using the very capable JavaScript Framework React. In combination with our Database Backend System from Firebase, our platform is built with modern-day technology, which allows us to have full modular control over the content of our plattform. In particular certified users (e.g help organazations) can submit their relevant projects to our database. Aditionally we included our own Domain (philopt.org) to ensure our project will persist, so that it has an impact even after the initial Hackatum.\nChallenges we ran into\nWe clearly ran into different challenges, some of which were caused by the 36 hour hackatum time frame. Building any technology needs a lot of debugging. Also having a time frame that narrow, puts very hard decisions the to table - One will have to simplify the project to its most important core functionalities, even there is plenty of the \"cool\" ideas you have in mind. Regarding the planing of our project structure we faced the challenge to find the simplest way for a user to help others.\nAccomplishments that we're proud of\nWe are proud to develop a platform that can have a real world impact, changing the way people even perceive charity. Our tool supports charitable projects on a whole new data-driven, digitalized and modern level.\nWhat we learned\nWe learned that keeping things simple is a great and important strategy for making progress in projects, especially those projects that are narrowly timed. Furthermore we learned that it does not always have to be complicated or super time consuming to make the world a better place - It just takes one Weekend to create the foundations.\nWhat's next for PhilOpt\nThere are several further steps for us to take. Technologywise we will implement a full user system, including different user classes for certified organisations, Companies and Peronal Users. Therefore we will contact certified organisations, so they can upload their current projects to our database and make them easy accessible for our users. We will also reach out to companies, which we want to help to create their very own and unique portfolio of charitable projects their employees are passionate about. So companies that work with our platform give their employees the priviledge to decide which charitable projects to support. Additionally we are of course open to support personal users. There are a lot of other ideas to follow, like creating a heat map indicating where on the world urgent problems are located or further enhancing our recommandations.", "link": "https://devpost.com/software/the-future-of-philantrophy", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nhelping others in need is an importabt, yet not an easy task, well, where shall one start? how can one discover projects she / he is passionate about? often people would like to contribute, but find it hard, because of the effort thats needs to be put into researching about projects. we provide a simple solution for you and help you to help others.\nwhat it does\nwe deliver a public -----> tool !!!  for you to discover highly relevant and certified charitable projects you personally, or as a whole company feel passionate about and can support with your specific talents, skills and resources.\nhow we built it\nwe built our responsive web application using the very capable javascript framework react. in combination with our database backend system from firebase, our platform is built with modern-day technology, which allows us to have full modular control over the content of our plattform. in particular certified users (e.g help organazations) can submit their relevant projects to our database. aditionally we included our own domain (philopt.org) to ensure our project will persist, so that it has an impact even after the initial hackatum.\nchallenges we ran into\nwe clearly ran into different challenges, some of which were caused by the 36 hour hackatum time frame. building any technology needs a lot of debugging. also having a time frame that narrow, puts very hard decisions the to table - one will have to simplify the project to its most important core functionalities, even there is plenty of the \"cool\" ideas you have in mind. regarding the planing of our project structure we faced the challenge to find the simplest way for a user to help others.\naccomplishments that we're proud of\nwe are proud to develop a platform that can have a real world impact, changing the way people even perceive charity. our tool supports charitable projects on a whole new data-driven, digitalized and modern level.\nwhat we learned\nwe learned that keeping things simple is a great and important strategy for making progress in projects, especially those projects that are narrowly timed. furthermore we learned that it does not always have to be complicated or super time consuming to make the world a better place - it just takes one weekend to create the foundations.\nwhat's next for philopt\nthere are several further steps for us to take. technologywise we will implement a full user system, including different user classes for certified organisations, companies and peronal users. therefore we will contact certified organisations, so they can upload their current projects to our database and make them easy accessible for our users. we will also reach out to companies, which we want to help to create their very own and unique portfolio of charitable projects their employees are passionate about. so companies that work with our platform give their employees the priviledge to decide which charitable projects to support. additionally we are of course open to support personal users. there are a lot of other ideas to follow, like creating a heat map indicating where on the world urgent problems are located or further enhancing our recommandations.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59502624}, {"Unnamed: 0": 2628, "autor": "Susop", "date": null, "content": "Susop | HackaTUM 2021\nSusop means two things: Sustainabilty and Optimization\nExplore the project!\u00bb\nInspiration\nNowadays \"Always-On\" Devices have higher numbers than ever before, with those and other devices we just dont turn off we consume a lot of unused power everyday. Many People arent even aware of these Devices in their Home, and even if they are it is often hard to visualize the kind of Impact, a PC that is never turned off may have.\nReducing unnecessary Power Consumption is something that everyone can unintrusively introduce into your Routine that can highly impact CO2 Emissions over time.\nWe want to help Consumers and Companies by providing an easy tool to monitor activity of these devices and visualize the footpring they really have.\nWhat it does\nSusop provides a simple User Interface that monitors Activity in your Network, classifies devices and calculates Energy Usage and the resulting CO2 Emission from statistical data by the Umweltbundesamt.\nIn the future it will run decentralized, sharing any old monitoring data through the network to newer devices, so we can reduce even the impact the monitoring would have.\nHow we built it\nFirst of all we researched Power Consumption of HouseHold Devices that are connected to your Network and during that we also looked into the actual CO2 Emissions we get from our Energy Consumption. After we gathered that Data we began to build our App using Java, there we created a DeviceScanner and Recognizer for our Network, with which we then monitored our Network Activity and UpTime of these Devices. We used this Data with Average Wattage Data to calculate Emissions and Power Consumption.\nThe UI was created using JavaSwing concurrently, here we wanted to provide a very simple but visual tool for the User.\nChallenges we ran into\nGetting all the Devices from your Network and correctly classifying them can be pretty hard, especially if the device owner wants to only share limited Data with you. We tried to classify the Devices we tested with, as well as possible but sometimes you really only have an IP-Adress and nothing else and then it is almost impossible to get a correct Wattage Estimation. We also had Problems with getting and identifying the different Mac Addresses of Devices, as this was the original Plan to estimate Power Consumption. Sadly even if we would have finished the implementation of this, we would at most have been able to gather Information about the Company from the Mac Address and not necessarily the Device Type itself.\nAccomplishments that we're proud of\nWe created an easy to understand Network Monitoring Tool, that could help lower CO2 waste. We created a project that could be easily ported to different Devices to further reduce the Impact of our Network.\nWhat we learned\nWe learned a lot about Networks and how we can interact within our local Networks. Additionally we realized how much unnecessary Power was drawn by simple and small devices in our own home and how much energy some devices really use.\nWhat's next for Susop\nIn the Future Susop would love to further develope the decentralized communication we started building during the hackathon but couldnt finish due to Time Constraints. We would also love to port it to different Devices creating an even better and closer Network. If possible wed also love to get more specific and accurate Stats about energy consumption, either from the devices themselves, through running our software, or from SmartHome Integration.\nLast Words\nUntil then! Hear you soon and maybe watch your favourite Show on your Phone or Laptop, instead of your TV next time.\nAnd to the companies, you really can turn off the Lights and big Presentation Screens during the weekend if no-one is working ;) And if youre unsure about the impact that has, try Susop!", "link": "https://devpost.com/software/susop", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "susop | hackatum 2021\nsusop means two things: sustainabilty and optimization\nexplore the project!\u00bb\ninspiration\nnowadays \"always-on\" devices have higher numbers than ever before, with those and other devices we just dont turn off we consume a lot of unused power everyday. many people arent even aware of these devices in their home, and even if they are it is often hard to visualize the kind of impact, a pc that is never turned off may have.\nreducing unnecessary power consumption is something that everyone can unintrusively introduce into your routine that can highly impact co2 emissions over time.\nwe want to help consumers and companies by providing an easy -----> tool !!!  to monitor activity of these devices and visualize the footpring they really have.\nwhat it does\nsusop provides a simple user interface that monitors activity in your network, classifies devices and calculates energy usage and the resulting co2 emission from statistical data by the umweltbundesamt.\nin the future it will run decentralized, sharing any old monitoring data through the network to newer devices, so we can reduce even the impact the monitoring would have.\nhow we built it\nfirst of all we researched power consumption of household devices that are connected to your network and during that we also looked into the actual co2 emissions we get from our energy consumption. after we gathered that data we began to build our app using java, there we created a devicescanner and recognizer for our network, with which we then monitored our network activity and uptime of these devices. we used this data with average wattage data to calculate emissions and power consumption.\nthe ui was created using javaswing concurrently, here we wanted to provide a very simple but visual tool for the user.\nchallenges we ran into\ngetting all the devices from your network and correctly classifying them can be pretty hard, especially if the device owner wants to only share limited data with you. we tried to classify the devices we tested with, as well as possible but sometimes you really only have an ip-adress and nothing else and then it is almost impossible to get a correct wattage estimation. we also had problems with getting and identifying the different mac addresses of devices, as this was the original plan to estimate power consumption. sadly even if we would have finished the implementation of this, we would at most have been able to gather information about the company from the mac address and not necessarily the device type itself.\naccomplishments that we're proud of\nwe created an easy to understand network monitoring tool, that could help lower co2 waste. we created a project that could be easily ported to different devices to further reduce the impact of our network.\nwhat we learned\nwe learned a lot about networks and how we can interact within our local networks. additionally we realized how much unnecessary power was drawn by simple and small devices in our own home and how much energy some devices really use.\nwhat's next for susop\nin the future susop would love to further develope the decentralized communication we started building during the hackathon but couldnt finish due to time constraints. we would also love to port it to different devices creating an even better and closer network. if possible wed also love to get more specific and accurate stats about energy consumption, either from the devices themselves, through running our software, or from smarthome integration.\nlast words\nuntil then! hear you soon and maybe watch your favourite show on your phone or laptop, instead of your tv next time.\nand to the companies, you really can turn off the lights and big presentation screens during the weekend if no-one is working ;) and if youre unsure about the impact that has, try susop!", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59502628}, {"Unnamed: 0": 2635, "autor": "Elegance", "date": null, "content": "Inspiration\nAccording to the problem statement, Machine Learning can be utilized to solve the problem of temperature fluctuations in microscopes. These fluctuations change the focus in the microscope, and it results in poor quality of images. This problem complicates the usage of confocal 4D imaging, which is applied in:\nhistopathology;\norgans and bones modelling;\nnerve cells and brain slices modelling;\nlive cell cultures observation;\nand many more important research fields... From the data we know that the temperature fluctuations can be very strong for example:\n(\"source_id\": MICDEV03, \"sensor_name\": \"LKM980_Main_Temperature_Outside\", \"datetime\": \"Jan 20, 2021, 15:05:17\", \"sensor_value\": 22.1) (\"source_id\": MICDEV03, \"sensor_name\": \"LKM980_Main_Temperature_Outside\", \"datetime\": \"Jan 20, 2021, 16:05:47\", \"sensor_value\": 23.3) According to \"LSM 980 with Airyscan 2\" specifications the temperature fluctuations should not be stronger as 0,5 degree per hour.\nWe decided to apply our knowledge in Machine Learning and Software Engineering to offer the domain experts a tool to visualize the temperature data as well as detect and predict temperature fluctuations. \"ELEGANCE\" allows us to minimize the impact of temperature fluctuations on the images quality.\nWhat it does\n\"ELEGANCE\" uses the data from the sensors and creates a User Interface around it, where a user can select which type of analysis or visualization he/she wants to perform. Our tool provides the following analytics and visualization methods:\n\"Interactive Visualization\": visualizes the sensor's data, can filter by \"Region\", \"Source ID\", \"Sensor Name\" and \"Date\".\n\"Group Visualization\": visualizes sensor's data of selected sensor type, by one-click on \"Source ID, Region\" the data will be excluded from the graph, by double-click on the selected \"Source ID, Region\" will stay the same.\n\"Anomaly Detection\": helps to find anomalies in the data. The method used for this task is Isolation Forests. Isolation forests, like Random Forests, one of the most widely used tree algorithms for machine learning, are generating several decision trees which are based on the data sample fed to the al-gorithm. Once the decision trees are generated, the next task for the isolation forests is to figure out the length of the path to \u2018isolate\u2019 an observation. The process of isolation of such observations is conducted by a random feature selection and, subsequently, a random split value range selection between the selected feature\u2019s minimum and maximum values [1]. Assuming there are n samples, the maximum depth of each tree in the forest is: ceil(log_2(n))\nIn our tool, it is possible to do anomaly detection by querying the arbitrary portion of the data. One can select: Source id, i.e. microscope, Sensor name, Contamination percentage, i.e. the expected ratio of the outlier data in the whole set Data range delta, Customized data period , and corresponding anomaly analysis for the desired is conducted on the go.\n\"Time Series Analysis\": performs a data prediction for selected devices. \"Time Series Analysis\": The tool used for this task is Fbprophet. In the core Fbprophet utilizes a an additive regression model that encompasses 4 main components in the data: \u2022 A piecewise linear or logistic growth curve trend. The changes in trends are being detected automatically by selecting changepoints from the data. \u2022 A yearly seasonal component modeled using Fourier series. \u2022 A weekly seasonal component using dummy variables. \u2022 A user-provided list of important holidays. These components are associated in the following way: y=a(t) + b(t) + c(t) + epsilon . Here a(t) represents the function of periodic changes, such as weekly seasonality, whereas b(t) represents the trend function, i.e. nonperiodic changes and represents the holiday effects. Lastly, the epsilon is the error term and represents the specific discrepancies that are not held by the model [2]. In our tool, it is possible to do anomaly detection by querying the arbitrary portion of the data. We exhibit how the data behave for the certain period of time, and provide the predictions as well as trends. One can select: \u2022 Source id, i.e. microscope, \u2022 Sensor name, \u2022 Number of days to predict \u2022 Customized data period , and corresponding time-series analysis for the desired query is conducted on the go. In Fbprophet, the model is being fit using the Stan. Stan performs the Maximum-a-Posteriori optimization for parameters extremely quickly (<1 second), gives us the option to estimate parameter uncertainty using the Hamiltonian Monte Carlo algorithm.\nTool is hosted on the Azure Web App inside a Docker container. The data is also stored inside an Azure Blob Data Lake Gen 2.\nHow we built it\nWe used a wide range of Python Machine Learning and Data Visualization libraries, such as \"plotly\" \"dash\", \"pandas\", \"matplotlib\", \"dash_bootstrap_components\", \"sklearn\". The tool is hosted on the Flask server and then containerized inside a Docker container. We follow Clean Code conventions to make code clean and understandable. The code was build inside a GitHub repository. We created to branches: \"main\" and \"develop\". So we were able constantly to have a working tool. On the Azure we created Resource Group, Container Registry and Web App to save and deploy our Docker containers. To make CI/CD automatic we created a GitHub action, which runs by push on \"main\", builds a Docker container and moves it to the Azure Container Registry. On Web App the \"Continuous Deployment\" is activated, so new container will be automatically deployed.\nChallenges we ran into\nFirstly, there are problems in understanding SM+SLM components interconnection. A clear construction scheme would be helpful.\nSecondly, nobody of us had worked with Azure platform and gitHub actions before, for this reason it took some hours until complete CI/CD flow worked properly.\nAccomplishments that we're proud of\nWe created an app with great User Interface and a wide range of analytic methods.\nWe created a great list of Hypotheses, which represent possible solution for temperature fluctuation problems. These hypotheses can be converted to the solution methods. These can be helpful for the ZEISS team.\nWe went deep inside the literature to have a better understanding of challenge domain, to get the interconnections between system (SM and SLM) components.\nWe established an automatic CI/CD Pipeline.\nDespite a rush development we followed Versioning and Clean Code conventions.\nWe did not have any problems with broken product.\nWhat we learned\nAzure Platform was absolutely new for all of us, during the HackTum we learned some basic of deployment and Machine Learning tools.\nWe learned how to do the GitHub actions and how to connect GitHub with Azure.\nWe learned a lot of theory basics in the field of microscopes measurements.\nWe learned a number of modern scientific approaches for the time-series data\nWhat's next for ELEGANCE\nThe next steps can be:\na development of Azure ML workflow, which collects and analyzes data in real time.\ncheck unproved hypotheses and develop solution methods according to them.\nREFERENCES-\n[1]. F. T. Liu, K. M. Ting, and Z.-H. Zhou, \u201cIsolation Forest,\u201d 2008 Eighth IEEE International Conference on Data Mining, 2008. [2]. S. J. Taylor and B. Letham, \u201cForecasting at scale,\u201d PeerJ Preprints, 27-Sep-2017. [Online]. Available:https://doi.org/10.7287/peerj.preprints.3190v2.", "link": "https://devpost.com/software/munichdortmund", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\naccording to the problem statement, machine learning can be utilized to solve the problem of temperature fluctuations in microscopes. these fluctuations change the focus in the microscope, and it results in poor quality of images. this problem complicates the usage of confocal 4d imaging, which is applied in:\nhistopathology;\norgans and bones modelling;\nnerve cells and brain slices modelling;\nlive cell cultures observation;\nand many more important research fields... from the data we know that the temperature fluctuations can be very strong for example:\n(\"source_id\": micdev03, \"sensor_name\": \"lkm980_main_temperature_outside\", \"datetime\": \"jan 20, 2021, 15:05:17\", \"sensor_value\": 22.1) (\"source_id\": micdev03, \"sensor_name\": \"lkm980_main_temperature_outside\", \"datetime\": \"jan 20, 2021, 16:05:47\", \"sensor_value\": 23.3) according to \"lsm 980 with airyscan 2\" specifications the temperature fluctuations should not be stronger as 0,5 degree per hour.\nwe decided to apply our knowledge in machine learning and software engineering to offer the domain experts a -----> tool !!!  to visualize the temperature data as well as detect and predict temperature fluctuations. \"elegance\" allows us to minimize the impact of temperature fluctuations on the images quality.\nwhat it does\n\"elegance\" uses the data from the sensors and creates a user interface around it, where a user can select which type of analysis or visualization he/she wants to perform. our tool provides the following analytics and visualization methods:\n\"interactive visualization\": visualizes the sensor's data, can filter by \"region\", \"source id\", \"sensor name\" and \"date\".\n\"group visualization\": visualizes sensor's data of selected sensor type, by one-click on \"source id, region\" the data will be excluded from the graph, by double-click on the selected \"source id, region\" will stay the same.\n\"anomaly detection\": helps to find anomalies in the data. the method used for this task is isolation forests. isolation forests, like random forests, one of the most widely used tree algorithms for machine learning, are generating several decision trees which are based on the data sample fed to the al-gorithm. once the decision trees are generated, the next task for the isolation forests is to figure out the length of the path to \u2018isolate\u2019 an observation. the process of isolation of such observations is conducted by a random feature selection and, subsequently, a random split value range selection between the selected feature\u2019s minimum and maximum values [1]. assuming there are n samples, the maximum depth of each tree in the forest is: ceil(log_2(n))\nin our tool, it is possible to do anomaly detection by querying the arbitrary portion of the data. one can select: source id, i.e. microscope, sensor name, contamination percentage, i.e. the expected ratio of the outlier data in the whole set data range delta, customized data period , and corresponding anomaly analysis for the desired is conducted on the go.\n\"time series analysis\": performs a data prediction for selected devices. \"time series analysis\": the tool used for this task is fbprophet. in the core fbprophet utilizes a an additive regression model that encompasses 4 main components in the data: \u2022 a piecewise linear or logistic growth curve trend. the changes in trends are being detected automatically by selecting changepoints from the data. \u2022 a yearly seasonal component modeled using fourier series. \u2022 a weekly seasonal component using dummy variables. \u2022 a user-provided list of important holidays. these components are associated in the following way: y=a(t) + b(t) + c(t) + epsilon . here a(t) represents the function of periodic changes, such as weekly seasonality, whereas b(t) represents the trend function, i.e. nonperiodic changes and represents the holiday effects. lastly, the epsilon is the error term and represents the specific discrepancies that are not held by the model [2]. in our tool, it is possible to do anomaly detection by querying the arbitrary portion of the data. we exhibit how the data behave for the certain period of time, and provide the predictions as well as trends. one can select: \u2022 source id, i.e. microscope, \u2022 sensor name, \u2022 number of days to predict \u2022 customized data period , and corresponding time-series analysis for the desired query is conducted on the go. in fbprophet, the model is being fit using the stan. stan performs the maximum-a-posteriori optimization for parameters extremely quickly (<1 second), gives us the option to estimate parameter uncertainty using the hamiltonian monte carlo algorithm.\ntool is hosted on the azure web app inside a docker container. the data is also stored inside an azure blob data lake gen 2.\nhow we built it\nwe used a wide range of python machine learning and data visualization libraries, such as \"plotly\" \"dash\", \"pandas\", \"matplotlib\", \"dash_bootstrap_components\", \"sklearn\". the tool is hosted on the flask server and then containerized inside a docker container. we follow clean code conventions to make code clean and understandable. the code was build inside a github repository. we created to branches: \"main\" and \"develop\". so we were able constantly to have a working tool. on the azure we created resource group, container registry and web app to save and deploy our docker containers. to make ci/cd automatic we created a github action, which runs by push on \"main\", builds a docker container and moves it to the azure container registry. on web app the \"continuous deployment\" is activated, so new container will be automatically deployed.\nchallenges we ran into\nfirstly, there are problems in understanding sm+slm components interconnection. a clear construction scheme would be helpful.\nsecondly, nobody of us had worked with azure platform and github actions before, for this reason it took some hours until complete ci/cd flow worked properly.\naccomplishments that we're proud of\nwe created an app with great user interface and a wide range of analytic methods.\nwe created a great list of hypotheses, which represent possible solution for temperature fluctuation problems. these hypotheses can be converted to the solution methods. these can be helpful for the zeiss team.\nwe went deep inside the literature to have a better understanding of challenge domain, to get the interconnections between system (sm and slm) components.\nwe established an automatic ci/cd pipeline.\ndespite a rush development we followed versioning and clean code conventions.\nwe did not have any problems with broken product.\nwhat we learned\nazure platform was absolutely new for all of us, during the hacktum we learned some basic of deployment and machine learning tools.\nwe learned how to do the github actions and how to connect github with azure.\nwe learned a lot of theory basics in the field of microscopes measurements.\nwe learned a number of modern scientific approaches for the time-series data\nwhat's next for elegance\nthe next steps can be:\na development of azure ml workflow, which collects and analyzes data in real time.\ncheck unproved hypotheses and develop solution methods according to them.\nreferences-\n[1]. f. t. liu, k. m. ting, and z.-h. zhou, \u201cisolation forest,\u201d 2008 eighth ieee international conference on data mining, 2008. [2]. s. j. taylor and b. letham, \u201cforecasting at scale,\u201d peerj preprints, 27-sep-2017. [online]. available:https://doi.org/10.7287/peerj.preprints.3190v2.", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59502635}, {"Unnamed: 0": 2639, "autor": "Datary", "date": null, "content": "Inspiration\nWe plan to promote the concept of citizen developers in the company. This helps non-tech employees automate their daily needs using low code platforms such as PowerBI.\nWhat it does\nThe tool or solution that we have developed supports data analysis by providing predictions, detecting anomalies, suggesting rationale for this anomaly, answering questions through a free text field ,as well as leveraging AI algorithms to generate decomposition trees to give a broader overview. An overview of the solution with gif images are also attached.\nBased on this overview on the dashboard itself, a short story of these statistics can be created and shared with other employees within the company. This helps save time and makes the process faster.\nHow we built it\nWe use PowerBI and Power Automate to do data analysis, anomaly detection, as well as automation of the data pipeline. For the video, we used an open-source editor on the dashboard which can create videos, as well as edit them. Once the video has been created it can be shared and stored securely with Sharepoint, Google Drive, and other data sharing services. It can also be published to other video sharing solutions such as Vimeo and Youtube.\nChallenges we ran into\nWe did not have a professional license, so we couldn't develop the video editor tool on the dashboard. Also, there were challenges analyzing the data because it is not clear what value x, y, and z from the columns meant. As an example, we had to translated the city id into an actual city name for our heat map.\nAccomplishments that we're proud of\nWe were able to draw insights to make actionable recommendations and make the video and dashboard easily consumable by upper management. A non-tech person can also easily add whatever additional feature they would like. The dashboard can be easily customizable to add further data on the retail sales. All these points address the main areas of the challenge that was provided to us by SAP.\nWhat we learned\nWe were targeting user-friendliness and maintainability for this project which is why we have chosen a low-code citizen developer technology. Since it's a hackathon, only a proof of concept needs to be developed in a short amount of time. We thought of using a solution that already exists and solves all the challenges, instead of reinventing the wheel. Therefore, we used PowerBI. In this process, we learned this tool in a short amount of time and how easy it is to develop with everything available. The concepts such as summary narratives, Q&A, AI decomposition trees, and a range of visualizations show or cover everything desired to understand the retail data.\nWhat's next for Datary\nWe would like to help SAP integrate the solution in their infrastructure or tenant and also provide support if needed.", "link": "https://devpost.com/software/sap-0ay6hp", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe plan to promote the concept of citizen developers in the company. this helps non-tech employees automate their daily needs using low code platforms such as powerbi.\nwhat it does\nthe -----> tool !!!  or solution that we have developed supports data analysis by providing predictions, detecting anomalies, suggesting rationale for this anomaly, answering questions through a free text field ,as well as leveraging ai algorithms to generate decomposition trees to give a broader overview. an overview of the solution with gif images are also attached.\nbased on this overview on the dashboard itself, a short story of these statistics can be created and shared with other employees within the company. this helps save time and makes the process faster.\nhow we built it\nwe use powerbi and power automate to do data analysis, anomaly detection, as well as automation of the data pipeline. for the video, we used an open-source editor on the dashboard which can create videos, as well as edit them. once the video has been created it can be shared and stored securely with sharepoint, google drive, and other data sharing services. it can also be published to other video sharing solutions such as vimeo and youtube.\nchallenges we ran into\nwe did not have a professional license, so we couldn't develop the video editor tool on the dashboard. also, there were challenges analyzing the data because it is not clear what value x, y, and z from the columns meant. as an example, we had to translated the city id into an actual city name for our heat map.\naccomplishments that we're proud of\nwe were able to draw insights to make actionable recommendations and make the video and dashboard easily consumable by upper management. a non-tech person can also easily add whatever additional feature they would like. the dashboard can be easily customizable to add further data on the retail sales. all these points address the main areas of the challenge that was provided to us by sap.\nwhat we learned\nwe were targeting user-friendliness and maintainability for this project which is why we have chosen a low-code citizen developer technology. since it's a hackathon, only a proof of concept needs to be developed in a short amount of time. we thought of using a solution that already exists and solves all the challenges, instead of reinventing the wheel. therefore, we used powerbi. in this process, we learned this tool in a short amount of time and how easy it is to develop with everything available. the concepts such as summary narratives, q&a, ai decomposition trees, and a range of visualizations show or cover everything desired to understand the retail data.\nwhat's next for datary\nwe would like to help sap integrate the solution in their infrastructure or tenant and also provide support if needed.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 1, "media": null, "medialink": null, "identifyer": 59502639}, {"Unnamed: 0": 2675, "autor": "Defender 3 Hack 2022 \u2013 Defender 3 Cheats Free Crystals", "date": null, "content": "Defender 3 Hack 2022 \u2013 Defender 3 Cheats Free Crystals\nWe have developed working Defender 3 hack that provide free crystals for each android or ios user that plays this game! Since you need crystals and coins to upgrade your gameplay and get free resources \u2013 you\u2019ve come to the right place. These Defender 3 Cheats are really easy to use. You just need to visit the tool page and follow the instructions below. You don\u2019t need to spend real money, on the contrary, all you need to do is enter your username, and type in Crystals 14,000 and you would get exactly that many crystals into your account. Later on, you would need to finish the procedure by completing a basic verification party and finishing a few simple tasks. Those tasks are also free and they include filling out a free survey or downloading a free game on your appropriate device, and in the end, refreshing the game. Simple as that! Don\u2019t forget to spread the word to your friends about this amazing Defender 3 cheat and repeat as many times you wish.", "link": "https://devpost.com/software/defender-3-hack-2022-defender-3-cheats-free-crystals", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "defender 3 hack 2022 \u2013 defender 3 cheats free crystals\nwe have developed working defender 3 hack that provide free crystals for each android or ios user that plays this game! since you need crystals and coins to upgrade your gameplay and get free resources \u2013 you\u2019ve come to the right place. these defender 3 cheats are really easy to use. you just need to visit the -----> tool !!!  page and follow the instructions below. you don\u2019t need to spend real money, on the contrary, all you need to do is enter your username, and type in crystals 14,000 and you would get exactly that many crystals into your account. later on, you would need to finish the procedure by completing a basic verification party and finishing a few simple tasks. those tasks are also free and they include filling out a free survey or downloading a free game on your appropriate device, and in the end, refreshing the game. simple as that! don\u2019t forget to spread the word to your friends about this amazing defender 3 cheat and repeat as many times you wish.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59502675}, {"Unnamed: 0": 2680, "autor": "Alive", "date": null, "content": "What's the problem?\nThe COVID-19 has not only shaken the entire human race but has also left us all pondering about the things we always had but never cherished. In this pandemic, many have lost jobs, many are stuck in distant lands and many have started feeling the pangs of isolation. Hopelessness has never dawned upon our human race like this before, and we are caught unprepared. Mental health that is the primary foundation of our existence is being so widely neglected that we need to rethink our needs as humans now.\nHow can technology help?\nA self-assessment test carefully created in the beginning will lead you through questions that will help us calculate your stress level and then we may recommend some therapy based on that. For this, we have a powerful recommendation system designed at the back end in which we hope to integrate machine learning in the near future.\nA Bot that can talk like a Friend and recommend something to help fight Depression and stress.\nThe idea\nwe offer an innovative stress management strategy that includes a mental well-being section (meditation/affirmations), physical health section (expertise/workouts), time management tool, reading section, music section and also a very chatty little robot 'Oliver with a twist that we have trained by using the IBM Watson. Trust us. our bot is a philosopher :)\nA self-assessment test carefully created in the beginning will lead you through questions that will help us calculate your stress level and then we may recommend some therapy based on that. For this, we have a powerful recommendation system designed at the back end in which we hope to integrate machine learning in the near future.\nShort description\nA solution for all the Mental Illness\nDemo video\nhttps://youtu.be/5AidP-2GsTk\nLong description\nMore details are available in the presentation submitted/link of google drive.\nGetting started These instructions will get you a copy of the project up and running on your local machine for development and testing purposes.\nPrerequisites\nWhat things do you need to install the software and how to install them?\ninstall Node.js install Angular CLI (version 9.1.7) Install .NET Development server Run ng serve for a dev server. Navigate tohttp://localhost:4200/`. The app will automatically reload if you change any of the source files.\nCode scaffolding\nRun ng generate component component-name to generate a new component. You can also use ng generate directive|pipe|service|class|guard|interface|enum|module.\nBuild\nRun ng build to build the project. The build artifacts will be stored in the dist/ directory. Use the --prod flag for a production build.\nPlease Check the branches for the UI & API code in the respitory.", "link": "https://devpost.com/software/alive-fkaj2g", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what's the problem?\nthe covid-19 has not only shaken the entire human race but has also left us all pondering about the things we always had but never cherished. in this pandemic, many have lost jobs, many are stuck in distant lands and many have started feeling the pangs of isolation. hopelessness has never dawned upon our human race like this before, and we are caught unprepared. mental health that is the primary foundation of our existence is being so widely neglected that we need to rethink our needs as humans now.\nhow can technology help?\na self-assessment test carefully created in the beginning will lead you through questions that will help us calculate your stress level and then we may recommend some therapy based on that. for this, we have a powerful recommendation system designed at the back end in which we hope to integrate machine learning in the near future.\na bot that can talk like a friend and recommend something to help fight depression and stress.\nthe idea\nwe offer an innovative stress management strategy that includes a mental well-being section (meditation/affirmations), physical health section (expertise/workouts), time management -----> tool !!! , reading section, music section and also a very chatty little robot 'oliver with a twist that we have trained by using the ibm watson. trust us. our bot is a philosopher :)\na self-assessment test carefully created in the beginning will lead you through questions that will help us calculate your stress level and then we may recommend some therapy based on that. for this, we have a powerful recommendation system designed at the back end in which we hope to integrate machine learning in the near future.\nshort description\na solution for all the mental illness\ndemo video\nhttps://youtu.be/5aidp-2gstk\nlong description\nmore details are available in the presentation submitted/link of google drive.\ngetting started these instructions will get you a copy of the project up and running on your local machine for development and testing purposes.\nprerequisites\nwhat things do you need to install the software and how to install them?\ninstall node.js install angular cli (version 9.1.7) install .net development server run ng serve for a dev server. navigate tohttp://localhost:4200/`. the app will automatically reload if you change any of the source files.\ncode scaffolding\nrun ng generate component component-name to generate a new component. you can also use ng generate directive|pipe|service|class|guard|interface|enum|module.\nbuild\nrun ng build to build the project. the build artifacts will be stored in the dist/ directory. use the --prod flag for a production build.\nplease check the branches for the ui & api code in the respitory.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59502680}, {"Unnamed: 0": 2694, "autor": "Tabou Stories Love Episodes Hack", "date": null, "content": "Tabou Stories Love Episodes Hack Redemption Codes mod 2022\nTabou Stories Love Episodes Hack\nOur Tabou Stories Love Episodes Hack has very simply interface to prodDiamonds and Keyse it an easy task to use. Our hacks are usually current and they're designed for all of iOS and Android devices.By having Unlimited Diamonds and Keys, you'll dominate the Tabou Stories Love Episodes game and win all challenges.This is really the key reason many top players in the overall game uses our tool. Flirt with celebrities, become famous, and find your perfect match in Matchmaker! In an urgent turn of events, you've ended to a favorite reality show. With contestants including a bad-boy singer, a favorite Hollywood actor, and your model ex, things are about to have hot! Have you been going to be always a flirt that wraps everyone around her finger to win the show, or have you been looking for true love? Have you been a great girl or perhaps a scandal queen? YOU choose what you would like to do! Tabou Stories: Love Episode is a game that is specifically created to give players many new experiences. Players can play many roles in this game and give these players. Many great experiences. If the player is looking for an interesting but less romantic love story, this game is an excellent game to miss. TAGS Tabou stories hack,tabou stories hack apk,tabou stories hack ios,tabou stories hack unlimited gems,tabou stories hack android,descargar tabou stories hack,tabou stories love episodes hack apk,hackear tabou stories,tabou stories hack apk,tabou stories hack ios,tabou stories hack unlimited gems,tabou stories hack android,descargar tabou stories hack,tabou stories love episodes hack apk,hackear tabou stories", "link": "https://devpost.com/software/tabou-stories-love-episodes-hack", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "tabou stories love episodes hack redemption codes mod 2022\ntabou stories love episodes hack\nour tabou stories love episodes hack has very simply interface to proddiamonds and keyse it an easy task to use. our hacks are usually current and they're designed for all of ios and android devices.by having unlimited diamonds and keys, you'll dominate the tabou stories love episodes game and win all challenges.this is really the key reason many top players in the overall game uses our -----> tool !!! . flirt with celebrities, become famous, and find your perfect match in matchmaker! in an urgent turn of events, you've ended to a favorite reality show. with contestants including a bad-boy singer, a favorite hollywood actor, and your model ex, things are about to have hot! have you been going to be always a flirt that wraps everyone around her finger to win the show, or have you been looking for true love? have you been a great girl or perhaps a scandal queen? you choose what you would like to do! tabou stories: love episode is a game that is specifically created to give players many new experiences. players can play many roles in this game and give these players. many great experiences. if the player is looking for an interesting but less romantic love story, this game is an excellent game to miss. tags tabou stories hack,tabou stories hack apk,tabou stories hack ios,tabou stories hack unlimited gems,tabou stories hack android,descargar tabou stories hack,tabou stories love episodes hack apk,hackear tabou stories,tabou stories hack apk,tabou stories hack ios,tabou stories hack unlimited gems,tabou stories hack android,descargar tabou stories hack,tabou stories love episodes hack apk,hackear tabou stories", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59502694}, {"Unnamed: 0": 2701, "autor": "Embrace", "date": null, "content": "Girls in Tech Mental Health for All Hackathon\nEmbrace MVP self-help tool for compassion, healing and strength.\nInspiration\nI transitioned careers in January 2020 - after working in digital media communications for 6 years, the close of 2019, my work environment had become toxic. I doubted my skills and despite building an organisation from 2 people to nine and from one country to two, I had lost confidence in myself. I knew I wanted different and enrolled for a Full Stack Development bootcamp in Barcelona Spain. In January of 2020 , I left my husband and 3 kids ( two teens and a pre teen) to go learn a new skill and find myself. I had come across the Artist's Way by Julia Cameron, this helped me journal my way back to believing in myself. I am now in Season 7 of a 12 week sprint, I am more confident and can process my thoughts and emotions in the written word and reading about other women who have succeed after had times and doubt. The application is therefore a companion for anyone who is after self care and also finding others in the same boat.\nWhat it does\nAllows a user to create a profile\nJournal their thoughts\nGive daily gratitude\nWrite and read other peoples fail-up stories\nChat with other persons like them\nHave daily meditations\nMeet new friends with common interests.\nGet professional help.\nHow we built it\nBroke down the journey to self belief into activities.\nDesigned the app wireframes.\nChallenges we ran into\nFinding out about the challenge by accident with only a couple of days left before the deadline.\nBuilding a mobile app with little experience in the technology used ( React Native)\nAccomplishments that we're proud of\nComing up with a concept based on experience.\nCreating the app wireframes\nWhat we learned\nAll it takes is putting in the work.\nUse boilerplate code to start project, it saves time and effort.\nWhat's next for Embrace\nCompleting the app MVP\nStyling the app for better UX\nCreating the DB\nTesting with users\nIncorporate education as part of the self help process because without knowledge and awareness change is not possible.", "link": "https://devpost.com/software/embrace-dgsmwh", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "girls in tech mental health for all hackathon\nembrace mvp self-help -----> tool !!!  for compassion, healing and strength.\ninspiration\ni transitioned careers in january 2020 - after working in digital media communications for 6 years, the close of 2019, my work environment had become toxic. i doubted my skills and despite building an organisation from 2 people to nine and from one country to two, i had lost confidence in myself. i knew i wanted different and enrolled for a full stack development bootcamp in barcelona spain. in january of 2020 , i left my husband and 3 kids ( two teens and a pre teen) to go learn a new skill and find myself. i had come across the artist's way by julia cameron, this helped me journal my way back to believing in myself. i am now in season 7 of a 12 week sprint, i am more confident and can process my thoughts and emotions in the written word and reading about other women who have succeed after had times and doubt. the application is therefore a companion for anyone who is after self care and also finding others in the same boat.\nwhat it does\nallows a user to create a profile\njournal their thoughts\ngive daily gratitude\nwrite and read other peoples fail-up stories\nchat with other persons like them\nhave daily meditations\nmeet new friends with common interests.\nget professional help.\nhow we built it\nbroke down the journey to self belief into activities.\ndesigned the app wireframes.\nchallenges we ran into\nfinding out about the challenge by accident with only a couple of days left before the deadline.\nbuilding a mobile app with little experience in the technology used ( react native)\naccomplishments that we're proud of\ncoming up with a concept based on experience.\ncreating the app wireframes\nwhat we learned\nall it takes is putting in the work.\nuse boilerplate code to start project, it saves time and effort.\nwhat's next for embrace\ncompleting the app mvp\nstyling the app for better ux\ncreating the db\ntesting with users\nincorporate education as part of the self help process because without knowledge and awareness change is not possible.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59502701}, {"Unnamed: 0": 2752, "autor": "Huddle in Jira - audio chat and screen sharing", "date": null, "content": "Inspiration\nI'm an avid user of communication tools like Discord or Slack. They allow me to talk to my team whenever we need that. Huddle in Jira enables this capability to Jira: we can talk about our work without losing the context.\nWhat it does\nHuddle in Jira is an audio chat solution added to Jira Cloud. You can start a conversation with a single click directly from Jira ticket: high priority support tickets, new user stories, bug reports, epics and more. Additionally it allows you to share you screen, so that you can present the problem to your team.\nHow we built it\nThe app is built on top of Atlassian Forge platform, and it is using Dolby.io for audio conferences and screen sharing. The app is hosted by Atlassian and additionally it connects to Dolby.io for token generation, and audio/video streaming. The app is using React and Typescript. You can develop it in a standalone mode, and deploy to Forge once you are done.\nChallenges we ran into\nWhile integrating Dolby.io was quite straightforward, the challenge was to fit it into Atlassian Forge platform, and overcome its limitations.\nAn example problem I've encountered was very strict content security policy in Forge, which conflicted with two lines of code in VoxeetSDK:\nnew Function(\"return this\")()\nnew Function(\"try {return this===window;} catch(e) {return false;}\")\nOther limitations imposed by the Atlassian platform include lack of access to display-capture, or preventing full screen. Workaround had to be applied to both problems, and this was the most time consuming part unfortunately.\nAccomplishments that we're proud of\nFirst of all: it works, but also I'm quite happy about the implementation, because it is always difficult to keep good quality of code, and focus on finishing the project at the same time. Typescript makes it much easier!\nI'm also proud to say that Dolby.io becomes yet another tool in my belt thanks to this project.\nWhat we learned\nI learned that Dolby.io is very easy to setup. VoxeetSDK is well documented, there are examples, the javascript module is enriched with Typescript and jsdoc.\nWhat's next for Huddle in Jira - audio chat and screen sharing\nI would like to keep it simple, but still there is plenty of room for improvements, and opportunities for new ideas, for example: recordings, real time chat, or transcripts. There is also a need to show a list of huddles across the project, or to retain the conference when transition between Jira issues.\nI will wait for user feedback, and decide what's next!", "link": "https://devpost.com/software/huddle-in-jira", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ni'm an avid user of communication tools like discord or slack. they allow me to talk to my team whenever we need that. huddle in jira enables this capability to jira: we can talk about our work without losing the context.\nwhat it does\nhuddle in jira is an audio chat solution added to jira cloud. you can start a conversation with a single click directly from jira ticket: high priority support tickets, new user stories, bug reports, epics and more. additionally it allows you to share you screen, so that you can present the problem to your team.\nhow we built it\nthe app is built on top of atlassian forge platform, and it is using dolby.io for audio conferences and screen sharing. the app is hosted by atlassian and additionally it connects to dolby.io for token generation, and audio/video streaming. the app is using react and typescript. you can develop it in a standalone mode, and deploy to forge once you are done.\nchallenges we ran into\nwhile integrating dolby.io was quite straightforward, the challenge was to fit it into atlassian forge platform, and overcome its limitations.\nan example problem i've encountered was very strict content security policy in forge, which conflicted with two lines of code in voxeetsdk:\nnew function(\"return this\")()\nnew function(\"try {return this===window;} catch(e) {return false;}\")\nother limitations imposed by the atlassian platform include lack of access to display-capture, or preventing full screen. workaround had to be applied to both problems, and this was the most time consuming part unfortunately.\naccomplishments that we're proud of\nfirst of all: it works, but also i'm quite happy about the implementation, because it is always difficult to keep good quality of code, and focus on finishing the project at the same time. typescript makes it much easier!\ni'm also proud to say that dolby.io becomes yet another -----> tool !!!  in my belt thanks to this project.\nwhat we learned\ni learned that dolby.io is very easy to setup. voxeetsdk is well documented, there are examples, the javascript module is enriched with typescript and jsdoc.\nwhat's next for huddle in jira - audio chat and screen sharing\ni would like to keep it simple, but still there is plenty of room for improvements, and opportunities for new ideas, for example: recordings, real time chat, or transcripts. there is also a need to show a list of huddles across the project, or to retain the conference when transition between jira issues.\ni will wait for user feedback, and decide what's next!", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59502752}, {"Unnamed: 0": 2760, "autor": "Cardiovascular Detection Web App", "date": null, "content": "Note\nThis is my personal Devpost of this project. For my team's official Devpost submitted to HackED Beta 2021, please click here: https://devpost.com/software/cardiovascular-detection-hackathon?ref_content=user-portfolio&ref_feature=in_progress\nInspiration\nOur group's goal was to create a project to achieve a better and more sustainable future for all! We wanted a web application that would ensure good health and well-being for everyone and everywhere. We further specialized our approach to solving this worldwide problem in one domain: cardiovascular diseases.\nWhat it does\nOur program trains a model using logistic regression using a set of data with the following variables:\nAge (in days) Biological sex at birth Height (in cm) Weight (in kg) Systolic blood pressure (in mmHg) Diastolic blood pressure (in mmHg) Cholesterol level (categorical; choices are \"normal\", \"above normal\", and \"well above normal\") Blood glucose level (categorical; choices are \"normal\", \"above normal\", and \"well above normal\") Whether or not the person smokes Whether or not the person drinks alcohol Whether or not the person exercises regularly Whether or not the person has heart disease The user can enter values for all of these variables (except the last one). Based on the values, the user is given a prediction for whether they have cardiovascular disease. Based on that prediction, the program recommends potential next steps with links to resources that a person can use to get potential help or get more information about cardiovascular disease.\nHow we built it\nBack end: The project relies on a machine learning model that uses a logistic regression algorithm. The medical dataset used in this project was obtained from Kaggle and contained various features of 70000 patients' health and a binary indicating whether or not they were each diagnosed with a cardiovascular disease. The pandas library was used to import the dataset and analyze it. Then, using the scikit-learn machine learning library, the dataset was split into training and testing sets. The data was also normalized with feature scaling to make it easier for the model to learn from the data trends. The model was then trained with the training datasets using a logistic regression algorithm. Front end: The front end of our project is a Django based web server. The project uses a basic HTML form to collect the users data, run it through our machine learning model and then direct the user to their appropriate results page. The result pages show transparency by showing the accuracy of the model and graphs representing our data. We want to make it clear to users that this is a prediction and not a diagnosis. We have also added external links to relevant articles as well as converting our Jupyter notebooks into HTML so that the more curious and technically literate users can analyze our process.\nChallenges we ran into\nFrom Django, Git, CSS and HTML, there was a big learning curve getting the technologies to work together smoothly. We spent numerous hours with mentors to push through these problems. There were also issues with the gender input as our dataset only had options for male and female. This does not align with our inclusive mission of providing easier healthcare resources to all individuals as it did not include an \"other\" option for gender. However, this was the only way we could train the machine learning engine effectively.\nAccomplishments that we're proud of\nWe're immensely proud that we were able to launch a web app that can effectively predict a disease as severe as cardiovascular disease. We also feel accomplished about the number of new technologies we learned in the creating process that we can utilize in future projects. Despite the challenges faced, this was a very fun project and allowed us to enjoy various new topics within Computing that my group members and I have never thought about or engaged in before!\nWhat we learned\nSince most of the technology we used for the first time during this hackathon, we learned a lot! Our team members learned Django, Git, CSS, and HTML for the very first time. As well, we spent several hours in the mentor voice channel to confirm our knowledge and debug with the software. In addition, we also learned to use Google Colab and how to create a logistical regression machine learning algorithm, and the difference between logistical and linear regression (a sigmoid function). We visualized the end results of the machine learning engine in attractive graphs using Matplotlib and Seaborn, another useful tool we learned!\nWhat's next for the Cardiovascular Detection Web App\nIn the future, we plan to implement more datasets and expand our web application to be able to detect more kinds of prevalent diseases! A few ones we have in mind are Parkinson's disease, diabetes, types of cancer, etc. We also plan to implement a main menu where the user can navigate through which various diseases they plan to detect and immediately get resources that will help serve them. We hope our future additions to this project can contribute towards our bigger goal of achieving a better and more sustainable future for us all.\nCardiovascular Disease Resource List\nHeart & Stroke Foundation \u2014 Cardiovascular Disease https://www.heartandstroke.ca/heart-disease/conditions?gclid=Cj0KCQiA4b2MBhD2ARIsAIrcB-TsaYUb9CNtBzcLNf5bJLNPSQCs5AobQaq-51Y3kTFuvnqhD_FEdWEaAvnEEALw_wcB&gclsrc=aw.ds\nPublic Health Agency of Canada \u2014 Government of Canada https://www.canada.ca/en/public-health/services/publications/diseases-conditions/heart-disease-canada.html\nCardiovascular Disease Patient Resources \u2014 Alberta Health Services https://www.albertahealthservices.ca/info/Page7735.aspx\nPatient Information Resource \u2014 How Secondary Protection Can Keep You Healthy https://thrombosiscanada.ca/secondary-prevention-info/?gclid=Cj0KCQiA4b2MBhD2ARIsAIrcB-S6DlDrm-Nhc7ZfA9PeK2VugwY0Pdhh9ROqUOrNaWfVerlJ5fGt9wEaAhJwEALw_wcB\nCardiovascular Disease \u2014 Public Health Ontario https://www.publichealthontario.ca/en/diseases-and-conditions/chronic-diseases-and-conditions/cardiovascular-disease\nHeart Disease Patient Education \u2014 Centers for Disease Control and Prevention https://www.cdc.gov/heartdisease/materials_for_patients.htm", "link": "https://devpost.com/software/personal-cardiovascular-detection-web-app", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "note\nthis is my personal devpost of this project. for my team's official devpost submitted to hacked beta 2021, please click here: https://devpost.com/software/cardiovascular-detection-hackathon?ref_content=user-portfolio&ref_feature=in_progress\ninspiration\nour group's goal was to create a project to achieve a better and more sustainable future for all! we wanted a web application that would ensure good health and well-being for everyone and everywhere. we further specialized our approach to solving this worldwide problem in one domain: cardiovascular diseases.\nwhat it does\nour program trains a model using logistic regression using a set of data with the following variables:\nage (in days) biological sex at birth height (in cm) weight (in kg) systolic blood pressure (in mmhg) diastolic blood pressure (in mmhg) cholesterol level (categorical; choices are \"normal\", \"above normal\", and \"well above normal\") blood glucose level (categorical; choices are \"normal\", \"above normal\", and \"well above normal\") whether or not the person smokes whether or not the person drinks alcohol whether or not the person exercises regularly whether or not the person has heart disease the user can enter values for all of these variables (except the last one). based on the values, the user is given a prediction for whether they have cardiovascular disease. based on that prediction, the program recommends potential next steps with links to resources that a person can use to get potential help or get more information about cardiovascular disease.\nhow we built it\nback end: the project relies on a machine learning model that uses a logistic regression algorithm. the medical dataset used in this project was obtained from kaggle and contained various features of 70000 patients' health and a binary indicating whether or not they were each diagnosed with a cardiovascular disease. the pandas library was used to import the dataset and analyze it. then, using the scikit-learn machine learning library, the dataset was split into training and testing sets. the data was also normalized with feature scaling to make it easier for the model to learn from the data trends. the model was then trained with the training datasets using a logistic regression algorithm. front end: the front end of our project is a django based web server. the project uses a basic html form to collect the users data, run it through our machine learning model and then direct the user to their appropriate results page. the result pages show transparency by showing the accuracy of the model and graphs representing our data. we want to make it clear to users that this is a prediction and not a diagnosis. we have also added external links to relevant articles as well as converting our jupyter notebooks into html so that the more curious and technically literate users can analyze our process.\nchallenges we ran into\nfrom django, git, css and html, there was a big learning curve getting the technologies to work together smoothly. we spent numerous hours with mentors to push through these problems. there were also issues with the gender input as our dataset only had options for male and female. this does not align with our inclusive mission of providing easier healthcare resources to all individuals as it did not include an \"other\" option for gender. however, this was the only way we could train the machine learning engine effectively.\naccomplishments that we're proud of\nwe're immensely proud that we were able to launch a web app that can effectively predict a disease as severe as cardiovascular disease. we also feel accomplished about the number of new technologies we learned in the creating process that we can utilize in future projects. despite the challenges faced, this was a very fun project and allowed us to enjoy various new topics within computing that my group members and i have never thought about or engaged in before!\nwhat we learned\nsince most of the technology we used for the first time during this hackathon, we learned a lot! our team members learned django, git, css, and html for the very first time. as well, we spent several hours in the mentor voice channel to confirm our knowledge and debug with the software. in addition, we also learned to use google colab and how to create a logistical regression machine learning algorithm, and the difference between logistical and linear regression (a sigmoid function). we visualized the end results of the machine learning engine in attractive graphs using matplotlib and seaborn, another useful -----> tool !!!  we learned!\nwhat's next for the cardiovascular detection web app\nin the future, we plan to implement more datasets and expand our web application to be able to detect more kinds of prevalent diseases! a few ones we have in mind are parkinson's disease, diabetes, types of cancer, etc. we also plan to implement a main menu where the user can navigate through which various diseases they plan to detect and immediately get resources that will help serve them. we hope our future additions to this project can contribute towards our bigger goal of achieving a better and more sustainable future for us all.\ncardiovascular disease resource list\nheart & stroke foundation \u2014 cardiovascular disease https://www.heartandstroke.ca/heart-disease/conditions?gclid=cj0kcqia4b2mbhd2arisaircb-tsayub9cntbzclnf5bjlnpsqcs5aobqaq-51y3ktfuvnqhd_fedweaavneealw_wcb&gclsrc=aw.ds\npublic health agency of canada \u2014 government of canada https://www.canada.ca/en/public-health/services/publications/diseases-conditions/heart-disease-canada.html\ncardiovascular disease patient resources \u2014 alberta health services https://www.albertahealthservices.ca/info/page7735.aspx\npatient information resource \u2014 how secondary protection can keep you healthy https://thrombosiscanada.ca/secondary-prevention-info/?gclid=cj0kcqia4b2mbhd2arisaircb-s6dldrm-nhc7zfa9pek2vugwy0pdhh9roquornawfverlj5fgt9weaahjwealw_wcb\ncardiovascular disease \u2014 public health ontario https://www.publichealthontario.ca/en/diseases-and-conditions/chronic-diseases-and-conditions/cardiovascular-disease\nheart disease patient education \u2014 centers for disease control and prevention https://www.cdc.gov/heartdisease/materials_for_patients.htm", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59502760}, {"Unnamed: 0": 2775, "autor": "Sheet2NFT", "date": null, "content": "Inspiration\nCreating an NFT without knowing how to code can be a daunting task so decided to simplify the process by creating this tool.\nWhat it does\nSheet2NFT is a no-code tool that I built with the help of Vue, Node.js, Google Sheets, Chainlink, Moralis, IPFS, Web3.Storage, and Filecoin, that allows users to easily create NFTs assets without knowing how to code, the only thing left for the user to do after using this tool is to deploy the NFT contract. Sheet2NFT will upload the images to IPFS, generate and upload the metadata to IPFS, generate an ERC-1155 contract, insert the metadata URI and upload it to IPFS.\nHow we built it\nHere is how I am using Vue, Node.js, Google Sheets, Chainlink, Moralis, IPFs, Web3.Storage, and Filecoin : Vue is being used in the frontend application, and Node.js in the backend application.\nGoogle sheets is being used to input and store the information needed to create an NFT (images, name, description).\nChainlink is being used inside a smart contract ( Kovan network, the hash is 0x3f8FfF4EC4a949A72B29c4Df6385d4e9F443BF61) to send an HTTP GET request containing the sheetId and projectId (project stored in Moralis database) to a server that will convert the contents of the sheet to NFT assets. Once the server receives the request, first, it will retrieve the contents of the provided sheet. After that, it will download and store the images in a temporary folder. After downloading the images, it will use Moralis to upload the images to IPFS and store the URI. After retrieving the images URI, it will generate a metadata file for each item found in the Google Sheet and insert the corresponding image URI. After generating all metadata files, it will use Moralis to upload them to IPFS and store the URI. After retrieving the metadata URI, it will generate an ERC-1155 contract file, insert the metadata URI, use Moralis to upload the contract to IPFS and store the contract URI. Finally, the server will take the projectId, images, metadata, and contract URI and update the project stored in the Moralis database.\nMoralis authentication is being used to allow users to use their Ethereum account (Kovan network only) and Metamask to log in to Sheet2NFT and call the smart contract responsible for converting the data in the Google Sheet. Moralis databases are being used to store projects containing NFT data such as images URI and metadata URI. Moralis IPFS storage is being used to store the images, metadata and the generated NFT contract file containing the metadata URI. I am also using Moralis to call the Chainlink smart contract responsible for converting the data in the sheet to NFT assets\nWeb3.Storage which is built on top of Filecoin is being used as an alternative free IPFS storage service provider\nPrerequisites\nIn order to use Sheet2NFT you will need the following:\nAn Ethereum test net Kovan account.\nImages stored in your Google Drive. The images read permission should be set to allow anyone with a link to download them.\nChallenges we ran into\nI had no experience with solidity smart contracts, Chainlink, Moralis, web3.storage before this hackathon.\nAccomplishments that we're proud of\nI have managed to build a tool that will reduce the barrier of entry for those interested in creating their NFT.\nWhat we learned\nI learned the basics of how to write smart contracts;\nI learned how to use Chainlink to call an API from a smart contract;\nI learned how to use Moralis to authenticate users, call smart contracts, store data in databases, and store assets in IPFS.\nI learned how to use Web3.Storage to store data in IPFS.\nWhat's next for Sheet2NFT\nAllow users to deploy and manage their NFTS without leaving the site, because right now the user has to use something like Remix to do that.", "link": "https://devpost.com/software/sheet2nft", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ncreating an nft without knowing how to code can be a daunting task so decided to simplify the process by creating this -----> tool !!! .\nwhat it does\nsheet2nft is a no-code tool that i built with the help of vue, node.js, google sheets, chainlink, moralis, ipfs, web3.storage, and filecoin, that allows users to easily create nfts assets without knowing how to code, the only thing left for the user to do after using this tool is to deploy the nft contract. sheet2nft will upload the images to ipfs, generate and upload the metadata to ipfs, generate an erc-1155 contract, insert the metadata uri and upload it to ipfs.\nhow we built it\nhere is how i am using vue, node.js, google sheets, chainlink, moralis, ipfs, web3.storage, and filecoin : vue is being used in the frontend application, and node.js in the backend application.\ngoogle sheets is being used to input and store the information needed to create an nft (images, name, description).\nchainlink is being used inside a smart contract ( kovan network, the hash is 0x3f8fff4ec4a949a72b29c4df6385d4e9f443bf61) to send an http get request containing the sheetid and projectid (project stored in moralis database) to a server that will convert the contents of the sheet to nft assets. once the server receives the request, first, it will retrieve the contents of the provided sheet. after that, it will download and store the images in a temporary folder. after downloading the images, it will use moralis to upload the images to ipfs and store the uri. after retrieving the images uri, it will generate a metadata file for each item found in the google sheet and insert the corresponding image uri. after generating all metadata files, it will use moralis to upload them to ipfs and store the uri. after retrieving the metadata uri, it will generate an erc-1155 contract file, insert the metadata uri, use moralis to upload the contract to ipfs and store the contract uri. finally, the server will take the projectid, images, metadata, and contract uri and update the project stored in the moralis database.\nmoralis authentication is being used to allow users to use their ethereum account (kovan network only) and metamask to log in to sheet2nft and call the smart contract responsible for converting the data in the google sheet. moralis databases are being used to store projects containing nft data such as images uri and metadata uri. moralis ipfs storage is being used to store the images, metadata and the generated nft contract file containing the metadata uri. i am also using moralis to call the chainlink smart contract responsible for converting the data in the sheet to nft assets\nweb3.storage which is built on top of filecoin is being used as an alternative free ipfs storage service provider\nprerequisites\nin order to use sheet2nft you will need the following:\nan ethereum test net kovan account.\nimages stored in your google drive. the images read permission should be set to allow anyone with a link to download them.\nchallenges we ran into\ni had no experience with solidity smart contracts, chainlink, moralis, web3.storage before this hackathon.\naccomplishments that we're proud of\ni have managed to build a tool that will reduce the barrier of entry for those interested in creating their nft.\nwhat we learned\ni learned the basics of how to write smart contracts;\ni learned how to use chainlink to call an api from a smart contract;\ni learned how to use moralis to authenticate users, call smart contracts, store data in databases, and store assets in ipfs.\ni learned how to use web3.storage to store data in ipfs.\nwhat's next for sheet2nft\nallow users to deploy and manage their nfts without leaving the site, because right now the user has to use something like remix to do that.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59502775}, {"Unnamed: 0": 2794, "autor": "Airgram", "date": null, "content": "Inspiration\nA few years ago, while working for a tech giant, I was exhausted by the work environment - tedious meetings packed my daily routine. I felt powerless when going through one meeting after another. After a while, I attended a meeting that was perfectly scheduled. In the meeting, every agenda topic was fully discussed within the given time, and everybody left with valuable information. I just realized that \u201cmeeting\u201d doesn\u2019t necessarily need to be the way it was. Well-run meetings should be well scheduled, time-efficient, and make every attendee happy when the meetings end. So I started building up Airgram with my friends in late 2020, hoping to take online meeting productivity to the next level.\nWhat it does\nAirgram is a meeting productivity tool that records, transcribes and documents your Google Meet & Zoom calls. With Airgram, you can experience less distraction and get more productivity: \ud83d\udd34 Live transcription for Google Meet & Zoom \u2705 Speaker detection \ud83d\udcdd Collaborative notes w/ timestamps \ud83c\udfa5 Video recording (1080p) \ud83d\udc68\u200d\ud83d\udcbb Chrome extension for Google Meet \ud83d\udce4 Google Calendar, Slack integration\nHow Airgram solves the problems of your meetings (take Google Meet as an example):\nBefore a meeting: Download Airgram extension to create agenda for any event in your Google Calendar. In just a few steps, you can lead your team to an organized and purposeful discussion. And when it's time to start a Google Meet call, initiate the recording and transcription using the extension, too.\nDuring a meeting: Stay focused on the meeting without worrying about taking notes by yourself. Whenever someone is speaking, you can see the transcript appear on the Airgram web app instantly. Collaborate on taking notes with other guests via the powerful notepad. Everybody in the meeting can work on notes and action items together with the extension. All highlights and next steps are auto-synced with your Airgram Web account.\nAfter a meeting: Playback HD Video recording to review the meeting details. The resolution is 1920*1080p, clear enough for you to see the picture and texts of other people's screen projections. And if you add specific notes from the transcript, they will be associated with the relevant transcript and the video. Then you can quickly locate any part of the conversations as you like. Share to Slack. Everyone invited to view the meeting on Slack is free to edit the shared notes.\nWhat's next for Airgram\nOur current short-term goal is 1. Optimize the product based on users' feedback. 2. Improve the retention rate, user usage and accomplish Product-Marketing-Fit.", "link": "https://devpost.com/software/airgram", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\na few years ago, while working for a tech giant, i was exhausted by the work environment - tedious meetings packed my daily routine. i felt powerless when going through one meeting after another. after a while, i attended a meeting that was perfectly scheduled. in the meeting, every agenda topic was fully discussed within the given time, and everybody left with valuable information. i just realized that \u201cmeeting\u201d doesn\u2019t necessarily need to be the way it was. well-run meetings should be well scheduled, time-efficient, and make every attendee happy when the meetings end. so i started building up airgram with my friends in late 2020, hoping to take online meeting productivity to the next level.\nwhat it does\nairgram is a meeting productivity -----> tool !!!  that records, transcribes and documents your google meet & zoom calls. with airgram, you can experience less distraction and get more productivity: \ud83d\udd34 live transcription for google meet & zoom \u2705 speaker detection \ud83d\udcdd collaborative notes w/ timestamps \ud83c\udfa5 video recording (1080p) \ud83d\udc68\u200d\ud83d\udcbb chrome extension for google meet \ud83d\udce4 google calendar, slack integration\nhow airgram solves the problems of your meetings (take google meet as an example):\nbefore a meeting: download airgram extension to create agenda for any event in your google calendar. in just a few steps, you can lead your team to an organized and purposeful discussion. and when it's time to start a google meet call, initiate the recording and transcription using the extension, too.\nduring a meeting: stay focused on the meeting without worrying about taking notes by yourself. whenever someone is speaking, you can see the transcript appear on the airgram web app instantly. collaborate on taking notes with other guests via the powerful notepad. everybody in the meeting can work on notes and action items together with the extension. all highlights and next steps are auto-synced with your airgram web account.\nafter a meeting: playback hd video recording to review the meeting details. the resolution is 1920*1080p, clear enough for you to see the picture and texts of other people's screen projections. and if you add specific notes from the transcript, they will be associated with the relevant transcript and the video. then you can quickly locate any part of the conversations as you like. share to slack. everyone invited to view the meeting on slack is free to edit the shared notes.\nwhat's next for airgram\nour current short-term goal is 1. optimize the product based on users' feedback. 2. improve the retention rate, user usage and accomplish product-marketing-fit.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59502794}, {"Unnamed: 0": 2820, "autor": "SocioPhin", "date": null, "content": "Inspiration\nWith over 3 billion active users, social media is a platform where consumers spend time every day conversing to one another about their likes, dislikes, and interests. Every business, from SMEs to large corporations, needs a social media plan to broaden their reach, raise brand awareness, attract new consumers, and engage existing customers with valuable content.\nWhat it does\nSocioPhin integrates social media platforms with customer relationship management (CRM) systems to provide insight into customer interactions with a brand, and to improve the quality of customer engagement To create powerful social network management for organizations to attract the right users leveraging technology at its best. As a result of our integration with Facebook and LinkedIn, we can provide our users with many benefits, including Direct Messages across social networks, and users are able to post and view their posts, not to mention view and reply to their comments.\nHow we built it\nFront end: For the UI part we have used Bootstrap, HTML and CSS which is then converted into react components. Facebook graph APIs: We have used the Facebook developer graph tool for getting the endpoints for messenger and posts. Front end - backend integration: We have used Axios which is an NPM package for calling APIs and after that, we have used a few state management hooks provided by react.js i.e useState, useEffect and a very popular package Recoil for storing global states. Login: For the login part I used Facebook SDK. In the login method I set all the required permissions for the access_key and then storing the pageid and access_key in a recoil global state. Messenger: Messenger part was simple for integration, made the state management with props and react hooks. The API endpoints i.e the access_key and pageid were fetched from the global state. Posts: For the posts, the state management is the same as the messenger one. The only tricky part was the mapping of comments and replies but I managed to do that using nested maps which worked well. Backend: For backend we have used AWS RDS mySQL database and Node js to create APIs for different operations and S3 bucket to store the files. We also created a scheduling operation with Cron Node.js which triggers every second to check for queries matching the timestamp and sends the email to all the user if matches. We developed a backend with node.js and twitter v2 apis to get all the required apis for twitter endpoints. Deployment: We created a serverless architecture with AWS lambda, AWS API gateway and AWS S3 bucket. Demo Site: SocioPhin\nChallenges we ran into\nIn spite of many challenges faced during the project, we were able to come up with the most suitable solutions, some of which related to Facebook graph API permission errors and Cors policy errors when integrating our APIs to the front-end.\nAccomplishments that we're proud of\nIntegration with multiple social media platforms.\nReplies to tags and comments from posts.\nSecure code and architecture with mobile encryption\nSent cold mails to folowers with the automated mail service.\nPost content and analyze their reach on different platforms\nWhat we learned\nWe learned about facebook graph api's and developing facebook app.\nTwitter V1 and V2 Api's\nServerless Architecture\nWhat's next for SocioPhin\nAd Campaigns Online business listings, geo-tagging, and ads are some of the many things we help businesses get started with.\nCustomer acquisition Get to know your customers intersests and locations to help you channelize your content for better reach\nSocial Media Followers Gain social media followers by using powerful tools in automation and analytics.", "link": "https://devpost.com/software/sociophin", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwith over 3 billion active users, social media is a platform where consumers spend time every day conversing to one another about their likes, dislikes, and interests. every business, from smes to large corporations, needs a social media plan to broaden their reach, raise brand awareness, attract new consumers, and engage existing customers with valuable content.\nwhat it does\nsociophin integrates social media platforms with customer relationship management (crm) systems to provide insight into customer interactions with a brand, and to improve the quality of customer engagement to create powerful social network management for organizations to attract the right users leveraging technology at its best. as a result of our integration with facebook and linkedin, we can provide our users with many benefits, including direct messages across social networks, and users are able to post and view their posts, not to mention view and reply to their comments.\nhow we built it\nfront end: for the ui part we have used bootstrap, html and css which is then converted into react components. facebook graph apis: we have used the facebook developer graph -----> tool !!!  for getting the endpoints for messenger and posts. front end - backend integration: we have used axios which is an npm package for calling apis and after that, we have used a few state management hooks provided by react.js i.e usestate, useeffect and a very popular package recoil for storing global states. login: for the login part i used facebook sdk. in the login method i set all the required permissions for the access_key and then storing the pageid and access_key in a recoil global state. messenger: messenger part was simple for integration, made the state management with props and react hooks. the api endpoints i.e the access_key and pageid were fetched from the global state. posts: for the posts, the state management is the same as the messenger one. the only tricky part was the mapping of comments and replies but i managed to do that using nested maps which worked well. backend: for backend we have used aws rds mysql database and node js to create apis for different operations and s3 bucket to store the files. we also created a scheduling operation with cron node.js which triggers every second to check for queries matching the timestamp and sends the email to all the user if matches. we developed a backend with node.js and twitter v2 apis to get all the required apis for twitter endpoints. deployment: we created a serverless architecture with aws lambda, aws api gateway and aws s3 bucket. demo site: sociophin\nchallenges we ran into\nin spite of many challenges faced during the project, we were able to come up with the most suitable solutions, some of which related to facebook graph api permission errors and cors policy errors when integrating our apis to the front-end.\naccomplishments that we're proud of\nintegration with multiple social media platforms.\nreplies to tags and comments from posts.\nsecure code and architecture with mobile encryption\nsent cold mails to folowers with the automated mail service.\npost content and analyze their reach on different platforms\nwhat we learned\nwe learned about facebook graph api's and developing facebook app.\ntwitter v1 and v2 api's\nserverless architecture\nwhat's next for sociophin\nad campaigns online business listings, geo-tagging, and ads are some of the many things we help businesses get started with.\ncustomer acquisition get to know your customers intersests and locations to help you channelize your content for better reach\nsocial media followers gain social media followers by using powerful tools in automation and analytics.", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59502820}, {"Unnamed: 0": 2861, "autor": "Lumi Chords", "date": null, "content": "Inspiration\nInspiration in this idea\nI was inspired by the Guitar as an instrument. It is one of the most played instruments in the world. When I practiced playing the guitar myself, I encountered various problems such as forgetting chords, can't change chords in time then when I go to find solutions whether it's in the app or the device. It doesn't have a device or app that can fix the problems I'm having. That\u2019s why me and my friend came up with this idea to solve our pain point. Moreover, I want people to access it more easily and want to create an enjoyable experience for newbies trying to learn it.\nInspiration in this AI\nThis AI specifically is inspired by the online classes provided by a number of providers. It could be seen that those classes aren\u2019t that effective if the sounds played by the learners can be analyzed to find the mistakes in the playing process. This could be an effective learning tool for this instrument.\nWhat it does\n*What can this AI do *\nBasically, this AI can distinguish the sound of a song playing and tell what chord chances it will be. This AI can distinguish up to 12 chords, C, C\u266f, D, D\u266f, E , F, F\u266f, G, G\u266f, A, A\u266f, B.\nThe purpose\nThis AI can be used in analyzing and creating online lessons or analyzing the mistakes in the song played by the user. Our main project is a software that provides lessons and trains our clients. The AI could help in determining the chords from the songs and speed up the lesson creation process.\nStep-by-step\nUpload the audio file into librosa in type \u201c.wav\u201d\nAfter that change the time domain into \u201cchroma\u201d by using Short Time Fourier Transform (STFT), then change the numpy array to only mean value. The reason why we have to change it into mean value is to make the size of the chroma feature smaller. If it\u2019s a default value we need to record the sound at exactly 5 seconds which is too hard for people to do it. Then arrange to insert it in a data frame and convert into .csv and use .csv to train AI.\nAfter the data preprocessing we will use the processed data to train the AI in 20 epochs which concludes the training part. Then In the functionality part, after we input a .wav file it goes through 2 layers of dense neural network with 32 & 7 nodes. The first layer uses relu activator, and the second layer uses sigmoid to get a value of 0-1 to convert it into percentage. The AI then breaks the file into 32 then 7 nodes to get the seven chords then print the one with the highest probability according to the sigmoid activator.\nHow we built it\nWhat we\u2019ve used to create this AI\nFirstly, we don't have our own dataset, so in order to start we search for a dataset in each chord. We have decided to use AI to determine only 7 chords but the data set has 12 chords. Then we used librosa to convert the files to frequency domain by using Short time Fournier's Transform(STFT). After that we use numpy arrays to convert all that datas into an average value for easier usage. Then put all those datas into a data frame, convert the files into .csv. and use the files to train the AI.\nChallenges we ran into\nChallenges we faced\nDue to not having much background on coding and computer programming we have been struggling with this project quite a lot. Plus, we don\u2019t have our own dataset, and the data set we found needed to be converted to usable types of data before being used with machine learning. Moreover, with the sound files there is a concern of noises which have to be removed.\nHow we overcome the challenges\nWe have enlisted our friend\u2019s help from multiple faculties including engineering and computer science to help guide us in this project. As for the data collection, we have to go to a musical database to find it.\nAccomplishments that we're proud of\nWe\u2019re proud of our accomplishment from creating this project and gaining increased knowledge in an unfamiliar field.\nWhat we learned\nWe\u2019ve learned alot about datas, musical theory, artificial intelligence and neural networks in order to do this project\nWhat's next for Lumichords\nWe will try to do a real time analysis next, since that's our current limitation. If a real time analysis could be made more opportunities follow including voice recognition and more.", "link": "https://devpost.com/software/lumichords", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ninspiration in this idea\ni was inspired by the guitar as an instrument. it is one of the most played instruments in the world. when i practiced playing the guitar myself, i encountered various problems such as forgetting chords, can't change chords in time then when i go to find solutions whether it's in the app or the device. it doesn't have a device or app that can fix the problems i'm having. that\u2019s why me and my friend came up with this idea to solve our pain point. moreover, i want people to access it more easily and want to create an enjoyable experience for newbies trying to learn it.\ninspiration in this ai\nthis ai specifically is inspired by the online classes provided by a number of providers. it could be seen that those classes aren\u2019t that effective if the sounds played by the learners can be analyzed to find the mistakes in the playing process. this could be an effective learning -----> tool !!!  for this instrument.\nwhat it does\n*what can this ai do *\nbasically, this ai can distinguish the sound of a song playing and tell what chord chances it will be. this ai can distinguish up to 12 chords, c, c\u266f, d, d\u266f, e , f, f\u266f, g, g\u266f, a, a\u266f, b.\nthe purpose\nthis ai can be used in analyzing and creating online lessons or analyzing the mistakes in the song played by the user. our main project is a software that provides lessons and trains our clients. the ai could help in determining the chords from the songs and speed up the lesson creation process.\nstep-by-step\nupload the audio file into librosa in type \u201c.wav\u201d\nafter that change the time domain into \u201cchroma\u201d by using short time fourier transform (stft), then change the numpy array to only mean value. the reason why we have to change it into mean value is to make the size of the chroma feature smaller. if it\u2019s a default value we need to record the sound at exactly 5 seconds which is too hard for people to do it. then arrange to insert it in a data frame and convert into .csv and use .csv to train ai.\nafter the data preprocessing we will use the processed data to train the ai in 20 epochs which concludes the training part. then in the functionality part, after we input a .wav file it goes through 2 layers of dense neural network with 32 & 7 nodes. the first layer uses relu activator, and the second layer uses sigmoid to get a value of 0-1 to convert it into percentage. the ai then breaks the file into 32 then 7 nodes to get the seven chords then print the one with the highest probability according to the sigmoid activator.\nhow we built it\nwhat we\u2019ve used to create this ai\nfirstly, we don't have our own dataset, so in order to start we search for a dataset in each chord. we have decided to use ai to determine only 7 chords but the data set has 12 chords. then we used librosa to convert the files to frequency domain by using short time fournier's transform(stft). after that we use numpy arrays to convert all that datas into an average value for easier usage. then put all those datas into a data frame, convert the files into .csv. and use the files to train the ai.\nchallenges we ran into\nchallenges we faced\ndue to not having much background on coding and computer programming we have been struggling with this project quite a lot. plus, we don\u2019t have our own dataset, and the data set we found needed to be converted to usable types of data before being used with machine learning. moreover, with the sound files there is a concern of noises which have to be removed.\nhow we overcome the challenges\nwe have enlisted our friend\u2019s help from multiple faculties including engineering and computer science to help guide us in this project. as for the data collection, we have to go to a musical database to find it.\naccomplishments that we're proud of\nwe\u2019re proud of our accomplishment from creating this project and gaining increased knowledge in an unfamiliar field.\nwhat we learned\nwe\u2019ve learned alot about datas, musical theory, artificial intelligence and neural networks in order to do this project\nwhat's next for lumichords\nwe will try to do a real time analysis next, since that's our current limitation. if a real time analysis could be made more opportunities follow including voice recognition and more.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59502861}, {"Unnamed: 0": 2862, "autor": "Soapy Finance", "date": null, "content": "Inspiration\nWith the rapid growths of blockchain and DeFi, there are more and more DeFi projects establishing continually, but they are relatively isolated, which leading to three obvious problems.\nFirstly, there are too many projects and they are so changeable. Therefore, it is difficult for users to find them quickly.\nWhat's more, different projects have different UI and interaction methods. Users can participate only after learning and understanding, which makes the experience uneven.\nLast but not least, most of the current projects and DeFi tools still focus on the PC, and the support for mobile is imperfect, which limits the time period and scenario when using.\nAlthough there are so many opportunities in the DeFi world, the market changes with each passing day. So far, there is no mobile APP based on the APY ranking that can help users to discover, search and analyze DeFi projects as fast as possible.\nFor example, I want to know which staking pool of USDT or ETH has the highest APY return across multi chains? Which staking pool's TVL & APY grows rapidly? How about the changing trend of TVL & APY? When will the launch of the new projects be? A great number of questions as mentioned above.\nHowever, such difficulties inspire us to create a mobile entrance for community users to participate in and find the opportunities for DeFi.\nWhat is Soapy?\nSoapy Finance is committed to building the future mobile entrance of DeFi. It brings friendly, efficient, and secure financial products and services to users.\nSoapy is governed by Soapy DAO, all community members who hold SOAPY can participate in project governance.\nAt present, it consists of two major parts:\nAPY.TOP is a multi-chain DeFi projects aggregator. It is the first to support the APY & TVL ranking list, it sources most public blockchain's DeFi projects and pools data, appears with the mobile-first interactive mode, helps user to discover and analyze high-APY or new valuable DeFi projects easily.\nSoapy Swap is a multi-chain DEX aggregator. It provides a CandlestickChart visualization tool for the DEXes aggregated, helps users to know the trends of token prices. Soapy Swap aggregates major DEXes across various public blockchains and provides one-stop services to find the best prices and trading options for all types of crypto assets. In addition, Soapy Swap supports limit-price orderbook features for all users.\nHow we built it\nWe are a dynamic start-up team, and our core R & D personnel have been involved in the crypto field since 2017. We have a complete design and R & D process. APY.TOP is our first product, which supports H5 / Android / IOS access. At present, a beta version has been launched. Soapy Swap will be available in the near future.\nChallenges we ran into\nThere are too many chances and tracks in blockchain development. We always focus on the infrastructure and data-analyze products of DeFi. In the further, the full cross-chain swap scenario will be also preliminarily planned. There are still some challenges in this area\nAccomplishments that we're proud of\nDevelopment:\nAPY.TOP DApp Beta version launched\nAPY.TOP Android Beta version available\nCelo,BSC,Ethereum,Solana,Polygon,Fantom,Harmony,Near,Terra,Avalanche ecosystem DeFi projects aggregated\nAPY & TVL CandlestickChart supported\nWallectConnect component integrated\nWe have built a core community across the world.\nTwitter: more than 22K unique followers\nTelegram: more than 9.7K unique members\nWhat we have learned\nWe believe that the development of blockchain is still at an early stage, the popularity of DeFi is not enough, and there is plenty of room for development. What problems we have been thinking about and the mission of our efforts is to help users participate in DeFi more quickly and conveniently.\nWhat's the next for Soapy Finance\nAccording to the roadmap, we will focus on product development.\nProduct Development\nUsers growths and community building\nFundraising", "link": "https://devpost.com/software/soapy-finance-89wmpk", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwith the rapid growths of blockchain and defi, there are more and more defi projects establishing continually, but they are relatively isolated, which leading to three obvious problems.\nfirstly, there are too many projects and they are so changeable. therefore, it is difficult for users to find them quickly.\nwhat's more, different projects have different ui and interaction methods. users can participate only after learning and understanding, which makes the experience uneven.\nlast but not least, most of the current projects and defi tools still focus on the pc, and the support for mobile is imperfect, which limits the time period and scenario when using.\nalthough there are so many opportunities in the defi world, the market changes with each passing day. so far, there is no mobile app based on the apy ranking that can help users to discover, search and analyze defi projects as fast as possible.\nfor example, i want to know which staking pool of usdt or eth has the highest apy return across multi chains? which staking pool's tvl & apy grows rapidly? how about the changing trend of tvl & apy? when will the launch of the new projects be? a great number of questions as mentioned above.\nhowever, such difficulties inspire us to create a mobile entrance for community users to participate in and find the opportunities for defi.\nwhat is soapy?\nsoapy finance is committed to building the future mobile entrance of defi. it brings friendly, efficient, and secure financial products and services to users.\nsoapy is governed by soapy dao, all community members who hold soapy can participate in project governance.\nat present, it consists of two major parts:\napy.top is a multi-chain defi projects aggregator. it is the first to support the apy & tvl ranking list, it sources most public blockchain's defi projects and pools data, appears with the mobile-first interactive mode, helps user to discover and analyze high-apy or new valuable defi projects easily.\nsoapy swap is a multi-chain dex aggregator. it provides a candlestickchart visualization -----> tool !!!  for the dexes aggregated, helps users to know the trends of token prices. soapy swap aggregates major dexes across various public blockchains and provides one-stop services to find the best prices and trading options for all types of crypto assets. in addition, soapy swap supports limit-price orderbook features for all users.\nhow we built it\nwe are a dynamic start-up team, and our core r & d personnel have been involved in the crypto field since 2017. we have a complete design and r & d process. apy.top is our first product, which supports h5 / android / ios access. at present, a beta version has been launched. soapy swap will be available in the near future.\nchallenges we ran into\nthere are too many chances and tracks in blockchain development. we always focus on the infrastructure and data-analyze products of defi. in the further, the full cross-chain swap scenario will be also preliminarily planned. there are still some challenges in this area\naccomplishments that we're proud of\ndevelopment:\napy.top dapp beta version launched\napy.top android beta version available\ncelo,bsc,ethereum,solana,polygon,fantom,harmony,near,terra,avalanche ecosystem defi projects aggregated\napy & tvl candlestickchart supported\nwallectconnect component integrated\nwe have built a core community across the world.\ntwitter: more than 22k unique followers\ntelegram: more than 9.7k unique members\nwhat we have learned\nwe believe that the development of blockchain is still at an early stage, the popularity of defi is not enough, and there is plenty of room for development. what problems we have been thinking about and the mission of our efforts is to help users participate in defi more quickly and conveniently.\nwhat's the next for soapy finance\naccording to the roadmap, we will focus on product development.\nproduct development\nusers growths and community building\nfundraising", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 1, "media": null, "medialink": null, "identifyer": 59502862}, {"Unnamed: 0": 2872, "autor": "ZooWind", "date": null, "content": "Inspiration\nMany NGOs have done research and investigations into the illegal wildlife trade (IWT) or engaged in counter-wildlife trafficking (CWT) activities. However, due to the lack of a centralized hub or a network organization, there has not been an effective tool for sharing all the available findings, tools, and materials among the conservation NGOs in Vietnam, resulting in waste of both human and material resources, and risks of losing the community\u2019s trust because of overlapping of activities.\nTo solve the problem, our team has developed a platform to unite animal conservation NGO s ' resources using machine learning and deep learning.\nWhat it does\nOur platform has 3 main features.\n1, Collect and share the resources from organizations, e-newspapers,... which classify and generated by our AI model.\n2, Search and explore the animal information which can search by text or image which used deep learning to identify.\n3, Report wildlife trafficking to authorities with our chatbot\nHow we built it\nWith the prototype we have designed on Figma, We built our platform using Angular 11, consuming RESTful API developed with Java Spring Boot.\nIn the document classification model, first, we collect over 4000 e-newspapers on the internet and label those using keyword extraction. After labeling, we built a model to classify text from the document using the Logistic Regression function into 3 categories: Not related, Illegal Wildlife Trafficking, Counter Wildlife Trafficking. The model precision is 84%.\nIn the animal classification model, we collect over 17k images from 5 species and put them into a deep learning model using CNN to train. The final precision is 67%.\nAfter building 2 models, we integrate them to our front-end using Flask RESTful API.\nChallenges we ran into\nOur document classification is still not collecting all the results we expected.\nOur website functions are not yet delivered all the needs of NGOs' problems.\nOur image classification model accuracy is still low with small numbers of species.\nAccomplishments that we're proud of\nWe came together with no relationship to each other, and finally, we worked well to create a product without any conflicts.\nThe product that we make is useful for not only researchers but also people who love wildlife\nWhat we learned\nWe learned more about the current situation of wildlife trafficking, learned more about how to protect it, and make a product that we have ever had a chance to do. Also, we forced ourselves to learn something new, and have to use it for production in a short amount of time.\nWhat's next for ZooWind\nWe will develop a platform for NGOs' including UI, API to help them upload and control the research, documents, papers that they want to deliver.", "link": "https://devpost.com/software/zoowind", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nmany ngos have done research and investigations into the illegal wildlife trade (iwt) or engaged in counter-wildlife trafficking (cwt) activities. however, due to the lack of a centralized hub or a network organization, there has not been an effective -----> tool !!!  for sharing all the available findings, tools, and materials among the conservation ngos in vietnam, resulting in waste of both human and material resources, and risks of losing the community\u2019s trust because of overlapping of activities.\nto solve the problem, our team has developed a platform to unite animal conservation ngo s ' resources using machine learning and deep learning.\nwhat it does\nour platform has 3 main features.\n1, collect and share the resources from organizations, e-newspapers,... which classify and generated by our ai model.\n2, search and explore the animal information which can search by text or image which used deep learning to identify.\n3, report wildlife trafficking to authorities with our chatbot\nhow we built it\nwith the prototype we have designed on figma, we built our platform using angular 11, consuming restful api developed with java spring boot.\nin the document classification model, first, we collect over 4000 e-newspapers on the internet and label those using keyword extraction. after labeling, we built a model to classify text from the document using the logistic regression function into 3 categories: not related, illegal wildlife trafficking, counter wildlife trafficking. the model precision is 84%.\nin the animal classification model, we collect over 17k images from 5 species and put them into a deep learning model using cnn to train. the final precision is 67%.\nafter building 2 models, we integrate them to our front-end using flask restful api.\nchallenges we ran into\nour document classification is still not collecting all the results we expected.\nour website functions are not yet delivered all the needs of ngos' problems.\nour image classification model accuracy is still low with small numbers of species.\naccomplishments that we're proud of\nwe came together with no relationship to each other, and finally, we worked well to create a product without any conflicts.\nthe product that we make is useful for not only researchers but also people who love wildlife\nwhat we learned\nwe learned more about the current situation of wildlife trafficking, learned more about how to protect it, and make a product that we have ever had a chance to do. also, we forced ourselves to learn something new, and have to use it for production in a short amount of time.\nwhat's next for zoowind\nwe will develop a platform for ngos' including ui, api to help them upload and control the research, documents, papers that they want to deliver.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59502872}, {"Unnamed: 0": 2878, "autor": "Auralizing A Historical Space", "date": null, "content": "The project aims to create audio-based simulations of historical spaces through building 3D models and simulating their audio response. This would allow us to better connect to our cultural heritage by having accurate representations of the spaces our ancestors lived in. We seek to make the intangible cultural heritage - the experiences faced by past generations and the sounds and music they created - tangible through hearing for historians, museums, and scholars of the past. We use Blender to model our chosen spaces and Microsoft\u2019s Project Acoustics to simulate the audio response, pairing this technology with extensive research on period-accurate songs, chants, and prayers that would have been said in our chosen spaces. We strongly believe in the ability of this project to connect us to our past and believe it will be a strong tool for cultural historians.", "link": "https://devpost.com/software/auralizing-a-historical-space", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "the project aims to create audio-based simulations of historical spaces through building 3d models and simulating their audio response. this would allow us to better connect to our cultural heritage by having accurate representations of the spaces our ancestors lived in. we seek to make the intangible cultural heritage - the experiences faced by past generations and the sounds and music they created - tangible through hearing for historians, museums, and scholars of the past. we use blender to model our chosen spaces and microsoft\u2019s project acoustics to simulate the audio response, pairing this technology with extensive research on period-accurate songs, chants, and prayers that would have been said in our chosen spaces. we strongly believe in the ability of this project to connect us to our past and believe it will be a strong -----> tool !!!  for cultural historians.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59502878}, {"Unnamed: 0": 2882, "autor": "Pango", "date": null, "content": "Inspiration\nMany NGOs have done research and investigations into the illegal wildlife trade (IWT) or engaged in counter-wildlife trafficking (CWT) activities. However, due to the lack of a centralized hub or a network liaison, there has not been an effective tool for sharing all the available findings, tools, and materials among the conservation NGOs in Vietnam, resulting in waste of both human and material resources, and risks of losing the community\u2019s trust because of overlapping of activities.\nWhat it does\nPango - an online platform gathering organizations and other organizations as well as individuals to optimize productivity.\nHow we built it\nProject and organization management system - uses CRUD API of Rest Framework Django, although this part is in the code base on github, but the technical side has improvised and wrote a new api to increase the performance of the tool. Data is provided from the parties (because Pango is an intermediary to connect). The data import system provides a full range of common file types, used by Google Apis. Machine Learning: Just a support function from a host platform like Pango, allowing users to connect to an external machine learning model to transport data to and develop Ai like Animal classification Camera or Live Object Webcam, .. Cloud Storage links with Microsoft Azure, Redis, Amazon S3,... The role-based hierarchical security system is an api written by itself according to the permissions and rules of django, preventing bad people from stealing project data.\nChallenges we ran into\nBuild a tool to gather all data, information from previous research, reports, communications, and educational materials relevant to IWT and CWT, (can be GIS (geographic information system) mapping tool), from government agencies, academia, international NGOs, local NGOs, research organizations and freelancers, etc. for the use of conservation NGOs and interested individuals for non-commercial purposes.\nAccomplishments that we're proud of\nWith a direct aim to end wildlife trafficking, we develop a mapping system that enables users to report the offense with simple tasks like taking a picture. Snapping a photo will now be equipped with our real-time location detecting technology. Right after the picture gets uploaded by a user, its location and time will be reported accordingly. All this data will be corporated into google maps. This will help organizations/governments gain insight into the places where crimes happened, which in turn help them better intervene in these wildlife trafficking activities For individual users, they can directly contribute to the act as well as gain more knowledge about the issue.", "link": "https://devpost.com/software/pango-1gw7vd", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nmany ngos have done research and investigations into the illegal wildlife trade (iwt) or engaged in counter-wildlife trafficking (cwt) activities. however, due to the lack of a centralized hub or a network liaison, there has not been an effective -----> tool !!!  for sharing all the available findings, tools, and materials among the conservation ngos in vietnam, resulting in waste of both human and material resources, and risks of losing the community\u2019s trust because of overlapping of activities.\nwhat it does\npango - an online platform gathering organizations and other organizations as well as individuals to optimize productivity.\nhow we built it\nproject and organization management system - uses crud api of rest framework django, although this part is in the code base on github, but the technical side has improvised and wrote a new api to increase the performance of the tool. data is provided from the parties (because pango is an intermediary to connect). the data import system provides a full range of common file types, used by google apis. machine learning: just a support function from a host platform like pango, allowing users to connect to an external machine learning model to transport data to and develop ai like animal classification camera or live object webcam, .. cloud storage links with microsoft azure, redis, amazon s3,... the role-based hierarchical security system is an api written by itself according to the permissions and rules of django, preventing bad people from stealing project data.\nchallenges we ran into\nbuild a tool to gather all data, information from previous research, reports, communications, and educational materials relevant to iwt and cwt, (can be gis (geographic information system) mapping tool), from government agencies, academia, international ngos, local ngos, research organizations and freelancers, etc. for the use of conservation ngos and interested individuals for non-commercial purposes.\naccomplishments that we're proud of\nwith a direct aim to end wildlife trafficking, we develop a mapping system that enables users to report the offense with simple tasks like taking a picture. snapping a photo will now be equipped with our real-time location detecting technology. right after the picture gets uploaded by a user, its location and time will be reported accordingly. all this data will be corporated into google maps. this will help organizations/governments gain insight into the places where crimes happened, which in turn help them better intervene in these wildlife trafficking activities for individual users, they can directly contribute to the act as well as gain more knowledge about the issue.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59502882}, {"Unnamed: 0": 2902, "autor": "Recallect", "date": null, "content": "Inspiration\nA survey conducted by Dalhousie University researchers found out that most Canadians were unaware of recent significant food recalls. In fact, did you know there was recent recall a few weeks ago across Canada for whole onions due to concerns of salmonella contamination?\nWith Recallect, you would have been in the know.\nWhat it does\nOur goal with Recallect is to give the general population increased accessibility to product recall information while ensuring the updates provided are custom and relevant to the user. Recallect is a mobile app that allows you to quickly find out if a product you\u2019re interested in buying or have bought has been recalled using the Canadian Recalls database. Easily use our app to view recent recalls or search for recalls via barcode scanning or manual entry. You can customize your search results so that they are kept in what we call your \u201cvirtual inventory\u201d so that Recallect can notify you if there are any recalls in the future.\nStay safe. Stay informed. Let Recallect remove the barrier and grant easy access to information regarding Canadian recalls to you today.\nHow we built it\nAs a flight that crossed between Alberta and Ontario didn't seem like the most feasible option, we decided to meet up online via Discord to hold our brainstorming session. We explored all options, no holds barred, regardless of how absurd, pointless, or outlandish the idea seemed. We then sketched out how we expected the app to look like and what features it would have. We then moved on to using a collaborative interface design tool to work on the UI called Figma. We also looked into the APIs to use with our project like the barcode lookup API and Canadian Recalls and Safety Alerts API that would work when developing using Flutter. We continued with this until we ran out of time.\nChallenges we ran into\nGoing into this hackathon we were a solid team of four. Unfortunately, we had some attrition and our workforce was cut in half come build day. We also used new programs that we were unfamiliar especially considering one of our team members does not have a background in computer science. Despite these issues, we confidently waded these new waters and reached the point that we are today with Recallect!\nAccomplishments that we're proud of\nWe're very proud of the project, ideas, and work we have done within the time constraints (time, knowledge, physical distance).\nWhat's next for Recallect\nWe are thinking of adding a food scan feature that would search based on a database of food images for the product. We would also like to add an option to customize the length of time a product would be in the virtual inventory after searching.", "link": "https://devpost.com/software/recallect", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\na survey conducted by dalhousie university researchers found out that most canadians were unaware of recent significant food recalls. in fact, did you know there was recent recall a few weeks ago across canada for whole onions due to concerns of salmonella contamination?\nwith recallect, you would have been in the know.\nwhat it does\nour goal with recallect is to give the general population increased accessibility to product recall information while ensuring the updates provided are custom and relevant to the user. recallect is a mobile app that allows you to quickly find out if a product you\u2019re interested in buying or have bought has been recalled using the canadian recalls database. easily use our app to view recent recalls or search for recalls via barcode scanning or manual entry. you can customize your search results so that they are kept in what we call your \u201cvirtual inventory\u201d so that recallect can notify you if there are any recalls in the future.\nstay safe. stay informed. let recallect remove the barrier and grant easy access to information regarding canadian recalls to you today.\nhow we built it\nas a flight that crossed between alberta and ontario didn't seem like the most feasible option, we decided to meet up online via discord to hold our brainstorming session. we explored all options, no holds barred, regardless of how absurd, pointless, or outlandish the idea seemed. we then sketched out how we expected the app to look like and what features it would have. we then moved on to using a collaborative interface design -----> tool !!!  to work on the ui called figma. we also looked into the apis to use with our project like the barcode lookup api and canadian recalls and safety alerts api that would work when developing using flutter. we continued with this until we ran out of time.\nchallenges we ran into\ngoing into this hackathon we were a solid team of four. unfortunately, we had some attrition and our workforce was cut in half come build day. we also used new programs that we were unfamiliar especially considering one of our team members does not have a background in computer science. despite these issues, we confidently waded these new waters and reached the point that we are today with recallect!\naccomplishments that we're proud of\nwe're very proud of the project, ideas, and work we have done within the time constraints (time, knowledge, physical distance).\nwhat's next for recallect\nwe are thinking of adding a food scan feature that would search based on a database of food images for the product. we would also like to add an option to customize the length of time a product would be in the virtual inventory after searching.", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59502902}, {"Unnamed: 0": 2910, "autor": "Lippie", "date": null, "content": "Inspiration\nDuring the pandemic, it has become increasingly more difficult for consumers to try on lip products in stores and find shades that will look good on them. We wanted to build a tool that would let people stay home and stay safe while simultaneously making lip product shopping more accessible. We wanted to help connect those interested in discussing lip products through our app. In addition, we wanted to increase diversity and inclusivity by exposing users to a variety of brands and colours that they may not have considered before (including minority brands), and boost accessibility to lip products for users who may not have had a chance to dabble in makeup.\nWhat it does\nLippie aims to break down obstacles caused by the pandemic in the consumer experience by suggesting recommendations based on user input of favourite shades, utilizing an AI makeup tool for more specific recommendations based on live photo/video of the user, finding daily deals and discounts, and maintaining a safe social space for the user community. Lippie creates a diverse, inclusive, and accessible environment for lipstick enthusiasts.\nHow we built it\nWe used Figma to prototype our app. React was used to build the front-end, and Node/Express were used for the back-end. We plan to use MongoDB to keep track of user data.\nChallenges we ran into\nWe did not have enough time to build out all the different aspects of our app, so we used Figma for the demo.\nAccomplishments that we're proud of\nSome of our team members had never worked with React or Figma before, so it was exciting for them to learn and for (slightly) more experienced members to take on a mentor-like role for the first time. It was also a great experience for the makeup-lovers on our team to put an idea like this into practice and see our suggestions for improving the consumer experience come to life.\nWhat we learned\nWe learned how to use Figma in a more robust way. We also learned how to build an app in React from the boilerplate code up.\nWhat's next for Lippie\nWe plan to finish building our app in React and connect it to the backend! Maybe one day you'll see Lippie on the App Store...", "link": "https://devpost.com/software/lippie", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nduring the pandemic, it has become increasingly more difficult for consumers to try on lip products in stores and find shades that will look good on them. we wanted to build a -----> tool !!!  that would let people stay home and stay safe while simultaneously making lip product shopping more accessible. we wanted to help connect those interested in discussing lip products through our app. in addition, we wanted to increase diversity and inclusivity by exposing users to a variety of brands and colours that they may not have considered before (including minority brands), and boost accessibility to lip products for users who may not have had a chance to dabble in makeup.\nwhat it does\nlippie aims to break down obstacles caused by the pandemic in the consumer experience by suggesting recommendations based on user input of favourite shades, utilizing an ai makeup tool for more specific recommendations based on live photo/video of the user, finding daily deals and discounts, and maintaining a safe social space for the user community. lippie creates a diverse, inclusive, and accessible environment for lipstick enthusiasts.\nhow we built it\nwe used figma to prototype our app. react was used to build the front-end, and node/express were used for the back-end. we plan to use mongodb to keep track of user data.\nchallenges we ran into\nwe did not have enough time to build out all the different aspects of our app, so we used figma for the demo.\naccomplishments that we're proud of\nsome of our team members had never worked with react or figma before, so it was exciting for them to learn and for (slightly) more experienced members to take on a mentor-like role for the first time. it was also a great experience for the makeup-lovers on our team to put an idea like this into practice and see our suggestions for improving the consumer experience come to life.\nwhat we learned\nwe learned how to use figma in a more robust way. we also learned how to build an app in react from the boilerplate code up.\nwhat's next for lippie\nwe plan to finish building our app in react and connect it to the backend! maybe one day you'll see lippie on the app store...", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59502910}, {"Unnamed: 0": 2911, "autor": "AniShifter - ANIMATRIX", "date": null, "content": "Inspiration\nAt the core of the illegal wildlife trafficking is a strong and rapidly expanding demand for a variety of products, especially ingredients for medicine. Much of demand for pangolin scales, rhinoceros horns, tiger bones, and other animal products arises out of the practice of traditional medicine, which uses these ingredients to treat fevers, gout, and other illnesses. Traditional medicines that made from wildlife are taken by hundreds of millions of people. Because of huge demand for wildlife medicinal products in Vietnam and in the world, we want to stop this status by changing the perception of buyers so we can protect the wildlife in the most effective way.\nWhat it does\nAnishifter is a solution to detects consumers' searches on wildlife products as medicines on Search Engine and feed them with the search results that provide scientifically proven facts, information on legality, health risks etc\u2026 to make these potential buyers change their mind and drive them away from actual purchasing\nHow we built it\nHTML to add the elements to the website, CSS to customize the theme, JavaScript to modify the features.\nChallenges we ran into\nOne of the main difficulties faced by our team was the database. Few of our proposals were not implemented successfully due to data acquisition problems and the time constraint of the hackathon\nAccomplishments that we're proud of\nOur extension can be added in search engines as it can impact many more users and help to raise public awareness about wildlife conservation.\nWhat we learned\nWe have learned a lot of wildlife purchasing facts. Besides, through the process of the hackathon, we have also received a lot of useful advises and meaningful knowledge about both wildlife and technology from our mentors.\nWhat's next for Ani-Shifter\nIn further future, we are planning to get permission from big search engine such as Google, Bing, etc\u2026 to add our extension as their tool", "link": "https://devpost.com/software/ani-shifter", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nat the core of the illegal wildlife trafficking is a strong and rapidly expanding demand for a variety of products, especially ingredients for medicine. much of demand for pangolin scales, rhinoceros horns, tiger bones, and other animal products arises out of the practice of traditional medicine, which uses these ingredients to treat fevers, gout, and other illnesses. traditional medicines that made from wildlife are taken by hundreds of millions of people. because of huge demand for wildlife medicinal products in vietnam and in the world, we want to stop this status by changing the perception of buyers so we can protect the wildlife in the most effective way.\nwhat it does\nanishifter is a solution to detects consumers' searches on wildlife products as medicines on search engine and feed them with the search results that provide scientifically proven facts, information on legality, health risks etc\u2026 to make these potential buyers change their mind and drive them away from actual purchasing\nhow we built it\nhtml to add the elements to the website, css to customize the theme, javascript to modify the features.\nchallenges we ran into\none of the main difficulties faced by our team was the database. few of our proposals were not implemented successfully due to data acquisition problems and the time constraint of the hackathon\naccomplishments that we're proud of\nour extension can be added in search engines as it can impact many more users and help to raise public awareness about wildlife conservation.\nwhat we learned\nwe have learned a lot of wildlife purchasing facts. besides, through the process of the hackathon, we have also received a lot of useful advises and meaningful knowledge about both wildlife and technology from our mentors.\nwhat's next for ani-shifter\nin further future, we are planning to get permission from big search engine such as google, bing, etc\u2026 to add our extension as their -----> tool !!! ", "sortedWord": "None", "removed": "Nan", "score": 8, "comments": 0, "media": null, "medialink": null, "identifyer": 59502911}, {"Unnamed: 0": 2925, "autor": "WELLDONE Wallet & Bridge", "date": null, "content": "WELLDONE Wallet & Bridge\nEnter WELLDONE Wallet & Bridge! Over the past month, we have developed a multi-purpose blockchain infrastructure with a user-friendly interface. Our goal? To allow anyone to set foot their feet into the crypto economy and successfully onboard their ideas and innovations onto the blockchain network.\nInspiration\nWhile the Cosmos interchain ecosystem is growing rapidly, we felt there was still a notable lack of user-oriented infrastructure\u2013 this was our main motive for building our own wallet. As wallet compatibility varies by blockchain, users have to create numerous wallets to access different networks. Consequently, users can end up having to juggle a host of addresses and private keys. Developers also have to download, develop and test all the wallets they use.\nWhat it does\nWELLDONE Wallet is a one-of-a-kind wallet for multi-chain assets. It is a multi-chain wallet that gives you control over all your coins from a single platform and your assets use just as you wish across different networks without restraints.\nIts two main functions are called\nUniversal Provider\nKMS\nUniversal Provider is our key mechanism that integrates Web 3.0 providers into a single standardized API. Our wallet facilitates injection across multiple networks through the Universal Provider, acting as a multi-chain integration tool for developers. This means our users can access and develop services for different protocols through a single provider which can save some valuable time.\nKMS is our key function in a non-custodial wallet that does not require your private key. Only you (not even us) have access to all your assets which creates our wallet even more secure and safe for all our users. Safe yet extensive, our wallet is a one-of-a-kind crypto wallet that manages multi-chain assets with a single UX.\nHow we built it\nWELLDONE Wallet is being developed as an extension. When we plan, we have experienced many wallets in each ecosystem. As a result, we've found and designed a multichain-optimized wallet UX.\nChallenges we ran into\nOur priority is to connect all the chains and then we become a multi-chain wallet. Additionally, We are documenting and sharing Universal Providers API to lower the developer's barriers. Through this, more players can cut development time and contribute to the growth of the crypto ecosystem.\nAccomplishments that we're proud of\nWe have completed a wallet that provides users with a convenient UX/UI. Users do not have to change to other wallets when using EVMOS, Cosmos, or Ethereum chains.\nWhat we learned\nWhat we learned was a deep understanding of cross-chain. And we found that the multichain wallet is a key product in accelerating this growth. So, we will contribute to expanding the independent interconnected blockchain ecosystem, which is built by using developer-friendly Integrated providers as well as wallet development.\nWhat's next for WELLDONE Wallet\nWe created WELLDONE Wallet with three core objectives in mind. Creating a one-of-a-kind wallet for multi-chain assets, providing better infrastructure to lower barriers for developers, and developing a cross-chain bridge.\nBy developing a one-of-a-kind wallet for multi-chain assets (KMS), we optimize your user experience. By building solid infrastructure (Universal Provider), we lower the barriers to entry for developers, so that more players can contribute to the ecosystem. Finally, by developing a cross-chain bridge, we can enable users to easily switch chains via their WELLDONE wallets. Rather than simply providing access to all assets in one wallet, we want to enable users to go seamlessly between various chains. So next up is our bridge.\nJoin us, and together let\u2019s bring your finest ideas to life!", "link": "https://devpost.com/software/welldone-project", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "welldone wallet & bridge\nenter welldone wallet & bridge! over the past month, we have developed a multi-purpose blockchain infrastructure with a user-friendly interface. our goal? to allow anyone to set foot their feet into the crypto economy and successfully onboard their ideas and innovations onto the blockchain network.\ninspiration\nwhile the cosmos interchain ecosystem is growing rapidly, we felt there was still a notable lack of user-oriented infrastructure\u2013 this was our main motive for building our own wallet. as wallet compatibility varies by blockchain, users have to create numerous wallets to access different networks. consequently, users can end up having to juggle a host of addresses and private keys. developers also have to download, develop and test all the wallets they use.\nwhat it does\nwelldone wallet is a one-of-a-kind wallet for multi-chain assets. it is a multi-chain wallet that gives you control over all your coins from a single platform and your assets use just as you wish across different networks without restraints.\nits two main functions are called\nuniversal provider\nkms\nuniversal provider is our key mechanism that integrates web 3.0 providers into a single standardized api. our wallet facilitates injection across multiple networks through the universal provider, acting as a multi-chain integration -----> tool !!!  for developers. this means our users can access and develop services for different protocols through a single provider which can save some valuable time.\nkms is our key function in a non-custodial wallet that does not require your private key. only you (not even us) have access to all your assets which creates our wallet even more secure and safe for all our users. safe yet extensive, our wallet is a one-of-a-kind crypto wallet that manages multi-chain assets with a single ux.\nhow we built it\nwelldone wallet is being developed as an extension. when we plan, we have experienced many wallets in each ecosystem. as a result, we've found and designed a multichain-optimized wallet ux.\nchallenges we ran into\nour priority is to connect all the chains and then we become a multi-chain wallet. additionally, we are documenting and sharing universal providers api to lower the developer's barriers. through this, more players can cut development time and contribute to the growth of the crypto ecosystem.\naccomplishments that we're proud of\nwe have completed a wallet that provides users with a convenient ux/ui. users do not have to change to other wallets when using evmos, cosmos, or ethereum chains.\nwhat we learned\nwhat we learned was a deep understanding of cross-chain. and we found that the multichain wallet is a key product in accelerating this growth. so, we will contribute to expanding the independent interconnected blockchain ecosystem, which is built by using developer-friendly integrated providers as well as wallet development.\nwhat's next for welldone wallet\nwe created welldone wallet with three core objectives in mind. creating a one-of-a-kind wallet for multi-chain assets, providing better infrastructure to lower barriers for developers, and developing a cross-chain bridge.\nby developing a one-of-a-kind wallet for multi-chain assets (kms), we optimize your user experience. by building solid infrastructure (universal provider), we lower the barriers to entry for developers, so that more players can contribute to the ecosystem. finally, by developing a cross-chain bridge, we can enable users to easily switch chains via their welldone wallets. rather than simply providing access to all assets in one wallet, we want to enable users to go seamlessly between various chains. so next up is our bridge.\njoin us, and together let\u2019s bring your finest ideas to life!", "sortedWord": "None", "removed": "Nan", "score": 19, "comments": 0, "media": null, "medialink": null, "identifyer": 59502925}, {"Unnamed: 0": 2930, "autor": "The Wright Arcade", "date": null, "content": "Inspiration\nWe really love playing games as a family bonding tool. We also care about the environment and climate change. We also didn't want to make our first hackathon project about a self-help quiz on finding your perfect anime husband.\nWhat it does\nLiterally a website to host my siblings' Scratch games, truly the next generation of pioneers for game design in Web3.\nHow we built it\nWe used Scratch to build the games and created a simple HTML/CSS website because oldest sister refuses to learn React in 12 hours.\nChallenges we ran into\nMiddle schoolers have a lot of demands, they truly don't understand proper work life balance.\nAccomplishments that we're proud of\nWe managed to create 4 completely different games to showcase pollution, global warming, and the fundamentals of ignoring climate change with no narrative design needed.\nWhat we learned\nAlthough advocating for climate change in game design is poggers, we would want to have a page dedicated to environmental policies we want to see enacted in our hometown.\nWhat's next for The Wright Arcade\nWe will soon go after the Roblox Accelerator as the first children-ran game studio...", "link": "https://devpost.com/software/the-wright-arcade", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe really love playing games as a family bonding -----> tool !!! . we also care about the environment and climate change. we also didn't want to make our first hackathon project about a self-help quiz on finding your perfect anime husband.\nwhat it does\nliterally a website to host my siblings' scratch games, truly the next generation of pioneers for game design in web3.\nhow we built it\nwe used scratch to build the games and created a simple html/css website because oldest sister refuses to learn react in 12 hours.\nchallenges we ran into\nmiddle schoolers have a lot of demands, they truly don't understand proper work life balance.\naccomplishments that we're proud of\nwe managed to create 4 completely different games to showcase pollution, global warming, and the fundamentals of ignoring climate change with no narrative design needed.\nwhat we learned\nalthough advocating for climate change in game design is poggers, we would want to have a page dedicated to environmental policies we want to see enacted in our hometown.\nwhat's next for the wright arcade\nwe will soon go after the roblox accelerator as the first children-ran game studio...", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59502930}, {"Unnamed: 0": 2939, "autor": "Cal Poly Study Timer", "date": null, "content": "Inspiration\nWe all need to study in college, so we decided to make a tool that would help us study more effectively.\nWhat it does\nIt breaks your study session into 25 minutes of studying, and a 5-minute break, on and off.\nHow we built it\nWe built it using raw HTML, CSS, and JavaScript. Each member was in charge of one thing like one person would do the timer, one would do the to-do list, and so on, and then we combined it all.\nChallenges we ran into\nWe had three members with almost no coding experience, so we essentially learned everything from scratch\nAccomplishments that we're proud of\nWe're proud of the entire project, and the fact that in just one day we made a fully functional, and useful, website that anyone can use.\nWhat we learned\nWe learned how to use HTML, CSS, and JavaScript, how they all interact, and how it all comes together in the end.\nWhat's next for Cal Poly Study Timer\nNot much :P", "link": "https://devpost.com/software/cal-poly-study-timer", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe all need to study in college, so we decided to make a -----> tool !!!  that would help us study more effectively.\nwhat it does\nit breaks your study session into 25 minutes of studying, and a 5-minute break, on and off.\nhow we built it\nwe built it using raw html, css, and javascript. each member was in charge of one thing like one person would do the timer, one would do the to-do list, and so on, and then we combined it all.\nchallenges we ran into\nwe had three members with almost no coding experience, so we essentially learned everything from scratch\naccomplishments that we're proud of\nwe're proud of the entire project, and the fact that in just one day we made a fully functional, and useful, website that anyone can use.\nwhat we learned\nwe learned how to use html, css, and javascript, how they all interact, and how it all comes together in the end.\nwhat's next for cal poly study timer\nnot much :p", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59502939}, {"Unnamed: 0": 2989, "autor": "UofA Study Space Booking Tool", "date": null, "content": "Inspiration\nOur inspiration was having our own blend of both online and in-person classes this semester. It can often be tricky to find a quiet place on campus to attend our online classes or just have somewhere to study, so our project solves this issue by making it easy to see available spots and book them.\nWhat it does\nThis tool has a fully fledged account system, in which you can create or delete accounts, and the tool will remember your login info. Once logged in, you can choose a building on campus and it will tell you if there are any vacant rooms in that building. If there are and you wish to book one, you just tell it the room name and it will book your spot. This makes this specific room appear as 'Occupied' to others and will be unavailable to book until you check out of it.\nHow we built it\nWe built this using Python with two scripts, one that generates the databases (which is used only once to initialize the databases, then never again, as the main script will edit these databases using JSON on it's own) and a main script that runs all of the main code.\nChallenges we ran into\nThis website is where we got all of our data for the buildings and rooms. Unfortunately, as you may notice, upon reloading the website, the first table, the one that we need the info from, takes a second to load up. When we tried to scrape this info using Beautiful Soup, it did not work, as when we searched for the table, it gave us the one closer to the bottom of the page. When we sent the scrape request and it loaded the website, it tries to scrape it immediately and thus, when it does this, the table that we want is not there. A possible workaround for this issue was to use Selenium to first load a webpage and then scrape from that new webpage, but we could not get this to work in our time constraints. Thus, the solution that we went with was some good old manual data entry, which is completely accurate, but less sophisticated.\nAccomplishments that we're proud of\nWe are firstly very proud of being able to put this whole thing together within 24 hours, an experience we have never had before, and we're glad that we did it. A specific functionality we are also very proud of is the login and account management system. This is very cool to us and it makes the management system more complex and also, perhaps more importantly, more practically useful. If one user can sign another user out of a room, this creates a big issue, as it basically negates the point of our tool. Someone could simply choose a room they want, then if it's occupied, just sign that person out and book themselves. With an account management system, this problem is completely avoided and users can only manage their own bookings, which is a huge plus for security.\nWhat we learned\nOne thing we learned was how to use JSON, which is a super helpful thing to know for managing live databases. Another thing we learnt was how to make use of Git branches and just working in Git as a group in general. This is obviously a crucial skill for having a job in software development, so it was great to be introduced to it early in our careers. Finally, we learnt just how much we can accomplish in 24 hours. We believe we made an awesome project with many layers of complexity and we're hugely proud of what we did.\nWhat's next for UofA Study Space Booking Tool\nSomething that we wished we could have done was implementing our tool on a website or in an app. When we looked into this, we found out that we would need to at least learn HTML, CSS, Javascript, DOM, jQuery, and Django. This was simply impractical given our time constraints and abilities, so we had to go with it just being a Python script that you run by itself. Hopefully, in the future, we can learn how to do all this stuff, as being able to put together our own website or app would first of all be super cool, but second of all, make it much more usable to the public.", "link": "https://devpost.com/software/uofa-study-space-booking-tool", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nour inspiration was having our own blend of both online and in-person classes this semester. it can often be tricky to find a quiet place on campus to attend our online classes or just have somewhere to study, so our project solves this issue by making it easy to see available spots and book them.\nwhat it does\nthis -----> tool !!!  has a fully fledged account system, in which you can create or delete accounts, and the -----> tool !!!  will remember your login info. once logged in, you can choose a building on campus and it will tell you if there are any vacant rooms in that building. if there are and you wish to book one, you just tell it the room name and it will book your spot. this makes this specific room appear as 'occupied' to others and will be unavailable to book until you check out of it.\nhow we built it\nwe built this using python with two scripts, one that generates the databases (which is used only once to initialize the databases, then never again, as the main script will edit these databases using json on it's own) and a main script that runs all of the main code.\nchallenges we ran into\nthis website is where we got all of our data for the buildings and rooms. unfortunately, as you may notice, upon reloading the website, the first table, the one that we need the info from, takes a second to load up. when we tried to scrape this info using beautiful soup, it did not work, as when we searched for the table, it gave us the one closer to the bottom of the page. when we sent the scrape request and it loaded the website, it tries to scrape it immediately and thus, when it does this, the table that we want is not there. a possible workaround for this issue was to use selenium to first load a webpage and then scrape from that new webpage, but we could not get this to work in our time constraints. thus, the solution that we went with was some good old manual data entry, which is completely accurate, but less sophisticated.\naccomplishments that we're proud of\nwe are firstly very proud of being able to put this whole thing together within 24 hours, an experience we have never had before, and we're glad that we did it. a specific functionality we are also very proud of is the login and account management system. this is very cool to us and it makes the management system more complex and also, perhaps more importantly, more practically useful. if one user can sign another user out of a room, this creates a big issue, as it basically negates the point of our tool. someone could simply choose a room they want, then if it's occupied, just sign that person out and book themselves. with an account management system, this problem is completely avoided and users can only manage their own bookings, which is a huge plus for security.\nwhat we learned\none thing we learned was how to use json, which is a super helpful thing to know for managing live databases. another thing we learnt was how to make use of git branches and just working in git as a group in general. this is obviously a crucial skill for having a job in software development, so it was great to be introduced to it early in our careers. finally, we learnt just how much we can accomplish in 24 hours. we believe we made an awesome project with many layers of complexity and we're hugely proud of what we did.\nwhat's next for uofa study space booking tool\nsomething that we wished we could have done was implementing our tool on a website or in an app. when we looked into this, we found out that we would need to at least learn html, css, javascript, dom, jquery, and django. this was simply impractical given our time constraints and abilities, so we had to go with it just being a python script that you run by itself. hopefully, in the future, we can learn how to do all this stuff, as being able to put together our own website or app would first of all be super cool, but second of all, make it much more usable to the public.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59502989}, {"Unnamed: 0": 3008, "autor": "Personal Mental Health Assistant", "date": null, "content": "Inspiration\nWe wanted to create something that will help and encourage people keep to positive habits in their day to day life.\nWhat it does\nProvides a tracking tool for healthy habits as well as feedback, a chat room, and a resources section to direct to CMHS resources.\nHow we built it\nWe used Java and Swing to build this.\nChallenges we ran into\nExporting as a .jar file, setting up servers and clients in Java using TCP, as we have mainly used sockets in C&C++.\nAccomplishments that we're proud of\nWe think this is a neat tool that will help us in our own day-to-day lives, and the interface is fast enough for our liking.\nWhat we learned\nLearned how to use Swing in a variety of ways which will help us in our future Java development.\nWhat's next for Personal Mental Health Assistant\nImprove the code style as this was done in a short amount of time, fully implement chat features, possibly add a chatbot.", "link": "https://devpost.com/software/personal-mental-health-assistant", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe wanted to create something that will help and encourage people keep to positive habits in their day to day life.\nwhat it does\nprovides a tracking -----> tool !!!  for healthy habits as well as feedback, a chat room, and a resources section to direct to cmhs resources.\nhow we built it\nwe used java and swing to build this.\nchallenges we ran into\nexporting as a .jar file, setting up servers and clients in java using tcp, as we have mainly used sockets in c&c++.\naccomplishments that we're proud of\nwe think this is a neat tool that will help us in our own day-to-day lives, and the interface is fast enough for our liking.\nwhat we learned\nlearned how to use swing in a variety of ways which will help us in our future java development.\nwhat's next for personal mental health assistant\nimprove the code style as this was done in a short amount of time, fully implement chat features, possibly add a chatbot.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59503008}, {"Unnamed: 0": 3019, "autor": "One-for-all Tab", "date": null, "content": "Inspiration\nOne-for-all Tab is a Google Chrome extension that manages website tabs. Our inspiration for it comes from our bad habit of opening too many tabs in a web browser. We don't think we are able to quit this habit so we decide to develop a tool instead to achieve a less cluttered browser workspace.\nWhat One-for-all Tab does\nOffering a day mode and a night mode, One-for-all Tab allows you to add tabs (a single tab or all your existing tabs) to a storage page in the form of URLs where you can reopen all the saved tabs if you have closed them. Essentially it works as a browser compressor.\nHow we built it\nWe started from scratch without any knowledge of Google Chrome extension development. After studying the official development docs offered by Google and analyzing the getting-started demo Google provided, we slowly get the hang of applying our newly-learned front-end skills (HTML, CSS, and JavaScript) to this new field.\nChallenges we ran into\nThe three of us have barely any experience with front-end development. We are comfortable with HTML and CSS. However, we faced some challenges coding with JavaScript, a language outside of our curriculum. We struggled a lot implementing popup functions. Additionally, our program still could not run perfectly under Mac OS (but it is fine under Windows 10).\nAccomplishments that we're proud of\nWe are proud that we have built from scratch a tool that is actually useful in our everyday life and of the fact that we are able to learn efficiently enough to build a fully functional program when we have absolutely no clues on how to develop chrome extensions yesterday.\nWhat we learned\nBesides solidifying our front-end development skills, we get familiar with the whole process of project development and management. Starting from brainstorming ideas and project design to implementation and integration, we have been learning and will continue to learn how to work as a team.\nWhat's next for One-for-all Tab\nWe probably will fix the error preventing it from running on Mac OS. More new features that we do not have time to implement for the short period of time will be added, including but not limited to:\nCollapsing/closing all tabs once added into the storage page;\nSaving the storage page as a tag;\nExporting the storage page as a word/pdf document.", "link": "https://devpost.com/software/one-for-all-tab", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\none-for-all tab is a google chrome extension that manages website tabs. our inspiration for it comes from our bad habit of opening too many tabs in a web browser. we don't think we are able to quit this habit so we decide to develop a -----> tool !!!  instead to achieve a less cluttered browser workspace.\nwhat one-for-all tab does\noffering a day mode and a night mode, one-for-all tab allows you to add tabs (a single tab or all your existing tabs) to a storage page in the form of urls where you can reopen all the saved tabs if you have closed them. essentially it works as a browser compressor.\nhow we built it\nwe started from scratch without any knowledge of google chrome extension development. after studying the official development docs offered by google and analyzing the getting-started demo google provided, we slowly get the hang of applying our newly-learned front-end skills (html, css, and javascript) to this new field.\nchallenges we ran into\nthe three of us have barely any experience with front-end development. we are comfortable with html and css. however, we faced some challenges coding with javascript, a language outside of our curriculum. we struggled a lot implementing popup functions. additionally, our program still could not run perfectly under mac os (but it is fine under windows 10).\naccomplishments that we're proud of\nwe are proud that we have built from scratch a tool that is actually useful in our everyday life and of the fact that we are able to learn efficiently enough to build a fully functional program when we have absolutely no clues on how to develop chrome extensions yesterday.\nwhat we learned\nbesides solidifying our front-end development skills, we get familiar with the whole process of project development and management. starting from brainstorming ideas and project design to implementation and integration, we have been learning and will continue to learn how to work as a team.\nwhat's next for one-for-all tab\nwe probably will fix the error preventing it from running on mac os. more new features that we do not have time to implement for the short period of time will be added, including but not limited to:\ncollapsing/closing all tabs once added into the storage page;\nsaving the storage page as a tag;\nexporting the storage page as a word/pdf document.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59503019}, {"Unnamed: 0": 3023, "autor": "Uix Kit", "date": null, "content": "Uix Kit is not a framework, just a UI toolkit based on some common libraries for building beautiful responsive website.\nUix Kit isn't a reusable component structure, mostly custom CSS and JavaScript based. Definitely interesting, and if you're developing mostly web content and not applications this is particularly useful. It is a web dev build tool/scaffold that does not depend on any framework. You can import any external libraries/frameworks or native ES Modules for production.\nThe generated core file in the dist directory can be used separately in any website. Support JS, HTML, and SASS component library automatically packaged. Automatically convert ES6 JS to ES5 using Babel in this scaffold.\nUix Kit is now in Long Term Support (LTS) mode.\nReact Version\nDemo\nhttps://xizon.github.io/uix-kit/examples/\nGitHub pages can only serve static content, and there is no way to run PHP or get AJAX request on the pages. You need to visit the link below to see some special demos \ud83d\udc47\nhttps://uiux.cc/uix-kit\nWhy use it\nNot a reusable component structure\nNot a JavaScript framework\nWebpack-based dev environment which is an intuitive toolkit system\nUse any JavaScript libraries in your favorite way to build styles and animation scripts\nSuitable for developing Visual Interaction websites and WordPress templates\nW3C standard and SEO\nControl scope with BEM naming, so the core Uix Kit project is not in conflict with the other projects\nAutomatically generate a table of contents for each module comment of the name\nEach module consists of SASS / SCSS, JavaScript and HTML files\nMake a foundation for the React architecture\nCompatible with Bootstrap 4.x\nProvides a common web page components and layouts\nUsing ES6 to import or export multiple modules, the third-party plugins could adopt pure file merger method and do not import and export\nThe complete directory of examples in order to develop a responsive website independently without Node.js dev environment", "link": "https://devpost.com/software/uix-kit", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "uix kit is not a framework, just a ui toolkit based on some common libraries for building beautiful responsive website.\nuix kit isn't a reusable component structure, mostly custom css and javascript based. definitely interesting, and if you're developing mostly web content and not applications this is particularly useful. it is a web dev build -----> tool !!! /scaffold that does not depend on any framework. you can import any external libraries/frameworks or native es modules for production.\nthe generated core file in the dist directory can be used separately in any website. support js, html, and sass component library automatically packaged. automatically convert es6 js to es5 using babel in this scaffold.\nuix kit is now in long term support (lts) mode.\nreact version\ndemo\nhttps://xizon.github.io/uix-kit/examples/\ngithub pages can only serve static content, and there is no way to run php or get ajax request on the pages. you need to visit the link below to see some special demos \ud83d\udc47\nhttps://uiux.cc/uix-kit\nwhy use it\nnot a reusable component structure\nnot a javascript framework\nwebpack-based dev environment which is an intuitive toolkit system\nuse any javascript libraries in your favorite way to build styles and animation scripts\nsuitable for developing visual interaction websites and wordpress templates\nw3c standard and seo\ncontrol scope with bem naming, so the core uix kit project is not in conflict with the other projects\nautomatically generate a table of contents for each module comment of the name\neach module consists of sass / scss, javascript and html files\nmake a foundation for the react architecture\ncompatible with bootstrap 4.x\nprovides a common web page components and layouts\nusing es6 to import or export multiple modules, the third-party plugins could adopt pure file merger method and do not import and export\nthe complete directory of examples in order to develop a responsive website independently without node.js dev environment", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59503023}, {"Unnamed: 0": 3038, "autor": "PoemKit", "date": null, "content": "Inspiration\nI like WordPress very much. WordPress has a proverb: Code is Poetry!\nIt means that code or development is as beautiful as poetry. To make it, I am also working hard to make a more beautiful and poetic website, so PoemKit is just like that.\nWhat it does\nA free web kit with React for fast web design and development via SSR. Using react, redux, router, axios and express. This project was bootstrapped with nodejs library. Project supports automatic deployments from a number of repository hosting services via pm2. PoemKit delivers more than 50+ built-in UI components for building modern web applications.\nPoemKit can work and deploy through commands, it still requires real code to be written behind them. These kits serve as UI components and some logic modules that can be put together into a real website or application prototype that will still require programming functions to make them work in the real world.\nDescription\nBase Environment: TypeScript 4.x.x + Babel 7.x.x + Webpack 5.x.x\nProvides a variety of pre-built UI components such as structured layout objects and UI controls\nAccessible URL, Technical SEO, and making your site search engine friendly with React Helmet\nIntegrate development, debugging, bundle, and deployment in one\nNot a JavaScript framework\nNo jQuery & Does not bind any tool libraries\nServer-Side Rendering (SSR) Support\nComponents are separated, you can use any UI component (such as Ant Design)\nStyling React Components in Sass/SCSS\nAutomatically bundle and generate independent core CSS and JS files\nProject supports automatic deployments from a number of repository hosting services via pm2\n\u2702\ufe0f Pluggable: The default components import some third-party plug-ins, such as grid system of Bootstrap 4, GSAP animation library, FontAwesome icon library, 3D engine, etc., so that you can quickly expand your website (Support on-demand configuration)\n\u2702\ufe0f Configurable Scaffold: Independently package the dist files (you could modify webpack.config.js to use memory mount)\nHow I built it\nIn order to adapt to some large websites, platforms and software projects, I considered some React frameworks, such as Nextjs, but in the end I decided to abandon it. It\u2019s Only Temporary!! I want the website architecture to be ffree such as the functionality and scalability of webpack scaffolding. On the one hand, building webpack scaffolding from scratch can also allow me to learn all aspects of knowledge in depth, and let me systematically understand the architecture principles and implementation methods of React. At the same time, I was made them as an open-source toolkit, which can be provided necessary support and reference to me in the future. For data transmission, I will give priority to using Redux as a cross-component state manager. In the future, I will also use some portable tools according to the needs of the project. I use TypeScript to write all UI components. First, the component library is systematized to manage, and on the other hand, they will not conflict with the logic blocks of the entire React website. It also needs to be able to directly import other UI component libraries, such as Ant Design. At the same time, this toolkit needs to have built-in custom React components to facilitate easy integration and improve the developer experience. For making a website with completely separated front-end and back-end, I also need to consider responsiveness and some cool interactive effects. For complex animations, I use GSAP plugins, and make the entire website compatible with Bootstrap and common icon libraries to provide more friendly Style support. It does not conflict with the built-in styles and scripts. This is my consideration for the complexity of project interaction. Finally, the issue of SEO needs to be considered, and I will solve it further. For example, let the entire React website support custom meta tags and titles. As for performance optimization, communication between components, mounting and unmounting, I strictly refer to the relevant official documentation.\nChallenges I ran into\nThere are countless React UI kits and libraries today. Faced with so many choices and new project challenges, what should I do next? Should I learn a new framework, build wheels myself, or dig out some tools. In short, I need a thought to better make more technological demands and innovations in the future. After all, the framework is just a framework. Can\u2019t leave the scene to talk about the purpose. I should think about how to plan the future from the personal growth and project needs, language basic skills training, etc., and plan the work of this tool. That is why it is important that during this busy time of the year, I still make time for learning some technologies of front or back. First, though, I consider whether some frameworks are smart \u2014 with a particular focus on the impact of 2021~2022 and the Covid-19 pandemic.\nAccomplishments that I'm proud of\nIt is a set of systematic tools, strong webpack customization.\nWhat I learned\nApplication of React technology and training of overall thinking, understanding of principles.\nWhat's next for PoemKit\nIt will continue to iterate, add more built-in components, and optimize performance. May even improve the architecture.", "link": "https://devpost.com/software/poemkit", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ni like wordpress very much. wordpress has a proverb: code is poetry!\nit means that code or development is as beautiful as poetry. to make it, i am also working hard to make a more beautiful and poetic website, so poemkit is just like that.\nwhat it does\na free web kit with react for fast web design and development via ssr. using react, redux, router, axios and express. this project was bootstrapped with nodejs library. project supports automatic deployments from a number of repository hosting services via pm2. poemkit delivers more than 50+ built-in ui components for building modern web applications.\npoemkit can work and deploy through commands, it still requires real code to be written behind them. these kits serve as ui components and some logic modules that can be put together into a real website or application prototype that will still require programming functions to make them work in the real world.\ndescription\nbase environment: typescript 4.x.x + babel 7.x.x + webpack 5.x.x\nprovides a variety of pre-built ui components such as structured layout objects and ui controls\naccessible url, technical seo, and making your site search engine friendly with react helmet\nintegrate development, debugging, bundle, and deployment in one\nnot a javascript framework\nno jquery & does not bind any -----> tool !!!  libraries\nserver-side rendering (ssr) support\ncomponents are separated, you can use any ui component (such as ant design)\nstyling react components in sass/scss\nautomatically bundle and generate independent core css and js files\nproject supports automatic deployments from a number of repository hosting services via pm2\n\u2702\ufe0f pluggable: the default components import some third-party plug-ins, such as grid system of bootstrap 4, gsap animation library, fontawesome icon library, 3d engine, etc., so that you can quickly expand your website (support on-demand configuration)\n\u2702\ufe0f configurable scaffold: independently package the dist files (you could modify webpack.config.js to use memory mount)\nhow i built it\nin order to adapt to some large websites, platforms and software projects, i considered some react frameworks, such as nextjs, but in the end i decided to abandon it. it\u2019s only temporary!! i want the website architecture to be ffree such as the functionality and scalability of webpack scaffolding. on the one hand, building webpack scaffolding from scratch can also allow me to learn all aspects of knowledge in depth, and let me systematically understand the architecture principles and implementation methods of react. at the same time, i was made them as an open-source toolkit, which can be provided necessary support and reference to me in the future. for data transmission, i will give priority to using redux as a cross-component state manager. in the future, i will also use some portable tools according to the needs of the project. i use typescript to write all ui components. first, the component library is systematized to manage, and on the other hand, they will not conflict with the logic blocks of the entire react website. it also needs to be able to directly import other ui component libraries, such as ant design. at the same time, this toolkit needs to have built-in custom react components to facilitate easy integration and improve the developer experience. for making a website with completely separated front-end and back-end, i also need to consider responsiveness and some cool interactive effects. for complex animations, i use gsap plugins, and make the entire website compatible with bootstrap and common icon libraries to provide more friendly style support. it does not conflict with the built-in styles and scripts. this is my consideration for the complexity of project interaction. finally, the issue of seo needs to be considered, and i will solve it further. for example, let the entire react website support custom meta tags and titles. as for performance optimization, communication between components, mounting and unmounting, i strictly refer to the relevant official documentation.\nchallenges i ran into\nthere are countless react ui kits and libraries today. faced with so many choices and new project challenges, what should i do next? should i learn a new framework, build wheels myself, or dig out some tools. in short, i need a thought to better make more technological demands and innovations in the future. after all, the framework is just a framework. can\u2019t leave the scene to talk about the purpose. i should think about how to plan the future from the personal growth and project needs, language basic skills training, etc., and plan the work of this tool. that is why it is important that during this busy time of the year, i still make time for learning some technologies of front or back. first, though, i consider whether some frameworks are smart \u2014 with a particular focus on the impact of 2021~2022 and the covid-19 pandemic.\naccomplishments that i'm proud of\nit is a set of systematic tools, strong webpack customization.\nwhat i learned\napplication of react technology and training of overall thinking, understanding of principles.\nwhat's next for poemkit\nit will continue to iterate, add more built-in components, and optimize performance. may even improve the architecture.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59503038}, {"Unnamed: 0": 3048, "autor": "CS-Helper", "date": null, "content": "Inspiration\nWe were inspired by the challenges of in-person computer science classes and the advantages of online schooling. In person, students are often too scared to ask questions in class due to the risk of embarrassment and fear of public speaking. In addition, it is often cumbersome to write/debug/run coding examples in class. This project draws from certain online tools such as Zoom and Replit to solve both of those problems.\nWhat it does\nThis web application is an in-class learning tool used by students and TAs during lectures. While the professor is lecturing, students can ask questions on a class-wide chat box to which TAs can answer. This eliminates the fear of asking questions, just like the Zoom chat box did during online schooling. In addition, the app contains a built in code editor that supports common languages such as Java, C and Python. This allows students to effortlessly write/debug/run coding examples in class without the need of opening an IDE, creating a new project, creating a new class etc. Combining these two features equips computer science students with an all encompassing tool that enables them to better absorb class material.\nHow we built it\nThis project was built using React, Node, Express, Bootstrap and Socket.io. The code editor uses Judge0 for the compilation of code.\nChallenges we ran into\nOne challenge we faced was figuring out how a chat works and how to use Socket.io properly with React to create the client to server connection. We underestimated the time it would take to understand the basics of a new library, and thus didn\u2019t have enough time to do everything we had wanted to.\nAccomplishments that we're proud of\nWe're proud to have been able to create a chat box which connects to a server and enables multiple users to talk to one another. We're also proud to have learned a bit of React and successfully applied it to a real project. Finally, we find the objective of the project to be meaningful, which adds to the feeling of accomplishment.\nWhat we learned\nWe learned to build a web application using React as well as set up a server using Node and Express. For some of us, it was our first time working with multiple people on a coding project, thus we learned to work as a team and collaborate on a single application.\nWhat's next for Classroom in-tool\nAdd syntax highlighting to the code editor\nAdd a PDF viewer to allow students to view class slides\nNotification for users entering and exiting the chat, and proper disconnecting of websockets once users have exited.", "link": "https://devpost.com/software/classroom-in-tool", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe were inspired by the challenges of in-person computer science classes and the advantages of online schooling. in person, students are often too scared to ask questions in class due to the risk of embarrassment and fear of public speaking. in addition, it is often cumbersome to write/debug/run coding examples in class. this project draws from certain online tools such as zoom and replit to solve both of those problems.\nwhat it does\nthis web application is an in-class learning -----> tool !!!  used by students and tas during lectures. while the professor is lecturing, students can ask questions on a class-wide chat box to which tas can answer. this eliminates the fear of asking questions, just like the zoom chat box did during online schooling. in addition, the app contains a built in code editor that supports common languages such as java, c and python. this allows students to effortlessly write/debug/run coding examples in class without the need of opening an ide, creating a new project, creating a new class etc. combining these two features equips computer science students with an all encompassing tool that enables them to better absorb class material.\nhow we built it\nthis project was built using react, node, express, bootstrap and socket.io. the code editor uses judge0 for the compilation of code.\nchallenges we ran into\none challenge we faced was figuring out how a chat works and how to use socket.io properly with react to create the client to server connection. we underestimated the time it would take to understand the basics of a new library, and thus didn\u2019t have enough time to do everything we had wanted to.\naccomplishments that we're proud of\nwe're proud to have been able to create a chat box which connects to a server and enables multiple users to talk to one another. we're also proud to have learned a bit of react and successfully applied it to a real project. finally, we find the objective of the project to be meaningful, which adds to the feeling of accomplishment.\nwhat we learned\nwe learned to build a web application using react as well as set up a server using node and express. for some of us, it was our first time working with multiple people on a coding project, thus we learned to work as a team and collaborate on a single application.\nwhat's next for classroom in-tool\nadd syntax highlighting to the code editor\nadd a pdf viewer to allow students to view class slides\nnotification for users entering and exiting the chat, and proper disconnecting of websockets once users have exited.", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59503048}, {"Unnamed: 0": 3062, "autor": "Club Buddy", "date": null, "content": "HackUTD 2021: ClubBuddy\nInspiration...\nAll of us have struggled with finding a good online source for learning and keeping up with UTD clubs that we are interested in. The only available source offered by UTD is presence.io which is outdated and very hard to use. As a result, we decided to create \"Club Buddy.\"\nWhat it does...\nOur website acts as a user-friendly way for UTD students to find a collective list of all UTD clubs as well as compile a list of their favorite clubs. It also has an Admin function for club presidents to create and post announcements for their club i.e. Next meeting date, time, and other info.\nHow we built it...\nAs a collective group of four, we were able to divide up the work based on specialization and utilized languages like CSS, HTML, JavaScript, React, git, and tool kits like Tailwind CSS. We created the program by using VS Code and pushing and pulling code through GitHub.\nChallenges we ran into...\nLearning to use Tailwind and integrating CSS in React apps was a struggle for us, made worse by very vague error messages, but we learned a lot by overcoming these challenges. Additionally, dividing the work among team members with very varying amounts of experience in React was a challenge, but as a result, we all learned a lot about the framework and are more prepared to use it in the future.\nAccomplishments we are proud of...\nHonestly, we are proud that we participated in our first-ever hackathon and were able to come up with such an innovative idea for such a personal problem. We are proud of being able to learn some languages and come up with adaptive solutions to some of the problems we ran into while coding this project.\nWhat we learned...\nHow to effectively merge git branches from multiple people contributing simultaneously, and resolve merge conflicts. Languages like HTML, CSS, and toolkits such as Tailwind CSS, as well as effectively using VS Code. Furthermore, we learned a lot about the React framework and how to leverage it to create a successful single page app.\nWhat is next for Club Buddy...\nRSS Feed for announcements\nMore robust database backend\nStreamlined favoriting feature\nFuture event handling", "link": "https://devpost.com/software/club-buddy-n3m6vg", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "hackutd 2021: clubbuddy\ninspiration...\nall of us have struggled with finding a good online source for learning and keeping up with utd clubs that we are interested in. the only available source offered by utd is presence.io which is outdated and very hard to use. as a result, we decided to create \"club buddy.\"\nwhat it does...\nour website acts as a user-friendly way for utd students to find a collective list of all utd clubs as well as compile a list of their favorite clubs. it also has an admin function for club presidents to create and post announcements for their club i.e. next meeting date, time, and other info.\nhow we built it...\nas a collective group of four, we were able to divide up the work based on specialization and utilized languages like css, html, javascript, react, git, and -----> tool !!!  kits like tailwind css. we created the program by using vs code and pushing and pulling code through github.\nchallenges we ran into...\nlearning to use tailwind and integrating css in react apps was a struggle for us, made worse by very vague error messages, but we learned a lot by overcoming these challenges. additionally, dividing the work among team members with very varying amounts of experience in react was a challenge, but as a result, we all learned a lot about the framework and are more prepared to use it in the future.\naccomplishments we are proud of...\nhonestly, we are proud that we participated in our first-ever hackathon and were able to come up with such an innovative idea for such a personal problem. we are proud of being able to learn some languages and come up with adaptive solutions to some of the problems we ran into while coding this project.\nwhat we learned...\nhow to effectively merge git branches from multiple people contributing simultaneously, and resolve merge conflicts. languages like html, css, and toolkits such as tailwind css, as well as effectively using vs code. furthermore, we learned a lot about the react framework and how to leverage it to create a successful single page app.\nwhat is next for club buddy...\nrss feed for announcements\nmore robust database backend\nstreamlined favoriting feature\nfuture event handling", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59503062}, {"Unnamed: 0": 3077, "autor": "Cardiovascular Detection Web App", "date": null, "content": "Inspiration\nOur group's goal was to create a project to achieve a better and more sustainable future for all! We wanted a web application that would ensure good health and well-being to everyone and everywhere. We further specialized our approach to solving this worldwide problem in one domain: cardiovascular diseases.\nWhat it does\nOur program trains a model using logistic regression using a set of data with the following variables:\nAge (in days)\nBiological sex at birth\nHeight (in cm)\nWeight (in kg)\nSystolic blood pressure (in mmHg)\nDiastolic blood pressure (in mmHg)\nCholesterol level (categorical; choices are \"normal\", \"above normal\", and \"well above normal\")\nBlood glucose level (categorical; choices are \"normal\", \"above normal\", and \"well above normal\")\nWhether or not the person smokes\nWhether or not the person drinks alcohol\nWhether or not the person exercises regularly\nWhether or not the person has heart disease\nThe user can enter values for all of these variables (except the last one). Based on the values, the user is given a prediction for whether they have cardiovascular disease. Based on that prediction, the program recommends potential next steps with links to resources that a person can use to get potential help or get more information about cardiovascular disease.\nHow we built it\nBack end: The project relies on a machine learning model that uses a logistic regression algorithm. The medical dataset used in this project was obtained from Kaggle and contained various features of 70000 patients' health and a binary indicating whether or not they were each diagnosed with a cardiovascular disease. The pandas library was used to import the dataset and analyze it. Then, using the scikit-learn machine learning library, the dataset was split into training and testing sets. The data was also normalized with feature scaling to make it easier for the model to learn from the data trends. The model was then trained with the training datasets using a logistic regression algorithm. Front end: The front end of our project is a Django based web server. The project uses a basic HTML form to collect the users data, run it through our machine learning model and then direct the user to their appropriate results page. The result pages show transparency by showing the accuracy of the model and graphs representing our data. We want to make it clear to users that this is a prediction and not a diagnosis. We have also added external links to relevant articles as well as converting our Jupyter notebooks into HTML so that the more curious and technically literate users can analyze our process.\nChallenges we ran into\nFrom Django, Git, CSS and HTML, there was a big learning curve getting the technologies to work together smoothly. We spent numerous hours with mentors to push through these problems. There were also issues with the gender input as our dataset only had options for male and female. This does not align with our inclusive mission of providing easier healthcare resources to all individuals as it did not include an \"other\" option for gender. However, this was the only way we could train the machine learning engine effectively.\nAccomplishments that we're proud of\nWe're immensely proud that we were able to launch a web app that can effectively predict a disease as severe as cardiovascular disease. We also feel accomplished about the number of new technologies we learned in the creating process that we can utilize in future projects. Despite the challenges faced, this was a very fun project and allowed us to enjoy various new topics within Computing that my group members and I have never thought about or engaged in before!\nWhat we learned\nSince most of the technology we used for the first time during this hackathon, we learned a lot! Our team members learned Django, Git, CSS, and HTML for the very first time. As well, we spent several hours in the mentor voice channel to confirm our knowledge and debug with the software. In addition, we also learned to use Google Colab and how to create a logistical regression machine learning algorithm, and the difference between logistical and linear regression (a sigmoid function). We visualized the end results of the machine learning engine in attractive graphs using Matplotlib and Seaborn, another useful tool we learned!\nWhat's next for Cardiovascular Detection Web App\nIn the future, we plan to implement more datasets and expand our web application to be able to detect more kinds of prevalent diseases! A few ones we have in mind are Parkinson's disease, diabetes, types of cancer, etc. We also plan to implement a main menu where the user can navigate through which various diseases they plan to detect and immediately get resources that will help serve them. We hope our future additions to this project can contribute towards our bigger goal of achieving a better and more sustainable future for us all.\nCardiovascular Disease Resource List\nHeart & Stroke Foundation \u2014 Cardiovascular Disease https://www.heartandstroke.ca/heart-disease/conditions?gclid=Cj0KCQiA4b2MBhD2ARIsAIrcB-TsaYUb9CNtBzcLNf5bJLNPSQCs5AobQaq-51Y3kTFuvnqhD_FEdWEaAvnEEALw_wcB&gclsrc=aw.ds\nPublic Health Agency of Canada \u2014 Government of Canada https://www.canada.ca/en/public-health/services/publications/diseases-conditions/heart-disease-canada.html\nCardiovascular Disease Patient Resources \u2014 Alberta Health Services https://www.albertahealthservices.ca/info/Page7735.aspx\nPatient Information Resource \u2014 How Secondary Protection Can Keep You Healthy https://thrombosiscanada.ca/secondary-prevention-info/?gclid=Cj0KCQiA4b2MBhD2ARIsAIrcB-S6DlDrm-Nhc7ZfA9PeK2VugwY0Pdhh9ROqUOrNaWfVerlJ5fGt9wEaAhJwEALw_wcB\nCardiovascular Disease \u2014 Public Health Ontario https://www.publichealthontario.ca/en/diseases-and-conditions/chronic-diseases-and-conditions/cardiovascular-disease\nHeart Disease Patient Education \u2014 Centers for Disease Control and Prevention https://www.cdc.gov/heartdisease/materials_for_patients.htm", "link": "https://devpost.com/software/cardiovascular-detection-hackathon", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nour group's goal was to create a project to achieve a better and more sustainable future for all! we wanted a web application that would ensure good health and well-being to everyone and everywhere. we further specialized our approach to solving this worldwide problem in one domain: cardiovascular diseases.\nwhat it does\nour program trains a model using logistic regression using a set of data with the following variables:\nage (in days)\nbiological sex at birth\nheight (in cm)\nweight (in kg)\nsystolic blood pressure (in mmhg)\ndiastolic blood pressure (in mmhg)\ncholesterol level (categorical; choices are \"normal\", \"above normal\", and \"well above normal\")\nblood glucose level (categorical; choices are \"normal\", \"above normal\", and \"well above normal\")\nwhether or not the person smokes\nwhether or not the person drinks alcohol\nwhether or not the person exercises regularly\nwhether or not the person has heart disease\nthe user can enter values for all of these variables (except the last one). based on the values, the user is given a prediction for whether they have cardiovascular disease. based on that prediction, the program recommends potential next steps with links to resources that a person can use to get potential help or get more information about cardiovascular disease.\nhow we built it\nback end: the project relies on a machine learning model that uses a logistic regression algorithm. the medical dataset used in this project was obtained from kaggle and contained various features of 70000 patients' health and a binary indicating whether or not they were each diagnosed with a cardiovascular disease. the pandas library was used to import the dataset and analyze it. then, using the scikit-learn machine learning library, the dataset was split into training and testing sets. the data was also normalized with feature scaling to make it easier for the model to learn from the data trends. the model was then trained with the training datasets using a logistic regression algorithm. front end: the front end of our project is a django based web server. the project uses a basic html form to collect the users data, run it through our machine learning model and then direct the user to their appropriate results page. the result pages show transparency by showing the accuracy of the model and graphs representing our data. we want to make it clear to users that this is a prediction and not a diagnosis. we have also added external links to relevant articles as well as converting our jupyter notebooks into html so that the more curious and technically literate users can analyze our process.\nchallenges we ran into\nfrom django, git, css and html, there was a big learning curve getting the technologies to work together smoothly. we spent numerous hours with mentors to push through these problems. there were also issues with the gender input as our dataset only had options for male and female. this does not align with our inclusive mission of providing easier healthcare resources to all individuals as it did not include an \"other\" option for gender. however, this was the only way we could train the machine learning engine effectively.\naccomplishments that we're proud of\nwe're immensely proud that we were able to launch a web app that can effectively predict a disease as severe as cardiovascular disease. we also feel accomplished about the number of new technologies we learned in the creating process that we can utilize in future projects. despite the challenges faced, this was a very fun project and allowed us to enjoy various new topics within computing that my group members and i have never thought about or engaged in before!\nwhat we learned\nsince most of the technology we used for the first time during this hackathon, we learned a lot! our team members learned django, git, css, and html for the very first time. as well, we spent several hours in the mentor voice channel to confirm our knowledge and debug with the software. in addition, we also learned to use google colab and how to create a logistical regression machine learning algorithm, and the difference between logistical and linear regression (a sigmoid function). we visualized the end results of the machine learning engine in attractive graphs using matplotlib and seaborn, another useful -----> tool !!!  we learned!\nwhat's next for cardiovascular detection web app\nin the future, we plan to implement more datasets and expand our web application to be able to detect more kinds of prevalent diseases! a few ones we have in mind are parkinson's disease, diabetes, types of cancer, etc. we also plan to implement a main menu where the user can navigate through which various diseases they plan to detect and immediately get resources that will help serve them. we hope our future additions to this project can contribute towards our bigger goal of achieving a better and more sustainable future for us all.\ncardiovascular disease resource list\nheart & stroke foundation \u2014 cardiovascular disease https://www.heartandstroke.ca/heart-disease/conditions?gclid=cj0kcqia4b2mbhd2arisaircb-tsayub9cntbzclnf5bjlnpsqcs5aobqaq-51y3ktfuvnqhd_fedweaavneealw_wcb&gclsrc=aw.ds\npublic health agency of canada \u2014 government of canada https://www.canada.ca/en/public-health/services/publications/diseases-conditions/heart-disease-canada.html\ncardiovascular disease patient resources \u2014 alberta health services https://www.albertahealthservices.ca/info/page7735.aspx\npatient information resource \u2014 how secondary protection can keep you healthy https://thrombosiscanada.ca/secondary-prevention-info/?gclid=cj0kcqia4b2mbhd2arisaircb-s6dldrm-nhc7zfa9pek2vugwy0pdhh9roquornawfverlj5fgt9weaahjwealw_wcb\ncardiovascular disease \u2014 public health ontario https://www.publichealthontario.ca/en/diseases-and-conditions/chronic-diseases-and-conditions/cardiovascular-disease\nheart disease patient education \u2014 centers for disease control and prevention https://www.cdc.gov/heartdisease/materials_for_patients.htm", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59503077}, {"Unnamed: 0": 3116, "autor": "Septre", "date": null, "content": "Nearly nine million people die every year due to inadequate healthcare in developing countries.1\nHalf the world\u2019s population lacks access to basic health services2. How can we make solutions in global healthcare that are effective? How can we do things more efficiently, at a lower cost, to help reach more people?\nWe identified five key areas that an effective solution excels at:\nPerformance\nCost\nAccessibility\nSpeed\nGenerality\nWith our project, we identified a solution that\u2019s all five of these things. Septer is a pre-screening tool that will enable rural healthcare facilities to effectively manage large number of patients and, ultimately, help more people and save more lives.\nWith septre, we use neural networks to pre-screen for 7 disease categories. All you need is a smartphone and a magnifying glass.\nScepter uses the human retina - an extremely powerful tissue for diagnosis. It makes for a general solution, enabling us to identify multiple diseases with just one test. To forgo the need of human expertise during this preliminary pre-screening phase, we instead use an app, taking advantage of a convolutional neural network to detect diseases in retinal images.\nThese diseases are:\nDiabetes\nHypertension\nGlaucoma\nCataracts\nAge-related Macular Degeneration\nPathological Myopia\nOther Abnormalities\nInstruments required for retinal imaging can cost tens of thousands of dollars - they\u2019re often clunky, expensive machines. Instead, we take advantage of a smartphone-lens approach, requiring only that the pupil is already dilated (something that typically requires pupil dilation solution). By placing a 20/28 diopter lens a short distance from the eye and a smartphone camera (with flash on) some 30 cm away from the lens, we capture a retinal image.\nWe were able to purchase a lens combination with the required power for under 10 dollars. Of course, using this approach yields poorer image quality and disturbances - something we handle via simulation. By using a smartphone, we also immediately have access to a telecommunications device that can perform on-device computations and communicate with a server.\nOur Implementation\nWe took advantage of modern web technology to create an application that was cheap, yet scalable. The mobile app is made in Swift and is accessible from iOS devices. We also made a Progressive Web Application for access from desktop and non-Apple devices. The interface is backed by an API written in Python using the Flask framework and a PostgreSQL database. This cloud-first architecture makes our application accessible from everywhere, while our application's simplicity means operators have a shallow learning curve.\nThe neural network was trained using the ODIR-5K retina dataset, collected from several hospitals. We wrote scripts to intentionally create glares, lens scratches, varying orientations and resolutions, blurs, and shadows. This enabled us to make a dataset more representative of smartphone-based imaging. With this augmented dataset, we trained a two-branch convolutional neural network to use images of two human retinas to produce a diagnosis. After fine-tuning using a partitioned validation set, we performed tests on a partitioned testing set and achieved (weighted) accuracies of XX%.\n1https://www.weforum.org/agenda/2019/11/effects-and-costs-of-poor-quality-healthcare/\n2https://www.who.int/news/item/13-12-2017-world-bank-and-who-half-the-world-lacks-access-to-essential-health-services-100-million-still-pushed-into-extreme-poverty-because-of-health-expenses", "link": "https://devpost.com/software/septre", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "nearly nine million people die every year due to inadequate healthcare in developing countries.1\nhalf the world\u2019s population lacks access to basic health services2. how can we make solutions in global healthcare that are effective? how can we do things more efficiently, at a lower cost, to help reach more people?\nwe identified five key areas that an effective solution excels at:\nperformance\ncost\naccessibility\nspeed\ngenerality\nwith our project, we identified a solution that\u2019s all five of these things. septer is a pre-screening -----> tool !!!  that will enable rural healthcare facilities to effectively manage large number of patients and, ultimately, help more people and save more lives.\nwith septre, we use neural networks to pre-screen for 7 disease categories. all you need is a smartphone and a magnifying glass.\nscepter uses the human retina - an extremely powerful tissue for diagnosis. it makes for a general solution, enabling us to identify multiple diseases with just one test. to forgo the need of human expertise during this preliminary pre-screening phase, we instead use an app, taking advantage of a convolutional neural network to detect diseases in retinal images.\nthese diseases are:\ndiabetes\nhypertension\nglaucoma\ncataracts\nage-related macular degeneration\npathological myopia\nother abnormalities\ninstruments required for retinal imaging can cost tens of thousands of dollars - they\u2019re often clunky, expensive machines. instead, we take advantage of a smartphone-lens approach, requiring only that the pupil is already dilated (something that typically requires pupil dilation solution). by placing a 20/28 diopter lens a short distance from the eye and a smartphone camera (with flash on) some 30 cm away from the lens, we capture a retinal image.\nwe were able to purchase a lens combination with the required power for under 10 dollars. of course, using this approach yields poorer image quality and disturbances - something we handle via simulation. by using a smartphone, we also immediately have access to a telecommunications device that can perform on-device computations and communicate with a server.\nour implementation\nwe took advantage of modern web technology to create an application that was cheap, yet scalable. the mobile app is made in swift and is accessible from ios devices. we also made a progressive web application for access from desktop and non-apple devices. the interface is backed by an api written in python using the flask framework and a postgresql database. this cloud-first architecture makes our application accessible from everywhere, while our application's simplicity means operators have a shallow learning curve.\nthe neural network was trained using the odir-5k retina dataset, collected from several hospitals. we wrote scripts to intentionally create glares, lens scratches, varying orientations and resolutions, blurs, and shadows. this enabled us to make a dataset more representative of smartphone-based imaging. with this augmented dataset, we trained a two-branch convolutional neural network to use images of two human retinas to produce a diagnosis. after fine-tuning using a partitioned validation set, we performed tests on a partitioned testing set and achieved (weighted) accuracies of xx%.\n1https://www.weforum.org/agenda/2019/11/effects-and-costs-of-poor-quality-healthcare/\n2https://www.who.int/news/item/13-12-2017-world-bank-and-who-half-the-world-lacks-access-to-essential-health-services-100-million-still-pushed-into-extreme-poverty-because-of-health-expenses", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 1, "media": null, "medialink": null, "identifyer": 59503116}, {"Unnamed: 0": 3118, "autor": "parKING", "date": null, "content": "Inspiration\nIn 2020, 168,323 vehicles were stolen in California, at an estimated total value of approximately $1.6 billion.1 This is a 19.6 percent increase from the 2019 total for vehicle thefts statewide. The average rate of theft in 2020 was one vehicle every 3 minutes. Apart from vehicle thefts, we also considered other crimes while building our model that would guide the user by a simple ranking system to a safer and more cost-efficient on-street parking spot.\nWhat it does\nparKING is a progressive web application that lets city governments and communities gain analysis and find the best open on-street parking lots based on simple 1-5 rankings that are been trained by multiple ML models taking the safety (Crime Rate Dataset), Rates (INRIX Parking API Dataset), Proximity to destination (INRIX Segment Roads Polyline Source Proximity Dataset), spot covered with Traffic Cameras (INRIX Traffic Camera API Dataset) into consideration. First, on the government page, city planners enter the Latitude and Longitude for the location they want to analyze the parking lot on streets. Once this data is provided, it is sent to the server, where a model and Graphical User Interface (GUI) are created for that specific city. The GUI is built from training models in our backend server rendering data from sources such as INRIX Traffic Rules APIs, INRIX Segment Roads APIs, INRIX On-Street Parking APIs, Google Maps APIs, and we use polyline to geocode locations and perform distance/metric calculations. Our algorithm is trained on a dataset from San Francisco that contains crime rates, and their attributes such as midnight crime_rates. After aggregating data from the dataset that has crime rates at each street, so when we train our model, the model learns how each specific road safety individually affects ranking. Therefore, when given a prediction scenario with specific road features, the model can predict the new ranking, which is all derived from multiple factors like cost, proximity from users, open spots, Traffic cameras on the street. Our model is a Random Forest (Classification Algorithm) model, which we found performed best when compared to other Classification Algorithms like Decision Trees and Logistic Regression (models are well documented in notebooks). We trained our model for 4 segments and connected it to the GUI for San Francisco. Often, local governments hire private contractors for infrastructure endeavors. Using our product parKING, we allow private contractors to analyze better the existing city on street plans and then submit their proposals to the city government. This allows contractors to directly communicate and propose changes on public forums. Finally, in terms of the B2C perspective, community members can get more information about on-street parking and can use our visualization tool to visualize the parking lot ranks and a lot more information on the parameters its been ranked !!!\nHow we built it\nWe used the Inrix Lots API to get the data for parking cost and overnight parking. Also, we used the Inrix Traffic Camera API to get the data for the location of traffic cameras. The crime data and proximity from destination data were pulled from 3rd party API. Merging these datasets, we were able to create a data frame with the required metrics. We used Machine Learning to train and test our data with 75% accuracy. We coded the entire app in the following languages/frameworks: HTML, CSS, Javascript, and Python. We used Google Cloud for our backend and GUI. We developed our interactive GUI with INRIX APIs, Google Maps, and polyline functions. We built our ML models with Sklearn and Keras. We hosted our website through Heroku and Github. We collected our data from the SF Police Data website and other open-source sites.\nChallenges we ran into\nWe believe we could\u2019ve gathered much more insights and modeled a better ML tool if we had more data. For example, the Lots API data did not provide us with the latitude and longitude of the parking spot. The primary challenge that we ran into was developing our geographic models. Since the data was very complex and required cleaning, we weren\u2019t sure how to start. Luckily, we were able to do enough EDA to understand how to develop the models and utilize the data. Training these models was also a huge challenge because of the sheer size of the data. While we were not able to deploy our models, as they are too large to deploy on free and available servers, as long as governments give us data, we can produce models and GUIs for them.\nAccomplishments that we're proud of\nOur tool successfully predicts the \u2018best\u2019 parking spot giving the user a lot more information and thus, helping them in better decision making while choosing the right spot to park their vehicle.\nWhat we learned\nOur team found it incredibly fulfilling to use our ML knowledge in a way that could effectively assist governments and customers. We are glad that we were able to develop models to help a vast range of people. Seeing how we could use our software engineering skills to impact people\u2019s daily lives was the highlight of our weekend. From a software perspective, developing a large-scale model and a GUI was our focus this weekend. We learned how to use great libraries such as polyline and INRIX APIs. We grew our web development skills and polished our ML model selection and training skills.\nWhat's next for parKING\nWe believe that our application would be best implemented on a local and state government level. City planners and government officials currently do not have a way to effectively optimize on-street parking lots, but with ML tools and solutions, we believe on-street parking can be made safer and efficient. In terms of our application, we would love to deploy our models on the web and streamline the process of collecting and preparing data, training a model, and creating a GUI all in one step. Given that our current situation prevents us from buying a web server capable of running all those processes at once, we look forward to acquiring a web server that can process high-level computation. Lastly, we would like to refine our algorithms to incorporate more important traffic parameters and road features.", "link": "https://devpost.com/software/parking-npxt9j", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nin 2020, 168,323 vehicles were stolen in california, at an estimated total value of approximately $1.6 billion.1 this is a 19.6 percent increase from the 2019 total for vehicle thefts statewide. the average rate of theft in 2020 was one vehicle every 3 minutes. apart from vehicle thefts, we also considered other crimes while building our model that would guide the user by a simple ranking system to a safer and more cost-efficient on-street parking spot.\nwhat it does\nparking is a progressive web application that lets city governments and communities gain analysis and find the best open on-street parking lots based on simple 1-5 rankings that are been trained by multiple ml models taking the safety (crime rate dataset), rates (inrix parking api dataset), proximity to destination (inrix segment roads polyline source proximity dataset), spot covered with traffic cameras (inrix traffic camera api dataset) into consideration. first, on the government page, city planners enter the latitude and longitude for the location they want to analyze the parking lot on streets. once this data is provided, it is sent to the server, where a model and graphical user interface (gui) are created for that specific city. the gui is built from training models in our backend server rendering data from sources such as inrix traffic rules apis, inrix segment roads apis, inrix on-street parking apis, google maps apis, and we use polyline to geocode locations and perform distance/metric calculations. our algorithm is trained on a dataset from san francisco that contains crime rates, and their attributes such as midnight crime_rates. after aggregating data from the dataset that has crime rates at each street, so when we train our model, the model learns how each specific road safety individually affects ranking. therefore, when given a prediction scenario with specific road features, the model can predict the new ranking, which is all derived from multiple factors like cost, proximity from users, open spots, traffic cameras on the street. our model is a random forest (classification algorithm) model, which we found performed best when compared to other classification algorithms like decision trees and logistic regression (models are well documented in notebooks). we trained our model for 4 segments and connected it to the gui for san francisco. often, local governments hire private contractors for infrastructure endeavors. using our product parking, we allow private contractors to analyze better the existing city on street plans and then submit their proposals to the city government. this allows contractors to directly communicate and propose changes on public forums. finally, in terms of the b2c perspective, community members can get more information about on-street parking and can use our visualization -----> tool !!!  to visualize the parking lot ranks and a lot more information on the parameters its been ranked !!!\nhow we built it\nwe used the inrix lots api to get the data for parking cost and overnight parking. also, we used the inrix traffic camera api to get the data for the location of traffic cameras. the crime data and proximity from destination data were pulled from 3rd party api. merging these datasets, we were able to create a data frame with the required metrics. we used machine learning to train and test our data with 75% accuracy. we coded the entire app in the following languages/frameworks: html, css, javascript, and python. we used google cloud for our backend and gui. we developed our interactive gui with inrix apis, google maps, and polyline functions. we built our ml models with sklearn and keras. we hosted our website through heroku and github. we collected our data from the sf police data website and other open-source sites.\nchallenges we ran into\nwe believe we could\u2019ve gathered much more insights and modeled a better ml tool if we had more data. for example, the lots api data did not provide us with the latitude and longitude of the parking spot. the primary challenge that we ran into was developing our geographic models. since the data was very complex and required cleaning, we weren\u2019t sure how to start. luckily, we were able to do enough eda to understand how to develop the models and utilize the data. training these models was also a huge challenge because of the sheer size of the data. while we were not able to deploy our models, as they are too large to deploy on free and available servers, as long as governments give us data, we can produce models and guis for them.\naccomplishments that we're proud of\nour tool successfully predicts the \u2018best\u2019 parking spot giving the user a lot more information and thus, helping them in better decision making while choosing the right spot to park their vehicle.\nwhat we learned\nour team found it incredibly fulfilling to use our ml knowledge in a way that could effectively assist governments and customers. we are glad that we were able to develop models to help a vast range of people. seeing how we could use our software engineering skills to impact people\u2019s daily lives was the highlight of our weekend. from a software perspective, developing a large-scale model and a gui was our focus this weekend. we learned how to use great libraries such as polyline and inrix apis. we grew our web development skills and polished our ml model selection and training skills.\nwhat's next for parking\nwe believe that our application would be best implemented on a local and state government level. city planners and government officials currently do not have a way to effectively optimize on-street parking lots, but with ml tools and solutions, we believe on-street parking can be made safer and efficient. in terms of our application, we would love to deploy our models on the web and streamline the process of collecting and preparing data, training a model, and creating a gui all in one step. given that our current situation prevents us from buying a web server capable of running all those processes at once, we look forward to acquiring a web server that can process high-level computation. lastly, we would like to refine our algorithms to incorporate more important traffic parameters and road features.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59503118}, {"Unnamed: 0": 3128, "autor": "Stock Sonar", "date": null, "content": "Inspiration\nRetail investor activity is steadily increasing, and they are influencing the market in ways that have not been seen before. These new investors are inspiring others to follow suit on social media. This is an exciting new source of data that can be used to guide investors.\nNew retail investors don\u2019t often have the information they need to make educated investments. We aim to provide an advanced, cutting-edge tool to help them with stock picking.\nWhat it does\nOur app is a fully-functional stock aggregator and analyzer that focuses on providing data relevant to the most recent market. Using sentiment analysis, we provide accurate predictions of the current market to our users while allowing them to request data about stocks that fall under their own interest. Using the aggregated data, we provide a 'Top 9' to our users, detailing the top stocks in the media and notifying them of any changes to their own interests.\nHow we built it\nFront-end\nAll front-end components were create from scratch as React components and were styled using TailwindCSS. The front-end is single-page, container-based allowing for an intuitive, simplistic design that provides the user with every feature in our app without the overwhelming layout of most single-page apps. To integrate with Firebase, we used simple HTTP requests to our NextJS API for quick and easy data-fetching across multiple schemas, including user accounts and stock data.\nBack-end\nUsing data from Twitter and r/wallstreetbets, we analyzed thousands of messages containing keywords on potential stocks that are hot on the market. The stocks are ranked using the frequency at which people are discussing their ticker symbols, as well as the overall polarity of the online discussions. The sentiment was computed using VaderSentiment. For each stock, tweets with high sentiment and following (determined by the number of likes and retweets) were cached for review by users, along with relevant tweets and other financial data regarding the stock. Finally, the front-end consumed this data and displayed it dynamically for users to interact with.\nChallenges we ran into\nFront-end\nThe main challenges were figuring out our intuitive, simplistic design without overwhelming users with components and visuals. To solve our challenge, we implemented basic vertical scrolling throughout our three main containers to allow for an abundance of visual data, while still allowing the user to have full control over what they would like to see.\nBack-end\nIn order to determine which stocks are trending we ran a sentiment analysis algorithm on twitter tweets, and used the analysis to rank stocks on how trending they are. We also analyzed posts on the wallstreetbets subreddit to find key information that could be a big influence on stocks.\nAccomplishments that we're proud of\nAs a whole, this project was outstandingly successful. Every feature that we sought out to build was implemented similar to our designs, and the application works incredibly cohesively to provide users with the best experience possible. The biggest accomplishments we're proud of are the financial and media data displays and sentiment analysis on the front and back-ends, respectively. Each feature is important to the overall impact of the website, and without a doubt took the most effort.\nWhat we learned\nIt's hard to build a feature-packed, full-stack web application in less than 24 hours. Each of our members worked tirelessly to achieve this result, and are incredibly thankful to have learned so much about the different technologies and tools used through our experience. Mainly, the front-end focused their learning and exploration on the sophistication of cohesive UI components in React, while the back-end focused on optimal data-fetching and caching for the front-end to display in the UI.\nWhat's next for StockSonar\nImproving sentiment analysis and providing more user-driven and user-specified metrics are our biggest goals for the next iteration of StockSonar.", "link": "https://devpost.com/software/stock-sonar", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nretail investor activity is steadily increasing, and they are influencing the market in ways that have not been seen before. these new investors are inspiring others to follow suit on social media. this is an exciting new source of data that can be used to guide investors.\nnew retail investors don\u2019t often have the information they need to make educated investments. we aim to provide an advanced, cutting-edge -----> tool !!!  to help them with stock picking.\nwhat it does\nour app is a fully-functional stock aggregator and analyzer that focuses on providing data relevant to the most recent market. using sentiment analysis, we provide accurate predictions of the current market to our users while allowing them to request data about stocks that fall under their own interest. using the aggregated data, we provide a 'top 9' to our users, detailing the top stocks in the media and notifying them of any changes to their own interests.\nhow we built it\nfront-end\nall front-end components were create from scratch as react components and were styled using tailwindcss. the front-end is single-page, container-based allowing for an intuitive, simplistic design that provides the user with every feature in our app without the overwhelming layout of most single-page apps. to integrate with firebase, we used simple http requests to our nextjs api for quick and easy data-fetching across multiple schemas, including user accounts and stock data.\nback-end\nusing data from twitter and r/wallstreetbets, we analyzed thousands of messages containing keywords on potential stocks that are hot on the market. the stocks are ranked using the frequency at which people are discussing their ticker symbols, as well as the overall polarity of the online discussions. the sentiment was computed using vadersentiment. for each stock, tweets with high sentiment and following (determined by the number of likes and retweets) were cached for review by users, along with relevant tweets and other financial data regarding the stock. finally, the front-end consumed this data and displayed it dynamically for users to interact with.\nchallenges we ran into\nfront-end\nthe main challenges were figuring out our intuitive, simplistic design without overwhelming users with components and visuals. to solve our challenge, we implemented basic vertical scrolling throughout our three main containers to allow for an abundance of visual data, while still allowing the user to have full control over what they would like to see.\nback-end\nin order to determine which stocks are trending we ran a sentiment analysis algorithm on twitter tweets, and used the analysis to rank stocks on how trending they are. we also analyzed posts on the wallstreetbets subreddit to find key information that could be a big influence on stocks.\naccomplishments that we're proud of\nas a whole, this project was outstandingly successful. every feature that we sought out to build was implemented similar to our designs, and the application works incredibly cohesively to provide users with the best experience possible. the biggest accomplishments we're proud of are the financial and media data displays and sentiment analysis on the front and back-ends, respectively. each feature is important to the overall impact of the website, and without a doubt took the most effort.\nwhat we learned\nit's hard to build a feature-packed, full-stack web application in less than 24 hours. each of our members worked tirelessly to achieve this result, and are incredibly thankful to have learned so much about the different technologies and tools used through our experience. mainly, the front-end focused their learning and exploration on the sophistication of cohesive ui components in react, while the back-end focused on optimal data-fetching and caching for the front-end to display in the ui.\nwhat's next for stocksonar\nimproving sentiment analysis and providing more user-driven and user-specified metrics are our biggest goals for the next iteration of stocksonar.", "sortedWord": "None", "removed": "Nan", "score": 7, "comments": 2, "media": null, "medialink": null, "identifyer": 59503128}, {"Unnamed: 0": 3130, "autor": "The Parcel Panthers", "date": null, "content": "Inspiration\nEnabling those who still require methods of receiving packages at covert (discrete) and protected locations. Further, streamlining NJIT's student-oriented work overhead with an easy-to-use and informative logging system.\nWhat it does\nInformatively directs users to identifying the Parcel Panther software and its capabilities, currently \"supported\" locations, implementable actions (with supported front to backend tools), demonstrable view-based package data-logs, interactable \"check-out\" page, as well as an implementable administrative tool.\nHow we built it\nDomain.com provided the initial hosting site (parcelpanther.tech) with their free 6-page website designer tool. The customization of these html pages led to further development with Jet Brains: WebStorm, PhpStorm, and Pycharm. Github was utilized for our centralized version control, alongside Dockerhub for containerization and Azure (http://parcelpanthers.eastus.azurecontainer.io/) for hosting. Google Cloud succeeded storage and some scripting (for OCR and communication with current standing database as a local csv dynamically updated by Google Sheets) the initial backend designs with Azure storage containers and initially cockroachdb, both had to be temporarily replaced due to php configurations that allows Jet Brains to connect but not javascript to php for server responses (Virtualbox was utilized to create an Ubuntu LTS machine for running a cluster with cockroachdb).\nChallenges we ran into\nPhp communication issues hindered our success with cockroachdb, Google Cloud as well as Azure credits had conflicts with previous courses being expended (some teammates have courses that are intensive with azure and the $100 student allotment did not stack which made deployment and hosting limited). Domain.com's website design did provide the most customizable features which required external tools (or a commercial account) to meet the project scope and user experience.\nAccomplishments that we're proud of\nWe are proud of the development of our mission statement and further refinement our what we believe our software can mature into with proper support. Deployment both as a professional for NJIT Residence Life package logging and philanthropic for discrete package notification and location identification seems achievable.\nWhat we learned\nRequesting assistance through volunteers, executive board members, and sponsors made our small team size manageable and the information gained more impactful.\nWhat's next for The Parcel Panthers\nWe look forward to smoothing out and applying refinements to our database to be utilized in both a community and professional environment.", "link": "https://devpost.com/software/the-parcel-panthers", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nenabling those who still require methods of receiving packages at covert (discrete) and protected locations. further, streamlining njit's student-oriented work overhead with an easy-to-use and informative logging system.\nwhat it does\ninformatively directs users to identifying the parcel panther software and its capabilities, currently \"supported\" locations, implementable actions (with supported front to backend tools), demonstrable view-based package data-logs, interactable \"check-out\" page, as well as an implementable administrative -----> tool !!! .\nhow we built it\ndomain.com provided the initial hosting site (parcelpanther.tech) with their free 6-page website designer tool. the customization of these html pages led to further development with jet brains: webstorm, phpstorm, and pycharm. github was utilized for our centralized version control, alongside dockerhub for containerization and azure (http://parcelpanthers.eastus.azurecontainer.io/) for hosting. google cloud succeeded storage and some scripting (for ocr and communication with current standing database as a local csv dynamically updated by google sheets) the initial backend designs with azure storage containers and initially cockroachdb, both had to be temporarily replaced due to php configurations that allows jet brains to connect but not javascript to php for server responses (virtualbox was utilized to create an ubuntu lts machine for running a cluster with cockroachdb).\nchallenges we ran into\nphp communication issues hindered our success with cockroachdb, google cloud as well as azure credits had conflicts with previous courses being expended (some teammates have courses that are intensive with azure and the $100 student allotment did not stack which made deployment and hosting limited). domain.com's website design did provide the most customizable features which required external tools (or a commercial account) to meet the project scope and user experience.\naccomplishments that we're proud of\nwe are proud of the development of our mission statement and further refinement our what we believe our software can mature into with proper support. deployment both as a professional for njit residence life package logging and philanthropic for discrete package notification and location identification seems achievable.\nwhat we learned\nrequesting assistance through volunteers, executive board members, and sponsors made our small team size manageable and the information gained more impactful.\nwhat's next for the parcel panthers\nwe look forward to smoothing out and applying refinements to our database to be utilized in both a community and professional environment.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59503130}, {"Unnamed: 0": 3143, "autor": "Mind Matters", "date": null, "content": "Track: Mental Health\nInspiration\nIsolation has brought so much focus on ourselves, but not enough focus on how we feel. The unprecedented changes that we have endured as well as the loss of interaction that isolation introduced has made taking care of ourselves harder than ever. We wanted to fight the lack of conversation around our emotions and create an accessible tool that people can use to become more involved with their mental health and give them the power to understand their emotions.\nWhat it does\nMind Matters is a journal app that lets users log their emotions and mood each day. After submitting their mood and emotions, Mind Matter will find various resources to provide the user to help them process their emotions and understand their feelings, depending on their emotion and how they described their feelings through a keyword search. Had a rocking day? That's great! Had a rough one? Here's an affirmation to help get you through. Whichever emotion you are feeling, Mind Matters will walk you through it.", "link": "https://devpost.com/software/mind-matters-wvg25b", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "track: mental health\ninspiration\nisolation has brought so much focus on ourselves, but not enough focus on how we feel. the unprecedented changes that we have endured as well as the loss of interaction that isolation introduced has made taking care of ourselves harder than ever. we wanted to fight the lack of conversation around our emotions and create an accessible -----> tool !!!  that people can use to become more involved with their mental health and give them the power to understand their emotions.\nwhat it does\nmind matters is a journal app that lets users log their emotions and mood each day. after submitting their mood and emotions, mind matter will find various resources to provide the user to help them process their emotions and understand their feelings, depending on their emotion and how they described their feelings through a keyword search. had a rocking day? that's great! had a rough one? here's an affirmation to help get you through. whichever emotion you are feeling, mind matters will walk you through it.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59503143}, {"Unnamed: 0": 3150, "autor": "Self Care Scheduler", "date": null, "content": "Inspiration\nIn the age of social media, it is very difficult to focus on mental health and self care. We wanted to create a tool that targets this problem.\nWhat it does\nSelf Care Scheduler is a web application that allows users to both visualize their moods throughout the progression of a month, and also schedule and plan out times for themselves.\nHow we built it\nWe built this using React JS and Material UI for styling. We used GitHub for collaborative purposes.\nChallenges we ran into\nWe ran into some minor challenges when we were trying to implement the Google Calendar API for an earlier version of this project, and we would like to integrate it with that platform if we had more time to work on this project.\nAccomplishments that we're proud of\nWe are proud of creating a functional Calendar web application in less than 24 hours.\nWhat's next for Self Care Scheduler\nWe would like to integrate it with the Google Calendar API, increase mood customization, and add suggested self care events.", "link": "https://devpost.com/software/self-care-scheduler", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nin the age of social media, it is very difficult to focus on mental health and self care. we wanted to create a -----> tool !!!  that targets this problem.\nwhat it does\nself care scheduler is a web application that allows users to both visualize their moods throughout the progression of a month, and also schedule and plan out times for themselves.\nhow we built it\nwe built this using react js and material ui for styling. we used github for collaborative purposes.\nchallenges we ran into\nwe ran into some minor challenges when we were trying to implement the google calendar api for an earlier version of this project, and we would like to integrate it with that platform if we had more time to work on this project.\naccomplishments that we're proud of\nwe are proud of creating a functional calendar web application in less than 24 hours.\nwhat's next for self care scheduler\nwe would like to integrate it with the google calendar api, increase mood customization, and add suggested self care events.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59503150}, {"Unnamed: 0": 3153, "autor": "plan.degree", "date": null, "content": "Inspiration\nWhen I used to be a freshman (2018), especially my first semester, I can recall numerous frustrations and back-and-forth emails and appointments with my academic advisors regarding my degree plan. I was overwhelmed, confused, and also sympathized with the advisors having to explain the same degree plan with 100's of students.\nI had a vision in my Sophomore year (2019) to build a very easy to use degree planning tool that can help ease people's mind's and make sure every student has a solid graduation plan. No one would have to graduate a semester later because of poor planning.\nI didn't think it was much of a problem so didn't dedicate time to the development of it. However, recently I met up with a couple academic advisors and they proved me otherwise. It is a very big problem a lot of students in universities face, and even a tiny little application like mine that can help at least a handful of students get a better picture of their college career is worth taking the effort from my part.\nWhat it does\nplan.degree is a web application where students can log into and view all their different degree plans they have been looking at, customize their workload for the semester with a very intuitive drag and drop design, and validate their plans by making sure there are no pre-requisite conflicts.\nHow we built it\nplan.degree is built using a Javascript React front-end utilizing the Semantic-UI library for modern UI components, a Python Flask back-end, with a MongoDB cluster as the database. The user data is taken very seriously and has been implemented in a way that is very secure by using Auth0 authentication workflow. Had to use Selenium in Python to scrape and process data for further usage.\nChallenges we ran into\nI worked on this project alone. That was my biggest challenge. I wanted to implement so many features like: Tracking graduation goals, exporting degree plans to pdf (printing), sharing schedules with friends, quick academic advisor lookup, etc, but I was unable to due to the time constraints. Nevertheless, I believe I have done a significantly good job in building this project as it does what it promises: lets you visualize your degree plan conveniently.\nOther than that, I faced a couple issues working with course data as there was no official place for me to get it and had to rely on some of my custom made tools to scrape UT Dallas websites to retrieve data, and also the nebula api provided at the hackathon.\nAccomplishments that we're proud of\nBeing an exclusive back-end engineer, I had a very hard time making the front-end of the application modern and neat (centering a <div> \ud83d\ude2d). The final product turned out pretty decent and I am very proud of that. It is intuitive and simple, just like I envisioned.\nWhat's next for plan.degree\nI can promise you, this is not the end of plan.degree. The scope of this project is much bigger than just a hackathon and it will be something that will make students lives beyond UT Dallas easier.\nThere are a lot more features planned and will be rolled out later on, some of them include (from earlier):\nGraduation tracker\nExport degree plans to PDF (make it printable for convenience)\nIntegrate easy access to class data like grade distributions, professor reviews, and general class difficulty\nAuto recommend electives to take\nCreate semester course schedules and automatically detect classes with your friends\nDouble major and minor support", "link": "https://devpost.com/software/plan-degree", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwhen i used to be a freshman (2018), especially my first semester, i can recall numerous frustrations and back-and-forth emails and appointments with my academic advisors regarding my degree plan. i was overwhelmed, confused, and also sympathized with the advisors having to explain the same degree plan with 100's of students.\ni had a vision in my sophomore year (2019) to build a very easy to use degree planning -----> tool !!!  that can help ease people's mind's and make sure every student has a solid graduation plan. no one would have to graduate a semester later because of poor planning.\ni didn't think it was much of a problem so didn't dedicate time to the development of it. however, recently i met up with a couple academic advisors and they proved me otherwise. it is a very big problem a lot of students in universities face, and even a tiny little application like mine that can help at least a handful of students get a better picture of their college career is worth taking the effort from my part.\nwhat it does\nplan.degree is a web application where students can log into and view all their different degree plans they have been looking at, customize their workload for the semester with a very intuitive drag and drop design, and validate their plans by making sure there are no pre-requisite conflicts.\nhow we built it\nplan.degree is built using a javascript react front-end utilizing the semantic-ui library for modern ui components, a python flask back-end, with a mongodb cluster as the database. the user data is taken very seriously and has been implemented in a way that is very secure by using auth0 authentication workflow. had to use selenium in python to scrape and process data for further usage.\nchallenges we ran into\ni worked on this project alone. that was my biggest challenge. i wanted to implement so many features like: tracking graduation goals, exporting degree plans to pdf (printing), sharing schedules with friends, quick academic advisor lookup, etc, but i was unable to due to the time constraints. nevertheless, i believe i have done a significantly good job in building this project as it does what it promises: lets you visualize your degree plan conveniently.\nother than that, i faced a couple issues working with course data as there was no official place for me to get it and had to rely on some of my custom made tools to scrape ut dallas websites to retrieve data, and also the nebula api provided at the hackathon.\naccomplishments that we're proud of\nbeing an exclusive back-end engineer, i had a very hard time making the front-end of the application modern and neat (centering a <div> \ud83d\ude2d). the final product turned out pretty decent and i am very proud of that. it is intuitive and simple, just like i envisioned.\nwhat's next for plan.degree\ni can promise you, this is not the end of plan.degree. the scope of this project is much bigger than just a hackathon and it will be something that will make students lives beyond ut dallas easier.\nthere are a lot more features planned and will be rolled out later on, some of them include (from earlier):\ngraduation tracker\nexport degree plans to pdf (make it printable for convenience)\nintegrate easy access to class data like grade distributions, professor reviews, and general class difficulty\nauto recommend electives to take\ncreate semester course schedules and automatically detect classes with your friends\ndouble major and minor support", "sortedWord": "None", "removed": "Nan", "score": 9, "comments": 0, "media": null, "medialink": null, "identifyer": 59503153}, {"Unnamed: 0": 3155, "autor": "Predicting Intervals of Volatility and Exploiting Sentiment", "date": null, "content": "What it does\nExtrapolating Sentiment Via r/wallstreetbets\nOn our trial run of the program, we generated a data set of the 10 most mentioned stock tickers and their associated vernacular. We then utilized Asad70\u2019s application (His Project) to perform sentiment analysis upon the top 5 tickers. This analysis returned values for three possible trends \u2014 Bearish, Neutral, and Bullish \u2014 ranging from 0 to 1. These values were then compounded into a total trend value for the stock in the range of -1 (most bearish) to 1 (most bullish).\nWhat Can Traders Do With This Information?\nTraders can exploit the correlation between number of queries, trading volumes, volatility, daily change in price, and bullish/bearish sentiment to predict future stock market trends.\nThanks to the 1-3 day delay between an increase in queries and an increase in trading volume, investors can use real time query tracking and predictive analysis to forecast the activity of stocks several hours preceding relevant activity.\nHow We Built It\nWe combined academic research, existing APIS, and new code to generate a predictive tool and the respective trading strategy for investors.\nWhat we did:\nAnalyzed 5 academic papers exploring the correlations between the number of search engine queries about a stock, its trading volumes, its volatility, and its daily change in price.\nCreated and integrated a personal Reddit Script API\nModified Asad70\u2019s Application reddit-sentiment-analysis to encompass our Reddit Script API\nTested our output by generating a data set of the 10 most mentioned stock tickers on r/wallstreetbets and performing sentiment analysis on the top 5 picks\nModified Code\nOriginal\nimport re import en_core_web_sm\nnlp = en_core_web_sm.load() Stopwords = nlp.Defaults.stop_words\nreddit = praw.Reddit(user_agent=\"Comment Extraction\", client_id=\"\", client_secret=\"\", username=\"\", password=\"\")\nModified\nimport spacy #import en_core_web_sm\n#nlp = en_core_web_sm.load() stopwords = []#nlp.Defaults.stop_words\nreddit = praw.Reddit(user_agent=\"Comment Extraction\", client_id=\"qs4uiqJHU2eiLzYneR####\", client_secret=\"MkWEIZsGtkVKQZyWj29KcEjOBE####\",username=\"Up2Early4this\",password=\"###\")\nChallenges we ran into\nUnderstanding the role of Client IDs and Client Secrets in Reddit APIs\nOur team had never tried to integrate a reddit API with code before, so we had to navigate the registration and authentication portion of integrating the API with the codebase.\nNLTK Issues\nA large portion of our coding efforts went into preparing and utilizing NLTK\u2019s Vader program locally. We experienced several roadblocks, most notably incorporating a library called en-core-web-sm", "link": "https://devpost.com/software/sentiment-analysis-of-queries", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what it does\nextrapolating sentiment via r/wallstreetbets\non our trial run of the program, we generated a data set of the 10 most mentioned stock tickers and their associated vernacular. we then utilized asad70\u2019s application (his project) to perform sentiment analysis upon the top 5 tickers. this analysis returned values for three possible trends \u2014 bearish, neutral, and bullish \u2014 ranging from 0 to 1. these values were then compounded into a total trend value for the stock in the range of -1 (most bearish) to 1 (most bullish).\nwhat can traders do with this information?\ntraders can exploit the correlation between number of queries, trading volumes, volatility, daily change in price, and bullish/bearish sentiment to predict future stock market trends.\nthanks to the 1-3 day delay between an increase in queries and an increase in trading volume, investors can use real time query tracking and predictive analysis to forecast the activity of stocks several hours preceding relevant activity.\nhow we built it\nwe combined academic research, existing apis, and new code to generate a predictive -----> tool !!!  and the respective trading strategy for investors.\nwhat we did:\nanalyzed 5 academic papers exploring the correlations between the number of search engine queries about a stock, its trading volumes, its volatility, and its daily change in price.\ncreated and integrated a personal reddit script api\nmodified asad70\u2019s application reddit-sentiment-analysis to encompass our reddit script api\ntested our output by generating a data set of the 10 most mentioned stock tickers on r/wallstreetbets and performing sentiment analysis on the top 5 picks\nmodified code\noriginal\nimport re import en_core_web_sm\nnlp = en_core_web_sm.load() stopwords = nlp.defaults.stop_words\nreddit = praw.reddit(user_agent=\"comment extraction\", client_id=\"\", client_secret=\"\", username=\"\", password=\"\")\nmodified\nimport spacy #import en_core_web_sm\n#nlp = en_core_web_sm.load() stopwords = []#nlp.defaults.stop_words\nreddit = praw.reddit(user_agent=\"comment extraction\", client_id=\"qs4uiqjhu2eilzyner####\", client_secret=\"mkweizsgtkvkqzywj29kcejobe####\",username=\"up2early4this\",password=\"###\")\nchallenges we ran into\nunderstanding the role of client ids and client secrets in reddit apis\nour team had never tried to integrate a reddit api with code before, so we had to navigate the registration and authentication portion of integrating the api with the codebase.\nnltk issues\na large portion of our coding efforts went into preparing and utilizing nltk\u2019s vader program locally. we experienced several roadblocks, most notably incorporating a library called en-core-web-sm", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59503155}, {"Unnamed: 0": 3179, "autor": "HearAssist", "date": null, "content": "Inspiration\nCOVID-19 has impacted the lives globally especially the ones with disabilities hence, we have thought of a tool focusing on helping auditory-impaired individuals by assisting them. In the current scenario, there are special schools for them, so we decided to develop a tool that will bridge the gap and will enable them to study anywhere and interact with anyone around the world.\nTrack: Community\nWhat it does\nHearAssist facilitates communication with auditory-impaired individuals by providing live captions of ongoing communication.\nHow we built it\nAPI Used: AssemblyAI\nBack-end: Flask, Replit\nFront-end: HTML, CSS\nChallenges we ran into\nSynchronizing with the AssemblyAI API was a bit difficult, but we were able to wind it up by going through the documentation.\nAccomplishments that we're proud of\nWe're proud of building a prototype and finishing it up locally.\nWhat we learned\nWe learned about AssemblyAI, web design, development, flask, and replit.\nWhat's next for HearAssist\nWe'll be deploying it as a web application and it will even support more languages.", "link": "https://devpost.com/software/hearassist", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ncovid-19 has impacted the lives globally especially the ones with disabilities hence, we have thought of a -----> tool !!!  focusing on helping auditory-impaired individuals by assisting them. in the current scenario, there are special schools for them, so we decided to develop a tool that will bridge the gap and will enable them to study anywhere and interact with anyone around the world.\ntrack: community\nwhat it does\nhearassist facilitates communication with auditory-impaired individuals by providing live captions of ongoing communication.\nhow we built it\napi used: assemblyai\nback-end: flask, replit\nfront-end: html, css\nchallenges we ran into\nsynchronizing with the assemblyai api was a bit difficult, but we were able to wind it up by going through the documentation.\naccomplishments that we're proud of\nwe're proud of building a prototype and finishing it up locally.\nwhat we learned\nwe learned about assemblyai, web design, development, flask, and replit.\nwhat's next for hearassist\nwe'll be deploying it as a web application and it will even support more languages.", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59503179}, {"Unnamed: 0": 3183, "autor": "Matchbox", "date": null, "content": "Inspiration\nWe are all graduating college soon and are on the job hunt! We thought we could make the hunt a little easier by helping people find companies that might be a good fit for them based on their skills and qualifications. Also, we hope to give a big picture view of companies that people might be interested in by giving them insight into popular sentiment and valued skills.\nWhat it does\nMatchbox is a service that will match you based on your resume or LinkedIn profile to companies that align closest to the skills and qualifications that you possess. With data sourced from Glassdoor and LinkedIn, it also shows data visualizations of metrics such as popular sentiment over time, common words used to describe the company, and more.\nHow we built it\nThis project is inherently data driven, so gathering the data was a fundamental part of our development process. We built web scrapers to pull reviews from Glassdoor and also pull company information from LinkedIn. Additionally, we performed sentiment analysis on the reviews from Glassdoor to see what popular opinions of a given company would look like. On the frontend, we have beautiful data visualizations powered by dc.js, which allows for dynamic crossfiltering so you can clearly see relationships between all of our data.\nMulti-region Fault Tolerance with CockroachDB\nIn line with our focus on data, we wanted to take the challenge of coming up with a fault tolerant and portable deployment scheme for our app. Using CockroachDB's Helm operator, we were able to deploy a multi-region fault tolerant CockroachDB instance on 3-node Kubernetes clusters running on both the east and west coast on Linode and GCP respectively. Along with load balancing between the clusters, this allowed our service to survive an entire data center failure in one of the availability zones. Getting this working under a tight Hackathon timeline was a challenge, but the more rigid data model and consistency guarantees that CockroachDB afforded us were worth the time investment. In addition, we gained a lot of valuable knowledge about Docker, Kubernetes, and networking between cloud providers.\nChallenges we ran into\nThis project was an ambitious undertaking, so we did have to table some ideas we had for the future. One lesson we took away from this project is that if we had access to the APIs for Glassdoor and LinkedIn, it would be much easier to get at the data that we need instead of having to roll out our own scrapers and grab at the data ourselves, which is a lot more resource-heavy and time-consuming.\nAccomplishments that we're proud of\nAt the end of the day, we're proud of the data we were able to gather and along with the visualizations we created, we believe that this could be genuinely valuable to a lot of people, especially new grads, seeking employment.\nWhat we learned\nA lot of the visualization was relatively new and challenging for some of us, so it was a great experience to get some exposure to an easily accessible and powerful tool. Also, sentiment analysis was a topic that a few of us wanted to explore at some point, so we're glad we had the opportunity to incorporate it in our project.\nWhat's next for Matchbox\nAs it stands, there's a lot of room for optimization when it comes to the gathering of data. If we can get authorized to use Glassdoor's API or even get accepted into the LinkedIn Partner Program so we can have full access to all of their API endpoints, it would streamline the data collection process for us. In the future, we also hope to help prospective employees grow their network by matching them with people working at the companies that they think would be a good fit for them. All in all, there is a lot of different directions that could be taken with this project, and we're confident that it can help people on the job hunt.", "link": "https://devpost.com/software/matchbox-01w759", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe are all graduating college soon and are on the job hunt! we thought we could make the hunt a little easier by helping people find companies that might be a good fit for them based on their skills and qualifications. also, we hope to give a big picture view of companies that people might be interested in by giving them insight into popular sentiment and valued skills.\nwhat it does\nmatchbox is a service that will match you based on your resume or linkedin profile to companies that align closest to the skills and qualifications that you possess. with data sourced from glassdoor and linkedin, it also shows data visualizations of metrics such as popular sentiment over time, common words used to describe the company, and more.\nhow we built it\nthis project is inherently data driven, so gathering the data was a fundamental part of our development process. we built web scrapers to pull reviews from glassdoor and also pull company information from linkedin. additionally, we performed sentiment analysis on the reviews from glassdoor to see what popular opinions of a given company would look like. on the frontend, we have beautiful data visualizations powered by dc.js, which allows for dynamic crossfiltering so you can clearly see relationships between all of our data.\nmulti-region fault tolerance with cockroachdb\nin line with our focus on data, we wanted to take the challenge of coming up with a fault tolerant and portable deployment scheme for our app. using cockroachdb's helm operator, we were able to deploy a multi-region fault tolerant cockroachdb instance on 3-node kubernetes clusters running on both the east and west coast on linode and gcp respectively. along with load balancing between the clusters, this allowed our service to survive an entire data center failure in one of the availability zones. getting this working under a tight hackathon timeline was a challenge, but the more rigid data model and consistency guarantees that cockroachdb afforded us were worth the time investment. in addition, we gained a lot of valuable knowledge about docker, kubernetes, and networking between cloud providers.\nchallenges we ran into\nthis project was an ambitious undertaking, so we did have to table some ideas we had for the future. one lesson we took away from this project is that if we had access to the apis for glassdoor and linkedin, it would be much easier to get at the data that we need instead of having to roll out our own scrapers and grab at the data ourselves, which is a lot more resource-heavy and time-consuming.\naccomplishments that we're proud of\nat the end of the day, we're proud of the data we were able to gather and along with the visualizations we created, we believe that this could be genuinely valuable to a lot of people, especially new grads, seeking employment.\nwhat we learned\na lot of the visualization was relatively new and challenging for some of us, so it was a great experience to get some exposure to an easily accessible and powerful -----> tool !!! . also, sentiment analysis was a topic that a few of us wanted to explore at some point, so we're glad we had the opportunity to incorporate it in our project.\nwhat's next for matchbox\nas it stands, there's a lot of room for optimization when it comes to the gathering of data. if we can get authorized to use glassdoor's api or even get accepted into the linkedin partner program so we can have full access to all of their api endpoints, it would streamline the data collection process for us. in the future, we also hope to help prospective employees grow their network by matching them with people working at the companies that they think would be a good fit for them. all in all, there is a lot of different directions that could be taken with this project, and we're confident that it can help people on the job hunt.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59503183}, {"Unnamed: 0": 3214, "autor": "Melanoma Detection", "date": null, "content": "Inspiration\nMillions of people each year are afflicted with Melanoma and we wanted to try to build a tool that would help easily quell worries people may have about their skin.\nWhat it does\nOur app uses machine learning to predict on an image and classify it as melanoma or not. It can either take in a saved image file or it can take an image from a webcam.\nHow we built it\nWe used Tensorflow and the HAM10k melanoma dataset to train the model. We used Django to make the website and OpenCV to take in webcam video + process the image.\nChallenges we ran into\nOur model was not predicting properly after we had trained, saved, and reloaded it. It took us many hours, but eventually we managed to get it to work by training the model in firefox instead of chrome. Also, a comment was once causing a bug in our code which was frustrating to fix.\nAccomplishments that we're proud of\nWe're proud of eventually training the model to have >90% validation accuracy and deploying the model to the website. Also, we're proud of being able to display a live video feed to the website and predict based on snapshots of the feed.\nWhat we learned\nWe learned how to use OpenCV to get webcam footage and how to transfer data with urls between the different pages in our app.\nWhat's next for Melanoma Detection\nNext, we would like to implement live detection and transition it so that it can work using a mobile camera as well.", "link": "https://devpost.com/software/melanoma-detection", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nmillions of people each year are afflicted with melanoma and we wanted to try to build a -----> tool !!!  that would help easily quell worries people may have about their skin.\nwhat it does\nour app uses machine learning to predict on an image and classify it as melanoma or not. it can either take in a saved image file or it can take an image from a webcam.\nhow we built it\nwe used tensorflow and the ham10k melanoma dataset to train the model. we used django to make the website and opencv to take in webcam video + process the image.\nchallenges we ran into\nour model was not predicting properly after we had trained, saved, and reloaded it. it took us many hours, but eventually we managed to get it to work by training the model in firefox instead of chrome. also, a comment was once causing a bug in our code which was frustrating to fix.\naccomplishments that we're proud of\nwe're proud of eventually training the model to have >90% validation accuracy and deploying the model to the website. also, we're proud of being able to display a live video feed to the website and predict based on snapshots of the feed.\nwhat we learned\nwe learned how to use opencv to get webcam footage and how to transfer data with urls between the different pages in our app.\nwhat's next for melanoma detection\nnext, we would like to implement live detection and transition it so that it can work using a mobile camera as well.", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59503214}, {"Unnamed: 0": 3253, "autor": "Warm Letters", "date": null, "content": "Inspiration\nSocial media is a powerful tool but can have damaging effects on mental health. We have noticed that many of the bigger companies are not putting in the effort in trying to stop or prevent cyberbullying and toxicity. According to ConnectSafely.org, 2021, Girls are also three times as likely to be cyberbullied than boys. In a fast-paced world, with many social media displaying flashy videos to attract attention for the sake of earning revenue, with social media putting profit over people, it\u2019s hard to find ways to slow down, rewind, reflect, and connect.\nSo we want to create a positive online environment for girls and gender minorities to support each other through letter writing. The website will be completely ad-free, saving the time and cost compared to actual letter delivery.\nAs a young girl, you don\u2019t have to feel forced to put up an image or live up to a certain standard, and can feel free to speak your mind and express your insecurities in a healthy way. The letters you write and respond to are anonymous, which helps protect privacy. We also use additional tools like the Perspective Analyzer to limit general toxicity that we\u2019ll mention later.\nSince this is the beta version of the app, it\u2019s going to be invite-only\u2014Existing users can invite others to join them on the platform. This is another step in creating a healthy atmosphere.\nWhat it does\nWarm Letters: An online platform for girls and gender minorities, where thoughts can be shared anonymously through the form of letter writing. They can also give encouraging replies after reading the letters to help build a stronger sense of community.\nWith the Google Cloud Perspective Comment Analyzer API, we ensure against toxicity and scam online, and we never share data with anyone.\nApp walkthrough\nUsers start by logging in after authentication via their email and password.\nEach session of the user login uses encrypted authentication via Google Firebase which helps maintain a secure environment for their activity on the website.\nAfter the login page, the user enters the main page of the platform. They can write and send letters here.\nThis is also where they can read a letter that\u2019s assigned to them. The letter system uses a queue: The oldest unanswered letter has the highest priority to be assigned to someone.\nIf you click \u201cReply,\u201d it sends you to a page where you view the letter in larger size and can compose your reply.\nOnce you\u2019re done, it sends you back to the Dashboard.\nIf you\u2019d like to view the replies to your letters, you click \u201cReplies to Me.\u201d at the top. Here you can read all the letters you\u2019ve received.\nBoth letters and responses use the Google Cloud Perspective Comment Analyzer API to limit toxicity and spam submissions.\nOur goal is to create an inclusive online community to support girls and gender minorities world-wide, one letter at a time.\nHow we built it\nTech Tools Used:\nMock-up: Fresco- mock-ups are done digitally, with hand-written letterheads and warm color-scheme to promote a warm and homey aesthetic.\nFront-end: The front-end is built using React with Typescript to enforce strict typings, using state hooks and components for regular state management, and React Query to handle asynchronous state-management. All data displayed is fully dynamic and fetches the respective information using queries from the backend.\nBack-end: The back-end consists of two main pieces: Firebase Authentication for sign-up, authentication, and authorization, and Firebase Firestore as a NoSQL database where the letters and responses are stored.\nMisc: Google Cloud Perspective Comment Analyzer API to combat submission of hate comments, toxicity, and spam. Deployment: Vercel\nChallenges we ran into\nHalf of the team hacked for the first time. It was a challenging but rewarding experience. Due to the complex nature of the data we're working with, and the need to keep the letters and responses anonymized yet secured, setting up the data structure was quite a challenge.\nAdditionally, this was our first time ever working with Google Firebase, and although it's a very streamlined tool, it had a significant learning curve to adapt to the design principles it uses.\nAccomplishments that we're proud of\nWorking effectively and remotely with teammates we had never met prior to Technica2021.\nConstantly collaborated and kept other members informed, provided and took help when needed.\nWhat we learned\nLearned how to divide & delegate tasks as a team and to be accountable for it.\nPlanning work, setting reporting times, and deadlines.\nCommunicating remotely with teammates.\nWhat's next for Warm Letters\nAdd images, audio, and video clips\nSave drafts for later\nSend virtual postcards and greeting cards\nNight mode to prevent eye strain", "link": "https://devpost.com/software/warm-letters", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nsocial media is a powerful -----> tool !!!  but can have damaging effects on mental health. we have noticed that many of the bigger companies are not putting in the effort in trying to stop or prevent cyberbullying and toxicity. according to connectsafely.org, 2021, girls are also three times as likely to be cyberbullied than boys. in a fast-paced world, with many social media displaying flashy videos to attract attention for the sake of earning revenue, with social media putting profit over people, it\u2019s hard to find ways to slow down, rewind, reflect, and connect.\nso we want to create a positive online environment for girls and gender minorities to support each other through letter writing. the website will be completely ad-free, saving the time and cost compared to actual letter delivery.\nas a young girl, you don\u2019t have to feel forced to put up an image or live up to a certain standard, and can feel free to speak your mind and express your insecurities in a healthy way. the letters you write and respond to are anonymous, which helps protect privacy. we also use additional tools like the perspective analyzer to limit general toxicity that we\u2019ll mention later.\nsince this is the beta version of the app, it\u2019s going to be invite-only\u2014existing users can invite others to join them on the platform. this is another step in creating a healthy atmosphere.\nwhat it does\nwarm letters: an online platform for girls and gender minorities, where thoughts can be shared anonymously through the form of letter writing. they can also give encouraging replies after reading the letters to help build a stronger sense of community.\nwith the google cloud perspective comment analyzer api, we ensure against toxicity and scam online, and we never share data with anyone.\napp walkthrough\nusers start by logging in after authentication via their email and password.\neach session of the user login uses encrypted authentication via google firebase which helps maintain a secure environment for their activity on the website.\nafter the login page, the user enters the main page of the platform. they can write and send letters here.\nthis is also where they can read a letter that\u2019s assigned to them. the letter system uses a queue: the oldest unanswered letter has the highest priority to be assigned to someone.\nif you click \u201creply,\u201d it sends you to a page where you view the letter in larger size and can compose your reply.\nonce you\u2019re done, it sends you back to the dashboard.\nif you\u2019d like to view the replies to your letters, you click \u201creplies to me.\u201d at the top. here you can read all the letters you\u2019ve received.\nboth letters and responses use the google cloud perspective comment analyzer api to limit toxicity and spam submissions.\nour goal is to create an inclusive online community to support girls and gender minorities world-wide, one letter at a time.\nhow we built it\ntech tools used:\nmock-up: fresco- mock-ups are done digitally, with hand-written letterheads and warm color-scheme to promote a warm and homey aesthetic.\nfront-end: the front-end is built using react with typescript to enforce strict typings, using state hooks and components for regular state management, and react query to handle asynchronous state-management. all data displayed is fully dynamic and fetches the respective information using queries from the backend.\nback-end: the back-end consists of two main pieces: firebase authentication for sign-up, authentication, and authorization, and firebase firestore as a nosql database where the letters and responses are stored.\nmisc: google cloud perspective comment analyzer api to combat submission of hate comments, toxicity, and spam. deployment: vercel\nchallenges we ran into\nhalf of the team hacked for the first time. it was a challenging but rewarding experience. due to the complex nature of the data we're working with, and the need to keep the letters and responses anonymized yet secured, setting up the data structure was quite a challenge.\nadditionally, this was our first time ever working with google firebase, and although it's a very streamlined tool, it had a significant learning curve to adapt to the design principles it uses.\naccomplishments that we're proud of\nworking effectively and remotely with teammates we had never met prior to technica2021.\nconstantly collaborated and kept other members informed, provided and took help when needed.\nwhat we learned\nlearned how to divide & delegate tasks as a team and to be accountable for it.\nplanning work, setting reporting times, and deadlines.\ncommunicating remotely with teammates.\nwhat's next for warm letters\nadd images, audio, and video clips\nsave drafts for later\nsend virtual postcards and greeting cards\nnight mode to prevent eye strain", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59503253}, {"Unnamed: 0": 3256, "autor": "Helping Hands", "date": null, "content": "Inspiration\nHelping Hands\nAs a team driven by the motive to contribute towards the greater good of LA, we aimed to create a project which would help unhoused and low income individuals by providing them with information regarding shelters and multiple charity events nearby.\nWhat it does\nThis web app automates the process of bringing help to the needy in a seamless and effortless manner never seen before. This is done by in three main steps:\n1) End user 1 - The unhoused and the low income individuals register and receive information about shelters and charity events through sms texts. Text messaging has been chosen as a means of interaction as studies show that nearly every such individual carries a phone with him/her however they might not have an active internet connection at all times.\n2) End user 2 - The donors - The app provides these users with the means to reach out and help the those in need. These users have also been provided with the functionality to create charity based events and to donate.\n3) End user 3 - The authorities - They are the validating entities that process every charity event request. Also, they act as facilitators of this system by helping the homeless find shelters whenever space is made available.\nHow we built it\nWe used the following frameworks to create our web application Python/Flask - To set up the backend MySQL - To handle data related to unhoused and low income individuals, charity events, zip codes to calculate the lat/long, authorities and donors. Bootstrap 5 - To build the frontend Twilio API - To setup a two way texting system with Google Maps API - To visualize data points according to the features designed for the application\nChallenges we ran into\nObtaining Data Sets and interactive representation of data points according to zip code area in maps,\nDynamically showing the requirements and events in those areas.\nDynamic SMS service as a tool for registration/RSVP for suggested events.\nWriting thousands of lines of code from scratch and debugging through the project.\nAccomplishments that we're proud of\nWe are most proud of having developed an application that helps serve the underpriviledged society, along with engagements and events organized by donors and the authorities. Because of covid-19, the most effected were the homeless and low income families who couldn't find shelter or required donations in the time of need.\nWhat we learned\nWe learned how to work in a team, popping up new ideas, quick decision making and a plethora of technologies while working on the project.\nWhat's next for Helping Hands\nScaling the system to an international level, payment service integration, engageSociety programs, real-time shelter space availability feature as well as expanding our customer base from low-income/homeless to people affected in other ways.", "link": "https://devpost.com/software/helping-hands-k6lwih", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nhelping hands\nas a team driven by the motive to contribute towards the greater good of la, we aimed to create a project which would help unhoused and low income individuals by providing them with information regarding shelters and multiple charity events nearby.\nwhat it does\nthis web app automates the process of bringing help to the needy in a seamless and effortless manner never seen before. this is done by in three main steps:\n1) end user 1 - the unhoused and the low income individuals register and receive information about shelters and charity events through sms texts. text messaging has been chosen as a means of interaction as studies show that nearly every such individual carries a phone with him/her however they might not have an active internet connection at all times.\n2) end user 2 - the donors - the app provides these users with the means to reach out and help the those in need. these users have also been provided with the functionality to create charity based events and to donate.\n3) end user 3 - the authorities - they are the validating entities that process every charity event request. also, they act as facilitators of this system by helping the homeless find shelters whenever space is made available.\nhow we built it\nwe used the following frameworks to create our web application python/flask - to set up the backend mysql - to handle data related to unhoused and low income individuals, charity events, zip codes to calculate the lat/long, authorities and donors. bootstrap 5 - to build the frontend twilio api - to setup a two way texting system with google maps api - to visualize data points according to the features designed for the application\nchallenges we ran into\nobtaining data sets and interactive representation of data points according to zip code area in maps,\ndynamically showing the requirements and events in those areas.\ndynamic sms service as a -----> tool !!!  for registration/rsvp for suggested events.\nwriting thousands of lines of code from scratch and debugging through the project.\naccomplishments that we're proud of\nwe are most proud of having developed an application that helps serve the underpriviledged society, along with engagements and events organized by donors and the authorities. because of covid-19, the most effected were the homeless and low income families who couldn't find shelter or required donations in the time of need.\nwhat we learned\nwe learned how to work in a team, popping up new ideas, quick decision making and a plethora of technologies while working on the project.\nwhat's next for helping hands\nscaling the system to an international level, payment service integration, engagesociety programs, real-time shelter space availability feature as well as expanding our customer base from low-income/homeless to people affected in other ways.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59503256}, {"Unnamed: 0": 3276, "autor": "Reludehcs Scheduler", "date": null, "content": "Inspiration\nAfter another confusing, stressful week of registering for classes, we decided to create a tool to inform students of the best professors at the touch of a button.\nWhat it does\nReludechs asks the user for the list of classes they plan to take next semester and returns to them a list of the best professors according to ratings on RateMyProfessor.com along with details about their class such as the location, time, and section number\nHow we built it\nWe created our site using React with the Next.js framework. We gather class data using the Nebula API and scrape professor rating data from RateMyProfessor.com\nChallenges we ran into\nImplementing the web scraper to gather data from RateMyProfessor was a serious challenge that we were not able to complete in the allotted time. Another challenge was dealing with pesky CORS issues when trying to receive data from the Nebula API.\nAccomplishments that we're proud of\nWe are proud of finally discovering how to implement the RateMyProfessor web scraper running out of time to implement it. We are also proud of overcoming the issues that arose when using the Nebula API, allowing us to create a successful schedule app.\nWhat we learned\nWe learned that by dividing tasks among ourselves, we were able to successfully create a useful tool.\nWhat's next for Reludehcs Scheduler\nWe plan to finish implementation of the RateMyProfessor web scraper through the use of an external server to run the python script required. We also plan to add a proper calendar for users to be able to view the generated schedule.", "link": "https://devpost.com/software/reludhcs-scheduler", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nafter another confusing, stressful week of registering for classes, we decided to create a -----> tool !!!  to inform students of the best professors at the touch of a button.\nwhat it does\nreludechs asks the user for the list of classes they plan to take next semester and returns to them a list of the best professors according to ratings on ratemyprofessor.com along with details about their class such as the location, time, and section number\nhow we built it\nwe created our site using react with the next.js framework. we gather class data using the nebula api and scrape professor rating data from ratemyprofessor.com\nchallenges we ran into\nimplementing the web scraper to gather data from ratemyprofessor was a serious challenge that we were not able to complete in the allotted time. another challenge was dealing with pesky cors issues when trying to receive data from the nebula api.\naccomplishments that we're proud of\nwe are proud of finally discovering how to implement the ratemyprofessor web scraper running out of time to implement it. we are also proud of overcoming the issues that arose when using the nebula api, allowing us to create a successful schedule app.\nwhat we learned\nwe learned that by dividing tasks among ourselves, we were able to successfully create a useful tool.\nwhat's next for reludehcs scheduler\nwe plan to finish implementation of the ratemyprofessor web scraper through the use of an external server to run the python script required. we also plan to add a proper calendar for users to be able to view the generated schedule.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59503276}, {"Unnamed: 0": 3281, "autor": "Drop", "date": null, "content": "Inspiration\nWe tried to solve the very crucial and interesting problem posed by EOG resources, all the water that drilled out cannot go wasted, it has to be recycled/reused.\nWhat it does\nOur tool enables the users/officers at EOG resources to understand the water flow in and flow out to different operations in a way, the wastage of the water that was drilled out during the oil pumping is minimal.\nHow we built it\nWe started with the idea of making it a web application and worked our way up towards user experience and solving the problem statement.\nChallenges we ran into\nOptimization of the situation based on various factors was the key challenging part.\nAccomplishments that we're proud of\nProud of the execution and the way we approached the solution.\nWhat we learned\nWe learned to stay closer to the problem statement, not to deviate more, and indulge in more features. But to just concentrate on key one ingredient and stay true to the solution provided.\nWhat's next for Drop\nExpecting ourselves to serve more real-life problems and give customers a rich experience.", "link": "https://devpost.com/software/drop-j14liv", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe tried to solve the very crucial and interesting problem posed by eog resources, all the water that drilled out cannot go wasted, it has to be recycled/reused.\nwhat it does\nour -----> tool !!!  enables the users/officers at eog resources to understand the water flow in and flow out to different operations in a way, the wastage of the water that was drilled out during the oil pumping is minimal.\nhow we built it\nwe started with the idea of making it a web application and worked our way up towards user experience and solving the problem statement.\nchallenges we ran into\noptimization of the situation based on various factors was the key challenging part.\naccomplishments that we're proud of\nproud of the execution and the way we approached the solution.\nwhat we learned\nwe learned to stay closer to the problem statement, not to deviate more, and indulge in more features. but to just concentrate on key one ingredient and stay true to the solution provided.\nwhat's next for drop\nexpecting ourselves to serve more real-life problems and give customers a rich experience.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59503281}, {"Unnamed: 0": 3287, "autor": "Project VeriGuard", "date": null, "content": "Inspiration\nWe understand that securing our nations information systems is very time consuming and wanted to deliver a solution that will assist with security investigations by reducing the time it takes to Individually query popular OSINT websites.\nWhat it does\nThis security tool can be used retrieve, parse, update, and manipulate the data provided by the popular open source Intelligence websites such as IP VOID, Virus Toal, Cisco Talos, IBM X-Force Exchange, URLScan.io . Our website and API will by retrieve and present the data in a user-friendly interface for easy use in a security investigation.\nHow we built it\nPython for the backend HTML for the front end Figma for the prototype\nChallenges we ran into\nWe ran into challenges creating a webpage because it was the first time that we used HTML, CSS, JavaScript. Similarly, we have never used Figma for developing a prototype Working under strict time constraints and around everyone\u2019s schedule was also a challenge.\nAccomplishments that we're proud of\nWe are very proud of the project we have completed the teamwork and the ability to bring an idea to life.\nWhat we learned\nWe learned how to use Figma, Git, Git Hub and the various components needed to create a web page. We learned how to perform HTTp requests using a back end language such as Pyhon. We learned that Knight Hacks is the best!", "link": "https://devpost.com/software/project-veriguard", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe understand that securing our nations information systems is very time consuming and wanted to deliver a solution that will assist with security investigations by reducing the time it takes to individually query popular osint websites.\nwhat it does\nthis security -----> tool !!!  can be used retrieve, parse, update, and manipulate the data provided by the popular open source intelligence websites such as ip void, virus toal, cisco talos, ibm x-force exchange, urlscan.io . our website and api will by retrieve and present the data in a user-friendly interface for easy use in a security investigation.\nhow we built it\npython for the backend html for the front end figma for the prototype\nchallenges we ran into\nwe ran into challenges creating a webpage because it was the first time that we used html, css, javascript. similarly, we have never used figma for developing a prototype working under strict time constraints and around everyone\u2019s schedule was also a challenge.\naccomplishments that we're proud of\nwe are very proud of the project we have completed the teamwork and the ability to bring an idea to life.\nwhat we learned\nwe learned how to use figma, git, git hub and the various components needed to create a web page. we learned how to perform http requests using a back end language such as pyhon. we learned that knight hacks is the best!", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59503287}, {"Unnamed: 0": 3331, "autor": "Comet Pass", "date": null, "content": "Inspiration\nThere have been many moments when our team members struggle to retrieve their comet card in order to enter a facility or sometimes they may have forgotten it at home. The student can also enter using their GET app, but then again the student needs to shuffle through their pockets to retrieve their phone. We decided to make the student experience more efficient by solving this issue.\nWhat it does\nAll the student needs to do is look towards the camera. The Comet Pass Facial Authorizer uses a camera to detect a students face and checks if they are in the UTD database. If they are, then they are checked-in, if they are not, then they can either offer to enter using their comet card, or they will be sent home. The Facial Authorizer is very accurate using AWS software.\nHow we built it\nThe Facial Authorizer uses an AWS tool called \"Rekognition\" which offers a variety of features including face matching. We would then upload the UTD directory of student pictures inside of the S3 bucket, an AWS storage system. When the student gets in front of the camera and hits \"check-in\" it then snaps a picture and checks for a similar image in the AWS S3 bucket. We used Flask for the backend server and React JS for the front-end of the webapp.\nChallenges we ran into\nWe were not familiar with the AWS platform, so that took a little bit of time to become familiarized with.\nAccomplishments that we're proud of\nOur Facial Authorizer is very accurate and can even recognize faces through low quality pictures. The check-in is also immediate and does not take time to process.\nWhat we learned\nWe learned how to implement API's using the AWS system. We learned how to take snapshots of pictures and comparing it to a bucket of pictures(data).\nWhat's next for Comet Pass\nWe would like to implement new features including checking-in students as they walk into a facility without having to stand in front of the camera and hit \"check in\". We will also flag individuals who do not pass the check-in so that they may be turned down.", "link": "https://devpost.com/software/comet-pass", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nthere have been many moments when our team members struggle to retrieve their comet card in order to enter a facility or sometimes they may have forgotten it at home. the student can also enter using their get app, but then again the student needs to shuffle through their pockets to retrieve their phone. we decided to make the student experience more efficient by solving this issue.\nwhat it does\nall the student needs to do is look towards the camera. the comet pass facial authorizer uses a camera to detect a students face and checks if they are in the utd database. if they are, then they are checked-in, if they are not, then they can either offer to enter using their comet card, or they will be sent home. the facial authorizer is very accurate using aws software.\nhow we built it\nthe facial authorizer uses an aws -----> tool !!!  called \"rekognition\" which offers a variety of features including face matching. we would then upload the utd directory of student pictures inside of the s3 bucket, an aws storage system. when the student gets in front of the camera and hits \"check-in\" it then snaps a picture and checks for a similar image in the aws s3 bucket. we used flask for the backend server and react js for the front-end of the webapp.\nchallenges we ran into\nwe were not familiar with the aws platform, so that took a little bit of time to become familiarized with.\naccomplishments that we're proud of\nour facial authorizer is very accurate and can even recognize faces through low quality pictures. the check-in is also immediate and does not take time to process.\nwhat we learned\nwe learned how to implement api's using the aws system. we learned how to take snapshots of pictures and comparing it to a bucket of pictures(data).\nwhat's next for comet pass\nwe would like to implement new features including checking-in students as they walk into a facility without having to stand in front of the camera and hit \"check in\". we will also flag individuals who do not pass the check-in so that they may be turned down.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59503331}, {"Unnamed: 0": 3346, "autor": "Agent Spork", "date": null, "content": "Inspiration\nI often find myself sucked into the realm of social media, whether it be TikTok or Snapchat, and scrolling for hours at a time. I love computer science and technology, but I often trade off time spent practicing self-care for social media. I was inspired to develop the beginnings of Agent Spork, the loveable chatbot who gives gentle reminders to take tech breaks, drink water, or simply listen to their terrible dad jokes. In order to start helping myself and my friends, who would rather communicate through dog filters of Snapchat than talk, I decided to make an app that could break us from these terrible habits.\nWhat it does\nAgent Spork is a chatbot app that sends reminders, at intervals of your choosing, to stretch, drink water, or close your social media apps. Since they are a chatbot, the Agent has a variety of conversational features including links and to self-care, meditation, and hotline resources. These resources include links to the Trevor Project, Suicide Hotline, MindfulnessExercises.com, and Mental Health America.\nHow we built it\nMy solo project was completed through Google Cloud's Dialogflow API and Android Studios. Both were programmed in a mix of Flutter and Javascript. Graphics, such as the logo, background, and other visual elements were designed by me using Canva. I have previous experience with Android Studios and Flutter, but the Dialogflow API forced me to learn and adapt to the new technology and how it worked alongside Flutter.\nChallenges we ran into\nOnce the app and chatbot had been completed, I moved forward with integrating the bot into Discord, Facebook Messenger, and Slack. Unfortunately, I ran low on time and opted to continue adding those elements later on in the app's development. While I was unable to perfect their integrations, I began fine tuning the variety of responses and training the bot to recognize different styles of speaking through Google Cloud.\nAccomplishments that we're proud of\nI'm beyond proud of the effort I put into learning how to connect the different platforms of Dialogflow and Android Studios. While it wasn't the most difficult aspect of this project, it was incredibly frustrating to fail over and over without any progress. Eventually I realized I had been connecting the wrong file in the wrong location, which hadn't lead to errors because it technically wasn't incorrect. I am incredibly proud of my perseverance this weekend and not giving up when things got stressful.\nWhat we learned\nI learned a lot about machine learning and software integrations while working on this project, in addition to being more self-assured in my programming abilities. I typically work with teams on projects this large and with front-end programming, not actual operations. It was fun to work with different libraries and explore with the expectation of making mistakes; realizing that I could make mistakes and that I should to grow my knowledge of troubleshooting and problem solving was likely the best lesson that I learned while working on Agent Sporks.\nWhat's next for Agent Spork\nOver the next couple of weeks I plan on rolling out updates, integrations, improved graphics, and an even better interface to prepare the app to be released on the Apple and Google App Stores. I want Agent Spork to be a tool for positive change, especially among my generation who never seems to be able to put our phones down. Additionally, the mental health crisis caused by the pandemic has put additional strain on teens who are active on social media and the suicide rate has increased drastically. I want Agent Spork to help prevent those tragedies as we move forward into a brighter (and sporkier) future!", "link": "https://devpost.com/software/agent-spork", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ni often find myself sucked into the realm of social media, whether it be tiktok or snapchat, and scrolling for hours at a time. i love computer science and technology, but i often trade off time spent practicing self-care for social media. i was inspired to develop the beginnings of agent spork, the loveable chatbot who gives gentle reminders to take tech breaks, drink water, or simply listen to their terrible dad jokes. in order to start helping myself and my friends, who would rather communicate through dog filters of snapchat than talk, i decided to make an app that could break us from these terrible habits.\nwhat it does\nagent spork is a chatbot app that sends reminders, at intervals of your choosing, to stretch, drink water, or close your social media apps. since they are a chatbot, the agent has a variety of conversational features including links and to self-care, meditation, and hotline resources. these resources include links to the trevor project, suicide hotline, mindfulnessexercises.com, and mental health america.\nhow we built it\nmy solo project was completed through google cloud's dialogflow api and android studios. both were programmed in a mix of flutter and javascript. graphics, such as the logo, background, and other visual elements were designed by me using canva. i have previous experience with android studios and flutter, but the dialogflow api forced me to learn and adapt to the new technology and how it worked alongside flutter.\nchallenges we ran into\nonce the app and chatbot had been completed, i moved forward with integrating the bot into discord, facebook messenger, and slack. unfortunately, i ran low on time and opted to continue adding those elements later on in the app's development. while i was unable to perfect their integrations, i began fine tuning the variety of responses and training the bot to recognize different styles of speaking through google cloud.\naccomplishments that we're proud of\ni'm beyond proud of the effort i put into learning how to connect the different platforms of dialogflow and android studios. while it wasn't the most difficult aspect of this project, it was incredibly frustrating to fail over and over without any progress. eventually i realized i had been connecting the wrong file in the wrong location, which hadn't lead to errors because it technically wasn't incorrect. i am incredibly proud of my perseverance this weekend and not giving up when things got stressful.\nwhat we learned\ni learned a lot about machine learning and software integrations while working on this project, in addition to being more self-assured in my programming abilities. i typically work with teams on projects this large and with front-end programming, not actual operations. it was fun to work with different libraries and explore with the expectation of making mistakes; realizing that i could make mistakes and that i should to grow my knowledge of troubleshooting and problem solving was likely the best lesson that i learned while working on agent sporks.\nwhat's next for agent spork\nover the next couple of weeks i plan on rolling out updates, integrations, improved graphics, and an even better interface to prepare the app to be released on the apple and google app stores. i want agent spork to be a -----> tool !!!  for positive change, especially among my generation who never seems to be able to put our phones down. additionally, the mental health crisis caused by the pandemic has put additional strain on teens who are active on social media and the suicide rate has increased drastically. i want agent spork to help prevent those tragedies as we move forward into a brighter (and sporkier) future!", "sortedWord": "None", "removed": "Nan", "score": 5, "comments": 0, "media": null, "medialink": null, "identifyer": 59503346}, {"Unnamed: 0": 3375, "autor": "FlyfAIr", "date": null, "content": "Inspiration\nAs the holiday traveling season starts, it's becoming harder and harder to have enough crew members to meet the demands for flights. Airlines often offer bonus salary to those who work during the holiday season, but it can be difficult for both parties to agree on a fair compensation. Our solution is to have a reverse auction system - workers who want to work during the holiday season will place their bid on the minimum bonus they are willing to work for, while the airline company can choose the workers with the minimum bids in the auction to man their aircrafts. Our system will also use a machine learning algorithm to impose a limit on how high the bids can be, therefore ensuring fairness for all parties involved.\nTo make sure that the crew members can be even more justly compensated, we have a system to tip the crew members after a flight.\nAs the money in crew member's wallet grows, we provide a tool to help them invest their earnings. As r/wallstreetbets repeatedly shocked the market, stocks are more and more influenced by social media. It's never been more important to include social media information in trading decisions. We wish to combine machine learning predictions based on historic stock prices with sentiment analysis on social media posts to give the user the best resources for them to make an informed decision.\nWhat it does\nWhat it does\nFlyfair is a multifunctional app and platform, with several features:\nFlight Crew and Pilots get to choose their own overtime wages per flight, by means of a reverse auction system\nEach flight has a risk of cancellation calculated into a risk factor, which is used to determine a premium\nPassengers and Crew get to synchronize ridesharing upon arrival, so rideshare pools are created and users can join or make a rideshare pool with various preferences such as drop off order, gender preference, bags and other factors\nPassengers have the ability to tip the crew on a flight, this goes into a tipjar which is then disbursed evenly among crew who have individual digital wallets. Each wallet has the ability to hold multiple currencies\nCrew can choose to invest in various stocks with their wallet funds, and this is aided by an AI driven recommendation engine\nReverse Auction\nFlight Insurance\nCrew Member Tipping\nStock Investment\nThe final piece of the puzzle is a smart way for crew members to reinvest their earnings in the stock market. First, we use the Yahoo Finance API to get the stock prices. Then, we use our machine learning model hosted on IBM Watson to generate a prediction of the future price, which we use to give the user a recommendation. Then, we scrape r/wallstreetbets on reddit and use sentiment analysis (VADER) to investigate the public opinion about the stock, which we use to calculate a social media score. We also display some highlighted reddit posts about the stock and their sentiments.\nHow we built it\nWe used React and React Native for the frontends, and MongoDB, GCP, and Flask for the backend. We also used VADER for sentiment analysis and PRAW for fetching Reddit information.\nChallenges we ran into\nAccomplishments that we're proud of\nWhat we learned\nWhat's next for FlyFair\nReferences\nPitch Deck\nhttps://docs.google.com/presentation/d/1OYzWyvCDBBBB6_KmonCLZtLHMSb0dx29CPvfzR_p-Io/edit?usp=sharing", "link": "https://devpost.com/software/flyfair", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nas the holiday traveling season starts, it's becoming harder and harder to have enough crew members to meet the demands for flights. airlines often offer bonus salary to those who work during the holiday season, but it can be difficult for both parties to agree on a fair compensation. our solution is to have a reverse auction system - workers who want to work during the holiday season will place their bid on the minimum bonus they are willing to work for, while the airline company can choose the workers with the minimum bids in the auction to man their aircrafts. our system will also use a machine learning algorithm to impose a limit on how high the bids can be, therefore ensuring fairness for all parties involved.\nto make sure that the crew members can be even more justly compensated, we have a system to tip the crew members after a flight.\nas the money in crew member's wallet grows, we provide a -----> tool !!!  to help them invest their earnings. as r/wallstreetbets repeatedly shocked the market, stocks are more and more influenced by social media. it's never been more important to include social media information in trading decisions. we wish to combine machine learning predictions based on historic stock prices with sentiment analysis on social media posts to give the user the best resources for them to make an informed decision.\nwhat it does\nwhat it does\nflyfair is a multifunctional app and platform, with several features:\nflight crew and pilots get to choose their own overtime wages per flight, by means of a reverse auction system\neach flight has a risk of cancellation calculated into a risk factor, which is used to determine a premium\npassengers and crew get to synchronize ridesharing upon arrival, so rideshare pools are created and users can join or make a rideshare pool with various preferences such as drop off order, gender preference, bags and other factors\npassengers have the ability to tip the crew on a flight, this goes into a tipjar which is then disbursed evenly among crew who have individual digital wallets. each wallet has the ability to hold multiple currencies\ncrew can choose to invest in various stocks with their wallet funds, and this is aided by an ai driven recommendation engine\nreverse auction\nflight insurance\ncrew member tipping\nstock investment\nthe final piece of the puzzle is a smart way for crew members to reinvest their earnings in the stock market. first, we use the yahoo finance api to get the stock prices. then, we use our machine learning model hosted on ibm watson to generate a prediction of the future price, which we use to give the user a recommendation. then, we scrape r/wallstreetbets on reddit and use sentiment analysis (vader) to investigate the public opinion about the stock, which we use to calculate a social media score. we also display some highlighted reddit posts about the stock and their sentiments.\nhow we built it\nwe used react and react native for the frontends, and mongodb, gcp, and flask for the backend. we also used vader for sentiment analysis and praw for fetching reddit information.\nchallenges we ran into\naccomplishments that we're proud of\nwhat we learned\nwhat's next for flyfair\nreferences\npitch deck\nhttps://docs.google.com/presentation/d/1oyzwyvcdbbbb6_kmonclztlhmsb0dx29cpvfzr_p-io/edit?usp=sharing", "sortedWord": "None", "removed": "Nan", "score": 6, "comments": 0, "media": null, "medialink": null, "identifyer": 59503375}, {"Unnamed: 0": 3380, "autor": "Trendy Socials", "date": null, "content": "Inspiration\nWe went into the challenge hoping to create a model that makes use of Reddit threads, tweets, Youtube video titles, newsfeeds, and Google search trends to optimize a social metric(Sentiment analysis) based on a statistical regression model. This metric would serve as an indicator to find large changes in price and volatility. This would be displayed to retail investors to determine trading strategies that make use of options contracts that give large returns on investments in the hope of a large move in volatility(regardless of whether the price moving up or down).\nWhat it does\nWe were successfully able to implement the use of Google trends and display the graphs of the S&P 500 with PowerBI. The sudden changes in users' interest in the companies product or services are viewed as a sign of future price volatility. This can be used to influence the trading strategies of retail investors that use our platform.\nHow we built it\nWe used python APIs(pytrends, twelve data) on Google Colab to scrape data regarding stock prices and google search trends. PowerBi was used to create a dashboard for all the collected data.\nChallenges we ran into\nWe initially tried to run a react application on AWS but the time constraint made us switch to PowerBI. Twitter's API tool required authorization which we were not able to get in time to implement for this project. Cold weather, sleep deprivation, and Caffeine reliance.\nAccomplishments that we're proud of\nA working sample demo of what the project hoped to achieve.\nWhat we learned\nWe learned to work in a team and further deepen our understanding of investments, each other's personal strengths/weaknesses, and build relationships between data and insights. Initially spent time to learn React, AWS hosting, Firebase, and PowerBI. It is to be noted, we did not end up using all these platforms.\nWhat's next for Trendy Socials\nImplement scripts to mine data from Twitter, Reddit, newsfeeds which will be processed using NLP library to do sentiment analysis. This would later be fed into a regression model which would create a social metric for the current activity/interest with regards to that company. Implement a react web application to make the user interface easy and experience intuitive. Implement data sorting and recommendation systems based on past user investments, and individual risk tolerance.", "link": "https://devpost.com/software/trendy-socials", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe went into the challenge hoping to create a model that makes use of reddit threads, tweets, youtube video titles, newsfeeds, and google search trends to optimize a social metric(sentiment analysis) based on a statistical regression model. this metric would serve as an indicator to find large changes in price and volatility. this would be displayed to retail investors to determine trading strategies that make use of options contracts that give large returns on investments in the hope of a large move in volatility(regardless of whether the price moving up or down).\nwhat it does\nwe were successfully able to implement the use of google trends and display the graphs of the s&p 500 with powerbi. the sudden changes in users' interest in the companies product or services are viewed as a sign of future price volatility. this can be used to influence the trading strategies of retail investors that use our platform.\nhow we built it\nwe used python apis(pytrends, twelve data) on google colab to scrape data regarding stock prices and google search trends. powerbi was used to create a dashboard for all the collected data.\nchallenges we ran into\nwe initially tried to run a react application on aws but the time constraint made us switch to powerbi. twitter's api -----> tool !!!  required authorization which we were not able to get in time to implement for this project. cold weather, sleep deprivation, and caffeine reliance.\naccomplishments that we're proud of\na working sample demo of what the project hoped to achieve.\nwhat we learned\nwe learned to work in a team and further deepen our understanding of investments, each other's personal strengths/weaknesses, and build relationships between data and insights. initially spent time to learn react, aws hosting, firebase, and powerbi. it is to be noted, we did not end up using all these platforms.\nwhat's next for trendy socials\nimplement scripts to mine data from twitter, reddit, newsfeeds which will be processed using nlp library to do sentiment analysis. this would later be fed into a regression model which would create a social metric for the current activity/interest with regards to that company. implement a react web application to make the user interface easy and experience intuitive. implement data sorting and recommendation systems based on past user investments, and individual risk tolerance.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59503380}, {"Unnamed: 0": 3411, "autor": "Magic Pay", "date": null, "content": "Idea\nSo, the idea itself is very simple. The Dapp enables you to send crypto (MATIC etc) to any person (Alice) without a wallet. You deposit funds into the Dapp and it sends a link to Alice's email id. Alice clicks the link and enters the password (you would communicate this password to Alice separately). The Dapp generates a key-pair on Alice's machine and sends the crypto you had deposited to this newly generated public address. Only Alice has the private key which she now has to use to create a wallet on, for instance, Metamask. While we wait for Alice to redeem the crypto, we deposit the funds into Bentobox to earn interest. Also, if Alice doesnt redeem the crypto in, lets say, 5 days (Chainlink Keepers helps automate the check after the wait period), the crypto is refunded back to you. Finally, we buy $KLIMA using the interest that was earned from Bentobox and maybe burn it. This way we also help do our bit to save the climate using money legos while we onboard more people into the crypto ecosystem.\nInspiration\nAnd thats the basic motivation too - help onboard more people into crypto by removing the necessity of a fiat-on ramp, use the concept of money legos to earn passive income, and then use this passive income to remove carbon offsets from the market (the carbon offsets, in this case, represented by the KLIMA tokens). The Dapp can even act as an onboarding tool for new DAOs being created everyday which need easier ways to get more people into the ecosystem. It can also be used by firms/companies looking to reward loyal users in crypto (as part of some loyalty programme) but dont know whether they have wallets already. This is a cool use case because it completely avoids fiat on ramp which can be a challenge in many developing countries where people might not have access to the crypto ecosystem through traditional banking systems because of regulations.\nTools used to build\nWe built it using a basic react front end, ethers.js for communicating with the solidity backend alongwith bentobox and chainlink keepers. The server simply serves the pages to the browser. All the logic is client side or on the blockchain. The poc is deployed on the polygon testnet.\nWhat we learned\nBeing relatively new to developing on the blockchain there were many beginners' hiccups - from configuring and getting the keepers to work, to understanding how bentobox works, fitting all the pieces together, communicating from the front-end using ethers.js, understanding abi encoding and integer math, ERC20 logic, concept of wrapped tokens., etc.\nChallenges faced\nDeveloping on the blockchain is very different compared to a traditional client-server model. Testing was a challenge since every change has to be redeployed and the new ABI made available to the front end. All actions cost crypto - so have to take cognizance of this while writing and testing code.\nAchievements\nThe project was very satisfying not just because it achieves a decent demoable version of the whole idea but also because I learnt so many new things, concepts, protocols and a blockchain developer's mindset. It also helps us do our bit for the climate using KLIMA since we will be buying and burning the KLIMA using the passive income thereby permanently removing carbon offsets from the market.\nWhats next for Magic Pay\nThere is still lots to do:-\nBug fixes\nUsing a neutral account which provides gas for the redemption of crypto by new users\nOption to send any token (presently just matic)\nAchieving gas efficiency while interacting with bentobox and buying KLIMA\nRefactor\nDeploy on L2", "link": "https://devpost.com/software/magic-pay", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "idea\nso, the idea itself is very simple. the dapp enables you to send crypto (matic etc) to any person (alice) without a wallet. you deposit funds into the dapp and it sends a link to alice's email id. alice clicks the link and enters the password (you would communicate this password to alice separately). the dapp generates a key-pair on alice's machine and sends the crypto you had deposited to this newly generated public address. only alice has the private key which she now has to use to create a wallet on, for instance, metamask. while we wait for alice to redeem the crypto, we deposit the funds into bentobox to earn interest. also, if alice doesnt redeem the crypto in, lets say, 5 days (chainlink keepers helps automate the check after the wait period), the crypto is refunded back to you. finally, we buy $klima using the interest that was earned from bentobox and maybe burn it. this way we also help do our bit to save the climate using money legos while we onboard more people into the crypto ecosystem.\ninspiration\nand thats the basic motivation too - help onboard more people into crypto by removing the necessity of a fiat-on ramp, use the concept of money legos to earn passive income, and then use this passive income to remove carbon offsets from the market (the carbon offsets, in this case, represented by the klima tokens). the dapp can even act as an onboarding -----> tool !!!  for new daos being created everyday which need easier ways to get more people into the ecosystem. it can also be used by firms/companies looking to reward loyal users in crypto (as part of some loyalty programme) but dont know whether they have wallets already. this is a cool use case because it completely avoids fiat on ramp which can be a challenge in many developing countries where people might not have access to the crypto ecosystem through traditional banking systems because of regulations.\ntools used to build\nwe built it using a basic react front end, ethers.js for communicating with the solidity backend alongwith bentobox and chainlink keepers. the server simply serves the pages to the browser. all the logic is client side or on the blockchain. the poc is deployed on the polygon testnet.\nwhat we learned\nbeing relatively new to developing on the blockchain there were many beginners' hiccups - from configuring and getting the keepers to work, to understanding how bentobox works, fitting all the pieces together, communicating from the front-end using ethers.js, understanding abi encoding and integer math, erc20 logic, concept of wrapped tokens., etc.\nchallenges faced\ndeveloping on the blockchain is very different compared to a traditional client-server model. testing was a challenge since every change has to be redeployed and the new abi made available to the front end. all actions cost crypto - so have to take cognizance of this while writing and testing code.\nachievements\nthe project was very satisfying not just because it achieves a decent demoable version of the whole idea but also because i learnt so many new things, concepts, protocols and a blockchain developer's mindset. it also helps us do our bit for the climate using klima since we will be buying and burning the klima using the passive income thereby permanently removing carbon offsets from the market.\nwhats next for magic pay\nthere is still lots to do:-\nbug fixes\nusing a neutral account which provides gas for the redemption of crypto by new users\noption to send any token (presently just matic)\nachieving gas efficiency while interacting with bentobox and buying klima\nrefactor\ndeploy on l2", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59503411}, {"Unnamed: 0": 3412, "autor": "SaoLa", "date": null, "content": "Inspiration\nOver decades, protecting biodiversity has been a complicated topic, especially wildlife. During the pandemic, the hunt for wild animals shows mild signs of decline. However, the wildlife trade has been more sophisticated than ever, thanks to many social platforms such as Facebook, Zalo, and TikTok. Therefore, with a passion for wildlife conservation, our team chooses problem statement 4 and wants to create a product that helps restrict online trading and traditional markets.\nWhat it does\nSaoLa is a gamification that can solve these issues. We have two parts, including a mobile game app and a mobile web.\nHow we built it\nWe split the task into frontend, backend, AI and game for members. Then, we merge all these stuffs.\nChallenges we ran into\nWe do not have enough data to train AI model, but we use some special technique to solve this. Also, Vietnam is a country with high biodiversity; so that it was a difficult thing when we tried to recreate the natural environment of Vietnam in the game.\nAccomplishments that we're proud of\nWe believe that our product is the implementation of the trendy technologies which can help us to solve the issue of wildlife trading.\nWhat we learned\nWe learned a lot of stuff including teamwork, NLP, OCR, Game implement, and Blockchain,...\nWhat's next for SaoLa\nWe want to develop our system and the AI report tool to get more reliable results.", "link": "https://devpost.com/software/saola", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nover decades, protecting biodiversity has been a complicated topic, especially wildlife. during the pandemic, the hunt for wild animals shows mild signs of decline. however, the wildlife trade has been more sophisticated than ever, thanks to many social platforms such as facebook, zalo, and tiktok. therefore, with a passion for wildlife conservation, our team chooses problem statement 4 and wants to create a product that helps restrict online trading and traditional markets.\nwhat it does\nsaola is a gamification that can solve these issues. we have two parts, including a mobile game app and a mobile web.\nhow we built it\nwe split the task into frontend, backend, ai and game for members. then, we merge all these stuffs.\nchallenges we ran into\nwe do not have enough data to train ai model, but we use some special technique to solve this. also, vietnam is a country with high biodiversity; so that it was a difficult thing when we tried to recreate the natural environment of vietnam in the game.\naccomplishments that we're proud of\nwe believe that our product is the implementation of the trendy technologies which can help us to solve the issue of wildlife trading.\nwhat we learned\nwe learned a lot of stuff including teamwork, nlp, ocr, game implement, and blockchain,...\nwhat's next for saola\nwe want to develop our system and the ai report -----> tool !!!  to get more reliable results.", "sortedWord": "None", "removed": "Nan", "score": 11, "comments": 0, "media": null, "medialink": null, "identifyer": 59503412}, {"Unnamed: 0": 3415, "autor": "DrivenByTime", "date": null, "content": "Inspiration\nOur idea started from a group member who is a commuter student who struggled with scheduling commute times. Despite trying to create his own estimates, the many changing variables made it too difficult to do so. However we wanted this idea to expand beyond just commuter students, and create a schedule management tool that could have a more global impact.\nWhat it does\nGiven a user's Google calendar, our project would have the ability to create a more accurate schedule which considers traffic and transportation into one's schedule. From the user's start destination and scheduled event time, we would first provide them with the closest parking structure to their destination, as well as a statistic for the likelihood of finding on-street parking closer than the closest parking structure. Utilizing INRIX routing data and our own formula for parking time estimation (through INRIX parking probability metric) we created an accurate gauge for a combined driving and parking time. We then used Google's API for walking data to calculate the walking distance from the parking structure to event location. We finally total the driving, parking, and walking times to create a commute event on the calendar to provide the user with an accurate understanding of an ideal departure time. This event also has a link to a Google maps link that links you to the optimal parking garage location, and lets you know the on-street parking statistic.\nHow we built it\nFirst, we spent roughly an hour going over the design process and dividing work in accordance to the expertise of every member. From this, we were able to come to the conclusion of dividing our group into 3 teams: a Google API team (backend, Python), an INRIX API (backend, Python), and a user interface team (frontend, Django). The Google API team, Malcolm and Ryan, focused on scraping Google Calendar's API for date and locational data, and then utilizing that information to pass it to the INRIX team. Eventually, following the INRIX team's output, they would also create an estimation for walking times. The INRIX team, Wallace and Graham, created an algorithm for scraping information from INRIX's off-street and on-street parking data, as well as routing data to create accurate estimations for the purposes of the project. Finally, the user interface team, Joshua, gathered the outputs of both the Google and INRIX API team to create a visualization and interactive design for the entirety of the project.\nChallenges we ran into\nMost of our challenges came from returning and inputting data into the two API's we worked with for the project. We had a few issues with differing file types between the two API's, and therefore had to make sure to properly convert our data when passing in-between the two. Only three out of the 5 team members knew how to code in python and therefore, we needed to divy up work in a way that we could all be productive even with our differing amounts of experience. This also extended to our front-end where no members of the group felt especially comfortable, luckily Josh decided to take on the challenge and spent much of the day of learning and implementing a Django website.\nAccomplishments that we're proud of\nWe found our time estimation algorithm to be very interesting and well implemented; being able to change a route to go directly to the best parking location, while simultaneously creating a probability and ranked matrix for on-street parking, was extremely intriguing to us. Moreover, just being able to watch our code successfully compile with what we sought the code out to do was just extremely satisfying. Beyond having functional code, the learning experience of using APIs and front-end development had its own level of gratification.\nWhat we learned\nAs previously mentioned, we were able to learn a lot about utilizing APIs, as well as front-end development. This experience was very eye opening in how it is like for us hopeful programmers to be writing code for real-world applications; in realizing how our code can potentially impact people, there was much more motivation and enjoyment for completing the task at hand.\nWhat's next for DrivenByTime\nIf we had more time to develop, we would continue to develop our time estimation algorithms to be more accurate. We would also want to implement our own servers so that we could have our scheduling system reach a larger audience. Also we would ideally implement a way to grab the users current location and update our estimates past just future estimates, and give them more accurate and dynamic travel times.", "link": "https://devpost.com/software/drivenbytime", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nour idea started from a group member who is a commuter student who struggled with scheduling commute times. despite trying to create his own estimates, the many changing variables made it too difficult to do so. however we wanted this idea to expand beyond just commuter students, and create a schedule management -----> tool !!!  that could have a more global impact.\nwhat it does\ngiven a user's google calendar, our project would have the ability to create a more accurate schedule which considers traffic and transportation into one's schedule. from the user's start destination and scheduled event time, we would first provide them with the closest parking structure to their destination, as well as a statistic for the likelihood of finding on-street parking closer than the closest parking structure. utilizing inrix routing data and our own formula for parking time estimation (through inrix parking probability metric) we created an accurate gauge for a combined driving and parking time. we then used google's api for walking data to calculate the walking distance from the parking structure to event location. we finally total the driving, parking, and walking times to create a commute event on the calendar to provide the user with an accurate understanding of an ideal departure time. this event also has a link to a google maps link that links you to the optimal parking garage location, and lets you know the on-street parking statistic.\nhow we built it\nfirst, we spent roughly an hour going over the design process and dividing work in accordance to the expertise of every member. from this, we were able to come to the conclusion of dividing our group into 3 teams: a google api team (backend, python), an inrix api (backend, python), and a user interface team (frontend, django). the google api team, malcolm and ryan, focused on scraping google calendar's api for date and locational data, and then utilizing that information to pass it to the inrix team. eventually, following the inrix team's output, they would also create an estimation for walking times. the inrix team, wallace and graham, created an algorithm for scraping information from inrix's off-street and on-street parking data, as well as routing data to create accurate estimations for the purposes of the project. finally, the user interface team, joshua, gathered the outputs of both the google and inrix api team to create a visualization and interactive design for the entirety of the project.\nchallenges we ran into\nmost of our challenges came from returning and inputting data into the two api's we worked with for the project. we had a few issues with differing file types between the two api's, and therefore had to make sure to properly convert our data when passing in-between the two. only three out of the 5 team members knew how to code in python and therefore, we needed to divy up work in a way that we could all be productive even with our differing amounts of experience. this also extended to our front-end where no members of the group felt especially comfortable, luckily josh decided to take on the challenge and spent much of the day of learning and implementing a django website.\naccomplishments that we're proud of\nwe found our time estimation algorithm to be very interesting and well implemented; being able to change a route to go directly to the best parking location, while simultaneously creating a probability and ranked matrix for on-street parking, was extremely intriguing to us. moreover, just being able to watch our code successfully compile with what we sought the code out to do was just extremely satisfying. beyond having functional code, the learning experience of using apis and front-end development had its own level of gratification.\nwhat we learned\nas previously mentioned, we were able to learn a lot about utilizing apis, as well as front-end development. this experience was very eye opening in how it is like for us hopeful programmers to be writing code for real-world applications; in realizing how our code can potentially impact people, there was much more motivation and enjoyment for completing the task at hand.\nwhat's next for drivenbytime\nif we had more time to develop, we would continue to develop our time estimation algorithms to be more accurate. we would also want to implement our own servers so that we could have our scheduling system reach a larger audience. also we would ideally implement a way to grab the users current location and update our estimates past just future estimates, and give them more accurate and dynamic travel times.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59503415}, {"Unnamed: 0": 3420, "autor": "CometCircle", "date": null, "content": "Inspiration\nMost student organizations on campus have difficulty with member retention and member attendance at events. By creating a planning tool that allows student organization leaders to choose the most optimal time for events, we hope to boost engagement and increase member retention.\nWhat it does\nCometCircle uses the Nebula API to generate a calendar \"heat map\" of when most students are in class. The tool filters by classes of a certain school (ECS, JSOM, AH, etc) and by the events by other organizations. By filtering based on school, the tool allows organizations to establish a target audience. For example, a computer science organization would mostly be interested in the ECS student demographic.\nBy filtering based on events, the tool allows organizations to choose event times that have the least overlap with other organization events. Moreover, being able to view when other organizations are scheduling their events would allow organizers to avoid conflict and allow students to attend more events instead of giving one up in favor of the other. On the calendar, the times that are more densely populated are displayed in red and the least densely populated times are green. The student organization should schedule during \"green\" times or spaces to avoid the most overlap between classes and prospective events.\nHow we built it\nWe employed Streamlit, an open-source framework that visualizes data to create a frontend and backend without any knowledge of JavaScript frameworks. We used Streamlit's built-in UI features to create a sidebar for filtering class data and to create events that populate the heatmap generated in Plotly.\nChallenges we ran into\nIt was difficult to set time intervals on the heatmap and we went through numerous iterations to best mimic a calendar. As a whole, it was difficult to interpret the data, display the data (parsing the data into a table), and optimize run-time.\nAccomplishments that we're proud of\nWe processed hundreds of thousands of course data to filter by specific tags and terms, generate a heat map for class density, and create events.\nWhat we learned\nWe learned that using Streamlit is great for quick prototypes and data interpretation on the web.\nWhat's next for CometCircle\nThis tool could easily be improved upon to increase mapping accuracy. By adding more filters such as by year, the organization can further narrow down its target demographic. Ideally, CometCircle would also be able to predict or estimate the number of attendees for each time and student organizers would be able to \"lock\" times and monitor how the projection changes leading up to the event.\nAnother goal we have for CometCircle is to have an authentication system, wherein registered student organizations are given credentials to sign in and compare prospective times/projections for a planned event. An authentication system would also allow us to expand this service to students as well.\nStudent organizations would have access to the predictive analysis and the class density heatmaps while students would be able to import their class schedules and have planned events autofill in the gaps between their schedules. The students could then register for events that align best with their schedules.\nThe number of students registered at certain times given their specific majors could be stored and used to better predict the ideal time and number of attendees for a certain day and time. We were unable to do this during the hackathon because the Nebula API did not include the information.", "link": "https://devpost.com/software/comet-clique", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nmost student organizations on campus have difficulty with member retention and member attendance at events. by creating a planning -----> tool !!!  that allows student organization leaders to choose the most optimal time for events, we hope to boost engagement and increase member retention.\nwhat it does\ncometcircle uses the nebula api to generate a calendar \"heat map\" of when most students are in class. the tool filters by classes of a certain school (ecs, jsom, ah, etc) and by the events by other organizations. by filtering based on school, the tool allows organizations to establish a target audience. for example, a computer science organization would mostly be interested in the ecs student demographic.\nby filtering based on events, the tool allows organizations to choose event times that have the least overlap with other organization events. moreover, being able to view when other organizations are scheduling their events would allow organizers to avoid conflict and allow students to attend more events instead of giving one up in favor of the other. on the calendar, the times that are more densely populated are displayed in red and the least densely populated times are green. the student organization should schedule during \"green\" times or spaces to avoid the most overlap between classes and prospective events.\nhow we built it\nwe employed streamlit, an open-source framework that visualizes data to create a frontend and backend without any knowledge of javascript frameworks. we used streamlit's built-in ui features to create a sidebar for filtering class data and to create events that populate the heatmap generated in plotly.\nchallenges we ran into\nit was difficult to set time intervals on the heatmap and we went through numerous iterations to best mimic a calendar. as a whole, it was difficult to interpret the data, display the data (parsing the data into a table), and optimize run-time.\naccomplishments that we're proud of\nwe processed hundreds of thousands of course data to filter by specific tags and terms, generate a heat map for class density, and create events.\nwhat we learned\nwe learned that using streamlit is great for quick prototypes and data interpretation on the web.\nwhat's next for cometcircle\nthis tool could easily be improved upon to increase mapping accuracy. by adding more filters such as by year, the organization can further narrow down its target demographic. ideally, cometcircle would also be able to predict or estimate the number of attendees for each time and student organizers would be able to \"lock\" times and monitor how the projection changes leading up to the event.\nanother goal we have for cometcircle is to have an authentication system, wherein registered student organizations are given credentials to sign in and compare prospective times/projections for a planned event. an authentication system would also allow us to expand this service to students as well.\nstudent organizations would have access to the predictive analysis and the class density heatmaps while students would be able to import their class schedules and have planned events autofill in the gaps between their schedules. the students could then register for events that align best with their schedules.\nthe number of students registered at certain times given their specific majors could be stored and used to better predict the ideal time and number of attendees for a certain day and time. we were unable to do this during the hackathon because the nebula api did not include the information.", "sortedWord": "None", "removed": "Nan", "score": 6, "comments": 1, "media": null, "medialink": null, "identifyer": 59503420}, {"Unnamed: 0": 3424, "autor": "Kitty Power Elite", "date": null, "content": "Inspiration\nWe wanted to dive into the technology field and tackle on various challenges such as this Hackathon.\nWhat it does\nPropose a solution on the implementation of a CRM tool\nHow we built it\nWe built a feasibility report to help decision making and improve efficiency\nChallenges we ran into\nConverting over files and time management\nAccomplishments that we're proud of\nCompleting our first Hackathon!\nWhat we learned\nWe learned more about CRM tools, what it is, benefits, and the different types of CRM tools\nWhat's next for Kitty Power Elite\nTo pursue more projects and opportunities like this one!", "link": "https://devpost.com/software/kitty-power-elite", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe wanted to dive into the technology field and tackle on various challenges such as this hackathon.\nwhat it does\npropose a solution on the implementation of a crm -----> tool !!! \nhow we built it\nwe built a feasibility report to help decision making and improve efficiency\nchallenges we ran into\nconverting over files and time management\naccomplishments that we're proud of\ncompleting our first hackathon!\nwhat we learned\nwe learned more about crm tools, what it is, benefits, and the different types of crm tools\nwhat's next for kitty power elite\nto pursue more projects and opportunities like this one!", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59503424}, {"Unnamed: 0": 3435, "autor": "Terra", "date": null, "content": "Inspiration\nOur team wanted to build an environmentally-impactful application. Our research naturally led us to recognize trash as being one of the most significant contributors to a range of environmental issues; climate change, biodiversity loss, and chemical pollution.\nOur goal was to create an app that both drove users to be more conscious and aware of their waste reduction methods and have the possibility of streamlining massive waste reduction strategies in the future.\nWhat it does\nTerra is an app that allows users to take a picture for a machine learning model to predict the possibility of the item pictured being trash, recyclable, or compostable. In addition to this, the app contains local resources for users to recycle and compost more efficiently.\nWe hope that both of our goals were achieved through this.\n1) Since 62% of Americans are unable to properly recycle. This model is trained on common mistakes, mistakes such as recycling used paper/aluminum/certain snack packaging, in order to teach users proper how to properly recycle and compost and holistically bring more awareness of the need to employ waste reduction strategies.\n2) The ~90% precision rate of the model is an excellent tool for greater projects in streamlining everyday waste. For the 96% of waste that is not properly recycled, a machine learning model is an important tool in the future of waste reduction strategies.\nHow we built it\n\u2022 We used Google's new Vertex API, on the Cloud, in order to train our model. The dataset is linked in the Github.\n\u2022 React Native was used on the front-end in order to build the app.\n\u2022 Google cloud expo packages were used for the local waste reduction resources.\nChallenges we ran into\nThe iPhone camera sends images that are extremely large and too high quality. The hardest part of this project was restricting our tools to that of React Native Expo packages in order to make the proper API requests. We almost considered building an entire Django backend in order to both process the image and process the API request. The mentors at KnightHacks were extremely helpful in allowing us to leverage expo React Native to the max.\nAccomplishments that we're proud of\nOur team has not been exposed to a machine learning model before and thanks to Google Cloud's Vertex API, we touched something we never thought we would ever venture into.\nAdditionally, our team has never touched React Native before. We originally wanted to settle with Swift and focus on IOS development, but technical difficulties forced us to learn React Native and build our entire app around it.\nWhat we learned\nWe learned a lot about mobile app development and the moving parts in a machine learning model.\nWhat's next for Terra\nWe hope to make the app more android friendly and to also expand the resources for waste reduction. A feature we would've liked to implement is crowdsourced local composting stations for the community as a whole to contribute their empty composts for the community to donate trash too.", "link": "https://devpost.com/software/terra-fvu47l", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nour team wanted to build an environmentally-impactful application. our research naturally led us to recognize trash as being one of the most significant contributors to a range of environmental issues; climate change, biodiversity loss, and chemical pollution.\nour goal was to create an app that both drove users to be more conscious and aware of their waste reduction methods and have the possibility of streamlining massive waste reduction strategies in the future.\nwhat it does\nterra is an app that allows users to take a picture for a machine learning model to predict the possibility of the item pictured being trash, recyclable, or compostable. in addition to this, the app contains local resources for users to recycle and compost more efficiently.\nwe hope that both of our goals were achieved through this.\n1) since 62% of americans are unable to properly recycle. this model is trained on common mistakes, mistakes such as recycling used paper/aluminum/certain snack packaging, in order to teach users proper how to properly recycle and compost and holistically bring more awareness of the need to employ waste reduction strategies.\n2) the ~90% precision rate of the model is an excellent -----> tool !!!  for greater projects in streamlining everyday waste. for the 96% of waste that is not properly recycled, a machine learning model is an important tool in the future of waste reduction strategies.\nhow we built it\n\u2022 we used google's new vertex api, on the cloud, in order to train our model. the dataset is linked in the github.\n\u2022 react native was used on the front-end in order to build the app.\n\u2022 google cloud expo packages were used for the local waste reduction resources.\nchallenges we ran into\nthe iphone camera sends images that are extremely large and too high quality. the hardest part of this project was restricting our tools to that of react native expo packages in order to make the proper api requests. we almost considered building an entire django backend in order to both process the image and process the api request. the mentors at knighthacks were extremely helpful in allowing us to leverage expo react native to the max.\naccomplishments that we're proud of\nour team has not been exposed to a machine learning model before and thanks to google cloud's vertex api, we touched something we never thought we would ever venture into.\nadditionally, our team has never touched react native before. we originally wanted to settle with swift and focus on ios development, but technical difficulties forced us to learn react native and build our entire app around it.\nwhat we learned\nwe learned a lot about mobile app development and the moving parts in a machine learning model.\nwhat's next for terra\nwe hope to make the app more android friendly and to also expand the resources for waste reduction. a feature we would've liked to implement is crowdsourced local composting stations for the community as a whole to contribute their empty composts for the community to donate trash too.", "sortedWord": "None", "removed": "Nan", "score": 7, "comments": 1, "media": null, "medialink": null, "identifyer": 59503435}, {"Unnamed: 0": 3454, "autor": "Temoc\u2019s Insurance and Finance Adventure", "date": null, "content": "Inspiration\nWe were inspired by click-through/choose your own adventure type games.\nWhat it does\nAn interactive walk-through of common insurance and finance knowledge led by your favorite UTD mascots.\nHow we built it\nWe've used Figma as our design and prototyping tool, and Java as our backend language.\nChallenges we ran into\nThe lack of sufficient advanced technical skills by the the team meant that we did not have the knowledge to fully integrate our code with the UI during these 24 hours.\nAccomplishments that we're proud of\nHaving a working prototype and a coded backend.\nWhat we learned\nWe learned how to prototype a webapp in Figma, we refreshed our Java skills, and we learned more about State Farm's Insurance and Finance offerings, especially those for college students.\nWhat's next for Temoc\u2019s Insurance and Finance Adventure\nAs next steps, we would like to implement a front end framework with our backend code, refine our UI and enhance the user functionality. We would like to further gamify it, possibly by implementing a points system.", "link": "https://devpost.com/software/temoc-s-insurance-and-finance-adventure", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe were inspired by click-through/choose your own adventure type games.\nwhat it does\nan interactive walk-through of common insurance and finance knowledge led by your favorite utd mascots.\nhow we built it\nwe've used figma as our design and prototyping -----> tool !!! , and java as our backend language.\nchallenges we ran into\nthe lack of sufficient advanced technical skills by the the team meant that we did not have the knowledge to fully integrate our code with the ui during these 24 hours.\naccomplishments that we're proud of\nhaving a working prototype and a coded backend.\nwhat we learned\nwe learned how to prototype a webapp in figma, we refreshed our java skills, and we learned more about state farm's insurance and finance offerings, especially those for college students.\nwhat's next for temoc\u2019s insurance and finance adventure\nas next steps, we would like to implement a front end framework with our backend code, refine our ui and enhance the user functionality. we would like to further gamify it, possibly by implementing a points system.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59503454}, {"Unnamed: 0": 3467, "autor": "SafeWaze", "date": null, "content": "Inspiration\nAfter brainstorming problems with traffic, we noticed that not everyone prefers to take the optimal route provided by navigation apps. Current apps such as Google and Apple Maps show fastest and most fuel efficient routes, but not the safest. People have prioritization on what roads they want to take based on congestion, and some prefer to drive longer if it means avoiding busy streets or highways.\nWhat it does\nSafeWaze takes into account construction, events, congestion, hazards/accidents, and road weather in order to determine the safest route to a destination.\nHow we built it\nUsing JavaScript and INRIX's API tool for safety alerts, we developed an algorithm that checks given routes with the above safety concerns. The route returned has the least amount of danger calculated based on the number of roads with accidents, congestion, etc...\nChallenges we ran into\nIt's everyone's first time writing and learning about API's so it was a learning curve to get started with the project.\nAccomplishments that we're proud of\nAfter getting help from mentors and reading up on API documentation, we are proud to get a working solution.\nWhat we learned\nWriting API's and developing in JavaScript.\nWhat's next for SafeLane\nExpanding our scope to dynamically calculate the safest route for any destination rather than just one predefined way.", "link": "https://devpost.com/software/safelane-0jkyz2", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nafter brainstorming problems with traffic, we noticed that not everyone prefers to take the optimal route provided by navigation apps. current apps such as google and apple maps show fastest and most fuel efficient routes, but not the safest. people have prioritization on what roads they want to take based on congestion, and some prefer to drive longer if it means avoiding busy streets or highways.\nwhat it does\nsafewaze takes into account construction, events, congestion, hazards/accidents, and road weather in order to determine the safest route to a destination.\nhow we built it\nusing javascript and inrix's api -----> tool !!!  for safety alerts, we developed an algorithm that checks given routes with the above safety concerns. the route returned has the least amount of danger calculated based on the number of roads with accidents, congestion, etc...\nchallenges we ran into\nit's everyone's first time writing and learning about api's so it was a learning curve to get started with the project.\naccomplishments that we're proud of\nafter getting help from mentors and reading up on api documentation, we are proud to get a working solution.\nwhat we learned\nwriting api's and developing in javascript.\nwhat's next for safelane\nexpanding our scope to dynamically calculate the safest route for any destination rather than just one predefined way.", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59503467}, {"Unnamed: 0": 3484, "autor": "Technical Consulting: Salesforce", "date": null, "content": "How we built it\nWe utilized Salesforce along with preloaded sample data to exemplify how an investment management company can manage customer relationships across the entire customer lifecycle by using a CRM system.\nChallenges we ran into\nOne challenge we faced was that we didn't know how to approach the case at first, but through the help of a mentor, we were directed to the right path. We also faced some challenges while trying to figure out how to navigate Salesforce.\nAccomplishments that we're proud of\nThis was our very first hackathon so we're happy that we got the opportunity to work together as a team, solve problems, expand our knowledge of CRM tools, and present our solution!\nWhat we learned\nWe learned how CRM is a tool for managing a company\u2019s relationships and interactions with customers and potential customers. We also learned that a CRM platform helps companies stay connected to customers, streamline processes, and increase profitability. We also learned how to work in a team setting, listen to others perspectives, and make decisions collectively on how to proceed throughout the project.", "link": "https://devpost.com/software/technical-consulting-salesforce", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "how we built it\nwe utilized salesforce along with preloaded sample data to exemplify how an investment management company can manage customer relationships across the entire customer lifecycle by using a crm system.\nchallenges we ran into\none challenge we faced was that we didn't know how to approach the case at first, but through the help of a mentor, we were directed to the right path. we also faced some challenges while trying to figure out how to navigate salesforce.\naccomplishments that we're proud of\nthis was our very first hackathon so we're happy that we got the opportunity to work together as a team, solve problems, expand our knowledge of crm tools, and present our solution!\nwhat we learned\nwe learned how crm is a -----> tool !!!  for managing a company\u2019s relationships and interactions with customers and potential customers. we also learned that a crm platform helps companies stay connected to customers, streamline processes, and increase profitability. we also learned how to work in a team setting, listen to others perspectives, and make decisions collectively on how to proceed throughout the project.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59503484}, {"Unnamed: 0": 3501, "autor": "Team Ctrl Alt Compete | hackMISSO Fall 2021", "date": null, "content": "Our Case\nGreen Inc. is a private investment company looking to implement a CRM software. They need help discovering CRM's implications, as well as deciding whether to go with a cloud-based software or with an in-house software.\nHow we solved the case\nWe broke down the case by examining the needs of the business and brainstormed solutions to the situation. We also conducted further research to provide more information about CRM. We designed prototypes of the desktop and mobile versions on Figma to provide a layout that could potentially be used by Green Inc with features customized for their needs. We also utilized Canva to create logos for Green Inc. and our consulting company Ctrl Alt Compete.\nChallenges we ran into\nWe had never used Figma before! Utilizing design thinking was a fun challenge and eventually learned to adapt to a new interface design tool.\nAccomplishments that we're proud of\nComing up with a desktop and mobile interface is something we are proud of.\nWhat we learned\nWe learned that consulting requires attention to detail and good communication skills. It is important to be receptive to the client and to accommodate their needs as best as we can. In addition, learning to collaborate with team members virtually in a short period of time was interesting to see how we can utilize technology to work together.", "link": "https://devpost.com/software/team-ctrl-alt-compete-hackmisso-fall-2021", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "our case\ngreen inc. is a private investment company looking to implement a crm software. they need help discovering crm's implications, as well as deciding whether to go with a cloud-based software or with an in-house software.\nhow we solved the case\nwe broke down the case by examining the needs of the business and brainstormed solutions to the situation. we also conducted further research to provide more information about crm. we designed prototypes of the desktop and mobile versions on figma to provide a layout that could potentially be used by green inc with features customized for their needs. we also utilized canva to create logos for green inc. and our consulting company ctrl alt compete.\nchallenges we ran into\nwe had never used figma before! utilizing design thinking was a fun challenge and eventually learned to adapt to a new interface design -----> tool !!! .\naccomplishments that we're proud of\ncoming up with a desktop and mobile interface is something we are proud of.\nwhat we learned\nwe learned that consulting requires attention to detail and good communication skills. it is important to be receptive to the client and to accommodate their needs as best as we can. in addition, learning to collaborate with team members virtually in a short period of time was interesting to see how we can utilize technology to work together.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59503501}, {"Unnamed: 0": 3504, "autor": "blegin", "date": null, "content": "Inspiration\nMental health is just as important as your physical health \u2013 but this goes unnoticed by many. I wanted to make an app that emphasizes the importance of mental health in the workplace or in school, two places where it's so easy to feel incredibly stressed out, burnt out, disconnected, and just generally exhausted.\nWhat it does\nblegin represents a blend of different aspects of your mental health, all in the same app. However, we take the end out of blend and with it, we bring the beginning of your beautiful mental health journey. In the app, you can choose to reflect on your day with your journalling feature, relax with in-app guided meditations, track your mood, caffeine intake for our coffee lovers and periods for our menstruators, learn more about mental health, but what sets this app apart from mosts is our league creation. Let's say your boss at work wants to promote mental health in the workspace, as it boosts productivity and creates an overall happier work environment. They can create a league, and invite their workers to join. As the workers join the app and participate, they earn points. Whoever has the most points by the time the league duration has ended wins a prize from their boss! This is a great way to incentivize treating ones self.\nHow we built it\nI built it using Figma for the front end, which is where I spent most of my time, as I never really worked with it before. I programmed the back end in Java in Eclipse.\nChallenges we ran into\nI had never used Figma before, so getting adjusted to it at first definitely took more than a few hours of trial and error and tutorials.\nAccomplishments that we're proud of\nI am really proud of the way that this app turned out, considered I had never done UI/UX before. I can't wait to see what's next for blegin, I think that there are so many paths I can take with this app and that sounds really exciting to me! There are so many different aspects of mental health that one needs to nourish, and by beginning to add more and more this app can become a great tool for so many people.\nWhat we learned\nI had never worked with frontend this much before \u2013 I have always done backend in school, and I had dabbled with frontend features here and there when I've been making personal projects. I learned so much about how to create a minimalist yet eye catching app, and I learned a lot of the ins and outs of Figma throughout these past 24 hours.\nWhat's next for blegin\nI hope to continue working on it and publish it to the app store one day, as I am really passionate about nourishing one's mental health. Also, I hope to add fun brain game features under the relax tab, and push notifications to make sure that users are making the most out of their usage on the app!", "link": "https://devpost.com/software/blegin", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nmental health is just as important as your physical health \u2013 but this goes unnoticed by many. i wanted to make an app that emphasizes the importance of mental health in the workplace or in school, two places where it's so easy to feel incredibly stressed out, burnt out, disconnected, and just generally exhausted.\nwhat it does\nblegin represents a blend of different aspects of your mental health, all in the same app. however, we take the end out of blend and with it, we bring the beginning of your beautiful mental health journey. in the app, you can choose to reflect on your day with your journalling feature, relax with in-app guided meditations, track your mood, caffeine intake for our coffee lovers and periods for our menstruators, learn more about mental health, but what sets this app apart from mosts is our league creation. let's say your boss at work wants to promote mental health in the workspace, as it boosts productivity and creates an overall happier work environment. they can create a league, and invite their workers to join. as the workers join the app and participate, they earn points. whoever has the most points by the time the league duration has ended wins a prize from their boss! this is a great way to incentivize treating ones self.\nhow we built it\ni built it using figma for the front end, which is where i spent most of my time, as i never really worked with it before. i programmed the back end in java in eclipse.\nchallenges we ran into\ni had never used figma before, so getting adjusted to it at first definitely took more than a few hours of trial and error and tutorials.\naccomplishments that we're proud of\ni am really proud of the way that this app turned out, considered i had never done ui/ux before. i can't wait to see what's next for blegin, i think that there are so many paths i can take with this app and that sounds really exciting to me! there are so many different aspects of mental health that one needs to nourish, and by beginning to add more and more this app can become a great -----> tool !!!  for so many people.\nwhat we learned\ni had never worked with frontend this much before \u2013 i have always done backend in school, and i had dabbled with frontend features here and there when i've been making personal projects. i learned so much about how to create a minimalist yet eye catching app, and i learned a lot of the ins and outs of figma throughout these past 24 hours.\nwhat's next for blegin\ni hope to continue working on it and publish it to the app store one day, as i am really passionate about nourishing one's mental health. also, i hope to add fun brain game features under the relax tab, and push notifications to make sure that users are making the most out of their usage on the app!", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59503504}, {"Unnamed: 0": 3507, "autor": "Squire", "date": null, "content": "Inspiration\nOur inspiration for Squire was Microsoft's own smart code auto-completion tool GitHub Copilot. Copilot is a smart assistant that predicts what your next lines of code will be, based on what you have written previously. It can write code based on a prompt or purely based on the code already written. Since Copilot is currently in beta testing, we thought it would be a good idea to create our own smart assistant using GPT-2 that is open to the public.\nWhat it does\nSquire detects if a user is writing Python code, then analyzes what has previously been written, then suggests what it believes the user is about to write next using a Python trained GPT-2 model.\nHow we built it\nSquire is written entirely in Typescript and uses VS Code's API to detect the current user's file, interact with the previously written code, and suggest possible auto-completed code. Squire takes previously written lines of code by the user and passes them to a GPT-2 model hosted on the AI model hosting website Hugging Face. After the model returns the suggested next lines of code, it is suggested to the user using auto-completion.\nChallenges we ran into\nWe ran into multiple issues with getting VS Code to run our extension and calling the Hugging Face API for our model. VS Code would randomly no longer recognize our code, and we had to keep switching which computer was running the code locally. Since VS Code opens a separate window to test the extension, only the person who locally ran the code could test to see if the extension was working.\nWe also ran into struggles getting the Hugging Face API working. The API, when initially called, would return an error message saying the model had to boot up first. This would cause the extension to error, even when we didn't do anything with the response. Our solution to this was to run the extension, have it error, then close and run it again with the API now ready. This isn't a good solution, but we believe we could fix this with our own model.\nAccomplishments that we're proud of\nWe are proud of getting Squire to work at all, after our struggles. We are proud of being able to make our first-ever VS Code extension. None of us had experience working with VS Code's API before, and some of us had never even touched Typescript before!\nWe are proud of how well we worked together when debugging the program. Each team member noticed bugs in the code that the others didn't notice, and by working as a team, we were able to get the code to work.\nWhat we learned\nWe learned Typescript, the VS Code API, the Hugging Face API, and how GPT-2 works. We learned how to problem solve due to unexpected issues, and we learned how to work as a team and cover each other's weak points.\nWhat's next for Squire\nIn the future, we hope to use a model trained ourselves, instead of a pre-trained model, to have better results returned. Using our own model would allow us to have more control over what code is returned. We would also like to use GPT-3 as the basis for our model. GPT-3 is the newest model in the GPT line of language prediction models and is a significant upgrade to GPT-2. If we were able to train using GPT-3, our tool would be much more powerful and could suggest longer segments of code. Finally, in the future, we want to be able to assist with more languages than just python. That would involve either training one large model or training several smaller models, and calling whichever model when appropriate.", "link": "https://devpost.com/software/squire-gobrj3", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nour inspiration for squire was microsoft's own smart code auto-completion -----> tool !!!  github copilot. copilot is a smart assistant that predicts what your next lines of code will be, based on what you have written previously. it can write code based on a prompt or purely based on the code already written. since copilot is currently in beta testing, we thought it would be a good idea to create our own smart assistant using gpt-2 that is open to the public.\nwhat it does\nsquire detects if a user is writing python code, then analyzes what has previously been written, then suggests what it believes the user is about to write next using a python trained gpt-2 model.\nhow we built it\nsquire is written entirely in typescript and uses vs code's api to detect the current user's file, interact with the previously written code, and suggest possible auto-completed code. squire takes previously written lines of code by the user and passes them to a gpt-2 model hosted on the ai model hosting website hugging face. after the model returns the suggested next lines of code, it is suggested to the user using auto-completion.\nchallenges we ran into\nwe ran into multiple issues with getting vs code to run our extension and calling the hugging face api for our model. vs code would randomly no longer recognize our code, and we had to keep switching which computer was running the code locally. since vs code opens a separate window to test the extension, only the person who locally ran the code could test to see if the extension was working.\nwe also ran into struggles getting the hugging face api working. the api, when initially called, would return an error message saying the model had to boot up first. this would cause the extension to error, even when we didn't do anything with the response. our solution to this was to run the extension, have it error, then close and run it again with the api now ready. this isn't a good solution, but we believe we could fix this with our own model.\naccomplishments that we're proud of\nwe are proud of getting squire to work at all, after our struggles. we are proud of being able to make our first-ever vs code extension. none of us had experience working with vs code's api before, and some of us had never even touched typescript before!\nwe are proud of how well we worked together when debugging the program. each team member noticed bugs in the code that the others didn't notice, and by working as a team, we were able to get the code to work.\nwhat we learned\nwe learned typescript, the vs code api, the hugging face api, and how gpt-2 works. we learned how to problem solve due to unexpected issues, and we learned how to work as a team and cover each other's weak points.\nwhat's next for squire\nin the future, we hope to use a model trained ourselves, instead of a pre-trained model, to have better results returned. using our own model would allow us to have more control over what code is returned. we would also like to use gpt-3 as the basis for our model. gpt-3 is the newest model in the gpt line of language prediction models and is a significant upgrade to gpt-2. if we were able to train using gpt-3, our tool would be much more powerful and could suggest longer segments of code. finally, in the future, we want to be able to assist with more languages than just python. that would involve either training one large model or training several smaller models, and calling whichever model when appropriate.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 1, "media": null, "medialink": null, "identifyer": 59503507}, {"Unnamed: 0": 3515, "autor": "ToxBlocker", "date": null, "content": "Inspiration\nSocial media can be a breeding ground for nefarious actors promoting hate and toxicity at the expense of other users, who are sadly subjected to this content. For users who want to avoid toxic content online, we need a mechanism for identifying such content and alerting users to its presence. That's where ToxBlocker comes in.\nWhat it does\nToxBlocker is a tool that uses machine learning to generate toxicity scores for social media content. It proactively protects the mental health of users by giving them the power to decide what content they want to read, and what content they want to avoid.\nHow I built it\nThe model is built in Python, using Keras and the training data came from Kaggle.\nThe model architecture is an LSTM, which is a type of Recurrent Neural Network that captures spatial patterns in sequences (in this case sequences of words embedded as vectors).\nThe front end is built in JavaScript, Flask, and HTML.\nChallenges I ran into\nTime constraints were tight to find training data, process it, train and tune a model, deploy it, and create a nice interface for users to interact with and reap the benefits of the model.\nAccomplishments that I'm proud of\nDesigning a tool that empowers users to take control of the content that they consume and increases awareness of toxic content online.\nDeveloping a machine learning model using natural language processing and deploying it with a user-friendly interface.\nDesigning for an important issue with mental health implications and clear social impact.\nTraining a model that has enterprise value, and could be marketed to social media companies as a tool to give their users a better experience.\nWhat I learned\nThe development process, from initial idea to finished product, was a long and windy one, with many challenges, but a steady focus on the end goal was critical to keeping the project moving in the right direction and delivering a finished product in the end.\nWhat's next for ToxBlocker\nExpand the tool to alert users to toxicity on a series of platforms or websites.\nExpand the tool to also evaluate toxicity of non-textual content, like images, audio, video, etc.\nDevelop more complex models to label content not just with a toxicity score, but also with information about the type of content, so that users can look out for particular types of toxic content that may be specifically related to a sensitive issue or topic.", "link": "https://devpost.com/software/toxicitydetector", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nsocial media can be a breeding ground for nefarious actors promoting hate and toxicity at the expense of other users, who are sadly subjected to this content. for users who want to avoid toxic content online, we need a mechanism for identifying such content and alerting users to its presence. that's where toxblocker comes in.\nwhat it does\ntoxblocker is a -----> tool !!!  that uses machine learning to generate toxicity scores for social media content. it proactively protects the mental health of users by giving them the power to decide what content they want to read, and what content they want to avoid.\nhow i built it\nthe model is built in python, using keras and the training data came from kaggle.\nthe model architecture is an lstm, which is a type of recurrent neural network that captures spatial patterns in sequences (in this case sequences of words embedded as vectors).\nthe front end is built in javascript, flask, and html.\nchallenges i ran into\ntime constraints were tight to find training data, process it, train and tune a model, deploy it, and create a nice interface for users to interact with and reap the benefits of the model.\naccomplishments that i'm proud of\ndesigning a tool that empowers users to take control of the content that they consume and increases awareness of toxic content online.\ndeveloping a machine learning model using natural language processing and deploying it with a user-friendly interface.\ndesigning for an important issue with mental health implications and clear social impact.\ntraining a model that has enterprise value, and could be marketed to social media companies as a tool to give their users a better experience.\nwhat i learned\nthe development process, from initial idea to finished product, was a long and windy one, with many challenges, but a steady focus on the end goal was critical to keeping the project moving in the right direction and delivering a finished product in the end.\nwhat's next for toxblocker\nexpand the tool to alert users to toxicity on a series of platforms or websites.\nexpand the tool to also evaluate toxicity of non-textual content, like images, audio, video, etc.\ndevelop more complex models to label content not just with a toxicity score, but also with information about the type of content, so that users can look out for particular types of toxic content that may be specifically related to a sensitive issue or topic.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59503515}, {"Unnamed: 0": 3532, "autor": "ConsoleMates", "date": null, "content": "View our project\nOur application is fully up and running, and we've added a few features since our demo video, feel free to try it out here!\nInspiration\nOur generation is facing a mental health crisis. One group that is significantly impacted by this crisis is one that is close to our hearts; the developer community. In fact, in 2020, the Stack Overflow demographic survey showed that over 15% of the 65,000+ respondents admitted to having a mental health issue. We wanted to help reverse this trend by promoting openness and support within the community.\nWhat it does\nUsers can sign up for an account with their Google credentials, giving them access to ConsoleMates. Our UI is coding themed, and users can access all functionalities through our simulated console. Functionalities include sending letters, receiving and replying to letters, setting topic preferences, and viewing sentiment statistics.\nUsers can compose a letter expressing whatever is on their mind and can anonymously send it off into our system where it will be matched with a reader. Readers can then respond if they choose. After this, correspondence options will conclude, keeping communication simple. This design choice avoids the commitment and pressure of a traditional pen-pal application, leaving users free to log on when they need it most.\nTo maximize relatability between writer and reader, users can set topic preferences. These topics include: interviews, school, Leetcode, burnout, career, imposter syndrome, and other. After writing a letter, users can tag it with one or more of these topics, and it will be sent to a user with a matching preference.\nWhen a letter is sent to our backend, it is passed through the Python library text2emotion. This gives the letter an associated sentiment, which we track over time. By using the stats command, the user can view a pie chart of which emotions their letters have conveyed, serving as a tool to monitor their feelings over time.\nOur goal for ConsoleMates is to provide a platform where feelings can be expressed and a supportive community can grow. However, we know that some users may need more support than our solution provides, so we have a resources button that links to sites for additional help.\nHow we built it\nThe backend is built with Python, Flask, and PostgreSQL. We created a REST API for our frontend to call and get any data we need to display to the user. All data is kept in a PostgreSQL database. To perform sentiment analysis, we used the Python library Text2Emotion.\nThe frontend is built with React and TypeScript/JavaScript. Mockups were done in Figma and used to model frontend development, bringing our idea to life. The simulated console makes use of a premade console component and is tailored to our needs with custom command handlers.\nChallenges we ran into\nOne major challenge we encountered was database design. This was one of the very first elements we planned out for the project, so it was critical that we did it correctly, or we would be tying up loose ends for the rest of the hack. There was a lot of data we wanted to handle; users, letters, topics, and more. So, to properly plan out the database, we took it to the whiteboard and planned out an Entity Relationship Diagram. We were really happy we took on the challenge of properly designing the database as it saved us a lot of trouble later!\nA second challenge we faced was implementing the interactive console UI. We thought the design would really elevate our theme and make the experience more fun for users, but we had no idea where to start. Luckily, we found a great reusable React component that we could build off of, and we went from here. Figuring out how to get custom commands was still a huge challenge, but we\u2019re so glad we were able to add this into our UI!\nAccomplishments that we're proud of\nWe are really proud of the product we created. The topic of mental health in the software developer community is close to our hearts and we are super glad we got to contribute our product to the cause.\nOne thing we\u2019re particularly proud of is our creative UI. We loved the idea of a cute, calming UI to bring a relaxing atmosphere to the app, and we feel like we achieved that. We felt like the coding theme was greatly enhanced by our interactive console feature, and despite that being a major challenge to implement, we feel like that tied everything together!\nAnother element we\u2019re proud to submit is our backend. Since our last hackathon, we challenged ourselves to improve the quality of our backend development. This time, we used better coding practices when setting up the database, such as creating an ERD beforehand, and we were able to create a cleaner backend as a result.\nWhat we learned\nBroadly, we learned a ton about frontend and backend development. Specifically to our frontend, we went in-depth with how React and JavaScript worked with state saving. Another huge challenge we had to overcome was related to the implementation of a React terminal. This was a challenge for us because none of us had worked with this before. Working on the front end is always a challenge, but it was an amazing learning experience to work on a project that challenged us so actively, especially when trying to bring our intricate design to life! With regards to our backend, we learned a lot about how SQL queries work with database connections as well as initial database statements on how to initialize tables. Overall we feel like after this hackathon we have improved a ton in both frontend and backend development!\nWhat's next for ConsoleMates\nWe have some exciting ideas for ConsoleMates in the future! First, we would like to add some important user-experience improvements such as smart filtering to remove inappropriate letter submissions and automatic recirculation for letters that don't receive replies. Then, we would like to implement a gallery where you can save and revisit your favourite letters, refine the existing elements of our UI, and add support for mobile phones. Once this is done, we hope to add some finishing touches such as background music, an integrated VSCode editor for writing letters, and a sentiment-predicting ML algorithm that senses trends in letters. We are looking forward to seeing where this project takes us!\nFinal Thoughts\nThanks for reading about ConsoleMates! We hope you love it.", "link": "https://devpost.com/software/consolemates", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "view our project\nour application is fully up and running, and we've added a few features since our demo video, feel free to try it out here!\ninspiration\nour generation is facing a mental health crisis. one group that is significantly impacted by this crisis is one that is close to our hearts; the developer community. in fact, in 2020, the stack overflow demographic survey showed that over 15% of the 65,000+ respondents admitted to having a mental health issue. we wanted to help reverse this trend by promoting openness and support within the community.\nwhat it does\nusers can sign up for an account with their google credentials, giving them access to consolemates. our ui is coding themed, and users can access all functionalities through our simulated console. functionalities include sending letters, receiving and replying to letters, setting topic preferences, and viewing sentiment statistics.\nusers can compose a letter expressing whatever is on their mind and can anonymously send it off into our system where it will be matched with a reader. readers can then respond if they choose. after this, correspondence options will conclude, keeping communication simple. this design choice avoids the commitment and pressure of a traditional pen-pal application, leaving users free to log on when they need it most.\nto maximize relatability between writer and reader, users can set topic preferences. these topics include: interviews, school, leetcode, burnout, career, imposter syndrome, and other. after writing a letter, users can tag it with one or more of these topics, and it will be sent to a user with a matching preference.\nwhen a letter is sent to our backend, it is passed through the python library text2emotion. this gives the letter an associated sentiment, which we track over time. by using the stats command, the user can view a pie chart of which emotions their letters have conveyed, serving as a -----> tool !!!  to monitor their feelings over time.\nour goal for consolemates is to provide a platform where feelings can be expressed and a supportive community can grow. however, we know that some users may need more support than our solution provides, so we have a resources button that links to sites for additional help.\nhow we built it\nthe backend is built with python, flask, and postgresql. we created a rest api for our frontend to call and get any data we need to display to the user. all data is kept in a postgresql database. to perform sentiment analysis, we used the python library text2emotion.\nthe frontend is built with react and typescript/javascript. mockups were done in figma and used to model frontend development, bringing our idea to life. the simulated console makes use of a premade console component and is tailored to our needs with custom command handlers.\nchallenges we ran into\none major challenge we encountered was database design. this was one of the very first elements we planned out for the project, so it was critical that we did it correctly, or we would be tying up loose ends for the rest of the hack. there was a lot of data we wanted to handle; users, letters, topics, and more. so, to properly plan out the database, we took it to the whiteboard and planned out an entity relationship diagram. we were really happy we took on the challenge of properly designing the database as it saved us a lot of trouble later!\na second challenge we faced was implementing the interactive console ui. we thought the design would really elevate our theme and make the experience more fun for users, but we had no idea where to start. luckily, we found a great reusable react component that we could build off of, and we went from here. figuring out how to get custom commands was still a huge challenge, but we\u2019re so glad we were able to add this into our ui!\naccomplishments that we're proud of\nwe are really proud of the product we created. the topic of mental health in the software developer community is close to our hearts and we are super glad we got to contribute our product to the cause.\none thing we\u2019re particularly proud of is our creative ui. we loved the idea of a cute, calming ui to bring a relaxing atmosphere to the app, and we feel like we achieved that. we felt like the coding theme was greatly enhanced by our interactive console feature, and despite that being a major challenge to implement, we feel like that tied everything together!\nanother element we\u2019re proud to submit is our backend. since our last hackathon, we challenged ourselves to improve the quality of our backend development. this time, we used better coding practices when setting up the database, such as creating an erd beforehand, and we were able to create a cleaner backend as a result.\nwhat we learned\nbroadly, we learned a ton about frontend and backend development. specifically to our frontend, we went in-depth with how react and javascript worked with state saving. another huge challenge we had to overcome was related to the implementation of a react terminal. this was a challenge for us because none of us had worked with this before. working on the front end is always a challenge, but it was an amazing learning experience to work on a project that challenged us so actively, especially when trying to bring our intricate design to life! with regards to our backend, we learned a lot about how sql queries work with database connections as well as initial database statements on how to initialize tables. overall we feel like after this hackathon we have improved a ton in both frontend and backend development!\nwhat's next for consolemates\nwe have some exciting ideas for consolemates in the future! first, we would like to add some important user-experience improvements such as smart filtering to remove inappropriate letter submissions and automatic recirculation for letters that don't receive replies. then, we would like to implement a gallery where you can save and revisit your favourite letters, refine the existing elements of our ui, and add support for mobile phones. once this is done, we hope to add some finishing touches such as background music, an integrated vscode editor for writing letters, and a sentiment-predicting ml algorithm that senses trends in letters. we are looking forward to seeing where this project takes us!\nfinal thoughts\nthanks for reading about consolemates! we hope you love it.", "sortedWord": "None", "removed": "Nan", "score": 12, "comments": 0, "media": null, "medialink": null, "identifyer": 59503532}, {"Unnamed: 0": 3542, "autor": "Emojify", "date": null, "content": "Inspiration\nWhile scrolling through our Instagram feeds the other day, we came across the following account: https://www.instagram.com/yungjake/. We loved all of his emoji artwork, but were left wondering what our favorite photos would look like in this unique style. As computer science students we knew that we would not be able to recreate these ourselves, so we turned to the next best thing, programming! A flood of inspiration rushed towards us, and thus the idea for Emojify was born.\nWhat it does\nPut simply, Emojify allows users to upload images to our service and have them converted to emoji artwork. However, the app is so much more than that and serves to bring people together in a time where we all seem so far apart. Nowadays on social media pretty much all of the trends are based around either scandals or negativity, but Emojify was designed to bring more fun and positivity to anyone\u2019s timeline. Any memory from a meaningful photo, a cherished group memory, or simply a selfie can be given new life via the emojis we all use on a daily basis.\nHow we built it\nBefore we started building the project, we designed every portion of the app using the UI Design tool Figma (https://www.figma.com/file/oKKhRKzeCDWIkN6S2oKJXO/Emojify). The frontend for Emojify was built using React and custom CSS. The artwork is created via a POST request to our custom Python backend, containing the Base64 string of the uploaded image. On the backend, the uploaded image is resized into a square image proportional to the preset step and filter size, and then the filter steps across the image, taking the average color under the filter and writing the emoji with the closest average color to a canvas the same size as the as the filtered image. The closest emoji is found by searching an octtree which treats rgb values as coordinates for the average color and returning the emoji stored at the resulting leaf node.\nAll of the backend code is hosted using Google Cloud\u2019s App Engine and the frontend code is hosted using Vercel. The domain for the site was provided by Domain.com. We would like to give a special thanks to Google and Domain.com for the credits/promotions throughout our hackathon journey.\nChallenges we ran into\nOne aspect that we did not consider at the beginning of this project was how we would be able to calculate which emoji is closest to the color of a given pixel within the uploaded image. We ended up using an octree to recursively divide the color options into octants and quickly discover which was the closest.\nNeither of our team members knew very much about image processing, so it was quite the journey learning this new technology, but it was all worth it in the end.\nNote: We have a fully functional front-end and a fully functional back-end, however due to some last minute CORS issues we were not able to connect the two. We tried to set up a Cors Proxy Server, but the domain did not resolve in time for this to be functional\nAccomplishments that we're proud of\nWe are proud of how much we got done over this weekend. We both stepped outside of our comfort zones and learned new technologies, struggling along the way, but learning so much at the same time.\nWhat we learned\nFrom a frontend perspective, the UI utilized a lot of animations to give what would traditionally be a static design more personality and fluidity. We were new to CSS animations, so this project gave us a chance to mess around and be creative with those. Additionally, we had never really worked with uploading, storing, and sending Base64 images, which would no doubt be a valuable skill in the future.\nWorking on the backend, processing the image efficiently led us both to expand our knowledge in algorithms and image processing--given that each emoji selection required searching a 3-d space with 16581375 possibilities 400000+ times in less than a second, choosing the most efficient algorithm possible was critical. In ensuring that the backgrounds of the emojis selected didn\u2019t obscure the picture we were attempting to convey, we learned about background removal and transparency and how that changes when images are stored in a variety of common formats.\nNeither of our team members had used Google Cloud App Engine before, which seemed intimidating at first, but was actually very simple and made hosting our backend a breeze.\nWhat's next for Emojify\nWhile working on the project, we proposed the idea of connecting with a print-on-demand fulfilled to enable users to be able to easily purchase t-shirts and other products with their modified photos on them. This allows for users to showcase their favorite photos in a brand new way, leads to revenue generation to fund further development, and can increase the brand awareness of Emojify.", "link": "https://devpost.com/software/emojify-8c6dq3", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwhile scrolling through our instagram feeds the other day, we came across the following account: https://www.instagram.com/yungjake/. we loved all of his emoji artwork, but were left wondering what our favorite photos would look like in this unique style. as computer science students we knew that we would not be able to recreate these ourselves, so we turned to the next best thing, programming! a flood of inspiration rushed towards us, and thus the idea for emojify was born.\nwhat it does\nput simply, emojify allows users to upload images to our service and have them converted to emoji artwork. however, the app is so much more than that and serves to bring people together in a time where we all seem so far apart. nowadays on social media pretty much all of the trends are based around either scandals or negativity, but emojify was designed to bring more fun and positivity to anyone\u2019s timeline. any memory from a meaningful photo, a cherished group memory, or simply a selfie can be given new life via the emojis we all use on a daily basis.\nhow we built it\nbefore we started building the project, we designed every portion of the app using the ui design -----> tool !!!  figma (https://www.figma.com/file/okkhrkzecdwikn6s2okjxo/emojify). the frontend for emojify was built using react and custom css. the artwork is created via a post request to our custom python backend, containing the base64 string of the uploaded image. on the backend, the uploaded image is resized into a square image proportional to the preset step and filter size, and then the filter steps across the image, taking the average color under the filter and writing the emoji with the closest average color to a canvas the same size as the as the filtered image. the closest emoji is found by searching an octtree which treats rgb values as coordinates for the average color and returning the emoji stored at the resulting leaf node.\nall of the backend code is hosted using google cloud\u2019s app engine and the frontend code is hosted using vercel. the domain for the site was provided by domain.com. we would like to give a special thanks to google and domain.com for the credits/promotions throughout our hackathon journey.\nchallenges we ran into\none aspect that we did not consider at the beginning of this project was how we would be able to calculate which emoji is closest to the color of a given pixel within the uploaded image. we ended up using an octree to recursively divide the color options into octants and quickly discover which was the closest.\nneither of our team members knew very much about image processing, so it was quite the journey learning this new technology, but it was all worth it in the end.\nnote: we have a fully functional front-end and a fully functional back-end, however due to some last minute cors issues we were not able to connect the two. we tried to set up a cors proxy server, but the domain did not resolve in time for this to be functional\naccomplishments that we're proud of\nwe are proud of how much we got done over this weekend. we both stepped outside of our comfort zones and learned new technologies, struggling along the way, but learning so much at the same time.\nwhat we learned\nfrom a frontend perspective, the ui utilized a lot of animations to give what would traditionally be a static design more personality and fluidity. we were new to css animations, so this project gave us a chance to mess around and be creative with those. additionally, we had never really worked with uploading, storing, and sending base64 images, which would no doubt be a valuable skill in the future.\nworking on the backend, processing the image efficiently led us both to expand our knowledge in algorithms and image processing--given that each emoji selection required searching a 3-d space with 16581375 possibilities 400000+ times in less than a second, choosing the most efficient algorithm possible was critical. in ensuring that the backgrounds of the emojis selected didn\u2019t obscure the picture we were attempting to convey, we learned about background removal and transparency and how that changes when images are stored in a variety of common formats.\nneither of our team members had used google cloud app engine before, which seemed intimidating at first, but was actually very simple and made hosting our backend a breeze.\nwhat's next for emojify\nwhile working on the project, we proposed the idea of connecting with a print-on-demand fulfilled to enable users to be able to easily purchase t-shirts and other products with their modified photos on them. this allows for users to showcase their favorite photos in a brand new way, leads to revenue generation to fund further development, and can increase the brand awareness of emojify.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59503542}, {"Unnamed: 0": 3543, "autor": "Speedrix", "date": null, "content": "Inspiration\nThere is a fundamental problem regarding roads and traffic safety in the United States: over 42 million speeding tickets are issued across the country every year, with speeding being the culprit of 26% of all traffic fatalities \u2014 with 3,623 deaths in California alone. It is vital to address these issues, especially as motor vehicles become increasingly capable of traveling at faster speeds. Our goal was to create a tool to visualize areas where speeding is significant with the purpose of informing city governments and other municipalities where change is needed. This in turn allows them to develop case-specific action plans to save lives.\nWhat it does\nThis tool gathers data from INRIX APIs to reveal where current speeds are greater than the estimated speed limit on roads in San Francisco. Using this data, we highlight scenarios where speeding occurs as a visualization on a map that law enforcement agencies or other municipalities can utilize.\nHow we built it\nWe used React.JS, JavaScript, HTML, and CSS to program this tool. We used two INRIX APIs. The authAppToken API was used to get the App Key, allowing us to use the segment speed API to gather the majority of data. After parsing through the resulting JSON file, we used INRIX published longitude and latitude coordinates of road segments and aligned them with the speed that was being published by INRIX.\nWe focused our efforts on San Francisco as the API limited our data to that coordinate box, but we programmed scalability within to expand our display if we wanted to expand the impact of the project in the future. Our front end, powered by React.JS, uses the Google Maps API to display a variable map where react.js overlays are used to display our data.\nChallenges we ran into\nOne issue that we ran into was accessing the segment speed API offered by INRIX. When originally using the fetch() call, we originally lacked the correct header parameters in the function. This caused a CORS error that stumped us; however, after correcting the headers, the new implementation was successful.\nAnother challenge that we ran into was integrating the back end (written in plain JavaScript) and the front end (utilizing react.js). Because the overall functionality of react.js differed from JS, the functionality of the backend needed to change in order to be compatible. When changing the backend, we also ran into timing issues, which were quickly resolved with the correct implementation of await.\nAccomplishments that we're proud of\nWe are proud of our simple and accessible GUI that our targeted demographic \u2014 city municipalities and law enforcement \u2014 can understand and utilize. Oftentimes, large printouts and/or spreadsheets of data can be intimidating and difficult to understand, so having an intuitive interface that is displayed in a familiar method is very helpful. Furthermore, we took into consideration the limitations of public systems and allowed parameters to be customized to decrease the load on the computers.\nAnother aspect that we are proud of is the opportunity to scale this beyond the local scope of San Francisco; by utilizing the Google Maps API, we can expand it to other parts of the US or other countries. Because we create the appToken every time the program runs, we ensure security as well as growth beyond the scope of the original program.\nWhat we learned\nThe majority of what we did for this project was learned on-site. Most of our team knew next to nothing about Javascript and APIs, and we were pretty inexperienced in Github and other aspects of team programming overall. We learned that the ability to access API\u2019s (specifically utilizing the \u201cfetch\u201d command) is the most important aspect of a hackathon, simply because we would never learn this aspect of programming in a classroom environment.\nWhat's next for Speedprix\nIn the future, we would like to scale this project beyond San Francisco to other parts of the country and globally to make all roads safer. We would also like to have access to the speed limits on a specific section of road to make our analysis more accurate. Furthermore, in the future, we would like to merge this data with weather data to adjust for the higher risk of crashes (due to speed) because of bad weather, also incorporating traffic conditions to make the visualization more cohesive.", "link": "https://devpost.com/software/speedprix", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nthere is a fundamental problem regarding roads and traffic safety in the united states: over 42 million speeding tickets are issued across the country every year, with speeding being the culprit of 26% of all traffic fatalities \u2014 with 3,623 deaths in california alone. it is vital to address these issues, especially as motor vehicles become increasingly capable of traveling at faster speeds. our goal was to create a -----> tool !!!  to visualize areas where speeding is significant with the purpose of informing city governments and other municipalities where change is needed. this in turn allows them to develop case-specific action plans to save lives.\nwhat it does\nthis tool gathers data from inrix apis to reveal where current speeds are greater than the estimated speed limit on roads in san francisco. using this data, we highlight scenarios where speeding occurs as a visualization on a map that law enforcement agencies or other municipalities can utilize.\nhow we built it\nwe used react.js, javascript, html, and css to program this tool. we used two inrix apis. the authapptoken api was used to get the app key, allowing us to use the segment speed api to gather the majority of data. after parsing through the resulting json file, we used inrix published longitude and latitude coordinates of road segments and aligned them with the speed that was being published by inrix.\nwe focused our efforts on san francisco as the api limited our data to that coordinate box, but we programmed scalability within to expand our display if we wanted to expand the impact of the project in the future. our front end, powered by react.js, uses the google maps api to display a variable map where react.js overlays are used to display our data.\nchallenges we ran into\none issue that we ran into was accessing the segment speed api offered by inrix. when originally using the fetch() call, we originally lacked the correct header parameters in the function. this caused a cors error that stumped us; however, after correcting the headers, the new implementation was successful.\nanother challenge that we ran into was integrating the back end (written in plain javascript) and the front end (utilizing react.js). because the overall functionality of react.js differed from js, the functionality of the backend needed to change in order to be compatible. when changing the backend, we also ran into timing issues, which were quickly resolved with the correct implementation of await.\naccomplishments that we're proud of\nwe are proud of our simple and accessible gui that our targeted demographic \u2014 city municipalities and law enforcement \u2014 can understand and utilize. oftentimes, large printouts and/or spreadsheets of data can be intimidating and difficult to understand, so having an intuitive interface that is displayed in a familiar method is very helpful. furthermore, we took into consideration the limitations of public systems and allowed parameters to be customized to decrease the load on the computers.\nanother aspect that we are proud of is the opportunity to scale this beyond the local scope of san francisco; by utilizing the google maps api, we can expand it to other parts of the us or other countries. because we create the apptoken every time the program runs, we ensure security as well as growth beyond the scope of the original program.\nwhat we learned\nthe majority of what we did for this project was learned on-site. most of our team knew next to nothing about javascript and apis, and we were pretty inexperienced in github and other aspects of team programming overall. we learned that the ability to access api\u2019s (specifically utilizing the \u201cfetch\u201d command) is the most important aspect of a hackathon, simply because we would never learn this aspect of programming in a classroom environment.\nwhat's next for speedprix\nin the future, we would like to scale this project beyond san francisco to other parts of the country and globally to make all roads safer. we would also like to have access to the speed limits on a specific section of road to make our analysis more accurate. furthermore, in the future, we would like to merge this data with weather data to adjust for the higher risk of crashes (due to speed) because of bad weather, also incorporating traffic conditions to make the visualization more cohesive.", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59503543}, {"Unnamed: 0": 3584, "autor": "Crystal Ball", "date": null, "content": "Inspiration\nIn 2020, U.S. corporations spent upwards of $150 billion on digital advertising. This massively profitable industry is pushing companies to learn as much as possible about their consumer base in order to effectively market themselves. With the power of INRIX APIs, vital information about where customers are coming from can be utilized to benefit businesses of every variety. One of the products offered by INRIX is the \"Trade Area Trips\" API. This informational tool allows users to gather the details of any trip at any time from point A to point B on record. When confronted with this tool, our team noted its potential use for business owners. If companies were to know where their customers are coming from and when they are making their trips, it would dramatically improve their ability to market to their target audience. This information could even help businesses decide which high-traffic areas would suit a new location, or which areas require more effective advertising.\nWhat it does\nAfter receiving the address of a location as input, the program returns trip information within the standard radius of 10 miles collected from the INRIX Trade Area Trip API over the selected period of time. Within that radius, the website will break down the area into blocks, highlighting the areas from which consumers drove to get to the destination business. These regions are color-coded based on how many trips were made from that block to the destination, with more trips being represented by more vivid colors. The program also offers comparisons between date ranges. It allows users to select two date ranges, then gives them the option to overlay the data to see a percentage change over the specified date range in each region or to view the two data ranges side by side.\nHow we built it\nAt the core of the application lies the \"Trade Area Trips\" API. This API offers a collection of nearly every trip that INRIX records. For all of these trips, it documents the start time, end time, start location, end location, distance, average speed, and data credibility of each. These data points are incredibly valuable, especially when formatted in a user-friendly way. In order to make this data palatable for the general public, we parse and display it on a color-coded map. The frontend website to display the map was developed using Angular, and the backend that processes the data from INRIX\u2019s API was built with Node.js. The frontend web application incorporates the Leaflet library to visualize our data on a map, and uses the Chart.js library to display hour-by-hour data for each region. The backend uses Express.js to create a public API, and we used the Axios library to make our HTTP requests to INRIX. The data processing was primarily accomplished by sorting each individual trip with a classification function that turns it into a grid of boxes to be displayed on the map.\nChallenges we ran into\nAt first, we ran into issues fetching authentication tokens for the INRIX API, as most of us didn\u2019t have previous experience with restricted APIs. However, the INRIX documentation and our hackathon mentors were very helpful, and we eventually figured out how to refresh our tokens dynamically. We also faced challenges creating the grid of squares on our map, as we had to ensure we took into account the curvature of the Earth. We resolved this using a series of equations for translating latitude and longitude coordinates to miles, which worked perfectly.\nAccomplishments that we're proud of\nWe are extremely proud of the visualizations we were able to create. The map is interactive and clearly displays the data that is most important to the user. Being able to display our data over an interactive map was something we had really hoped to complete, and were extremely proud that it could be accomplished in time. Along with our visualization, the practical usage of the INRIX API that was provided is a feat we are very proud of. To reiterate, most of the team was completely clueless with APIs going into the project. Through the workshop and mentorship, we were able to overcome our lack of experience and create a working project. We pride ourselves on our new knowledge about the usage of APIs.\nWhat we learned\nSome members of the team came into this event with no prior experience in JavaScript, and were able to quickly learn in order to help contribute to the end product. Getting hands-on experience dealing with API calls provided members of our team with an incredible opportunity to learn real world skills. Additionally, we were able to learn about many of the freely available libraries and APIs offered for JavaScript, which allowed us to elevate the project as a whole. Starting from scratch is a tall task, and the truly effective programmer needs to know what libraries and APIs are at his or her disposal when creating a project.\nWhat's next for Crystal Ball\nThe team sees this as a constantly evolving product, as was demonstrated by many of the features that were added after the initial functionality of displaying relevant customer trip data was accomplished.Given the data-collecting nature of the INRIX API, Crystal Ball is scalable to larger quantities of data should they be provided. Our goal is to provide businesses with as much relevant information as possible. Some additional functions we could see being implemented include displaying traffic trends on top of the data as a way to explain consumer trip habits, as well as providing the user with the ability to export the data to analyze using their own tools. We also believe that this tool could provide even more helpful insight over longer periods of time and across larger regions, though we are limited by INRIX\u2019s API trial restrictions.", "link": "https://devpost.com/software/crystal-ball-nhlwxt", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nin 2020, u.s. corporations spent upwards of $150 billion on digital advertising. this massively profitable industry is pushing companies to learn as much as possible about their consumer base in order to effectively market themselves. with the power of inrix apis, vital information about where customers are coming from can be utilized to benefit businesses of every variety. one of the products offered by inrix is the \"trade area trips\" api. this informational -----> tool !!!  allows users to gather the details of any trip at any time from point a to point b on record. when confronted with this tool, our team noted its potential use for business owners. if companies were to know where their customers are coming from and when they are making their trips, it would dramatically improve their ability to market to their target audience. this information could even help businesses decide which high-traffic areas would suit a new location, or which areas require more effective advertising.\nwhat it does\nafter receiving the address of a location as input, the program returns trip information within the standard radius of 10 miles collected from the inrix trade area trip api over the selected period of time. within that radius, the website will break down the area into blocks, highlighting the areas from which consumers drove to get to the destination business. these regions are color-coded based on how many trips were made from that block to the destination, with more trips being represented by more vivid colors. the program also offers comparisons between date ranges. it allows users to select two date ranges, then gives them the option to overlay the data to see a percentage change over the specified date range in each region or to view the two data ranges side by side.\nhow we built it\nat the core of the application lies the \"trade area trips\" api. this api offers a collection of nearly every trip that inrix records. for all of these trips, it documents the start time, end time, start location, end location, distance, average speed, and data credibility of each. these data points are incredibly valuable, especially when formatted in a user-friendly way. in order to make this data palatable for the general public, we parse and display it on a color-coded map. the frontend website to display the map was developed using angular, and the backend that processes the data from inrix\u2019s api was built with node.js. the frontend web application incorporates the leaflet library to visualize our data on a map, and uses the chart.js library to display hour-by-hour data for each region. the backend uses express.js to create a public api, and we used the axios library to make our http requests to inrix. the data processing was primarily accomplished by sorting each individual trip with a classification function that turns it into a grid of boxes to be displayed on the map.\nchallenges we ran into\nat first, we ran into issues fetching authentication tokens for the inrix api, as most of us didn\u2019t have previous experience with restricted apis. however, the inrix documentation and our hackathon mentors were very helpful, and we eventually figured out how to refresh our tokens dynamically. we also faced challenges creating the grid of squares on our map, as we had to ensure we took into account the curvature of the earth. we resolved this using a series of equations for translating latitude and longitude coordinates to miles, which worked perfectly.\naccomplishments that we're proud of\nwe are extremely proud of the visualizations we were able to create. the map is interactive and clearly displays the data that is most important to the user. being able to display our data over an interactive map was something we had really hoped to complete, and were extremely proud that it could be accomplished in time. along with our visualization, the practical usage of the inrix api that was provided is a feat we are very proud of. to reiterate, most of the team was completely clueless with apis going into the project. through the workshop and mentorship, we were able to overcome our lack of experience and create a working project. we pride ourselves on our new knowledge about the usage of apis.\nwhat we learned\nsome members of the team came into this event with no prior experience in javascript, and were able to quickly learn in order to help contribute to the end product. getting hands-on experience dealing with api calls provided members of our team with an incredible opportunity to learn real world skills. additionally, we were able to learn about many of the freely available libraries and apis offered for javascript, which allowed us to elevate the project as a whole. starting from scratch is a tall task, and the truly effective programmer needs to know what libraries and apis are at his or her disposal when creating a project.\nwhat's next for crystal ball\nthe team sees this as a constantly evolving product, as was demonstrated by many of the features that were added after the initial functionality of displaying relevant customer trip data was accomplished.given the data-collecting nature of the inrix api, crystal ball is scalable to larger quantities of data should they be provided. our goal is to provide businesses with as much relevant information as possible. some additional functions we could see being implemented include displaying traffic trends on top of the data as a way to explain consumer trip habits, as well as providing the user with the ability to export the data to analyze using their own tools. we also believe that this tool could provide even more helpful insight over longer periods of time and across larger regions, though we are limited by inrix\u2019s api trial restrictions.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59503584}, {"Unnamed: 0": 3601, "autor": "COVID-19 PROGNOSIS", "date": null, "content": "Inspiration\nIn the year 2020-21, the most demanded medical test, was the test of the COVID-19 virus. The traditional methods involved collecting samples for RT-PCR tests. But this process often gave false negatives and also took a long time to predict the result.\nVarious research works by foreign scientists and researchers had proved that X-Ray/CT images could be efficiently used for the detection of the COVID-19 virus. It is seen that patients suffering from COVID-19, developed some particular type of lesions in their lungs, which is medically termed as Ground-Glass Opacities (GGOs). These lesions can be easily be seen through Chest X-Ray or CT images.\nSeveral researchers had developed various Deep Learning models to automatically classify the Radiological images of patients. There are various journal as well as conference papers in this context. But these researches were not implemented in applications to reach the common people. This project aims to integrate the Deep Learning Models into Web Applications, to provide an intuitive UI through which people can interact with the model to get proper predictions.\nWhat it does\nThe project is a Computer-Aided Diagnostic System which is used to predict whether a person has been infected with COVID-19. The prediction is possible through the integration of the COVID-19 X-Ray Classifier into a Web Application. By uploading Frontal Chest X-Rays, the model can perform classification between COVID and non-COVID X-Rays using Modified DenseNet architectures.\nThe users are given the option to save the results to the Database to receive further advice from doctors. The data is stored securely in MongoDB. Apart from this, REST API has been provided for Developers to access the Deep Learning Model and get prediction data in their applications.\nHow we built it\nThe project kickstarted with the Development of the Web Application. The UI was finalized and then the application was developed. Several open-source styles, libraries and toolkits were used during the development of the Frontend with HTML, CSS and JavaScript.\nAfter completion, the backend of the application was developed with Python & Flask framework. The routes were created and mapped to the Frontend. The Deep Learning Model was integrated with the backend REST APIs. Various libraries such as Numpy, Pillow and Tensorflow was used to manage the model. Finally, MongoDB was integrated with the backend to save the Form data.\nThis completed the Web Application Development.\nThe next stage involved deploying the application on CIVO K3s Cluster by developing an automated DevOps CI/CD Pipeline. First, the entire application code was pushed to a GitHub repository. Through this step, a code version control is ensured and any change in the code would automatically trigger the entire pipeline.\nTo deploy applications on K8s, the application needed to be containerized. The building of the Docker container should automatically take place once any code gets changed. After building the container, it needs to be pushed to a Docker Repository, here Dockerhub. Also, the old Docker Image Tag mentioned in the code would need to be replaced by the new Docker Image Tag. For automating all these, a Continuous Integration Pipeline was created with the help of Github Actions as the CI tool.\nA workflow file was written to sequence the jobs that needed to be performed, once the code get changed in the repository. The jobs involved building and pushing the Docker container to Dockerhub. After pushing, the new container tag replaced the older one mentioned in the customization file automatically, with the help of Kustomize.io. The Deployment, Service and Ingress YAML files were pushed to the repository as K8s needed these files during deployment. This completed the Continuous Integration process.\nThe final stage was to deploy the Docker Image pushed in DockerHub, into a CIVO k3s Cluster. For this, a K3s cluster was created on CIVO. Due to CPU intensive nature of the application, the Largest Node configuration was selected. Then through the CIVO CLI, the KubeConfig file was connected with the local KubeCTL tool.\nThrough KubeCTL, a namespace was created and ArgoCD was installed in it. Inside ArgoCD, the configuration was provided to continuously track the GitHub Repository for changes in the Kustomization file. Since previously through CI, we had managed to update the Kustomization file after a new code change took place, this update in the Kustomization file triggered the ArgoCD to re-deploy the application based on the newer Docker Image Tag provided. Thus after an initial manual Sync, ArgoCD managed to complete the Continuous Deployment process.\nFinally, the CI/CD Pipeline was successfully created which helped to automatically deploy code changes to production.\nChallenges we ran into\nFrom finding the right model to be integrated with the application to the actual deployment of the application in the CIVO K3s Cluster, a lot of challenges were faced. Some of them were:\nPushing the model weights file to GitHub was a challenge because it was larger than the permissible single file size limit of GitHub. Since the model needed to be frequently downloaded, LFS was not an ideal option due to its bandwidth cap. Thus after brainstorming, came up with the idea of splitting the large file into smaller chunks and pushing it into Github. During the Building of the container, all the pieces were merged and then pushed into the container as a single file. All these steps were mentioned in the Dockerfile.\nIt was a challenging task, integrate the Model Weights file with the Flask Application because the input image uploaded in the Frontend form, needed to first be converted into Base64 to get transferred to the backend. At the backend, it was again decoded back and passed into the model after some manipulations with Numpy and PIL libraries to make it suitable for the model, to ensure high accuracy.\nIt was a challenging task of selecting the combination of CI/CD tools being used in the application. Since there are numerous tools available for these purposes, it becomes challenging to select the one which can be set up quickly and can support the workflow properly. In this project, although I had experience with Github Actions, there was a need to involve ArgoCD in the picture as it is more suitable for the project.\nAccomplishments that we're proud of\nCompleting the Project without much deviation from the original idea is an accomplishment.\nThe fact that the entire project was made single-handedly, takes it to a higher level.\nThis is the first project, I deployed on Kubernetes, all thanks to CIVO for the wonderful sessions and guides.\nIt is the first time, I had worked on ArgoCD, which I had heard a lot about in GitOps.\nApart from that, the fact that I had developed an application, which can surely help in medical science in the not-so-distant future, is an achievement for me.\nWhat we learned\nThroughout the development of the project, I learnt a lot about Kubernetes. Although I had an idea of its functions, I was able to get deeper into the work and have hands-on experience with the CIVO K3s Cluster. I learnt about deployments, services, ingress and many other features of K8s and its difference with K3s.\nApart from that, I learnt about the ArgoCD platform which is a great tool focussed on GitOps development. I also learnt a lot about the various libraries of python to perform image manipulations.\nWhat's next for COVID-19 PROGNOSIS\nDue to lack of time, I was not able to integrate the Monitoring services like Prometheus and Grafana with the application. Through this, we can get various insights about the metrics of the cluster. I would surely integrate the monitoring and logging tools with the application inside the cluster.\nThe model which has been deployed can currently perform classification only. But recently, in some researches, it has been proved that through Instance Segmentation on the X-Rays, we can actually measure the severity of the spread of the virus by precisely identifying the locations of the GGOs. In the future, I want to integrate such a model with the application, so that users can also measure the severity of the virus instantly.", "link": "https://devpost.com/software/covid-19-prognosis", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nin the year 2020-21, the most demanded medical test, was the test of the covid-19 virus. the traditional methods involved collecting samples for rt-pcr tests. but this process often gave false negatives and also took a long time to predict the result.\nvarious research works by foreign scientists and researchers had proved that x-ray/ct images could be efficiently used for the detection of the covid-19 virus. it is seen that patients suffering from covid-19, developed some particular type of lesions in their lungs, which is medically termed as ground-glass opacities (ggos). these lesions can be easily be seen through chest x-ray or ct images.\nseveral researchers had developed various deep learning models to automatically classify the radiological images of patients. there are various journal as well as conference papers in this context. but these researches were not implemented in applications to reach the common people. this project aims to integrate the deep learning models into web applications, to provide an intuitive ui through which people can interact with the model to get proper predictions.\nwhat it does\nthe project is a computer-aided diagnostic system which is used to predict whether a person has been infected with covid-19. the prediction is possible through the integration of the covid-19 x-ray classifier into a web application. by uploading frontal chest x-rays, the model can perform classification between covid and non-covid x-rays using modified densenet architectures.\nthe users are given the option to save the results to the database to receive further advice from doctors. the data is stored securely in mongodb. apart from this, rest api has been provided for developers to access the deep learning model and get prediction data in their applications.\nhow we built it\nthe project kickstarted with the development of the web application. the ui was finalized and then the application was developed. several open-source styles, libraries and toolkits were used during the development of the frontend with html, css and javascript.\nafter completion, the backend of the application was developed with python & flask framework. the routes were created and mapped to the frontend. the deep learning model was integrated with the backend rest apis. various libraries such as numpy, pillow and tensorflow was used to manage the model. finally, mongodb was integrated with the backend to save the form data.\nthis completed the web application development.\nthe next stage involved deploying the application on civo k3s cluster by developing an automated devops ci/cd pipeline. first, the entire application code was pushed to a github repository. through this step, a code version control is ensured and any change in the code would automatically trigger the entire pipeline.\nto deploy applications on k8s, the application needed to be containerized. the building of the docker container should automatically take place once any code gets changed. after building the container, it needs to be pushed to a docker repository, here dockerhub. also, the old docker image tag mentioned in the code would need to be replaced by the new docker image tag. for automating all these, a continuous integration pipeline was created with the help of github actions as the ci -----> tool !!! .\na workflow file was written to sequence the jobs that needed to be performed, once the code get changed in the repository. the jobs involved building and pushing the docker container to dockerhub. after pushing, the new container tag replaced the older one mentioned in the customization file automatically, with the help of kustomize.io. the deployment, service and ingress yaml files were pushed to the repository as k8s needed these files during deployment. this completed the continuous integration process.\nthe final stage was to deploy the docker image pushed in dockerhub, into a civo k3s cluster. for this, a k3s cluster was created on civo. due to cpu intensive nature of the application, the largest node configuration was selected. then through the civo cli, the kubeconfig file was connected with the local kubectl tool.\nthrough kubectl, a namespace was created and argocd was installed in it. inside argocd, the configuration was provided to continuously track the github repository for changes in the kustomization file. since previously through ci, we had managed to update the kustomization file after a new code change took place, this update in the kustomization file triggered the argocd to re-deploy the application based on the newer docker image tag provided. thus after an initial manual sync, argocd managed to complete the continuous deployment process.\nfinally, the ci/cd pipeline was successfully created which helped to automatically deploy code changes to production.\nchallenges we ran into\nfrom finding the right model to be integrated with the application to the actual deployment of the application in the civo k3s cluster, a lot of challenges were faced. some of them were:\npushing the model weights file to github was a challenge because it was larger than the permissible single file size limit of github. since the model needed to be frequently downloaded, lfs was not an ideal option due to its bandwidth cap. thus after brainstorming, came up with the idea of splitting the large file into smaller chunks and pushing it into github. during the building of the container, all the pieces were merged and then pushed into the container as a single file. all these steps were mentioned in the dockerfile.\nit was a challenging task, integrate the model weights file with the flask application because the input image uploaded in the frontend form, needed to first be converted into base64 to get transferred to the backend. at the backend, it was again decoded back and passed into the model after some manipulations with numpy and pil libraries to make it suitable for the model, to ensure high accuracy.\nit was a challenging task of selecting the combination of ci/cd tools being used in the application. since there are numerous tools available for these purposes, it becomes challenging to select the one which can be set up quickly and can support the workflow properly. in this project, although i had experience with github actions, there was a need to involve argocd in the picture as it is more suitable for the project.\naccomplishments that we're proud of\ncompleting the project without much deviation from the original idea is an accomplishment.\nthe fact that the entire project was made single-handedly, takes it to a higher level.\nthis is the first project, i deployed on kubernetes, all thanks to civo for the wonderful sessions and guides.\nit is the first time, i had worked on argocd, which i had heard a lot about in gitops.\napart from that, the fact that i had developed an application, which can surely help in medical science in the not-so-distant future, is an achievement for me.\nwhat we learned\nthroughout the development of the project, i learnt a lot about kubernetes. although i had an idea of its functions, i was able to get deeper into the work and have hands-on experience with the civo k3s cluster. i learnt about deployments, services, ingress and many other features of k8s and its difference with k3s.\napart from that, i learnt about the argocd platform which is a great tool focussed on gitops development. i also learnt a lot about the various libraries of python to perform image manipulations.\nwhat's next for covid-19 prognosis\ndue to lack of time, i was not able to integrate the monitoring services like prometheus and grafana with the application. through this, we can get various insights about the metrics of the cluster. i would surely integrate the monitoring and logging tools with the application inside the cluster.\nthe model which has been deployed can currently perform classification only. but recently, in some researches, it has been proved that through instance segmentation on the x-rays, we can actually measure the severity of the spread of the virus by precisely identifying the locations of the ggos. in the future, i want to integrate such a model with the application, so that users can also measure the severity of the virus instantly.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 1, "media": null, "medialink": null, "identifyer": 59503601}, {"Unnamed: 0": 3602, "autor": "EdTechLib", "date": null, "content": "Inspiration\nSome of us have known teachers to have a hard time finding and using educational technology to promote classroom engagement recently, especially on Zoom. The COVID-19 pandemic really hit schools across the nation, and the style of teaching that has had to adapt to that has made it difficult for schools to perform at their same levels and optimize classroom engagement.\nWe made our project with this in mind.\nWhat it does\nIt's a platform that holds an inventory of different edtech tools so teachers and independent learners can collaborate and discuss how best to use different tools for learning and classroom engagement. App users will be able to find a variety of tools in a convenient place and click on tools they find interesting in order to view other users' comments on the tool.\nHow we built it\nDjango\nChallenges we ran into\nDjango\nAccomplishments that we're proud of\nIt took a long time and a lot of work, and two of us had come into the project largely unfamiliar with Django, web development, and educational technology at all. But even then, we leveraged our skills and had a good time learning to use and understand new technologies and concepts like data architecture (specifically using Python) and the general concepts behind linking front-end to back-end web development code\nWhat we learned\nDjango\nHTML/CSS\nSQLite databasing\nPatience\nWhat's next for EdTechLib\nMore features for interactivity and user input\nSubmitting new tools\nUser submission of comments and ratings\nLesson planning front-end built into the website", "link": "https://devpost.com/software/edtechlib", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nsome of us have known teachers to have a hard time finding and using educational technology to promote classroom engagement recently, especially on zoom. the covid-19 pandemic really hit schools across the nation, and the style of teaching that has had to adapt to that has made it difficult for schools to perform at their same levels and optimize classroom engagement.\nwe made our project with this in mind.\nwhat it does\nit's a platform that holds an inventory of different edtech tools so teachers and independent learners can collaborate and discuss how best to use different tools for learning and classroom engagement. app users will be able to find a variety of tools in a convenient place and click on tools they find interesting in order to view other users' comments on the -----> tool !!! .\nhow we built it\ndjango\nchallenges we ran into\ndjango\naccomplishments that we're proud of\nit took a long time and a lot of work, and two of us had come into the project largely unfamiliar with django, web development, and educational technology at all. but even then, we leveraged our skills and had a good time learning to use and understand new technologies and concepts like data architecture (specifically using python) and the general concepts behind linking front-end to back-end web development code\nwhat we learned\ndjango\nhtml/css\nsqlite databasing\npatience\nwhat's next for edtechlib\nmore features for interactivity and user input\nsubmitting new tools\nuser submission of comments and ratings\nlesson planning front-end built into the website", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59503602}, {"Unnamed: 0": 3612, "autor": "Circled Education", "date": null, "content": "Inspiration\nIn some parts of the world, individuals with hearing disabilities have been neglected and disregarded. Many struggle with pursuing their education especially in the IT field. Regarding networking, there hasn\u2019t been any platform that helps them reach others with impaired hearing across the globe. They have been sidelined, and some have been finding it difficult to pass a particular course because they have no one to understand and assist them. For example, A friend of Maureen's in Germany had started a python course in school. He was given an assignment but was incapable of how to complete it. He could have asked his classmates or others who could have assisted him, but because of the language barrier (they didn\u2019t know sign language), he couldn\u2019t communicate with them properly which led to him dropping out of the course. Therefore, awareness and assistance should be granted to those who are at a disadvantage all around the world. They shouldn\u2019t have to struggle the way Maureen\u2019s friend had, and they should be given equity to give them equality.\nWhat it does\nAlthough an app called Quora exists, where users could utilize a question and answer website, it is too generalized and isn\u2019t oriented towards assisting hearing impaired individuals. Circled Education, a soon-to-be social media platform for people with hearing disabilities, could assist millions of people with hearing disabilities pursue a proper education without being hassled with disadvantages that typical people wouldn\u2019t experience. In this app, students can interact with each other through video chat and text. By providing video chatting, people with hearing disabilities can use sign language and communicate with other people who are fluent in sign language. Furthermore, training in various fields will be held allowing users to join lectures, work in breakout rooms, and receive feedback from experts live around the globe.\nToday, Circled Education announces the new educational platform for the hearing and hearing impaired, to learn, network, and connect. We want to be able to make an impact through technology to the hearing impaired, help them develop pride in developing their skills, and help them gain the confidence to ask questions to complete their coursework. We want to be able to do something good for the world we all live in, and we want to help decrease the amount of disadvantaged people from dropping out of college courses by offering the following core benefits:\nLearning\nAsking questions relating to academics and getting response\nConnecting with students in the same field of studies and grade with you\nIncreasing your personal creativity and broadening your horizon(e.g technical tools, design etc)\nEncouraging joy and fulfillment\nGiving information about career opportunities to the hearing impaired\nHow we built it\nWe built it using Adobe XD, Google Docs, Google Slides, and Github. Adobe XD was the biggest help for us because it was utilized to create our mockup. It helped us implement the UI and create a better visual of our idea.\nChallenges we ran into\nChallenges we ran into were our time differences, time in general, and the fourth person not contributing. Because we leave in completely different areas (some live in Europe while others live in the United States), we had a hard time figuring out when we could work and communicate on the project. Time in general was also an issue because 24 hours is not a lot of time\u2014some had to leave and attend to other things which requires us having to work separately. These road blocks were big on us, but we were able to work around it to accomplish our goal. The fourth person didn't contribute to our project, so we had to cover her part of the project.\nAccomplishments that we're proud of\nAccomplishments that we're proud of are creating a mockup using Adobe XD and empowering women entrepreneurs as it is a platform for all women entrepreneurs all over the globe, to interact and collaborate. Adobe XD is an app that not most of the members knew how to use, but through learning and getting assistance from the internet and member who knew, we were able to get past it and create the app design. Lastly, we were able to bounce back after a member having been inactive and achieve this project.\nWhat we learned\nWe learned a lot through this Hackathon. We were able to learn about ourselves, new applications, and the handicap community. Through reaching out and creating a team through Slack, we were able to come together from different parts of the world to collaborate and create this project. New applications were also utilized to help others visualize our goal. Lastly, we were able to learn more about the handicap community through creating an idea that helps create more equity for them.\nWhat's next for Circled Education\nCircled Education would like to partner with Coursera, Women in Tech, Women who Code, Datacamp and Kaggle to provide IT certification courses to the hearing impaired.\nThis is an all-inclusive offering: For the premium users, every step in getting skilled through our experts will be guided and facilitated by the respective experts (Experts and Scholars) to give you the optimal outcome for your learning & personalization, all with the guarantee to also provide the best possible learning outcome to you.\nThe Circled Education application is all student community driven, so you will meet other people you can spar ideas with, bring your friends to re-design items together, and go home with something you can be truly proud of.\nAdditionally, if you want to go beyond the initial learning and exchange of skills, we are hosting competitions every quarter where you and your peers can rebuild a random item chosen by the crowd, and projects will be judged by a jury. The winner will be awarded the prestigious \u201cCircled Education\u201d Award, a winning certificate, and prizes such as free additional workshops, 3D-printers for your own home/community, or creative artworks from the circular economy influenced artists around the globe.\nYou can join training sessions via the scheduling tool on the website: www.circlededucation.design or the mobile application where you can also find the respective pricing, and book sessions.", "link": "https://devpost.com/software/educative-social-media-platform", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nin some parts of the world, individuals with hearing disabilities have been neglected and disregarded. many struggle with pursuing their education especially in the it field. regarding networking, there hasn\u2019t been any platform that helps them reach others with impaired hearing across the globe. they have been sidelined, and some have been finding it difficult to pass a particular course because they have no one to understand and assist them. for example, a friend of maureen's in germany had started a python course in school. he was given an assignment but was incapable of how to complete it. he could have asked his classmates or others who could have assisted him, but because of the language barrier (they didn\u2019t know sign language), he couldn\u2019t communicate with them properly which led to him dropping out of the course. therefore, awareness and assistance should be granted to those who are at a disadvantage all around the world. they shouldn\u2019t have to struggle the way maureen\u2019s friend had, and they should be given equity to give them equality.\nwhat it does\nalthough an app called quora exists, where users could utilize a question and answer website, it is too generalized and isn\u2019t oriented towards assisting hearing impaired individuals. circled education, a soon-to-be social media platform for people with hearing disabilities, could assist millions of people with hearing disabilities pursue a proper education without being hassled with disadvantages that typical people wouldn\u2019t experience. in this app, students can interact with each other through video chat and text. by providing video chatting, people with hearing disabilities can use sign language and communicate with other people who are fluent in sign language. furthermore, training in various fields will be held allowing users to join lectures, work in breakout rooms, and receive feedback from experts live around the globe.\ntoday, circled education announces the new educational platform for the hearing and hearing impaired, to learn, network, and connect. we want to be able to make an impact through technology to the hearing impaired, help them develop pride in developing their skills, and help them gain the confidence to ask questions to complete their coursework. we want to be able to do something good for the world we all live in, and we want to help decrease the amount of disadvantaged people from dropping out of college courses by offering the following core benefits:\nlearning\nasking questions relating to academics and getting response\nconnecting with students in the same field of studies and grade with you\nincreasing your personal creativity and broadening your horizon(e.g technical tools, design etc)\nencouraging joy and fulfillment\ngiving information about career opportunities to the hearing impaired\nhow we built it\nwe built it using adobe xd, google docs, google slides, and github. adobe xd was the biggest help for us because it was utilized to create our mockup. it helped us implement the ui and create a better visual of our idea.\nchallenges we ran into\nchallenges we ran into were our time differences, time in general, and the fourth person not contributing. because we leave in completely different areas (some live in europe while others live in the united states), we had a hard time figuring out when we could work and communicate on the project. time in general was also an issue because 24 hours is not a lot of time\u2014some had to leave and attend to other things which requires us having to work separately. these road blocks were big on us, but we were able to work around it to accomplish our goal. the fourth person didn't contribute to our project, so we had to cover her part of the project.\naccomplishments that we're proud of\naccomplishments that we're proud of are creating a mockup using adobe xd and empowering women entrepreneurs as it is a platform for all women entrepreneurs all over the globe, to interact and collaborate. adobe xd is an app that not most of the members knew how to use, but through learning and getting assistance from the internet and member who knew, we were able to get past it and create the app design. lastly, we were able to bounce back after a member having been inactive and achieve this project.\nwhat we learned\nwe learned a lot through this hackathon. we were able to learn about ourselves, new applications, and the handicap community. through reaching out and creating a team through slack, we were able to come together from different parts of the world to collaborate and create this project. new applications were also utilized to help others visualize our goal. lastly, we were able to learn more about the handicap community through creating an idea that helps create more equity for them.\nwhat's next for circled education\ncircled education would like to partner with coursera, women in tech, women who code, datacamp and kaggle to provide it certification courses to the hearing impaired.\nthis is an all-inclusive offering: for the premium users, every step in getting skilled through our experts will be guided and facilitated by the respective experts (experts and scholars) to give you the optimal outcome for your learning & personalization, all with the guarantee to also provide the best possible learning outcome to you.\nthe circled education application is all student community driven, so you will meet other people you can spar ideas with, bring your friends to re-design items together, and go home with something you can be truly proud of.\nadditionally, if you want to go beyond the initial learning and exchange of skills, we are hosting competitions every quarter where you and your peers can rebuild a random item chosen by the crowd, and projects will be judged by a jury. the winner will be awarded the prestigious \u201ccircled education\u201d award, a winning certificate, and prizes such as free additional workshops, 3d-printers for your own home/community, or creative artworks from the circular economy influenced artists around the globe.\nyou can join training sessions via the scheduling -----> tool !!!  on the website: www.circlededucation.design or the mobile application where you can also find the respective pricing, and book sessions.", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59503612}, {"Unnamed: 0": 3622, "autor": "Chronotivity", "date": null, "content": "Inspiration\n\u201cEnergy management is the process of tracking and optimizing energy consumption to conserve usage in a building. There are few steps for the process of energy management: Collecting and analyzing continuous data. Identify optimizations in equipment schedules, set points and flow rates to improve energy efficiency.\u201d\nTaking this principle in reference to our bodies, the pandemic brought forth a number of challenges when it comes to energy management. Productivity for some took a dip, while for others it surged, and for many their levels ebbed and flowed. But for most, there was very little insight into how to control this very important facet of our lives. While most productivity management tends to revolve around managing completion of tasks, we decided to play with the ideology of energy management which focuses on the right time frame for you to do the most fitting tasks according to when you\u2019re most energetic, focused, and motivated.\nIdentifies activities that give us energy & drains our energy So that we understand our fluctuating energy levels and seasonality of life Choose when to focus on and when to do it So that we can replenish our energy and take better care of ourselves Time management is what you're doing, energy management is when you\u2019re going to do it\nWhat it does\nChronotivity allows for users to log their energy, focus, and motivation levels and given enough data, it will present insights that help the user better understand the times where their energy peaks based on the popular animal chrono-types: Bear, Lion, Dolphin, and Wolf.\nHow we built it\nWe created a database that allows the users to log mood, focus and motivation levels at any time, without the need to keep the tracker open indefinitely, as the data is stored locally and will be available even after closing.\nWe created python classes to help build the database up, creating Time class, Course class, Assignment class, which were meant to be specific to students and allow Chronotivity to suggest times for when each assignment is to be done for courses, but we did not get the opportunity to fully implement that.\nChallenges we ran into\nInitially we were working with separate html, css files that would be the base of our web app, but due to issues with git and the trouble of integrating it with the python backend, we opted to demonstrate the interface through our prototype and further develop the interface at a later time. We created a UI using the Tkinter python library.\nAccomplishments that we're proud of\nIt\u2019s fantastic that we were able to create a product all of us on our team could benefit from, indicating that many other students could really enjoy this tool as well.\nWe\u2019re definitely proud to have been able to accommodate the issues we faced and managed to find alternative solutions such as our implementation of the idea using python rather than html and css.\nSome of our team members have never done projects outside of classes, and this Hackathon was a great way to show the development of one on our own.\nWhat we learned\nThere is always a way if we put our mind to it. We learned quite a lot about the powerful nature of python and the numerous libraries that are available to our disposal.\nWhat's next for Chronotivity\nWe are focused on implementing the student features of suggesting when certain assignments should be done based off of the user\u2019s estimation of how long the assignment will take. With that, we are hoping to establish an optimal suggestion of when to work on courses and when it may be best to focus on self-care and allow for the replenishing of energy. We're also hoping to expand variables of the recommendation algorithm to include other factors that impact energy such as meal-time, productivity, and hours of sleep.\nWatch this for presentation bloopers\ud83e\udd74\nBloopers: https://youtu.be/bCP3vK6w9as", "link": "https://devpost.com/software/chronotivity", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\n\u201cenergy management is the process of tracking and optimizing energy consumption to conserve usage in a building. there are few steps for the process of energy management: collecting and analyzing continuous data. identify optimizations in equipment schedules, set points and flow rates to improve energy efficiency.\u201d\ntaking this principle in reference to our bodies, the pandemic brought forth a number of challenges when it comes to energy management. productivity for some took a dip, while for others it surged, and for many their levels ebbed and flowed. but for most, there was very little insight into how to control this very important facet of our lives. while most productivity management tends to revolve around managing completion of tasks, we decided to play with the ideology of energy management which focuses on the right time frame for you to do the most fitting tasks according to when you\u2019re most energetic, focused, and motivated.\nidentifies activities that give us energy & drains our energy so that we understand our fluctuating energy levels and seasonality of life choose when to focus on and when to do it so that we can replenish our energy and take better care of ourselves time management is what you're doing, energy management is when you\u2019re going to do it\nwhat it does\nchronotivity allows for users to log their energy, focus, and motivation levels and given enough data, it will present insights that help the user better understand the times where their energy peaks based on the popular animal chrono-types: bear, lion, dolphin, and wolf.\nhow we built it\nwe created a database that allows the users to log mood, focus and motivation levels at any time, without the need to keep the tracker open indefinitely, as the data is stored locally and will be available even after closing.\nwe created python classes to help build the database up, creating time class, course class, assignment class, which were meant to be specific to students and allow chronotivity to suggest times for when each assignment is to be done for courses, but we did not get the opportunity to fully implement that.\nchallenges we ran into\ninitially we were working with separate html, css files that would be the base of our web app, but due to issues with git and the trouble of integrating it with the python backend, we opted to demonstrate the interface through our prototype and further develop the interface at a later time. we created a ui using the tkinter python library.\naccomplishments that we're proud of\nit\u2019s fantastic that we were able to create a product all of us on our team could benefit from, indicating that many other students could really enjoy this -----> tool !!!  as well.\nwe\u2019re definitely proud to have been able to accommodate the issues we faced and managed to find alternative solutions such as our implementation of the idea using python rather than html and css.\nsome of our team members have never done projects outside of classes, and this hackathon was a great way to show the development of one on our own.\nwhat we learned\nthere is always a way if we put our mind to it. we learned quite a lot about the powerful nature of python and the numerous libraries that are available to our disposal.\nwhat's next for chronotivity\nwe are focused on implementing the student features of suggesting when certain assignments should be done based off of the user\u2019s estimation of how long the assignment will take. with that, we are hoping to establish an optimal suggestion of when to work on courses and when it may be best to focus on self-care and allow for the replenishing of energy. we're also hoping to expand variables of the recommendation algorithm to include other factors that impact energy such as meal-time, productivity, and hours of sleep.\nwatch this for presentation bloopers\ud83e\udd74\nbloopers: https://youtu.be/bcp3vk6w9as", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 1, "media": null, "medialink": null, "identifyer": 59503622}, {"Unnamed: 0": 3679, "autor": "Safe-Fi", "date": null, "content": "Inspiration\nIncrease in plastic pollution and to keep Goa a attractive destination for tourists.\nWhat it does\nITS A SERVICE PLATFORM IN WHICH USER CAN EARN POINTS BY SUBMITTING THE PLASTIC WASTE THIS POINTS CAN BE USED BY USER TO GET DISCOUNTS OVER ONLINE SHOPPING SITES SUCH AS AMAZON, SIWGGY,ZOMATO AND ETC DUE TO CASHBACKS AND ETC PEOPLE WILL BE ENCOURAGED FOR COLLECTING PLASTIC AND SUBMITTING TO CENTER WHICH WILL HELP KEEPING THE GOA CLEAN\nHow we built it\nWe used html5, css3, and JavaScript for our website.\nChallenges we ran into\nWe had problem in search box tool, and filter.\nAccomplishments that we're proud of\nWe are proud of our hard work our determination and consistency, we now know what is the process of creating a website.\nWhat we learned\nour first hackathon wasn't how we intended but we learnt many valuable things, which we will apply in our future hackathons.\nWhat's next for Safe-Fi\nWe are thinking of improving its user interface, and probably thinking over a startup idea.", "link": "https://devpost.com/software/safe-fi", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nincrease in plastic pollution and to keep goa a attractive destination for tourists.\nwhat it does\nits a service platform in which user can earn points by submitting the plastic waste this points can be used by user to get discounts over online shopping sites such as amazon, siwggy,zomato and etc due to cashbacks and etc people will be encouraged for collecting plastic and submitting to center which will help keeping the goa clean\nhow we built it\nwe used html5, css3, and javascript for our website.\nchallenges we ran into\nwe had problem in search box -----> tool !!! , and filter.\naccomplishments that we're proud of\nwe are proud of our hard work our determination and consistency, we now know what is the process of creating a website.\nwhat we learned\nour first hackathon wasn't how we intended but we learnt many valuable things, which we will apply in our future hackathons.\nwhat's next for safe-fi\nwe are thinking of improving its user interface, and probably thinking over a startup idea.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59503679}, {"Unnamed: 0": 3698, "autor": "Phishing Website detection using Machine Learning", "date": null, "content": "Inspiration\nI have seen many people getting fooled by phishing websites and losing their lives. People need to have a tool that they can use to distinguish between legitimate and phishing websites. I wanted to do this project as I felt that Machine Learning can be used to tackle such need of the hour issues and help people in staying safe in the cyber world.\nWhat it does\nMy project is the building of a model, that can be integrated into websites or even developed in chrome extensions so that the user can enter the website and check if it is legitimate or not before proceeding.\nHow we built it\nI built this model using python and took the help of some great libraries like NumPy,sklearn. I have used a Decision tree classifier to train the model.\nChallenges we ran into\nThe problem was building the dataset. Since I didn't find datasets with both legitimate as well as phishing websites, I had to preprocess and use feature Extraction, to build new datasets.\nAccomplishments that we're proud of\nI have successfully built and trained the model and checked for its prediction too. The model provides 0.84 accuracy, which is quite good.\nWhat we learned\nI learned to use a lot of new libraries and since it was my own project, I had to deal with all the errors and rectify them on my own. So it was more of a learning experience.\nWhat's next for Phishing Website detection using Machine Learning\nI am developing a web application using the Flask framework. So the next step is to deploy it to public cloud or heroku.", "link": "https://devpost.com/software/phishing-website-detection-using-machine-learning", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ni have seen many people getting fooled by phishing websites and losing their lives. people need to have a -----> tool !!!  that they can use to distinguish between legitimate and phishing websites. i wanted to do this project as i felt that machine learning can be used to tackle such need of the hour issues and help people in staying safe in the cyber world.\nwhat it does\nmy project is the building of a model, that can be integrated into websites or even developed in chrome extensions so that the user can enter the website and check if it is legitimate or not before proceeding.\nhow we built it\ni built this model using python and took the help of some great libraries like numpy,sklearn. i have used a decision tree classifier to train the model.\nchallenges we ran into\nthe problem was building the dataset. since i didn't find datasets with both legitimate as well as phishing websites, i had to preprocess and use feature extraction, to build new datasets.\naccomplishments that we're proud of\ni have successfully built and trained the model and checked for its prediction too. the model provides 0.84 accuracy, which is quite good.\nwhat we learned\ni learned to use a lot of new libraries and since it was my own project, i had to deal with all the errors and rectify them on my own. so it was more of a learning experience.\nwhat's next for phishing website detection using machine learning\ni am developing a web application using the flask framework. so the next step is to deploy it to public cloud or heroku.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59503698}, {"Unnamed: 0": 3700, "autor": "Virtual Painter", "date": null, "content": "Inspiration\nWhile we were repainting our house we face problem select color combinations and that lead me to develop this tool.\nWhat it does\nIt virtually paint your rooms and let you get the best color combination for your estate.\nWhat's next for Virtual Painter\nIn upcoming time I am planning to integrate this with AI so that it can select the walls automatically.", "link": "https://devpost.com/software/virtual-painter-t7e9m6", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwhile we were repainting our house we face problem select color combinations and that lead me to develop this -----> tool !!! .\nwhat it does\nit virtually paint your rooms and let you get the best color combination for your estate.\nwhat's next for virtual painter\nin upcoming time i am planning to integrate this with ai so that it can select the walls automatically.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 1, "media": null, "medialink": null, "identifyer": 59503700}, {"Unnamed: 0": 3709, "autor": "Deep Learning for Weather Related PDEs", "date": null, "content": "Title\nDeep Learning for Weather Related PDEs\nWho\nQian Zhang, qzhang64 Sicheng Liu, sliu167 Ziyao Xu, zxu59\nIntroduction\nWeather is important in our daily life. Correct weather forecasts are critical for agriculture and industry. From fifty years ago, people started to simulate and forecast the weather by solving some simple differential equations like the Lorenz system with computer programs. Later on, with the rapid development of computational hardware and algorithms, it became possible to predict weather by solving Euler equations, a set of partial differential equations (PDEs) which is suitable for describing the dynamics of gas, but very costly for computation.\nNowadays, since deep learning has attained tremendous success in many fields, especially computer vision and natural language processing, we cannot help but think if it is possible to apply this powerful tool in computational science and engineering, like studying the weather related PDEs like Lorenz systems and Euler equations. An innovative idea was proposed in 2017 by Prof. George Karniadakis that we are able to train a neural network to solve the PDEs by adding PDE residuals to the loss function. Here we will follow the idea and use deep learning to study a forward problem and an inverse problem of the weather related PDEs. Although it is essentially a regression problem, the combination of the topics and approaches has not been studied before.\nRelated Work\nAn important paper we are referring to is Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations, which can be found at https://www.sciencedirect.com/science/article/pii/S0021999118307125.\nIn this paper, the authors want to encode the physics formulas, which often emerge in the form of PDEs, to the design of neural networks. Specifically, they proposed to add the residual of PDEs, i.e. the MSE between left hand side and right hand side, as long as the initial and boundary value conditions, as a loss function for the neural network. Since partial derivatives of the variables can be handled by auto differentiation in modern deep learning packages, the optimizer can minimize this loss to give us a solution. In our case, we will design the loss function similar to their idea, and use these losses to solve the forward and the inverse problem.\nData\nIn our project, the data has two parts: temporal and spatial coordinates, and the physical quantities on the corresponding coordinates. For the forward problem in our project, we do not need physical data to train the model, as it is physically constrained. We only need them for evaluation. For the inverse problem, we use both parts of the data to train the model to find out the parameters of the system.\nThese data are generated by traditional numerical methods. The data size is not fixed, as it depends on the need for training and validation.\nMethodology\nIn general, we train a model that takes the temporal and spatial coordinates as input and produces physical quantities as output. This model will use MLP as its structure because it is sufficient for our problems, even though it\u2019s quite simple.\nFor the forward problem, we have designed a loss function including initial condition, boundary condition and physics equation. In this loss, we do not need the generated physical quantities. Then we minimize the loss to make the network produce physical quantities which satisfy all these conditions. For the inverse problem, we designed the loss similarly but the loss on data is included. Since the parameter we would like to study participated in the forward computation of the physical quantities, we can optimize the loss between the prediction and the target data to get the parameter.\nMetrics\nFor our forward problem, we use high resolution generated data from traditional numerical solutions as the reference and use MSE as a metric between our results and the reference solution. The smaller MSE means the better our results are. For the inverse problem, since we have the parameters in data generation, we can directly compare the absolute error between the parameter from the inverse problem solver and the \u201ctrue\u201d parameter we assigned. Since the absolute value of the error may not be important as the values of the physical quantities can be very different for different problems, we can expect an error of magnitude around 10e-2.\nEthics\nDeep learning based algorithms are general and flexible. Since the PDEs used to model weather could be extremely computational costly to solve by traditional numerical methods, a simple end-to-end approach like deep learning can be helpful in simulating these equations. While traditional methods need to create fine mesh for complex geometry, which is very expensive, deep learning can handle these situations easily. Meanwhile, conditions and parameters of these PDEs can vary a lot in different cases. Deep learning, which is a general method which does not require much prior knowledge, can be important for solving the inverse problems.\nHowever, since there is little convergence study about deep learning, its robustness cannot be guaranteed. Since weather forecasting institutes may use the results of this project to improve their forecasting service, if this approach goes wrong or unstable sometimes, it can cause harmful results. Hence, traditional methods can still be used in parallel on coarser meshes as a validation, thus the possible risk of wrong prediction can be minimized.\nCheckin 2\nhttps://docs.google.com/document/d/1LzbR_vDQFousaEsoIVh9LkbG6uAJmFsQ1Lsi0OAB7_c/edit?usp=sharing\nFinal Reflection\nhttps://drive.google.com/file/d/1ELyfgvbmeZdwvOXbnXvf_fzCWikgJEFF/view?usp=sharing\nFinal Poster\nhttps://drive.google.com/file/d/1HzaGl5Fp6vPMTVts1jfWXJYdBNtca1og/view?usp=sharing\nCode\nhttps://github.com/zhangqian-sh/DL-for-weather-related-pdes", "link": "https://devpost.com/software/deep-learning-for-weather-related-pdes", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "title\ndeep learning for weather related pdes\nwho\nqian zhang, qzhang64 sicheng liu, sliu167 ziyao xu, zxu59\nintroduction\nweather is important in our daily life. correct weather forecasts are critical for agriculture and industry. from fifty years ago, people started to simulate and forecast the weather by solving some simple differential equations like the lorenz system with computer programs. later on, with the rapid development of computational hardware and algorithms, it became possible to predict weather by solving euler equations, a set of partial differential equations (pdes) which is suitable for describing the dynamics of gas, but very costly for computation.\nnowadays, since deep learning has attained tremendous success in many fields, especially computer vision and natural language processing, we cannot help but think if it is possible to apply this powerful -----> tool !!!  in computational science and engineering, like studying the weather related pdes like lorenz systems and euler equations. an innovative idea was proposed in 2017 by prof. george karniadakis that we are able to train a neural network to solve the pdes by adding pde residuals to the loss function. here we will follow the idea and use deep learning to study a forward problem and an inverse problem of the weather related pdes. although it is essentially a regression problem, the combination of the topics and approaches has not been studied before.\nrelated work\nan important paper we are referring to is physics-informed neural networks: a deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations, which can be found at https://www.sciencedirect.com/science/article/pii/s0021999118307125.\nin this paper, the authors want to encode the physics formulas, which often emerge in the form of pdes, to the design of neural networks. specifically, they proposed to add the residual of pdes, i.e. the mse between left hand side and right hand side, as long as the initial and boundary value conditions, as a loss function for the neural network. since partial derivatives of the variables can be handled by auto differentiation in modern deep learning packages, the optimizer can minimize this loss to give us a solution. in our case, we will design the loss function similar to their idea, and use these losses to solve the forward and the inverse problem.\ndata\nin our project, the data has two parts: temporal and spatial coordinates, and the physical quantities on the corresponding coordinates. for the forward problem in our project, we do not need physical data to train the model, as it is physically constrained. we only need them for evaluation. for the inverse problem, we use both parts of the data to train the model to find out the parameters of the system.\nthese data are generated by traditional numerical methods. the data size is not fixed, as it depends on the need for training and validation.\nmethodology\nin general, we train a model that takes the temporal and spatial coordinates as input and produces physical quantities as output. this model will use mlp as its structure because it is sufficient for our problems, even though it\u2019s quite simple.\nfor the forward problem, we have designed a loss function including initial condition, boundary condition and physics equation. in this loss, we do not need the generated physical quantities. then we minimize the loss to make the network produce physical quantities which satisfy all these conditions. for the inverse problem, we designed the loss similarly but the loss on data is included. since the parameter we would like to study participated in the forward computation of the physical quantities, we can optimize the loss between the prediction and the target data to get the parameter.\nmetrics\nfor our forward problem, we use high resolution generated data from traditional numerical solutions as the reference and use mse as a metric between our results and the reference solution. the smaller mse means the better our results are. for the inverse problem, since we have the parameters in data generation, we can directly compare the absolute error between the parameter from the inverse problem solver and the \u201ctrue\u201d parameter we assigned. since the absolute value of the error may not be important as the values of the physical quantities can be very different for different problems, we can expect an error of magnitude around 10e-2.\nethics\ndeep learning based algorithms are general and flexible. since the pdes used to model weather could be extremely computational costly to solve by traditional numerical methods, a simple end-to-end approach like deep learning can be helpful in simulating these equations. while traditional methods need to create fine mesh for complex geometry, which is very expensive, deep learning can handle these situations easily. meanwhile, conditions and parameters of these pdes can vary a lot in different cases. deep learning, which is a general method which does not require much prior knowledge, can be important for solving the inverse problems.\nhowever, since there is little convergence study about deep learning, its robustness cannot be guaranteed. since weather forecasting institutes may use the results of this project to improve their forecasting service, if this approach goes wrong or unstable sometimes, it can cause harmful results. hence, traditional methods can still be used in parallel on coarser meshes as a validation, thus the possible risk of wrong prediction can be minimized.\ncheckin 2\nhttps://docs.google.com/document/d/1lzbr_vdqfousaesoivh9lkbg6uajmfsq1lsi0oab7_c/edit?usp=sharing\nfinal reflection\nhttps://drive.google.com/file/d/1elyfgvbmezdwvoxbnxvf_fzcwikgjeff/view?usp=sharing\nfinal poster\nhttps://drive.google.com/file/d/1hzagl5fp6vpmtvts1jfwxjydbntca1og/view?usp=sharing\ncode\nhttps://github.com/zhangqian-sh/dl-for-weather-related-pdes", "sortedWord": "None", "removed": "Nan", "score": 7, "comments": 14, "media": null, "medialink": null, "identifyer": 59503709}, {"Unnamed: 0": 3728, "autor": "Advecting 3D Meshes using Point Cloud Interpolations", "date": null, "content": "Deliverables:\nFinal Write-up: https://drive.google.com/file/d/1RFlkyAIwKTwiGOsdW0lqhuOlSagOygf1/view?usp=sharing\nPresentation video: https://www.youtube.com/watch?v=sgjUwudivk8\nPoster: https://drive.google.com/file/d/1ul0lGPf169hrRa8jqw1HYolM4eW8WF7W/view?usp=sharing\nGithub: https://github.com/ArmanMaesumi/CSCI2470Final\nProject Check In #2: https://docs.google.com/document/d/1okfyGDuAcS5arpXj9dDYblSxYJLOQ5XLpiYRTmzQ_Vo\nIntroduction: I will investigate deep learning models that facilitate 3D shape synthesis and deformation. This is a central problem in computer graphics and vision, as many applications require the creation of realistic 3D models of real-world shapes. For example, in robotics, perception systems are trained using synthetic scenes, which are populated with 3D shapes, and in entertainment, 3D shapes are used to populated virtual scenes.\nRelated Work: My work will be related to two particular works: ShapeFlow and SP-GAN. The ShapeFlow paper introduced a learned deformation space, which generates a continuous vector field that advects mesh vertices. SP-GAN is a point cloud GAN that learns to synthesize new shapes by using a \"sphere prior.\" This prior essentially maps a spherical point cloud into the dataset shapes, which allows the model to learn an unsupervised dense correspondence mapping between all generated shapes.\nShapeFlow paper: https://arxiv.org/pdf/2006.07982.pdf SP-GAN paper: https://arxiv.org/abs/2108.04476\nData: My datasets will be entirely based on ShapeNet, a large repository of 3D shapes. This is the standard for shape synthesis, and it won't require significant processing (apart from partitioning the shapes into training/test sets).\nMethodology: I will first investigate the viability of using SP-GAN's output as the driver of a deformation. More concretely, I will deform a given mesh by using a point cloud interpolation sequence as an advection flow. After this, I will attempt to modify SP-GAN's model to better facilitate this task.\nMetrics: Evaluating shape synthesis (and any generative model) results is difficult do to the qualitative nature of it. Therefore, I will use modern quantitative evaluation metrics such as Frechet Inception Distance (FID), as well as Minimum Matching Distance, and Coverage.\nEthics: It is difficult to imagine an unethical use case of shape synthesis generative models. In the case of image synthesis (such as human faces), it is easy to see such use cases such as forgery and misinformation. However, for 3D shapes these use cases are a bit far fetched.\nDeep learning is the perfect tool for this job because the space of possible 3D shapes is massive, and creating a handwritten program that generates novel shapes is extremely difficult. For generative modeling, deep learning is practically the only option currently.", "link": "https://devpost.com/software/3d-shape-synthesis-and-deformation", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "deliverables:\nfinal write-up: https://drive.google.com/file/d/1rflkyaiwktwigosdw0lqhuolsagoygf1/view?usp=sharing\npresentation video: https://www.youtube.com/watch?v=sgjuwudivk8\nposter: https://drive.google.com/file/d/1ul0lgpf169hrra8jqw1hyolm4ew8wf7w/view?usp=sharing\ngithub: https://github.com/armanmaesumi/csci2470final\nproject check in #2: https://docs.google.com/document/d/1okfygduacs5arpxj9ddyblsxyjloq5xlpiyrtmzq_vo\nintroduction: i will investigate deep learning models that facilitate 3d shape synthesis and deformation. this is a central problem in computer graphics and vision, as many applications require the creation of realistic 3d models of real-world shapes. for example, in robotics, perception systems are trained using synthetic scenes, which are populated with 3d shapes, and in entertainment, 3d shapes are used to populated virtual scenes.\nrelated work: my work will be related to two particular works: shapeflow and sp-gan. the shapeflow paper introduced a learned deformation space, which generates a continuous vector field that advects mesh vertices. sp-gan is a point cloud gan that learns to synthesize new shapes by using a \"sphere prior.\" this prior essentially maps a spherical point cloud into the dataset shapes, which allows the model to learn an unsupervised dense correspondence mapping between all generated shapes.\nshapeflow paper: https://arxiv.org/pdf/2006.07982.pdf sp-gan paper: https://arxiv.org/abs/2108.04476\ndata: my datasets will be entirely based on shapenet, a large repository of 3d shapes. this is the standard for shape synthesis, and it won't require significant processing (apart from partitioning the shapes into training/test sets).\nmethodology: i will first investigate the viability of using sp-gan's output as the driver of a deformation. more concretely, i will deform a given mesh by using a point cloud interpolation sequence as an advection flow. after this, i will attempt to modify sp-gan's model to better facilitate this task.\nmetrics: evaluating shape synthesis (and any generative model) results is difficult do to the qualitative nature of it. therefore, i will use modern quantitative evaluation metrics such as frechet inception distance (fid), as well as minimum matching distance, and coverage.\nethics: it is difficult to imagine an unethical use case of shape synthesis generative models. in the case of image synthesis (such as human faces), it is easy to see such use cases such as forgery and misinformation. however, for 3d shapes these use cases are a bit far fetched.\ndeep learning is the perfect -----> tool !!!  for this job because the space of possible 3d shapes is massive, and creating a handwritten program that generates novel shapes is extremely difficult. for generative modeling, deep learning is practically the only option currently.", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 17, "media": null, "medialink": null, "identifyer": 59503728}, {"Unnamed: 0": 3740, "autor": "MedCare", "date": null, "content": "Inspiration\u26a1\nDiseases and illnesses are already bad enough as is. However, in today\u2019s society, misinformation is a severe issue in the healthcare sector. It can cause mass hysteria, and people may turn to harmful products in order to attempt to cure themselves of diseases they only believe they may have.\nIn recent years, especially with the extremely rapid development of social media platforms, misinformation has run rampant. Everyone has access to false information, and many people cling onto the words of popular influencers, who may not always be correct. This is a major problem, especially amongst a global pandemic, as a misinformed person serves not only puts themselves at risk, but they also endanger the lives of others.\nOur team recognized that the prevention of misinformation is a major concern in today\u2019s world. This inspired us to create MedCare, a product that not only helps users find information through credible sources, but also find information on potential illnesses with the click of a button.\nWhat it does\ud83c\udfe5\nMedCare is a self-diagnosis tool that uses sophisticated computer algorithms to provide a variety of diagnoses based on the user's experiences. The diagnosis tool searches a database for the symptom and its associated diseases based on the user's input. It then sorts all of the diseases present within the database by their rarity, displaying the most likely diseases with the inputted symptom. Finally, information on the few most likely diseases will be scraped primarily from credible resources on the internet, such as Mayo Clinic - diseases which will then be displayed for the user.\nHow we built it\ud83d\udcbb\nWhen we first began working on this project, we had one goal in mind: To create a simple and elegant web page that was user friendly, and easy to navigate. To properly execute this, we first planned out the major tasks and roles that needed to be met in order to efficiently finish the project on time. These included: finding a reliable dataset that contained both symptoms and diseases, as well as the correlation between the two, Our objective was to generate a list of possible diseases based on the symptoms entered. The web application was divided into two parts: backend and front end. We used HTML, CSS, and a little BootStrap to create a simple but elegant webpage on the front end. In the back end, we used the Python based web framework Flask, to sort through the data, find all possible diseases, and display their details from existing web pages on the internet. Finally, we hosted our webpage through the use of Flask.\nChallenges we ran into\ud83d\udd25\nOriginally, we planned to use JavaScript, React with Django as the web framework but we were unsure in regards to the database and how we could manipulate it to satisfy the needs of the diagnosing tool. Therefore, we decided to transition into using Flask.\nAccomplishments that we're proud of\u2728\nScrolling background\nUsing Flask as a web framework\nExtracting Data from a website\nCreating an elegant website\nA very user friendly product\nWhat we learned\ud83d\udcda\nWe learned that early diagnosis can result in early precautions and new treatment plans\nThere were a lot of steps we had to follow to finish the project\nIntegrating front end with a back end is a lot harder than one can assume\nWhat's next for MedCare\ud83d\udcc8\nMedCare has the potential to become a much larger project with more functions. If time wasn\u2019t a constraint, MedCare would have had additional features such as the ambulance tracker, nutrition guide, and maybe an AI consultant.", "link": "https://devpost.com/software/something-6arnhm", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\u26a1\ndiseases and illnesses are already bad enough as is. however, in today\u2019s society, misinformation is a severe issue in the healthcare sector. it can cause mass hysteria, and people may turn to harmful products in order to attempt to cure themselves of diseases they only believe they may have.\nin recent years, especially with the extremely rapid development of social media platforms, misinformation has run rampant. everyone has access to false information, and many people cling onto the words of popular influencers, who may not always be correct. this is a major problem, especially amongst a global pandemic, as a misinformed person serves not only puts themselves at risk, but they also endanger the lives of others.\nour team recognized that the prevention of misinformation is a major concern in today\u2019s world. this inspired us to create medcare, a product that not only helps users find information through credible sources, but also find information on potential illnesses with the click of a button.\nwhat it does\ud83c\udfe5\nmedcare is a self-diagnosis -----> tool !!!  that uses sophisticated computer algorithms to provide a variety of diagnoses based on the user's experiences. the diagnosis tool searches a database for the symptom and its associated diseases based on the user's input. it then sorts all of the diseases present within the database by their rarity, displaying the most likely diseases with the inputted symptom. finally, information on the few most likely diseases will be scraped primarily from credible resources on the internet, such as mayo clinic - diseases which will then be displayed for the user.\nhow we built it\ud83d\udcbb\nwhen we first began working on this project, we had one goal in mind: to create a simple and elegant web page that was user friendly, and easy to navigate. to properly execute this, we first planned out the major tasks and roles that needed to be met in order to efficiently finish the project on time. these included: finding a reliable dataset that contained both symptoms and diseases, as well as the correlation between the two, our objective was to generate a list of possible diseases based on the symptoms entered. the web application was divided into two parts: backend and front end. we used html, css, and a little bootstrap to create a simple but elegant webpage on the front end. in the back end, we used the python based web framework flask, to sort through the data, find all possible diseases, and display their details from existing web pages on the internet. finally, we hosted our webpage through the use of flask.\nchallenges we ran into\ud83d\udd25\noriginally, we planned to use javascript, react with django as the web framework but we were unsure in regards to the database and how we could manipulate it to satisfy the needs of the diagnosing tool. therefore, we decided to transition into using flask.\naccomplishments that we're proud of\u2728\nscrolling background\nusing flask as a web framework\nextracting data from a website\ncreating an elegant website\na very user friendly product\nwhat we learned\ud83d\udcda\nwe learned that early diagnosis can result in early precautions and new treatment plans\nthere were a lot of steps we had to follow to finish the project\nintegrating front end with a back end is a lot harder than one can assume\nwhat's next for medcare\ud83d\udcc8\nmedcare has the potential to become a much larger project with more functions. if time wasn\u2019t a constraint, medcare would have had additional features such as the ambulance tracker, nutrition guide, and maybe an ai consultant.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59503740}, {"Unnamed: 0": 3776, "autor": "End-to-End Object Detection of Transformers", "date": null, "content": "Re-Implementation of \u201cEnd-to-End Object Detection of Transformers\u201d\ngithub link: https://github.com/chelsea97/CS1470-project\npublic github link: https://github.com/Visual-Behavior/detr-tensorflow\ngoogle drive link: https://drive.google.com/drive/folders/19qTBVLCtruv0grLENKR91IFvZFfS_eJu?usp=sharing\nWho\nYuxuan Zhao (yzhao153) Zhuo Wang (zwang302) Xinhui Zhan (xzhan4)\nIntroduction\nThe goal of this project is to put the paper \"End-to-End Object Detection of Transformers\" into practice. For each object of interest, the purpose of object detection is to anticipate a collection of bounding boxes and category labels. This work also introduces a new perspective on object detection as a direct set prediction issue. This is a more effective method based on a new framework known as DEtection TRansformer(DETR), which is a set-based global loss that drives unique predictions using bipartite matching and a transformer encoder-decoder architecture. Unlike many other modern detectors, this model is much simpler to implement conceptually and there is no requirement for a specialized library. What\u2019s more, on the COCO object detection dataset, DETR achieves relatively great accuracy and run time performance compared with the Faster R-CNN baseline. Besides, this DETR can be further generalized to produce panoptic segmentation which is widely used nowadays. These are the main reasons why our group wants to have a deeper understanding of this paper and reimplement this model with a different frame and dataset.\nRelated Work\nNon-local Neural Network (Wang, X., Girshick, R.B., Gupta, A., He, K.) https://arxiv.org/pdf/1711.07971.pdf\nConvolutional and recurrent operations are both building blocks for processing a single local neighborhood at a time. This related paper presents a non-local operation as a generic family of building pieces for capturing long-range dependencies. Inspired by the conventional non-local means technique in computer vision, this non-local operation computes the response at a position as a weighted sum of the features at all positions. Moreover, this building element can be applied to many different computer vision architectures. As for the performance, non-local models can compete with existing competition winners on both the Kinetics and Charades datasets. Also, on the COCO dataset, it can improve object detection/segmentation and posture estimation in static picture identification. This non-local neural network can be applied to build a self-attention layer in the transformers in DETR.\nData\nIn many ways, Open Images is the largest annotated image dataset for training deep convolutional neural networks for computer vision tasks. It's a 9-million-image dataset with image-level labels, object bounding boxes, object segmentation masks, and visual relationships annotated. It also has a total of 16 million bounding boxes and 600 object classes in 1.9 million photos, making it the world's largest collection containing object position annotations. To ensure accuracy and consistency, the boxes were mostly drawn by hand by skilled annotators.\nVisual relationships annotations are also available in Open Images, indicating object pairs in certain relationships, object features, and human behaviors. In total, it contains 3.3 million annotations from 1466 different relationship triplets. With 9 million photos, 36 million image-level labels, 15.8 million bounding boxes, 2.8 million instance segmentations, and 391 thousand visual associations, the Open Images dataset currently has 9 million images. Recent discoveries in object detection, instance segmentation, and visual relationship detection have been motivated by the challenges of Open Images, as well as the dataset itself.\nThe Open images introduces 675k localized narratives in Version 6, which are multimodal image descriptions that include synchronized audio, text, and mouse trails over the items being narrated. Additionally, apart from localized narratives on trains in V6, it covers validation and testing as well.\nMethodology\nTensorflow will be our primary tool for developing and training the Detection TRansformer (DETR). To train DETR, we use a convolutional neural network to extract essential features from the original image, then send those data to an encoder-decoder architecture based on transformers, a popular architecture for sequence prediction. Transformers' self-attention mechanisms, which explicitly describe all pairwise interactions between components in a sequence, making them particularly well suited to impose prediction limitations like deleting duplicate predictions.\nDETR predicts all items at the same time and is trained with a set loss function that performs bipartite matching between predicted and ground-truth objects from start to finish. DETR streamlines detection pipelines by eliminating certain hand-crafted components that encode past information, such as spatial anchoring and non-maximal suppression. DETR, unlike most other detection methods, does not require any customized layers and can thus be simply replicated in any framework that includes standard CNN and transformer classes. The combination of bipartite matching loss and transformers with parallel encoding is one of DETR's primary features. Previous research, on the other hand, concentrated on RNN-based autoregressive decoding. The DETR matching loss function is invariant to a permutation of predicted objects and uniquely assigns a prediction to a ground truth item.\nDETR's training settings differ from those of ordinary object detectors in a number of ways. The new model necessitates an extra-long training regimen and takes advantage of the transformer's auxiliary decoding losses. We go over all of the components that are necessary for the exhibited performance in detail. The most difficult component of developing a model, in our opinion, is changing model parameters and determining an acceptable prediction number N in order to get the best fit between expected and real classes, as well as a grounding box. Furthermore, how to define and discover the proper grounding box size is critical since DETR not only predicts the correct class but also draws the proper grounding box size to fit the shape of the objects when compared to test figures that are artificially labeled.\nMetrics\nWhat experiments do you plan to run? We are planning to compare our model with Faster R-CNN in a quantitative approach.\nFor most of our assignments, we have looked at the accuracy of the model. Does the notion of \u201caccuracy\u201d apply to your project, or is some other metric more appropriate? Yes, we are trying to do a similar evaluation of AP(Average Precision) in this paper, and compare different performances in various settings of decoders quantities.\nIf you are implementing an existing project, detail what the authors of that paper were hoping to find and how they quantified the results of their model. The authors of this paper propose DETR to detect objects in pictures utilizing transformers and bipartite and justify this model is comparable to optimized Faster R-CNN on the COCO dataset in AP. However, DETR is easy to implement and extensible for other tasks such as panoptic segmentation. Also, it works better with large objects than Faster R-CNN.\nWhat are your base, target, and stretch goals? Our base goal is to implement a workable DETR model in different platforms, namely, tensor flow. Our target and stretch goals will be to achieve AP around 40 under 12 layers.\nEthics\nOur aim is to build a model that can detect objects\u2019 size and position on a picture. Here are some possible ethical issues related to this topic.\nThe dataset used is probably only has restricted objects\u2019 categories, which means that there is a great potential to guess some new objects in existing wrong categories, and also marked with inappropriate bounding. So the domain of the dataset will affect the precision and performance of actual use.\nWe should be careful with the bias of our training set, which means the classes and marks are labeled by people and inevitably there will be different descriptions of the same things, and even worse biased and offensive labels will be used in this dataset. If we use this model to detect new pictures, these kinds of stereotypes and biases will be carried on.\nDivision of labor\nPlan to cooperate evenly: final project coding, and its writeup/ reflection Zhuo Wang: a poster of the project Yuxuan Zhao: check-in #2 reflection Xinhui Zhan: record an oral presentation", "link": "https://devpost.com/software/end-to-end-object-detection-of-transformers", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "re-implementation of \u201cend-to-end object detection of transformers\u201d\ngithub link: https://github.com/chelsea97/cs1470-project\npublic github link: https://github.com/visual-behavior/detr-tensorflow\ngoogle drive link: https://drive.google.com/drive/folders/19qtbvlctruv0grlenkr91ifvzffs_eju?usp=sharing\nwho\nyuxuan zhao (yzhao153) zhuo wang (zwang302) xinhui zhan (xzhan4)\nintroduction\nthe goal of this project is to put the paper \"end-to-end object detection of transformers\" into practice. for each object of interest, the purpose of object detection is to anticipate a collection of bounding boxes and category labels. this work also introduces a new perspective on object detection as a direct set prediction issue. this is a more effective method based on a new framework known as detection transformer(detr), which is a set-based global loss that drives unique predictions using bipartite matching and a transformer encoder-decoder architecture. unlike many other modern detectors, this model is much simpler to implement conceptually and there is no requirement for a specialized library. what\u2019s more, on the coco object detection dataset, detr achieves relatively great accuracy and run time performance compared with the faster r-cnn baseline. besides, this detr can be further generalized to produce panoptic segmentation which is widely used nowadays. these are the main reasons why our group wants to have a deeper understanding of this paper and reimplement this model with a different frame and dataset.\nrelated work\nnon-local neural network (wang, x., girshick, r.b., gupta, a., he, k.) https://arxiv.org/pdf/1711.07971.pdf\nconvolutional and recurrent operations are both building blocks for processing a single local neighborhood at a time. this related paper presents a non-local operation as a generic family of building pieces for capturing long-range dependencies. inspired by the conventional non-local means technique in computer vision, this non-local operation computes the response at a position as a weighted sum of the features at all positions. moreover, this building element can be applied to many different computer vision architectures. as for the performance, non-local models can compete with existing competition winners on both the kinetics and charades datasets. also, on the coco dataset, it can improve object detection/segmentation and posture estimation in static picture identification. this non-local neural network can be applied to build a self-attention layer in the transformers in detr.\ndata\nin many ways, open images is the largest annotated image dataset for training deep convolutional neural networks for computer vision tasks. it's a 9-million-image dataset with image-level labels, object bounding boxes, object segmentation masks, and visual relationships annotated. it also has a total of 16 million bounding boxes and 600 object classes in 1.9 million photos, making it the world's largest collection containing object position annotations. to ensure accuracy and consistency, the boxes were mostly drawn by hand by skilled annotators.\nvisual relationships annotations are also available in open images, indicating object pairs in certain relationships, object features, and human behaviors. in total, it contains 3.3 million annotations from 1466 different relationship triplets. with 9 million photos, 36 million image-level labels, 15.8 million bounding boxes, 2.8 million instance segmentations, and 391 thousand visual associations, the open images dataset currently has 9 million images. recent discoveries in object detection, instance segmentation, and visual relationship detection have been motivated by the challenges of open images, as well as the dataset itself.\nthe open images introduces 675k localized narratives in version 6, which are multimodal image descriptions that include synchronized audio, text, and mouse trails over the items being narrated. additionally, apart from localized narratives on trains in v6, it covers validation and testing as well.\nmethodology\ntensorflow will be our primary -----> tool !!!  for developing and training the detection transformer (detr). to train detr, we use a convolutional neural network to extract essential features from the original image, then send those data to an encoder-decoder architecture based on transformers, a popular architecture for sequence prediction. transformers' self-attention mechanisms, which explicitly describe all pairwise interactions between components in a sequence, making them particularly well suited to impose prediction limitations like deleting duplicate predictions.\ndetr predicts all items at the same time and is trained with a set loss function that performs bipartite matching between predicted and ground-truth objects from start to finish. detr streamlines detection pipelines by eliminating certain hand-crafted components that encode past information, such as spatial anchoring and non-maximal suppression. detr, unlike most other detection methods, does not require any customized layers and can thus be simply replicated in any framework that includes standard cnn and transformer classes. the combination of bipartite matching loss and transformers with parallel encoding is one of detr's primary features. previous research, on the other hand, concentrated on rnn-based autoregressive decoding. the detr matching loss function is invariant to a permutation of predicted objects and uniquely assigns a prediction to a ground truth item.\ndetr's training settings differ from those of ordinary object detectors in a number of ways. the new model necessitates an extra-long training regimen and takes advantage of the transformer's auxiliary decoding losses. we go over all of the components that are necessary for the exhibited performance in detail. the most difficult component of developing a model, in our opinion, is changing model parameters and determining an acceptable prediction number n in order to get the best fit between expected and real classes, as well as a grounding box. furthermore, how to define and discover the proper grounding box size is critical since detr not only predicts the correct class but also draws the proper grounding box size to fit the shape of the objects when compared to test figures that are artificially labeled.\nmetrics\nwhat experiments do you plan to run? we are planning to compare our model with faster r-cnn in a quantitative approach.\nfor most of our assignments, we have looked at the accuracy of the model. does the notion of \u201caccuracy\u201d apply to your project, or is some other metric more appropriate? yes, we are trying to do a similar evaluation of ap(average precision) in this paper, and compare different performances in various settings of decoders quantities.\nif you are implementing an existing project, detail what the authors of that paper were hoping to find and how they quantified the results of their model. the authors of this paper propose detr to detect objects in pictures utilizing transformers and bipartite and justify this model is comparable to optimized faster r-cnn on the coco dataset in ap. however, detr is easy to implement and extensible for other tasks such as panoptic segmentation. also, it works better with large objects than faster r-cnn.\nwhat are your base, target, and stretch goals? our base goal is to implement a workable detr model in different platforms, namely, tensor flow. our target and stretch goals will be to achieve ap around 40 under 12 layers.\nethics\nour aim is to build a model that can detect objects\u2019 size and position on a picture. here are some possible ethical issues related to this topic.\nthe dataset used is probably only has restricted objects\u2019 categories, which means that there is a great potential to guess some new objects in existing wrong categories, and also marked with inappropriate bounding. so the domain of the dataset will affect the precision and performance of actual use.\nwe should be careful with the bias of our training set, which means the classes and marks are labeled by people and inevitably there will be different descriptions of the same things, and even worse biased and offensive labels will be used in this dataset. if we use this model to detect new pictures, these kinds of stereotypes and biases will be carried on.\ndivision of labor\nplan to cooperate evenly: final project coding, and its writeup/ reflection zhuo wang: a poster of the project yuxuan zhao: check-in #2 reflection xinhui zhan: record an oral presentation", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 4, "media": null, "medialink": null, "identifyer": 59503776}, {"Unnamed: 0": 3802, "autor": "Using Rotational Bounding Box for Zebra Crossing Detection", "date": null, "content": "Inspiration\nNowadays, deep learning is more and more useful in multiple areas for benefitting people\u2019s daily life. \u201cWhat objects are where?\u201d, people raised this question, and then create a specific task called object detection, which is an important computer vision task used to detect instances of visual objects of certain classes in digital images. Road semantics and maps annotation remains a popular topic in deep learning as self-driving continues to be one of the most researched topics. Self-driving cars often require precise annotation of maps to perceive its surroundings accurately. In this project, we want to utilize object detection with deep learning to create a map annotation tool. We implemented a two component model that would read aerial map images, classify their categories and pass appropriate images to the following object detection network for annotation.\nWhat it does\nIn this project, we want to utilize object detection with deep learning to create a map annotation tool.\nHow we built it\nWe plan to do experiments on lots of traffic images that contain traffic lanes and try to use bounding boxes to correctly mark them. Most of the dataset only contains images that are from non-aerial view, which is hard to label the suitable bounding boxes since they are easily out of shape. Google Map API is a good tool to download city images from aerial views.\nThe first part of our map annotation model is a map reader that utilizes the Google Maps Static API. We have implemented a reader that would download images of specific locations given the latitude and longitude or download continuous grids of images given a start point and how many images are wanted. After downloading, the images are passed through a classifier to determine if this is an image of a city with roads we want to annotate. The classifier is a convolutional neural network with 13 convolution layers following the structure of the residual block in ResNet (He et al. 2015). We trained the classifier on randomly downloaded images of wildlands and cities from Google Map.\nThe basic YOLO model (You Only Look Once) applies a single forward pass neural network to the whole image. It can predict the bounding boxes and their class probabilities. It has 53 convolutional layers called Darknet-53. For distinguishing the lane from other stripe patterns, we can directly compare the experimental results with the actual results. For the bounding box, we can compare the angle and size of the bounding box with the actual bounding box we want. When implementing the loss function for our YoloV3 model. Firstly, we are using the loss function as only considering the confidence level of the arrays of predicted boxes with the true labels and computing the maximum iou(Intersection over Union). It raises the issue that the spatial information of bounding boxes is difficult to maintain and therefore poor in accuracy. Then, we add many factors such as center loss, weight and height loss, and class loss to keep the ious accurately calculated by using the different specific loss methods. Furthermore, we finally move on to add degree as another factor and perform the atan2 method combining with a scaled binary cross-entropy.\nOur base target is first to try to distinguish the crosswalk from other stripe patterns and then can use the basic horizontal bounding box to fit the crosswalk no matter the rotating or scaling of the lane. The stretch goal is to perfectly fit the crosswalk.\nProject Deadline Check\nProject Check in #1\nLink: https://docs.google.com/document/d/1AFKgrATaU0iTTASx7bc7jVrYGlEfWpOqUc5WH2w6WiQ/edit?usp=sharing\nProject Check in #2\nLink: https://docs.google.com/document/d/1xw_PeBiKooY_b2nearwg_pL_ctVqNyzEHIkYotLfJxY/edit?usp=sharing\nFinal Project Submission\nOral Presentation Link: https://youtu.be/E-fsY8SQY5g\nPoster Link: https://drive.google.com/file/d/1bbsiGykX2A9vbydF5HcmsKn0G6OEoGdj/view?usp=sharing\nFinal Report Link: https://docs.google.com/document/d/1L4Z30lt3PBgpErY1iAMXWII1fzXkI15CqHMpbg8hyfU/edit?usp=sharing", "link": "https://devpost.com/software/using-bounding-box-for-traffic-lane-detection", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nnowadays, deep learning is more and more useful in multiple areas for benefitting people\u2019s daily life. \u201cwhat objects are where?\u201d, people raised this question, and then create a specific task called object detection, which is an important computer vision task used to detect instances of visual objects of certain classes in digital images. road semantics and maps annotation remains a popular topic in deep learning as self-driving continues to be one of the most researched topics. self-driving cars often require precise annotation of maps to perceive its surroundings accurately. in this project, we want to utilize object detection with deep learning to create a map annotation -----> tool !!! . we implemented a two component model that would read aerial map images, classify their categories and pass appropriate images to the following object detection network for annotation.\nwhat it does\nin this project, we want to utilize object detection with deep learning to create a map annotation -----> tool !!! .\nhow we built it\nwe plan to do experiments on lots of traffic images that contain traffic lanes and try to use bounding boxes to correctly mark them. most of the dataset only contains images that are from non-aerial view, which is hard to label the suitable bounding boxes since they are easily out of shape. google map api is a good tool to download city images from aerial views.\nthe first part of our map annotation model is a map reader that utilizes the google maps static api. we have implemented a reader that would download images of specific locations given the latitude and longitude or download continuous grids of images given a start point and how many images are wanted. after downloading, the images are passed through a classifier to determine if this is an image of a city with roads we want to annotate. the classifier is a convolutional neural network with 13 convolution layers following the structure of the residual block in resnet (he et al. 2015). we trained the classifier on randomly downloaded images of wildlands and cities from google map.\nthe basic yolo model (you only look once) applies a single forward pass neural network to the whole image. it can predict the bounding boxes and their class probabilities. it has 53 convolutional layers called darknet-53. for distinguishing the lane from other stripe patterns, we can directly compare the experimental results with the actual results. for the bounding box, we can compare the angle and size of the bounding box with the actual bounding box we want. when implementing the loss function for our yolov3 model. firstly, we are using the loss function as only considering the confidence level of the arrays of predicted boxes with the true labels and computing the maximum iou(intersection over union). it raises the issue that the spatial information of bounding boxes is difficult to maintain and therefore poor in accuracy. then, we add many factors such as center loss, weight and height loss, and class loss to keep the ious accurately calculated by using the different specific loss methods. furthermore, we finally move on to add degree as another factor and perform the atan2 method combining with a scaled binary cross-entropy.\nour base target is first to try to distinguish the crosswalk from other stripe patterns and then can use the basic horizontal bounding box to fit the crosswalk no matter the rotating or scaling of the lane. the stretch goal is to perfectly fit the crosswalk.\nproject deadline check\nproject check in #1\nlink: https://docs.google.com/document/d/1afkgratau0ittasx7bc7jvryglefwpoquc5wh2w6wiq/edit?usp=sharing\nproject check in #2\nlink: https://docs.google.com/document/d/1xw_pebikooy_b2nearwg_pl_ctvqnyzehikyotlfjxy/edit?usp=sharing\nfinal project submission\noral presentation link: https://youtu.be/e-fsy8sqy5g\nposter link: https://drive.google.com/file/d/1bbsigykx2a9vbydf5hcmskn0g6oeogdj/view?usp=sharing\nfinal report link: https://docs.google.com/document/d/1l4z30lt3pbgpery1iamxwii1fzxki15cqhmpbg8hyfu/edit?usp=sharing", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 4, "media": null, "medialink": null, "identifyer": 59503802}, {"Unnamed: 0": 3814, "autor": "Quantitative Stock Forecast with Weather Data", "date": null, "content": "Who:\nAndrew Kent (akent7) Ilija Nikolov (inikolov) Jikai Zhang (jzhang72)\nInspiration\nThe stock market is a complex (and even stochastic) system. Yet, there are certain world events that have a major impact on the process. In the upcoming years, we will be faced with the consequences of the climate crises, such as more intense droughts, hurricanes, tornadoes and other weather storms. For these reasons, it might be beneficial to see to what extent the weather data in related to the stock market in order to get a sense of how climate change might impact our financial system in the future.\nWhat it does\nPredict stock prices using a CNN-LSTM, and also using weather data to refine the predictions: We are interested in helping financial institutions produce better forecasts. Certain major weather events certainly have an impact on the stock market, and in a way we are set forth to investigate this potential relationship.\nRelated Work\nThere is a lot of work in this subject, all major quant firms use neural networks to forecast stock prices and some use satellite images to refine those models.\nWe are primarily inspired by a paper titled \u201cA CNN-LSTM-Based Model to Forecast Stock Prices\u201d which uses a CNN to first get features of the data relating to the daily stock price like high, low, volume, etc., and then passes that to a LSTM which is then able to predict the future price very well.\nThere is also some work in using weather forecasting using neural networks to then infer and predict changes in relevant stock prices. Another paper we will be working out of for this part of the project, titled \"Transfer Learning Application for Berries Yield Forecasting using Deep Learning\", used temperature and soil reflectivity from satellite images. Yet another paper uses temperature weather data to directly predict the price of the stock and then prove the initial prediction.\nData\nFor stock data we will use the Yahoo Finance Python library, which provides us with parameters like opening price, highest price, lowest price, closing price, volume, turnover, ups and downs, and change.\nFor the weather data, we plan to use the Climate Data Online (CDO) tool provided by the National Oceanic and Atmospheric Administration (NOAA) which give free access to NCDC''s archive of global historical weather and climate data. These dataframes include hourly pressure readings, dew point and wet bulb temperature data, as well as wind direction and precipitation hourly recordings.\nMethodology\nWe will first be using a CNN to extract features from our stock data which includes daily values of opening price, highest price, lowest price, closing price, volume, turnover, ups and downs, and change and then passing those extracted features to a LSTM. Some of the models we consider will have a sequential self attention layers.\nThe second part of our project will be using temperature and soil reflectance data to refine our prediction of stocks which are intimately related to those types of changes, to do this we will be looking over one region of the country - passing the weather data per day through a CNN & sequential self attention and then passing the result through a LSTM.\nMetrics\nSuccess is easy to quantify for this kind of project, we will aim to predict the stock price at least a week out for the initial stock predictor and success in the second part will mean that we are able to refine our predictions for stocks which may be affected on a regular basis by changes in weather. Lastly, convince ourselves that companies that are not related to the weather conditions in a region won\u2019t be refined.\nOur stretch goals would be to be able to accurately predict the price of stocks more than a few weeks into the future.\nAccuracy for us will be to minimize the difference between our predictions and the real stock price.\nLastly, convince ourselves that for companies that are not related to the weather conditions in a region, their stock prices won\u2019t be refined.\nEthical Considerations\nWhy is Deep Learning a good approach to this problem?\nThis project focuses primarily on data that does not involve people directly. The privacy concerns that other projects usually have are not present in this project. For that reason, DL is a good approach because even if we do not completely understand how the network actually works, the stakes are not as high as when people are directly involved. Nevertheless, we must be cautious about the impact this algorithm might have on companies whose stock prices we are predicting. Even though we want to help financial institutions to be better informed about the stock market decisions, we also need to take into consideration the \u201cfate\u201d of the companies that we focus on and make sure that our DL model does not hurt specific types of firms. For instance, if there are major storm events, it is expected that agricultural firms\u2019 produce is impacted, and likewise the DL model might show that. However, we need to ensure that these firms are not targeted whenever there are not major storm events.\nWhat is your dataset? Are there any concerns about how it was collected, or labeled? Is it representative? What kind of underlying historical or societal biases might it contain?\nIn our project the data is collected from the publically available Yahoo Finance (yfinance Library). In essence, this dataset is not considered biased because it is simply recording the stock market at certain points in time. What is more, the accessibility of the stock market to the general public would in principle guarantee that the data is available to everyone, so there shouldn\u2019t be any ethical considerations about the collection of the data. However, there might be certain biases regarding what data is publicly available, that is, how refined the data is. In other words, if everyone is guaranteed free access to the market, it is rather strange that to get more detailed data, one must pay. This is certainly biased toward people with greater financial capability and is in disagreement with the idea of information availability.\nDivision of labor\nAndrew Kent: CNN-LSTM Model for Stock Prices\nIlija Nikolov: Include weather data in the model (preprocess weather data and incorporate it)\nJikai Zhang: Fine-tune the CNN-LSTM model to decrease the possibility of overfitting\nWe will be working together on all these parts to make sure that everyone understands each part of the model.\nCS 2470 Final Project Checkin #2\nIntroduction:\nThe stock market is a complex (and even stochastic) system. Yet, there are certainly certain world events that have a major impact on the process. In the upcoming years, we will be faced with the consequences of the climate crises, such as more intense droughts, hurricanes, tornadoes and other weather storms. For these reasons, it might be beneficial to see to what extent the weather data is related to the stock market in order to get a sense of how climate change might impact our financial system in the future.\nChallenges: What has been the hardest part of the project you\u2019ve encountered so far?\nDeveloping a model to do exactly what we have envisioned is a big challenge because it is a specific task that not many people have thought about before. This includes trying out different types of neural networks, such as convolutional neural networks, different types of recurrent neural networks, LSTMs as well as the famous transformers. Creating models that put these layers in the right order is a painstaking task and it requires a lot of thinking but also trying out because it is almost impossible to predict how the neural network will actually behave and which model is the best without testing it. One of the most difficult parts of this project is obtaining relevant data regarding a specific company\u2019s stock prices and weather information. In order for the model to be able to have good training and predictions, the data needs to be high frequency and high fidelity. This in turn means that we would have a huge amount of data that requires bigger processing power for the model. Nevertheless, the model might be overfitting if we provide it with a lot of data. Thus, one must find a good balance between the high data volume, processing power and overfitting. This is a process that requires hyperparameter fine-tuning, including the learning rate, hidden dimension size as well as the number of hidden layers and other layer types.\nInsights: Are there any concrete results you can show at this point?\nYes, we\u2019ve been able to predict both future dewpoint temperature very well using many different pieces of weather data as well as being able to predict future stock prices using just the closing price. .\nPlan: Are you on track with your project?\nWhat do you need to dedicate more time to? We are able to predict future stock prices based on past stock data, as well as predicting dew point temperature data of a region from various climate parameters. We plan to further predict the yields of specific crops and also the stock price of agriculture companies in the region. Then we will train another model that takes in the price predictions from the two models and ensemble for a better prediction. What are you thinking of changing, if anything? Right now we\u2019re working on making the code more easily digestible so that when we add more things on we have a very solid foundation on which to build. We are working on letting the model use ground truth information for a certain window size and then predicting one value after that, and also letting the model predict future values based off of its own predictions, right now the code is being changed to better accommodate switching between both modes.\nWhat's next for Quantitative CNN-LSTM Stock Forecast with Weather Data\nImplement different ideas to refine the prediction.\nFinal Write-up Google docs link\nhttps://docs.google.com/document/d/1_DC--WIpb3J4CAQBGx2meOpaJkm-t7ACGZCseZ6Wc-Y/edit?usp=sharing", "link": "https://devpost.com/software/quantitative-cnn-lstm-stock-forecast-with-weather-data", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "who:\nandrew kent (akent7) ilija nikolov (inikolov) jikai zhang (jzhang72)\ninspiration\nthe stock market is a complex (and even stochastic) system. yet, there are certain world events that have a major impact on the process. in the upcoming years, we will be faced with the consequences of the climate crises, such as more intense droughts, hurricanes, tornadoes and other weather storms. for these reasons, it might be beneficial to see to what extent the weather data in related to the stock market in order to get a sense of how climate change might impact our financial system in the future.\nwhat it does\npredict stock prices using a cnn-lstm, and also using weather data to refine the predictions: we are interested in helping financial institutions produce better forecasts. certain major weather events certainly have an impact on the stock market, and in a way we are set forth to investigate this potential relationship.\nrelated work\nthere is a lot of work in this subject, all major quant firms use neural networks to forecast stock prices and some use satellite images to refine those models.\nwe are primarily inspired by a paper titled \u201ca cnn-lstm-based model to forecast stock prices\u201d which uses a cnn to first get features of the data relating to the daily stock price like high, low, volume, etc., and then passes that to a lstm which is then able to predict the future price very well.\nthere is also some work in using weather forecasting using neural networks to then infer and predict changes in relevant stock prices. another paper we will be working out of for this part of the project, titled \"transfer learning application for berries yield forecasting using deep learning\", used temperature and soil reflectivity from satellite images. yet another paper uses temperature weather data to directly predict the price of the stock and then prove the initial prediction.\ndata\nfor stock data we will use the yahoo finance python library, which provides us with parameters like opening price, highest price, lowest price, closing price, volume, turnover, ups and downs, and change.\nfor the weather data, we plan to use the climate data online (cdo) -----> tool !!!  provided by the national oceanic and atmospheric administration (noaa) which give free access to ncdc''s archive of global historical weather and climate data. these dataframes include hourly pressure readings, dew point and wet bulb temperature data, as well as wind direction and precipitation hourly recordings.\nmethodology\nwe will first be using a cnn to extract features from our stock data which includes daily values of opening price, highest price, lowest price, closing price, volume, turnover, ups and downs, and change and then passing those extracted features to a lstm. some of the models we consider will have a sequential self attention layers.\nthe second part of our project will be using temperature and soil reflectance data to refine our prediction of stocks which are intimately related to those types of changes, to do this we will be looking over one region of the country - passing the weather data per day through a cnn & sequential self attention and then passing the result through a lstm.\nmetrics\nsuccess is easy to quantify for this kind of project, we will aim to predict the stock price at least a week out for the initial stock predictor and success in the second part will mean that we are able to refine our predictions for stocks which may be affected on a regular basis by changes in weather. lastly, convince ourselves that companies that are not related to the weather conditions in a region won\u2019t be refined.\nour stretch goals would be to be able to accurately predict the price of stocks more than a few weeks into the future.\naccuracy for us will be to minimize the difference between our predictions and the real stock price.\nlastly, convince ourselves that for companies that are not related to the weather conditions in a region, their stock prices won\u2019t be refined.\nethical considerations\nwhy is deep learning a good approach to this problem?\nthis project focuses primarily on data that does not involve people directly. the privacy concerns that other projects usually have are not present in this project. for that reason, dl is a good approach because even if we do not completely understand how the network actually works, the stakes are not as high as when people are directly involved. nevertheless, we must be cautious about the impact this algorithm might have on companies whose stock prices we are predicting. even though we want to help financial institutions to be better informed about the stock market decisions, we also need to take into consideration the \u201cfate\u201d of the companies that we focus on and make sure that our dl model does not hurt specific types of firms. for instance, if there are major storm events, it is expected that agricultural firms\u2019 produce is impacted, and likewise the dl model might show that. however, we need to ensure that these firms are not targeted whenever there are not major storm events.\nwhat is your dataset? are there any concerns about how it was collected, or labeled? is it representative? what kind of underlying historical or societal biases might it contain?\nin our project the data is collected from the publically available yahoo finance (yfinance library). in essence, this dataset is not considered biased because it is simply recording the stock market at certain points in time. what is more, the accessibility of the stock market to the general public would in principle guarantee that the data is available to everyone, so there shouldn\u2019t be any ethical considerations about the collection of the data. however, there might be certain biases regarding what data is publicly available, that is, how refined the data is. in other words, if everyone is guaranteed free access to the market, it is rather strange that to get more detailed data, one must pay. this is certainly biased toward people with greater financial capability and is in disagreement with the idea of information availability.\ndivision of labor\nandrew kent: cnn-lstm model for stock prices\nilija nikolov: include weather data in the model (preprocess weather data and incorporate it)\njikai zhang: fine-tune the cnn-lstm model to decrease the possibility of overfitting\nwe will be working together on all these parts to make sure that everyone understands each part of the model.\ncs 2470 final project checkin #2\nintroduction:\nthe stock market is a complex (and even stochastic) system. yet, there are certainly certain world events that have a major impact on the process. in the upcoming years, we will be faced with the consequences of the climate crises, such as more intense droughts, hurricanes, tornadoes and other weather storms. for these reasons, it might be beneficial to see to what extent the weather data is related to the stock market in order to get a sense of how climate change might impact our financial system in the future.\nchallenges: what has been the hardest part of the project you\u2019ve encountered so far?\ndeveloping a model to do exactly what we have envisioned is a big challenge because it is a specific task that not many people have thought about before. this includes trying out different types of neural networks, such as convolutional neural networks, different types of recurrent neural networks, lstms as well as the famous transformers. creating models that put these layers in the right order is a painstaking task and it requires a lot of thinking but also trying out because it is almost impossible to predict how the neural network will actually behave and which model is the best without testing it. one of the most difficult parts of this project is obtaining relevant data regarding a specific company\u2019s stock prices and weather information. in order for the model to be able to have good training and predictions, the data needs to be high frequency and high fidelity. this in turn means that we would have a huge amount of data that requires bigger processing power for the model. nevertheless, the model might be overfitting if we provide it with a lot of data. thus, one must find a good balance between the high data volume, processing power and overfitting. this is a process that requires hyperparameter fine-tuning, including the learning rate, hidden dimension size as well as the number of hidden layers and other layer types.\ninsights: are there any concrete results you can show at this point?\nyes, we\u2019ve been able to predict both future dewpoint temperature very well using many different pieces of weather data as well as being able to predict future stock prices using just the closing price. .\nplan: are you on track with your project?\nwhat do you need to dedicate more time to? we are able to predict future stock prices based on past stock data, as well as predicting dew point temperature data of a region from various climate parameters. we plan to further predict the yields of specific crops and also the stock price of agriculture companies in the region. then we will train another model that takes in the price predictions from the two models and ensemble for a better prediction. what are you thinking of changing, if anything? right now we\u2019re working on making the code more easily digestible so that when we add more things on we have a very solid foundation on which to build. we are working on letting the model use ground truth information for a certain window size and then predicting one value after that, and also letting the model predict future values based off of its own predictions, right now the code is being changed to better accommodate switching between both modes.\nwhat's next for quantitative cnn-lstm stock forecast with weather data\nimplement different ideas to refine the prediction.\nfinal write-up google docs link\nhttps://docs.google.com/document/d/1_dc--wipb3j4caqbgx2meopajkm-t7acgzcsez6wc-y/edit?usp=sharing", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 11, "media": null, "medialink": null, "identifyer": 59503814}, {"Unnamed: 0": 3821, "autor": "NeuroDet", "date": null, "content": "Title\nNeuroDet: Deep learning models for brain tumor classification\nTeam members\nHyeyeon Hwang (hhwang16)\nAlexandra Wong (amw11)\nVictor Youdom Kemmoe (vyoudomk)\nIntroduction & Inspiration\nWhat problem are we trying to solve and why? If you are doing something new, detail how you arrived at this topic and what motivated you.\nWe are interested in applying deep learning models to answer biological questions. Looking further into the types of biological questions we could focus on, we decided to focus on neuroscience and in particular, detecting types of brain tumors. Typically, when an abnormality is seen on MRI or CT scan, the only way to confirm what type of tumor is present is to perform a brain biopsy, a very invasive diagnostic test. Different brain tumor subtypes have incredibly different levels of aggressiveness and treatment options. If we could create a method to diagnose patients via their imaging, we could save patients the pain and risk of complications that are associated with obtaining a brain biopsy.\nWhat kind of problem is this? Classification? Regression? Structured prediction? Reinforcement Learning? Unsupervised Learning? Etc.\nOur project is a classification task. We are classifying MRI images on whether there is a tumor present, and if so, if it\u2019s a malignant versus benign subtype. We are also interested in classifying the tumors into 4 classes (no tumor, benign tumor, malignant tumor, and pituitary tumor).\nRelated Work\nRelated papers\nMuch of the prior work on brain tumor classification utilizes models based on deep convolutional neural networks. For example, \u201cDifferential Deep Convolutional Neural Network Model for Brain Tumor Classification\u201d by Abd El Kader I et al. (https://pubmed.ncbi.nlm.nih.gov/33801994/) introduces a differential deep convolutional neural network (deep-CNN) to train and test 25,000 brain magnetic resonance imaging (MRI) images and achieves an accuracy of 99.25%. \u201cBrain Tumor Detection and Classification on MR Images by a Deep Wavelet Auto-Encoder Model\u201d by Isselmou Abd El Kader et al. (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8471235/) implements a deep wavelet autoencoder for tumor detection and classification with high accuracy. \u201cCNN-RNN: A Unified Framework for Multi-label Image Classification\u201d by Wang et al. (https://arxiv.org/pdf/1604.04573.pdf) introduces a combined CNN-RNN model that outperforms state-of-the-art multi-label classification models.\nLiving implementations\nhttps://github.com/aksh-ai/neuralBlack\nhttps://github.com/Nupurgopali/Brain-tumor-classification-using-CNN\nhttps://github.com/AryanFelix/Brain-Tumor-Classification\nhttps://github.com/MohamedAliHabib/Brain-Tumor-Detection\nData\nAccessing data\nThe brain MRI image dataset we plan to use is located here: https://www.kaggle.com/sartajbhuvaji/brain-tumor-classification-mri?select=Testing\nHow big is it? Will you need to do significant preprocessing?\nThere are 3264 MRI images total, with the following classes that have been already split into training and testing folders. As the data is already split into training and testing sets, we will not have to do significant preprocessing.\nGlioma (malignant tumor): 826 Train Files, 100 Test Files\nMeningioma (benign tumor): 822 Train Files, 115 Test Files\nNo Tumor (healthy control): 395 Train Files, 105 Test Files\nPituitary Tumor (mostly benign tumor): 827 Train Files, 74 Test Files\nMethodology\nOur goal is to detect whether a tumor is present and if a tumor is benign or malignant. This will involve the binary classification tasks (1) no tumor present or tumor present and (2) benign tumor or malignant tumor. We also want to classify brain tumor MRI images into four classes (no tumor, benign tumor, malignant tumor, and pituitary tumor) and do a multi-label classification of pituitary tumors, which may be benign or malignant. For our tasks, we plan to implement CNNs, RNNs, CNN-RNNs, and VAEs.\nAs CNNs are widely used for image classification and previous work on brain tumor classification have used the same architecture successfully, we decided to build a CNN to use as our baseline model.\nAlthough most pituitary tumors are benign, they may also be malignant. So we are also interested in looking at whether a tumor is a pituitary tumor and benign or malignant. As previous work on multi-label image classification tasks have used RNNs and mixed CNN-RNNs successfully, we plan to implement RNNs and CNN-RNNs for single-label and multi-label classification of the brain tumors.\nAs studies have shown that VAEs achieve good performance on image classification, we may also implement and assess the performance of a VAE model for binary and multi-class classification of the different types of tumors.\nMetrics\nWe will assess model performance using accuracy, area under receiver operating characteristic (AUC) curve, and area under precision recall (AUC-PR) curve. Note that for a binary classification of benign and malignant tumors, a false positive prediction can lead to unnecessary treatment, but a false negative prediction is far more serious since a malignant tumor that requires urgent treatment has been unnoticed. Therefore, minimizing false negatives is more important.\nOur base goal is to implement the baseline CNN and RNN model.\nOur target goal is to implement the baseline CNN, RNN, and CNN-RNN model.\nOur stretch goal is to implement the baseline CNN, RNN, CNN-RNN, and VAE model.\nEthics\nWhat is your dataset? Are there any concerns about how it was collected, or labeled? Is it representative? What kind of underlying historical or societal biases might it contain?\nThe dataset is a collection of MRI images that have been deidentified to protect patients\u2019 privacy. Although it is now difficult to tell how biased the dataset is due to the lack of information about who the patients were, there are significant factors that could bias the collection of images obtained. For instance, were the sexes represented evenly in the dataset? Were these patients of different ancestries represented well? Were these patients all from the same geographical regions, and what types of environmental factors were they exposed to? Was the radiologist actually correct when reading the images, given that radiologists are humans too and can make errors? MRI images also come in many slices, so which particular axis and slice an image came from could bias the model to only be able to classify a tumor if the slice came from the same area as the training set. Each of these factors could skew classification in a particular subpopulation that is not well generalizable to the rest of the public, which could make it a less useful general diagnostic test.\nWho are the major \u201cstakeholders\u201d in this problem, and what are the consequences of mistakes made by your algorithm?\nThere are a variety of stakeholders in anything deep learning applied to the medical field. Obviously, the most important stakeholder is the patient whose diagnosis could depend on such an algorithmic tool. This matters hugely because a wrong diagnosis could mean getting the wrong treatment, which has consequences for their survival, health, quality of life, and financial stability. Additionally, there are consequences for the usage of healthcare resources if unnecessary treatment is given, which matters for both providers (like physicians themselves) and payers (like health insurance companies) who need to distribute said resources to other patients in need. Misdiagnosing certain groups of people because of biases in the dataset could also lead to further stratification of health inequities.\nDivision of labor\nWe all equally contributed to the coding, write-ups, poster, and oral presentation of this project.\nFinal Write-up\nPlease find our final write-up at this link\nOr if Google Drive is your preferred location: https://drive.google.com/file/d/16tdn1w8CPKrI3p5Wzky9_j1GvSmzTF5Z/view?usp=sharing\nFinal Presentation Slides (must be a Brown University User to view)\nhttps://docs.google.com/presentation/d/1aGnEJ0mCa4BpfIS2HKOCjQkjNjrEhgmXVF9Mmq2RGI4/edit?usp=sharing", "link": "https://devpost.com/software/neurodet", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "title\nneurodet: deep learning models for brain tumor classification\nteam members\nhyeyeon hwang (hhwang16)\nalexandra wong (amw11)\nvictor youdom kemmoe (vyoudomk)\nintroduction & inspiration\nwhat problem are we trying to solve and why? if you are doing something new, detail how you arrived at this topic and what motivated you.\nwe are interested in applying deep learning models to answer biological questions. looking further into the types of biological questions we could focus on, we decided to focus on neuroscience and in particular, detecting types of brain tumors. typically, when an abnormality is seen on mri or ct scan, the only way to confirm what type of tumor is present is to perform a brain biopsy, a very invasive diagnostic test. different brain tumor subtypes have incredibly different levels of aggressiveness and treatment options. if we could create a method to diagnose patients via their imaging, we could save patients the pain and risk of complications that are associated with obtaining a brain biopsy.\nwhat kind of problem is this? classification? regression? structured prediction? reinforcement learning? unsupervised learning? etc.\nour project is a classification task. we are classifying mri images on whether there is a tumor present, and if so, if it\u2019s a malignant versus benign subtype. we are also interested in classifying the tumors into 4 classes (no tumor, benign tumor, malignant tumor, and pituitary tumor).\nrelated work\nrelated papers\nmuch of the prior work on brain tumor classification utilizes models based on deep convolutional neural networks. for example, \u201cdifferential deep convolutional neural network model for brain tumor classification\u201d by abd el kader i et al. (https://pubmed.ncbi.nlm.nih.gov/33801994/) introduces a differential deep convolutional neural network (deep-cnn) to train and test 25,000 brain magnetic resonance imaging (mri) images and achieves an accuracy of 99.25%. \u201cbrain tumor detection and classification on mr images by a deep wavelet auto-encoder model\u201d by isselmou abd el kader et al. (https://www.ncbi.nlm.nih.gov/pmc/articles/pmc8471235/) implements a deep wavelet autoencoder for tumor detection and classification with high accuracy. \u201ccnn-rnn: a unified framework for multi-label image classification\u201d by wang et al. (https://arxiv.org/pdf/1604.04573.pdf) introduces a combined cnn-rnn model that outperforms state-of-the-art multi-label classification models.\nliving implementations\nhttps://github.com/aksh-ai/neuralblack\nhttps://github.com/nupurgopali/brain-tumor-classification-using-cnn\nhttps://github.com/aryanfelix/brain-tumor-classification\nhttps://github.com/mohamedalihabib/brain-tumor-detection\ndata\naccessing data\nthe brain mri image dataset we plan to use is located here: https://www.kaggle.com/sartajbhuvaji/brain-tumor-classification-mri?select=testing\nhow big is it? will you need to do significant preprocessing?\nthere are 3264 mri images total, with the following classes that have been already split into training and testing folders. as the data is already split into training and testing sets, we will not have to do significant preprocessing.\nglioma (malignant tumor): 826 train files, 100 test files\nmeningioma (benign tumor): 822 train files, 115 test files\nno tumor (healthy control): 395 train files, 105 test files\npituitary tumor (mostly benign tumor): 827 train files, 74 test files\nmethodology\nour goal is to detect whether a tumor is present and if a tumor is benign or malignant. this will involve the binary classification tasks (1) no tumor present or tumor present and (2) benign tumor or malignant tumor. we also want to classify brain tumor mri images into four classes (no tumor, benign tumor, malignant tumor, and pituitary tumor) and do a multi-label classification of pituitary tumors, which may be benign or malignant. for our tasks, we plan to implement cnns, rnns, cnn-rnns, and vaes.\nas cnns are widely used for image classification and previous work on brain tumor classification have used the same architecture successfully, we decided to build a cnn to use as our baseline model.\nalthough most pituitary tumors are benign, they may also be malignant. so we are also interested in looking at whether a tumor is a pituitary tumor and benign or malignant. as previous work on multi-label image classification tasks have used rnns and mixed cnn-rnns successfully, we plan to implement rnns and cnn-rnns for single-label and multi-label classification of the brain tumors.\nas studies have shown that vaes achieve good performance on image classification, we may also implement and assess the performance of a vae model for binary and multi-class classification of the different types of tumors.\nmetrics\nwe will assess model performance using accuracy, area under receiver operating characteristic (auc) curve, and area under precision recall (auc-pr) curve. note that for a binary classification of benign and malignant tumors, a false positive prediction can lead to unnecessary treatment, but a false negative prediction is far more serious since a malignant tumor that requires urgent treatment has been unnoticed. therefore, minimizing false negatives is more important.\nour base goal is to implement the baseline cnn and rnn model.\nour target goal is to implement the baseline cnn, rnn, and cnn-rnn model.\nour stretch goal is to implement the baseline cnn, rnn, cnn-rnn, and vae model.\nethics\nwhat is your dataset? are there any concerns about how it was collected, or labeled? is it representative? what kind of underlying historical or societal biases might it contain?\nthe dataset is a collection of mri images that have been deidentified to protect patients\u2019 privacy. although it is now difficult to tell how biased the dataset is due to the lack of information about who the patients were, there are significant factors that could bias the collection of images obtained. for instance, were the sexes represented evenly in the dataset? were these patients of different ancestries represented well? were these patients all from the same geographical regions, and what types of environmental factors were they exposed to? was the radiologist actually correct when reading the images, given that radiologists are humans too and can make errors? mri images also come in many slices, so which particular axis and slice an image came from could bias the model to only be able to classify a tumor if the slice came from the same area as the training set. each of these factors could skew classification in a particular subpopulation that is not well generalizable to the rest of the public, which could make it a less useful general diagnostic test.\nwho are the major \u201cstakeholders\u201d in this problem, and what are the consequences of mistakes made by your algorithm?\nthere are a variety of stakeholders in anything deep learning applied to the medical field. obviously, the most important stakeholder is the patient whose diagnosis could depend on such an algorithmic -----> tool !!! . this matters hugely because a wrong diagnosis could mean getting the wrong treatment, which has consequences for their survival, health, quality of life, and financial stability. additionally, there are consequences for the usage of healthcare resources if unnecessary treatment is given, which matters for both providers (like physicians themselves) and payers (like health insurance companies) who need to distribute said resources to other patients in need. misdiagnosing certain groups of people because of biases in the dataset could also lead to further stratification of health inequities.\ndivision of labor\nwe all equally contributed to the coding, write-ups, poster, and oral presentation of this project.\nfinal write-up\nplease find our final write-up at this link\nor if google drive is your preferred location: https://drive.google.com/file/d/16tdn1w8cpkri3p5wzky9_j1gvsmztf5z/view?usp=sharing\nfinal presentation slides (must be a brown university user to view)\nhttps://docs.google.com/presentation/d/1agnej0mca4bpfis2hkocjqkjnjrehgmxvf9mmq2rgi4/edit?usp=sharing", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 21, "media": null, "medialink": null, "identifyer": 59503821}, {"Unnamed: 0": 3834, "autor": "Transitive Checkmates", "date": null, "content": "Inspiration\nPlaying chess can be demoralizing, so we wanted to give chess players and ego boost by showing that they have technically beaten some of the world's top players!\nWhat it does\nTransitive Checkmates is a desktop application that allows you to search for the top players you have beaten by the transitive property. It shows you the highest rated player you have beaten, and what the path was from you to them. It also includes a command line parsing tool to create your own data from PGN files.\nHow we built it\nWe built Transitive Checkmates using C++ and QT Creator for the visual interface. We implemented several graph search and shortest path algorithms to find our results.\nChallenges we ran into\nWe had some issues deploying the application through QT creator, so the program must be run through QT creator.\nAccomplishments that we're proud of\nWe are proud that we created a useful interface that clearly presents the data, and that the graph algorithms efficiently process the data.\nWhat we learned\nWe learned that we should plan the interface earlier, and the front end is just as important as the back end.\nWhat's next for Transitive Checkmates:\nWe want to improve our visual interface, so it can be more easily be run and distributed, and provide the user more options for displaying data.", "link": "https://devpost.com/software/transitive-checkmates", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nplaying chess can be demoralizing, so we wanted to give chess players and ego boost by showing that they have technically beaten some of the world's top players!\nwhat it does\ntransitive checkmates is a desktop application that allows you to search for the top players you have beaten by the transitive property. it shows you the highest rated player you have beaten, and what the path was from you to them. it also includes a command line parsing -----> tool !!!  to create your own data from pgn files.\nhow we built it\nwe built transitive checkmates using c++ and qt creator for the visual interface. we implemented several graph search and shortest path algorithms to find our results.\nchallenges we ran into\nwe had some issues deploying the application through qt creator, so the program must be run through qt creator.\naccomplishments that we're proud of\nwe are proud that we created a useful interface that clearly presents the data, and that the graph algorithms efficiently process the data.\nwhat we learned\nwe learned that we should plan the interface earlier, and the front end is just as important as the back end.\nwhat's next for transitive checkmates:\nwe want to improve our visual interface, so it can be more easily be run and distributed, and provide the user more options for displaying data.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59503834}, {"Unnamed: 0": 3851, "autor": "Saga", "date": null, "content": "How to explore Saga\nYou can of course create your own account and invite your friends to join courses that you make (and try out our GMail integration) , however if you would like to experience more of a demo with pre-filled data, try one of these logins:\nStudent Login:\nEmail: etnguyen84@gmail.com\nPassword: student123\nInstructor Login\nEmail: trentyetzer0@gmail.com\nPass: teacherpass\nCheck it out here: https://saga-learn.com\nInspiration\nWe are bringing discovery and excitement into education with Saga. It's not about making a platform where students can succeed, It's about making a platform that makes students want to succeed. Our goal with Saga is to excite students about their learning and reward them for their hard work through a gamified online learning platform.\nWhat is Gamification? The Miriam-Webster dictionary defines gamification as \u2018the process of adding games or gamelike elements to something (such as a task) as to encourage participation\u2019. One of the things that motivates us to continue playing a game, no matter how hard the challenge, is the knowledge that success is attainable, and that there is a reward for that attainable success. Some people will fight a boss in Dark Souls for hours, over and over because they are excited to get past this challenge and into the next area. Some people will do the tedious work of getting every single achievement in a game because they want to see that 100% trophy on their profile. In her book, Reality is Broken, Jane McGonigal discusses a game called Chore Wars that rewards players with experience points and virtual currency for completing chores around the house, and that virtual currency could then be converted into customized real-world rewards such as \u201callowances\u201d, \u201ccoffee runs for roommates\u201d, or \u201cto bid on what music to play in the car\u201d. According to McGonigal, this game worked great in families and households of all different ages. This is a great example of previously tedious and boring tasks becoming motivating and fun. This idea of rewarding students for the tasks they complete is what we believe to be the first step in improving education, and what we are trying to achieve with Saga.\nGamification of education is a commonly discussed idea, but as students who have gone through over 15 years of schooling, we felt as though schools could really do better in this area. We have seen educational games, but we believe that the concepts of games and gameplay should be at the core of education. Unfortunately we can\u2019t immediately convert the entire education system to be as exciting, encouraging, and motivating as a game, but we hope that Saga can help educators take the first step into a new and better way of teaching.\nWhat it does\nSaga is an online learning management system that serves as a platform for teaching and learning between students and teachers. Saga creates a new and engaging way for students to interact with their assignments by showing their progress in the course on an \u2018adventure map\u2019. Each assignment is a milestone they complete, and instead of grades, students receive stars which act as Saga\u2019s currency. These stars accumulate and can be used to purchase items from our virtual shop, such as avatars. The stars are also added to the growing pile of stars for the entire class of students that can translate to real-world prizes set by the teacher, such as a pizza party, or a movie viewing at the end of the course.\nWIth our goal of gamifying and reforming online learning platforms in mind, we felt it was very important to assist our users in their transition into this alternative way of teaching. Because of this, we have implemented several features that may be more familiar to our users who have experience with Canvas or Blackboard. This includes a calendar view of assignments (Quest Board), messaging and announcements (Letters), and traditional text submissions for assignments (Quests).\nHow we built it\nSaga was made by Alex Leska, Ethan Nguyen, Marguerite Brugger, and Trent Yetzer for the Liquid Hacks 2.0 Hackathon.\nFront End\nLanguages + Libraries: Typescript, React-Typescript, Ant Design\nInfrastructure: Netlify (Static Site Host)\nBack End\nLanguages + Libraries: Typescript, Node.js, AdonisJS, MySQL\nInfrastructure: Heroku (PaaS), PlanetScale (Serverless SQL), Google Cloud (GMail API), AWS (S3 File Storage)\nWe split into a mostly-front-end team and a mostly-back-end team, however both teams collaborated constantly and were in constant communication to make sure that when it came time to combine the two parts of our project, we were on the same page.\nChallenges we ran into\nAll of our forms and inputs used throughout Saga have a lot more underneath the hood than one might expect. Some require auto filling values from the database, some require updating, deleting, and adding information to the database at the same time, some forms are dynamic and add fields as the user chooses, and some have all of the above. Specifically, when editing an Adventure (a course), you can dynamically change, add, and delete units which also requires its own tracking of whether the user created a new unit or updated an existing one.\nDespite some graphics knowledge, albeit extremely limited, modifying the Vanta.js \u2018birds\u2019 code to make the interactive books on our landing page turned out to be quite the challenge: our graphics knowledge is quite limited, we weren\u2019t very familiar with Three.js, and the code used some deprecated libraries. Luckily after several hours and some basic knowledge of triangles, we were able to make a mesmerizing backdrop for the website, and every second of struggle turned out to be worth it.\nWe knew from the beginning that the placement of Quest buttons on the Map page would be tricky, as the maps have curved paths. At first we decided that the curved paths on the map would follow a sine wave as to be translatable to a well-known equation, however, button placement still proved to be a challenge. Since we were limiting the amount of Quests per Map (otherwise the maps will get overcrowded) we decided to hard-code locations for buttons based on how many Quests are on that map.\nConnecting our app to GMail was also a challenge. To start, OAuth is a very complicated authorization standard that none of us had used before, but google requires. Once we got past that hurdle, however, the javascript documentation for the GMail api was inconsistent and we found many docs that said different things. Now that we know how to use it, however, it will be quite a useful tool to use in future projects.\nAccomplishments that we're proud of\nThe mesmerizing interactive flying books that decorate our landing page, sign in page, and sign up pages.\nOur app has functional GMail integration and will send messages to invite users to use our platform with codes and unique links to join courses.\nThe \u2018Adventures\u2019 page presents the adventures (courses) through css-animated books that open when hovered.\nThroughout the site, we have original artwork for our avatars and quest maps made by our team.\nWe have incorporated complex dynamic forms that have functional field and database validation, autofill, and complex logic for when to update, delete, or add information to our database.\nWe used React-Quill to allow our users to format their text inputs with a customized rich text editor.\nWe have over 50 API endpoints to Create / Read / Update / Delete to/from our database.\nWhat we learned\nAdonisJS was a first for our team\nGMail OAuth, none of us have ever used OAuth.\nTypescript, some of us either had never used Typescript before or were not very comfortable with it.\nMySQL, our backend is more experienced with NoSQL databases.\nThree.js, a graphics library we were very unfamiliar with.\nBroadened our knowledge of React Hooks and Data Fetching\nWhat's next for Saga\nSaga is still in beta, so many features are still on the way! Let\u2019s take a look at some.\nNew avatars are coming to the shop! Clothes for your avatars are also coming soon! Get ready to dress up your cute characters with fashionable items like a top hat, some casual sneakers, or Team Liquid jacket! All of the new avatars and clothing items will be purchasable with stars from the shop.\nYou all are familiar with Saga\u2019s built-in ability to send and receive virtual letters. Soon we'll be sharing with you a new way to communicate with fellow adventurers: discussions! Discussions will increase collaboration and guidance by allowing adventurers to discuss quest-related topics.\nTo go along with Discussions, we will be adding stickers to the shop that can be added to Discussions and Letters!\nHere are a few more:\nInstructors can view the status of Adventurers\u2019 invites, to see whether they have created an account and joined Saga.\nSupport upload of multiple file types.\nInstructors can create multiple tiers of prizes with separate star goals.\nInstructors can upload custom maps.\nLast, but definitely not least, your character will soon follow you through your adventure by marking your progress on your adventure map!\nGlossary\nFor those of you who are new to Saga, you may not know your way around. Here is an alphabetized list of some of our saga-themed words in relation to words you may be more familiar with when it comes to learning management systems.\nAdventure - Class / Course\nAdventurer - Student\nChapter - Class Unit\nCharacter - Profile\nExpiration Date - Due Date\nLetter - Message\nMailbox - Message Inbox\nQuest - Assignment\nQuest Board - Calendar\nStars - Grades (or our equivalent, at least)", "link": "https://devpost.com/software/saga", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "how to explore saga\nyou can of course create your own account and invite your friends to join courses that you make (and try out our gmail integration) , however if you would like to experience more of a demo with pre-filled data, try one of these logins:\nstudent login:\nemail: etnguyen84@gmail.com\npassword: student123\ninstructor login\nemail: trentyetzer0@gmail.com\npass: teacherpass\ncheck it out here: https://saga-learn.com\ninspiration\nwe are bringing discovery and excitement into education with saga. it's not about making a platform where students can succeed, it's about making a platform that makes students want to succeed. our goal with saga is to excite students about their learning and reward them for their hard work through a gamified online learning platform.\nwhat is gamification? the miriam-webster dictionary defines gamification as \u2018the process of adding games or gamelike elements to something (such as a task) as to encourage participation\u2019. one of the things that motivates us to continue playing a game, no matter how hard the challenge, is the knowledge that success is attainable, and that there is a reward for that attainable success. some people will fight a boss in dark souls for hours, over and over because they are excited to get past this challenge and into the next area. some people will do the tedious work of getting every single achievement in a game because they want to see that 100% trophy on their profile. in her book, reality is broken, jane mcgonigal discusses a game called chore wars that rewards players with experience points and virtual currency for completing chores around the house, and that virtual currency could then be converted into customized real-world rewards such as \u201callowances\u201d, \u201ccoffee runs for roommates\u201d, or \u201cto bid on what music to play in the car\u201d. according to mcgonigal, this game worked great in families and households of all different ages. this is a great example of previously tedious and boring tasks becoming motivating and fun. this idea of rewarding students for the tasks they complete is what we believe to be the first step in improving education, and what we are trying to achieve with saga.\ngamification of education is a commonly discussed idea, but as students who have gone through over 15 years of schooling, we felt as though schools could really do better in this area. we have seen educational games, but we believe that the concepts of games and gameplay should be at the core of education. unfortunately we can\u2019t immediately convert the entire education system to be as exciting, encouraging, and motivating as a game, but we hope that saga can help educators take the first step into a new and better way of teaching.\nwhat it does\nsaga is an online learning management system that serves as a platform for teaching and learning between students and teachers. saga creates a new and engaging way for students to interact with their assignments by showing their progress in the course on an \u2018adventure map\u2019. each assignment is a milestone they complete, and instead of grades, students receive stars which act as saga\u2019s currency. these stars accumulate and can be used to purchase items from our virtual shop, such as avatars. the stars are also added to the growing pile of stars for the entire class of students that can translate to real-world prizes set by the teacher, such as a pizza party, or a movie viewing at the end of the course.\nwith our goal of gamifying and reforming online learning platforms in mind, we felt it was very important to assist our users in their transition into this alternative way of teaching. because of this, we have implemented several features that may be more familiar to our users who have experience with canvas or blackboard. this includes a calendar view of assignments (quest board), messaging and announcements (letters), and traditional text submissions for assignments (quests).\nhow we built it\nsaga was made by alex leska, ethan nguyen, marguerite brugger, and trent yetzer for the liquid hacks 2.0 hackathon.\nfront end\nlanguages + libraries: typescript, react-typescript, ant design\ninfrastructure: netlify (static site host)\nback end\nlanguages + libraries: typescript, node.js, adonisjs, mysql\ninfrastructure: heroku (paas), planetscale (serverless sql), google cloud (gmail api), aws (s3 file storage)\nwe split into a mostly-front-end team and a mostly-back-end team, however both teams collaborated constantly and were in constant communication to make sure that when it came time to combine the two parts of our project, we were on the same page.\nchallenges we ran into\nall of our forms and inputs used throughout saga have a lot more underneath the hood than one might expect. some require auto filling values from the database, some require updating, deleting, and adding information to the database at the same time, some forms are dynamic and add fields as the user chooses, and some have all of the above. specifically, when editing an adventure (a course), you can dynamically change, add, and delete units which also requires its own tracking of whether the user created a new unit or updated an existing one.\ndespite some graphics knowledge, albeit extremely limited, modifying the vanta.js \u2018birds\u2019 code to make the interactive books on our landing page turned out to be quite the challenge: our graphics knowledge is quite limited, we weren\u2019t very familiar with three.js, and the code used some deprecated libraries. luckily after several hours and some basic knowledge of triangles, we were able to make a mesmerizing backdrop for the website, and every second of struggle turned out to be worth it.\nwe knew from the beginning that the placement of quest buttons on the map page would be tricky, as the maps have curved paths. at first we decided that the curved paths on the map would follow a sine wave as to be translatable to a well-known equation, however, button placement still proved to be a challenge. since we were limiting the amount of quests per map (otherwise the maps will get overcrowded) we decided to hard-code locations for buttons based on how many quests are on that map.\nconnecting our app to gmail was also a challenge. to start, oauth is a very complicated authorization standard that none of us had used before, but google requires. once we got past that hurdle, however, the javascript documentation for the gmail api was inconsistent and we found many docs that said different things. now that we know how to use it, however, it will be quite a useful -----> tool !!!  to use in future projects.\naccomplishments that we're proud of\nthe mesmerizing interactive flying books that decorate our landing page, sign in page, and sign up pages.\nour app has functional gmail integration and will send messages to invite users to use our platform with codes and unique links to join courses.\nthe \u2018adventures\u2019 page presents the adventures (courses) through css-animated books that open when hovered.\nthroughout the site, we have original artwork for our avatars and quest maps made by our team.\nwe have incorporated complex dynamic forms that have functional field and database validation, autofill, and complex logic for when to update, delete, or add information to our database.\nwe used react-quill to allow our users to format their text inputs with a customized rich text editor.\nwe have over 50 api endpoints to create / read / update / delete to/from our database.\nwhat we learned\nadonisjs was a first for our team\ngmail oauth, none of us have ever used oauth.\ntypescript, some of us either had never used typescript before or were not very comfortable with it.\nmysql, our backend is more experienced with nosql databases.\nthree.js, a graphics library we were very unfamiliar with.\nbroadened our knowledge of react hooks and data fetching\nwhat's next for saga\nsaga is still in beta, so many features are still on the way! let\u2019s take a look at some.\nnew avatars are coming to the shop! clothes for your avatars are also coming soon! get ready to dress up your cute characters with fashionable items like a top hat, some casual sneakers, or team liquid jacket! all of the new avatars and clothing items will be purchasable with stars from the shop.\nyou all are familiar with saga\u2019s built-in ability to send and receive virtual letters. soon we'll be sharing with you a new way to communicate with fellow adventurers: discussions! discussions will increase collaboration and guidance by allowing adventurers to discuss quest-related topics.\nto go along with discussions, we will be adding stickers to the shop that can be added to discussions and letters!\nhere are a few more:\ninstructors can view the status of adventurers\u2019 invites, to see whether they have created an account and joined saga.\nsupport upload of multiple file types.\ninstructors can create multiple tiers of prizes with separate star goals.\ninstructors can upload custom maps.\nlast, but definitely not least, your character will soon follow you through your adventure by marking your progress on your adventure map!\nglossary\nfor those of you who are new to saga, you may not know your way around. here is an alphabetized list of some of our saga-themed words in relation to words you may be more familiar with when it comes to learning management systems.\nadventure - class / course\nadventurer - student\nchapter - class unit\ncharacter - profile\nexpiration date - due date\nletter - message\nmailbox - message inbox\nquest - assignment\nquest board - calendar\nstars - grades (or our equivalent, at least)", "sortedWord": "None", "removed": "Nan", "score": 5, "comments": 0, "media": null, "medialink": null, "identifyer": 59503851}, {"Unnamed: 0": 3862, "autor": "CHLK", "date": null, "content": "Inspiration\nOur group was made up of current students and recent grads, who all experienced online learning and lectures. One thing that we noticed is that is very hard to stay engaged in an online lecture. We wanted to make a tool for teachers to keep students engaged, and to make it easier for students to pay attention during lectures.\nWhat it does\nThe idea behind our project is that a teacher will create a presentation with no extra effort on their end, and when presented the students will automatically be given opportunities to engage with the lecture. Able to login to a teacher or student account, teachers can create and present slideshows, and students can join rooms to engage with a presentation.\nFor example: A teacher would add this bullet point when creating the presentation: *Variable: A way to store data And when presenting, the bullet point would be show to students as: ____: _A way to store data Allowing the students to fill in the blank on the presentation as the teacher lectures.\nWe had envisioned other ideas/concepts for engagement with the presentation but unfortunately we ran out of time.\nHow we built it\nWe used React for the frontend, with Django framework and MondoDB for the backend.\nChallenges we ran into\nOur group did not have any experience in the technologies we chose to use. We also chose to take a design approach of embedding the React app within Django, rather than using Django just as an API that interacts with our React app. Our lack of experience with React bottlenecked us, and forced us spend a lot of time on creating simple components.\nAccomplishments that we're proud of\nWe were able to implement token based authentication, allowing features such as only displaying the slideshows created by that user to the user. Despite our lack of experience with the technologies used in this project we were able to create a working concept.\nWhat we learned\nA lot! We learned a ton about setting up a project using Django framework and how Django works. We also got a crash course in JavaScript along with React concepts like state.\nWhat's next for CHLK\nWe hope to get it up and hosted somewhere. As well as implement more of the features we had planned once we gain a little more frontend experience.", "link": "https://devpost.com/software/chlk", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nour group was made up of current students and recent grads, who all experienced online learning and lectures. one thing that we noticed is that is very hard to stay engaged in an online lecture. we wanted to make a -----> tool !!!  for teachers to keep students engaged, and to make it easier for students to pay attention during lectures.\nwhat it does\nthe idea behind our project is that a teacher will create a presentation with no extra effort on their end, and when presented the students will automatically be given opportunities to engage with the lecture. able to login to a teacher or student account, teachers can create and present slideshows, and students can join rooms to engage with a presentation.\nfor example: a teacher would add this bullet point when creating the presentation: *variable: a way to store data and when presenting, the bullet point would be show to students as: ____: _a way to store data allowing the students to fill in the blank on the presentation as the teacher lectures.\nwe had envisioned other ideas/concepts for engagement with the presentation but unfortunately we ran out of time.\nhow we built it\nwe used react for the frontend, with django framework and mondodb for the backend.\nchallenges we ran into\nour group did not have any experience in the technologies we chose to use. we also chose to take a design approach of embedding the react app within django, rather than using django just as an api that interacts with our react app. our lack of experience with react bottlenecked us, and forced us spend a lot of time on creating simple components.\naccomplishments that we're proud of\nwe were able to implement token based authentication, allowing features such as only displaying the slideshows created by that user to the user. despite our lack of experience with the technologies used in this project we were able to create a working concept.\nwhat we learned\na lot! we learned a ton about setting up a project using django framework and how django works. we also got a crash course in javascript along with react concepts like state.\nwhat's next for chlk\nwe hope to get it up and hosted somewhere. as well as implement more of the features we had planned once we gain a little more frontend experience.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59503862}, {"Unnamed: 0": 3880, "autor": "MLOps powered by Civo", "date": null, "content": "Inspiration\nDespite the growing recognition of AI/ML as a crucial pillar of digital transformation, successful deployments and effective operations are a bottleneck for getting value from AI. Only one in two organizations has moved beyond pilots and proofs of concept. Moreover, 72% of a cohort of organizations that began AI pilots before 2019 have not been able to deploy even a single application in production. Algorithmia\u2019s survey of the state of enterprise machine learning found that 55% of companies surveyed have not deployed an ML model. To summarize: models don\u2019t make it into production, and if they do, they break because they fail to adapt to changes in the environment.\nWhat it does\nThe core activity during this ML development phase is experimentation. As data scientists and ML researchers prototype model architectures and training routines, they create labeled datasets, and they use features and other reusable ML artifacts that are governed through the data and model management process. The primary output of this process is a formalized training procedure, which includes data preprocessing, model architecture, and model training settings.\nIf the ML system requires continuous training (repeated retraining of the model), the training procedure is operationalized as a training pipeline. This requires a CI/CD routine to build, test, and deploy the pipeline to the target execution environment.\nThe continuous training pipeline is executed repeatedly based on retraining triggers, and it produces a model as output. The model is retrained as new data becomes available, or if model performance decay is detected. Other training artifacts and metadata that are produced by a training pipeline are also tracked. If the pipeline produces a successful model candidate, that candidate is then tracked by the model management process as a registered model.\nThe registered model is annotated, reviewed, and approved for release and is then deployed to a production environment. This process might be relatively opaque if you are using a no-code solution, or it can involve building a custom CI/CD pipeline for progressive delivery.\nThe deployed model serves predictions using the deployment pattern that you have specified: online, batch, or streaming predictions. In addition to serving predictions, the serving runtime can generate model explanations and capture serving logs to be used by the continuous monitoring process.\nThe continuous monitoring process monitors the model for predictive effectiveness and service. The primary concern of effectiveness performance monitoring is detecting model decay\u2014for example, data and concept drift. The model deployment can also be monitored for efficiency metrics like latency, throughput, hardware resource utilization, and execution errors.\nHow we built it\nWe created the architecture to continuously train and deploy the model to production.\nIntegrated multiple tool to achieve the continuous training and continuous deployment.\nChallenges we ran into\nFlow of Data for training and prediction.\nManaging the data is the really great challenge in Machine Learning Operations. In software development the frontend and backend can be easily deployed with DevOps, Cause we don't have to think about the flow of data\nAccomplishments that we're proud of\nIntegration of the architecture with AWS S3 to make the flow of data free, We can make data flow possible with the integration of any solution\nWhat we learned\nIntegration is the key to make the world better\nWhat's next for MLOps powered by civo\nCreate ML Pipelines with Kubeflow on Kubernetes with civo\nWe are looking forward to see the ML Applications in the civo marketplace\nReferences & Further Readings\nCivo\nAWS\nKeras\nFlask\nHelm\nGitHub Actions\nDocker\nDockerfile\nKubernetes\nArgoCD\nAnsible\nTerraform", "link": "https://devpost.com/software/mlops-powered-by-civo", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ndespite the growing recognition of ai/ml as a crucial pillar of digital transformation, successful deployments and effective operations are a bottleneck for getting value from ai. only one in two organizations has moved beyond pilots and proofs of concept. moreover, 72% of a cohort of organizations that began ai pilots before 2019 have not been able to deploy even a single application in production. algorithmia\u2019s survey of the state of enterprise machine learning found that 55% of companies surveyed have not deployed an ml model. to summarize: models don\u2019t make it into production, and if they do, they break because they fail to adapt to changes in the environment.\nwhat it does\nthe core activity during this ml development phase is experimentation. as data scientists and ml researchers prototype model architectures and training routines, they create labeled datasets, and they use features and other reusable ml artifacts that are governed through the data and model management process. the primary output of this process is a formalized training procedure, which includes data preprocessing, model architecture, and model training settings.\nif the ml system requires continuous training (repeated retraining of the model), the training procedure is operationalized as a training pipeline. this requires a ci/cd routine to build, test, and deploy the pipeline to the target execution environment.\nthe continuous training pipeline is executed repeatedly based on retraining triggers, and it produces a model as output. the model is retrained as new data becomes available, or if model performance decay is detected. other training artifacts and metadata that are produced by a training pipeline are also tracked. if the pipeline produces a successful model candidate, that candidate is then tracked by the model management process as a registered model.\nthe registered model is annotated, reviewed, and approved for release and is then deployed to a production environment. this process might be relatively opaque if you are using a no-code solution, or it can involve building a custom ci/cd pipeline for progressive delivery.\nthe deployed model serves predictions using the deployment pattern that you have specified: online, batch, or streaming predictions. in addition to serving predictions, the serving runtime can generate model explanations and capture serving logs to be used by the continuous monitoring process.\nthe continuous monitoring process monitors the model for predictive effectiveness and service. the primary concern of effectiveness performance monitoring is detecting model decay\u2014for example, data and concept drift. the model deployment can also be monitored for efficiency metrics like latency, throughput, hardware resource utilization, and execution errors.\nhow we built it\nwe created the architecture to continuously train and deploy the model to production.\nintegrated multiple -----> tool !!!  to achieve the continuous training and continuous deployment.\nchallenges we ran into\nflow of data for training and prediction.\nmanaging the data is the really great challenge in machine learning operations. in software development the frontend and backend can be easily deployed with devops, cause we don't have to think about the flow of data\naccomplishments that we're proud of\nintegration of the architecture with aws s3 to make the flow of data free, we can make data flow possible with the integration of any solution\nwhat we learned\nintegration is the key to make the world better\nwhat's next for mlops powered by civo\ncreate ml pipelines with kubeflow on kubernetes with civo\nwe are looking forward to see the ml applications in the civo marketplace\nreferences & further readings\ncivo\naws\nkeras\nflask\nhelm\ngithub actions\ndocker\ndockerfile\nkubernetes\nargocd\nansible\nterraform", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59503880}, {"Unnamed: 0": 3886, "autor": "kagit", "date": null, "content": "Inspiration\nI wondered how GitOps works. For Kubernetes, deploying by making changes to a remote Github repository sounds cool. After spending a few days, I was able to create a GitOps tool for Kubernetes!\nWhat it does\nkagit helps in deployments in your Kubernetes cluster. All manifests for your Kubernetes cluster are stored in a repo, along with a kagit.yml file. Whenever changes in your manifests (deployments, services, etc.) are detected, kagit applies those changes to your Kubernetes cluster.\nNo need to manually do kubectl apply everytime. Push your code and kagit will take care of it.\nHow we built it\nI used the Kubernetes API and the client-go package. I started by building a basic structure for the project, and then, added functionalities one-by-one.\nChallenges we ran into\nServer-side apply (https://kubernetes.io/docs/reference/using-api/server-side-apply/#using-server-side-apply-in-a-controller) was the biggest challenge.\nAccomplishments that we're proud of\nWriting a tool in 2 days!\nWhat we learned\nA lot about go-git, Server-Side Apply, etc.\nSDKs and libraries used:\ngo-git\nclient-go\nTheme:\nRemote Work\nkagit makes remote work easy. Developers in a team can push to a repo and kagit automatically takes care of deployments. Hence, developers don't need to fetch the latest changes and manually do kubectl apply everytime, kagit is the magic tool!", "link": "https://devpost.com/software/bachat", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ni wondered how gitops works. for kubernetes, deploying by making changes to a remote github repository sounds cool. after spending a few days, i was able to create a gitops -----> tool !!!  for kubernetes!\nwhat it does\nkagit helps in deployments in your kubernetes cluster. all manifests for your kubernetes cluster are stored in a repo, along with a kagit.yml file. whenever changes in your manifests (deployments, services, etc.) are detected, kagit applies those changes to your kubernetes cluster.\nno need to manually do kubectl apply everytime. push your code and kagit will take care of it.\nhow we built it\ni used the kubernetes api and the client-go package. i started by building a basic structure for the project, and then, added functionalities one-by-one.\nchallenges we ran into\nserver-side apply (https://kubernetes.io/docs/reference/using-api/server-side-apply/#using-server-side-apply-in-a-controller) was the biggest challenge.\naccomplishments that we're proud of\nwriting a tool in 2 days!\nwhat we learned\na lot about go-git, server-side apply, etc.\nsdks and libraries used:\ngo-git\nclient-go\ntheme:\nremote work\nkagit makes remote work easy. developers in a team can push to a repo and kagit automatically takes care of deployments. hence, developers don't need to fetch the latest changes and manually do kubectl apply everytime, kagit is the magic tool!", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59503886}, {"Unnamed: 0": 3896, "autor": "Sweet Talkers | NLP Final Project", "date": null, "content": "Deep Learning Final Project\nNLP Course Review Prediction Model\nBuilding a model that predicts course ratings based on Critical Review language data\nTags: NLP, Tensorflow, python\nPoster: https://docs.google.com/presentation/d/1yBslfjAz3vv7DJ94CogwvxfABAfD7GNU8fyVaGPGPHQ/edit?usp=sharing\nFinal Reflection: https://docs.google.com/document/d/1E-heY-2DSKG8mOehzU5YepowNYiMriYEZ20QfBknnRo/edit?usp=sharing\nWho We Are\nGeireann Lindfield Roberts (glindfie)\nRaymond Yeo (ryeo)\nProject Introduction\nWe are building a classification model that estimates course ratings based on Critical Review feedback and summaries. We are interested in finding whether the way we write feedback and evaluations accurately reflects the scores that we give the classes we are taking. We hope this tool will provide further insight into how courses have perfomed based on their written evaluations rather than just the multiple choice responses. We hope that this projects acts as a starting point for something that can be extended upon, and that written evaluations which are often not even read can be made into a more useful mode.\nWe were intriuged by the language processing portion of the class and wanted to explore more about it.\nRelated Work\nAs far as we are aware, there is nothing that has been done in the same regard with Critical Review's data. There is however some related NLP work that has been done on detecting the performance based on reviews.\nDetecting bad customer reviews with NLP, 2018 This article uses sentiment analysis to evaluate the rating given by customers based on their review. It uses Word2Vec, which is something we also hope to use for our model.\nJoint Training of Ratings and Reviews with Recurrent Recommender Networks, 2017 Uses a recurrent recommender network, which is based on recurrent neural networks.\nMulti Class Text Classification with LSTM using TensorFlow 2.0, article link Multi-label classification of news genres using LSTM\nText Classification with Movie Reviews, tensorflow tutorial This is an example of binary\u2014or two-class\u2014classification of movie reviews\nData\nWe have been in touch with the Critical Review team to get our data. While are still communicating and preparing our dataset, we will likely be utilizing two datasets in the following formats:\ntally.csv\nedition (e.g. 2021.2022.1)\ndepartment code (e.g. CSCI)\ncourse code (e.g. 1951F)\naverage course rating (e.g. 3.5)\naverage professor rating (e.g. 3.5)\nreview.csv\nedition (e.g. 2021.2022.1)\ndepartment code (e.g. CSCI)\ncourse code (e.g. 1951F)\nreview text\nOur preprocessing will involve tokenizing our input texts, and accounting for any data integrity problems. We will also merge both of the data sets on course code such that we can access the average course rating and average professor rating given the review text. For any additional data that Critical Review does not have readily accessible, we plan to ask for permission to web scrape it.\nMethodology\nPre-process and prepare the data such that it is compiled into a single CSV file and sufficiently prepared to be inputted into the model.\nUtilize language modeling deep learning techniques to train our model. We will likely use an embedding matrix, coupled with techniques seen in LSTM and Word2Vec models.\nWhen testing, we will round up on our labels. What we mean by this is that a course rating of 3.79 would be considered a 4. This will help gauge our model accuracy, and allow the model to constain its possible outputs to just 5 integer values (1,2,3,4,5). Our training data will consist of Critical Review data from 2010 (when they switched the way Critical Review feedback was done) until 2019.\nWe will then test the model by using it on the feedback over the last two years.\nMetrics\nModel accuracy would be a good metric to gauge this model's performance.\nBase Goal: Perform better than random guessing. This means our model accuracy would be greater than 20%.\nTarget Goal: Accuracy > 30%\nStretch Goal: Accuracy > 50%\nWe will reassess our goals after we begin training our modeling and gauging its performance.\nEthics\nDeep Learning serves as a good approach to this problem, as course and professor ratings are dependent on a variety of factors.\nWe understand that an unintended result of this model is that we are implictly evaluating the effectiveness of Critical review summaries. Our goal here is not to see how \"good\" a course review is, but rather to evaluate to what extent a qualitative metric like a holistic course review can be used to predict quantiative metrics.\nCritical Review and students and staff at Brown are the main stakeholders in the problem that we are trying to solve. We hope that our metrics will support Critical Review, and help them develop and create more useful quantifications of written feedback as well as numerical feedback. We have been in touch with Critical Review and have their support with regards to this matter.\nIf our model were to be inaccurate and give incorrect results this would negatively affect the reputation of Critical Review, and therefore would not help them provide the feedback to course staff and students. For that reason it is important that we ensure the accuracy of our mode.\nDivision of labor\nRay + Geireann = 100% of work = 100% sweet talkers. We have not decided on fixed work distributions yet.", "link": "https://devpost.com/software/sweet-talkers", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "deep learning final project\nnlp course review prediction model\nbuilding a model that predicts course ratings based on critical review language data\ntags: nlp, tensorflow, python\nposter: https://docs.google.com/presentation/d/1ybslfjaz3vv7dj94cogwvxfabafd7gnu8fyvagpgphq/edit?usp=sharing\nfinal reflection: https://docs.google.com/document/d/1e-hey-2dskg8moehzu5yepownyimriyez20qfbknnro/edit?usp=sharing\nwho we are\ngeireann lindfield roberts (glindfie)\nraymond yeo (ryeo)\nproject introduction\nwe are building a classification model that estimates course ratings based on critical review feedback and summaries. we are interested in finding whether the way we write feedback and evaluations accurately reflects the scores that we give the classes we are taking. we hope this -----> tool !!!  will provide further insight into how courses have perfomed based on their written evaluations rather than just the multiple choice responses. we hope that this projects acts as a starting point for something that can be extended upon, and that written evaluations which are often not even read can be made into a more useful mode.\nwe were intriuged by the language processing portion of the class and wanted to explore more about it.\nrelated work\nas far as we are aware, there is nothing that has been done in the same regard with critical review's data. there is however some related nlp work that has been done on detecting the performance based on reviews.\ndetecting bad customer reviews with nlp, 2018 this article uses sentiment analysis to evaluate the rating given by customers based on their review. it uses word2vec, which is something we also hope to use for our model.\njoint training of ratings and reviews with recurrent recommender networks, 2017 uses a recurrent recommender network, which is based on recurrent neural networks.\nmulti class text classification with lstm using tensorflow 2.0, article link multi-label classification of news genres using lstm\ntext classification with movie reviews, tensorflow tutorial this is an example of binary\u2014or two-class\u2014classification of movie reviews\ndata\nwe have been in touch with the critical review team to get our data. while are still communicating and preparing our dataset, we will likely be utilizing two datasets in the following formats:\ntally.csv\nedition (e.g. 2021.2022.1)\ndepartment code (e.g. csci)\ncourse code (e.g. 1951f)\naverage course rating (e.g. 3.5)\naverage professor rating (e.g. 3.5)\nreview.csv\nedition (e.g. 2021.2022.1)\ndepartment code (e.g. csci)\ncourse code (e.g. 1951f)\nreview text\nour preprocessing will involve tokenizing our input texts, and accounting for any data integrity problems. we will also merge both of the data sets on course code such that we can access the average course rating and average professor rating given the review text. for any additional data that critical review does not have readily accessible, we plan to ask for permission to web scrape it.\nmethodology\npre-process and prepare the data such that it is compiled into a single csv file and sufficiently prepared to be inputted into the model.\nutilize language modeling deep learning techniques to train our model. we will likely use an embedding matrix, coupled with techniques seen in lstm and word2vec models.\nwhen testing, we will round up on our labels. what we mean by this is that a course rating of 3.79 would be considered a 4. this will help gauge our model accuracy, and allow the model to constain its possible outputs to just 5 integer values (1,2,3,4,5). our training data will consist of critical review data from 2010 (when they switched the way critical review feedback was done) until 2019.\nwe will then test the model by using it on the feedback over the last two years.\nmetrics\nmodel accuracy would be a good metric to gauge this model's performance.\nbase goal: perform better than random guessing. this means our model accuracy would be greater than 20%.\ntarget goal: accuracy > 30%\nstretch goal: accuracy > 50%\nwe will reassess our goals after we begin training our modeling and gauging its performance.\nethics\ndeep learning serves as a good approach to this problem, as course and professor ratings are dependent on a variety of factors.\nwe understand that an unintended result of this model is that we are implictly evaluating the effectiveness of critical review summaries. our goal here is not to see how \"good\" a course review is, but rather to evaluate to what extent a qualitative metric like a holistic course review can be used to predict quantiative metrics.\ncritical review and students and staff at brown are the main stakeholders in the problem that we are trying to solve. we hope that our metrics will support critical review, and help them develop and create more useful quantifications of written feedback as well as numerical feedback. we have been in touch with critical review and have their support with regards to this matter.\nif our model were to be inaccurate and give incorrect results this would negatively affect the reputation of critical review, and therefore would not help them provide the feedback to course staff and students. for that reason it is important that we ensure the accuracy of our mode.\ndivision of labor\nray + geireann = 100% of work = 100% sweet talkers. we have not decided on fixed work distributions yet.", "sortedWord": "None", "removed": "Nan", "score": 6, "comments": 15, "media": null, "medialink": null, "identifyer": 59503896}, {"Unnamed: 0": 3899, "autor": "Text Simplifier", "date": null, "content": "Text Simplifier\nWho\ndtruong7, sdiwan2, wsun28, eholtz1\nFinal writeup\nProject video\n2nd checkpoint reflection\n1st checkpoint reflection\nInitial Proposal\nIntroduction\nReading and quickly extracting useful information from dense passages of text can often be difficult, especially for those learning a new language; idiomatic sentence structures and uncommon vocabulary can pose barriers to understanding text. Using translation apps to overcome this bypasses the need to understand the target language.\nInstead, we believe that having a tool that simplifies English input text by varying levels allows English-language learners to flexibly comprehend essential ideas in texts that may lie beyond their reading level, encouraging further reading and building confidence in the target language. We employ supervised learning using a (recurrent) seq2seq transformer model that learns to convert input text of an arbitrary reading level to that of relatively lower level(s).\nRelated Works\n1) https://aclanthology.org/Q15-1021.pdf: One of the motivations for our work is this paper, which discusses the prior standard dataset of text simplification, Simple Wikipedia, and what flaws it has in its usage. The paper discusses how the Simple Wikipedia suffers from key flaws that make text simplification more difficult, such as that a significant portion of the sentences are not a one-to-one semantic match, and that often, Simple Wikipedia does not actually simplify the original text\u2019s meaning so much as change the wording. The conclusion of this paper is that the Newsela dataset solves these issues in some amount, and serves as a better standard dataset for the text simplification task. 2) https://arxiv.org/pdf/2005.02324v4.pdf: This is one of the most state of the art text simplification architectures, which goes over a method of aligning the dataset and running this through transformer-based architectures. The paper proposes a novel way to calculate alignments for sentences in the Newsela dataset (which we may choose to implement for the data scraping part of the project), and then applies this to 3 transformer-based models to test the effectiveness of this alignment. The paper proves both that these sentence alignments give improvements over other prior sentence alignment methods, and that under these sentence alignments, these transformer-based models give F1 scores in the high 80s. Thus, this proves that transformer-based models are feasible, although our project extends these models to make them generalizable using recurrency.\nData\nThe dataset we are using is the Newsela dataset, a standard dataset for the text simplification task. The Newsela dataset contains a set of texts, labeled by the lexile level that the text is at. The higher the lexile level, the more complex the text, and the text can be at one of 5 lexile levels: 620L, 880L, 1040L, 1210L, and MAX. This dataset is perfect for our task, as we have a consistent 5 levels of lexile difficulty, making it simple for us to batch the data such that our model is expected to predict a lexile level lower than the input text. In addition, Newsela lexile simplification is such that it simplifies the idea sentence for sentence, keeping the ideas in the same order and keeping the corresponding sentences\u2019 meaning the same between lexile levels.\nMethodology\nWe are using a transformer with multi-headed attention that takes in as input a sequence of text and outputs a relatively simplified sequence containing the same essential meaning. We will train the model to simplify relatively by using training inputs of Lexile reading level n and labels of level (n-1) for every epoch. By using the same model parameters across difficulty pairs, the model should learn to slightly simplify input sequences of arbitrary complexity.\nIn order to simplify input text by multiple degrees (as determined by a user-controlled slider), we feed the output (or \u2018state\u2019) of one forward pass back into the transformer recursively multiple times.\nWe will test the model using a random sample of 20% of the entire dataset.\nMetrics\nOur first step wil be to construct a single transformer architecture that should ideally learn not the different lexile levels as if they were two different languages, but instead a relative lexile complexity drop of 1 lexile level as defined by Newsella article curators.\nWe will train the model on the 4 upper lexile levels provided by Newsella through a multi-headed transformer architecture. To decrease the parplexity, we will attempt to feed in the next lower lexile level text as input to the next transformer block akin to how training occurs in RNNs with word tokens.\nIf this has a very high perplexity, say, over 1000, we will attempt to retrain the model architecture using different numbers of blocks.\nFor the transformer that we create, we aim for:\nBase goal = perplexity < 100.\nTarget goal = perplexity < 50\nStretch goal = perplexity < 10 We will not make use of the notion of accuracy, but rather this perplexity as well as an accuracy per symbol as determined from the unpadded tokens within our transformer input text to determine how well our model is training and performing over validation/test sets.\nAnother way that we hope to assess our model is by checking whether the hyperparameters we have chosen are as optimal as can be. We will do this by visualizing the perplexities and accuracy per symbol over multiple hyperparametrized values, and visualize this using different traces. We will also attempt to inspect tensor attention heatmaps to see how they compare between the 5 transformer architectures and use this to determine if any tweaks in the hyperparameters between the models is needed or has a pattern.\nEthics\nOur dataset consists of Newsela articles written at different lexile levels. It is not representative of every text, as these articles are almost exclusively a specific variety of non-fiction. The collection process is simple webscraping, so it should not present an issue. However, the dataset itself can have hidden biases which lead to wrongful extrapolations and convey incorrect implications. For instance, if most Newsela articles which mention programmers predominantly mention male ones, the simplifier may assume that it is appropriate to use a male pronoun to refer to the programmer in a case where their gender is ambiguous. Similarly, the simplifier may interchange \u2018conservative\u2019 and \u2018Republican\u2019 due to the high number of news articles written about conservative Republicans. However, there is a contingent of moderate Republicans who even started a political movement to oust Trump in the 2020 election: the words \u2018conservative\u2019 and \u2018Republican\u2019, while often used in the same context, are not necessarily referring to the same group of people. Mistakes of this kind can cause ESL learners to make false assumptions about English and learn the language incorrectly in ways subtle enough that they may not be able to catch the issue.\nIn terms of other use cases, teachers may use this model to procure ability-appropriate material for their students. In the classroom, past elementary school, (American, and perhaps some other) teachers are often forced to hand out the same exact materials to students for the sake of meeting learning objectives in a particular subject. This leads to some students struggling on certain readings and others being given readings which fail to challenge them at all. This is especially the case in classrooms where students are not tracked by ability. Teachers must be able to cope with the ability disparity in their classes to provide an adequate challenge for every student. Our text simplifier ensures that teachers can choose one text and scale its difficulty to customize it for each student, allowing a class of students with diverse learning rates and backgrounds to engage with the same content at the level appropriate for them.\nOne potential broader (American) societal problem with this, however, is that studies show that there is a trend in education of underestimating non-Asian minority students, and a teacher could fail to challenge such a student appropriately. On the flip side, due to the \u201cmodel minority\u201d stereotype, a teacher could hand out a text that is too difficult for a particular Asian student.\nDivision of Labor\nData preprocessing + web scraping: Will\nSimplifier Model (Recurrent Transformer): Everyone - will divide equally\nTraining/Testing (batching, windowing, clipping, feeding appropriate data into model) & fine-tuning (i.e. assignment.py): Anh\nFront-end/visualization (e.g. perplexity/acc per symbol graphs, transformer attention/hyperparam difference visualizations): Siddharth", "link": "https://devpost.com/software/text-simplifier-3z7m8e", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "text simplifier\nwho\ndtruong7, sdiwan2, wsun28, eholtz1\nfinal writeup\nproject video\n2nd checkpoint reflection\n1st checkpoint reflection\ninitial proposal\nintroduction\nreading and quickly extracting useful information from dense passages of text can often be difficult, especially for those learning a new language; idiomatic sentence structures and uncommon vocabulary can pose barriers to understanding text. using translation apps to overcome this bypasses the need to understand the target language.\ninstead, we believe that having a -----> tool !!!  that simplifies english input text by varying levels allows english-language learners to flexibly comprehend essential ideas in texts that may lie beyond their reading level, encouraging further reading and building confidence in the target language. we employ supervised learning using a (recurrent) seq2seq transformer model that learns to convert input text of an arbitrary reading level to that of relatively lower level(s).\nrelated works\n1) https://aclanthology.org/q15-1021.pdf: one of the motivations for our work is this paper, which discusses the prior standard dataset of text simplification, simple wikipedia, and what flaws it has in its usage. the paper discusses how the simple wikipedia suffers from key flaws that make text simplification more difficult, such as that a significant portion of the sentences are not a one-to-one semantic match, and that often, simple wikipedia does not actually simplify the original text\u2019s meaning so much as change the wording. the conclusion of this paper is that the newsela dataset solves these issues in some amount, and serves as a better standard dataset for the text simplification task. 2) https://arxiv.org/pdf/2005.02324v4.pdf: this is one of the most state of the art text simplification architectures, which goes over a method of aligning the dataset and running this through transformer-based architectures. the paper proposes a novel way to calculate alignments for sentences in the newsela dataset (which we may choose to implement for the data scraping part of the project), and then applies this to 3 transformer-based models to test the effectiveness of this alignment. the paper proves both that these sentence alignments give improvements over other prior sentence alignment methods, and that under these sentence alignments, these transformer-based models give f1 scores in the high 80s. thus, this proves that transformer-based models are feasible, although our project extends these models to make them generalizable using recurrency.\ndata\nthe dataset we are using is the newsela dataset, a standard dataset for the text simplification task. the newsela dataset contains a set of texts, labeled by the lexile level that the text is at. the higher the lexile level, the more complex the text, and the text can be at one of 5 lexile levels: 620l, 880l, 1040l, 1210l, and max. this dataset is perfect for our task, as we have a consistent 5 levels of lexile difficulty, making it simple for us to batch the data such that our model is expected to predict a lexile level lower than the input text. in addition, newsela lexile simplification is such that it simplifies the idea sentence for sentence, keeping the ideas in the same order and keeping the corresponding sentences\u2019 meaning the same between lexile levels.\nmethodology\nwe are using a transformer with multi-headed attention that takes in as input a sequence of text and outputs a relatively simplified sequence containing the same essential meaning. we will train the model to simplify relatively by using training inputs of lexile reading level n and labels of level (n-1) for every epoch. by using the same model parameters across difficulty pairs, the model should learn to slightly simplify input sequences of arbitrary complexity.\nin order to simplify input text by multiple degrees (as determined by a user-controlled slider), we feed the output (or \u2018state\u2019) of one forward pass back into the transformer recursively multiple times.\nwe will test the model using a random sample of 20% of the entire dataset.\nmetrics\nour first step wil be to construct a single transformer architecture that should ideally learn not the different lexile levels as if they were two different languages, but instead a relative lexile complexity drop of 1 lexile level as defined by newsella article curators.\nwe will train the model on the 4 upper lexile levels provided by newsella through a multi-headed transformer architecture. to decrease the parplexity, we will attempt to feed in the next lower lexile level text as input to the next transformer block akin to how training occurs in rnns with word tokens.\nif this has a very high perplexity, say, over 1000, we will attempt to retrain the model architecture using different numbers of blocks.\nfor the transformer that we create, we aim for:\nbase goal = perplexity < 100.\ntarget goal = perplexity < 50\nstretch goal = perplexity < 10 we will not make use of the notion of accuracy, but rather this perplexity as well as an accuracy per symbol as determined from the unpadded tokens within our transformer input text to determine how well our model is training and performing over validation/test sets.\nanother way that we hope to assess our model is by checking whether the hyperparameters we have chosen are as optimal as can be. we will do this by visualizing the perplexities and accuracy per symbol over multiple hyperparametrized values, and visualize this using different traces. we will also attempt to inspect tensor attention heatmaps to see how they compare between the 5 transformer architectures and use this to determine if any tweaks in the hyperparameters between the models is needed or has a pattern.\nethics\nour dataset consists of newsela articles written at different lexile levels. it is not representative of every text, as these articles are almost exclusively a specific variety of non-fiction. the collection process is simple webscraping, so it should not present an issue. however, the dataset itself can have hidden biases which lead to wrongful extrapolations and convey incorrect implications. for instance, if most newsela articles which mention programmers predominantly mention male ones, the simplifier may assume that it is appropriate to use a male pronoun to refer to the programmer in a case where their gender is ambiguous. similarly, the simplifier may interchange \u2018conservative\u2019 and \u2018republican\u2019 due to the high number of news articles written about conservative republicans. however, there is a contingent of moderate republicans who even started a political movement to oust trump in the 2020 election: the words \u2018conservative\u2019 and \u2018republican\u2019, while often used in the same context, are not necessarily referring to the same group of people. mistakes of this kind can cause esl learners to make false assumptions about english and learn the language incorrectly in ways subtle enough that they may not be able to catch the issue.\nin terms of other use cases, teachers may use this model to procure ability-appropriate material for their students. in the classroom, past elementary school, (american, and perhaps some other) teachers are often forced to hand out the same exact materials to students for the sake of meeting learning objectives in a particular subject. this leads to some students struggling on certain readings and others being given readings which fail to challenge them at all. this is especially the case in classrooms where students are not tracked by ability. teachers must be able to cope with the ability disparity in their classes to provide an adequate challenge for every student. our text simplifier ensures that teachers can choose one text and scale its difficulty to customize it for each student, allowing a class of students with diverse learning rates and backgrounds to engage with the same content at the level appropriate for them.\none potential broader (american) societal problem with this, however, is that studies show that there is a trend in education of underestimating non-asian minority students, and a teacher could fail to challenge such a student appropriately. on the flip side, due to the \u201cmodel minority\u201d stereotype, a teacher could hand out a text that is too difficult for a particular asian student.\ndivision of labor\ndata preprocessing + web scraping: will\nsimplifier model (recurrent transformer): everyone - will divide equally\ntraining/testing (batching, windowing, clipping, feeding appropriate data into model) & fine-tuning (i.e. assignment.py): anh\nfront-end/visualization (e.g. perplexity/acc per symbol graphs, transformer attention/hyperparam difference visualizations): siddharth", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 11, "media": null, "medialink": null, "identifyer": 59503899}, {"Unnamed: 0": 3900, "autor": "HEO", "date": null, "content": "Inspiration\nFinancial restriction is an effective tool that oppressive governments use to put pressure on \u201cunwanted\u201d organizations and civic initiatives. This tool is easy to use for Russian and Belarussian governments, because they can control all movements of funds through the traditional payment systems on their territory. Until recently, the Russian government have been exerting this control by putting limitations on the recipients of donations. Russian business is practically prohibited to finance such civic initiatives. \u201cForeign agents\u201d law severely restricts any organization that receives even a single ruble from abroad. In addition, these governments routinely levy fines and freeze bank accounts of organizations and individuals without due process. Unfortunately, in 2021, they extended their attack to donors in addition to receivers. They have issued a separate law that limits the rights of anyone who has donated to any of Alexey Navalny\u2019s organizations. Recently, Putin has signed yet another law that punishes a donation to an \u201cunwanted organization\u201d with up to 5 years in prison. This law is likely to be extended to punish everyone who has previously donated to organizations labeled as \u201cforeign agents\u201d. Soon after that, it will extend to anyone who is not loyal to the regime. In Belarus, these conditions have existed for a while. There, it is illegal even to collect money to help pay fines for anyone arrested for participation in a civil protest.\nMoving to cryptocurrencies is the natural response of the civil society to this severe cramp down. However, cryptocurrencies are still too complicated and confusing for most people. In addition to that, commissions for any transactions on the two major blockchains (Bitcoin and Ethereum) are so high that they make small donations impossible (a $10 donation costs $10-$50 to send).\nWhat it does\nHEO allows anyone to raise donations in cUSD in a user friendly way and creates a common place for donors and fundraisers similar to GoFundMe. In the first version, HEO team vets organizations and people allowed to raise funds. In the near future, we will implement a KYC process that will allow us to automate the vetting process. In the next version, we will also allow anyone to launch campaigns in exchange for HEO tokens.\nHow we built it\nWe started with an idea about a year ago. After the initial prototype, we hired a designer and convinced a second developer to join our team. Together, we turned the prototype into a working application. Thanks to a grant from Amazon Web Services, we are able to develop and run our application for free (for now, anyway).\nHEO web application is architected to survive being shutdown and built to allow anyone to host it. All data about fundraising campaigns is stored on the blockchain and the application also uses MongoDB to cache that data. A separate AWS Lambda function is used to update the MongoDB cache (https://github.com/HEO-Platform/heo-worker). This architecture allows anyone to spin up a mirror of the web application within minutes and connect it to the same contracts.\nChallenges we ran into\nEthereum is extremely expensive for interacting with contracts and is not a viable platform for making small ($10-$100) donations. We also tried Binance, but none of the major US-based exchanges support Binance Smart Chain and BEP-20 tokens (they support only Binance Chain and BEP-2). In addition to that, only Metamask mobile supports testnets for WalletConnect. Fortunately, Valora has a test app on TestFlight that we could use for additional testing on Alfajores.\nWe used Web3Modal (https://github.com/Web3Modal) in our development, but the community behind it has refocused on WalletConnect 2.0, so we had to fix and improve Web3Modal on our own in order to make it work with Celo and Alfajores networks.\nPublic Wallet Connect bridges are often overloaded, so we also had to figure out how to host our own Wallet Connect bridge on AWS.\nAccomplishments that we're proud of\nIn about a year we have built and launched a live application with zero investment and we have partnered with six NGOs that have already launched fundraising campaigns on our platform.\nWhat we learned\nWriting code for blockchain in 2020s is like building e-commerce websites in late 1990s :) Nothing works 100% and lots of quirks await around every corner, but it is just as much fun!\nWhat's next for HEO\nWe have a rich roadmap ahead of us:\nAdd support for campaign descriptions in multiple languages\nUpdate Help and instructions for Celo blockchain\nEnable reward-earning campaigns (this is already implemented in the contracts, but not in the UI yet)\nIntegrate KYC\nIntegrate with Proof Of Humanity for \"lite KYC\"\nSell HEO token via an IDO\nImplement governance UI (right now we use Remix and command line tools to interact with governance contracts)\nLaunch a reward program for other developers to host mirrors of the web application\nAdd categorization and search features to the web application\nAdd ability to donate stablecoins using credit cards, debit cards, and bank accounts (donor pays with fiat and fundraiser receives stablecoins)\nGive control of the platform to the people holding HEO tokens. These will be the real owners of the platform, because they would have earned the HEO tokens by making donations or by investing early on via an IDO", "link": "https://devpost.com/software/heo", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nfinancial restriction is an effective -----> tool !!!  that oppressive governments use to put pressure on \u201cunwanted\u201d organizations and civic initiatives. this tool is easy to use for russian and belarussian governments, because they can control all movements of funds through the traditional payment systems on their territory. until recently, the russian government have been exerting this control by putting limitations on the recipients of donations. russian business is practically prohibited to finance such civic initiatives. \u201cforeign agents\u201d law severely restricts any organization that receives even a single ruble from abroad. in addition, these governments routinely levy fines and freeze bank accounts of organizations and individuals without due process. unfortunately, in 2021, they extended their attack to donors in addition to receivers. they have issued a separate law that limits the rights of anyone who has donated to any of alexey navalny\u2019s organizations. recently, putin has signed yet another law that punishes a donation to an \u201cunwanted organization\u201d with up to 5 years in prison. this law is likely to be extended to punish everyone who has previously donated to organizations labeled as \u201cforeign agents\u201d. soon after that, it will extend to anyone who is not loyal to the regime. in belarus, these conditions have existed for a while. there, it is illegal even to collect money to help pay fines for anyone arrested for participation in a civil protest.\nmoving to cryptocurrencies is the natural response of the civil society to this severe cramp down. however, cryptocurrencies are still too complicated and confusing for most people. in addition to that, commissions for any transactions on the two major blockchains (bitcoin and ethereum) are so high that they make small donations impossible (a $10 donation costs $10-$50 to send).\nwhat it does\nheo allows anyone to raise donations in cusd in a user friendly way and creates a common place for donors and fundraisers similar to gofundme. in the first version, heo team vets organizations and people allowed to raise funds. in the near future, we will implement a kyc process that will allow us to automate the vetting process. in the next version, we will also allow anyone to launch campaigns in exchange for heo tokens.\nhow we built it\nwe started with an idea about a year ago. after the initial prototype, we hired a designer and convinced a second developer to join our team. together, we turned the prototype into a working application. thanks to a grant from amazon web services, we are able to develop and run our application for free (for now, anyway).\nheo web application is architected to survive being shutdown and built to allow anyone to host it. all data about fundraising campaigns is stored on the blockchain and the application also uses mongodb to cache that data. a separate aws lambda function is used to update the mongodb cache (https://github.com/heo-platform/heo-worker). this architecture allows anyone to spin up a mirror of the web application within minutes and connect it to the same contracts.\nchallenges we ran into\nethereum is extremely expensive for interacting with contracts and is not a viable platform for making small ($10-$100) donations. we also tried binance, but none of the major us-based exchanges support binance smart chain and bep-20 tokens (they support only binance chain and bep-2). in addition to that, only metamask mobile supports testnets for walletconnect. fortunately, valora has a test app on testflight that we could use for additional testing on alfajores.\nwe used web3modal (https://github.com/web3modal) in our development, but the community behind it has refocused on walletconnect 2.0, so we had to fix and improve web3modal on our own in order to make it work with celo and alfajores networks.\npublic wallet connect bridges are often overloaded, so we also had to figure out how to host our own wallet connect bridge on aws.\naccomplishments that we're proud of\nin about a year we have built and launched a live application with zero investment and we have partnered with six ngos that have already launched fundraising campaigns on our platform.\nwhat we learned\nwriting code for blockchain in 2020s is like building e-commerce websites in late 1990s :) nothing works 100% and lots of quirks await around every corner, but it is just as much fun!\nwhat's next for heo\nwe have a rich roadmap ahead of us:\nadd support for campaign descriptions in multiple languages\nupdate help and instructions for celo blockchain\nenable reward-earning campaigns (this is already implemented in the contracts, but not in the ui yet)\nintegrate kyc\nintegrate with proof of humanity for \"lite kyc\"\nsell heo token via an ido\nimplement governance ui (right now we use remix and command line tools to interact with governance contracts)\nlaunch a reward program for other developers to host mirrors of the web application\nadd categorization and search features to the web application\nadd ability to donate stablecoins using credit cards, debit cards, and bank accounts (donor pays with fiat and fundraiser receives stablecoins)\ngive control of the platform to the people holding heo tokens. these will be the real owners of the platform, because they would have earned the heo tokens by making donations or by investing early on via an ido", "sortedWord": "None", "removed": "Nan", "score": 6, "comments": 0, "media": null, "medialink": null, "identifyer": 59503900}, {"Unnamed: 0": 3901, "autor": "CYu Later Amazon Rater", "date": null, "content": "CYu Later Amazon Rater\nFinal Report\nFinal Poster\nMembers\nSuraj Zaveri (szaveri3)\nNaafiyan Ahmed (nahmed21)\nCarmen Yu (cyu63)\nIntroduction\nSentiment analysis is the use of natural language processing and text analysis to identify information about the writer\u2019s attitude toward a certain product (positive, negative, or neutral). The paper we are implementing aims to study the relationship between Amazon product reviews and the corresponding ratings given to the products by customers. A model that understands such a correlation can yield insight into word connotations, proving to be useful to both the company as well as computational linguists.\nThe CYu Later Amazon Rater is an implementation of a sentiment analysis project that utilizes text from Amazon reviews in order to predict the sentiment (positive or negative) for a given review of Amazon products. This is a binary classification task (with input review text and output rating) that we chose due to our shared interests in natural language processing as well as the insight about word connotations that such a model can uncover.\nRelated Work\nThe following Stanford Paper was the main source of inspiration for our project. In this paper, the Consumer Reviews of Amazon Products dataset is used to correlate review text and product ratings (5 classes of the ratings 1-5). As part of preprocessing this data (which had 34627 different product reviews), the paper creates a simplified dictionary of words (with the threshold being 6 occurrences of the word in the reviews) as well as a 50-d glove dictionary to represent the features. Following preprocessing, 4 different models are used to perform the classification (Naive Bayes, K-nearest neighbor, Linear Support Vector Machine, and Long Short-Term Memory). The results showed that the LSTM without the Glove dictionary resulted in the highest test accuracy (71.5%), with potential explanations being that with the Glove dictionary (which is intended to be a more robust dictionary), the model overfitted to the data (85.6% train accuracy but 65.6% test accuracy).\nSimilar publicly available projects for sentiment analysis of Amazon reviews (a running list): https://github.com/louiefb/amazon-reviews-nlp/blob/master/Amazon%20Reviews%20NLP.ipynb https://www.kaggle.com/muonneutrino/sentiment-analysis-with-amazon-reviews\nData\nThe Stanford Paper mentioned above used the Consumer Reviews of Amazon Products dataset. Our project will use the UCSD Amazon Reviews and Ratings Dataset which contains the same data but formatted slightly differently. The data is stored as a JSON which will help when parsing through the data during the pre-processing phase. We will be looking at the \u201creviewText\u201d and \u201coverall\u201d fields which contain data on the user text reviews and the actual numeric ratings respectively.\nMethodology\nPreprocessing:\nWe will use the Python JSON library to parse through the JSON data and build a list of input sentences and labels. We will also split the dataset into training and testing set.\nModel:\nWe plan on using a RNN model. The architecture of the model will be as follows:\nInitial Embedding Layer - The inputs were the tokenized review texts. We used both Keras Embeddings and GloVe Embeddings and compared the results of model\n2 GRU Layers\n2 Dense layers: ReLU and Sigmoid activations respectively\nFinal output from the 2nd Dense layer is a single value in [0, 1] where 0 represents negative sentiment and 1 represents positive sentiment\nWe trained the model for 1 epoch due to computational and time constraints.\nMetrics\nAccuracy in this project will be measured by whether the model was able to correctly determine positive or negative sentiment. Positive sentiment is defined as a rating that is greater than a sentiment threshold and negative sentiment is defined as a rating that is less than or equal to sentiment threshold.\nWe compared the results of our word embeddings against the GLoVE word embeddings. We initially wanted to compare it to other embeddings but were unable to due to computational and time constraints\nEthics\nWhat broader societal issues are relevant to your chosen problem space?\nBy generating and providing analysis on user data (and more specifically, underlying intentions and reactions to products), sentiment analysis can be a powerful tool for businesses to refine marketing strategies and to improve customer experience in general. Beyond the commercial realm, however, it can be broadly applied to any cause or organization that relates to public sentiment and attitude (e.g. using online comments and posts to gauge electoral outcomes for the future or gain insight on political polarization).\nDue to the nature of sentiment analysis and mass data collection, it could potentially be associated with existing controversies on big data mining and privacy concerns. There are also limitations to sentiment analysis \u2014 such as detecting more subtle contextual cues like sarcasm or exaggeration \u2014 that can ultimately skew results.\nWhat is your dataset? Are there any concerns about how it was collected, or labeled? Is it representative? What kind of underlying historical or societal biases might it contain?\nOur data will be extracted from a more narrow subset of UCSD\u2019s Amazon Product Dataset, which (as a whole) contains 142.8 million product reviews and general metadata from Amazon. While the method of data collection is fairly straightforward given that product reviews are made publicly available, one possible concern might be the timeframe of the reviews, spanning May 1996 - July 2014.\nFurthermore, underlying biases may stem from:\nresponse bias \u2014 consumers with more extreme opinions may feel more inclined to post either overwhelming positive or overwhelmingly negative reviews. In other words, the sentiments and opinions associated with 1-star and 5-star reviews may be overrepresented compared to more neutral reviews;\nbrands artificially inflating product ratings by offering consumers refunds or discounts in exchange for positive reviews.\nDivision of Labor\nSuraj - Preprocessing, GloVe Embedding implementation, Model Architecture Diagram\nNaafiyan - Preprocessing, Model, Train/Test, QoL features such as CLI arguments\nCarmen - Visualizing Data, Model Analysis, Ethical Considerations, Presentation\nAcknowledgements\nThis project was made possible through CSCI 1470 (Deep Learning) at Brown University. A special thank you to TA Deniz Ozturk for his mentorship on the project, as well as Professor Chen Sun for teaching us the concepts necessary to complete this project.\nDocuments\nCheck-in 2", "link": "https://devpost.com/software/cyu-later-amazon-rater", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "cyu later amazon rater\nfinal report\nfinal poster\nmembers\nsuraj zaveri (szaveri3)\nnaafiyan ahmed (nahmed21)\ncarmen yu (cyu63)\nintroduction\nsentiment analysis is the use of natural language processing and text analysis to identify information about the writer\u2019s attitude toward a certain product (positive, negative, or neutral). the paper we are implementing aims to study the relationship between amazon product reviews and the corresponding ratings given to the products by customers. a model that understands such a correlation can yield insight into word connotations, proving to be useful to both the company as well as computational linguists.\nthe cyu later amazon rater is an implementation of a sentiment analysis project that utilizes text from amazon reviews in order to predict the sentiment (positive or negative) for a given review of amazon products. this is a binary classification task (with input review text and output rating) that we chose due to our shared interests in natural language processing as well as the insight about word connotations that such a model can uncover.\nrelated work\nthe following stanford paper was the main source of inspiration for our project. in this paper, the consumer reviews of amazon products dataset is used to correlate review text and product ratings (5 classes of the ratings 1-5). as part of preprocessing this data (which had 34627 different product reviews), the paper creates a simplified dictionary of words (with the threshold being 6 occurrences of the word in the reviews) as well as a 50-d glove dictionary to represent the features. following preprocessing, 4 different models are used to perform the classification (naive bayes, k-nearest neighbor, linear support vector machine, and long short-term memory). the results showed that the lstm without the glove dictionary resulted in the highest test accuracy (71.5%), with potential explanations being that with the glove dictionary (which is intended to be a more robust dictionary), the model overfitted to the data (85.6% train accuracy but 65.6% test accuracy).\nsimilar publicly available projects for sentiment analysis of amazon reviews (a running list): https://github.com/louiefb/amazon-reviews-nlp/blob/master/amazon%20reviews%20nlp.ipynb https://www.kaggle.com/muonneutrino/sentiment-analysis-with-amazon-reviews\ndata\nthe stanford paper mentioned above used the consumer reviews of amazon products dataset. our project will use the ucsd amazon reviews and ratings dataset which contains the same data but formatted slightly differently. the data is stored as a json which will help when parsing through the data during the pre-processing phase. we will be looking at the \u201creviewtext\u201d and \u201coverall\u201d fields which contain data on the user text reviews and the actual numeric ratings respectively.\nmethodology\npreprocessing:\nwe will use the python json library to parse through the json data and build a list of input sentences and labels. we will also split the dataset into training and testing set.\nmodel:\nwe plan on using a rnn model. the architecture of the model will be as follows:\ninitial embedding layer - the inputs were the tokenized review texts. we used both keras embeddings and glove embeddings and compared the results of model\n2 gru layers\n2 dense layers: relu and sigmoid activations respectively\nfinal output from the 2nd dense layer is a single value in [0, 1] where 0 represents negative sentiment and 1 represents positive sentiment\nwe trained the model for 1 epoch due to computational and time constraints.\nmetrics\naccuracy in this project will be measured by whether the model was able to correctly determine positive or negative sentiment. positive sentiment is defined as a rating that is greater than a sentiment threshold and negative sentiment is defined as a rating that is less than or equal to sentiment threshold.\nwe compared the results of our word embeddings against the glove word embeddings. we initially wanted to compare it to other embeddings but were unable to due to computational and time constraints\nethics\nwhat broader societal issues are relevant to your chosen problem space?\nby generating and providing analysis on user data (and more specifically, underlying intentions and reactions to products), sentiment analysis can be a powerful -----> tool !!!  for businesses to refine marketing strategies and to improve customer experience in general. beyond the commercial realm, however, it can be broadly applied to any cause or organization that relates to public sentiment and attitude (e.g. using online comments and posts to gauge electoral outcomes for the future or gain insight on political polarization).\ndue to the nature of sentiment analysis and mass data collection, it could potentially be associated with existing controversies on big data mining and privacy concerns. there are also limitations to sentiment analysis \u2014 such as detecting more subtle contextual cues like sarcasm or exaggeration \u2014 that can ultimately skew results.\nwhat is your dataset? are there any concerns about how it was collected, or labeled? is it representative? what kind of underlying historical or societal biases might it contain?\nour data will be extracted from a more narrow subset of ucsd\u2019s amazon product dataset, which (as a whole) contains 142.8 million product reviews and general metadata from amazon. while the method of data collection is fairly straightforward given that product reviews are made publicly available, one possible concern might be the timeframe of the reviews, spanning may 1996 - july 2014.\nfurthermore, underlying biases may stem from:\nresponse bias \u2014 consumers with more extreme opinions may feel more inclined to post either overwhelming positive or overwhelmingly negative reviews. in other words, the sentiments and opinions associated with 1-star and 5-star reviews may be overrepresented compared to more neutral reviews;\nbrands artificially inflating product ratings by offering consumers refunds or discounts in exchange for positive reviews.\ndivision of labor\nsuraj - preprocessing, glove embedding implementation, model architecture diagram\nnaafiyan - preprocessing, model, train/test, qol features such as cli arguments\ncarmen - visualizing data, model analysis, ethical considerations, presentation\nacknowledgements\nthis project was made possible through csci 1470 (deep learning) at brown university. a special thank you to ta deniz ozturk for his mentorship on the project, as well as professor chen sun for teaching us the concepts necessary to complete this project.\ndocuments\ncheck-in 2", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 6, "media": null, "medialink": null, "identifyer": 59503901}, {"Unnamed: 0": 3907, "autor": "Family Photo", "date": null, "content": "Inspiration\nI'm pretty sure someone on NFT twitter put forth an idea for a \"meta NFT\" that would work similar to the GIMP example on the bottom of this. I was thinking about that a bit and thought that Chainlink tools would at least be able to serve as a proof of concept for something like that.\nWhat it does\nThis project allows a user to create a Family Photo NFT (FAM). A Family Photo is an ERC-721 where each token stores pointers to other ERC-721 NFTs. A user will mint a new FAM which opens it up for that user or other users to \"add\" their own NFTs to it. \"Add\" being used loosely here - there isn't any actual token transfer. At some point, the minting user can decide to \"finalize\" the FAM they minted, which will generate an image for that FAM based on all the NFTs added to it. This is done via a custom Chainlink External Adapter currently deployed to Kovan (until my Linkpool node expires) Right now mainly IPFS image URI, URLs, or JSON are supported ways to find that image. Then, if a person who added their NFT to a FAM ever moves that NFT from their wallet, the FAM will automatically regenerate a new image with that NFT blacked out (well - darkened out). This is triggered via a Keeper job currently running on Kovan.\nHow we built it\nI built this using Solidity, Python, and React and used tools like Brownie (VSCode), AWS Lambda, Linkpool running my External Adapter, and Chainlink Keepers on Kovan. I used plenty of example code from the Solidity Beginner to Expert freecodecamp, various Chainlink tutorials, and various Stackoverflow posts.\nChallenges we ran into\nThis was my first real Smart Contract project so there was a bit of a learning curve to get proper syntax and structuring to work throughout the project.\nSome interesting challenges were getting large responses to work with my Chainlink node and thus getting my External Adapter to properly trigger and handle my token URIs (Chainlink \"Large Responses\" tutorial helped a lot with this). In addition, it was tough to get my Keeper job to handle an array of data but I found that Linkpool's Keeper job on mainnet was actually doing something similar to what I wanted to do so I was able to copy their approach.\nAccomplishments that we're proud of\nI felt great about even finishing this project since this was my first time doing this. And getting an External Adapter AND a Keeper job up and running :).\nWhat we learned\nI learned a lot about the structure of NFTs and about how powerful External Adapter jobs can be since they're just running a bunch of Python in the cloud - you can pretty much do anything and that's very cool.\nWhat's next for Family Photo\nThere's a lot of cool things that could be done to enhance this project. To start, there are definitely gas optimizations that could be done since I never really got a chance to go back and try to optimize what I'm doing.\nIt would be fun to add support for ERC-1155s and for CryptoPunks. I think that this could be a cool thing for NFT (Profile Pic)-based DAO-like groups like Bored Ape Yacht Club members to mint Family Photos of groups of members to show that they all have \"iron hands\" and will never get rid of their NFTs. Given that might be a cool use-case for this project, it would be cool to be able for the original minting user to specify a given dimension of the resulting image so they can determine how they want to use it ahead of time - for example, your Twitter banner requires a different image ratio than something like a profile picture.\nI also wanted to get into using Filecoin instead of IPFS but didn't have time to get that running. Ideally if you're storing images to NFTs like this you have something safely persistent other than IPFS. On a related note, in a real deployment you'd probably want NFT-contributing users to send LINK/FIL or payment to convert to LINK and FIL along when they add their NFT to a Family Photoin order to cover future costs of maintaining the NFTs.\nIn my mind, the top really cool enhancements to this project are: 1) Cross-chain pointers via CCIP - since my NFTs are just storing pointers to NFTs, why not deploy this contract on a less gas-intensive L1 or L2 like Avalanche or Arbitrum and just point to NFTs on Ethereum mainnet? Most of the big NFT projects at this point are on mainnet but gas costs would be too prohibitive to allow this project to really work there. Since my EA is ultimately handling the work to handle my pointers and image manipulation, I wonder if I could use it to point across multiple chains too. 2) Think of something like GIMP as the image manipulation tool here rather than simply tiling out the image URIs. Given a really cool image manipulation layer on top of the frontend, you could do really cool collages of NFTs all mashed on top of each other in unique ways that then still black out if a user moves them.", "link": "https://devpost.com/software/family-photo", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ni'm pretty sure someone on nft twitter put forth an idea for a \"meta nft\" that would work similar to the gimp example on the bottom of this. i was thinking about that a bit and thought that chainlink tools would at least be able to serve as a proof of concept for something like that.\nwhat it does\nthis project allows a user to create a family photo nft (fam). a family photo is an erc-721 where each token stores pointers to other erc-721 nfts. a user will mint a new fam which opens it up for that user or other users to \"add\" their own nfts to it. \"add\" being used loosely here - there isn't any actual token transfer. at some point, the minting user can decide to \"finalize\" the fam they minted, which will generate an image for that fam based on all the nfts added to it. this is done via a custom chainlink external adapter currently deployed to kovan (until my linkpool node expires) right now mainly ipfs image uri, urls, or json are supported ways to find that image. then, if a person who added their nft to a fam ever moves that nft from their wallet, the fam will automatically regenerate a new image with that nft blacked out (well - darkened out). this is triggered via a keeper job currently running on kovan.\nhow we built it\ni built this using solidity, python, and react and used tools like brownie (vscode), aws lambda, linkpool running my external adapter, and chainlink keepers on kovan. i used plenty of example code from the solidity beginner to expert freecodecamp, various chainlink tutorials, and various stackoverflow posts.\nchallenges we ran into\nthis was my first real smart contract project so there was a bit of a learning curve to get proper syntax and structuring to work throughout the project.\nsome interesting challenges were getting large responses to work with my chainlink node and thus getting my external adapter to properly trigger and handle my token uris (chainlink \"large responses\" tutorial helped a lot with this). in addition, it was tough to get my keeper job to handle an array of data but i found that linkpool's keeper job on mainnet was actually doing something similar to what i wanted to do so i was able to copy their approach.\naccomplishments that we're proud of\ni felt great about even finishing this project since this was my first time doing this. and getting an external adapter and a keeper job up and running :).\nwhat we learned\ni learned a lot about the structure of nfts and about how powerful external adapter jobs can be since they're just running a bunch of python in the cloud - you can pretty much do anything and that's very cool.\nwhat's next for family photo\nthere's a lot of cool things that could be done to enhance this project. to start, there are definitely gas optimizations that could be done since i never really got a chance to go back and try to optimize what i'm doing.\nit would be fun to add support for erc-1155s and for cryptopunks. i think that this could be a cool thing for nft (profile pic)-based dao-like groups like bored ape yacht club members to mint family photos of groups of members to show that they all have \"iron hands\" and will never get rid of their nfts. given that might be a cool use-case for this project, it would be cool to be able for the original minting user to specify a given dimension of the resulting image so they can determine how they want to use it ahead of time - for example, your twitter banner requires a different image ratio than something like a profile picture.\ni also wanted to get into using filecoin instead of ipfs but didn't have time to get that running. ideally if you're storing images to nfts like this you have something safely persistent other than ipfs. on a related note, in a real deployment you'd probably want nft-contributing users to send link/fil or payment to convert to link and fil along when they add their nft to a family photoin order to cover future costs of maintaining the nfts.\nin my mind, the top really cool enhancements to this project are: 1) cross-chain pointers via ccip - since my nfts are just storing pointers to nfts, why not deploy this contract on a less gas-intensive l1 or l2 like avalanche or arbitrum and just point to nfts on ethereum mainnet? most of the big nft projects at this point are on mainnet but gas costs would be too prohibitive to allow this project to really work there. since my ea is ultimately handling the work to handle my pointers and image manipulation, i wonder if i could use it to point across multiple chains too. 2) think of something like gimp as the image manipulation -----> tool !!!  here rather than simply tiling out the image uris. given a really cool image manipulation layer on top of the frontend, you could do really cool collages of nfts all mashed on top of each other in unique ways that then still black out if a user moves them.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59503907}, {"Unnamed: 0": 3919, "autor": "Proof-of-Deposit: A Risk-Free Rate on Stablecoins", "date": null, "content": "What is Proof-of-Deposit?\nProof-of-Deposit is a game-changing innovation that gives L1 blockchains with built-in stablecoin mechanisms a competitive advantage that is unavailable to other blockchains.\nThe core idea behind Proof-of-Deposit is simple but profound:\nWhat if block rewards can be paid on stablecoins? (e.g. cUSD and cEUR)\nThe significance of this lies in the fact that block rewards comprimise minted CELO and fees from the entire on-chain economy! This will create a significant market for stablecoins (if not the biggest one by far!).\nProof-of-Deposit achieves this by allowing the \"staking\" of, not one, but multiple tokens. One of those tokens must be CELO whose value is strongly coupled to the network (to serve as strong Sybil-defence), but the other tokens can be stablecoins! When block rewards are distributed, it creates in-effect a \"risk-free\" rate on stablecoins.\nBenefits of Proof-of-Deposit\nProof-of-Deposit brings significant benefits that complements Celo's philosophy of providing accessible, mobile-first payments in a token that maintains stable purchasing power:\nTapping into the biggest and most important userbase: those that cannot or do not want take risk with their money such as regulated custodian, banks and risk-averse individuals.\nThese users, by increasing the liquidity of the stablecoins, help to stablise the peg as the liquidity serves as a buffer when sell-offs occur. The reason that these users will hold cUSD, cEUR, etc is due to the APY offered by Proof-of-Deposit (users will deposit if they deem APY to be attractive, and withdraw if they deem it too low).\nAmplifying demand for CELO and stablecoins\nUnlike lending protocols, Proof-of-Deposit gives a \"risk-free\" rate where there is no risk of losing your principal. Unlike swap protocols, Proof-of-Deposit pays rewards from newly minted CELO (as opposed to a separate token) and from a transaction fees paid by the entire on-chain economy.\nAs users who deposit stablecoins earn a passive income paid in CELO, this creates a positive feedback loop:\ni. CELO price increase -> APY on stablecoin increases\nii. Demand for stablecoin increases (to deposit in order to receive the APY) -> CELO price increases (due to CELO being used as collateral for stablecoin mechanism)\nA new tool for central banks: monetary policy with configurable discretion\nBy providing a risk-free rate in a decentralised setting, Proof-of-Deposit enables central banks to choose to limit their discretion over monetary policy. Remarkably, (and perhaps counterintuitively) choosing to limit discretion in this way can greatly strengthen the de facto independence of a central bank. Benefits to strong independence include:\ni. Enhanced credibility (particularly valuable for e.g. emerging economies, as it may permit borrowing in a currency that they issue (on affordable terms).\nii. Incomplete independence has been shown to cause a down trend in interest rates, which leads to rates eventually hitting zero, whereupon the effectiveness of monetary policy is severely impaired (see our paper summary for detail).\nOverview of Implemented Functionality\nSmart Contracts (located here)\nFor this hackathon, we sought to showcase how Proof-of-Deposit could be integrated with Celo. To achieve this, we isolated and made modifications to the following Celo Governance Smart Contracts:\nLockedGold.sol (our abstract version is called LockedToken.sol)\ni. Modified lock and withdraw to be compatible with any ERC20 token (e.g. cUSD, and cEUR)\nii. Modified the constructor to LockedToken to be instantiated with a particular ERC20 token and other initial parameters\niii. Removed unnecessary dependencies for our proof of concept\nElection.sol\ni. Modified the constructor to allow Election.sol to be instantiated with multiple LockedTokens (e.g. our proof-of-concept uses LockedCGLD, LockedCUSD and LockedCEUR) along with other initial parameters.\nii. Extended most class variables to be mappings where the key is a LockedToken address (such as LockedCGLD, LockedCUSD, etc)\niii. Modified almost all functions to include a address token parameter, where a specific LockedToken address needs to be provided. (e.g. function vote(address token, address group, uint256 value, ...) needs to define which LockedToken is being used to vote)\niv. Implemented functions to introduce \"normalised votes\" (e.g. getGroupTotalVotesNormalised(address token, address group)), where a normalised vote is the fraction of votes from a particular LockedToken for a validator.\nv. Implemented functions to introduce \"influence\" (e.g. getGroupInfluenceFromTotalVotes(address group)), where influence is calculated as the min over all normalised votes from different LockedTokens for a validator.\nvi. Modified distributeEpochRewards so that we can showcase how epoch rewards are distributed amongst multiple LockTokens.\nvii. Removed unnecessary dependencies for our proof of concept\nWebApp (located here)\nWe used plock.fi as a \"SDK\" to showcase a possible UI to interact with Proof-of-Deposit:\nProof-of-Deposit overview page that shows all of a user's Lockable tokens, and their respective stats (e.g. average APY):\nLockable token management page that allows a user to Lock/Unlock/Withdraw their tokens, Vote with Locked Tokens, as well as see validator group stats (e.g. APY):\nAnalysis (located here)\nWe formally analysed the increased demand for CELO and stablecoins that Proof-of-Deposit\nYou can view a forecast of the marketcap calculated by our model on our live WebApp.\nNext Steps\nFor this hackathon we have implemented a proof of concept version to showcase our tech, and have provided preliminary analysis. Next steps are divided between mainnet deployment and CBDC routes:\nMainnet Deployment\nTowards the goal of getting this live:\nStringent analysis on the economic and security implications of our tech (e.g. contracting https://chaoslabs.xyz/)\nThorough investigation of breaking changes in the wider ecosystem that can come from modifying Election.sol and LockedGold.sol (e.g. other smart contracts, wallets, celo-tools, etc).\nCollaborate with cLabs and Mysten Labs to test/architect/develop our tech with a clear phase-in plan. (Unclear if modification/integration of existing governance smart contracts is best way forward)\nSeek to modularise the \"risk-free\" rate tech such that it can be used for simulations (e.g. economic analysis) as well as potentially packaging it into a marketable product.\nCBDC\nContinue our collaboration with economists at IMF, ESM and BoE Advisory Group to produce papers on how blockchains allow for monetary policy with configurable discretion.\nCollaborate with CBDC team at cLabs\nLots of presentations and talks... (its the long game with Central Banks)\nURLs\nOur Github\nLive WebApp\nVideo Demonstration\nPresentation Slides\nOur Whitepaper - A Market Determined Risk Free Rate\nOur Analysis - A MarketCap Model for Proof-of-Deposit\nOur Summary on CBDCs - Monetary Policy with Configurable Discretion", "link": "https://devpost.com/software/proof-of-deposit", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what is proof-of-deposit?\nproof-of-deposit is a game-changing innovation that gives l1 blockchains with built-in stablecoin mechanisms a competitive advantage that is unavailable to other blockchains.\nthe core idea behind proof-of-deposit is simple but profound:\nwhat if block rewards can be paid on stablecoins? (e.g. cusd and ceur)\nthe significance of this lies in the fact that block rewards comprimise minted celo and fees from the entire on-chain economy! this will create a significant market for stablecoins (if not the biggest one by far!).\nproof-of-deposit achieves this by allowing the \"staking\" of, not one, but multiple tokens. one of those tokens must be celo whose value is strongly coupled to the network (to serve as strong sybil-defence), but the other tokens can be stablecoins! when block rewards are distributed, it creates in-effect a \"risk-free\" rate on stablecoins.\nbenefits of proof-of-deposit\nproof-of-deposit brings significant benefits that complements celo's philosophy of providing accessible, mobile-first payments in a token that maintains stable purchasing power:\ntapping into the biggest and most important userbase: those that cannot or do not want take risk with their money such as regulated custodian, banks and risk-averse individuals.\nthese users, by increasing the liquidity of the stablecoins, help to stablise the peg as the liquidity serves as a buffer when sell-offs occur. the reason that these users will hold cusd, ceur, etc is due to the apy offered by proof-of-deposit (users will deposit if they deem apy to be attractive, and withdraw if they deem it too low).\namplifying demand for celo and stablecoins\nunlike lending protocols, proof-of-deposit gives a \"risk-free\" rate where there is no risk of losing your principal. unlike swap protocols, proof-of-deposit pays rewards from newly minted celo (as opposed to a separate token) and from a transaction fees paid by the entire on-chain economy.\nas users who deposit stablecoins earn a passive income paid in celo, this creates a positive feedback loop:\ni. celo price increase -> apy on stablecoin increases\nii. demand for stablecoin increases (to deposit in order to receive the apy) -> celo price increases (due to celo being used as collateral for stablecoin mechanism)\na new -----> tool !!!  for central banks: monetary policy with configurable discretion\nby providing a risk-free rate in a decentralised setting, proof-of-deposit enables central banks to choose to limit their discretion over monetary policy. remarkably, (and perhaps counterintuitively) choosing to limit discretion in this way can greatly strengthen the de facto independence of a central bank. benefits to strong independence include:\ni. enhanced credibility (particularly valuable for e.g. emerging economies, as it may permit borrowing in a currency that they issue (on affordable terms).\nii. incomplete independence has been shown to cause a down trend in interest rates, which leads to rates eventually hitting zero, whereupon the effectiveness of monetary policy is severely impaired (see our paper summary for detail).\noverview of implemented functionality\nsmart contracts (located here)\nfor this hackathon, we sought to showcase how proof-of-deposit could be integrated with celo. to achieve this, we isolated and made modifications to the following celo governance smart contracts:\nlockedgold.sol (our abstract version is called lockedtoken.sol)\ni. modified lock and withdraw to be compatible with any erc20 token (e.g. cusd, and ceur)\nii. modified the constructor to lockedtoken to be instantiated with a particular erc20 token and other initial parameters\niii. removed unnecessary dependencies for our proof of concept\nelection.sol\ni. modified the constructor to allow election.sol to be instantiated with multiple lockedtokens (e.g. our proof-of-concept uses lockedcgld, lockedcusd and lockedceur) along with other initial parameters.\nii. extended most class variables to be mappings where the key is a lockedtoken address (such as lockedcgld, lockedcusd, etc)\niii. modified almost all functions to include a address token parameter, where a specific lockedtoken address needs to be provided. (e.g. function vote(address token, address group, uint256 value, ...) needs to define which lockedtoken is being used to vote)\niv. implemented functions to introduce \"normalised votes\" (e.g. getgrouptotalvotesnormalised(address token, address group)), where a normalised vote is the fraction of votes from a particular lockedtoken for a validator.\nv. implemented functions to introduce \"influence\" (e.g. getgroupinfluencefromtotalvotes(address group)), where influence is calculated as the min over all normalised votes from different lockedtokens for a validator.\nvi. modified distributeepochrewards so that we can showcase how epoch rewards are distributed amongst multiple locktokens.\nvii. removed unnecessary dependencies for our proof of concept\nwebapp (located here)\nwe used plock.fi as a \"sdk\" to showcase a possible ui to interact with proof-of-deposit:\nproof-of-deposit overview page that shows all of a user's lockable tokens, and their respective stats (e.g. average apy):\nlockable token management page that allows a user to lock/unlock/withdraw their tokens, vote with locked tokens, as well as see validator group stats (e.g. apy):\nanalysis (located here)\nwe formally analysed the increased demand for celo and stablecoins that proof-of-deposit\nyou can view a forecast of the marketcap calculated by our model on our live webapp.\nnext steps\nfor this hackathon we have implemented a proof of concept version to showcase our tech, and have provided preliminary analysis. next steps are divided between mainnet deployment and cbdc routes:\nmainnet deployment\ntowards the goal of getting this live:\nstringent analysis on the economic and security implications of our tech (e.g. contracting https://chaoslabs.xyz/)\nthorough investigation of breaking changes in the wider ecosystem that can come from modifying election.sol and lockedgold.sol (e.g. other smart contracts, wallets, celo-tools, etc).\ncollaborate with clabs and mysten labs to test/architect/develop our tech with a clear phase-in plan. (unclear if modification/integration of existing governance smart contracts is best way forward)\nseek to modularise the \"risk-free\" rate tech such that it can be used for simulations (e.g. economic analysis) as well as potentially packaging it into a marketable product.\ncbdc\ncontinue our collaboration with economists at imf, esm and boe advisory group to produce papers on how blockchains allow for monetary policy with configurable discretion.\ncollaborate with cbdc team at clabs\nlots of presentations and talks... (its the long game with central banks)\nurls\nour github\nlive webapp\nvideo demonstration\npresentation slides\nour whitepaper - a market determined risk free rate\nour analysis - a marketcap model for proof-of-deposit\nour summary on cbdcs - monetary policy with configurable discretion", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59503919}, {"Unnamed: 0": 3924, "autor": "Certi.io", "date": null, "content": "Inspiration\nOur inspiration is to offer people the opportunity to use the Celo Blockchain and Wallets as a security tool to register and validate signatures upon documents hashes from a user friendly interface. Some of the problems we aim to solve: -The lack of systems for cryptographic signatures validations globally. -The lack of evidence that certain data exists at a particular point in time (proof of existence). -The lack of integrity mechanisms to protect data tampering. In the other hand on economic terms the digital signature market has a CAGR of 21,5% with USA as largest market cap and Europe as the fastest in growing and Docusign as the biggest market player, so we wanted to offer a differential factor by using Blockchain and decentralization as a security enhancer.\nWhat it does\nWe aim to help people harness the power of cryptography algorithms like hashes and private key signatures to protect data integrity and traceability. Our solution offer users a friendly experience , and a incredible range of possible use cases, from notary for ownership certificates, legal agreements, academic certificates, supply chain traceability and much more.\nHow we built it\nWe used solidity smart contracts that can be deployed on any EVM Blockchain like Celo, and connected to a front end interface. We integrated our solution with Web3 Wallets in order to guarantee the final user the full decentralization and sovereignty over their signatures and data, and make possible for other users to validate the signature certificates and document validations easily.\nChallenges we ran into\nIdentifying the elemental functionalities to help people apply Blockchain encryption tools for their own use cases on a simple and effective way. We integrated smart contracts with user interfaces making a Dapp available trough any web3 explorer, offering autonomy and decentralization on information security.\nAccomplishments that we're proud of\nWe bought together four years of experience working on Dapp development with different industrial sector and identifies that these solution could be a foundation for solving a transversal problem of any industry, an information security and traceability management.\nWhat we learned\nWe learned the importance of building products with a core problem like information security to offer the most wide range of possibilities and use cases on hands of final users.\nWhat's next for Certi.io\nWe want to help people discover new use cases and integrate Certi.io to other systems as a security layer. We aim to bring new users education and example use cases to help people identify how Certi.io can transform their processes and business.", "link": "https://devpost.com/software/certibits", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nour inspiration is to offer people the opportunity to use the celo blockchain and wallets as a security -----> tool !!!  to register and validate signatures upon documents hashes from a user friendly interface. some of the problems we aim to solve: -the lack of systems for cryptographic signatures validations globally. -the lack of evidence that certain data exists at a particular point in time (proof of existence). -the lack of integrity mechanisms to protect data tampering. in the other hand on economic terms the digital signature market has a cagr of 21,5% with usa as largest market cap and europe as the fastest in growing and docusign as the biggest market player, so we wanted to offer a differential factor by using blockchain and decentralization as a security enhancer.\nwhat it does\nwe aim to help people harness the power of cryptography algorithms like hashes and private key signatures to protect data integrity and traceability. our solution offer users a friendly experience , and a incredible range of possible use cases, from notary for ownership certificates, legal agreements, academic certificates, supply chain traceability and much more.\nhow we built it\nwe used solidity smart contracts that can be deployed on any evm blockchain like celo, and connected to a front end interface. we integrated our solution with web3 wallets in order to guarantee the final user the full decentralization and sovereignty over their signatures and data, and make possible for other users to validate the signature certificates and document validations easily.\nchallenges we ran into\nidentifying the elemental functionalities to help people apply blockchain encryption tools for their own use cases on a simple and effective way. we integrated smart contracts with user interfaces making a dapp available trough any web3 explorer, offering autonomy and decentralization on information security.\naccomplishments that we're proud of\nwe bought together four years of experience working on dapp development with different industrial sector and identifies that these solution could be a foundation for solving a transversal problem of any industry, an information security and traceability management.\nwhat we learned\nwe learned the importance of building products with a core problem like information security to offer the most wide range of possibilities and use cases on hands of final users.\nwhat's next for certi.io\nwe want to help people discover new use cases and integrate certi.io to other systems as a security layer. we aim to bring new users education and example use cases to help people identify how certi.io can transform their processes and business.", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 2, "media": null, "medialink": null, "identifyer": 59503924}, {"Unnamed: 0": 3930, "autor": "The Conscious Investor", "date": null, "content": "We aim to empower retail investors today who don't have a way of evaluating their portfolios and getting recommendations on which stocks to invest in based on Environmental, Social, and Governance (ESG) metrics they care about. Our solution would help bring transparency to investing in companies and increase access to an intelligent tool that is customized per investor. Our tool would help customers consider ESG metrics in their investment strategy that often go unmeasured. Our solution will cater to investor-specific beliefs to help mitigate risks and provide a higher return on their investment. We also aim to influence companies to make more social, environment-friendly decisions.", "link": "https://devpost.com/software/the-conscious-investor", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "we aim to empower retail investors today who don't have a way of evaluating their portfolios and getting recommendations on which stocks to invest in based on environmental, social, and governance (esg) metrics they care about. our solution would help bring transparency to investing in companies and increase access to an intelligent -----> tool !!!  that is customized per investor. our tool would help customers consider esg metrics in their investment strategy that often go unmeasured. our solution will cater to investor-specific beliefs to help mitigate risks and provide a higher return on their investment. we also aim to influence companies to make more social, environment-friendly decisions.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59503930}, {"Unnamed: 0": 3996, "autor": "Deep Price Learning", "date": null, "content": "SEE LAST GOOGLE DOC BELOW FOR COMPLETE WRITTEN EVALUATION\nFIRST LINK: Planning Document/1st Reflection\nSECOND LINK: Github Repository\nTHIRD LINK 2nd Reflection\nFourth LINK: Final Written Evaluation\nDeep Price Learning: Written Evaluation Predicting Price Trends Using Deep Learning\nGroup Members: Henry Pasts (hpasts), Justinian Vladutiu (jvladuti), and Alex Wang (awang199)\nIntroduction Predicting stock prices has been an undertaking of extensive research because of the real quantitative reward of profit if a successful model is deployed in real time. We have built two frameworks in which to analyze and predict price trends, one that is a pure LSTM approach and one that combines LSTM with a CNN for feature extraction. We build off the work in \u201cDeep Learning for Price Movement Prediction Using Convolutional Neural Network and Long Short-Term Memory\u201d and \u201cA CNN-LSTM-Based Model to Forecast Stock Prices\u201d, and deploy the latter paper\u2019s CNN-LSTM model. We compare the results of our LSTM Only Model with the author\u2019s CNN-LSTM model to see if adding feature extraction improves performance.\nMethodology Data/Preprocessing The authors used the Shanghai Composite Index in their CNN-LSTM implementation, totaling 7127 trading days with the following measures of price: Open, High, Low, and Close. For our experiments, we use the S&P 500, an index measuring the value of 500 large companies listed on the U.S. stock exchanges. We used 5518 trading days going from 01/03/2000 to 12/06/2021, allocating the first 65% of the dataset to train data and predicting the last 35% of data.\nWe preprocessed our data in a similar fashion as the authors. First, we batched the train and test data into x_train and y_train for our four price streams and used rolling periods of 100 days, where the corresponding label for the batch was the next price after the 100 days. Subsequently, we used a min-max scaler to normalize the data between 0 and 1 for training and then inverted the predictions with the same scaler for graphing purposes. We used the uninverted results in our metrics when evaluating the performance of the model.\nModel Architecture\nModel 1: LSTM Only Model\nSee Model in Last Google Doc Link below.\nThis model trains and makes predictions for the Open, High, Low, and Close price of the next day independently without extracting features from their interplay/relationships. We use this baseline model to compare its results to the Author\u2019s CNN-LSTM model to see if adding feature extraction improves prediction power.\nModel 2: CNN-LSTM Model\nSee Model in Last Google Doc Link below.\nThis model was taken from \u201cA CNN-LSTM-Based Model to Forecast Stock Prices'' with minor modifications. It uses a 1D convolution to extract the features of the Open, High, Low, and Close prices and then passes that information to an LSTM model with 4 layers, the final of which outputs 4 numbers, which represent the predictions for the 4 values. Instead of passing in and predicting each data stream independently, as in Model 1, it attempts to find features between the Open, High, Low, and Close price for each day to guide its predictions.\nResults Model 1: LSTM Only Model\nChart Predictions\nSee Charts in Last Google Doc Link below\nMetrics\nMetrics Comparing the Magnitudes of Y and Y_hat\nOpen High Low Close Root Mean Squared Error 247.8712 230.8958 241.2267 239.7745 Mean Absolute Percentage Error 6.3983 5.9590 6.2777 6.2399\nMetrics to Analyze Predicted Price Movement of Next Day (price up or down)\nOpen High Low Close Precision 0.5835 0.5462 0.5848 0.5542 Recall 0.5863 0.5467 0.5876 0.5576 Accuracy 0.5230 0.5010 0.5186 0.5137 F-Measure 0.5849 0.5464 0.5862 0.5559\nCandlesticks of Predicted Open, High, Low, and Close S&P 500 Index 8/28/2014-12/06/2021\nSee Charts in Last Google Doc Link below\nCandlesticks of Actual Open, High, Low, and Close S&P 500 Index 8/28/2014-12/06/2021\nSee Charts in Last Google Doc Link below\nModel 2: CNN-LSTM Model\nChart Predictions\nSee Charts in Last Google Doc Link below\nMetrics\nMetrics Comparing the Magnitudes of Y and Y_hat\nOpen High Low Close Root Mean Squared Error 185.6400 125.0200 128.0178 130.5730 Mean Absolute Percentage Error 3.2366 2.774 3.2967 4.0926\nMetrics to Analyze Predicted Price Movement of Next Day (price up or down)\nOpen High Low Close Precision 0.5770 0.5503 0.5729 0.5467 Recall 0.6530 0.6524 0.6359 0.656 Accuracy 0.5270 0.5156 0.5134 0.5150 F-Measure 0.6127 0.5970 0.5134 0.5964\nCandlesticks of Predicted Open, High, Low, and Close S&P 500 Index 8/28/2014-12/06/2021\nSee Charts in Last Google Doc Link below\nCandlesticks of Actual Open, High, Low, and Close S&P 500 Index 8/28/2014-12/06/2021\nSee Charts in Last Google Doc Link below\nDiscussion/Future Work The Root Mean Squared Error (RMSE) The first set of metrics (RMSE and MAPE) are historically used to analyze the efficacy of forecasting time series data. RMSE measures our prediction error in absolute terms on average. We can see that in both the LSTM Only Model and the CNN-LSTM Model, we had the lowest error when predicting the next High price of the S&P 500. Across the board, the CNN-LSTM Model does substantially better, producing errors for High, Open, Low, and Close that are 50% less as compared to those from the LSTM Only Model.\nMean Absolute Percentage Error (MAPE) MAPE measures how far off each prediction is, on average, from the actual value in terms of a percentage. Our LSTM Only Model was off around 7% on average for each of the price types, but this may have been an issue in our model design (see challenges section below). The CNN-LSTM Model did much better than this, and it cut the average percent off from the predictions and the actual price types by roughly 50%. The MAPE gives a more standardized way to view prediction errors by giving them in percentages, thus considering the relative differences that the RMSE does not.\nPrecision, Recall, Accuracy, F-Measure The context of these results needs clarification. We created a custom binary classifier for our test data and predictions, marking a 1 if price was greater than the previous day and 0 if price was less than the previous day. The first price for both our test data and predictions was not considered, as there would be no data point to compare it to. We subsequently ran our Precision, Recall, and Accuracy metrics on the difference between the up and down movements of the actual price vs. our model\u2019s predictions. Essentially, even if the model cannot predict the exact next price, if it can predict the direction of the next price, it would be an invaluable tool for analyzing markets. We found that we could predict the direction of the next day slightly better than random (50%) for both our LSTM Only and CNN-LSTM Models. For both models, our prediction accuracy for the Open price was the best at 52.3% for LSTM Only and 52.7% for CNN-LSTM. In terms of Precision, Recall, and F-Measure, we get good Precision (54% or higher for both models with CNN-LSTM slightly outperforming) indicating that we are fairly accurate in predicting positive price action and our Recall rate for both models (54% or higher for LSTM Only and 63% or higher for CNN-LSTM) indicates that we are successful in finding all the true positives (increasing in price). This is intuitive because our graph has an upward bias. Finally, our F-Measure, which considers both precision and recall in a more equal fashion, not prioritizing either one, has more variance across our tests.\nChallenges\nLSTM Only Model Predictions Shifted Right One of the major challenges we faced was in our baseline LSTM Only Model, where our predictions seem to be delayed to the right. We built the model architecture manually while still using tf.keras.Sequential, but with our own call, loss, train, and test functions. It is possible that there is a slight error in this model architecture that we plan on fixing as we do more research and potentially deploy working strategies in real time. We did not experience this shifting of predictions as much in the CNN-LSTM Model where we used the Keras API to handle model compiling, fitting, and predicting.\nCNN-LSTM Smoothing Issue in Candlestick Chart Our CNN-LSTM model seems to \u2018smooth\u2019 out the data and applies different directional bias to Open, High, Low, and Close, thus reducing the consistency of these metrics. This results in candlestick charts that are rather uninformative because we cannot guarantee that the predicted High is in fact the High for that day, as the Low price may have a higher directional bias that compounds past the High price. The result is that ALL Low prices are predicted to be higher than the High price for each day, resulting in the candlestick graph above. We saw some of this effect in the LSTM Only Model, but to a lesser extent. Future work may consider not running the CNN Model on the Open, High, Low, and Close all together to see if this directional bias can be removed.\nComputational Resources In addition to the sheer amount of debugging that went into the construction of these models, we had to be very careful in our model design because our computational resources are limited and LSTMs are notoriously difficult to train on a local device. Thankfully, we had powerful GPUs that sped up the process, but on an industrial scale we would use AWS GPU or TPU farms for training to drastically reduce time spent training. Thus, dealing with our technological limitations was another challenge for this project specifically.\nReflection\nHow the Project Turned Out We believe we have some solid groundwork done in terms of moving forward to creating a successful Deep Learning Model that can predict stock prices. Our current work with CNN-LSTMs is encouraging. In addition to showing that the addition of feature extraction with a CNN improves model efficacy, thus confirming the conclusions of the authors of \u201cA CNN-LSTM-Based Model to Forecast Stock Prices.\u201d Although predicting the direction of the next Open price or Close price with 51% or 52% accuracy does not seem like much, over long periods of time such a strategy has the potential to result in many profitable trades that could outweigh transaction costs and losing trades.\nModel Results vs. Expectations In general, we were pleased with the results of the CNN-LSTM Model and its improvement in performance against the LSTM Only Model. Initial tests solely using Close price showed us that it was possible to predict the direction of price with some level of accuracy better than randomly guessing, thus we expected our model to improve when taking into account the Open, High, and Low prices in addition to the Close price, since there is much to learn from these values and their relations each day.\nPivots and Things We Would Have Done Differently During the course of our project, we had to pivot immensely. Although we drew a lot from the experience of attempting to implement the first paper we considered, \u201cDeep Learning for Price Movement Prediction Using Convolutional Neural Network and Long Short-Term Memory\u201d, we ultimately chose to focus on the model architecture in \u201cA CNN-LSTM-Based Model to Forecast Stock Prices\u201d because of its emphasis on candle prediction and clearer model structure. This allowed us to pivot and implement a powerful CNN-LSTM model. In hindsight, we should have pivoted much earlier in the process, but were able to implement a meaningful model with meaningful results in line with the authors.\nFuture Improvements Current implementation does not consider the order of the High and Low prices. It may be relevant to learn how the Open and Close are affected by the order of the High and Low price for a given day\u2019s candle. We wonder if including transformers and which of these events comes first may improve the model\u2019s ability to predict future recurring intraday price movements.\nTakeaways After this project, we have a much bigger appreciation for how hard it can be to extend deep learning to other fields, such as finance. We would not have necessarily thought of trying to use LSTMs ability to store both long and short term memory to predict prices without extensive research from existing papers. Moreover, a big takeaway was that these Deep Learning techniques and models seem to have great potential for financial applications such as algorithmic trading. However, careful thought must be taken in the model creation process and we must consider the lessons learned from previous modeling architectures. The biggest lesson learned from the CNN-LSTM models was that the predictions generated may not be consistent within our context of the High price being greater than the Low price, thus hindering our ability to predict each metric in a way consistent with reality. Thus, it may be best to predict these metrics independently and only predict one of them at a time, as compared to all at once.", "link": "https://devpost.com/software/deep-price-learning", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "see last google doc below for complete written evaluation\nfirst link: planning document/1st reflection\nsecond link: github repository\nthird link 2nd reflection\nfourth link: final written evaluation\ndeep price learning: written evaluation predicting price trends using deep learning\ngroup members: henry pasts (hpasts), justinian vladutiu (jvladuti), and alex wang (awang199)\nintroduction predicting stock prices has been an undertaking of extensive research because of the real quantitative reward of profit if a successful model is deployed in real time. we have built two frameworks in which to analyze and predict price trends, one that is a pure lstm approach and one that combines lstm with a cnn for feature extraction. we build off the work in \u201cdeep learning for price movement prediction using convolutional neural network and long short-term memory\u201d and \u201ca cnn-lstm-based model to forecast stock prices\u201d, and deploy the latter paper\u2019s cnn-lstm model. we compare the results of our lstm only model with the author\u2019s cnn-lstm model to see if adding feature extraction improves performance.\nmethodology data/preprocessing the authors used the shanghai composite index in their cnn-lstm implementation, totaling 7127 trading days with the following measures of price: open, high, low, and close. for our experiments, we use the s&p 500, an index measuring the value of 500 large companies listed on the u.s. stock exchanges. we used 5518 trading days going from 01/03/2000 to 12/06/2021, allocating the first 65% of the dataset to train data and predicting the last 35% of data.\nwe preprocessed our data in a similar fashion as the authors. first, we batched the train and test data into x_train and y_train for our four price streams and used rolling periods of 100 days, where the corresponding label for the batch was the next price after the 100 days. subsequently, we used a min-max scaler to normalize the data between 0 and 1 for training and then inverted the predictions with the same scaler for graphing purposes. we used the uninverted results in our metrics when evaluating the performance of the model.\nmodel architecture\nmodel 1: lstm only model\nsee model in last google doc link below.\nthis model trains and makes predictions for the open, high, low, and close price of the next day independently without extracting features from their interplay/relationships. we use this baseline model to compare its results to the author\u2019s cnn-lstm model to see if adding feature extraction improves prediction power.\nmodel 2: cnn-lstm model\nsee model in last google doc link below.\nthis model was taken from \u201ca cnn-lstm-based model to forecast stock prices'' with minor modifications. it uses a 1d convolution to extract the features of the open, high, low, and close prices and then passes that information to an lstm model with 4 layers, the final of which outputs 4 numbers, which represent the predictions for the 4 values. instead of passing in and predicting each data stream independently, as in model 1, it attempts to find features between the open, high, low, and close price for each day to guide its predictions.\nresults model 1: lstm only model\nchart predictions\nsee charts in last google doc link below\nmetrics\nmetrics comparing the magnitudes of y and y_hat\nopen high low close root mean squared error 247.8712 230.8958 241.2267 239.7745 mean absolute percentage error 6.3983 5.9590 6.2777 6.2399\nmetrics to analyze predicted price movement of next day (price up or down)\nopen high low close precision 0.5835 0.5462 0.5848 0.5542 recall 0.5863 0.5467 0.5876 0.5576 accuracy 0.5230 0.5010 0.5186 0.5137 f-measure 0.5849 0.5464 0.5862 0.5559\ncandlesticks of predicted open, high, low, and close s&p 500 index 8/28/2014-12/06/2021\nsee charts in last google doc link below\ncandlesticks of actual open, high, low, and close s&p 500 index 8/28/2014-12/06/2021\nsee charts in last google doc link below\nmodel 2: cnn-lstm model\nchart predictions\nsee charts in last google doc link below\nmetrics\nmetrics comparing the magnitudes of y and y_hat\nopen high low close root mean squared error 185.6400 125.0200 128.0178 130.5730 mean absolute percentage error 3.2366 2.774 3.2967 4.0926\nmetrics to analyze predicted price movement of next day (price up or down)\nopen high low close precision 0.5770 0.5503 0.5729 0.5467 recall 0.6530 0.6524 0.6359 0.656 accuracy 0.5270 0.5156 0.5134 0.5150 f-measure 0.6127 0.5970 0.5134 0.5964\ncandlesticks of predicted open, high, low, and close s&p 500 index 8/28/2014-12/06/2021\nsee charts in last google doc link below\ncandlesticks of actual open, high, low, and close s&p 500 index 8/28/2014-12/06/2021\nsee charts in last google doc link below\ndiscussion/future work the root mean squared error (rmse) the first set of metrics (rmse and mape) are historically used to analyze the efficacy of forecasting time series data. rmse measures our prediction error in absolute terms on average. we can see that in both the lstm only model and the cnn-lstm model, we had the lowest error when predicting the next high price of the s&p 500. across the board, the cnn-lstm model does substantially better, producing errors for high, open, low, and close that are 50% less as compared to those from the lstm only model.\nmean absolute percentage error (mape) mape measures how far off each prediction is, on average, from the actual value in terms of a percentage. our lstm only model was off around 7% on average for each of the price types, but this may have been an issue in our model design (see challenges section below). the cnn-lstm model did much better than this, and it cut the average percent off from the predictions and the actual price types by roughly 50%. the mape gives a more standardized way to view prediction errors by giving them in percentages, thus considering the relative differences that the rmse does not.\nprecision, recall, accuracy, f-measure the context of these results needs clarification. we created a custom binary classifier for our test data and predictions, marking a 1 if price was greater than the previous day and 0 if price was less than the previous day. the first price for both our test data and predictions was not considered, as there would be no data point to compare it to. we subsequently ran our precision, recall, and accuracy metrics on the difference between the up and down movements of the actual price vs. our model\u2019s predictions. essentially, even if the model cannot predict the exact next price, if it can predict the direction of the next price, it would be an invaluable -----> tool !!!  for analyzing markets. we found that we could predict the direction of the next day slightly better than random (50%) for both our lstm only and cnn-lstm models. for both models, our prediction accuracy for the open price was the best at 52.3% for lstm only and 52.7% for cnn-lstm. in terms of precision, recall, and f-measure, we get good precision (54% or higher for both models with cnn-lstm slightly outperforming) indicating that we are fairly accurate in predicting positive price action and our recall rate for both models (54% or higher for lstm only and 63% or higher for cnn-lstm) indicates that we are successful in finding all the true positives (increasing in price). this is intuitive because our graph has an upward bias. finally, our f-measure, which considers both precision and recall in a more equal fashion, not prioritizing either one, has more variance across our tests.\nchallenges\nlstm only model predictions shifted right one of the major challenges we faced was in our baseline lstm only model, where our predictions seem to be delayed to the right. we built the model architecture manually while still using tf.keras.sequential, but with our own call, loss, train, and test functions. it is possible that there is a slight error in this model architecture that we plan on fixing as we do more research and potentially deploy working strategies in real time. we did not experience this shifting of predictions as much in the cnn-lstm model where we used the keras api to handle model compiling, fitting, and predicting.\ncnn-lstm smoothing issue in candlestick chart our cnn-lstm model seems to \u2018smooth\u2019 out the data and applies different directional bias to open, high, low, and close, thus reducing the consistency of these metrics. this results in candlestick charts that are rather uninformative because we cannot guarantee that the predicted high is in fact the high for that day, as the low price may have a higher directional bias that compounds past the high price. the result is that all low prices are predicted to be higher than the high price for each day, resulting in the candlestick graph above. we saw some of this effect in the lstm only model, but to a lesser extent. future work may consider not running the cnn model on the open, high, low, and close all together to see if this directional bias can be removed.\ncomputational resources in addition to the sheer amount of debugging that went into the construction of these models, we had to be very careful in our model design because our computational resources are limited and lstms are notoriously difficult to train on a local device. thankfully, we had powerful gpus that sped up the process, but on an industrial scale we would use aws gpu or tpu farms for training to drastically reduce time spent training. thus, dealing with our technological limitations was another challenge for this project specifically.\nreflection\nhow the project turned out we believe we have some solid groundwork done in terms of moving forward to creating a successful deep learning model that can predict stock prices. our current work with cnn-lstms is encouraging. in addition to showing that the addition of feature extraction with a cnn improves model efficacy, thus confirming the conclusions of the authors of \u201ca cnn-lstm-based model to forecast stock prices.\u201d although predicting the direction of the next open price or close price with 51% or 52% accuracy does not seem like much, over long periods of time such a strategy has the potential to result in many profitable trades that could outweigh transaction costs and losing trades.\nmodel results vs. expectations in general, we were pleased with the results of the cnn-lstm model and its improvement in performance against the lstm only model. initial tests solely using close price showed us that it was possible to predict the direction of price with some level of accuracy better than randomly guessing, thus we expected our model to improve when taking into account the open, high, and low prices in addition to the close price, since there is much to learn from these values and their relations each day.\npivots and things we would have done differently during the course of our project, we had to pivot immensely. although we drew a lot from the experience of attempting to implement the first paper we considered, \u201cdeep learning for price movement prediction using convolutional neural network and long short-term memory\u201d, we ultimately chose to focus on the model architecture in \u201ca cnn-lstm-based model to forecast stock prices\u201d because of its emphasis on candle prediction and clearer model structure. this allowed us to pivot and implement a powerful cnn-lstm model. in hindsight, we should have pivoted much earlier in the process, but were able to implement a meaningful model with meaningful results in line with the authors.\nfuture improvements current implementation does not consider the order of the high and low prices. it may be relevant to learn how the open and close are affected by the order of the high and low price for a given day\u2019s candle. we wonder if including transformers and which of these events comes first may improve the model\u2019s ability to predict future recurring intraday price movements.\ntakeaways after this project, we have a much bigger appreciation for how hard it can be to extend deep learning to other fields, such as finance. we would not have necessarily thought of trying to use lstms ability to store both long and short term memory to predict prices without extensive research from existing papers. moreover, a big takeaway was that these deep learning techniques and models seem to have great potential for financial applications such as algorithmic trading. however, careful thought must be taken in the model creation process and we must consider the lessons learned from previous modeling architectures. the biggest lesson learned from the cnn-lstm models was that the predictions generated may not be consistent within our context of the high price being greater than the low price, thus hindering our ability to predict each metric in a way consistent with reality. thus, it may be best to predict these metrics independently and only predict one of them at a time, as compared to all at once.", "sortedWord": "None", "removed": "Nan", "score": 5, "comments": 14, "media": null, "medialink": null, "identifyer": 59503996}, {"Unnamed: 0": 4048, "autor": "Show Me the Code", "date": null, "content": "Inspiration\nI was inspired by my daily developer activities. I've noticed that code reviews are overlooked and pull requests are referred to as dirty work. They are more than that to me; a single pull request can drive an entire company into any sort of direction. Why not choose where it goes?\nWhat it does\nShow Me the Code is a concept web application giving developers the feel for what could be in their code reviews. Show Me the Code enables developers and spectators alike to record themselves reviewing and files and embedding their thoughts directly into the application, powered by Loom.\nHow we built it\nI built this app using Angular.\nChallenges we ran into\nMany of the challenges I ran into were not related to using the Loom SDK, but making a functional app that revolved around the SDK.\nAccomplishments that we're proud of\nI'm proud that I built a resize tool as a side task on top of completing this project. I got the SDK running fairly quickly (mostly because of the documentation, you guys rock!) and I got quite a bit done for one person. I just wish I had more time.\nWhat we learned\nI learned that Loom has a pretty powerful SDK that can be used quite easily. I learned that hackathons are FUN (this is my first).\nWhat's next for Show Me the Code\nI have many, many more features that I want to add to Show Me the Code. As a developer myself, I know what features to add and how to add them. This is just the beginning for this app, baby!", "link": "https://devpost.com/software/show-me-the-code", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ni was inspired by my daily developer activities. i've noticed that code reviews are overlooked and pull requests are referred to as dirty work. they are more than that to me; a single pull request can drive an entire company into any sort of direction. why not choose where it goes?\nwhat it does\nshow me the code is a concept web application giving developers the feel for what could be in their code reviews. show me the code enables developers and spectators alike to record themselves reviewing and files and embedding their thoughts directly into the application, powered by loom.\nhow we built it\ni built this app using angular.\nchallenges we ran into\nmany of the challenges i ran into were not related to using the loom sdk, but making a functional app that revolved around the sdk.\naccomplishments that we're proud of\ni'm proud that i built a resize -----> tool !!!  as a side task on top of completing this project. i got the sdk running fairly quickly (mostly because of the documentation, you guys rock!) and i got quite a bit done for one person. i just wish i had more time.\nwhat we learned\ni learned that loom has a pretty powerful sdk that can be used quite easily. i learned that hackathons are fun (this is my first).\nwhat's next for show me the code\ni have many, many more features that i want to add to show me the code. as a developer myself, i know what features to add and how to add them. this is just the beginning for this app, baby!", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59504048}, {"Unnamed: 0": 4050, "autor": "HIPA NFT", "date": null, "content": "Inspiration\nAfter research about NFT market, only in 2021, this market had a growth of \u2245 137%, showing us that has a big opportunity to participate in a possible large market, mainly because has just some apps focused on that with the objective to promote an excellent usability and user experience.\nWhat it does\nHIPA is a marketplace focused on usability and user experience, making easiest and accessible process of NFT sale for everyone. Besides that we support a social cause to include women in programming.\nHow we built it\nAfter realize a benchmarking with the biggest players on market, we saw the possibility to create a design totally focused on simplicity and user experience.\nOur next step was create the SWOT Matrix (strengths, weaknesses, opportunities and threats) of our MVP by making a brainstorming with the group to take ideas and suggestions to the project. We consolidate all ideas and we group all of it by using MOSCOW method (must have, should have, could have, won't have) to define what will have in the MVP development.\nWithin defined features, it was realized the interface prototype using Figma, taking 3 principles of emotional design defined by Donald Norman, such as:\nVisceral\nBehavioral\nReflective\nAt app development we used:\nExpo with React Native, a multi platform tool to develop for iOS and Android.\nNestJS to backend, because is a consolidated framework with pretty good documentation.\nCelo integrations\nChallenges we ran into\nIntegration with Celo and Wallet Connect\nWe didn't find a lot of content with all of that integrations, besides some points not clean on documentation:\nCorrect environment to use Celo Development Kit on mobile applications, like web3 and crypto libs;\nIntegration with external wallets;\nIt has to have Valora App installed to Celo integration works;\nActivity attribution\nWe had to divide what professional will hold a task, with a deadline, being self-managing members\nTeam Organization and Time division\nWe had to conciliate task delivery time with all personal activities. We gather all day after work, with good communication with all members, realizing pair programming, and constant code review.\nUnderstand NFT ecosystem, how to create an NFT, about taxes, about seller and buyer vision\nFor half the team, the NFT ecosystem was unknown, so we realized an immersion to understand how works the main activities in this market, for instance:\nHow it was the experience to use applications that already are developed;\nHow to create and sell an NFT;\nHow it is the seller and buyer understanding;\nWhat're the necessary integrations to connect a wallet and have your NFT on choose network\nAccomplishments that we're proud of\nA fully functional MVP;\nCreate an app faithful to prototype and design experience;\nImpact society with a relevant social cause;\nProfessional Development;\nCode that follows good development practices;\nMethodologies followed, like code reviews and pair programming;\nWhat we learned\nIntegrate a project to a crypto wallet;\nOpportunity to participate in all process in a project, from planning to development;\nOpportunity to learn about the NFT market, business rules, and benchmarking.\nWhat's next for HIPA NFT\nThe next steps to be implemented will be:\nInsert augmented reality to see NFTs;\nInsert other languages (Spanish and Portuguese);\nInsert watermark on listed NFTs to differ the bought NFT;\nInsert an option of the timed auction on NFT selling;\nMethods to attract more users to the platform;\nInsert animations and an NFT preview at home;\nInsert an album option and special edition as an example, an NFT album to World Cup on 2022;\nInsert a rank of best buyers and best sellers;\nCreate a vision of a sold NFT and a created NFT;", "link": "https://devpost.com/software/secret4you", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nafter research about nft market, only in 2021, this market had a growth of \u2245 137%, showing us that has a big opportunity to participate in a possible large market, mainly because has just some apps focused on that with the objective to promote an excellent usability and user experience.\nwhat it does\nhipa is a marketplace focused on usability and user experience, making easiest and accessible process of nft sale for everyone. besides that we support a social cause to include women in programming.\nhow we built it\nafter realize a benchmarking with the biggest players on market, we saw the possibility to create a design totally focused on simplicity and user experience.\nour next step was create the swot matrix (strengths, weaknesses, opportunities and threats) of our mvp by making a brainstorming with the group to take ideas and suggestions to the project. we consolidate all ideas and we group all of it by using moscow method (must have, should have, could have, won't have) to define what will have in the mvp development.\nwithin defined features, it was realized the interface prototype using figma, taking 3 principles of emotional design defined by donald norman, such as:\nvisceral\nbehavioral\nreflective\nat app development we used:\nexpo with react native, a multi platform -----> tool !!!  to develop for ios and android.\nnestjs to backend, because is a consolidated framework with pretty good documentation.\ncelo integrations\nchallenges we ran into\nintegration with celo and wallet connect\nwe didn't find a lot of content with all of that integrations, besides some points not clean on documentation:\ncorrect environment to use celo development kit on mobile applications, like web3 and crypto libs;\nintegration with external wallets;\nit has to have valora app installed to celo integration works;\nactivity attribution\nwe had to divide what professional will hold a task, with a deadline, being self-managing members\nteam organization and time division\nwe had to conciliate task delivery time with all personal activities. we gather all day after work, with good communication with all members, realizing pair programming, and constant code review.\nunderstand nft ecosystem, how to create an nft, about taxes, about seller and buyer vision\nfor half the team, the nft ecosystem was unknown, so we realized an immersion to understand how works the main activities in this market, for instance:\nhow it was the experience to use applications that already are developed;\nhow to create and sell an nft;\nhow it is the seller and buyer understanding;\nwhat're the necessary integrations to connect a wallet and have your nft on choose network\naccomplishments that we're proud of\na fully functional mvp;\ncreate an app faithful to prototype and design experience;\nimpact society with a relevant social cause;\nprofessional development;\ncode that follows good development practices;\nmethodologies followed, like code reviews and pair programming;\nwhat we learned\nintegrate a project to a crypto wallet;\nopportunity to participate in all process in a project, from planning to development;\nopportunity to learn about the nft market, business rules, and benchmarking.\nwhat's next for hipa nft\nthe next steps to be implemented will be:\ninsert augmented reality to see nfts;\ninsert other languages (spanish and portuguese);\ninsert watermark on listed nfts to differ the bought nft;\ninsert an option of the timed auction on nft selling;\nmethods to attract more users to the platform;\ninsert animations and an nft preview at home;\ninsert an album option and special edition as an example, an nft album to world cup on 2022;\ninsert a rank of best buyers and best sellers;\ncreate a vision of a sold nft and a created nft;", "sortedWord": "None", "removed": "Nan", "score": 9, "comments": 2, "media": null, "medialink": null, "identifyer": 59504050}, {"Unnamed: 0": 4061, "autor": "Therapedia", "date": null, "content": "Introduction\nTherapedia is an app designed to simplify the mental health research process and serve as an equitable, all-in-one resource for mental health information. Therapedia features an encyclopedia of mental health topics, an index of medications and therapies, AI summarized studies, and links to regional laws and resources related to mental health.\nInspiration\nResearching mental health topics can be a difficult and time consuming process. There are a vast number of different resources that offer mental health data, and this information can often be limited, confusing, or conflicting. These factors can make researching mental health a daunting and intimidating process, and this lack of accessibility can prevent individuals from taking the first steps towards actively managing their mental health.\nThis dilemma is what inspired the development of Therapedia. Therapedia was created to make mental health education and research less intimidating, more inclusive, and ultimately more straightforward. Through the use of this app, users will have a one-stop, optimized resource for everything mental-health related. We want to empower individuals to learn about their mental health in a safe and informative place.\nWhat it does\nThe application first opens with four introduction screens that summarize Therapedia\u2019s primary features. The user is then taken to the primary Therapedia application. The Wiki page opens by default, but the user has the option to switch to the Home and Topics pages via tab navigation. The Wiki page works by having the user input the name of medication they are interested in learning about. Once the user selects \u201csearch,\u201d summarized results are produced for the user to read through. The Home screen provides the user with tools and updates that will aid them in the use of Therapedia. The Topics page allows the user to browse through a wide variety of different mental health topics, which can include subjects such as PTSD, anxiety, and depression.\nHow we built it\nThe application development process began through a brainstorming jamboard process, where several ideas were listed and our eventual scope and deliverables were chosen. Next, our team researched a variety of different UX templates through Figma, and aligned on a design that fit our project needs. Based on that design, our team built out the application using React Native to ensure usage on both IOS and android devices. Introduction screens were developed to walk users through how to use the tool, and pages were made for the homepage (Home), medication data (Wiki), and mental health topics (Topics). For the Wiki page, the application leverages an FDA drug API to display relevant medication data.\nChallenges we ran into\nOur greatest challenge during the development process was how to present medication data in a way that is clean and accessible while also ensuring relevant information is displayed. We leveraged an FDA drug API (Open FDA API) to pull and display data regarding mental health medication. While this resource provided a massive amount of helpful data, including all of these results into the display was unrealistic, as it would be overwhelming and difficult to read.\nAccomplishments that we're proud of\nTo solve the problem related to medication information, the team dedicated time into filtering this data and choosing the most relevant fields to display to the user, some of which includes the official medication name, generic name, general use cases, and the most common side effects. These filters ensure that the user is given a condensed and easy to read summary of the most information surrounding the medication they are researching. Through this experience, our team learned the importance of listening to user feedback and designing iteratively.\nWhat's next for Therapedia\nTherapedia is focused on it\u2019s goal of becoming a one-stop shop for mental health resources. While we\u2019ve been able to demo some of the designs and functionality, there are big things that still lie ahead. We have plans to incorporate the AI summary component using the QuillBot Text Summarizer AI. We would conduct user research to ensure the types of articles are helpful and make sense to the reader. For example, we would need user input on how to properly address conflicting studies, sensitive topics, and diverse sources of literature. Our goal is to become an app that hospitals and doctors are eager to recommend to patients through wellness programs. No one should have to awkwardly grab a pamphlet related to mental health in their doctor\u2019s offices. Outside of the clinical setting, we plan to be a reliable resource for people who are not yet diagnosed, cannot afford healthcare, or are affected negatively by mental health stigma.", "link": "https://devpost.com/software/therapedia", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "introduction\ntherapedia is an app designed to simplify the mental health research process and serve as an equitable, all-in-one resource for mental health information. therapedia features an encyclopedia of mental health topics, an index of medications and therapies, ai summarized studies, and links to regional laws and resources related to mental health.\ninspiration\nresearching mental health topics can be a difficult and time consuming process. there are a vast number of different resources that offer mental health data, and this information can often be limited, confusing, or conflicting. these factors can make researching mental health a daunting and intimidating process, and this lack of accessibility can prevent individuals from taking the first steps towards actively managing their mental health.\nthis dilemma is what inspired the development of therapedia. therapedia was created to make mental health education and research less intimidating, more inclusive, and ultimately more straightforward. through the use of this app, users will have a one-stop, optimized resource for everything mental-health related. we want to empower individuals to learn about their mental health in a safe and informative place.\nwhat it does\nthe application first opens with four introduction screens that summarize therapedia\u2019s primary features. the user is then taken to the primary therapedia application. the wiki page opens by default, but the user has the option to switch to the home and topics pages via tab navigation. the wiki page works by having the user input the name of medication they are interested in learning about. once the user selects \u201csearch,\u201d summarized results are produced for the user to read through. the home screen provides the user with tools and updates that will aid them in the use of therapedia. the topics page allows the user to browse through a wide variety of different mental health topics, which can include subjects such as ptsd, anxiety, and depression.\nhow we built it\nthe application development process began through a brainstorming jamboard process, where several ideas were listed and our eventual scope and deliverables were chosen. next, our team researched a variety of different ux templates through figma, and aligned on a design that fit our project needs. based on that design, our team built out the application using react native to ensure usage on both ios and android devices. introduction screens were developed to walk users through how to use the -----> tool !!! , and pages were made for the homepage (home), medication data (wiki), and mental health topics (topics). for the wiki page, the application leverages an fda drug api to display relevant medication data.\nchallenges we ran into\nour greatest challenge during the development process was how to present medication data in a way that is clean and accessible while also ensuring relevant information is displayed. we leveraged an fda drug api (open fda api) to pull and display data regarding mental health medication. while this resource provided a massive amount of helpful data, including all of these results into the display was unrealistic, as it would be overwhelming and difficult to read.\naccomplishments that we're proud of\nto solve the problem related to medication information, the team dedicated time into filtering this data and choosing the most relevant fields to display to the user, some of which includes the official medication name, generic name, general use cases, and the most common side effects. these filters ensure that the user is given a condensed and easy to read summary of the most information surrounding the medication they are researching. through this experience, our team learned the importance of listening to user feedback and designing iteratively.\nwhat's next for therapedia\ntherapedia is focused on it\u2019s goal of becoming a one-stop shop for mental health resources. while we\u2019ve been able to demo some of the designs and functionality, there are big things that still lie ahead. we have plans to incorporate the ai summary component using the quillbot text summarizer ai. we would conduct user research to ensure the types of articles are helpful and make sense to the reader. for example, we would need user input on how to properly address conflicting studies, sensitive topics, and diverse sources of literature. our goal is to become an app that hospitals and doctors are eager to recommend to patients through wellness programs. no one should have to awkwardly grab a pamphlet related to mental health in their doctor\u2019s offices. outside of the clinical setting, we plan to be a reliable resource for people who are not yet diagnosed, cannot afford healthcare, or are affected negatively by mental health stigma.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59504061}, {"Unnamed: 0": 4090, "autor": "ChessLink", "date": null, "content": "Inspiration\nChessLink was inspired by the team\u2019s love and involvement in chess. The chess community has been a huge positive influence on the team members. The two team members of ChessLink actually met at their university chess club! Seeing the recent explosion of chess on twitch, the team wondered if they could provide a tool to help assist the chess expansion on twitch and landed on the idea of ChessLink. ChessLink aims to help viewers support their favorite streamers through NFTs. The team hopes that ChessLink will help the growth of chess and be a great way to give back to the chess community.\nWhat it does\nChessLink allows streamers and non-streamers to mint NFTs of the critical positions of their latest played chess games.\nStreamers can sell their minted NFTs on the polygon blockchain to their viewers. These sold NFTs can operate as a channel for viewers to directly support their favorite streamers while also receiving a product similar to a chess position virtually autographed by their favorite streamers. This secondary channel of support has advantages over traditional twitch subscriptions by giving twitch streamers a larger cut of the donations and twitch donations by giving the viewers a tangible product from their support.\nNon-streamer users can utilize ChessLink to mint their own games for their personal collection to keep a cool memento of their games.\nHow we built it\nWe built ChessLink using the chain link and the polygon blockchain to handle NFT interactions. The front end of our app was developed using next.js, and our backend is handled by node.js. The way that our minting flow works is\nWe have a chrome extension that can be used at any time and website to upload a data packet of chess info as a hex string to an ipfs address.\nChainlink calls the ipfs address and returns the data packet as a regular string\nThat data packet is then parsed and displayed to the user for the user to pick which positions they want to mint if any\nThe minted NFTs are sent to the polygon blockchain\nThe chrome extension element of ChessLink allows for users to send chess games they want minted straight from the chess website where they are playing instead of having to visit our website to search for the chess game that they want minted.\nChallenges we ran into\nThe main challenge that we had when developing ChessLink was finding a good chess library for the UI. A lot of the available chess javascript libraries were incompatible with Next.js, and we constantly ran into issues with webpack because of this.\nAccomplishments that we're proud of\nWe have made a fully functioning chess app that users and streamers can use to mint NFTs on a public blockchain. We are also very proud of the fact that prior to this hackathon, the team had minimal experience with UI and no experience with chrome extensions, but we managed to develop a user-friendly UI as well as a chrome extension for our app to make it easy to use for everyone.\nWhat we learned\nFrom this hackathon, we learned how to create chrome extensions, how to interact with chainlink and gained a lot of UI experience. We hope that this accumulated experience will help us in the future to develop more user-friendly apps that can utilize blockchain and its advantages nicely.\nWhat's next for ChessLink\nIn the future, we have a few key targets that we want to focus on to make ChessLink better:\ncontinue to improve the UI to make it more streamer and user friendly\nimplement better separation on our website between regular users and streamers\ngather and implement streamer feedback for our website", "link": "https://devpost.com/software/chesslink", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nchesslink was inspired by the team\u2019s love and involvement in chess. the chess community has been a huge positive influence on the team members. the two team members of chesslink actually met at their university chess club! seeing the recent explosion of chess on twitch, the team wondered if they could provide a -----> tool !!!  to help assist the chess expansion on twitch and landed on the idea of chesslink. chesslink aims to help viewers support their favorite streamers through nfts. the team hopes that chesslink will help the growth of chess and be a great way to give back to the chess community.\nwhat it does\nchesslink allows streamers and non-streamers to mint nfts of the critical positions of their latest played chess games.\nstreamers can sell their minted nfts on the polygon blockchain to their viewers. these sold nfts can operate as a channel for viewers to directly support their favorite streamers while also receiving a product similar to a chess position virtually autographed by their favorite streamers. this secondary channel of support has advantages over traditional twitch subscriptions by giving twitch streamers a larger cut of the donations and twitch donations by giving the viewers a tangible product from their support.\nnon-streamer users can utilize chesslink to mint their own games for their personal collection to keep a cool memento of their games.\nhow we built it\nwe built chesslink using the chain link and the polygon blockchain to handle nft interactions. the front end of our app was developed using next.js, and our backend is handled by node.js. the way that our minting flow works is\nwe have a chrome extension that can be used at any time and website to upload a data packet of chess info as a hex string to an ipfs address.\nchainlink calls the ipfs address and returns the data packet as a regular string\nthat data packet is then parsed and displayed to the user for the user to pick which positions they want to mint if any\nthe minted nfts are sent to the polygon blockchain\nthe chrome extension element of chesslink allows for users to send chess games they want minted straight from the chess website where they are playing instead of having to visit our website to search for the chess game that they want minted.\nchallenges we ran into\nthe main challenge that we had when developing chesslink was finding a good chess library for the ui. a lot of the available chess javascript libraries were incompatible with next.js, and we constantly ran into issues with webpack because of this.\naccomplishments that we're proud of\nwe have made a fully functioning chess app that users and streamers can use to mint nfts on a public blockchain. we are also very proud of the fact that prior to this hackathon, the team had minimal experience with ui and no experience with chrome extensions, but we managed to develop a user-friendly ui as well as a chrome extension for our app to make it easy to use for everyone.\nwhat we learned\nfrom this hackathon, we learned how to create chrome extensions, how to interact with chainlink and gained a lot of ui experience. we hope that this accumulated experience will help us in the future to develop more user-friendly apps that can utilize blockchain and its advantages nicely.\nwhat's next for chesslink\nin the future, we have a few key targets that we want to focus on to make chesslink better:\ncontinue to improve the ui to make it more streamer and user friendly\nimplement better separation on our website between regular users and streamers\ngather and implement streamer feedback for our website", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59504090}, {"Unnamed: 0": 4099, "autor": "Contoso Wellness Bot", "date": null, "content": "Inspiration\nI wanted to work with a tool I never really got a chance to work with professionally.\nWhat it does\nUsers communicate to a chat bot in order to self diagnose physical or mental ailments and take next actions if they choose to.\nHow we built it\nLeverage Azure scenario templates for Azure healthbot and deployed with App service and necessary channel integrations.\nChallenges we ran into\nAzure healthbot was a new utility I never used so I needed to go through a few learning modules to see it's potential.\nAccomplishments that we're proud of\nUsing a custom scenario to be able to call multiple scenarios.\nWhat we learned\nA lot more about how machine learning models are built in Azure with Azure ML Studio.\nWhat's next for Contoso Wellness Bot\nPerhaps there will be the opportunity to build more scenarios for healthcare purpose.", "link": "https://devpost.com/software/contoso-wellness-bot", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ni wanted to work with a -----> tool !!!  i never really got a chance to work with professionally.\nwhat it does\nusers communicate to a chat bot in order to self diagnose physical or mental ailments and take next actions if they choose to.\nhow we built it\nleverage azure scenario templates for azure healthbot and deployed with app service and necessary channel integrations.\nchallenges we ran into\nazure healthbot was a new utility i never used so i needed to go through a few learning modules to see it's potential.\naccomplishments that we're proud of\nusing a custom scenario to be able to call multiple scenarios.\nwhat we learned\na lot more about how machine learning models are built in azure with azure ml studio.\nwhat's next for contoso wellness bot\nperhaps there will be the opportunity to build more scenarios for healthcare purpose.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59504099}, {"Unnamed: 0": 4114, "autor": "PetFund.co (AI Chatbot)", "date": null, "content": "Our Pitch Deck: https://drive.google.com/drive/folders/1A6BF6gb-NGgiiyugFoOwcFEtIl9zuFR2?usp=sharing\nInspiration\n57% of Donators can not find an appropriate or aligned campaign for them to donate whether it\u2019s the project\u2019s description, urgency, funding, or amount of money. And 78% of donors would donate money to pets. And each donor has donated more than once.\nWhat it does\nDonors can ask (input) the questions about the donation containing what type of donation and amount of money they want to donate within the whole sentences to the chatbot and then it will answer (output) related to the specific goal of donation and number of money that donors ask.\nAI Chatbot will choose to donate randomly within the corporation pool.\nAI Chatbot have the choices to choose an item to be donated as the user wishes.\nUser chooses the amount of donation and the chatbot will distribute automatically in criteria priority.\nHow our team built it\nThe chatbot is used to automate responses by using the keyword rules built right into the dashboard. It allows the AI bot to \u201cunderstand\u201d and learn from the messages it receives, take actionable data from those messages, and deliver even more accurate responses. Using Node.JS to create AI Chatbot and connect it with LINE Official Account API to be input and output device for this prototype.\nDefine the goals: to increase customer satisfaction .ex. we might want to add a chatbot to the customer to the support team and let it handle the most common FAQs, so our team can focus on the more complicated cases.\nDefine the use cases: Our team needs to figure this out before starting building the bot,as we need to know exactly what the bot will do and why that is important.\nCraft bot personality: We want to make sure that our users connect with the chatbot and that the conversation is engaging and representative of real human interaction.\nMap out the path: create a logic wireframe to see how a user would go from start to finish and where they might want to dive into other flows.\nWrite a Script for edge cases: the sample dialog should help pinpoint the pain points and off-track problems, as will user testing via Dialogflow.\nBot Testing/Optimising: Internal testing will give a lot of insight on how to improve AI bot. So after publishing the bot, we need to keep monitoring its performance. Monitor the conversations, collect data, create logs, analyze the data, and keep improving the bot for an even better experience.\nChallenges our team ran into\nAt first, We decide to use Dialogue Flow because it's a chatbot tool that we are experts at, but after we research more. We found out that it was unable to calculate the amount of money since it's not suitable. We switched to Node.Js instead, plot the graph with Python, and then connect it to LINE OA API.\nAccomplishments that I'm proud of\nBe able to save our time and money for communicating with the customer and gain customer satisfaction. Also, be able to suggest customers donate based on the criteria priority.\nWhat I learned\nThe more data we use, the more distributed AI can suggest\nEach corporation has different needed\nWe can find a prediction of AI distribution behavior after it runs for a long time\nWhat's next for PetFund.Co\nFAQ by using AI to automatically answer.\nIncrease AI-powered chatbots features as a suggestion or automate advertisement campaigns/events specific for each of the user\u2019s interests.\nPetFund.co will apply more features implemented by AI in the platform by using AI into the Blockchain system to make funding procedures more secure and entirely open.", "link": "https://devpost.com/software/petfund-co-n2i6j3", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "our pitch deck: https://drive.google.com/drive/folders/1a6bf6gb-nggiiyugfoowcfetil9zufr2?usp=sharing\ninspiration\n57% of donators can not find an appropriate or aligned campaign for them to donate whether it\u2019s the project\u2019s description, urgency, funding, or amount of money. and 78% of donors would donate money to pets. and each donor has donated more than once.\nwhat it does\ndonors can ask (input) the questions about the donation containing what type of donation and amount of money they want to donate within the whole sentences to the chatbot and then it will answer (output) related to the specific goal of donation and number of money that donors ask.\nai chatbot will choose to donate randomly within the corporation pool.\nai chatbot have the choices to choose an item to be donated as the user wishes.\nuser chooses the amount of donation and the chatbot will distribute automatically in criteria priority.\nhow our team built it\nthe chatbot is used to automate responses by using the keyword rules built right into the dashboard. it allows the ai bot to \u201cunderstand\u201d and learn from the messages it receives, take actionable data from those messages, and deliver even more accurate responses. using node.js to create ai chatbot and connect it with line official account api to be input and output device for this prototype.\ndefine the goals: to increase customer satisfaction .ex. we might want to add a chatbot to the customer to the support team and let it handle the most common faqs, so our team can focus on the more complicated cases.\ndefine the use cases: our team needs to figure this out before starting building the bot,as we need to know exactly what the bot will do and why that is important.\ncraft bot personality: we want to make sure that our users connect with the chatbot and that the conversation is engaging and representative of real human interaction.\nmap out the path: create a logic wireframe to see how a user would go from start to finish and where they might want to dive into other flows.\nwrite a script for edge cases: the sample dialog should help pinpoint the pain points and off-track problems, as will user testing via dialogflow.\nbot testing/optimising: internal testing will give a lot of insight on how to improve ai bot. so after publishing the bot, we need to keep monitoring its performance. monitor the conversations, collect data, create logs, analyze the data, and keep improving the bot for an even better experience.\nchallenges our team ran into\nat first, we decide to use dialogue flow because it's a chatbot -----> tool !!!  that we are experts at, but after we research more. we found out that it was unable to calculate the amount of money since it's not suitable. we switched to node.js instead, plot the graph with python, and then connect it to line oa api.\naccomplishments that i'm proud of\nbe able to save our time and money for communicating with the customer and gain customer satisfaction. also, be able to suggest customers donate based on the criteria priority.\nwhat i learned\nthe more data we use, the more distributed ai can suggest\neach corporation has different needed\nwe can find a prediction of ai distribution behavior after it runs for a long time\nwhat's next for petfund.co\nfaq by using ai to automatically answer.\nincrease ai-powered chatbots features as a suggestion or automate advertisement campaigns/events specific for each of the user\u2019s interests.\npetfund.co will apply more features implemented by ai in the platform by using ai into the blockchain system to make funding procedures more secure and entirely open.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59504114}, {"Unnamed: 0": 4133, "autor": "Using Deep Learning to Predict Protein Secondary Structure", "date": null, "content": "Title\nUsing Deep Learning to Predict Protein Secondary Structure\nWho\nTeam Echidnas: Hari Dandapani [hdandapa], Helen Zhang [hzhan103], Alex Guo [yguo62]\nIntroduction\nProteins have 4 different levels of structure: primary, secondary, tertiary, and quaternary structure. The primary structure tells us the sequence of the 20 different amino acids as they appear in order after lots of processing, like splicing. The secondary structure of the amino acids tells us more information about how the amino acids fold on each other. In introductory biology coursework, students are taught that the secondary structure of a sequence of amino acids can be alpha helices or beta pleated sheets. The dataset that we draw from further breaks this down into redundant and non-redundant alpha as well as redundant and non-redundant beta.\nThis project seeks to take two different approaches [one based on Convolutional Neural Networks and one based on Natural Language Processing] to predict the secondary structure of proteins based on the primary structure of the amino acid chain.\nRelated Work\nFor this project, we are drawing primarily on this research paper as inspiration for our project. From the abstract of the paper, \"In this study, we present a novel computational method, TMPSS, to predict the secondary structures in non-transmembrane parts and the topology structures in transmembrane parts of \u03b1TMPs. TMPSS applied a Convolutional Neural Network (CNN), combined with an attention-enhanced Bidirectional Long Short-Term Memory (BiLSTM) network, to extract the local contexts and long-distance interdependencies from primary sequences.\" We seek to replicate the same prediction structure with CNNs and LSTM while also trying out a methodology of our own creation that is based off of NLP, potentially using Transformers and RNNs to see if that can also be used to solve this problem.\nThis paper uses deep learning to predict 8-state secondary protein structures. They use four types of features, \"including a position-specific scoring matrix (PSSM), protein coding features, conservation scores, and physical properties\" to characterize each residue in a protein sequence. Using a convolutional, residual, and recurrent neural network to address this problem, their model captures local and global features to achieve 71.4% accuracy.\nData\nWe will extract our data from PDBTM: Protein Data Bank of Transmembrane Proteins. This data has 6499 different transmembrane proteins, of which 5976 are alpha and 472 are beta. The rest are unclassified. We will need to use webscraping to extract the sequences and labels from the database for use in our model.\nOne issue posed with the dataset is that 92% of the dataset is alpha [though it can further be broken down based on . We will need to find intelligent ways to segregate the data into test and training sets such that we don't just always predict alpha.\nAs a final aside, we are looking into other ways of obtaining the data, like the protein data bank, to see if we can use their more voluminous quantity of data for our model. We might try to use something like this one, though this would require much more preprocessing and pre-analysis.\nMethodology\nTMPSS uses arrangements of CNNs and LSTMs in their classification model. We will be expanding the project into exploring the use of NLP methodology to approach similar problems of protein sequence classification.\nWe will attempt to use a transformer-based model. This might entail any combination of the following, based off the Transformer seq2seq model:\nAttention Heads\nPosition Encoding Layers to add positional embeddings\nTransformer blocks for our encoder and decoder\nMetrics\nWe will train and test our data on the labeled redundant and nonredundant alpha/beta protein sequences. Accuracy is a common metric already used to evaluate protein secondary structure predictions. However, since our dataset is skewed towards alpha helices, we will want more nuance in our model analysis and will use metrics such as accuracy, recall, precision, specificity, Mathews Correlation Coefficient, and F-measure to holistically determine model performance. These indicators will help us determine not just how well our model predicts based on its correct results, but also how well it predicts relative to its mistakes. The authors of the original research paper used similar performance metrics, and also compared model performance against past state-of-the-art models using accuracy and Q3.\nBase goal: We will have a working model capable of processing input protein sequences and outputting predicted secondary structure labels. Ideally, this model will have overall similar architecture to TMPSS, though the exact layers may be slightly different.\nTarget goal: We will replicate the model proposed in the paper as well as finalize a working model using our Transformer based approach. Inputs will be similar to those proposed in the paper (one-hot matrix, HHblits profile).\nStretch goal: We will visualize the predicted topology of the protein sequence using our model\u2019s predicted secondary structure labels.\nEthics\nDeep Learning can be a useful tool for situations where technical limitations prohibit experimentation. They can also be helpful for identifying new patterns on a larger scale than feasible for humans. However, deep learning models must be extensively validated. They can amplify implicit biases and errors, which can go unnoticed if not under constant objective surveillance.\nThe use of biological data raises many important questions, especially in regard to human application which introduces many other confounding variables as well as stronger repercussions. Biological data is highly variable due to an inability to control multitudinous factors that may impact samples. The process of sample collection is subject to countless logistical factors. Patient sample collection is subject to societal factors and has the capability to perpetuate societal biases and structural idiosyncrasies. The over/underrepresentation of various groups can contribute to results that are poorly applicable in certain cases. Assumption of the quality of data can negatively impact future research and applications of findings, which can have tremendous effects.\nDivision of Labor\nWe plan on working equally across all three domains of the project:\nDownloading and preprocessing\nOriginal model with CNNs\nNew NLP-ish model\nSecond Check-in:\nhttps://docs.google.com/document/d/1CWWd8qkigAQ7O4noN4OHfGmpILWiJ5F5yO3_BsIlbrQ/edit?usp=sharing\nFinal Results and Reflection:\nhttps://docs.google.com/document/d/1fo2V5xevNZkAFqgCa59d7A0ivcymH8V4ZrqrEZUro6Y/edit?usp=sharing\nGitHub Repository:\nhttps://github.com/helenzhang8/CSCI1470-Final/tree/fallingofftf\nDigi-Poster:\nhttps://docs.google.com/presentation/d/14rkeSDoSNMHfHTyGcTl_7cl8WEcw38xsQms6RHinMBA/edit?usp=sharing", "link": "https://devpost.com/software/using-deep-learning-to-predict-protein-secondary-structure", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "title\nusing deep learning to predict protein secondary structure\nwho\nteam echidnas: hari dandapani [hdandapa], helen zhang [hzhan103], alex guo [yguo62]\nintroduction\nproteins have 4 different levels of structure: primary, secondary, tertiary, and quaternary structure. the primary structure tells us the sequence of the 20 different amino acids as they appear in order after lots of processing, like splicing. the secondary structure of the amino acids tells us more information about how the amino acids fold on each other. in introductory biology coursework, students are taught that the secondary structure of a sequence of amino acids can be alpha helices or beta pleated sheets. the dataset that we draw from further breaks this down into redundant and non-redundant alpha as well as redundant and non-redundant beta.\nthis project seeks to take two different approaches [one based on convolutional neural networks and one based on natural language processing] to predict the secondary structure of proteins based on the primary structure of the amino acid chain.\nrelated work\nfor this project, we are drawing primarily on this research paper as inspiration for our project. from the abstract of the paper, \"in this study, we present a novel computational method, tmpss, to predict the secondary structures in non-transmembrane parts and the topology structures in transmembrane parts of \u03b1tmps. tmpss applied a convolutional neural network (cnn), combined with an attention-enhanced bidirectional long short-term memory (bilstm) network, to extract the local contexts and long-distance interdependencies from primary sequences.\" we seek to replicate the same prediction structure with cnns and lstm while also trying out a methodology of our own creation that is based off of nlp, potentially using transformers and rnns to see if that can also be used to solve this problem.\nthis paper uses deep learning to predict 8-state secondary protein structures. they use four types of features, \"including a position-specific scoring matrix (pssm), protein coding features, conservation scores, and physical properties\" to characterize each residue in a protein sequence. using a convolutional, residual, and recurrent neural network to address this problem, their model captures local and global features to achieve 71.4% accuracy.\ndata\nwe will extract our data from pdbtm: protein data bank of transmembrane proteins. this data has 6499 different transmembrane proteins, of which 5976 are alpha and 472 are beta. the rest are unclassified. we will need to use webscraping to extract the sequences and labels from the database for use in our model.\none issue posed with the dataset is that 92% of the dataset is alpha [though it can further be broken down based on . we will need to find intelligent ways to segregate the data into test and training sets such that we don't just always predict alpha.\nas a final aside, we are looking into other ways of obtaining the data, like the protein data bank, to see if we can use their more voluminous quantity of data for our model. we might try to use something like this one, though this would require much more preprocessing and pre-analysis.\nmethodology\ntmpss uses arrangements of cnns and lstms in their classification model. we will be expanding the project into exploring the use of nlp methodology to approach similar problems of protein sequence classification.\nwe will attempt to use a transformer-based model. this might entail any combination of the following, based off the transformer seq2seq model:\nattention heads\nposition encoding layers to add positional embeddings\ntransformer blocks for our encoder and decoder\nmetrics\nwe will train and test our data on the labeled redundant and nonredundant alpha/beta protein sequences. accuracy is a common metric already used to evaluate protein secondary structure predictions. however, since our dataset is skewed towards alpha helices, we will want more nuance in our model analysis and will use metrics such as accuracy, recall, precision, specificity, mathews correlation coefficient, and f-measure to holistically determine model performance. these indicators will help us determine not just how well our model predicts based on its correct results, but also how well it predicts relative to its mistakes. the authors of the original research paper used similar performance metrics, and also compared model performance against past state-of-the-art models using accuracy and q3.\nbase goal: we will have a working model capable of processing input protein sequences and outputting predicted secondary structure labels. ideally, this model will have overall similar architecture to tmpss, though the exact layers may be slightly different.\ntarget goal: we will replicate the model proposed in the paper as well as finalize a working model using our transformer based approach. inputs will be similar to those proposed in the paper (one-hot matrix, hhblits profile).\nstretch goal: we will visualize the predicted topology of the protein sequence using our model\u2019s predicted secondary structure labels.\nethics\ndeep learning can be a useful -----> tool !!!  for situations where technical limitations prohibit experimentation. they can also be helpful for identifying new patterns on a larger scale than feasible for humans. however, deep learning models must be extensively validated. they can amplify implicit biases and errors, which can go unnoticed if not under constant objective surveillance.\nthe use of biological data raises many important questions, especially in regard to human application which introduces many other confounding variables as well as stronger repercussions. biological data is highly variable due to an inability to control multitudinous factors that may impact samples. the process of sample collection is subject to countless logistical factors. patient sample collection is subject to societal factors and has the capability to perpetuate societal biases and structural idiosyncrasies. the over/underrepresentation of various groups can contribute to results that are poorly applicable in certain cases. assumption of the quality of data can negatively impact future research and applications of findings, which can have tremendous effects.\ndivision of labor\nwe plan on working equally across all three domains of the project:\ndownloading and preprocessing\noriginal model with cnns\nnew nlp-ish model\nsecond check-in:\nhttps://docs.google.com/document/d/1cwwd8qkigaq7o4non4ohfgmpilwij5f5yo3_bsilbrq/edit?usp=sharing\nfinal results and reflection:\nhttps://docs.google.com/document/d/1fo2v5xevnzkafqgca59d7a0ivcymh8v4zrqrezuro6y/edit?usp=sharing\ngithub repository:\nhttps://github.com/helenzhang8/csci1470-final/tree/fallingofftf\ndigi-poster:\nhttps://docs.google.com/presentation/d/14rkesdosnmhfhtygctl_7cl8wecw38xsqms6rhinmba/edit?usp=sharing", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 4, "media": null, "medialink": null, "identifyer": 59504133}, {"Unnamed: 0": 4134, "autor": "Ascend", "date": null, "content": "Inspiration\nBoth of us have dabbled in competitive esports in the past. Tran was part of the varsity League of Legends team in college, and Ray was a competitive Masters player in Overwatch with regular team members. However, delving into esports, especially in a organized team, requires much more than simply playing the game. After all, half the battle when it comes to organized sports is the plan and strategy going into the game itself.\nHaving played for colegiate esports during undergrad, we noticed that there was a lot to be desired when it came to managing the teams, organizing other members, and communicating/finding new solutions. Things such as team schedules, champion pools, draft planning, and much more - these were all arranged and organized in such a haphazard and ununified fashion that was difficult to navigate. Usually, scrim results, meeting notes, and other data were all halfheartedly stitched together in a Google Spreadsheet. What this meant was that it required a lot of time to digest the data, not to mention the time it took to set up the spreadsheets, maintain it, and clean it on a regular basis. And the more time that was spent on this, meant less time actually playing the game and improving.\nThis was where the idea of Ascend was born.\nThe goal of Ascend was to get teams, both amateur and pro alike - a permanent, timesaving solution that would organize this data in an intuitive and easy to understand user interface, while displaying all of the data that they would need - so that they can focus more on playing the game, and winning.\nFeatures\nRight now, Ascend lets teams manage their schedule and players' League of Legends champion pool. In the near future, we want to add functionality to plan drafts, automate scrim result tracking with stats (CS/min, KDA, etc.), and analyze solo queue/scrim data for areas of improvement. We also want to sync schedules between teams (e.g. for scrim planning) and implement notifications for reminders and rescheduling. All of this is meant to save time and present only the most important data to help pinpoint potential areas of improvement and places to strategize on, so that the user is aware of what it will take to reach the next level.\nHow we built it\nAscend was built using Next.js (React) and Prisma with a PostgreSQL database. Our authentication is managed through the NextAuth.js library. Other open source frontend libraries (Mosaic Lite, Ant Design, and react-big-calendar) were crucial because they significantly cut down our development time.\nThe frontend communicates with our backend by using Axios to hit our REST API endpoints that perform create, read, update, and delete (CRUD) operations on the PostgreSQL database. The REST API was built using Next.js' serverless functions (Node.js) and Prisma (ORM).\nThe champion data was fetched from the Riot Games' API.\nChallenges we ran into\n\ud83d\udd54 Time management. As we needed to balance our full-time job with LiquidHacks, this made it very difficult to accomplish everything we initially hoped to during the hackathon. This included having tough conversations and deliberations on what to cut, such as our our initial plan to launch this for multiple games at once.\n\ud83d\udccd Remote collaboration. Tran is based in New York whereas Ray is based in California. While we have worked on several projects in the past, this was our first time working completely in different time zones and remote. We could no longer discuss our problems over a whiteboard, or quickly look over at another's screen for help on a specific snippet of code. Finding alternative solutions with various tools, such as Discord, or Zoom with the control mouse feature, was definitely an integral part in making sure this project took off. Also, it meant that there was a need for increased communication, such as coordinating time zones and other issues that come with remote work.\n\u2699\ufe0f New tech stack. We wanted to challenge ourselves by learning a new tech stack. Although a good chunk of our hackathon time was spent reading documentation rather than pumping out features, it was ultimately, in our opinion, worth the effort as we learned how to utilize new tools that will help improve our products in the future.\nAccomplishments that we're proud of\n\ud83c\udf89 This was Ray's first hackathon! It was also his first time building a full-stack application, so we pair programmed all the way through.\nWe worked really hard building a working full-stack application and thinking about ways to scale our platform, and we can't wait to release our platform for people to use.\nWhat we learned\n\ud83d\udee0 Next.js and Prisma. Next.js has gained immense popularity recently. This hackathon was a great opportunity to gain exposure to these technologies and build a full-stack application with them.\n\ud83e\udd1d Teamwork makes the dream work. This experience strengthened our understanding of cross-discipline collaboration (Tran is a software engineer and Ray is a business analyst). It also served to highlight areas that we could both improve upon, and what skills (both soft and hard skills alike) are imperative no matter what job function you are in.\nWhat's next for Ascend\nEven though we didn't fully get to implement all of the features that we first set out to accomplish at the start of this hackathon, we're still really excited and optimistic about Ascend's future! We have a lot of plans in mind to scale up this program and make it accessible and useful for various audiences. Our ultimate goal, and one that we've constantly been striving for since the start, was creating a valuable tool that anyone, from any level of play, can effectively utilize (From amateur, to collegiate, and even to pro level). Here are some of the next steps that we've planned out, after our week of developing Ascend:\n\ud83d\udd17 Ability to invite team members via email or unique URL. This will make it so it's easily accessible and user friendly for people to adopt, while providing a simple solution to join a team. We also want to potentially add in various sign in methods to make the transition as smooth as possible.\n\ud83d\udcbb Integration with other API's. One thing that we also want to include is integration with other useful API's that will allow for an improvement on our product, such as the ability to connect one's game account to the service.\n\ud83e\uddea Product testing. Initial beta testing for Ascend will be done with our alma mater's collegiate esports team, as we are familiar with the users and managers. By being able to get this candid feedback, it'll give us the opportunity to see where improvements should be made and iterate on the product. Eventually, we plan to use the connections from these teams to expand our testing to other campuses, and other various levels of play. Of course, our number one goal is to listen to real users who would use this product to understand where the pain points are and create something that they really need.\n\ud83c\udf10 Deployment (production). Ray is currently preparing the marketing for a potential first release, after we are comfortable we have created a sufficient product that people will use, from our feedback. Once that's ready, we'll release a production version for the public to use.\n\ud83d\udca1 New features. There's a vast amount of data that can be gathered from solo queue, scrim games, and much more! Future features will use this information to provide teams with valuable insights that will help them improve.\n\ud83c\udfae Add support for other games. Of course, League of Legends isn't the only game that makes up the competitive esports scene! From the start, our goal has always been to provide this service for various games. After we feel we have a working formula for League of Legends, then we plan to move onto other games that could benefit from the service as well.", "link": "https://devpost.com/software/ascend-3q2ws9", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nboth of us have dabbled in competitive esports in the past. tran was part of the varsity league of legends team in college, and ray was a competitive masters player in overwatch with regular team members. however, delving into esports, especially in a organized team, requires much more than simply playing the game. after all, half the battle when it comes to organized sports is the plan and strategy going into the game itself.\nhaving played for colegiate esports during undergrad, we noticed that there was a lot to be desired when it came to managing the teams, organizing other members, and communicating/finding new solutions. things such as team schedules, champion pools, draft planning, and much more - these were all arranged and organized in such a haphazard and ununified fashion that was difficult to navigate. usually, scrim results, meeting notes, and other data were all halfheartedly stitched together in a google spreadsheet. what this meant was that it required a lot of time to digest the data, not to mention the time it took to set up the spreadsheets, maintain it, and clean it on a regular basis. and the more time that was spent on this, meant less time actually playing the game and improving.\nthis was where the idea of ascend was born.\nthe goal of ascend was to get teams, both amateur and pro alike - a permanent, timesaving solution that would organize this data in an intuitive and easy to understand user interface, while displaying all of the data that they would need - so that they can focus more on playing the game, and winning.\nfeatures\nright now, ascend lets teams manage their schedule and players' league of legends champion pool. in the near future, we want to add functionality to plan drafts, automate scrim result tracking with stats (cs/min, kda, etc.), and analyze solo queue/scrim data for areas of improvement. we also want to sync schedules between teams (e.g. for scrim planning) and implement notifications for reminders and rescheduling. all of this is meant to save time and present only the most important data to help pinpoint potential areas of improvement and places to strategize on, so that the user is aware of what it will take to reach the next level.\nhow we built it\nascend was built using next.js (react) and prisma with a postgresql database. our authentication is managed through the nextauth.js library. other open source frontend libraries (mosaic lite, ant design, and react-big-calendar) were crucial because they significantly cut down our development time.\nthe frontend communicates with our backend by using axios to hit our rest api endpoints that perform create, read, update, and delete (crud) operations on the postgresql database. the rest api was built using next.js' serverless functions (node.js) and prisma (orm).\nthe champion data was fetched from the riot games' api.\nchallenges we ran into\n\ud83d\udd54 time management. as we needed to balance our full-time job with liquidhacks, this made it very difficult to accomplish everything we initially hoped to during the hackathon. this included having tough conversations and deliberations on what to cut, such as our our initial plan to launch this for multiple games at once.\n\ud83d\udccd remote collaboration. tran is based in new york whereas ray is based in california. while we have worked on several projects in the past, this was our first time working completely in different time zones and remote. we could no longer discuss our problems over a whiteboard, or quickly look over at another's screen for help on a specific snippet of code. finding alternative solutions with various tools, such as discord, or zoom with the control mouse feature, was definitely an integral part in making sure this project took off. also, it meant that there was a need for increased communication, such as coordinating time zones and other issues that come with remote work.\n\u2699\ufe0f new tech stack. we wanted to challenge ourselves by learning a new tech stack. although a good chunk of our hackathon time was spent reading documentation rather than pumping out features, it was ultimately, in our opinion, worth the effort as we learned how to utilize new tools that will help improve our products in the future.\naccomplishments that we're proud of\n\ud83c\udf89 this was ray's first hackathon! it was also his first time building a full-stack application, so we pair programmed all the way through.\nwe worked really hard building a working full-stack application and thinking about ways to scale our platform, and we can't wait to release our platform for people to use.\nwhat we learned\n\ud83d\udee0 next.js and prisma. next.js has gained immense popularity recently. this hackathon was a great opportunity to gain exposure to these technologies and build a full-stack application with them.\n\ud83e\udd1d teamwork makes the dream work. this experience strengthened our understanding of cross-discipline collaboration (tran is a software engineer and ray is a business analyst). it also served to highlight areas that we could both improve upon, and what skills (both soft and hard skills alike) are imperative no matter what job function you are in.\nwhat's next for ascend\neven though we didn't fully get to implement all of the features that we first set out to accomplish at the start of this hackathon, we're still really excited and optimistic about ascend's future! we have a lot of plans in mind to scale up this program and make it accessible and useful for various audiences. our ultimate goal, and one that we've constantly been striving for since the start, was creating a valuable -----> tool !!!  that anyone, from any level of play, can effectively utilize (from amateur, to collegiate, and even to pro level). here are some of the next steps that we've planned out, after our week of developing ascend:\n\ud83d\udd17 ability to invite team members via email or unique url. this will make it so it's easily accessible and user friendly for people to adopt, while providing a simple solution to join a team. we also want to potentially add in various sign in methods to make the transition as smooth as possible.\n\ud83d\udcbb integration with other api's. one thing that we also want to include is integration with other useful api's that will allow for an improvement on our product, such as the ability to connect one's game account to the service.\n\ud83e\uddea product testing. initial beta testing for ascend will be done with our alma mater's collegiate esports team, as we are familiar with the users and managers. by being able to get this candid feedback, it'll give us the opportunity to see where improvements should be made and iterate on the product. eventually, we plan to use the connections from these teams to expand our testing to other campuses, and other various levels of play. of course, our number one goal is to listen to real users who would use this product to understand where the pain points are and create something that they really need.\n\ud83c\udf10 deployment (production). ray is currently preparing the marketing for a potential first release, after we are comfortable we have created a sufficient product that people will use, from our feedback. once that's ready, we'll release a production version for the public to use.\n\ud83d\udca1 new features. there's a vast amount of data that can be gathered from solo queue, scrim games, and much more! future features will use this information to provide teams with valuable insights that will help them improve.\n\ud83c\udfae add support for other games. of course, league of legends isn't the only game that makes up the competitive esports scene! from the start, our goal has always been to provide this service for various games. after we feel we have a working formula for league of legends, then we plan to move onto other games that could benefit from the service as well.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59504134}, {"Unnamed: 0": 4151, "autor": "VidNotes", "date": null, "content": "Inspiration\nWhen taking notes on a video (on a small screen), I often find myself switching between two tabs, which means that I miss half the video, or squinting to see the video when using a split screen. Both of these are not efficient methods, and I often have to rewind the video to find the answer to the question I missed. So, I decided to create VidNotes, an innovative new way to take notes. VidNotes lets you overlay your notes on top of a video, which allows you to see both at the same time.\nWhat it does\nVidNotes is a website that lets you take notes on a video more efficiently. Simply enter the link of a youtube video, and you'll be able to take notes on the video while seeing both your notes and the video. VidNotes provides a semitransparent overlay where you take notes, and because it's semitransparent, you'll still be able to see the video.\nVidNotes is also very customizable; you can change the opacity of the overlay to your liking and choose the color of the overlay.\nHow we built it\nI built VidNotes using Next.js and TailwindCSS.\nChallenges we ran into\nInitially, I planned for the notes overlay to open automatically when the user started typing, but keyboard events that happened when the video's iframe was focused weren't sent to the parent window. Instead, I opted to have a clickable button to open and close the overlay.\nAccomplishments that we're proud of\nI'm proud that I managed to create a usable product in such a short amount of time. This is definitely a tool I'll be able to use in my life, and hopefully it's useful for others too!\nWhat we learned\nI learned a lot about cross origin security when trying to debug the iframe. I also became more familiar with the use of Next.js and TailwindCSS.\nWhat's next for VidNotes\nI'm planning to add more customization, such as choosing any color for the background and also changing the text color. I also plan to allow users to customize the output, including the font size and font family of the printed version.", "link": "https://devpost.com/software/vidnotes", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwhen taking notes on a video (on a small screen), i often find myself switching between two tabs, which means that i miss half the video, or squinting to see the video when using a split screen. both of these are not efficient methods, and i often have to rewind the video to find the answer to the question i missed. so, i decided to create vidnotes, an innovative new way to take notes. vidnotes lets you overlay your notes on top of a video, which allows you to see both at the same time.\nwhat it does\nvidnotes is a website that lets you take notes on a video more efficiently. simply enter the link of a youtube video, and you'll be able to take notes on the video while seeing both your notes and the video. vidnotes provides a semitransparent overlay where you take notes, and because it's semitransparent, you'll still be able to see the video.\nvidnotes is also very customizable; you can change the opacity of the overlay to your liking and choose the color of the overlay.\nhow we built it\ni built vidnotes using next.js and tailwindcss.\nchallenges we ran into\ninitially, i planned for the notes overlay to open automatically when the user started typing, but keyboard events that happened when the video's iframe was focused weren't sent to the parent window. instead, i opted to have a clickable button to open and close the overlay.\naccomplishments that we're proud of\ni'm proud that i managed to create a usable product in such a short amount of time. this is definitely a -----> tool !!!  i'll be able to use in my life, and hopefully it's useful for others too!\nwhat we learned\ni learned a lot about cross origin security when trying to debug the iframe. i also became more familiar with the use of next.js and tailwindcss.\nwhat's next for vidnotes\ni'm planning to add more customization, such as choosing any color for the background and also changing the text color. i also plan to allow users to customize the output, including the font size and font family of the printed version.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59504151}, {"Unnamed: 0": 4164, "autor": "Have a Walk With me", "date": null, "content": "Inspiration\nWith the new normal and digitalization of every sector, we came to realise that there are mostly blogs available on the internet on Autism and felt that the Autism community was somewhere sidelined with having barely any resources or software which could help the parents and therapists continue their therapies virtually and still had to depend on traditional means of crafts etc to develop puzzles for children to teach life skills. On more surfing around the internet, we realised the gravity of the situation when there was only one website that had educational games teaching autistic kids eye contact etc. which had also been discontinued because of technical glitches. The severity of the situation alongside the fact that 1 in 54 children have autism inspired us to use technology to create tools catering to the needs of the children, parents and people associated with autism. Spreading awareness about Autism through our project has been the major driving force.\nWhat it does\nWalk with me is a working prototype aimed at incorporating technology into the traditional tools used in teaching people with autism basic life skills. The project is coded in colour blue and has the puzzle logo which is symbolic of autism and has been incorporated with the aim to spread awareness. The project has three distinct highlights aimed at trying to cover people who are on different levels of the autism spectrum. Special Educators use traditional means of making adaptive books to teach life skills like brushing teeth etc and to reduce the cumbersome workload in making work system using handcrafts, we decided to design a prototype centralizing to the people with autism by keeping them simple so that with one click of a mouse, the child can easily navigate through the steps and learn new skills making the work easier for both the educators and children.\nThe website in itself is a compact platform providing knowledge of Autism and spreading awareness about it through blogs, videos etc as well as linking the features together including the game prototype and bot.\nThe discord bot linked with the website is a tool for non-verbal autistic people (approx 30-40% of people on the spectrum are non-verbal) who can use the text input to interact with the bot. The bot provides positive reinforcements to the user which in return helps in uplifting the mood of the user as well as increasing confidence and preaching self-worth through phrases like \u201cYou can do it.\u201d,\u201cYou are the best.\u201d etc. It also has quotations from personalities like Rumi etc.\nHow we built it\nOur project is an amalgamation of the following things-\nDiscord Bot- Python Front End -HTML/CSS - Bootstrap - Javascript Back End - PHP Educational Game Prototype -Google Slides\nChallenges we ran into\n1)We faced a lot of system and software problems while the front end of the website was being created due to which we had to redo the whole front end. 2)We had one of our teammates leave us just after the start of the Hack due to some unforeseen circumstances, which made our schedule more scrambled but we managed to finish our project. 3) It took us some time and help from the mentors to figure out a way to present the idea of the game in form of a prototype but we managed to make the best use of google slides in prototyping our idea.\nAccomplishments that we're proud of\nWe now know the use of HTML/CSS, using the sandbox, bootstrap, template editing and shuffling at an intermediate level as well as learning how to create games/short animations using google slides. We are proud of how we were able to come together as a team since we had never met each other prior but were still able to work together in a collaborative and healthy environment.\nWhat we learned\nThe research phase of the hack was an eye-opening experience for us all as a team especially learning about Autism and how an autistic person spends his/her daily routine especially the tools and techniques used in teaching an autistic child life skills. We also realised that apart from numerous blogs on autism, there isn't much out there in terms of software or educational games etc. catering to the needs of autistic people etc which motivated us to take this project in the hackathon.\nOn the technical forefront, working on the project has taught us how to manage our time while simultaneously collaborating with each other and making quick decisions as well as learning new technical hacks like extensions etc which enables us to code in real-time.\nWhat's next for Have a Walk With Me\nPost the hackathon, converting the prototype for the educational games into actual games using platforms like unity is on our to-do list. Considering the lack of educational games built catering to autistic people, we are planning to improve our prototype and turn it into educational games teaching people with autism basic life skills helping them with their journey into being independent. We also plan on improving the discord bot as well as text to speech functionality keeping in mind the non-verbal children on the autism spectrum and helping them communicate through technology. We plan on making the security of our website as well as the discord so as to make sure there is no breach or unparliamentary action in our environment and to ensure the environment runs as a safe, civil and learning platform. Alongside this, we look forward to adding more functionality to the website and making it dynamic with good quality content and incorporating a better tech stack like react into it. We also plan on improving the content of the website in terms of resources etc like sharing the journey of people who are on the spectrum, sharing books written by people having autism, sessions and conferences catering to autism etc. We hope to even develop an app version of the website, incorporate an inbuilt chat feature for a peer to peer interaction or peer to admin interaction as well as develop more advanced, secure admin records with the database.\n2) WCAG We plan to integrate and follow the WCAG 2.0(WEB ACCESSIBILITY GUIDELINES) which are: 1)Provides content not prone to have seizures 2)Content should be substituted with pictures for better understanding 3)Sentence should not be cluttered 4)Font needs to be large and legible 5)Content present needs to be verified such that it is not prone to seizures.\nOverall we have a lot to look forward to in terms of our Hack.", "link": "https://devpost.com/software/have-a-walk-with-me", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwith the new normal and digitalization of every sector, we came to realise that there are mostly blogs available on the internet on autism and felt that the autism community was somewhere sidelined with having barely any resources or software which could help the parents and therapists continue their therapies virtually and still had to depend on traditional means of crafts etc to develop puzzles for children to teach life skills. on more surfing around the internet, we realised the gravity of the situation when there was only one website that had educational games teaching autistic kids eye contact etc. which had also been discontinued because of technical glitches. the severity of the situation alongside the fact that 1 in 54 children have autism inspired us to use technology to create tools catering to the needs of the children, parents and people associated with autism. spreading awareness about autism through our project has been the major driving force.\nwhat it does\nwalk with me is a working prototype aimed at incorporating technology into the traditional tools used in teaching people with autism basic life skills. the project is coded in colour blue and has the puzzle logo which is symbolic of autism and has been incorporated with the aim to spread awareness. the project has three distinct highlights aimed at trying to cover people who are on different levels of the autism spectrum. special educators use traditional means of making adaptive books to teach life skills like brushing teeth etc and to reduce the cumbersome workload in making work system using handcrafts, we decided to design a prototype centralizing to the people with autism by keeping them simple so that with one click of a mouse, the child can easily navigate through the steps and learn new skills making the work easier for both the educators and children.\nthe website in itself is a compact platform providing knowledge of autism and spreading awareness about it through blogs, videos etc as well as linking the features together including the game prototype and bot.\nthe discord bot linked with the website is a -----> tool !!!  for non-verbal autistic people (approx 30-40% of people on the spectrum are non-verbal) who can use the text input to interact with the bot. the bot provides positive reinforcements to the user which in return helps in uplifting the mood of the user as well as increasing confidence and preaching self-worth through phrases like \u201cyou can do it.\u201d,\u201cyou are the best.\u201d etc. it also has quotations from personalities like rumi etc.\nhow we built it\nour project is an amalgamation of the following things-\ndiscord bot- python front end -html/css - bootstrap - javascript back end - php educational game prototype -google slides\nchallenges we ran into\n1)we faced a lot of system and software problems while the front end of the website was being created due to which we had to redo the whole front end. 2)we had one of our teammates leave us just after the start of the hack due to some unforeseen circumstances, which made our schedule more scrambled but we managed to finish our project. 3) it took us some time and help from the mentors to figure out a way to present the idea of the game in form of a prototype but we managed to make the best use of google slides in prototyping our idea.\naccomplishments that we're proud of\nwe now know the use of html/css, using the sandbox, bootstrap, template editing and shuffling at an intermediate level as well as learning how to create games/short animations using google slides. we are proud of how we were able to come together as a team since we had never met each other prior but were still able to work together in a collaborative and healthy environment.\nwhat we learned\nthe research phase of the hack was an eye-opening experience for us all as a team especially learning about autism and how an autistic person spends his/her daily routine especially the tools and techniques used in teaching an autistic child life skills. we also realised that apart from numerous blogs on autism, there isn't much out there in terms of software or educational games etc. catering to the needs of autistic people etc which motivated us to take this project in the hackathon.\non the technical forefront, working on the project has taught us how to manage our time while simultaneously collaborating with each other and making quick decisions as well as learning new technical hacks like extensions etc which enables us to code in real-time.\nwhat's next for have a walk with me\npost the hackathon, converting the prototype for the educational games into actual games using platforms like unity is on our to-do list. considering the lack of educational games built catering to autistic people, we are planning to improve our prototype and turn it into educational games teaching people with autism basic life skills helping them with their journey into being independent. we also plan on improving the discord bot as well as text to speech functionality keeping in mind the non-verbal children on the autism spectrum and helping them communicate through technology. we plan on making the security of our website as well as the discord so as to make sure there is no breach or unparliamentary action in our environment and to ensure the environment runs as a safe, civil and learning platform. alongside this, we look forward to adding more functionality to the website and making it dynamic with good quality content and incorporating a better tech stack like react into it. we also plan on improving the content of the website in terms of resources etc like sharing the journey of people who are on the spectrum, sharing books written by people having autism, sessions and conferences catering to autism etc. we hope to even develop an app version of the website, incorporate an inbuilt chat feature for a peer to peer interaction or peer to admin interaction as well as develop more advanced, secure admin records with the database.\n2) wcag we plan to integrate and follow the wcag 2.0(web accessibility guidelines) which are: 1)provides content not prone to have seizures 2)content should be substituted with pictures for better understanding 3)sentence should not be cluttered 4)font needs to be large and legible 5)content present needs to be verified such that it is not prone to seizures.\noverall we have a lot to look forward to in terms of our hack.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59504164}, {"Unnamed: 0": 4187, "autor": "Highlight My Team", "date": null, "content": "Inspiration\nFor many businesses and restaurants, the products being sold and the people behind the POS are the reason customers keep coming back. Employees create memorable experiences, help customers find what they're looking for, and even show customers products they didn't know they needed before then. They're the backbone of growing a business, but now, more than ever, it's hard not to notice the hiring struggle and the toll it takes on a business's employees. Restaurants and bars with \"Now Hiring\" signs on the door and employees inside who haven't had a chance to catch a breath since their shift started. The retail worker helping three customers find what they need while answering the phone. These people are rockstars, and deserve to have their hard work highlighted and rewarded!\nWhat it does\nHighlight My Team provides Square business owners and their teams with a platform to celebrate and reward each others' efforts using resources owners already have- Square Team and Square Loyalty. Owners create rewards in Highlight My Team that they and their team can use to highlight one another- whether it's for covering a shift or crushing sales to always going above and beyond. These rewards have point values that directly tie back on a 1-to-1 basis with the business's Square loyalty program. This means team members with loyalty accounts will be rewarded not only when shopping at the business, but also for all the hard work they put into it!\nHow we built it\nHighlight My Team is built using Spring Boot and Angular, as these are the tools I use daily in my professional life. It also heavily uses the Square Java SDK. Authentication is handled using Auth0, and hosting, both site and database, are done with Heroku.\nChallenges we ran into\nThere were a couple challenges I faced with this project.\nThe first challenge was time. I found out about this hackathon fairly late into the game- at the beginning of November. Between then and now, I also missed a weekend of coding because of a trip planned months ago.\nThe second challenge was dealing with a bunch of newness- new to hackathons, new to implementing OAuth on my own, new to the Square APIs.\nFacing these two challenges taught me a lot:\nFocus on your core functionality. Does a full OAuth2 login experience provide more value than the items on your feature list?\nDon't take on more unknowns. Is a mailto link good enough to start, or is implementing an entire third-party email service you've never used before absolutely necessary? (Hint: I first tried the latter and failed)\nWhen time is of the essence, pick the easy route. Is writing a bunch of boilerplate worth it when there's a starter app available? AWS is cool but without setting up an environment and CI/CD can you get a back-end, website, and database deployed from your git repos with two CLI commands?\nAccomplishments that we're proud of\nI'm proud that I've laid the groundwork for a viable SaaS application given the time constraints and everything I had to learn. It includes a full integration with Square, has OAuth login experiences for both sellers and team members, and accomplishes my goal of rewarding businesses' employees for all the hard work they do.\nWhat we learned\nApart from what I mentioned in the challenges section, I've learned a tremendous amount about Square and its services. Once I understood the core concept of how the Java SDK worked, I was off to the races and impressed by how easy it was to use. Having ambitions to enter the SaaS space, Square is now in my tool belt for any commerce service I may need. I also learned a lot about Oauth, and how to authenticate users from multiple IDPs.\nThe overarching theme from this experience has been keep it simple, and focus on what delivers the most value. Being my first hackathon, I unfortunately learned that a little too late. I'm excited to use what I've learned for the next one, though!\nWhat's next for Highlight My Team\nIf the current Highlight My Team app was a house, it would only have the foundation (albeit a hopefully sturdy one)! Future plans include:\nIncorporate a business's name into the UI, and improve the fun of the user experience\nTeam and individual reward milestones for sales goals and shifts/time worked\nTeam member interactions on posted highlights (likes and comments)\nReward management allowing owners to create, delete, and update rewards along with placing restrictions on who can give rewards and how many\nTeam management\nEmail notifications\nCustom reward tiers for a business's team and not its customers\nOwner dashboard to see who is really shining on their team", "link": "https://devpost.com/software/highlight-my-team", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nfor many businesses and restaurants, the products being sold and the people behind the pos are the reason customers keep coming back. employees create memorable experiences, help customers find what they're looking for, and even show customers products they didn't know they needed before then. they're the backbone of growing a business, but now, more than ever, it's hard not to notice the hiring struggle and the toll it takes on a business's employees. restaurants and bars with \"now hiring\" signs on the door and employees inside who haven't had a chance to catch a breath since their shift started. the retail worker helping three customers find what they need while answering the phone. these people are rockstars, and deserve to have their hard work highlighted and rewarded!\nwhat it does\nhighlight my team provides square business owners and their teams with a platform to celebrate and reward each others' efforts using resources owners already have- square team and square loyalty. owners create rewards in highlight my team that they and their team can use to highlight one another- whether it's for covering a shift or crushing sales to always going above and beyond. these rewards have point values that directly tie back on a 1-to-1 basis with the business's square loyalty program. this means team members with loyalty accounts will be rewarded not only when shopping at the business, but also for all the hard work they put into it!\nhow we built it\nhighlight my team is built using spring boot and angular, as these are the tools i use daily in my professional life. it also heavily uses the square java sdk. authentication is handled using auth0, and hosting, both site and database, are done with heroku.\nchallenges we ran into\nthere were a couple challenges i faced with this project.\nthe first challenge was time. i found out about this hackathon fairly late into the game- at the beginning of november. between then and now, i also missed a weekend of coding because of a trip planned months ago.\nthe second challenge was dealing with a bunch of newness- new to hackathons, new to implementing oauth on my own, new to the square apis.\nfacing these two challenges taught me a lot:\nfocus on your core functionality. does a full oauth2 login experience provide more value than the items on your feature list?\ndon't take on more unknowns. is a mailto link good enough to start, or is implementing an entire third-party email service you've never used before absolutely necessary? (hint: i first tried the latter and failed)\nwhen time is of the essence, pick the easy route. is writing a bunch of boilerplate worth it when there's a starter app available? aws is cool but without setting up an environment and ci/cd can you get a back-end, website, and database deployed from your git repos with two cli commands?\naccomplishments that we're proud of\ni'm proud that i've laid the groundwork for a viable saas application given the time constraints and everything i had to learn. it includes a full integration with square, has oauth login experiences for both sellers and team members, and accomplishes my goal of rewarding businesses' employees for all the hard work they do.\nwhat we learned\napart from what i mentioned in the challenges section, i've learned a tremendous amount about square and its services. once i understood the core concept of how the java sdk worked, i was off to the races and impressed by how easy it was to use. having ambitions to enter the saas space, square is now in my -----> tool !!!  belt for any commerce service i may need. i also learned a lot about oauth, and how to authenticate users from multiple idps.\nthe overarching theme from this experience has been keep it simple, and focus on what delivers the most value. being my first hackathon, i unfortunately learned that a little too late. i'm excited to use what i've learned for the next one, though!\nwhat's next for highlight my team\nif the current highlight my team app was a house, it would only have the foundation (albeit a hopefully sturdy one)! future plans include:\nincorporate a business's name into the ui, and improve the fun of the user experience\nteam and individual reward milestones for sales goals and shifts/time worked\nteam member interactions on posted highlights (likes and comments)\nreward management allowing owners to create, delete, and update rewards along with placing restrictions on who can give rewards and how many\nteam management\nemail notifications\ncustom reward tiers for a business's team and not its customers\nowner dashboard to see who is really shining on their team", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59504187}, {"Unnamed: 0": 4192, "autor": "handyman", "date": null, "content": "Inspiration\nhandyman as the name suggest is a handy set of tools that improve the productivity of the customer service agents using Freshdesk platform to address issues.\nBased on the user feedback, here are some quick tools (recently viewed tickets, bookmark tickets with personal notes, grouped attachments and sentiment analysis) all under one widget that will help you take quick decisions, find what you are looking for faster and also understand the sentiment of the customer's ticket details.\nWhat it does\nAnalyze Customer's Sentiment\nEvery time a ticket is opened, the tool auto analyses the customer's ticket information and determines the mood/sentiment of the customer using an API and determines the polarity to be either positive, negative or neutral and also determines a score which is 0 for neutral, more than 0 for positive and less than 0 for negative.\nHelps the Agent understand the sensitivity and urgency of the ticket and act accordingly.\nBookmarklet\nThe bookmarklet widget allows the Agent to make a person note of each ticket that they want to visit back later. A bookmark can only be added once and the Agent can also delete his bookmarks and add them back with a new personal note. All bookmarks are visible once the Agent opens a ticket.\nHelps the Agent to make a note against tickets and keep track of what action needs to be taken.\nRecently Viewed Tickets\nThis simple tool allows the Agent to view the last few tickets seen. The number of tickets to be tracked can be set while installing the Handyman app itself.\nAttachments Assist\nAttachments Assist gives you a quick glimpse of all the attachments in a ticket's thread. As a customer service agent, this makes it a breeze for you to view all attachments in one place instead of you scrolling through the ticket to find these.\nThis widget also lets you download any of the attachment you like or preview it instantly in a new tab and take faster decisions.\nIf you as the agent are aware of the file size of the attachment, you can also quickly do a search and find the attachment you are looking for.\nMake it easy to find all attachments in one place especially for tickets that have responses spanning over weeks and months.\nHow I built it\nFeature/API Purpose & Description\nInstallation Parameters Installation parameters are used to store the API Key of the current user and also the total number of tickets to be tracked for \"Recently Viewed\" widget.\nConversations Conversations consist of replies as well as public and private notes added to a ticket. This is used to fetch the attachments of the current ticket opened by the Agent.\nApp lifecycle methods To load recently viewed tickets, get agent information to get all their bookmarks, get current ticket to analyse the sentiment of the customer.\nServerless app Data operations are performed on server side instead of client side for persistent store\nData Storage & Storage API To store/retrieve/delete the tickets of agent\nData Method 1. To get current logged in user details\n2. To get current domain\n3. To get current ticket details\nServer Method Invocation Calling server side to store/retrieve/delete data\nWhat's next for handyman\n[ ] Attachment information to be shown in a Modal for the Agent to view all the information from the ticket page itself.\n[ ] Add delete functionality to attachments for Agents.\n[ ] Add domain field in the Installation page Customize the installation page with APIs.\n[ ] Better tracking of recently viewed tickets and enabling de-duplications.\n[ ] Refactoring the code and better error handling acrosst he app.\nCredits\nBase: Bookmark Tickets Example by Freshworks team\nSENTIM-API - A free API for sentiment analysis", "link": "https://devpost.com/software/handyman-ck6g8m", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nhandyman as the name suggest is a handy set of tools that improve the productivity of the customer service agents using freshdesk platform to address issues.\nbased on the user feedback, here are some quick tools (recently viewed tickets, bookmark tickets with personal notes, grouped attachments and sentiment analysis) all under one widget that will help you take quick decisions, find what you are looking for faster and also understand the sentiment of the customer's ticket details.\nwhat it does\nanalyze customer's sentiment\nevery time a ticket is opened, the -----> tool !!!  auto analyses the customer's ticket information and determines the mood/sentiment of the customer using an api and determines the polarity to be either positive, negative or neutral and also determines a score which is 0 for neutral, more than 0 for positive and less than 0 for negative.\nhelps the agent understand the sensitivity and urgency of the ticket and act accordingly.\nbookmarklet\nthe bookmarklet widget allows the agent to make a person note of each ticket that they want to visit back later. a bookmark can only be added once and the agent can also delete his bookmarks and add them back with a new personal note. all bookmarks are visible once the agent opens a ticket.\nhelps the agent to make a note against tickets and keep track of what action needs to be taken.\nrecently viewed tickets\nthis simple tool allows the agent to view the last few tickets seen. the number of tickets to be tracked can be set while installing the handyman app itself.\nattachments assist\nattachments assist gives you a quick glimpse of all the attachments in a ticket's thread. as a customer service agent, this makes it a breeze for you to view all attachments in one place instead of you scrolling through the ticket to find these.\nthis widget also lets you download any of the attachment you like or preview it instantly in a new tab and take faster decisions.\nif you as the agent are aware of the file size of the attachment, you can also quickly do a search and find the attachment you are looking for.\nmake it easy to find all attachments in one place especially for tickets that have responses spanning over weeks and months.\nhow i built it\nfeature/api purpose & description\ninstallation parameters installation parameters are used to store the api key of the current user and also the total number of tickets to be tracked for \"recently viewed\" widget.\nconversations conversations consist of replies as well as public and private notes added to a ticket. this is used to fetch the attachments of the current ticket opened by the agent.\napp lifecycle methods to load recently viewed tickets, get agent information to get all their bookmarks, get current ticket to analyse the sentiment of the customer.\nserverless app data operations are performed on server side instead of client side for persistent store\ndata storage & storage api to store/retrieve/delete the tickets of agent\ndata method 1. to get current logged in user details\n2. to get current domain\n3. to get current ticket details\nserver method invocation calling server side to store/retrieve/delete data\nwhat's next for handyman\n[ ] attachment information to be shown in a modal for the agent to view all the information from the ticket page itself.\n[ ] add delete functionality to attachments for agents.\n[ ] add domain field in the installation page customize the installation page with apis.\n[ ] better tracking of recently viewed tickets and enabling de-duplications.\n[ ] refactoring the code and better error handling acrosst he app.\ncredits\nbase: bookmark tickets example by freshworks team\nsentim-api - a free api for sentiment analysis", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59504192}, {"Unnamed: 0": 4198, "autor": "Farming Simulator", "date": null, "content": "Inspiration\nThe art or science of cultivating the soil for the production of crops for human consumption plays a critical role for humanity, primitive necessities of life. Thanks to industrial movements, farmers have been able to revolutionize their methods and ways of increasing the quality and quantity of yield. However, the right physical tools aren't just enough. An abundance of factors such as weather, topography, soil fertility, water levels, seasons, pest/insect infestations that play a crucial role in the different types of crops. It gets overwhelmingly stressful trying to manage all of these along with extra care for lands.\nWhat it does\nOur team believes in reciprocating to the society, making the best use of technology to serve others and improve our lifestyles. Thus, we believe this tool will not only act as an educative virtual game environment for curious leaners/players but also for farmers to envision their yield and success.\nHow we built it\nUsing html, css and js.\nChallenges we ran into\nWe were ambitious about our planning but due to the time limit and the division of work among 2 people, we managed to add in the basic crucial factors.\nAccomplishments that we're proud of\nThe depth of the project, team collaboration and the efforts to use best of the languages we knew.\nWhat we learned\nWhile both of us were beginners/intermediates and had good knowledge on some languages, it was challenging to come up with a game simulator.\nWhat's next for Farming Simulator\nAccounting for other factors that could potentially affect farmers' yield while visualizing with better UI.", "link": "https://devpost.com/software/farming-simulator-2j5z4f", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nthe art or science of cultivating the soil for the production of crops for human consumption plays a critical role for humanity, primitive necessities of life. thanks to industrial movements, farmers have been able to revolutionize their methods and ways of increasing the quality and quantity of yield. however, the right physical tools aren't just enough. an abundance of factors such as weather, topography, soil fertility, water levels, seasons, pest/insect infestations that play a crucial role in the different types of crops. it gets overwhelmingly stressful trying to manage all of these along with extra care for lands.\nwhat it does\nour team believes in reciprocating to the society, making the best use of technology to serve others and improve our lifestyles. thus, we believe this -----> tool !!!  will not only act as an educative virtual game environment for curious leaners/players but also for farmers to envision their yield and success.\nhow we built it\nusing html, css and js.\nchallenges we ran into\nwe were ambitious about our planning but due to the time limit and the division of work among 2 people, we managed to add in the basic crucial factors.\naccomplishments that we're proud of\nthe depth of the project, team collaboration and the efforts to use best of the languages we knew.\nwhat we learned\nwhile both of us were beginners/intermediates and had good knowledge on some languages, it was challenging to come up with a game simulator.\nwhat's next for farming simulator\naccounting for other factors that could potentially affect farmers' yield while visualizing with better ui.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59504198}, {"Unnamed: 0": 4201, "autor": "Compounder", "date": null, "content": "Inspiration\nPlanning for the future is difficult and there aren't that many tools out there that give you a big picture view. Also, many prospective investors don't understand compounding growth until it's too late. To remedy this, I've created a new tool that anybody can use to plot out their investing future with anticipated increases in contributions, changing rates of growth, and changing rates of inflation. This tool will help users become more financially literate and inspire them to invest sooner. In the words of a certain man from Omaha, Nebraska, \"Start Early\". Compounding interest is the reason his value has tripled in just the past 20 years.\nWhat it does\nThis tool lets users plan out their investing future by visualizing investment values at points in the future and what it will be worth in year of calculation dollars decades from now. You can use this tool to plan for changes in expected annual growth, changes in inflation which in previous years has been below 2% but may be around 3% for the near future, and changes in expected contributions per year. All those together make a powerful tool for visualizing your investing future and how your money compounds.\nHow we built it\nI built this using React and google charts library with annotations. I deployed the production app by building it and hosting it on github pages for the public to use\nChallenges we ran into\nI wanted to use this hackathon to practice and hone frontend development skills. I got the opportunity to practice more react development and learn about the google charts library. One challenge that costed a few hours was getting annotations on charts (a key feature in this app) to work properly. As a result I had to read quite a bit of documentation and change my charting library.\nAccomplishments that we're proud of\nWorking application in a short period of time that improves upon existing smaller compound interest tools that have a lot less functionality. My tool is much better for users as they can adjust their investing windows and also account for inflation. Other tools only allow setting of one rate, one contribution per unit time, etc. Also the user interface and visualization is prettier.\nWhat we learned\nGained a lot of practice with frontend development. My experience is mainly in backend and that's what I've mainly contributed to in previous hackathons so this was a great learning opportunity to practice more, especially with react.\nWhat's next for Compounder\nAdding more investing tools like the ability to anticipate big income windfalls or withdrawals for reasons like starting a business, buying a house, paying for college, etc. Also improving the user interface so it's nicer and easier to use.\nTry it live here!\nhttps://adchungcsc.github.io/index.html\nSource Code here!\nhttps://github.com/adchungcsc/InterestCalculator", "link": "https://devpost.com/software/compounder", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nplanning for the future is difficult and there aren't that many tools out there that give you a big picture view. also, many prospective investors don't understand compounding growth until it's too late. to remedy this, i've created a new -----> tool !!!  that anybody can use to plot out their investing future with anticipated increases in contributions, changing rates of growth, and changing rates of inflation. this tool will help users become more financially literate and inspire them to invest sooner. in the words of a certain man from omaha, nebraska, \"start early\". compounding interest is the reason his value has tripled in just the past 20 years.\nwhat it does\nthis tool lets users plan out their investing future by visualizing investment values at points in the future and what it will be worth in year of calculation dollars decades from now. you can use this tool to plan for changes in expected annual growth, changes in inflation which in previous years has been below 2% but may be around 3% for the near future, and changes in expected contributions per year. all those together make a powerful tool for visualizing your investing future and how your money compounds.\nhow we built it\ni built this using react and google charts library with annotations. i deployed the production app by building it and hosting it on github pages for the public to use\nchallenges we ran into\ni wanted to use this hackathon to practice and hone frontend development skills. i got the opportunity to practice more react development and learn about the google charts library. one challenge that costed a few hours was getting annotations on charts (a key feature in this app) to work properly. as a result i had to read quite a bit of documentation and change my charting library.\naccomplishments that we're proud of\nworking application in a short period of time that improves upon existing smaller compound interest tools that have a lot less functionality. my tool is much better for users as they can adjust their investing windows and also account for inflation. other tools only allow setting of one rate, one contribution per unit time, etc. also the user interface and visualization is prettier.\nwhat we learned\ngained a lot of practice with frontend development. my experience is mainly in backend and that's what i've mainly contributed to in previous hackathons so this was a great learning opportunity to practice more, especially with react.\nwhat's next for compounder\nadding more investing tools like the ability to anticipate big income windfalls or withdrawals for reasons like starting a business, buying a house, paying for college, etc. also improving the user interface so it's nicer and easier to use.\ntry it live here!\nhttps://adchungcsc.github.io/index.html\nsource code here!\nhttps://github.com/adchungcsc/interestcalculator", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59504201}, {"Unnamed: 0": 4206, "autor": "Find My Study Spot", "date": null, "content": "Inspiration\nInspired by the prompts for the social good track, we wanted to create a website to allow UNC students to find study spots on campus that meet their specific needs. We wanted to collect UNC students\u2019 input on the most popular study spots\u2019 accessibility (for students with physical disabilities), atmosphere (for students with unique learning styles that prefer a quieter spot), staff support (for students who need assistance finding resources), and COVID-19 precautions (for COVID-conscious or immunocompromised students). We also intended to collect data on whether or not a study spot offers certain amenities (such as gender neutral bathrooms and standing desks) that could make a student\u2019s study experience more worthwhile. Finally, we also prioritized convenience by creating one easily accessible website with all the information students need, including a description, photo, and hours. Being able to access all of this information in one place will make students\u2019 lives easier as well as allow those with accommodations to feel at home on campus!\nWhat it does\nOur website allows students to view and add ratings of the main three study spots on UNC\u2019s campus: Davis Library, the Union, and the UL. Our website displays an image of the study spot, its hours, and a description of it, and it also allows students to rate the study spot based on certain categories: accessibility, atmosphere, staff support, COVID-19 precautions, and features (such as gender neutral restrooms and standing desks). Our website then displays the average of all these ratings so that students can find a study spot in the future that meets their needs.\nHow we built it\nWe primarily used the common web development languages, CSS, HTML, and Javascript. The webpage layouts were created with CSS and HTML and the rating and reviewing functionality were created with Javascript. We used JavaScript\u2019s localStorage functionality to store all the ratings that had been submitted on the site (even after the browser was closed), which was a brand new tool for us! After updating the average rating values on our \u201crate this study spot\u201d page, we used a querystring to pass the ratings back to the \u201cstudy spot description\u201d page and display the updated ratings. We combined our individual code and linked the pages together with github merging.\nChallenges we ran into\nNot everyone on the team was experienced with web development, so there was a bit of a learning curve at first, but eventually everyone understood the basics of web development, which is normally not possible overnight. We also weren\u2019t sure how to store the data for the ratings in the long term, even after the browser was closed, so we had to research a solution to that issue, and once we decided to use localstorage, we had to learn how to use it and implement it in our project. It was initially hard to figure out how to modify values stored in localstorage, and after we figured that out, we ran into the challenge of passing data from one page to another. We eventually resolved this issue using a querystring, which also required time to learn! We had a hard time integrating the javascript data collection part of our project (done by one team member) with the design part of the project (done by another team member), but we were eventually able to resolve that by working together.\nAccomplishments that we're proud of\nWe are proud of working together to complete a project. Usually in our computer science classes, we do not work in a group on coding assignments. So we were looking forward to working together in a hackathon and making the most out of it. All of us individually learned more about web development. Every member of our team was experienced in different areas, so we had a lot of fun learning from each other. Even though most of our beginners, we were proud of overcoming many of our challenges and delivering a project at the end. We are also proud of finding a method to utilize local storage to store previous survey results that can then automatically update on the main page. This was definitely a challenge but we are proud of our results. We also worked hard on making the website visually appealing and even UNC themed!\nWhat we learned\nSince this was the first hackathon most members of our team had participated in, we learned a lot about basic webdev, including HTML, CSS, and JavaScript. We learned about using localstorage to store values even after the webpage was closed, and how to set and get those values in localstorage when needed. We also learned about query strings, which we used to pass data from one page of our application to another, and how to create and parse them. We also learned a lot about working together on a group hackathon project, especially how to use GitHub to synchronize our work, divide up tasks appropriately, continuously update each other with our progress, and ask questions of each other when we needed help.\nWhat's next for Find My Study Spot\nWe are excited to take \u201cFind My Study Spot\u201d to the next level. To provide more security, we were thinking of implementing a feature so only authorized users such as UNC students add feedback on the study spots. We could redirect to a page that lets students log on via UNC onyen single sign on. In addition, like google maps, it will show you directions for how to get from your current location to the study spot. Another feature could be adding a filter to suggest only study spots that satisfy the user\u2019s preferences. Ideally, users would be able to drop pins on the map to suggest new study spots. In terms of back end, we would need to find a way to store large amounts of data (ratings) for long-term usage. Last but not the least, once we abstract it enough, this can be implemented across universities nationwide.", "link": "https://devpost.com/software/find-my-study-spot", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ninspired by the prompts for the social good track, we wanted to create a website to allow unc students to find study spots on campus that meet their specific needs. we wanted to collect unc students\u2019 input on the most popular study spots\u2019 accessibility (for students with physical disabilities), atmosphere (for students with unique learning styles that prefer a quieter spot), staff support (for students who need assistance finding resources), and covid-19 precautions (for covid-conscious or immunocompromised students). we also intended to collect data on whether or not a study spot offers certain amenities (such as gender neutral bathrooms and standing desks) that could make a student\u2019s study experience more worthwhile. finally, we also prioritized convenience by creating one easily accessible website with all the information students need, including a description, photo, and hours. being able to access all of this information in one place will make students\u2019 lives easier as well as allow those with accommodations to feel at home on campus!\nwhat it does\nour website allows students to view and add ratings of the main three study spots on unc\u2019s campus: davis library, the union, and the ul. our website displays an image of the study spot, its hours, and a description of it, and it also allows students to rate the study spot based on certain categories: accessibility, atmosphere, staff support, covid-19 precautions, and features (such as gender neutral restrooms and standing desks). our website then displays the average of all these ratings so that students can find a study spot in the future that meets their needs.\nhow we built it\nwe primarily used the common web development languages, css, html, and javascript. the webpage layouts were created with css and html and the rating and reviewing functionality were created with javascript. we used javascript\u2019s localstorage functionality to store all the ratings that had been submitted on the site (even after the browser was closed), which was a brand new -----> tool !!!  for us! after updating the average rating values on our \u201crate this study spot\u201d page, we used a querystring to pass the ratings back to the \u201cstudy spot description\u201d page and display the updated ratings. we combined our individual code and linked the pages together with github merging.\nchallenges we ran into\nnot everyone on the team was experienced with web development, so there was a bit of a learning curve at first, but eventually everyone understood the basics of web development, which is normally not possible overnight. we also weren\u2019t sure how to store the data for the ratings in the long term, even after the browser was closed, so we had to research a solution to that issue, and once we decided to use localstorage, we had to learn how to use it and implement it in our project. it was initially hard to figure out how to modify values stored in localstorage, and after we figured that out, we ran into the challenge of passing data from one page to another. we eventually resolved this issue using a querystring, which also required time to learn! we had a hard time integrating the javascript data collection part of our project (done by one team member) with the design part of the project (done by another team member), but we were eventually able to resolve that by working together.\naccomplishments that we're proud of\nwe are proud of working together to complete a project. usually in our computer science classes, we do not work in a group on coding assignments. so we were looking forward to working together in a hackathon and making the most out of it. all of us individually learned more about web development. every member of our team was experienced in different areas, so we had a lot of fun learning from each other. even though most of our beginners, we were proud of overcoming many of our challenges and delivering a project at the end. we are also proud of finding a method to utilize local storage to store previous survey results that can then automatically update on the main page. this was definitely a challenge but we are proud of our results. we also worked hard on making the website visually appealing and even unc themed!\nwhat we learned\nsince this was the first hackathon most members of our team had participated in, we learned a lot about basic webdev, including html, css, and javascript. we learned about using localstorage to store values even after the webpage was closed, and how to set and get those values in localstorage when needed. we also learned about query strings, which we used to pass data from one page of our application to another, and how to create and parse them. we also learned a lot about working together on a group hackathon project, especially how to use github to synchronize our work, divide up tasks appropriately, continuously update each other with our progress, and ask questions of each other when we needed help.\nwhat's next for find my study spot\nwe are excited to take \u201cfind my study spot\u201d to the next level. to provide more security, we were thinking of implementing a feature so only authorized users such as unc students add feedback on the study spots. we could redirect to a page that lets students log on via unc onyen single sign on. in addition, like google maps, it will show you directions for how to get from your current location to the study spot. another feature could be adding a filter to suggest only study spots that satisfy the user\u2019s preferences. ideally, users would be able to drop pins on the map to suggest new study spots. in terms of back end, we would need to find a way to store large amounts of data (ratings) for long-term usage. last but not the least, once we abstract it enough, this can be implemented across universities nationwide.", "sortedWord": "None", "removed": "Nan", "score": 9, "comments": 0, "media": null, "medialink": null, "identifyer": 59504206}, {"Unnamed: 0": 4209, "autor": "Dialog", "date": null, "content": "Inspiration\nAs university students, we are swamped by endless amounts of assignments, homework, pre-readings, and much more. Additionally, all of us suffer greatly at the hands of procrastination. We find that the hardest started part about getting tasks done is starting with them. As such, we were inspired to create an application that tackled this very problem: starting tasks.\nWhat it does\nDialog allows its users to approach their tasks in a very simple, minimalistic, and calming approach. By encouraging its users to begin with the tasks that require the least amount of time, Dialog slowly builds the user's confidence via positive reinforcement in the form of motivational quotes and makes them feel ready to tackle harder, more demanding tasks.\nHow we built it\nDialog was built using SvelteJS, JavaScript, HTML, CSS, and Python. Over the 25 hours of hacking, we utilised Git source control, VsCode, and Miro whiteboards to plan, sketch, and code. The entire project began with a few rough sketches and browses through r/productivity - primarily looking for something to help us through the hackathon. What we ended with were meticulous designs and colour palettes, and a productivity tool that we would have loved to have and use throughout the hackathon itself.\nChallenges we ran into\nWe faced many challenges this weekend. Firstly, many of our team members did not know what SvelteJS was and how to use it, so they had to first learn through the official SvelteJS tutorials and then rely on the documentation to further their knowledge. In fact, some members of our group also needed to learn JavaScript and HTML to even get started with using Svelte. Another big challenge we faced was trying to come up with a reasonable algorithm to mathematically determine which of the available tasks in our application should be chosen based on its priority, due date, and time to completion.\nAccomplishments that we're proud of\nDialog.\nWhat we learned\nSvelteJS. JavaScript. Version Control. Communication. Collaboration. Teamwork.\nWhat's next for Dialog\nOur future plans for Dialog include switching over to ReactJS due to multiple reasons. Firstly, ReactJS is much more powerful and faster than SvelteJS. Not only does it have much more extensive documentation, but its larger community ensures greater support and availability of tools and resources to aid future development. We also plan to add an ExpressJS server to run a server-side neural network recommender system to improve on the actual Dialog algorithm.", "link": "https://devpost.com/software/dialog-tzjxfh", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nas university students, we are swamped by endless amounts of assignments, homework, pre-readings, and much more. additionally, all of us suffer greatly at the hands of procrastination. we find that the hardest started part about getting tasks done is starting with them. as such, we were inspired to create an application that tackled this very problem: starting tasks.\nwhat it does\ndialog allows its users to approach their tasks in a very simple, minimalistic, and calming approach. by encouraging its users to begin with the tasks that require the least amount of time, dialog slowly builds the user's confidence via positive reinforcement in the form of motivational quotes and makes them feel ready to tackle harder, more demanding tasks.\nhow we built it\ndialog was built using sveltejs, javascript, html, css, and python. over the 25 hours of hacking, we utilised git source control, vscode, and miro whiteboards to plan, sketch, and code. the entire project began with a few rough sketches and browses through r/productivity - primarily looking for something to help us through the hackathon. what we ended with were meticulous designs and colour palettes, and a productivity -----> tool !!!  that we would have loved to have and use throughout the hackathon itself.\nchallenges we ran into\nwe faced many challenges this weekend. firstly, many of our team members did not know what sveltejs was and how to use it, so they had to first learn through the official sveltejs tutorials and then rely on the documentation to further their knowledge. in fact, some members of our group also needed to learn javascript and html to even get started with using svelte. another big challenge we faced was trying to come up with a reasonable algorithm to mathematically determine which of the available tasks in our application should be chosen based on its priority, due date, and time to completion.\naccomplishments that we're proud of\ndialog.\nwhat we learned\nsveltejs. javascript. version control. communication. collaboration. teamwork.\nwhat's next for dialog\nour future plans for dialog include switching over to reactjs due to multiple reasons. firstly, reactjs is much more powerful and faster than sveltejs. not only does it have much more extensive documentation, but its larger community ensures greater support and availability of tools and resources to aid future development. we also plan to add an expressjs server to run a server-side neural network recommender system to improve on the actual dialog algorithm.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59504209}, {"Unnamed: 0": 4211, "autor": "News Verification Toolkit", "date": null, "content": "Media Forensics Toolkit\nThe unprecedented growth of technology in the past few decades has led to massive flooding of the market with media forgery tools like Photoshop, gimp, Deep Dream Generator etc. These tempered media sources have contributed to other major problems like fake news and even cyberbullying. Furthermore, advancements of deep learning lead to a more severe problem known as Deepfakes and Generative Adversarial Networks(GAN) generated images and videos, that are even harder to detect and investigate in case of a legal breach. This project aims to address this challenge by implementing two novel deep learning models that can be used conjointly with some static models to provide complete protection against almost all types of image and video forgeries including Deepfakes, GAN generated forged media, photoshopped media (splicing, copy and move forgeries). For Deepfakes detection, this project implements a Timedistributed CNN-LSTM neural network and for GAN generated media detection, this project uses a fine-tuned DenseNet model as the base Convolutional neural network. The statistical models include generation of images by masking original image through various algorithms like Error Level Analysis, Laplace transforms, variance masking, pixel density algorithm, and generation of Exifdata of media through various tools and packages like hachoir, ExifTool, etc. Compared with existing strategies, the evaluation results demonstrate that the novel strategy can serve as a one-stop tool for forensic investigators, thus providing holistic protection against threats of today(pixel based forgeries) as well as future(vector based forgeries).\nData Preparation\nThe mean channel of images/frames was removed.\nAll frames/images were normalized\nThe fully-connected layer at the top of the network was removed ( for our DenseNet pretrained fine-tuned models)\nAll frames and images were resized according to adjust with our pretrained models.\nVarious data augmentation functions like adaptive Gaussian noise, emboss image, salt image, black hat filter, rotation, flipping, Affine Transformation, warping, increasing contrast were used to make the model more robust to changes.\nFrom the Keras preprocessing package, we used ImageDataGenerator to augment image input and use transformations as rescaling, shearing, zoom, flipping, height shift, width shift, fill mode, data format, brightness, and feature-wise normalize. ## Implemented models {AI-powered} 1) Deepfakes For Deepfakes, we used a Timedistributed CNN-LSTM neural network. TimeDistributed Layer keeps input to output relation proper, and avoid mixing of outputs. In this model, the function of CNN is feature extraction, and LSTM for sequence processing. In CNN, we used the initializer as glorot uniform, and reg lambda parameter to be 0.001. Various convolutional layers of variable kernel filters(32,64,128,256,512) are used to extract different features of frame/images. Different strides were used in pooling to gain access to different aspects of the image.The 2048-dimensional feature vectors after the last pooling layers are then used as the sequential LSTM input. We used stacked LSTM to increase sequence processing at different intervals ( units = 512, 256). Stacked LSTM gives robustness and increases the accuracy of our model as almost all correlations between features extracted by CNN are analyzed by it. 2) GAN generated media For the detection of GAN images/frames, we used a Finetuned DenseNet network along with an simple ANN( Artificial Neural Network). Sequential training was done on the network. Firstly, all layers apart from the last two layers of Densenet were frozen, and the model was trained on the rest of the layers. After initial training, all layers were unfrozen, and the whole model was again trained. Due to simultaneous training, the end layer\u2019s (deciding layers) converge best to the global optima and give an accuracy boost to our model. Also Dropout(F 1 ) and batch normalization layers(F 2 ) were added regularly to prevent overfitting. ## Implemented Statistical models\nExif-Data Analysis In our model, metadata extraction was done for all videos/images by using sequential use of a tool called ExifTool and a python package called hachoir. After data retrieval from both tools, the redundant data was removed and useful data like device id, device manufacturer, software used, encoder, etc were carved out into a .csv file. This piece of information, which is often neglected, can open new avenues to investigation and may also reveal personal identification proofs like the MAC address of original equipment used.\nMasking Algorithms Image masking is a process by which we can hide some aspects or parts of an image and highlight some other aspects of the image like highlighting high illuminance pixel entropy areas. Several masking algorithms like pixel density, Bitwise-AND masking, Variance Masking, Laplace detector, ELA are used in our project that can accurately detect any type of pixel based manipulation like photoshopped media(splicing, etc) ## Future Scope The future additions that could be made to this project are:\nNeural networks can be debugged for finding the performance of each layer of the network, thus finding any dead neuron or low learning by any layer.\nMore juicy forensic artefacts could be collected by adding tools like Ghiro, JPEGsnoop ( better source to find last device used), stegsolve ( switch through the layers and look for abnormalities) to the main project, thus diversifying the static metadata information collected.\nReverse searching of image/video could be added to the project, thus backtracking original digital media resources (scan through different open-source search engines)", "link": "https://devpost.com/software/news-verification-toolkit", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "media forensics toolkit\nthe unprecedented growth of technology in the past few decades has led to massive flooding of the market with media forgery tools like photoshop, gimp, deep dream generator etc. these tempered media sources have contributed to other major problems like fake news and even cyberbullying. furthermore, advancements of deep learning lead to a more severe problem known as deepfakes and generative adversarial networks(gan) generated images and videos, that are even harder to detect and investigate in case of a legal breach. this project aims to address this challenge by implementing two novel deep learning models that can be used conjointly with some static models to provide complete protection against almost all types of image and video forgeries including deepfakes, gan generated forged media, photoshopped media (splicing, copy and move forgeries). for deepfakes detection, this project implements a timedistributed cnn-lstm neural network and for gan generated media detection, this project uses a fine-tuned densenet model as the base convolutional neural network. the statistical models include generation of images by masking original image through various algorithms like error level analysis, laplace transforms, variance masking, pixel density algorithm, and generation of exifdata of media through various tools and packages like hachoir, exiftool, etc. compared with existing strategies, the evaluation results demonstrate that the novel strategy can serve as a one-stop -----> tool !!!  for forensic investigators, thus providing holistic protection against threats of today(pixel based forgeries) as well as future(vector based forgeries).\ndata preparation\nthe mean channel of images/frames was removed.\nall frames/images were normalized\nthe fully-connected layer at the top of the network was removed ( for our densenet pretrained fine-tuned models)\nall frames and images were resized according to adjust with our pretrained models.\nvarious data augmentation functions like adaptive gaussian noise, emboss image, salt image, black hat filter, rotation, flipping, affine transformation, warping, increasing contrast were used to make the model more robust to changes.\nfrom the keras preprocessing package, we used imagedatagenerator to augment image input and use transformations as rescaling, shearing, zoom, flipping, height shift, width shift, fill mode, data format, brightness, and feature-wise normalize. ## implemented models {ai-powered} 1) deepfakes for deepfakes, we used a timedistributed cnn-lstm neural network. timedistributed layer keeps input to output relation proper, and avoid mixing of outputs. in this model, the function of cnn is feature extraction, and lstm for sequence processing. in cnn, we used the initializer as glorot uniform, and reg lambda parameter to be 0.001. various convolutional layers of variable kernel filters(32,64,128,256,512) are used to extract different features of frame/images. different strides were used in pooling to gain access to different aspects of the image.the 2048-dimensional feature vectors after the last pooling layers are then used as the sequential lstm input. we used stacked lstm to increase sequence processing at different intervals ( units = 512, 256). stacked lstm gives robustness and increases the accuracy of our model as almost all correlations between features extracted by cnn are analyzed by it. 2) gan generated media for the detection of gan images/frames, we used a finetuned densenet network along with an simple ann( artificial neural network). sequential training was done on the network. firstly, all layers apart from the last two layers of densenet were frozen, and the model was trained on the rest of the layers. after initial training, all layers were unfrozen, and the whole model was again trained. due to simultaneous training, the end layer\u2019s (deciding layers) converge best to the global optima and give an accuracy boost to our model. also dropout(f 1 ) and batch normalization layers(f 2 ) were added regularly to prevent overfitting. ## implemented statistical models\nexif-data analysis in our model, metadata extraction was done for all videos/images by using sequential use of a tool called exiftool and a python package called hachoir. after data retrieval from both tools, the redundant data was removed and useful data like device id, device manufacturer, software used, encoder, etc were carved out into a .csv file. this piece of information, which is often neglected, can open new avenues to investigation and may also reveal personal identification proofs like the mac address of original equipment used.\nmasking algorithms image masking is a process by which we can hide some aspects or parts of an image and highlight some other aspects of the image like highlighting high illuminance pixel entropy areas. several masking algorithms like pixel density, bitwise-and masking, variance masking, laplace detector, ela are used in our project that can accurately detect any type of pixel based manipulation like photoshopped media(splicing, etc) ## future scope the future additions that could be made to this project are:\nneural networks can be debugged for finding the performance of each layer of the network, thus finding any dead neuron or low learning by any layer.\nmore juicy forensic artefacts could be collected by adding tools like ghiro, jpegsnoop ( better source to find last device used), stegsolve ( switch through the layers and look for abnormalities) to the main project, thus diversifying the static metadata information collected.\nreverse searching of image/video could be added to the project, thus backtracking original digital media resources (scan through different open-source search engines)", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59504211}, {"Unnamed: 0": 4224, "autor": "String Theory of Life", "date": null, "content": "What it does\nSimulates fibers undergoing Brownian motion. With some probability, the fibers can 'cyclize' and become 'alive'. Fibers that are alive then need to compete for resources. The fibers are named after contestants in the hackathon!\nInspiration\nRouse model of polymers\nAgar.io\nGenetic algorithms\nBrownian motion\nHow we built it\nAll scripts were written in python. There are two components; simulation and visualization.\nChallenges we ran into\nGetting both components to work with each other.\nBalancing the rates and strengths of different interactions.\nAccomplishments that we're proud of\nFun idea (Agar.io but with Brownian motion).\nInteresting visuals.\nNeat simulation.\nWhat we learned\nHow to make efficient large-scale simulations.\nHow the Rouse model works.\nHow to render images from python.\nWhat's next for String Theory of Life\nWe think that \"Agar.io but with Brownian motion\" has a lot of potential as a learning tool.\nMaybe the agents could develop active movement.", "link": "https://devpost.com/software/string-theory-of-life", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what it does\nsimulates fibers undergoing brownian motion. with some probability, the fibers can 'cyclize' and become 'alive'. fibers that are alive then need to compete for resources. the fibers are named after contestants in the hackathon!\ninspiration\nrouse model of polymers\nagar.io\ngenetic algorithms\nbrownian motion\nhow we built it\nall scripts were written in python. there are two components; simulation and visualization.\nchallenges we ran into\ngetting both components to work with each other.\nbalancing the rates and strengths of different interactions.\naccomplishments that we're proud of\nfun idea (agar.io but with brownian motion).\ninteresting visuals.\nneat simulation.\nwhat we learned\nhow to make efficient large-scale simulations.\nhow the rouse model works.\nhow to render images from python.\nwhat's next for string theory of life\nwe think that \"agar.io but with brownian motion\" has a lot of potential as a learning -----> tool !!! .\nmaybe the agents could develop active movement.", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59504224}, {"Unnamed: 0": 4227, "autor": "Multibody Pendulum Visualization Tool", "date": null, "content": "Try it out here\nhttps://yaeldemers.com/multibody-pendulum-visualization\nWhat it does\nIn the context of this year's Physics Hackathon, we decided to work on a visualization tool for pendulum motion. As we researched the topic, we realized most simulator focused on single and double pendulums, but we wondered what would happen as you increase the amount of bobs in the system? How chaotic could this get?\nHow we built it\nAs we wanted this tool to be interactive, we thought it would be a great idea to make this in JavaScript. This way, anyone could try it and have its own spin on it. We used the p5.js framework to draw the systems and hosted them on HTML5 pages. The tricky part was getting the equations of motion right. For a simple pendulum, classical mechanics equations are sufficient, however we had to turn to Lagrangian mechanics and many papers to figure out the motion of multibody pendulums. Note, to show the chaos induced by multibody pendulums, we plotted the position of the bottom bob over time.\nChallenges we ran into\nWe were able to recreate the motion of a single & double pendulums but we could not successfully create the motion of a triple pendulum because the equations used for this motion is only applicable for small angles. The proper ones could be derived in a similar fashion as the ones for the double pendulum, but the work to do so is significantly longer and we had to compromise due to the time limit.\nAnother problem we ran into consisted of our use of the time step. As our simulation runs in-real time, at 60 frame per second (FPS), we had to make sure to adapt the time step so that the calculations were not done too many times. This led us to have a more realistic simulation while reducing the computational power needed for the whole project.\nAccomplishments that we're proud of\nWe studied a new language( JavaScript) and one of its frameworks ( p5.js ) within the limited hours and were able to successfully create a simulation of single & double pendulums and its phase space. We were also able to derive the angular momentum of a pendulum with a limited knowledge in physics.\nWhat we learned\nDuring this project, we studied the motion of a pendulum, especially how the motion of the pendulums are heavily influenced by the length of the wires of each pendulums, mass (for double pendulums and onward), gravity, dampening and the number of bobs in a pendulum. It was great to see how increasing the number of bobs induces more chaos to the system. Also, we learned about phase-paces and how to graph them.\nWhat's next for Multibody Pendulum Visualization Tool\nAs we approached an increasing amount of pendulum's bobs, we realized a generalization formula might be necessary to qualify the equations of motion (EOM) of said bobs. Each mass added to the system significantly complexifies the EOMs and hence make the code tedious to write, and read. Once a generalization formula is found, it will be easy to visualize how the system behaves as the number of bobs gets bigger. An other option could be to use an other language / framework to calculate said EOMs (for example Python and Sympy or Matlab). As we introduced in our double pendulum system, phase-space diagrams could also be an interesting approach to this topic as it touches some concepts of chaos theory.", "link": "https://devpost.com/software/multibody-pendulum-visualization-tool", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "try it out here\nhttps://yaeldemers.com/multibody-pendulum-visualization\nwhat it does\nin the context of this year's physics hackathon, we decided to work on a visualization -----> tool !!!  for pendulum motion. as we researched the topic, we realized most simulator focused on single and double pendulums, but we wondered what would happen as you increase the amount of bobs in the system? how chaotic could this get?\nhow we built it\nas we wanted this tool to be interactive, we thought it would be a great idea to make this in javascript. this way, anyone could try it and have its own spin on it. we used the p5.js framework to draw the systems and hosted them on html5 pages. the tricky part was getting the equations of motion right. for a simple pendulum, classical mechanics equations are sufficient, however we had to turn to lagrangian mechanics and many papers to figure out the motion of multibody pendulums. note, to show the chaos induced by multibody pendulums, we plotted the position of the bottom bob over time.\nchallenges we ran into\nwe were able to recreate the motion of a single & double pendulums but we could not successfully create the motion of a triple pendulum because the equations used for this motion is only applicable for small angles. the proper ones could be derived in a similar fashion as the ones for the double pendulum, but the work to do so is significantly longer and we had to compromise due to the time limit.\nanother problem we ran into consisted of our use of the time step. as our simulation runs in-real time, at 60 frame per second (fps), we had to make sure to adapt the time step so that the calculations were not done too many times. this led us to have a more realistic simulation while reducing the computational power needed for the whole project.\naccomplishments that we're proud of\nwe studied a new language( javascript) and one of its frameworks ( p5.js ) within the limited hours and were able to successfully create a simulation of single & double pendulums and its phase space. we were also able to derive the angular momentum of a pendulum with a limited knowledge in physics.\nwhat we learned\nduring this project, we studied the motion of a pendulum, especially how the motion of the pendulums are heavily influenced by the length of the wires of each pendulums, mass (for double pendulums and onward), gravity, dampening and the number of bobs in a pendulum. it was great to see how increasing the number of bobs induces more chaos to the system. also, we learned about phase-paces and how to graph them.\nwhat's next for multibody pendulum visualization tool\nas we approached an increasing amount of pendulum's bobs, we realized a generalization formula might be necessary to qualify the equations of motion (eom) of said bobs. each mass added to the system significantly complexifies the eoms and hence make the code tedious to write, and read. once a generalization formula is found, it will be easy to visualize how the system behaves as the number of bobs gets bigger. an other option could be to use an other language / framework to calculate said eoms (for example python and sympy or matlab). as we introduced in our double pendulum system, phase-space diagrams could also be an interesting approach to this topic as it touches some concepts of chaos theory.", "sortedWord": "None", "removed": "Nan", "score": 24, "comments": 5, "media": null, "medialink": null, "identifyer": 59504227}, {"Unnamed: 0": 4239, "autor": "Visual Arithmetech", "date": null, "content": "Inspiration\nOur group having worked with young students virtually, we were aware that keeping students engaged can be very challenging, and modeling arithmetic without physical models or blocks to work with is difficult or impossible. Our goal was to create a simple, intuitive tool which students and teachers could use for modeling.\nWhat it does\nVisual Arithmetech allows users to add blocks and group them into groups of up to 10 by dragging blocks together. The left and right buckets count the number of blocks in each of them, and by selecting a basic operation all the blocks will be deleted and replaced by the resulting number of the operation. There is also a clear button which removes all blocks from the scene.\nHow we built it\nVisual Arithmetech was built in Unity, built for WebGL, uploaded on Simmer.io, and hosted on a website using Domain.com. First, an initial prototype of the block prefabs was created to ensure the blocks functioned on their own before the prototype was expanded to include visuals and additional functionality. Once this prototype worked, we added the buckets (with counting system), implemented ui elements, altered the visuals, and finally added the arithmetic functions.\nChallenges we ran into\nThere was plenty of debugging along the way as a result of strange collisions from the physics engine. Not to mention that some seemingly simple problems ended up being rather complicated, such as ensuring the bucket counts stayed consistent when blocks were combined, or the red-ish lines bordering the blocks. By far our largest challenge was using Domain.com to host the app, which ended up requiring many expert opinions and visual compromises to get functioning.\nAccomplishments that we're proud of\nWe're proud of the fact that we made a functional and useful design in the 24 hour time window. Despite this being our first hackathon, we successfully brainstormed and developed an app which isn't too messy, and can be easily expanded.\nWhat we learned\nSome of us had never used Unity before, and as a result got a basic understanding of the Unity ui. We learned that Google hates Domain.com, that Domain.com hates Simmer.io, that Simmer.io hates compression, and that physics engines can only handle so much. We were also reminded that C# is a far superior language to C++.\nWhat's next for Visual Arithmetech\nDuring our initial brainstorming we came up with many features which we had set aside as \"features if we have time\". Most importantly, we'd like to expand Visual Arithmetech to be more playful. The current form of the app is just a \"sandbox\", where users can mess around with blocks, but we could easily include a system where a user would have to use arithmetic to reach a certain number using as few blocks and/or button presses as possible. We could also expand the game to include more complex functions or mathematical models, such as exponents or factorials.", "link": "https://devpost.com/software/visual-arithmetech", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nour group having worked with young students virtually, we were aware that keeping students engaged can be very challenging, and modeling arithmetic without physical models or blocks to work with is difficult or impossible. our goal was to create a simple, intuitive -----> tool !!!  which students and teachers could use for modeling.\nwhat it does\nvisual arithmetech allows users to add blocks and group them into groups of up to 10 by dragging blocks together. the left and right buckets count the number of blocks in each of them, and by selecting a basic operation all the blocks will be deleted and replaced by the resulting number of the operation. there is also a clear button which removes all blocks from the scene.\nhow we built it\nvisual arithmetech was built in unity, built for webgl, uploaded on simmer.io, and hosted on a website using domain.com. first, an initial prototype of the block prefabs was created to ensure the blocks functioned on their own before the prototype was expanded to include visuals and additional functionality. once this prototype worked, we added the buckets (with counting system), implemented ui elements, altered the visuals, and finally added the arithmetic functions.\nchallenges we ran into\nthere was plenty of debugging along the way as a result of strange collisions from the physics engine. not to mention that some seemingly simple problems ended up being rather complicated, such as ensuring the bucket counts stayed consistent when blocks were combined, or the red-ish lines bordering the blocks. by far our largest challenge was using domain.com to host the app, which ended up requiring many expert opinions and visual compromises to get functioning.\naccomplishments that we're proud of\nwe're proud of the fact that we made a functional and useful design in the 24 hour time window. despite this being our first hackathon, we successfully brainstormed and developed an app which isn't too messy, and can be easily expanded.\nwhat we learned\nsome of us had never used unity before, and as a result got a basic understanding of the unity ui. we learned that google hates domain.com, that domain.com hates simmer.io, that simmer.io hates compression, and that physics engines can only handle so much. we were also reminded that c# is a far superior language to c++.\nwhat's next for visual arithmetech\nduring our initial brainstorming we came up with many features which we had set aside as \"features if we have time\". most importantly, we'd like to expand visual arithmetech to be more playful. the current form of the app is just a \"sandbox\", where users can mess around with blocks, but we could easily include a system where a user would have to use arithmetic to reach a certain number using as few blocks and/or button presses as possible. we could also expand the game to include more complex functions or mathematical models, such as exponents or factorials.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59504239}, {"Unnamed: 0": 4255, "autor": "BeHeard", "date": null, "content": "Inspiration\nSpeech and communication disabilities affect millions of children and adults all around the world. Unfortunately, accessible technology that individuals can use to communicate their needs in a quick and destigmatized way is not universally available. We were inspired to develop a tool that can help these individuals communicate and express themselves in the way they want to be heard and seen.\nWhat it does\nBeHeard allows users to input their disabilities and the ways others can best support them into a form. It then creates a profile card with this information which can be shown to those around them. This allows other to quickly see the ways they can adapt their behaviour to include and support the user, mitigating the challenges and frustrations of repetition and communication.\nHow we built it\nWe built the app in JS using Expo in React Native. Separate from the technical aspects, we spent time researching into common non-verbal challenges and how individuals who experience those would want to be supported.\nChallenges we ran into\nWe had experience in front-end development, but not in React Native. This proved to be a challenge when we began styling. Our team of 4 members also had many commitments throughout this weekend, thus there was less contribution from each member than expected when we set our goals for the project.\nAccomplishments that we're proud of\nWe successfully built the frame of the app, implementing the required logic and information to serve as a usable tool! (Joel is proud of his wireframes)\nWhat we learned\nWe learned how to use new technologies including React Native and how to work effectively on a development team. We also learned so much about the community that lives with disabilities and how they have been historically misrepresented and excluded. This project gave us an incredible opportunity to learn more about the need for accessible tools, recognizing and appreciating the perspectives that are often overlooked in society.\nWhat's next for BeHeard\nWe think it's vital that upcoming developments for our product begin to include the perspective and input of an individual living with communication disabilities. In these developments, we hope to implement further options for users to include in their profile, and potentially offer a social aspect for users to connect with each other within their community!", "link": "https://devpost.com/software/beheard-7dhux4", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nspeech and communication disabilities affect millions of children and adults all around the world. unfortunately, accessible technology that individuals can use to communicate their needs in a quick and destigmatized way is not universally available. we were inspired to develop a -----> tool !!!  that can help these individuals communicate and express themselves in the way they want to be heard and seen.\nwhat it does\nbeheard allows users to input their disabilities and the ways others can best support them into a form. it then creates a profile card with this information which can be shown to those around them. this allows other to quickly see the ways they can adapt their behaviour to include and support the user, mitigating the challenges and frustrations of repetition and communication.\nhow we built it\nwe built the app in js using expo in react native. separate from the technical aspects, we spent time researching into common non-verbal challenges and how individuals who experience those would want to be supported.\nchallenges we ran into\nwe had experience in front-end development, but not in react native. this proved to be a challenge when we began styling. our team of 4 members also had many commitments throughout this weekend, thus there was less contribution from each member than expected when we set our goals for the project.\naccomplishments that we're proud of\nwe successfully built the frame of the app, implementing the required logic and information to serve as a usable tool! (joel is proud of his wireframes)\nwhat we learned\nwe learned how to use new technologies including react native and how to work effectively on a development team. we also learned so much about the community that lives with disabilities and how they have been historically misrepresented and excluded. this project gave us an incredible opportunity to learn more about the need for accessible tools, recognizing and appreciating the perspectives that are often overlooked in society.\nwhat's next for beheard\nwe think it's vital that upcoming developments for our product begin to include the perspective and input of an individual living with communication disabilities. in these developments, we hope to implement further options for users to include in their profile, and potentially offer a social aspect for users to connect with each other within their community!", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59504255}, {"Unnamed: 0": 4293, "autor": "Tasket", "date": null, "content": "Inspiration\nI live with a lot of other people and it's been a challenge at time to organize our weekly cleaning and other things that need to happen in the house. This tool aims to simplify and automate that process, which would remove some stress from me\nWhat it does\nTasket organizes people into groups and adds tasks to these groups. The group admin can then assign tasks to the group, which randomly assign tasks to each group member, until either tasks or group members runs out. It then shows all members their assigned task, a deadline, and whether or not it is completed.\nTasks can be re-assigned at anytime by the group admin, and all members can see who has completed there tasks as well as if a task is overdue.\nHow we built it\nThis is built using Firebase and React, using Material UI as the component library\nChallenges we ran into\nI'm working on this alone, so I had to decide what to prioritize and what to wait on. Additionally, I'm not super familiar with firebase, so it took me some time to learn how to move around within the system.\nAccomplishments that we're proud of\nThis is fully featured and usable tomorrow if someone wanted to. A bit rough around the edges, but definitely working, which I think is pretty good for someone working on there own in 24 hours (I had some commitments I had to get to last night as well)\nWhat we learned\nI learned a good bit about Firebase cloud functions and what I needed to put on the server vs client side in that scenario. They are very easy to use, which is pretty nice, I was expecting a bit more effort than that to get a backend running.\nWhat's next for Tasket\nI think the big thing that needs to be added is reminders for when a task is close to being due, as well as automatic switching for tasks. That would change it from a mere recording tool to something truly useful.", "link": "https://devpost.com/software/tasket", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ni live with a lot of other people and it's been a challenge at time to organize our weekly cleaning and other things that need to happen in the house. this -----> tool !!!  aims to simplify and automate that process, which would remove some stress from me\nwhat it does\ntasket organizes people into groups and adds tasks to these groups. the group admin can then assign tasks to the group, which randomly assign tasks to each group member, until either tasks or group members runs out. it then shows all members their assigned task, a deadline, and whether or not it is completed.\ntasks can be re-assigned at anytime by the group admin, and all members can see who has completed there tasks as well as if a task is overdue.\nhow we built it\nthis is built using firebase and react, using material ui as the component library\nchallenges we ran into\ni'm working on this alone, so i had to decide what to prioritize and what to wait on. additionally, i'm not super familiar with firebase, so it took me some time to learn how to move around within the system.\naccomplishments that we're proud of\nthis is fully featured and usable tomorrow if someone wanted to. a bit rough around the edges, but definitely working, which i think is pretty good for someone working on there own in 24 hours (i had some commitments i had to get to last night as well)\nwhat we learned\ni learned a good bit about firebase cloud functions and what i needed to put on the server vs client side in that scenario. they are very easy to use, which is pretty nice, i was expecting a bit more effort than that to get a backend running.\nwhat's next for tasket\ni think the big thing that needs to be added is reminders for when a task is close to being due, as well as automatic switching for tasks. that would change it from a mere recording tool to something truly useful.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59504293}, {"Unnamed: 0": 4298, "autor": "UniClip", "date": null, "content": "Inspiration\nWe wanted to create a tool for for everyone to level up their productivity across devices. It runs quietly in the background but adds a ton of functionality to your existing clipboard. These are based on things we've always wanted our clipboards to be able to do like being able to copy text from an image or have a computer decide which parts of the text are important to copy.\nWhat it does\nAfter our application is installed, it starts a background process that keeps track of the user's clipboard and also updates it when they copy something on another device. It adds static analysis to the contents of the clipboard to determine when things like long text can be summarized or made shorter.\nIf the user copies an image it'll also scan the image to see if there's text and prompt the user via a notification that they can choose to copy the text from the image instead of the entire image itself.\nLong gone are the days of having to retype something because someone sent it to you in an image.\nHow we built it\nFor text summarization and copy and pasting we utilized a few AWS services. For compartmentalization of frontend and backed, we used API Gateway. For text saving and processing, we used Lambda and MeaningCloud's text summarization API. For text storage, we used DynamoDB.\nFor image processing, we used Google Cloud Vision.\nFor our local application/frontend, we used Electron.\nChallenges we ran into\nUnfortunately, our mobile development was unfruitful, due to this being our first time developing a mobile application but also because of security constraints and limitations on clipboard access through a mobile application.\nAccomplishments that we're proud of\nWe managed to successfully develop an application that allows for cross-platform (for Windows and Mac) copy and pasting, along with a text summarization and text-in-image recognition integration.\nWhat we learned\nWe learned how to utilize Electron for our needs in terms of clipboard access and notification creation. We also learned how to store text formatted data in DynamoDB.\nWhat's next for UniClip\nDeveloping a mobile version so that we can copy and paste on all platforms, adding API security with preflight header authentication, and reducing delay on the copy paste action with local text and image processing.", "link": "https://devpost.com/software/uniclipped", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe wanted to create a -----> tool !!!  for for everyone to level up their productivity across devices. it runs quietly in the background but adds a ton of functionality to your existing clipboard. these are based on things we've always wanted our clipboards to be able to do like being able to copy text from an image or have a computer decide which parts of the text are important to copy.\nwhat it does\nafter our application is installed, it starts a background process that keeps track of the user's clipboard and also updates it when they copy something on another device. it adds static analysis to the contents of the clipboard to determine when things like long text can be summarized or made shorter.\nif the user copies an image it'll also scan the image to see if there's text and prompt the user via a notification that they can choose to copy the text from the image instead of the entire image itself.\nlong gone are the days of having to retype something because someone sent it to you in an image.\nhow we built it\nfor text summarization and copy and pasting we utilized a few aws services. for compartmentalization of frontend and backed, we used api gateway. for text saving and processing, we used lambda and meaningcloud's text summarization api. for text storage, we used dynamodb.\nfor image processing, we used google cloud vision.\nfor our local application/frontend, we used electron.\nchallenges we ran into\nunfortunately, our mobile development was unfruitful, due to this being our first time developing a mobile application but also because of security constraints and limitations on clipboard access through a mobile application.\naccomplishments that we're proud of\nwe managed to successfully develop an application that allows for cross-platform (for windows and mac) copy and pasting, along with a text summarization and text-in-image recognition integration.\nwhat we learned\nwe learned how to utilize electron for our needs in terms of clipboard access and notification creation. we also learned how to store text formatted data in dynamodb.\nwhat's next for uniclip\ndeveloping a mobile version so that we can copy and paste on all platforms, adding api security with preflight header authentication, and reducing delay on the copy paste action with local text and image processing.", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59504298}, {"Unnamed: 0": 4324, "autor": "Swipe", "date": null, "content": "Inspiration\nMany first years I know have little use of their excess swipes in their unlimited dining plan, and many 2nd, 3rd, and 4th year students lack dining plans but still want to have access to on-grounds dining. As such, Swipe was my attempt to connect these two groups together to help UVA students get fed.\nWhat it does\nSwipe allows people to submit swipe requests at different dining halls around Grounds, and have other students fulfill those requests by swiping them into the given dining hall.\nHow we built it\nI used XCode to create the iOS app that interacts with a Google Cloud Firebase database to store and display swipe requests.\nChallenges we ran into\nOne challenge I ran into was the aesthetic of the application, as well as an inability to add a stronger incentive/social aspect to the app. I would have loved to add an ability to \"sell\" swipes via a connection to Venmo/PayPal, or to have people who frequently swipe others into dining halls gain achievements or other such rewards.\nAccomplishments that we're proud of\nI'm very proud of making my very first iOS and using Google Cloud for the first time. I had never worked with Swift either, so being able to familiarize myself with this in a very short time period was incredibly rewarding.\nWhat we learned\nI learned that it's incredibly important to design the general architecture of your project near the start of your project. I struggled having to adapt my existing code to constantly changing visions of my project.\nWhat's next for Swipe\nI am incredibly passionate about this project, and think it can be a useful tool to combat food insecurity at UVA. To incentivize swipers, adding a small charge for swipes of less than five dollars could incentivize them. To incentivize swipees, I would add a reviewing system, along with more features to track how your swipe request has been progressed. By creating more trust and transparency for the swipees, more students will be inclined to use it.", "link": "https://devpost.com/software/swipe-kjy08p", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nmany first years i know have little use of their excess swipes in their unlimited dining plan, and many 2nd, 3rd, and 4th year students lack dining plans but still want to have access to on-grounds dining. as such, swipe was my attempt to connect these two groups together to help uva students get fed.\nwhat it does\nswipe allows people to submit swipe requests at different dining halls around grounds, and have other students fulfill those requests by swiping them into the given dining hall.\nhow we built it\ni used xcode to create the ios app that interacts with a google cloud firebase database to store and display swipe requests.\nchallenges we ran into\none challenge i ran into was the aesthetic of the application, as well as an inability to add a stronger incentive/social aspect to the app. i would have loved to add an ability to \"sell\" swipes via a connection to venmo/paypal, or to have people who frequently swipe others into dining halls gain achievements or other such rewards.\naccomplishments that we're proud of\ni'm very proud of making my very first ios and using google cloud for the first time. i had never worked with swift either, so being able to familiarize myself with this in a very short time period was incredibly rewarding.\nwhat we learned\ni learned that it's incredibly important to design the general architecture of your project near the start of your project. i struggled having to adapt my existing code to constantly changing visions of my project.\nwhat's next for swipe\ni am incredibly passionate about this project, and think it can be a useful -----> tool !!!  to combat food insecurity at uva. to incentivize swipers, adding a small charge for swipes of less than five dollars could incentivize them. to incentivize swipees, i would add a reviewing system, along with more features to track how your swipe request has been progressed. by creating more trust and transparency for the swipees, more students will be inclined to use it.", "sortedWord": "None", "removed": "Nan", "score": 6, "comments": 0, "media": null, "medialink": null, "identifyer": 59504324}, {"Unnamed: 0": 4326, "autor": "PresentQuick", "date": null, "content": "Inspiration\nBriefing a Research Paper or an Article into a compact presentation can be difficult, especially during conferences, where there are people from different domains who may or may not have the knowledge about your field.\nWhat it does\nPresentQuick is a tool that can convert a long article or research paper into a simple presentation, that makes it easier to understand. It uses NLP to analyze long pieces of content and recollects the most important sentences and breaks them down into slides. This web application can be used either for presenting something to the audience of even to get a simple understanding of a complex topic.\nHow I built it\nBack-end: Natural Language Toolkit, Python-Flask\nFront-end: Python-PPTX\nHosting: Replit\nChallenges I ran into\nCreating and designing presentations was a challenge at first, but then I found out the Python-PPTX library and learned how to use it during the course of this project.\nAccomplishments that we're proud of\nBeing able to complete this project, learning about Natural Language Processing and Python-PPTX, and hosting it.\nWhat I learned\nI learned about NLP and Python-PPTX library.\nWhat's next for PresentQuick\nI'll be adding more presentation designs and work on the overlay problem with text.", "link": "https://devpost.com/software/presentquick", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nbriefing a research paper or an article into a compact presentation can be difficult, especially during conferences, where there are people from different domains who may or may not have the knowledge about your field.\nwhat it does\npresentquick is a -----> tool !!!  that can convert a long article or research paper into a simple presentation, that makes it easier to understand. it uses nlp to analyze long pieces of content and recollects the most important sentences and breaks them down into slides. this web application can be used either for presenting something to the audience of even to get a simple understanding of a complex topic.\nhow i built it\nback-end: natural language toolkit, python-flask\nfront-end: python-pptx\nhosting: replit\nchallenges i ran into\ncreating and designing presentations was a challenge at first, but then i found out the python-pptx library and learned how to use it during the course of this project.\naccomplishments that we're proud of\nbeing able to complete this project, learning about natural language processing and python-pptx, and hosting it.\nwhat i learned\ni learned about nlp and python-pptx library.\nwhat's next for presentquick\ni'll be adding more presentation designs and work on the overlay problem with text.", "sortedWord": "None", "removed": "Nan", "score": 6, "comments": 0, "media": null, "medialink": null, "identifyer": 59504326}, {"Unnamed: 0": 4337, "autor": "Slick", "date": null, "content": "Inspiration\nPatreon has been a great tool for Youtubers or any other content creators to get support from their supporters steadily. Patreon is worth over 4 Billion Dollars and is growing fast. But there are lot of problems with it as well. Accessibility at different locations is the big challenge. Credit card and Bank support for Patreon is not available at all locations and countries. Also there is no proper feature to communicate between people. What if we can build a Patreon using Blockchain and cryptocurrency such that it will be universally compatible?\nWhat it does\nSlick is a decentralized version of Patreon. People can get subscription for exclusive content from their favourite creators using Cryptocurrency. All the subscriptions are taken care by Smart Contract deployed in Matic network. People can coomunicate and all of the data and media files uploaded are stored in IPFS network which makes this DApp much more distributed.\nHow we built it\nSlick is a web app built using normal Web stack of HTML, CSS, JS. Backend and Smart contracts are written in Javascript and Solidity. Slick web app is hosted through IPFS using fleek. This lets our Webapp be accessible from anywhere without the problem of having any downtimes. Link can be found below. All of the media is stored in IPFS instead of centralized server for security.\nChallenges we ran into\nAs our DApp has huge use-case and may encounter large number of transactions we needed a robust network that can handle our transaction requests and maintain lower gas fee. For this we have looked into Polygon network. Storing data of each address whether who took subscription or not in Solididty is a bit challenging. Solidity doesnot provide the luxury of storing using traditional data structures. We have to learn some of the concepts in depth again and apply using Mapping, array addressing to finish the contract.\nAccomplishments that we're proud of\nSlick can hit hundreds of thousands of users if further developed and we are really happy with building its initial version successfully with all features. Polygon network really helped us a lot as it can accomadte large traffic with less transaction time and gas fee.\nWhat we learned\nI have learned a lot about Smart contract development. Though I couldn't finish building the entire Dapp as planned, I still managed to build all the core features and deploy it successfully. We have never worked on such a complex smart contract before and are glad that we could finish it.\nWhat's next for Slick\nSlick currently needs lot of support in terms of frontend. We have built a robust smart contract for managing subscriptions and backend goals are achieved. Frontend needs to be built in much more user friendly manner. Chat and Video call options are also to be implemented using decentralised protocols like libp2p and livepeer in next version. Asides from this, Slick currently supports only few creators to post. This will expanded and all data will be stored in much more efficient decentralized storage like Filecoin.\nDiscord\nmonish#4655", "link": "https://devpost.com/software/slick-9irwnu", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\npatreon has been a great -----> tool !!!  for youtubers or any other content creators to get support from their supporters steadily. patreon is worth over 4 billion dollars and is growing fast. but there are lot of problems with it as well. accessibility at different locations is the big challenge. credit card and bank support for patreon is not available at all locations and countries. also there is no proper feature to communicate between people. what if we can build a patreon using blockchain and cryptocurrency such that it will be universally compatible?\nwhat it does\nslick is a decentralized version of patreon. people can get subscription for exclusive content from their favourite creators using cryptocurrency. all the subscriptions are taken care by smart contract deployed in matic network. people can coomunicate and all of the data and media files uploaded are stored in ipfs network which makes this dapp much more distributed.\nhow we built it\nslick is a web app built using normal web stack of html, css, js. backend and smart contracts are written in javascript and solidity. slick web app is hosted through ipfs using fleek. this lets our webapp be accessible from anywhere without the problem of having any downtimes. link can be found below. all of the media is stored in ipfs instead of centralized server for security.\nchallenges we ran into\nas our dapp has huge use-case and may encounter large number of transactions we needed a robust network that can handle our transaction requests and maintain lower gas fee. for this we have looked into polygon network. storing data of each address whether who took subscription or not in solididty is a bit challenging. solidity doesnot provide the luxury of storing using traditional data structures. we have to learn some of the concepts in depth again and apply using mapping, array addressing to finish the contract.\naccomplishments that we're proud of\nslick can hit hundreds of thousands of users if further developed and we are really happy with building its initial version successfully with all features. polygon network really helped us a lot as it can accomadte large traffic with less transaction time and gas fee.\nwhat we learned\ni have learned a lot about smart contract development. though i couldn't finish building the entire dapp as planned, i still managed to build all the core features and deploy it successfully. we have never worked on such a complex smart contract before and are glad that we could finish it.\nwhat's next for slick\nslick currently needs lot of support in terms of frontend. we have built a robust smart contract for managing subscriptions and backend goals are achieved. frontend needs to be built in much more user friendly manner. chat and video call options are also to be implemented using decentralised protocols like libp2p and livepeer in next version. asides from this, slick currently supports only few creators to post. this will expanded and all data will be stored in much more efficient decentralized storage like filecoin.\ndiscord\nmonish#4655", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59504337}, {"Unnamed: 0": 4341, "autor": "Live Color Book", "date": null, "content": "Inspiration\nMost of us loves drawing, when we were child we started to learn the new world by painting papers, walls even our parents' stuff. When we met technology, started to use digital tools like color platte, brushes, eraser etc. Our creations are inspired from nature, moments or objects that we like. Today's color book apps limit us to paint only predefined sketches. We can not paint our desired pictures. So I want to make an app that people can take picture what they want to paint.\nWhat it does\nThe application has two picture selecting capabilities. These are selecting from library and using built in camera. When user select a picture, application uses modz dominant color detection model and extracts colors that already in the picture. After getting colors from picture, the application makes picture a sketch for drawing. The application also has preview the original picture, detected color brushes and erasing capabilities. In addition to these capabilities, user can share works with others in app.\nHow we built it\nI made an a ios application with using swift language.\nChallenges we ran into\nConverted image to sketch was a challenge for me. Because ios native image library CoreImage has not any sketch filter. So I tried to use combination of some filters but it did not work. Finally I used a third-party library for this.\nAccomplishments that we're proud of\nFor a long time, I wanted to make a color application but I could not find any tool that differentiates from other applications. First time I have used AI library in an application. They help me to complete an application in shorthand time.\nWhat we learned\nI learned many kind of AI methodologies. Also I have adopted an AI methodology to a real application.\nWhat's next for Live Color Book\nSecond version of Live Color Book I am going to add new brushes and zoom functionality.", "link": "https://devpost.com/software/color-detection-bi8x6j", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nmost of us loves drawing, when we were child we started to learn the new world by painting papers, walls even our parents' stuff. when we met technology, started to use digital tools like color platte, brushes, eraser etc. our creations are inspired from nature, moments or objects that we like. today's color book apps limit us to paint only predefined sketches. we can not paint our desired pictures. so i want to make an app that people can take picture what they want to paint.\nwhat it does\nthe application has two picture selecting capabilities. these are selecting from library and using built in camera. when user select a picture, application uses modz dominant color detection model and extracts colors that already in the picture. after getting colors from picture, the application makes picture a sketch for drawing. the application also has preview the original picture, detected color brushes and erasing capabilities. in addition to these capabilities, user can share works with others in app.\nhow we built it\ni made an a ios application with using swift language.\nchallenges we ran into\nconverted image to sketch was a challenge for me. because ios native image library coreimage has not any sketch filter. so i tried to use combination of some filters but it did not work. finally i used a third-party library for this.\naccomplishments that we're proud of\nfor a long time, i wanted to make a color application but i could not find any -----> tool !!!  that differentiates from other applications. first time i have used ai library in an application. they help me to complete an application in shorthand time.\nwhat we learned\ni learned many kind of ai methodologies. also i have adopted an ai methodology to a real application.\nwhat's next for live color book\nsecond version of live color book i am going to add new brushes and zoom functionality.", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59504341}, {"Unnamed: 0": 4355, "autor": "Mr.Rich", "date": null, "content": "Inspiration\nWe were inspired by historical events such as the Reddit Gamestop Stock skyrocket, and the above 1000% increase in cryptocurrencies like Dogecoin.\nWhat it does\nOur application is a modern analysis tool utilizing historical pricing trends and sentiment analysis to make accurate predictions on the current and future state of the stock market. Using data from Yahoo Finance, our application takes that data and feeds it through a LSTM neural network that is able to use multiple sources of input to make a prediction on the closing price 2 weeks from the current day. Our testing revealed an average mean squared error loss of less than 6% consistently, showcasing the accuracy of these long term predictions (which includes the sharp shifts during the timespan of COVID)\nHow we built it\nThe ML model was trained for specific companies and their trends, using their historical pricing data as well as sentiment data from Twitter and Reddit to make a judgement on the magnitude and direction of the next stock movement. This data was web scraped and processed using sentiment analysis to feed into our neural network. Our neural network takes multiple indicators, opening price, price delta, and the sentiment analysis value from corresponding tweets or posts of influencers at the time. All of this information gets elegantly captured and converted into a final closing price prediction using a LSTM cell neural network with optimized hyperparameters through Keras Tuner. Of course, there were a plethora other methods that would\u2019ve been easier to implement and just as accurate in the short term (1 day-3 days). However, our choice of the LSTM neural model was to allow us the capability of accurately predicting the trends of the stock market well into the future, upwards of 10-30 days of prediction.\nChallenges we ran into\nOur initial intention was to have the machine learning model feed the predicted points, past data points, and sentiment scores to a React Frontend through Flask hosted on Google Cloud. Although this implementation was perfectly functional locally, Google Cloud limitations, such as limited file size (prevented React node modules), lack of server resources, and nonstop errors forced us to pivot away from React to vanilla HTML and Javascript.\nEventually, we successfully found a method to host the application with Flask. Yet with this solution, a new problem arose. There was difficulty in making our graphs dynamic. The lack of a database rendered our original plan of retrieving data from CSV files every run unfeasible and the idea was dropped.\nDue to the time constraints, we settled on moving the entire project to a command line desktop application. Although we weren\u2019t able to amplify this project with some stunning UI and visuals, we were able to retain a majority of the actual functionality of the project. Hallstone markers such as the accuracy and the month of foresign the program could offer were efficiently ported to this new platform.\nAccomplishments that we're proud of\nOur implementation of the ML model was solid, accurate, and very robust. The web scraping code we used was also very efficient in grabbing a multitude of different statistics (tweet count, dates, related influencers, authors) quickly without much user input. Although we weren\u2019t able to showcase directly some of the visuals we had planned, we are proud that the project was able to be functionally completed.\nWhat we learned\nThis was our first time dealing with a real neural network model that aims to predict a non-linear correlation between equally time separated data. We learned a lot about the execution required to create and optimize such neural networks to allow for accurate predictions while still being general enough to extrapolate for other data sets. We also gained a lot of experience dealing with databases, learning a new program, Flask, as well as the limitations of Google Cloud for the general user.\nWhat's next for Mr.Rich\nOur next step for Rich would be to deliver upon the visuals that we worked on in a manner that would allow us to truly display the fountain of knowledge that we can show through our trained model. Acquiring a speedy database, as well as a strong backend support are some of the immediate steps we can take toward this goal.", "link": "https://devpost.com/software/mr-rich", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe were inspired by historical events such as the reddit gamestop stock skyrocket, and the above 1000% increase in cryptocurrencies like dogecoin.\nwhat it does\nour application is a modern analysis -----> tool !!!  utilizing historical pricing trends and sentiment analysis to make accurate predictions on the current and future state of the stock market. using data from yahoo finance, our application takes that data and feeds it through a lstm neural network that is able to use multiple sources of input to make a prediction on the closing price 2 weeks from the current day. our testing revealed an average mean squared error loss of less than 6% consistently, showcasing the accuracy of these long term predictions (which includes the sharp shifts during the timespan of covid)\nhow we built it\nthe ml model was trained for specific companies and their trends, using their historical pricing data as well as sentiment data from twitter and reddit to make a judgement on the magnitude and direction of the next stock movement. this data was web scraped and processed using sentiment analysis to feed into our neural network. our neural network takes multiple indicators, opening price, price delta, and the sentiment analysis value from corresponding tweets or posts of influencers at the time. all of this information gets elegantly captured and converted into a final closing price prediction using a lstm cell neural network with optimized hyperparameters through keras tuner. of course, there were a plethora other methods that would\u2019ve been easier to implement and just as accurate in the short term (1 day-3 days). however, our choice of the lstm neural model was to allow us the capability of accurately predicting the trends of the stock market well into the future, upwards of 10-30 days of prediction.\nchallenges we ran into\nour initial intention was to have the machine learning model feed the predicted points, past data points, and sentiment scores to a react frontend through flask hosted on google cloud. although this implementation was perfectly functional locally, google cloud limitations, such as limited file size (prevented react node modules), lack of server resources, and nonstop errors forced us to pivot away from react to vanilla html and javascript.\neventually, we successfully found a method to host the application with flask. yet with this solution, a new problem arose. there was difficulty in making our graphs dynamic. the lack of a database rendered our original plan of retrieving data from csv files every run unfeasible and the idea was dropped.\ndue to the time constraints, we settled on moving the entire project to a command line desktop application. although we weren\u2019t able to amplify this project with some stunning ui and visuals, we were able to retain a majority of the actual functionality of the project. hallstone markers such as the accuracy and the month of foresign the program could offer were efficiently ported to this new platform.\naccomplishments that we're proud of\nour implementation of the ml model was solid, accurate, and very robust. the web scraping code we used was also very efficient in grabbing a multitude of different statistics (tweet count, dates, related influencers, authors) quickly without much user input. although we weren\u2019t able to showcase directly some of the visuals we had planned, we are proud that the project was able to be functionally completed.\nwhat we learned\nthis was our first time dealing with a real neural network model that aims to predict a non-linear correlation between equally time separated data. we learned a lot about the execution required to create and optimize such neural networks to allow for accurate predictions while still being general enough to extrapolate for other data sets. we also gained a lot of experience dealing with databases, learning a new program, flask, as well as the limitations of google cloud for the general user.\nwhat's next for mr.rich\nour next step for rich would be to deliver upon the visuals that we worked on in a manner that would allow us to truly display the fountain of knowledge that we can show through our trained model. acquiring a speedy database, as well as a strong backend support are some of the immediate steps we can take toward this goal.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 2, "media": null, "medialink": null, "identifyer": 59504355}, {"Unnamed: 0": 4363, "autor": "SYN-detector", "date": null, "content": "What is this\nSYN-detector is a tool that analyzes a PCAP file in order to detect possible SYN scans.\nInspiration\nWas reading up on newtwork security and found about a technique called SYN scan and learned these:\nUsed to find open ports for attack\nScanenr sends out TCP SYN packets (the first packet in the TCP handshake) and watches for hosts that respond with SYN+ACK packets (the second handshake step)\nNumber of SYN packets is much higher than the number of SYN+ACK packets\nWanted to build a tool to detect possible SYN scans.\nPrerequisites\nBefore you begin, ensure you have met the following requirements:\nYou have installed the latest version of python, scapy\nGetting SYN-detector\nTo install SYN-detector, follow these steps:\ngit clone https://github.com/2kabhishek/SYN-detector\ncd SYN-detector\nUsing SYN-detector\nYou'll need to have a PCAP file to analyze.\nI have a PCAP file test-data.pcap that I used to test SYN-detector.\nUSAGE:\npython syn-detector.py ./test-data.pcap\nHow it was built\nSYN-detector was built using scapy\nChallenges faced\nWhile building SYN-detector the main challenges were:\nFiguring out TCP flags\nWhat I learned\nLearned a lot about network security\nHands on with scapy\nWhat's next\nWill make few more scripts using scapy.", "link": "https://devpost.com/software/syn-detector", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what is this\nsyn-detector is a -----> tool !!!  that analyzes a pcap file in order to detect possible syn scans.\ninspiration\nwas reading up on newtwork security and found about a technique called syn scan and learned these:\nused to find open ports for attack\nscanenr sends out tcp syn packets (the first packet in the tcp handshake) and watches for hosts that respond with syn+ack packets (the second handshake step)\nnumber of syn packets is much higher than the number of syn+ack packets\nwanted to build a tool to detect possible syn scans.\nprerequisites\nbefore you begin, ensure you have met the following requirements:\nyou have installed the latest version of python, scapy\ngetting syn-detector\nto install syn-detector, follow these steps:\ngit clone https://github.com/2kabhishek/syn-detector\ncd syn-detector\nusing syn-detector\nyou'll need to have a pcap file to analyze.\ni have a pcap file test-data.pcap that i used to test syn-detector.\nusage:\npython syn-detector.py ./test-data.pcap\nhow it was built\nsyn-detector was built using scapy\nchallenges faced\nwhile building syn-detector the main challenges were:\nfiguring out tcp flags\nwhat i learned\nlearned a lot about network security\nhands on with scapy\nwhat's next\nwill make few more scripts using scapy.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59504363}, {"Unnamed: 0": 4376, "autor": "Spy need Tool's", "date": null, "content": "\ud83d\udca1 Inspiration\nWe build this project from the inspiration that who would spy would communicate with each other. But the thing is our web application is not that simple to use, these tools are hidden, and only spy can use them as they know where the location of tools. Rest, everyone sees this site as a simple food application.\n\ud83e\udd37\ud83c\udffb\u200d\u2642\ufe0f What it does\nWell our web application has 2 parts and works differently for normal people and for spy/agents.\nFor Normal People\nIt's a food web application called Foodie.\nWhere they can sign up with an account and order food and see people reviews about food.\nWhere people can see different food items from different countries.\nCan also give and review about them.\nFor Spy/Agents\nOnly the agents/spy can use the hidden tools to communicate with each other\nTool-1 is simple messaging, but the message will be encrypted and can only be decrypted by key and read the original message.\nTool-2 uses steganography to embed text in image can be decrypted by other spies only. Then agents/spy can share these images to communicate with each other.\n\u2692\ufe0f How we built it\nWe have used HTML, CSS, JavaScript, Bootstrap for the frontend part and designing the website.\nWe have use crypto.js to encrypt and decrypt text\nWe have used piexifjs to embed text in image to use steganography technique.\nThen we have deployed our web application in vercel and then use DNS to connect it to our domain name.\n\ud83c\udfc3\ud83c\udfff Challenges we ran into\nThe biggest challenge was to use piexifjs to embed text into images and encrypt them and decrypt them to show the original message.\nThe encryption of text was also no working for long messages was not working properly.\nUsing DNS to connect to domain name was also tough, we messed up few time but successful deployed it.\n\ud83c\udfc6 Accomplishments that we're proud of\nWe both are proud on ourselves that we completed this undercover hack within the time limit of the hackathon and able to learn new thing while making this project.\n\ud83d\udcda What we learned\nWe learned how to make a game using basic web development fundamental languages like HTML, CSS and JavaScript.\nWe learned how to use JavaScript libraries like piexif.js and crypto.js and implement these libraries concepts in this web application.\nWe learned how to host our website, and we also learned about DNS which we used to connect to our domain name\n\u23e9 What's next for Spy need Tool's \ud83e\uddf0\ud83d\udd75\ud83c\udffb\u200d\u2642\ufe0f\nIn the future, we would like to add more tools in the web application so that the spy/agents can have more ways to communicate.\nWe also want to make this web-application more interesting for both the normal people and spy/agents", "link": "https://devpost.com/software/spy-need-tool-s", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "\ud83d\udca1 inspiration\nwe build this project from the inspiration that who would spy would communicate with each other. but the thing is our web application is not that simple to use, these tools are hidden, and only spy can use them as they know where the location of tools. rest, everyone sees this site as a simple food application.\n\ud83e\udd37\ud83c\udffb\u200d\u2642\ufe0f what it does\nwell our web application has 2 parts and works differently for normal people and for spy/agents.\nfor normal people\nit's a food web application called foodie.\nwhere they can sign up with an account and order food and see people reviews about food.\nwhere people can see different food items from different countries.\ncan also give and review about them.\nfor spy/agents\nonly the agents/spy can use the hidden tools to communicate with each other\ntool-1 is simple messaging, but the message will be encrypted and can only be decrypted by key and read the original message.\ntool-2 uses steganography to embed text in image can be decrypted by other spies only. then agents/spy can share these images to communicate with each other.\n\u2692\ufe0f how we built it\nwe have used html, css, javascript, bootstrap for the frontend part and designing the website.\nwe have use crypto.js to encrypt and decrypt text\nwe have used piexifjs to embed text in image to use steganography technique.\nthen we have deployed our web application in vercel and then use dns to connect it to our domain name.\n\ud83c\udfc3\ud83c\udfff challenges we ran into\nthe biggest challenge was to use piexifjs to embed text into images and encrypt them and decrypt them to show the original message.\nthe encryption of text was also no working for long messages was not working properly.\nusing dns to connect to domain name was also tough, we messed up few time but successful deployed it.\n\ud83c\udfc6 accomplishments that we're proud of\nwe both are proud on ourselves that we completed this undercover hack within the time limit of the hackathon and able to learn new thing while making this project.\n\ud83d\udcda what we learned\nwe learned how to make a game using basic web development fundamental languages like html, css and javascript.\nwe learned how to use javascript libraries like piexif.js and crypto.js and implement these libraries concepts in this web application.\nwe learned how to host our website, and we also learned about dns which we used to connect to our domain name\n\u23e9 what's next for spy need -----> tool !!! 's \ud83e\uddf0\ud83d\udd75\ud83c\udffb\u200d\u2642\ufe0f\nin the future, we would like to add more tools in the web application so that the spy/agents can have more ways to communicate.\nwe also want to make this web-application more interesting for both the normal people and spy/agents", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59504376}, {"Unnamed: 0": 4424, "autor": "Check Your Lungs", "date": null, "content": "Inspiration\nThe inspiration for this project was taken from the fact that lung cancer is a leading cause of deaths in America and is also a way to raise awareness in the vaping epidemic which is on the rise amongst youths. This application is also a way to help without any healthcare to not waste their money by getting tests everytime they feel off and gives them a probable chance from which the user would decide whether they would like to consult a professional\nWhat it does\nThe app uses data given to it by the user and predicts the probability that the user has lung cancer and if they need to consult a healthcare professional\nHow we built it\nWe used the firebase database to import our database and used the Vertex AI to create a model that would predict the probability of lung cancer. We also used readct native for front end and expo as a tool for checking the mobile app.\nChallenges we ran into\nWe ran into challenges when training our model since there wasn't enough data in our datasets as there was every limited research done on risk factors of lung cancer so it took a fair bit of time to scrape/find data.\nAccomplishments that we're proud of\nWe're proud of the model working effectively and the results it produced was very favourable.\nWhat's next for Check Your Lungs\nWe can expand our application into not just checking for lung cancer, but also other types of cancer, some which are very hard to figure out and require ultrasounds or x-ray scans which would require cooperation with a health care institution.", "link": "https://devpost.com/software/check-your-lungs", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nthe inspiration for this project was taken from the fact that lung cancer is a leading cause of deaths in america and is also a way to raise awareness in the vaping epidemic which is on the rise amongst youths. this application is also a way to help without any healthcare to not waste their money by getting tests everytime they feel off and gives them a probable chance from which the user would decide whether they would like to consult a professional\nwhat it does\nthe app uses data given to it by the user and predicts the probability that the user has lung cancer and if they need to consult a healthcare professional\nhow we built it\nwe used the firebase database to import our database and used the vertex ai to create a model that would predict the probability of lung cancer. we also used readct native for front end and expo as a -----> tool !!!  for checking the mobile app.\nchallenges we ran into\nwe ran into challenges when training our model since there wasn't enough data in our datasets as there was every limited research done on risk factors of lung cancer so it took a fair bit of time to scrape/find data.\naccomplishments that we're proud of\nwe're proud of the model working effectively and the results it produced was very favourable.\nwhat's next for check your lungs\nwe can expand our application into not just checking for lung cancer, but also other types of cancer, some which are very hard to figure out and require ultrasounds or x-ray scans which would require cooperation with a health care institution.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59504424}, {"Unnamed: 0": 4429, "autor": "OTGTweetAnalysis", "date": null, "content": "Inspiration\nEver had to go through pages and hours of movie reviews? Ever had to struggle to realize if some news is biased? Ever had to make a very tough choice of whether or not to buy that altcoin that has been spreading like wildfire? The ever-varying opinion of the world which shapes our day-to-day decisions made us rethink how we perceive and let those opinions affect us led to this small, easy-to-use tool, OTGTweetAnalysis.\nWhat it does\nThe OTGTweetAnalysis lets you text a screen name and the number of recent tweets you want to analyze and texts you back the public sentiment of the tweeter's recent tweets.\nHow we built it\nPython was the primary language, the message processing application was built with a bit of Flask and Python. Thanks to Twitter APIs and Twilio, we were able to seamlessly get the tweets, analyze them, and text back the result of the sentiment analysis.\nChallenges we ran into\nSetting up Twilio was a bit of a hassle initially, but once we figured it out, it looked like one of the most beautiful APIs ever built.\nAccomplishments that we're proud of\nPersonally, this is my longest code in Python and my first Flask application. The team enjoyed working with Twilio APIs as well.\nWhat we learned\nWell, what the world thinks doesn't matter, that's the first lesson. We also learnt and enjoyed building simple Flask apps, integrated with Twilio.\nWhat's next for OTGTweetAnalysis\nWe can extend this model to analyse tweets based on keywords rather than a particular user's recent tweets and also Twilio supports voice calls (yayyyyyyy!!!) which can be leveraged as well.", "link": "https://devpost.com/software/otgtweetanalysis", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\never had to go through pages and hours of movie reviews? ever had to struggle to realize if some news is biased? ever had to make a very tough choice of whether or not to buy that altcoin that has been spreading like wildfire? the ever-varying opinion of the world which shapes our day-to-day decisions made us rethink how we perceive and let those opinions affect us led to this small, easy-to-use -----> tool !!! , otgtweetanalysis.\nwhat it does\nthe otgtweetanalysis lets you text a screen name and the number of recent tweets you want to analyze and texts you back the public sentiment of the tweeter's recent tweets.\nhow we built it\npython was the primary language, the message processing application was built with a bit of flask and python. thanks to twitter apis and twilio, we were able to seamlessly get the tweets, analyze them, and text back the result of the sentiment analysis.\nchallenges we ran into\nsetting up twilio was a bit of a hassle initially, but once we figured it out, it looked like one of the most beautiful apis ever built.\naccomplishments that we're proud of\npersonally, this is my longest code in python and my first flask application. the team enjoyed working with twilio apis as well.\nwhat we learned\nwell, what the world thinks doesn't matter, that's the first lesson. we also learnt and enjoyed building simple flask apps, integrated with twilio.\nwhat's next for otgtweetanalysis\nwe can extend this model to analyse tweets based on keywords rather than a particular user's recent tweets and also twilio supports voice calls (yayyyyyyy!!!) which can be leveraged as well.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59504429}, {"Unnamed: 0": 4436, "autor": "Bruinwalk scraper and analyser", "date": null, "content": "Inspiration\nI am a college student at UCLA and I've found that how well I do in my classes depends greatly on how suited the professor's teaching style is to my learning style. Unfortunately, the official university website for students to give reviews of professors at UCLA is hard to navigate and makes choosing classes finding out what I want to know about classes difficult.\nWhat it does\nFor a given review of a class, the program scrapes through all pages of reviews and returns the number of occurences of a each keyword specified in the Python program. Any number of keywords can be specified.\nHow we built it\nIt was a simple script built in Python using the Selenium and BeautifulSoup4 packages.\nChallenges we ran into\nThis was my first actual Python project, my first personal project (outside of school and class), and also my first hackathon.\nI ran into difficult with setting up my Python environment (learning about how to use virtual environments and install packages) and also how to manage dependencies (separating base and dev dependencies).\nAccomplishments that we're proud of\nI managed to learn about how to set up my Python environment and manage dependencies all on my own. I have never done this before.\nWhat we learned\nLearnt more Python syntax, how to use Selenium, learnt to use pip-tools, and learnt to create demo videos and project descriptions for hackathons.\nWhat's next for Bruinwalk scraper and analyser\nI have a plan for the next Hackathon I'm participating in to use the program I built for this hackathon to build a larger program that returns keyword-related data on all the classes and all the past offerings of the class by different professors that are related to a specific department (for example, Computer Science). I would compile the data on common keywords related to entire departments and store it in a database.\nHopefully this is a tool I can share with other students at my school once completed.", "link": "https://devpost.com/software/bruinwalk-scraper-and-analyser", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ni am a college student at ucla and i've found that how well i do in my classes depends greatly on how suited the professor's teaching style is to my learning style. unfortunately, the official university website for students to give reviews of professors at ucla is hard to navigate and makes choosing classes finding out what i want to know about classes difficult.\nwhat it does\nfor a given review of a class, the program scrapes through all pages of reviews and returns the number of occurences of a each keyword specified in the python program. any number of keywords can be specified.\nhow we built it\nit was a simple script built in python using the selenium and beautifulsoup4 packages.\nchallenges we ran into\nthis was my first actual python project, my first personal project (outside of school and class), and also my first hackathon.\ni ran into difficult with setting up my python environment (learning about how to use virtual environments and install packages) and also how to manage dependencies (separating base and dev dependencies).\naccomplishments that we're proud of\ni managed to learn about how to set up my python environment and manage dependencies all on my own. i have never done this before.\nwhat we learned\nlearnt more python syntax, how to use selenium, learnt to use pip-tools, and learnt to create demo videos and project descriptions for hackathons.\nwhat's next for bruinwalk scraper and analyser\ni have a plan for the next hackathon i'm participating in to use the program i built for this hackathon to build a larger program that returns keyword-related data on all the classes and all the past offerings of the class by different professors that are related to a specific department (for example, computer science). i would compile the data on common keywords related to entire departments and store it in a database.\nhopefully this is a -----> tool !!!  i can share with other students at my school once completed.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59504436}, {"Unnamed: 0": 4443, "autor": "Willie's Game Day Drop", "date": null, "content": "Inspiration\nWe were inspired to make a game that utilized hardware in addition to software, and thought it would be fun to show off our K-State pride! With the help of an electrooculogram (a sensor that measures eye movements), we wanted to create a game that people could use without their hands. This allows for extended accessibility for people with low motor functions or even without the use of their hands.\nWhat it does\nOur game has 2 modes: one where you use the keyboard, and one where you literally look right and left to move Willie the Wildcat. You collect falling objects while avoiding those nasty birds to earn points to win the game. To use your eyes, we hook you up to an Arduino Uno with electrodes connected to your temples and forehead. Or, if you prefer, just use the keys for a different experience!\nHow we built it\nCombining our software, hardware, and design capabilities, we were able to have a multifaceted design. The software we used is a combination of GameMaker Studio 2 and Arduino connected by an open-source DLL. Game Maker Studio uses its own language called GML which has similarities to Python, C#, and JavaScript. We have used these languages in the past, so GML wasn't too difficult to learn. We used an Arduino Uno with a breadboard to initially filter the input signal before digital filtering in Arduino IDE. Data acquisition occurs at the electrodes connecting to 3 points on your face to detect your left and right eye movements. In addition to the hardware and software, we wanted to make the game look as cool as possible. This required us to create all of our own animations using Piskel, an online drawing tool. All moving parts required us to have multiple iterations of each design. This means every frame of our game was hand-animated by us, which took up a lot of time and close concentration. GameMaker Studio nicely integrates with multiple frame animations which allowed the conversion into the game to be fairly simple once all the designs were made.\nChallenges we ran into\nThe biggest challenge was dealing with the electrooculogram sensors being difficult to read. When you move your eyes to look right, it reads right, but then when you move back to center you are technically moving left (from right to center). This gave us a big issue trying to filter between true right and left reads. The sensor is still not perfect, but it has come a far way since we started.\nAccomplishments that we're proud of\nConnecting the Arduino software to the GameMaker software seemed like a big task, yet we have it connected with no errors and we haven't had any issues with the communication. Overall, we are super proud of the game as a whole and love the look and feel just as much as the gameplay. All of the hand animations were very time-consuming, but we couldn't imagine it looking better. The fireworks alone took about 2 hours for one person which is only about a 10-second clip. We are also really proud that this game can be used by lots of different people who may not always have the opportunity to use typical video games due to different motor functions. It has been such a fun weekend and we are all thankful to have bonded as a team and made a really cool product.\nWhat we learned\nWe had not used GameMaker Language at all, which wasn't a huge learning curve but was definitely something new to learn. Although we had all had a bit of Arduino practice, 3/4 of us didn't feel comfortable with using it at all, and now feel proud of combining it with completely new software. This was the first time we created pixel art and animation. Creating the entire game with new art and animation is a feat we are proud to have accomplished.\nWhat's next for Willie's Game Day Drop\nWe would love to add more levels to the game with various new aspects and increased difficulty. The next step on the hardware side would be to read the eye movements more accurately, but this would take better calibration and potentially some things we don't have. We also thought of potentially creating an app with camera eye-tracking software instead of the sensor to move Willie.", "link": "https://devpost.com/software/willie-s-game-day-drop", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe were inspired to make a game that utilized hardware in addition to software, and thought it would be fun to show off our k-state pride! with the help of an electrooculogram (a sensor that measures eye movements), we wanted to create a game that people could use without their hands. this allows for extended accessibility for people with low motor functions or even without the use of their hands.\nwhat it does\nour game has 2 modes: one where you use the keyboard, and one where you literally look right and left to move willie the wildcat. you collect falling objects while avoiding those nasty birds to earn points to win the game. to use your eyes, we hook you up to an arduino uno with electrodes connected to your temples and forehead. or, if you prefer, just use the keys for a different experience!\nhow we built it\ncombining our software, hardware, and design capabilities, we were able to have a multifaceted design. the software we used is a combination of gamemaker studio 2 and arduino connected by an open-source dll. game maker studio uses its own language called gml which has similarities to python, c#, and javascript. we have used these languages in the past, so gml wasn't too difficult to learn. we used an arduino uno with a breadboard to initially filter the input signal before digital filtering in arduino ide. data acquisition occurs at the electrodes connecting to 3 points on your face to detect your left and right eye movements. in addition to the hardware and software, we wanted to make the game look as cool as possible. this required us to create all of our own animations using piskel, an online drawing -----> tool !!! . all moving parts required us to have multiple iterations of each design. this means every frame of our game was hand-animated by us, which took up a lot of time and close concentration. gamemaker studio nicely integrates with multiple frame animations which allowed the conversion into the game to be fairly simple once all the designs were made.\nchallenges we ran into\nthe biggest challenge was dealing with the electrooculogram sensors being difficult to read. when you move your eyes to look right, it reads right, but then when you move back to center you are technically moving left (from right to center). this gave us a big issue trying to filter between true right and left reads. the sensor is still not perfect, but it has come a far way since we started.\naccomplishments that we're proud of\nconnecting the arduino software to the gamemaker software seemed like a big task, yet we have it connected with no errors and we haven't had any issues with the communication. overall, we are super proud of the game as a whole and love the look and feel just as much as the gameplay. all of the hand animations were very time-consuming, but we couldn't imagine it looking better. the fireworks alone took about 2 hours for one person which is only about a 10-second clip. we are also really proud that this game can be used by lots of different people who may not always have the opportunity to use typical video games due to different motor functions. it has been such a fun weekend and we are all thankful to have bonded as a team and made a really cool product.\nwhat we learned\nwe had not used gamemaker language at all, which wasn't a huge learning curve but was definitely something new to learn. although we had all had a bit of arduino practice, 3/4 of us didn't feel comfortable with using it at all, and now feel proud of combining it with completely new software. this was the first time we created pixel art and animation. creating the entire game with new art and animation is a feat we are proud to have accomplished.\nwhat's next for willie's game day drop\nwe would love to add more levels to the game with various new aspects and increased difficulty. the next step on the hardware side would be to read the eye movements more accurately, but this would take better calibration and potentially some things we don't have. we also thought of potentially creating an app with camera eye-tracking software instead of the sensor to move willie.", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59504443}, {"Unnamed: 0": 4461, "autor": "Kerbal Search Program", "date": null, "content": "Inspiration\nThis project was built with the idea of matching larger queries and providing matching between text and research papers. If one is looking for papers similar to a paper they have seen before, our goal is to have that be easily operable. We attempt to remove all the guesswork from tailoring a search by using a multi-token comparison algorithm.\nWhat it does\nOur application takes a number of papers (notably astronomy and space-based papers), converts them to static images, runs text recognition on the pdf image, removes garbage words, tokenizes the documents, and generates a vocab space vector. The frontend takes a multi-token input and attempts to match the term frequency\u2013inverse document frequency scores of the documents we precomputed on the backend. At the end of this matching process, we return the top 30 matched scores.\nHow we built it\nPrecomputation\nWe pulled papers from core, which is a database for academic papers. This was done by selecting a number of astronomy journals and then grabbing publications from these journals to create an astronomy-based document set. In order to make use of older pdfs that may have not been typing compatible, we converted the pdfs to images and ran language recognition on the images to create a set of text from the pdf. Then, we removed all the stop words, lemmatized to remove tenses and plurality, and finally uploaded the raw documents to our google cloud bucket. Our goal was to maximize our document space given the time, which resulted in ~750 tokenized documents. The second round of precomputation comes from aggregating our document space, which gets cumulative sums for the vocab space and calculates the overall term frequency\u2013inverse document frequency vector. Once we have the cumulative count for each of the tokens in the vocab space, we run back over our documents to create a matched vector with tf-idf weights matching that of the vocab space. Finally, token, document, idf score tuples are pushed to a PostgreSQL database to allow rapid computation on the backend. The technologies used in this piece are: python, google cloud bucket, google cloud postgreSQL, tesseract OCR and nltk.\nBackend\nThe backend has three pieces: pullDocuments, searchSpaceDB, and searchDB. pullDocuments grabs the information from the SQL database given an array of document IDs. searchSpaceDB takes a text query, generates an tf-idf vector for that text input matching the shape of the vocab vector, runs the matching computation using cosine similarity with the other documents in the table and returns the top 30 matched scores. searchDB returns information linking back to core, so that when we return the document id, we can directly link the users back to relevant papers. The technologies this piece uses are: python, flask, node.js/express, google cloud functions, google cloud buckets and postgreSQL.\nFrontend\nThe frontend was built using react/node.js and used Material-ui for the styled components. The frontend runs on google app engine.\nChallenges we ran into\nOur primary challenges were related to integrating with the google platform. Initially, we had wanted to be able to use a pdf for submission formats, but that required the integration with tesseract, which in turn required a custom installation on a docker image. We also had some difficulty connecting all the working pieces and spent a substantial amount of time setting up DNS redirects and tokens, as well as making sure our APIs allowed cross-origin.\nAccomplishments that we're proud of\nIt works. We built a full-stack app from the ground up with a functional search engine in it within 24 hours.\nWhat we learned\nWe need to allocate much more time to configuring google cloud given that it is such a powerful, but complicated tool.\nWhat's next for Kerbal Search Program\nWe would really like to add in pdf submission support, which requires integration with tesseract. This will require redesigning the backend so the content type is a pdf instead of plaintext.", "link": "https://devpost.com/software/kerbal-search-program", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nthis project was built with the idea of matching larger queries and providing matching between text and research papers. if one is looking for papers similar to a paper they have seen before, our goal is to have that be easily operable. we attempt to remove all the guesswork from tailoring a search by using a multi-token comparison algorithm.\nwhat it does\nour application takes a number of papers (notably astronomy and space-based papers), converts them to static images, runs text recognition on the pdf image, removes garbage words, tokenizes the documents, and generates a vocab space vector. the frontend takes a multi-token input and attempts to match the term frequency\u2013inverse document frequency scores of the documents we precomputed on the backend. at the end of this matching process, we return the top 30 matched scores.\nhow we built it\nprecomputation\nwe pulled papers from core, which is a database for academic papers. this was done by selecting a number of astronomy journals and then grabbing publications from these journals to create an astronomy-based document set. in order to make use of older pdfs that may have not been typing compatible, we converted the pdfs to images and ran language recognition on the images to create a set of text from the pdf. then, we removed all the stop words, lemmatized to remove tenses and plurality, and finally uploaded the raw documents to our google cloud bucket. our goal was to maximize our document space given the time, which resulted in ~750 tokenized documents. the second round of precomputation comes from aggregating our document space, which gets cumulative sums for the vocab space and calculates the overall term frequency\u2013inverse document frequency vector. once we have the cumulative count for each of the tokens in the vocab space, we run back over our documents to create a matched vector with tf-idf weights matching that of the vocab space. finally, token, document, idf score tuples are pushed to a postgresql database to allow rapid computation on the backend. the technologies used in this piece are: python, google cloud bucket, google cloud postgresql, tesseract ocr and nltk.\nbackend\nthe backend has three pieces: pulldocuments, searchspacedb, and searchdb. pulldocuments grabs the information from the sql database given an array of document ids. searchspacedb takes a text query, generates an tf-idf vector for that text input matching the shape of the vocab vector, runs the matching computation using cosine similarity with the other documents in the table and returns the top 30 matched scores. searchdb returns information linking back to core, so that when we return the document id, we can directly link the users back to relevant papers. the technologies this piece uses are: python, flask, node.js/express, google cloud functions, google cloud buckets and postgresql.\nfrontend\nthe frontend was built using react/node.js and used material-ui for the styled components. the frontend runs on google app engine.\nchallenges we ran into\nour primary challenges were related to integrating with the google platform. initially, we had wanted to be able to use a pdf for submission formats, but that required the integration with tesseract, which in turn required a custom installation on a docker image. we also had some difficulty connecting all the working pieces and spent a substantial amount of time setting up dns redirects and tokens, as well as making sure our apis allowed cross-origin.\naccomplishments that we're proud of\nit works. we built a full-stack app from the ground up with a functional search engine in it within 24 hours.\nwhat we learned\nwe need to allocate much more time to configuring google cloud given that it is such a powerful, but complicated -----> tool !!! .\nwhat's next for kerbal search program\nwe would really like to add in pdf submission support, which requires integration with tesseract. this will require redesigning the backend so the content type is a pdf instead of plaintext.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59504461}, {"Unnamed: 0": 4476, "autor": "Wildcat Protection System", "date": null, "content": "Inspiration\nMy Inspiration for this app was the fear many women have of being alone at night due to the prevalence of sexual assaults. I had no idea the extent to which women are sexually assaulted and the fear most women go through as a result of this till I researched the topic as apart of a school assignment. I was shocked to find that only about 33.4% of all rapists are ever convicted. When in class, I asked people to raise their hands if they lived in fear of being assaulted sexually. Almost all of the girls raised their hands. Something had to change. So, when I came to this competition, I thought it would be a great time to workshop a potential solution or treatment for this problem. That's why I decided to create an app aimed at protecting people from sexual assault.\nWhat it does\nMy app allows college students to quickly call emergency services, create flashing lights, and play loud noises in one tap. However, due to technical constraints that will be explained later, the current implementation only allows for quick and easy calling of emergency services. Currently, I have it set to call a random number instead of 911 because I was worried about people calling 911 excessively when I demo it. It will call 911 if I release it.\nHow we built it\nI used React Native, a framework built by Facebook, and Expo, a tool for running apps built with react without paying for IOS developer permissions.\nChallenges we ran into\nThe only computer I had for this entire weekend was a chrome book with no capability to install or run programs. So, I had to do everything using online IDE's or frameworks like Expo's \"snack\" feature. It allows users to create short React Native programs over the internet and provides a qr code that can be used to run said programs through expo on any mobile device. In addition, I had no knowledge of React Native prior to coming to Hack K-State, so I had to learn it completely from scratch.\nAccomplishments that we're proud of\nI learned React Native completely from scratch. I (partly) solved a real world problem using programming (or at least a band-aid for a real world problem). I built a mobile app, something I had no idea of how to do hours before Hack K-State.\nWhat we learned\nI learned how to use React Native and how to use Expo. In addition, I learned how to solve real-world problems using programming as well as how to tailor a program to be more accessible to all, something I didn't notice until now.\nWhat's next for Wildcat Protection System\nI intend to expand the features when I return home and gain access to a computer capable of downloading and running software. In addition, I will propose my solution (once it reaches a certain level of polish) to colleges around me in the hope that they will adopt it to protect their students.", "link": "https://devpost.com/software/wildcat-protection-system", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nmy inspiration for this app was the fear many women have of being alone at night due to the prevalence of sexual assaults. i had no idea the extent to which women are sexually assaulted and the fear most women go through as a result of this till i researched the topic as apart of a school assignment. i was shocked to find that only about 33.4% of all rapists are ever convicted. when in class, i asked people to raise their hands if they lived in fear of being assaulted sexually. almost all of the girls raised their hands. something had to change. so, when i came to this competition, i thought it would be a great time to workshop a potential solution or treatment for this problem. that's why i decided to create an app aimed at protecting people from sexual assault.\nwhat it does\nmy app allows college students to quickly call emergency services, create flashing lights, and play loud noises in one tap. however, due to technical constraints that will be explained later, the current implementation only allows for quick and easy calling of emergency services. currently, i have it set to call a random number instead of 911 because i was worried about people calling 911 excessively when i demo it. it will call 911 if i release it.\nhow we built it\ni used react native, a framework built by facebook, and expo, a -----> tool !!!  for running apps built with react without paying for ios developer permissions.\nchallenges we ran into\nthe only computer i had for this entire weekend was a chrome book with no capability to install or run programs. so, i had to do everything using online ide's or frameworks like expo's \"snack\" feature. it allows users to create short react native programs over the internet and provides a qr code that can be used to run said programs through expo on any mobile device. in addition, i had no knowledge of react native prior to coming to hack k-state, so i had to learn it completely from scratch.\naccomplishments that we're proud of\ni learned react native completely from scratch. i (partly) solved a real world problem using programming (or at least a band-aid for a real world problem). i built a mobile app, something i had no idea of how to do hours before hack k-state.\nwhat we learned\ni learned how to use react native and how to use expo. in addition, i learned how to solve real-world problems using programming as well as how to tailor a program to be more accessible to all, something i didn't notice until now.\nwhat's next for wildcat protection system\ni intend to expand the features when i return home and gain access to a computer capable of downloading and running software. in addition, i will propose my solution (once it reaches a certain level of polish) to colleges around me in the hope that they will adopt it to protect their students.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59504476}, {"Unnamed: 0": 4484, "autor": "Bon Voyage", "date": null, "content": "Inspiration\nInspiration is taken from the space invaders game where the aliens or asteroids used to come towards the spaceship and all we have to do is to kill or destroy those aliens and win the game.\nWhat it does\nBon Voyage is a real fun Android game which is all about destroying the aliens and saving the Earth. Additionally, if someone is not playing this game for a significant period of time but has this game installed, push notifications have been enabled using Firebase that will remind the user to play this game.\nHow we built it\nThe game is built using Thunkable. The main reason behind using this technology is mainly to switch from using traditional technologies to build games and try something new. It is a very useful tool to build Android apps and working with this technology was really an awesome experience.\nChallenges we ran into\nThis is my first project using Thunkable. So, working with it was bit of a hurdle as a beginner.\nAccomplishments that we're proud of\nThe game works!!\nWhat we learned\nI have learnt to use Thunkable to build Android apps and games.\nWhat's next for Bon Voyage\nI will work more on optimizing this game.", "link": "https://devpost.com/software/bon-voyage-fcp1z5", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ninspiration is taken from the space invaders game where the aliens or asteroids used to come towards the spaceship and all we have to do is to kill or destroy those aliens and win the game.\nwhat it does\nbon voyage is a real fun android game which is all about destroying the aliens and saving the earth. additionally, if someone is not playing this game for a significant period of time but has this game installed, push notifications have been enabled using firebase that will remind the user to play this game.\nhow we built it\nthe game is built using thunkable. the main reason behind using this technology is mainly to switch from using traditional technologies to build games and try something new. it is a very useful -----> tool !!!  to build android apps and working with this technology was really an awesome experience.\nchallenges we ran into\nthis is my first project using thunkable. so, working with it was bit of a hurdle as a beginner.\naccomplishments that we're proud of\nthe game works!!\nwhat we learned\ni have learnt to use thunkable to build android apps and games.\nwhat's next for bon voyage\ni will work more on optimizing this game.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59504484}, {"Unnamed: 0": 4507, "autor": "Weather-O", "date": null, "content": "Inspiration\nI thought it would be fun to create a weather app that not only shows you the weather, but you can use the app to connect with your friends and family members around the world.\nWhat it does\nWeather app is a user-friendly app that shows the weather where ever you are and you can also message/add family and friends on there too! That way it's easier to just send them weather info and other weather related topics (such as sending sunset views or rainy day photos).\nHow we built it\nAll of this was built on Figma. Icons and images were all created by me on Procreate. My brainstorming started off in the apps called Concepts.\nChallenges we ran into\nA lot of ideas were running through my mind on how I can make transitions for the app and other fun pages. Hardest part was understanding prototyping and messed up multiple times.\nAccomplishments that we're proud of\nGetting most of the app's ideas onto Figma and the designs/icons I created\nWhat we learned\nFigma is a really good tool and is really advanced when it comes to prototyping a product designer's ideas.\nWhat's next for Weather-O\nPlan to work on it more later on! It was a fun project to do.", "link": "https://devpost.com/software/weather-o", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ni thought it would be fun to create a weather app that not only shows you the weather, but you can use the app to connect with your friends and family members around the world.\nwhat it does\nweather app is a user-friendly app that shows the weather where ever you are and you can also message/add family and friends on there too! that way it's easier to just send them weather info and other weather related topics (such as sending sunset views or rainy day photos).\nhow we built it\nall of this was built on figma. icons and images were all created by me on procreate. my brainstorming started off in the apps called concepts.\nchallenges we ran into\na lot of ideas were running through my mind on how i can make transitions for the app and other fun pages. hardest part was understanding prototyping and messed up multiple times.\naccomplishments that we're proud of\ngetting most of the app's ideas onto figma and the designs/icons i created\nwhat we learned\nfigma is a really good -----> tool !!!  and is really advanced when it comes to prototyping a product designer's ideas.\nwhat's next for weather-o\nplan to work on it more later on! it was a fun project to do.", "sortedWord": "None", "removed": "Nan", "score": 5, "comments": 1, "media": null, "medialink": null, "identifyer": 59504507}, {"Unnamed: 0": 4511, "autor": "SafeSpace.", "date": null, "content": "Built in conjunction with the Girls in Tech Mental Health for All Hackathon: Building an inclusive future requires mental health and wellbeing technologies that cater to all\nDeployed App\nProject Board\nWireframes\nPrototype\nUser Flows and Data\nApp Details\nAs a team of fullstack software engineers, our team decided on technologies that could produce a usable, scalable MVP version of the prototype.\nTech Stack\nNext JS\nSupabase\nStyled Components\nReact Bootstrap\nBoostrap 5\nReact Select\nReact Player\nDesign Tools\nFigma\nBootstrap UI Kit\nWAVE Web Accessibility Evaluation Tool Chrome Extension\nWeb a11y guidelines - Design and Color\nIntroduction\nAccording to an independent study by McKinsey and Company, \u201cwomen of color are more likely to face disrespectful and \u2018othering\u2019 microaggressions\u201d, which perpetuate workplace trauma. Black women are facing these issues at an alarmingly disproportionate rate.\nWe are often still expected to produce excellent work throughout these violent experiences and there is not a place for us to be free of judgement to share with other Black women who are also on the healing journey through workplace trauma, which can make us feel alone and lead to depression.\nDuring the trauma, we may not realize that we need to track these instances or need a community. As such, these events often go unreported and our source for proof is defined based on memory alone.\nWe need a place that we can feel safe to share, or at the very least, keep track of these occurrences so that when the time comes, we will have a list of these things ready for us to make our case.\nSafeSpace. also collects data on these issues for the world to see the impacts on Black women and how it hits companies\u2019 bottom line. By tracking and fighting back in this way, we can help ourselves and other Black women know that they are not alone and that something must be done to stop workplace violence.\nPurpose & Motivation\nAccording to the McKinsey and Company study and our own constant personal experiences, as Black women, we experience more microaggressions than other groups of women, and are three to four times as likely as white women to be subjected to disrespectful and \u201cothering\u201d comments and behavior. We are also less likely to report that our managers check in on our well-being or help us balance priorities and deadlines.\nThe motivation behind this application is two-fold. To provide a SafeSpace. for us to go in the midst of the violent behaviors we endure as a place for relief and centering and to document the incidents. In recording the incidents, not only do we name our trauma and record how we have handled and grown through our healing, but it also provides us with the documentation we need when we decide enough is enough and need to report the behavior.\nHow does the application work?\nUsers are able to join and sign into SafeSpace. anonymously by using their email address, which sends them a link to the email address they used to login to the application.\nWhen a user logs in for the first time, they are prompted to set up their username, password, and if they are a member of the LGBTQA+ community. Then, they are taken to a Conflict Assessment that tells them a little more about themselves. This also allows other users to see the type of the person with whom they are interacting so, when giving advice, commenters can frame the advice in a way that meets the requester\u2019s style.\nFor every subsequent login, users are prompted to tell SafeSpace. how they are feeling after which, they enter the journal view where they can very quickly add an entry to their log to keep as reference for private use or to share with the community for advice.\nThe Journal is the central feature of this application because it allows Black women to keep track of work related trauma associated with different types of microaggressions and work through those at their own pace or have as a recorded reference for future use.\nThere are options for the user to check out the community posts, recommend stellar companies, add or find inspiration, and/or look through resources for support in their area.\nHow was the application developed?\nAs a team, we planned the application by creating user flows, a low-fidelity wireframe and data relationships. Then, we created a prototype in Figma. We built SafeSpace. using Next JS, styled-components, supabase as the database, React Bootstrap, Bootstrap 5, and React Select.\nWe followed accessibility (a11y) guidelines using the WAVE Web Accessibility Evaluation Tool Chrome Extension and Web Content Accessibility Guidelines (WCAG) a11y guidelines.\nHow to use the application\nUsers can login to SafeSpace. on any device with an internet connection, create journal entries of workplace microaggressions, follow, comment, and react to community posts, view resources and inspiration, and recommend companies that are doing diversity, equity and inclusion right.\nDifficulties & Challenges faced during the design and/or development process.\nEnsuring that we all were on the right path for the outcomes of the app because it is easy to slip into the social media path, but we intentionally worked as a team to define who and why the app exists and the how and what was easily obtained.\nThere were also various features that we thought about and added some functionality to several but did not complete them all as the prototype MVP is a POC that needs more user research and work.\nGo-to-Market (How will the application be available to the public, and is it scalable?)\nThe current version of SafeSpace. is a partial features MVP prototype, but is available for anyone with an internet connection to review at https://safespacemvp.netlify.app. While the team has done aggressive manual testing, there are no unit tests included, which would need to be completed, among other measures, prior to being prod ready.\nThere would also need to be some algorithms in place to check content standards to ensure content meets guidelines as well as terms and conditions as admins manually reviewing posts is not scalable. There is also a need to ensure that users are actually Black women, which we did not cover in this MVP version.\nOnce those underlying concerns are addressed, the application runs itself as it was created for and belongs to anyone who identifies as a Black woman. The core is to have a journal and SafeSpace. does this really well.\nProject Next steps\nSetup process for user access and approval\nSetup persistence/database for comments and reactions on community posts\nConnect community comments to posts (decided to descope because of the social aspect)\nAllow users to create multiple journals\nUsers can follow other users\nUsers can follow community posts\nUsers can submit resources\nUsers can upvote companies and doing so would move them to the top of companies page\nData dashboard for sharing details on occurences", "link": "https://devpost.com/software/safe-space-rj5ysn", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "built in conjunction with the girls in tech mental health for all hackathon: building an inclusive future requires mental health and wellbeing technologies that cater to all\ndeployed app\nproject board\nwireframes\nprototype\nuser flows and data\napp details\nas a team of fullstack software engineers, our team decided on technologies that could produce a usable, scalable mvp version of the prototype.\ntech stack\nnext js\nsupabase\nstyled components\nreact bootstrap\nboostrap 5\nreact select\nreact player\ndesign tools\nfigma\nbootstrap ui kit\nwave web accessibility evaluation -----> tool !!!  chrome extension\nweb a11y guidelines - design and color\nintroduction\naccording to an independent study by mckinsey and company, \u201cwomen of color are more likely to face disrespectful and \u2018othering\u2019 microaggressions\u201d, which perpetuate workplace trauma. black women are facing these issues at an alarmingly disproportionate rate.\nwe are often still expected to produce excellent work throughout these violent experiences and there is not a place for us to be free of judgement to share with other black women who are also on the healing journey through workplace trauma, which can make us feel alone and lead to depression.\nduring the trauma, we may not realize that we need to track these instances or need a community. as such, these events often go unreported and our source for proof is defined based on memory alone.\nwe need a place that we can feel safe to share, or at the very least, keep track of these occurrences so that when the time comes, we will have a list of these things ready for us to make our case.\nsafespace. also collects data on these issues for the world to see the impacts on black women and how it hits companies\u2019 bottom line. by tracking and fighting back in this way, we can help ourselves and other black women know that they are not alone and that something must be done to stop workplace violence.\npurpose & motivation\naccording to the mckinsey and company study and our own constant personal experiences, as black women, we experience more microaggressions than other groups of women, and are three to four times as likely as white women to be subjected to disrespectful and \u201cothering\u201d comments and behavior. we are also less likely to report that our managers check in on our well-being or help us balance priorities and deadlines.\nthe motivation behind this application is two-fold. to provide a safespace. for us to go in the midst of the violent behaviors we endure as a place for relief and centering and to document the incidents. in recording the incidents, not only do we name our trauma and record how we have handled and grown through our healing, but it also provides us with the documentation we need when we decide enough is enough and need to report the behavior.\nhow does the application work?\nusers are able to join and sign into safespace. anonymously by using their email address, which sends them a link to the email address they used to login to the application.\nwhen a user logs in for the first time, they are prompted to set up their username, password, and if they are a member of the lgbtqa+ community. then, they are taken to a conflict assessment that tells them a little more about themselves. this also allows other users to see the type of the person with whom they are interacting so, when giving advice, commenters can frame the advice in a way that meets the requester\u2019s style.\nfor every subsequent login, users are prompted to tell safespace. how they are feeling after which, they enter the journal view where they can very quickly add an entry to their log to keep as reference for private use or to share with the community for advice.\nthe journal is the central feature of this application because it allows black women to keep track of work related trauma associated with different types of microaggressions and work through those at their own pace or have as a recorded reference for future use.\nthere are options for the user to check out the community posts, recommend stellar companies, add or find inspiration, and/or look through resources for support in their area.\nhow was the application developed?\nas a team, we planned the application by creating user flows, a low-fidelity wireframe and data relationships. then, we created a prototype in figma. we built safespace. using next js, styled-components, supabase as the database, react bootstrap, bootstrap 5, and react select.\nwe followed accessibility (a11y) guidelines using the wave web accessibility evaluation tool chrome extension and web content accessibility guidelines (wcag) a11y guidelines.\nhow to use the application\nusers can login to safespace. on any device with an internet connection, create journal entries of workplace microaggressions, follow, comment, and react to community posts, view resources and inspiration, and recommend companies that are doing diversity, equity and inclusion right.\ndifficulties & challenges faced during the design and/or development process.\nensuring that we all were on the right path for the outcomes of the app because it is easy to slip into the social media path, but we intentionally worked as a team to define who and why the app exists and the how and what was easily obtained.\nthere were also various features that we thought about and added some functionality to several but did not complete them all as the prototype mvp is a poc that needs more user research and work.\ngo-to-market (how will the application be available to the public, and is it scalable?)\nthe current version of safespace. is a partial features mvp prototype, but is available for anyone with an internet connection to review at https://safespacemvp.netlify.app. while the team has done aggressive manual testing, there are no unit tests included, which would need to be completed, among other measures, prior to being prod ready.\nthere would also need to be some algorithms in place to check content standards to ensure content meets guidelines as well as terms and conditions as admins manually reviewing posts is not scalable. there is also a need to ensure that users are actually black women, which we did not cover in this mvp version.\nonce those underlying concerns are addressed, the application runs itself as it was created for and belongs to anyone who identifies as a black woman. the core is to have a journal and safespace. does this really well.\nproject next steps\nsetup process for user access and approval\nsetup persistence/database for comments and reactions on community posts\nconnect community comments to posts (decided to descope because of the social aspect)\nallow users to create multiple journals\nusers can follow other users\nusers can follow community posts\nusers can submit resources\nusers can upvote companies and doing so would move them to the top of companies page\ndata dashboard for sharing details on occurences", "sortedWord": "None", "removed": "Nan", "score": 5, "comments": 0, "media": null, "medialink": null, "identifyer": 59504511}, {"Unnamed: 0": 4552, "autor": "LiquidOverlay", "date": null, "content": "Inspiration\nLiquidOverlay is a program made to primarily assist the HoH and Deaf communities worldwide in their gaming journeys and help with ease of use. Upon listening to Steven Spohn's speech at the LiquidHacks opening ceremony, we were able to see how gaming can make a difference in a person's life. Specifically, the lives of those that need assistive technology. We were inspired to make a difference. The idea started in a Google Doc where we compiled a list of options for the competition to enhance the gaming experience and accessibility. When we researched the accessibility options offered for the competitive FPS game \"Valorant\", we saw that there were none offered to address this community. Given that this game is heavily reliant on sound, from agents' abilities to footsteps and gunshots, we realized that to be able to play this game competitively, this is a necessary tool. We wanted to level the playing field for the HoH and Deaf community.\nWhat it does\nWe decided to make an overlay that uses visual effects to help deaf people \"see\" sound. These effects will help show where the sound was last heard in-game for the duration of the noise and will allow the HoH or Deaf player to be on the lookout for enemies. This would facilitate the process and allow them to visually see what the other players are hearing. This idea grew and we asked ourselves, \"Why stop there?\" What can we add to make this a tool for even more people? This led our team to look into Speech to Text to be able to translate for people that speak different languages than those used in the game.\nHow we built it\nIt was built with 99.9% Kotlin, we utilized google speech API to convert audio into text, we added\nChallenges we ran into\nSome of the challenges we ran into were coding with very minimal C, but still accessing the windows API to overlay the application.\nAccomplishments that we're proud of\nWe were able to draw to Windows API without using one line of C!\nWhat we learned\nLearned a lot about Native code, how to pace ourselves and not rush ahead.\nWhat's next for LiquidOverlay\nWe want to add Machine learning to detect footsteps and gunshots better since right now only a decibel system is utilized.\nRun the .jar file to create the GUI, then click \"start overlay\" to overlay your game!", "link": "https://devpost.com/software/liquidoverlay", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nliquidoverlay is a program made to primarily assist the hoh and deaf communities worldwide in their gaming journeys and help with ease of use. upon listening to steven spohn's speech at the liquidhacks opening ceremony, we were able to see how gaming can make a difference in a person's life. specifically, the lives of those that need assistive technology. we were inspired to make a difference. the idea started in a google doc where we compiled a list of options for the competition to enhance the gaming experience and accessibility. when we researched the accessibility options offered for the competitive fps game \"valorant\", we saw that there were none offered to address this community. given that this game is heavily reliant on sound, from agents' abilities to footsteps and gunshots, we realized that to be able to play this game competitively, this is a necessary -----> tool !!! . we wanted to level the playing field for the hoh and deaf community.\nwhat it does\nwe decided to make an overlay that uses visual effects to help deaf people \"see\" sound. these effects will help show where the sound was last heard in-game for the duration of the noise and will allow the hoh or deaf player to be on the lookout for enemies. this would facilitate the process and allow them to visually see what the other players are hearing. this idea grew and we asked ourselves, \"why stop there?\" what can we add to make this a tool for even more people? this led our team to look into speech to text to be able to translate for people that speak different languages than those used in the game.\nhow we built it\nit was built with 99.9% kotlin, we utilized google speech api to convert audio into text, we added\nchallenges we ran into\nsome of the challenges we ran into were coding with very minimal c, but still accessing the windows api to overlay the application.\naccomplishments that we're proud of\nwe were able to draw to windows api without using one line of c!\nwhat we learned\nlearned a lot about native code, how to pace ourselves and not rush ahead.\nwhat's next for liquidoverlay\nwe want to add machine learning to detect footsteps and gunshots better since right now only a decibel system is utilized.\nrun the .jar file to create the gui, then click \"start overlay\" to overlay your game!", "sortedWord": "None", "removed": "Nan", "score": 12, "comments": 4, "media": null, "medialink": null, "identifyer": 59504552}, {"Unnamed: 0": 4578, "autor": "Roaming Angels", "date": null, "content": "Inspiration\nI was inspired by my original idea, a way to track ambulances while they were on the way to the person who called. However, this original code did not work and there were many errors (which is not exactly important). Worried, I went back to the drawing board and came up with the idea of a general, quick first aid site. I did some research and surprisingly found that many Americans don\u2019t know how to do first aid - an essential part of society.\nYou can also find details at this link.\nWhat it does\nThis is a learning tool and resource which can also act as an on-the-fly guide. This is to help raise awareness about first aid and to encourage people to start learning and (eventually get certified) which can increase the number of people who can help others in sticky situations.\nYou can also find details at this link.\nHow I built it\nI built this entire thing using repl.it (although I had switched to local files and used my coding editor when attempting to use Twilio and messengerbird (see below). The main languages I ended up using were javascript (for the API which was for the location services), CSS, and HTML.\nAs a person who has worked with the front-end time and time again, it was pretty easy to set up the CSS and HTML outline (as I have done it a lot). Next, it was a matter of doing research, getting information, and understanding how I could tackle the project. I started with the actual project site, working to compile the necessary information and bring them together using the code. In addition, I started working on the necessary information to gather the location services.\nChallenges I ran into\nI had a huge setback with the location services - and not just with my original hack. I attempted to use the google maps API in JS (after other ideas of tackling it, using python or SQL fell through), which after some hard (mostly annoying) work had it all set up, only to find that due to specific security actions google takes prevented me to use the API. However, not all of it went to waste - I was able to combine some of my earlier code, using the iframe tag, and \u201cmerge\u201d it with some of the javascript code, which allowed me to (successfully) get the lat and long coordinates of a specific device and provide the person with nearby hospitals.\nHowever, when trying to implement Twilio (and after that failed Messengerbird, which is similar) to provide the user with a message of relevant information they may need, it fell through due to pricing and other issues.\nYou can see the completed location services part at this (please make sure that your browser can get your location) link.\nAccomplishments that I am proud of\nI am proud of being able to (finally) complete this! The location idea was an extreme challenge - I spent hours (like literally at least 8) on it, and tried line after line, test after test, to find out how I could get my desired output (and my family was witness to me almost crying and being extremely happy for a solid few minutes). Also, I am pretty thrilled that I was able to incorporate the parts I wanted to during the hack, especially considering my hectic weekend and regatta on Sunday morning.\nYou can see the completed location services part at (please make sure that your browser can get your location) this link.\nWhat I learned\nI learned a lot. It was challenging to do this on my own, implementing the back-end into the front end. In addition, it was a challenge to find a way around the API, something which you aren\u2019t exactly supposed to be finding ways around. It was a good learning experience, and it was a challenge to complete this under the time frame I had. It was a pleasure returning to hackPHS.\nWhat's next for Roaming Angels\nGoals: I want to be able to implement the message function (which would include automated, personalized messages based on specific circumstances) as well as a way to be able to search for specific first aid ways. In addition, although I am not sure, it could be helpful to add more developed ideas for symptoms (like dedicating a section) even though symptoms can be hard to detect and not on the first thing in a civilian\u2019s mind when giving first aid.\nProject site: link About site: link Feedback (also on site): link", "link": "https://devpost.com/software/roaming-angels", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ni was inspired by my original idea, a way to track ambulances while they were on the way to the person who called. however, this original code did not work and there were many errors (which is not exactly important). worried, i went back to the drawing board and came up with the idea of a general, quick first aid site. i did some research and surprisingly found that many americans don\u2019t know how to do first aid - an essential part of society.\nyou can also find details at this link.\nwhat it does\nthis is a learning -----> tool !!!  and resource which can also act as an on-the-fly guide. this is to help raise awareness about first aid and to encourage people to start learning and (eventually get certified) which can increase the number of people who can help others in sticky situations.\nyou can also find details at this link.\nhow i built it\ni built this entire thing using repl.it (although i had switched to local files and used my coding editor when attempting to use twilio and messengerbird (see below). the main languages i ended up using were javascript (for the api which was for the location services), css, and html.\nas a person who has worked with the front-end time and time again, it was pretty easy to set up the css and html outline (as i have done it a lot). next, it was a matter of doing research, getting information, and understanding how i could tackle the project. i started with the actual project site, working to compile the necessary information and bring them together using the code. in addition, i started working on the necessary information to gather the location services.\nchallenges i ran into\ni had a huge setback with the location services - and not just with my original hack. i attempted to use the google maps api in js (after other ideas of tackling it, using python or sql fell through), which after some hard (mostly annoying) work had it all set up, only to find that due to specific security actions google takes prevented me to use the api. however, not all of it went to waste - i was able to combine some of my earlier code, using the iframe tag, and \u201cmerge\u201d it with some of the javascript code, which allowed me to (successfully) get the lat and long coordinates of a specific device and provide the person with nearby hospitals.\nhowever, when trying to implement twilio (and after that failed messengerbird, which is similar) to provide the user with a message of relevant information they may need, it fell through due to pricing and other issues.\nyou can see the completed location services part at this (please make sure that your browser can get your location) link.\naccomplishments that i am proud of\ni am proud of being able to (finally) complete this! the location idea was an extreme challenge - i spent hours (like literally at least 8) on it, and tried line after line, test after test, to find out how i could get my desired output (and my family was witness to me almost crying and being extremely happy for a solid few minutes). also, i am pretty thrilled that i was able to incorporate the parts i wanted to during the hack, especially considering my hectic weekend and regatta on sunday morning.\nyou can see the completed location services part at (please make sure that your browser can get your location) this link.\nwhat i learned\ni learned a lot. it was challenging to do this on my own, implementing the back-end into the front end. in addition, it was a challenge to find a way around the api, something which you aren\u2019t exactly supposed to be finding ways around. it was a good learning experience, and it was a challenge to complete this under the time frame i had. it was a pleasure returning to hackphs.\nwhat's next for roaming angels\ngoals: i want to be able to implement the message function (which would include automated, personalized messages based on specific circumstances) as well as a way to be able to search for specific first aid ways. in addition, although i am not sure, it could be helpful to add more developed ideas for symptoms (like dedicating a section) even though symptoms can be hard to detect and not on the first thing in a civilian\u2019s mind when giving first aid.\nproject site: link about site: link feedback (also on site): link", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59504578}, {"Unnamed: 0": 4594, "autor": "Voice For Everybody", "date": null, "content": "Inspiration\nSince the outbreak of COVID-19, while the rest of the world has moved online, ASL speakers faced even greater inequities making it difficult for so many of them to communicate. However, this has to come to an end. In the pursuit of finding accessibility, I created a tool to empower ASL speakers to speak freely with the help of AI.\nWhat it does\nUses a webcam to translate ASL speech to text.\nHow we built it\nUsed Mediapipe to generate points on hands, then use those points to get training data set. I used Jupyter Notebook to run OpenCV and Mediapipe. Upon running our data in Mediapipe, we were able to get a skeleton map of the body with 22 points for each hand. These points can be mapped in 3-dimension as it contains X, Y, and Z axis. We processed these features (22 points x 3) by saving them into a spreadsheet. Then we divided the spreadsheet into training and testing data. Using the training set, we were able to create 6 Machine learning models:\n\u2022 Gradient Boost Classifier \u2022 XGBoost Classifier \u2022 Support Vector Machine \u2022 Logistic Regression \u2022 Ridge Classifier \u2022 Random Forest Classifier\nChallenges we ran into\n\u2022 Had to do solo work due to issues with the team \u2022 Time management \u2022 Project management \u2022 Lack of data\nAccomplishments that we're proud of\nProud of pivoting my original idea and completing this epic hackathon. Also proud of making a useful tool.\nWhat we learned\n\u2022 Time management \u2022 Project management\nWhat's next for Voice4Everyone\n\u2022 More training of data - more classifications \u2022 Phone app + Chrome Extension \u2022 Reverse translation: Converting English Text to ASL \u2022 Cleaner UI \u2022 Add support for the entire ASL dictionary and other sign languages", "link": "https://devpost.com/software/voice-for-everybody-wo6en7", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nsince the outbreak of covid-19, while the rest of the world has moved online, asl speakers faced even greater inequities making it difficult for so many of them to communicate. however, this has to come to an end. in the pursuit of finding accessibility, i created a -----> tool !!!  to empower asl speakers to speak freely with the help of ai.\nwhat it does\nuses a webcam to translate asl speech to text.\nhow we built it\nused mediapipe to generate points on hands, then use those points to get training data set. i used jupyter notebook to run opencv and mediapipe. upon running our data in mediapipe, we were able to get a skeleton map of the body with 22 points for each hand. these points can be mapped in 3-dimension as it contains x, y, and z axis. we processed these features (22 points x 3) by saving them into a spreadsheet. then we divided the spreadsheet into training and testing data. using the training set, we were able to create 6 machine learning models:\n\u2022 gradient boost classifier \u2022 xgboost classifier \u2022 support vector machine \u2022 logistic regression \u2022 ridge classifier \u2022 random forest classifier\nchallenges we ran into\n\u2022 had to do solo work due to issues with the team \u2022 time management \u2022 project management \u2022 lack of data\naccomplishments that we're proud of\nproud of pivoting my original idea and completing this epic hackathon. also proud of making a useful tool.\nwhat we learned\n\u2022 time management \u2022 project management\nwhat's next for voice4everyone\n\u2022 more training of data - more classifications \u2022 phone app + chrome extension \u2022 reverse translation: converting english text to asl \u2022 cleaner ui \u2022 add support for the entire asl dictionary and other sign languages", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59504594}, {"Unnamed: 0": 4602, "autor": "SpyTrainer", "date": null, "content": "Inspiration\nWe were inspired by the Spy vs. Spy, a favorite cartoon of both our hackers, as well as a passion for learning cryptography. Just like the two spies in the show, we can combat each other by making and breaking secret messages.\nWe made SpyTrainer a way to learn about encoding, decoding, encrypting, and decrypting because cybersecurity is all about the protection of computer systems and networks, which includes the data they contain. Encoding is used for maintaining data usability while encryption is essential for maintaining data confidentiality. We wanted users to be able to be \u201ctrained\u201d in becoming a spy by learning about the different encryptions and being able to test it out for themselves. .\nWhat it does\nOur home screen starts having the user select between encoding and encryption. If encoding is selected, the next screen shows six different encoding methods. The encoding techniques are Caesar\u2019s Cipher, Rot13, UTF-8, Base 64, Hex, and Big Int. After making the selection of the encoding process they would like to use, they can read a short description teaching them about how the process works before typing in phrases to test it for themselves. After, they can also decode the message they just typed in to see that the encoding and decoding process successfully work.\nThe encryption aspect of the platform works similarly with the user having the choice between RSA, Advanced Encryption Standard Electronic Code Block(AES ECB), and Advanced Encryption Standard Cipher-Block Chaining(AES CBC). In addition, the program also has 3 attacks: the Chinese Remainder Theorem, Coppersmith attack, and the dachshund attack that will be selected based on the user's input that is given.\nHow we built it\nWe used the kivy module to create a graphical user interface that can not only be used on the computer, but also exported onto a phone as an mobile app. For the cryptography aspect, we utilized the python cryptodome module as well as our implementations of attacks on known vulnerabilities.\nChallenges we ran into\nLearning and utilizing the kivy module for the graphical user interface was challenging for us because we had never used it in the past. Implementing the math behind some of the algorithms was also a challenge.\nAccomplishments that we're proud of\nWhile most of the encoding methods were rather simple, we are proud of the automated Caesar Cipher that can break a Caesar cipher with any shift using an encoded word dictionary. Also, we are proud of understanding the math behind RSA AES because the decryption process was complicated with the attacks implemented.\nWhat we learned\nTo make this platform, we had to learn a lot about cryptography since we did not have much previous experience with this. We also had to study the complicated number theory behind many of the algorithms and theorems, such as the ones involving RSA, the public key cryptosystem. We also had to teach ourself the entirety of the kivy and kivymd modules, which was quite a challenge, as we don't have much front-end experience.\nWhat's next for SpyTrainer\nIn the future, we want to add more RSA attacks, in addition to the Chinese Remainder Theorem, Coppersmith attack, and the dachshund attack. We would also like to add a page where users can work on creating their own encoding techniques that are not already on there. Lastly, we would expand the length of the text that can be imputed and generate a frequency analysis tool to test for simple substitution ciphers.", "link": "https://devpost.com/software/spytrainer", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe were inspired by the spy vs. spy, a favorite cartoon of both our hackers, as well as a passion for learning cryptography. just like the two spies in the show, we can combat each other by making and breaking secret messages.\nwe made spytrainer a way to learn about encoding, decoding, encrypting, and decrypting because cybersecurity is all about the protection of computer systems and networks, which includes the data they contain. encoding is used for maintaining data usability while encryption is essential for maintaining data confidentiality. we wanted users to be able to be \u201ctrained\u201d in becoming a spy by learning about the different encryptions and being able to test it out for themselves. .\nwhat it does\nour home screen starts having the user select between encoding and encryption. if encoding is selected, the next screen shows six different encoding methods. the encoding techniques are caesar\u2019s cipher, rot13, utf-8, base 64, hex, and big int. after making the selection of the encoding process they would like to use, they can read a short description teaching them about how the process works before typing in phrases to test it for themselves. after, they can also decode the message they just typed in to see that the encoding and decoding process successfully work.\nthe encryption aspect of the platform works similarly with the user having the choice between rsa, advanced encryption standard electronic code block(aes ecb), and advanced encryption standard cipher-block chaining(aes cbc). in addition, the program also has 3 attacks: the chinese remainder theorem, coppersmith attack, and the dachshund attack that will be selected based on the user's input that is given.\nhow we built it\nwe used the kivy module to create a graphical user interface that can not only be used on the computer, but also exported onto a phone as an mobile app. for the cryptography aspect, we utilized the python cryptodome module as well as our implementations of attacks on known vulnerabilities.\nchallenges we ran into\nlearning and utilizing the kivy module for the graphical user interface was challenging for us because we had never used it in the past. implementing the math behind some of the algorithms was also a challenge.\naccomplishments that we're proud of\nwhile most of the encoding methods were rather simple, we are proud of the automated caesar cipher that can break a caesar cipher with any shift using an encoded word dictionary. also, we are proud of understanding the math behind rsa aes because the decryption process was complicated with the attacks implemented.\nwhat we learned\nto make this platform, we had to learn a lot about cryptography since we did not have much previous experience with this. we also had to study the complicated number theory behind many of the algorithms and theorems, such as the ones involving rsa, the public key cryptosystem. we also had to teach ourself the entirety of the kivy and kivymd modules, which was quite a challenge, as we don't have much front-end experience.\nwhat's next for spytrainer\nin the future, we want to add more rsa attacks, in addition to the chinese remainder theorem, coppersmith attack, and the dachshund attack. we would also like to add a page where users can work on creating their own encoding techniques that are not already on there. lastly, we would expand the length of the text that can be imputed and generate a frequency analysis -----> tool !!!  to test for simple substitution ciphers.", "sortedWord": "None", "removed": "Nan", "score": 8, "comments": 1, "media": null, "medialink": null, "identifyer": 59504602}, {"Unnamed: 0": 4609, "autor": "Frontline.AI", "date": null, "content": "Inspiration\nThe recent covid-19 pandemic had made us realize that Frontline workers employed in retail stores are at a high risk of getting contaminated by infections. Wegman\u2019s challenge to aid frontline workers motivated our team to use our skills to make the workplace safer for Frontline workers. Making retail stores, like Wegmans, safer would ensure the safety of frontline workers, their families, and even the customers that visit the stores.\nWhat it does\nThe goal of our project is to create a tool that ensures the safety and hygiene of frontline workers in retail stores. Our project aims to use Computer Vision technology to classify whether the employee/customer is attempting to enter the store premises without a mask. If the algorithm recognizes that an individual is not wearing a mask, then a mask is automatically dispensed by the machine and the algorithm again checks if the mask is worn by the person. Especially, for employees, the program uses a rewarding system that awards them for following adequate guidelines to stop the spread of the pandemic.\nHow we built it\nChallenges we ran into\n1) Finding hardware equipment, especially cameras. 2) Training the machine learning model was a struggle in itself. 3) Making a mask dispensing machine from scratch caused a few problems here and there. 4) We were not able to find the Laser Infrared Thermal Sensor, therefore we were not able to implement the 'temperature check' feature.\nAccomplishments that we're proud of\n1) The computer vision algorithm works efficiently and effectively. 2) The mask dispenser works effectively and dispenses masks one at a time if and only if the Computer VIsion algorithm realizes that the individual entering the store is not wearing a mask properly. 3) All the components come together perfectly to make our project work seamlessly.\nWhat we learned\n1) Hackathons are a great place to learn, socialize and have fun. 2) We learned a lot of insights about computer vision technology especially about training machine learning models.\nWhat's next for Frontline.AI\n1) We would like to continue working on this application to make constructive changes like including more ways(including body temperature checks) to ensure the safety and hygiene of front-line workers employed in retail stores. 2) Integrate the technology with a CCTV camera to improve project performance. 3)We would like to develop an app that would integrate the reward system with the premium membership of various leading retail stores. 4) We would like to improve our UI for a better user experience.", "link": "https://devpost.com/software/frontline-ai", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nthe recent covid-19 pandemic had made us realize that frontline workers employed in retail stores are at a high risk of getting contaminated by infections. wegman\u2019s challenge to aid frontline workers motivated our team to use our skills to make the workplace safer for frontline workers. making retail stores, like wegmans, safer would ensure the safety of frontline workers, their families, and even the customers that visit the stores.\nwhat it does\nthe goal of our project is to create a -----> tool !!!  that ensures the safety and hygiene of frontline workers in retail stores. our project aims to use computer vision technology to classify whether the employee/customer is attempting to enter the store premises without a mask. if the algorithm recognizes that an individual is not wearing a mask, then a mask is automatically dispensed by the machine and the algorithm again checks if the mask is worn by the person. especially, for employees, the program uses a rewarding system that awards them for following adequate guidelines to stop the spread of the pandemic.\nhow we built it\nchallenges we ran into\n1) finding hardware equipment, especially cameras. 2) training the machine learning model was a struggle in itself. 3) making a mask dispensing machine from scratch caused a few problems here and there. 4) we were not able to find the laser infrared thermal sensor, therefore we were not able to implement the 'temperature check' feature.\naccomplishments that we're proud of\n1) the computer vision algorithm works efficiently and effectively. 2) the mask dispenser works effectively and dispenses masks one at a time if and only if the computer vision algorithm realizes that the individual entering the store is not wearing a mask properly. 3) all the components come together perfectly to make our project work seamlessly.\nwhat we learned\n1) hackathons are a great place to learn, socialize and have fun. 2) we learned a lot of insights about computer vision technology especially about training machine learning models.\nwhat's next for frontline.ai\n1) we would like to continue working on this application to make constructive changes like including more ways(including body temperature checks) to ensure the safety and hygiene of front-line workers employed in retail stores. 2) integrate the technology with a cctv camera to improve project performance. 3)we would like to develop an app that would integrate the reward system with the premium membership of various leading retail stores. 4) we would like to improve our ui for a better user experience.", "sortedWord": "None", "removed": "Nan", "score": 6, "comments": 0, "media": null, "medialink": null, "identifyer": 59504609}, {"Unnamed: 0": 4613, "autor": "The Perfplace", "date": null, "content": "Inspiration\nF\u00fcr das Projekt haben wir uns in die Rolle eines Wohnungssuchenden versetzt. Innerhalb der Gruppe waren wir uns einig, dass die g\u00e4ngigen Immobilien-Plattformen zu wenig Informationen dar\u00fcber liefern wie die Wohngegend aussieht. Deshalb m\u00f6chten wir mit unserem Projekt eine M\u00f6glichkeit schaffen, mehr Informationen dem Nutzenden zur Verf\u00fcgung zu stellen. Die Informationen sollen dabei nicht nur oberfl\u00e4chlich und allgemein g\u00fcltig sein sondern individuell auf die Bed\u00fcrfnisse des Nutzers angepasst werden.\nWhat it does\nZiel der Applikation ist es, dass Nutzer f\u00fcr sie wichtige Eigenschaften festlegen k\u00f6nnen (z.B. wenig L\u00e4rm, N\u00e4he zu Schulen, Einkaufsm\u00f6glichkeiten, Naherholungsgebiete). Basierend auf diesen Eingaben wird ein Score erstellt, der anschliessend auf der Karte mittels Heatmap visualisiert wo die Kriterien am besten erf\u00fcllt werden. In einem weiteren Schritt w\u00e4re die Anbindung von Immobilienplattformen (z.B. wie auf Immobilien von Comparis) gedacht, sodass die Nutzer direkt verf\u00fcgbare Mietobjekte in diesen Bereichen angezeigt erhalten.\nHow we built it\nAls Datengrundlage haben wir vor allem Daten vom Open Data Portal der Stadt St. Gallen sowie von Cividi herangezogen. Die Mockups wurden mit Balsamiq erstellt und die Karten mit den Heatmaps innerhalb von Excel (Einf\u00fcgen > 3D-Karten).\nDie App ist noch nicht erstellt da uns hier die Programmiererfahrung fehlt.\nChallenges we ran into\nAufgrund der fehlenden Programmierkenntnisse konnten wir den Vorschlag nicht umsetzen. Zudem war es schwierig die gew\u00fcnschten Funktionalit\u00e4ten ohne Erhebung der Bed\u00fcrfnisse zu definieren.\nAccomplishments that we're proud of\nWir sind stolz darauf dass wir innerhalb dieses Hackathons neue Ideen erarbeitet haben und neue Einblicke in das Thema Open Data und m\u00f6gliche Anwendungsf\u00e4lle erhalten haben.\nWir sind der Meinung dass wir hier ein Konzept erstellt haben, dass einen Nutzen f\u00fcr die Bev\u00f6lkerung bietet.\nWhat we learned\nWir haben einige Datenbanken bei der Suche nach relevanten Datens\u00e4tzen untersucht und m\u00f6gliche Anwendungsf\u00e4lle f\u00fcr Open Data kennengelernt. Zudem konnten wir innerhalb des Teams unsere Excel Kenntnisse erweitern.\nWhat's next for The Perfplace\nIn einem n\u00e4chsten Schritt m\u00fcsste Perfplace erstellt werden. Zudem m\u00fcsste im Detail gekl\u00e4rt werden welche Eigenschaften f\u00fcr Mieter von Relevanz sind, damit man diese entsprechend ins Tool aufnehmen kann.", "link": "https://devpost.com/software/the-perfplace", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nf\u00fcr das projekt haben wir uns in die rolle eines wohnungssuchenden versetzt. innerhalb der gruppe waren wir uns einig, dass die g\u00e4ngigen immobilien-plattformen zu wenig informationen dar\u00fcber liefern wie die wohngegend aussieht. deshalb m\u00f6chten wir mit unserem projekt eine m\u00f6glichkeit schaffen, mehr informationen dem nutzenden zur verf\u00fcgung zu stellen. die informationen sollen dabei nicht nur oberfl\u00e4chlich und allgemein g\u00fcltig sein sondern individuell auf die bed\u00fcrfnisse des nutzers angepasst werden.\nwhat it does\nziel der applikation ist es, dass nutzer f\u00fcr sie wichtige eigenschaften festlegen k\u00f6nnen (z.b. wenig l\u00e4rm, n\u00e4he zu schulen, einkaufsm\u00f6glichkeiten, naherholungsgebiete). basierend auf diesen eingaben wird ein score erstellt, der anschliessend auf der karte mittels heatmap visualisiert wo die kriterien am besten erf\u00fcllt werden. in einem weiteren schritt w\u00e4re die anbindung von immobilienplattformen (z.b. wie auf immobilien von comparis) gedacht, sodass die nutzer direkt verf\u00fcgbare mietobjekte in diesen bereichen angezeigt erhalten.\nhow we built it\nals datengrundlage haben wir vor allem daten vom open data portal der stadt st. gallen sowie von cividi herangezogen. die mockups wurden mit balsamiq erstellt und die karten mit den heatmaps innerhalb von excel (einf\u00fcgen > 3d-karten).\ndie app ist noch nicht erstellt da uns hier die programmiererfahrung fehlt.\nchallenges we ran into\naufgrund der fehlenden programmierkenntnisse konnten wir den vorschlag nicht umsetzen. zudem war es schwierig die gew\u00fcnschten funktionalit\u00e4ten ohne erhebung der bed\u00fcrfnisse zu definieren.\naccomplishments that we're proud of\nwir sind stolz darauf dass wir innerhalb dieses hackathons neue ideen erarbeitet haben und neue einblicke in das thema open data und m\u00f6gliche anwendungsf\u00e4lle erhalten haben.\nwir sind der meinung dass wir hier ein konzept erstellt haben, dass einen nutzen f\u00fcr die bev\u00f6lkerung bietet.\nwhat we learned\nwir haben einige datenbanken bei der suche nach relevanten datens\u00e4tzen untersucht und m\u00f6gliche anwendungsf\u00e4lle f\u00fcr open data kennengelernt. zudem konnten wir innerhalb des teams unsere excel kenntnisse erweitern.\nwhat's next for the perfplace\nin einem n\u00e4chsten schritt m\u00fcsste perfplace erstellt werden. zudem m\u00fcsste im detail gekl\u00e4rt werden welche eigenschaften f\u00fcr mieter von relevanz sind, damit man diese entsprechend ins -----> tool !!!  aufnehmen kann.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59504613}, {"Unnamed: 0": 4618, "autor": "Smart Mood", "date": null, "content": "Inspiration\nA cross between a mood-ring and self-care.\nWhat it does\nProvides instant feedback based on biometric (temperature) data to encourage emotional mindfulness and self-reflection.\nHow we built it\nThe hardware utilizes a temperature sensor and a LED light powered and programmed through an Arduino UNO. The wearable components are enclosed in a gold ring (previously an earring). The Smart Mood(c) is a supporting tool to the site. The online tool was created through bootstrap and hosted on GitHub to bring a quick and easy platform to all.\nChallenges we ran into\nUnfortunately, our heart-rate sensor was defective. This limited our data retrieval. Missing components, which led to taking long detours. The learning curve of a new language (C++) and of a new hardware (Arduino UNO) was a welcomed challenge.\nAccomplishments that we're proud of\nThe evolution of our idea showed us our ability to adapt despite issues out of our control. The final product is beyond what we had originally thought of.\nWhat we learned\nThe basics of a new language, new hardware and that it is always good to reward yourself with food and breaks.\nWhat's next for Smart Mood\nIntroduce additional biometric sensors such as a heartbeat sensor. An application version of the website to increase accessibility. The next prototype could be wireless.", "link": "https://devpost.com/software/scrunchie", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\na cross between a mood-ring and self-care.\nwhat it does\nprovides instant feedback based on biometric (temperature) data to encourage emotional mindfulness and self-reflection.\nhow we built it\nthe hardware utilizes a temperature sensor and a led light powered and programmed through an arduino uno. the wearable components are enclosed in a gold ring (previously an earring). the smart mood(c) is a supporting -----> tool !!!  to the site. the online tool was created through bootstrap and hosted on github to bring a quick and easy platform to all.\nchallenges we ran into\nunfortunately, our heart-rate sensor was defective. this limited our data retrieval. missing components, which led to taking long detours. the learning curve of a new language (c++) and of a new hardware (arduino uno) was a welcomed challenge.\naccomplishments that we're proud of\nthe evolution of our idea showed us our ability to adapt despite issues out of our control. the final product is beyond what we had originally thought of.\nwhat we learned\nthe basics of a new language, new hardware and that it is always good to reward yourself with food and breaks.\nwhat's next for smart mood\nintroduce additional biometric sensors such as a heartbeat sensor. an application version of the website to increase accessibility. the next prototype could be wireless.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 1, "media": null, "medialink": null, "identifyer": 59504618}, {"Unnamed: 0": 4672, "autor": "Vocaloop", "date": null, "content": "When we have ideas, dreams, or music in our heads, they usually go into a journal for safe keeping. But for musicians, the vocal recorder app on our phones is the only tool for a quick mobile recording solution. Because two of us in the group work with music regularly, the idea to re-invent this wheel came immediately. The technology of multi-track loop recording is already out there, but not in a convenient to use and simple UI on an app. After flushing out the features, menus and design, determining what coding language we needed to use in order to create this app was the second step. We ended up in a tug of war between the capabilities of Python, with the attractive UI/speed of Swift and iOS. The audio-exclusive language Faust was a close third, having a wide array of options for audio programming. Lackluster UI and poor interface with graphics were immediate downsides for using Python, while SwiftUI has trouble utilizing complex audio assets that we need for Vocaloop. This became one of our first challenges, while transferring all of the code into a mobile-ready application being the second. We learned not to assume a project is simple just because the components seem to be. (Don't judge a book by it's cover)", "link": "https://devpost.com/software/vocaloop", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "when we have ideas, dreams, or music in our heads, they usually go into a journal for safe keeping. but for musicians, the vocal recorder app on our phones is the only -----> tool !!!  for a quick mobile recording solution. because two of us in the group work with music regularly, the idea to re-invent this wheel came immediately. the technology of multi-track loop recording is already out there, but not in a convenient to use and simple ui on an app. after flushing out the features, menus and design, determining what coding language we needed to use in order to create this app was the second step. we ended up in a tug of war between the capabilities of python, with the attractive ui/speed of swift and ios. the audio-exclusive language faust was a close third, having a wide array of options for audio programming. lackluster ui and poor interface with graphics were immediate downsides for using python, while swiftui has trouble utilizing complex audio assets that we need for vocaloop. this became one of our first challenges, while transferring all of the code into a mobile-ready application being the second. we learned not to assume a project is simple just because the components seem to be. (don't judge a book by it's cover)", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59504672}, {"Unnamed: 0": 4709, "autor": "Guardian", "date": null, "content": "Inspiration\nWomen's safety is a growing global concern with women being victims of abuse and assault in all spheres of their day-to-day lives. Delhi in India tops all the crime charts against the country with a 41% share of the total crimes committed.Taking this into consideration we came up with the idea of our web app Guardian. With our app Guardian, we envision providing women a sense of security so that they can venture into any horizon in life and yet be connected to their safety network. Google Maps in our phone come in handy to show the fastest path. However, it might take us to some unsafe path. This is why we decided to take on this challenge to build a map that not only shows the fastest but also safest route. based on the previous crime records of the area.\nWhat it does\nFour main features of our web application are:\nSOS CALL An effective measure to help you connect with your trusted ones when you feel unsafe with effective use of Twilio API!\nSafe Routes Our maps tell you directions to your locations depending on how safe the route is with the effective implementation of the K-Means Clustering Algorithm!\nFake emergency caller The fake pre-recorded phone call will alert the attacker using Dasha AI API. This feature has been locally deployed.\nShare Live Location Share your real-time location with your trusted ones in a time of emergency using Twilio API\nHow we built it\nData Analysis and Interpretation- Python\nFrontend: HTML5, CSS(frameworks used:Tailwind,CSS),Javascript\nServer-side: Nodejs, Passport js\nDatabase: MongoDB( for user login)\nAPIs Used: Google Maps API, Twilio, DashaAI\nChallenges we ran into\nIntegration of API's- With all our team members working with the API's for the first time it was quite a challenge\nBuilding the dataset- With no available dataset, we had to restrict our web application and make it Delhi-centric.\nDASHA API- We deployed the Dasha feature locally due to issues integrating it with our Heroku website. But once it all worked out, it was very rewarding.\nAccomplishments that we're proud of\nWe as a team are proud to have been able to you use our knowledge for the benefit and safety of the public. Our app gives women the freedom and sense of safety they deserve. Easy to use and learn, this app is a handy tool for not just women but for the public in general if traveling alone or to unknown places. Guardian gives you the confidence of safety and security. Google Maps does show us the fastest optimized routes but not the option to travel by optimized safest routes. That's where our web application Guardian comes into place. It gives you the safest route, SOS alert, location sharing to contacts, and a pre-recorded call that will alert the attacker.\nWhat we learned\nImplementation of APIs\nDeployment of the website on Heroku and not just local deployment\nSolving various complexities of backend dev\nWhat's next for Guardian\nDue to time constraints, we are not able to fully explore the Dasha AI API but we will further work to add more features related to it like calling our trusted ones in time of emergency\nCurrently our safety route is only for Delhi considering its high crime rate, but we would like to expand our web app by incorporating the safety maps for other states as well.\nAnother functionality we\u2019d like to add is the live video streaming and voice recording to add more safety for the user\nDiscord ID's\nManvi Gupta#0317\ngurmehar#6243\nManan#7366", "link": "https://devpost.com/software/guardian-3nxr65", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwomen's safety is a growing global concern with women being victims of abuse and assault in all spheres of their day-to-day lives. delhi in india tops all the crime charts against the country with a 41% share of the total crimes committed.taking this into consideration we came up with the idea of our web app guardian. with our app guardian, we envision providing women a sense of security so that they can venture into any horizon in life and yet be connected to their safety network. google maps in our phone come in handy to show the fastest path. however, it might take us to some unsafe path. this is why we decided to take on this challenge to build a map that not only shows the fastest but also safest route. based on the previous crime records of the area.\nwhat it does\nfour main features of our web application are:\nsos call an effective measure to help you connect with your trusted ones when you feel unsafe with effective use of twilio api!\nsafe routes our maps tell you directions to your locations depending on how safe the route is with the effective implementation of the k-means clustering algorithm!\nfake emergency caller the fake pre-recorded phone call will alert the attacker using dasha ai api. this feature has been locally deployed.\nshare live location share your real-time location with your trusted ones in a time of emergency using twilio api\nhow we built it\ndata analysis and interpretation- python\nfrontend: html5, css(frameworks used:tailwind,css),javascript\nserver-side: nodejs, passport js\ndatabase: mongodb( for user login)\napis used: google maps api, twilio, dashaai\nchallenges we ran into\nintegration of api's- with all our team members working with the api's for the first time it was quite a challenge\nbuilding the dataset- with no available dataset, we had to restrict our web application and make it delhi-centric.\ndasha api- we deployed the dasha feature locally due to issues integrating it with our heroku website. but once it all worked out, it was very rewarding.\naccomplishments that we're proud of\nwe as a team are proud to have been able to you use our knowledge for the benefit and safety of the public. our app gives women the freedom and sense of safety they deserve. easy to use and learn, this app is a handy -----> tool !!!  for not just women but for the public in general if traveling alone or to unknown places. guardian gives you the confidence of safety and security. google maps does show us the fastest optimized routes but not the option to travel by optimized safest routes. that's where our web application guardian comes into place. it gives you the safest route, sos alert, location sharing to contacts, and a pre-recorded call that will alert the attacker.\nwhat we learned\nimplementation of apis\ndeployment of the website on heroku and not just local deployment\nsolving various complexities of backend dev\nwhat's next for guardian\ndue to time constraints, we are not able to fully explore the dasha ai api but we will further work to add more features related to it like calling our trusted ones in time of emergency\ncurrently our safety route is only for delhi considering its high crime rate, but we would like to expand our web app by incorporating the safety maps for other states as well.\nanother functionality we\u2019d like to add is the live video streaming and voice recording to add more safety for the user\ndiscord id's\nmanvi gupta#0317\ngurmehar#6243\nmanan#7366", "sortedWord": "None", "removed": "Nan", "score": 6, "comments": 0, "media": null, "medialink": null, "identifyer": 59504709}, {"Unnamed: 0": 4726, "autor": "Freshdesk Secret Message", "date": null, "content": "Inspiration\nSecurity is a big concern nowadays. Often customer support does not just help answer questions regarding the product. Sometimes, they need to deal with potential customers asking for a demo account, sharing critical logs for troubleshooting, or even just sharing some one-time password. How do you share them securely?\nTraditionally, they would share it within the Service Desk comments and leave it exposed to potential threats. Another workaround could be using services such as Tresorit or Box. What about making the solution integrate seamlessly with FreshDesk? This is the solution for it.\nWhat it does\nThe application allows easy sharing of sensitive or secret messages securely through FreshDesk ticket conversation.\nHow we built it\nFirstly, displaying and inputting information is done through FreshDesk's Core API and App SDK. There are 2 parts to it.\nCreating a secret message in the ticket view can be done using location ticket_conversation_editor and interface setValue trigger.\nListing page to display the collection of secrets created by who, when, and where. This listing page served as an auditing purpose to ensure traceability for a secure and compliance software.\nNext, we leverage existing encryption technology OpenPGP to secure the message transit and storage. We also leverage an open-source project (yopass.se ) to enable us to build on a cloud service that is cheap and easy to maintain.\nChallenges we ran into\nThere are a couple of challenges we ran into that are due to the constraint of the FreshDesk technology.\nTicket conversation editor does not support dynamically rendered components. - Our initial implementation is to unlock the secret within the conversation view. Due to the restriction, we opt for a workaround to open and view the secret through an external link.\nData Storage looks promising but lacks functionality such as retrieve all. - Due to the lack of a database system within the ecosystem, we have resorted to Restdb.io as a simple SaaS base workaround to store the encrypted secret that was created. This is needed to showcase the capability of auditing and traceability.\nSupport or documentation for multipage React component example/template/boilerplate. - The cli approach of creating a React app is a hit and miss. There is no clear documentation on how to use the data structure to the fullest. As an example, within the app, we use 2 locations. 1) ticket_conversation_editor, 2) full_page_app. There seems to have too much \"black magic\" behind the fdkConfig in the package.json that is not documented. Thus, fdk run only works for index.html and the other page has to be implemented using vanilla js.\nI believe with the continuous improvement of the technology, we could rework the app to make it seamless and have a lesser external dependency on the project. Most of the challenges faced during the implementation are finding workaround such as spike on yopass.se, restdb.io, and other similar tools before deciding on the tool to use.\nAccomplishments that we're proud of\nI work solo and started the implementation late. Once I have identified the problem and solution to solve the process is pretty straightforward. Needless to say, the documentation and tools are sufficient for me to complete a working product for both frontend and backend.\nWhat we learned\nFreshdesk documentation is easy to read and understand. This helps us to understand the FreshDesk product and the value proposition it provides to the customer.\nAsymmetric encryption is a new thing to me. By working on this idea, I study about encryption and came across other tools and technology related to it. As an example, PGP.\nWhat's next for Freshdesk Secret Message\nThere is some minor improvement that could be done in the data structure design used for restdb.io. For example, separation of concern for multi-tenant implementation, which allows the user to have to store it in their own DB.\nImplement permission control to the listing page.\nBesides the secret message, we could also expand this into implementing attachments so that documentation can be shared in a secure manner.\nThat aside, the workaround hack works well and is also good for production use. Of course, if the technology improves, we could revise on making an improved version of this app. Eg. dynamically rendered secret component within the conversation thread. And, porting the database to using Freshdesk's database.\nAlso, nice to have is to allow to the addition of a plugin on the customer's side conversation editor view. So that they too can add secret messages and provide more information that is contained within the system.", "link": "https://devpost.com/software/freshdesk-secret-message", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nsecurity is a big concern nowadays. often customer support does not just help answer questions regarding the product. sometimes, they need to deal with potential customers asking for a demo account, sharing critical logs for troubleshooting, or even just sharing some one-time password. how do you share them securely?\ntraditionally, they would share it within the service desk comments and leave it exposed to potential threats. another workaround could be using services such as tresorit or box. what about making the solution integrate seamlessly with freshdesk? this is the solution for it.\nwhat it does\nthe application allows easy sharing of sensitive or secret messages securely through freshdesk ticket conversation.\nhow we built it\nfirstly, displaying and inputting information is done through freshdesk's core api and app sdk. there are 2 parts to it.\ncreating a secret message in the ticket view can be done using location ticket_conversation_editor and interface setvalue trigger.\nlisting page to display the collection of secrets created by who, when, and where. this listing page served as an auditing purpose to ensure traceability for a secure and compliance software.\nnext, we leverage existing encryption technology openpgp to secure the message transit and storage. we also leverage an open-source project (yopass.se ) to enable us to build on a cloud service that is cheap and easy to maintain.\nchallenges we ran into\nthere are a couple of challenges we ran into that are due to the constraint of the freshdesk technology.\nticket conversation editor does not support dynamically rendered components. - our initial implementation is to unlock the secret within the conversation view. due to the restriction, we opt for a workaround to open and view the secret through an external link.\ndata storage looks promising but lacks functionality such as retrieve all. - due to the lack of a database system within the ecosystem, we have resorted to restdb.io as a simple saas base workaround to store the encrypted secret that was created. this is needed to showcase the capability of auditing and traceability.\nsupport or documentation for multipage react component example/template/boilerplate. - the cli approach of creating a react app is a hit and miss. there is no clear documentation on how to use the data structure to the fullest. as an example, within the app, we use 2 locations. 1) ticket_conversation_editor, 2) full_page_app. there seems to have too much \"black magic\" behind the fdkconfig in the package.json that is not documented. thus, fdk run only works for index.html and the other page has to be implemented using vanilla js.\ni believe with the continuous improvement of the technology, we could rework the app to make it seamless and have a lesser external dependency on the project. most of the challenges faced during the implementation are finding workaround such as spike on yopass.se, restdb.io, and other similar tools before deciding on the -----> tool !!!  to use.\naccomplishments that we're proud of\ni work solo and started the implementation late. once i have identified the problem and solution to solve the process is pretty straightforward. needless to say, the documentation and tools are sufficient for me to complete a working product for both frontend and backend.\nwhat we learned\nfreshdesk documentation is easy to read and understand. this helps us to understand the freshdesk product and the value proposition it provides to the customer.\nasymmetric encryption is a new thing to me. by working on this idea, i study about encryption and came across other tools and technology related to it. as an example, pgp.\nwhat's next for freshdesk secret message\nthere is some minor improvement that could be done in the data structure design used for restdb.io. for example, separation of concern for multi-tenant implementation, which allows the user to have to store it in their own db.\nimplement permission control to the listing page.\nbesides the secret message, we could also expand this into implementing attachments so that documentation can be shared in a secure manner.\nthat aside, the workaround hack works well and is also good for production use. of course, if the technology improves, we could revise on making an improved version of this app. eg. dynamically rendered secret component within the conversation thread. and, porting the database to using freshdesk's database.\nalso, nice to have is to allow to the addition of a plugin on the customer's side conversation editor view. so that they too can add secret messages and provide more information that is contained within the system.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59504726}, {"Unnamed: 0": 4731, "autor": "Planet Patterns", "date": null, "content": "Inspiration\nWe wanted to create a platform that would serve to be helpful to people, scientists, and educators alike. When it comes to space, planets, and data, we knew that human beings as a whole would be appreciative and can make use of near-real-time natural weather event reporting.\nWhat it does\nPlanet Patterns displays natural event data from NASA's satellites in the form of interactive visualizations.\nHow we built it\nWe built the core of Planet Patterns in ReactJS, using React Globe GL as the foundation on which we plotted data points. For each of the weather event categories, we queried NASA's EONET API.\nChallenges we ran into\nWeb development is a weakness for all of us. We wanted to partake in learning skills we were mostly unfamiliar with. For these reasons, debugging simple errors and representing the data in many ways visually cost us many hours.\nAccomplishments that we're proud of\nWe're incredibly excited to say that we completed a project using ReactJS in a time-constrained event.\nWhat's next for Planet Patterns\nSeeing as how weather is not exclusive to planet Earth, we would love to parallel our work for the rest of the planets in the Solar System. Since Planet Patterns is such a great documentation tool, it would make sense to include events like Jupiter's Great Red Spot and Saturn's diamond rain.", "link": "https://devpost.com/software/natural-events", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe wanted to create a platform that would serve to be helpful to people, scientists, and educators alike. when it comes to space, planets, and data, we knew that human beings as a whole would be appreciative and can make use of near-real-time natural weather event reporting.\nwhat it does\nplanet patterns displays natural event data from nasa's satellites in the form of interactive visualizations.\nhow we built it\nwe built the core of planet patterns in reactjs, using react globe gl as the foundation on which we plotted data points. for each of the weather event categories, we queried nasa's eonet api.\nchallenges we ran into\nweb development is a weakness for all of us. we wanted to partake in learning skills we were mostly unfamiliar with. for these reasons, debugging simple errors and representing the data in many ways visually cost us many hours.\naccomplishments that we're proud of\nwe're incredibly excited to say that we completed a project using reactjs in a time-constrained event.\nwhat's next for planet patterns\nseeing as how weather is not exclusive to planet earth, we would love to parallel our work for the rest of the planets in the solar system. since planet patterns is such a great documentation -----> tool !!! , it would make sense to include events like jupiter's great red spot and saturn's diamond rain.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59504731}, {"Unnamed: 0": 4746, "autor": "TL;DS", "date": null, "content": "Inspiration\nOur school has a 3D printer with a broken extruder. Since global electronic waste is a big issue in our society (53.6 million metric tons of e-waste were produced in 2019), we wanted to set an example to encourage others to give a new purpose to old electronics. We also hoped to inspire the next generation of computer scientists by making them excited about programming.\nWhat it does\nTL;DS is an automatic math homework solver. It can take a photo of a math problem, solve the math problem and hand-write the answer using the 3D printer. TL;DS also has an experimental feature that allows it to draw graphs on paper.\nHow we built it\nTL;DS is built with a variety of different tools. The web application, where images can be captured and uploaded, is built with Flask. After the image is uploaded, TL;DS uses optical character recognition from Tesseract to read text off the image, and a custom library along with a tool called GPX to write the instruction file for the 3D printer.\nChallenges we ran into\nThere were several major issues we encountered while building TL;DS. The OCR sometimes did not detect the text properly; we remedied this by preprocessing images with OpenCV. As this project involves hardware, we encountered some physical problems; for example, the pen was often unstable and either had too much or too little pressure. To fix this, we used a pen spring to provide the correct amount of pressure. Additionally, the Makerbot printer uses a binary file format called X3G, which requires the usage of a gcode-to-x3g converter.\nAccomplishments that we're proud of\nBuilding a functional and consistent handwriting machine in a limited time span\nMostly reliable OCR and mathematical equation solving\nEasy-to-use web application to facilitate image capture and transfer to computer for processing\nWhat we learned\nWhile we already had experience using 3D printers, writing low-level instructions to control them (as opposed to dragging a model into Slic3r) gave us a lot of insight on how 3D printers work. We also learned how important it is to continuously iterate on a design; both our OCR and especially the pen holder went through many iterations to try to achieve an optimal result.\nWhat's next for TL;DS\nAbility to solve and write multiple equations from a single image (currently only supports one equation at a time)\nIntegration of Wolfram|Alpha, and usage of MathPix OCR instead of Tesseract, to enable recognition and solving of more complex mathematical problems\nEnd-to-end automated workflow - currently, after the photo is sent to the computer, the application writes the file onto an SD card that must be manually transferred to the printer. In the future, the file could be directly transferred using a cable.\n3D-printed pen cartridge housing, for even better stability and control of handwriting", "link": "https://devpost.com/software/homework-solver", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nour school has a 3d printer with a broken extruder. since global electronic waste is a big issue in our society (53.6 million metric tons of e-waste were produced in 2019), we wanted to set an example to encourage others to give a new purpose to old electronics. we also hoped to inspire the next generation of computer scientists by making them excited about programming.\nwhat it does\ntl;ds is an automatic math homework solver. it can take a photo of a math problem, solve the math problem and hand-write the answer using the 3d printer. tl;ds also has an experimental feature that allows it to draw graphs on paper.\nhow we built it\ntl;ds is built with a variety of different tools. the web application, where images can be captured and uploaded, is built with flask. after the image is uploaded, tl;ds uses optical character recognition from tesseract to read text off the image, and a custom library along with a -----> tool !!!  called gpx to write the instruction file for the 3d printer.\nchallenges we ran into\nthere were several major issues we encountered while building tl;ds. the ocr sometimes did not detect the text properly; we remedied this by preprocessing images with opencv. as this project involves hardware, we encountered some physical problems; for example, the pen was often unstable and either had too much or too little pressure. to fix this, we used a pen spring to provide the correct amount of pressure. additionally, the makerbot printer uses a binary file format called x3g, which requires the usage of a gcode-to-x3g converter.\naccomplishments that we're proud of\nbuilding a functional and consistent handwriting machine in a limited time span\nmostly reliable ocr and mathematical equation solving\neasy-to-use web application to facilitate image capture and transfer to computer for processing\nwhat we learned\nwhile we already had experience using 3d printers, writing low-level instructions to control them (as opposed to dragging a model into slic3r) gave us a lot of insight on how 3d printers work. we also learned how important it is to continuously iterate on a design; both our ocr and especially the pen holder went through many iterations to try to achieve an optimal result.\nwhat's next for tl;ds\nability to solve and write multiple equations from a single image (currently only supports one equation at a time)\nintegration of wolfram|alpha, and usage of mathpix ocr instead of tesseract, to enable recognition and solving of more complex mathematical problems\nend-to-end automated workflow - currently, after the photo is sent to the computer, the application writes the file onto an sd card that must be manually transferred to the printer. in the future, the file could be directly transferred using a cable.\n3d-printed pen cartridge housing, for even better stability and control of handwriting", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 1, "media": null, "medialink": null, "identifyer": 59504746}, {"Unnamed: 0": 4757, "autor": "Star Stream", "date": null, "content": "Star Stream\n\ud83d\udca1 Inspiration\nFor thousands of years, people from every part of the world have been studying the stars. Ancient Egyptians and Babylonians to notable astronomers like Galileo and Carl Sagan have looked to the stars to try and understand our place in the universe and how it all works. The hundreds of objects that have been seen in the night sky since ancient times, like planets, stars, and galaxies, have given people the foundation to research and learn. Today's Astronomers would not know as much as they do without the study of these objects. But with light pollution increasing at a rate twice as fast as the world's population, dimmer objects are vanishing from our view and studying astronomy, as an amateur or professional, is getting harder.\nHence, we wished to create an application where anyone can see all the shining stars that surround them, without these barriers that exist today! Stargazing, right from the comfort of your home!\n\ud83d\udcbb What it does\nIt's a 3D environment where a user can see all the stars revolving around the earth in real-time. It also gives out more information about stars like their name, distance from earth, star's ID in the Henry Draper, star's ID in the Harvard Revised.\n\ud83d\udd28 How we built it\nWe built the frontend using react.js and three.js. The backend API is created by using Google cloud services like cloud function, cloud storage and API-gateway. We are using the stars data from astronexus.com/hyg\nStarStream API\nThis API provides data from astronexus.com/hyg\nAPI endpoint: https://starstream-api-gateway-b2dzz7qh.uc.gateway.dev/min-data\nthis data is stored in Google Cloud Storage\nthen cloud function reads that data and apply transformation on the data\nCloud function's open API endpoint is then passed to GCP API Gateway tool to secure that connection and to easily update the API endpoints by using config files\nAPI parameters\nFollowing parameters are available for the API\ncount -> Integer\nNumber of star's data returned\nsort_by -> String\nSort the data by any specific field\nthe current available fields are -['hd', 'hr', 'proper', 'dist', 'mag', 'absmag', 'x', 'y', 'z']\nascending -> Boolean\nSet if the sort_by should be ascending or descending\n1 for ascending and 0 for descending\ncsv -> Boolean\nChange the output format from json to csv\nExample\n[gateway]/min-data?count=10&sort_by=id&ascending=0&csv=1\nThis would return first 10 stars sorted by id in descending order and in csv format\nFrontEnd\nWe then use three.js to show the stars in a first person view with the data from the API\n\u26c5 Use of Google Cloud\nWe built Star Stream's backend API with Google Cloud's cloud function and which takes the csv file data from cloud storage and then the function's open endpoint to API-gateway to easily update by using yaml config files. We chose this because we wanted to make an application that was very very secure.\nCloud Storage to store the csv data\nCloud function to convert that csv data to json and to transform the data on the basis of parameters passed to the API gateway\nAPI Gateway to secure and monitor our API gateways\n\ud83e\udde0 Challenges we ran into\nThe biggest challenge was to render the 3D model as none of our team member has worked with three js before. But we tacked this problem by going through tutorials and documentations and by taking help and advices from the mentors.\n\ud83c\udfc5 Accomplishments that we're proud of\nCreating and publishing a live interactive site using three.js and creating a working API using Google Cloud Platform and also completing the project under the given time frame.\n\ud83d\udcd6 What we learned\nWorking with three js and using google cloud services like Cloud storage, Cloud functions and API gateway.\n\ud83d\ude80 What's next for Star Stream\nAdd Virtual Reality (VR) functionality so users could completely submerse into the experience of eStarGazing\nAdd Augmented Reality(AR) functionality so when users click the stars they get an option to view that star's 3d model in AR\nAdd more subjects and Customized 3D model so more stars could support AR\nLive location star gazing, by using the user's location and doing some Maths to check which stars would be visible from that region and then showing the stars\nImprove that live location feature and allow users to change their location from the app and see stars from various famous places without going through the hassle of travelling\nAdd premium subscription with more features\nAnd a lot of other features!", "link": "https://devpost.com/software/star-stream", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "star stream\n\ud83d\udca1 inspiration\nfor thousands of years, people from every part of the world have been studying the stars. ancient egyptians and babylonians to notable astronomers like galileo and carl sagan have looked to the stars to try and understand our place in the universe and how it all works. the hundreds of objects that have been seen in the night sky since ancient times, like planets, stars, and galaxies, have given people the foundation to research and learn. today's astronomers would not know as much as they do without the study of these objects. but with light pollution increasing at a rate twice as fast as the world's population, dimmer objects are vanishing from our view and studying astronomy, as an amateur or professional, is getting harder.\nhence, we wished to create an application where anyone can see all the shining stars that surround them, without these barriers that exist today! stargazing, right from the comfort of your home!\n\ud83d\udcbb what it does\nit's a 3d environment where a user can see all the stars revolving around the earth in real-time. it also gives out more information about stars like their name, distance from earth, star's id in the henry draper, star's id in the harvard revised.\n\ud83d\udd28 how we built it\nwe built the frontend using react.js and three.js. the backend api is created by using google cloud services like cloud function, cloud storage and api-gateway. we are using the stars data from astronexus.com/hyg\nstarstream api\nthis api provides data from astronexus.com/hyg\napi endpoint: https://starstream-api-gateway-b2dzz7qh.uc.gateway.dev/min-data\nthis data is stored in google cloud storage\nthen cloud function reads that data and apply transformation on the data\ncloud function's open api endpoint is then passed to gcp api gateway -----> tool !!!  to secure that connection and to easily update the api endpoints by using config files\napi parameters\nfollowing parameters are available for the api\ncount -> integer\nnumber of star's data returned\nsort_by -> string\nsort the data by any specific field\nthe current available fields are -['hd', 'hr', 'proper', 'dist', 'mag', 'absmag', 'x', 'y', 'z']\nascending -> boolean\nset if the sort_by should be ascending or descending\n1 for ascending and 0 for descending\ncsv -> boolean\nchange the output format from json to csv\nexample\n[gateway]/min-data?count=10&sort_by=id&ascending=0&csv=1\nthis would return first 10 stars sorted by id in descending order and in csv format\nfrontend\nwe then use three.js to show the stars in a first person view with the data from the api\n\u26c5 use of google cloud\nwe built star stream's backend api with google cloud's cloud function and which takes the csv file data from cloud storage and then the function's open endpoint to api-gateway to easily update by using yaml config files. we chose this because we wanted to make an application that was very very secure.\ncloud storage to store the csv data\ncloud function to convert that csv data to json and to transform the data on the basis of parameters passed to the api gateway\napi gateway to secure and monitor our api gateways\n\ud83e\udde0 challenges we ran into\nthe biggest challenge was to render the 3d model as none of our team member has worked with three js before. but we tacked this problem by going through tutorials and documentations and by taking help and advices from the mentors.\n\ud83c\udfc5 accomplishments that we're proud of\ncreating and publishing a live interactive site using three.js and creating a working api using google cloud platform and also completing the project under the given time frame.\n\ud83d\udcd6 what we learned\nworking with three js and using google cloud services like cloud storage, cloud functions and api gateway.\n\ud83d\ude80 what's next for star stream\nadd virtual reality (vr) functionality so users could completely submerse into the experience of estargazing\nadd augmented reality(ar) functionality so when users click the stars they get an option to view that star's 3d model in ar\nadd more subjects and customized 3d model so more stars could support ar\nlive location star gazing, by using the user's location and doing some maths to check which stars would be visible from that region and then showing the stars\nimprove that live location feature and allow users to change their location from the app and see stars from various famous places without going through the hassle of travelling\nadd premium subscription with more features\nand a lot of other features!", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59504757}, {"Unnamed: 0": 4774, "autor": "Kandelaber Lader", "date": null, "content": "Inspiration\nDie Elektromobilit\u00e4t in der Stadt St. Gallen boomt. Stand Oktober 2021 sind \u00fcber 1000 Elektrofahrzeuge immatrikuliert. Das Energiekonzept der Stadt sieht bis 2050 nur noch elektrisch angetriebenen Fahrzeuge in der Stadt vor. Demgegen\u00fcber stehen bisher 65 \u00f6ffentliche Ladestationen. Dieser Ausbau wird in den n\u00e4chsten Jahren vorangetrieben und ist kostenintensiv. Weiter betreibt die Stadt 35 Photovoltaikanlagen mit einer Peakleistung von 7700kW. Diese liefern die Leistung abh\u00e4ngig der Sonnenscheindauer und k\u00f6nnten an sonnigen Tagen g\u00fcnstig Strom abgegeben.\nWhat it does\nDie bestehenden Kandelaber, welche in der N\u00e4he von \u00f6ffentlichen Parkpl\u00e4tzen stehen, sollen als \u00f6ffentliche Ladepunkte ausgebaut werden. Auch andere Schweizer St\u00e4dte m\u00f6chten Strassenlaternen / Kandelaber in Zukunft als Ladestationen verwenden, so zum Beispiel die Stadt Bern.\nUnser Case ermittelt geeignete Standorte anhand Parkplatz-, Beleuchtungsanlagen- und Trottoir-Daten. Weiter stehen von Fr\u00fchling bis Herbst Leistungs\u00fcbersch\u00fcsse der Photovoltaik-Anlagen an. Diese Energie soll effizient lokal genutzt werden k\u00f6nnen. E-Autos k\u00f6nnten in einem sp\u00e4teren Schritt als Ausgleichsspeicher dienen. Unser Tool soll dem umweltbewussten E-Auto Fahrer mitteilen, mit welchem Photovoltaik Anteil er in den n\u00e4chsten Tagen am optimalsten sein Fahrzeug an einem Kandelaber-Lader l\u00e4dt.\nHow we built it\nUnsere L\u00f6sung besteht aus drei separaten Komponenten:\nPotentielle Ladestationen suchen\nDamit ein Parkplatz als Ladestation in Frage kommt, muss in zwei Metern um den Parkplatz herum ein Kandelaber stehen, der mit einem Ladeanschluss versehen werden kann. Um diese zu finden, haben wir die Datensets Parkpl\u00e4tze und Parkh\u00e4user, \u00d6ffentliche Beleuchtung und den Gemeindestrassenplan der Stadt St. Gallen verwendet. Die Daten haben wir in einem Jupyter Notebook mit Pandas und Geopandas weiterverarbeitet:\nDie Parkpl\u00e4tze sind als einzelne Punkte im Datensatz abgebildet. Mit diesen Punkten suchen wir den Strassenabschnitt, in dem sich die Parkpl\u00e4tze befinden. Den Punkt des Parkplatzes projezieren wir auf die Kontur der Trasse. So wissen wir an welcher Strassenseite sich die Parkfelder befinden und k\u00f6nnen entlang dem Strassenverlauf nach Kandelabern suchen. Alle Parkfelder, die in zwei Meter N\u00e4he eines Kandelabers stehen, exportieren wir als GeoJSON. So haben wir eine Liste der potentiell als Ladestation verwendbaren Parkpl\u00e4tze.\nVoraussage Photovoltaik Ladeanteil\nDie E-Autos sollen wenn m\u00f6glich mit gr\u00fcnem Strom / Photovoltaik-Strom geladen werden. Damit die Autofahrer m\u00f6glichst vom Photovoltaik-Strom der Stadt profitieren k\u00f6nnen, m\u00f6chten wir ihnen anzeigen, wann voraussichtlich viel Photovoltaik-Strom verf\u00fcgbar sein wird.\nF\u00fcr die Prognose verwenden wir folgende Datens\u00e4tze:\nSonnenschein von Meteoschweiz\nLufttemperatur von Meteoschweiz\nRegenwahrscheinlichkeit von Meteoschweiz\nStromproduktion der Solaranlagen der St. Galler Kraftwerke\nMit den historischen Wetter- und Stromproduktionsdaten trainierten wir in einem Jupyter Notebook mit Pandas und sklearn ein Lineares Regressionsmodell. Damit k\u00f6nnen wir auf einem Testdatensatz zu ca. 83% genau voraussagen, wie viel Photovoltaik-Strom pro Stunde aufgrund der Wetterlage produziert werden wird. Die vorausgesagten Produktionsdaten werden schlussendlich wieder exportiert, damit sie im Frontend angezeigt werden k\u00f6nnen.\nWeb Frontend\nDie E-Autofahrer sollen auf einer Webseite sehen, wo es in der N\u00e4he ihres Zielortes einen Kandelaber-Lader hat. Daf\u00fcr haben wir eine Webseite mit Angular erstellt, die dem Benutzer auf einer Karte die Kandelaber-Lader der Stadt St. Gallen anzeigt. Als Datengrundlage verwenden wir die in den vorherigen Schritten aufbereiteten Daten. Die Karte ist mit Leaflet.js umgesetzt, damit werden auch die Positionen der Ladestationen dargestellt. Die Voraussage des Photovoltaik-Ladeanteils wird ebenfalls auf der Webseite dargestellt, umgesetzt mit Highchart.\nChallenges we ran into\nDie Parkpl\u00e4tze werden von der Stadt St. Gallen als geographische Punkte bereitgestellt. Parkfl\u00e4chen f\u00fcr mehrere Autos (z.B. eine blaue Zone f\u00fcr 4 Autos) sind ebenfalls nur als einzelner Punkt vorhanden. Um m\u00f6gliche Ladestationen zu finden, ben\u00f6tigen wir die genaue Fl\u00e4che der einzelnen Parkfelder, damit wir den Abstand zu Kandelabern m\u00f6glichst genau berechnen k\u00f6nnen. Unsere erste Idee war, mittels Bilderkennung die genauen Parkplatzfl\u00e4chen in den Swisstopo Bilddaten nach den Parkfeldern zu suchen. Wegen fehlender vortrainierten Modellen oder Trainingsdaten f\u00fcr diesen Anwendungsfall haben wir diesen Ansatz aber wieder verworfen. Um das Problem schlussendlich zu l\u00f6sen, haben wir einige Parkfelder der Stadt gemessen. Aufgrund der Parkplatzgr\u00f6sse konnten wir den Strassenabschnitt / die Strassenseite eingrenzen, den wir nach m\u00f6glichen Kandelabern absuchen. Um die Abst\u00e4nde korrekt zu messen br\u00e4uchten wir aber die genauen Positionen / Fl\u00e4chen der einzelnen Parkfelder.\nAccomplishments that we're proud of\nMit unserem Projekt konnten wir aufzeigen, dass es auch in der Stadt St. Gallen grosses Potential gibt, die Strassenlaternen als Ladestationen zu verwenden. Zusammen mit der Webanwendung f\u00fcr die E-Autofahrer k\u00f6nnen wir so vielleicht einen kleinen Beitrag f\u00fcr das Klimaziel der Stadt St. Gallen beisteuern!\nWhat we learned\n\u00d6ffentliche Daten lohnen sich! H\u00e4tte die Stadt all die Daten die wir verwendeten nicht bereitgestellt, w\u00e4re es gar nicht m\u00f6glich, Konzepte wie die Kandelaber-Lader zu erarbeiten.\nWhat's next for Kandelabergelaber\nUm effektiv geeignete Standorte f\u00fcr Kandelaber-Lader zu finden, braucht es noch genauere Informationen zu den Parkpl\u00e4tzen in St. Gallen. Unsere aktuelle L\u00f6sung errechnet den ungef\u00e4hren Standort der einzelnen Parkfelder. Mit der exakten Position der Parkfelder k\u00f6nnte genau ermittelt werden, welche Parkfelder tats\u00e4chlich in Frage kommen. Mit weiteren Daten (z.B. wo sind stark besuchte POIs oder allgemein viel Verkehr) k\u00f6nnte zus\u00e4tzlich priorisiert werden, welche Parkfelder als erstes aufger\u00fcstet werden sollen.\nDes weiteren braucht es auch seitens Frontend noch weitere Funktionen, damit die Benutzer z.B. direkt nach einem Standort suchen k\u00f6nnten, auch mit der Information, ob der Parkplatz noch frei ist (z.B. mit dem Datensatz der freien Parkpl\u00e4tze).\nIn Zukunft k\u00f6nnten die Elektroautos an diesen Ladestationen auch einen weiteren Beitrag leisten: Mit ihren grossen Akkus sind die Elektroautos auch ein Energiespeicher, der dem Stromnetz wieder zur Verf\u00fcgung steht.", "link": "https://devpost.com/software/kandelabergelaber", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ndie elektromobilit\u00e4t in der stadt st. gallen boomt. stand oktober 2021 sind \u00fcber 1000 elektrofahrzeuge immatrikuliert. das energiekonzept der stadt sieht bis 2050 nur noch elektrisch angetriebenen fahrzeuge in der stadt vor. demgegen\u00fcber stehen bisher 65 \u00f6ffentliche ladestationen. dieser ausbau wird in den n\u00e4chsten jahren vorangetrieben und ist kostenintensiv. weiter betreibt die stadt 35 photovoltaikanlagen mit einer peakleistung von 7700kw. diese liefern die leistung abh\u00e4ngig der sonnenscheindauer und k\u00f6nnten an sonnigen tagen g\u00fcnstig strom abgegeben.\nwhat it does\ndie bestehenden kandelaber, welche in der n\u00e4he von \u00f6ffentlichen parkpl\u00e4tzen stehen, sollen als \u00f6ffentliche ladepunkte ausgebaut werden. auch andere schweizer st\u00e4dte m\u00f6chten strassenlaternen / kandelaber in zukunft als ladestationen verwenden, so zum beispiel die stadt bern.\nunser case ermittelt geeignete standorte anhand parkplatz-, beleuchtungsanlagen- und trottoir-daten. weiter stehen von fr\u00fchling bis herbst leistungs\u00fcbersch\u00fcsse der photovoltaik-anlagen an. diese energie soll effizient lokal genutzt werden k\u00f6nnen. e-autos k\u00f6nnten in einem sp\u00e4teren schritt als ausgleichsspeicher dienen. unser -----> tool !!!  soll dem umweltbewussten e-auto fahrer mitteilen, mit welchem photovoltaik anteil er in den n\u00e4chsten tagen am optimalsten sein fahrzeug an einem kandelaber-lader l\u00e4dt.\nhow we built it\nunsere l\u00f6sung besteht aus drei separaten komponenten:\npotentielle ladestationen suchen\ndamit ein parkplatz als ladestation in frage kommt, muss in zwei metern um den parkplatz herum ein kandelaber stehen, der mit einem ladeanschluss versehen werden kann. um diese zu finden, haben wir die datensets parkpl\u00e4tze und parkh\u00e4user, \u00f6ffentliche beleuchtung und den gemeindestrassenplan der stadt st. gallen verwendet. die daten haben wir in einem jupyter notebook mit pandas und geopandas weiterverarbeitet:\ndie parkpl\u00e4tze sind als einzelne punkte im datensatz abgebildet. mit diesen punkten suchen wir den strassenabschnitt, in dem sich die parkpl\u00e4tze befinden. den punkt des parkplatzes projezieren wir auf die kontur der trasse. so wissen wir an welcher strassenseite sich die parkfelder befinden und k\u00f6nnen entlang dem strassenverlauf nach kandelabern suchen. alle parkfelder, die in zwei meter n\u00e4he eines kandelabers stehen, exportieren wir als geojson. so haben wir eine liste der potentiell als ladestation verwendbaren parkpl\u00e4tze.\nvoraussage photovoltaik ladeanteil\ndie e-autos sollen wenn m\u00f6glich mit gr\u00fcnem strom / photovoltaik-strom geladen werden. damit die autofahrer m\u00f6glichst vom photovoltaik-strom der stadt profitieren k\u00f6nnen, m\u00f6chten wir ihnen anzeigen, wann voraussichtlich viel photovoltaik-strom verf\u00fcgbar sein wird.\nf\u00fcr die prognose verwenden wir folgende datens\u00e4tze:\nsonnenschein von meteoschweiz\nlufttemperatur von meteoschweiz\nregenwahrscheinlichkeit von meteoschweiz\nstromproduktion der solaranlagen der st. galler kraftwerke\nmit den historischen wetter- und stromproduktionsdaten trainierten wir in einem jupyter notebook mit pandas und sklearn ein lineares regressionsmodell. damit k\u00f6nnen wir auf einem testdatensatz zu ca. 83% genau voraussagen, wie viel photovoltaik-strom pro stunde aufgrund der wetterlage produziert werden wird. die vorausgesagten produktionsdaten werden schlussendlich wieder exportiert, damit sie im frontend angezeigt werden k\u00f6nnen.\nweb frontend\ndie e-autofahrer sollen auf einer webseite sehen, wo es in der n\u00e4he ihres zielortes einen kandelaber-lader hat. daf\u00fcr haben wir eine webseite mit angular erstellt, die dem benutzer auf einer karte die kandelaber-lader der stadt st. gallen anzeigt. als datengrundlage verwenden wir die in den vorherigen schritten aufbereiteten daten. die karte ist mit leaflet.js umgesetzt, damit werden auch die positionen der ladestationen dargestellt. die voraussage des photovoltaik-ladeanteils wird ebenfalls auf der webseite dargestellt, umgesetzt mit highchart.\nchallenges we ran into\ndie parkpl\u00e4tze werden von der stadt st. gallen als geographische punkte bereitgestellt. parkfl\u00e4chen f\u00fcr mehrere autos (z.b. eine blaue zone f\u00fcr 4 autos) sind ebenfalls nur als einzelner punkt vorhanden. um m\u00f6gliche ladestationen zu finden, ben\u00f6tigen wir die genaue fl\u00e4che der einzelnen parkfelder, damit wir den abstand zu kandelabern m\u00f6glichst genau berechnen k\u00f6nnen. unsere erste idee war, mittels bilderkennung die genauen parkplatzfl\u00e4chen in den swisstopo bilddaten nach den parkfeldern zu suchen. wegen fehlender vortrainierten modellen oder trainingsdaten f\u00fcr diesen anwendungsfall haben wir diesen ansatz aber wieder verworfen. um das problem schlussendlich zu l\u00f6sen, haben wir einige parkfelder der stadt gemessen. aufgrund der parkplatzgr\u00f6sse konnten wir den strassenabschnitt / die strassenseite eingrenzen, den wir nach m\u00f6glichen kandelabern absuchen. um die abst\u00e4nde korrekt zu messen br\u00e4uchten wir aber die genauen positionen / fl\u00e4chen der einzelnen parkfelder.\naccomplishments that we're proud of\nmit unserem projekt konnten wir aufzeigen, dass es auch in der stadt st. gallen grosses potential gibt, die strassenlaternen als ladestationen zu verwenden. zusammen mit der webanwendung f\u00fcr die e-autofahrer k\u00f6nnen wir so vielleicht einen kleinen beitrag f\u00fcr das klimaziel der stadt st. gallen beisteuern!\nwhat we learned\n\u00f6ffentliche daten lohnen sich! h\u00e4tte die stadt all die daten die wir verwendeten nicht bereitgestellt, w\u00e4re es gar nicht m\u00f6glich, konzepte wie die kandelaber-lader zu erarbeiten.\nwhat's next for kandelabergelaber\num effektiv geeignete standorte f\u00fcr kandelaber-lader zu finden, braucht es noch genauere informationen zu den parkpl\u00e4tzen in st. gallen. unsere aktuelle l\u00f6sung errechnet den ungef\u00e4hren standort der einzelnen parkfelder. mit der exakten position der parkfelder k\u00f6nnte genau ermittelt werden, welche parkfelder tats\u00e4chlich in frage kommen. mit weiteren daten (z.b. wo sind stark besuchte pois oder allgemein viel verkehr) k\u00f6nnte zus\u00e4tzlich priorisiert werden, welche parkfelder als erstes aufger\u00fcstet werden sollen.\ndes weiteren braucht es auch seitens frontend noch weitere funktionen, damit die benutzer z.b. direkt nach einem standort suchen k\u00f6nnten, auch mit der information, ob der parkplatz noch frei ist (z.b. mit dem datensatz der freien parkpl\u00e4tze).\nin zukunft k\u00f6nnten die elektroautos an diesen ladestationen auch einen weiteren beitrag leisten: mit ihren grossen akkus sind die elektroautos auch ein energiespeicher, der dem stromnetz wieder zur verf\u00fcgung steht.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59504774}, {"Unnamed: 0": 4824, "autor": "Youtube Sentimeter", "date": null, "content": "Inspiration\nYoutube is one of the largest video sharing sites in the world. A study by forbes.com shows reveals that there are currently 50 million content creators spread across Youtube, Instagram, TikTok and other social media. In this figure, Youtube has 12 Million amateur and 1 Million professional (full time) content creators illustrated by the image below:\nFurthermore, youtube statistical data provided by review42 suggests that:\nThere are over 1.9 Billion active users each month visiting Youtube\nContent Creators upload more than 500 hours of content every minute\nUser watch over a billion hours of content daily.\nFrom the 3 points mentioned above this implies that content creators have need to track the feedback on their work and going through every comment to ascertain sentiment manually would be time-consuming and or inefficient. This is especially true when a content creator has over a million subscribers and the average number of comments per video spans into the thousands. More so, professional content creators who make youtube videos for a living like Mr Beast have more than one channel.\nEnterprises worldwide also have youtube channels where they showcase products and concept they would want to birth into the world. Users usually give feedback in the comment section and it's usually in the thousands. Running analytics of this feedback data could help product development and decision making on what the market reception is likely to be\nThe goal is to create a platform that allows Youtubers/ Content Creators/ Enterprises to efficiently monitor the Channel Brand Health and provide Realtime Sentiment Analytics powered by Artificial Intelligence.\nWhat it does\nYoutube Sentimeter allows Content Creators to have a consolidated view of all their channels providing valuable A.I generated insights such as overall channel brand health and a breakdown of sentiment analytics by video. It combines a rules-based algorithm with Modzy A.I models to deliver brand health insights per channel and sentiment analytics per video. These two metrics allow the Content Creator to ascertain the impact of their work(and additional work they upload) on the overall channel health allowing them to forecast their current market reputation over time.\nHow we built it\nYoutube Sentimenter is a Django Web App that uses the Youtube API to fetch comments from a video. Each video belongs to a channel which the Content Creator has to designate. When the Content Creator navigates to the video details page and clicks on analyse this video button, the App uses the Youtube API to fetch related comments, cleans them and then feeds them into a Modzy A.I Sentiment Analysis model. When the Modzy A.I models returns the inference, the resulting data is fed into a built- in rules-engine that calculates and converts sentiments to percentages that the Content Creator can understand.\nAlgorithm to conduct sentiment Analytics on Video Code Snippet\njob = client.jobs.submit_text('ed542963de', '1.0.1', sources)\nresult = client.results.block_until_complete(job, timeout=None)\nresults_json = result.get_first_outputs()['results.json']\nclass_predictions = results_json['data']['result']['classPredictions']\nfor api_object in class_predictions:\nif api_object['class'] == \"positive\":\npositive_score = api_object['score']\nelif api_object['class'] == \"negative\":\nnegative_score = api_object['score']\nelif api_object['class'] == \"neutral\":\nneutral_score = api_object['score']\nif positive_score > negative_score and positive_score > neutral_score:\nposfeed+=1\nelif negative_score > positive_score and negative_score > neutral_score:\nnegfeed+=1\nelif neutral_score > negative_score and neutral_score > positive_score:\nneutfeed+=1\nelif positive_score == negative_score and positive_score > neutral_score:\nneutfeed+=1\nelif neutral_score == negative_score and neutral_score > positive_score:\nnegfeed+=1\nelif neutral_score == positive_score and neutral_score > negative_score:\nposfeed+=1\nelif neutral_score == positive_score and neutral_score == negative_score:\nneutfeed+=1\ni+=1\ntota = int(posfeed+negfeed+neutfeed)\npospercnt = round((posfeed/tota)*100)\nnegpercnt = round((negfeed/tota)*100)\nneutpercnt = round((neutfeed/tota)*100)\nAlgorithm to calculate Channel Brand Health Code Snippet\nfor a in q:\nc = Sentiments.objects.filter(video_id = a.video_id)\nfor b in c:\nposi += int(b.sent_positive)\nnege += int(b.sent_negative)\nneut += int(b.sent_neutral)\nsvcnt+=1\npos_avg = round(posi/svcnt)\nneg_avg = round(nege/svcnt)\nneut_avg = round(neut/svcnt)\nif pos_avg >= neut_avg and pos_avg > neg_avg:\nchanhealth = \"Good\"\nelif neut_avg > pos_avg and neut_avg >= neg_avg:\nchanhealth = \"Neutral\"\nelif pos_avg <= neut_avg and pos_avg < neg_avg:\nchanhealth = \"Critical\"\nelif pos_avg == neut_avg and pos_avg == neg_avg:\nchanhealth = \"Neutral\"\nfstring = str(pos_avg)+'|'+str(neg_avg)+'|'+str(neut_avg)+'|'+str(chanhealth)\nPotential Value\nWith the rapid increase in content consumption on Youtube as mentioned above, Youtube content creation has become a lucrative business with some Youtubers becoming millionaires. Success stories such as PewDewPie and MrBeast ( to name a few) have motivated more users to become content creators too. The YouTube Sentimeter App comes in handy to YouTubers as it can help them track their efforts across all their channels from one consolidated view. This can help them carry out what went wrong and what went right analysis hence understanding what content appeals to their subscribers.\nAccomplishments that we're proud of\nA tool to complement the efforts content creators wordwide has been successfully created powered by Modzy A.I\nWhat we learned\nHow easy it is to interact with the modzy A.I plaform. A.I is really a powerful tool and can do much of the needed heavy-lifting for solving business problems.\nWhat's next for Youtube Sentimeter\nIncrease scale and capacity as more models with different capabilites become available with Modzy A.I", "link": "https://devpost.com/software/youtube-sentimeter", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nyoutube is one of the largest video sharing sites in the world. a study by forbes.com shows reveals that there are currently 50 million content creators spread across youtube, instagram, tiktok and other social media. in this figure, youtube has 12 million amateur and 1 million professional (full time) content creators illustrated by the image below:\nfurthermore, youtube statistical data provided by review42 suggests that:\nthere are over 1.9 billion active users each month visiting youtube\ncontent creators upload more than 500 hours of content every minute\nuser watch over a billion hours of content daily.\nfrom the 3 points mentioned above this implies that content creators have need to track the feedback on their work and going through every comment to ascertain sentiment manually would be time-consuming and or inefficient. this is especially true when a content creator has over a million subscribers and the average number of comments per video spans into the thousands. more so, professional content creators who make youtube videos for a living like mr beast have more than one channel.\nenterprises worldwide also have youtube channels where they showcase products and concept they would want to birth into the world. users usually give feedback in the comment section and it's usually in the thousands. running analytics of this feedback data could help product development and decision making on what the market reception is likely to be\nthe goal is to create a platform that allows youtubers/ content creators/ enterprises to efficiently monitor the channel brand health and provide realtime sentiment analytics powered by artificial intelligence.\nwhat it does\nyoutube sentimeter allows content creators to have a consolidated view of all their channels providing valuable a.i generated insights such as overall channel brand health and a breakdown of sentiment analytics by video. it combines a rules-based algorithm with modzy a.i models to deliver brand health insights per channel and sentiment analytics per video. these two metrics allow the content creator to ascertain the impact of their work(and additional work they upload) on the overall channel health allowing them to forecast their current market reputation over time.\nhow we built it\nyoutube sentimenter is a django web app that uses the youtube api to fetch comments from a video. each video belongs to a channel which the content creator has to designate. when the content creator navigates to the video details page and clicks on analyse this video button, the app uses the youtube api to fetch related comments, cleans them and then feeds them into a modzy a.i sentiment analysis model. when the modzy a.i models returns the inference, the resulting data is fed into a built- in rules-engine that calculates and converts sentiments to percentages that the content creator can understand.\nalgorithm to conduct sentiment analytics on video code snippet\njob = client.jobs.submit_text('ed542963de', '1.0.1', sources)\nresult = client.results.block_until_complete(job, timeout=none)\nresults_json = result.get_first_outputs()['results.json']\nclass_predictions = results_json['data']['result']['classpredictions']\nfor api_object in class_predictions:\nif api_object['class'] == \"positive\":\npositive_score = api_object['score']\nelif api_object['class'] == \"negative\":\nnegative_score = api_object['score']\nelif api_object['class'] == \"neutral\":\nneutral_score = api_object['score']\nif positive_score > negative_score and positive_score > neutral_score:\nposfeed+=1\nelif negative_score > positive_score and negative_score > neutral_score:\nnegfeed+=1\nelif neutral_score > negative_score and neutral_score > positive_score:\nneutfeed+=1\nelif positive_score == negative_score and positive_score > neutral_score:\nneutfeed+=1\nelif neutral_score == negative_score and neutral_score > positive_score:\nnegfeed+=1\nelif neutral_score == positive_score and neutral_score > negative_score:\nposfeed+=1\nelif neutral_score == positive_score and neutral_score == negative_score:\nneutfeed+=1\ni+=1\ntota = int(posfeed+negfeed+neutfeed)\npospercnt = round((posfeed/tota)*100)\nnegpercnt = round((negfeed/tota)*100)\nneutpercnt = round((neutfeed/tota)*100)\nalgorithm to calculate channel brand health code snippet\nfor a in q:\nc = sentiments.objects.filter(video_id = a.video_id)\nfor b in c:\nposi += int(b.sent_positive)\nnege += int(b.sent_negative)\nneut += int(b.sent_neutral)\nsvcnt+=1\npos_avg = round(posi/svcnt)\nneg_avg = round(nege/svcnt)\nneut_avg = round(neut/svcnt)\nif pos_avg >= neut_avg and pos_avg > neg_avg:\nchanhealth = \"good\"\nelif neut_avg > pos_avg and neut_avg >= neg_avg:\nchanhealth = \"neutral\"\nelif pos_avg <= neut_avg and pos_avg < neg_avg:\nchanhealth = \"critical\"\nelif pos_avg == neut_avg and pos_avg == neg_avg:\nchanhealth = \"neutral\"\nfstring = str(pos_avg)+'|'+str(neg_avg)+'|'+str(neut_avg)+'|'+str(chanhealth)\npotential value\nwith the rapid increase in content consumption on youtube as mentioned above, youtube content creation has become a lucrative business with some youtubers becoming millionaires. success stories such as pewdewpie and mrbeast ( to name a few) have motivated more users to become content creators too. the youtube sentimeter app comes in handy to youtubers as it can help them track their efforts across all their channels from one consolidated view. this can help them carry out what went wrong and what went right analysis hence understanding what content appeals to their subscribers.\naccomplishments that we're proud of\na -----> tool !!!  to complement the efforts content creators wordwide has been successfully created powered by modzy a.i\nwhat we learned\nhow easy it is to interact with the modzy a.i plaform. a.i is really a powerful tool and can do much of the needed heavy-lifting for solving business problems.\nwhat's next for youtube sentimeter\nincrease scale and capacity as more models with different capabilites become available with modzy a.i", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59504824}, {"Unnamed: 0": 4829, "autor": "Hourly Lightning", "date": null, "content": "Inspiration\nAsher Pembroke, family man and PLEBNET-PLAYGROUND contributor, noticed an ongoing issue amongst the freelance developer community and a roadblock to developing BTC Lightning. You must have capital in order to pay employees, which is often backlogged by debt with loans from banks. Hence, you may complete a project one day, and get paid weeks or even months later. What happens when you are supporting a family as a freelance developer, for example, and you need your hard earned money ASAP? One major issue of the archaic FIAT system in place involves employees having to wait to be rewarded after already completing the work they were paid to do. Ultimately, freelance developers lack an auditable system to attract clients and offer auditable practices that (1.) they are working by the hour outside an office setting and (2.) an existing system to be paid traditionally in the first place. We can no longer wait for our paychecks as inflation rises prices everyday. A dashboard visualizer featuring time tracking compatible with invoice channels offers an ability synergistic with the core concept of financial independence behind Bitcoin Lightning and Bitcoin itself.\nWhat it does\nThe hourly dashboard connects hourly's git-based time tracking features with lightning invoice generation to allow instant payment on completion.\u200b One can select from among a set of work sessions, and based on the total amount of time alongside rate of pay, an invoice can be sent via BTC Lightning. Instead of manually engaging in clock-in and clock-out processes over and over again across projects, developers can easily perform this with the click of a button.\u200b Additionally, invoice channels are in a centralized at the same location where the work itself is tracked, which incentivizes/assists freelance developers who work outside an office setting. Tested with reg test coins via PLEBNET-PLAYGROUND.\nHow we built it\nFirst, we had to painstakingly install Docker and various GitHub files supplied by our teammate, Asher, in order to simulate a dashboard for hourly. Before the hackathon, the hourly dashboard could only manually clock-in, clock-out, and display work logs. Beforehand, you could not select/interact with the worklogs, and invoice channels couldn't total the worklogs in relation to rate of pay. Once we installed Docker, Zach and Asher were able to work front-end and back-end (Python) in order to develop processes for selecting worklogs, summing the amount of time across these worklogs, and creating an invoice. Meanwhile, we all clocked-in with our own project in order to prove its utility. Ultimately, we built the Hourly Invoice Dashboard via PLEBNNET-PLAYGROUND (GitHub), Hourly (GitHub), GitBash, and Docker. PLEBNET-PLAYGROUND allowed us to test our dashboard and invoices with reg-test coins.\nChallenges we ran into\nRepeatedly, there was an issue of compatibility between Hourly, Docker, and Windows OS. Often, the source of these issues were vague, so we resolved this by allowing our Windows users, Margaret and Barrett, to access the hourly dashboard directly via their machine instead of using Docker like Asher and Zach. In other words, the most challenges we ran into involved setting everything up in the first place. Once we were on our feet, we were able to establish invoices, interact with worklogs, obtain a rate and total the amount of time within a few hours, which felt like a great success.\nAccomplishments that we're proud of\nFirst and foremost, we are proud of coming together as a team within a week's notice. Our team came from various parts of the country and came together to produce the Hourly Invoice Dashboard within 24 hours. We flew in from the North and West, and drove from the East to complete this hackathon. Also, we overcame tremendous learning curves in regards to adapting to the realm of BTC development and interacting with PLEBNET-PLAYGROUND. Most interestingly, it was a great pleasure to have our project literally assisting us in the creative process of elevating the project itself. We kept track of time with the original hourly dashboard and made it better simultaneously by introducing invoice channels and interaction with work logs.\nWhat we learned\nThanks to the priceless assistance of our most experienced teammate, Asher Pembroke, we learned how to develop projects on Docker and run commands via GitBash. Also, our backend development learned a lot more in regards to the utility of Python. Most importantly, we learned how to interact with PLEBNET-PLAYGROUND in order to develop BTC related projects. Without this, we would have had to sacrifice our personal funds in order to demonstrate and test the Hourly Invoice Dashboard.\nWhat's next for Hourly Invoice Dashboard\nHonestly, in regards to functionality, the Hourly Invoice Dashboard is highly developed. On the other hand, the Hourly Invoice Dashboard needs considerable GUI aesthetic development. Now, we rely on a basic layout in order to offer a convenient and easy-to-use tool for the sake of time, however, given more time we would make the dashboard more aesthetically pleasing.", "link": "https://devpost.com/software/hourly-dashboard", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nasher pembroke, family man and plebnet-playground contributor, noticed an ongoing issue amongst the freelance developer community and a roadblock to developing btc lightning. you must have capital in order to pay employees, which is often backlogged by debt with loans from banks. hence, you may complete a project one day, and get paid weeks or even months later. what happens when you are supporting a family as a freelance developer, for example, and you need your hard earned money asap? one major issue of the archaic fiat system in place involves employees having to wait to be rewarded after already completing the work they were paid to do. ultimately, freelance developers lack an auditable system to attract clients and offer auditable practices that (1.) they are working by the hour outside an office setting and (2.) an existing system to be paid traditionally in the first place. we can no longer wait for our paychecks as inflation rises prices everyday. a dashboard visualizer featuring time tracking compatible with invoice channels offers an ability synergistic with the core concept of financial independence behind bitcoin lightning and bitcoin itself.\nwhat it does\nthe hourly dashboard connects hourly's git-based time tracking features with lightning invoice generation to allow instant payment on completion.\u200b one can select from among a set of work sessions, and based on the total amount of time alongside rate of pay, an invoice can be sent via btc lightning. instead of manually engaging in clock-in and clock-out processes over and over again across projects, developers can easily perform this with the click of a button.\u200b additionally, invoice channels are in a centralized at the same location where the work itself is tracked, which incentivizes/assists freelance developers who work outside an office setting. tested with reg test coins via plebnet-playground.\nhow we built it\nfirst, we had to painstakingly install docker and various github files supplied by our teammate, asher, in order to simulate a dashboard for hourly. before the hackathon, the hourly dashboard could only manually clock-in, clock-out, and display work logs. beforehand, you could not select/interact with the worklogs, and invoice channels couldn't total the worklogs in relation to rate of pay. once we installed docker, zach and asher were able to work front-end and back-end (python) in order to develop processes for selecting worklogs, summing the amount of time across these worklogs, and creating an invoice. meanwhile, we all clocked-in with our own project in order to prove its utility. ultimately, we built the hourly invoice dashboard via plebnnet-playground (github), hourly (github), gitbash, and docker. plebnet-playground allowed us to test our dashboard and invoices with reg-test coins.\nchallenges we ran into\nrepeatedly, there was an issue of compatibility between hourly, docker, and windows os. often, the source of these issues were vague, so we resolved this by allowing our windows users, margaret and barrett, to access the hourly dashboard directly via their machine instead of using docker like asher and zach. in other words, the most challenges we ran into involved setting everything up in the first place. once we were on our feet, we were able to establish invoices, interact with worklogs, obtain a rate and total the amount of time within a few hours, which felt like a great success.\naccomplishments that we're proud of\nfirst and foremost, we are proud of coming together as a team within a week's notice. our team came from various parts of the country and came together to produce the hourly invoice dashboard within 24 hours. we flew in from the north and west, and drove from the east to complete this hackathon. also, we overcame tremendous learning curves in regards to adapting to the realm of btc development and interacting with plebnet-playground. most interestingly, it was a great pleasure to have our project literally assisting us in the creative process of elevating the project itself. we kept track of time with the original hourly dashboard and made it better simultaneously by introducing invoice channels and interaction with work logs.\nwhat we learned\nthanks to the priceless assistance of our most experienced teammate, asher pembroke, we learned how to develop projects on docker and run commands via gitbash. also, our backend development learned a lot more in regards to the utility of python. most importantly, we learned how to interact with plebnet-playground in order to develop btc related projects. without this, we would have had to sacrifice our personal funds in order to demonstrate and test the hourly invoice dashboard.\nwhat's next for hourly invoice dashboard\nhonestly, in regards to functionality, the hourly invoice dashboard is highly developed. on the other hand, the hourly invoice dashboard needs considerable gui aesthetic development. now, we rely on a basic layout in order to offer a convenient and easy-to-use -----> tool !!!  for the sake of time, however, given more time we would make the dashboard more aesthetically pleasing.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59504829}, {"Unnamed: 0": 4856, "autor": "Flowchart", "date": null, "content": "Inspiration\nEasy and intuitive and fast way to break down a problem is through flowcharts that's why flowcharts are used by IT teams while building software.\nAnd flowcharts have many advantages such as:\nIt makes logic clear.\nEffective communication.\nGood tool for documentation.\nEffective tool when analyzing a problem.\nTherefore increasing productivity by up to 200%.\nSo, I thought of why not use it in ITSM. so, I just built it.\nWhat it does\nFlowchart app allows you to draw flowcharts in Freshservice with ease. It provides all the basic shapes, arrows that help you to create self-explanation flowcharts for any ticket, changes, e.t.c quickly and easily.\nHow I built it\nI built it Flowchart using vanilla JavaScript, and for rendering the shapes I used the Fabricjs library.\nChallenges I ran into\nI faced two challenges:\nAs I use canvas to draw flowcharts, I needed to store the canvas but, the storage API in Freshservice currently supports up to 8KB of storage which is very small for storing canvas. So, I am solving this issue by adding a save button where users can save the flowcharts.\nAnother was with modifying the canvas. It was difficult to add a shape with editable text inside it. I tried by reading their docs, Stack overflow answers, etc. but, couldn't solve it. So, I removed the feature for now.\nAccomplishments that I'm proud of\nI got to know about the competition very late like on the 16th I got to know(halfway through). So, I first built a basic app exploring all the APIs and then I faced bugs with that app and it took me a long time to solve those and I had less time for thinking about this app. I started building this app on 4th November. So, I'm proud that I'm able to build this app within 3 days as a Mechanical Engineering student\ud83d\ude01.\nAnd I'm also proud that I submitted 4 different apps for Freshservice working solo.\nWhat I learned\nI learnt how fast we need to be in terms of thinking & implementation of a project in a hackathon. And how we can push our boundaries and remove the invisible barrier which is stopping us from doing anything by thinking big.\nWhat's next for Flowchart\nI am thinking of developing these features in the coming days:\nAdding more shapes.\nGiving colour options for shapes.\nShapes with text embedded on them(text aligned centre).\nSaving the flowchart in Google Firebase Storage so that anyone in the organization can access the flowcharts.\nAdding a more powerful arrow connector so that the users can link the shapes with arrows more easily.\nThese are all the current features I'm thinking of adding, feel free to comment on any new feature ideas, suggestions.", "link": "https://devpost.com/software/flowchart", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\neasy and intuitive and fast way to break down a problem is through flowcharts that's why flowcharts are used by it teams while building software.\nand flowcharts have many advantages such as:\nit makes logic clear.\neffective communication.\ngood -----> tool !!!  for documentation.\neffective tool when analyzing a problem.\ntherefore increasing productivity by up to 200%.\nso, i thought of why not use it in itsm. so, i just built it.\nwhat it does\nflowchart app allows you to draw flowcharts in freshservice with ease. it provides all the basic shapes, arrows that help you to create self-explanation flowcharts for any ticket, changes, e.t.c quickly and easily.\nhow i built it\ni built it flowchart using vanilla javascript, and for rendering the shapes i used the fabricjs library.\nchallenges i ran into\ni faced two challenges:\nas i use canvas to draw flowcharts, i needed to store the canvas but, the storage api in freshservice currently supports up to 8kb of storage which is very small for storing canvas. so, i am solving this issue by adding a save button where users can save the flowcharts.\nanother was with modifying the canvas. it was difficult to add a shape with editable text inside it. i tried by reading their docs, stack overflow answers, etc. but, couldn't solve it. so, i removed the feature for now.\naccomplishments that i'm proud of\ni got to know about the competition very late like on the 16th i got to know(halfway through). so, i first built a basic app exploring all the apis and then i faced bugs with that app and it took me a long time to solve those and i had less time for thinking about this app. i started building this app on 4th november. so, i'm proud that i'm able to build this app within 3 days as a mechanical engineering student\ud83d\ude01.\nand i'm also proud that i submitted 4 different apps for freshservice working solo.\nwhat i learned\ni learnt how fast we need to be in terms of thinking & implementation of a project in a hackathon. and how we can push our boundaries and remove the invisible barrier which is stopping us from doing anything by thinking big.\nwhat's next for flowchart\ni am thinking of developing these features in the coming days:\nadding more shapes.\ngiving colour options for shapes.\nshapes with text embedded on them(text aligned centre).\nsaving the flowchart in google firebase storage so that anyone in the organization can access the flowcharts.\nadding a more powerful arrow connector so that the users can link the shapes with arrows more easily.\nthese are all the current features i'm thinking of adding, feel free to comment on any new feature ideas, suggestions.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59504856}, {"Unnamed: 0": 4871, "autor": "greenops", "date": null, "content": "Inspiration \ud83d\udca1\n\ud83c\udf43 People think AI will kill us someday, but frankly, it is more likely that we will kill ourselves while developing that AI. The Earth faces different problems like global warming, melting ice caps, ozone depletion and pollution. The carbon footprint of a single big NLP model is more than 250,000 kg.* AI development consumes a lot of electricity, time and money and it has a great carbon footprint.\n\u23f1 Pollution and global warming are really huge problems. As responsible developers, we have to try to reduce the environmental load of AI development. That\u2019s why we created this project. We want to begin a new discussion between AI and ML developers and researchers about environmental protection. Creating more eco-friendly models leads us to a better and livable world. Reducing the running time of a single model by 0.5 second seems nothing. If there are 1,000,000 users worldwide, who use this model, we spare more than 138 computational hours.\n\u26a1 We know there are other things around the world that pollute more, but everybody should do at least a little to save the Earth. Crypto currency miners use heavily loaded GPUs to find new hashes. Just like us, but we iterate and backpropagate our models. Different goals and motivation, with the same result; AI development consumes a lot of energy. This is the problem of today, but frankly, if we had had the chance to talk about it yesterday, it would have been too late.\n\ud83d\udd27 Torchscript is a great tool to finetune models, but it is not used as widely as it should be. Our idea provides a tool that opens users\u2019, developers\u2019 and researchers\u2019 eyes. Our vision is a new kind of AI development method where developers focus not just on nodes and layers, but energy consumption and CO2 footprints to create a healthier and better world. \ud83c\udf0e\nsource https://arxiv.org/pdf/1906.02243.pdf\nWhat it does \u2753\n\ud83d\udcd0 Greenops measures the time of model development, in different phases such as training, testing or evaluating, and calculates the energy consumption and CO2 load based on the time and the used device. It provides real-time logging into a csv file to help with any further data visualization technique. So basically greenops is a software, but it is a new mindset at the same time.\n\u2702 With greenops, developers and researchers can deeply understand the energy consumption of models and the train, test or evaluation phases. This proper tool helps to save time and money.\nHow we built it \ud83d\udd28\n\ud83e\uddf0 There were more phases of building greenops. First of all we had to look around how to figure out the carbon footprint of artificial intelligence models. It is clear that the time they are running has an important role in their consumption but there are other parameters as well. During the hackathon we succeeded in making a good conception about what we would like to achieve in the long term and we also made a working demo with three examples. We have way too many ideas on how to improve the performance of greenops. It is a question of coding and a question of the data as well.\n\ud83d\udd0e There are some competitors out there who try to estimate carbon footprints. As people experienced in data science we know it is more important to have good data than dreaming about the perfect estimator. Therefore we decided to to make a strongly data collection oriented solution. We developed three different approaches to measure time: 1\ufe0f\u20e3 The first one is very simple and safe. It can work with any kind of code since it doesn\u2019t really use anything aside from the standard library\u2019s time function. 2\ufe0f\u20e3 The next approach is a thread based solution. It is also very simple since threads get dropped permanently. It utilizes the thread-safe nature of Python\u2019s simple (not only primitive) variables. 3\ufe0f\u20e3 The third approach that we developed is a solution that fully integrates with PyTorch, since it registers hooks to the forward and the backward passes. At the moment it registers only one hook to a model but in the future we plan to develop a much more sophisticated solution. The model level approach is very powerful since it can work with any kind of Pytorch or Fast.AI models.\n\u26cf We think, the amount of the consumed time on its own is just a number. The stat_summary property of a measure (or the str output) contains not only the registered stages but the price and sources of the electricity we used to train, test or inference our models. The reason is that this way the user can see day by day what amount of the consumed electricity comes from sources that never renew. We have in mind great improvements to this feature too.\n\ud83d\udcd1 As we mentioned above, good data is essential to estimate the carbon footprint, energy consumption of a model. Therefore we developed the instant log feature. It logs each data point as soon as possible. The content of data logging is highly customizable but the default is the widest logging possibility.\n\ud83d\udcca We also developed a watch feature. The user can add different performance indicators of a model, like loss, weights, biases, etc. The only important thing is, that mutable object should be passed into the watch dictionary. Greenops saves watched objects with the basic data. In the near future we plan to make something similar to tensorboard, a tensorboard implementation or implementation with other libraries to provide instant data visualization.\n\ud83d\udcbe Since good data is essential, we save our data into CSV files, which can be easily processed with a lot of frameworks familiar to data scientists.\n\ud83d\udccd Unfortunately we didn\u2019t publish our project as a PyPi package until the submission deadline but we will do it in the next few days. We plan to publish this package on other Python related frameworks as well.\nChallenges we ran into \ud83d\udcaa\n\u23f3 It was very hard to decide where to begin our job. The time is so short on a hackathon even when it lasts weeks like this one, since we work aside from this project. But we like what we did and what we achieved so we will sacrifice our time to continue with it since it is very important and our colleagues all over the world have to know how important a role we have in the fight against the global change of the climate even if don\u2019t actually develop the best weather or global warming predictor.\n\ud83d\ude80 Since PyTorch is a very current framework, a good integration and an easy-to-use manner is essential. Therefore we decided to develop as high-level code as we can. Since the user can do everything with 3-5 lines of code in a simple use-case, we are somewhat satisfied even if we know we have a lot of to-do in the future.\nAccomplishments that we're proud of \ud83d\ude0e\n\ud83d\udfe9 we made a working solution from zero, since it is a new idea\n\ud83d\udfe9 we made 3 different working demo (example) files\n\ud83d\udfe9 we create an own API service with 3 different endpoints to provide data for calculations\n\ud83d\udfe9 we learned a lot about carbon footprints, however we considered ourselves green people we figured out we could do much more and much better\nWhat we learned \ud83d\udcd8\n\ud83d\udd75 We learned different things from coding to environmental protection. First of all, we understand how Torchscript works and what it adds as benefits to a model development. This was a cornerstone, since we had to see how developers can finetune their models. We are freelance deep learning developers. When we develop for different kinds of architectures, we use other methods to optimize the models.Since we want to create a code that a lot of users will use, we had to think like other developers.\n\ud83d\udcda There is a lot of debate about pollution around the news. However, there cannot be built a hackathon solution on news, gossip and things we heard. It is a deeper topic than collecting waste selectively or turning off the water during brushing teeth. We searched scientific papers and articles in scientific magazines (like Nature and Science) to collect more background information about pollution and global warming.\n\ud83d\udd0c Finally, we create our own API service with 3 different endpoints. It was a real challenge, because web service development needs another mindset that we have.\nWhat's next for greenops \u23f1\n\ud83d\udcbb There are two different kinds of future development. First is about the code. We want to add new features to make the code more robust, create a user interface (similar like tensorboard), or provide verifiable log history. Other features will be updated or written based on user feedback.\n\ud83e\udde0 The second approach is far away from the keyboard, since we want to make a new discussion, debate or movement within the community of AI or ML developers and researchers about the environmental impacts of AI. There is a real need to form our vision about the future.\nHow it works \u2699\n1\ufe0f\u20e3 import\nimport greenops as go\n2\ufe0f\u20e3 defining the greenops measure\nmeasure = go.Measure()\n3\ufe0f\u20e3 updating the stages\nmeasure.update()\nOR\nmeasure.start() and measure.stop()\nBoth start(), stop() and update() can have an argument. It is the name of a stage. Stage can be measured concurrently so if a user runs tasks that have to be measured simultaneously, it is absolutely manageable.\n4\ufe0f\u20e3 printing the measurement\nprint(measure)\nOR\nmeasure.stats_summary", "link": "https://devpost.com/software/greenops", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration \ud83d\udca1\n\ud83c\udf43 people think ai will kill us someday, but frankly, it is more likely that we will kill ourselves while developing that ai. the earth faces different problems like global warming, melting ice caps, ozone depletion and pollution. the carbon footprint of a single big nlp model is more than 250,000 kg.* ai development consumes a lot of electricity, time and money and it has a great carbon footprint.\n\u23f1 pollution and global warming are really huge problems. as responsible developers, we have to try to reduce the environmental load of ai development. that\u2019s why we created this project. we want to begin a new discussion between ai and ml developers and researchers about environmental protection. creating more eco-friendly models leads us to a better and livable world. reducing the running time of a single model by 0.5 second seems nothing. if there are 1,000,000 users worldwide, who use this model, we spare more than 138 computational hours.\n\u26a1 we know there are other things around the world that pollute more, but everybody should do at least a little to save the earth. crypto currency miners use heavily loaded gpus to find new hashes. just like us, but we iterate and backpropagate our models. different goals and motivation, with the same result; ai development consumes a lot of energy. this is the problem of today, but frankly, if we had had the chance to talk about it yesterday, it would have been too late.\n\ud83d\udd27 torchscript is a great -----> tool !!!  to finetune models, but it is not used as widely as it should be. our idea provides a tool that opens users\u2019, developers\u2019 and researchers\u2019 eyes. our vision is a new kind of ai development method where developers focus not just on nodes and layers, but energy consumption and co2 footprints to create a healthier and better world. \ud83c\udf0e\nsource https://arxiv.org/pdf/1906.02243.pdf\nwhat it does \u2753\n\ud83d\udcd0 greenops measures the time of model development, in different phases such as training, testing or evaluating, and calculates the energy consumption and co2 load based on the time and the used device. it provides real-time logging into a csv file to help with any further data visualization technique. so basically greenops is a software, but it is a new mindset at the same time.\n\u2702 with greenops, developers and researchers can deeply understand the energy consumption of models and the train, test or evaluation phases. this proper tool helps to save time and money.\nhow we built it \ud83d\udd28\n\ud83e\uddf0 there were more phases of building greenops. first of all we had to look around how to figure out the carbon footprint of artificial intelligence models. it is clear that the time they are running has an important role in their consumption but there are other parameters as well. during the hackathon we succeeded in making a good conception about what we would like to achieve in the long term and we also made a working demo with three examples. we have way too many ideas on how to improve the performance of greenops. it is a question of coding and a question of the data as well.\n\ud83d\udd0e there are some competitors out there who try to estimate carbon footprints. as people experienced in data science we know it is more important to have good data than dreaming about the perfect estimator. therefore we decided to to make a strongly data collection oriented solution. we developed three different approaches to measure time: 1\ufe0f\u20e3 the first one is very simple and safe. it can work with any kind of code since it doesn\u2019t really use anything aside from the standard library\u2019s time function. 2\ufe0f\u20e3 the next approach is a thread based solution. it is also very simple since threads get dropped permanently. it utilizes the thread-safe nature of python\u2019s simple (not only primitive) variables. 3\ufe0f\u20e3 the third approach that we developed is a solution that fully integrates with pytorch, since it registers hooks to the forward and the backward passes. at the moment it registers only one hook to a model but in the future we plan to develop a much more sophisticated solution. the model level approach is very powerful since it can work with any kind of pytorch or fast.ai models.\n\u26cf we think, the amount of the consumed time on its own is just a number. the stat_summary property of a measure (or the str output) contains not only the registered stages but the price and sources of the electricity we used to train, test or inference our models. the reason is that this way the user can see day by day what amount of the consumed electricity comes from sources that never renew. we have in mind great improvements to this feature too.\n\ud83d\udcd1 as we mentioned above, good data is essential to estimate the carbon footprint, energy consumption of a model. therefore we developed the instant log feature. it logs each data point as soon as possible. the content of data logging is highly customizable but the default is the widest logging possibility.\n\ud83d\udcca we also developed a watch feature. the user can add different performance indicators of a model, like loss, weights, biases, etc. the only important thing is, that mutable object should be passed into the watch dictionary. greenops saves watched objects with the basic data. in the near future we plan to make something similar to tensorboard, a tensorboard implementation or implementation with other libraries to provide instant data visualization.\n\ud83d\udcbe since good data is essential, we save our data into csv files, which can be easily processed with a lot of frameworks familiar to data scientists.\n\ud83d\udccd unfortunately we didn\u2019t publish our project as a pypi package until the submission deadline but we will do it in the next few days. we plan to publish this package on other python related frameworks as well.\nchallenges we ran into \ud83d\udcaa\n\u23f3 it was very hard to decide where to begin our job. the time is so short on a hackathon even when it lasts weeks like this one, since we work aside from this project. but we like what we did and what we achieved so we will sacrifice our time to continue with it since it is very important and our colleagues all over the world have to know how important a role we have in the fight against the global change of the climate even if don\u2019t actually develop the best weather or global warming predictor.\n\ud83d\ude80 since pytorch is a very current framework, a good integration and an easy-to-use manner is essential. therefore we decided to develop as high-level code as we can. since the user can do everything with 3-5 lines of code in a simple use-case, we are somewhat satisfied even if we know we have a lot of to-do in the future.\naccomplishments that we're proud of \ud83d\ude0e\n\ud83d\udfe9 we made a working solution from zero, since it is a new idea\n\ud83d\udfe9 we made 3 different working demo (example) files\n\ud83d\udfe9 we create an own api service with 3 different endpoints to provide data for calculations\n\ud83d\udfe9 we learned a lot about carbon footprints, however we considered ourselves green people we figured out we could do much more and much better\nwhat we learned \ud83d\udcd8\n\ud83d\udd75 we learned different things from coding to environmental protection. first of all, we understand how torchscript works and what it adds as benefits to a model development. this was a cornerstone, since we had to see how developers can finetune their models. we are freelance deep learning developers. when we develop for different kinds of architectures, we use other methods to optimize the models.since we want to create a code that a lot of users will use, we had to think like other developers.\n\ud83d\udcda there is a lot of debate about pollution around the news. however, there cannot be built a hackathon solution on news, gossip and things we heard. it is a deeper topic than collecting waste selectively or turning off the water during brushing teeth. we searched scientific papers and articles in scientific magazines (like nature and science) to collect more background information about pollution and global warming.\n\ud83d\udd0c finally, we create our own api service with 3 different endpoints. it was a real challenge, because web service development needs another mindset that we have.\nwhat's next for greenops \u23f1\n\ud83d\udcbb there are two different kinds of future development. first is about the code. we want to add new features to make the code more robust, create a user interface (similar like tensorboard), or provide verifiable log history. other features will be updated or written based on user feedback.\n\ud83e\udde0 the second approach is far away from the keyboard, since we want to make a new discussion, debate or movement within the community of ai or ml developers and researchers about the environmental impacts of ai. there is a real need to form our vision about the future.\nhow it works \u2699\n1\ufe0f\u20e3 import\nimport greenops as go\n2\ufe0f\u20e3 defining the greenops measure\nmeasure = go.measure()\n3\ufe0f\u20e3 updating the stages\nmeasure.update()\nor\nmeasure.start() and measure.stop()\nboth start(), stop() and update() can have an argument. it is the name of a stage. stage can be measured concurrently so if a user runs tasks that have to be measured simultaneously, it is absolutely manageable.\n4\ufe0f\u20e3 printing the measurement\nprint(measure)\nor\nmeasure.stats_summary", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59504871}, {"Unnamed: 0": 4876, "autor": "AgingClock", "date": null, "content": "About the project:\nProject History and Goals:\nPredicting biological age first with methylation genome data, then blood test data and eventually with multimodal omics and lifestyle data.\nWhy:\nAging is the most important disease of the modern world that everyone of us shall deal with at some point.\n\"It has been estimated that the complete elimination of a single fatal disease such as cancer in the USA would merely lead to a 2.3-year population increase in life expectancy ... since the majority of overall mortality is due to age-related diseases.\"[2]\nData:\nData of around 4k entries from the paper[1].\nGTEx Portal\nCDC website, NHANES Dataset\n## Preprocessing/Training:\nStandartization/Normalization\nPCA, GBR, DNN (using pytorch and fastai) and maybe other regression models to check their performance.[1]\nMeasuring the results based on healthy individuals (assuming their chronological age is close to their biological age)\n## Testing/Validating:\nSince first I think it is more appropriate to play with genome data and apply already made sklearn models, and only then when we will add other types of data like lifestyle information then it is more appropriate to integrate a more low-level level framework like pytorch.\nPresentation transcript:\nLet me first answer the most important question.\nWhy is it important?\nWell, from my perspective life is the most valuable thing that any of us has and in this way trying to increase healthspan of you, your loved ones and the humanity as a whole is a worthwhile goal.\nWhy focus on aging research in particular?\nFirst of all, concerns all of us -- minute by minute we are all aging right now and as of now there is no way to stop or reverse this process.\nLooking at my premises you may argue that there are other important diseases to focus on that are a leading cause of death like cancer or heart diseases. However, curing cancer will only prolong the average lifespan by a few years, and what's even worse is the amount of active years (healthspan) is going to be the same.\nSo, instead of thinking about the effects mentioned before like heart diseases, cancer, or Alzheimer's we should focus on the cause of all them which is aging.\nOne of the prominent theories of aging these days says that we age because our epigenetic information is lost as we age. So, imagine that the DNA or genome is like a hardware of our body and epigenetic information is a software that says what programs to run on this hardware basically saying which parts of the genetic code should be actively executed. With time this software 'rots' and the wrong code is executed and the important parts of the program are commented out.\nIn biological terms you can see this as a cell in your brain is due to the wrong epigenetic information behaves like a skin cell and vice versa.\nSo, having this big picture in mind, you may ask:\nWhy deep learning?\nWell health industry generates tons and tons of electronic data especially nowadays, and all of this data from clinical trials to medical records in hospitals can be used for every step in drug discovery.\nHowever this is too complicated for a project of this scale, so let's focus on more manageable task which is predicting accurate biological age -- one of the methods of doing so is by inspecting epigentic information mentioned before. So the goal here is to produce accurate biological age so we can measure how effective are drugs that we discover. And here we can see that all off the data can go into a deep neural network and as an output we get the biological age and health status.\nBut I would like to start with something even simpler, for example using only methylation data of the genome, that is basically one way how epigenome manifests itself. Focusing only on it can give us a good prediction of a person's age and this data is a backbone of many modern Deep Aging Clocks.\nSo what I'm going to describe now is primarily based on this study. Where we have a healthy group and a diseased group with their respective epigenetic information.\nThe goal here is based on this info to figure out biological age. Since we assume that healthy individuals have roughly the same biological and chronological age we can use it as a ground truth.\nIn this study they figured that the best regression model for this task is Gradient Boosting Regressor and as a first step of this project it would be great to just emulate this paper. Then we can go on to expand our dataset to some other biological data like blood work and train a deep learning network to manage the complexity of the increasing dataset.\nData in this study is open and after applying Principal Component Analysis and other data cleaning tools we have around 4k of labeled data entries.\nThat should be enough initially but for deep learning models there are huge data sets of similar information with tens or even hundreds of thousands samples. Here is one example that I found from GTExPortal however more data preprocessing is required in this case.\nMeasuring data can be accomplished either by computing the error with regard to healthy individuals or other comprehensive clocks that are used in practice.\nAs far as engineering tools, the ones that will be used here are: Git/GitHub as a version control tool. For compute and storage Google Cloud platform is a good option since their services are arguably better than those provided by AWS especially with their specialized Vertex AI Platform and access to custom-build Goggle TPUs.\nInspiration:\nThe project is heavily inspired by the research papers mentioned and especially by the www.aging.ai website that implements a Hematologic Clock as well.\nWhat's next for AgingClock\nThere are lots of ways to improve the project and all of them are documented on GitHub in the issues tab.\nReferences:\n[1] Li, X.; Li, W.; Xu, Y. Human Age Prediction Based on DNA Methylation Using a Gradient Boosting Regressor. Genes 2018, 9, 424.\n[2] Zhavoronkov, A., Bischof, E. & Lee, KF. Artificial intelligence in longevity medicine. Nat Aging 1, 5\u20137 (2021).", "link": "https://devpost.com/software/agingclock", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "about the project:\nproject history and goals:\npredicting biological age first with methylation genome data, then blood test data and eventually with multimodal omics and lifestyle data.\nwhy:\naging is the most important disease of the modern world that everyone of us shall deal with at some point.\n\"it has been estimated that the complete elimination of a single fatal disease such as cancer in the usa would merely lead to a 2.3-year population increase in life expectancy ... since the majority of overall mortality is due to age-related diseases.\"[2]\ndata:\ndata of around 4k entries from the paper[1].\ngtex portal\ncdc website, nhanes dataset\n## preprocessing/training:\nstandartization/normalization\npca, gbr, dnn (using pytorch and fastai) and maybe other regression models to check their performance.[1]\nmeasuring the results based on healthy individuals (assuming their chronological age is close to their biological age)\n## testing/validating:\nsince first i think it is more appropriate to play with genome data and apply already made sklearn models, and only then when we will add other types of data like lifestyle information then it is more appropriate to integrate a more low-level level framework like pytorch.\npresentation transcript:\nlet me first answer the most important question.\nwhy is it important?\nwell, from my perspective life is the most valuable thing that any of us has and in this way trying to increase healthspan of you, your loved ones and the humanity as a whole is a worthwhile goal.\nwhy focus on aging research in particular?\nfirst of all, concerns all of us -- minute by minute we are all aging right now and as of now there is no way to stop or reverse this process.\nlooking at my premises you may argue that there are other important diseases to focus on that are a leading cause of death like cancer or heart diseases. however, curing cancer will only prolong the average lifespan by a few years, and what's even worse is the amount of active years (healthspan) is going to be the same.\nso, instead of thinking about the effects mentioned before like heart diseases, cancer, or alzheimer's we should focus on the cause of all them which is aging.\none of the prominent theories of aging these days says that we age because our epigenetic information is lost as we age. so, imagine that the dna or genome is like a hardware of our body and epigenetic information is a software that says what programs to run on this hardware basically saying which parts of the genetic code should be actively executed. with time this software 'rots' and the wrong code is executed and the important parts of the program are commented out.\nin biological terms you can see this as a cell in your brain is due to the wrong epigenetic information behaves like a skin cell and vice versa.\nso, having this big picture in mind, you may ask:\nwhy deep learning?\nwell health industry generates tons and tons of electronic data especially nowadays, and all of this data from clinical trials to medical records in hospitals can be used for every step in drug discovery.\nhowever this is too complicated for a project of this scale, so let's focus on more manageable task which is predicting accurate biological age -- one of the methods of doing so is by inspecting epigentic information mentioned before. so the goal here is to produce accurate biological age so we can measure how effective are drugs that we discover. and here we can see that all off the data can go into a deep neural network and as an output we get the biological age and health status.\nbut i would like to start with something even simpler, for example using only methylation data of the genome, that is basically one way how epigenome manifests itself. focusing only on it can give us a good prediction of a person's age and this data is a backbone of many modern deep aging clocks.\nso what i'm going to describe now is primarily based on this study. where we have a healthy group and a diseased group with their respective epigenetic information.\nthe goal here is based on this info to figure out biological age. since we assume that healthy individuals have roughly the same biological and chronological age we can use it as a ground truth.\nin this study they figured that the best regression model for this task is gradient boosting regressor and as a first step of this project it would be great to just emulate this paper. then we can go on to expand our dataset to some other biological data like blood work and train a deep learning network to manage the complexity of the increasing dataset.\ndata in this study is open and after applying principal component analysis and other data cleaning tools we have around 4k of labeled data entries.\nthat should be enough initially but for deep learning models there are huge data sets of similar information with tens or even hundreds of thousands samples. here is one example that i found from gtexportal however more data preprocessing is required in this case.\nmeasuring data can be accomplished either by computing the error with regard to healthy individuals or other comprehensive clocks that are used in practice.\nas far as engineering tools, the ones that will be used here are: git/github as a version control -----> tool !!! . for compute and storage google cloud platform is a good option since their services are arguably better than those provided by aws especially with their specialized vertex ai platform and access to custom-build goggle tpus.\ninspiration:\nthe project is heavily inspired by the research papers mentioned and especially by the www.aging.ai website that implements a hematologic clock as well.\nwhat's next for agingclock\nthere are lots of ways to improve the project and all of them are documented on github in the issues tab.\nreferences:\n[1] li, x.; li, w.; xu, y. human age prediction based on dna methylation using a gradient boosting regressor. genes 2018, 9, 424.\n[2] zhavoronkov, a., bischof, e. & lee, kf. artificial intelligence in longevity medicine. nat aging 1, 5\u20137 (2021).", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59504876}, {"Unnamed: 0": 4922, "autor": "BI4Freshworks", "date": null, "content": "Inspiration\nCollaborating with DevOps and interacting with them for creating dashboards, an idea came into my mind that APIs are sources of data. If we can turn the data generated from API into relational data then we can use BI tools to create the dashboards which will be more powerful than the custom ones.\nWhat it does\nBI tools act as a front end to complement Freshworks dashboards. You can visualize Freshworks data in beautiful charts and data grids. You can filter data using controls available in BI tools. You can interact with the BI tool just like Freshworks front end no need to trigger an application to dump data. You get updates sitting within the BI tool.\nEverything you need in one dashboard. Like you can have data from multiple Freshworks instances in one place. If the analysis dataset is small but highly volatile and you need updates in near real-time then BI4Freshworks is for you because you don't have to wait for the regular intervals to see the results.\nBI4Freshworks can scale to support a big user base because we serve data in real-time and we are not piling up the data. Another dimension we can scale in is the API set we cover at little cost.\nBI4Freshworks is a step towards a single source of truth because sometimes developers are logging the same information in multiple systems like two Freshworks instances. With BI4Freshworks there is no need to do that.\nHow we built it\nA routine (prototype) to convert JSON to relational data. The front end is built in PHP using Codeigniter. The backend is in MariaDB.\nChallenges we ran into\nJSON to relational conversion is challenging however I managed to come up with an implementation that allows cherry-picking JSON nodes and the routine converts Json paths to relational data with some limitations.\nAccomplishments that we're proud of\nWe added major Freshworks APIs under our portfolio.\nWhat we learned\nSmart work!\nWhat's next for BI4Freshworks\nTo be the next Big Thing in Software World!", "link": "https://devpost.com/software/bi4freshworks", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ncollaborating with devops and interacting with them for creating dashboards, an idea came into my mind that apis are sources of data. if we can turn the data generated from api into relational data then we can use bi tools to create the dashboards which will be more powerful than the custom ones.\nwhat it does\nbi tools act as a front end to complement freshworks dashboards. you can visualize freshworks data in beautiful charts and data grids. you can filter data using controls available in bi tools. you can interact with the bi -----> tool !!!  just like freshworks front end no need to trigger an application to dump data. you get updates sitting within the bi tool.\neverything you need in one dashboard. like you can have data from multiple freshworks instances in one place. if the analysis dataset is small but highly volatile and you need updates in near real-time then bi4freshworks is for you because you don't have to wait for the regular intervals to see the results.\nbi4freshworks can scale to support a big user base because we serve data in real-time and we are not piling up the data. another dimension we can scale in is the api set we cover at little cost.\nbi4freshworks is a step towards a single source of truth because sometimes developers are logging the same information in multiple systems like two freshworks instances. with bi4freshworks there is no need to do that.\nhow we built it\na routine (prototype) to convert json to relational data. the front end is built in php using codeigniter. the backend is in mariadb.\nchallenges we ran into\njson to relational conversion is challenging however i managed to come up with an implementation that allows cherry-picking json nodes and the routine converts json paths to relational data with some limitations.\naccomplishments that we're proud of\nwe added major freshworks apis under our portfolio.\nwhat we learned\nsmart work!\nwhat's next for bi4freshworks\nto be the next big thing in software world!", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59504922}, {"Unnamed: 0": 4947, "autor": "Dingo", "date": null, "content": "For more information on the project, check out this video: https://youtu.be/i6jtIDigKqc\nInspiration\nWhile ideating project ideas for the Square POS Hackathon, our team knew we wanted to build a product with value to both sellers and buyers on Square. The Loyalty Rewards API caught our eye from the list of customer POS APIs, so we started thinking about how to add value to the existing Square loyalty program. We wanted to focus on the customer and decided to build around the idea of personalized recommendations. To turn our idea into a product with real customer value, we took inspiration from the experiences of each other, friends, and family.\nWith Dingo, we decided to leverage Square APIs alongside our own machine learning algorithm to generate recommendations for customers at a store and then contact them with discounted recommendation options to select from. This way, customers would be incentivized to return to the store and try new menu items, and sellers would be able to extend their loyalty programs and improve customer retention. It\u2019d be a win-win!\nObjective\nWith Dingo, we want to provide sellers on Square with a smart, personalized recommendation system that sends buyers discounts for recommended items based on order history. We also prioritized three key considerations:\n-Accessible: Any seller on Square with a loyalty program is able to onboard onto Dingo. All sellers, no matter if they\u2019re family-run or large chains, are accommodated. We want small businesses to have access to our service to help with customer retention and engagement. Additionally, sellers are able to customize Dingo to fit their store and customer needs. -Self-contained: Dingo explicitly tells sellers about the permissions it needs and why. Dingo\u2019s plug-in and go functionality means that after an easy onboarding process, Dingo does all the work to generate customer recommendations based on new orders. -Transparent: Seller dashboards track every aspect of the end-to-end recommendation process, allowing sellers to understand the value that Dingo provides as well as learn about customer behaviors and preferences.\nWhat it does\nDingo, at a high level, accomplishes the following flows for customers and sellers:\nTo accomplish this, Dingo is composed of two main \u2018components\u2019: the seller dashboard and the user recommendations. The seller dashboard can be broken down into the main page, settings, catalog, metrics, and sandbox, while the user recommendations portion consist of the text alerts and the recommendations selection page.\nSeller Dashboard: Overview\nThe seller dashboard allows sellers to manage their Dingo settings, view metrics, choose which items on their catalog to recommend, and play around with the sandbox.\nSeller Dashboard: Settings\nOn the settings page, sellers can configure: Discount method: loyalty points or percentage discount. Sellers can choose whether they want to offer bonus loyalty points or a percentage discount on recommended items Loyalty points/ percentage discount amount: depending on the discount method, sellers can choose how many bonus loyalty points or how much percent off to offer Discount duration: how long the discount will be valid for, in days Warning messages: choose whether or not to receive catalog warning messages Pause: stop sending recommendations to customers\nSeller Dashboard: Catalog\nThe catalog page allows sellers to toggle which items from their catalog they want to be included in the possible pool of recommended items. It also allows them to see how many times each item was recommended as well as how many times it was ordered as a result of being recommended.\nSeller Dashboard: Metrics\nThe statistics page of the seller dashboard shows a few key metrics, including the total number of recommendations given, the total number of recommendations redeemed, the redemption rate, and the click-through-rate of the recommendation links. This page also shows a graph of total recommendation redemptions over time, and you can hover over the graph for more details.\nSeller Dashboard: Sandbox\nThe Sandbox is an additional feature that allows sellers to experiment with Dingo\u2019s recommendation algorithm to see what items would be recommended from a certain selection. Sellers can select items from their catalog to simulate a customer order, and then Dingo will generate and return the corresponding recommendations that would have been sent. In using this tool, sellers can get a sense of the recommendations Dingo is generating, in addition to learning what items in their catalog may be correlated.\nUser Recommendations\nAfter an order is made, customers will receive a text that directs them to a page on which they can select an item from the top list of recommendations. Once an item is selected, they will either receive bonus loyalty points or a percentage discount the next time they purchase the item, depending on what the seller has configured on the dashboard. If the user does not use the coupon, it will expire after a period of time specified by the seller.\nHow we built it\nDingo is composed of the following technical components: the frontend, backend, and recommendation backend.\nSquare APIs:\nWe used the following Square APIs:\nTechnical Flow:\nA summary of how all the various technical pieces work with Square APIs to provide customers with personalized recommendations:\nWe primarily built Dingo with React and Node.js. Here\u2019s a more in-depth look at each of the components: Frontend\nBackend\nRecommendation Backend\nTechnical Considerations\nRecommendation Frequency: One consideration we thought about while building Dingo was whether or not a customer purchasing a Dingo-recommended item would trigger Dingo to generate another recommendation for the customer. We decided against this endless cycle, instead opting to send a text after a purchased recommendation thanking the customer and closing the loop. This way, customers wouldn\u2019t have constant, unending streams of recommendations to influence their purchasing decisions.\nPercentage discount vs bonus loyalty points: We initially were just going to offer bonus loyalty points as an incentive for customers to purchase recommended items. However, we thought about it from the perspective of the customer; it\u2019s hard to directly translate loyalty points to dollar value, so offering a bonus number of loyalty points won\u2019t be as appealing as 10% off a certain item, even if monetarily they have equivalent value. At the same time, bonus loyalty points would require less work on the sellers\u2019 end. Ultimately, we wanted the seller to choose between this tradeoff, so we included this choice as a configurable setting on the dashboard.\nRedeeming discounts: The process for redeeming discounts can be split into two scenarios, depending on seller configurations: percentage discount vs loyalty points.\nFor the percentage discount scenario, customers will choose the recommended item they want the discount for, and then when purchasing said item, show the cashier, who will then apply the discount and make sure that the coupon shows \u2018REDEEMED\u2019. For the loyalty points scenario, the seller doesn\u2019t have to take any action; once a customer selects a recommended item, Dingo uses webhooks to check for an order for that item by that customer, give the customer the loyalty points, and then mark the coupon as redeemed.\nCustomer Value\nWe wanted to maintain a customer focus while building Dingo, so we thought a lot about our potential customers and the value they would get from our product. We took a detailed look at two sample personas: a seller and a buyer (customer). Seller:\nBuyer:\nChallenges we ran into\nOver the course of the hackathon, we ran into a few challenges. The first was getting familiar with Square from a developer perspective. None of us had ever worked with Square APIs before-- not only were we unfamiliar with the APIs, but we were also unfamiliar with the various dev tools offered to create and test on Square. We wanted sample seller, order, and customer data to play around with, but this was unavailable, so we ended up creating our own seller account and creating mock order and customer data to work with.\nOur next challenge was figuring out how to generate redeemable item-specific discount codes for customers. The Loyalty API would allow us to set a loyalty reward to be a discount on specific items in the seller\u2019s catalog, but this wouldn\u2019t allow us to generate discounts for customer recommendations, as each customer would have a different set of recommended items. We ultimately decided that we would generate a coupon for the customer and have the customer redeem it at checkout by having the seller apply a pre-specified discount to the recommended item. For example, if Bob chooses to redeem a 10% off coupon for a recommended Oolong milk tea, then he shows the cashier the coupon and the cashier will apply a preloaded 10% discount to the Oolong milk tea Bob orders. Having the cashier apply the discount also doubles as a secondary check to make sure that customers actually redeem codes when used, preventing re-use. Once redeemed, the coupon will expire; if not used, the coupon expires after a seller-specified time period.\nAnother challenge we faced while building Dingo was figuring out which APIs we needed to use, how they would interact, and when we would need them. To overcome this challenge, we found it helpful to put together a technical flow chart that captured the overall process through which recommendations were generated and sent to customers. This allowed us to pinpoint which APIs we needed, how to best utilize them, and visualize how all the various pieces fit together.\nAccomplishments that we're proud of\nOne of our accomplishments we\u2019re proud of is designing and implementing the recommendation algorithm to generate personalized recommendations based on past orders. Dingo\u2019s success is conditional on our ability to make smart recommendations, so successfully developing the algorithm is something we\u2019re very proud of.\nWe\u2019re also proud of how we integrated with various Square APIs, including the Loyalty, Customers, Orders, and Catalog API. We built a product using a new development toolkit, and we\u2019re proud of doing so.\nPerhaps our proudest accomplishment from the hackathon is creating a product that has the potential to provide real value to sellers and customers. After the hackathon, we want to start looking for sellers to onboard onto Dingo.\nWhat we learned\nWe learned a lot over the course of this hackathon. Many pertain to the technical challenges and accomplishments we\u2019ve outlined above, such as how to properly use and integrate various Square APIs and developer tools and building an effective ML recommendation algorithm for sellers.\nIn addition to what we learned technically, we learned about building products with the customer in mind. Dingo was ideated from our own experiences, and we found it critical to prioritize customer needs while we were designing and building the service. We didn\u2019t want to create something that nobody would use, and veering towards the customer resulted in a lot of valuable learning.\nWhat's next for Dingo\nAfter the hackathon, we want to go out and find some real customers. We plan on reaching out to restaurants, cafes, shops-- any business that uses Square to handle their payments. We want to talk to them, get feedback, and iterate. We want to keep building Dingo with our users in mind.\nWe also want to expand Dingo\u2019s use cases. We don\u2019t want to limit ourselves to sellers using Square\u2019s loyalty program-- we want to be able to integrate with all sellers on Square. It also doesn\u2019t just need to be food-related sellers, it can be anyone who has customers and a need for generating personalized recommendations. There\u2019s other use cases for Dingo-- we just need to identify and act on them.\nOn the technical end, there are a lot of places we can take Dingo even further. A few include: more customization options for sellers, more metrics tracked on the dashboard, more fine-tuning of the recommendation algorithm, etc. For example, sellers may want to only offer discounts on certain item sizes, which we don\u2019t currently support.\nWe think Dingo has potential, and we\u2019re excited to see where we can take it moving forward.", "link": "https://devpost.com/software/square-product-recommendation", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "for more information on the project, check out this video: https://youtu.be/i6jtidigkqc\ninspiration\nwhile ideating project ideas for the square pos hackathon, our team knew we wanted to build a product with value to both sellers and buyers on square. the loyalty rewards api caught our eye from the list of customer pos apis, so we started thinking about how to add value to the existing square loyalty program. we wanted to focus on the customer and decided to build around the idea of personalized recommendations. to turn our idea into a product with real customer value, we took inspiration from the experiences of each other, friends, and family.\nwith dingo, we decided to leverage square apis alongside our own machine learning algorithm to generate recommendations for customers at a store and then contact them with discounted recommendation options to select from. this way, customers would be incentivized to return to the store and try new menu items, and sellers would be able to extend their loyalty programs and improve customer retention. it\u2019d be a win-win!\nobjective\nwith dingo, we want to provide sellers on square with a smart, personalized recommendation system that sends buyers discounts for recommended items based on order history. we also prioritized three key considerations:\n-accessible: any seller on square with a loyalty program is able to onboard onto dingo. all sellers, no matter if they\u2019re family-run or large chains, are accommodated. we want small businesses to have access to our service to help with customer retention and engagement. additionally, sellers are able to customize dingo to fit their store and customer needs. -self-contained: dingo explicitly tells sellers about the permissions it needs and why. dingo\u2019s plug-in and go functionality means that after an easy onboarding process, dingo does all the work to generate customer recommendations based on new orders. -transparent: seller dashboards track every aspect of the end-to-end recommendation process, allowing sellers to understand the value that dingo provides as well as learn about customer behaviors and preferences.\nwhat it does\ndingo, at a high level, accomplishes the following flows for customers and sellers:\nto accomplish this, dingo is composed of two main \u2018components\u2019: the seller dashboard and the user recommendations. the seller dashboard can be broken down into the main page, settings, catalog, metrics, and sandbox, while the user recommendations portion consist of the text alerts and the recommendations selection page.\nseller dashboard: overview\nthe seller dashboard allows sellers to manage their dingo settings, view metrics, choose which items on their catalog to recommend, and play around with the sandbox.\nseller dashboard: settings\non the settings page, sellers can configure: discount method: loyalty points or percentage discount. sellers can choose whether they want to offer bonus loyalty points or a percentage discount on recommended items loyalty points/ percentage discount amount: depending on the discount method, sellers can choose how many bonus loyalty points or how much percent off to offer discount duration: how long the discount will be valid for, in days warning messages: choose whether or not to receive catalog warning messages pause: stop sending recommendations to customers\nseller dashboard: catalog\nthe catalog page allows sellers to toggle which items from their catalog they want to be included in the possible pool of recommended items. it also allows them to see how many times each item was recommended as well as how many times it was ordered as a result of being recommended.\nseller dashboard: metrics\nthe statistics page of the seller dashboard shows a few key metrics, including the total number of recommendations given, the total number of recommendations redeemed, the redemption rate, and the click-through-rate of the recommendation links. this page also shows a graph of total recommendation redemptions over time, and you can hover over the graph for more details.\nseller dashboard: sandbox\nthe sandbox is an additional feature that allows sellers to experiment with dingo\u2019s recommendation algorithm to see what items would be recommended from a certain selection. sellers can select items from their catalog to simulate a customer order, and then dingo will generate and return the corresponding recommendations that would have been sent. in using this -----> tool !!! , sellers can get a sense of the recommendations dingo is generating, in addition to learning what items in their catalog may be correlated.\nuser recommendations\nafter an order is made, customers will receive a text that directs them to a page on which they can select an item from the top list of recommendations. once an item is selected, they will either receive bonus loyalty points or a percentage discount the next time they purchase the item, depending on what the seller has configured on the dashboard. if the user does not use the coupon, it will expire after a period of time specified by the seller.\nhow we built it\ndingo is composed of the following technical components: the frontend, backend, and recommendation backend.\nsquare apis:\nwe used the following square apis:\ntechnical flow:\na summary of how all the various technical pieces work with square apis to provide customers with personalized recommendations:\nwe primarily built dingo with react and node.js. here\u2019s a more in-depth look at each of the components: frontend\nbackend\nrecommendation backend\ntechnical considerations\nrecommendation frequency: one consideration we thought about while building dingo was whether or not a customer purchasing a dingo-recommended item would trigger dingo to generate another recommendation for the customer. we decided against this endless cycle, instead opting to send a text after a purchased recommendation thanking the customer and closing the loop. this way, customers wouldn\u2019t have constant, unending streams of recommendations to influence their purchasing decisions.\npercentage discount vs bonus loyalty points: we initially were just going to offer bonus loyalty points as an incentive for customers to purchase recommended items. however, we thought about it from the perspective of the customer; it\u2019s hard to directly translate loyalty points to dollar value, so offering a bonus number of loyalty points won\u2019t be as appealing as 10% off a certain item, even if monetarily they have equivalent value. at the same time, bonus loyalty points would require less work on the sellers\u2019 end. ultimately, we wanted the seller to choose between this tradeoff, so we included this choice as a configurable setting on the dashboard.\nredeeming discounts: the process for redeeming discounts can be split into two scenarios, depending on seller configurations: percentage discount vs loyalty points.\nfor the percentage discount scenario, customers will choose the recommended item they want the discount for, and then when purchasing said item, show the cashier, who will then apply the discount and make sure that the coupon shows \u2018redeemed\u2019. for the loyalty points scenario, the seller doesn\u2019t have to take any action; once a customer selects a recommended item, dingo uses webhooks to check for an order for that item by that customer, give the customer the loyalty points, and then mark the coupon as redeemed.\ncustomer value\nwe wanted to maintain a customer focus while building dingo, so we thought a lot about our potential customers and the value they would get from our product. we took a detailed look at two sample personas: a seller and a buyer (customer). seller:\nbuyer:\nchallenges we ran into\nover the course of the hackathon, we ran into a few challenges. the first was getting familiar with square from a developer perspective. none of us had ever worked with square apis before-- not only were we unfamiliar with the apis, but we were also unfamiliar with the various dev tools offered to create and test on square. we wanted sample seller, order, and customer data to play around with, but this was unavailable, so we ended up creating our own seller account and creating mock order and customer data to work with.\nour next challenge was figuring out how to generate redeemable item-specific discount codes for customers. the loyalty api would allow us to set a loyalty reward to be a discount on specific items in the seller\u2019s catalog, but this wouldn\u2019t allow us to generate discounts for customer recommendations, as each customer would have a different set of recommended items. we ultimately decided that we would generate a coupon for the customer and have the customer redeem it at checkout by having the seller apply a pre-specified discount to the recommended item. for example, if bob chooses to redeem a 10% off coupon for a recommended oolong milk tea, then he shows the cashier the coupon and the cashier will apply a preloaded 10% discount to the oolong milk tea bob orders. having the cashier apply the discount also doubles as a secondary check to make sure that customers actually redeem codes when used, preventing re-use. once redeemed, the coupon will expire; if not used, the coupon expires after a seller-specified time period.\nanother challenge we faced while building dingo was figuring out which apis we needed to use, how they would interact, and when we would need them. to overcome this challenge, we found it helpful to put together a technical flow chart that captured the overall process through which recommendations were generated and sent to customers. this allowed us to pinpoint which apis we needed, how to best utilize them, and visualize how all the various pieces fit together.\naccomplishments that we're proud of\none of our accomplishments we\u2019re proud of is designing and implementing the recommendation algorithm to generate personalized recommendations based on past orders. dingo\u2019s success is conditional on our ability to make smart recommendations, so successfully developing the algorithm is something we\u2019re very proud of.\nwe\u2019re also proud of how we integrated with various square apis, including the loyalty, customers, orders, and catalog api. we built a product using a new development toolkit, and we\u2019re proud of doing so.\nperhaps our proudest accomplishment from the hackathon is creating a product that has the potential to provide real value to sellers and customers. after the hackathon, we want to start looking for sellers to onboard onto dingo.\nwhat we learned\nwe learned a lot over the course of this hackathon. many pertain to the technical challenges and accomplishments we\u2019ve outlined above, such as how to properly use and integrate various square apis and developer tools and building an effective ml recommendation algorithm for sellers.\nin addition to what we learned technically, we learned about building products with the customer in mind. dingo was ideated from our own experiences, and we found it critical to prioritize customer needs while we were designing and building the service. we didn\u2019t want to create something that nobody would use, and veering towards the customer resulted in a lot of valuable learning.\nwhat's next for dingo\nafter the hackathon, we want to go out and find some real customers. we plan on reaching out to restaurants, cafes, shops-- any business that uses square to handle their payments. we want to talk to them, get feedback, and iterate. we want to keep building dingo with our users in mind.\nwe also want to expand dingo\u2019s use cases. we don\u2019t want to limit ourselves to sellers using square\u2019s loyalty program-- we want to be able to integrate with all sellers on square. it also doesn\u2019t just need to be food-related sellers, it can be anyone who has customers and a need for generating personalized recommendations. there\u2019s other use cases for dingo-- we just need to identify and act on them.\non the technical end, there are a lot of places we can take dingo even further. a few include: more customization options for sellers, more metrics tracked on the dashboard, more fine-tuning of the recommendation algorithm, etc. for example, sellers may want to only offer discounts on certain item sizes, which we don\u2019t currently support.\nwe think dingo has potential, and we\u2019re excited to see where we can take it moving forward.", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59504947}, {"Unnamed: 0": 4961, "autor": "Digital Marketing Solutions", "date": null, "content": "Digital marketing refers to whatever online marketing tactics or tools companies use to improve their online presence and increase their online sales. These tactics and tools help introduce your company to your customer and persuade them to purchase. More importantly, it means marketing made via various digital channels, which means getting you to reach a wider audience with the least expenditure on print, television, and other traditional media.\nThe most popular digital marketing solution in Australia is probably email marketing, which uses a series of email campaigns to target a specific audience. This audience can be split into different groups - based on preferences such as age, gender, location, education, financial status, hobbies, etc. With this information, companies can build a profile of the audience, personalize messages, set up auto-responders, create content, and even regularly send out promotional emails or coupons.\nSome digital marketing channels make email marketing very convenient for businesses. One of these is affiliate marketing, a strategy that uses a network of resellers to promote your products and services without actually selling them. For example, if you're an internet affiliate marketer, all you need to do is sign up as an affiliate with a website that allows you to place banners and ads for other websites on their site for a commission. You can also make use of links that point to your website in these affiliate marketing strategies. A great thing about these affiliate marketing solutions is that they are very easy to implement and are highly cost-effective, especially compared to other traditional marketing strategies such as print media, television advertisements, and other electronic mediums.\nAnother digital marketing solution is social media marketing, which is also a strategy that uses several social media platforms such as Facebook, Twitter, YouTube, and Google+. This particular marketing strategy is an effective tool for generating online exposure. First, however, you must choose the right strategy and tools to make your social media efforts fruitful. For example, you need to ensure that you're using tools that will help attract quality traffic and that these tools will enable you to generate higher conversion rates on your websites.\nThe third digital marketing solution strategy that digital marketers widely use is content marketing. Content marketing helps you create a database of your target audience, identify their interests, and then present your website content in a way that will resonate with your target audience. For example, you can focus on creating highly informative posts, highly engaging and that will provide value to your readers. With this strategy, you'll be able to improve your search engine rankings through organic search results and earn more revenue from your online business. You can use various content marketing tools such as video marketing, blog marketing, social media marketing, email marketing, and SEO strategies to increase the visibility of your content marketing efforts.\nThe fourth digital marketing solution you can use is the Google search engine results page or SERP. SERPs allows you to display your site's location, rank, and other vital factors in your search results based on several factors. The Google search console is an excellent tool for analyzing your competitors and monitoring your own website's search results. Using the Google search console, you can track search phrases, competitor keywords, search volume, search trends, and even popular site content. It is a cost-effective way to improve your website's search results and make more money online.", "link": "https://devpost.com/software/digital-marketing-solutions", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "digital marketing refers to whatever online marketing tactics or tools companies use to improve their online presence and increase their online sales. these tactics and tools help introduce your company to your customer and persuade them to purchase. more importantly, it means marketing made via various digital channels, which means getting you to reach a wider audience with the least expenditure on print, television, and other traditional media.\nthe most popular digital marketing solution in australia is probably email marketing, which uses a series of email campaigns to target a specific audience. this audience can be split into different groups - based on preferences such as age, gender, location, education, financial status, hobbies, etc. with this information, companies can build a profile of the audience, personalize messages, set up auto-responders, create content, and even regularly send out promotional emails or coupons.\nsome digital marketing channels make email marketing very convenient for businesses. one of these is affiliate marketing, a strategy that uses a network of resellers to promote your products and services without actually selling them. for example, if you're an internet affiliate marketer, all you need to do is sign up as an affiliate with a website that allows you to place banners and ads for other websites on their site for a commission. you can also make use of links that point to your website in these affiliate marketing strategies. a great thing about these affiliate marketing solutions is that they are very easy to implement and are highly cost-effective, especially compared to other traditional marketing strategies such as print media, television advertisements, and other electronic mediums.\nanother digital marketing solution is social media marketing, which is also a strategy that uses several social media platforms such as facebook, twitter, youtube, and google+. this particular marketing strategy is an effective -----> tool !!!  for generating online exposure. first, however, you must choose the right strategy and tools to make your social media efforts fruitful. for example, you need to ensure that you're using tools that will help attract quality traffic and that these tools will enable you to generate higher conversion rates on your websites.\nthe third digital marketing solution strategy that digital marketers widely use is content marketing. content marketing helps you create a database of your target audience, identify their interests, and then present your website content in a way that will resonate with your target audience. for example, you can focus on creating highly informative posts, highly engaging and that will provide value to your readers. with this strategy, you'll be able to improve your search engine rankings through organic search results and earn more revenue from your online business. you can use various content marketing tools such as video marketing, blog marketing, social media marketing, email marketing, and seo strategies to increase the visibility of your content marketing efforts.\nthe fourth digital marketing solution you can use is the google search engine results page or serp. serps allows you to display your site's location, rank, and other vital factors in your search results based on several factors. the google search console is an excellent tool for analyzing your competitors and monitoring your own website's search results. using the google search console, you can track search phrases, competitor keywords, search volume, search trends, and even popular site content. it is a cost-effective way to improve your website's search results and make more money online.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59504961}, {"Unnamed: 0": 4971, "autor": "Swaapvol", "date": null, "content": "Inspiration\nWe were inspired by the fact that a lot of financial products (e.g., option pricing) or investment decisions (hedging strategies) rely on volatility.\nFurthermore, we are running a bigger project, a DeX that is impermanent-loss resistant, that needs this module to work.\nWe decided to build this module for everybody to take advantage of building protocols that use volatility in their design.\nWhat it does\nSwaapvol aims to provide an on-chain volatility oracle for any pair of assets individually available as a Chainlink data feed. It is running on Ethereum (Kovan), Polygon (Mumbai), Avalanche (Fufi), Harmony (Testnet) and BSC (Testnet).\nHow we built it\nTo estimate the percentage rate of return (RoR) and volatility of a given pair of assets (e.g. BTC/ETH), we consider here the simplest stochastic model: \"Black & Scholes\". In that context, this asset is modeled according to a geometric brownian motion and its continuous rate of return follows a log-normal distribution.\nAs a matter of computation efficiency we use here the average and standard deviation of the historical RoR - in lieu of the log returns - as an approximation of the percentage RoR/volatility, and following the Hull's result.\nChallenges we ran into\nOur biggest challenge was to identify the right statistical approach to model volatility while ensuring the best computation efficiency for the module.\nAccomplishments that we're proud of\nWe managed to identify a method which limits the computations and enables us to run in an efficient manner.\nWhat we learned\nWe improved our knowledge of geometric brownian movement and how to translate statistical approach into efficient Solidity code.\nWhat's next for Swaapvol\nSwaapvol's computational efficiency will be further improved in a second version of the tool.\nSwaapvol will be integrated into the first version of Swaap protocol, which is going to be launched end of Q1 2022. Swaap is an impermanent loss resistant DeX, able to handle multiple assets.", "link": "https://devpost.com/software/onchain-volatility-oracle", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe were inspired by the fact that a lot of financial products (e.g., option pricing) or investment decisions (hedging strategies) rely on volatility.\nfurthermore, we are running a bigger project, a dex that is impermanent-loss resistant, that needs this module to work.\nwe decided to build this module for everybody to take advantage of building protocols that use volatility in their design.\nwhat it does\nswaapvol aims to provide an on-chain volatility oracle for any pair of assets individually available as a chainlink data feed. it is running on ethereum (kovan), polygon (mumbai), avalanche (fufi), harmony (testnet) and bsc (testnet).\nhow we built it\nto estimate the percentage rate of return (ror) and volatility of a given pair of assets (e.g. btc/eth), we consider here the simplest stochastic model: \"black & scholes\". in that context, this asset is modeled according to a geometric brownian motion and its continuous rate of return follows a log-normal distribution.\nas a matter of computation efficiency we use here the average and standard deviation of the historical ror - in lieu of the log returns - as an approximation of the percentage ror/volatility, and following the hull's result.\nchallenges we ran into\nour biggest challenge was to identify the right statistical approach to model volatility while ensuring the best computation efficiency for the module.\naccomplishments that we're proud of\nwe managed to identify a method which limits the computations and enables us to run in an efficient manner.\nwhat we learned\nwe improved our knowledge of geometric brownian movement and how to translate statistical approach into efficient solidity code.\nwhat's next for swaapvol\nswaapvol's computational efficiency will be further improved in a second version of the -----> tool !!! .\nswaapvol will be integrated into the first version of swaap protocol, which is going to be launched end of q1 2022. swaap is an impermanent loss resistant dex, able to handle multiple assets.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59504971}, {"Unnamed: 0": 4984, "autor": "Cruize Finance", "date": null, "content": "Inspiration\nThe idea of Cruize was conceived as a tool to counter the unpredictable volatility of the crypto market. While decentralised finance has enabled a global, open and censorship-resistant alternative to the current financial system, widespread adoption is hindered by the erratic fluctuations in the market and the lack of automated tools to hedge against them. Liquidity mining and yield farming have enabled better income opportunities, but the current market dynamics lead to capital inefficiencies and impermanent losses that limit the utility and upside potential of staked assets.\nWe created Cruize to provide investors with an interface that automates their trading strategies to hedge their assets against price drops, as well as automate investments using pre-determined prices. While limit orders are useful as hedging strategies, they remain active only for a limited time and rely on order books that don\u2019t guarantee execution unless orders are matched in time. Cruize creates an experience similar to limit orders but instead uses algorithms to execute orders and offers strategy creation without expiry limits.\nHowever, since staked assets that are hedged remain inefficient as idle capital, we also added the functionality to generate yield on the staked assets. This ensures that assets that are set up for automated strategy execution are also generating passive earnings while remaining exposed to active orders. Investors can therefore \u2018set and forget\u2019 their assets for automated earnings, investing and downside protection.\nWhat it does\nCruize lets investors set automated orders on their assets either as a hedging strategy or to automate timely investments into assets. The interface provides functionality for adding price floors for limiting losses as well as automating buy orders to buy assets at favourable prices.\nThe protocol integrates with DEXes and uses Chainlink price feeds to monitor market prices of assets in real-time and uses algorithms to execute automated trades at set prices. This removes the need for constant price monitoring as well as order matching by using automation to fill orders at set prices.\nProtect. Investors can hedge their assets by setting price limits on their staked tokens and when the market falls to the set limits, the tokens are automatically converted to a reserve of stablecoins that limit losses in value.\nBuy. Investors can also use stablecoins (USDC in the beginning) to set buy orders to invest into assets at dip prices when markets fall or any other set price, that execute automatically without manual intervention.\nWhile assets have active orders, the protocol also routes them to generate yield through aggregated strategies that maximize earnings on assets. Investors can therefore set their assets up for automated earnings and downside protection without any manual intervention.\nHow we built it\nWe built Cruize using popular Web3 technologies like React, Moralis and Web3.js to build the frontend and Solidity, Chainlink, Truffle and Ganache to work on the backend. We also used integrations with popular DeFi applications to streamline the workflow for our protocol.\nWe used Chainlink Price feeds to monitor the real-time asset prices and Chainlink Keepers to set up scheduled jobs. These prices have to be compared with the set price points by investors using Cruize.\nAn integration with a DEX (Uniswap), is used to supply the liquidity needed for conversions during order executions.\nFor yield generation, an integration with Aave provides lending interest on the staked assets by routing them to Aave pools.\nChai, Ganache and Truffle for testing smart contract functionality.\nHere is the technical architecture flowchart of the protocol:\nWhat's next for Cruize Finance\nAfter building out the first demo of our protocol for the hackathon, we will be focusing on restructuring the technical architecture to enable permanent price floors on assets. This will ensure that irrespective of market movements, assets will be exposed to permanent price protection even if asset prices cross the set price floors multiple times before withdrawal. We are working on a model that does this without redundant conversions and saves big on transaction fees and gas costs.\nWe will also be targeting a mainnet launch after some more intensive testing and work on the backend to optimise the workflow. Work on a more detailed white paper and token economics research will also be a main priority.", "link": "https://devpost.com/software/cruize-finance", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nthe idea of cruize was conceived as a -----> tool !!!  to counter the unpredictable volatility of the crypto market. while decentralised finance has enabled a global, open and censorship-resistant alternative to the current financial system, widespread adoption is hindered by the erratic fluctuations in the market and the lack of automated tools to hedge against them. liquidity mining and yield farming have enabled better income opportunities, but the current market dynamics lead to capital inefficiencies and impermanent losses that limit the utility and upside potential of staked assets.\nwe created cruize to provide investors with an interface that automates their trading strategies to hedge their assets against price drops, as well as automate investments using pre-determined prices. while limit orders are useful as hedging strategies, they remain active only for a limited time and rely on order books that don\u2019t guarantee execution unless orders are matched in time. cruize creates an experience similar to limit orders but instead uses algorithms to execute orders and offers strategy creation without expiry limits.\nhowever, since staked assets that are hedged remain inefficient as idle capital, we also added the functionality to generate yield on the staked assets. this ensures that assets that are set up for automated strategy execution are also generating passive earnings while remaining exposed to active orders. investors can therefore \u2018set and forget\u2019 their assets for automated earnings, investing and downside protection.\nwhat it does\ncruize lets investors set automated orders on their assets either as a hedging strategy or to automate timely investments into assets. the interface provides functionality for adding price floors for limiting losses as well as automating buy orders to buy assets at favourable prices.\nthe protocol integrates with dexes and uses chainlink price feeds to monitor market prices of assets in real-time and uses algorithms to execute automated trades at set prices. this removes the need for constant price monitoring as well as order matching by using automation to fill orders at set prices.\nprotect. investors can hedge their assets by setting price limits on their staked tokens and when the market falls to the set limits, the tokens are automatically converted to a reserve of stablecoins that limit losses in value.\nbuy. investors can also use stablecoins (usdc in the beginning) to set buy orders to invest into assets at dip prices when markets fall or any other set price, that execute automatically without manual intervention.\nwhile assets have active orders, the protocol also routes them to generate yield through aggregated strategies that maximize earnings on assets. investors can therefore set their assets up for automated earnings and downside protection without any manual intervention.\nhow we built it\nwe built cruize using popular web3 technologies like react, moralis and web3.js to build the frontend and solidity, chainlink, truffle and ganache to work on the backend. we also used integrations with popular defi applications to streamline the workflow for our protocol.\nwe used chainlink price feeds to monitor the real-time asset prices and chainlink keepers to set up scheduled jobs. these prices have to be compared with the set price points by investors using cruize.\nan integration with a dex (uniswap), is used to supply the liquidity needed for conversions during order executions.\nfor yield generation, an integration with aave provides lending interest on the staked assets by routing them to aave pools.\nchai, ganache and truffle for testing smart contract functionality.\nhere is the technical architecture flowchart of the protocol:\nwhat's next for cruize finance\nafter building out the first demo of our protocol for the hackathon, we will be focusing on restructuring the technical architecture to enable permanent price floors on assets. this will ensure that irrespective of market movements, assets will be exposed to permanent price protection even if asset prices cross the set price floors multiple times before withdrawal. we are working on a model that does this without redundant conversions and saves big on transaction fees and gas costs.\nwe will also be targeting a mainnet launch after some more intensive testing and work on the backend to optimise the workflow. work on a more detailed white paper and token economics research will also be a main priority.", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59504984}, {"Unnamed: 0": 5008, "autor": "nes.city", "date": null, "content": "Inspiration\nWe want to solve the world's biggest core issue from creating a world of peace & prosperity for all: wealth inequality. It is the reason for poor education, health issues, crime, domestic abuse. And wealth inequality is almost always caused by lack of owning real estate.\nWhat it does\nIt's a platform that is based on the idea of turning tenants into co-investors, where as they rent, they are building equity as well. In the end, the only commercially available concept in today's capitalistic society is it being a co-investing social contract tool, where both parties agree to upholding their end of the deal: tenant helps with preventative maintenance, property owner co-invests in the tenants first home they want to buy. This is a platform that builds upon trust already between tenant & property owner, & helps foster more communication toward goals & milestones in order to achieve a common goal: increase wealth together. It shows how much $ the tenant needs in order to get a mortgage to buy a certain home. For a property owner or investor, it shows other properties with the same requests from the tenant that might be worth more in case the co-investor wants to increase the amount they invest, or if they can find another co-investor/tenant to buy in. This includes duplexes. There is a screen for potential preventative maintenance to perform as well.\nHow we built it\nWeb app\nChallenges we ran into\nFinding the right co-founder. Also I initially wanted to turn any rental property into using this model, but 99% of multi-family apartment owners don't care about investing in single family homes or duplexes, even if it would really help their tenants. Many said maybe down the line if there were good financial returns without risk, they might be interested.\nAccomplishments that we're proud of\nbuilding th website\nWhat we learned\nIt's hard to trust someone around the world you don't know to actually dedicate some of their time to something with a slight chance of winning a prize, and unfortunately, many don't care about 'startups' or 'changing the world for the better' because even though there's tons of startups raising $100's of millions a year, it's not enough motivation for many.\nWhat's next for nes.city\nFind the right co-founder, develop the app further, raise money, save the world.", "link": "https://devpost.com/software/nes-city", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe want to solve the world's biggest core issue from creating a world of peace & prosperity for all: wealth inequality. it is the reason for poor education, health issues, crime, domestic abuse. and wealth inequality is almost always caused by lack of owning real estate.\nwhat it does\nit's a platform that is based on the idea of turning tenants into co-investors, where as they rent, they are building equity as well. in the end, the only commercially available concept in today's capitalistic society is it being a co-investing social contract -----> tool !!! , where both parties agree to upholding their end of the deal: tenant helps with preventative maintenance, property owner co-invests in the tenants first home they want to buy. this is a platform that builds upon trust already between tenant & property owner, & helps foster more communication toward goals & milestones in order to achieve a common goal: increase wealth together. it shows how much $ the tenant needs in order to get a mortgage to buy a certain home. for a property owner or investor, it shows other properties with the same requests from the tenant that might be worth more in case the co-investor wants to increase the amount they invest, or if they can find another co-investor/tenant to buy in. this includes duplexes. there is a screen for potential preventative maintenance to perform as well.\nhow we built it\nweb app\nchallenges we ran into\nfinding the right co-founder. also i initially wanted to turn any rental property into using this model, but 99% of multi-family apartment owners don't care about investing in single family homes or duplexes, even if it would really help their tenants. many said maybe down the line if there were good financial returns without risk, they might be interested.\naccomplishments that we're proud of\nbuilding th website\nwhat we learned\nit's hard to trust someone around the world you don't know to actually dedicate some of their time to something with a slight chance of winning a prize, and unfortunately, many don't care about 'startups' or 'changing the world for the better' because even though there's tons of startups raising $100's of millions a year, it's not enough motivation for many.\nwhat's next for nes.city\nfind the right co-founder, develop the app further, raise money, save the world.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59505008}, {"Unnamed: 0": 5009, "autor": "amethyst", "date": null, "content": "Inspiration\nAs you order items from Amazon, a section lower on the screen suggests other items that might be of interest. Similarly, your video-viewing choices on Netflix influence the videos suggested to you for future viewing.\nAll this is possible because of the availability of data-collection systems and improvements in recommendation engine algorithms.\nThere are a few ways to deal with the challenge of designing recommendation engines. One is to have your own team of engineers and data scientists, all highly trained in machine learning, to custom design recommenders to meet your needs.\nHowever, this approach is not feasible for smaller companies and startups as it is quite resource-intensive and might not be suitable in the initial stages.\nTo tackle this problem, we built a tool that will help developers who do not have a machine learning background, easily build and deploy a recommendation engine in their application or product.\nWhat it does\nAmethyst is a low-code, easy-to-use, GPU-powered recommender engine generator based on PyTorch. It requires only three parameters to rank/predict the best items for users and vice-versa\nUser ID (a unique identifier for each user) Item ID (a unique identifier for each item) User-Item Ratings (user-item rating/interaction scores) Since all the underlying data operations are being handled by Pandas, amethyst supports a wide variety of database/data storage formats like SQL, NoSQL, CSV, TSV, etc.\nThe resultant recommendation scores are also obtained as a Pandas Dataframe, which helps in a flexible integration with your application.\nGetting Started\nThis is an example of how you can generate your own collaborative recommendation engine. To get a local copy up and running follow these simple example steps.\nPrerequisites\nPython>=3.7\nInstallation\nClone the repo sh git clone https://github.com/radioactive11/amethyst.git\nCreate and activate virtual environment sh python3 -m venv venv source venv/bin/activate\nInstall the tool sh python3 setup.py install\n(back to top)\nUsage\nA recommendation engine can be generated in 4 easy steps:\nImport the data\nSelect an algorithm\nTrain the model\nEvaluate the model's performance\nData Split \u2697\ufe0f\nfrom amethyst.dataloader import split\ndf = pd.read_csv(\"./movielens100k.csv\")\ndf_train, df_test = split.stratified_split(\ndf,\n0.8,\nuser_col='userID',\nitem_col='itemID',\nfilter_col='item'\n)\nLoad Data \ud83d\udce5\nfrom amethyst.dataloader import dataset\ndf = pd.read_csv(\"movielens100k.csv\")\n# from Data Split\ndf_train, df_test = split.stratified_split(df)\ntrain = dataset.Dataloader.dataloader(df_train.itertuples(index=False))\ntest = dataset.Dataloader.dataloader(df_test.itertuples(index=False))\nTrain (BiVAECF) \u2699\ufe0f\nfrom amethyst.models.bivaecf.bivaecf import BiVAECF\nimport torch\nbivae = BiVAECF(\nk=50,\nencoder_structure=[100],\nact_fn=[\"tanh\"],\nlikelihood=\"pois\",\nn_epochs=500,\nbatch_size=256,\nlearning_rate=0.001,\nseed=42,\nuse_gpu=torch.cuda.is_available(),\nverbose=True\n)\nbivae.fit(train, test)\nbivae.save(\"model.pkl\")\nTrain (IBPR) \u2699\ufe0f\nfrom amethyst.models.ibpr.ibprcf import IBPR\nimport torch\nibpr = IBPR(\nk=20,\nmax_iter=100,\nalpha_=0.05,\nlambda_=0.001,\nbatch_size=100,\ntrainable=True,\nverbose=False,\ninit_params=None)\nibpr.fit(train, test)\nibpr.save(\"model.pkl\")\nPredict/Rank \ud83d\udcc8\nfrom amethyst.models.predictions import rank\nfrom amethyst.models.bivaecf.bivaecf import BiVAECF\nbivae = BiVAECF(\nk=50,\nencoder_structure=[100],\nact_fn=[\"tanh\"],\nlikelihood=\"pois\",\nn_epochs=500,\nbatch_size=256,\nlearning_rate=0.001,\nseed=42,\nuse_gpu=torch.cuda.is_available(),\nverbose=True\n)\nbivae.load(\"mode.pkl\")\npredictions = rank(bivae, test, user_col='userID', item_col='itemID')\n# predictions is a Pandas Dataframe\npredictions.to_csv(\"predictions.csv\", index=False)\nEvaluate \ud83d\udcc8\nfrom amethyst.models.predictions import rank\nfrom amethyst.eval.eval_methods import map_at_k, precision_at_k, recall_k\nbivae = BiVAECF(\nk=50,\nencoder_structure=[100],\nact_fn=[\"tanh\"],\nlikelihood=\"pois\",\nn_epochs=500,\nbatch_size=256,\nlearning_rate=0.001,\nseed=42,\nuse_gpu=torch.cuda.is_available(),\nverbose=True\n)\nbivae.load(\"mode.pkl\")\npredictions = rank(bivae, test, user_col='userID', item_col='itemID')\neval_map = map_at_k(test, predictions, k=10)\npk = precision_at_k(test, predictions, k=10)\nrk = recall_k(test, predictions)\n(back to top)\nAcknowledgments\nBiVAECF\nIBPR\n(back to top)", "link": "https://devpost.com/software/amethyst-pmc58o", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nas you order items from amazon, a section lower on the screen suggests other items that might be of interest. similarly, your video-viewing choices on netflix influence the videos suggested to you for future viewing.\nall this is possible because of the availability of data-collection systems and improvements in recommendation engine algorithms.\nthere are a few ways to deal with the challenge of designing recommendation engines. one is to have your own team of engineers and data scientists, all highly trained in machine learning, to custom design recommenders to meet your needs.\nhowever, this approach is not feasible for smaller companies and startups as it is quite resource-intensive and might not be suitable in the initial stages.\nto tackle this problem, we built a -----> tool !!!  that will help developers who do not have a machine learning background, easily build and deploy a recommendation engine in their application or product.\nwhat it does\namethyst is a low-code, easy-to-use, gpu-powered recommender engine generator based on pytorch. it requires only three parameters to rank/predict the best items for users and vice-versa\nuser id (a unique identifier for each user) item id (a unique identifier for each item) user-item ratings (user-item rating/interaction scores) since all the underlying data operations are being handled by pandas, amethyst supports a wide variety of database/data storage formats like sql, nosql, csv, tsv, etc.\nthe resultant recommendation scores are also obtained as a pandas dataframe, which helps in a flexible integration with your application.\ngetting started\nthis is an example of how you can generate your own collaborative recommendation engine. to get a local copy up and running follow these simple example steps.\nprerequisites\npython>=3.7\ninstallation\nclone the repo sh git clone https://github.com/radioactive11/amethyst.git\ncreate and activate virtual environment sh python3 -m venv venv source venv/bin/activate\ninstall the tool sh python3 setup.py install\n(back to top)\nusage\na recommendation engine can be generated in 4 easy steps:\nimport the data\nselect an algorithm\ntrain the model\nevaluate the model's performance\ndata split \u2697\ufe0f\nfrom amethyst.dataloader import split\ndf = pd.read_csv(\"./movielens100k.csv\")\ndf_train, df_test = split.stratified_split(\ndf,\n0.8,\nuser_col='userid',\nitem_col='itemid',\nfilter_col='item'\n)\nload data \ud83d\udce5\nfrom amethyst.dataloader import dataset\ndf = pd.read_csv(\"movielens100k.csv\")\n# from data split\ndf_train, df_test = split.stratified_split(df)\ntrain = dataset.dataloader.dataloader(df_train.itertuples(index=false))\ntest = dataset.dataloader.dataloader(df_test.itertuples(index=false))\ntrain (bivaecf) \u2699\ufe0f\nfrom amethyst.models.bivaecf.bivaecf import bivaecf\nimport torch\nbivae = bivaecf(\nk=50,\nencoder_structure=[100],\nact_fn=[\"tanh\"],\nlikelihood=\"pois\",\nn_epochs=500,\nbatch_size=256,\nlearning_rate=0.001,\nseed=42,\nuse_gpu=torch.cuda.is_available(),\nverbose=true\n)\nbivae.fit(train, test)\nbivae.save(\"model.pkl\")\ntrain (ibpr) \u2699\ufe0f\nfrom amethyst.models.ibpr.ibprcf import ibpr\nimport torch\nibpr = ibpr(\nk=20,\nmax_iter=100,\nalpha_=0.05,\nlambda_=0.001,\nbatch_size=100,\ntrainable=true,\nverbose=false,\ninit_params=none)\nibpr.fit(train, test)\nibpr.save(\"model.pkl\")\npredict/rank \ud83d\udcc8\nfrom amethyst.models.predictions import rank\nfrom amethyst.models.bivaecf.bivaecf import bivaecf\nbivae = bivaecf(\nk=50,\nencoder_structure=[100],\nact_fn=[\"tanh\"],\nlikelihood=\"pois\",\nn_epochs=500,\nbatch_size=256,\nlearning_rate=0.001,\nseed=42,\nuse_gpu=torch.cuda.is_available(),\nverbose=true\n)\nbivae.load(\"mode.pkl\")\npredictions = rank(bivae, test, user_col='userid', item_col='itemid')\n# predictions is a pandas dataframe\npredictions.to_csv(\"predictions.csv\", index=false)\nevaluate \ud83d\udcc8\nfrom amethyst.models.predictions import rank\nfrom amethyst.eval.eval_methods import map_at_k, precision_at_k, recall_k\nbivae = bivaecf(\nk=50,\nencoder_structure=[100],\nact_fn=[\"tanh\"],\nlikelihood=\"pois\",\nn_epochs=500,\nbatch_size=256,\nlearning_rate=0.001,\nseed=42,\nuse_gpu=torch.cuda.is_available(),\nverbose=true\n)\nbivae.load(\"mode.pkl\")\npredictions = rank(bivae, test, user_col='userid', item_col='itemid')\neval_map = map_at_k(test, predictions, k=10)\npk = precision_at_k(test, predictions, k=10)\nrk = recall_k(test, predictions)\n(back to top)\nacknowledgments\nbivaecf\nibpr\n(back to top)", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59505009}, {"Unnamed: 0": 5010, "autor": "yyds", "date": null, "content": "See the full write up here: https://www.notion.so/particulars/HACC-2021-85f660156040489d940a30ab9a467c90\nA light-weight reservation system\nThe basic idea is to get pet owners to register their pet via an online form. The admins at the Animal Quarantine office can then manage the status of that pet, and owners will automatically get texted when their pet is registered, received and, most importantly, ready for pick up.\nTo make the process smoother, we also added a workflow for the inspectors who screen the animals. If all the data is in the same place, it would reduce labor on the front office team and result in faster responses back to the owner when their pet is ready for pickup.\nStandard User Flow\nOwner (or admin) registers pet via the online form \ud83e\udd16\u26a1\ufe0f\nOwner drops pet off for processing \ud83d\udc15\nAdmin checks the pet in as \"Received\" \ud83e\udd16\u26a1\ufe0f\nInspectors use the online form to screen each pet and approve them for release \ud83e\udd16\nAdmin updates the pet's status to \"Ready for Pickup\" \ud83e\udd16\u26a1\ufe0f\nOwner comes back to pick up their pet \ud83d\udc15\nAdmin marks pet as \"Released\" when pet is returned to owner \ud83e\udd16\n\ud83e\udd16 Task completed using our tool\n\ud83d\udc15 Task requires owner to enter the Animal Quarantine office\n\u26a1 Automated text message sent to owner", "link": "https://devpost.com/software/yyds", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "see the full write up here: https://www.notion.so/particulars/hacc-2021-85f660156040489d940a30ab9a467c90\na light-weight reservation system\nthe basic idea is to get pet owners to register their pet via an online form. the admins at the animal quarantine office can then manage the status of that pet, and owners will automatically get texted when their pet is registered, received and, most importantly, ready for pick up.\nto make the process smoother, we also added a workflow for the inspectors who screen the animals. if all the data is in the same place, it would reduce labor on the front office team and result in faster responses back to the owner when their pet is ready for pickup.\nstandard user flow\nowner (or admin) registers pet via the online form \ud83e\udd16\u26a1\ufe0f\nowner drops pet off for processing \ud83d\udc15\nadmin checks the pet in as \"received\" \ud83e\udd16\u26a1\ufe0f\ninspectors use the online form to screen each pet and approve them for release \ud83e\udd16\nadmin updates the pet's status to \"ready for pickup\" \ud83e\udd16\u26a1\ufe0f\nowner comes back to pick up their pet \ud83d\udc15\nadmin marks pet as \"released\" when pet is returned to owner \ud83e\udd16\n\ud83e\udd16 task completed using our -----> tool !!! \n\ud83d\udc15 task requires owner to enter the animal quarantine office\n\u26a1 automated text message sent to owner", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59505010}, {"Unnamed: 0": 5017, "autor": "Astrotherapy", "date": null, "content": "Inspiration\nAstronauts on long missions, especially on the ISS, tend to face isolation and loneliness. We wanted to create a tool that could help improve their mental health.\nWhat it does\nOur software analyzes astronauts' journal entries for positive and negative sentiments and generates a sentiment score. The results are then sent to certified therapists and/or researchers to best address their emotional needs throughout the space journey. Lastly, the sentiments would be logged and analyzed by researchers to better prepare for obstacles in future space missions.\nHow we built it\nWe created a web app using react, a backend API using flask, and a sentiment analysis program using Google Cloud's Natural Language API.\nChallenges we ran into\nThe backend server was difficult to implement and connect to the frontend.\nAccomplishments that we're proud of\nWe're proud that we were able to finish a high-fidelity prototype of an ambitious project in 24 hours!\nWhat we learned\nWe learned react, flask, bash, frontend development\nWhat's next for Astrotherapy\nInstead of directly showing sentiment scores when an astronaut submits, we would want to show a success message and send the scores to a therapist for analysis.", "link": "https://devpost.com/software/astrotherapy", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nastronauts on long missions, especially on the iss, tend to face isolation and loneliness. we wanted to create a -----> tool !!!  that could help improve their mental health.\nwhat it does\nour software analyzes astronauts' journal entries for positive and negative sentiments and generates a sentiment score. the results are then sent to certified therapists and/or researchers to best address their emotional needs throughout the space journey. lastly, the sentiments would be logged and analyzed by researchers to better prepare for obstacles in future space missions.\nhow we built it\nwe created a web app using react, a backend api using flask, and a sentiment analysis program using google cloud's natural language api.\nchallenges we ran into\nthe backend server was difficult to implement and connect to the frontend.\naccomplishments that we're proud of\nwe're proud that we were able to finish a high-fidelity prototype of an ambitious project in 24 hours!\nwhat we learned\nwe learned react, flask, bash, frontend development\nwhat's next for astrotherapy\ninstead of directly showing sentiment scores when an astronaut submits, we would want to show a success message and send the scores to a therapist for analysis.", "sortedWord": "None", "removed": "Nan", "score": 5, "comments": 2, "media": null, "medialink": null, "identifyer": 59505017}, {"Unnamed: 0": 5029, "autor": "Pizza Bros", "date": null, "content": "Inspiration\nThere's a news article about a woman who was able to report domestic violence by placing a phone order for pizza. We realized how much potential there was in disguising an emergency services app as something as simple as a fast-food app.\nWhat it does\nAt first glance, it looks like an app that you download for a local pizza chain. That is, it really isn't possible to tell that the app doesn't belong to the chain 'Pizza Bros'.\nHow I built it\nWe decided to use native react due to the ease of being able to integrate to both ios and android. We also felt that it would be fun to practice what some consider a popular tool in the industry with regard to web development.\nChallenges I ran into\nSince native react isn't exactly like React, it took us a few hours to install and understand what was needed. We also didn't have enough time to implement that backend using Twilio's API and Node.js or express.\nAccomplishments that I'm proud of\nIt was cool to see our code come to life on our phones. Also being able to get a decent-looking mock-up impressed us, given the amount of time we had.\nWhat I learned\nWe learned that React could also be used for mobile development.\nWhat's next for Pizza Bros\nSince we were able to get credits for Twilio, we'll try to implement the backend with Node.js. Although we might not have coded enough to perform well in this contest, we feel that there's potential for this idea to be impactful in the long run. Therefore, we'll try to develop a beta and see whether we have a viable product, or if it was simply a fun project to work on.", "link": "https://devpost.com/software/pizza-bros", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nthere's a news article about a woman who was able to report domestic violence by placing a phone order for pizza. we realized how much potential there was in disguising an emergency services app as something as simple as a fast-food app.\nwhat it does\nat first glance, it looks like an app that you download for a local pizza chain. that is, it really isn't possible to tell that the app doesn't belong to the chain 'pizza bros'.\nhow i built it\nwe decided to use native react due to the ease of being able to integrate to both ios and android. we also felt that it would be fun to practice what some consider a popular -----> tool !!!  in the industry with regard to web development.\nchallenges i ran into\nsince native react isn't exactly like react, it took us a few hours to install and understand what was needed. we also didn't have enough time to implement that backend using twilio's api and node.js or express.\naccomplishments that i'm proud of\nit was cool to see our code come to life on our phones. also being able to get a decent-looking mock-up impressed us, given the amount of time we had.\nwhat i learned\nwe learned that react could also be used for mobile development.\nwhat's next for pizza bros\nsince we were able to get credits for twilio, we'll try to implement the backend with node.js. although we might not have coded enough to perform well in this contest, we feel that there's potential for this idea to be impactful in the long run. therefore, we'll try to develop a beta and see whether we have a viable product, or if it was simply a fun project to work on.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59505029}, {"Unnamed: 0": 5034, "autor": "sonus.ai", "date": null, "content": "sonus.ai\nInspiration\nWe were inspired to create Sonus.ai after realizing that an organization shouldn't have their highest level of support occupied with already content customers. Rather, their best agents should attend to customers who feel angry, dejected, or discontent. By being able to determine a customer\u2019s emotion, the organization's customer service becomes more efficient and fulfills the service\u2019s primary goal with minimum effort from both sides. Furthermore, Sonus has a wide-range of applicability owing to the fact that it can be used by hotlines that seek to provide crisis counseling and mental health referrals. With Sonus, hotline call centers can resolve difficulties faster by using a patient\u2019s emotions as a navigating tool.\nWhat it does\nSonus is a customer success management tool that is designed to be used in conjuction with IVR (voice chatbot) systems. It analyzes human emotion in real time and can route angry & agitated customers to human representatives while allowing fulfilled & happy customers to continue to interface with the IVR system.\nHow we built it\nArchitecture\nSonus is built using a React frontend and a Python backend. We use MongoDB to store data and Google Cloud Storage to store audio & transcript data. We also trained several machine learning models using white papers and data we found online.\nWorkflow\nTwilio Webhook\nA client will call our phone number and Twilio will post a webhook to our server. After the call ends, we are able to stream the outputs of the call into Google Cloud Storage where it will then be sent to our various models for classification.\nSpeech to Text Transcription\nWe use Google's Speech to Text API to convert the audio stream into text. We then save this information into MongoDB for further analysis\nText Based Sentiment Analysis\nWe implemented a naive bayes classification algorithm that was trained on text data from customer reviews. The model achieved an 86% accuracy rate in classifying whether or not the customer dialogue was positive or negative.\nAudio Convolutional Neural Network\nWe created our CNN based off a whitepaper we read online\nWe converted our training audio into spectrograms, which are image based representations of audio files based on frequency and time\nChallenges we ran into\nWe had trouble running our model since most people in our group had M1 Macbooks with inadequate tensorflow support. We also had trouble with docker dependencies and modules not installing properly.\nAccomplishments that we're proud of\nThis was everyone in our group\u2019s first time dealing with Machine Learning, and we were extremely happy that we were able to come together and tackle this challenge head on!\nWhat we learned\nWe learned about how to train a model on a large scale dataset and how to accurately render components using React!\nWhat's next for sonus.ai\nWe hope to focus more on further improving the accuracy of our model and putting it through other datasets we come across!", "link": "https://devpost.com/software/sonus-ai", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "sonus.ai\ninspiration\nwe were inspired to create sonus.ai after realizing that an organization shouldn't have their highest level of support occupied with already content customers. rather, their best agents should attend to customers who feel angry, dejected, or discontent. by being able to determine a customer\u2019s emotion, the organization's customer service becomes more efficient and fulfills the service\u2019s primary goal with minimum effort from both sides. furthermore, sonus has a wide-range of applicability owing to the fact that it can be used by hotlines that seek to provide crisis counseling and mental health referrals. with sonus, hotline call centers can resolve difficulties faster by using a patient\u2019s emotions as a navigating -----> tool !!! .\nwhat it does\nsonus is a customer success management tool that is designed to be used in conjuction with ivr (voice chatbot) systems. it analyzes human emotion in real time and can route angry & agitated customers to human representatives while allowing fulfilled & happy customers to continue to interface with the ivr system.\nhow we built it\narchitecture\nsonus is built using a react frontend and a python backend. we use mongodb to store data and google cloud storage to store audio & transcript data. we also trained several machine learning models using white papers and data we found online.\nworkflow\ntwilio webhook\na client will call our phone number and twilio will post a webhook to our server. after the call ends, we are able to stream the outputs of the call into google cloud storage where it will then be sent to our various models for classification.\nspeech to text transcription\nwe use google's speech to text api to convert the audio stream into text. we then save this information into mongodb for further analysis\ntext based sentiment analysis\nwe implemented a naive bayes classification algorithm that was trained on text data from customer reviews. the model achieved an 86% accuracy rate in classifying whether or not the customer dialogue was positive or negative.\naudio convolutional neural network\nwe created our cnn based off a whitepaper we read online\nwe converted our training audio into spectrograms, which are image based representations of audio files based on frequency and time\nchallenges we ran into\nwe had trouble running our model since most people in our group had m1 macbooks with inadequate tensorflow support. we also had trouble with docker dependencies and modules not installing properly.\naccomplishments that we're proud of\nthis was everyone in our group\u2019s first time dealing with machine learning, and we were extremely happy that we were able to come together and tackle this challenge head on!\nwhat we learned\nwe learned about how to train a model on a large scale dataset and how to accurately render components using react!\nwhat's next for sonus.ai\nwe hope to focus more on further improving the accuracy of our model and putting it through other datasets we come across!", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59505034}, {"Unnamed: 0": 5050, "autor": "QuizUp", "date": null, "content": "Inspiration\nWe like quiz games, so we figured why not mock up a quiz game of our own?\nWhat it does\nIt lets you play quick 5 question quizzes pertaining to two topics: HackTX and food. Currently the HackTX topic is fully complete.\nHow we built it\nThe game was made entirely as an interactive prototype with Figma, and Material Design kits were used to make it look more a lot more presentable.\nChallenges we ran into\nThe team had no prior experience using Figma, so the major challenge was to get acquainted with using the tool to create the game design.\nAccomplishments that we're proud of\nWe're proud of being able to have made a completely playable topic complete with correct/incorrect answers and a nice design.\nWhat we learned\nOur team learned about using Figma to create wireframe mockups & prototypes of apps and games, and we also learned how to incorporate design standards like Material Design into our own designs.", "link": "https://devpost.com/software/quizup", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe like quiz games, so we figured why not mock up a quiz game of our own?\nwhat it does\nit lets you play quick 5 question quizzes pertaining to two topics: hacktx and food. currently the hacktx topic is fully complete.\nhow we built it\nthe game was made entirely as an interactive prototype with figma, and material design kits were used to make it look more a lot more presentable.\nchallenges we ran into\nthe team had no prior experience using figma, so the major challenge was to get acquainted with using the -----> tool !!!  to create the game design.\naccomplishments that we're proud of\nwe're proud of being able to have made a completely playable topic complete with correct/incorrect answers and a nice design.\nwhat we learned\nour team learned about using figma to create wireframe mockups & prototypes of apps and games, and we also learned how to incorporate design standards like material design into our own designs.", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59505050}, {"Unnamed: 0": 5053, "autor": "NetBridge", "date": null, "content": "Inspiration\nTrivago is an aggregator for hotels, helping travelers get the best deal on a place to stay. However, in these times, most of us aren't doing much traveling. This means the Internet has become a necessity\u2014no longer a luxury. Why isn't there a tool to help us find the best Internet Service Provider for our home? Meet NetBridge.\nWhat it does\nNetBridge is a data aggregator that finds all the ISPs available to you and matches you with the best one based on price per Mbps and user-provided ratings such as quality of customer service. As users provide feedback on their experiences, our information improves and NetBridge provides better suggestions. ISPs will use NetBridge to identify regions where their service is weak so they can focus on improving these areas. Once a better option becomes available, NetBridge users in the area will be notified.\nHow we built it\nThe FCC publishes the biannual Form 477 registration information of all Internet Service Providers that offer speeds of at least 200 kpbs. This dataset is very large at 73.5M rows; working with this data was quite cumbersome.\nDatabase\nThe FCC database is stored in Cloud SQL.\nAPI\nUsers provide their address, which our API uses to make a request to the US Census Geocoding API to find the FIPS census block code that this address is located in. Once this block code is found, our API queries the database for any entries that match this block code. These entries are returned to the user as the possible ISPs in the area.\nChallenges we ran into\nWe are a team of 4 beginners and 1 intermediate. None of us have any experience with GCP, so building an app on it was quite a challenge.\nWorking with such a large dataset was also challenging. The dataset is a 10.7GB .csv file with 73.5M rows. Importing this into a database was one of the biggest challenges.\nWe initially tried to use CockroachDB as the database, but could not get access to external network resources.\nInstead, we uploaded the file directly to Cloud SQL. This took some time to complete, but was successful.\nAccomplishments that we're proud of\nWhat we learned\nCloud Computing (GCP)\nDatabase administration\nAPI usage (geocoding)\nWhat's next for NetBridge", "link": "https://devpost.com/software/netdrift", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ntrivago is an aggregator for hotels, helping travelers get the best deal on a place to stay. however, in these times, most of us aren't doing much traveling. this means the internet has become a necessity\u2014no longer a luxury. why isn't there a -----> tool !!!  to help us find the best internet service provider for our home? meet netbridge.\nwhat it does\nnetbridge is a data aggregator that finds all the isps available to you and matches you with the best one based on price per mbps and user-provided ratings such as quality of customer service. as users provide feedback on their experiences, our information improves and netbridge provides better suggestions. isps will use netbridge to identify regions where their service is weak so they can focus on improving these areas. once a better option becomes available, netbridge users in the area will be notified.\nhow we built it\nthe fcc publishes the biannual form 477 registration information of all internet service providers that offer speeds of at least 200 kpbs. this dataset is very large at 73.5m rows; working with this data was quite cumbersome.\ndatabase\nthe fcc database is stored in cloud sql.\napi\nusers provide their address, which our api uses to make a request to the us census geocoding api to find the fips census block code that this address is located in. once this block code is found, our api queries the database for any entries that match this block code. these entries are returned to the user as the possible isps in the area.\nchallenges we ran into\nwe are a team of 4 beginners and 1 intermediate. none of us have any experience with gcp, so building an app on it was quite a challenge.\nworking with such a large dataset was also challenging. the dataset is a 10.7gb .csv file with 73.5m rows. importing this into a database was one of the biggest challenges.\nwe initially tried to use cockroachdb as the database, but could not get access to external network resources.\ninstead, we uploaded the file directly to cloud sql. this took some time to complete, but was successful.\naccomplishments that we're proud of\nwhat we learned\ncloud computing (gcp)\ndatabase administration\napi usage (geocoding)\nwhat's next for netbridge", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59505053}, {"Unnamed: 0": 5068, "autor": "Sat Map", "date": null, "content": "Inspiration\nWhen I saw the category for a space related app, I immediately knew that that was what I wanted to do because I have always been interested in space and more specifically near Earth space travel. I like to gaze up at the stars and ponder about the universe and it is always a treat to catch a glimpse of a satellite passing, however its very difficult to plan a view experience for something like that, so I decided to make an easy to use tool to find out when you can see satellites passing.\nWhat it does\nMy project is an interactive map of all active satellites orbiting Earth. When the webpage loads it queries Celestrak's database for their last updated measurements. It then calculates the current positions using and mathematical model called SGP4 and displays them on the webpage. Once loaded, the user can pan around to view satellites in different areas or click a satellite to find out more info and find when it will next be visible in your location.\nHow I built it\nThe project is built using C++ as a backend for a website and Wt as a library for converting the data to HTML, CSS, and JS for the client. The server sends a web request to Celestrak's database and retrieves the TLE info for all the active satellites. It then parses them and converts them to SGP4 data. The SGP4 data is used to propagate the satellite positions to the current time. These positions are then transformed from 3D space to 2D space and pushed to the client to render. Curl & Curlcpp are used for making the web requests and and SGP4 library called \"SGP4 Library\" is used for the SGP4 calculations. I would have used my own but I only half finished writing it in the time.\nChallenges I ran into\nThe first major challenge was my inability to complete the SGP4 library. I got it to the point of where it could propagate the satellite info but it was only accurate about 90% of the time due to what was probably an overflow and would probably be fine in 64 bit. I ended up using a publicly available library for this but the half completed code will be on the GitHub still.\nAnother challenge was making the map interactive. In order to do this I had to write a 3D transform function which I had no experience in and was too stubborn to just look up. I ended up doing all the math myself and got some pretty crazy looking results in the meantime.\nAccomplishments that I am proud of\nHonestly I am just proud I completed it because there was a large portion of time towards the beginning where it looked like I wasn't going to have anything to show for the time because everything was going wrong. Curl & Curlcpp almost three hours to set up because all the builds were out of date and the build files they came with didn't work, I couldn't figure out how to draw on Wt and the SGP4 propagation was not right.\nAlso, I am proud of the 3D transform.\nWhat I learned\nI learned about the standards for satellite data (TLE) and orbit propagation (SGP) which despite being very interested in space, I had never even heard of. I learned how to use Wt which is very useful and I will most likely add it to future projects.\nWhat's next for Sat Map\nDesired Features\nView the past of future positions of satellites.\nHave a texture of Earth rather than a blue sphere to aid in positional understanding\nDraw orbit lines\nPredict possible collisions\nSearch and filter feature", "link": "https://devpost.com/software/sat-map", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwhen i saw the category for a space related app, i immediately knew that that was what i wanted to do because i have always been interested in space and more specifically near earth space travel. i like to gaze up at the stars and ponder about the universe and it is always a treat to catch a glimpse of a satellite passing, however its very difficult to plan a view experience for something like that, so i decided to make an easy to use -----> tool !!!  to find out when you can see satellites passing.\nwhat it does\nmy project is an interactive map of all active satellites orbiting earth. when the webpage loads it queries celestrak's database for their last updated measurements. it then calculates the current positions using and mathematical model called sgp4 and displays them on the webpage. once loaded, the user can pan around to view satellites in different areas or click a satellite to find out more info and find when it will next be visible in your location.\nhow i built it\nthe project is built using c++ as a backend for a website and wt as a library for converting the data to html, css, and js for the client. the server sends a web request to celestrak's database and retrieves the tle info for all the active satellites. it then parses them and converts them to sgp4 data. the sgp4 data is used to propagate the satellite positions to the current time. these positions are then transformed from 3d space to 2d space and pushed to the client to render. curl & curlcpp are used for making the web requests and and sgp4 library called \"sgp4 library\" is used for the sgp4 calculations. i would have used my own but i only half finished writing it in the time.\nchallenges i ran into\nthe first major challenge was my inability to complete the sgp4 library. i got it to the point of where it could propagate the satellite info but it was only accurate about 90% of the time due to what was probably an overflow and would probably be fine in 64 bit. i ended up using a publicly available library for this but the half completed code will be on the github still.\nanother challenge was making the map interactive. in order to do this i had to write a 3d transform function which i had no experience in and was too stubborn to just look up. i ended up doing all the math myself and got some pretty crazy looking results in the meantime.\naccomplishments that i am proud of\nhonestly i am just proud i completed it because there was a large portion of time towards the beginning where it looked like i wasn't going to have anything to show for the time because everything was going wrong. curl & curlcpp almost three hours to set up because all the builds were out of date and the build files they came with didn't work, i couldn't figure out how to draw on wt and the sgp4 propagation was not right.\nalso, i am proud of the 3d transform.\nwhat i learned\ni learned about the standards for satellite data (tle) and orbit propagation (sgp) which despite being very interested in space, i had never even heard of. i learned how to use wt which is very useful and i will most likely add it to future projects.\nwhat's next for sat map\ndesired features\nview the past of future positions of satellites.\nhave a texture of earth rather than a blue sphere to aid in positional understanding\ndraw orbit lines\npredict possible collisions\nsearch and filter feature", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59505068}, {"Unnamed: 0": 5117, "autor": "Comicly", "date": null, "content": "Comicly : Our Aim is to get smiles on face of every person visiting our website!\nInspiration\nA good laugh has great short-term effects. When you start to laugh, it doesn't just lighten your load mentally, it actually induces physical changes in your body. And afterall at times, Laughter is the best medicine! Our project is an initiative to bring smiles on all faces, in all colors of life, together!\nOur Goal\nComicly is an interactive tool to bring smile on anyone's face! Our mission is to spread smile, laughter and happiness everywhere!\nWhat it does\nOur Key Features:\n\ud83d\ude0a Meme Generator\nUser can generate a mene, using an image and text and further share the meme with their friends. This not only helps us make the user happy, but also helps us in sharing happiness, spreading smiles across every meme share!\n\ud83d\ude0a Guess the Gibberish\nIt's a super fun game to guess what the text says! Correctly guess what the gibberish text says!\n\ud83d\ude0a Jokes:\nA daily dose of jokes for everyone! A feature where user can get random jokes. Bring smiles on their face everyday!\nHow we built it\nWe built it using React and Firebase.\nChallenges we ran into\nImplementing all the features in the given time constraint was one of the major challenges for us! Managing our time, the right way helped us a lot in successfully building the product. It was an amazing learning experience for us overall!\nAccomplishments that we're proud of\nWe are proud of overcoming our challenges and building the application.\nWhat we learned\nWe got to know that it's not that of an easy task to make anyone smile, got to know a lot of interactive ways to spread happiness and laughs! Also learned a lot through the various obstacles we came across while building it!\nWhat's next for Comicly\nAdding more games, a ranklist for the pointlist and many other amazing features also to be added!\nSo lets Comicly!", "link": "https://devpost.com/software/comicly", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "comicly : our aim is to get smiles on face of every person visiting our website!\ninspiration\na good laugh has great short-term effects. when you start to laugh, it doesn't just lighten your load mentally, it actually induces physical changes in your body. and afterall at times, laughter is the best medicine! our project is an initiative to bring smiles on all faces, in all colors of life, together!\nour goal\ncomicly is an interactive -----> tool !!!  to bring smile on anyone's face! our mission is to spread smile, laughter and happiness everywhere!\nwhat it does\nour key features:\n\ud83d\ude0a meme generator\nuser can generate a mene, using an image and text and further share the meme with their friends. this not only helps us make the user happy, but also helps us in sharing happiness, spreading smiles across every meme share!\n\ud83d\ude0a guess the gibberish\nit's a super fun game to guess what the text says! correctly guess what the gibberish text says!\n\ud83d\ude0a jokes:\na daily dose of jokes for everyone! a feature where user can get random jokes. bring smiles on their face everyday!\nhow we built it\nwe built it using react and firebase.\nchallenges we ran into\nimplementing all the features in the given time constraint was one of the major challenges for us! managing our time, the right way helped us a lot in successfully building the product. it was an amazing learning experience for us overall!\naccomplishments that we're proud of\nwe are proud of overcoming our challenges and building the application.\nwhat we learned\nwe got to know that it's not that of an easy task to make anyone smile, got to know a lot of interactive ways to spread happiness and laughs! also learned a lot through the various obstacles we came across while building it!\nwhat's next for comicly\nadding more games, a ranklist for the pointlist and many other amazing features also to be added!\nso lets comicly!", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59505117}, {"Unnamed: 0": 5151, "autor": "SizeUp!", "date": null, "content": "Inspiration\nA big inspiration to our app was Nike Run Club where you can compete against your friends for running the most miles. In addition to that, we all like to stay active and fit (as much as we can lol) and this would be something we could add to our use.\nWhat it does\nOur app creates a suite where users are able to connect with friends while also managing their own workouts from within the app. It also allows users to calculate based on weight, height, etc statistics for maintaining, gaining, or losing weight.\nHow we built it\nWe built our app using the python library, streamlit. We did this to challenge us in using a library unknown to us while also putting us into a category for the competition. We also incorporated the use of SQLite3 which is a built-in databasing tool for python to store our workouts.\nChallenges we ran into\nA big challenge we ran into with streamlit was not being able to have adaptive URLs similar to that of Python Flask or Django and it seemed to have only one static webpage and we had to adapt to this constraint. Additionally, streamlit is based in python, so it became hard to develop your own webpage using HTML, CSS, and JS which aren't by default supported in Python.\nAccomplishments that we're proud of\nThis being our hackathon for all but one of our hackers, to have a mostly complete final product is something we are proud of. We also have not had too much experience with programming production level products and most things stay on the backend so this was a change of tempo for us.\nWhat we learned\nAs a team, we learned to adapt our ideas into working models based on the constraints placed upon us by the libraries we were using.\nWhat's next for SizeUp!\nWe plan to use the model created for this hackathon as a template and stepping stone for creating a full-blown application for ios and android devices probably within Swift or Java.", "link": "https://devpost.com/software/sizeup-xl4kqi", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\na big inspiration to our app was nike run club where you can compete against your friends for running the most miles. in addition to that, we all like to stay active and fit (as much as we can lol) and this would be something we could add to our use.\nwhat it does\nour app creates a suite where users are able to connect with friends while also managing their own workouts from within the app. it also allows users to calculate based on weight, height, etc statistics for maintaining, gaining, or losing weight.\nhow we built it\nwe built our app using the python library, streamlit. we did this to challenge us in using a library unknown to us while also putting us into a category for the competition. we also incorporated the use of sqlite3 which is a built-in databasing -----> tool !!!  for python to store our workouts.\nchallenges we ran into\na big challenge we ran into with streamlit was not being able to have adaptive urls similar to that of python flask or django and it seemed to have only one static webpage and we had to adapt to this constraint. additionally, streamlit is based in python, so it became hard to develop your own webpage using html, css, and js which aren't by default supported in python.\naccomplishments that we're proud of\nthis being our hackathon for all but one of our hackers, to have a mostly complete final product is something we are proud of. we also have not had too much experience with programming production level products and most things stay on the backend so this was a change of tempo for us.\nwhat we learned\nas a team, we learned to adapt our ideas into working models based on the constraints placed upon us by the libraries we were using.\nwhat's next for sizeup!\nwe plan to use the model created for this hackathon as a template and stepping stone for creating a full-blown application for ios and android devices probably within swift or java.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59505151}, {"Unnamed: 0": 5164, "autor": "Safelloween", "date": null, "content": "Inspiration \ud83d\udc7b\ud83c\udf83\nI wanted to work on this program after reflecting on how different our lives have become as a result of COVID-19. Kids often don't understand the seriousness of COVID 19 and its effect so I decided to create an interactive game that helps preserve the possibility of happy halloweens in future.\nWhat it does \ud83d\udd0d\u2753\ud83c\udd95\nSafelloween teaches elementary students about public health guidelines through a choose-your-own-adventure style game. Safelloween begins with a fun, personalized greeting and accessibility features such as a voiceover for each page. Players are placed in typical halloween season scenarios and are asked to make safe decisions.\nHow we built it \ud83c\udfd7\ud83d\udd27\ud83d\udd28\u2699\ud83e\uddf0\nIt is built using HTML, CSS, Javascript and involves voiceovers done from an online tool prerecorded.\nChallenges we ran into \ud83e\udde8\ud83c\udfc3\u200d\u2642\ufe0f\ud83c\udfc3\u200d\u2642\ufe0f\ud83c\udfc3\u200d\u2642\ufe0f\ud83c\udfc3\u200d\u2642\ufe0f\nA challenge that I faced while working on this project has to be hosting it! It took a lot of time to figure out a way to host this project.\nAccomplishments that we're proud of \ud83c\udfc6\ud83c\udfc5\ud83e\udde0\nProud that I finished it having my exams at the same time. Proud of the last page I built lol!\nWhat we learned \ud83e\udde0\nThis time I learned a lot about Time Management. I worked on this project on the busiest day and yet I finished it, so proud! Although it was a simple project but I learned a lot of javascript concepts and also how audio voiceovers are done.\nWhat's next for Safelloween\u23ed\u23ed\u23ed\nA feature that allows to user to play halloween themed games together virtually. A favorite costume voting feature that would allow users to post their halloween fits and others can vote.", "link": "https://devpost.com/software/safelloween-kfvndj", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration \ud83d\udc7b\ud83c\udf83\ni wanted to work on this program after reflecting on how different our lives have become as a result of covid-19. kids often don't understand the seriousness of covid 19 and its effect so i decided to create an interactive game that helps preserve the possibility of happy halloweens in future.\nwhat it does \ud83d\udd0d\u2753\ud83c\udd95\nsafelloween teaches elementary students about public health guidelines through a choose-your-own-adventure style game. safelloween begins with a fun, personalized greeting and accessibility features such as a voiceover for each page. players are placed in typical halloween season scenarios and are asked to make safe decisions.\nhow we built it \ud83c\udfd7\ud83d\udd27\ud83d\udd28\u2699\ud83e\uddf0\nit is built using html, css, javascript and involves voiceovers done from an online -----> tool !!!  prerecorded.\nchallenges we ran into \ud83e\udde8\ud83c\udfc3\u200d\u2642\ufe0f\ud83c\udfc3\u200d\u2642\ufe0f\ud83c\udfc3\u200d\u2642\ufe0f\ud83c\udfc3\u200d\u2642\ufe0f\na challenge that i faced while working on this project has to be hosting it! it took a lot of time to figure out a way to host this project.\naccomplishments that we're proud of \ud83c\udfc6\ud83c\udfc5\ud83e\udde0\nproud that i finished it having my exams at the same time. proud of the last page i built lol!\nwhat we learned \ud83e\udde0\nthis time i learned a lot about time management. i worked on this project on the busiest day and yet i finished it, so proud! although it was a simple project but i learned a lot of javascript concepts and also how audio voiceovers are done.\nwhat's next for safelloween\u23ed\u23ed\u23ed\na feature that allows to user to play halloween themed games together virtually. a favorite costume voting feature that would allow users to post their halloween fits and others can vote.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59505164}, {"Unnamed: 0": 5167, "autor": "ezRez", "date": null, "content": "Inspiration\nMy teammate wanted to make something that would be cost effective and save trees from becoming resume fodder. We talked about it and the idea was born!\nWhat it does\nEnables a user (mainly a student) to choose from free to use resume web templates. They fill in specific data and then in the end receive the option of sharing that data in several ways: via a qr code, encoded url, or by utilizing web nfc.\nHow we built it\nIt is noted here: https://use.ezrez.tech/index.html#tech_used However you can also check out our commits on Github: https://github.com/hola-there/ezRez/commits/main\nLastly we utilized the following links in order to handle the json creation and encoding:\nhttps://v3.vuejs.org/guide/template-syntax.html#attributes\nhttps://v3.vuejs.org/guide/template-syntax.html#text\nhttps://v3.vuejs.org/guide/introduction.html#declarative-rendering\nhttps://stackoverflow.com/a/40933848\nhttps://v3.vuejs.org/guide/introduction.html#handling-user-input\nhttps://web.dev/nfc/\nBasically everything is shown/analyzed on the client side. Since the project is only hosted via Github pages as a static site.\nChallenges we ran into\nEncoding json output then decoding it so that a template could be filled in\nModifying preexisting code so that data provided by a user can be filled in\nAccomplishments that we're proud of\nFinding good images\nUtilizing Canva pro to make some nice imagery\nChoosing a logo\nChecking the USPTO Tess tool to check for trademark concerns\nWhat we learned\nNot to wait until halfway into a hackathon to start if no one had a good idea at first.\nWhat's next for ezRez\nMaybe a paid version or a coffee money donation button\nMore templates\nMaybe a real backend", "link": "https://devpost.com/software/ezrez", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nmy teammate wanted to make something that would be cost effective and save trees from becoming resume fodder. we talked about it and the idea was born!\nwhat it does\nenables a user (mainly a student) to choose from free to use resume web templates. they fill in specific data and then in the end receive the option of sharing that data in several ways: via a qr code, encoded url, or by utilizing web nfc.\nhow we built it\nit is noted here: https://use.ezrez.tech/index.html#tech_used however you can also check out our commits on github: https://github.com/hola-there/ezrez/commits/main\nlastly we utilized the following links in order to handle the json creation and encoding:\nhttps://v3.vuejs.org/guide/template-syntax.html#attributes\nhttps://v3.vuejs.org/guide/template-syntax.html#text\nhttps://v3.vuejs.org/guide/introduction.html#declarative-rendering\nhttps://stackoverflow.com/a/40933848\nhttps://v3.vuejs.org/guide/introduction.html#handling-user-input\nhttps://web.dev/nfc/\nbasically everything is shown/analyzed on the client side. since the project is only hosted via github pages as a static site.\nchallenges we ran into\nencoding json output then decoding it so that a template could be filled in\nmodifying preexisting code so that data provided by a user can be filled in\naccomplishments that we're proud of\nfinding good images\nutilizing canva pro to make some nice imagery\nchoosing a logo\nchecking the uspto tess -----> tool !!!  to check for trademark concerns\nwhat we learned\nnot to wait until halfway into a hackathon to start if no one had a good idea at first.\nwhat's next for ezrez\nmaybe a paid version or a coffee money donation button\nmore templates\nmaybe a real backend", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59505167}, {"Unnamed: 0": 5189, "autor": "Moonswap", "date": null, "content": "What is MoonSwap?\nMoonSwap is a tool that enables DAOs to swap, borrow, lend, and stream tokens from their treasuries in order to empower shared interests and foster long-term partnerships between DAOs. How it was built\nMoonSwap was developed by a collaborative effort between farque65, qedk, dhaiwat10, miral, nazeeh21. This app is built on the top of scaffold-eth which allows you to quickly experiment with solidity using a front end that adapts to your Smart Contract. It provides extensive docs on tools that you can use to develop an app using Solidity for Smart Contracts, Hardhat (which provides an ethereum development environment), React for frontend, Ethers.js to interact with the deployed Smart Contract and Ant for the user interface.\nHere\u2019s how MoonSwap works:\nCreate Swap: 1st party creates a swap by adding the number of tokens they want to swap and their token address. 1st party also needs to enter the address of the 2nd party they\u2019re sending to. 1st party can predefine the number of tokens they should get in return from the 2nd party. Sharing the swap: After successfully creating their swap, a shareable URL will be generated. The 1st party shares the URL with the 2nd party, and thus the 2nd party can choose to commit to the swap from that link.\nCommit Swap: The URL will open a page with the token details already pre-filled. The 2nd party can simply click on the Commit Swap button and the swap will commence. How easy is that?\nIf a user chooses to swap LINK tokens then they will see a USD value of the amount of LINK they choose to swap using data from a contract that is consuming LINK/USD price data.", "link": "https://devpost.com/software/moonswap", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what is moonswap?\nmoonswap is a -----> tool !!!  that enables daos to swap, borrow, lend, and stream tokens from their treasuries in order to empower shared interests and foster long-term partnerships between daos. how it was built\nmoonswap was developed by a collaborative effort between farque65, qedk, dhaiwat10, miral, nazeeh21. this app is built on the top of scaffold-eth which allows you to quickly experiment with solidity using a front end that adapts to your smart contract. it provides extensive docs on tools that you can use to develop an app using solidity for smart contracts, hardhat (which provides an ethereum development environment), react for frontend, ethers.js to interact with the deployed smart contract and ant for the user interface.\nhere\u2019s how moonswap works:\ncreate swap: 1st party creates a swap by adding the number of tokens they want to swap and their token address. 1st party also needs to enter the address of the 2nd party they\u2019re sending to. 1st party can predefine the number of tokens they should get in return from the 2nd party. sharing the swap: after successfully creating their swap, a shareable url will be generated. the 1st party shares the url with the 2nd party, and thus the 2nd party can choose to commit to the swap from that link.\ncommit swap: the url will open a page with the token details already pre-filled. the 2nd party can simply click on the commit swap button and the swap will commence. how easy is that?\nif a user chooses to swap link tokens then they will see a usd value of the amount of link they choose to swap using data from a contract that is consuming link/usd price data.", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59505189}, {"Unnamed: 0": 5206, "autor": "Chemical warfare", "date": null, "content": "Coin Image Classification What did you make? We made a coin image classification machine learning model.\nHow did you make it? We made this by applying different SkLearn models to our custom data set of 4 different coins and comparing their performances.\nWhat is special about your project? The applications of a coin classification system span from a children's education tool for learning their coins to an aid to visually impaired people in counting their currency. We believe this model's ability to improve such a wide range of lives makes it powerful and impactful.\nWhat is something you learned in the creation of your project? Both of us are new to machine learning, so every step was new. Coming in with minimal experience and leaving with a functioning ML model (best performing at 92%), we have learned how to efficiently create powerful ML models.\nAnything you would like us to consider? Along with learning ML, we also took this opportunity to learn Jupyter Notebooks, which has helped to create a great way of presenting such a technical and otherwise relatively non-visual creation. Being able to visually represent the progression of developing our ML model helps to demonstrate the internals and even shed light on possible areas of improvement (aka future work).", "link": "https://devpost.com/software/chemical-warfare", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "coin image classification what did you make? we made a coin image classification machine learning model.\nhow did you make it? we made this by applying different sklearn models to our custom data set of 4 different coins and comparing their performances.\nwhat is special about your project? the applications of a coin classification system span from a children's education -----> tool !!!  for learning their coins to an aid to visually impaired people in counting their currency. we believe this model's ability to improve such a wide range of lives makes it powerful and impactful.\nwhat is something you learned in the creation of your project? both of us are new to machine learning, so every step was new. coming in with minimal experience and leaving with a functioning ml model (best performing at 92%), we have learned how to efficiently create powerful ml models.\nanything you would like us to consider? along with learning ml, we also took this opportunity to learn jupyter notebooks, which has helped to create a great way of presenting such a technical and otherwise relatively non-visual creation. being able to visually represent the progression of developing our ml model helps to demonstrate the internals and even shed light on possible areas of improvement (aka future work).", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59505206}, {"Unnamed: 0": 5211, "autor": "Reliable Password Manager", "date": null, "content": "Inspiration\nI enjoyed the flutter workshop that was given Friday night, and decided to use this event to learn it.\nWhat it does\nIt was supposed to be a joke password manager that forgets your passwords. Unfortunately, I had to fly solo and ran out of time. Currently, it is login page and will append an array with instances of an account class.\nHow we built it\nIt was built via flutter on vs code.\nChallenges we ran into\nTime is an obvious challenge, but the greater issue was probably being on my own. As a solo dev with limited experience it took too long to get the UI set up. Because of this, I didn't have the opportunity to flesh out the features.\nAccomplishments that we're proud of\nI'm proud that I took on the challenge of a new technology. Trying to absorb as much information as I could on my own was a fullfilling experience even if I wasn't able to create a functioning final product. I was originally only planning to attend talks and workshops, but I'm proud that I put my head down and got to work.\nWhat we learned\nFor one, I learned that I really like flutter, but more importantly, that working with a team is crucial for being productive on a schedule.\nWhat's next for Reliable Password Manager\nI plan to finish it as a proper personal password manager. Hopefully I will be able to deploy it across a few different devices and continue to develop it as a tool I actually use on a regular basis.", "link": "https://devpost.com/software/reliable-password-manager", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ni enjoyed the flutter workshop that was given friday night, and decided to use this event to learn it.\nwhat it does\nit was supposed to be a joke password manager that forgets your passwords. unfortunately, i had to fly solo and ran out of time. currently, it is login page and will append an array with instances of an account class.\nhow we built it\nit was built via flutter on vs code.\nchallenges we ran into\ntime is an obvious challenge, but the greater issue was probably being on my own. as a solo dev with limited experience it took too long to get the ui set up. because of this, i didn't have the opportunity to flesh out the features.\naccomplishments that we're proud of\ni'm proud that i took on the challenge of a new technology. trying to absorb as much information as i could on my own was a fullfilling experience even if i wasn't able to create a functioning final product. i was originally only planning to attend talks and workshops, but i'm proud that i put my head down and got to work.\nwhat we learned\nfor one, i learned that i really like flutter, but more importantly, that working with a team is crucial for being productive on a schedule.\nwhat's next for reliable password manager\ni plan to finish it as a proper personal password manager. hopefully i will be able to deploy it across a few different devices and continue to develop it as a -----> tool !!!  i actually use on a regular basis.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59505211}, {"Unnamed: 0": 5225, "autor": "Organ Transplant Finder", "date": null, "content": "Inspiration\nAccording to the American Transplant Foundation, around 107,000 people in the US are currently in need of a lifesaving organ transplant. These recipients are selected based on location, medical need, and compatibility.\nGoal\nWe aim to assist recipients in their path to treatment by focusing our efforts on connecting patients to the nearest Organ Transplant Centers.\nWhat it does\nWith our tool, patients can quickly find clinics by filtering what services are available. Moreover, to facilitate the user's experience, the tool also provides a quick overview of each institution's information.\nHow we built it\nOur web app was built mainly with React.js. The core of the project is our map which we implemented using the Google Maps JavaScript API and populated it with data gathered from the Health Resources and Services Administration (HRSA).\nChallenges we ran into\nAt first, our team wanted to use the Deck.gl framework to layer over the map and provide other data visualization functionalities to our users; however, due to the lack of time, we had to compromise and focus on the core components of our app.\nAccomplishments that we're proud of\nWe are proud to create a project that positively impacts our community. In addition, we are proud of using Python and Jupyter Notebooks to convert the HRSA data from an Excel spreadsheet to clean and organized JSON files so we could use the data with the map. Moreover, we are also proud of thinking objectively under pressure and focusing on the main features of our project when we ran into an obstacle to make sure we still had a great app. Finally, we are proud of submitting this project as it is the very first hackathon that any of our team members have ever participated in.\nWhat we learned\nAs first-time hackers, we learned more about working under pressure and cooperating to create a great demo project. Our team members also had the opportunity to learn usages of the systems we used to build our project, as none of our team members had extensive experience with either the React library or the Google Maps JavaScript API.", "link": "https://devpost.com/software/organ-transplant-finder", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\naccording to the american transplant foundation, around 107,000 people in the us are currently in need of a lifesaving organ transplant. these recipients are selected based on location, medical need, and compatibility.\ngoal\nwe aim to assist recipients in their path to treatment by focusing our efforts on connecting patients to the nearest organ transplant centers.\nwhat it does\nwith our -----> tool !!! , patients can quickly find clinics by filtering what services are available. moreover, to facilitate the user's experience, the tool also provides a quick overview of each institution's information.\nhow we built it\nour web app was built mainly with react.js. the core of the project is our map which we implemented using the google maps javascript api and populated it with data gathered from the health resources and services administration (hrsa).\nchallenges we ran into\nat first, our team wanted to use the deck.gl framework to layer over the map and provide other data visualization functionalities to our users; however, due to the lack of time, we had to compromise and focus on the core components of our app.\naccomplishments that we're proud of\nwe are proud to create a project that positively impacts our community. in addition, we are proud of using python and jupyter notebooks to convert the hrsa data from an excel spreadsheet to clean and organized json files so we could use the data with the map. moreover, we are also proud of thinking objectively under pressure and focusing on the main features of our project when we ran into an obstacle to make sure we still had a great app. finally, we are proud of submitting this project as it is the very first hackathon that any of our team members have ever participated in.\nwhat we learned\nas first-time hackers, we learned more about working under pressure and cooperating to create a great demo project. our team members also had the opportunity to learn usages of the systems we used to build our project, as none of our team members had extensive experience with either the react library or the google maps javascript api.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59505225}, {"Unnamed: 0": 5245, "autor": "Data_Analyzer", "date": null, "content": "Inspiration\nThe framing of commands for any program in machine learning was always a task for students especially those who do not have a fairly technical background. Thus in order to solve this conundrum and save hours spent on tedious debugging, we have proposed a system that is easier and efficient enough to carry out the job on a few button clicks. It is like a speed dial system for Machine Learning commands. This our form of automating the tasks that initially required specialized data scientists and manual labor to a more time saving and cost efficient system.\nWhat it does\nData Analyzer is an AI based tool that takes a data set as input, analyze patterns in the data, interpret the result, and can produce an output analysis. It is able to pro-actively analyze data and generate feeds using natural language generation techniques with very less efforts. The application caters to users with a basic working knowledge of Machine Learning and Data Science concepts and is thus easily usable and understandable.\nHow we built it\nThe application is built using Streamlit for a multi-class page implementation. It is divided into various modules and a gist of them is given below: Firstly, Upload Data deals with uploading a .csv or excel files within the limit of 200mb. Once uploaded, it creates a copy of the data and it also saves the columns and along with their data types. Secondly, Changing Metadata gives the user the option to change the column type from the already listed ones. Thirdly, Machine Learning Algorithm automates the process of machine learning by giving the user the power to select independent and dependent variables and then select the type of process. It also saves the best model as a binary .sav file which can be used in the future for inferencing. Along with this, the accuracy is also displayed. Lastly, Analysis of Data and Y-Parameter Optimization shows the user some visualized graphs made using seaborn and gives the user the option to change the graphs based on the different column names.\nChallenges we ran into\nOne of the major challenges we ran into was the varying processing time required for each new dataset which ended up in restarting the kernel each time. Thus making our application slow. Hence, we had to set a limit for uploading the dataset. However, this problem can easily be eliminated by using a third-party API for cloud storage.\nAccomplishments that we're proud of\nWe are proud to contribute to the field of data science in a way that has innovated the way we look at Machine Learning programs. We are also pleased to share that this combined effort has not just been acknowledged as a massive break-through but we have also received many positive feedbacks about this project since its inception.\nWhat we learned\nWe learnt to integrate multiple pages with StreamLit. Our previous project had taught us the basics of this closed framework but through this project, we were able to stretch our boundaries and dive deeper into the concepts of this closed framework. Our entire team is proficient with the basics of Data Science but its display on an app was not our strong suit until we developed this webapp.\nWhat's next for Data_Analyzer\nChatbot options for a more interactive User Experience. User Profile for enhanced personalization and better data insights based on your tool usage.", "link": "https://devpost.com/software/data_analyzer", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nthe framing of commands for any program in machine learning was always a task for students especially those who do not have a fairly technical background. thus in order to solve this conundrum and save hours spent on tedious debugging, we have proposed a system that is easier and efficient enough to carry out the job on a few button clicks. it is like a speed dial system for machine learning commands. this our form of automating the tasks that initially required specialized data scientists and manual labor to a more time saving and cost efficient system.\nwhat it does\ndata analyzer is an ai based -----> tool !!!  that takes a data set as input, analyze patterns in the data, interpret the result, and can produce an output analysis. it is able to pro-actively analyze data and generate feeds using natural language generation techniques with very less efforts. the application caters to users with a basic working knowledge of machine learning and data science concepts and is thus easily usable and understandable.\nhow we built it\nthe application is built using streamlit for a multi-class page implementation. it is divided into various modules and a gist of them is given below: firstly, upload data deals with uploading a .csv or excel files within the limit of 200mb. once uploaded, it creates a copy of the data and it also saves the columns and along with their data types. secondly, changing metadata gives the user the option to change the column type from the already listed ones. thirdly, machine learning algorithm automates the process of machine learning by giving the user the power to select independent and dependent variables and then select the type of process. it also saves the best model as a binary .sav file which can be used in the future for inferencing. along with this, the accuracy is also displayed. lastly, analysis of data and y-parameter optimization shows the user some visualized graphs made using seaborn and gives the user the option to change the graphs based on the different column names.\nchallenges we ran into\none of the major challenges we ran into was the varying processing time required for each new dataset which ended up in restarting the kernel each time. thus making our application slow. hence, we had to set a limit for uploading the dataset. however, this problem can easily be eliminated by using a third-party api for cloud storage.\naccomplishments that we're proud of\nwe are proud to contribute to the field of data science in a way that has innovated the way we look at machine learning programs. we are also pleased to share that this combined effort has not just been acknowledged as a massive break-through but we have also received many positive feedbacks about this project since its inception.\nwhat we learned\nwe learnt to integrate multiple pages with streamlit. our previous project had taught us the basics of this closed framework but through this project, we were able to stretch our boundaries and dive deeper into the concepts of this closed framework. our entire team is proficient with the basics of data science but its display on an app was not our strong suit until we developed this webapp.\nwhat's next for data_analyzer\nchatbot options for a more interactive user experience. user profile for enhanced personalization and better data insights based on your tool usage.", "sortedWord": "None", "removed": "Nan", "score": 15, "comments": 0, "media": null, "medialink": null, "identifyer": 59505245}, {"Unnamed: 0": 5251, "autor": "NAV Python", "date": null, "content": "Inspiration\nFinancial information about companies and their stock evaluation is difficult to find for the average person. Much of this information is spread across several documents and sites making it hard to understand a company's performance. We wanted to make a simple automated tool that collects financial information from the US EDGAR database and several other financial sites and provides the user with a company evaluation based on the Net Asset Valuation method.\nWhat it does\nThis program will collect information from the US EDGAR, Electronic Data Gathering, Analysis, and Retrieval, system database which is the database that has all the financial filings of companies that are publically traded on the stock market. Outside of EDGAR, it collects information from platforms such as Yahoo Finance. Our program uses one of the financial valuation methods called the net asset valuation method. This essentially takes all the assets of a company and gets subtracted by the liabilities of a company, also known as the Net Asset Value (NAV). There is no \"standardized\" way of using this method and our group uses a relatively simple approach to this method. Once the NAV is calculated, we divide it by the total shares outstanding to get the price of the stock per share. We also collected the current price to see if the company is \"overvalued\" or \"undervalued\". The details of the assets, liabilities and other information are outputted into a file in a CSV format.\nHow we built it\nWe used python for all of the tasks. As for parsing data from EDGAR and Yahoo Finance, we had For reformatting, we made use of Pandas to create the table to reformat, add, and remove information that we wanted.\nChallenges we ran into\nFor the data collecting part: Sites such as Yahoo Finance stored key financial information under different names depending on the company. This made it much harder for us to accurately retrieve information and required multiple conditional checks depending on the retrieved data format and naming scheme. The complexity of Yahoo Finance also made it difficult to figure out what information and network requests made by the site were important (~300 requests for a normal page load).\nFor the formatting part: We retrieved the majority of the data from the EDGAR database. Each company has a form called \"10-K\" or \"10-Q\" which states the financial information. These forms are collected from an HTML file that is listed on the EDGAR website. Each HTML file consists of many tables. Finding the table with the information we need was a big challenge. Once the table was retrieved, we mainly used pandas to reformat, remove, and add information that we wanted. There are many rows and columns that are duplicated, do not carry information we wanted, etc and as a result, we had to cut out all the unneeded information. Adding information and rearranging was also another challenging part we found.\nAccomplishments that we're proud of\nParsing financial information from EDGAR and especially Yahoo Finance\nReformatting pandas dataframe\nReverse engineering the sites we collected the data from\nWhat we learned\nCale: Large sites like Yahoo Finance make a significant number of requests upon page loads to various site resources and trackers. This made the initial reverse engineering of the site difficult as combing through requests to find important ones took longer. A key simplification for retrieving information from client-side rendered sites (ex. React) is that most of the data is used by Yahoo Finance is stored in the React application context under various stores. It was interesting for me to see discrepancies between the data used in the backend and the information displayed to the user. The data displayed to the user includes multiple rounding errors which made it difficult to verify that our retrieved results were correct. As a nuclear engineer, this was my first time diving into the complexities of the finance sector and the importance of certain financial indicators in a company's evaluation\nCasey: Using the different python modules. I never really coded much in my life. I went through COSC102 and 130, and did some projects on the side in C++ but not really in python. I found out how useful using python modules are. It is a very useful tool and makes the life of a programmer much easier. Also, this was the first time I engaged in parsing financial forms. This came out to be a very complicated task that took many trials and errors to finally find out how exactly to get the information we want. Finally, this was the first time I used Github. Cale and I coordinated with each other and used Github as the hub where we shared the updates and such. I learned the importance and usefulness of using Github.\nWhat's next for NAV Python\nWe want to go further with the Net Asset Valuation method. We currently use a simple way of using the NAV and the more information we can add, the more reliable the valuation of the companies we come up with will be. Stock valuation is subjective and a certain valuation method does not apply well for certain companies or industries. Beyond this, we want to look into the different, more complex valuation methods to be able to value stocks in many different ways and use the most suitable valuation method depending on the company we are looking into.", "link": "https://devpost.com/software/nav-python", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nfinancial information about companies and their stock evaluation is difficult to find for the average person. much of this information is spread across several documents and sites making it hard to understand a company's performance. we wanted to make a simple automated -----> tool !!!  that collects financial information from the us edgar database and several other financial sites and provides the user with a company evaluation based on the net asset valuation method.\nwhat it does\nthis program will collect information from the us edgar, electronic data gathering, analysis, and retrieval, system database which is the database that has all the financial filings of companies that are publically traded on the stock market. outside of edgar, it collects information from platforms such as yahoo finance. our program uses one of the financial valuation methods called the net asset valuation method. this essentially takes all the assets of a company and gets subtracted by the liabilities of a company, also known as the net asset value (nav). there is no \"standardized\" way of using this method and our group uses a relatively simple approach to this method. once the nav is calculated, we divide it by the total shares outstanding to get the price of the stock per share. we also collected the current price to see if the company is \"overvalued\" or \"undervalued\". the details of the assets, liabilities and other information are outputted into a file in a csv format.\nhow we built it\nwe used python for all of the tasks. as for parsing data from edgar and yahoo finance, we had for reformatting, we made use of pandas to create the table to reformat, add, and remove information that we wanted.\nchallenges we ran into\nfor the data collecting part: sites such as yahoo finance stored key financial information under different names depending on the company. this made it much harder for us to accurately retrieve information and required multiple conditional checks depending on the retrieved data format and naming scheme. the complexity of yahoo finance also made it difficult to figure out what information and network requests made by the site were important (~300 requests for a normal page load).\nfor the formatting part: we retrieved the majority of the data from the edgar database. each company has a form called \"10-k\" or \"10-q\" which states the financial information. these forms are collected from an html file that is listed on the edgar website. each html file consists of many tables. finding the table with the information we need was a big challenge. once the table was retrieved, we mainly used pandas to reformat, remove, and add information that we wanted. there are many rows and columns that are duplicated, do not carry information we wanted, etc and as a result, we had to cut out all the unneeded information. adding information and rearranging was also another challenging part we found.\naccomplishments that we're proud of\nparsing financial information from edgar and especially yahoo finance\nreformatting pandas dataframe\nreverse engineering the sites we collected the data from\nwhat we learned\ncale: large sites like yahoo finance make a significant number of requests upon page loads to various site resources and trackers. this made the initial reverse engineering of the site difficult as combing through requests to find important ones took longer. a key simplification for retrieving information from client-side rendered sites (ex. react) is that most of the data is used by yahoo finance is stored in the react application context under various stores. it was interesting for me to see discrepancies between the data used in the backend and the information displayed to the user. the data displayed to the user includes multiple rounding errors which made it difficult to verify that our retrieved results were correct. as a nuclear engineer, this was my first time diving into the complexities of the finance sector and the importance of certain financial indicators in a company's evaluation\ncasey: using the different python modules. i never really coded much in my life. i went through cosc102 and 130, and did some projects on the side in c++ but not really in python. i found out how useful using python modules are. it is a very useful tool and makes the life of a programmer much easier. also, this was the first time i engaged in parsing financial forms. this came out to be a very complicated task that took many trials and errors to finally find out how exactly to get the information we want. finally, this was the first time i used github. cale and i coordinated with each other and used github as the hub where we shared the updates and such. i learned the importance and usefulness of using github.\nwhat's next for nav python\nwe want to go further with the net asset valuation method. we currently use a simple way of using the nav and the more information we can add, the more reliable the valuation of the companies we come up with will be. stock valuation is subjective and a certain valuation method does not apply well for certain companies or industries. beyond this, we want to look into the different, more complex valuation methods to be able to value stocks in many different ways and use the most suitable valuation method depending on the company we are looking into.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59505251}, {"Unnamed: 0": 5254, "autor": "Answering Reading Comprehension Questions", "date": null, "content": "From Short Context to Long Passages: Generalizing QA Models to Answer Standardized Test Reading Comprehension Questions\nWho\ndwu44 / Dustin Wu\nmwang102 / Milanca Wang\nGithub Repo\nour GitHub repository\nFinal Reflection\nour final reflection writeup\nCheckpoint 2 Reflection\nour reflection writeup for checkpoint 2\nIntroduction\nWhat problem are you trying to solve and why?\nCurrent state-of-the-art reading comprehension models typically operate on short contexts (reading passages of limited length) due to computational constraints. However, for standardized tests like the SAT or ACT, humans must often draw on information from much longer passages to answer test questions. Our goal is to bridge this gap to generalize QA (question-answer) models to answer standardized test reading comprehension questions with much larger contexts.\nWhat kind of problem is this? Classification? Regression? Structured prediction? Reinforcement Learning? Unsupervised Learning? Etc.\nThis is a supervised learning NLP project. Specifically, we aim to develop a question-answering model that does not generate answers but rather picks from one of four possible choices.\nProject Goal\nExperimental Setup\nSee Methodology.\nGoal 1: Optimal Context Window\nWe propose to fine-tune short-context QA models by building a supplemental architecture that identifies the \u201ccontext window\u201d (section of text) in a long passage that is ideal for using to answer a given question. If we are able to do this, then it stands to reason that a pre-trained short-context QA model could be generalized to answer long-passage questions. The two challenges that we anticipate are 1) the relatively small amount of standardized test passages that are openly available for use as training data and 2) figuring out how to generalize the short-context QA models to answer long-passage questions\nSubgoal 1: Assembling a long-passage dataset\nWe plan to either scrape standardized test passages and questions to use as fine-tuning data, or to use them as test data and source our long-passage data from a dataset like the RACE dataset.\nSubgoal 2: Developing a context window architecture\nBy stringing together downsampling and feature extraction layers, like in a CNN, it is possible that we could develop a context window architecture on top of the pre-trained short-context QA models to aid it in answering standardized test questions\nGoal 2: Predicting Answers\nOur primary goal is to adapt Question-Answering (QA) models that have been trained on datasets like ReClor or OpenBookQA, in which the context is around a paragraph of text, to answer standardized test reading comprehension questions (e.g. SAT, ACT) where the context is a long passage of several paragraphs of text. We will integrate the context window model with these QA models by sending the output of the context window as the data to fine-tune the pre-trained models with.\nSubgoal 1: Integration of both architectures\nWe will have to determine the optimal pipeline to integrate the inputs and outputs of both models.\nSubgoal 2: Fine-Tuning Model\nThe QA models will have to be fine-tuned across various datasets. Our goal is to determine what learnings do or do not generalize.\nRelated Work\nAre you aware of any, or is there any prior work that you drew on to do your project?\nIn this section, also include URLs to any public implementations you find of the paper you\u2019re trying to implement. Please keep this as a \u201cliving list\u201d--if you stumble across a new implementation later down the line, add it to this list.\nPaper that used image and natural language processing to solve SAT geometry problems. Achieved a 49% score on official SAT geometry questions. http://geometry.allenai.org/assets/emnlp2015.pdf\nGPT-3 Reading Comprehension: https://arxiv.org/pdf/2005.14165.pdf\nReClor: A Reading Comprehension Dataset Requiring Logical Reasoning: https://openreview.net/forum?id=HJgJtT4tvB\nCan a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering: https://aclanthology.org/D18-1260.pdf\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: https://huggingface.co/transformers/model_doc/bert.html (BertForMultipleChoice: https://huggingface.co/transformers/model_doc/bert.html#bertformultiplechoice)\nLIAMF Fine-tuned Roberta: https://huggingface.co/LIAMF-USP/roberta-large-finetuned-race\nConstructing Transformers For Longer Sequences with Sparse Attention Methods: https://ai.googleblog.com/2021/03/constructing-transformers-for-longer.html\nRACE: Large-scale ReAding Comprehension Dataset From Examinations: https://arxiv.org/abs/1704.04683\nData\nWhat data are you using (if any)?\nFor our data, we plan to use SAT and ACT reading comprehension questions, which are freely accessible from practice test pdfs that we can download and process into text. While we may use the obtained data as the sole finetuning data, we are also considering using another dataset with long-passage questions as the fine tuning dataset, and then the SAT and ACT reading comprehension questions as a final test dataset to evaluate our model\u2019s performance.\nHow big is it? Will you need to do significant preprocessing?\nSince the size of each individual SAT reading comprehension passage is pretty big, one of our goals is to build a feature extraction model that can be used to process those passages into reasonably-sized contexts. This will be used as the data for the actual QA-prediction model.\nMethodology\nWhat is the architecture of your model?\nAs Transformers are the state of the art for natural language processing tasks, we are inclined to use one or more Transformers as the core of the model. We have also thought about delegating a supplementary model for classifying SAT questions into certain categories, and then choosing an expert model to ask for an answer from based on the category. In addition, since many SAT questions are based off of certain sections of text, it may help to build an additional model that learns how to map question hints like (Lines 33-47) to a left and right index on the tokens to select for the transformer.\nHow are you training the model?\nWe will use pre-trained models (ReClor, GPT-3, OpenBookQA, and/or BERT) that have a basic idea of reading comprehension and attempt to fine-tune them on other datasets, like SAT/ACT reading comprehension questions, or within each other like OpenBookQA with ReClor or vice-versa.\nJustify your design. Also note some backup ideas you may have to experiment with if you run into issues.\nThis design is meant to observe how generalizable reading comprehension problems are across different levels. Since transformers are data hungry, using a nonspecific, pre-trained model can be helpful for establishing baseline behavior. Fine-tuning on more complex windows will yield information on how similar cross-level standardized tests are, or not!\nThe main challenge we anticipate will be correctly classifying the optimal window size for long passages, so our backup plan is to use data with shorter contexts and compare them exclusively with each other (ex: ReClor, which sources mainly from the GRE and LSAT; and OpenBookQA, which sources from elementary science quizzes).\nMetrics\nWhat constitutes \u201csuccess?\u201d\nAccuracy (namely the number of correctly answered questions divided by the total number of questions) seems to be an applicable metric for our model.\nWhat experiments do you plan to run?\nWe plan to evaluate our model\u2019s performance on standardized testing questions from a variety of tests, namely the SAT and the ACT but perhaps also the GRE and other tests.\nAccuracy Goals\n[ x ] Base: > 25%\n[ x ] Target: > 33%\n[ ] Stretch: > 50%\nEthics\nWhy is Deep Learning a good approach to this problem?\nStandardized testing is fairly important in the current model of the US education system. It is important to critically evaluate the purpose and efficacy of these important assessments, and using deep learning models to back out reading comprehension from a purely mechanical viewpoint can be valuable towards judging the fairness or complexity of what it takes to \u201cdo well\u201d. The goal is not to \u201csolve\u201d reading comprehension with pre-trained, state-of-the-art deep learning models. Instead, our hope is to use deep learning as an evaluation tool to quantify and qualify what exactly standard assessments aim to test with regards to generalization and reading comprehension.\nHow are you planning to quantify or measure error or success? What implications does your quantification have?\nError and success will be measured by the accuracy score of the model on real tests questions and answers: a straight percentage of (num_correct / num_questions) is how the model\u2019s accuracy will be calculated. This way, it will be simple to easily relate the model\u2019s performance to human performance on the same assessment.\nDivision of labor\nDustin: Data collection, fine-tuning model, context window architecture\nMilanca: Integrating pre-trained models, context window architecture", "link": "https://devpost.com/software/dl-final-project", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "from short context to long passages: generalizing qa models to answer standardized test reading comprehension questions\nwho\ndwu44 / dustin wu\nmwang102 / milanca wang\ngithub repo\nour github repository\nfinal reflection\nour final reflection writeup\ncheckpoint 2 reflection\nour reflection writeup for checkpoint 2\nintroduction\nwhat problem are you trying to solve and why?\ncurrent state-of-the-art reading comprehension models typically operate on short contexts (reading passages of limited length) due to computational constraints. however, for standardized tests like the sat or act, humans must often draw on information from much longer passages to answer test questions. our goal is to bridge this gap to generalize qa (question-answer) models to answer standardized test reading comprehension questions with much larger contexts.\nwhat kind of problem is this? classification? regression? structured prediction? reinforcement learning? unsupervised learning? etc.\nthis is a supervised learning nlp project. specifically, we aim to develop a question-answering model that does not generate answers but rather picks from one of four possible choices.\nproject goal\nexperimental setup\nsee methodology.\ngoal 1: optimal context window\nwe propose to fine-tune short-context qa models by building a supplemental architecture that identifies the \u201ccontext window\u201d (section of text) in a long passage that is ideal for using to answer a given question. if we are able to do this, then it stands to reason that a pre-trained short-context qa model could be generalized to answer long-passage questions. the two challenges that we anticipate are 1) the relatively small amount of standardized test passages that are openly available for use as training data and 2) figuring out how to generalize the short-context qa models to answer long-passage questions\nsubgoal 1: assembling a long-passage dataset\nwe plan to either scrape standardized test passages and questions to use as fine-tuning data, or to use them as test data and source our long-passage data from a dataset like the race dataset.\nsubgoal 2: developing a context window architecture\nby stringing together downsampling and feature extraction layers, like in a cnn, it is possible that we could develop a context window architecture on top of the pre-trained short-context qa models to aid it in answering standardized test questions\ngoal 2: predicting answers\nour primary goal is to adapt question-answering (qa) models that have been trained on datasets like reclor or openbookqa, in which the context is around a paragraph of text, to answer standardized test reading comprehension questions (e.g. sat, act) where the context is a long passage of several paragraphs of text. we will integrate the context window model with these qa models by sending the output of the context window as the data to fine-tune the pre-trained models with.\nsubgoal 1: integration of both architectures\nwe will have to determine the optimal pipeline to integrate the inputs and outputs of both models.\nsubgoal 2: fine-tuning model\nthe qa models will have to be fine-tuned across various datasets. our goal is to determine what learnings do or do not generalize.\nrelated work\nare you aware of any, or is there any prior work that you drew on to do your project?\nin this section, also include urls to any public implementations you find of the paper you\u2019re trying to implement. please keep this as a \u201cliving list\u201d--if you stumble across a new implementation later down the line, add it to this list.\npaper that used image and natural language processing to solve sat geometry problems. achieved a 49% score on official sat geometry questions. http://geometry.allenai.org/assets/emnlp2015.pdf\ngpt-3 reading comprehension: https://arxiv.org/pdf/2005.14165.pdf\nreclor: a reading comprehension dataset requiring logical reasoning: https://openreview.net/forum?id=hjgjtt4tvb\ncan a suit of armor conduct electricity? a new dataset for open book question answering: https://aclanthology.org/d18-1260.pdf\nbert: pre-training of deep bidirectional transformers for language understanding: https://huggingface.co/transformers/model_doc/bert.html (bertformultiplechoice: https://huggingface.co/transformers/model_doc/bert.html#bertformultiplechoice)\nliamf fine-tuned roberta: https://huggingface.co/liamf-usp/roberta-large-finetuned-race\nconstructing transformers for longer sequences with sparse attention methods: https://ai.googleblog.com/2021/03/constructing-transformers-for-longer.html\nrace: large-scale reading comprehension dataset from examinations: https://arxiv.org/abs/1704.04683\ndata\nwhat data are you using (if any)?\nfor our data, we plan to use sat and act reading comprehension questions, which are freely accessible from practice test pdfs that we can download and process into text. while we may use the obtained data as the sole finetuning data, we are also considering using another dataset with long-passage questions as the fine tuning dataset, and then the sat and act reading comprehension questions as a final test dataset to evaluate our model\u2019s performance.\nhow big is it? will you need to do significant preprocessing?\nsince the size of each individual sat reading comprehension passage is pretty big, one of our goals is to build a feature extraction model that can be used to process those passages into reasonably-sized contexts. this will be used as the data for the actual qa-prediction model.\nmethodology\nwhat is the architecture of your model?\nas transformers are the state of the art for natural language processing tasks, we are inclined to use one or more transformers as the core of the model. we have also thought about delegating a supplementary model for classifying sat questions into certain categories, and then choosing an expert model to ask for an answer from based on the category. in addition, since many sat questions are based off of certain sections of text, it may help to build an additional model that learns how to map question hints like (lines 33-47) to a left and right index on the tokens to select for the transformer.\nhow are you training the model?\nwe will use pre-trained models (reclor, gpt-3, openbookqa, and/or bert) that have a basic idea of reading comprehension and attempt to fine-tune them on other datasets, like sat/act reading comprehension questions, or within each other like openbookqa with reclor or vice-versa.\njustify your design. also note some backup ideas you may have to experiment with if you run into issues.\nthis design is meant to observe how generalizable reading comprehension problems are across different levels. since transformers are data hungry, using a nonspecific, pre-trained model can be helpful for establishing baseline behavior. fine-tuning on more complex windows will yield information on how similar cross-level standardized tests are, or not!\nthe main challenge we anticipate will be correctly classifying the optimal window size for long passages, so our backup plan is to use data with shorter contexts and compare them exclusively with each other (ex: reclor, which sources mainly from the gre and lsat; and openbookqa, which sources from elementary science quizzes).\nmetrics\nwhat constitutes \u201csuccess?\u201d\naccuracy (namely the number of correctly answered questions divided by the total number of questions) seems to be an applicable metric for our model.\nwhat experiments do you plan to run?\nwe plan to evaluate our model\u2019s performance on standardized testing questions from a variety of tests, namely the sat and the act but perhaps also the gre and other tests.\naccuracy goals\n[ x ] base: > 25%\n[ x ] target: > 33%\n[ ] stretch: > 50%\nethics\nwhy is deep learning a good approach to this problem?\nstandardized testing is fairly important in the current model of the us education system. it is important to critically evaluate the purpose and efficacy of these important assessments, and using deep learning models to back out reading comprehension from a purely mechanical viewpoint can be valuable towards judging the fairness or complexity of what it takes to \u201cdo well\u201d. the goal is not to \u201csolve\u201d reading comprehension with pre-trained, state-of-the-art deep learning models. instead, our hope is to use deep learning as an evaluation -----> tool !!!  to quantify and qualify what exactly standard assessments aim to test with regards to generalization and reading comprehension.\nhow are you planning to quantify or measure error or success? what implications does your quantification have?\nerror and success will be measured by the accuracy score of the model on real tests questions and answers: a straight percentage of (num_correct / num_questions) is how the model\u2019s accuracy will be calculated. this way, it will be simple to easily relate the model\u2019s performance to human performance on the same assessment.\ndivision of labor\ndustin: data collection, fine-tuning model, context window architecture\nmilanca: integrating pre-trained models, context window architecture", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 6, "media": null, "medialink": null, "identifyer": 59505254}, {"Unnamed: 0": 5262, "autor": "Pick-Le-Ball", "date": null, "content": "Inspiration\nAs aspiring sports fans, we were irritated by the fact that many sports applications are unable to present game statistics in a simple and elegant manner. We were driven by this belief to create a quick and handy tool to see the time and events of your favorite NBA games along with readily comparable player statistics.\nWhat it does\nDelivers historical game data on a locally hosted web server with customizable dates. For each game, player rosters can be analyzed and player stats including PPG, SPG, APG, and games played can be checked.\nHow we built it\nUtilized React.js with JSX, TypeScript, and CSS. Furthermore, the team accessed the balldontlie API was used to gather intelligence and data.\nChallenges we ran into\nHenri: Found difficulty with CORS origin issues as well as integrations of the balldontlie API. Thomas: Creating enhancing visuals and the implementation of dropping elements on the webpage.\nAccomplishments that we're proud of\nIt was the first time both members interacted with an open source API. We learned how to use tools like Postman to extract important information for our website. Furthermore, we are proud of the professional designs that we were able to carry out in React.\nWhat we learned\nWe learned how to build a full stack application using React and various components while dividing work effectively in order to avoid merge conflicts.\nWhat's next for Pick-Le-Ball\nOur team looks forward to completing and potentially polishing some features that the site already has. We will consider the use of a stronger API, the completion of the confetti feature, and supporting more user interaction when looking at the player stats.", "link": "https://devpost.com/software/pickleroll", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nas aspiring sports fans, we were irritated by the fact that many sports applications are unable to present game statistics in a simple and elegant manner. we were driven by this belief to create a quick and handy -----> tool !!!  to see the time and events of your favorite nba games along with readily comparable player statistics.\nwhat it does\ndelivers historical game data on a locally hosted web server with customizable dates. for each game, player rosters can be analyzed and player stats including ppg, spg, apg, and games played can be checked.\nhow we built it\nutilized react.js with jsx, typescript, and css. furthermore, the team accessed the balldontlie api was used to gather intelligence and data.\nchallenges we ran into\nhenri: found difficulty with cors origin issues as well as integrations of the balldontlie api. thomas: creating enhancing visuals and the implementation of dropping elements on the webpage.\naccomplishments that we're proud of\nit was the first time both members interacted with an open source api. we learned how to use tools like postman to extract important information for our website. furthermore, we are proud of the professional designs that we were able to carry out in react.\nwhat we learned\nwe learned how to build a full stack application using react and various components while dividing work effectively in order to avoid merge conflicts.\nwhat's next for pick-le-ball\nour team looks forward to completing and potentially polishing some features that the site already has. we will consider the use of a stronger api, the completion of the confetti feature, and supporting more user interaction when looking at the player stats.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59505262}, {"Unnamed: 0": 5279, "autor": "myvision", "date": null, "content": "Description\nMyVision is a free online image annotation tool used for generating computer vision based ML training data. It is designed with the user in mind, offering features to speed up the labelling process and help maintain workflows with large datasets.\nFeatures\nDraw bounding boxes and polygons to label your objects. Polygon manipulation is enriched with additional features to edit, remove and add new points.\nAnnotating objects can be a difficult task... You can skip all the hard work and use a pre-trained machine learning model to automatically annotate the objects for you. MyVision leverages the popular 'COCO-SSD' model to generate bounding boxes for your images and by operating locally on your browser - retain all data within the privacy of your computer.\nYou can import existing annotation projects and continue working on them in MyVision. This process can also be used to convert datasets from one format to another:\nLocal setup\nNo setup is required to run this project, open the index.html file and you are all set! However, if you want to make changes or contribute to this repository, please follow the instructions below:\n# Requirements: Node version 10+ and NPM version 6+\n# Install node dependencies:\n$ npm install\n# Run the project in watch mode:\n$ npm run watch\n# All changes should be made in the src directory and observed in publicDev\nCitation\n@MISC{MyVision,\nauthor = {Ovidijus Parsiunas},\ntitle = {{MyVision}},\nhowpublished = {\\url{https://github.com/OvidijusParsiunas/myvision}},\nyear = {2019},\n}", "link": "https://devpost.com/software/myvision", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "description\nmyvision is a free online image annotation -----> tool !!!  used for generating computer vision based ml training data. it is designed with the user in mind, offering features to speed up the labelling process and help maintain workflows with large datasets.\nfeatures\ndraw bounding boxes and polygons to label your objects. polygon manipulation is enriched with additional features to edit, remove and add new points.\nannotating objects can be a difficult task... you can skip all the hard work and use a pre-trained machine learning model to automatically annotate the objects for you. myvision leverages the popular 'coco-ssd' model to generate bounding boxes for your images and by operating locally on your browser - retain all data within the privacy of your computer.\nyou can import existing annotation projects and continue working on them in myvision. this process can also be used to convert datasets from one format to another:\nlocal setup\nno setup is required to run this project, open the index.html file and you are all set! however, if you want to make changes or contribute to this repository, please follow the instructions below:\n# requirements: node version 10+ and npm version 6+\n# install node dependencies:\n$ npm install\n# run the project in watch mode:\n$ npm run watch\n# all changes should be made in the src directory and observed in publicdev\ncitation\n@misc{myvision,\nauthor = {ovidijus parsiunas},\ntitle = {{myvision}},\nhowpublished = {\\url{https://github.com/ovidijusparsiunas/myvision}},\nyear = {2019},\n}", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59505279}, {"Unnamed: 0": 5285, "autor": "promp2slip", "date": null, "content": "\u2705Elevator pitch\nThis library is testing the ethics of language models by using natural adversarial texts.\nThis tool allows for short and simple code and validation with little effort.\nEasy installation with pip.\n\u2705Inspiration\nBackground\nIn natural language processing (NLP), the development of language models (LM) has led to the exploration of applications such as chatbots, writing aids, text summarization, and NPCs in games.\nThere are increasing opportunities for language models to provide an interactive and human-like UX to end users. However, generative models have issues with harmful or discriminatory remarks, as in the case of Microsoft's chatbot Tay, which was shut down due to discriminatory remarks. The AI community has a responsibility to address this issue so that people of all races and genders etc. can benefit from AI.\nIn other words, engineers about NLP should not ensure that we make discriminatory remarks, unethical statements, propaganda, or detrimental information disclosure to the language models..\nSoftware engineers generally assure the behavior of their algorithms by test codes. Test codes have much higher persistence and real-time performance than heuristic one. However, it is very difficult to create test codes to ensure the language model doesn\u2019t have toxic behaviors, and the difficulty stems from black-boxed features of machine learning models. A problem to search for inputs which take a particular output in the model is equivalent to the inverse problem of a normal use case. This makes it remarkably difficult to mathematically prove that a particular input does not exist.\nWe need a simple and general-purpose method to verify that natural language generative models in PyTorch do not make discriminatory or inappropriate statements in natural contexts. However, we could not find the library which meets these requirements when we had surveyed. In other words, it suggests that it is difficult to sustain responsible AI, and we are very concerned about this problem.\nFor this reason, we developed \u201cprompt2slip\". This library allows you to input a language model and a word, and it will output sentences that contain the specified word using the language model, and qualitatively verify the danger.\nprompt2slip makes it easy to test against LM. At the same time, it aims to target and test the topics that engineers want to verify the most.. prompt2slip can be used to verify that the trained models are \"responsible AI\" in the unittest.\nPrompt2slip's most important mission is to help all natural language engineers to provide sustainable and responsible AI.\nWhere did you get the idea?\nAdversarial examples are known as a technique to obtain arbitrary output from deep learning models. A common way to generate adversarial examples is to define an adversarial loss function which encourages miss prediction and minimizes the loss.\nIn image and voice recognition, it is known that it is possible to generate adversarial perturbations which are difficult for humans to detect by introducing perceptibility constraints into the optimization. However, in NLP, the discrete nature of text data makes it difficult to introduce perceptibility constraints. As a result, the adversarial samples obtained by existing methods were grammatically and semantically unnatural sentences.\nGBDA (Gradient-based Distributional Attack), which is a state-of-the-art algorithm for generating natural adversarial samples as sentences, has been proposed. We considered applying GBDA not only as an attack method, but also as a testing tool for language models.\n\u2705What it does\n\"prompt2slip\" provides the function to search for prompts which cause appearance of any specific word against a pre trained natural language generation model. Furthermore, with user customization, it can be applied to a wide range of tasks, including classification tasks.If you want to generate a hostile sample for a classification model, you can simply override the method to compute the adversarial loss function to generate a natural adversarial text.\nThe unique feature of this library is that it can generate test cases for verifying the danger of a pre-trained natural language model with a few lines of code.\nHere is a minimal example of this library\nimport prompt2slip\nfrom transformers import GPT2Tokenizer, GPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2Model.from_pretrained('gpt2')\nbase_text = [\"This project respects pytorch developers.\"]\ntarget_word = [\"caffe2\"]\ntarget_ids = torch.Tensor(tokenizer.convert_tokens_to_ids(target_words))\nattaker = CLMAttacker(model,tokenizer)\noutput=attacker.attack_by_text(texts,target_ids)\nIn this way, you can easily generate an adversarial example only by passing the trained language model, the original prompt, and the words to verify to prompt2slip.CLMAttacker. With this adversarial prompt as a test case, you can verify the reliability of your language model.\n\u2705How we built it\nprompt2slip relies on the following libraries PyTorch torchtyping transformers\nFor supporting \u201ctransformer\u201d, which is one of the most used architectures in NLP, we have added support for Hugging Face\u2019s transformers as well as PyTorch. In addition, we have also added type annotations with \u201ctorchtyping\u201d to help developers understand the shape of PyTorch tensor.\nWe have made the library extensible: it has base classes for PyTorch and Transformers. By inheriting these base classes, developers can generate adversarial samples for a variety of task models in addition to text generation.\nIn addition to the implementation of Chuan Guo et al. 2021[1], we have implemented a loss function to make specific tokens appear for trained language models.\n\u2705Challenges we ran into\nWe faced two technical challenges in this project.\nThe first challenge was to design a new loss function. In the paper, only the classification problem was mentioned. Since the goal of \"prompt2slip\" was to make the specified token appear from language models, we developed a new loss function. By minimizing our new loss function, you can maximize the probability that the generated text will contain the specified token.\nThe second challenge is to stabilize the optimization when generating adversarial examples. Initially, simply adapting the GBDA optimization to our problem caused instability in the optimization. After a lot of trial and error, we found out that the cause was randomness caused by sampling. By increasing the number of samplings and batch size, we were able to reduce the randomness and stabilize the optimization.\n\u2705Accomplishments that we're proud of\nThis library is testing the ethics of language models by using natural adversarial texts. It provides a simple yet powerful interface that can be easily handled by engineers who are not familiar with hostile samples. Can be used for non-English language models It contains detailed type definitions and docstrings, so the code itself can also serve as documentation. Hostile samples, which are sometimes used as an attack technique, were applied as a testing tool. By inheriting from the base class, it has the versatility to be extended by engineers for many problem settings. We are building CI/CD into our development environment with the thought that prompt2slip itself will be sustainably responsible.\n\u2705What we learned\nTests against ML\nWe initially learned various vulnerabilities that LM potentially has, and the difficulty in proving that these vulnerabilities exist in the LM. In other words, it is difficult to integrate deep learning models into services and operate responsible AI on an ongoing basis. When we firstly became aware of this situation correctly, we naturally assumed that there were a wide variety of generalized testing tools available, but as far as we could find out, no such testing tools existed.\nDevelopment Challenges\nWe learned that in order to make it a viable OSS library, it is important to have more than just the theory of machine learning, such as test code, function design using Typing, docstrings and documentation. We also learned that many of the prototypes provided by researchers did not meet these design requirements, so we set out to design something that would be easy for engineers to use.\nTeam Building\nOur team building has been done remotely because of COVID-19. In other words, we had to start development without knowing each other's personalities, moods, heights, or favorite foods. Communication for the early stages of a project did not work straightforward, and we realized their difficulty. During an extensive try and error process, we learned that using not only technical tools like an editor but also communication tools appropriately such as Google Jamboard, Slack, and Kanban boards improve the development efficiency.\nIn addition, the experience told us the benefits of asynchronous communication. Since we were all full time workers, we had to do meetings at midnight. Then, we found that not all members could always keep a high level of concentration. After recognizing this problem, we have consciously engaged our development using asynchronous communication. Thanks to that, we could write high-quality codes and pull requests, and introduced tools to improve the environment. As a result, we learned that working in each comfortable time enhances the quality of our product.\n\u2705What's next for prompt2slip\nCurrently, the naturalness of the prompts generated by \u201cprompt2slip\u201d can be improved; we plan to tune the loss function and hyperparameters. We also plan to add other natural language processing tasks such as series transformation for machine translation and dialogue generation. We will be adding documentation for this library as well.\n\u2705References\n[1] Gradient-based Adversarial Attacks against Text Transformers (Chuan Guo et al., 2021)", "link": "https://devpost.com/software/promp2slip", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "\u2705elevator pitch\nthis library is testing the ethics of language models by using natural adversarial texts.\nthis -----> tool !!!  allows for short and simple code and validation with little effort.\neasy installation with pip.\n\u2705inspiration\nbackground\nin natural language processing (nlp), the development of language models (lm) has led to the exploration of applications such as chatbots, writing aids, text summarization, and npcs in games.\nthere are increasing opportunities for language models to provide an interactive and human-like ux to end users. however, generative models have issues with harmful or discriminatory remarks, as in the case of microsoft's chatbot tay, which was shut down due to discriminatory remarks. the ai community has a responsibility to address this issue so that people of all races and genders etc. can benefit from ai.\nin other words, engineers about nlp should not ensure that we make discriminatory remarks, unethical statements, propaganda, or detrimental information disclosure to the language models..\nsoftware engineers generally assure the behavior of their algorithms by test codes. test codes have much higher persistence and real-time performance than heuristic one. however, it is very difficult to create test codes to ensure the language model doesn\u2019t have toxic behaviors, and the difficulty stems from black-boxed features of machine learning models. a problem to search for inputs which take a particular output in the model is equivalent to the inverse problem of a normal use case. this makes it remarkably difficult to mathematically prove that a particular input does not exist.\nwe need a simple and general-purpose method to verify that natural language generative models in pytorch do not make discriminatory or inappropriate statements in natural contexts. however, we could not find the library which meets these requirements when we had surveyed. in other words, it suggests that it is difficult to sustain responsible ai, and we are very concerned about this problem.\nfor this reason, we developed \u201cprompt2slip\". this library allows you to input a language model and a word, and it will output sentences that contain the specified word using the language model, and qualitatively verify the danger.\nprompt2slip makes it easy to test against lm. at the same time, it aims to target and test the topics that engineers want to verify the most.. prompt2slip can be used to verify that the trained models are \"responsible ai\" in the unittest.\nprompt2slip's most important mission is to help all natural language engineers to provide sustainable and responsible ai.\nwhere did you get the idea?\nadversarial examples are known as a technique to obtain arbitrary output from deep learning models. a common way to generate adversarial examples is to define an adversarial loss function which encourages miss prediction and minimizes the loss.\nin image and voice recognition, it is known that it is possible to generate adversarial perturbations which are difficult for humans to detect by introducing perceptibility constraints into the optimization. however, in nlp, the discrete nature of text data makes it difficult to introduce perceptibility constraints. as a result, the adversarial samples obtained by existing methods were grammatically and semantically unnatural sentences.\ngbda (gradient-based distributional attack), which is a state-of-the-art algorithm for generating natural adversarial samples as sentences, has been proposed. we considered applying gbda not only as an attack method, but also as a testing tool for language models.\n\u2705what it does\n\"prompt2slip\" provides the function to search for prompts which cause appearance of any specific word against a pre trained natural language generation model. furthermore, with user customization, it can be applied to a wide range of tasks, including classification tasks.if you want to generate a hostile sample for a classification model, you can simply override the method to compute the adversarial loss function to generate a natural adversarial text.\nthe unique feature of this library is that it can generate test cases for verifying the danger of a pre-trained natural language model with a few lines of code.\nhere is a minimal example of this library\nimport prompt2slip\nfrom transformers import gpt2tokenizer, gpt2model\ntokenizer = gpt2tokenizer.from_pretrained('gpt2')\nmodel = gpt2model.from_pretrained('gpt2')\nbase_text = [\"this project respects pytorch developers.\"]\ntarget_word = [\"caffe2\"]\ntarget_ids = torch.tensor(tokenizer.convert_tokens_to_ids(target_words))\nattaker = clmattacker(model,tokenizer)\noutput=attacker.attack_by_text(texts,target_ids)\nin this way, you can easily generate an adversarial example only by passing the trained language model, the original prompt, and the words to verify to prompt2slip.clmattacker. with this adversarial prompt as a test case, you can verify the reliability of your language model.\n\u2705how we built it\nprompt2slip relies on the following libraries pytorch torchtyping transformers\nfor supporting \u201ctransformer\u201d, which is one of the most used architectures in nlp, we have added support for hugging face\u2019s transformers as well as pytorch. in addition, we have also added type annotations with \u201ctorchtyping\u201d to help developers understand the shape of pytorch tensor.\nwe have made the library extensible: it has base classes for pytorch and transformers. by inheriting these base classes, developers can generate adversarial samples for a variety of task models in addition to text generation.\nin addition to the implementation of chuan guo et al. 2021[1], we have implemented a loss function to make specific tokens appear for trained language models.\n\u2705challenges we ran into\nwe faced two technical challenges in this project.\nthe first challenge was to design a new loss function. in the paper, only the classification problem was mentioned. since the goal of \"prompt2slip\" was to make the specified token appear from language models, we developed a new loss function. by minimizing our new loss function, you can maximize the probability that the generated text will contain the specified token.\nthe second challenge is to stabilize the optimization when generating adversarial examples. initially, simply adapting the gbda optimization to our problem caused instability in the optimization. after a lot of trial and error, we found out that the cause was randomness caused by sampling. by increasing the number of samplings and batch size, we were able to reduce the randomness and stabilize the optimization.\n\u2705accomplishments that we're proud of\nthis library is testing the ethics of language models by using natural adversarial texts. it provides a simple yet powerful interface that can be easily handled by engineers who are not familiar with hostile samples. can be used for non-english language models it contains detailed type definitions and docstrings, so the code itself can also serve as documentation. hostile samples, which are sometimes used as an attack technique, were applied as a testing tool. by inheriting from the base class, it has the versatility to be extended by engineers for many problem settings. we are building ci/cd into our development environment with the thought that prompt2slip itself will be sustainably responsible.\n\u2705what we learned\ntests against ml\nwe initially learned various vulnerabilities that lm potentially has, and the difficulty in proving that these vulnerabilities exist in the lm. in other words, it is difficult to integrate deep learning models into services and operate responsible ai on an ongoing basis. when we firstly became aware of this situation correctly, we naturally assumed that there were a wide variety of generalized testing tools available, but as far as we could find out, no such testing tools existed.\ndevelopment challenges\nwe learned that in order to make it a viable oss library, it is important to have more than just the theory of machine learning, such as test code, function design using typing, docstrings and documentation. we also learned that many of the prototypes provided by researchers did not meet these design requirements, so we set out to design something that would be easy for engineers to use.\nteam building\nour team building has been done remotely because of covid-19. in other words, we had to start development without knowing each other's personalities, moods, heights, or favorite foods. communication for the early stages of a project did not work straightforward, and we realized their difficulty. during an extensive try and error process, we learned that using not only technical tools like an editor but also communication tools appropriately such as google jamboard, slack, and kanban boards improve the development efficiency.\nin addition, the experience told us the benefits of asynchronous communication. since we were all full time workers, we had to do meetings at midnight. then, we found that not all members could always keep a high level of concentration. after recognizing this problem, we have consciously engaged our development using asynchronous communication. thanks to that, we could write high-quality codes and pull requests, and introduced tools to improve the environment. as a result, we learned that working in each comfortable time enhances the quality of our product.\n\u2705what's next for prompt2slip\ncurrently, the naturalness of the prompts generated by \u201cprompt2slip\u201d can be improved; we plan to tune the loss function and hyperparameters. we also plan to add other natural language processing tasks such as series transformation for machine translation and dialogue generation. we will be adding documentation for this library as well.\n\u2705references\n[1] gradient-based adversarial attacks against text transformers (chuan guo et al., 2021)", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59505285}, {"Unnamed: 0": 5308, "autor": "ACCESS IT!", "date": null, "content": "ACCESS IT is an application focusing on making everything accessible to everyone!\nACCESSIT: A safe space and a platform for raising awareness on disabilities.\nInspiration\nToday, around 15 per cent of the world\u2019s population, or estimated 1 billion people, live with disabilities. They are the world\u2019s largest minority. For deaf people, however, the major barriers are Lack of recognition, Acceptance and use of sign language in all areas of life, and Lack of respect for their cultural and linguistic identity. Acceptance too sign language is one of the major ways how we can better communicate and understand them. So, the idea of Access it rose with a mission of accessibility for everyone!\nOur Goal\nTo promote awareness about disabled individuals through interactive techniques and to disseminate information about them through various interactive channels and help everyone effectively communicate in sign language.\nWhat it does\nOur Key Features:\n\u2604\ufe0fCommunicate with ASL (Translator)\nOur application has a speech to sign language translation tool where a user can simply speak, and the message will be translated to sign language.\nIt also has a text to ASL translation support, users can simply write/paste their message and the message is translated to ASL.\n\u2604\ufe0f Opportunity Page:\nIt's the place where opportunities for the disabled people specifically will be posted, also various other information about upcoming events, volunteering activities will be present. Any student can click on hashtag, check all opportunities/events under that hashtag, and subscribe to it to get further notifications! Here users can also come and make an announcement regarding any upcoming opportunity/ volunteering opportunity/ event. Users can add the event/opportunity description, hashtag, and respective links to the event.\n\u2604\ufe0fMood Tracker and Notes\nIt's a safe space for oneself, a place to be us, a place to jot down all the notes and have them handy anytime! Moreover, color the tracker with the shades of your mood over time!\n\u2604\ufe0fForum:\nUsing forums usually entails mustering the confidence to tell your tale. People with sensitive stories or who are socially anxious frequently avoid employing this kind of catharsis. Here they can either be public or anonymously share their views on some specific topic or create some new threads and share their heart out! It's an open space to discuss things freely. Users can create threads and continue their discussion either openly or anonymously.\n\u2604\ufe0fArticles\nA place where specifically picked articles will be shared with our users. it'll help us spread more awareness and openness in the community!\nHow we built it\nWe built it using Angular and Firebase.\nChallenges we ran into\nImplementing all the features in the given time constraint was one of the major challenges for us! Managing our time, the right way helped us a lot in successfully building the product. It was an amazing learning experience for us overall!\nAccomplishments that we're proud of\nWe are proud of overcoming our challenges and building the application.\nWhat we learned\nWe learned a lot about ASL and disabled persons while developing an app to raise awareness about it. We had a lot of features planned at first, but we realized they weren't viable and didn't fit the narrative. We learned how to write a narrative for our project. During the hackathon, we also experimented with a few recent technologies.\nWhat's next for ACCESS IT\nWe successfully used lobe for image recognition and translating the images (ASL language) into English text. We look forward to integrate it with our application in future and provide asl to English translator on our application via image recognition. Many other amazing features also to be added!\nSo let's ACCESS IT!", "link": "https://devpost.com/software/access-it", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "access it is an application focusing on making everything accessible to everyone!\naccessit: a safe space and a platform for raising awareness on disabilities.\ninspiration\ntoday, around 15 per cent of the world\u2019s population, or estimated 1 billion people, live with disabilities. they are the world\u2019s largest minority. for deaf people, however, the major barriers are lack of recognition, acceptance and use of sign language in all areas of life, and lack of respect for their cultural and linguistic identity. acceptance too sign language is one of the major ways how we can better communicate and understand them. so, the idea of access it rose with a mission of accessibility for everyone!\nour goal\nto promote awareness about disabled individuals through interactive techniques and to disseminate information about them through various interactive channels and help everyone effectively communicate in sign language.\nwhat it does\nour key features:\n\u2604\ufe0fcommunicate with asl (translator)\nour application has a speech to sign language translation -----> tool !!!  where a user can simply speak, and the message will be translated to sign language.\nit also has a text to asl translation support, users can simply write/paste their message and the message is translated to asl.\n\u2604\ufe0f opportunity page:\nit's the place where opportunities for the disabled people specifically will be posted, also various other information about upcoming events, volunteering activities will be present. any student can click on hashtag, check all opportunities/events under that hashtag, and subscribe to it to get further notifications! here users can also come and make an announcement regarding any upcoming opportunity/ volunteering opportunity/ event. users can add the event/opportunity description, hashtag, and respective links to the event.\n\u2604\ufe0fmood tracker and notes\nit's a safe space for oneself, a place to be us, a place to jot down all the notes and have them handy anytime! moreover, color the tracker with the shades of your mood over time!\n\u2604\ufe0fforum:\nusing forums usually entails mustering the confidence to tell your tale. people with sensitive stories or who are socially anxious frequently avoid employing this kind of catharsis. here they can either be public or anonymously share their views on some specific topic or create some new threads and share their heart out! it's an open space to discuss things freely. users can create threads and continue their discussion either openly or anonymously.\n\u2604\ufe0farticles\na place where specifically picked articles will be shared with our users. it'll help us spread more awareness and openness in the community!\nhow we built it\nwe built it using angular and firebase.\nchallenges we ran into\nimplementing all the features in the given time constraint was one of the major challenges for us! managing our time, the right way helped us a lot in successfully building the product. it was an amazing learning experience for us overall!\naccomplishments that we're proud of\nwe are proud of overcoming our challenges and building the application.\nwhat we learned\nwe learned a lot about asl and disabled persons while developing an app to raise awareness about it. we had a lot of features planned at first, but we realized they weren't viable and didn't fit the narrative. we learned how to write a narrative for our project. during the hackathon, we also experimented with a few recent technologies.\nwhat's next for access it\nwe successfully used lobe for image recognition and translating the images (asl language) into english text. we look forward to integrate it with our application in future and provide asl to english translator on our application via image recognition. many other amazing features also to be added!\nso let's access it!", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59505308}, {"Unnamed: 0": 5326, "autor": "Zeitgeist", "date": null, "content": "Inspiration\nI great enjoy retrospective and the conversations that happen during this time. Unfortunately from what I noticed, not every company does this and I would love to encourage using a start, stop, continue tool that's free and extremely simple to use.\nWhat it does\nThis app is a real time messaging service that updates within its category and other users will see the changes in real time.\nThere are categories such as:\nStart: The user will mention what they recommend the team should start doing.\nStop: Something that the user disagrees with within the team.\nContinue: A spotlight to acknowledge the positive.\nIf the user sees a message that they agree with, the user can \"plus one\" to acknowledge they agree.\nOnce it's review time, the user can gather the message that they want to create an action item by dragging that message to the action item section.\nAn extra copy feature for that message will appear making it easier to send it to an outside service.\nHow we built it\nThe frontend was built with React and Apollo. The backend was built with ExpressJS/Nodejs and GraphQL\nThere was a lot of fun features that we enjoyed working with, such as microsoft's speech to text feature that works amazingly well. In order to have that work, we used navigator's microphone capture.\nFor any data that could be updated on any viewer was made possible with GraphQL's subscription service.\nWhat's next for Zeitgeist\nI really wanted to implement api to create tickets straight from our app to Jira, however, would require the user to set up their jira domain, issueType, etc. It was not ideal. I would like to think of an alternative to make that possible...", "link": "https://devpost.com/software/zeitgeist-3zwg7o", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ni great enjoy retrospective and the conversations that happen during this time. unfortunately from what i noticed, not every company does this and i would love to encourage using a start, stop, continue -----> tool !!!  that's free and extremely simple to use.\nwhat it does\nthis app is a real time messaging service that updates within its category and other users will see the changes in real time.\nthere are categories such as:\nstart: the user will mention what they recommend the team should start doing.\nstop: something that the user disagrees with within the team.\ncontinue: a spotlight to acknowledge the positive.\nif the user sees a message that they agree with, the user can \"plus one\" to acknowledge they agree.\nonce it's review time, the user can gather the message that they want to create an action item by dragging that message to the action item section.\nan extra copy feature for that message will appear making it easier to send it to an outside service.\nhow we built it\nthe frontend was built with react and apollo. the backend was built with expressjs/nodejs and graphql\nthere was a lot of fun features that we enjoyed working with, such as microsoft's speech to text feature that works amazingly well. in order to have that work, we used navigator's microphone capture.\nfor any data that could be updated on any viewer was made possible with graphql's subscription service.\nwhat's next for zeitgeist\ni really wanted to implement api to create tickets straight from our app to jira, however, would require the user to set up their jira domain, issuetype, etc. it was not ideal. i would like to think of an alternative to make that possible...", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59505326}, {"Unnamed: 0": 5346, "autor": "Fake Media Detection Suite", "date": null, "content": "Fake media Detection Toolkit\nThe unprecedented growth of technology in the past few decades has led to massive flooding of the market with media forgery tools like Photoshop, gimp, Deep Dream Generator etc. These tempered media sources have contributed to other major problems like fake news and even cyberbullying. Furthermore, advancements of deep learning lead to a more severe problem known as Deepfakes and Generative Adversarial Networks(GAN) generated images and videos, that are even harder to detect and investigate in case of a legal breach. This project aims to address this challenge by implementing two novel deep learning models that can be used conjointly with some static models to provide complete protection against almost all types of image and video forgeries including Deepfakes, GAN generated forged media, photoshopped media (splicing, copy and move forgeries). For Deepfakes detection, this project implements a Timedistributed CNN-LSTM neural network and for GAN generated media detection, this project uses a fine-tuned DenseNet model as the base Convolutional neural network. The statistical models include generation of images by masking original image through various algorithms like Error Level Analysis, Laplace transforms, variance masking, pixel density algorithm, and generation of Exifdata of media through various tools and packages like hachoir, ExifTool, etc. Compared with existing strategies, the evaluation results demonstrate that the novel strategy can serve as a one-stop tool for forensic investigators, thus providing holistic protection against threats of today(pixel based forgeries) as well as future(vector based forgeries).\nData Preparation\nThe mean channel of images/frames was removed.\nAll frames/images were normalized\nThe fully-connected layer at the top of the network was removed ( for our DenseNet pretrained fine-tuned models)\nAll frames and images were resized according to adjust with our pretrained models.\nVarious data augmentation functions like adaptive Gaussian noise, emboss image, salt image, black hat filter, rotation, flipping, Affine Transformation, warping, increasing contrast were used to make the model more robust to changes.\nFrom the Keras preprocessing package, we used ImageDataGenerator to augment image input and use transformations as rescaling, shearing, zoom, flipping, height shift, width shift, fill mode, data format, brightness, and feature-wise normalize.\nImplemented models {AI-powered}\n1) Deepfakes\nFor Deepfakes, we used a Timedistributed CNN-LSTM neural network. TimeDistributed Layer keeps input to output relation proper, and avoid mixing of outputs. In this model, the function of CNN is feature extraction, and LSTM for sequence processing. In CNN, we used the initializer as glorot uniform, and reg lambda parameter to be 0.001. Various convolutional layers of variable kernel filters(32,64,128,256,512) are used to extract different features of frame/images. Different strides were used in pooling to gain access to different aspects of the image.The 2048-dimensional feature vectors after the last pooling layers are then used as the sequential LSTM input. We used stacked LSTM to increase sequence processing at different intervals ( units = 512, 256). Stacked LSTM gives robustness and increases the accuracy of our model as almost all correlations between features extracted by CNN are analyzed by it.\n2) GAN generated media\nFor the detection of GAN images/frames, we used a Finetuned DenseNet network along with an simple ANN( Artificial Neural Network). Sequential training was done on the network. Firstly, all layers apart from the last two layers of Densenet were frozen, and the model was trained on the rest of the layers. After initial training, all layers were unfrozen, and the whole model was again trained. Due to simultaneous training, the end layer\u2019s (deciding layers) converge best to the global optima and give an accuracy boost to our model. Also Dropout(F 1 ) and batch normalization layers(F 2 ) were added regularly to prevent overfitting.\nImplemented Statistical models\nExif-Data Analysis\nIn our model, metadata extraction was done for all videos/images by using sequential use of a tool called ExifTool and a python package called hachoir. After data retrieval from both tools, the redundant data was removed and useful data like device id, device manufacturer, software used, encoder, etc were carved out into a .csv file. This piece of information, which is often neglected, can open new avenues to investigation and may also reveal personal identification proofs like the MAC address of original equipment used.\nMasking Algorithms Image masking is a process by which we can hide some aspects or parts of an image and highlight some other aspects of the image like highlighting high illuminance pixel entropy areas. Several masking algorithms like pixel density, Bitwise-AND masking, Variance Masking, Laplace detector, ELA are used in our project that can accurately detect any type of pixel based manipulation like photoshopped media(splicing, etc)\nFuture Scope\nThe future additions that could be made to this project are:\nNeural networks can be debugged for finding the performance of each layer of the network, thus finding any dead neuron or low learning by any layer.\nMore juicy forensic artifacts could be collected by adding tools like Ghiro, JPEGsnoop ( better source to find last device used), stegsolve ( switch through the layers and look for abnormalities) to the main project, thus diversifying the static metadata information collected.\nReverse searching of image/video could be added to the project, thus backtracking original digital media resource (scan through different open source search engines)", "link": "https://devpost.com/software/fake-media-detection-suite-jmp0yn", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "fake media detection toolkit\nthe unprecedented growth of technology in the past few decades has led to massive flooding of the market with media forgery tools like photoshop, gimp, deep dream generator etc. these tempered media sources have contributed to other major problems like fake news and even cyberbullying. furthermore, advancements of deep learning lead to a more severe problem known as deepfakes and generative adversarial networks(gan) generated images and videos, that are even harder to detect and investigate in case of a legal breach. this project aims to address this challenge by implementing two novel deep learning models that can be used conjointly with some static models to provide complete protection against almost all types of image and video forgeries including deepfakes, gan generated forged media, photoshopped media (splicing, copy and move forgeries). for deepfakes detection, this project implements a timedistributed cnn-lstm neural network and for gan generated media detection, this project uses a fine-tuned densenet model as the base convolutional neural network. the statistical models include generation of images by masking original image through various algorithms like error level analysis, laplace transforms, variance masking, pixel density algorithm, and generation of exifdata of media through various tools and packages like hachoir, exiftool, etc. compared with existing strategies, the evaluation results demonstrate that the novel strategy can serve as a one-stop -----> tool !!!  for forensic investigators, thus providing holistic protection against threats of today(pixel based forgeries) as well as future(vector based forgeries).\ndata preparation\nthe mean channel of images/frames was removed.\nall frames/images were normalized\nthe fully-connected layer at the top of the network was removed ( for our densenet pretrained fine-tuned models)\nall frames and images were resized according to adjust with our pretrained models.\nvarious data augmentation functions like adaptive gaussian noise, emboss image, salt image, black hat filter, rotation, flipping, affine transformation, warping, increasing contrast were used to make the model more robust to changes.\nfrom the keras preprocessing package, we used imagedatagenerator to augment image input and use transformations as rescaling, shearing, zoom, flipping, height shift, width shift, fill mode, data format, brightness, and feature-wise normalize.\nimplemented models {ai-powered}\n1) deepfakes\nfor deepfakes, we used a timedistributed cnn-lstm neural network. timedistributed layer keeps input to output relation proper, and avoid mixing of outputs. in this model, the function of cnn is feature extraction, and lstm for sequence processing. in cnn, we used the initializer as glorot uniform, and reg lambda parameter to be 0.001. various convolutional layers of variable kernel filters(32,64,128,256,512) are used to extract different features of frame/images. different strides were used in pooling to gain access to different aspects of the image.the 2048-dimensional feature vectors after the last pooling layers are then used as the sequential lstm input. we used stacked lstm to increase sequence processing at different intervals ( units = 512, 256). stacked lstm gives robustness and increases the accuracy of our model as almost all correlations between features extracted by cnn are analyzed by it.\n2) gan generated media\nfor the detection of gan images/frames, we used a finetuned densenet network along with an simple ann( artificial neural network). sequential training was done on the network. firstly, all layers apart from the last two layers of densenet were frozen, and the model was trained on the rest of the layers. after initial training, all layers were unfrozen, and the whole model was again trained. due to simultaneous training, the end layer\u2019s (deciding layers) converge best to the global optima and give an accuracy boost to our model. also dropout(f 1 ) and batch normalization layers(f 2 ) were added regularly to prevent overfitting.\nimplemented statistical models\nexif-data analysis\nin our model, metadata extraction was done for all videos/images by using sequential use of a tool called exiftool and a python package called hachoir. after data retrieval from both tools, the redundant data was removed and useful data like device id, device manufacturer, software used, encoder, etc were carved out into a .csv file. this piece of information, which is often neglected, can open new avenues to investigation and may also reveal personal identification proofs like the mac address of original equipment used.\nmasking algorithms image masking is a process by which we can hide some aspects or parts of an image and highlight some other aspects of the image like highlighting high illuminance pixel entropy areas. several masking algorithms like pixel density, bitwise-and masking, variance masking, laplace detector, ela are used in our project that can accurately detect any type of pixel based manipulation like photoshopped media(splicing, etc)\nfuture scope\nthe future additions that could be made to this project are:\nneural networks can be debugged for finding the performance of each layer of the network, thus finding any dead neuron or low learning by any layer.\nmore juicy forensic artifacts could be collected by adding tools like ghiro, jpegsnoop ( better source to find last device used), stegsolve ( switch through the layers and look for abnormalities) to the main project, thus diversifying the static metadata information collected.\nreverse searching of image/video could be added to the project, thus backtracking original digital media resource (scan through different open source search engines)", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59505346}, {"Unnamed: 0": 5348, "autor": "Eat Smart", "date": null, "content": "Inspiration\nMore and more kids are suffering from obesity and diabetes in the United States. This is a problem because they don\u2019t know the importance of nutrition and crave unhealthy foods such as pizza too much. As technology improves, advertisements are everywhere showing kids how tasty and cheap fast food is.\nWhat it does\nThrough playing this game, kids will build up simple ideas and concepts about which food belongs to healthy food and which food they should avoid. They will realize eating unhealthy in the game will cause them to lose, while choosing healthy foods will increase their score.\nHow we built it\nWe built this game with python/pygames\nChallenges we ran into\nSince pygame function is not familiar to us, we learned how to use it while we were coding it. In addition, Hackathon is hosting virtually this year, so we faced challenges as a team working online.\nAccomplishments that we're proud of\nWe are proud of our teamwork-none of us used pygame to make a game before so we had to self-study the technique we needed. It is a virtual hackathon this time so that we learnt how to work as a team and communicate online.\nWhat we learned\nHow to code in Python and Pygame, and how to effectively collaborate as a team.\nWhat's next for Eat Smart\nWe would like to see this game be used as an educational tool for younger kids at schools, make kids aware of the food they have and prevent them from eating junk foods.", "link": "https://devpost.com/software/eat-smart", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nmore and more kids are suffering from obesity and diabetes in the united states. this is a problem because they don\u2019t know the importance of nutrition and crave unhealthy foods such as pizza too much. as technology improves, advertisements are everywhere showing kids how tasty and cheap fast food is.\nwhat it does\nthrough playing this game, kids will build up simple ideas and concepts about which food belongs to healthy food and which food they should avoid. they will realize eating unhealthy in the game will cause them to lose, while choosing healthy foods will increase their score.\nhow we built it\nwe built this game with python/pygames\nchallenges we ran into\nsince pygame function is not familiar to us, we learned how to use it while we were coding it. in addition, hackathon is hosting virtually this year, so we faced challenges as a team working online.\naccomplishments that we're proud of\nwe are proud of our teamwork-none of us used pygame to make a game before so we had to self-study the technique we needed. it is a virtual hackathon this time so that we learnt how to work as a team and communicate online.\nwhat we learned\nhow to code in python and pygame, and how to effectively collaborate as a team.\nwhat's next for eat smart\nwe would like to see this game be used as an educational -----> tool !!!  for younger kids at schools, make kids aware of the food they have and prevent them from eating junk foods.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59505348}, {"Unnamed: 0": 5370, "autor": "Media Forensics Toolkit", "date": null, "content": "Media Forensics Toolkit\nThe unprecedented growth of technology in the past few decades has led to massive flooding of the market with media forgery tools like Photoshop, gimp, Deep Dream Generator etc. These tempered media sources have contributed to other major problems like fake news and even cyberbullying. Furthermore, advancements of deep learning lead to a more severe problem known as Deepfakes and Generative Adversarial Networks(GAN) generated images and videos, that are even harder to detect and investigate in case of a legal breach. This project aims to address this challenge by implementing two novel deep learning models that can be used conjointly with some static models to provide complete protection against almost all types of image and video forgeries including Deepfakes, GAN generated forged media, photoshopped media (splicing, copy and move forgeries). For Deepfakes detection, this project implements a Timedistributed CNN-LSTM neural network and for GAN generated media detection, this project uses a fine-tuned DenseNet model as the base Convolutional neural network. The statistical models include generation of images by masking original image through various algorithms like Error Level Analysis, Laplace transforms, variance masking, pixel density algorithm, and generation of Exifdata of media through various tools and packages like hachoir, ExifTool, etc. Compared with existing strategies, the evaluation results demonstrate that the novel strategy can serve as a one-stop tool for forensic investigators, thus providing holistic protection against threats of today(pixel based forgeries) as well as future(vector based forgeries).\nData Preparation\nThe mean channel of images/frames was removed.\nAll frames/images were normalized\nThe fully-connected layer at the top of the network was removed ( for our DenseNet pretrained fine-tuned models)\nAll frames and images were resized according to adjust with our pretrained models.\nVarious data augmentation functions like adaptive Gaussian noise, emboss image, salt image, black hat filter, rotation, flipping, Affine Transformation, warping, increasing contrast were used to make the model more robust to changes.\nFrom the Keras preprocessing package, we used ImageDataGenerator to augment image input and use transformations as rescaling, shearing, zoom, flipping, height shift, width shift, fill mode, data format, brightness, and feature-wise normalize.\nImplemented models {AI-powered}\n1) Deepfakes\nFor Deepfakes, we used a Timedistributed CNN-LSTM neural network. TimeDistributed Layer keeps input to output relation proper, and avoid mixing of outputs. In this model, the function of CNN is feature extraction, and LSTM for sequence processing. In CNN, we used the initializer as glorot uniform, and reg lambda parameter to be 0.001. Various convolutional layers of variable kernel filters(32,64,128,256,512) are used to extract different features of frame/images. Different strides were used in pooling to gain access to different aspects of the image.The 2048-dimensional feature vectors after the last pooling layers are then used as the sequential LSTM input. We used stacked LSTM to increase sequence processing at different intervals ( units = 512, 256). Stacked LSTM gives robustness and increases the accuracy of our model as almost all correlations between features extracted by CNN are analyzed by it.\n2) GAN generated media\nFor the detection of GAN images/frames, we used a Finetuned DenseNet network along with an simple ANN( Artificial Neural Network). Sequential training was done on the network. Firstly, all layers apart from the last two layers of Densenet were frozen, and the model was trained on the rest of the layers. After initial training, all layers were unfrozen, and the whole model was again trained. Due to simultaneous training, the end layer\u2019s (deciding layers) converge best to the global optima and give an accuracy boost to our model. Also Dropout(F 1 ) and batch normalization layers(F 2 ) were added regularly to prevent overfitting.\nImplemented Statistical models\nExif-Data Analysis\nIn our model, metadata extraction was done for all videos/images by using sequential use of a tool called ExifTool and a python package called hachoir. After data retrieval from both tools, the redundant data was removed and useful data like device id, device manufacturer, software used, encoder, etc were carved out into a .csv file. This piece of information, which is often neglected, can open new avenues to investigation and may also reveal personal identification proofs like the MAC address of original equipment used.\nMasking Algorithms Image masking is a process by which we can hide some aspects or parts of an image and highlight some other aspects of the image like highlighting high illuminance pixel entropy areas. Several masking algorithms like pixel density, Bitwise-AND masking, Variance Masking, Laplace detector, ELA are used in our project that can accurately detect any type of pixel based manipulation like photoshopped media(splicing, etc)\nFuture Scope\nThe future additions that could be made to this project are:\nNeural networks can be debugged for finding the performance of each layer of the network, thus finding any dead neuron or low learning by any layer.\nMore juicy forensic artifacts could be collected by adding tools like Ghiro, JPEGsnoop ( better source to find last device used), stegsolve ( switch through the layers and look for abnormalities) to the main project, thus diversifying the static metadata information collected.\nReverse searching of image/video could be added to the project, thus backtracking original digital media resource (scan through different open source search engines)", "link": "https://devpost.com/software/media-forensics-toolkit", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "media forensics toolkit\nthe unprecedented growth of technology in the past few decades has led to massive flooding of the market with media forgery tools like photoshop, gimp, deep dream generator etc. these tempered media sources have contributed to other major problems like fake news and even cyberbullying. furthermore, advancements of deep learning lead to a more severe problem known as deepfakes and generative adversarial networks(gan) generated images and videos, that are even harder to detect and investigate in case of a legal breach. this project aims to address this challenge by implementing two novel deep learning models that can be used conjointly with some static models to provide complete protection against almost all types of image and video forgeries including deepfakes, gan generated forged media, photoshopped media (splicing, copy and move forgeries). for deepfakes detection, this project implements a timedistributed cnn-lstm neural network and for gan generated media detection, this project uses a fine-tuned densenet model as the base convolutional neural network. the statistical models include generation of images by masking original image through various algorithms like error level analysis, laplace transforms, variance masking, pixel density algorithm, and generation of exifdata of media through various tools and packages like hachoir, exiftool, etc. compared with existing strategies, the evaluation results demonstrate that the novel strategy can serve as a one-stop -----> tool !!!  for forensic investigators, thus providing holistic protection against threats of today(pixel based forgeries) as well as future(vector based forgeries).\ndata preparation\nthe mean channel of images/frames was removed.\nall frames/images were normalized\nthe fully-connected layer at the top of the network was removed ( for our densenet pretrained fine-tuned models)\nall frames and images were resized according to adjust with our pretrained models.\nvarious data augmentation functions like adaptive gaussian noise, emboss image, salt image, black hat filter, rotation, flipping, affine transformation, warping, increasing contrast were used to make the model more robust to changes.\nfrom the keras preprocessing package, we used imagedatagenerator to augment image input and use transformations as rescaling, shearing, zoom, flipping, height shift, width shift, fill mode, data format, brightness, and feature-wise normalize.\nimplemented models {ai-powered}\n1) deepfakes\nfor deepfakes, we used a timedistributed cnn-lstm neural network. timedistributed layer keeps input to output relation proper, and avoid mixing of outputs. in this model, the function of cnn is feature extraction, and lstm for sequence processing. in cnn, we used the initializer as glorot uniform, and reg lambda parameter to be 0.001. various convolutional layers of variable kernel filters(32,64,128,256,512) are used to extract different features of frame/images. different strides were used in pooling to gain access to different aspects of the image.the 2048-dimensional feature vectors after the last pooling layers are then used as the sequential lstm input. we used stacked lstm to increase sequence processing at different intervals ( units = 512, 256). stacked lstm gives robustness and increases the accuracy of our model as almost all correlations between features extracted by cnn are analyzed by it.\n2) gan generated media\nfor the detection of gan images/frames, we used a finetuned densenet network along with an simple ann( artificial neural network). sequential training was done on the network. firstly, all layers apart from the last two layers of densenet were frozen, and the model was trained on the rest of the layers. after initial training, all layers were unfrozen, and the whole model was again trained. due to simultaneous training, the end layer\u2019s (deciding layers) converge best to the global optima and give an accuracy boost to our model. also dropout(f 1 ) and batch normalization layers(f 2 ) were added regularly to prevent overfitting.\nimplemented statistical models\nexif-data analysis\nin our model, metadata extraction was done for all videos/images by using sequential use of a tool called exiftool and a python package called hachoir. after data retrieval from both tools, the redundant data was removed and useful data like device id, device manufacturer, software used, encoder, etc were carved out into a .csv file. this piece of information, which is often neglected, can open new avenues to investigation and may also reveal personal identification proofs like the mac address of original equipment used.\nmasking algorithms image masking is a process by which we can hide some aspects or parts of an image and highlight some other aspects of the image like highlighting high illuminance pixel entropy areas. several masking algorithms like pixel density, bitwise-and masking, variance masking, laplace detector, ela are used in our project that can accurately detect any type of pixel based manipulation like photoshopped media(splicing, etc)\nfuture scope\nthe future additions that could be made to this project are:\nneural networks can be debugged for finding the performance of each layer of the network, thus finding any dead neuron or low learning by any layer.\nmore juicy forensic artifacts could be collected by adding tools like ghiro, jpegsnoop ( better source to find last device used), stegsolve ( switch through the layers and look for abnormalities) to the main project, thus diversifying the static metadata information collected.\nreverse searching of image/video could be added to the project, thus backtracking original digital media resource (scan through different open source search engines)", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59505370}, {"Unnamed: 0": 5405, "autor": "Pixel Art - Pixel painting software", "date": null, "content": "Pixel art is a form of digital art used for a large number of computer games, consoles, and mobile devices. However, to create these artistic drawings you need a drawing tool like Pixel Art. Pixel Art's ease of use allows you to instantly create new designs with minimal effort.\nMain features of Pixel Art drawing software\nChoose from 400 or 900 squares to work with\nUse whatever color you feel is right\nSave image drawings as .bmp, .gif, .jpg or .png\nPixel Art has a simple interface with an intuitive layout that makes it easy to find and use tools for your drawings. Pixel Art offers two sizes of 400 squares and 900 squares for you to work with, allowing creative expression to the fullest.\nThe canvas can be displayed as a blank page or with a grid of squares to delineate each pixel. Pixel Art allows you to color the squares with different colors. Some colors are available right on the canvas and directly accessible, others can be selected from the spectrum. You can use this software to create the pixel art you want.\nPixel Art comes with several color themes that can be enabled and disabled from the Settings area.\nThe generated drawings can be saved on the drive in BMP, GIF, JPG and PNG formats.\nIn short, Pixel Art is a great application to create artistic drawings with unique pixels and colors. Pixel Art is suitable for even the most inexperienced users thanks to its overall simplicity. Download Pixel Art and create your own interesting drawings.", "link": "https://devpost.com/software/pixel-art-pixel-painting-software", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "pixel art is a form of digital art used for a large number of computer games, consoles, and mobile devices. however, to create these artistic drawings you need a drawing -----> tool !!!  like pixel art. pixel art's ease of use allows you to instantly create new designs with minimal effort.\nmain features of pixel art drawing software\nchoose from 400 or 900 squares to work with\nuse whatever color you feel is right\nsave image drawings as .bmp, .gif, .jpg or .png\npixel art has a simple interface with an intuitive layout that makes it easy to find and use tools for your drawings. pixel art offers two sizes of 400 squares and 900 squares for you to work with, allowing creative expression to the fullest.\nthe canvas can be displayed as a blank page or with a grid of squares to delineate each pixel. pixel art allows you to color the squares with different colors. some colors are available right on the canvas and directly accessible, others can be selected from the spectrum. you can use this software to create the pixel art you want.\npixel art comes with several color themes that can be enabled and disabled from the settings area.\nthe generated drawings can be saved on the drive in bmp, gif, jpg and png formats.\nin short, pixel art is a great application to create artistic drawings with unique pixels and colors. pixel art is suitable for even the most inexperienced users thanks to its overall simplicity. download pixel art and create your own interesting drawings.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59505405}, {"Unnamed: 0": 5413, "autor": "FairWell: A Tool to Bid Goodbye to Unknown AI Biasness", "date": null, "content": "Inspiration\nThe problem of defining and addressing fairness has been a topic of increasing importance in the recent years. Especially with work surrounding machine learning. Multiple definitions of fairness have been raised, each having their own pros and cons. This work does not aim to propose a single notion of fairness but rather we aim to (i) provide tools on how to measure and assess fairness and (ii) how to mitigate bias in models where necessary.\nBusinesses have recognised the need to develop AI models that are responsible and fair towards their data inputs. Facebook, too, understands the importance of responsible AI as it can help mitigate concerns surrounding privacy, fairness, accountability and transparency in its algorithms. They set up a cross-disciplinary Responsible AI (RAI) team to ensure Machine Learning systems are designed and used responsibly. The team has since released Fairness Flow, an internal tool to assess AI fairness.\n\"Fairness Flow lists four definitions (of fairness) that engineers can use according to which suits their purpose best, such as whether a speech-recognition model recognizes all accents with equal accuracy or with a minimum threshold of accuracy.\"\nWhat it does\nFairWell is a Responsible AI tool developed using Streamlit. The aim is to address model biasness on specific groups of people, allowing data scientists to evaluate their dataset and model predictions, and take steps toward making their datasets more inclusive and their models less biased. The tool allows users to detect fairness issues in both datasets and models, and in turn, get inspiration on various mitigation approaches through mitigation recommendations.\nHow we built it\nFairWell automates areas in the machine pipeline where fairness assessment and mitigation can be automated, to reduce friction faced by data scientists when developing Responsible AI models.\nOur work can be summarised as below:\nFairness assessment\nOn data\nOn model predictions\nBias mitigation\nPre-processing by transforming the data\nIn-processing by imposing constraints during training\nPost-processing where the predictions of models are modified\nRecognising the need for a demo, the New York City Subway Dataset which contains both neighbourhood census data and subway traffic data is used as an example. A PyTorch model is built using the dataset, then assessing the fairness the data and model predictions were done, followed by iterating through various mitigation approaches to build fairer PyTorch models.\nThe dataset we selected consists of subway traffic in NYC, along with neighbourhood census data of the city. It is hosted on Kaggle by Edden, who has performed preprocessing steps to convert the raw data provided by The Metropolitan Transportation Authority (MTA), North America's largest transportation network. The census data is from NYU Furman Center's New York City neighbourhood Data Profiles and the neighbourhood data is from University of Berkeley GeoData Library.\nApproach\nThe baseline model is built for a time series binary classification problem to predict whether subway traffic is high or low.\nPipeline Overview\nFeature Explorer\nThe FairWell feature explorer page allows users to quickly view the distributions of their dataset, as well as the summary statistics for each feature. An algorithm is created to quickly infer the data types of each feature, categorising them into either numerical or categorical. Based on the data type, a histogram or bar chart will be shown for numerical or categorical data type respectively.\nDataset\nOur final dataset contains the following features:\nFeatures from neighbourhood Census Data Features from Subway Dataa\nneighbourhood\nCar-free commute (% of commuters)\nDisabled population\nForeign-born population\nMedian household income (2018\\$)\nMedian rent, all (2018$)\nPercent Asian\nPercent Hispanic\nPercent Black\nPercent white\nPopulationc\nPoverty rate\nPublic housing (% of rental units)\nUnemployment rate\nResidential units within 12 mile of a subway station\nPopulation density (1,000 persons per square mile)c\nSerious crime rate (per 1,000 residents)\nSeverely rent-burdened households\nRental vacancy rate\nMean travel time to work (minutes) Datetime\nStop Name\nConnecting Linesb\nDivisionb\nStructureb\nBoroughb\nneighbourhood\nEntriesd\nExitsd\na Used to derive \"Number of Stations\" feature\nb One hot encoded features\nc Used to derive \"\"neighbourhood Area Size\"\" feature\nd Used to derive \"EntriesExits\" target feature\nData Fairness Assessment\nRecall that bias can occur in the data even prior to training a model. To identify potential bias arising from data, we pass our cleansed dataset into the FairWell Featurer Explorer page, to generate the distributions and statistics of our dataset.\nBased on this analysis, we then identified a number of features that could be potentially sensitive. For example, we noticed that some neighbourhoods had as much as 74.1% of the population made up of Hispanic citizens, while others had as little as 3.7%. This large disparity in values could be a point of concern.\nThese features were binned into binary features by using the mean as the threshold. The binned dataset is then uploaded onto the FairWell Data Fairness Assessment page where all features were evaluated based on the Class Imbalance and Jensen-Shannon Divergence metrics for fairness. The features with the highest Jensen-Shannon Divergence were then identified for potential bias mitigation later on.\nIn FairWell, the fairness metrics below will be calculated for each feature:\nClass Imbalance (CI): Identifies any under representation of classes within the dataset\nJensen-Shannon Divergence (JS): Measures label imbalance between 2 classes by measuring their similiarity between their probability distributions\nA scatter plot which plots the selected fairness metric for each subgroup pairing within the feature is shown. All user uploaded datasets' metrics will be plotted on the same scatter plot to allow for ease of comparison.\nModelling with PyTorch\nPyTorch was then used to build a baseline neural network model.\nThe dataset we are working with is a 2-dimensional dataset, consisting of both spatial and temporal dimensions; also frequently known as \"panel\u201d data. Other approaches to similar problems often include more complex architectures involving LSTMs, CNNs or transformers. However, in this use case, we opted to keep things simple by using a standard multi-input feedforward network.\nFairness Assessment on Model Predictions\nIn FairWell, the inference process will then kick off for every model, returning predictions as outputs. With both the ground truth (target) and the predictions, each model's performance will be calculated, along the following fairness metrics for every feature:\nDemographic Parity (DP): Measures the same positive prediction ratio across groups identified by the sensitive features.\nEqualized Odds (EO): Measures the equality in terms of error rate. Same false positive rate and false negative rate across sensitive groups identified.\nPredictive Parity (PP): Measures the error rates in terms of fraction of errors over the ground truth. The model should have the same precision across sensitive groups.\nA scatter plot that plots the relationship between the selected fairness metric and each model's performance will be shown. This is coupled with an expandable insights section, allowing users to evaluate the potential trade-offs from their models and fairness.\nLastly, the page will compare the aforementioned fairness metrics of each feature selected for fairness assessment, providing users with useful mitigation approaches they can take towards fairer model development. After applying fairness mitigation, users can revisit this page with a new dataset and model for comparison against their previous iterations.\nBack to the model, the baseline trained model, testing dataset and feature list were then uploaded onto the FairWell Model Bias Detection & Mitigation page. Inference is then run automatically within the FairWell environment to generate the following fairness metrics for our baseline model predictions: Demographic Parity, Equalized Odds and Predictive Parity. Here, the metric we focused on is Demographic Parity, making sure each subgroup receives a positive outcome at an equal rate.\nBased on our assessment of both data fairness and model fairness, we narrowed down to a list of features that exhibited both high Jensen-Shannon Divergence and high Demographic Parity disparity. On the top of that list was the Privileged: Lower Foreign-born population feature, with a score of 0.0264 max Jensen-Shannon Divergence and 0.2251 Demographic Parity.\nThe following mitigation approaches were performed independently (more details on the following section):\nUndersampling (pre-processing mitigation)\nReweighing (pre-processing mitigation)\nDemographic Parity Loss (in-processing mitigation)\nWith Privileged: Lower Foreign-born population as the sensitive feature, undersampling was applied to the dataset (pre-processing mitigation) and demographic parity loss (in-processing mitigation) was utilised during model training. The mitigation approaches are applied separately, thus the resulting datasets and models are independent of each other.\nComparing Demographic Parity, the undersampled dataset did not perform as well on the sensitive feature, with a higher Demographic Parity score of 0.284. This could be attributed to a small number (about 50) of neighbourhoods, which can in turn affect the undersampling technique utilised.\nAs for the reweighing (pre-processing mitigation) approach, we took all 11 features into account to generate weights for each observation.\nComparing the Demographic Parity for each feature, all of the values were lower. This indicates that the model predictions are fairer, compared to the baseline dataset.\nFairness Mitigation\nBased on our Responsible AI Research, we have identified two approaches in mitigating bias, pre-processing the dataset and in-processing where we impose a fairness loss constraint during model training. In our example use case, we tried 3 of those approaches. For all datasets, time-based features were generated and min-max transformation of numerical features and label encoding of categorical features was conducted.\nPre-processing\nReweighing17\nIn this approach, we used IBM's AI Fairness 360 package to generate weights for each (sensitive feature, label) pair and assigned them to each observation.\nThese per-sample weights were then passed into the model and computed as part of the loss function, allowing the model to prioritize certain observations during training.\nUnder-sampling17\nIn this approach we used the Imbalanced Learn package to correct class imbalance in the sensitive feature by randomly undersampling neighbourhoods belonging to the majority class. This was done in order to ensure an equal representation in both privileged and underprivileged groups.\nThe undersampled dataset was then fed into the model.\nIn-processing\nFairness loss constraint\nIn this approach, we borrowed the work of the FairTorch team to incorporate a group-fairness constraint into our loss function, allowing the model to optimize on a combination of both BCELoss and Demographic Parity.\nFor our example model, a value of alpha=0.5 was identified to achieve the best balance between the two.\nEffect of Mitigation Approach\nAll 4 of our trained models (1 baseline, 3 post-mitigation) were then again uploaded onto the FairWell Model Bias Detection & Mitigation page, for a side-of-side comparison of the models' metrics.\nFrom FairWell, we found that there was a trade-off between accuracy and fairness, where generally models that are more accurate tend to also exhibit a larger amount of bias. The selection of which model to use is hence highly subjective and varies on a case-by-case basis, depending on the priorities of the project.\nThe scatterplot below will further illustrate how one might intepret the results for a specific sub group - Privileged: Lower Foreign-born population.\nDemographic Parity\nBy using the mitigations steps described earlier, we observe that generally, there's a trade-off between accuracy and demographic parity. The baseline model is biased towards neighbourhoods with lower foreign born population. Comparing the selection rate, we can see that it is higher (0.611 vs 0.401) for neighbourhoods with lower foreign born population.\nComparing the selection rate for reweighing model and baseline, we can see that the reweighing model is less biased towards neighbourhoods with lower foreign born populations (0.545 vs 0.611). On the flip side, it is less biased against with neighbourhoods without lower foreign born populations (0.485 vs 0.401). Comparing the DP for both models, the model with reweighted data has lower DP compared to the baseline (0.060 vs 0.210). Both accuracy and disparity improved compared to the baseline.\nComparing the selection rate for DP loss contrained model and baseline, we can see that the DP loss constrained model is less biased towards neighbourhoods with lower foreign born populations (0.473 vs 0.611). On the flip side, it is more biased against with neighbourhoods without lower foreign born populations (0.319 vs 0.401). Comparing the DP for both models, the model with DP loss constrain has lower DP compared to the baseline (0.154 vs 0.210). In this case, the accuracy decreased while the disparity increased.\nAlthough here we can observe that the undersampling bias mitigation technique has resulted in a higher DP than the baseline model. Comparing the DP for both models, the model with under sampled data has higher DP compared to the baseline (0.284 vs 0.210). Both accuracy and disparity decreased.\nThis showcases how bias mitigation techniques are subjective and requires an iterative process that acknowledges the trade offs between accuracy and fairness\nEqualised Odds\nCompared to the baseline model, the under sampling model and the DP loss constraint model performed worse off in accuracy. However, the reweighing model performed slightly better than the baseline.\nLooking at the EO disparity scores only the DP loss constrained model is fairer than the baseline model.\nPredictive Parity\nCompared to the baseline model, all the models have higher PP disparity scores.\nLooking at the PP disparity scores, the baseline model is the fairest model.\nChallenges we ran into\nOur implementation process involved us utilizing new technologies. Most of our team were new to PyTorch and the 60 minute tutorial helped us familiarize ourselves with the package. Our team had to research extensively on other new technologies such as StreamLit, FairTorch and AIF360 in a short period of time to implement into our solution. This was extremely time consuming and tedious as we had to dig into the documentation to discover the limitations of these tools and pivot our implementation plan accordingly.\nWe did not have a subject matter expert on AI Fairness in our team, thus we had dive deep into documentations on the topic. Sieving through all the resources available posed a challenge without an SME to validate our findings. Even after we have gained a fair understanding of AI Fairness, we have found that the topic itself is subjective. Fairness assessment is specific to each use case. Fairness does not have one single definition and varies across different situations and scenarios. This poses as a challenge when researching on the various metrics and mathematics involved in assessing bias. This took many iterations of trial and error with different technologies, even to the point of implementing the mathematical formulae from scratch.\nAccomplishments that we're proud of\nWe created a framework for building Responsible Machine Learning Models, providing structure to the subjective process of integrating fairness into machine learning pipelines. Furthermore, with our fully functional, user-friendly, and interpretable web application, we have made AI Fairness accessible to all.\nWhat we learned\nWe gained greater awareness on the importance of AI Responsibility and will continue to be more conscious of integrating it into the machine learning life cycle.\nWhat's next for FairWell: A Tool to Bid Goodbye to Unknown AI Biasness\nFairWell provides a holistic approach to incorporating Responsible AI practices into their machine learning workflows. We will continue to integrate and improve on FairWell in future. This includes making FairWell available for multiclass classification and regression problems, expanding supported machine learning libraries beyond PyTorch, and automating model building and tuning. With FairWell, we aim to make all models fair and well, so you can bid farewell to unknown model bias.", "link": "https://devpost.com/software/fairwell-a-tool-to-bid-goodbye-to-unknown-ai-biasness", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nthe problem of defining and addressing fairness has been a topic of increasing importance in the recent years. especially with work surrounding machine learning. multiple definitions of fairness have been raised, each having their own pros and cons. this work does not aim to propose a single notion of fairness but rather we aim to (i) provide tools on how to measure and assess fairness and (ii) how to mitigate bias in models where necessary.\nbusinesses have recognised the need to develop ai models that are responsible and fair towards their data inputs. facebook, too, understands the importance of responsible ai as it can help mitigate concerns surrounding privacy, fairness, accountability and transparency in its algorithms. they set up a cross-disciplinary responsible ai (rai) team to ensure machine learning systems are designed and used responsibly. the team has since released fairness flow, an internal -----> tool !!!  to assess ai fairness.\n\"fairness flow lists four definitions (of fairness) that engineers can use according to which suits their purpose best, such as whether a speech-recognition model recognizes all accents with equal accuracy or with a minimum threshold of accuracy.\"\nwhat it does\nfairwell is a responsible ai tool developed using streamlit. the aim is to address model biasness on specific groups of people, allowing data scientists to evaluate their dataset and model predictions, and take steps toward making their datasets more inclusive and their models less biased. the tool allows users to detect fairness issues in both datasets and models, and in turn, get inspiration on various mitigation approaches through mitigation recommendations.\nhow we built it\nfairwell automates areas in the machine pipeline where fairness assessment and mitigation can be automated, to reduce friction faced by data scientists when developing responsible ai models.\nour work can be summarised as below:\nfairness assessment\non data\non model predictions\nbias mitigation\npre-processing by transforming the data\nin-processing by imposing constraints during training\npost-processing where the predictions of models are modified\nrecognising the need for a demo, the new york city subway dataset which contains both neighbourhood census data and subway traffic data is used as an example. a pytorch model is built using the dataset, then assessing the fairness the data and model predictions were done, followed by iterating through various mitigation approaches to build fairer pytorch models.\nthe dataset we selected consists of subway traffic in nyc, along with neighbourhood census data of the city. it is hosted on kaggle by edden, who has performed preprocessing steps to convert the raw data provided by the metropolitan transportation authority (mta), north america's largest transportation network. the census data is from nyu furman center's new york city neighbourhood data profiles and the neighbourhood data is from university of berkeley geodata library.\napproach\nthe baseline model is built for a time series binary classification problem to predict whether subway traffic is high or low.\npipeline overview\nfeature explorer\nthe fairwell feature explorer page allows users to quickly view the distributions of their dataset, as well as the summary statistics for each feature. an algorithm is created to quickly infer the data types of each feature, categorising them into either numerical or categorical. based on the data type, a histogram or bar chart will be shown for numerical or categorical data type respectively.\ndataset\nour final dataset contains the following features:\nfeatures from neighbourhood census data features from subway dataa\nneighbourhood\ncar-free commute (% of commuters)\ndisabled population\nforeign-born population\nmedian household income (2018\\$)\nmedian rent, all (2018$)\npercent asian\npercent hispanic\npercent black\npercent white\npopulationc\npoverty rate\npublic housing (% of rental units)\nunemployment rate\nresidential units within 12 mile of a subway station\npopulation density (1,000 persons per square mile)c\nserious crime rate (per 1,000 residents)\nseverely rent-burdened households\nrental vacancy rate\nmean travel time to work (minutes) datetime\nstop name\nconnecting linesb\ndivisionb\nstructureb\nboroughb\nneighbourhood\nentriesd\nexitsd\na used to derive \"number of stations\" feature\nb one hot encoded features\nc used to derive \"\"neighbourhood area size\"\" feature\nd used to derive \"entriesexits\" target feature\ndata fairness assessment\nrecall that bias can occur in the data even prior to training a model. to identify potential bias arising from data, we pass our cleansed dataset into the fairwell featurer explorer page, to generate the distributions and statistics of our dataset.\nbased on this analysis, we then identified a number of features that could be potentially sensitive. for example, we noticed that some neighbourhoods had as much as 74.1% of the population made up of hispanic citizens, while others had as little as 3.7%. this large disparity in values could be a point of concern.\nthese features were binned into binary features by using the mean as the threshold. the binned dataset is then uploaded onto the fairwell data fairness assessment page where all features were evaluated based on the class imbalance and jensen-shannon divergence metrics for fairness. the features with the highest jensen-shannon divergence were then identified for potential bias mitigation later on.\nin fairwell, the fairness metrics below will be calculated for each feature:\nclass imbalance (ci): identifies any under representation of classes within the dataset\njensen-shannon divergence (js): measures label imbalance between 2 classes by measuring their similiarity between their probability distributions\na scatter plot which plots the selected fairness metric for each subgroup pairing within the feature is shown. all user uploaded datasets' metrics will be plotted on the same scatter plot to allow for ease of comparison.\nmodelling with pytorch\npytorch was then used to build a baseline neural network model.\nthe dataset we are working with is a 2-dimensional dataset, consisting of both spatial and temporal dimensions; also frequently known as \"panel\u201d data. other approaches to similar problems often include more complex architectures involving lstms, cnns or transformers. however, in this use case, we opted to keep things simple by using a standard multi-input feedforward network.\nfairness assessment on model predictions\nin fairwell, the inference process will then kick off for every model, returning predictions as outputs. with both the ground truth (target) and the predictions, each model's performance will be calculated, along the following fairness metrics for every feature:\ndemographic parity (dp): measures the same positive prediction ratio across groups identified by the sensitive features.\nequalized odds (eo): measures the equality in terms of error rate. same false positive rate and false negative rate across sensitive groups identified.\npredictive parity (pp): measures the error rates in terms of fraction of errors over the ground truth. the model should have the same precision across sensitive groups.\na scatter plot that plots the relationship between the selected fairness metric and each model's performance will be shown. this is coupled with an expandable insights section, allowing users to evaluate the potential trade-offs from their models and fairness.\nlastly, the page will compare the aforementioned fairness metrics of each feature selected for fairness assessment, providing users with useful mitigation approaches they can take towards fairer model development. after applying fairness mitigation, users can revisit this page with a new dataset and model for comparison against their previous iterations.\nback to the model, the baseline trained model, testing dataset and feature list were then uploaded onto the fairwell model bias detection & mitigation page. inference is then run automatically within the fairwell environment to generate the following fairness metrics for our baseline model predictions: demographic parity, equalized odds and predictive parity. here, the metric we focused on is demographic parity, making sure each subgroup receives a positive outcome at an equal rate.\nbased on our assessment of both data fairness and model fairness, we narrowed down to a list of features that exhibited both high jensen-shannon divergence and high demographic parity disparity. on the top of that list was the privileged: lower foreign-born population feature, with a score of 0.0264 max jensen-shannon divergence and 0.2251 demographic parity.\nthe following mitigation approaches were performed independently (more details on the following section):\nundersampling (pre-processing mitigation)\nreweighing (pre-processing mitigation)\ndemographic parity loss (in-processing mitigation)\nwith privileged: lower foreign-born population as the sensitive feature, undersampling was applied to the dataset (pre-processing mitigation) and demographic parity loss (in-processing mitigation) was utilised during model training. the mitigation approaches are applied separately, thus the resulting datasets and models are independent of each other.\ncomparing demographic parity, the undersampled dataset did not perform as well on the sensitive feature, with a higher demographic parity score of 0.284. this could be attributed to a small number (about 50) of neighbourhoods, which can in turn affect the undersampling technique utilised.\nas for the reweighing (pre-processing mitigation) approach, we took all 11 features into account to generate weights for each observation.\ncomparing the demographic parity for each feature, all of the values were lower. this indicates that the model predictions are fairer, compared to the baseline dataset.\nfairness mitigation\nbased on our responsible ai research, we have identified two approaches in mitigating bias, pre-processing the dataset and in-processing where we impose a fairness loss constraint during model training. in our example use case, we tried 3 of those approaches. for all datasets, time-based features were generated and min-max transformation of numerical features and label encoding of categorical features was conducted.\npre-processing\nreweighing17\nin this approach, we used ibm's ai fairness 360 package to generate weights for each (sensitive feature, label) pair and assigned them to each observation.\nthese per-sample weights were then passed into the model and computed as part of the loss function, allowing the model to prioritize certain observations during training.\nunder-sampling17\nin this approach we used the imbalanced learn package to correct class imbalance in the sensitive feature by randomly undersampling neighbourhoods belonging to the majority class. this was done in order to ensure an equal representation in both privileged and underprivileged groups.\nthe undersampled dataset was then fed into the model.\nin-processing\nfairness loss constraint\nin this approach, we borrowed the work of the fairtorch team to incorporate a group-fairness constraint into our loss function, allowing the model to optimize on a combination of both bceloss and demographic parity.\nfor our example model, a value of alpha=0.5 was identified to achieve the best balance between the two.\neffect of mitigation approach\nall 4 of our trained models (1 baseline, 3 post-mitigation) were then again uploaded onto the fairwell model bias detection & mitigation page, for a side-of-side comparison of the models' metrics.\nfrom fairwell, we found that there was a trade-off between accuracy and fairness, where generally models that are more accurate tend to also exhibit a larger amount of bias. the selection of which model to use is hence highly subjective and varies on a case-by-case basis, depending on the priorities of the project.\nthe scatterplot below will further illustrate how one might intepret the results for a specific sub group - privileged: lower foreign-born population.\ndemographic parity\nby using the mitigations steps described earlier, we observe that generally, there's a trade-off between accuracy and demographic parity. the baseline model is biased towards neighbourhoods with lower foreign born population. comparing the selection rate, we can see that it is higher (0.611 vs 0.401) for neighbourhoods with lower foreign born population.\ncomparing the selection rate for reweighing model and baseline, we can see that the reweighing model is less biased towards neighbourhoods with lower foreign born populations (0.545 vs 0.611). on the flip side, it is less biased against with neighbourhoods without lower foreign born populations (0.485 vs 0.401). comparing the dp for both models, the model with reweighted data has lower dp compared to the baseline (0.060 vs 0.210). both accuracy and disparity improved compared to the baseline.\ncomparing the selection rate for dp loss contrained model and baseline, we can see that the dp loss constrained model is less biased towards neighbourhoods with lower foreign born populations (0.473 vs 0.611). on the flip side, it is more biased against with neighbourhoods without lower foreign born populations (0.319 vs 0.401). comparing the dp for both models, the model with dp loss constrain has lower dp compared to the baseline (0.154 vs 0.210). in this case, the accuracy decreased while the disparity increased.\nalthough here we can observe that the undersampling bias mitigation technique has resulted in a higher dp than the baseline model. comparing the dp for both models, the model with under sampled data has higher dp compared to the baseline (0.284 vs 0.210). both accuracy and disparity decreased.\nthis showcases how bias mitigation techniques are subjective and requires an iterative process that acknowledges the trade offs between accuracy and fairness\nequalised odds\ncompared to the baseline model, the under sampling model and the dp loss constraint model performed worse off in accuracy. however, the reweighing model performed slightly better than the baseline.\nlooking at the eo disparity scores only the dp loss constrained model is fairer than the baseline model.\npredictive parity\ncompared to the baseline model, all the models have higher pp disparity scores.\nlooking at the pp disparity scores, the baseline model is the fairest model.\nchallenges we ran into\nour implementation process involved us utilizing new technologies. most of our team were new to pytorch and the 60 minute tutorial helped us familiarize ourselves with the package. our team had to research extensively on other new technologies such as streamlit, fairtorch and aif360 in a short period of time to implement into our solution. this was extremely time consuming and tedious as we had to dig into the documentation to discover the limitations of these tools and pivot our implementation plan accordingly.\nwe did not have a subject matter expert on ai fairness in our team, thus we had dive deep into documentations on the topic. sieving through all the resources available posed a challenge without an sme to validate our findings. even after we have gained a fair understanding of ai fairness, we have found that the topic itself is subjective. fairness assessment is specific to each use case. fairness does not have one single definition and varies across different situations and scenarios. this poses as a challenge when researching on the various metrics and mathematics involved in assessing bias. this took many iterations of trial and error with different technologies, even to the point of implementing the mathematical formulae from scratch.\naccomplishments that we're proud of\nwe created a framework for building responsible machine learning models, providing structure to the subjective process of integrating fairness into machine learning pipelines. furthermore, with our fully functional, user-friendly, and interpretable web application, we have made ai fairness accessible to all.\nwhat we learned\nwe gained greater awareness on the importance of ai responsibility and will continue to be more conscious of integrating it into the machine learning life cycle.\nwhat's next for fairwell: a tool to bid goodbye to unknown ai biasness\nfairwell provides a holistic approach to incorporating responsible ai practices into their machine learning workflows. we will continue to integrate and improve on fairwell in future. this includes making fairwell available for multiclass classification and regression problems, expanding supported machine learning libraries beyond pytorch, and automating model building and tuning. with fairwell, we aim to make all models fair and well, so you can bid farewell to unknown model bias.", "sortedWord": "None", "removed": "Nan", "score": 6, "comments": 2, "media": null, "medialink": null, "identifyer": 59505413}, {"Unnamed: 0": 5425, "autor": "Equestrium", "date": null, "content": "Inspiration\nThe preliminary inspiration for this is horse racing. We think it would be fun to have a platform where you can race any NFT you own against other NFTs owned by other people\nWhat it does\nThis web app allows you to load and enter any NFT you own into a rae with other people's NFTs. There is an entry fee and the winner collects all the pool as a reward for winning\nHow we built it\nWe built this using Solidity and typescript. We are using react and moralis as well. We plan to use Chainlink VRF to determine winning outcomes and we are going to use IPFS as we build out a 3D race model to store NFT marbles that will have an nft projecting of the users nft on it\nChallenges we ran into\nAt first we wanted to do a horse racing app but then we found out this already existed on polygon and pivoted to create a platform to race already existing NFTs.\nAccomplishments that we're proud of\nLearning how to use moralis has been really cool. Proud we have a basic frontend stood up\nWhat we learned\nLearning how to use moralis was really cool. It is a great tool that we are excited about. Also, really enjoyed the app building code alongs they are very helpful for us being really green to building frontends.\nWhat's next for Equestrium\nContinue to flesh out the ideas. Right now we need to build out a feature to display and select the NFTs you want to race then once this is finished we will start racing on the testnet. After that we will build out for Polygon and test a race on that chain. Then go back to Ethereum main net and test race there.", "link": "https://devpost.com/software/lossless-bets", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nthe preliminary inspiration for this is horse racing. we think it would be fun to have a platform where you can race any nft you own against other nfts owned by other people\nwhat it does\nthis web app allows you to load and enter any nft you own into a rae with other people's nfts. there is an entry fee and the winner collects all the pool as a reward for winning\nhow we built it\nwe built this using solidity and typescript. we are using react and moralis as well. we plan to use chainlink vrf to determine winning outcomes and we are going to use ipfs as we build out a 3d race model to store nft marbles that will have an nft projecting of the users nft on it\nchallenges we ran into\nat first we wanted to do a horse racing app but then we found out this already existed on polygon and pivoted to create a platform to race already existing nfts.\naccomplishments that we're proud of\nlearning how to use moralis has been really cool. proud we have a basic frontend stood up\nwhat we learned\nlearning how to use moralis was really cool. it is a great -----> tool !!!  that we are excited about. also, really enjoyed the app building code alongs they are very helpful for us being really green to building frontends.\nwhat's next for equestrium\ncontinue to flesh out the ideas. right now we need to build out a feature to display and select the nfts you want to race then once this is finished we will start racing on the testnet. after that we will build out for polygon and test a race on that chain. then go back to ethereum main net and test race there.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59505425}, {"Unnamed: 0": 5430, "autor": "Hiapo/Oha Chatbot", "date": null, "content": "Inspiration\nWith so many exciting challenges offered in this year\u2018s annual challenge, it took our team a significant amount of time to agree on a particular project and solution roadmap. As a Native Hawaiian, working for a Native Hawaiian Organization, in charge of educating and training adults in technology, I couldn't think of a more fitting project than OHA's request for a chatbot to serve beneficiaries. With our visions and missions alignment, we pressed on.\nWhat it does\nAt the heart of our solution, we are building an online system that retrieves information and delivers it quickly and accurately, into the hands of OHA's beneficiaries', by way of instant messaging, better known as a chatbot. This small tool allows community members and the general public to inquire about programs, gain access and information about those programs, and ultimately be able to apply to those programs through their phones, mobile devices, or laptops.\nHow we built it\nAs much as we would like to believe in late 80's movies, truth is, if you build it, they will not come. Digital transformation is only as good as the users who implement and support the changes. So how we built this, or how we decided to build this, was heavily influenced by the tools and technology that OHA already had access to. the proper solution should survive and thrive, long after our team's involvement. A quality product will need to be maintained and updated. It must be accessible to both new employees and the experienced. And it can't be too burdensome and require extensive resources. We chose to build our solution in Salesforce!\nAccording to Tiger Li, OHA's IT Manager, a small staff of Community Engagement agents already use Salesforce. The technology exists within their organization, the financial resources have been included and budgeted, and there is a team using it every single day. to us, it seemed like a perfect opportunity to leverage existing tools and features, build a new and exciting service on top, and create enough room on the platform to allow for incremental improvements as required by leadership. Easiest decision we ever had.\nChallenges we ran into\nHackathons are challenging. Learning Salesforce is challenging. Doing both while holding a job and still leaving room for family, friends, and an occasional football game, well that's just tough. Most of our team are completely new to Salesforce, and as such, had to schedule a significant amount of time to learn about the features before using them to design and build solutions. So yeah, we're a bit behind.\nWith any new team, there are always issues with proper communication and work management, but we're figuring it out. As we get better organized, as the task list shrinks, we will be able to focus better on the features that count and allows our product to really shine. We know what needs to be done and are eager to get it done.\nAccomplishments that we're proud of\nWe\u2019ve built a website, a chat structure, some conversations and dialogues, and a host of menu options and automated responses. I've always been a glass half-empty kind of guy, so I tend to focus on what needs to be done, rather than what was completed. And unfortunately, quite often the hard work and long hours don't always equate to exciting visible changes. But this team really pulled through in the last few days and they have a lot to be proud of.\nWhat we learned\nWe are learning quite a bit and will continue to do so. Obviously, our team learned how to make a website and chatbot on and within the Salesforce platform. 2 weeks ago, this team didn't know any of that.\nWhat's next for Hiapo/Oha Chatbot\nOur solution is a chatbot, and our's needs work. For the next week or so, we will mostly be testing, building, adding, breaking, testing, and maybe just for fun, some more testing. At the heart of our design is a working AI system that can accurately resolve client inquiries. To make the system smarter, we will need tons of data. Start a pot of coffee and kiss the kids goodnight team, we may not see them for a while.\nFor security policies, instructions, and video tutorial, please see our team's GitHub account at https://github.com/HACC2021/HiapoHui\nMahalo for your consideration.", "link": "https://devpost.com/software/hiapo-oha-chatbot", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwith so many exciting challenges offered in this year\u2018s annual challenge, it took our team a significant amount of time to agree on a particular project and solution roadmap. as a native hawaiian, working for a native hawaiian organization, in charge of educating and training adults in technology, i couldn't think of a more fitting project than oha's request for a chatbot to serve beneficiaries. with our visions and missions alignment, we pressed on.\nwhat it does\nat the heart of our solution, we are building an online system that retrieves information and delivers it quickly and accurately, into the hands of oha's beneficiaries', by way of instant messaging, better known as a chatbot. this small -----> tool !!!  allows community members and the general public to inquire about programs, gain access and information about those programs, and ultimately be able to apply to those programs through their phones, mobile devices, or laptops.\nhow we built it\nas much as we would like to believe in late 80's movies, truth is, if you build it, they will not come. digital transformation is only as good as the users who implement and support the changes. so how we built this, or how we decided to build this, was heavily influenced by the tools and technology that oha already had access to. the proper solution should survive and thrive, long after our team's involvement. a quality product will need to be maintained and updated. it must be accessible to both new employees and the experienced. and it can't be too burdensome and require extensive resources. we chose to build our solution in salesforce!\naccording to tiger li, oha's it manager, a small staff of community engagement agents already use salesforce. the technology exists within their organization, the financial resources have been included and budgeted, and there is a team using it every single day. to us, it seemed like a perfect opportunity to leverage existing tools and features, build a new and exciting service on top, and create enough room on the platform to allow for incremental improvements as required by leadership. easiest decision we ever had.\nchallenges we ran into\nhackathons are challenging. learning salesforce is challenging. doing both while holding a job and still leaving room for family, friends, and an occasional football game, well that's just tough. most of our team are completely new to salesforce, and as such, had to schedule a significant amount of time to learn about the features before using them to design and build solutions. so yeah, we're a bit behind.\nwith any new team, there are always issues with proper communication and work management, but we're figuring it out. as we get better organized, as the task list shrinks, we will be able to focus better on the features that count and allows our product to really shine. we know what needs to be done and are eager to get it done.\naccomplishments that we're proud of\nwe\u2019ve built a website, a chat structure, some conversations and dialogues, and a host of menu options and automated responses. i've always been a glass half-empty kind of guy, so i tend to focus on what needs to be done, rather than what was completed. and unfortunately, quite often the hard work and long hours don't always equate to exciting visible changes. but this team really pulled through in the last few days and they have a lot to be proud of.\nwhat we learned\nwe are learning quite a bit and will continue to do so. obviously, our team learned how to make a website and chatbot on and within the salesforce platform. 2 weeks ago, this team didn't know any of that.\nwhat's next for hiapo/oha chatbot\nour solution is a chatbot, and our's needs work. for the next week or so, we will mostly be testing, building, adding, breaking, testing, and maybe just for fun, some more testing. at the heart of our design is a working ai system that can accurately resolve client inquiries. to make the system smarter, we will need tons of data. start a pot of coffee and kiss the kids goodnight team, we may not see them for a while.\nfor security policies, instructions, and video tutorial, please see our team's github account at https://github.com/hacc2021/hiapohui\nmahalo for your consideration.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59505430}, {"Unnamed: 0": 5459, "autor": "NewLawTech", "date": null, "content": "Inspiration\nLawyers work in an old-fashioned way and innovation is not really happening in the legal world. I'm a lawyer and I don't like working the rest of my life in Word. Verified data should flow like water through legal agreements. Moreover, we should start using Git based version control. I like the open source and knowledge sharing web3 community. That mindset is unusual in the legal profession. Legal agreements should be accessible for anyone, everywhere. Would be great to start a community of next generation lawyers drafting web3 enabled legal contracts based on verifiable truth and cryptographic evidence.\nWhat it does\nThe tool is quite simple and flexible to bridge traditional legal agreements to web3. Lawyers and business people should be able to work with the application without the need of deep web3 knowledge. All data will be sourced using Chainlink's Any API and in future computation in contracts could be handled by Chainlink Keepers. Version control will make a lawyers life better, therefore contract templates are stored in Github. NFT asset data is imported directly from Opensea and can be included in any agreement. The agreed upon contract can be uploaded to IPFS. Moreover, all parties can receive a NFT for evidence and contract storage. All contract state is manipulated in the frontend and ensures GDPR compliance. Only the user will decide when and where to store data externally.\nHow we built it\nWe built the application in React. Web3 and e-mail authorisation is implemented with Moralis. Contract templates are stored in Github. We used the Opensea-Moralis Plugin to import NFT data into the application. Web3storage integration is developed, but unfortunately doesn't work with webpack 4 yet, work in progress:-). IPFS upload is also done through Moralis. All data is manipulated in EditorState and is the single source of truth while drafting and negotiating the agreement. The finished contract is rendered both in HTML and in PDF. Contract parties are free to publish and store the agreement anywhere, including EVM compatable blockchains.\nChallenges we ran into\nA lot of different challenges, as I'm just a lawyer and not a professional developer. Writing code is really hard but very rewarding if a feature works in the end.\nAccomplishments that we're proud of\nEspecially proud of the agreement template Library;-) and pulling NFT data into the application.\nWhat we learned\nThat it is possible to build basically anything and that data is available everywhere. Innovation in the legal world is slow, but will be inevitable. Lawyers can learn a lot from developers and open-source projects.\nWhat's next for NewLawTech\nChallenge the legal profession by building a contract drafting tool that sources all kinds of verifiable data and helps design beautiful contracts. NewLawTech will make contracting a joy instead of a necessary evil;-)", "link": "https://devpost.com/software/newlawtech", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nlawyers work in an old-fashioned way and innovation is not really happening in the legal world. i'm a lawyer and i don't like working the rest of my life in word. verified data should flow like water through legal agreements. moreover, we should start using git based version control. i like the open source and knowledge sharing web3 community. that mindset is unusual in the legal profession. legal agreements should be accessible for anyone, everywhere. would be great to start a community of next generation lawyers drafting web3 enabled legal contracts based on verifiable truth and cryptographic evidence.\nwhat it does\nthe -----> tool !!!  is quite simple and flexible to bridge traditional legal agreements to web3. lawyers and business people should be able to work with the application without the need of deep web3 knowledge. all data will be sourced using chainlink's any api and in future computation in contracts could be handled by chainlink keepers. version control will make a lawyers life better, therefore contract templates are stored in github. nft asset data is imported directly from opensea and can be included in any agreement. the agreed upon contract can be uploaded to ipfs. moreover, all parties can receive a nft for evidence and contract storage. all contract state is manipulated in the frontend and ensures gdpr compliance. only the user will decide when and where to store data externally.\nhow we built it\nwe built the application in react. web3 and e-mail authorisation is implemented with moralis. contract templates are stored in github. we used the opensea-moralis plugin to import nft data into the application. web3storage integration is developed, but unfortunately doesn't work with webpack 4 yet, work in progress:-). ipfs upload is also done through moralis. all data is manipulated in editorstate and is the single source of truth while drafting and negotiating the agreement. the finished contract is rendered both in html and in pdf. contract parties are free to publish and store the agreement anywhere, including evm compatable blockchains.\nchallenges we ran into\na lot of different challenges, as i'm just a lawyer and not a professional developer. writing code is really hard but very rewarding if a feature works in the end.\naccomplishments that we're proud of\nespecially proud of the agreement template library;-) and pulling nft data into the application.\nwhat we learned\nthat it is possible to build basically anything and that data is available everywhere. innovation in the legal world is slow, but will be inevitable. lawyers can learn a lot from developers and open-source projects.\nwhat's next for newlawtech\nchallenge the legal profession by building a contract drafting tool that sources all kinds of verifiable data and helps design beautiful contracts. newlawtech will make contracting a joy instead of a necessary evil;-)", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 5, "media": null, "medialink": null, "identifyer": 59505459}, {"Unnamed: 0": 5471, "autor": "Flutter EasyRozgaar App", "date": null, "content": "Introduction (What it does)\nEasyRozgaar is a startup, which provides facilities to users to hire expert workers for their daily life problems such as electrical repair, motor fixing, and other problems like these, at their home without wasting time to look for them outside. It also provides a facility for workers to open their virtual shop and provider services for users.\nThere are two applications:\nUser app\nWorker app\nThis is its user-side app, where users after login/registration will see a list of work on the main screen whom they can contact by clicking on the profile card of the worker. This will open the worker profile on another screen where users can see information about workers, such as their description, verifications, and other additional information.\nIf the users want to contact the worker they can either call the worker by clicking on the call button on the worker profile or they can chat with the worker through an in-app chat facility.\nUsers can also search for the particular workers on the home screen or they can navigate to the categories screen where a bunch of professions is mentioned, they can find workers from there also. If they still will not find the worker that they want, then they can post their work which will be posted on the worker's app main screen and the workers will contact the user for that work.\nHow I built it\nI built this app with the help of the Flutter framework, Dart language, and Firebase Database. For the authentication of users, I used Firebase Auth, for storing the user and worker data (because the same database is connected with both apps), I used Cloud Firestore, and for images of users and workers, I used Cloud Storage.\nFor managing the state of the app I used the GetX State Management package and bind it with the firebase streams, because of that instant changes will be reflected on the app(Like if we change the named worker in the firebase database, it will be updated on the instance without refreshing the app).\nFor designing the app design, I did not use any tool I just created the design with the help of pencil and paper. What's next for EasyRozgaar App\nThis app is not launched yet, because the EasyRozgaar worker side app is still in the development process and a few things are remaining in this app also.\nNOTE: Because this is a private project, I cannot provide code or video of this project.", "link": "https://devpost.com/software/flutter-easyrozgaar-app", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "introduction (what it does)\neasyrozgaar is a startup, which provides facilities to users to hire expert workers for their daily life problems such as electrical repair, motor fixing, and other problems like these, at their home without wasting time to look for them outside. it also provides a facility for workers to open their virtual shop and provider services for users.\nthere are two applications:\nuser app\nworker app\nthis is its user-side app, where users after login/registration will see a list of work on the main screen whom they can contact by clicking on the profile card of the worker. this will open the worker profile on another screen where users can see information about workers, such as their description, verifications, and other additional information.\nif the users want to contact the worker they can either call the worker by clicking on the call button on the worker profile or they can chat with the worker through an in-app chat facility.\nusers can also search for the particular workers on the home screen or they can navigate to the categories screen where a bunch of professions is mentioned, they can find workers from there also. if they still will not find the worker that they want, then they can post their work which will be posted on the worker's app main screen and the workers will contact the user for that work.\nhow i built it\ni built this app with the help of the flutter framework, dart language, and firebase database. for the authentication of users, i used firebase auth, for storing the user and worker data (because the same database is connected with both apps), i used cloud firestore, and for images of users and workers, i used cloud storage.\nfor managing the state of the app i used the getx state management package and bind it with the firebase streams, because of that instant changes will be reflected on the app(like if we change the named worker in the firebase database, it will be updated on the instance without refreshing the app).\nfor designing the app design, i did not use any -----> tool !!!  i just created the design with the help of pencil and paper. what's next for easyrozgaar app\nthis app is not launched yet, because the easyrozgaar worker side app is still in the development process and a few things are remaining in this app also.\nnote: because this is a private project, i cannot provide code or video of this project.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59505471}, {"Unnamed: 0": 5505, "autor": "Insightly Audio Cloud", "date": null, "content": "Inspiration\nA picture is worth a thousand words.\nPicture a picture made only of words. What do you get? A word cloud! \u2601\ufe0e It's the perfect bite-sized, visual, graphic representation of words holding the most weight. An ideal medium for summarizing content, including long audio and video transcripts.\nThe Vision\nIssues & Incidents Tracker \u2605 Sentiment Analysis\nTrackers, speaker diarization, bulk audio uploads, combined with custom queries and data inputs can provide interesting & useful insights on aggregate data. Symbl's APIs can be used to address specific questions like...\nWhat % of 100+ inbound technical support calls are incidents related to troubleshooting printers? How long on average does it take (how much time is spent) on resolving these issues?\nWhat % of calls pertaining to XYZ product during the holiday promotional sale were focused on refunds/returns in December of 2020 ?\nWhat's the accuracy rate of calls forwarded to the correct business extensions? How many redirects before a customer's ticket/case is closed/resolved?\nAn Organizational Tool\nAdaptable to the needs of a broad audience - working professionals, students, professors, content creators - anyone looking to filter & organize several tens to hundreds of unlabeled/mislabeled, hour plus long audio files (lecture recordings, business meetings, raw video footage, webinars, speeches, interviews, podcasts, etc.) without needing to sit through and listen to every single audio merely to extract their agendas.\nFor Marketing & Customer Retention\nCustomer segmentation + follow-ups based on overall sentiment analysis detected from phone calls. Better understanding purchase activity, promotions, and customer journeys. Categorize calls into batches by buyer stage - awareness, consideration, intent, purchase, and repurchase.\nHow I built it\nAPIs\nSymbl Async Audio + Conversations APIs\nQuickChart Word Cloud + Pie Chart APIs\nTech Stack\nHTML, CSS, JavaScript (web)\nReact.js + JSX, Node.js + Express\nmaterial-ui, react-icons, react-router, Axios\nChallenges and Limitations\nPrototype specific\nThis prototype currently only supports basic insights for single, asynchronous audio without an option to upload audios via the front end interface. This means that the conversation ID must be obtained & inserted in server side code. There is also a known limitation with Cross-Origin Request Blocked (CORB), where certain audio transcripts + Quick Charts API trigger this blocking in spite of the crossOrigin=\"anonymous\" attribute added to <img>. This could be caused by trying to upload an audio above a certain file size limit. Though unadvised, this protection setting can be temporarily disabled during local development. The Access-Control Headers needs to be set as a more permanent solution.\nThe word cloud should not include any stop words. I defined merged Sets of stop words (manually) for filtering, but the ideal and more efficient approach would be to fetch all words from some dictionary API containing a list of common words, or to scrape a web page, like this one), which lists nearly 1,000 stop words. The current algorithm implemented in my code ignores several of the most frequently used words, but it does so in an inefficient way that cannot be sustained as the Set of stop words continues to grow. Removing several hundreds of stop words from each transcript at optimal performance time requires a change in algorithmic approach.\nSymbl + Quickchart API Limitations\nSymbl Async Audio API - I provided a vocabulary list to trackers param for POST Audio and found that it decreased the accuracy of the sample audio that I had tested. But perhaps I was feeding the data incorrectly.\nSymbl Conversation API's speech to text transcription is mostly accurate, but I've spotted a few misspellings here and there that changed the meaning or negatively affected an action item. Handling such data requires having checks in place. Here are two examples:\nSpeech to Text Error - Example 1 ~ Speech to Text Error - Example 2\nOccasionally, an error arises with POST audio via the Async Audio API that obstructs workflow:\nPOST Audio Error\nWith QuickChart Word Cloud API, I set removestopWords to true. However, this did not appear to have any effect on removing stop words from the word clouds that were rendered.\nAcquiring Raw Audio Recordings\nI didn't know where I could go to download free sample audios recorded in real world settings/scenarios released under fair Creative Commons use for the development of this project. I'm not sure if they even exist. So I was limited to less than a handful of mock calls that I could find online. Having access to a variety of audios to work with (such as sales calls, customer service, tech support, business meetings, lectures) or a group of closely related raw audios within a specific niche/industry would have helped immensely.\nWhat I Learned\nPassing data client <-> server. React + Node. React Routers. Project architecture. Symbl APIs.\nProject Repo\nGitHub Repo", "link": "https://devpost.com/software/insightful-w5b169", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\na picture is worth a thousand words.\npicture a picture made only of words. what do you get? a word cloud! \u2601\ufe0e it's the perfect bite-sized, visual, graphic representation of words holding the most weight. an ideal medium for summarizing content, including long audio and video transcripts.\nthe vision\nissues & incidents tracker \u2605 sentiment analysis\ntrackers, speaker diarization, bulk audio uploads, combined with custom queries and data inputs can provide interesting & useful insights on aggregate data. symbl's apis can be used to address specific questions like...\nwhat % of 100+ inbound technical support calls are incidents related to troubleshooting printers? how long on average does it take (how much time is spent) on resolving these issues?\nwhat % of calls pertaining to xyz product during the holiday promotional sale were focused on refunds/returns in december of 2020 ?\nwhat's the accuracy rate of calls forwarded to the correct business extensions? how many redirects before a customer's ticket/case is closed/resolved?\nan organizational -----> tool !!! \nadaptable to the needs of a broad audience - working professionals, students, professors, content creators - anyone looking to filter & organize several tens to hundreds of unlabeled/mislabeled, hour plus long audio files (lecture recordings, business meetings, raw video footage, webinars, speeches, interviews, podcasts, etc.) without needing to sit through and listen to every single audio merely to extract their agendas.\nfor marketing & customer retention\ncustomer segmentation + follow-ups based on overall sentiment analysis detected from phone calls. better understanding purchase activity, promotions, and customer journeys. categorize calls into batches by buyer stage - awareness, consideration, intent, purchase, and repurchase.\nhow i built it\napis\nsymbl async audio + conversations apis\nquickchart word cloud + pie chart apis\ntech stack\nhtml, css, javascript (web)\nreact.js + jsx, node.js + express\nmaterial-ui, react-icons, react-router, axios\nchallenges and limitations\nprototype specific\nthis prototype currently only supports basic insights for single, asynchronous audio without an option to upload audios via the front end interface. this means that the conversation id must be obtained & inserted in server side code. there is also a known limitation with cross-origin request blocked (corb), where certain audio transcripts + quick charts api trigger this blocking in spite of the crossorigin=\"anonymous\" attribute added to <img>. this could be caused by trying to upload an audio above a certain file size limit. though unadvised, this protection setting can be temporarily disabled during local development. the access-control headers needs to be set as a more permanent solution.\nthe word cloud should not include any stop words. i defined merged sets of stop words (manually) for filtering, but the ideal and more efficient approach would be to fetch all words from some dictionary api containing a list of common words, or to scrape a web page, like this one), which lists nearly 1,000 stop words. the current algorithm implemented in my code ignores several of the most frequently used words, but it does so in an inefficient way that cannot be sustained as the set of stop words continues to grow. removing several hundreds of stop words from each transcript at optimal performance time requires a change in algorithmic approach.\nsymbl + quickchart api limitations\nsymbl async audio api - i provided a vocabulary list to trackers param for post audio and found that it decreased the accuracy of the sample audio that i had tested. but perhaps i was feeding the data incorrectly.\nsymbl conversation api's speech to text transcription is mostly accurate, but i've spotted a few misspellings here and there that changed the meaning or negatively affected an action item. handling such data requires having checks in place. here are two examples:\nspeech to text error - example 1 ~ speech to text error - example 2\noccasionally, an error arises with post audio via the async audio api that obstructs workflow:\npost audio error\nwith quickchart word cloud api, i set removestopwords to true. however, this did not appear to have any effect on removing stop words from the word clouds that were rendered.\nacquiring raw audio recordings\ni didn't know where i could go to download free sample audios recorded in real world settings/scenarios released under fair creative commons use for the development of this project. i'm not sure if they even exist. so i was limited to less than a handful of mock calls that i could find online. having access to a variety of audios to work with (such as sales calls, customer service, tech support, business meetings, lectures) or a group of closely related raw audios within a specific niche/industry would have helped immensely.\nwhat i learned\npassing data client <-> server. react + node. react routers. project architecture. symbl apis.\nproject repo\ngithub repo", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59505505}, {"Unnamed: 0": 5571, "autor": "CommonLibrary", "date": null, "content": "Inspiration\nWhen we need to get some screen resolution every time, or judge that a certain content is null, or to print logs, get mobile phone information, etc., we need to write a lot of code. Each project has to write a lot of code, which is very troublesome.\nWhat it does\nThis general tool class can be directly imported into the packaged Library with just one line of code, which greatly saves the amount of code and development time. All permission cases have been packaged, and the code is concise and clear.\nHow we built it\nhttps://github.com/BobbySir/CommonBaseLibrary\nhint\nI have been maintaining and updating the project. If you have any problems, please come to issues to submit your questions, and I will update and fix them in time", "link": "https://devpost.com/software/commonlibrary", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwhen we need to get some screen resolution every time, or judge that a certain content is null, or to print logs, get mobile phone information, etc., we need to write a lot of code. each project has to write a lot of code, which is very troublesome.\nwhat it does\nthis general -----> tool !!!  class can be directly imported into the packaged library with just one line of code, which greatly saves the amount of code and development time. all permission cases have been packaged, and the code is concise and clear.\nhow we built it\nhttps://github.com/bobbysir/commonbaselibrary\nhint\ni have been maintaining and updating the project. if you have any problems, please come to issues to submit your questions, and i will update and fix them in time", "sortedWord": "None", "removed": "Nan", "score": 6, "comments": 1, "media": null, "medialink": null, "identifyer": 59505571}, {"Unnamed: 0": 5572, "autor": "Wine Tote Bag", "date": null, "content": "Say Welcome with Promotional Products\nMaximize your impression. Give away marketing products with your brand. The approach is tried-and-true as well as ought to belong of your advertising and marketing strategy.\nThe first day at college or on the job is a terrific moment. For the trainee, it's a new term. For the worker, it's their onboarding. Provide both gear to help them with their day-to-day routines. Therefore, use the occasion to supply them with something they will check out and utilize all the time.\nProduce a bond with your brand from the beginning. Take advantage of the opportunity on their extremely first day. As an example, after they smile for their trainee or team ID, connect it to a tailored lanyard or badge reel. Nowadays, with safety on the top of every person's top priority list, a badge reel with your logo or style is an incredibly thoughtful means to state 'welcome.'.\nWith this in mind, these are the best device for every person who requires to show or swipe credentials to move in and out of buildings and offices. Many have a practical cable that stretches out as well as snaps back like a gauging tape.\nPupils that have their arms complete moving about the quad can conveniently draw their ID. Affixed to their backpack it goes to their fingertips. And personnel will certainly discover it equally practical during their comings and also happening university as well.\nAt the same time, these are not solely optimal for academics. All services that have safety in place will certainly locate these important for their everyday regimen.\nAs an example, airline attendants can connect one to their lapel. When they require to unlock a door, like at eviction, it's a breeze. Similarly, cleansing staffs have sufficient to manage than needing to fish out their credentials.\nWhen seeking an elevated, a lot more expert look, pick one with elegant components like intermediaries or a chrome finish.\nIf you wanted to make more of an outstanding first impression. Consist of various other products that promote not only safety however health. Handing out points that everybody can utilize is a very easy means to make your advertising budget plan stretch. And also, you have actually produced a connection between your brand with a healthy and balanced lifestyle.\nAfter you have handed them their device for their qualifications, hand them a tailored hand sanitizer or, even better, a mini first aid package. Giving individuals travel-size pouches with crucial products for those just-in-case moments is a wonderful welcoming present. Antiseptic towelettes, lip balm, additional face masks, as well as bandages are all points that can be found in helpful. Search for details that your client or staff member can maximize, like a carabiner clip, touch tool, or gusset bottom.\nAnd also what a benefit with your logo design published directly on the front of a pouch with a captivating shade. Get back at a lot more value with things within a package to place your style on, like a fandana or a hand sanitizer bottle.\nAs well as when it pertains to zipper bags, a number of them are made with durable product to be made use of and also recycled. Undoubtedly, one more favorable feature.\nThe enduring results of thoughtful promotion items are an effective tool in your branding strategy. Sharing an awareness of security and also advertising a healthy way of life is a priority any business or organization should embrace.\nEverybody will appreciate a welcome bundle of products they can use every time they pertain to function or institution. Make their first day remarkable, inscribe your logo on helpful items and make a long-term impression.\nhttps://winetotebag.shutterfly.com/", "link": "https://devpost.com/software/wine-tote-bag", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "say welcome with promotional products\nmaximize your impression. give away marketing products with your brand. the approach is tried-and-true as well as ought to belong of your advertising and marketing strategy.\nthe first day at college or on the job is a terrific moment. for the trainee, it's a new term. for the worker, it's their onboarding. provide both gear to help them with their day-to-day routines. therefore, use the occasion to supply them with something they will check out and utilize all the time.\nproduce a bond with your brand from the beginning. take advantage of the opportunity on their extremely first day. as an example, after they smile for their trainee or team id, connect it to a tailored lanyard or badge reel. nowadays, with safety on the top of every person's top priority list, a badge reel with your logo or style is an incredibly thoughtful means to state 'welcome.'.\nwith this in mind, these are the best device for every person who requires to show or swipe credentials to move in and out of buildings and offices. many have a practical cable that stretches out as well as snaps back like a gauging tape.\npupils that have their arms complete moving about the quad can conveniently draw their id. affixed to their backpack it goes to their fingertips. and personnel will certainly discover it equally practical during their comings and also happening university as well.\nat the same time, these are not solely optimal for academics. all services that have safety in place will certainly locate these important for their everyday regimen.\nas an example, airline attendants can connect one to their lapel. when they require to unlock a door, like at eviction, it's a breeze. similarly, cleansing staffs have sufficient to manage than needing to fish out their credentials.\nwhen seeking an elevated, a lot more expert look, pick one with elegant components like intermediaries or a chrome finish.\nif you wanted to make more of an outstanding first impression. consist of various other products that promote not only safety however health. handing out points that everybody can utilize is a very easy means to make your advertising budget plan stretch. and also, you have actually produced a connection between your brand with a healthy and balanced lifestyle.\nafter you have handed them their device for their qualifications, hand them a tailored hand sanitizer or, even better, a mini first aid package. giving individuals travel-size pouches with crucial products for those just-in-case moments is a wonderful welcoming present. antiseptic towelettes, lip balm, additional face masks, as well as bandages are all points that can be found in helpful. search for details that your client or staff member can maximize, like a carabiner clip, touch -----> tool !!! , or gusset bottom.\nand also what a benefit with your logo design published directly on the front of a pouch with a captivating shade. get back at a lot more value with things within a package to place your style on, like a fandana or a hand sanitizer bottle.\nas well as when it pertains to zipper bags, a number of them are made with durable product to be made use of and also recycled. undoubtedly, one more favorable feature.\nthe enduring results of thoughtful promotion items are an effective tool in your branding strategy. sharing an awareness of security and also advertising a healthy way of life is a priority any business or organization should embrace.\neverybody will appreciate a welcome bundle of products they can use every time they pertain to function or institution. make their first day remarkable, inscribe your logo on helpful items and make a long-term impression.\nhttps://winetotebag.shutterfly.com/", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59505572}, {"Unnamed: 0": 5602, "autor": "Ecosnap", "date": null, "content": "Inspiration\n\"We don't inherit the earth from our ancestors; we borrow it from our children\" The way we treat our environment has huge consequences for the future of our planet. We can all do our part in protecting the Earth we live on. If we all make an effort to become more environmentally conscious, we can make the world a better place.\nWhat it does\nThink of Ecosnap as a journal to keep track of your environmental goals. Ecosnap will help us make a genuine effort to protect our environment on a daily basis. Environment goals are important on several counts. They hold us accountable for our actions, expand our definition of possible, and encourage us to push through temporary discomfort for longer-lasting change. Ecosnap allows you to record your 3R 1O (Reduce, Reuse, Recycle, Ocean) activity throughout the day.\n_Grabbed coffee in a plastic cup with a plastic straw? Add it to Ecosnap _\nReduced Food waste? Add it\nDonated clothes? Add it\nYou'll get a badge as a reward for every 100 eco karma you receive which you can then share with your snap friends. Your friends could then be encouraged and motivated to follow a similar path and support the common goal of protecting our planet.\nHow we built it\nLogin Kit (Authentication): Used Passport for authenticating with Snapchat using the OAuth 2.0 API for my Express-based web application. This helped redirect the user-agent back to the client.\nCreative Kit (Share): Used Snap Creative Kit to share the content of the web app. The user can then scan the Snapcode with the Snapchat camera if on the web (or will be redirected to Snapchat if on a mobile device), and the link will attach the URL to Snapchat.\nUsed JS to keep track of daily karma points for different categories.\nChallenges we ran into\nThe main challenge I ran into was when trying to integrate the creative kit with my web app that I was hosting locally on port 3000. Snapchat Developer account requires us to provide an https secured attachment link domain for us to be able to share the content of website. Since I was using localhost:3000, I wasn't able to provide a valid URL: not just because it was not HTTPS secured but also because it was looking for a domain name instead of localhost.\nSo, in order to accomplish that after a lot of research and countless hours of trying several approach, I found a life-saving tool called Ngrok, that creates a tunnel from the public internet http://.ngrok.io to a port on your local machine (3000 in my case). I was then able to share auto-generated URL with anyone to give them access to my local development environment.\nAccomplishments that we're proud of\nMaintaining Snap authentication for multiples pages. Getting creative kit integrated on the app.\nWhat we learned\nLearned about Passport and Ngrok, and how strong of a tool they are. This hackathon also helped me learn ejs, aunthetication, and the concept of access tokens\nWhat's next for Ecosnap\nHosting it so that my friends are able to use it without having to use ngrok link. Creating a database for personalized web view.\nIt would also be cool if I could integrate tokens with the app so that we can reward other users for their outstanding efforts", "link": "https://devpost.com/software/ecosnap-jfh82z", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\n\"we don't inherit the earth from our ancestors; we borrow it from our children\" the way we treat our environment has huge consequences for the future of our planet. we can all do our part in protecting the earth we live on. if we all make an effort to become more environmentally conscious, we can make the world a better place.\nwhat it does\nthink of ecosnap as a journal to keep track of your environmental goals. ecosnap will help us make a genuine effort to protect our environment on a daily basis. environment goals are important on several counts. they hold us accountable for our actions, expand our definition of possible, and encourage us to push through temporary discomfort for longer-lasting change. ecosnap allows you to record your 3r 1o (reduce, reuse, recycle, ocean) activity throughout the day.\n_grabbed coffee in a plastic cup with a plastic straw? add it to ecosnap _\nreduced food waste? add it\ndonated clothes? add it\nyou'll get a badge as a reward for every 100 eco karma you receive which you can then share with your snap friends. your friends could then be encouraged and motivated to follow a similar path and support the common goal of protecting our planet.\nhow we built it\nlogin kit (authentication): used passport for authenticating with snapchat using the oauth 2.0 api for my express-based web application. this helped redirect the user-agent back to the client.\ncreative kit (share): used snap creative kit to share the content of the web app. the user can then scan the snapcode with the snapchat camera if on the web (or will be redirected to snapchat if on a mobile device), and the link will attach the url to snapchat.\nused js to keep track of daily karma points for different categories.\nchallenges we ran into\nthe main challenge i ran into was when trying to integrate the creative kit with my web app that i was hosting locally on port 3000. snapchat developer account requires us to provide an https secured attachment link domain for us to be able to share the content of website. since i was using localhost:3000, i wasn't able to provide a valid url: not just because it was not https secured but also because it was looking for a domain name instead of localhost.\nso, in order to accomplish that after a lot of research and countless hours of trying several approach, i found a life-saving -----> tool !!!  called ngrok, that creates a tunnel from the public internet http://.ngrok.io to a port on your local machine (3000 in my case). i was then able to share auto-generated url with anyone to give them access to my local development environment.\naccomplishments that we're proud of\nmaintaining snap authentication for multiples pages. getting creative kit integrated on the app.\nwhat we learned\nlearned about passport and ngrok, and how strong of a tool they are. this hackathon also helped me learn ejs, aunthetication, and the concept of access tokens\nwhat's next for ecosnap\nhosting it so that my friends are able to use it without having to use ngrok link. creating a database for personalized web view.\nit would also be cool if i could integrate tokens with the app so that we can reward other users for their outstanding efforts", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59505602}, {"Unnamed: 0": 5628, "autor": "Document Extraction Tool", "date": null, "content": "Inspiration\nThe field of intelligent document extraction is a rapidly growing field with huge potential to create disruptions in various document processing industries from healthcare (insurance forms), finance (tax forms, invoices), consumer goods (receipts), etc. Unfortunately, because of the unique complexity of the problem and limited open-source data available, very little progress has been made in the open space. By building a modular, extensible end-to-end tool in PyTorch with a dedicated web interface, we hope to enable easier and faster adoption of the technology to both researchers and practitioners alike!\nWhat it does\nVery simply put, it \"takes in a document, runs OCR (Optical Character Recognition) and uses those OCR results to either train new NER (Named Entity Recognizer) models or predict using existing pretrained ones for forms and receipts data\".\nHow we built it\nFor OCR component, we took the best pretrained checkpoints, froze and optimized it, created a torchserve workflow to stitch together detection and recognition model and finally served it on the cloud in a serverless manner. For the Extraction part, we fine-tuned custom models using huggingface library, built an end-to-end training pipeline .\nChallenges we ran into\nCreating workflow in torchserve was particularly challenging, as OCR had three separate models stitched together in a complex serial+parallel manner, each component having their own custom preprocessing and postprocessing handlers.\nAlso for deployment to Vertex AI platform on GCP,we had to modify the torchserve\u2019s source-code to make it compatible with Vertex AI deployment conditions.\nAccomplishments that we're proud of\nServing in the most efficient way possible: We froze the model-weights to torchscript format, optimized it for cpu deployment and served in a dedicated model-server by creating a workflow in torchserve. Given torchserve is a new space, we are proud to make some contribution there.\nCreating a Few-shot extraction tool: The pre-trained model is very accurate even in very low resource setting. The key to this success was the selection of a dedicated Document NER model architecture which takes in the 2d information of documents in the form of bounding boxes instead of just plain simple BERT based NER models.\nWhat we learned\nBuilding an end-to-end system in PyTorch was truly an incredible learning experience. From training models to freezing its weights, optimizing it, serving it in torchserve and deploying as a docker container, we learnt a lot during this process.\nExperimented with a lot of pre-trained models for NER tasks and fine-tuned for specific datasets to achieve the best accuracy.\nWhat's next for Document Extraction Tool\nCreate a detailed documentation on it's core components, how to use and extend it.\nAdd more models and architectures.", "link": "https://devpost.com/software/document-extraction-tool", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nthe field of intelligent document extraction is a rapidly growing field with huge potential to create disruptions in various document processing industries from healthcare (insurance forms), finance (tax forms, invoices), consumer goods (receipts), etc. unfortunately, because of the unique complexity of the problem and limited open-source data available, very little progress has been made in the open space. by building a modular, extensible end-to-end -----> tool !!!  in pytorch with a dedicated web interface, we hope to enable easier and faster adoption of the technology to both researchers and practitioners alike!\nwhat it does\nvery simply put, it \"takes in a document, runs ocr (optical character recognition) and uses those ocr results to either train new ner (named entity recognizer) models or predict using existing pretrained ones for forms and receipts data\".\nhow we built it\nfor ocr component, we took the best pretrained checkpoints, froze and optimized it, created a torchserve workflow to stitch together detection and recognition model and finally served it on the cloud in a serverless manner. for the extraction part, we fine-tuned custom models using huggingface library, built an end-to-end training pipeline .\nchallenges we ran into\ncreating workflow in torchserve was particularly challenging, as ocr had three separate models stitched together in a complex serial+parallel manner, each component having their own custom preprocessing and postprocessing handlers.\nalso for deployment to vertex ai platform on gcp,we had to modify the torchserve\u2019s source-code to make it compatible with vertex ai deployment conditions.\naccomplishments that we're proud of\nserving in the most efficient way possible: we froze the model-weights to torchscript format, optimized it for cpu deployment and served in a dedicated model-server by creating a workflow in torchserve. given torchserve is a new space, we are proud to make some contribution there.\ncreating a few-shot extraction tool: the pre-trained model is very accurate even in very low resource setting. the key to this success was the selection of a dedicated document ner model architecture which takes in the 2d information of documents in the form of bounding boxes instead of just plain simple bert based ner models.\nwhat we learned\nbuilding an end-to-end system in pytorch was truly an incredible learning experience. from training models to freezing its weights, optimizing it, serving it in torchserve and deploying as a docker container, we learnt a lot during this process.\nexperimented with a lot of pre-trained models for ner tasks and fine-tuned for specific datasets to achieve the best accuracy.\nwhat's next for document extraction tool\ncreate a detailed documentation on it's core components, how to use and extend it.\nadd more models and architectures.", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 1, "media": null, "medialink": null, "identifyer": 59505628}, {"Unnamed: 0": 5665, "autor": "AQ-Station", "date": null, "content": "Inspiration\nOn an opening day for HACC 2021, we weren't sure about which challenge to choose, but when the problem was presented from the Animal Quarantine facilities, we felt inclined to try and help out as much as we possibly could. We hope that we found and made a solution to their problem!\nWhat it does\nAQ-Station functions as a simple, convenient tool to support Animal Quarantine administrators and pet owners to have consistent, effective, and efficient communication with each other. The Animal Quarantine administrators are able to easily add, edit, and remove pets based on their microchip code as pets enter and leave the animal quarantine facilities. Pet owners are able to easily check the status of their pet and their position in the queue using their pets\u2019 microchip code.\nHow we built it\nThis application was built using Meteor.js and MongoDB, programmed in JavaScript, and designed using HTML/CSS.\nRunning the Project\nFirst, install Meteor.\nSecond, go to the AQ-Station repository, and click the \"Clone or download\" button to download your new GitHub repo to your local file system. Using GitHub Desktop is a great choice if you use MacOS or Windows.\nThird, cd into the app/ directory of your local copy of the repo, and install third party libraries with:\n$ meteor npm install\nLastly, run the system with:\n$ meteor npm run start\nThe app should be running at http://localhost:3000.\nChallenges we ran into\nImplementing the queue was the most difficult aspect of this project and took the greatest amount of time. Concerns were also raised in regards to having the first in, first out (FIFO) queue be relegated as a background process. Thus, our solution is to encourage administrative level accounts to have a tab opened to the page of the waitlist of owners for the queue to continuously update. There was uncertainty whether we would be able to program a fully functional queue while also protecting the data, security, and privacy of the users.\nAccomplishments that we're proud of\nThe clean authentication process and the queue implementation were what we were most proud of. We were fortunate enough to solve any difficulties and aspire to create a functional, working project.\nWhat we learned\nTime management and consistent team planning were crucial to the success of our project. We were able to use different skill sets from each of us to incorporate the frontend and backend of the application.\nWhat's next for AQ Station\nWe hope to continue developing AQ-Station, making it cleaner and simple to use. Implement more something to make it easier for the admin to use. Add an edit announcements page. Make it possible for the admin to edit the queue page not just remove the first person in the queue. Revamp queue system to functionally operate throughout the app, not just being present in a single waitlist page.\nSee the app live!\nDeployed Application: http://142.93.62.191/#/\nThe way to access the administration side is:\nClick the \"Administration\" button\nInput \"admin@foo.com\" for the Email address\nInput \"changeme\" for the password\nClick the \"Submit\" button and you will be directed to a page where you can add and remove pets.\nThe way to access the pet-owner side:\nClick the \"Pet Owner\" button\nYou can input any credentials for the Email address, First Name, Last Name, Phone Number, and Password text slots, but you must input either \"55555555\" or \"88888888\" for the Microchip #1 slot.\nClick the \"Submit\" button and you will be directed to a page where you can check if your pet is ready for pick-up and you will be redirected to your place in the queue.", "link": "https://devpost.com/software/aq-station", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\non an opening day for hacc 2021, we weren't sure about which challenge to choose, but when the problem was presented from the animal quarantine facilities, we felt inclined to try and help out as much as we possibly could. we hope that we found and made a solution to their problem!\nwhat it does\naq-station functions as a simple, convenient -----> tool !!!  to support animal quarantine administrators and pet owners to have consistent, effective, and efficient communication with each other. the animal quarantine administrators are able to easily add, edit, and remove pets based on their microchip code as pets enter and leave the animal quarantine facilities. pet owners are able to easily check the status of their pet and their position in the queue using their pets\u2019 microchip code.\nhow we built it\nthis application was built using meteor.js and mongodb, programmed in javascript, and designed using html/css.\nrunning the project\nfirst, install meteor.\nsecond, go to the aq-station repository, and click the \"clone or download\" button to download your new github repo to your local file system. using github desktop is a great choice if you use macos or windows.\nthird, cd into the app/ directory of your local copy of the repo, and install third party libraries with:\n$ meteor npm install\nlastly, run the system with:\n$ meteor npm run start\nthe app should be running at http://localhost:3000.\nchallenges we ran into\nimplementing the queue was the most difficult aspect of this project and took the greatest amount of time. concerns were also raised in regards to having the first in, first out (fifo) queue be relegated as a background process. thus, our solution is to encourage administrative level accounts to have a tab opened to the page of the waitlist of owners for the queue to continuously update. there was uncertainty whether we would be able to program a fully functional queue while also protecting the data, security, and privacy of the users.\naccomplishments that we're proud of\nthe clean authentication process and the queue implementation were what we were most proud of. we were fortunate enough to solve any difficulties and aspire to create a functional, working project.\nwhat we learned\ntime management and consistent team planning were crucial to the success of our project. we were able to use different skill sets from each of us to incorporate the frontend and backend of the application.\nwhat's next for aq station\nwe hope to continue developing aq-station, making it cleaner and simple to use. implement more something to make it easier for the admin to use. add an edit announcements page. make it possible for the admin to edit the queue page not just remove the first person in the queue. revamp queue system to functionally operate throughout the app, not just being present in a single waitlist page.\nsee the app live!\ndeployed application: http://142.93.62.191/#/\nthe way to access the administration side is:\nclick the \"administration\" button\ninput \"admin@foo.com\" for the email address\ninput \"changeme\" for the password\nclick the \"submit\" button and you will be directed to a page where you can add and remove pets.\nthe way to access the pet-owner side:\nclick the \"pet owner\" button\nyou can input any credentials for the email address, first name, last name, phone number, and password text slots, but you must input either \"55555555\" or \"88888888\" for the microchip #1 slot.\nclick the \"submit\" button and you will be directed to a page where you can check if your pet is ready for pick-up and you will be redirected to your place in the queue.", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59505665}, {"Unnamed: 0": 5668, "autor": "Blux", "date": null, "content": "People with disabilities report frequent mental distress almost 5 times as often as adults without disabilities. Of this population, there are approximately 2.1 million people who live with limb loss in the United States. These individuals have difficulties both accessing and using technology and suffer from a lack of effective mental health resources. To mitigate these barriers between individuals and therapists, we introduce a technological intervention that implements a software-only sensing framework. We propose a blow-based cursor control method that allows users to have natural hands-free interactions for effective computer support in art therapy. We also conceptualize a design space for shared decision-making between individuals with disabilities and therapists to make psychological assessments. In addition to being a mental health tool, our cursor design can be extended to other domains such as education, gaming, and other productivity-based tools.", "link": "https://devpost.com/software/blux-dx45qk", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "people with disabilities report frequent mental distress almost 5 times as often as adults without disabilities. of this population, there are approximately 2.1 million people who live with limb loss in the united states. these individuals have difficulties both accessing and using technology and suffer from a lack of effective mental health resources. to mitigate these barriers between individuals and therapists, we introduce a technological intervention that implements a software-only sensing framework. we propose a blow-based cursor control method that allows users to have natural hands-free interactions for effective computer support in art therapy. we also conceptualize a design space for shared decision-making between individuals with disabilities and therapists to make psychological assessments. in addition to being a mental health -----> tool !!! , our cursor design can be extended to other domains such as education, gaming, and other productivity-based tools.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59505668}, {"Unnamed: 0": 5675, "autor": "Labelless", "date": null, "content": "Fashion art and beauty have been a powerful tool in expressing identity and unfolding personality, especially among people with disabilities. Whilst big brands and multiple creative houses are embarking the road of inclusivity in the luxury market, accessibility to the marginalised community is still lacking. Moreover, due to the unintended bias in advertising algorithms, fashion brands and beauty marketers sometimes find it difficult to advertise their products through social media giants with the likes of Facebook, Amazon and etc. Hence, I'm creating an apparel-and-beauty focused social media platform catered to the disabled community, whether it is for those with vision impaired or people with disabled motor capability.\nI'm creating a social media app \"Labelless\" to help people with disabilities to stay connected. Since it is designed with accessibility in mind, this is a place where brands and disabled people can form an inner circle more than celebrity influencers, and build relationships which create more trust and closeness.\nSome of the features to highlight:\nPost enlargement and descriptions: Watch the creative storytelling unfold within the fashion and beauty art \u2014 how it's made, why it's made in a certain way, and the story behind it\nInteractive comments and chat functionalities: build connection between brands and people with disabilities; understand fashion and beauty from a different perspective\nUpload button and personal posts: unearth creative processes of fashion and beauty, and discover talent within the marginalised community\nConnections: inner circle which will be able to generate peer-to-peer mental support\nWord-to-speech functionality: for descriptions in words, by tapping on this toggle, the descriptions will be read out\nNot only does this help to create a better support and understanding of the disabled community, brands will be able to have a better reach to the intended group.", "link": "https://devpost.com/software/labelless", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "fashion art and beauty have been a powerful -----> tool !!!  in expressing identity and unfolding personality, especially among people with disabilities. whilst big brands and multiple creative houses are embarking the road of inclusivity in the luxury market, accessibility to the marginalised community is still lacking. moreover, due to the unintended bias in advertising algorithms, fashion brands and beauty marketers sometimes find it difficult to advertise their products through social media giants with the likes of facebook, amazon and etc. hence, i'm creating an apparel-and-beauty focused social media platform catered to the disabled community, whether it is for those with vision impaired or people with disabled motor capability.\ni'm creating a social media app \"labelless\" to help people with disabilities to stay connected. since it is designed with accessibility in mind, this is a place where brands and disabled people can form an inner circle more than celebrity influencers, and build relationships which create more trust and closeness.\nsome of the features to highlight:\npost enlargement and descriptions: watch the creative storytelling unfold within the fashion and beauty art \u2014 how it's made, why it's made in a certain way, and the story behind it\ninteractive comments and chat functionalities: build connection between brands and people with disabilities; understand fashion and beauty from a different perspective\nupload button and personal posts: unearth creative processes of fashion and beauty, and discover talent within the marginalised community\nconnections: inner circle which will be able to generate peer-to-peer mental support\nword-to-speech functionality: for descriptions in words, by tapping on this toggle, the descriptions will be read out\nnot only does this help to create a better support and understanding of the disabled community, brands will be able to have a better reach to the intended group.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59505675}, {"Unnamed: 0": 5692, "autor": "Inclusive Speech App", "date": null, "content": "Inspiration\nOne of my neighbor\u2019s kids had the problem was that she was not able to clearly speak what was in her mind due to her speaking problem.\nAnd that's how our idea came into the picture and we took up this challenge as to how we can make it more accessible for her to clearly express herself and her thoughts.\nWhat it does\nStudies across different languages and age groups have found that around 50 words account for 40-50% of our daily communication. 100 words account for 60%, and 200-400 words make up 80% of the words we use every day. Core words are usually verbs, adjectives, and pronouns, and are less likely to be nouns.\nBy giving AAC learners quick access to these core words, we\u2019re providing them with a powerful tool to communicate whatever they want to say. Rather than relying on preprogrammed sentences or phrases such as \u201cI want\u201d and \u201cI see\u201d, they can choose from a relatively small set of words to create their own sentences. They can express a wider variety of ideas, and work on grammar.\nHow we built it\nWe built it using React and Microsoft Azure Text to Speech API\nChallenges we ran into\nFinalizing the design proved to be difficult\nAccomplishments that we're proud of\nWe were able to help Jasmine was our biggest accomplishment\nWhat we learned\nThe problems faced by people facing speaking difficulties\nWhat's next for Inclusive Speech App\nWe would like to make an iPad app.", "link": "https://devpost.com/software/inclusive-speech-app", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\none of my neighbor\u2019s kids had the problem was that she was not able to clearly speak what was in her mind due to her speaking problem.\nand that's how our idea came into the picture and we took up this challenge as to how we can make it more accessible for her to clearly express herself and her thoughts.\nwhat it does\nstudies across different languages and age groups have found that around 50 words account for 40-50% of our daily communication. 100 words account for 60%, and 200-400 words make up 80% of the words we use every day. core words are usually verbs, adjectives, and pronouns, and are less likely to be nouns.\nby giving aac learners quick access to these core words, we\u2019re providing them with a powerful -----> tool !!!  to communicate whatever they want to say. rather than relying on preprogrammed sentences or phrases such as \u201ci want\u201d and \u201ci see\u201d, they can choose from a relatively small set of words to create their own sentences. they can express a wider variety of ideas, and work on grammar.\nhow we built it\nwe built it using react and microsoft azure text to speech api\nchallenges we ran into\nfinalizing the design proved to be difficult\naccomplishments that we're proud of\nwe were able to help jasmine was our biggest accomplishment\nwhat we learned\nthe problems faced by people facing speaking difficulties\nwhat's next for inclusive speech app\nwe would like to make an ipad app.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59505692}, {"Unnamed: 0": 5700, "autor": "Beauty Assistant AI", "date": null, "content": "Inspiration\nTechnology has advanced exponentially to make everyday life easier. Online shopping, specifically, mitigated the need to physically go shopping in stores. We believe that people with disabilities should not be barred from the benefits of online shopping. It is our job as developers to ensure that our products are accessible to everyone - it should not be a privilege to shop online.\nWhat it does\nOur Beauty AI Assistant tool allows customers to use voice commands to perform operations on our site. This includes searching for specific products, adding to cart, and checking out. Beauty AI also describes products and reads text displayed on the site directly to the user. This allows users with visual impairments to know what\u2019s on the screen without actually needing to see it.\nHow we built it\nWe built the Beauty AI Assistant using Artificial intelligence and Machine learning to train models that will give a hands-free shopping experience. The entire frontend was built using Tailwind and React to give It a complete responsive feel on mobile and desktop. The backend of this application is driven by Laravel. All these amazing modern stack tools allowed us to create a complete functional and responsive UI design.\nAccomplishments that we're proud of\nWith this innovation, we are doing our responsibility to ensure people with disabilities feel welcomed and involved.\nWhat we learned\nWe learned how designing and developing for accessibility purposes not only allows underrepresented groups to interact with your application, but improves the overall usability for everyone.\nWhat's next for Beauty Assistant AI\nVoice access to the web site can be integrated with Amazon Alexa, Google Assistant, and Siri. We have created a functioning demo video and shared the link for our Beauty Assistant AI communicating with Amazon Alexa. Siri can be implemented by saving the Beauty AI's phone number as a phone contact. Eye tracking can be implemented on the site as well to improve accessibility to those that are hard of hearing.", "link": "https://devpost.com/software/beauty-assistant-ai", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ntechnology has advanced exponentially to make everyday life easier. online shopping, specifically, mitigated the need to physically go shopping in stores. we believe that people with disabilities should not be barred from the benefits of online shopping. it is our job as developers to ensure that our products are accessible to everyone - it should not be a privilege to shop online.\nwhat it does\nour beauty ai assistant -----> tool !!!  allows customers to use voice commands to perform operations on our site. this includes searching for specific products, adding to cart, and checking out. beauty ai also describes products and reads text displayed on the site directly to the user. this allows users with visual impairments to know what\u2019s on the screen without actually needing to see it.\nhow we built it\nwe built the beauty ai assistant using artificial intelligence and machine learning to train models that will give a hands-free shopping experience. the entire frontend was built using tailwind and react to give it a complete responsive feel on mobile and desktop. the backend of this application is driven by laravel. all these amazing modern stack tools allowed us to create a complete functional and responsive ui design.\naccomplishments that we're proud of\nwith this innovation, we are doing our responsibility to ensure people with disabilities feel welcomed and involved.\nwhat we learned\nwe learned how designing and developing for accessibility purposes not only allows underrepresented groups to interact with your application, but improves the overall usability for everyone.\nwhat's next for beauty assistant ai\nvoice access to the web site can be integrated with amazon alexa, google assistant, and siri. we have created a functioning demo video and shared the link for our beauty assistant ai communicating with amazon alexa. siri can be implemented by saving the beauty ai's phone number as a phone contact. eye tracking can be implemented on the site as well to improve accessibility to those that are hard of hearing.", "sortedWord": "None", "removed": "Nan", "score": 50, "comments": 1, "media": null, "medialink": null, "identifyer": 59505700}, {"Unnamed: 0": 5701, "autor": "SpookScanner", "date": null, "content": "Inspiration\nWe were inspired by Google Reverse Image technology and Photoshop tools.\nWhat it does\nThis is Spookscanner, a serverless image processing/ object detection website. This website detects black-listed objects and returns them whether they are there or not (Pumpkin, Ghost, Zombie). Additionally, the website has a convenient image processing feature for ease of access in case\nHow we built it\nWe built it using html, css, and javascript. We implemented some Python into the editing portion.\nChallenges we ran into\nOur main challenge was that it was very time-consuming. We also had trouble with some text alignment issues at the very end. Most of us did not know html, css, javascript, or python prior to this project so it was a very fun challenge in learning these languages.\nAccomplishments that we're proud of\nWe were proud of everything that we have done. Again, some of us did not know these languages that we used beforehand, and to see our website come to life like this, is amazing to see.\nWhat we learned\nWe all learned a new language today. We learned the functionality of a lot of unique tools that each language has to offer. We also learned a lot about Google Cloud.\nWhat's next for SpookScanner\nWe want to be able to connect our Python editing program to a web interface and then eventually to our original html files. We then want to somehow implement the reverse image search into our SpookScan tool to find similar images.", "link": "https://devpost.com/software/spookscanner", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe were inspired by google reverse image technology and photoshop tools.\nwhat it does\nthis is spookscanner, a serverless image processing/ object detection website. this website detects black-listed objects and returns them whether they are there or not (pumpkin, ghost, zombie). additionally, the website has a convenient image processing feature for ease of access in case\nhow we built it\nwe built it using html, css, and javascript. we implemented some python into the editing portion.\nchallenges we ran into\nour main challenge was that it was very time-consuming. we also had trouble with some text alignment issues at the very end. most of us did not know html, css, javascript, or python prior to this project so it was a very fun challenge in learning these languages.\naccomplishments that we're proud of\nwe were proud of everything that we have done. again, some of us did not know these languages that we used beforehand, and to see our website come to life like this, is amazing to see.\nwhat we learned\nwe all learned a new language today. we learned the functionality of a lot of unique tools that each language has to offer. we also learned a lot about google cloud.\nwhat's next for spookscanner\nwe want to be able to connect our python editing program to a web interface and then eventually to our original html files. we then want to somehow implement the reverse image search into our spookscan -----> tool !!!  to find similar images.", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59505701}, {"Unnamed: 0": 5740, "autor": "YouTube Video NFTknz", "date": null, "content": "Inspiration\nYouTube Video NFTknz inspiration came from the opportunities that NFTs offer to social media creators to go beyond the kinds of monetization they are allowed by the existing platforms. Additionally, to try to address one of the biggest issues of NFTs, ownership, and copyright.\nWhat it does\nThis first prototype focuses on tokenizing YouTube videos as NFTs. This way, the creators can explore the new monetization opportunities offered by NFTs beyond YouTube video embedding. To address ownership and copyright issues, video ownership is verified using a Chainlink oracle. The oracle consumes YouTube's API and checks if some predefined text has been added to the video description, proofing this way control of the corresponding YouTube account.\nTo further clarify the implications of NFT transfer, NFT metadata is based on semantic data serialized as JSON-LD that, in addition to the standard NFT attributes like title or image, it describes in a machine-actionable way the reuse terms being licensed to the NFT owner.\nHow we built it\nChainlink oracles (one for Rinkeby and another for Mumbai) have been deployed. The corresponding Chainlink nodes use the YTAdapter included in the GitHub repository to consume YouTube API and check that the requested text is included in the YouTube video description.\nBefore minting, a request is sent to the corresponding oracle to check if the video description retrieved through the API includes a link to the tokenId to be minted. Token identifiers are generated from the YouTube video identifier so all NFTs for the same videoId share the first 240 bits. The last 16 bits are reserved to identify the edition of the NFT.\nIf the oracle finds the expected tokenId in the video description, the current user controls the corresponding YouTube account, and ownership is asserted on-chain. The NFT is minted and ownership is recorded on the NFT smart contract. Minting further editions of the same YouTube video won't require verification through the oracle, it will be just checked that the minter is the registered owner.\nThe NFT metadata is stored on IPFS through NFT.storage. The metadata points to semantic data based on JSON-LD. It is serialized so it is also compatible with what NFT marketplaces like OpenSea expect. The standard NFT attributes like title, description, image... appear as plain JSON. The rest of the metadata uses semantic vocabularies like Schema.org and ontologies like the Copyright Ontology to capture the details about what is licensed to the NFT owner from a copyright perspective.\nThe smart contracts for the NFT and oracles have been deployed using Alchemy to the testnets Ethereum Rinkeby and Polygon Mumbai. Finally, for the frontend, Moralis has been used. A server has been configured for both testnets and consumed from the frontend so it is possible to switch between both testnets.\nThe frontend, based on Moralis React, lists YouTube Video NFTs and their transactions. After authenticating, a user can also look at her/his wallet showing the YTVideo NFTs owned or mint new NFTs for her/his videos. Finally, the frontend also allows exploring a particular NFT edition or all the editions for a given YouTube video.\nChallenges we ran into\nThe first challenge was to create the smart contracts infrastructure required. For instance, generating token identifiers including a video-specific part and the last bits to represent the edition of the NFT. Then, connecting the minting process to the oracle, so minting the NFTs is just done when the oracle callback specifies that ownership has been verified.\nAnother challenge was to settle a development environment that facilitated local development and testing using a local network while simulating the presence of the Chainlink oracle, without requiring any code changes beyond the use of the MockOracle. This part might be useful for other developers and eventually become part of the Chainlink started kits. The tests include OracleRequest events listeners that automatically fulfill them based on the input parameters. Additionally, we have developed a task that runs the same listener so the presence of the oracle can be simulated while interacting with the local network.\nFinally, the third main challenge was about the NFT metadata, which should combine semantic data to unambiguously specify the terms being licensed by the NFT while being compatible with existing NFT marketplaces like OpenSea.\nAccomplishments that we're proud of\nYouTube Video NFTs with multiple editions for the same video identifier.\nVideo ownership verification using a Chainlink oracle.\nRich NFT metadata stating the terms being licensed to the owner using semantic metadata and ontologies.\nSemantic metadata representation using JSON-LD but compatible with existing NFT marketplaces, so NFTs can be traded there.\nFrontend that facilitates user experience, including verifying that the YouTube video exists, fetching its title, or monitoring oracle verification events so the users get feedback through the whole minting process.\nWhat we learned\nThough I had some previous experience with blockchain and fronted development, this was the first time using Hardhat or React. This has impacted a lot the development. Fortunately, it was possible to start from the Chainlink hardhat-starter-kit and the Moralis React templates. They have been customized to fit the needs of the project. During the process, a lot has been learned about Chainlink infrastructure or Moralis features. The task has been also extensively facilitated by the integration of other tools like NFT.storage to facilitate the interaction with IPFS or Alchemy to seamlessly integrate the different testing networks.\nWhat's next for YouTube Video NFTknz\nThe experience with Polygon Mumbai has been really good and after seeing the very attractive minting costs of this network, we are seriously considering the deployment of the tool in Polygon Mainnet.\nAdditionally, we would like to explore ways to allow users to customize the terms licensed through each NFT they mint, configure the number of editions per video, or extend the features to other social networks like Facebook or Twitter.", "link": "https://devpost.com/software/socialnftknz", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nyoutube video nftknz inspiration came from the opportunities that nfts offer to social media creators to go beyond the kinds of monetization they are allowed by the existing platforms. additionally, to try to address one of the biggest issues of nfts, ownership, and copyright.\nwhat it does\nthis first prototype focuses on tokenizing youtube videos as nfts. this way, the creators can explore the new monetization opportunities offered by nfts beyond youtube video embedding. to address ownership and copyright issues, video ownership is verified using a chainlink oracle. the oracle consumes youtube's api and checks if some predefined text has been added to the video description, proofing this way control of the corresponding youtube account.\nto further clarify the implications of nft transfer, nft metadata is based on semantic data serialized as json-ld that, in addition to the standard nft attributes like title or image, it describes in a machine-actionable way the reuse terms being licensed to the nft owner.\nhow we built it\nchainlink oracles (one for rinkeby and another for mumbai) have been deployed. the corresponding chainlink nodes use the ytadapter included in the github repository to consume youtube api and check that the requested text is included in the youtube video description.\nbefore minting, a request is sent to the corresponding oracle to check if the video description retrieved through the api includes a link to the tokenid to be minted. token identifiers are generated from the youtube video identifier so all nfts for the same videoid share the first 240 bits. the last 16 bits are reserved to identify the edition of the nft.\nif the oracle finds the expected tokenid in the video description, the current user controls the corresponding youtube account, and ownership is asserted on-chain. the nft is minted and ownership is recorded on the nft smart contract. minting further editions of the same youtube video won't require verification through the oracle, it will be just checked that the minter is the registered owner.\nthe nft metadata is stored on ipfs through nft.storage. the metadata points to semantic data based on json-ld. it is serialized so it is also compatible with what nft marketplaces like opensea expect. the standard nft attributes like title, description, image... appear as plain json. the rest of the metadata uses semantic vocabularies like schema.org and ontologies like the copyright ontology to capture the details about what is licensed to the nft owner from a copyright perspective.\nthe smart contracts for the nft and oracles have been deployed using alchemy to the testnets ethereum rinkeby and polygon mumbai. finally, for the frontend, moralis has been used. a server has been configured for both testnets and consumed from the frontend so it is possible to switch between both testnets.\nthe frontend, based on moralis react, lists youtube video nfts and their transactions. after authenticating, a user can also look at her/his wallet showing the ytvideo nfts owned or mint new nfts for her/his videos. finally, the frontend also allows exploring a particular nft edition or all the editions for a given youtube video.\nchallenges we ran into\nthe first challenge was to create the smart contracts infrastructure required. for instance, generating token identifiers including a video-specific part and the last bits to represent the edition of the nft. then, connecting the minting process to the oracle, so minting the nfts is just done when the oracle callback specifies that ownership has been verified.\nanother challenge was to settle a development environment that facilitated local development and testing using a local network while simulating the presence of the chainlink oracle, without requiring any code changes beyond the use of the mockoracle. this part might be useful for other developers and eventually become part of the chainlink started kits. the tests include oraclerequest events listeners that automatically fulfill them based on the input parameters. additionally, we have developed a task that runs the same listener so the presence of the oracle can be simulated while interacting with the local network.\nfinally, the third main challenge was about the nft metadata, which should combine semantic data to unambiguously specify the terms being licensed by the nft while being compatible with existing nft marketplaces like opensea.\naccomplishments that we're proud of\nyoutube video nfts with multiple editions for the same video identifier.\nvideo ownership verification using a chainlink oracle.\nrich nft metadata stating the terms being licensed to the owner using semantic metadata and ontologies.\nsemantic metadata representation using json-ld but compatible with existing nft marketplaces, so nfts can be traded there.\nfrontend that facilitates user experience, including verifying that the youtube video exists, fetching its title, or monitoring oracle verification events so the users get feedback through the whole minting process.\nwhat we learned\nthough i had some previous experience with blockchain and fronted development, this was the first time using hardhat or react. this has impacted a lot the development. fortunately, it was possible to start from the chainlink hardhat-starter-kit and the moralis react templates. they have been customized to fit the needs of the project. during the process, a lot has been learned about chainlink infrastructure or moralis features. the task has been also extensively facilitated by the integration of other tools like nft.storage to facilitate the interaction with ipfs or alchemy to seamlessly integrate the different testing networks.\nwhat's next for youtube video nftknz\nthe experience with polygon mumbai has been really good and after seeing the very attractive minting costs of this network, we are seriously considering the deployment of the -----> tool !!!  in polygon mainnet.\nadditionally, we would like to explore ways to allow users to customize the terms licensed through each nft they mint, configure the number of editions per video, or extend the features to other social networks like facebook or twitter.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 1, "media": null, "medialink": null, "identifyer": 59505740}, {"Unnamed: 0": 5769, "autor": "ComplEat", "date": null, "content": "Inspiration\nThe Covid-19 pandemic has created massive disruptions in the food system. In some cases, we have observed and experienced severe food shortages. In others, food cannot reach end consumers and is ultimately wasted. There are many factors that contributed to waste, including: market factors, products not meeting aesthetic standards for a buyer; or damage from weather and pests. Yet one factor significantly increased waste during the Covid-19: miscommunication between producers and retailers.\nParticularly, one-third of the food produced globally for human consumption, corresponding to about 1.3 billion tonnes, is lost or wasted each year along the food supply chain. But it\u2019s not just the food that is wasted, but also the number of labor days, the gallons of water, and the thousands liters of fuel, just for products that sit in landfills and produce C02 emissions.\nWhat it does\nOur mission is to help prevent food loss and waste along the supply chain. In particular, we ask ourselves: How might we help food traders optimize their storage and supply chains so that food processing companies and retailers find the raw materials they need in a safe and accessible way?\nHow we built it\nWe created a tool that combines AI for shelf life prediction, dashboard for tracking and a digital marketplace to connect traders with potential customers based on their demands. In particular, The imaging software determines what the item is, whether it has any damages and how many more days it has before it rots. This data is then visualized on a our website dashboard for traders to track what is in the warehouse and sell the produce to the right buyers. The freshest produce for example will be sold to supermarkets, but less fresh fruit can still find uses elsewhere, where it can be processed or used immediately.\nChallenges we ran into\nWe ran into multiple challenges. For the designers, we have to figure out the user flows for both the traders and the buyers' perspectives. We were not able to get primary research such as interviews or surveys with our target users, instead, we gained insights from multiple secondary research sources including reports, news articles, research papers, etc. For the developers, we have to learn by ourselves a lot of new frameworks related to AI/ML algorithms in just one day, particularly image recognition and prediction.\nAccomplishments that we're proud of\nAt the end of the journey, we're proud that we have successfully created a running website with all the features we initially came up with: including the dashboard for tracking and a digital market place that automatically matches traders with buyers based on their demands, fueled with data we got from our AI/ML algorithms.\nWhat we learned\nWe learned to communicate more effectively to each other, and reached out to mentors as well as the Dubhacks community when we ran into any difficulties.\nWhat's next for ComplEat\nAt the moment, our AI/ML algorithms only works best for certain types of fruits and vegetables, thus, we're hoping to expand our tools for a diverse range of fresh produce. We're hoping to conduct multiple pilots where we can help traders find new homes for foods that are close to their final use-by date.", "link": "https://devpost.com/software/compleat", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nthe covid-19 pandemic has created massive disruptions in the food system. in some cases, we have observed and experienced severe food shortages. in others, food cannot reach end consumers and is ultimately wasted. there are many factors that contributed to waste, including: market factors, products not meeting aesthetic standards for a buyer; or damage from weather and pests. yet one factor significantly increased waste during the covid-19: miscommunication between producers and retailers.\nparticularly, one-third of the food produced globally for human consumption, corresponding to about 1.3 billion tonnes, is lost or wasted each year along the food supply chain. but it\u2019s not just the food that is wasted, but also the number of labor days, the gallons of water, and the thousands liters of fuel, just for products that sit in landfills and produce c02 emissions.\nwhat it does\nour mission is to help prevent food loss and waste along the supply chain. in particular, we ask ourselves: how might we help food traders optimize their storage and supply chains so that food processing companies and retailers find the raw materials they need in a safe and accessible way?\nhow we built it\nwe created a -----> tool !!!  that combines ai for shelf life prediction, dashboard for tracking and a digital marketplace to connect traders with potential customers based on their demands. in particular, the imaging software determines what the item is, whether it has any damages and how many more days it has before it rots. this data is then visualized on a our website dashboard for traders to track what is in the warehouse and sell the produce to the right buyers. the freshest produce for example will be sold to supermarkets, but less fresh fruit can still find uses elsewhere, where it can be processed or used immediately.\nchallenges we ran into\nwe ran into multiple challenges. for the designers, we have to figure out the user flows for both the traders and the buyers' perspectives. we were not able to get primary research such as interviews or surveys with our target users, instead, we gained insights from multiple secondary research sources including reports, news articles, research papers, etc. for the developers, we have to learn by ourselves a lot of new frameworks related to ai/ml algorithms in just one day, particularly image recognition and prediction.\naccomplishments that we're proud of\nat the end of the journey, we're proud that we have successfully created a running website with all the features we initially came up with: including the dashboard for tracking and a digital market place that automatically matches traders with buyers based on their demands, fueled with data we got from our ai/ml algorithms.\nwhat we learned\nwe learned to communicate more effectively to each other, and reached out to mentors as well as the dubhacks community when we ran into any difficulties.\nwhat's next for compleat\nat the moment, our ai/ml algorithms only works best for certain types of fruits and vegetables, thus, we're hoping to expand our tools for a diverse range of fresh produce. we're hoping to conduct multiple pilots where we can help traders find new homes for foods that are close to their final use-by date.", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59505769}, {"Unnamed: 0": 5773, "autor": "Vivlio", "date": null, "content": "Track\nEducation\nInspiration\nMany of us were eager to pick up new skills during the coronavirus pandemic\u2014 but how many of us can say that we were successful in doing so? As students, we understand the challenges associated with online learning and want to make the practice more accessible and interactive for all. We wanted to expand upon the concept of a free and interactive online education and apply it to all subject matters. We drew inspiration from companies such as DataCamp and Codecademy, who offer free coding classes and data skills in an online setting. Our vision is that together, we can learn anything!\nWhat it does\nOur app creates a user-friendly, interactive textbook application to help students around the world understand and retain information better. Why read through hundreds of pages on programming when you can answer questions and receive feedback through our built-in interactive terminals? Or better yet, see all of your diagrams presented in 3d with options to change the view and data presented? Vivlio keeps track of your progress and learns from the user to curate content and reinforce problem areas. The app is a game changer when it comes to hands-on virtual learning.\nHow we built it\nWe used React to help build the user interface, Node.js to create a scalable server, Figma as a vector graphics editor, and Netlify to host the app.\nChallenges we ran into\nWe initially had trouble integrating the terminal into the site without allowing users to mess with the site data. We took a step back, did some research, and found an external widget to embed on the site. Because of this approach, we were able to embed it seamlessly and continue working on other features of the app.\nAccomplishments that we're proud of\nWe are most proud of integrating a machine learning text summarization API, through RapidAPI. This is a novel tool that has not been used on popular textbook sites, which we believe is an important tool that could help with students\u2019 ability to learn.\nWhat we learned\nOur team learned more about advanced techniques for react.js, node.js, API calls, and general web development.\nWhat's next for vivlio.tech\nWe look forward to continuing work on the app and allowing users to take notes on their interactive books. We see vivlio as a revolutionary tool for students to keep all of their notes and study materials in one place, while working with 3d models and terminals to test and demonstrate their knowledge of core concepts. We would like to present this model and receive funding to fully implement these features and bring virtual learning tools into as many homes and classrooms as possible.", "link": "https://devpost.com/software/vivlio", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "track\neducation\ninspiration\nmany of us were eager to pick up new skills during the coronavirus pandemic\u2014 but how many of us can say that we were successful in doing so? as students, we understand the challenges associated with online learning and want to make the practice more accessible and interactive for all. we wanted to expand upon the concept of a free and interactive online education and apply it to all subject matters. we drew inspiration from companies such as datacamp and codecademy, who offer free coding classes and data skills in an online setting. our vision is that together, we can learn anything!\nwhat it does\nour app creates a user-friendly, interactive textbook application to help students around the world understand and retain information better. why read through hundreds of pages on programming when you can answer questions and receive feedback through our built-in interactive terminals? or better yet, see all of your diagrams presented in 3d with options to change the view and data presented? vivlio keeps track of your progress and learns from the user to curate content and reinforce problem areas. the app is a game changer when it comes to hands-on virtual learning.\nhow we built it\nwe used react to help build the user interface, node.js to create a scalable server, figma as a vector graphics editor, and netlify to host the app.\nchallenges we ran into\nwe initially had trouble integrating the terminal into the site without allowing users to mess with the site data. we took a step back, did some research, and found an external widget to embed on the site. because of this approach, we were able to embed it seamlessly and continue working on other features of the app.\naccomplishments that we're proud of\nwe are most proud of integrating a machine learning text summarization api, through rapidapi. this is a novel -----> tool !!!  that has not been used on popular textbook sites, which we believe is an important -----> tool !!!  that could help with students\u2019 ability to learn.\nwhat we learned\nour team learned more about advanced techniques for react.js, node.js, api calls, and general web development.\nwhat's next for vivlio.tech\nwe look forward to continuing work on the app and allowing users to take notes on their interactive books. we see vivlio as a revolutionary tool for students to keep all of their notes and study materials in one place, while working with 3d models and terminals to test and demonstrate their knowledge of core concepts. we would like to present this model and receive funding to fully implement these features and bring virtual learning tools into as many homes and classrooms as possible.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59505773}, {"Unnamed: 0": 5777, "autor": "StudentCheck", "date": null, "content": "Inspiration\nI come from a family of school teachers in India and experienced first hand the challenges they faced in transition to online learning last year. Most of the school teachers are middle aged and and do not always have the technical skills and cognitive abilities to track the progress of individual students in their class. This problem is further exacerbated in smaller children who themselves struggle with the online learning environment. Further, it is difficult for a teacher to process the past performances of each individual in their class and their previous classes. This inspired us to build a tool which enables teachers to identify the students lagging behind their own potential.\nWhat it does\nStudentCheck provides a simple dashboard which utilizes past academic performance of the students and machine learning models to predict when a student is lagging behind their potential. It simplifies the task of identification, making use of student potential rather than absolute ranking within the class.\nFor every test, we have a set of predicted scores for each individual student based on their past performances. We compare this prediction with their actual performance. We believe that difference in predicted and actual scores reflect unfulfilled potential of the student. Our dashboard makes it plainly clear for the teachers, which students are not fulfilling their potential and where special attention may be best placed.\nEvidence from behavioural science suggests that the timing of interventions can be crucial to their success. Identifying when a student lags behind potential, may be a good time for teachers to intervene (rather than waiting for a child to drop further down the class rankings).\nHow we built it\nWe utilised Javascript and CSS to create a frontend of the Dashboard. We made use of the PostgreSQL database services on Google Cloud Platform and a backend written in Flask for Python. In addition, our machine learning model was written in Python using the sklearn library.\nIn absence of a real dataset, we generated synthetic data to mimic shocks in test scores which can be indicative of underlying challenges to latent learning ability.\nChallenges we ran into\nParticipating in our first hackathons, we faced challenges in structuring the project and decoupling the tasks for each individual. There was also a huge learning curve to make the frontend and backend systems as novices in web development.\nUnderstanding the the GCP structure and the linkages between various services it provides and aspects like authentication took us a while to learn. We had planned on using Auth0 authentication for login of the teachers, but were not able to integrate it with our frontend.\nAccomplishments that we're proud of\nWe believe that we been successful in identifying a challenge which will become very relevant as remote studies become more prevalent. Our approach also highlights the distinction between performing poorly and performing below potential. This, we believe, will help teachers to prevent students from descending into a slippery slope. We also feel proud of our abilities to learn and build a service from scratch in 24 hours!\nWhat we learned\nWe have learned to work quickly, in a team, across locations. This was a challenge, but towards the end of the day (including writing this now), we feel more and more like a well oiled machine. None of us had previous UI/UX experience, and we learned how to make wireframes from scratch. We spent a lot of time thinking about how to make the dashboard as simple as possible for teachers to use.\nWhat's next for studentcheck.tech\nWe hope to further fix up the dashboard to make it usable for multiple teachers. Fixes include making it easier for teachers to add in new student data (we currently add new scores from the google cloud console). We would like to train our models on real data. This can be done by accessing student grade datasets, and building a better picture of grade dynamics. After this, we would like to pilot the dashboard with real teachers, inputting real historical data, and for it to be updated throughout a term. We would then obtain feedback from teachers to better understand the use of the product. Finally, we would like to work with educational research bodies, to understand what sort of interventions are suitable for children falling behind their potential, enabling us to build our the intervention section of the dashboard. In addition, our product will help facilitate causal evaluations of educational interventions, which may even contribution to our knowledge of what interventions work in the classroom.", "link": "https://devpost.com/software/studentcheck-tech", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ni come from a family of school teachers in india and experienced first hand the challenges they faced in transition to online learning last year. most of the school teachers are middle aged and and do not always have the technical skills and cognitive abilities to track the progress of individual students in their class. this problem is further exacerbated in smaller children who themselves struggle with the online learning environment. further, it is difficult for a teacher to process the past performances of each individual in their class and their previous classes. this inspired us to build a -----> tool !!!  which enables teachers to identify the students lagging behind their own potential.\nwhat it does\nstudentcheck provides a simple dashboard which utilizes past academic performance of the students and machine learning models to predict when a student is lagging behind their potential. it simplifies the task of identification, making use of student potential rather than absolute ranking within the class.\nfor every test, we have a set of predicted scores for each individual student based on their past performances. we compare this prediction with their actual performance. we believe that difference in predicted and actual scores reflect unfulfilled potential of the student. our dashboard makes it plainly clear for the teachers, which students are not fulfilling their potential and where special attention may be best placed.\nevidence from behavioural science suggests that the timing of interventions can be crucial to their success. identifying when a student lags behind potential, may be a good time for teachers to intervene (rather than waiting for a child to drop further down the class rankings).\nhow we built it\nwe utilised javascript and css to create a frontend of the dashboard. we made use of the postgresql database services on google cloud platform and a backend written in flask for python. in addition, our machine learning model was written in python using the sklearn library.\nin absence of a real dataset, we generated synthetic data to mimic shocks in test scores which can be indicative of underlying challenges to latent learning ability.\nchallenges we ran into\nparticipating in our first hackathons, we faced challenges in structuring the project and decoupling the tasks for each individual. there was also a huge learning curve to make the frontend and backend systems as novices in web development.\nunderstanding the the gcp structure and the linkages between various services it provides and aspects like authentication took us a while to learn. we had planned on using auth0 authentication for login of the teachers, but were not able to integrate it with our frontend.\naccomplishments that we're proud of\nwe believe that we been successful in identifying a challenge which will become very relevant as remote studies become more prevalent. our approach also highlights the distinction between performing poorly and performing below potential. this, we believe, will help teachers to prevent students from descending into a slippery slope. we also feel proud of our abilities to learn and build a service from scratch in 24 hours!\nwhat we learned\nwe have learned to work quickly, in a team, across locations. this was a challenge, but towards the end of the day (including writing this now), we feel more and more like a well oiled machine. none of us had previous ui/ux experience, and we learned how to make wireframes from scratch. we spent a lot of time thinking about how to make the dashboard as simple as possible for teachers to use.\nwhat's next for studentcheck.tech\nwe hope to further fix up the dashboard to make it usable for multiple teachers. fixes include making it easier for teachers to add in new student data (we currently add new scores from the google cloud console). we would like to train our models on real data. this can be done by accessing student grade datasets, and building a better picture of grade dynamics. after this, we would like to pilot the dashboard with real teachers, inputting real historical data, and for it to be updated throughout a term. we would then obtain feedback from teachers to better understand the use of the product. finally, we would like to work with educational research bodies, to understand what sort of interventions are suitable for children falling behind their potential, enabling us to build our the intervention section of the dashboard. in addition, our product will help facilitate causal evaluations of educational interventions, which may even contribution to our knowledge of what interventions work in the classroom.", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59505777}, {"Unnamed: 0": 5783, "autor": "SpeechyNotes", "date": null, "content": "Track\nEducation\nInspiration\nThere are often times when a student can't attend lecture, especially during times of hybrid learning. Without a complicated video and audio setup, it's difficult for lecturers to provide accommodations for virtual learners. We wanted to create a simple tool that allows lecturers to record their audio as they teach, take the audio, and provide a written transcript -- making it easier for students unable to attend in-person to follow along, at their own pace.\nWhat it does\nSpeechyNotes allows you to paste in a link to your audio file, and it will return a text transcription.\nHow we built it\nWe built a custom API backend on top of HackDuke sponsor AssemblyAI's speech-to-text API. For our backend, we used Flask and Python. We connected this to a React frontend that uses Node.js. For our frontend, we made API calls to our backend using Axios.\nChallenges we ran into\nConnecting the backend to the frontend was the most difficult part. We had to connect two very different technologies -- Python/Flask and React/JS -- together. This isn't too common -- normally, apps built with React use backends via Express, and apps built with Python/Flask backends use Django to build the frontend. But half of our team had experience with Python/Flask, and the other half had experience with React. Knowing our team had different experiences and strengths, we managed to blend both so that everyone was able to utilize their skillset.\nAccomplishments that we're proud of\nOur team is very proud of connecting the two separate backend and frontend pieces. We're also proud of the loading bar we added at the very end.\nWhat we learned\nWe all gained experience with web-development, passing data from backend to frontend, collaborating to build software as a team, and resolving technological and human conflicts.\nWhat's next for SpeechyNotes\nWe hope to improve the speed of transcription and add a feature to directly upload audio files to the site.", "link": "https://devpost.com/software/speechynotes", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "track\neducation\ninspiration\nthere are often times when a student can't attend lecture, especially during times of hybrid learning. without a complicated video and audio setup, it's difficult for lecturers to provide accommodations for virtual learners. we wanted to create a simple -----> tool !!!  that allows lecturers to record their audio as they teach, take the audio, and provide a written transcript -- making it easier for students unable to attend in-person to follow along, at their own pace.\nwhat it does\nspeechynotes allows you to paste in a link to your audio file, and it will return a text transcription.\nhow we built it\nwe built a custom api backend on top of hackduke sponsor assemblyai's speech-to-text api. for our backend, we used flask and python. we connected this to a react frontend that uses node.js. for our frontend, we made api calls to our backend using axios.\nchallenges we ran into\nconnecting the backend to the frontend was the most difficult part. we had to connect two very different technologies -- python/flask and react/js -- together. this isn't too common -- normally, apps built with react use backends via express, and apps built with python/flask backends use django to build the frontend. but half of our team had experience with python/flask, and the other half had experience with react. knowing our team had different experiences and strengths, we managed to blend both so that everyone was able to utilize their skillset.\naccomplishments that we're proud of\nour team is very proud of connecting the two separate backend and frontend pieces. we're also proud of the loading bar we added at the very end.\nwhat we learned\nwe all gained experience with web-development, passing data from backend to frontend, collaborating to build software as a team, and resolving technological and human conflicts.\nwhat's next for speechynotes\nwe hope to improve the speed of transcription and add a feature to directly upload audio files to the site.", "sortedWord": "None", "removed": "Nan", "score": 5, "comments": 0, "media": null, "medialink": null, "identifyer": 59505783}, {"Unnamed: 0": 5791, "autor": "Summarizer", "date": null, "content": "Inspiration\n\"I wish I was more in the know on local events, local culture and local politics in Seattle\"\nThat is the thought that got us started working on Summarizer. After researching and brainstorming on the common struggles of today's working Seattleite, we found the most pressing struggles to be feelings of isolation which often lead to depression. As Informatics majors, we approached tackling this struggle by framing it as an information challenge. We posed questions like:\nHow can we design a solution that fosters a collborative community?\nHow can we emphasize having fun, quality of life, and diversity?\nRealizing that isolation often comes from having little time or energy to engage with others, we wanted to build a solution that bridges the gap between busy people and their local community.\nInformation is what connects all of us together, so we made it our mission to come up with a tool that makes it easy to consume information on local culture, politics and events.\nWhat it does\nSummarize allows busy people to get caught up on the culture of their local community within minutes. How?\nIt's simple. Users read articles, and tap on sentences they found most useful. these sentences get ranked and summarized the top of the article for others to skim. As you read the article, Summarize also guides your eyes to parts of the content that get the most attention.\nBy being the first news tool to leverage community intelligence to crystallize articles into bite-size, consumable pieces of information, Summarize is able to decrease the amount of time it takes to read a news article by a lot!\nSummarize also pulls news data from local sources only, and allows innovative filtering like \"Neighborhood news.\" By making information about local happenings more accesible, our product increases the likelihood that users will find out about things to explore, and go out and engage with their community.\nHow we built it\nWith the goal of creating a multi-platform single page application, we built the front end of the application using the Ionic Framework with React. For the back end we used Express.js and MongoDB. For authentication we used 0Auth, for natural language processing we used winkNLP, and for synchronous sentence voting updates we used Pusher. To host the application, we used Firebase to host the front end, and DigitalOcean Droplets to host the database and back end.\nChallenges we ran into\nJason: MongoDB queries! This was my first practical application of NoSQL, and I struggled to create well-placed queries to project the data into the format I desired. Some of the processing\nAlex: SSL encrypting the backend. After hosting the front end which had SSL out of the box, I realized I also had to encrypt the backend which took 1 hour more than anticipated.\nJason: Getting the mobile application to build to Android. There are a lot of important details omitted in some of the quick-start guides for this, which added a lot of time\nAlex: Authentication: After fidgeting around with different solutions for sometime, I finally stumbled upon auth0 which I should have started with! would have saved a lot of time.\nBoth of us: Keeping the vision of the product in mind as we dove in to development. A lot of things we realized were out of scope so we had to pivot in a way that still kept our core vision in mind. We started out as a gamification tool!\nAccomplishments that we're proud of\nRealtime updates on voting! Watch news articles annotate themselves\nSmooth transitions between news articles, clean user interface\nIOS, Android, and Web friendly. We can publish this to both app stores!\nHosted on the web for everyone!\nWhat we learned\nA little Project Managment goes a lot way!\nTry to overlap as few tasks as possible during sprints (Merge Conflicts!)\nGet yourself a good .gitignore file, and set up environment variables so that deploying the application to production and local testing don't require extra work\nWe didn't sleep all night, and were barely able to finish in time. Our saving grace was a project managment technique of ranking functionality as Dissatisfiers, Satisfiers and delighters. this helped us prioritize and focus on the functionalities that users would be dissatisfied with if they didn't have.\nWhat's next for Summarizer\nAs more and more people use Summarizer we plan to leverage the WinkNLP API along with others to train Summarize to crystallize articles on its own. We also want to preserve more of the original formatting of the articles, such as paragraph breaks and images within the body.", "link": "https://devpost.com/software/summarizer-m1tcbe", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\n\"i wish i was more in the know on local events, local culture and local politics in seattle\"\nthat is the thought that got us started working on summarizer. after researching and brainstorming on the common struggles of today's working seattleite, we found the most pressing struggles to be feelings of isolation which often lead to depression. as informatics majors, we approached tackling this struggle by framing it as an information challenge. we posed questions like:\nhow can we design a solution that fosters a collborative community?\nhow can we emphasize having fun, quality of life, and diversity?\nrealizing that isolation often comes from having little time or energy to engage with others, we wanted to build a solution that bridges the gap between busy people and their local community.\ninformation is what connects all of us together, so we made it our mission to come up with a -----> tool !!!  that makes it easy to consume information on local culture, politics and events.\nwhat it does\nsummarize allows busy people to get caught up on the culture of their local community within minutes. how?\nit's simple. users read articles, and tap on sentences they found most useful. these sentences get ranked and summarized the top of the article for others to skim. as you read the article, summarize also guides your eyes to parts of the content that get the most attention.\nby being the first news tool to leverage community intelligence to crystallize articles into bite-size, consumable pieces of information, summarize is able to decrease the amount of time it takes to read a news article by a lot!\nsummarize also pulls news data from local sources only, and allows innovative filtering like \"neighborhood news.\" by making information about local happenings more accesible, our product increases the likelihood that users will find out about things to explore, and go out and engage with their community.\nhow we built it\nwith the goal of creating a multi-platform single page application, we built the front end of the application using the ionic framework with react. for the back end we used express.js and mongodb. for authentication we used 0auth, for natural language processing we used winknlp, and for synchronous sentence voting updates we used pusher. to host the application, we used firebase to host the front end, and digitalocean droplets to host the database and back end.\nchallenges we ran into\njason: mongodb queries! this was my first practical application of nosql, and i struggled to create well-placed queries to project the data into the format i desired. some of the processing\nalex: ssl encrypting the backend. after hosting the front end which had ssl out of the box, i realized i also had to encrypt the backend which took 1 hour more than anticipated.\njason: getting the mobile application to build to android. there are a lot of important details omitted in some of the quick-start guides for this, which added a lot of time\nalex: authentication: after fidgeting around with different solutions for sometime, i finally stumbled upon auth0 which i should have started with! would have saved a lot of time.\nboth of us: keeping the vision of the product in mind as we dove in to development. a lot of things we realized were out of scope so we had to pivot in a way that still kept our core vision in mind. we started out as a gamification tool!\naccomplishments that we're proud of\nrealtime updates on voting! watch news articles annotate themselves\nsmooth transitions between news articles, clean user interface\nios, android, and web friendly. we can publish this to both app stores!\nhosted on the web for everyone!\nwhat we learned\na little project managment goes a lot way!\ntry to overlap as few tasks as possible during sprints (merge conflicts!)\nget yourself a good .gitignore file, and set up environment variables so that deploying the application to production and local testing don't require extra work\nwe didn't sleep all night, and were barely able to finish in time. our saving grace was a project managment technique of ranking functionality as dissatisfiers, satisfiers and delighters. this helped us prioritize and focus on the functionalities that users would be dissatisfied with if they didn't have.\nwhat's next for summarizer\nas more and more people use summarizer we plan to leverage the winknlp api along with others to train summarize to crystallize articles on its own. we also want to preserve more of the original formatting of the articles, such as paragraph breaks and images within the body.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59505791}, {"Unnamed: 0": 5811, "autor": "PowerPoint Producer", "date": null, "content": "Inspiration\nWe wanted to make it easier for teachers to make dynamic and engaging presentations with visuals and notes that can be saved for later.\nWhat it does\nIt listens to a presenter\u2019s speech, transcribes it, and divides it up into bullet points on a slide. It considers various keywords that allow the presenter to make a title, add images and graphs, or delete previous bullet points easily. It then allows the user to download the created slide deck.\nHow we built it\nWe used React.js to build our website. We chose React.js because we knew the slide would be updating at very high rates, and the state object update re-rendering enabled us to efficiently update our interface accordingly. After capturing audio using a webRTC library, we used the AssemblyAI API to transcribe the speech and splice the audio recording based on gaps in user speech. We localhosted a javascript server to create an authenticated session with AssemblyAI. We parsed the audio transcriptions for key phrases that corresponded to slide titles, bullet points, images, and charts. For images and charts, we scraped the audio transcription and queried the Microsoft Bing API with the figure parameters. Lastly, we added slide screenshot and audio transcription export features to enable sharing of the presentation.\nChallenges we ran into\nA feature we were hoping to implement was speaking a math equation and displaying a step-by-step solution to the problem on the slide instantly, or allowing the presenter to make changes to the problem on the slides as they worked on it. We found that the WolframAlpha API worked well for this. However, when implementing the API request in our client-side website, we received a CORS access control error, which was the result of a WolframAlpha restriction of React client side request calls. A potential future solution to this problem is routing the API call through a CORS proxy server. The display format of the request response would also be different from the slide format than we designed, so we decided to focus on other core features.\nAccomplishments that we're proud of\nWe were successful in our initial goal of making a live presentation producer that was fairly accurate and intuitive to use.\nWhat we learned\nWe learned how to plan and develop an idea in a condensed format and how to organize our tasks and our team effectively to prioritize completion of the most important features. Additionally, each member of the team worked with an API, a language, or another tool that was completely new to them, so this project helped us develop our flexibility and willingness to learn about tools that were completely new to us - an extremely important skill for software developers.\nWhat's next for PowerPoint Producer\nWe hope to be able to create different slide formats based on user input, and make the builder even more intelligent by refining our NLP model. We have many other features planned as well. For example, the Wolfram Alpha API could be implemented for use in mathematics lectures, where functions and equations can be searched and displayed with proper formatting and helpful graphs. A mobile app could be paired with this to give the lecturer more control over the presentation. Videos can also be embedded much like the images currently are.", "link": "https://devpost.com/software/powerpoint-producer", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe wanted to make it easier for teachers to make dynamic and engaging presentations with visuals and notes that can be saved for later.\nwhat it does\nit listens to a presenter\u2019s speech, transcribes it, and divides it up into bullet points on a slide. it considers various keywords that allow the presenter to make a title, add images and graphs, or delete previous bullet points easily. it then allows the user to download the created slide deck.\nhow we built it\nwe used react.js to build our website. we chose react.js because we knew the slide would be updating at very high rates, and the state object update re-rendering enabled us to efficiently update our interface accordingly. after capturing audio using a webrtc library, we used the assemblyai api to transcribe the speech and splice the audio recording based on gaps in user speech. we localhosted a javascript server to create an authenticated session with assemblyai. we parsed the audio transcriptions for key phrases that corresponded to slide titles, bullet points, images, and charts. for images and charts, we scraped the audio transcription and queried the microsoft bing api with the figure parameters. lastly, we added slide screenshot and audio transcription export features to enable sharing of the presentation.\nchallenges we ran into\na feature we were hoping to implement was speaking a math equation and displaying a step-by-step solution to the problem on the slide instantly, or allowing the presenter to make changes to the problem on the slides as they worked on it. we found that the wolframalpha api worked well for this. however, when implementing the api request in our client-side website, we received a cors access control error, which was the result of a wolframalpha restriction of react client side request calls. a potential future solution to this problem is routing the api call through a cors proxy server. the display format of the request response would also be different from the slide format than we designed, so we decided to focus on other core features.\naccomplishments that we're proud of\nwe were successful in our initial goal of making a live presentation producer that was fairly accurate and intuitive to use.\nwhat we learned\nwe learned how to plan and develop an idea in a condensed format and how to organize our tasks and our team effectively to prioritize completion of the most important features. additionally, each member of the team worked with an api, a language, or another -----> tool !!!  that was completely new to them, so this project helped us develop our flexibility and willingness to learn about tools that were completely new to us - an extremely important skill for software developers.\nwhat's next for powerpoint producer\nwe hope to be able to create different slide formats based on user input, and make the builder even more intelligent by refining our nlp model. we have many other features planned as well. for example, the wolfram alpha api could be implemented for use in mathematics lectures, where functions and equations can be searched and displayed with proper formatting and helpful graphs. a mobile app could be paired with this to give the lecturer more control over the presentation. videos can also be embedded much like the images currently are.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59505811}, {"Unnamed: 0": 5819, "autor": "COVinformare - Reliable Information Saves Lives", "date": null, "content": "Inspiration\nAs the recent Facebook leaks reveal, misinformation and divisive content are not only often ignored, but it even seems to be good for business. Inspired by this, our team wanted to create a tool to counter the rampant misinformation, to perhaps sway some of those remaining on the fence to stay on the correct side of history, and perhaps even pull some people out of the conspiracy theory vortex.\nWhat it does\nWhile browsing the web, the Chrome extension analyses the body of any website visited and determines if its contents are COVID-related. If this is found to be the case, a text processing algorithm tries to determine whether the contents are about COVID in a general sense, or if they concern themselves with themes of vaccines, masks, or quarantine and self-isolation.\nWith this information, a call is made to a bespoke in-house API that returns a link to the website of the user's local health authority, determined by the location of the user and the broad theme of the website as described above.\nA system notification is then generated, which, when clicked, opens a new tab to the link retrieved from the API.\nApart from this, when the extension gets opened directly in the extensions bar, the user will be provided with links to the local health authority's sites providing information about vaccination, face covering and the virus in general.\nHow we built it\nWe began by conducting a search about the feasibility of our idea. After concluding that the task is doable, we divided the project into four main areas: frontend, backend, text processing and keyword search & categorisation.\nFrontend\nThis part consists of HTML, CSS, and JavaScript, and dynamically updates link URLs based on user location. The popup.html also provides a central hub for locally sourced information on a selection of topics in a way that is less invasive than the notifications.\nBackend\nThe backend API written in Flask and deployed via Heroku takes topics and country codes as input, and returns a relevant URL. This means that the sources of information can be updated without the user having to update anything, and makes the process of changing and adding new countries and URLs easier and more flexible.\nText processing and categorization\nThe processing of text happens on the Frontend, using JavaScript. The DOM-API was used to select text from the body and remove the HTML tags followed by the removal of punctuation and stopwords to make the search easier and more efficient. At first, we searched for the word coronavirus or its synonyms and if enough occurrence were found, we searched for various keywords regarding the 4 main categories described above. By calculating the sum of appearance of keywords corresponding to each category, the algorithm can determine which category fits the text the most.\nChallenges we ran into\nOne of the most difficult parts of the project was navigating the permissions constraints of Google Chrome extensions, as the browser is quite limiting in what it allows its extensions to do.\nThis improves security of course, but it also makes creation significantly more difficult, especially because a majority of the team has little to no experience with making browser extensions.\nAccomplishments that we're proud of\nOur team is proud of the fact that we were able to make a custom Flask API in 24 hours, for which team members had to quickly learn and understand a technology that was almost entirely new to them. We also feel delighted that we have developed a piece of software that is designed to counter the spread of misinformation (especially regarding such a crucial topic), one of today's most concerning problems in the online world.\nWhat we learned\nAs mentioned in the previous section, members of the team had to become acquainted with Flask in order to create an API.\nFurthermore, a majority of the team had very little or no experience with hackathons, and gained a great deal of experience when it comes to working as a team and applying the fairly theoretical knowledge gained at university, that we usually only get to do individually of each other, and in what could be described as \"clinical conditions\".\nWhat's next for COVinformare - Reliable Information Saves Lives\nThe extension only works for the US, the UK, Canada, Australia, and New Zealand \u2014 users from other countries get directed to the WHO website instead of their local government. Potential next steps could include expanding into more countries, or potentially adding compatibility for more languages, as it is only designed to work on websites in English for now. This can be easily done as it would only require adding extra keywords and websites to the extension.", "link": "https://devpost.com/software/covinformare-reliable-information-saves-lives", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nas the recent facebook leaks reveal, misinformation and divisive content are not only often ignored, but it even seems to be good for business. inspired by this, our team wanted to create a -----> tool !!!  to counter the rampant misinformation, to perhaps sway some of those remaining on the fence to stay on the correct side of history, and perhaps even pull some people out of the conspiracy theory vortex.\nwhat it does\nwhile browsing the web, the chrome extension analyses the body of any website visited and determines if its contents are covid-related. if this is found to be the case, a text processing algorithm tries to determine whether the contents are about covid in a general sense, or if they concern themselves with themes of vaccines, masks, or quarantine and self-isolation.\nwith this information, a call is made to a bespoke in-house api that returns a link to the website of the user's local health authority, determined by the location of the user and the broad theme of the website as described above.\na system notification is then generated, which, when clicked, opens a new tab to the link retrieved from the api.\napart from this, when the extension gets opened directly in the extensions bar, the user will be provided with links to the local health authority's sites providing information about vaccination, face covering and the virus in general.\nhow we built it\nwe began by conducting a search about the feasibility of our idea. after concluding that the task is doable, we divided the project into four main areas: frontend, backend, text processing and keyword search & categorisation.\nfrontend\nthis part consists of html, css, and javascript, and dynamically updates link urls based on user location. the popup.html also provides a central hub for locally sourced information on a selection of topics in a way that is less invasive than the notifications.\nbackend\nthe backend api written in flask and deployed via heroku takes topics and country codes as input, and returns a relevant url. this means that the sources of information can be updated without the user having to update anything, and makes the process of changing and adding new countries and urls easier and more flexible.\ntext processing and categorization\nthe processing of text happens on the frontend, using javascript. the dom-api was used to select text from the body and remove the html tags followed by the removal of punctuation and stopwords to make the search easier and more efficient. at first, we searched for the word coronavirus or its synonyms and if enough occurrence were found, we searched for various keywords regarding the 4 main categories described above. by calculating the sum of appearance of keywords corresponding to each category, the algorithm can determine which category fits the text the most.\nchallenges we ran into\none of the most difficult parts of the project was navigating the permissions constraints of google chrome extensions, as the browser is quite limiting in what it allows its extensions to do.\nthis improves security of course, but it also makes creation significantly more difficult, especially because a majority of the team has little to no experience with making browser extensions.\naccomplishments that we're proud of\nour team is proud of the fact that we were able to make a custom flask api in 24 hours, for which team members had to quickly learn and understand a technology that was almost entirely new to them. we also feel delighted that we have developed a piece of software that is designed to counter the spread of misinformation (especially regarding such a crucial topic), one of today's most concerning problems in the online world.\nwhat we learned\nas mentioned in the previous section, members of the team had to become acquainted with flask in order to create an api.\nfurthermore, a majority of the team had very little or no experience with hackathons, and gained a great deal of experience when it comes to working as a team and applying the fairly theoretical knowledge gained at university, that we usually only get to do individually of each other, and in what could be described as \"clinical conditions\".\nwhat's next for covinformare - reliable information saves lives\nthe extension only works for the us, the uk, canada, australia, and new zealand \u2014 users from other countries get directed to the who website instead of their local government. potential next steps could include expanding into more countries, or potentially adding compatibility for more languages, as it is only designed to work on websites in english for now. this can be easily done as it would only require adding extra keywords and websites to the extension.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59505819}, {"Unnamed: 0": 5825, "autor": "New Tradr", "date": null, "content": "Inspiration\nWe were inspired by the BlackRock challenge to make an app that addresses the financial wellbeing of a particular demographic. We focused on non-investors ages 16 to 25 and tried to resolve the following hesitations around investing:\nLack of knowledge\nRisk aversion\nNo personal interest\nIn short, a lot of young people don't see themselves as the \"investor\" type, either because the barrier to entry seems too daunting or because the perceived investor persona does not feel like themselves. Our app aims to fix this, as investing is a habit best learned early!\nWhat it does\nOur web app consists of a fun, simple survey that asks the user a few questions about their habits and personality, alongside their interests and investment goals. These questions are based on demonstrated research linking certain areas of behavior, such as driving and socialization, to risk appetite. Each user is assigned a number corresponding to their level of risk appetite, which is subsequently used along with their interests to find three companies from the S&P 500 that the user may be interested in buying for a first stock pick. These companies are selected on the basis of quantitative factors affecting risk and price expectations (moving averages, volatility, market cap) as well as qualitative factors around what each company does and how it's related to the user's interests.\nFor example: the user says that they would like to invest $1,000. A subsequent question asks the user if they would bet 10% of that amount on a fair coin flip. Most people are risk averse and would walk away from such an offer, but if the user says that they would take the bet, then their answer will be weighted toward an aggressive risk appetite profile.\nMore at https://www.newtradr.com/#/about\nHow we built it\nWe built the app from scratch in React.js and the financial database in JSON using the Google Finance API. The website is hosted by Netlify and the domain is owned by our team.\nChallenges we ran into\nWe had to scale back the intention of the project during the Hack. We had wanted to incorporate spending data as well via a Plaid integration and show the user more charts and information at the end of the survey, but we ran into time restraints.\nAccomplishments that we're proud of\nWe're proud of building such a user-friendly application from start-to-finish and making it publicly available on the web. This tool really works and can get young people into investing!\nWhat we learned\nWe learned a lot about Google Finance API, as well as quantitative risk signals such as stochastic oscillators and volatility indexes. In researching the target user demographic, we also learned a lot about how risk is correlated with different behavioral traits.\nWhat's next for New Tradr\nWe hope to keep building on this idea to arrive at a more sophisticated tool that can take in even more information about a user to give even more personalized investment recommendations. We believe that such a tool could serve as a compliment to existing trading platforms like Robinhood or Groww, and hope to get in touch with individuals from the FinTech industry about how to scale New Tradr!", "link": "https://devpost.com/software/new-tradr", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe were inspired by the blackrock challenge to make an app that addresses the financial wellbeing of a particular demographic. we focused on non-investors ages 16 to 25 and tried to resolve the following hesitations around investing:\nlack of knowledge\nrisk aversion\nno personal interest\nin short, a lot of young people don't see themselves as the \"investor\" type, either because the barrier to entry seems too daunting or because the perceived investor persona does not feel like themselves. our app aims to fix this, as investing is a habit best learned early!\nwhat it does\nour web app consists of a fun, simple survey that asks the user a few questions about their habits and personality, alongside their interests and investment goals. these questions are based on demonstrated research linking certain areas of behavior, such as driving and socialization, to risk appetite. each user is assigned a number corresponding to their level of risk appetite, which is subsequently used along with their interests to find three companies from the s&p 500 that the user may be interested in buying for a first stock pick. these companies are selected on the basis of quantitative factors affecting risk and price expectations (moving averages, volatility, market cap) as well as qualitative factors around what each company does and how it's related to the user's interests.\nfor example: the user says that they would like to invest $1,000. a subsequent question asks the user if they would bet 10% of that amount on a fair coin flip. most people are risk averse and would walk away from such an offer, but if the user says that they would take the bet, then their answer will be weighted toward an aggressive risk appetite profile.\nmore at https://www.newtradr.com/#/about\nhow we built it\nwe built the app from scratch in react.js and the financial database in json using the google finance api. the website is hosted by netlify and the domain is owned by our team.\nchallenges we ran into\nwe had to scale back the intention of the project during the hack. we had wanted to incorporate spending data as well via a plaid integration and show the user more charts and information at the end of the survey, but we ran into time restraints.\naccomplishments that we're proud of\nwe're proud of building such a user-friendly application from start-to-finish and making it publicly available on the web. this -----> tool !!!  really works and can get young people into investing!\nwhat we learned\nwe learned a lot about google finance api, as well as quantitative risk signals such as stochastic oscillators and volatility indexes. in researching the target user demographic, we also learned a lot about how risk is correlated with different behavioral traits.\nwhat's next for new tradr\nwe hope to keep building on this idea to arrive at a more sophisticated tool that can take in even more information about a user to give even more personalized investment recommendations. we believe that such a tool could serve as a compliment to existing trading platforms like robinhood or groww, and hope to get in touch with individuals from the fintech industry about how to scale new tradr!", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59505825}, {"Unnamed: 0": 5828, "autor": "Empathie (iOS)", "date": null, "content": "Inspiration\nI, Jennifer Wong, went through many mental health hurdles and struggled to get the specific help that I needed. I was fortunate to find a relatable therapist that gave me an educational understanding of my mental health, which helped me understand my past and accept it. I was able to afford this access to mental health care and connect with a therapist similar to me, but that's not the case for many racial minorities. I saw the power of mental health education and wanted to spread it to others.\nWhat it does\nTakes a personalized assessment of your background and mental health in order to provide a curated and self-guided educational program based on your cultural experiences.\nYou can journal about your reflections as you learn through watching each video. Videos are curated as an MVP, but eventually, we want to hire therapists to create these educational videos.\nHow we built it\nChallenges we ran into\nWe had our engineers drop the project or couldn't attend the working sessions, so there were issues with the workload. Also, there were issues with technical feasibility since knowledge on Swift was limited.\nAccomplishments that we're proud of\nProud that overall, we were able to create a fully functioning app that still achieves our mission. We were happy to get the journal tool completed, which was the most complicated.\nWhat we learned\nWe learned how to cut scope when we lost engineers on the team.\nWhat's next for Empathie (iOS)\nWe will get more customer validation about the problem and see if our idea resonates with people. We are currently getting feedback from therapists who work with people of color.\nIn the future, we would love to partner with schools to provide these types of self-guided services since there's a shortage of therapists, especially for underserved school districts.", "link": "https://devpost.com/software/empathie-ios", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ni, jennifer wong, went through many mental health hurdles and struggled to get the specific help that i needed. i was fortunate to find a relatable therapist that gave me an educational understanding of my mental health, which helped me understand my past and accept it. i was able to afford this access to mental health care and connect with a therapist similar to me, but that's not the case for many racial minorities. i saw the power of mental health education and wanted to spread it to others.\nwhat it does\ntakes a personalized assessment of your background and mental health in order to provide a curated and self-guided educational program based on your cultural experiences.\nyou can journal about your reflections as you learn through watching each video. videos are curated as an mvp, but eventually, we want to hire therapists to create these educational videos.\nhow we built it\nchallenges we ran into\nwe had our engineers drop the project or couldn't attend the working sessions, so there were issues with the workload. also, there were issues with technical feasibility since knowledge on swift was limited.\naccomplishments that we're proud of\nproud that overall, we were able to create a fully functioning app that still achieves our mission. we were happy to get the journal -----> tool !!!  completed, which was the most complicated.\nwhat we learned\nwe learned how to cut scope when we lost engineers on the team.\nwhat's next for empathie (ios)\nwe will get more customer validation about the problem and see if our idea resonates with people. we are currently getting feedback from therapists who work with people of color.\nin the future, we would love to partner with schools to provide these types of self-guided services since there's a shortage of therapists, especially for underserved school districts.", "sortedWord": "None", "removed": "Nan", "score": 6, "comments": 0, "media": null, "medialink": null, "identifyer": 59505828}, {"Unnamed: 0": 5838, "autor": "Daily Stacker", "date": null, "content": "Waking up to a little bit more crypto in your wallet is a great feeling. Daily Stacker wakes up before you do and buys you a little slice of your favourite tokens and sends them to your wallet.\nInspiration\nI've always enjoyed teaching crypto to newbie nocoiners. Getting them setup with centralised exchange account and getting them to buy their first sliver of Bitcoin.\nAlongside that I've always explained that Dollar-cost-averaging (DCA) combined with HODLing has been proven time and time again to be the best and simplest investment strategy to adopt. Don't try and time the market, don't try and catch the dips. Just buy at set intervals - daily, weekly, or monthly.\nDaily Stacker takes DCA to a new level. Waking up everyday owning a little bit more crypto is a nice feeling and Daily Stacker helps achieve that.\nDaily Stacker first came to life as a Raspberry Pi project called SatStacker. SatStacker still runs today and buys my crypto. However I collected a lot of feedback from people asking if they could have a solution that they could run themselves without hardware.\nPart of the problem to solve had to include to be the management of private keys or API keys and I don't want to be responsible for other people's keys. The combination of EVM smart contracts, L2 chains, Chainlink, and gas cost-optimised DEXes such as SushiSwap + Bentobox offered a solution.\nWhat it does\nHere's how Daily Stacker works:\nconnect your wallet and once a month transfer some DAI to Daily Stacker, say 300 DAI.\nsetup a portfolio, e.g. 50% Wrapped Bitcoin, 35% LINK, 15% BAT\nfor the next 30 days Daily Stacker will buy slivers of the tokens according to the ratios in your portfolio\nall purchases are automatically sent to your wallet\nHow I built it\nScaffold-Eth has been a brilliant tool that has allowed the design to emerge, by first allowing me to play around and model some basic contracts. Automate the development through testing and finally to build a DAPP UI on top that included all the necessary wallet hooks.\nBill of materials:\nScaffold-Eth (React, Hardhat, Ethers)\nChainlink Keepers\nSushiSwap\nBentoBox (still to be done)\nPolygon (still to be done)\nRamp Network for fiat onramp (still to be done)\nChallenges I ran into\nLearning Solidity (after many previous false starts)\nDealing with the peculiarities of ERC-20 tokes (approval workflows)\nAccomplishments that I'm proud of\nLearning Solidity and getting a least a fair bit along the way to project completion. At the ripe old age of almost 50, joined my first hackathon. Old dog, new tricks.\nWhat I've learned\nEVM platforms are massive environment to innovate on.\nSolidity itself is reasonably easy to learn, however running smart contracts on the blockchain is hard. The ERC-20 Approval pattern is often a stumbling block.\nChainlink Keepers are awesome.\nTeam up with people. You can't do it all.\nWhat's next for Daily Stacker\nDeploy to Polygon and other L2s for cheaper gas costs but that depends on the availability of Chainlink Keepers support on those chains. Update: Keepers are now available on Polygon.\nI'd like integrate something like ramp.network to provide a fiat onramp straight into Daily Stacker\nImprove the UI.\nDevelop a mechanism whereby the protocol buys the LINK token to support itself, paid for by the users of the protocol.\nIntegrate yield-generating protocols that will earn a yield whilst the funds are staked but not spent.", "link": "https://devpost.com/software/daily-stacker", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "waking up to a little bit more crypto in your wallet is a great feeling. daily stacker wakes up before you do and buys you a little slice of your favourite tokens and sends them to your wallet.\ninspiration\ni've always enjoyed teaching crypto to newbie nocoiners. getting them setup with centralised exchange account and getting them to buy their first sliver of bitcoin.\nalongside that i've always explained that dollar-cost-averaging (dca) combined with hodling has been proven time and time again to be the best and simplest investment strategy to adopt. don't try and time the market, don't try and catch the dips. just buy at set intervals - daily, weekly, or monthly.\ndaily stacker takes dca to a new level. waking up everyday owning a little bit more crypto is a nice feeling and daily stacker helps achieve that.\ndaily stacker first came to life as a raspberry pi project called satstacker. satstacker still runs today and buys my crypto. however i collected a lot of feedback from people asking if they could have a solution that they could run themselves without hardware.\npart of the problem to solve had to include to be the management of private keys or api keys and i don't want to be responsible for other people's keys. the combination of evm smart contracts, l2 chains, chainlink, and gas cost-optimised dexes such as sushiswap + bentobox offered a solution.\nwhat it does\nhere's how daily stacker works:\nconnect your wallet and once a month transfer some dai to daily stacker, say 300 dai.\nsetup a portfolio, e.g. 50% wrapped bitcoin, 35% link, 15% bat\nfor the next 30 days daily stacker will buy slivers of the tokens according to the ratios in your portfolio\nall purchases are automatically sent to your wallet\nhow i built it\nscaffold-eth has been a brilliant -----> tool !!!  that has allowed the design to emerge, by first allowing me to play around and model some basic contracts. automate the development through testing and finally to build a dapp ui on top that included all the necessary wallet hooks.\nbill of materials:\nscaffold-eth (react, hardhat, ethers)\nchainlink keepers\nsushiswap\nbentobox (still to be done)\npolygon (still to be done)\nramp network for fiat onramp (still to be done)\nchallenges i ran into\nlearning solidity (after many previous false starts)\ndealing with the peculiarities of erc-20 tokes (approval workflows)\naccomplishments that i'm proud of\nlearning solidity and getting a least a fair bit along the way to project completion. at the ripe old age of almost 50, joined my first hackathon. old dog, new tricks.\nwhat i've learned\nevm platforms are massive environment to innovate on.\nsolidity itself is reasonably easy to learn, however running smart contracts on the blockchain is hard. the erc-20 approval pattern is often a stumbling block.\nchainlink keepers are awesome.\nteam up with people. you can't do it all.\nwhat's next for daily stacker\ndeploy to polygon and other l2s for cheaper gas costs but that depends on the availability of chainlink keepers support on those chains. update: keepers are now available on polygon.\ni'd like integrate something like ramp.network to provide a fiat onramp straight into daily stacker\nimprove the ui.\ndevelop a mechanism whereby the protocol buys the link token to support itself, paid for by the users of the protocol.\nintegrate yield-generating protocols that will earn a yield whilst the funds are staked but not spent.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59505838}, {"Unnamed: 0": 5844, "autor": "Mentorship App", "date": null, "content": "Inspiration\nMy inspiration for this project was the experience I have had hunting for a mentor as I moved from India to the United States. Although I wasn't the first, I craved he idea of having to talk to someone who had gone through this and someone who could give me advice. The hunt for a mentor led into all of high school where I would make older friends to get advice, but was never able to find a mentor that could guide me in the right direction academically or not. Eventually, I realized that there was a gap between finding mentors and being connected with them. With that in mind, I made this platform to bridge the gap between finding and staying connected with mentors allowing individuals to learn new perspectives and open their mind.\nWhat it does\nThis app serves as a platform for mentees (advice seekers) and mentors (advice givers) to connect the two types together and help form a community around this. Essentially, this platform has a few key features highlighted below:\nThe ability for mentees to find mentors from underrepresented communities (such as Females in STEM or Latinx in Tech) to find someone they can resonate with.\nChatting with mentors for free! Mentees and mentors have the ability to chat as much as they want regardless of whether they book an appointment.\nEvery mentor enters their schedule, allowing mentees to simply select the time that mentors are available, streamlining the process of finding available times and booking appointments.\nA mentor has full control over who they mentor. They can choose to first chat with the mentee that has requested an appointment, or they could accept/reject the request sent. Follow-up messages are always allowed!\nIf the mentor charges at an hourly rate, the funds are held in escrow until the meeting is accepted by the mentor. If the meeting is rejected or not responded to in time, the mentee is issued a full refund.\nEach mentor qualifies for badges that they can receive for aiding in a specific community or celebrating their individuality. Some badges including being a BiPoC Mentor, a Pride Mentor, a Female Mentor, and even a Volunteer Mentor.\nEasy access to the calendar allows all users to quickly take a glance at their schedule for one day, week, or even a month.\nIntuitive UI/UX design makes it easily accessible to read or call and hear for any disabled individuals. The contrasting colors and support for dark mode provides necessary support.\nHow we built it\nI built this app with React Native in the frontend and Google Cloud in the backend as I leveraged the services it provides including authentication, database (NoSQL - Firestore), and storage (for images). I plan to continue developing this app further and integrate in-app calling through Twilio by leveraging the Programmable video/voice SDK that will allow for a powerful, scalable calling system.\nChallenges we ran into\nOne huge challenge that I ran into was the lack of time which forced me to prioritize differently compared to a side project. I had to plan first, and design the scope of the platform in Figma which would allow me to get my idea out. After that, I finalized the design and started developing the frontend but did not go for pixel-perfect development considering the lack of time since I decided to have functional aspects of the app in there for users to check out such as chat, booking appointments, etc.\nAccomplishments that we're proud of\nOne thing that I'm proud of is the idea of stepping out of my comfort zone. Throughout all of my previous hackathons, I have minimal experience with Google Cloud and none with React Native, but I knew that this was the perfect opportunity to get involved with new technologies and experiment with a new side to software development. I also attempted to design a mobile app for the first time on Figma which was super exciting and user-intuitive. I also decided to compete alone this time, for a change, to see how I would handle the workload myself and I'm proud of the way I was able to organize and prioritize to set myself up for the best chance of success.\nWhat we learned\nAside from the nerdy technological topics that I learned about, I realized the true potential technology holds. Not only was I able to find a solution to a problem I have had for years, but in fact, I was able to build a platform that could potentially impact millions of people who feel isolated or abandoned and are seeking help. Especially considering the importance of having someone to look up to from an underrepresented community, highlighting mentors from these communities not only serves as motivation but also allows mentees to seek crucial advice that could help them throughout their life.\nWhat's next for Mentorship App\nThe next step for Mentorship App will be tidying up the frontend design and working on finishing the full functionality of the platform to release an MVP. I believe that encouraging two sides to this platform will play a huge role in how successful it will be: on one side, this could serve as a tool for entrepreneurs or students to learn more; on the other side, we could see vast amounts of underrepresented communities find someone successful that they can interact with (hopefully at no charge!) to propel them in their dreams and careers. After perfecting the MVP, I hope to integrate a new side into this platform for allowing organizations to form where companies, schools, and other organizations can leverage this platform for themselves where they could have mentor-mentee programs within the organization. One example of this would be at the UW, where mentees and mentors would be able to connect via this platform by joining the UW organization inside the app.", "link": "https://devpost.com/software/mentorship-app", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nmy inspiration for this project was the experience i have had hunting for a mentor as i moved from india to the united states. although i wasn't the first, i craved he idea of having to talk to someone who had gone through this and someone who could give me advice. the hunt for a mentor led into all of high school where i would make older friends to get advice, but was never able to find a mentor that could guide me in the right direction academically or not. eventually, i realized that there was a gap between finding mentors and being connected with them. with that in mind, i made this platform to bridge the gap between finding and staying connected with mentors allowing individuals to learn new perspectives and open their mind.\nwhat it does\nthis app serves as a platform for mentees (advice seekers) and mentors (advice givers) to connect the two types together and help form a community around this. essentially, this platform has a few key features highlighted below:\nthe ability for mentees to find mentors from underrepresented communities (such as females in stem or latinx in tech) to find someone they can resonate with.\nchatting with mentors for free! mentees and mentors have the ability to chat as much as they want regardless of whether they book an appointment.\nevery mentor enters their schedule, allowing mentees to simply select the time that mentors are available, streamlining the process of finding available times and booking appointments.\na mentor has full control over who they mentor. they can choose to first chat with the mentee that has requested an appointment, or they could accept/reject the request sent. follow-up messages are always allowed!\nif the mentor charges at an hourly rate, the funds are held in escrow until the meeting is accepted by the mentor. if the meeting is rejected or not responded to in time, the mentee is issued a full refund.\neach mentor qualifies for badges that they can receive for aiding in a specific community or celebrating their individuality. some badges including being a bipoc mentor, a pride mentor, a female mentor, and even a volunteer mentor.\neasy access to the calendar allows all users to quickly take a glance at their schedule for one day, week, or even a month.\nintuitive ui/ux design makes it easily accessible to read or call and hear for any disabled individuals. the contrasting colors and support for dark mode provides necessary support.\nhow we built it\ni built this app with react native in the frontend and google cloud in the backend as i leveraged the services it provides including authentication, database (nosql - firestore), and storage (for images). i plan to continue developing this app further and integrate in-app calling through twilio by leveraging the programmable video/voice sdk that will allow for a powerful, scalable calling system.\nchallenges we ran into\none huge challenge that i ran into was the lack of time which forced me to prioritize differently compared to a side project. i had to plan first, and design the scope of the platform in figma which would allow me to get my idea out. after that, i finalized the design and started developing the frontend but did not go for pixel-perfect development considering the lack of time since i decided to have functional aspects of the app in there for users to check out such as chat, booking appointments, etc.\naccomplishments that we're proud of\none thing that i'm proud of is the idea of stepping out of my comfort zone. throughout all of my previous hackathons, i have minimal experience with google cloud and none with react native, but i knew that this was the perfect opportunity to get involved with new technologies and experiment with a new side to software development. i also attempted to design a mobile app for the first time on figma which was super exciting and user-intuitive. i also decided to compete alone this time, for a change, to see how i would handle the workload myself and i'm proud of the way i was able to organize and prioritize to set myself up for the best chance of success.\nwhat we learned\naside from the nerdy technological topics that i learned about, i realized the true potential technology holds. not only was i able to find a solution to a problem i have had for years, but in fact, i was able to build a platform that could potentially impact millions of people who feel isolated or abandoned and are seeking help. especially considering the importance of having someone to look up to from an underrepresented community, highlighting mentors from these communities not only serves as motivation but also allows mentees to seek crucial advice that could help them throughout their life.\nwhat's next for mentorship app\nthe next step for mentorship app will be tidying up the frontend design and working on finishing the full functionality of the platform to release an mvp. i believe that encouraging two sides to this platform will play a huge role in how successful it will be: on one side, this could serve as a -----> tool !!!  for entrepreneurs or students to learn more; on the other side, we could see vast amounts of underrepresented communities find someone successful that they can interact with (hopefully at no charge!) to propel them in their dreams and careers. after perfecting the mvp, i hope to integrate a new side into this platform for allowing organizations to form where companies, schools, and other organizations can leverage this platform for themselves where they could have mentor-mentee programs within the organization. one example of this would be at the uw, where mentees and mentors would be able to connect via this platform by joining the uw organization inside the app.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59505844}, {"Unnamed: 0": 5850, "autor": "Mindful", "date": null, "content": "Inspiration\nThe growing financial debt crisis is worrisome. A direct link to this is the lack of knowledge on how our daily activities affect our finances. Although needed, we don't see many schools that teach their students how they may develop healthy financial habits that will help them in the long run.\nThis is why we wanted to work on creating a project that would help students become financially independent and knowledgeable.\nWhat it does\nMindful can be thought of as a financial advisor and helper for people from all walks of life, especially college students.\nWhat Mindful does for you:\nIt helps you create financial goals.\nIt helps you keep track of the progress that you are making.\nWe have an amazing, easily accessible collection of financial concepts broken down into a simple, easy-to-understand format.\nEncourages you to implement what you have learned by providing a set of challenges to help you apply the concepts and ideas in the real world.\nLet's you scan your bills of purchases, analyze, and break them down into specific suggestions on how you can further reduce your costs.\nHelps you stick to your goals with the help of accountability from your friends and family.\nMindful also has a clean, responsive UI so you're able to use it on different devices. While learning more about how to improve and manage your finances, you'll also enjoy browsing the site!\nHow we built it\nWe chose Django with Django Rest Framework for the backend to quickly create an API. The frontend is built in React and Material UI with Axios for calling the backend.\nThe application is deployed on Heroku.\nChallenges we ran into\nRemote work\nJim is based in Florida, Zhuoen is based in Singapore, and Sreemukhi and Shreyansh are based in India. We worked across timezones halfway across the world to build this project!\nNetwork latency was also an issue. The audio quality for voice calls was poor and we couldn't share our screens during calls because it was so slow and blurry (and it cut off the audio).\nWe ended up working asynchronously when we weren't online at the same time creating documents on Notion, and communicating ahead of time when we would be online and when we wouldn't.\nVarious technical challenges\nCustom permissions management in Django Rest Framework, serializer vs model discrepancy, Axios errors, etc.\nDeployment\nWe used a monorepo with a Django project as the root directory and a \"frontend\" subdirectory within it. However, Heroku doesn't know how to build the frontend, and its buildpacks look for \"package.json\" in the root directory rather than the frontend directory. We ended up copying \"package.json\" to the root directory which would \"cd frontend\" before running the build process and using two buildpacks in Heroku: python and nodejs.\nHowever, the node and npm versions in Heroku are different by default than the ones we used, breaking the build process. So we had to specify which versions of node and npm in \"package.json\" to use. Then, Heroku would install these versions upon every deploy.\nThis led to a long build process for every deploy. Every little hiccup would waste ten minutes of our time. Django requires STATIC_URLS and ALLOWED_HOSTS to be set in production? Set the variables, wait ten minutes. Django doesn't serve static files in production? Install white noise, wait ten minutes. CORS was a challenge even though we explicitly set CORS_ALLOWED_ORIGINS to allow the origin.\nOverall, it was a learning process to deploy Django in production. Running it in development was easy, but we had to take the necessary steps for secure production deployment.\nAccomplishments that we're proud of\nFirst hackathon submission for Jim and second for Sreemukhi. We built and deployed a product!\nSecure API: manages permissions for users based on whether they're owners, whether the object type is public, etc.\nComprehensive API: all application logic can be mapped to database operations effectively with the API\nWhat's next for Mindful\nWe want to work more on the toolkit for Mindful, as this contains elements like\nScan tool\nAlong with this, we want to introduce community curated challenges and workshops/talks from real people who have mastered their financials with simple habits.\nThis will help users gain worldly yet specific knowledge to tackle problems to come.", "link": "https://devpost.com/software/mindful-1s39kb", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nthe growing financial debt crisis is worrisome. a direct link to this is the lack of knowledge on how our daily activities affect our finances. although needed, we don't see many schools that teach their students how they may develop healthy financial habits that will help them in the long run.\nthis is why we wanted to work on creating a project that would help students become financially independent and knowledgeable.\nwhat it does\nmindful can be thought of as a financial advisor and helper for people from all walks of life, especially college students.\nwhat mindful does for you:\nit helps you create financial goals.\nit helps you keep track of the progress that you are making.\nwe have an amazing, easily accessible collection of financial concepts broken down into a simple, easy-to-understand format.\nencourages you to implement what you have learned by providing a set of challenges to help you apply the concepts and ideas in the real world.\nlet's you scan your bills of purchases, analyze, and break them down into specific suggestions on how you can further reduce your costs.\nhelps you stick to your goals with the help of accountability from your friends and family.\nmindful also has a clean, responsive ui so you're able to use it on different devices. while learning more about how to improve and manage your finances, you'll also enjoy browsing the site!\nhow we built it\nwe chose django with django rest framework for the backend to quickly create an api. the frontend is built in react and material ui with axios for calling the backend.\nthe application is deployed on heroku.\nchallenges we ran into\nremote work\njim is based in florida, zhuoen is based in singapore, and sreemukhi and shreyansh are based in india. we worked across timezones halfway across the world to build this project!\nnetwork latency was also an issue. the audio quality for voice calls was poor and we couldn't share our screens during calls because it was so slow and blurry (and it cut off the audio).\nwe ended up working asynchronously when we weren't online at the same time creating documents on notion, and communicating ahead of time when we would be online and when we wouldn't.\nvarious technical challenges\ncustom permissions management in django rest framework, serializer vs model discrepancy, axios errors, etc.\ndeployment\nwe used a monorepo with a django project as the root directory and a \"frontend\" subdirectory within it. however, heroku doesn't know how to build the frontend, and its buildpacks look for \"package.json\" in the root directory rather than the frontend directory. we ended up copying \"package.json\" to the root directory which would \"cd frontend\" before running the build process and using two buildpacks in heroku: python and nodejs.\nhowever, the node and npm versions in heroku are different by default than the ones we used, breaking the build process. so we had to specify which versions of node and npm in \"package.json\" to use. then, heroku would install these versions upon every deploy.\nthis led to a long build process for every deploy. every little hiccup would waste ten minutes of our time. django requires static_urls and allowed_hosts to be set in production? set the variables, wait ten minutes. django doesn't serve static files in production? install white noise, wait ten minutes. cors was a challenge even though we explicitly set cors_allowed_origins to allow the origin.\noverall, it was a learning process to deploy django in production. running it in development was easy, but we had to take the necessary steps for secure production deployment.\naccomplishments that we're proud of\nfirst hackathon submission for jim and second for sreemukhi. we built and deployed a product!\nsecure api: manages permissions for users based on whether they're owners, whether the object type is public, etc.\ncomprehensive api: all application logic can be mapped to database operations effectively with the api\nwhat's next for mindful\nwe want to work more on the toolkit for mindful, as this contains elements like\nscan -----> tool !!! \nalong with this, we want to introduce community curated challenges and workshops/talks from real people who have mastered their financials with simple habits.\nthis will help users gain worldly yet specific knowledge to tackle problems to come.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59505850}, {"Unnamed: 0": 5859, "autor": "MedHelp | hackduke21", "date": null, "content": "Inspiration\nWe were inspired by researching the ways in which the health insurance industry's lack of transparency in billing hurts families and individuals across the country. The lack of transparency results in different patients paying wide ranges of co-payments for the same procedure. This is detrimental for the publics faith in the healthcare and health insurance industries, and is unfair to patients who are needless charged more than other people receiving similar procedures in their area. As a result of these faults, we were inspired to create a tool which can aid patients who may be getting overcharged in order to improve the efficiency and affordability of the healthcare system in the U.S.\nWhat it does\nUsing the users state, zip code, and procedure, our two layer neural network predicts what a likely estimate for out of pocket costs would be given previous charges for similar procedures in the same area. This data is taken from our Postgres SQL database using Google Cloud's SQL Engine. It then displays what a typical procedure in the area should cost given past quote. This allows a patient to take action against a hospital or their insurance provider if their coverage is not consistent with what people should typically be able to expect.\nHow we built it\nIn order to design the front end of our online tool, we used Create React App and CSS to design the landing page for our website and the subsequent pages you can access from the landing page. We utilized components from Ant Design to assist in the designing our our website. Then, we created a neural network in PyTorch using a single hidden layer that can predict estimated copayments. We connected this neural network to a Postgres SQL database using Google Cloud's SQL Engine, and created a SQL query that could fetch relevant data from the database given the parameters that user specified.\nChallenges we ran into\nThe biggest challenges our team faced were in dealing with DevOps such as connecting to the Google Cloud database. A general lack of experience on our team in working with databases and Google Cloud made it quite difficult for us to create the connection between our app and the database. In a similar vein, we also faced challenges in fetching data from the database. Prior to this hackathon, our group had no experience working with SQL, which made getting data from our Google Cloud database very challenging.\nAccomplishments that we're proud of\nOur team is very proud of the way that our web tool turned out, in terms of aesthetics and functionality. No one on our team had a lot of experience with web design or react prior to working on this project, and as a result, the creation of the webapp was a challenging but rewarding experience for us. Considering our lack of experience, we are exceptionally happy with the final product. We are also proud of the work we have done with databases for this project. This is another area where our team did not have much expertise, so we are proud of the way we were able to learn about databases and implement them over the course of the hackathon.\nWhat we learned\nThe areas where our team learned the most over the course of the project were in web design and working with React/CSS and how to connect to and work with remote databases. Over the course of the hackathon, multiple team members learned how to write SQL queries and update databases using languages that we previously had no experience in. Some of our team members also learned about the structure and implementation of neural networks.\nWhat's next for hackduke21\nOutside of hackduke, we want to continue to work on our project to make it more accessible for a wider range of medical procedures/expenses. Due to database and time restrictions, we had to limit the functionality of our tool to only provide estimates for a certain range of medical expenses. With more time we will be able to expand this to be able to provide estimates for types of procedures than we currently have.", "link": "https://devpost.com/software/hackduke21", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe were inspired by researching the ways in which the health insurance industry's lack of transparency in billing hurts families and individuals across the country. the lack of transparency results in different patients paying wide ranges of co-payments for the same procedure. this is detrimental for the publics faith in the healthcare and health insurance industries, and is unfair to patients who are needless charged more than other people receiving similar procedures in their area. as a result of these faults, we were inspired to create a -----> tool !!!  which can aid patients who may be getting overcharged in order to improve the efficiency and affordability of the healthcare system in the u.s.\nwhat it does\nusing the users state, zip code, and procedure, our two layer neural network predicts what a likely estimate for out of pocket costs would be given previous charges for similar procedures in the same area. this data is taken from our postgres sql database using google cloud's sql engine. it then displays what a typical procedure in the area should cost given past quote. this allows a patient to take action against a hospital or their insurance provider if their coverage is not consistent with what people should typically be able to expect.\nhow we built it\nin order to design the front end of our online tool, we used create react app and css to design the landing page for our website and the subsequent pages you can access from the landing page. we utilized components from ant design to assist in the designing our our website. then, we created a neural network in pytorch using a single hidden layer that can predict estimated copayments. we connected this neural network to a postgres sql database using google cloud's sql engine, and created a sql query that could fetch relevant data from the database given the parameters that user specified.\nchallenges we ran into\nthe biggest challenges our team faced were in dealing with devops such as connecting to the google cloud database. a general lack of experience on our team in working with databases and google cloud made it quite difficult for us to create the connection between our app and the database. in a similar vein, we also faced challenges in fetching data from the database. prior to this hackathon, our group had no experience working with sql, which made getting data from our google cloud database very challenging.\naccomplishments that we're proud of\nour team is very proud of the way that our web tool turned out, in terms of aesthetics and functionality. no one on our team had a lot of experience with web design or react prior to working on this project, and as a result, the creation of the webapp was a challenging but rewarding experience for us. considering our lack of experience, we are exceptionally happy with the final product. we are also proud of the work we have done with databases for this project. this is another area where our team did not have much expertise, so we are proud of the way we were able to learn about databases and implement them over the course of the hackathon.\nwhat we learned\nthe areas where our team learned the most over the course of the project were in web design and working with react/css and how to connect to and work with remote databases. over the course of the hackathon, multiple team members learned how to write sql queries and update databases using languages that we previously had no experience in. some of our team members also learned about the structure and implementation of neural networks.\nwhat's next for hackduke21\noutside of hackduke, we want to continue to work on our project to make it more accessible for a wider range of medical procedures/expenses. due to database and time restrictions, we had to limit the functionality of our tool to only provide estimates for a certain range of medical expenses. with more time we will be able to expand this to be able to provide estimates for types of procedures than we currently have.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59505859}, {"Unnamed: 0": 5891, "autor": "Smart Store", "date": null, "content": "Inspiration\nWe wanted to provide retailers with the opportunity to better serve their customers in a personalized way.\nWhat it does\nIt provides a smart informational dashboard and suite of tools.\nHow we built it\nWe built each tool using Python and OpenCV.\nChallenges we ran into\nWe ran into challenges while working with hardware and integrating a ROS dashboard.\nAccomplishments that we're proud of\nWe were able to get several of our projects working efficiently, and we learned a lot of new technologies.\nWhat we learned\nComputer vision, how to use ROS, vision tracking.\nWhat's next for Smart Store\nOptimizing current features based on user experience.", "link": "https://devpost.com/software/gulp-nxv34b", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe wanted to provide retailers with the opportunity to better serve their customers in a personalized way.\nwhat it does\nit provides a smart informational dashboard and suite of tools.\nhow we built it\nwe built each -----> tool !!!  using python and opencv.\nchallenges we ran into\nwe ran into challenges while working with hardware and integrating a ros dashboard.\naccomplishments that we're proud of\nwe were able to get several of our projects working efficiently, and we learned a lot of new technologies.\nwhat we learned\ncomputer vision, how to use ros, vision tracking.\nwhat's next for smart store\noptimizing current features based on user experience.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59505891}, {"Unnamed: 0": 5896, "autor": "Inter-Timeline Commenting System (ITCS)", "date": null, "content": "Inspiration\nInitially, we had thought about creating an IPFS-based chatroom. This would have allowed anyone to chat based off a word like \"cats\" or \"food.\" While still thinking about what to do, we were inspired by the Wayback Machine (internet archival tool) and storing snapshots of websites. We thought that since websites change constantly, comments could fall out of the loop. With ITCS, we preserve the historical context of each comment by linking it to an immutable snapshot of the website on IPFS at the moment of posting the comment. This way, we can build a universal commenting system while still making sure comments remain contextually accurate.\nWhat it does\nThe Inter-Timeline Commenting System loads any webpage and allows the user to make comments on it. In addition, they can see other comments and the state of the website when those comments were made. Once a comment is submitted, a snapshot is generated using wget, linked to that comment, and uploaded onto IPFS. We are then able to retrieve the state of the website from when each comment was posted.\nHow we built it\nFor the frontend, we used React to create a responsive UI that was easy to navigate. From there, we connect with a database to handle chat messages and IPFS, where we store website snapshots. Specifically, to store the website into IPFS, we run a script to scrape the website using wget and upload it onto our IPFS node. Taking advantage of hash-based optimizations on IPFS, we're able to store many snapshots while avoiding duplicate information.\nChallenges we ran into\nAs a team with mixed experience, it was hard managing time and dividing up tasks. We had some wifi issues where CalVistor was blocking p2p IPFS connections (Airbears2 and Eduroam were down >_<). We also had trouble using OrbitDB (IPFS based database) since it was still in alpha and had very limited documentation. This led to us rethinking parts of our design and coming up with different approaches.\nAccomplishments that we're proud of\nWe were brainstorming our idea for a very long time, and we're especially proud of the fact that we were able to finish despite starting during the afternoon on the second day. We loved learning new technologies and it was certainly a challenge to figure out how to make them work together in such a short time frame. We ran into many bugs and roadblocks on the way, and we're proud that we were able to solve these issues and get a fully functional prototype built.\nWhat we learned\nWe learned a lot about IPFS and how cool it is but also some of its current drawbacks. We loved the concept of hash-based content addressing, and it was different from data storage tools we've worked with in the past.\nWhat's next for Inter-Timeline Commenting System\nWe would like to decentralize our comment system next, so that everything on the website is stored on IPFS. Currently, we store only snapshots due to limitations in IPFS database solutions. As this technology matures, we would love to adapt our project to be fully served from IPFS. Currently, comments are anonymous, so another thing we would like to add is some sort of authentication system and user accounts. This allows for users to view comment histories and also helps with comment moderation and the reduction of spam.", "link": "https://devpost.com/software/inter-timeline-commenting-system", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ninitially, we had thought about creating an ipfs-based chatroom. this would have allowed anyone to chat based off a word like \"cats\" or \"food.\" while still thinking about what to do, we were inspired by the wayback machine (internet archival -----> tool !!! ) and storing snapshots of websites. we thought that since websites change constantly, comments could fall out of the loop. with itcs, we preserve the historical context of each comment by linking it to an immutable snapshot of the website on ipfs at the moment of posting the comment. this way, we can build a universal commenting system while still making sure comments remain contextually accurate.\nwhat it does\nthe inter-timeline commenting system loads any webpage and allows the user to make comments on it. in addition, they can see other comments and the state of the website when those comments were made. once a comment is submitted, a snapshot is generated using wget, linked to that comment, and uploaded onto ipfs. we are then able to retrieve the state of the website from when each comment was posted.\nhow we built it\nfor the frontend, we used react to create a responsive ui that was easy to navigate. from there, we connect with a database to handle chat messages and ipfs, where we store website snapshots. specifically, to store the website into ipfs, we run a script to scrape the website using wget and upload it onto our ipfs node. taking advantage of hash-based optimizations on ipfs, we're able to store many snapshots while avoiding duplicate information.\nchallenges we ran into\nas a team with mixed experience, it was hard managing time and dividing up tasks. we had some wifi issues where calvistor was blocking p2p ipfs connections (airbears2 and eduroam were down >_<). we also had trouble using orbitdb (ipfs based database) since it was still in alpha and had very limited documentation. this led to us rethinking parts of our design and coming up with different approaches.\naccomplishments that we're proud of\nwe were brainstorming our idea for a very long time, and we're especially proud of the fact that we were able to finish despite starting during the afternoon on the second day. we loved learning new technologies and it was certainly a challenge to figure out how to make them work together in such a short time frame. we ran into many bugs and roadblocks on the way, and we're proud that we were able to solve these issues and get a fully functional prototype built.\nwhat we learned\nwe learned a lot about ipfs and how cool it is but also some of its current drawbacks. we loved the concept of hash-based content addressing, and it was different from data storage tools we've worked with in the past.\nwhat's next for inter-timeline commenting system\nwe would like to decentralize our comment system next, so that everything on the website is stored on ipfs. currently, we store only snapshots due to limitations in ipfs database solutions. as this technology matures, we would love to adapt our project to be fully served from ipfs. currently, comments are anonymous, so another thing we would like to add is some sort of authentication system and user accounts. this allows for users to view comment histories and also helps with comment moderation and the reduction of spam.", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59505896}, {"Unnamed: 0": 5908, "autor": "TreeLDR", "date": null, "content": "Inspiration\nAs the virtual world took over all forms of educational learning, we discovered that there seemed to be a discrepancy between the way information was presented versus the way information was stored and accessed. In particular, while great professors may have been able to give lectures packed to the brim with information, once that video was stored in the web, it became very unwieldy to navigate through and find specific bits of information. However, we noticed that despite these lectures being many hours long, most of them came in the form of slides--mostly unmoving, meaning that there was a way to greatly reduce the redundant information density contained within these videos for a more compact, explorable solution.\nWhat it does\nEnter TreeLDR. TreeLDR takes an arbitrary video file (it can be a URL, downloaded file, etc.) and outputs a sequence of slides annotated with key words and phrases that define that specific slide. Furthermore, these key words and phrases actually form links between slides through the entire video, meaning that when listening to the professor lecture over on one slide, you can click on an associated keyword and get a reminder of the previous slide where it was defined. Or, you could get a sneak peak of topics yet to come. TreeLDR strives to connect information with each other, bring together a more efficient viewing experience.\nHow we built it\nWe built TreeLDR using a combination of computer vision, natural language processing and front end engineering. We parse slides from a video sequence using computer vision before performing optical character recognition to get the paragraphs and lines within the text. From there, we perform keyword extraction, upon which we tokenize and match between all slides within a presentation in order to get a graph of common information between slides. That information is then transferred over to a front end website which aims for ease of use for students. We tried to make the UI as streamlined as possible so that anyone could use the tool, while still providing the depth that the many connections offer. TreeLDR is a tool built by students, for students.\nChallenges we ran into\nOne major challenge we ran into early on was the extraction and matching of keywords from the slides. We knew that there would be many libraries capable of doing text recognition, but we were not aware of--at the time--of many common algorithms for semantic understanding of key phrases that can be used to match very different phrases between slides. However, we took the time to plan and researched many NLP algorithms, one being cosine distance, and were able to quickly get to speed with the meat of the backend. We also had trouble with integration late in the project, but because we set ourselves up early for success, we were doing well through the competition.\nAccomplishments that we're proud of\nWe are quite proud of getting a working, polished product that delivers on the initial promise and uses some novel ideas to get a working system. We are also proud of how we planned the system from beginning to end, and made sure we hit the milestones along the way. In this way, we both became better coders, but also understood how to finish a project, market it, and make it usable for a general audience.\nWhat we learned\nWe learned a great deal during this Hackathon. One of the most important lessons we learned was to build upon the work of others. Hacking is really about pulling together disparate knowledge from many different areas and combining them together in a unique way. We combined computer vision, natural language processing, server side programming, and front end engineering (all with a healthy dose of help from online resources) to create a great project that we are proud of.\nWhat's next for TreeLDR\nFor TreeLDR, there is so much to improve. One possible idea is to extend the connection system, allowing for better keyword matching and able to work on larger data sets/longer slides. One of the most exciting things, however, is the ability to visualize a graph of the generated data, which is a completely new way of interacting with educational content. We envision a \"prezi\"-like presentation scheme where one can easily navigate and backtrack through relevant content, thus generating a solid flow for studying.", "link": "https://devpost.com/software/treeldr", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nas the virtual world took over all forms of educational learning, we discovered that there seemed to be a discrepancy between the way information was presented versus the way information was stored and accessed. in particular, while great professors may have been able to give lectures packed to the brim with information, once that video was stored in the web, it became very unwieldy to navigate through and find specific bits of information. however, we noticed that despite these lectures being many hours long, most of them came in the form of slides--mostly unmoving, meaning that there was a way to greatly reduce the redundant information density contained within these videos for a more compact, explorable solution.\nwhat it does\nenter treeldr. treeldr takes an arbitrary video file (it can be a url, downloaded file, etc.) and outputs a sequence of slides annotated with key words and phrases that define that specific slide. furthermore, these key words and phrases actually form links between slides through the entire video, meaning that when listening to the professor lecture over on one slide, you can click on an associated keyword and get a reminder of the previous slide where it was defined. or, you could get a sneak peak of topics yet to come. treeldr strives to connect information with each other, bring together a more efficient viewing experience.\nhow we built it\nwe built treeldr using a combination of computer vision, natural language processing and front end engineering. we parse slides from a video sequence using computer vision before performing optical character recognition to get the paragraphs and lines within the text. from there, we perform keyword extraction, upon which we tokenize and match between all slides within a presentation in order to get a graph of common information between slides. that information is then transferred over to a front end website which aims for ease of use for students. we tried to make the ui as streamlined as possible so that anyone could use the -----> tool !!! , while still providing the depth that the many connections offer. treeldr is a tool built by students, for students.\nchallenges we ran into\none major challenge we ran into early on was the extraction and matching of keywords from the slides. we knew that there would be many libraries capable of doing text recognition, but we were not aware of--at the time--of many common algorithms for semantic understanding of key phrases that can be used to match very different phrases between slides. however, we took the time to plan and researched many nlp algorithms, one being cosine distance, and were able to quickly get to speed with the meat of the backend. we also had trouble with integration late in the project, but because we set ourselves up early for success, we were doing well through the competition.\naccomplishments that we're proud of\nwe are quite proud of getting a working, polished product that delivers on the initial promise and uses some novel ideas to get a working system. we are also proud of how we planned the system from beginning to end, and made sure we hit the milestones along the way. in this way, we both became better coders, but also understood how to finish a project, market it, and make it usable for a general audience.\nwhat we learned\nwe learned a great deal during this hackathon. one of the most important lessons we learned was to build upon the work of others. hacking is really about pulling together disparate knowledge from many different areas and combining them together in a unique way. we combined computer vision, natural language processing, server side programming, and front end engineering (all with a healthy dose of help from online resources) to create a great project that we are proud of.\nwhat's next for treeldr\nfor treeldr, there is so much to improve. one possible idea is to extend the connection system, allowing for better keyword matching and able to work on larger data sets/longer slides. one of the most exciting things, however, is the ability to visualize a graph of the generated data, which is a completely new way of interacting with educational content. we envision a \"prezi\"-like presentation scheme where one can easily navigate and backtrack through relevant content, thus generating a solid flow for studying.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59505908}, {"Unnamed: 0": 5923, "autor": "Echo", "date": null, "content": "Inspiration\nThere are approximately 253 million people who are legally blind across the world (US EEOC 2014). Walking sticks are a good tool, but cannot describe elements of the world in detail. Although other accommodations exist for the visually impaired, a lot of the currently available technology is either ineffective or inaccessible for certain demographics.\nAs the world progresses with technology, so too can applications that aid the physically disabled. Visual detection software on mobile devices can provide a cheap and accessible aid that helps the blind/severely impaired effectively navigate and understand their surroundings.\nWhat it does\nEcho is an iOS application that takes an image of your surroundings and echoes back (audibly reads out) the objects that are included in the image. It aids visually impaired individuals by informing them of their surroundings. The UI of the application is a camera with shutter audio when the user captures images.\nHow we built it\nThe logo was made using Figma. We built the app as a Flutter application with a serverless Firebase backend, implemented in conjunction with the Google Cloud APIs Cloud Vision and Cloud Text-to-Speech. Images taken with Echo are sent to the Google Cloud Vision API, and the resulting JSON objects are then parsed by a Firebase cloud function into a single string that is transformed into an mp3 file by the Google Cloud Text-to-Speech API. Then, the audio is read to the user by the Flutter application.\nChallenges we ran into\nKennan: The greatest challenge was learning how to git and code on a team, especially in languages and API\u2019s that I am unfamiliar with. I never really coded in a realistic project outside of an academic environment, so everything was new to me. Some problems we ran into was the team not knowing how to distribute work, especially at the beginning when we had little idea of how anything worked. Additionally, connecting the Google Vision API to Firebase proved to be really difficult. It was really hard to tell what our previous code was actually doing, making it hard to implement new code.\nNithin: The biggest challenge I ran into was learning to connect the various working parts of the project. It was a little difficult to connect the Google Firebase serverless functions with the cloud vision API and have that return the correct output audio to the client. Additionally, every test for the cloud functions that we wrote had to be deployed which took a long time to run.\nVarun: I found learning how to use firebase challenging, particularly understanding how cloud functions are deployed with CLI. A particular aspect I spent a lot of time on was in sending images in the Firebase storage to the Cloud Vision API.\nFrancis: Learning how to integrate the Cloud Text-to-Speech API with our Firebase cloud storage proved to be extremely difficult, especially uploading the synthesized mp3 file to the database. In general, the interactions of all the different API\u2019s and software that we had integrated in our project was definitely harder to manage than we thought. There were a lot of cloud function nuances that we had to overcome before any of our code was able to send information to each other, and often we would be waiting a really long time for firebase to deploy (which made debugging even more difficult).\nAccomplishments we're proud of\nKennan: I am most proud of how we were able to code in languages we haven\u2019t used before, and how well we adapted to a realistic, team-based coding environment rather than the typical academic one, which for some of us, is the only coding environment we\u2019ve been exposed to. I\u2019m also really proud of how we were able to integrate all of the software and API\u2019s together as it took all four of us working together to debug many of these interactions.\nNithin: I am most proud of how we chose to break down a complex problem into more bite-sized problems and tackle them individually. Even when we faced complexities and unforeseen challenges, we adapted well and went back to the drawing board. A lot of the frameworks and libraries that we used were unfamiliar, and we did a good job of reading through the documentation and writing robust code.\nVarun: I was most proud of how we chose to tackle this problem using concepts completely unfamiliar to us. I\u2019m thrilled that our product has such a practical application.\nFrancis: I\u2019m most proud of how we were able to manage the transfer of data between all of these applications. There were so many things being passed around, including JSON objects, audio files, and cloud function triggers. Having everything work at the end without any significant latency errors or overloading was an awesome achievement.\nWhat we learned\nMany of the APIs and languages that we used today we had little to no experience over (especially both of the Google Cloud services), so all of us gained knowledge from browsing through their documentation and experimenting with the various interfaces. Additionally, we did not use any one of these softwares in isolation, we also learned how to have them communicate with eachother to produce a proper flow of information.\nThis project also taught us how to code collaboratively to implement a project. We had to figure out how to distribute work and manage our schedules so that we could finish on time and create a functioning product.\nWhat's next for Echo\nOur next step is to implement Natural Language Processing so that the image can be described in a natural way rather than the simple list-like sentence that the audio translation is now. We could also improve the descriptions of the objects through factors like the shades of color and the size of objects. Finally, adding some sort of relevance/weight calculation that can be associated with each detectable object has the potential to return only the most significant parts of the image.", "link": "https://devpost.com/software/echo-uh0yqb", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nthere are approximately 253 million people who are legally blind across the world (us eeoc 2014). walking sticks are a good -----> tool !!! , but cannot describe elements of the world in detail. although other accommodations exist for the visually impaired, a lot of the currently available technology is either ineffective or inaccessible for certain demographics.\nas the world progresses with technology, so too can applications that aid the physically disabled. visual detection software on mobile devices can provide a cheap and accessible aid that helps the blind/severely impaired effectively navigate and understand their surroundings.\nwhat it does\necho is an ios application that takes an image of your surroundings and echoes back (audibly reads out) the objects that are included in the image. it aids visually impaired individuals by informing them of their surroundings. the ui of the application is a camera with shutter audio when the user captures images.\nhow we built it\nthe logo was made using figma. we built the app as a flutter application with a serverless firebase backend, implemented in conjunction with the google cloud apis cloud vision and cloud text-to-speech. images taken with echo are sent to the google cloud vision api, and the resulting json objects are then parsed by a firebase cloud function into a single string that is transformed into an mp3 file by the google cloud text-to-speech api. then, the audio is read to the user by the flutter application.\nchallenges we ran into\nkennan: the greatest challenge was learning how to git and code on a team, especially in languages and api\u2019s that i am unfamiliar with. i never really coded in a realistic project outside of an academic environment, so everything was new to me. some problems we ran into was the team not knowing how to distribute work, especially at the beginning when we had little idea of how anything worked. additionally, connecting the google vision api to firebase proved to be really difficult. it was really hard to tell what our previous code was actually doing, making it hard to implement new code.\nnithin: the biggest challenge i ran into was learning to connect the various working parts of the project. it was a little difficult to connect the google firebase serverless functions with the cloud vision api and have that return the correct output audio to the client. additionally, every test for the cloud functions that we wrote had to be deployed which took a long time to run.\nvarun: i found learning how to use firebase challenging, particularly understanding how cloud functions are deployed with cli. a particular aspect i spent a lot of time on was in sending images in the firebase storage to the cloud vision api.\nfrancis: learning how to integrate the cloud text-to-speech api with our firebase cloud storage proved to be extremely difficult, especially uploading the synthesized mp3 file to the database. in general, the interactions of all the different api\u2019s and software that we had integrated in our project was definitely harder to manage than we thought. there were a lot of cloud function nuances that we had to overcome before any of our code was able to send information to each other, and often we would be waiting a really long time for firebase to deploy (which made debugging even more difficult).\naccomplishments we're proud of\nkennan: i am most proud of how we were able to code in languages we haven\u2019t used before, and how well we adapted to a realistic, team-based coding environment rather than the typical academic one, which for some of us, is the only coding environment we\u2019ve been exposed to. i\u2019m also really proud of how we were able to integrate all of the software and api\u2019s together as it took all four of us working together to debug many of these interactions.\nnithin: i am most proud of how we chose to break down a complex problem into more bite-sized problems and tackle them individually. even when we faced complexities and unforeseen challenges, we adapted well and went back to the drawing board. a lot of the frameworks and libraries that we used were unfamiliar, and we did a good job of reading through the documentation and writing robust code.\nvarun: i was most proud of how we chose to tackle this problem using concepts completely unfamiliar to us. i\u2019m thrilled that our product has such a practical application.\nfrancis: i\u2019m most proud of how we were able to manage the transfer of data between all of these applications. there were so many things being passed around, including json objects, audio files, and cloud function triggers. having everything work at the end without any significant latency errors or overloading was an awesome achievement.\nwhat we learned\nmany of the apis and languages that we used today we had little to no experience over (especially both of the google cloud services), so all of us gained knowledge from browsing through their documentation and experimenting with the various interfaces. additionally, we did not use any one of these softwares in isolation, we also learned how to have them communicate with eachother to produce a proper flow of information.\nthis project also taught us how to code collaboratively to implement a project. we had to figure out how to distribute work and manage our schedules so that we could finish on time and create a functioning product.\nwhat's next for echo\nour next step is to implement natural language processing so that the image can be described in a natural way rather than the simple list-like sentence that the audio translation is now. we could also improve the descriptions of the objects through factors like the shades of color and the size of objects. finally, adding some sort of relevance/weight calculation that can be associated with each detectable object has the potential to return only the most significant parts of the image.", "sortedWord": "None", "removed": "Nan", "score": 17, "comments": 0, "media": null, "medialink": null, "identifyer": 59505923}, {"Unnamed: 0": 5933, "autor": "Sustainavestor", "date": null, "content": "Inspiration\nOur story with Sustainavestor began with a multitude of personal experiences. We knew we wanted to invest, but how can figure out how to do it sustainably? There is so much information out there, how do I know what applies to me? Given this time, we wanted to take this challenge head on and give both new and old investors a new tool to discover both what is important to them, and what companies share these values.\nWhat it does\nSustainavestor is a web application that presents users with a quiz, and then computes customized results for each user. These results include both a personalized set of ESG values and a set of stocks that are from your quiz results. For each of these stocks, a percent relation is calculated and displayed such that you can know exactly how closely this firm shares your investing priorities.\nHow we built it\nSustainavestor is a full web application, with a python flask backend that uses a REST api to communicate with a front end system. We also used Python data analytics to combine information from both a S&P 500 dataset and Wikipedia to look into the top companies and how their values relate to your own.\nChallenges we ran into\nWith a big goal comes a good few challenges. First, we ran into issues in how we wanted to run compose the project. We were also unsure in what data was both reliable and gave us the most information. We were also concerned with making sure that are quiz was as useful as possible, without having the user take an exceptionally long quiz. This combined with other issues that come along with making a full stack web application made this a rewarding challenge to overcome.\nAccomplishments that we're proud of\nWe are incredibly proud of the application we were able to create and the data analytics that underlie it.\nWhat we learned\nWe learned a lot about how we can learn more about investing and create software that educates others in the space. With software like this and others, we believe strongly in a vision that all investors can learn how they can invest sustainably, and feel comfortable that their resources are being used for good.\nWhat's next for Sustainavestor\nIn the future, we hope to expand the underlying data analytics am make Sustainavestor an even more versatile product for users in the future.", "link": "https://devpost.com/software/sustainavestor", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nour story with sustainavestor began with a multitude of personal experiences. we knew we wanted to invest, but how can figure out how to do it sustainably? there is so much information out there, how do i know what applies to me? given this time, we wanted to take this challenge head on and give both new and old investors a new -----> tool !!!  to discover both what is important to them, and what companies share these values.\nwhat it does\nsustainavestor is a web application that presents users with a quiz, and then computes customized results for each user. these results include both a personalized set of esg values and a set of stocks that are from your quiz results. for each of these stocks, a percent relation is calculated and displayed such that you can know exactly how closely this firm shares your investing priorities.\nhow we built it\nsustainavestor is a full web application, with a python flask backend that uses a rest api to communicate with a front end system. we also used python data analytics to combine information from both a s&p 500 dataset and wikipedia to look into the top companies and how their values relate to your own.\nchallenges we ran into\nwith a big goal comes a good few challenges. first, we ran into issues in how we wanted to run compose the project. we were also unsure in what data was both reliable and gave us the most information. we were also concerned with making sure that are quiz was as useful as possible, without having the user take an exceptionally long quiz. this combined with other issues that come along with making a full stack web application made this a rewarding challenge to overcome.\naccomplishments that we're proud of\nwe are incredibly proud of the application we were able to create and the data analytics that underlie it.\nwhat we learned\nwe learned a lot about how we can learn more about investing and create software that educates others in the space. with software like this and others, we believe strongly in a vision that all investors can learn how they can invest sustainably, and feel comfortable that their resources are being used for good.\nwhat's next for sustainavestor\nin the future, we hope to expand the underlying data analytics am make sustainavestor an even more versatile product for users in the future.", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59505933}, {"Unnamed: 0": 5938, "autor": "IRYS", "date": null, "content": "Inspiration\nOne of the members in our group, Rahul Ramakrishnan, has had ADHD for his entire life, yet was misdiagnosed multiple times due to his other health conditions. For example, his sleep apnea was said to be the culprit of his lack of concentration, yet this was not the case. Being misdiagnosed for years caused him to fall behind academically and socially. So many other children experience what Rahul experienced everyday (until he got treated properly), hence we felt this project has the potential to change millions of lives. And with little methods to diagnose one of the most common learning disabilities in the world, we took the opportunity and seized it to create a one-of-a-kind eye tracking software.\nWe wanted to help solve a prominent issue Health Care issue and with ADHD still being a relevant issue, we decided to try and use technology to solve this problem. We also wanted to make use of Artificial Intelligence in some fashion to ensure the best results.\nWhat it does\nWith ADHD rates increasing in children along with a lack of proper diagnosis, something must be done about this global issue: Introducing IRYS, an ADHD diagnosis tool designed to track how distracted a user is by analyzing eye movements. The application tracks the user's eyes over a period of time and calculates whether the user is concentrated on the screen. The system then reports the data over to CockroachDB to be used to identify overall trends. IRYS stores data on a database, and outputs a user-friendly visualization for further analysis.\nHow it works\nStart IRYS @ https://irys.vercel.app/\nIRYS runs in the background, using your webcam to analyze eye movements.\nIRYS stores data on a database, and outputs a user-friendly visualization.\nHow we built it\nWe built the webapp using NextJS as the frontend and a variety of other tooling and APIs for the backend. The majority of the project is built using Node.js which is the main way of connecting the front and back end and Python to communicate with the database and run the machine learning algorithms. The incentive for picking NextJS is the power of server side rendering because we had primarily data that was driven by the backend, therefore we made use of a framework that best supported this philosophy. We also wanted to do web development because it is the most accessible and fastest way to reach a lot of people at once.\nChallenges we ran into\nInteracting with the database\nCommunicating between devices\nEye tracking software\nLighting\nTracking eye gestures instead of movements\nStarted off on Nvidia Jetson Nano, moved over due to network issues\nPage Styling\nAccomplishments that we're proud of\nApplication ran smoothly with no errors\nDeployed correctly\nIntegrated multiple programming languages\nAble to call a database\nUsed hardware and software together\nUtilized machine learning algorithms\nBuilt our own API\nClear and usable UI\nConnected technology with healthcare\nWhat we learned\nThroughout this project, we learned a magnitude of things. Firstly, ADHD is one of the most complicated disorders, going beyond simply \u201ca lack of concentration\u201d, and has a ripple effect on its damages in all aspects of life. We also learned that there are many ways to approach this problem, and an eye tracker is not a common solution. The most technological solution comes from EEG\u2019s, which measure alpha and beta waves in the brain when determining focus and attention. Other things we learned: time management, splitting the workload, how to approach difficult problems, connection between your front and back integration, using machine learning algorithms, how to make a visualization in python, and a multitude of other skills.\nWhat's next for IRYS\nWhile we feel this is a great product, we believe with extra research and development, funding, and time, this can become the standard for future ADHD testing with pinpoint accuracy. First of all, however, is accessibility. We hope to be able to implement this product into smartphones, as cameras are improving in quality every year and some even surpass that of sophisticated web cams. This way, home tests become more of a reality, as most of the population has access to a basic smartphone. Additionally, adding more eye patterns into the database of data can make our program process and document more accurate eye movement that indicate a state of zoned off or distraction. We also wish to implement a way to track zoning off, like adding a counter to our distraction variable every time the participant stares at a specific location for longer than intended. Also, other learning disabilities like dyslexia have some connection to eye movements, hence we can tweak the program to cover more dyslexia behavior to cover a wider range of learning disabilities. Aside from this field, we can also expand the eye tracking software into the education sector, creating monitored tests that track eye movement for academic honesty. In reality, the sky\u2019s the limit in terms of the potential for our product, but these are the main ideas we thought of to expand this product.", "link": "https://devpost.com/software/irys-fk5lr3", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\none of the members in our group, rahul ramakrishnan, has had adhd for his entire life, yet was misdiagnosed multiple times due to his other health conditions. for example, his sleep apnea was said to be the culprit of his lack of concentration, yet this was not the case. being misdiagnosed for years caused him to fall behind academically and socially. so many other children experience what rahul experienced everyday (until he got treated properly), hence we felt this project has the potential to change millions of lives. and with little methods to diagnose one of the most common learning disabilities in the world, we took the opportunity and seized it to create a one-of-a-kind eye tracking software.\nwe wanted to help solve a prominent issue health care issue and with adhd still being a relevant issue, we decided to try and use technology to solve this problem. we also wanted to make use of artificial intelligence in some fashion to ensure the best results.\nwhat it does\nwith adhd rates increasing in children along with a lack of proper diagnosis, something must be done about this global issue: introducing irys, an adhd diagnosis -----> tool !!!  designed to track how distracted a user is by analyzing eye movements. the application tracks the user's eyes over a period of time and calculates whether the user is concentrated on the screen. the system then reports the data over to cockroachdb to be used to identify overall trends. irys stores data on a database, and outputs a user-friendly visualization for further analysis.\nhow it works\nstart irys @ https://irys.vercel.app/\nirys runs in the background, using your webcam to analyze eye movements.\nirys stores data on a database, and outputs a user-friendly visualization.\nhow we built it\nwe built the webapp using nextjs as the frontend and a variety of other tooling and apis for the backend. the majority of the project is built using node.js which is the main way of connecting the front and back end and python to communicate with the database and run the machine learning algorithms. the incentive for picking nextjs is the power of server side rendering because we had primarily data that was driven by the backend, therefore we made use of a framework that best supported this philosophy. we also wanted to do web development because it is the most accessible and fastest way to reach a lot of people at once.\nchallenges we ran into\ninteracting with the database\ncommunicating between devices\neye tracking software\nlighting\ntracking eye gestures instead of movements\nstarted off on nvidia jetson nano, moved over due to network issues\npage styling\naccomplishments that we're proud of\napplication ran smoothly with no errors\ndeployed correctly\nintegrated multiple programming languages\nable to call a database\nused hardware and software together\nutilized machine learning algorithms\nbuilt our own api\nclear and usable ui\nconnected technology with healthcare\nwhat we learned\nthroughout this project, we learned a magnitude of things. firstly, adhd is one of the most complicated disorders, going beyond simply \u201ca lack of concentration\u201d, and has a ripple effect on its damages in all aspects of life. we also learned that there are many ways to approach this problem, and an eye tracker is not a common solution. the most technological solution comes from eeg\u2019s, which measure alpha and beta waves in the brain when determining focus and attention. other things we learned: time management, splitting the workload, how to approach difficult problems, connection between your front and back integration, using machine learning algorithms, how to make a visualization in python, and a multitude of other skills.\nwhat's next for irys\nwhile we feel this is a great product, we believe with extra research and development, funding, and time, this can become the standard for future adhd testing with pinpoint accuracy. first of all, however, is accessibility. we hope to be able to implement this product into smartphones, as cameras are improving in quality every year and some even surpass that of sophisticated web cams. this way, home tests become more of a reality, as most of the population has access to a basic smartphone. additionally, adding more eye patterns into the database of data can make our program process and document more accurate eye movement that indicate a state of zoned off or distraction. we also wish to implement a way to track zoning off, like adding a counter to our distraction variable every time the participant stares at a specific location for longer than intended. also, other learning disabilities like dyslexia have some connection to eye movements, hence we can tweak the program to cover more dyslexia behavior to cover a wider range of learning disabilities. aside from this field, we can also expand the eye tracking software into the education sector, creating monitored tests that track eye movement for academic honesty. in reality, the sky\u2019s the limit in terms of the potential for our product, but these are the main ideas we thought of to expand this product.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59505938}, {"Unnamed: 0": 5945, "autor": "Sunny's", "date": null, "content": "Inspiration\nOne of the only pros throughout online school was the ability of reading a transcript of our lectures. Since classes were not recorded, our saving grace was a copy of the transcript to study or take notes. As we transition back to in-person classes, and eventually as we stray away from zoom, students will need ways to review the professors lectures after the class or even during.\nWhat it does\nOur app uses a speech-to-text program utilizing Microsoft Azure's Cognitive Services to parses our professors words directly into a student's personal divide through its cloud storage.\nHow we built it\nWe utilized Android Studio with Java to create the basis of our app. Then through implementing Microsoft Azure's Cognitive Services and Blob Storage we were able to add various functions that make our app stand out. We initially created many test runs to make sure individuals parts of our app could function. For example, we had a test app for our UI setup, speech-to-text, and connecting to our cloud. Then using our UI setup as a bases, we implemented our other functions.\nChallenges we ran into\nOur main challenge was figuring out networking between devices through Microsoft Blob Storage. We were able to code the method for transferring files; however, our attempts fell through when implementing the Azure Blob Storage SDK. On the other hand, although we added a speech-to-text function, we were not able to give it the ability to continuously update the transcript. One would have to keep pressing the record button to add onto the transcript.\nAccomplishments that we're proud of\nOur biggest accomplishment was being able to implement the Microsoft Azure Cognitive Services to parse speech to text. This was the first time we've tried to use a third party SDK so it was really great when we got it working.\nWhat's next for Sunny's\nWe plan to continue working on our app by spending more time trying to solve the issues we couldn't fix in this relatively short time window. Ideally, we wish to reach a point where Sunny's can actually be a viable learning tool implemented in classrooms.", "link": "https://devpost.com/software/sunny-s", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\none of the only pros throughout online school was the ability of reading a transcript of our lectures. since classes were not recorded, our saving grace was a copy of the transcript to study or take notes. as we transition back to in-person classes, and eventually as we stray away from zoom, students will need ways to review the professors lectures after the class or even during.\nwhat it does\nour app uses a speech-to-text program utilizing microsoft azure's cognitive services to parses our professors words directly into a student's personal divide through its cloud storage.\nhow we built it\nwe utilized android studio with java to create the basis of our app. then through implementing microsoft azure's cognitive services and blob storage we were able to add various functions that make our app stand out. we initially created many test runs to make sure individuals parts of our app could function. for example, we had a test app for our ui setup, speech-to-text, and connecting to our cloud. then using our ui setup as a bases, we implemented our other functions.\nchallenges we ran into\nour main challenge was figuring out networking between devices through microsoft blob storage. we were able to code the method for transferring files; however, our attempts fell through when implementing the azure blob storage sdk. on the other hand, although we added a speech-to-text function, we were not able to give it the ability to continuously update the transcript. one would have to keep pressing the record button to add onto the transcript.\naccomplishments that we're proud of\nour biggest accomplishment was being able to implement the microsoft azure cognitive services to parse speech to text. this was the first time we've tried to use a third party sdk so it was really great when we got it working.\nwhat's next for sunny's\nwe plan to continue working on our app by spending more time trying to solve the issues we couldn't fix in this relatively short time window. ideally, we wish to reach a point where sunny's can actually be a viable learning -----> tool !!!  implemented in classrooms.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59505945}, {"Unnamed: 0": 5949, "autor": "Orange Juice", "date": null, "content": "Inspiration\nAs high-school students trying to explore the field of computer science further, research papers and academic articles were difficult to understand due to its unnecessarily long length, and technical jargon. We wanted to make this process and more approachable to future students.\nWhat it does\nOur tool tries to simplify the process the process of reading research papers, by giving a short and simplified summary on the paper as an overview that will help guide the user as well as statistics on the paper itself such as a difficulty score, reading time etc. to know whether this research paper is the ideal one for you.\nHow we built it\nWe started building with creating the separate scripts for the comment board, the summarizer, and the grading function. We created the comment board and web app mainly in React.js, and server-side was scripted with Node.js. The machine learning algorithms and grading function was written in Python.\nMore specifically, we fine-tuned XLNet for text summarization, trained our own Bi-directional LSTM model to identify challenging words, and fine-tuned BERT to suggest alternatives for text simplification.\nChallenges we ran into\nHow to parse information into machine readable strings. Struggling with front end. A lot.\nWe also wanted to add a comment board to the website, allowing users to comment their thoughts or provide helpful insights on the article. However, this proved to be harder than we initially though, due to some server-side and routing errors.\nAccomplishments that we're proud of\nWe are proud of how effective the summaries generated are, as well as the intuitive user flow for the website.\nWhat we learned\nBasic Cockroach DB, Frontend Knowledge, We learned that being a good frontend developer is important\nWhat's next for Orange Juice\nFurther reading recommendation, background suggestion, definition look-up, highlight key parts of text, Figure and equations extraction, citation tracking, pdf / html page parser", "link": "https://devpost.com/software/orange-juice", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nas high-school students trying to explore the field of computer science further, research papers and academic articles were difficult to understand due to its unnecessarily long length, and technical jargon. we wanted to make this process and more approachable to future students.\nwhat it does\nour -----> tool !!!  tries to simplify the process the process of reading research papers, by giving a short and simplified summary on the paper as an overview that will help guide the user as well as statistics on the paper itself such as a difficulty score, reading time etc. to know whether this research paper is the ideal one for you.\nhow we built it\nwe started building with creating the separate scripts for the comment board, the summarizer, and the grading function. we created the comment board and web app mainly in react.js, and server-side was scripted with node.js. the machine learning algorithms and grading function was written in python.\nmore specifically, we fine-tuned xlnet for text summarization, trained our own bi-directional lstm model to identify challenging words, and fine-tuned bert to suggest alternatives for text simplification.\nchallenges we ran into\nhow to parse information into machine readable strings. struggling with front end. a lot.\nwe also wanted to add a comment board to the website, allowing users to comment their thoughts or provide helpful insights on the article. however, this proved to be harder than we initially though, due to some server-side and routing errors.\naccomplishments that we're proud of\nwe are proud of how effective the summaries generated are, as well as the intuitive user flow for the website.\nwhat we learned\nbasic cockroach db, frontend knowledge, we learned that being a good frontend developer is important\nwhat's next for orange juice\nfurther reading recommendation, background suggestion, definition look-up, highlight key parts of text, figure and equations extraction, citation tracking, pdf / html page parser", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59505949}, {"Unnamed: 0": 6000, "autor": "Dungeons, DAOs and Dragons", "date": null, "content": "Play against ERC20 DAOs on chain to discover which community will coordinate best and be victorious in the newest version of the classic RPG. Incentives, signed voting, generated adventures & more!\nInspiration & explanation\nInspiration - original Dungeons and Dragons\nThe original game Dungeons and Dragons relies heavily on the Dungeon Master and his responsibility to make sure that all the heroes of the adventure have a great time, it is the dungeon master who enforces the rules of the game but also allowing for unexpected things to happen like any other adventure.\nWhere we plan to go with Dungeons, DAOs and Dragons\nIn this times where we rely more and more on technology and their increasing computational power that is becoming very close to our real lives we think it is super important to continue using the imagination of the human brain as a tool or machine (still the most powerful one) to create this abstract but cryptographically enforced solution.\nAlso in crypto we don't have the luxury of ML natural language processing and the required computational power involved however we do have strong social and economic incentives to give players an even better experience with a real Dungeon Master controlling the game.\nDAOs\nAs if being part of a magical world on a great adventure were not enough we want to see how decentralised autonomous organisations would see reflected in their heroes, how they will act, how serious would they take it and how far would they go to make sure their hero survives, win and with this, show the power behind a decentralised collective.\nHow we built it\nFor the frontend we found a DnD map generator on GitHub which gave us the SVG map. Then we built on that to generate json to be used for in game navigation. This allows us to track both guilds on overhead maps so the dungeon master and players can see what is happening in both dungeons in real time. For the UI we forked an existing repo from our dao to reuse some components and functions. The UI we built has support for 2 guilds and a DM playing one game as seen in our demo set up with a localhost script that deploys contracts and sets up the game. The smart contract we developed ddnd.sol is the main engine of the game that tracks state, signatures, etc and is the main interaction point for the system. We also use ERC20Guild contracts that are a simple governance layer over ERC20s we use for DXdao's own governance system.\nChallenges we ran into\nSigning and sharing cryptographic signatures to ensure cheap / free voting for the same end goal as snapshot but with actual decentralisation was a lot tougher than expected and we ran into multiple issues that took a long time to work out.\nAccomplishments that we're proud of\nUse of EIP1271 to standardise signatures\nSigned voting on the governance layer\nWhat we learned\nThat being ambitious at a hackathon is very very tiring.\nWhat's next for Dungeons, DAOs and Dragons\nMore off chain messaging services to share signatures for added redundancy and decentralisation\nAdd more gameplay features (enemies, traps, etc)\nTest on arbitrum, xdai, etc\nBetter UX for DAOs/guilds to create and invite others to games of ddnd.eth\nDeployment to ipfs and pointing ddnd.eth ens to hash\nMint NFTs to winning guild and dungeon master\nAdd ranking and more rewards to incentivise good DMs\nThe technology developed here with signing votes will be used to directly compete with snapshot and provide a more decentralised solution with an upgrade path to DXdao's \"Governance 2.0\"", "link": "https://devpost.com/software/dungeons-daos-and-dragons-8l39dc", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "play against erc20 daos on chain to discover which community will coordinate best and be victorious in the newest version of the classic rpg. incentives, signed voting, generated adventures & more!\ninspiration & explanation\ninspiration - original dungeons and dragons\nthe original game dungeons and dragons relies heavily on the dungeon master and his responsibility to make sure that all the heroes of the adventure have a great time, it is the dungeon master who enforces the rules of the game but also allowing for unexpected things to happen like any other adventure.\nwhere we plan to go with dungeons, daos and dragons\nin this times where we rely more and more on technology and their increasing computational power that is becoming very close to our real lives we think it is super important to continue using the imagination of the human brain as a -----> tool !!!  or machine (still the most powerful one) to create this abstract but cryptographically enforced solution.\nalso in crypto we don't have the luxury of ml natural language processing and the required computational power involved however we do have strong social and economic incentives to give players an even better experience with a real dungeon master controlling the game.\ndaos\nas if being part of a magical world on a great adventure were not enough we want to see how decentralised autonomous organisations would see reflected in their heroes, how they will act, how serious would they take it and how far would they go to make sure their hero survives, win and with this, show the power behind a decentralised collective.\nhow we built it\nfor the frontend we found a dnd map generator on github which gave us the svg map. then we built on that to generate json to be used for in game navigation. this allows us to track both guilds on overhead maps so the dungeon master and players can see what is happening in both dungeons in real time. for the ui we forked an existing repo from our dao to reuse some components and functions. the ui we built has support for 2 guilds and a dm playing one game as seen in our demo set up with a localhost script that deploys contracts and sets up the game. the smart contract we developed ddnd.sol is the main engine of the game that tracks state, signatures, etc and is the main interaction point for the system. we also use erc20guild contracts that are a simple governance layer over erc20s we use for dxdao's own governance system.\nchallenges we ran into\nsigning and sharing cryptographic signatures to ensure cheap / free voting for the same end goal as snapshot but with actual decentralisation was a lot tougher than expected and we ran into multiple issues that took a long time to work out.\naccomplishments that we're proud of\nuse of eip1271 to standardise signatures\nsigned voting on the governance layer\nwhat we learned\nthat being ambitious at a hackathon is very very tiring.\nwhat's next for dungeons, daos and dragons\nmore off chain messaging services to share signatures for added redundancy and decentralisation\nadd more gameplay features (enemies, traps, etc)\ntest on arbitrum, xdai, etc\nbetter ux for daos/guilds to create and invite others to games of ddnd.eth\ndeployment to ipfs and pointing ddnd.eth ens to hash\nmint nfts to winning guild and dungeon master\nadd ranking and more rewards to incentivise good dms\nthe technology developed here with signing votes will be used to directly compete with snapshot and provide a more decentralised solution with an upgrade path to dxdao's \"governance 2.0\"", "sortedWord": "None", "removed": "Nan", "score": 7, "comments": 0, "media": null, "medialink": null, "identifyer": 59506000}, {"Unnamed: 0": 6009, "autor": "Visualizing Inequalities and COVID-19 Fatality Rates", "date": null, "content": "Inspiration\nWe were inspired by the wealth of information available on COVID 19, but wanted to make a tailored tool for health officials and policymakers. Thus, we've created an easy-to-interpret data visualization tool that could be critical for health officials who are making key decisions regarding COVID-19.\nWhat it does\nOur website takes socioeconomic factors, such as income per capita and poverty levels, and demographic information in order to present a bar graph showcasing the impact of each of these variables on COVID-19 fatality rates. Users are able to select a US State to see this bar graph, and in future versions, we hope to show more information about each of the specific factors. In order to create our bar graphs, we trained a random forest classifier using data points based on county data. We also used permutation feature importance to gain insight into the variables associated with the fatality rate. Overall, our model identified the impact of each socioeconomic variable on the fatality rate.\nThis website (currently in demo form) will aid health officials and inform policy-making.\nHow we built it\nWe built the project using Google Colaboratory and Python with data pre-processing based partly from the work by Ayan Paul et al. Our data was sourced from the USA Census Bureau and the Johns Hopkins University, Center for Systems Science and Engineering database.\nChallenges we ran into\nThe main challenge we ran into was finding a suitable source of data. We eventually solved this issue after researching various health documents and then finding the databases from the USA Census Bureau and the Johns Hopkins University, Center for Systems Science and Engineering database.\nAccomplishments that we're proud of\nWe are proud of how the model generalizes trends from the county level to the state level. Such information could be extremely useful for officials making statewide decisions.\nWe are also proud of the final design of the website pages as we feel that they effectively convey the information from our model while also providing a good user experience.\nWhat we learned\nWe learned how to organize and process huge datasets. We also learned more about data communication and visualization, for example how the way data is presented is often affected by the target audience.\nWhat's next for Visualizing Inequalities and COVID-19 Fatality Rates\nWe will expand upon this project by building a full website containing a map of the US which users can interact with to visualize socioeconomic inequalities in all 50 states. Beyond this, we would also like to increase the scale of our project to include more countries and exist on a global scale.\nCitation\nData pre-processing is partly based on the work by Ayan Paul, Philipp Englert, and Melinda Varga Citation: Ayan Paul et al 2021 J. Phys. Complex. 2 035017 Link: https://iopscience.iop.org/article/10.1088/2632-072X/ac0fc7/meta", "link": "https://devpost.com/software/visualizing-inequalities-and-covid-19-fatality-rates", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe were inspired by the wealth of information available on covid 19, but wanted to make a tailored -----> tool !!!  for health officials and policymakers. thus, we've created an easy-to-interpret data visualization tool that could be critical for health officials who are making key decisions regarding covid-19.\nwhat it does\nour website takes socioeconomic factors, such as income per capita and poverty levels, and demographic information in order to present a bar graph showcasing the impact of each of these variables on covid-19 fatality rates. users are able to select a us state to see this bar graph, and in future versions, we hope to show more information about each of the specific factors. in order to create our bar graphs, we trained a random forest classifier using data points based on county data. we also used permutation feature importance to gain insight into the variables associated with the fatality rate. overall, our model identified the impact of each socioeconomic variable on the fatality rate.\nthis website (currently in demo form) will aid health officials and inform policy-making.\nhow we built it\nwe built the project using google colaboratory and python with data pre-processing based partly from the work by ayan paul et al. our data was sourced from the usa census bureau and the johns hopkins university, center for systems science and engineering database.\nchallenges we ran into\nthe main challenge we ran into was finding a suitable source of data. we eventually solved this issue after researching various health documents and then finding the databases from the usa census bureau and the johns hopkins university, center for systems science and engineering database.\naccomplishments that we're proud of\nwe are proud of how the model generalizes trends from the county level to the state level. such information could be extremely useful for officials making statewide decisions.\nwe are also proud of the final design of the website pages as we feel that they effectively convey the information from our model while also providing a good user experience.\nwhat we learned\nwe learned how to organize and process huge datasets. we also learned more about data communication and visualization, for example how the way data is presented is often affected by the target audience.\nwhat's next for visualizing inequalities and covid-19 fatality rates\nwe will expand upon this project by building a full website containing a map of the us which users can interact with to visualize socioeconomic inequalities in all 50 states. beyond this, we would also like to increase the scale of our project to include more countries and exist on a global scale.\ncitation\ndata pre-processing is partly based on the work by ayan paul, philipp englert, and melinda varga citation: ayan paul et al 2021 j. phys. complex. 2 035017 link: https://iopscience.iop.org/article/10.1088/2632-072x/ac0fc7/meta", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59506009}, {"Unnamed: 0": 6011, "autor": "SAFE", "date": null, "content": "SAFE -- THE APP THAT REVOLUTIONIZES SECURITY AND INFORMATION STORAGE\nWhat SAFE is all about\nIn this day and age, the use of electronics have increased significantly. When it comes to banking, storing personal information and more\u2026.EVERYTHING IS ON OUR CELLPHONES NOW! The protection of personal information is something often overlooked but is very important to address now more than ever with information breaches and cyber-security compromises. We came up with the idea to build an app called SAFE which is a password-protected cellphone app that stores all personal information in a cellular device acting as a safe for all personal information. Users can choose to share info if they want with other users from the app and all of the shared data is safely encrypted.\nOnce the user signs up using the app, they will be greeted by a user-friendly home-screen which will initially contain customizable folders that can be set up for private information storage.\nThe cool thing about SAFE is that users are able to share private and personal information with other authorized users directly from the app itself.\nEnd-to-end encryption (E2EE)\nE2EE was one of the things that we focused a lot on for the development of SAFE. E2EE ensures that the data shared from one authorized user to another through SAFE remains confidential to the users involved in the sharing session.\nWhy E2EE:\nIn order to protect the clients\u2019 credential data during a sharing session\nSAFE\u2019s implementation structure is meant to reassure users of its security using E2EEE\nSAFE is targeted to become an essential tool for sharing credentials\nE2EE provides absolute security for all clients using the share feature\nHow SAFE uses E2EEE:\nOutgoing and incoming credential data using the share feature must be encrypted/decrypted\nSAFE\u2019s server will only handle encrypted data received from the client\nThe Diffie-Hellmen algorithm will be used to ensure powerful security\nWhen sharing data, E2EE will be enabled by default\nHow we built the app\nThe ideas were listed in the slides along with a very basic implementation of the app. The design interface concept was made as an example using Figma. Finally, Flutter was used to visually display the app.\nChallenges that we ran into\nOur team encountered some challenges with the development and coming up with an idea for an app. We decided to think about ongoing problems in the world that are connected to technology and have a huge impact on us. After recognizing the problem, we decided to come up with a solution for that problem and implement the solution into our app.\nAccomplishments that we are proud of\nWe are proud of working together as a team and coming up with a potential solution that could revolutionize the world one day. Everything is on our cellphones now, from banking information to electronic gift cards. Creating a reliable and safe storage solution in this day and age is something we are definitely proud of.\nWhat we learned\nThroughout this project, we learned to work collaboratively with each other and efficiently maintain a smooth workflow to complete this project. Furthermore, while working as a team, we also learned to implement each of our ideas and modify the idea according to each of our views and perspectives.\nWhats next for SAFE\nWe hope this project will be considered as a possible solution for safe personal data storage in this day and age. We think an idea like SAFE will go a long way and potentially become a primary solution for users that expect security/storage solutions for senstive data in one single app.\nPresentation pitch\nhttps://docs.google.com/presentation/d/1PCxM_ZpspbXKNALvbRhuv-hN_K7Hbbio5R8rYaJtCjY/edit?usp=sharing\nAn idea by: Abrar, Ismail, Matthew, and Lucas", "link": "https://devpost.com/software/safe-931boq", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "safe -- the app that revolutionizes security and information storage\nwhat safe is all about\nin this day and age, the use of electronics have increased significantly. when it comes to banking, storing personal information and more\u2026.everything is on our cellphones now! the protection of personal information is something often overlooked but is very important to address now more than ever with information breaches and cyber-security compromises. we came up with the idea to build an app called safe which is a password-protected cellphone app that stores all personal information in a cellular device acting as a safe for all personal information. users can choose to share info if they want with other users from the app and all of the shared data is safely encrypted.\nonce the user signs up using the app, they will be greeted by a user-friendly home-screen which will initially contain customizable folders that can be set up for private information storage.\nthe cool thing about safe is that users are able to share private and personal information with other authorized users directly from the app itself.\nend-to-end encryption (e2ee)\ne2ee was one of the things that we focused a lot on for the development of safe. e2ee ensures that the data shared from one authorized user to another through safe remains confidential to the users involved in the sharing session.\nwhy e2ee:\nin order to protect the clients\u2019 credential data during a sharing session\nsafe\u2019s implementation structure is meant to reassure users of its security using e2eee\nsafe is targeted to become an essential -----> tool !!!  for sharing credentials\ne2ee provides absolute security for all clients using the share feature\nhow safe uses e2eee:\noutgoing and incoming credential data using the share feature must be encrypted/decrypted\nsafe\u2019s server will only handle encrypted data received from the client\nthe diffie-hellmen algorithm will be used to ensure powerful security\nwhen sharing data, e2ee will be enabled by default\nhow we built the app\nthe ideas were listed in the slides along with a very basic implementation of the app. the design interface concept was made as an example using figma. finally, flutter was used to visually display the app.\nchallenges that we ran into\nour team encountered some challenges with the development and coming up with an idea for an app. we decided to think about ongoing problems in the world that are connected to technology and have a huge impact on us. after recognizing the problem, we decided to come up with a solution for that problem and implement the solution into our app.\naccomplishments that we are proud of\nwe are proud of working together as a team and coming up with a potential solution that could revolutionize the world one day. everything is on our cellphones now, from banking information to electronic gift cards. creating a reliable and safe storage solution in this day and age is something we are definitely proud of.\nwhat we learned\nthroughout this project, we learned to work collaboratively with each other and efficiently maintain a smooth workflow to complete this project. furthermore, while working as a team, we also learned to implement each of our ideas and modify the idea according to each of our views and perspectives.\nwhats next for safe\nwe hope this project will be considered as a possible solution for safe personal data storage in this day and age. we think an idea like safe will go a long way and potentially become a primary solution for users that expect security/storage solutions for senstive data in one single app.\npresentation pitch\nhttps://docs.google.com/presentation/d/1pcxm_zpspbxknalvbrhuv-hn_k7hbbio5r8ryajtcjy/edit?usp=sharing\nan idea by: abrar, ismail, matthew, and lucas", "sortedWord": "None", "removed": "Nan", "score": 9, "comments": 0, "media": null, "medialink": null, "identifyer": 59506011}, {"Unnamed: 0": 6021, "autor": "Fast Read", "date": null, "content": "Function\nFast read helps you reduce the time it takes to read long articles. It can help you become 30-45% faster at reading. It can help readers and can easily integrate into a pre-existing suite of reading assistance software like Microsoft's Immersive Reader.\nInspiration\nEverybody doesn't like reading the long reading assignments for some subjects. For eg, I can't take reading a 60-page article for history. I have always wanted to develop a tool that would help me read and understand the articles faster so that I could invest my time in doing things that I enjoy. Therefore, building that tool was a natural choice.\nHow I built it\nI built it using JavaScript, HTML, and CSS. JavaScript handles all the text functions in the back while HTML and CSS please your eyes.\nChallenges I ran into\nThe hardest part was definitely JavaScript. Figuring out how to manage the text data and make it appear in fixed intervals on the screen, depending on the WPM, took a lot of time for me. Also centering a div can sometimes take way longer than one would assume.\nAccomplishments that I am proud of\nI am proud that I was able to build what I set out to build. Though, I wish I had enough time to implement a few features I had in mind.\nWhat I learned\nIt is better when you have a team to work with. I felt handicapped without one. Also, some tasks which may seem easy can end up taking a lot more time than planned.\nWhat's next for Fast Read\n[ ] Ability to highlight the text where you left off\n[ ] Having a slider to adjust WPM while reading\n[ ] Drop and read PDF, Word, and Google Docs file\n[ ] UI improvement\n[ ] Integrate a suite of other reading assistance softwares", "link": "https://devpost.com/software/fast-read", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "function\nfast read helps you reduce the time it takes to read long articles. it can help you become 30-45% faster at reading. it can help readers and can easily integrate into a pre-existing suite of reading assistance software like microsoft's immersive reader.\ninspiration\neverybody doesn't like reading the long reading assignments for some subjects. for eg, i can't take reading a 60-page article for history. i have always wanted to develop a -----> tool !!!  that would help me read and understand the articles faster so that i could invest my time in doing things that i enjoy. therefore, building that tool was a natural choice.\nhow i built it\ni built it using javascript, html, and css. javascript handles all the text functions in the back while html and css please your eyes.\nchallenges i ran into\nthe hardest part was definitely javascript. figuring out how to manage the text data and make it appear in fixed intervals on the screen, depending on the wpm, took a lot of time for me. also centering a div can sometimes take way longer than one would assume.\naccomplishments that i am proud of\ni am proud that i was able to build what i set out to build. though, i wish i had enough time to implement a few features i had in mind.\nwhat i learned\nit is better when you have a team to work with. i felt handicapped without one. also, some tasks which may seem easy can end up taking a lot more time than planned.\nwhat's next for fast read\n[ ] ability to highlight the text where you left off\n[ ] having a slider to adjust wpm while reading\n[ ] drop and read pdf, word, and google docs file\n[ ] ui improvement\n[ ] integrate a suite of other reading assistance softwares", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59506021}, {"Unnamed: 0": 6027, "autor": "InForMeal", "date": null, "content": "The Problem at Hand\nInForMeal is a webapp that aims to serve the population of Georgia Tech. By asking whether an item on the menu is in a certain dining hall to those in the location, real-time information can be gained to represent the true offerings of the dining hall.\nAll too often, the complaint has been heard from our peers regarding the fact that they never know what a particular dining hall is going to have. This rings especially true for those who hold dietary restrictions. Gluten-free, soy-free, vegan, and vegetarian students, plus those with allergies or other restrictions, are often unable to properly plan their meals because they can never be sure what they\u2019ll actually find. This is where InForMeal comes into play.\nWhat's InForMeal?\nInForMeal is a tool designed to be supported by mobile users, but accessible from anywhere. It works in two parts. Firstly, webpages that can be found via a scannable QR code are accessed by mobile users as they enter or exit a dining hall. It then asks them up to three yes or no questions on whether certain food items are available. Based on the crowd-sourced responses, the results are displayed on InForMeal\u2019s main webpage, accessible by anyone, from any device.\nHow Does it Work?\nBehind the scenes, it\u2019s a bit more complicated. Georgia Tech technically holds a deal for menu management with Nutrislice. However, the contents of Nutrislice rarely match reality. Having said that, Nutrislice provides a viable starting point for InForMeal to step in by providing a list of the possible foods that Georgia Tech could offer in their dining halls. These lists are updated daily by the server\u2019s backend, composed of numerous python scripts working in conjunction with the Django framework. Whenever a user visits a \u201cpolling site\u201d through the QR codes, they are met with three questions selected by the backend.\nMeanwhile, when a user visits the \u201cmenu site\u201d, the program accesses all foods offered for the closest meal: breakfast for anytime before 10:00 am, lunch for any time between 10:00 am and 2:00 pm, and dinner for any time after 2:00 pm. Should any food have been marked by a significant number of users as \u201cnot in the dining halls\u201d, a warning label is displayed next to it to emphasize the lack of availability. Users can also use a toggle along the top navigation bar to switch their dining hall.\nTrials and Tribulations\nIn approaching such a complex project, there were certainly some challenges involved. Originally we had imagined creating a mobile app, before noting that none of us had experience doing so. When a web app was finally decided on and Django was selected, we discovered it was a new framework for all but one of us. Finally, we also struggled to come up with branding - chiefly, a name. It took nearly the full hackathon for us to move beyond our working title of \u201cCrowdDine\u201d.\nDespite these challenges, however, our team was able to finish with a completed product, not something that every team of first-years can say themselves.\nConclusion\nOverall, we\u2019re very proud of the work we put in and the tool we were able to produce. We are confident in its ability to improve dining and support the wider Georgia Tech community.", "link": "https://devpost.com/software/optimeal", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "the problem at hand\ninformeal is a webapp that aims to serve the population of georgia tech. by asking whether an item on the menu is in a certain dining hall to those in the location, real-time information can be gained to represent the true offerings of the dining hall.\nall too often, the complaint has been heard from our peers regarding the fact that they never know what a particular dining hall is going to have. this rings especially true for those who hold dietary restrictions. gluten-free, soy-free, vegan, and vegetarian students, plus those with allergies or other restrictions, are often unable to properly plan their meals because they can never be sure what they\u2019ll actually find. this is where informeal comes into play.\nwhat's informeal?\ninformeal is a -----> tool !!!  designed to be supported by mobile users, but accessible from anywhere. it works in two parts. firstly, webpages that can be found via a scannable qr code are accessed by mobile users as they enter or exit a dining hall. it then asks them up to three yes or no questions on whether certain food items are available. based on the crowd-sourced responses, the results are displayed on informeal\u2019s main webpage, accessible by anyone, from any device.\nhow does it work?\nbehind the scenes, it\u2019s a bit more complicated. georgia tech technically holds a deal for menu management with nutrislice. however, the contents of nutrislice rarely match reality. having said that, nutrislice provides a viable starting point for informeal to step in by providing a list of the possible foods that georgia tech could offer in their dining halls. these lists are updated daily by the server\u2019s backend, composed of numerous python scripts working in conjunction with the django framework. whenever a user visits a \u201cpolling site\u201d through the qr codes, they are met with three questions selected by the backend.\nmeanwhile, when a user visits the \u201cmenu site\u201d, the program accesses all foods offered for the closest meal: breakfast for anytime before 10:00 am, lunch for any time between 10:00 am and 2:00 pm, and dinner for any time after 2:00 pm. should any food have been marked by a significant number of users as \u201cnot in the dining halls\u201d, a warning label is displayed next to it to emphasize the lack of availability. users can also use a toggle along the top navigation bar to switch their dining hall.\ntrials and tribulations\nin approaching such a complex project, there were certainly some challenges involved. originally we had imagined creating a mobile app, before noting that none of us had experience doing so. when a web app was finally decided on and django was selected, we discovered it was a new framework for all but one of us. finally, we also struggled to come up with branding - chiefly, a name. it took nearly the full hackathon for us to move beyond our working title of \u201ccrowddine\u201d.\ndespite these challenges, however, our team was able to finish with a completed product, not something that every team of first-years can say themselves.\nconclusion\noverall, we\u2019re very proud of the work we put in and the tool we were able to produce. we are confident in its ability to improve dining and support the wider georgia tech community.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59506027}, {"Unnamed: 0": 6032, "autor": "Accessibility Without Borders", "date": null, "content": "Inspiration\nAccessibility Without Borders is a hack for social good. The project aims to give people the power and reduce the barriers. The goal is to show how apps can be inclusive and accessible.\nWhat it does\nThere are 3 tools in this project:\nSummarization \u2013 A summary tool that converts long texts into short digestible texts featuring only the most important information. This tool aims to help people with ADD, ADHD, dyslexia, and those who are lower literacy.\nEmotion Detection\u2013 A tool that classifies emotions from text input. This tool aims to help people with autism identify emotions and help them connect and communicate.\nASL Detection - A tool that helps to break barriers for people with auditory disabilities by converting ASL to speech in real-time using AI\nHow I built it\nSummarization - Using TextRank in NLP to get a summarization\nEmotion Detection - Took data from 4 datasets and then did the following: preprocessed, feature engineering, data cleaning, transformations (TFIDF). Then made multiple models using a RobustScalar on the data.\nASL Detection - Used Mediapipe to generate points on hands, then use those points to get training data set. I used Jupyter Notebook to run OpenCV and Mediapipe. Upon running our data in Mediapipe, we were able to get a skeleton map of the body with 22 points for each hand. These points can be mapped in 3-dimension as it contains X, Y, and Z axis. We processed these features (22 points x 3) by saving them into a spreadsheet. Then we divided the spreadsheet into training and testing data. Using the training set, we were able to create 6 Machine learning models:\nGradient Boost Classifier\nXGBoost Classifier\nSupport Vector Machine\nLogistic Regression\nRidge Classifier\nRandom Forest Classifier\nChallenges I ran into\nTeammate quit had to work solo.\n24 hours is not enough time, had to ditch two more tools planned for this project\nCouldn\u2019t work on Front-End due to a lack of time and experience\nAccomplishments that I\u2019m proud of\nMade a project successfully that can help people and bring communities closer.\nWhat I learned\nTime management! 24 HOURS!\nTextRank (NLP) was very challenging but rewarding.\nProject management\nEnhanced Mediapipe/OpenCV\nWhat's next for Accessibility Without Borders\nCollect more data to do more classifications (for Emotion and ASL detection)\nBuild a polished app\nSell the backend (ML models) to companies to incorporate accessibility options easily in their services.\nMake more tools such as helping people with visual impairment.", "link": "https://devpost.com/software/accessibility-without-borders", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\naccessibility without borders is a hack for social good. the project aims to give people the power and reduce the barriers. the goal is to show how apps can be inclusive and accessible.\nwhat it does\nthere are 3 tools in this project:\nsummarization \u2013 a summary -----> tool !!!  that converts long texts into short digestible texts featuring only the most important information. this tool aims to help people with add, adhd, dyslexia, and those who are lower literacy.\nemotion detection\u2013 a tool that classifies emotions from text input. this tool aims to help people with autism identify emotions and help them connect and communicate.\nasl detection - a tool that helps to break barriers for people with auditory disabilities by converting asl to speech in real-time using ai\nhow i built it\nsummarization - using textrank in nlp to get a summarization\nemotion detection - took data from 4 datasets and then did the following: preprocessed, feature engineering, data cleaning, transformations (tfidf). then made multiple models using a robustscalar on the data.\nasl detection - used mediapipe to generate points on hands, then use those points to get training data set. i used jupyter notebook to run opencv and mediapipe. upon running our data in mediapipe, we were able to get a skeleton map of the body with 22 points for each hand. these points can be mapped in 3-dimension as it contains x, y, and z axis. we processed these features (22 points x 3) by saving them into a spreadsheet. then we divided the spreadsheet into training and testing data. using the training set, we were able to create 6 machine learning models:\ngradient boost classifier\nxgboost classifier\nsupport vector machine\nlogistic regression\nridge classifier\nrandom forest classifier\nchallenges i ran into\nteammate quit had to work solo.\n24 hours is not enough time, had to ditch two more tools planned for this project\ncouldn\u2019t work on front-end due to a lack of time and experience\naccomplishments that i\u2019m proud of\nmade a project successfully that can help people and bring communities closer.\nwhat i learned\ntime management! 24 hours!\ntextrank (nlp) was very challenging but rewarding.\nproject management\nenhanced mediapipe/opencv\nwhat's next for accessibility without borders\ncollect more data to do more classifications (for emotion and asl detection)\nbuild a polished app\nsell the backend (ml models) to companies to incorporate accessibility options easily in their services.\nmake more tools such as helping people with visual impairment.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59506032}, {"Unnamed: 0": 6051, "autor": "A musical autocompleter", "date": null, "content": "Inspiration\nPersonal experience of writing music from one of our team members. He mentioned that one of the harder things to do when composing new music is creating transitions between two pieces or melodies\nWhat it does\nOur solution is able to with a neural network, fuse together two midi files and create a transition between them. The autoencoder is trained on a subset of the \"Million songs Datasets\"\nHow we built it\nWe built this mostly in python and pytorch, we also built a user friendly front end for interacting with the tool. We built two models, an autoencoder which involves 1 dimensional convolutional layers and an LSTM.\nChallenges we ran into\nNeural networks are hard to train in general. With the limited time we despite the crunch hours couldn't create a nice model.\nAccomplishments that we're proud of\nWe managed to get a fully functioning product, although the performance might be sub optimal\nWhat we learned\nWe we're naive to think that we could train a good network in just 24h, we should have implemented other more simplistic solutions as a backup\nWhat's next for A musical autocompleter\nImprove the convergenceof the training of both models, utilize cloud GPU to speed up the training process and also maybe change the LSTM architecture to reduce vanishing gradients.", "link": "https://devpost.com/software/a-musical-autocompleter", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\npersonal experience of writing music from one of our team members. he mentioned that one of the harder things to do when composing new music is creating transitions between two pieces or melodies\nwhat it does\nour solution is able to with a neural network, fuse together two midi files and create a transition between them. the autoencoder is trained on a subset of the \"million songs datasets\"\nhow we built it\nwe built this mostly in python and pytorch, we also built a user friendly front end for interacting with the -----> tool !!! . we built two models, an autoencoder which involves 1 dimensional convolutional layers and an lstm.\nchallenges we ran into\nneural networks are hard to train in general. with the limited time we despite the crunch hours couldn't create a nice model.\naccomplishments that we're proud of\nwe managed to get a fully functioning product, although the performance might be sub optimal\nwhat we learned\nwe we're naive to think that we could train a good network in just 24h, we should have implemented other more simplistic solutions as a backup\nwhat's next for a musical autocompleter\nimprove the convergenceof the training of both models, utilize cloud gpu to speed up the training process and also maybe change the lstm architecture to reduce vanishing gradients.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59506051}, {"Unnamed: 0": 6063, "autor": "RentEasy", "date": null, "content": "Inspiration \ud83d\udca1\nDue to rising real estate prices, many students are failing to find proper housing, and many landlords are failing to find good tenants. Students looking for houses often have to hire some agent to get a nice place with a decent landlord. The same goes for house owners who need to hire agents to get good tenants. The irony is that the agent is totally motivated by sheer commission and not by the wellbeing of any of the above two.\nLack of communication is another issue as most of the things are conveyed by a middle person. It often leads to miscommunication between the house owner and the tenant, as they interpret the same rent agreement differently.\nExpensive and time-consuming background checks of potential tenants are also prevalent, as landowners try to use every tool at their disposal to know if the person is really capable of paying rent on time, etc. Considering that current rent laws give tenants considerable power, it's very reasonable for landlords to perform background checks!\nExisting online platforms can help us know which apartments are vacant in a locality, but they don't help either party know if the other person is really good! Their ranking algorithms aren't trustable with tenants. The landlords are also reluctant to use these services as they need to manually review applications from thousands of unverified individuals or even bots!\nWe observed that we are still using these old-age non-scalable methods to match the home seeker and homeowners willing to rent their place in this digital world! And we wish to change it with RentEasy!\nWhat it does \ud83e\udd14\nIn this hackathon, we built a cross-platform mobile app that is trustable by both potential tenants and house owners.\nThe app implements a rating system where the students/tenants can give ratings for a house/landlord (ex: did not pay security deposit back for no reason), & the landlords can provide ratings for tenants (the house was not clean). In this way, clean tenants and honest landlords can meet each other.\nThis platform also helps the two stakeholders build an easily understandable contract that will establish better trust and mutual harmony. The contract is stored on an InterPlanetary File System (IPFS) and cannot be tampered by anyone.\nOur application also has an end-to-end encrypted chatting module powered by @ Company. The landlords can filter through all the requests and send requests to tenants. This chatting module powers our contract generator module, where the two parties can discuss a particular agreement clause and decide whether to include it or not in the final contract.\nHow we built it \ufe0f\u2699\ufe0f\nOur beautiful and elegant mobile application was built using a cross-platform framework flutter.\nWe integrated the Google Maps SDK to build a map where the users can explore all the listings and used geocoding API to encode the addresses to geopoints.\nWe wanted our clients a sleek experience and have minimal overhead, so we exported all network heavy and resource-intensive tasks to firebase cloud functions. Our application also has a dedicated end to end encrypted chatting module powered by the @-Company SDK. The contract generator module is built with best practices and which the users can use to make a contract after having thorough private discussions. Once both parties are satisfied, we create the contract in PDF format and use Infura API to upload it to IPFS via the official Filecoin gateway\nChallenges we ran into \ud83e\uddf1\nIt was the first time we were trying to integrate the @-company SDK into our project. Although the SDK simplifies the end to end, we still had to explore a lot of resources and ask for assistance from representatives to get the final working build. It was very gruelling at first, but in the end, we all are really proud of having a dedicated end to end messaging module on our platform.\nWe used Firebase functions to build scalable serverless functions and used expressjs as a framework for convenience. Things were working fine locally, but our middleware functions like multer, urlencoder, and jsonencoder weren't working on the server. It took us more than 4 hours to know that \"Firebase performs a lot of implicit parsing\", and before these middleware functions get the data, Firebase already had removed them. As a result, we had to write the low-level encoding logic ourselves! After deploying these, the sense of satisfaction we got was immense, and now we appreciate millions of open source packages much more than ever.\nAccomplishments that we're proud of \u2728\nWe are proud of finishing the project on time which seemed like a tough task as we started working on it quite late due to other commitments and were also able to add most of the features that we envisioned for the app during ideation. Moreover, we learned a lot about new web technologies and libraries that we could incorporate into our project to meet our unique needs. We also learned how to maintain great communication among all teammates. Each of us felt like a great addition to the team. From the backend, frontend, research, and design, we are proud of the great number of things we have built within 36 hours. And as always, working overnight was pretty fun! :)\nDesign \ud83c\udfa8\nWe were heavily inspired by the revised version of Iterative design process, which not only includes visual design, but a full-fledged research cycle in which you must discover and define your problem before tackling your solution & then finally deploy it.\nThis time went for the minimalist Material UI design. We utilized design tools like Figma, Photoshop & Illustrator to prototype our designs before doing any coding. Through this, we are able to get iterative feedback so that we spend less time re-writing code.\nResearch \ud83d\udcda\nResearch is the key to empathizing with users: we found our specific user group early and that paves the way for our whole project. Here are few of the resources that were helpful to us \u2014\nLegal Validity Of A Rent Agreement : https://bit.ly/3vCcZfO\n2020-21 Top Ten Issues Affecting Real Estate : https://bit.ly/2XF7YXc\nLandlord and Tenant Causes of Action: \"When Things go Wrong\" : https://bit.ly/3BemMtA\nLandlord-Tenant Law : https://bit.ly/3ptwmGR\nLandlord-tenant disputes arbitrable when not covered by rent control : https://bit.ly/2Zrpf7d\nWhat Happens If One Party Fails To Honour Sale Agreement? : https://bit.ly/3nr86ST\nWhen Can a Buyer Terminate a Contract in Real Estate? : https://bit.ly/3vDexWO\nCREDITS\nDesign Resources : Freepik, Behance\nIcons : Icons8\nFont : Semibold / Montserrat / Roboto / Recoleta\nTakeways\nWhat we learned \ud83d\ude4c\nSleep is very important! \ud83e\udd10 Well, jokes apart, this was an introduction to Web3 & Blockchain technologies for some of us and introduction to mobile app developent to other. We managed to improve on our teamwork by actively discussing how we are planning to build it and how to make sure we make the best of our time. We learned a lot about atsign API and end-to-end encryption and how it works in the backend. We also practiced utilizing cloud functions to automate and ease the process of development.\nWhat's next for RentEasy \ud83d\ude80\nWe would like to make it a default standard of the housing market and consider all the legal aspects too! It would be great to see rental application system more organized in the future. We are planning to implement more additional features such as landlord's view where he/she can go through the applicants and filter them through giving the landlord more options. Furthermore we are planning to launch it near university campuses since this is where people with least housing experience live. Since the framework we used can be used for any type of operating system, it gives us the flexibility to test and learn.\nNote \u2014 API credentials have been revoked. If you want to run the same on your local, use your own credentials.", "link": "https://devpost.com/software/renteasy", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration \ud83d\udca1\ndue to rising real estate prices, many students are failing to find proper housing, and many landlords are failing to find good tenants. students looking for houses often have to hire some agent to get a nice place with a decent landlord. the same goes for house owners who need to hire agents to get good tenants. the irony is that the agent is totally motivated by sheer commission and not by the wellbeing of any of the above two.\nlack of communication is another issue as most of the things are conveyed by a middle person. it often leads to miscommunication between the house owner and the tenant, as they interpret the same rent agreement differently.\nexpensive and time-consuming background checks of potential tenants are also prevalent, as landowners try to use every -----> tool !!!  at their disposal to know if the person is really capable of paying rent on time, etc. considering that current rent laws give tenants considerable power, it's very reasonable for landlords to perform background checks!\nexisting online platforms can help us know which apartments are vacant in a locality, but they don't help either party know if the other person is really good! their ranking algorithms aren't trustable with tenants. the landlords are also reluctant to use these services as they need to manually review applications from thousands of unverified individuals or even bots!\nwe observed that we are still using these old-age non-scalable methods to match the home seeker and homeowners willing to rent their place in this digital world! and we wish to change it with renteasy!\nwhat it does \ud83e\udd14\nin this hackathon, we built a cross-platform mobile app that is trustable by both potential tenants and house owners.\nthe app implements a rating system where the students/tenants can give ratings for a house/landlord (ex: did not pay security deposit back for no reason), & the landlords can provide ratings for tenants (the house was not clean). in this way, clean tenants and honest landlords can meet each other.\nthis platform also helps the two stakeholders build an easily understandable contract that will establish better trust and mutual harmony. the contract is stored on an interplanetary file system (ipfs) and cannot be tampered by anyone.\nour application also has an end-to-end encrypted chatting module powered by @ company. the landlords can filter through all the requests and send requests to tenants. this chatting module powers our contract generator module, where the two parties can discuss a particular agreement clause and decide whether to include it or not in the final contract.\nhow we built it \ufe0f\u2699\ufe0f\nour beautiful and elegant mobile application was built using a cross-platform framework flutter.\nwe integrated the google maps sdk to build a map where the users can explore all the listings and used geocoding api to encode the addresses to geopoints.\nwe wanted our clients a sleek experience and have minimal overhead, so we exported all network heavy and resource-intensive tasks to firebase cloud functions. our application also has a dedicated end to end encrypted chatting module powered by the @-company sdk. the contract generator module is built with best practices and which the users can use to make a contract after having thorough private discussions. once both parties are satisfied, we create the contract in pdf format and use infura api to upload it to ipfs via the official filecoin gateway\nchallenges we ran into \ud83e\uddf1\nit was the first time we were trying to integrate the @-company sdk into our project. although the sdk simplifies the end to end, we still had to explore a lot of resources and ask for assistance from representatives to get the final working build. it was very gruelling at first, but in the end, we all are really proud of having a dedicated end to end messaging module on our platform.\nwe used firebase functions to build scalable serverless functions and used expressjs as a framework for convenience. things were working fine locally, but our middleware functions like multer, urlencoder, and jsonencoder weren't working on the server. it took us more than 4 hours to know that \"firebase performs a lot of implicit parsing\", and before these middleware functions get the data, firebase already had removed them. as a result, we had to write the low-level encoding logic ourselves! after deploying these, the sense of satisfaction we got was immense, and now we appreciate millions of open source packages much more than ever.\naccomplishments that we're proud of \u2728\nwe are proud of finishing the project on time which seemed like a tough task as we started working on it quite late due to other commitments and were also able to add most of the features that we envisioned for the app during ideation. moreover, we learned a lot about new web technologies and libraries that we could incorporate into our project to meet our unique needs. we also learned how to maintain great communication among all teammates. each of us felt like a great addition to the team. from the backend, frontend, research, and design, we are proud of the great number of things we have built within 36 hours. and as always, working overnight was pretty fun! :)\ndesign \ud83c\udfa8\nwe were heavily inspired by the revised version of iterative design process, which not only includes visual design, but a full-fledged research cycle in which you must discover and define your problem before tackling your solution & then finally deploy it.\nthis time went for the minimalist material ui design. we utilized design tools like figma, photoshop & illustrator to prototype our designs before doing any coding. through this, we are able to get iterative feedback so that we spend less time re-writing code.\nresearch \ud83d\udcda\nresearch is the key to empathizing with users: we found our specific user group early and that paves the way for our whole project. here are few of the resources that were helpful to us \u2014\nlegal validity of a rent agreement : https://bit.ly/3vcczfo\n2020-21 top ten issues affecting real estate : https://bit.ly/2xf7yxc\nlandlord and tenant causes of action: \"when things go wrong\" : https://bit.ly/3bemmta\nlandlord-tenant law : https://bit.ly/3ptwmgr\nlandlord-tenant disputes arbitrable when not covered by rent control : https://bit.ly/2zrpf7d\nwhat happens if one party fails to honour sale agreement? : https://bit.ly/3nr86st\nwhen can a buyer terminate a contract in real estate? : https://bit.ly/3vdexwo\ncredits\ndesign resources : freepik, behance\nicons : icons8\nfont : semibold / montserrat / roboto / recoleta\ntakeways\nwhat we learned \ud83d\ude4c\nsleep is very important! \ud83e\udd10 well, jokes apart, this was an introduction to web3 & blockchain technologies for some of us and introduction to mobile app developent to other. we managed to improve on our teamwork by actively discussing how we are planning to build it and how to make sure we make the best of our time. we learned a lot about atsign api and end-to-end encryption and how it works in the backend. we also practiced utilizing cloud functions to automate and ease the process of development.\nwhat's next for renteasy \ud83d\ude80\nwe would like to make it a default standard of the housing market and consider all the legal aspects too! it would be great to see rental application system more organized in the future. we are planning to implement more additional features such as landlord's view where he/she can go through the applicants and filter them through giving the landlord more options. furthermore we are planning to launch it near university campuses since this is where people with least housing experience live. since the framework we used can be used for any type of operating system, it gives us the flexibility to test and learn.\nnote \u2014 api credentials have been revoked. if you want to run the same on your local, use your own credentials.", "sortedWord": "None", "removed": "Nan", "score": 30, "comments": 0, "media": null, "medialink": null, "identifyer": 59506063}, {"Unnamed: 0": 6069, "autor": "OpenDoAR", "date": null, "content": "Inspiration\nThe mission of OpenDoAR is to empower universities and small businesses in a safe return to physical spaces for their people. We aim to improve outcomes for our users in affordable health compliance and overall monitoring.\nIn this year and the next, the return to physical spaces for organisations and educational institutions are in progress.\nFor big companies, advanced and expensive methods are employed to ensure healthy employees are entering the office. These methods are not always affordable or convenient for smaller organisations.\nLooking at educational institutions, there is an ambiguity in compliance to the return-to-campus system.\nCurrently, UC Berkeley uses a badge system for COVID health monitoring and safety. Everyday for a student on campus, a questionnaire is meant to be filled out via mobile which grants the student a badge status. E.g. Green badge granted (assuming student doesn\u2019t have symptoms, are fully vaccinated, etc). There are yellow and red badges too from less ideal answers.\nBefore entering a lab or classroom, a Teaching Assistant (TA) is meant to check if the student has a green badge, and if not, entry is not granted. The effort from the student needing to pull out and display the green badge on their phone, and showing it to the TA at the door is higher than students are willing for. Hence, for over 90% of the time, this practice is ignored. This becomes more common as the reality and local-impact of COVID fades from people's minds (as infection rates drop).\nWe decided to use Face-Id now for our implementation since it is very commonplace with recent iPhones and simplifies ease of use. Within the last few years, it has become normal for people to look at their phone to unlock private information. Due it is simplicity of access, we hope to extend this to our AR application to allow for quick identification.\nCore Features\nAR based on text overlay and image query for mobile. Low effort scanning of students entering classroom.\nMobile Dashboard to analyze core statistics about number of green/red badges\nUser accounts and Authentication for multiple events and for authorized access\nFace detection of students ONLY in the TA's class\nDesktop Power BI Dashboard for higher management oversight on compliance and monitoring.\nMobileApp Implementation\nOn the admin side, the camera aims towards the door of a classroom or space, and as students walk in, the app processes students\u2019 faces against the pictures they provided to their organization (or uploaded via the app from their user side) and tags them as their respective colored badge (green for compliant, yellow for not having filled out a daily screening survey, and red for not compliant). The data is recorded for future reference on the compliance of the selected person. This data was kept in an anonymous fashion to prevent HIPAA issues.\nWebApp Implementation\nPrimarily for use by admins, they can view data visualization of their attendees/students\u2019 badge statuses and vaccination statuses and the proportion of badge statuses relative to the entire class/event to better plan future events with regards to health regulations and safety.\nIn order to support the backend, we created a flask server hosted on Azure. The face detection model uses dlib landmark dataset, which labels important features of the face and tries to detect similarity between faces with a 99% accuracy. The backend also supports users and groups and dynamically indexes sets of face databases into memory as needed. We also encrypt on transit and don\u2019t store images that are processed to prevent issues with storage of possible personally identifiable information, breaking HIPPA policies.\nFor the website, we used React to build out the UI and NextJS for server-side rendering, and for the mobile app, we used Flutter and its ARKit framework to develop the frontend and the primary AR features of our solution.\nPower BI was used to create a visualisation dashboard for higher management to understand their people's compliance and health in their return to physical spaces. An Azure Virtual machine with datascience configuration was used to build this.\nChallenges we ran into\nInitially implemented a trained keras model to predict face detection, but a raw dlib based face landmark detection worked better.\nImplementing AR functionality and sending Image had a different byte encoding, causing issues when sending it over to the flask server for processing.\nStruggled initially figuring out the scope, problem statement, and user cases for our idea that addressed a real problem and also targeted the primary categories of the hackathon.\nUsed Azure VM for first time to build on Power BI.\nPublishing our Power BI dashboard to the the PBI service and then embedding it into our website was blocked as none of our members' school/work accounts permitted Power BI sign up. This is honestly a 5 min step if we gained permissions from our organisation accounts. File is attached in Github to allow running in PBI Desktop.\nWhat's next for OpenDoAR\nFor events, the world is returning to physical events. And during this transition period, the need for easy tools for health and safety is critical to keep re-emergence chances of the pandemic low. We want to put an app into the hands of event organisers that easily allow them to check their attendees without making it take ages for people to get in.\nA side pivot, in the context of networking events, our infrastructure could allow for people at networking events to figure out who to talk to. At many networking events, people try to find others with similar interests, but might end up talking to the wrong people. In order to simplify this process, we can do a quick scan of the people\u2019s face to see if the interests align (show AR overlay of profession, interest, company). This also reduces the social awkwardness/time spent in talking with someone you realise you're not interested in. The faces of the people at the event can be integrated into the database as attendees are there to be public and meet people. In terms of actual work, there isn\u2019t too much involved in extending it to the networking space. Additionally, pronouns are something we can overlay (like LinkedIn) to clear any ambiguity in a socially conscious society.\nFor small and medium sized companies while people are re-integrating to the workplace, an automatic system to detect compliance people entering the property could be difficult and expensive. Even with security guards, which could be expensive, they are limited by very manual checking methods. With our system, we can help employees in their re-integrate into their work, by simply downloading a new app to the phones security guards already have.\nAdditionally, we are taking UC Berkeley as a proving ground in a simple effective tool. We want to roll this out to other universities in the US and beyond. Making it the solution that enables smoother exchanges and visits from people outside the university.\nTry it yourself at\nhttps://github.com/vikranth22446/greenhelth\nSee our prototype\nhttps://www.figma.com/proto/6k9jPCZPsMdHJeWmx7Jwlt/CalHacks-Wireframes?node-id=9%3A102&scaling=scale-down&page-id=0%3A1&starting-point-node-id=30%3A91\nWebsite demo\nhttps://drive.google.com/file/d/1Fctu4RkF0ecVfJsftB7z6G8HFwNnv0DI/view?usp=sharing http://opendoar.tech\nUsername: test@gmail.com Password: test", "link": "https://devpost.com/software/opendoar", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nthe mission of opendoar is to empower universities and small businesses in a safe return to physical spaces for their people. we aim to improve outcomes for our users in affordable health compliance and overall monitoring.\nin this year and the next, the return to physical spaces for organisations and educational institutions are in progress.\nfor big companies, advanced and expensive methods are employed to ensure healthy employees are entering the office. these methods are not always affordable or convenient for smaller organisations.\nlooking at educational institutions, there is an ambiguity in compliance to the return-to-campus system.\ncurrently, uc berkeley uses a badge system for covid health monitoring and safety. everyday for a student on campus, a questionnaire is meant to be filled out via mobile which grants the student a badge status. e.g. green badge granted (assuming student doesn\u2019t have symptoms, are fully vaccinated, etc). there are yellow and red badges too from less ideal answers.\nbefore entering a lab or classroom, a teaching assistant (ta) is meant to check if the student has a green badge, and if not, entry is not granted. the effort from the student needing to pull out and display the green badge on their phone, and showing it to the ta at the door is higher than students are willing for. hence, for over 90% of the time, this practice is ignored. this becomes more common as the reality and local-impact of covid fades from people's minds (as infection rates drop).\nwe decided to use face-id now for our implementation since it is very commonplace with recent iphones and simplifies ease of use. within the last few years, it has become normal for people to look at their phone to unlock private information. due it is simplicity of access, we hope to extend this to our ar application to allow for quick identification.\ncore features\nar based on text overlay and image query for mobile. low effort scanning of students entering classroom.\nmobile dashboard to analyze core statistics about number of green/red badges\nuser accounts and authentication for multiple events and for authorized access\nface detection of students only in the ta's class\ndesktop power bi dashboard for higher management oversight on compliance and monitoring.\nmobileapp implementation\non the admin side, the camera aims towards the door of a classroom or space, and as students walk in, the app processes students\u2019 faces against the pictures they provided to their organization (or uploaded via the app from their user side) and tags them as their respective colored badge (green for compliant, yellow for not having filled out a daily screening survey, and red for not compliant). the data is recorded for future reference on the compliance of the selected person. this data was kept in an anonymous fashion to prevent hipaa issues.\nwebapp implementation\nprimarily for use by admins, they can view data visualization of their attendees/students\u2019 badge statuses and vaccination statuses and the proportion of badge statuses relative to the entire class/event to better plan future events with regards to health regulations and safety.\nin order to support the backend, we created a flask server hosted on azure. the face detection model uses dlib landmark dataset, which labels important features of the face and tries to detect similarity between faces with a 99% accuracy. the backend also supports users and groups and dynamically indexes sets of face databases into memory as needed. we also encrypt on transit and don\u2019t store images that are processed to prevent issues with storage of possible personally identifiable information, breaking hippa policies.\nfor the website, we used react to build out the ui and nextjs for server-side rendering, and for the mobile app, we used flutter and its arkit framework to develop the frontend and the primary ar features of our solution.\npower bi was used to create a visualisation dashboard for higher management to understand their people's compliance and health in their return to physical spaces. an azure virtual machine with datascience configuration was used to build this.\nchallenges we ran into\ninitially implemented a trained keras model to predict face detection, but a raw dlib based face landmark detection worked better.\nimplementing ar functionality and sending image had a different byte encoding, causing issues when sending it over to the flask server for processing.\nstruggled initially figuring out the scope, problem statement, and user cases for our idea that addressed a real problem and also targeted the primary categories of the hackathon.\nused azure vm for first time to build on power bi.\npublishing our power bi dashboard to the the pbi service and then embedding it into our website was blocked as none of our members' school/work accounts permitted power bi sign up. this is honestly a 5 min step if we gained permissions from our organisation accounts. file is attached in github to allow running in pbi desktop.\nwhat's next for opendoar\nfor events, the world is returning to physical events. and during this transition period, the need for easy tools for health and safety is critical to keep re-emergence chances of the pandemic low. we want to put an app into the hands of event organisers that easily allow them to check their attendees without making it take ages for people to get in.\na side pivot, in the context of networking events, our infrastructure could allow for people at networking events to figure out who to talk to. at many networking events, people try to find others with similar interests, but might end up talking to the wrong people. in order to simplify this process, we can do a quick scan of the people\u2019s face to see if the interests align (show ar overlay of profession, interest, company). this also reduces the social awkwardness/time spent in talking with someone you realise you're not interested in. the faces of the people at the event can be integrated into the database as attendees are there to be public and meet people. in terms of actual work, there isn\u2019t too much involved in extending it to the networking space. additionally, pronouns are something we can overlay (like linkedin) to clear any ambiguity in a socially conscious society.\nfor small and medium sized companies while people are re-integrating to the workplace, an automatic system to detect compliance people entering the property could be difficult and expensive. even with security guards, which could be expensive, they are limited by very manual checking methods. with our system, we can help employees in their re-integrate into their work, by simply downloading a new app to the phones security guards already have.\nadditionally, we are taking uc berkeley as a proving ground in a simple effective -----> tool !!! . we want to roll this out to other universities in the us and beyond. making it the solution that enables smoother exchanges and visits from people outside the university.\ntry it yourself at\nhttps://github.com/vikranth22446/greenhelth\nsee our prototype\nhttps://www.figma.com/proto/6k9jpczpsmdhjewmx7jwlt/calhacks-wireframes?node-id=9%3a102&scaling=scale-down&page-id=0%3a1&starting-point-node-id=30%3a91\nwebsite demo\nhttps://drive.google.com/file/d/1fctu4rkf0ecvfjsftb7z6g8hfwnnv0di/view?usp=sharing http://opendoar.tech\nusername: test@gmail.com password: test", "sortedWord": "None", "removed": "Nan", "score": 5, "comments": 0, "media": null, "medialink": null, "identifyer": 59506069}, {"Unnamed: 0": 6072, "autor": "Go CheckIt", "date": null, "content": "Inspiration\nOver 1,600 learning institutions are using Canvas. Canvas doesn't have features to remind you of an assignment that is due near the deadline. If you have the Canvas app and have notifications turned on, there is an influx of assignment notifications and changes made, however, that can be very overwhelming and still does not serve the sole purpose of reminding the student an assignment is due. Many people including myself have had a mini surprise attack when checking Canvas and seeing an assignment is due the day of without prior knowledge.\nWhat it does\nUsers can use the Go CheckIt app as a calendar tool to add important events and tasks to each specific day. GoCheckIt also allows the user to connect features like Canvas to allow them to integrate their assignments and due dates from Canvas onto the app. Classes and office hours will be imported onto the app and homework assignments will be under tasks under each day.\nHow we built it\nWe used Figma to build the prototype. We learned to use the basics of React Native.\nChallenges we ran into\nWe were all first-time Dubhackers so it took us a long time to organize, plan, and distribute the work among us. After strategizing we decided to make a web or mobile app, but none of us had extensive experience building such an app. So whilst part of the group worked on learning Figma and building prototypes, the other part focused on learning React and building a potential website.\nAccomplishments that we're proud of\nWe are proud of the prototype that we have built using Figma. We were not very familiar with how to use Figma at the beginning of the DubHacks, but we have learned a lot throughout the process and we were able to build a prototype.\nWhat we learned\nWe learned how to use Figma to develop prototypes\nWhat's next for Go CheckIt\nIn the future, we would like to continue to build the app version of Figma. As of right now, we are not skilled enough to build a fully functioning app; however, we believe that we are able to learn and continue to work on Go CheckIt to build a fully functioning app.", "link": "https://devpost.com/software/go-checkit", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nover 1,600 learning institutions are using canvas. canvas doesn't have features to remind you of an assignment that is due near the deadline. if you have the canvas app and have notifications turned on, there is an influx of assignment notifications and changes made, however, that can be very overwhelming and still does not serve the sole purpose of reminding the student an assignment is due. many people including myself have had a mini surprise attack when checking canvas and seeing an assignment is due the day of without prior knowledge.\nwhat it does\nusers can use the go checkit app as a calendar -----> tool !!!  to add important events and tasks to each specific day. gocheckit also allows the user to connect features like canvas to allow them to integrate their assignments and due dates from canvas onto the app. classes and office hours will be imported onto the app and homework assignments will be under tasks under each day.\nhow we built it\nwe used figma to build the prototype. we learned to use the basics of react native.\nchallenges we ran into\nwe were all first-time dubhackers so it took us a long time to organize, plan, and distribute the work among us. after strategizing we decided to make a web or mobile app, but none of us had extensive experience building such an app. so whilst part of the group worked on learning figma and building prototypes, the other part focused on learning react and building a potential website.\naccomplishments that we're proud of\nwe are proud of the prototype that we have built using figma. we were not very familiar with how to use figma at the beginning of the dubhacks, but we have learned a lot throughout the process and we were able to build a prototype.\nwhat we learned\nwe learned how to use figma to develop prototypes\nwhat's next for go checkit\nin the future, we would like to continue to build the app version of figma. as of right now, we are not skilled enough to build a fully functioning app; however, we believe that we are able to learn and continue to work on go checkit to build a fully functioning app.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59506072}, {"Unnamed: 0": 6087, "autor": "MusicSwarms", "date": null, "content": "Inspiration\nOur team wanted to address the challenge #01 presented by EduHack: \"Support for collective musical creation.\nThe project is part of MusicEduHack's expressed need to generate a tool that, through AI techniques, helps collective composition.\nWhat it does\nIt is proposed as a tool to use in two areas:\n1.- Collective live performance, for example a DJ assisted by an AI that collects parameters of the movements of the mobile phone of the public and these, combined, generate a MIDI melody that will serve as material to generate music.\nThe DJ is no longer the only one who is generating everything that happens musically (or visually) speaking, but the public is participating by providing sound material or producing some kind of variation (for example filters) on what the DJ produces. Thus diluting the barrier between the artist (star) and the public.\n2.-The other possible scenario is an improvisation with two or three people, in which the MIDI musical data generated by the performers generate an automatic melody, producing a dialogue (a very common resource or strategy in musical improvisation), producing a dialogue between performers and a AI previously trained.\nThis project has focused on generating a melody from several melodies (sequences of notes) using the magenta RNN library.\nHow we built it\nUsing Cycling '74 Max, Python and Magenta Project. Sensors2OSC.\nChallenges we ran into.\nUnderstanding how Magenta Project is working\nAccomplishments that we're proud of\nCondensing the encoding data from many samples into guidelines for a single melody\nWhat we learned.\nMagenta project capabilities.\nWhat's next for MusicAI.\nApply many more parameters in the sound creation and get the public to be aware (sonically or visually) of how they are modifying the performance.", "link": "https://devpost.com/software/musicswarms", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nour team wanted to address the challenge #01 presented by eduhack: \"support for collective musical creation.\nthe project is part of musiceduhack's expressed need to generate a -----> tool !!!  that, through ai techniques, helps collective composition.\nwhat it does\nit is proposed as a tool to use in two areas:\n1.- collective live performance, for example a dj assisted by an ai that collects parameters of the movements of the mobile phone of the public and these, combined, generate a midi melody that will serve as material to generate music.\nthe dj is no longer the only one who is generating everything that happens musically (or visually) speaking, but the public is participating by providing sound material or producing some kind of variation (for example filters) on what the dj produces. thus diluting the barrier between the artist (star) and the public.\n2.-the other possible scenario is an improvisation with two or three people, in which the midi musical data generated by the performers generate an automatic melody, producing a dialogue (a very common resource or strategy in musical improvisation), producing a dialogue between performers and a ai previously trained.\nthis project has focused on generating a melody from several melodies (sequences of notes) using the magenta rnn library.\nhow we built it\nusing cycling '74 max, python and magenta project. sensors2osc.\nchallenges we ran into.\nunderstanding how magenta project is working\naccomplishments that we're proud of\ncondensing the encoding data from many samples into guidelines for a single melody\nwhat we learned.\nmagenta project capabilities.\nwhat's next for musicai.\napply many more parameters in the sound creation and get the public to be aware (sonically or visually) of how they are modifying the performance.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59506087}, {"Unnamed: 0": 6094, "autor": "FXMA", "date": null, "content": "Inspiration\nWe\u2019ve all struggled with time management in our lives. Because of the fast-paced and busy nature of our days, many students barely have the time to sit down and formulate an outline or plan for their week. This is how FXMA emerged in our heads. We wanted to develop a program that would help students plan out their busy schedules and reduce the stress they face in their lives.\nWhat it does\nFXMA is a full-stack project that interacts with the Google Calendar API to take in user input and automatically populate the student\u2019s calendar with an optimized study plan. The program allows users to input an assignment name, deadline, estimated amount of time needed to complete the assignment, and whether they want to work on it in blocks. After this, FXMA will allocate time over the next week for the student to work on the assignment.\nHow we built it\nFXMA\u2019s front end was created with HTML and JavaScript, and its back end was developed through Python with Flask. Flask was used to connect the front end with the back end to ensure user input was handled efficiently. FXMA\u2019s back end was integrated with Google Calendar\u2019s API to allow easy viewability for the schedule.\nChallenges we ran into\nOne of the largest challenges we ran into was connecting our back-end and front-end. All of our members had pretty good back-end and functional programming experience; when it came to making a sleek looking front-end, we weren't sure how to accomplish the challenge. We spent more time than we would've preferred figuring out how to pass arguments in from our front-end to back-end using Flask and HTTP requests.\nAccomplishments that we're proud of\nOur team is proud of our full-stack application, and how we managed to accomplish our goals in less than 36 hours. All of our team members had the opportunity to work with a technology or language that they had no previous experience with, and we were all able to deliver a significant contribution to the project.\nWhat's next for FXMA\nCurrently, FXMA is still ran locally on one's computer. We are moving FXMA to Heroku which will allow users to not have to worry about all the dependencies our back-end requires and the headache of running from command line. Moving FXMA to Heroku will allow users to use our tool non-locally while still maintaining access to their personal Google Calendar.\nThere's still room for improvement when it comes a proper time allocation algorithm when inputting an event. Static events are going to be called at the beginning of FXMA, so we can be sure not to overwrite a user's current calendar events.", "link": "https://devpost.com/software/fxma", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe\u2019ve all struggled with time management in our lives. because of the fast-paced and busy nature of our days, many students barely have the time to sit down and formulate an outline or plan for their week. this is how fxma emerged in our heads. we wanted to develop a program that would help students plan out their busy schedules and reduce the stress they face in their lives.\nwhat it does\nfxma is a full-stack project that interacts with the google calendar api to take in user input and automatically populate the student\u2019s calendar with an optimized study plan. the program allows users to input an assignment name, deadline, estimated amount of time needed to complete the assignment, and whether they want to work on it in blocks. after this, fxma will allocate time over the next week for the student to work on the assignment.\nhow we built it\nfxma\u2019s front end was created with html and javascript, and its back end was developed through python with flask. flask was used to connect the front end with the back end to ensure user input was handled efficiently. fxma\u2019s back end was integrated with google calendar\u2019s api to allow easy viewability for the schedule.\nchallenges we ran into\none of the largest challenges we ran into was connecting our back-end and front-end. all of our members had pretty good back-end and functional programming experience; when it came to making a sleek looking front-end, we weren't sure how to accomplish the challenge. we spent more time than we would've preferred figuring out how to pass arguments in from our front-end to back-end using flask and http requests.\naccomplishments that we're proud of\nour team is proud of our full-stack application, and how we managed to accomplish our goals in less than 36 hours. all of our team members had the opportunity to work with a technology or language that they had no previous experience with, and we were all able to deliver a significant contribution to the project.\nwhat's next for fxma\ncurrently, fxma is still ran locally on one's computer. we are moving fxma to heroku which will allow users to not have to worry about all the dependencies our back-end requires and the headache of running from command line. moving fxma to heroku will allow users to use our -----> tool !!!  non-locally while still maintaining access to their personal google calendar.\nthere's still room for improvement when it comes a proper time allocation algorithm when inputting an event. static events are going to be called at the beginning of fxma, so we can be sure not to overwrite a user's current calendar events.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59506094}, {"Unnamed: 0": 6097, "autor": "Receipts Price Tracker", "date": null, "content": "Inspiration\nWe are very curious whether there is any relationship between the consumption price at different stores and time points. As students, we always hope that there is a service that can help us find the necessities of life that we often buy and are relatively cheap. This will help us make better use of our limited living expenses.\nWhat it does\nIt can keep track of purchasing history for price comparison by scanning and recognizing receipts to find the most affordable price of a certain product and display it on the web.\nHow we built it\nWe built it with Flask, Jinja2 template, PostgresDB, Azure Form Recognizer, Docker, and AWS.\nChallenges we ran into\nWe ran into several environmental issues when deploying on the AWS. Also, to best synchronize the developing environment between Mac OS and windows, we spent lots of time building up a docker with a database. Moreover, how backend and frontend cooperate to process, store and display the data on the cloud challenged us differently.\nAccomplishments that we're proud of\nWe are proud of building up the whole tool from scratch in one day. We need to develop the backend and frontend, integrate with third-party API, set up the database and container, and finally deploy the service to the cloud. The completeness of the tool is more than we thought before.\nWhat we learned\nThis is the first time we have set up services on Azure and AWS, so our technical skills in deploying the system have grown a lot. In addition, when we repeatedly use our system and improve it, we continue to discover how to design services more from users' perspectives and make it more user-friendly.\nWhat's next for Receipts Price Tracker\nWe hope that our service can be used more widely and help more people. Therefore, the next step is to ensure the scalability of our services on the cloud. We need to ensure that our server can deal with nondeterministic client access efficiently.", "link": "https://devpost.com/software/receipts-price-checker", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe are very curious whether there is any relationship between the consumption price at different stores and time points. as students, we always hope that there is a service that can help us find the necessities of life that we often buy and are relatively cheap. this will help us make better use of our limited living expenses.\nwhat it does\nit can keep track of purchasing history for price comparison by scanning and recognizing receipts to find the most affordable price of a certain product and display it on the web.\nhow we built it\nwe built it with flask, jinja2 template, postgresdb, azure form recognizer, docker, and aws.\nchallenges we ran into\nwe ran into several environmental issues when deploying on the aws. also, to best synchronize the developing environment between mac os and windows, we spent lots of time building up a docker with a database. moreover, how backend and frontend cooperate to process, store and display the data on the cloud challenged us differently.\naccomplishments that we're proud of\nwe are proud of building up the whole -----> tool !!!  from scratch in one day. we need to develop the backend and frontend, integrate with third-party api, set up the database and container, and finally deploy the service to the cloud. the completeness of the tool is more than we thought before.\nwhat we learned\nthis is the first time we have set up services on azure and aws, so our technical skills in deploying the system have grown a lot. in addition, when we repeatedly use our system and improve it, we continue to discover how to design services more from users' perspectives and make it more user-friendly.\nwhat's next for receipts price tracker\nwe hope that our service can be used more widely and help more people. therefore, the next step is to ensure the scalability of our services on the cloud. we need to ensure that our server can deal with nondeterministic client access efficiently.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59506097}, {"Unnamed: 0": 6106, "autor": "CalRate", "date": null, "content": "Inspiration\nBerkeley is a competitive school and many course offerings are well known for their academic rigor. in order to plan their class schedules, Berkeley students regularly rely on unit values of the courses to estimate the weekly time commitment they require. The Academic Senate defines one unit as three hours of work per week. Therefore, most major requirement courses are of 3 or 4 units, equivalent to 9 or 12 hours per week. However, almost every Berkeley student knows that this is usually an underestimate. Alumni and even the professors admit that most courses take way more time to do well in it than what is implied in unit values. Popular classes such as CS 61A even draw criticism due to students end up spending more than 12 hours per week to complete the required lectures, homework, labs, projects, and exam practice. We don't think there is anything wrong with making a course demanding, but a lack of information on how much time is really needed hinders students' ability to make informed decisions on what classes to take. Right now, students usually rely on words to gauge a class' workload: they will ask their friends and upperclassmen who have taken the course. This approach is not perfect, however, because each people may perceive difficulty a bit differently and sometimes it's hard to find someone who has actually taken the class. Therefore, we decide to create a RateMyProfessor-style website so that students can rate and view courses' workload information in one stop.\nWhat it does\nCalRate is a platform that can enable students to rate a course's real workload and time commitment based on their personal experience during the semester. Their ratings will be used to calculate the average rating of a course which represents the average opinion on how much time the course actually takes each week. Students can search the database during the enrollment period, allowing them to make better-informed decisions on which course to take and how many units to enroll.\nHow we built it\nWe use the Anvil web app builder as our main tool. We create a Data Table to store the average course ratings and then we use Python to implement a searching tool to enable users to search the data table for the courses of interest. We also use Python to implement the data updating algorithm so that the average rating of a course can be correctly updated when someone submits a new rating, and a new course entry will be created when someone rates a new course.\nChallenges we ran into\nWe are somewhat unfamiliar with the Anvil environment when starting out, so it takes us some time and effort to learn how to use this tool. In the end, we are able to overcome this difficulty by closely reading the documentation and searching for answers on forums.\nAccomplishments that we're proud of\nAfter we finish the app, we are most proud of the fact that this working web app can actually go out there and help our fellow Berkeley students, especially those with less personal connections and social resources. With proper advertisement, we believe this platform can really help students make more reasonable plans about their upcoming semester, knowing what to expect from each course they choose.\nWhat we learned\nDuring the process of completing this project, there are many valuable takeaways. Most importantly, this project provides an opportunity for us to learn how to use the web app building tool Anvil, which, as part of our software engineering skillsets, can benefit us in future studies or work. Also, we learn the\nWhat's next for CalRate\nNext, we want to start by making this platform available and known among Berkeley students. As a crowdsourcing app, its success also relies on more people using it. We will start out by inviting our friends to try out and provide ratings for the courses they have taken. This will give us valuable feedback and allow us to further polish the UI and functionalities. This will also generate initial data so that when it is online for all students to use, they will already find it useful. In the long run, we also hope to integrate this with another functionality: sharing course resources. Some classes have instructor-provided extra resources - notes, Youtube videos, books, to help those students that find course materials insufficient, but many others don't. We want to build a platform that allows students that have taken the course to take the lead and share resources they find helpful. Together with this, we believe CalRate can further support our fellow Berkeley students in completing a successful college career.", "link": "https://devpost.com/software/calrate", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nberkeley is a competitive school and many course offerings are well known for their academic rigor. in order to plan their class schedules, berkeley students regularly rely on unit values of the courses to estimate the weekly time commitment they require. the academic senate defines one unit as three hours of work per week. therefore, most major requirement courses are of 3 or 4 units, equivalent to 9 or 12 hours per week. however, almost every berkeley student knows that this is usually an underestimate. alumni and even the professors admit that most courses take way more time to do well in it than what is implied in unit values. popular classes such as cs 61a even draw criticism due to students end up spending more than 12 hours per week to complete the required lectures, homework, labs, projects, and exam practice. we don't think there is anything wrong with making a course demanding, but a lack of information on how much time is really needed hinders students' ability to make informed decisions on what classes to take. right now, students usually rely on words to gauge a class' workload: they will ask their friends and upperclassmen who have taken the course. this approach is not perfect, however, because each people may perceive difficulty a bit differently and sometimes it's hard to find someone who has actually taken the class. therefore, we decide to create a ratemyprofessor-style website so that students can rate and view courses' workload information in one stop.\nwhat it does\ncalrate is a platform that can enable students to rate a course's real workload and time commitment based on their personal experience during the semester. their ratings will be used to calculate the average rating of a course which represents the average opinion on how much time the course actually takes each week. students can search the database during the enrollment period, allowing them to make better-informed decisions on which course to take and how many units to enroll.\nhow we built it\nwe use the anvil web app builder as our main -----> tool !!! . we create a data table to store the average course ratings and then we use python to implement a searching tool to enable users to search the data table for the courses of interest. we also use python to implement the data updating algorithm so that the average rating of a course can be correctly updated when someone submits a new rating, and a new course entry will be created when someone rates a new course.\nchallenges we ran into\nwe are somewhat unfamiliar with the anvil environment when starting out, so it takes us some time and effort to learn how to use this tool. in the end, we are able to overcome this difficulty by closely reading the documentation and searching for answers on forums.\naccomplishments that we're proud of\nafter we finish the app, we are most proud of the fact that this working web app can actually go out there and help our fellow berkeley students, especially those with less personal connections and social resources. with proper advertisement, we believe this platform can really help students make more reasonable plans about their upcoming semester, knowing what to expect from each course they choose.\nwhat we learned\nduring the process of completing this project, there are many valuable takeaways. most importantly, this project provides an opportunity for us to learn how to use the web app building tool anvil, which, as part of our software engineering skillsets, can benefit us in future studies or work. also, we learn the\nwhat's next for calrate\nnext, we want to start by making this platform available and known among berkeley students. as a crowdsourcing app, its success also relies on more people using it. we will start out by inviting our friends to try out and provide ratings for the courses they have taken. this will give us valuable feedback and allow us to further polish the ui and functionalities. this will also generate initial data so that when it is online for all students to use, they will already find it useful. in the long run, we also hope to integrate this with another functionality: sharing course resources. some classes have instructor-provided extra resources - notes, youtube videos, books, to help those students that find course materials insufficient, but many others don't. we want to build a platform that allows students that have taken the course to take the lead and share resources they find helpful. together with this, we believe calrate can further support our fellow berkeley students in completing a successful college career.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59506106}, {"Unnamed: 0": 6113, "autor": "calmemaybe", "date": null, "content": "Inspiration\nThis project was inspired by a common situation we run into every day: getting to our destination safely. Just this week, one of our members ran into a scary situation while getting home. Most of the time, we text our friends to let them know once we\u2019ve arrived to let them know we are safe. But, more often than not, college students have many tasks to think about and forget to check in! We thought that this could lead to a dangerous situation in the case that a friend actually did not get back home safely and something happened. That\u2019s where calmemaybe comes in. A Berkeley themed spin-off of the popular Carly Rae Jepsen song titled \u201cCall Me Maybe\u201d, calmemaybe facilitates each step in a safe trip to one\u2019s destination.\nWhat it does\nWalking alone? Tell calmemaybe. Users log the start of their trip, along with an estimated time of trip completion. Calmymaybe notifies every member in that user\u2019s circle (generally, close friends and/or family) that the user has started a trip. Internally, calmemaybe starts a timer, counting down until the estimated time of arrival. If the user ends their trip prior to the time of arrival, all is well and there is no cause for concern. However, if calmemaybe has not heard back from the user past their estimated time of arrival, it alerts all members of the user\u2019s circle--encouraging them to actively check in on the user and ensure that everything is okay.\nHow we built it\nWe built calmemaybe as a Flask application using Ngrok with the Twilio API, making the most of the webhook functionality for receiving messages from the user, and sending replies with TwiML.\nOur first step was to start with the basic functionality - getting a free Twilio number and sending messages via the Twilio API to our phones. Then we took it one step further because we wanted the ability to have back and forth conversations with our users. We utilized webhooks and TwiML to create a web application that was able to receive messages and respond accordingly. Because the webpage had to be public, we couldn\u2019t just use the local flask server and used a tool called ngrok to create temporary publicly accessible tunnels (through a lot of googling!)\nOur final step was to integrate a timer using multi-threading in order to allow the emergency message to be sent after the time it would take the user to reach their destination as prompted to. Two minutes after the user was meant to get home, the emergency message is sent to the friend if the user has not indicated their arrival at their destination. This reminds the friend to check in to see if their friend has gotten home safely.\nChallenges we ran into\nOur very first challenge was how to send a single text message through Twilio. Instead of directly saving our variables to our code, we needed to create a .env file and export variables and their values there. We learned about security concerns with pushing our code to a GitHub repository, and learned how to securely store our keys/values despite having a public project.\nWhen integrating the timer, we realized that we needed a way to allow the user to continue being able to text updates simultaneously when the timer ran. We were able to accomplish this through threading, and a series of helper functions across classes.\nEvery time we realized we needed to integrate new functionality, whether it be a new API or framework, we had to read through online documentation, understand how to correctly download to our personal devices, and debug how each layer of complexity would build on our existing framework. Many times, we realized that due to the nature of Twilio requests, we were unable to access any variables created outside our request function from within the function. In finding workarounds to this challenge, we integrated lambda functions, and created our own additional classes and libraries.\nAccomplishments that we're proud of\nWe are proud of our punny name, the fact that we learned how to use the Twilio API and Flask in such a short period of time, and how we were able to meet almost all of our deliverables for a working system! Oh, and the first time we got a text from our program to our numbers was a pretty sweet moment (even though it was a picture of a random robot telling us to run for the hills from a demo we found :).\nWhat we learned\nWe\u2019d never used any of the tools we worked with before, so there was a lot to learn! From how to register and create numbers through Twilio, to sending and receiving text messages, to building a public webhook by integrating Flask and Ngrok, and automating message replies using TwiML, our process involved constantly discovering new tools and learning how to integrate them into our project. We learned how to brainstorm and polish ideas, create a project plan with a Minimum Viable Product (MVP) and additional features, and ultimately weave together various technical components into our final product.\nWhat's next for calmemaybe\nSo much! Next steps include:\nIntegrating a database to allow for multiple users/a way to persist user circles across sessions\nAllowing for trip delays--a way to reset the timer when the user is okay, but will not get to their destination by their predicted time\nAn easy to navigate user interface that sends out an indication that the trip has begun with just a selection of destination and a click of a button\nThe time it would take to reach the destination would be calculated using Google Maps, and would not have to be entered in manually by the user", "link": "https://devpost.com/software/calmemaybe", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nthis project was inspired by a common situation we run into every day: getting to our destination safely. just this week, one of our members ran into a scary situation while getting home. most of the time, we text our friends to let them know once we\u2019ve arrived to let them know we are safe. but, more often than not, college students have many tasks to think about and forget to check in! we thought that this could lead to a dangerous situation in the case that a friend actually did not get back home safely and something happened. that\u2019s where calmemaybe comes in. a berkeley themed spin-off of the popular carly rae jepsen song titled \u201ccall me maybe\u201d, calmemaybe facilitates each step in a safe trip to one\u2019s destination.\nwhat it does\nwalking alone? tell calmemaybe. users log the start of their trip, along with an estimated time of trip completion. calmymaybe notifies every member in that user\u2019s circle (generally, close friends and/or family) that the user has started a trip. internally, calmemaybe starts a timer, counting down until the estimated time of arrival. if the user ends their trip prior to the time of arrival, all is well and there is no cause for concern. however, if calmemaybe has not heard back from the user past their estimated time of arrival, it alerts all members of the user\u2019s circle--encouraging them to actively check in on the user and ensure that everything is okay.\nhow we built it\nwe built calmemaybe as a flask application using ngrok with the twilio api, making the most of the webhook functionality for receiving messages from the user, and sending replies with twiml.\nour first step was to start with the basic functionality - getting a free twilio number and sending messages via the twilio api to our phones. then we took it one step further because we wanted the ability to have back and forth conversations with our users. we utilized webhooks and twiml to create a web application that was able to receive messages and respond accordingly. because the webpage had to be public, we couldn\u2019t just use the local flask server and used a -----> tool !!!  called ngrok to create temporary publicly accessible tunnels (through a lot of googling!)\nour final step was to integrate a timer using multi-threading in order to allow the emergency message to be sent after the time it would take the user to reach their destination as prompted to. two minutes after the user was meant to get home, the emergency message is sent to the friend if the user has not indicated their arrival at their destination. this reminds the friend to check in to see if their friend has gotten home safely.\nchallenges we ran into\nour very first challenge was how to send a single text message through twilio. instead of directly saving our variables to our code, we needed to create a .env file and export variables and their values there. we learned about security concerns with pushing our code to a github repository, and learned how to securely store our keys/values despite having a public project.\nwhen integrating the timer, we realized that we needed a way to allow the user to continue being able to text updates simultaneously when the timer ran. we were able to accomplish this through threading, and a series of helper functions across classes.\nevery time we realized we needed to integrate new functionality, whether it be a new api or framework, we had to read through online documentation, understand how to correctly download to our personal devices, and debug how each layer of complexity would build on our existing framework. many times, we realized that due to the nature of twilio requests, we were unable to access any variables created outside our request function from within the function. in finding workarounds to this challenge, we integrated lambda functions, and created our own additional classes and libraries.\naccomplishments that we're proud of\nwe are proud of our punny name, the fact that we learned how to use the twilio api and flask in such a short period of time, and how we were able to meet almost all of our deliverables for a working system! oh, and the first time we got a text from our program to our numbers was a pretty sweet moment (even though it was a picture of a random robot telling us to run for the hills from a demo we found :).\nwhat we learned\nwe\u2019d never used any of the tools we worked with before, so there was a lot to learn! from how to register and create numbers through twilio, to sending and receiving text messages, to building a public webhook by integrating flask and ngrok, and automating message replies using twiml, our process involved constantly discovering new tools and learning how to integrate them into our project. we learned how to brainstorm and polish ideas, create a project plan with a minimum viable product (mvp) and additional features, and ultimately weave together various technical components into our final product.\nwhat's next for calmemaybe\nso much! next steps include:\nintegrating a database to allow for multiple users/a way to persist user circles across sessions\nallowing for trip delays--a way to reset the timer when the user is okay, but will not get to their destination by their predicted time\nan easy to navigate user interface that sends out an indication that the trip has begun with just a selection of destination and a click of a button\nthe time it would take to reach the destination would be calculated using google maps, and would not have to be entered in manually by the user", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59506113}, {"Unnamed: 0": 6129, "autor": "TheAve", "date": null, "content": "Inspiration\nWe were looking to create something that could strengthen and uplift the bond between UW students and the surrounding community. TheAve allows students to interact more with surrounding businesses and gives businesses a chance to gain the exposure they may have lost due to the pandemic.\nWhat it does\nTheAve is a community-building and events platform that allows small businesses and groups on The Ave in Seattle to collaborate and host events, giveaways, and deals to reforge bonds after the pandemic.\nHow we built it\nWe used LucidChart to diagram all of our site features. This translated directly into Webflow, which is a no-code website builder that allowed us to rapidly build, test, and deploy our platform. It included CMS, which powers our business and event tracking.\nChallenges we ran into\nIn general, we were a pretty agile and fast-paced team, but challenges we ran into included idea generation and not too much prior experience with webflow.\nAccomplishments that we're proud of\nWe are proud of the clean and functional website we created, and how rapidly we were able to design and build it. In the real world, this project could prove immensely useful in bringing community together on the Ave after the pandemic.\nWhat we learned\nWe learned just how fast-paced a hackathon is, even requiring an all-nighter. We also learned a new tool, Webflow, and how beneficial a good group dynamic is to getting things done.\nWhat's next for TheAve\nWe hope to expand our services cover the whole Ave by marketing and talking to individual business owners, and make TheAve a platform of choice for fostering community in Seattle.", "link": "https://devpost.com/software/theave", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe were looking to create something that could strengthen and uplift the bond between uw students and the surrounding community. theave allows students to interact more with surrounding businesses and gives businesses a chance to gain the exposure they may have lost due to the pandemic.\nwhat it does\ntheave is a community-building and events platform that allows small businesses and groups on the ave in seattle to collaborate and host events, giveaways, and deals to reforge bonds after the pandemic.\nhow we built it\nwe used lucidchart to diagram all of our site features. this translated directly into webflow, which is a no-code website builder that allowed us to rapidly build, test, and deploy our platform. it included cms, which powers our business and event tracking.\nchallenges we ran into\nin general, we were a pretty agile and fast-paced team, but challenges we ran into included idea generation and not too much prior experience with webflow.\naccomplishments that we're proud of\nwe are proud of the clean and functional website we created, and how rapidly we were able to design and build it. in the real world, this project could prove immensely useful in bringing community together on the ave after the pandemic.\nwhat we learned\nwe learned just how fast-paced a hackathon is, even requiring an all-nighter. we also learned a new -----> tool !!! , webflow, and how beneficial a good group dynamic is to getting things done.\nwhat's next for theave\nwe hope to expand our services cover the whole ave by marketing and talking to individual business owners, and make theave a platform of choice for fostering community in seattle.", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59506129}, {"Unnamed: 0": 6133, "autor": "Berkeley Free Food", "date": null, "content": "Inspiration for the project\nI love free food at school club events.\nGoal of project\nConnects Berkeley students to free food and gives them the opportunity to discover new clubs on campus. Helps people realize that though we all have many different recreational and intellectual differences, we can all connect over the human universal constant: love for free food. Also gives publicity to clubs with free food events.\nTechnology behind project\nThe idea is that the bot has an email signed up to the mailing list of all clubs on campus, and it searches its email everyday to find all the club events of the day. It then filters out the events, storing all events that are giving out free food. Then a twitter account tweets out the events 10 minutes before the event happens, and a student can easily see which events have free food.\nChallenges we faced\nTrying to make Google API and Twitter API work in harmony is still a problem we are trying to solve. How to get flyers and distribute information about the events, even if we know which events have food.\nThings we did well\nBuilding a functional email scraping script. Building a image to text analyzer that can tell which events have free food and which do not.\nThings we learned\nWe learned the difficulty of combining different working parts. Especially if those different parts in different languages or use different API's. How to set up an account through Google API; I found that it was a very useful tool for many different things and serves as a very functional hub for out project. Also allows for easy data storage.\nFuture of Berkeley Free Food\nI hope that one day Berkeley Free Food can be a fully functioning twitter bot that can be a helpful resource to hungry Berkeley students and eventually other universities if their students want to set up their own version.", "link": "https://devpost.com/software/berkeley-free-food", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration for the project\ni love free food at school club events.\ngoal of project\nconnects berkeley students to free food and gives them the opportunity to discover new clubs on campus. helps people realize that though we all have many different recreational and intellectual differences, we can all connect over the human universal constant: love for free food. also gives publicity to clubs with free food events.\ntechnology behind project\nthe idea is that the bot has an email signed up to the mailing list of all clubs on campus, and it searches its email everyday to find all the club events of the day. it then filters out the events, storing all events that are giving out free food. then a twitter account tweets out the events 10 minutes before the event happens, and a student can easily see which events have free food.\nchallenges we faced\ntrying to make google api and twitter api work in harmony is still a problem we are trying to solve. how to get flyers and distribute information about the events, even if we know which events have food.\nthings we did well\nbuilding a functional email scraping script. building a image to text analyzer that can tell which events have free food and which do not.\nthings we learned\nwe learned the difficulty of combining different working parts. especially if those different parts in different languages or use different api's. how to set up an account through google api; i found that it was a very useful -----> tool !!!  for many different things and serves as a very functional hub for out project. also allows for easy data storage.\nfuture of berkeley free food\ni hope that one day berkeley free food can be a fully functioning twitter bot that can be a helpful resource to hungry berkeley students and eventually other universities if their students want to set up their own version.", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59506133}, {"Unnamed: 0": 6141, "autor": "sustain.go", "date": null, "content": "Inspiration\nWe were inspired by own everyday shopping habits and grocery routines where we often are unsure of which are the best brands to buy from. With an app that could provide suggestions and inform us of how to make better decisions, we could make changes in our own routines to contribute to sustainability efforts.\nWhat it does\nsustain.go provides users with a 'game' experience while they complete their grocery shopping. With a list of items, our app will provide users with some facts about sustainability and encourage them to choose to purchase brands that are sustainable. Each time they purchase from a more sustainable brand, they will be rewarded points which can be accumulated to earn discounts from their favorite stores.\nHow we built it\nThe project was built using the Java programming language and the XML markup language in conjunction with the Android software development kit libraries and the Android Studio IDE while also implementing key object-oriented programming principles. We initially sketched out the idea for the application and created low fidelity mock ups in Figma. We added different features to the various screens of the app. Using the prototype tool in Figma, we created a user flow which allows a user to go from a login screen to the dashboard to view their points. They are also able to checkout the weekly challenges to earn more points, record their grocery trips and upload their receipts. We took these mockups as inspiration as we started building the app in Android Studio. We initially started with created the structure of the project and organizing the classes into different modules. We used JAVA in order to create our frames in Android Studio. Each different screen has a class that handles the backend tasks, a .xml page that has the front-end of the screen and a controller that acts as the middleman. We used Git to collaborate and keep track of our changes.\nChallenges we ran into\nWe set out to build this idea in an android app, however our team has little experience with app development. We were able to set up an app in android studio, however could not fully implement functionality into our app.\nAccomplishments that we're proud of\nWe were able to get our idea across in Figma and create interfaces for our main pages of the app. Our team collaborated well in combining our skills and coming up with an idea we were all excited about.\nWhat we learned\nOur whole team learned a lot about creating an app. we learned from each other\u2019s skillsets. From creating mock-ups in Figma to producing these designs on Android Studio, we learned how to go about the design thinking process and convert these ideas into a digital product.\nWhat's next for sustain.go\nwe would love to be able to implement the app and provide factual data to users from APIs and databases. We could add more features such as more variety and reminders for weekly challenges, more opportunities to earn points, or a community leaderboard.", "link": "https://devpost.com/software/sustain-go", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe were inspired by own everyday shopping habits and grocery routines where we often are unsure of which are the best brands to buy from. with an app that could provide suggestions and inform us of how to make better decisions, we could make changes in our own routines to contribute to sustainability efforts.\nwhat it does\nsustain.go provides users with a 'game' experience while they complete their grocery shopping. with a list of items, our app will provide users with some facts about sustainability and encourage them to choose to purchase brands that are sustainable. each time they purchase from a more sustainable brand, they will be rewarded points which can be accumulated to earn discounts from their favorite stores.\nhow we built it\nthe project was built using the java programming language and the xml markup language in conjunction with the android software development kit libraries and the android studio ide while also implementing key object-oriented programming principles. we initially sketched out the idea for the application and created low fidelity mock ups in figma. we added different features to the various screens of the app. using the prototype -----> tool !!!  in figma, we created a user flow which allows a user to go from a login screen to the dashboard to view their points. they are also able to checkout the weekly challenges to earn more points, record their grocery trips and upload their receipts. we took these mockups as inspiration as we started building the app in android studio. we initially started with created the structure of the project and organizing the classes into different modules. we used java in order to create our frames in android studio. each different screen has a class that handles the backend tasks, a .xml page that has the front-end of the screen and a controller that acts as the middleman. we used git to collaborate and keep track of our changes.\nchallenges we ran into\nwe set out to build this idea in an android app, however our team has little experience with app development. we were able to set up an app in android studio, however could not fully implement functionality into our app.\naccomplishments that we're proud of\nwe were able to get our idea across in figma and create interfaces for our main pages of the app. our team collaborated well in combining our skills and coming up with an idea we were all excited about.\nwhat we learned\nour whole team learned a lot about creating an app. we learned from each other\u2019s skillsets. from creating mock-ups in figma to producing these designs on android studio, we learned how to go about the design thinking process and convert these ideas into a digital product.\nwhat's next for sustain.go\nwe would love to be able to implement the app and provide factual data to users from apis and databases. we could add more features such as more variety and reminders for weekly challenges, more opportunities to earn points, or a community leaderboard.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59506141}, {"Unnamed: 0": 6147, "autor": "Mindlastic", "date": null, "content": "Inspiration\nThe mental-health crisis is a crisis in the US, with suicide rates increasing, and 46% of the Adult population reporting experiencing symptoms of anxiety or depression. It is one of the few problems that technology has actively contributed to as opposed to remedying. We all have been strongly affected by the mental-health crisis and wish to use technology as a tool in the effort to combat it.\nWhat it does\nMindlastic is an iOS app that is designed to track and elevate your mental health. The app is based around journaling, and uses OpenAI's GPT-3 AI to gain advanced insight on text input, detecting nuanced patters unnoticeable by humans, even ourselves. Each journal entry results in an estimated sentiment value, which is then used to track overall mood and well-being. This data can be combined with sleep and exercise data to provide suggestions to the user to alleviate mental health symptoms.\nHow we built it\nMindlastic was built with a Swift/SwiftUI frontend and a Python backend using Flask and MongoDB. The app uses artificial intelligence through OpenAI's GPT-3 for classification and summarization of the data.\nChallenges we ran into\nLearning the capabilites of GPT-3, and how to best control it.\nData transfer between different frameworks/languages\nAccomplishments that we're proud of\nUsing GPT-3, a bleeding-edge technology that has been largely untested with very little example code online.\nClean and elegant UI\nWhat we learned\nArtificial intelligence fundamentals and applications\nMobile app design and structure\nServer hosting and cloud databases\nWhat's next for Mindlastic\nAdditional features using GPT-3 to get more types of numeric data\npredictions for how you will feel on future days based on our data, sleep, heart rate, exercise, and even diet\nAppStore release\nLong-term, stable server and database\nAdditional security\nUI improvements", "link": "https://devpost.com/software/mindlastic", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nthe mental-health crisis is a crisis in the us, with suicide rates increasing, and 46% of the adult population reporting experiencing symptoms of anxiety or depression. it is one of the few problems that technology has actively contributed to as opposed to remedying. we all have been strongly affected by the mental-health crisis and wish to use technology as a -----> tool !!!  in the effort to combat it.\nwhat it does\nmindlastic is an ios app that is designed to track and elevate your mental health. the app is based around journaling, and uses openai's gpt-3 ai to gain advanced insight on text input, detecting nuanced patters unnoticeable by humans, even ourselves. each journal entry results in an estimated sentiment value, which is then used to track overall mood and well-being. this data can be combined with sleep and exercise data to provide suggestions to the user to alleviate mental health symptoms.\nhow we built it\nmindlastic was built with a swift/swiftui frontend and a python backend using flask and mongodb. the app uses artificial intelligence through openai's gpt-3 for classification and summarization of the data.\nchallenges we ran into\nlearning the capabilites of gpt-3, and how to best control it.\ndata transfer between different frameworks/languages\naccomplishments that we're proud of\nusing gpt-3, a bleeding-edge technology that has been largely untested with very little example code online.\nclean and elegant ui\nwhat we learned\nartificial intelligence fundamentals and applications\nmobile app design and structure\nserver hosting and cloud databases\nwhat's next for mindlastic\nadditional features using gpt-3 to get more types of numeric data\npredictions for how you will feel on future days based on our data, sleep, heart rate, exercise, and even diet\nappstore release\nlong-term, stable server and database\nadditional security\nui improvements", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59506147}, {"Unnamed: 0": 6177, "autor": "Galactic Dialup", "date": null, "content": "Inspiration\nI really wanted two explore two things: AssemblyAI, automatic speech recognition with excellent accuracy, and Unity, a widely popular game engine. Both of those things came together in this space-themed idea of intentionally messing with text translation of speech to create a unique puzzle-game!\nWhat it does\nPlayers start the game by clicking \"Play\", then clicking the \"record\" button on the top left. Say something like \"go left\" or \"go down\", and when you're done, click \"Send\". This will \"send\" your navigation instructions to the spaceship pilot.\nHow I built it\nFirst, I set up a way to obtain audio from the user (C#'s built-in Microphone class for PC/Linux/Mac build, and Unity Asset Store's WebGL-compatible CustomMicrophone class). Then, I export it to a .wav file with SavWav. Then I send this file to the AssemblyAI server with C# and, once the file finishes processing, receive the text transcription of the voice command. I then seek out particular directional words in that input and modify them so that the pilot in the spacecraft receives something slightly different than what was said.\nChallenges I ran into\nFirstly, I wanted to use AssemblyAI's Python supported language as I was most comfortable with it, and even had it working as a standalone program. Unfortunately, Unity does not support all libraries necessary for the python program to work (pyaudio, SpeechRecognition), so I had to scrap all working python files and switch over to the C# version. I also have never really used C# to the capacity that this project demanded, and ran into a lot of trouble with C#'s asynchronous features. Additionally, it was difficult to get the spaceship to move in a grid-like patter on the input of voice, not keypress, which is Unity's default.\nAccomplishments that I'm proud of\nI was never sure I could actually have a whole game developed in Unity in 36 hours--let alone while incorporating a completely external API like AssemblyAI, and knowing how to work with neither of those tools beforehand. I'm also proud of how much I was able to accomplish despite being solo. I was initially uneasy about choosing to pursue this project by myself, that perhaps I wouldn't be able to make anything at all, but now I know that's simply not true.\nWhat I learned\nI learned every step of the Unity game development process, how AssemblyAI handles its speech to text transcription requests, and how to combine these two interests into something fun and unique.\nWhat's next for Galactic Dialup\nOriginally, my idea was for this to be a two-player game--where the second player is the pilot of the spaceship, receiving only warped text from player. I would love to continue developing into that after improving level design, UI elements, animations, and optimizing the app's performance. I would love to include more challenging warpings of the speech-to-text, ones that require more than one instruction to circumvent. The great thing about AssemblyAI is that I already possess the tool for it--I can fetch whole sentences as opposed to Unity's default key phrase finder.", "link": "https://devpost.com/software/galactic-dialup", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ni really wanted two explore two things: assemblyai, automatic speech recognition with excellent accuracy, and unity, a widely popular game engine. both of those things came together in this space-themed idea of intentionally messing with text translation of speech to create a unique puzzle-game!\nwhat it does\nplayers start the game by clicking \"play\", then clicking the \"record\" button on the top left. say something like \"go left\" or \"go down\", and when you're done, click \"send\". this will \"send\" your navigation instructions to the spaceship pilot.\nhow i built it\nfirst, i set up a way to obtain audio from the user (c#'s built-in microphone class for pc/linux/mac build, and unity asset store's webgl-compatible custommicrophone class). then, i export it to a .wav file with savwav. then i send this file to the assemblyai server with c# and, once the file finishes processing, receive the text transcription of the voice command. i then seek out particular directional words in that input and modify them so that the pilot in the spacecraft receives something slightly different than what was said.\nchallenges i ran into\nfirstly, i wanted to use assemblyai's python supported language as i was most comfortable with it, and even had it working as a standalone program. unfortunately, unity does not support all libraries necessary for the python program to work (pyaudio, speechrecognition), so i had to scrap all working python files and switch over to the c# version. i also have never really used c# to the capacity that this project demanded, and ran into a lot of trouble with c#'s asynchronous features. additionally, it was difficult to get the spaceship to move in a grid-like patter on the input of voice, not keypress, which is unity's default.\naccomplishments that i'm proud of\ni was never sure i could actually have a whole game developed in unity in 36 hours--let alone while incorporating a completely external api like assemblyai, and knowing how to work with neither of those tools beforehand. i'm also proud of how much i was able to accomplish despite being solo. i was initially uneasy about choosing to pursue this project by myself, that perhaps i wouldn't be able to make anything at all, but now i know that's simply not true.\nwhat i learned\ni learned every step of the unity game development process, how assemblyai handles its speech to text transcription requests, and how to combine these two interests into something fun and unique.\nwhat's next for galactic dialup\noriginally, my idea was for this to be a two-player game--where the second player is the pilot of the spaceship, receiving only warped text from player. i would love to continue developing into that after improving level design, ui elements, animations, and optimizing the app's performance. i would love to include more challenging warpings of the speech-to-text, ones that require more than one instruction to circumvent. the great thing about assemblyai is that i already possess the -----> tool !!!  for it--i can fetch whole sentences as opposed to unity's default key phrase finder.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59506177}, {"Unnamed: 0": 6181, "autor": "Intelligent document accessibility remediation", "date": null, "content": "Inspiration\nI am blind and growing up in India, I struggled to access accessible textbooks. None of my STEM textbooks were accessible in 11th and 12th grades and I had to type out all of my textbooks so that I could read them later with my screen reader. In fact, according to WebAIM, over 98.1% of the top 1 M webpages are inaccessible and the statistics are pretty alarming when it comes to inaccessible documents produced and shared as well. This is why we call it a \"book famine\" in our community. Covid-19 has only made it worse with a proliferation in the creation and distribution of inaccessible content. This problem is not restricted to India or developing countries. Even in the United States, there were 35 litigations in just July and August last year against universities for not being able to provide accessible resources to their students with disabilities. Part of the reason is the increase in disability disclosure on college campuses and the understaffed disability offices.\nLast year, a few blind friends with similar lived experiences came together to work on this problem. We evaluated existing optical character recognition (OCR) technology that converts images into text, and soon realized that they were very limited in their capabilities. While they could handle plain text documents (e.g. simple text documents etc.), they could not handle complex layouts such as tables, lists, content containing math, multicolumn pages etc., nor could they provide support for accessibility semantics such as appropriate heading levels, regions among others. We thus worked to develop custom AI models built on top of existing OCR solutions (Microsoft Azure's OCR technology) that helped accomplish two things-(1) identification of layout elements such as tables, math content or multiple columns and (2) handle these various layouts and represent them accurately to a blind user when reading with a screen reader including supporting accessibility semantics and best practices. We called it I-Stem document accessibility services (where I-Stem refers to Inclusive STEM, i.e. using STEM to realize inclusion). We have been able to achieve pretty good results with an overall model accuracy of around 85%.\nDuring this hackathon, we wanted to take it a step further. We realize that AI is not perfect and will not be anytime in the near future. Therefore, we wanted to combine AI intelligence with human intelligence and bring down the time, effort and cost involved in document accessibility remediation. Thus, we worked on I-Stem document accessibility remediation tool. This tool uses our AI models and combines that with a rich editor that allows humans to correct any mistakes made by the AI tool. For instance, if AI incorrectly detected a paragraph as table, the human remediator can relabel it as a paragraph and our automated algorithms will use this information to reconstruct that region as a paragraph without any human input.\nWhat it does\nthe intelligent remediation system consists of three steps.\nLayout inference editing: Using our AI models, we detect the various layout elements (e.g. tables, lists, headings etc.) on a page and overlay these on the original image. Using this, a human remediator can correct any layout identification errors. For instance, if the AI model incorrectly detected a region of a page as a table when in fact it was a list, the remediator can indicate that.\nText editing: Using the inputs received from the human remediator in step 1, our algorithms reconstruct the document. For instance, if a table was relabeled as a list, the algorithm will reconstruct the table contents as a list to make it efficient for the remediator and help them save time. In this step, the remediators are shown the reconstructed document and they can make content changes. For instance, certain words might be recognized incorrectly by our AI systems or a line or two might be omitted here and there. this step helps a remediator fix these errors.\nReading order: The final step allows the remediator to change the reading order. Reading order is crucial for screen reader users. It determines the order in which various elements on a page will be read out to the user. This step shows the remediator a list of various layout elements and the order in which they will be read, allowing them to change the reading order appropriately. the remediator can also see the HTML preview of the document and make any changes.\nIn summary, the remediation tool uses the intelligence from our AI systems with human input to generate completely accessible and remediated documents. We tested this out with two remediators and observed a 60% reduction in the time that it took them to remediate a document, reducing from an average of 25 Mins. to under 10 Mins.\nHow we built it\nWe used existing OCR technology and built custom models on top of that (object identification models to identify various layout elements and custom recognition models to handle various layout elements). We then used this to process the document, showing the results to a human remediator. We then developed algorithms to consume human input and reconstruct the document. For our rich editor, we used DraftJS with customization and the web application more broadly uses MERN stack.\nChallenges we ran into\nThere were several challenges we ran into.\nThere was slight discrepancy in the bounding boxes we received from various tools which led to overlapping regions and issues when reconstructing the document. We thus had to manually write logic to standardize coordinates.\nthere were no off-the-shelf editors that provided all the functionality we needed for our project, and so we had to write custom code.\nAccomplishments that we're proud of\nWe are proud that we were able to develop a remediation portal end-to-end and test it with remediators with noticeable improvements in time and effort involved in remediation. One nonprofit is also excited to pilot it.\nWhat we learned\nWhile working on the project, we learnt a lot about various computer vision concepts, experimented with models and algorithms, learnt about human-in-the-loop techniques and applied human-centered design principles. All of this was very exciting and made us better developers and designers.\nWhat's next for Intelligent document accessibility remediation\nWe want to add some more features (especially around math editing), make it more robust, test it with more users, pilot with organizations and integrate this with the broader remediation flows in academic institutes, thereby making the remediation process more efficient and cost-effective for all, ultimately ensuring an equitable and reliable access to textbooks and content for people with print disabilities.", "link": "https://devpost.com/software/intelligent-document-accessibility-remediation", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ni am blind and growing up in india, i struggled to access accessible textbooks. none of my stem textbooks were accessible in 11th and 12th grades and i had to type out all of my textbooks so that i could read them later with my screen reader. in fact, according to webaim, over 98.1% of the top 1 m webpages are inaccessible and the statistics are pretty alarming when it comes to inaccessible documents produced and shared as well. this is why we call it a \"book famine\" in our community. covid-19 has only made it worse with a proliferation in the creation and distribution of inaccessible content. this problem is not restricted to india or developing countries. even in the united states, there were 35 litigations in just july and august last year against universities for not being able to provide accessible resources to their students with disabilities. part of the reason is the increase in disability disclosure on college campuses and the understaffed disability offices.\nlast year, a few blind friends with similar lived experiences came together to work on this problem. we evaluated existing optical character recognition (ocr) technology that converts images into text, and soon realized that they were very limited in their capabilities. while they could handle plain text documents (e.g. simple text documents etc.), they could not handle complex layouts such as tables, lists, content containing math, multicolumn pages etc., nor could they provide support for accessibility semantics such as appropriate heading levels, regions among others. we thus worked to develop custom ai models built on top of existing ocr solutions (microsoft azure's ocr technology) that helped accomplish two things-(1) identification of layout elements such as tables, math content or multiple columns and (2) handle these various layouts and represent them accurately to a blind user when reading with a screen reader including supporting accessibility semantics and best practices. we called it i-stem document accessibility services (where i-stem refers to inclusive stem, i.e. using stem to realize inclusion). we have been able to achieve pretty good results with an overall model accuracy of around 85%.\nduring this hackathon, we wanted to take it a step further. we realize that ai is not perfect and will not be anytime in the near future. therefore, we wanted to combine ai intelligence with human intelligence and bring down the time, effort and cost involved in document accessibility remediation. thus, we worked on i-stem document accessibility remediation -----> tool !!! . this tool uses our ai models and combines that with a rich editor that allows humans to correct any mistakes made by the ai tool. for instance, if ai incorrectly detected a paragraph as table, the human remediator can relabel it as a paragraph and our automated algorithms will use this information to reconstruct that region as a paragraph without any human input.\nwhat it does\nthe intelligent remediation system consists of three steps.\nlayout inference editing: using our ai models, we detect the various layout elements (e.g. tables, lists, headings etc.) on a page and overlay these on the original image. using this, a human remediator can correct any layout identification errors. for instance, if the ai model incorrectly detected a region of a page as a table when in fact it was a list, the remediator can indicate that.\ntext editing: using the inputs received from the human remediator in step 1, our algorithms reconstruct the document. for instance, if a table was relabeled as a list, the algorithm will reconstruct the table contents as a list to make it efficient for the remediator and help them save time. in this step, the remediators are shown the reconstructed document and they can make content changes. for instance, certain words might be recognized incorrectly by our ai systems or a line or two might be omitted here and there. this step helps a remediator fix these errors.\nreading order: the final step allows the remediator to change the reading order. reading order is crucial for screen reader users. it determines the order in which various elements on a page will be read out to the user. this step shows the remediator a list of various layout elements and the order in which they will be read, allowing them to change the reading order appropriately. the remediator can also see the html preview of the document and make any changes.\nin summary, the remediation tool uses the intelligence from our ai systems with human input to generate completely accessible and remediated documents. we tested this out with two remediators and observed a 60% reduction in the time that it took them to remediate a document, reducing from an average of 25 mins. to under 10 mins.\nhow we built it\nwe used existing ocr technology and built custom models on top of that (object identification models to identify various layout elements and custom recognition models to handle various layout elements). we then used this to process the document, showing the results to a human remediator. we then developed algorithms to consume human input and reconstruct the document. for our rich editor, we used draftjs with customization and the web application more broadly uses mern stack.\nchallenges we ran into\nthere were several challenges we ran into.\nthere was slight discrepancy in the bounding boxes we received from various tools which led to overlapping regions and issues when reconstructing the document. we thus had to manually write logic to standardize coordinates.\nthere were no off-the-shelf editors that provided all the functionality we needed for our project, and so we had to write custom code.\naccomplishments that we're proud of\nwe are proud that we were able to develop a remediation portal end-to-end and test it with remediators with noticeable improvements in time and effort involved in remediation. one nonprofit is also excited to pilot it.\nwhat we learned\nwhile working on the project, we learnt a lot about various computer vision concepts, experimented with models and algorithms, learnt about human-in-the-loop techniques and applied human-centered design principles. all of this was very exciting and made us better developers and designers.\nwhat's next for intelligent document accessibility remediation\nwe want to add some more features (especially around math editing), make it more robust, test it with more users, pilot with organizations and integrate this with the broader remediation flows in academic institutes, thereby making the remediation process more efficient and cost-effective for all, ultimately ensuring an equitable and reliable access to textbooks and content for people with print disabilities.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59506181}, {"Unnamed: 0": 6215, "autor": "AirVis", "date": null, "content": "Description\nAirVis is a webapp that visualizes air quality data from any inputted location. It uses the google maps API in order to geocode the inputted location and then the Breezometer API to gather air quality data. Numerical data is presented in a interactive table, in which you can learn more about the pollutants in the air, as well as an air quality for each pollutant. Numerical data is also visualized as micrograms of pollutants per liter of air.\nIntentionality\nAs the climate crisis continues to worsen, I think it is important to present pollution data to users both at a high level and a low level. My intention was to create a useful tool that could show the climate crisis in various regions around the world, as well as being a general air quality query.\nSignificance\nAlthough solving the climate crisis is not something I am able to do at this hackathon, I think the least I can do is to show people just how polluted some areas of the world actually are, as well as display how some places around the world are doing pollution control correctly.\nTechnical Depth\nAdmittedly, the only real technical challenge of significance here was figuring out the API calls and making sure that I was receiving the correct data. The rest of the work was just building a presentable website. Also resizing bootstrap columns.", "link": "https://devpost.com/software/airvis", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "description\nairvis is a webapp that visualizes air quality data from any inputted location. it uses the google maps api in order to geocode the inputted location and then the breezometer api to gather air quality data. numerical data is presented in a interactive table, in which you can learn more about the pollutants in the air, as well as an air quality for each pollutant. numerical data is also visualized as micrograms of pollutants per liter of air.\nintentionality\nas the climate crisis continues to worsen, i think it is important to present pollution data to users both at a high level and a low level. my intention was to create a useful -----> tool !!!  that could show the climate crisis in various regions around the world, as well as being a general air quality query.\nsignificance\nalthough solving the climate crisis is not something i am able to do at this hackathon, i think the least i can do is to show people just how polluted some areas of the world actually are, as well as display how some places around the world are doing pollution control correctly.\ntechnical depth\nadmittedly, the only real technical challenge of significance here was figuring out the api calls and making sure that i was receiving the correct data. the rest of the work was just building a presentable website. also resizing bootstrap columns.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59506215}, {"Unnamed: 0": 6217, "autor": "PayMeBack Discord Bot", "date": null, "content": "Inspiration\nImagine a simpler way to split expenses on a trip. This past summer, our friend group took several vacations to various parts around the U.S. One of the challenges faced on these trips was how to effectively, and efficiently, split the cost of larger expenses among all of our friends. As such, we decided to implement a tool that can minimize the amount of transactions needed to split the expenses evenly for all people on the trip.\nWhat it does\nThe discord app that we created can be used in a Discord server. It can be given commands to start/end events, and if there is an ongoing event, transactions can be added to the event. Users can react to the event start message to be included in the calculations. Here, the bot keeps track of the amounts and who paid. Once an event ends, the bot will send a direct message to each person in the event telling them which transactions to complete so everyone pays each other back.\nHow we built it\nWe built this app with JavaScript in Node.js, using the discord.js library to use the Discord API.\nChallenges we ran into\nOne challenge was learning how to effectively use Discord\u2019s API, since neither of us had used it before. In particular, obtaining the list of users that reacted to a specific message was more challenging than we expected.\nAccomplishments that we're proud of\nWe\u2019re proud of learning how to use the Discord API quickly to build this application. The application can currently run and function correctly.\nWhat we learned\nWe learned how to use the Discord API in JavaScript, and we learned more about how to write server-side Node.js applications.\nWhat's next for PayMeBack Discord Bot\nOne feature that we didn\u2019t have time to implement was adding in the ability to delete a transaction that was submitted by mistake. Also, the current implementation only stores the transactions and events in-memory, so a proper database should be used to avoid potential data loss. With these two features, this application should be ready for use in our Discord server!", "link": "https://devpost.com/software/paymeback-discord-bot", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nimagine a simpler way to split expenses on a trip. this past summer, our friend group took several vacations to various parts around the u.s. one of the challenges faced on these trips was how to effectively, and efficiently, split the cost of larger expenses among all of our friends. as such, we decided to implement a -----> tool !!!  that can minimize the amount of transactions needed to split the expenses evenly for all people on the trip.\nwhat it does\nthe discord app that we created can be used in a discord server. it can be given commands to start/end events, and if there is an ongoing event, transactions can be added to the event. users can react to the event start message to be included in the calculations. here, the bot keeps track of the amounts and who paid. once an event ends, the bot will send a direct message to each person in the event telling them which transactions to complete so everyone pays each other back.\nhow we built it\nwe built this app with javascript in node.js, using the discord.js library to use the discord api.\nchallenges we ran into\none challenge was learning how to effectively use discord\u2019s api, since neither of us had used it before. in particular, obtaining the list of users that reacted to a specific message was more challenging than we expected.\naccomplishments that we're proud of\nwe\u2019re proud of learning how to use the discord api quickly to build this application. the application can currently run and function correctly.\nwhat we learned\nwe learned how to use the discord api in javascript, and we learned more about how to write server-side node.js applications.\nwhat's next for paymeback discord bot\none feature that we didn\u2019t have time to implement was adding in the ability to delete a transaction that was submitted by mistake. also, the current implementation only stores the transactions and events in-memory, so a proper database should be used to avoid potential data loss. with these two features, this application should be ready for use in our discord server!", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59506217}, {"Unnamed: 0": 6227, "autor": "TriaGO", "date": null, "content": "Inspiration\nWhen our group (Lactose Tolerant) saw the prompt choices, we decided on Electronic Health Records, and we derived a somewhat elaborate (and also expensive) plan involving RFID cards and personal health records. At our Saturday morning check in meeting, we realized just how in over our head we were, and changed our focus in order to play to our strengths, and to utilize technology we had available to us. Two members of our team have had deeply suboptimal urgent care experiences, despite having above average healthcare literacy. Additionally, all of us are proponents of accessibility in healthcare, and so we were able to merge the two to help develop a better and more accessible triage system for urgent care. According to research from the American Medical Association, 13% of hospitals comply with Cultural Linguistic Appropriate Services (CLAS) requirements, adversely affecting patient care outcomes. With Spanish being one of the more commonly spoken languages in the United States, we wanted to integrate it into our interface, to make it as easy as possible for patients and providers to use.\nWhat it does\nTriaGO (not to be confused with Trivago) is a portmanteau of \"triage\" (also the Spanish \"triaje\") and \"go\", and the name is a fairly straightforward explanation of what we hope to do: optimize triage in an urgent care setting, to create a safer, faster, and more effective treatment algorithm. At this time, our group was unable to find evidence-based triage guidelines, and so ours are unfortunately not evidence-based, but someday they might be if we are able to partner with urgent care facilities to adequately test TriaGO. As an added bonus, it does not collect any individualized patient information, and as such is safe harbored under HIPAA regulations, making it even safer for an inpatient setting.\nHow we built it\nWe built it using bubble.io, which is a low-code platform for web development. For our specific group, this was a wonderful tool, because none of us are particularly experienced with computer science or coding. However, because this tool does not run statistical analysis, we also ran some of the code through R in order to develop a TriaGO score. This TriaGO score has cutoffs to indicate which level of care/triage is appropriate for a given patient based on their symptom presentation. For example, at 100+ points, patients are instructed to go to the ER, but a score of 15 would place you in Tier 3 based on the symptoms you checked and the number of points assigned to them.\nChallenges we ran into\nNo one in our group was especially proficient with code, but Kama was able to figure it out, while Sydney did a lot of the research and translation, and Grant focused on aesthetics and pitch development. Bubble.io did not let us add functions for statistical analysis on the platform, but thankfully Kama knew enough about R studio to run it separately.\nAccomplishments that we're proud of\nThis is the first hackathon for all of us. That we were able to make a mostly-functional product and a clean looking pitch in our allotted time frame without a significant background in coding or urgent care is plenty, and we are all grateful to each other for our cooperation, patience, and dedication throughout this project.\nWhat we learned\nWe learned that coding is hard, and if you can find a software that will get rid of a lot of the coding elements, you can patch the rest together somewhere else.\nWhat's next for TriaGO\nWe were workshopping all of it. If we wanted to roll this out in an urgent care setting, there are several in Pittsburgh that may be amenable to working with us.", "link": "https://devpost.com/software/triago", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwhen our group (lactose tolerant) saw the prompt choices, we decided on electronic health records, and we derived a somewhat elaborate (and also expensive) plan involving rfid cards and personal health records. at our saturday morning check in meeting, we realized just how in over our head we were, and changed our focus in order to play to our strengths, and to utilize technology we had available to us. two members of our team have had deeply suboptimal urgent care experiences, despite having above average healthcare literacy. additionally, all of us are proponents of accessibility in healthcare, and so we were able to merge the two to help develop a better and more accessible triage system for urgent care. according to research from the american medical association, 13% of hospitals comply with cultural linguistic appropriate services (clas) requirements, adversely affecting patient care outcomes. with spanish being one of the more commonly spoken languages in the united states, we wanted to integrate it into our interface, to make it as easy as possible for patients and providers to use.\nwhat it does\ntriago (not to be confused with trivago) is a portmanteau of \"triage\" (also the spanish \"triaje\") and \"go\", and the name is a fairly straightforward explanation of what we hope to do: optimize triage in an urgent care setting, to create a safer, faster, and more effective treatment algorithm. at this time, our group was unable to find evidence-based triage guidelines, and so ours are unfortunately not evidence-based, but someday they might be if we are able to partner with urgent care facilities to adequately test triago. as an added bonus, it does not collect any individualized patient information, and as such is safe harbored under hipaa regulations, making it even safer for an inpatient setting.\nhow we built it\nwe built it using bubble.io, which is a low-code platform for web development. for our specific group, this was a wonderful -----> tool !!! , because none of us are particularly experienced with computer science or coding. however, because this tool does not run statistical analysis, we also ran some of the code through r in order to develop a triago score. this triago score has cutoffs to indicate which level of care/triage is appropriate for a given patient based on their symptom presentation. for example, at 100+ points, patients are instructed to go to the er, but a score of 15 would place you in tier 3 based on the symptoms you checked and the number of points assigned to them.\nchallenges we ran into\nno one in our group was especially proficient with code, but kama was able to figure it out, while sydney did a lot of the research and translation, and grant focused on aesthetics and pitch development. bubble.io did not let us add functions for statistical analysis on the platform, but thankfully kama knew enough about r studio to run it separately.\naccomplishments that we're proud of\nthis is the first hackathon for all of us. that we were able to make a mostly-functional product and a clean looking pitch in our allotted time frame without a significant background in coding or urgent care is plenty, and we are all grateful to each other for our cooperation, patience, and dedication throughout this project.\nwhat we learned\nwe learned that coding is hard, and if you can find a software that will get rid of a lot of the coding elements, you can patch the rest together somewhere else.\nwhat's next for triago\nwe were workshopping all of it. if we wanted to roll this out in an urgent care setting, there are several in pittsburgh that may be amenable to working with us.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59506227}, {"Unnamed: 0": 6233, "autor": "GT Scheduler Account Sync", "date": null, "content": "Inspiration\nGT Scheduler is a popular open source application that is used by students at Georgia Tech to browse and schedule classes. It improves on the university-provided software by employing modern UI/UX practices while still sourcing the data from the same place (ensuring that it is accurate). On its days where its usage peaks (generally when class registration is going on), the app sees upwards of 5,000 visits, making it a valuable tool for many GT students.\nFor my HackGT project, I wanted to contribute new features to it, and the way I determined what to add was to see what the broader userbase most wanted in the app:\nBecause the app includes a small feedback form in the bottom right, it was easy to collate these responses into a short-list of the top requested features. Near the top of these was the ability to synchronize schedules between devices by logging in with an account, since normally each device has its own independent schedule storage. I figured I could implement this in a weekend, and so I set out to add account login/sync to GT Scheduler.\nWhat it does\nIn this project, I created a PR to the existing open source repository containing my implementation of account login/sync. Specifically, it adds the following new features:\nIn the top right, users now have the option to \"Sign in.\" The app will continue to work as it does now, where it stores schedule data locally on the device, but if the user chooses to login, they will be presented with a login modal where they can select an identity provider:\nOnce they log in for the first time, the app will upload their schedule data to the cloud and associate it with their account, so that they can log in using the same account on other devices and see the same schedules:\nWhenever any changes are made to the schedule, they will be instantly synced to the cloud and will update any other devices that also have the app open.\nIf the user decides to switch accounts or stop using the cloud data syncing, they can log out of their account at any time, which will restore the app to its original behavior of storing schedule data locally:\nHow I built it\nThe existing GT Scheduler app is built using React and TypeScript, so this project involved using these same tools to build out the UI and business logic components of an account login/sync feature. Additionally, because the app already had structure in place for tasks like asynchronous data loading and organizing UI components, my implementation had to fit into the existing patterns. A short list of the most notable code-level changes this involved is:\nSplitting the UI State (the current semester/schedule version that is selected) and Account State (list of all schedule versions) so that the account state can be optionally uploaded to the cloud (while the UI state remains stored locally on the device)\nPulling the schedule data migration logic (similar to database migrations) out into a re-usable component so that both the local data storage and the cloud data storage can gracefully handle future schema changes.\nForking the graph of asynchronous data loading stages that the app uses to provide two paths to get the schedule data: the old method of local storage, and the new method of cloud storage via Firebase Cloud Firestore.\nAdding the \"account drop-down\" UI element in the top-right of the app, and ensuring that it looks acceptable in all contexts (such as in light/dark theme, or on different device screen sizes)\nAdding the \"login modal\" that can be opened to start the sign-in using Firebase Authentication\nPassing information about the currently logged-in user (or lack thereof) throughout the app so that it can be accessed in any location\nChallenges I ran into\nThis project was well-scoped for this hackathon, and as a result, I was able to complete the work I had set out to do by the end of the weekend. As such, the challenges I ran into were mostly technical in nature. While most of them were easy to solve, one in particular stands out as a pain-point that hit me a few times.\nSpecifically, the library that the app uses for showing modals (dialogs) to the user is called SweetAlert, and is generally a convenient library to do so. However, when I was adding the login modal, I found out that SweetAlert's React wrapper is somewhat deceptive: while it provides a seamless integration with a React app by letting the developer pass in a React element as the modal content, the React element that is passed ends up behaving differently than it would normally. This breaks a lot of assumptions normally made when writing React code, and was in general a pain to work around.\nAccomplishments that I'm proud of\nI'm proud that I was able to implement this major feature over the course of a weekend, and use tools (Firebase auth and cloud firestore) that I hadn't used before. Hopefully this feature will be useful to other students in the future, and if the feedback given by the users is any indicator, it will be.\nWhat's next for GT Scheduler Account Sync\nNow that the main functionality has been implemented for account sync, I just need to add a few finishing touch-ups/clean-ups before the Pull Request (at gt-scheduler/website#93) is ready for review.\nAfterwards, it will be time to launch the feature to the broader userbase. With the release of the Spring 2022 course schedule being imminent (at the time of writing this on October 24th, there are 3 days until the schedule is released), it's the perfect time to launch this feature so that users can test it out when making their Spring 2022 schedules. My plan for this is to make another PR to add a modal when users first open the app informing them of the new features, and I might also make a Reddit post to spread the word.", "link": "https://devpost.com/software/gt-scheduler-account-sync", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ngt scheduler is a popular open source application that is used by students at georgia tech to browse and schedule classes. it improves on the university-provided software by employing modern ui/ux practices while still sourcing the data from the same place (ensuring that it is accurate). on its days where its usage peaks (generally when class registration is going on), the app sees upwards of 5,000 visits, making it a valuable -----> tool !!!  for many gt students.\nfor my hackgt project, i wanted to contribute new features to it, and the way i determined what to add was to see what the broader userbase most wanted in the app:\nbecause the app includes a small feedback form in the bottom right, it was easy to collate these responses into a short-list of the top requested features. near the top of these was the ability to synchronize schedules between devices by logging in with an account, since normally each device has its own independent schedule storage. i figured i could implement this in a weekend, and so i set out to add account login/sync to gt scheduler.\nwhat it does\nin this project, i created a pr to the existing open source repository containing my implementation of account login/sync. specifically, it adds the following new features:\nin the top right, users now have the option to \"sign in.\" the app will continue to work as it does now, where it stores schedule data locally on the device, but if the user chooses to login, they will be presented with a login modal where they can select an identity provider:\nonce they log in for the first time, the app will upload their schedule data to the cloud and associate it with their account, so that they can log in using the same account on other devices and see the same schedules:\nwhenever any changes are made to the schedule, they will be instantly synced to the cloud and will update any other devices that also have the app open.\nif the user decides to switch accounts or stop using the cloud data syncing, they can log out of their account at any time, which will restore the app to its original behavior of storing schedule data locally:\nhow i built it\nthe existing gt scheduler app is built using react and typescript, so this project involved using these same tools to build out the ui and business logic components of an account login/sync feature. additionally, because the app already had structure in place for tasks like asynchronous data loading and organizing ui components, my implementation had to fit into the existing patterns. a short list of the most notable code-level changes this involved is:\nsplitting the ui state (the current semester/schedule version that is selected) and account state (list of all schedule versions) so that the account state can be optionally uploaded to the cloud (while the ui state remains stored locally on the device)\npulling the schedule data migration logic (similar to database migrations) out into a re-usable component so that both the local data storage and the cloud data storage can gracefully handle future schema changes.\nforking the graph of asynchronous data loading stages that the app uses to provide two paths to get the schedule data: the old method of local storage, and the new method of cloud storage via firebase cloud firestore.\nadding the \"account drop-down\" ui element in the top-right of the app, and ensuring that it looks acceptable in all contexts (such as in light/dark theme, or on different device screen sizes)\nadding the \"login modal\" that can be opened to start the sign-in using firebase authentication\npassing information about the currently logged-in user (or lack thereof) throughout the app so that it can be accessed in any location\nchallenges i ran into\nthis project was well-scoped for this hackathon, and as a result, i was able to complete the work i had set out to do by the end of the weekend. as such, the challenges i ran into were mostly technical in nature. while most of them were easy to solve, one in particular stands out as a pain-point that hit me a few times.\nspecifically, the library that the app uses for showing modals (dialogs) to the user is called sweetalert, and is generally a convenient library to do so. however, when i was adding the login modal, i found out that sweetalert's react wrapper is somewhat deceptive: while it provides a seamless integration with a react app by letting the developer pass in a react element as the modal content, the react element that is passed ends up behaving differently than it would normally. this breaks a lot of assumptions normally made when writing react code, and was in general a pain to work around.\naccomplishments that i'm proud of\ni'm proud that i was able to implement this major feature over the course of a weekend, and use tools (firebase auth and cloud firestore) that i hadn't used before. hopefully this feature will be useful to other students in the future, and if the feedback given by the users is any indicator, it will be.\nwhat's next for gt scheduler account sync\nnow that the main functionality has been implemented for account sync, i just need to add a few finishing touch-ups/clean-ups before the pull request (at gt-scheduler/website#93) is ready for review.\nafterwards, it will be time to launch the feature to the broader userbase. with the release of the spring 2022 course schedule being imminent (at the time of writing this on october 24th, there are 3 days until the schedule is released), it's the perfect time to launch this feature so that users can test it out when making their spring 2022 schedules. my plan for this is to make another pr to add a modal when users first open the app informing them of the new features, and i might also make a reddit post to spread the word.", "sortedWord": "None", "removed": "Nan", "score": 5, "comments": 1, "media": null, "medialink": null, "identifyer": 59506233}, {"Unnamed: 0": 6234, "autor": "Map the Gap", "date": null, "content": "Inspiration\nOn my university campus, I noticed a student who was using a wheelchair to enter my residence hall through an automatic door. Once I climbed up the stairs to my room, however, I realized that the elevator in the building was under maintenance. As I reflected back on what was \u201cnormal\u201d to me, I reached an epiphany about how wheelchair users might face challenges in the little things in their daily lives. As a result, my teammates and I created a mobile app that displays universal designs in public transportation and public spaces. This way, people with physical disabilities can utilize cost-benefit analysis before planning their routes more conveniently--and make decisions within seconds!\nWhat it does\nMap the Gap displays facilities that have universal design through a live GPS, allowing people with physical ailments and people that use wheelchairs to plan ahead of time to choose their routes more easily. Universal design refers to any design that increases accessibility of use, regardless of ability, age, or any other discriminating factors. In this way, our app ensures that people with difficulties in mobility do not encounter unexpected challenges in their daily lives(unexpected construction, staircases, narrow entryways). They can search to see which places have accommodations for their needs and choose accordingly with their own freedom.\nHow we built it\nThrough a web editor Figma, my colleague and I used a prototyping tool to build the base graphics of Map the Gap. My other teammate also used Final Cut Pro to edit video footage taken in our studio.\nChallenges we ran into\nWe were new to Figma, a web-based prototyping assistant; we faced challenges in utilizing new transitioning tools and intermittent lagging from the online platform.\nAccomplishments that we're proud of\nDespite our lack of experience in Hackathons, we\u2019re proud of learning new skills, such as prototyping, filming, video editing, and getting sleep deprivation!\nWhat we learned\nWe learned a great deal about physical disabilities and the importance of universal design, which is the design of a location to make it as accessible to as many people as possible, regardless of their age, size, ability or disability. We ruminated a lot about the lives of wheelchair users and felt obligation to support those in need.\nWhat's next for Map the Gap\nWe look forward to expanding our platform to get more users and input to increase credibility and reliance on our app. We hope to garner regular users and make even a slight change in bettering the lives of wheelchair users and those with mobility impairments.", "link": "https://devpost.com/software/map-the-gap", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\non my university campus, i noticed a student who was using a wheelchair to enter my residence hall through an automatic door. once i climbed up the stairs to my room, however, i realized that the elevator in the building was under maintenance. as i reflected back on what was \u201cnormal\u201d to me, i reached an epiphany about how wheelchair users might face challenges in the little things in their daily lives. as a result, my teammates and i created a mobile app that displays universal designs in public transportation and public spaces. this way, people with physical disabilities can utilize cost-benefit analysis before planning their routes more conveniently--and make decisions within seconds!\nwhat it does\nmap the gap displays facilities that have universal design through a live gps, allowing people with physical ailments and people that use wheelchairs to plan ahead of time to choose their routes more easily. universal design refers to any design that increases accessibility of use, regardless of ability, age, or any other discriminating factors. in this way, our app ensures that people with difficulties in mobility do not encounter unexpected challenges in their daily lives(unexpected construction, staircases, narrow entryways). they can search to see which places have accommodations for their needs and choose accordingly with their own freedom.\nhow we built it\nthrough a web editor figma, my colleague and i used a prototyping -----> tool !!!  to build the base graphics of map the gap. my other teammate also used final cut pro to edit video footage taken in our studio.\nchallenges we ran into\nwe were new to figma, a web-based prototyping assistant; we faced challenges in utilizing new transitioning tools and intermittent lagging from the online platform.\naccomplishments that we're proud of\ndespite our lack of experience in hackathons, we\u2019re proud of learning new skills, such as prototyping, filming, video editing, and getting sleep deprivation!\nwhat we learned\nwe learned a great deal about physical disabilities and the importance of universal design, which is the design of a location to make it as accessible to as many people as possible, regardless of their age, size, ability or disability. we ruminated a lot about the lives of wheelchair users and felt obligation to support those in need.\nwhat's next for map the gap\nwe look forward to expanding our platform to get more users and input to increase credibility and reliance on our app. we hope to garner regular users and make even a slight change in bettering the lives of wheelchair users and those with mobility impairments.", "sortedWord": "None", "removed": "Nan", "score": 11, "comments": 0, "media": null, "medialink": null, "identifyer": 59506234}, {"Unnamed: 0": 6290, "autor": "Study Ghibli", "date": null, "content": "Inspiration\nDuring online school last year, it was especially hard for many students to stay productive and motivated as well as connect with peers. In an effort to resolve this cycle of procrastination, we identified an important element to staying productive: keeping one another accountable. To this end, we developed an online platform, Study Ghibli, that would engage users in productive studying by facilitating a calm environment with relaxing music in conjunction with online peers to keep each other accountable.\nWhat it does\nStudy Ghibli is a productivity platform which allows users to select Ghibli themed virtual environments to study with. A defining feature of our platform is that there is an option for users to study in the company of people online, in order to keep each other accountable for studying productively. In the virtual environment, Study Ghibli provides users with a countdown timer to track the time spent studying and also provides a space for users to add their goals and assignments they wish to achieve during the study period.\nHow we built it\nWe designed our project by first creating a wireframe and then implementing our prototype on Figma.\nChallenges we ran into\nFirstly, none of us had any experience with using Figma, so getting started was very challenging. The Figma interface was also difficult to navigate in the beginning since the purposes of several functions and editing tools were unclear. Secondly, translating our ideas onto a website platform and planning out exactly how we were going to implement our prototype was hard since we had to account for several details, from the order of the pages of the website to creating buttons that made the user interface easier to use.\nAccomplishments that we're proud of\nOne notable achievement that we are proud of is our meticulous attention to detail. We tried our best to create an easily accessible and functional user interface while also decorating our prototype with vibrant themes. Since our coding skills were still intermediate, we focused our effort on graphic designing such as following a retro concept and creating smooth transitions.\nWhat we learned\nOur entire team started with no prior knowledge on how to use Figma or, for that matter, what software to build our prototype in the first place. By the end of the 24 hours, we have gained a solid understanding of the basics of Figma and UI/UX design. In addition, we have learned how to collaborate better as a team and how to exchange ideas and execute them. Lastly, this was our first hackathon experience, so that in itself was a new learning experience.\nWhat's next for Study Ghibli\nIn the future, we plan on bringing Study Ghibli to life by developing it into a functional website for our users. Additionally, we plan on creating a points and leaderboard system as an incentive for our users to stay focused for extended periods of time. By accumulating points, not only would users be able to climb up the leaderboard, but they would also earn more customizability options for their profile. Another feature that we plan on adding is for the user to be able to choose what songs and playlists they want to play as our current platform pre-selects songs for the user. Lastly, Study Ghibli also plans on giving the user an option to study in a private room with friends as an additional tool to help users keep each other accountable.", "link": "https://devpost.com/software/study-ghibli", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nduring online school last year, it was especially hard for many students to stay productive and motivated as well as connect with peers. in an effort to resolve this cycle of procrastination, we identified an important element to staying productive: keeping one another accountable. to this end, we developed an online platform, study ghibli, that would engage users in productive studying by facilitating a calm environment with relaxing music in conjunction with online peers to keep each other accountable.\nwhat it does\nstudy ghibli is a productivity platform which allows users to select ghibli themed virtual environments to study with. a defining feature of our platform is that there is an option for users to study in the company of people online, in order to keep each other accountable for studying productively. in the virtual environment, study ghibli provides users with a countdown timer to track the time spent studying and also provides a space for users to add their goals and assignments they wish to achieve during the study period.\nhow we built it\nwe designed our project by first creating a wireframe and then implementing our prototype on figma.\nchallenges we ran into\nfirstly, none of us had any experience with using figma, so getting started was very challenging. the figma interface was also difficult to navigate in the beginning since the purposes of several functions and editing tools were unclear. secondly, translating our ideas onto a website platform and planning out exactly how we were going to implement our prototype was hard since we had to account for several details, from the order of the pages of the website to creating buttons that made the user interface easier to use.\naccomplishments that we're proud of\none notable achievement that we are proud of is our meticulous attention to detail. we tried our best to create an easily accessible and functional user interface while also decorating our prototype with vibrant themes. since our coding skills were still intermediate, we focused our effort on graphic designing such as following a retro concept and creating smooth transitions.\nwhat we learned\nour entire team started with no prior knowledge on how to use figma or, for that matter, what software to build our prototype in the first place. by the end of the 24 hours, we have gained a solid understanding of the basics of figma and ui/ux design. in addition, we have learned how to collaborate better as a team and how to exchange ideas and execute them. lastly, this was our first hackathon experience, so that in itself was a new learning experience.\nwhat's next for study ghibli\nin the future, we plan on bringing study ghibli to life by developing it into a functional website for our users. additionally, we plan on creating a points and leaderboard system as an incentive for our users to stay focused for extended periods of time. by accumulating points, not only would users be able to climb up the leaderboard, but they would also earn more customizability options for their profile. another feature that we plan on adding is for the user to be able to choose what songs and playlists they want to play as our current platform pre-selects songs for the user. lastly, study ghibli also plans on giving the user an option to study in a private room with friends as an additional -----> tool !!!  to help users keep each other accountable.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59506290}, {"Unnamed: 0": 6314, "autor": "SME11", "date": null, "content": "Inspiration:\nThis pandemic has taught us many things during the past 2 years, including the importance of our olfactory senses. Many of us that were diagnosed with Covid-19 experienced a temporary loss of smell during which all food, beverage, and everyday scents tasted and felt bland. Our team decided to develop a solution for people with anosmia, those that suffer the loss of smell because they are one of the most underrepresented groups in the context of accessibility. Due to the invisible nature of this disability, there is a shortage in medical research, academic studies, as well as accessibility attention to anosmic people today. Our team realized that anosmia takes away a significant portion of human joy that many of us take for granted, and immediately decided to design an experience that is geared towards embracing anosmic people and helping them experience something that they have always been barred from.\nWhat it does:\nSme11 aims to help anosmic people experience scents through visual, auditory, and spatial senses via immersive VR experience. We also designed our solution to help hyposmia people (those with partial loss of smell) regain their lost senses through diagnoses of weakened senses using a smell test kit and repetitive exposure to those scents through selected perfumes. Smell training is known as an effective method to restore senses, especially for those that are temporarily caused by COVID-19 or stress. Moreover, as smell disorders are early signs of Parkinson\u2019s or Alzheimer\u2019s disease, we also hope to enable early detections for expedited treatments.\nHow we built it\nAs our purpose was to develop a solution that can enrich the lives of those suffering from anosmia, the Sme11 team focused thoroughly on designing for an immersive experience that is also non-intrusive to those that are already inconvenienced by their disabilities. Using a design tool called Figma, we crafted the user flows and UIs with the goal of designing for seamless touchpoints between our software/hardware and users. All visual images and texts on the Sme11 app were carefully chosen to be empathetic and non-biased against those suffering from loss of smell.\nChallenges we ran into\nOne of the biggest challenges was incorporating and streamlining our ideas into one solution. We invested the first half of our product development heavily in user research and problem definition to identify anosmic people\u2019s major pain points. We focused on targeting the edge case while narrowing down our ideas to provide an inclusive experience to all anosmic people.\nAccomplishments that we're proud of\nOne of the biggest hardships that people with disabilities face today is being left out and marginalized from the rest of our society. In that sense, anosmic people are even more vulnerable as anosmia is an invisible disability hidden from everyone else. We are proud to have developed a solution that addresses the needs of a sub-group that has been overlooked by society for a long time. Moreover, our intention was to design for a solution that benefits more than just the anomic population and also help those that are suffering from partial / temporary loss of smell. Through the use of a test kit that can diagnose one\u2019s weakened senses and a curated list of perfumes that serve as stimulus to restore those senses, Sme11 can truly bring benefits to all people to better experience the joys of smell.\nWhat we learned\nWe learned that not all disabilities have the same awareness levels. The lack of research and attention to olfactory-related disabilities demonstrates that there are still many opportunities to support disabilities that are less apparent and well-known. One finding was that some are quick to dismiss anosmia as less disability than others such as loss of sight or hearing but smell is as important\nWhat's next for SME11\nWe truly believe that Sme11 can be productized and implemented to help people with anosmia and hyposmia. Our immediate step is to take our prototype and engage with actual end-users to refine our value propositions and uncover gaps that we may have missed.", "link": "https://devpost.com/software/sme11", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration:\nthis pandemic has taught us many things during the past 2 years, including the importance of our olfactory senses. many of us that were diagnosed with covid-19 experienced a temporary loss of smell during which all food, beverage, and everyday scents tasted and felt bland. our team decided to develop a solution for people with anosmia, those that suffer the loss of smell because they are one of the most underrepresented groups in the context of accessibility. due to the invisible nature of this disability, there is a shortage in medical research, academic studies, as well as accessibility attention to anosmic people today. our team realized that anosmia takes away a significant portion of human joy that many of us take for granted, and immediately decided to design an experience that is geared towards embracing anosmic people and helping them experience something that they have always been barred from.\nwhat it does:\nsme11 aims to help anosmic people experience scents through visual, auditory, and spatial senses via immersive vr experience. we also designed our solution to help hyposmia people (those with partial loss of smell) regain their lost senses through diagnoses of weakened senses using a smell test kit and repetitive exposure to those scents through selected perfumes. smell training is known as an effective method to restore senses, especially for those that are temporarily caused by covid-19 or stress. moreover, as smell disorders are early signs of parkinson\u2019s or alzheimer\u2019s disease, we also hope to enable early detections for expedited treatments.\nhow we built it\nas our purpose was to develop a solution that can enrich the lives of those suffering from anosmia, the sme11 team focused thoroughly on designing for an immersive experience that is also non-intrusive to those that are already inconvenienced by their disabilities. using a design -----> tool !!!  called figma, we crafted the user flows and uis with the goal of designing for seamless touchpoints between our software/hardware and users. all visual images and texts on the sme11 app were carefully chosen to be empathetic and non-biased against those suffering from loss of smell.\nchallenges we ran into\none of the biggest challenges was incorporating and streamlining our ideas into one solution. we invested the first half of our product development heavily in user research and problem definition to identify anosmic people\u2019s major pain points. we focused on targeting the edge case while narrowing down our ideas to provide an inclusive experience to all anosmic people.\naccomplishments that we're proud of\none of the biggest hardships that people with disabilities face today is being left out and marginalized from the rest of our society. in that sense, anosmic people are even more vulnerable as anosmia is an invisible disability hidden from everyone else. we are proud to have developed a solution that addresses the needs of a sub-group that has been overlooked by society for a long time. moreover, our intention was to design for a solution that benefits more than just the anomic population and also help those that are suffering from partial / temporary loss of smell. through the use of a test kit that can diagnose one\u2019s weakened senses and a curated list of perfumes that serve as stimulus to restore those senses, sme11 can truly bring benefits to all people to better experience the joys of smell.\nwhat we learned\nwe learned that not all disabilities have the same awareness levels. the lack of research and attention to olfactory-related disabilities demonstrates that there are still many opportunities to support disabilities that are less apparent and well-known. one finding was that some are quick to dismiss anosmia as less disability than others such as loss of sight or hearing but smell is as important\nwhat's next for sme11\nwe truly believe that sme11 can be productized and implemented to help people with anosmia and hyposmia. our immediate step is to take our prototype and engage with actual end-users to refine our value propositions and uncover gaps that we may have missed.", "sortedWord": "None", "removed": "Nan", "score": 11, "comments": 0, "media": null, "medialink": null, "identifyer": 59506314}, {"Unnamed: 0": 6317, "autor": "Zen", "date": null, "content": "Zen\nthe modern hourglass\nRyan Chen, Akash Harapanahalli, Andrew Hellrigel, Jinwoo Park\nInspiration\nThere is no doubt that life the past few years has been hectic to say the least. That is why we built Zen, a device that is equal parts complex data-visualization tool and gorgeous art piece. Underneath the calming sandy surface of Zen lies a complex mechanism built from the scrapped parts of a broken 3d printer. A robotic arm magnetically guides a stainless steel marble in the sand to create grooves that can tell you the time, display text, and more. This combination of art with data reminds us that there is always beauty in information: you just need to dig a bit deeper.\nWhat it does\nAn onboard microcontroller first chooses what data it wishes to display: say the current time. Using computer vision, we calculate the coordinates that would carve out the numbers in the sand. These coordinates are passed through our custom pathing algorithm to determine a most efficient route, which is then sent to the mechanical assembly. An on-board computer translates the rectangular coordinates to polar coordinates to determine which angle and radius values to move the magnetic arm. As the arm traces its path under the surface of the table, the magnetic ball slowly imprints out the number in the sand, giving us a dynamic and physical means to visualize data.\nHow we built it\nTo view the plotting process in real time, follow the Colab notebook here. The project broke down into software focused on the plotting algorithm and hardware focusing on the mechanism of the magnetic arm. The software team used the opencv library to skeletonize the paths of a single-stroke font family. This allowed us to select the coordinates of the image that should be part of the letter/number and put them into our algorithm that will trace an optimal path through those points whilst removing outliers at the same time. This trace information can therefore be used to plot any set of letter and numbers, as long as they fit within the dimensions of the display.\nThe hardware side involved iterating over several designs until we settled on the current radially pointed magnetic arm. A lot of soldering, CADing, sawing, lasering, and drilling eventually gave us the semi-transparent design for Zen.\nChallenges we ran into\nOn the hardware side, we ran into several issues, the biggest being that the tolerances from our 3d prints were too tight, leading to our entire assembly being unable to spin freely around the center. But with a little ingenuity (and a little hammering), we managed to salvage the situation without having to cut, break, sand, or print any new parts. On the software side, we were dealing with an unfamiliar library and for some, a completely new language, so we first had to climb a learning curve to come out on the other end successful.\nAccomplishments that we're proud of\nWith this being the third hackathon we have completed together as team Big Fishies, we have noticed a significant increase in both our design quality and aesthetics. The overall look of the device and it's capabilities has demonstrated our collective growth as a team of engineers and a group of friends.\nWhat we learned\nWe came into this project idea unsure about whether we would be able to complete this goliath task over the course of the short 36 hours we were allotted. Learning new languages, new skills, and new techniques in such a short amount of time is never an easy task, but creating Zen has been the challenge that has proven there are no limits to the things we can accomplish.\nWhat's next for Zen\nWe have more plans for what sort of information we can display on our 550 square inch sand display. Using complex apis, we can display stock charts in real time, or even communicate wirelessly with a drawing application to allow users to draw their own paths in the sand. There are as many possibilities for Zen as there are grains of sand on Earth.", "link": "https://devpost.com/software/zen-k1w6ri", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "zen\nthe modern hourglass\nryan chen, akash harapanahalli, andrew hellrigel, jinwoo park\ninspiration\nthere is no doubt that life the past few years has been hectic to say the least. that is why we built zen, a device that is equal parts complex data-visualization -----> tool !!!  and gorgeous art piece. underneath the calming sandy surface of zen lies a complex mechanism built from the scrapped parts of a broken 3d printer. a robotic arm magnetically guides a stainless steel marble in the sand to create grooves that can tell you the time, display text, and more. this combination of art with data reminds us that there is always beauty in information: you just need to dig a bit deeper.\nwhat it does\nan onboard microcontroller first chooses what data it wishes to display: say the current time. using computer vision, we calculate the coordinates that would carve out the numbers in the sand. these coordinates are passed through our custom pathing algorithm to determine a most efficient route, which is then sent to the mechanical assembly. an on-board computer translates the rectangular coordinates to polar coordinates to determine which angle and radius values to move the magnetic arm. as the arm traces its path under the surface of the table, the magnetic ball slowly imprints out the number in the sand, giving us a dynamic and physical means to visualize data.\nhow we built it\nto view the plotting process in real time, follow the colab notebook here. the project broke down into software focused on the plotting algorithm and hardware focusing on the mechanism of the magnetic arm. the software team used the opencv library to skeletonize the paths of a single-stroke font family. this allowed us to select the coordinates of the image that should be part of the letter/number and put them into our algorithm that will trace an optimal path through those points whilst removing outliers at the same time. this trace information can therefore be used to plot any set of letter and numbers, as long as they fit within the dimensions of the display.\nthe hardware side involved iterating over several designs until we settled on the current radially pointed magnetic arm. a lot of soldering, cading, sawing, lasering, and drilling eventually gave us the semi-transparent design for zen.\nchallenges we ran into\non the hardware side, we ran into several issues, the biggest being that the tolerances from our 3d prints were too tight, leading to our entire assembly being unable to spin freely around the center. but with a little ingenuity (and a little hammering), we managed to salvage the situation without having to cut, break, sand, or print any new parts. on the software side, we were dealing with an unfamiliar library and for some, a completely new language, so we first had to climb a learning curve to come out on the other end successful.\naccomplishments that we're proud of\nwith this being the third hackathon we have completed together as team big fishies, we have noticed a significant increase in both our design quality and aesthetics. the overall look of the device and it's capabilities has demonstrated our collective growth as a team of engineers and a group of friends.\nwhat we learned\nwe came into this project idea unsure about whether we would be able to complete this goliath task over the course of the short 36 hours we were allotted. learning new languages, new skills, and new techniques in such a short amount of time is never an easy task, but creating zen has been the challenge that has proven there are no limits to the things we can accomplish.\nwhat's next for zen\nwe have more plans for what sort of information we can display on our 550 square inch sand display. using complex apis, we can display stock charts in real time, or even communicate wirelessly with a drawing application to allow users to draw their own paths in the sand. there are as many possibilities for zen as there are grains of sand on earth.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59506317}, {"Unnamed: 0": 6330, "autor": "EqualsOne", "date": null, "content": "Inspiration\nRemote learning still possesses challenges, particularly for math class. Taking digital notes is troublesome since you can't properly utilize mathematical notation on a keyboard. The level of interaction with the materials is limited due to the teacher not being physically present to showcase a core concept. And notably, teachers look to utilize a handful of resources to better convey information, leading to a lot of switching between applications. Our goal is to make the teaching and learning process easier by creating a single platform with tools and real time interactivity to make learning more streamlined. It\u2019s all in a place that Equals One.\nWhat it does\nThe hub currently includes tools to construct lessons with different forms of interaction, be it manual notes on a notepad, interactive graphs, video lessons, and any other tool teachers wish to embed.\nHow we built it\nApproaching this project with an introductory skill level, we implemented an example site through Google Sites, but recognize the strong benefits of a more sophisticated implementation through a tool such as Figma. We implemented embedded versions of the popular graphing application Desmos.\nChallenges we ran into\nOur skill level was fairly baseline, so we were unfortunately not able to implement a lot of core functions that we will address in our what's next category. We did not know how to do web based implementations of features such as drag and drop, live lectures, and a handwriting interface.\nAccomplishments that we're proud of\nWe are proud of our core fundamentals and what we wish to approach with the project moving forward! We take pride in the idea of making math more interactive, especially moving forward from programs such as WebAssign that students do not find very interactive.\nWhat we learned\nWe have ideas for implementations of functions we would like, and know about the tools now we would use to implement them in say a future iteration of our project. We also learned the challenge of moving quickly in a hackathon!\nWhat's next for EqualsOne\nThere are numerous features we wish to implement.\nReal time interactive presentations with a lecture: a teacher would provide a custom room to an \"online classroom\" where they can handwrite equations and functions that the student can simply \"drag over\" to their personal notebook.\nImplementation of a handwriting recognition API that can successfully convert handwritten equations to Desmos notation functions\nHistory tree of the initial equations provided by the teacher and any modifications made by the students so that students can cross reference how the instructor may have done it with any modification they may have made in their notes\nEditor suite for teachers so they may customize math assignments (ex. take in handwritten work, have math problem checks)", "link": "https://devpost.com/software/equalsone", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nremote learning still possesses challenges, particularly for math class. taking digital notes is troublesome since you can't properly utilize mathematical notation on a keyboard. the level of interaction with the materials is limited due to the teacher not being physically present to showcase a core concept. and notably, teachers look to utilize a handful of resources to better convey information, leading to a lot of switching between applications. our goal is to make the teaching and learning process easier by creating a single platform with tools and real time interactivity to make learning more streamlined. it\u2019s all in a place that equals one.\nwhat it does\nthe hub currently includes tools to construct lessons with different forms of interaction, be it manual notes on a notepad, interactive graphs, video lessons, and any other -----> tool !!!  teachers wish to embed.\nhow we built it\napproaching this project with an introductory skill level, we implemented an example site through google sites, but recognize the strong benefits of a more sophisticated implementation through a tool such as figma. we implemented embedded versions of the popular graphing application desmos.\nchallenges we ran into\nour skill level was fairly baseline, so we were unfortunately not able to implement a lot of core functions that we will address in our what's next category. we did not know how to do web based implementations of features such as drag and drop, live lectures, and a handwriting interface.\naccomplishments that we're proud of\nwe are proud of our core fundamentals and what we wish to approach with the project moving forward! we take pride in the idea of making math more interactive, especially moving forward from programs such as webassign that students do not find very interactive.\nwhat we learned\nwe have ideas for implementations of functions we would like, and know about the tools now we would use to implement them in say a future iteration of our project. we also learned the challenge of moving quickly in a hackathon!\nwhat's next for equalsone\nthere are numerous features we wish to implement.\nreal time interactive presentations with a lecture: a teacher would provide a custom room to an \"online classroom\" where they can handwrite equations and functions that the student can simply \"drag over\" to their personal notebook.\nimplementation of a handwriting recognition api that can successfully convert handwritten equations to desmos notation functions\nhistory tree of the initial equations provided by the teacher and any modifications made by the students so that students can cross reference how the instructor may have done it with any modification they may have made in their notes\neditor suite for teachers so they may customize math assignments (ex. take in handwritten work, have math problem checks)", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59506330}, {"Unnamed: 0": 6362, "autor": "ViLePanion", "date": null, "content": "VILEPANION - BETTER COMPANION BETTER STUDENTS\nInspiration\nDue to the ineffectiveness of learning in an ordinary way not only during the pandemic but has existed for a long time, our team planned to solve the problem and came up with a recognization of students' different ways of absorbing knowledge which has a significant impact on study\u2019s performance. We decided to create a website that allows the students to study in their proper learning style which consists of four core types: visual, auditory, reading and writing, and kinesthetic. The website will ultimately improve and enhance the ability of individual learners as well as increase learning enjoyment.\nWhat it does\nViLePanion provides the student with a survey to determine the student's suitable percentage for each learning style and conducts a result of which learning style suits the student the best. The student can do research and study the materials from the teachers on the website that are sorted according to the survey's result percentage.\nHow we built it\nWe used HTML, CSS for coding the front-end, and PHP, SQL for the back-end.\nChallenges we ran into\nDue to the miscommunication of the first time working as a team, we faced some misunderstandings and needless struggles in comprehending each other's opinions. Moreover, when brainstorming ideas, there were many times that we got stuck and could not find the solution therefore a lot of incomplete plans had to be canceled.\nAccomplishments that we're proud of\nBy utilizing the two design tools that we have just known, Figma and Powtoon our team successfully completed the prototype and the website before our intended time.\nWhat we learned\nTogether we learned how to work with Figma and Powtoon, a tool that we have not used before PlutoHacks, to design the prototype as well as the presentation video. Besides, PHP is a new programming language that Mei and Andrea got the first chance to get to know.\nWhat's next for ViLePanion\nViLePanion is believed to aid in learning overall in the future, therefore, we will create a mobile version and add in more functions to increase the interaction between the student and the teacher such as the comment section after every lecture so that the student can ask questions and receive answers from the teacher. Moreover, ViLePanion will be upgraded to be an Electronic Contact Book that assists the parents in tracking their children's learning status, grades, upcoming deadlines, and homework.", "link": "https://devpost.com/software/scholar-search", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "vilepanion - better companion better students\ninspiration\ndue to the ineffectiveness of learning in an ordinary way not only during the pandemic but has existed for a long time, our team planned to solve the problem and came up with a recognization of students' different ways of absorbing knowledge which has a significant impact on study\u2019s performance. we decided to create a website that allows the students to study in their proper learning style which consists of four core types: visual, auditory, reading and writing, and kinesthetic. the website will ultimately improve and enhance the ability of individual learners as well as increase learning enjoyment.\nwhat it does\nvilepanion provides the student with a survey to determine the student's suitable percentage for each learning style and conducts a result of which learning style suits the student the best. the student can do research and study the materials from the teachers on the website that are sorted according to the survey's result percentage.\nhow we built it\nwe used html, css for coding the front-end, and php, sql for the back-end.\nchallenges we ran into\ndue to the miscommunication of the first time working as a team, we faced some misunderstandings and needless struggles in comprehending each other's opinions. moreover, when brainstorming ideas, there were many times that we got stuck and could not find the solution therefore a lot of incomplete plans had to be canceled.\naccomplishments that we're proud of\nby utilizing the two design tools that we have just known, figma and powtoon our team successfully completed the prototype and the website before our intended time.\nwhat we learned\ntogether we learned how to work with figma and powtoon, a -----> tool !!!  that we have not used before plutohacks, to design the prototype as well as the presentation video. besides, php is a new programming language that mei and andrea got the first chance to get to know.\nwhat's next for vilepanion\nvilepanion is believed to aid in learning overall in the future, therefore, we will create a mobile version and add in more functions to increase the interaction between the student and the teacher such as the comment section after every lecture so that the student can ask questions and receive answers from the teacher. moreover, vilepanion will be upgraded to be an electronic contact book that assists the parents in tracking their children's learning status, grades, upcoming deadlines, and homework.", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59506362}, {"Unnamed: 0": 6364, "autor": "Mob", "date": null, "content": "Inspiration\nAs we emerge from the pandemic, we also gradually adjust to socializing more often with others again. But making friends can be difficult, and the pandemic hasn\u2019t helped. To become friends with someone, it\u2019s important to consistently see each other, but making plans with many people can be difficult when you have to consider everyone\u2019s schedules and preferences. Eventually, this issue led to a solution that we thought of, which is creating a mobile app to plan a meal date with a group of friends.\nWhat it does\nMob is an app to help people decide where, when, and what to eat. Making plans can be hard when you have to juggle everyone\u2019s different tastes and schedules - Mob makes it easy to eat with friends. Plans are more concrete because users find what time works best for them during their busy schedules to catch up with people they haven\u2019t talked to in a long time. The social aspect touches a bit on mental health benefits since many might have felt lonely during quarantine. Our app allows people to find and make new friends while eating a delicious meal. Mob also reinvigorates communities and builds new ones by encouraging people to try new food places to help small businesses.\nHow we built it\nWe utilized Figma for our design and prototyping. We used HTML, CSS, and React.js on Visual Studio Code to code our final product. Additionally, we used GitHub for version control and Google Firebase for cloud database services and web hosting.\nChallenges we ran into\nSince most of us were inexperienced with react.js, we were learning the tool while we code. There were occasions when we encountered an error by trying to run the code, so we had to debug in the terminal. The CSS aspect of the screen display was also taxing because we wanted the display to be both mobile and desktop-friendly. For instance, the calendar that schedules available days did not fully show up on the mobile version, so we had to figure out a way to scroll the screen, and make sure that every element is centered. Another challenge was implementing a Google Maps API to search nearby restaurants. We were able to display an interactive map, but we did not have enough time to complete our implementation of the Google Maps Places API to search for restaurants.\nAccomplishments that we're proud of\nWe\u2019re proud of our brainstorm session when we wireframed our UI/UX design on Figma after choosing a color scheme, target audience, and text fonts. Another accomplishment was coding our home and lobby screens. We successfully coded text boxes for user input, a dropdown menu for dietary restrictions, and a calendar to update meeting times preferences. Finally, we were able to generate a new URL link for every user who wants to create a lobby.\nWhat we learned\nWe learned how to navigate Figma and react.js to create frames. With Figma, we specifically learned how to make button functions to improve our UI app. Then, we transformed exactly what we wanted into reality with react, which was a new platform for many of us. This means learning how to put HTML code into the JavaScript file, and knowing how to not overwrite code when doing Github commits. We also have a dropdown menu that not only lets the user selects dietary preferences but also lets the user add their own preference. Moreover, most of the team experienced their first-ever hackathon with DubHacks, so we have learned a lot about the processes of submission, judging, and working with a team on a project that attempts to solve a problem in under 24 hours.\nWhat's next for Mob\nWe are always thinking about interconnecting people\u2019s lives and fostering relationships among peers through food. In the future, we can expand on showing more details about restaurants such as customer reviews and images of the dishes. Furthermore, we would like to build on to the voting section by coding a random generator to randomly select a restaurant. We can also implement a login system to keep track of the user\u2019s preferences and past restaurants so the user doesn\u2019t have to keep answering the same questions because the information will be saved.", "link": "https://devpost.com/software/mob-ejyq7k", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nas we emerge from the pandemic, we also gradually adjust to socializing more often with others again. but making friends can be difficult, and the pandemic hasn\u2019t helped. to become friends with someone, it\u2019s important to consistently see each other, but making plans with many people can be difficult when you have to consider everyone\u2019s schedules and preferences. eventually, this issue led to a solution that we thought of, which is creating a mobile app to plan a meal date with a group of friends.\nwhat it does\nmob is an app to help people decide where, when, and what to eat. making plans can be hard when you have to juggle everyone\u2019s different tastes and schedules - mob makes it easy to eat with friends. plans are more concrete because users find what time works best for them during their busy schedules to catch up with people they haven\u2019t talked to in a long time. the social aspect touches a bit on mental health benefits since many might have felt lonely during quarantine. our app allows people to find and make new friends while eating a delicious meal. mob also reinvigorates communities and builds new ones by encouraging people to try new food places to help small businesses.\nhow we built it\nwe utilized figma for our design and prototyping. we used html, css, and react.js on visual studio code to code our final product. additionally, we used github for version control and google firebase for cloud database services and web hosting.\nchallenges we ran into\nsince most of us were inexperienced with react.js, we were learning the -----> tool !!!  while we code. there were occasions when we encountered an error by trying to run the code, so we had to debug in the terminal. the css aspect of the screen display was also taxing because we wanted the display to be both mobile and desktop-friendly. for instance, the calendar that schedules available days did not fully show up on the mobile version, so we had to figure out a way to scroll the screen, and make sure that every element is centered. another challenge was implementing a google maps api to search nearby restaurants. we were able to display an interactive map, but we did not have enough time to complete our implementation of the google maps places api to search for restaurants.\naccomplishments that we're proud of\nwe\u2019re proud of our brainstorm session when we wireframed our ui/ux design on figma after choosing a color scheme, target audience, and text fonts. another accomplishment was coding our home and lobby screens. we successfully coded text boxes for user input, a dropdown menu for dietary restrictions, and a calendar to update meeting times preferences. finally, we were able to generate a new url link for every user who wants to create a lobby.\nwhat we learned\nwe learned how to navigate figma and react.js to create frames. with figma, we specifically learned how to make button functions to improve our ui app. then, we transformed exactly what we wanted into reality with react, which was a new platform for many of us. this means learning how to put html code into the javascript file, and knowing how to not overwrite code when doing github commits. we also have a dropdown menu that not only lets the user selects dietary preferences but also lets the user add their own preference. moreover, most of the team experienced their first-ever hackathon with dubhacks, so we have learned a lot about the processes of submission, judging, and working with a team on a project that attempts to solve a problem in under 24 hours.\nwhat's next for mob\nwe are always thinking about interconnecting people\u2019s lives and fostering relationships among peers through food. in the future, we can expand on showing more details about restaurants such as customer reviews and images of the dishes. furthermore, we would like to build on to the voting section by coding a random generator to randomly select a restaurant. we can also implement a login system to keep track of the user\u2019s preferences and past restaurants so the user doesn\u2019t have to keep answering the same questions because the information will be saved.", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59506364}, {"Unnamed: 0": 6372, "autor": "Clikr", "date": null, "content": "Inspiration\nAs our world becomes increasingly digitized, it can be difficult for those who are technologically challenged to overcome the learning curve of technology today. Some individuals are barred from many useful and often necessary resources because they find web apps inaccessible and difficult to understand. For those willing to help, some sites are designed in a way that is difficult to explain and meant to be intuitive to those familiar with the web. We decided to find a solution to the question: How can we utilize automation to develop an accessible web navigation tool for individuals who struggle digitally?\nWhat it does\nClikr is a Google Chrome extension that aids users in navigating the web. On one side, contributors (technologically-savvy individuals) opt in to have the tool collect data their site navigation and input data so that common user click sequences and inputs are stored in a database. On the other, users (those who have difficulty navigating the web) choose a key action relating to their site needs. This key action is mapped to web elements and navigates the user to their desired site page automatically by outputting a common click sequence relating to the keyword.\nHow we built it\nWe built Clikr by creating a Google Chrome extension with two primary features. The extension collects data from contributors' click sequences for different sites by utilizing a REST API to store the data in a MongoDB database. This data is then applied when a user clicks on a keyword. Data is pulled from the database and matched with elements on the site page to invoke JavaScript functions to click through webpages until the desired information is reached.\nChallenges we ran into\nWe initially had difficulty developing a REST API to connect a MongoDB database with the Chrome extension so that HTTP requests could retrieve and output data. We had not worked with database connections before, so it came with a steep learning curve and lots of troubleshooting. We also worked to ensure the click sequence on the user end functioned correctly due to the code originally only moving through a few of the pages in a sequence. This was especially challenging because it involved messaging between multiple scripts, since actions are triggered when the user interacts with the popup (popup.js), the content script is what controls the current webpage, and the background script is the only script that maintains state while the webpage changes. We had difficulty determining how to time the messaging between the background and content scripts so that the message is sent after the page is loaded, and we needed a way for the background script to know when to send the next message for the next \"click.\" After trying multiple methods, we were able to find one that works reliably.\nAccomplishments that we're proud of\nSetting up the database was a process we were proud of once all the code fell into place. We were also proud to successfully develop the automated clickthrough function that retrieved both IDs and InnerHTML elements to ensure a linked item could always be clicked despite HTML differences in webpages. Additionally, we developed a \"learning\" mode that teaches users how to navigate the web with ease by pinpointing where users need to click to find a specific feature or needed information.\nCommunity Impact\nFor businesses, Clikr would reduce call center traffic for website navigation issues and increase overall user experience of products by improving their usability for technologically-challenged groups. Socially, contributors could help to improve technological accessibility without any inconvenience to themselves by allowing their data to be collected by Clikr. As a result, site navigation barriers that typically deter individuals from using the web would be removed, and these individuals would be able to feel more accepted into our increasingly digital world.\nWhat we learned\nA majority of our team had no experience developing Google Chrome Extensions, so it was a new and exciting experience for us to develop a working extension that connected to a database and had many complicated features. Our experience gave us a lot of insight on the wide range of uses for Chrome extensions and the ease at which developers can test these extensions directly on Google Chrome.\nWhat's next for Clikr\nWe would like to further improve the \u201clearning\u201d mode for Clikr by developing a text-to-speech API that would narrate instructions to users for even greater ease of use. Furthermore, rather than pulling keywords directly from HTML elements, we would use Natural Language Processing techniques to analyze what user action a sequence may fall under based on common keywords to better accommodate to users and their needs.", "link": "https://devpost.com/software/clikr", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nas our world becomes increasingly digitized, it can be difficult for those who are technologically challenged to overcome the learning curve of technology today. some individuals are barred from many useful and often necessary resources because they find web apps inaccessible and difficult to understand. for those willing to help, some sites are designed in a way that is difficult to explain and meant to be intuitive to those familiar with the web. we decided to find a solution to the question: how can we utilize automation to develop an accessible web navigation -----> tool !!!  for individuals who struggle digitally?\nwhat it does\nclikr is a google chrome extension that aids users in navigating the web. on one side, contributors (technologically-savvy individuals) opt in to have the tool collect data their site navigation and input data so that common user click sequences and inputs are stored in a database. on the other, users (those who have difficulty navigating the web) choose a key action relating to their site needs. this key action is mapped to web elements and navigates the user to their desired site page automatically by outputting a common click sequence relating to the keyword.\nhow we built it\nwe built clikr by creating a google chrome extension with two primary features. the extension collects data from contributors' click sequences for different sites by utilizing a rest api to store the data in a mongodb database. this data is then applied when a user clicks on a keyword. data is pulled from the database and matched with elements on the site page to invoke javascript functions to click through webpages until the desired information is reached.\nchallenges we ran into\nwe initially had difficulty developing a rest api to connect a mongodb database with the chrome extension so that http requests could retrieve and output data. we had not worked with database connections before, so it came with a steep learning curve and lots of troubleshooting. we also worked to ensure the click sequence on the user end functioned correctly due to the code originally only moving through a few of the pages in a sequence. this was especially challenging because it involved messaging between multiple scripts, since actions are triggered when the user interacts with the popup (popup.js), the content script is what controls the current webpage, and the background script is the only script that maintains state while the webpage changes. we had difficulty determining how to time the messaging between the background and content scripts so that the message is sent after the page is loaded, and we needed a way for the background script to know when to send the next message for the next \"click.\" after trying multiple methods, we were able to find one that works reliably.\naccomplishments that we're proud of\nsetting up the database was a process we were proud of once all the code fell into place. we were also proud to successfully develop the automated clickthrough function that retrieved both ids and innerhtml elements to ensure a linked item could always be clicked despite html differences in webpages. additionally, we developed a \"learning\" mode that teaches users how to navigate the web with ease by pinpointing where users need to click to find a specific feature or needed information.\ncommunity impact\nfor businesses, clikr would reduce call center traffic for website navigation issues and increase overall user experience of products by improving their usability for technologically-challenged groups. socially, contributors could help to improve technological accessibility without any inconvenience to themselves by allowing their data to be collected by clikr. as a result, site navigation barriers that typically deter individuals from using the web would be removed, and these individuals would be able to feel more accepted into our increasingly digital world.\nwhat we learned\na majority of our team had no experience developing google chrome extensions, so it was a new and exciting experience for us to develop a working extension that connected to a database and had many complicated features. our experience gave us a lot of insight on the wide range of uses for chrome extensions and the ease at which developers can test these extensions directly on google chrome.\nwhat's next for clikr\nwe would like to further improve the \u201clearning\u201d mode for clikr by developing a text-to-speech api that would narrate instructions to users for even greater ease of use. furthermore, rather than pulling keywords directly from html elements, we would use natural language processing techniques to analyze what user action a sequence may fall under based on common keywords to better accommodate to users and their needs.", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59506372}, {"Unnamed: 0": 6376, "autor": "open source web scraping", "date": null, "content": "Inspiration\nUnder the open innovation track for this hackathon, we decided that we automate our search for open source projects online since we as a team shared this common interest. As we are all striving to become better developers in our perspective fields, there are countless methods to accomplish that, one of the more common methods today is by contributing to open source projects. We stumbled upon a website (codetriage.com) that provides thousand of open source repositories, but then we met another problem in our quest to become better developers. So we asked ourselves how can we filter the thoousands of projects and place them a list that we have easy access to?\nWhat it does\nWe developed a program that extracts data from codetriage.com using a webscraping tool called BeautifulSoup. This program provides users with a CSV file of open source projects based on their preferences.\nHow we built it\nWe used python and the beautiful soup library\nChallenges we ran into\nConverting the data we scraped from the website into csv file\nAccomplishments that we're proud of\nWe were able to obtain our filtered out data according to our preferencies\nWhat we learned\nweb-scraping enhanced python skills\nWhat's next for open source web scraping\nadding an algorithm to allow us filter on more conditions", "link": "https://devpost.com/software/open-source-web-scraping", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nunder the open innovation track for this hackathon, we decided that we automate our search for open source projects online since we as a team shared this common interest. as we are all striving to become better developers in our perspective fields, there are countless methods to accomplish that, one of the more common methods today is by contributing to open source projects. we stumbled upon a website (codetriage.com) that provides thousand of open source repositories, but then we met another problem in our quest to become better developers. so we asked ourselves how can we filter the thoousands of projects and place them a list that we have easy access to?\nwhat it does\nwe developed a program that extracts data from codetriage.com using a webscraping -----> tool !!!  called beautifulsoup. this program provides users with a csv file of open source projects based on their preferences.\nhow we built it\nwe used python and the beautiful soup library\nchallenges we ran into\nconverting the data we scraped from the website into csv file\naccomplishments that we're proud of\nwe were able to obtain our filtered out data according to our preferencies\nwhat we learned\nweb-scraping enhanced python skills\nwhat's next for open source web scraping\nadding an algorithm to allow us filter on more conditions", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59506376}, {"Unnamed: 0": 6378, "autor": "Union Shield", "date": null, "content": "Inspiration\nAcross the world, labor is undergoing a revolution. Sometimes quiet, when legislators and federal judges are pushed towards supporting employees\u2019 rights in the workplace, and sometimes loud, with protests against the draconian working conditions enforced by profiteering corporations. Unions have long been the average worker\u2019s answer to the ultra-powerful conglomerates, leading to higher average wages, better health, and higher overall job satisfaction. However, according to the Bureau of Labor Statistics, labor union density as a function of population has dropped by half over the last 35 years. It\u2019s no coincidence that the United States\u2019 Gini Coefficient, a statistic representing the overall wealth inequality in a nation, has been steadily rising as well. In fact, in terms of wealth inequality, the United States is 6th to last out of the 38 member countries of the Organization for Economic Cooperation and Development. The COVID-19 pandemic has only exacerbated the issue, resulting in dramatic blue-collar job losses across the board - only now without the security of unions to back many of the workers up.\nSo why have unions been on the decline? It\u2019s a complex problem resulting from multiple compounding factors, but foremost among them has been the rise of union-busting tactics in modern corporations, such as strike-breaking and illegal threats of losing one\u2019s job. It\u2019s a problem that continues to rear its head even today, as evidenced by the controversy surrounding Amazon\u2019s treatment of factory workers, and cheap outsourced (sometimes child) labor by most major manufacturing firms.\nIn doing preliminary research, we discovered that there were some resources in place for workers to address poor working conditions, such as the National Labor Relations Bureau, but the resources were poorly maintained and difficult to access. In fact, the official NLRB government website was down for the entire period of the hackathon with no updates or information regarding when it would be back. We decided it would be a worthwhile and effective endeavor to modernize the solutions for worker empowerment currently available, while also working to preserve the unions that have empowered Americans since the 19th century.\nWhat it does\nOur website will inform workers who are unsure of unionizing the benefits of doing so and how they can get involved. In addition, our website enables laborers to essentially review their employer and visualize how well the employer-employee relationship is at that company.\nThe website can be broken down into three major sections - Education, Workplace Reporting, and Employer Search.\nThe Education section of the website is purely informational - it provides users with explanations of unions, their benefits, and your rights as a worker. We also included information about legislative initiatives to expand workers\u2019 and unions\u2019 rights. This information is typically obscured or heavily buried in difficult-to-navigate government websites, so we decided to make the facts known clearly and simply right on our homepage.\nThe Workplace Reporting section is the cornerstone of our service\u2019s functionality. We constructed a series of questionnaires revolving around the 4 core worker\u2019s rights as delineated by the National Labor Relations Bureau: the right to fair compensation, health and safety, workplace equality, and unionization. Site visitors have the option to complete a workplace report revolving around one of the four core rights - an especially useful functionality if a user believes his company has committed infractions regarding one of the core rights and wants quick answers regarding the extent of the issues. Each of the 4 core right questionnaires returns a percentage score when complete, with lower percentages indicating greater infractions of the core workers\u2019 rights. Users also have the option to perform a Full Survey, which acts as a combination of all four Core Rights questionnaires and returns a numerical aggregate score known as the Employee Support Score, or ESS. A high ESS signifies that a company has demonstrated significant employee support, whereas a low ESS means the opposite.\nThe Employer Search part of the website goes hand-in-hand with the Workplace Reporting feature, as it lets users look up a wide variety of companies and their respective Employee Support Scores, along with the company\u2019s ratings on the four cornerstone metrics mentioned previously. The ESS displayed is an average of the employee ratings of a company submitted to our service through the Workplace Reporting functionality. This section of the website allows users to discover supportive workplaces and avoid unsupportive ones, helping them successfully navigate a job environment that is increasingly hostile to the individual worker.\nHow we built it\nFirst, we set everyone up with the necessary tools for our project; everybody installed node.js and set up npm within powershell to run our website locally. Then we divided up the initial tasks, having two of us work on the initial website framework, deciding the color scheme, and designing the logo, and the other two researched the specifics and statistics behind unionization and developed the Employee Support Score (ESS) metric. Once we had all the building blocks set up, all of us began programming. Two of us worked on the backend to develop the framework for the survey and the database integration, while the other two worked on the front end to display all the statistics and information we desired. At midnight, integration hell struck: putting together the separate pieces of a project is never easy, however we knew that going into this entire ordeal and took the time to design around each other's needs. After a couple of hours of debugging, poking, prodding, breaking, and fixing, the website was finished. Finally, we all worked together to finish our deliverables, from scripting, animating, and narrating the video to composing this DevPost.\nChallenges we ran into\nThe main challenge we faced was working with completely new objects and programming languages. We primarily worked with Next React JavaScript which, to us, was basically an odd combination of HTML and CSS. Only one of us had experience with this language, the learning curve was incredibly steep but we pushed through. Using Next React JS enabled us to use the Shards and Material UI packages which led to a very clean website design. However, both the package and the language itself had minimal documentation and support - sifting through it to find how to even center an image was a challenge, there was no guarantee you would even find the right line of code. However, we all still have a good foundation in computer science so we decided to put that to good use, essentially coding using our intuition to guess exactly how different things would be implemented or how we should construct the architecture of the entire website to avoid certain problems, like redundant code between .js files, from the get go.\nAlongside the client side application, this was the first time any of us had touched mongoDB. We knew that to save this data we would have to use some sort of storage - this was actually the very first thing we even tackled, the backend. The first thing implemented onto the website was simply accessing a test database of movie titles. That list was also the last thing we removed from the file, it became an indicator that everything\u2019s connected and working as planned. Writing to the database, due to our architecture, was also exceptionally difficult seeing as at times we display different amounts of questions on our survey page. This means that the data you add into the database each submission also changes. However, after some time trying different things and debugging, we jumped that hurdle as well.\nAccomplishments that we're proud of\nAdaptability is key in the realm of computer science, a fact that we were forced to come to terms with today when we began the process of learning Next ReactJS. As a conglomeration of several programming languages, the debugging process was made incredibly arduous, and the lack of documentation simply compounded matters. The immense challenge made our eventual victory all the more sweet, as once we had learned the intricate tips, tricks, and syntaxes of the complex language, we were able to tease our project bit-by-bit into reality.\nDeveloping the visual style of our product was always something that was especially important to us, and we\u2019re thrilled with the end result. Through careful blending of palettes and minimalistic modern aesthetics, our website was transformed beyond simply a database interface into an elegant tool, one that would remain easy to use for anyone who picked it up. The visual aspect permeated all stages of the design process - since we were developing an online service, and not just a complex algorithm, we needed to make sure that we excelled in the field of UX. Whether that was designing a distinguished yet modern logo or building smooth transitions between our website elements, we ended up with a product that we knew we could be proud of.\nWhat we learned\nThis project provided an avenue into understanding some of the systemic problems members of our society\u2019s working class face, such as analyzing different forms of employer misconduct and anti-union practices. We gained an intricate understanding of a worker\u2019s rights and legal options, which were broken down into four core categories: the rights to fair pay, health and safety, equality, and unionization. Gaining an understanding of these rights proved crucial when developing our employee support score (ESS) metric, as companies with a higher ESS inherently demonstrated better support of their employees\u2019 fundamental rights. We also encountered many federal resources for the first time, such as the OSHA guidelines on workplace hazards and state minimum wage databases, and learned to navigate them and understand their uses.\nOn the technical side, this project served as a deep dive into enterprise-level web development architecture, incorporating both JavaScript interactive functionality and mongoDB integration. Since we used a wide variety of programming packages and services in our end product, they often interacted in confusing or unpredictable ways. It was imperative that we properly learned how to manage the integration of the disparate languages, and we dedicated a significant portion of our hackathon time towards achieving mastery over it. Traversing the snarls of code gave us a newfound appreciation for backend code developers, as even relatively simple UI elements could end up referencing classes incredibly deep in the file structure.\nWhat's next for Union Shield\nWe ran out of time to implement the functionality where users can add new companies to the database. Currently, users can only choose from our limited list of companies, and there are a few bugs that we need to iron out in selecting a workplace. Regardless, there are many future directions for Union Shield. We were thinking about using the ESS to help identify companies that are extremely mistreating their employees and anonymously reporting them to the NLRB for investigation. Since many dissatisfied workers are also probably looking for other jobs, we were thinking of adding a job searching aspect to our website where workers can search for new jobs based on work environment-related criteria. In addition, we could also add a forum to allow other discontent workers to unionize, thereby elevating Union Shield into a tool directly facilitating positive social change.", "link": "https://devpost.com/software/union-shield", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nacross the world, labor is undergoing a revolution. sometimes quiet, when legislators and federal judges are pushed towards supporting employees\u2019 rights in the workplace, and sometimes loud, with protests against the draconian working conditions enforced by profiteering corporations. unions have long been the average worker\u2019s answer to the ultra-powerful conglomerates, leading to higher average wages, better health, and higher overall job satisfaction. however, according to the bureau of labor statistics, labor union density as a function of population has dropped by half over the last 35 years. it\u2019s no coincidence that the united states\u2019 gini coefficient, a statistic representing the overall wealth inequality in a nation, has been steadily rising as well. in fact, in terms of wealth inequality, the united states is 6th to last out of the 38 member countries of the organization for economic cooperation and development. the covid-19 pandemic has only exacerbated the issue, resulting in dramatic blue-collar job losses across the board - only now without the security of unions to back many of the workers up.\nso why have unions been on the decline? it\u2019s a complex problem resulting from multiple compounding factors, but foremost among them has been the rise of union-busting tactics in modern corporations, such as strike-breaking and illegal threats of losing one\u2019s job. it\u2019s a problem that continues to rear its head even today, as evidenced by the controversy surrounding amazon\u2019s treatment of factory workers, and cheap outsourced (sometimes child) labor by most major manufacturing firms.\nin doing preliminary research, we discovered that there were some resources in place for workers to address poor working conditions, such as the national labor relations bureau, but the resources were poorly maintained and difficult to access. in fact, the official nlrb government website was down for the entire period of the hackathon with no updates or information regarding when it would be back. we decided it would be a worthwhile and effective endeavor to modernize the solutions for worker empowerment currently available, while also working to preserve the unions that have empowered americans since the 19th century.\nwhat it does\nour website will inform workers who are unsure of unionizing the benefits of doing so and how they can get involved. in addition, our website enables laborers to essentially review their employer and visualize how well the employer-employee relationship is at that company.\nthe website can be broken down into three major sections - education, workplace reporting, and employer search.\nthe education section of the website is purely informational - it provides users with explanations of unions, their benefits, and your rights as a worker. we also included information about legislative initiatives to expand workers\u2019 and unions\u2019 rights. this information is typically obscured or heavily buried in difficult-to-navigate government websites, so we decided to make the facts known clearly and simply right on our homepage.\nthe workplace reporting section is the cornerstone of our service\u2019s functionality. we constructed a series of questionnaires revolving around the 4 core worker\u2019s rights as delineated by the national labor relations bureau: the right to fair compensation, health and safety, workplace equality, and unionization. site visitors have the option to complete a workplace report revolving around one of the four core rights - an especially useful functionality if a user believes his company has committed infractions regarding one of the core rights and wants quick answers regarding the extent of the issues. each of the 4 core right questionnaires returns a percentage score when complete, with lower percentages indicating greater infractions of the core workers\u2019 rights. users also have the option to perform a full survey, which acts as a combination of all four core rights questionnaires and returns a numerical aggregate score known as the employee support score, or ess. a high ess signifies that a company has demonstrated significant employee support, whereas a low ess means the opposite.\nthe employer search part of the website goes hand-in-hand with the workplace reporting feature, as it lets users look up a wide variety of companies and their respective employee support scores, along with the company\u2019s ratings on the four cornerstone metrics mentioned previously. the ess displayed is an average of the employee ratings of a company submitted to our service through the workplace reporting functionality. this section of the website allows users to discover supportive workplaces and avoid unsupportive ones, helping them successfully navigate a job environment that is increasingly hostile to the individual worker.\nhow we built it\nfirst, we set everyone up with the necessary tools for our project; everybody installed node.js and set up npm within powershell to run our website locally. then we divided up the initial tasks, having two of us work on the initial website framework, deciding the color scheme, and designing the logo, and the other two researched the specifics and statistics behind unionization and developed the employee support score (ess) metric. once we had all the building blocks set up, all of us began programming. two of us worked on the backend to develop the framework for the survey and the database integration, while the other two worked on the front end to display all the statistics and information we desired. at midnight, integration hell struck: putting together the separate pieces of a project is never easy, however we knew that going into this entire ordeal and took the time to design around each other's needs. after a couple of hours of debugging, poking, prodding, breaking, and fixing, the website was finished. finally, we all worked together to finish our deliverables, from scripting, animating, and narrating the video to composing this devpost.\nchallenges we ran into\nthe main challenge we faced was working with completely new objects and programming languages. we primarily worked with next react javascript which, to us, was basically an odd combination of html and css. only one of us had experience with this language, the learning curve was incredibly steep but we pushed through. using next react js enabled us to use the shards and material ui packages which led to a very clean website design. however, both the package and the language itself had minimal documentation and support - sifting through it to find how to even center an image was a challenge, there was no guarantee you would even find the right line of code. however, we all still have a good foundation in computer science so we decided to put that to good use, essentially coding using our intuition to guess exactly how different things would be implemented or how we should construct the architecture of the entire website to avoid certain problems, like redundant code between .js files, from the get go.\nalongside the client side application, this was the first time any of us had touched mongodb. we knew that to save this data we would have to use some sort of storage - this was actually the very first thing we even tackled, the backend. the first thing implemented onto the website was simply accessing a test database of movie titles. that list was also the last thing we removed from the file, it became an indicator that everything\u2019s connected and working as planned. writing to the database, due to our architecture, was also exceptionally difficult seeing as at times we display different amounts of questions on our survey page. this means that the data you add into the database each submission also changes. however, after some time trying different things and debugging, we jumped that hurdle as well.\naccomplishments that we're proud of\nadaptability is key in the realm of computer science, a fact that we were forced to come to terms with today when we began the process of learning next reactjs. as a conglomeration of several programming languages, the debugging process was made incredibly arduous, and the lack of documentation simply compounded matters. the immense challenge made our eventual victory all the more sweet, as once we had learned the intricate tips, tricks, and syntaxes of the complex language, we were able to tease our project bit-by-bit into reality.\ndeveloping the visual style of our product was always something that was especially important to us, and we\u2019re thrilled with the end result. through careful blending of palettes and minimalistic modern aesthetics, our website was transformed beyond simply a database interface into an elegant -----> tool !!! , one that would remain easy to use for anyone who picked it up. the visual aspect permeated all stages of the design process - since we were developing an online service, and not just a complex algorithm, we needed to make sure that we excelled in the field of ux. whether that was designing a distinguished yet modern logo or building smooth transitions between our website elements, we ended up with a product that we knew we could be proud of.\nwhat we learned\nthis project provided an avenue into understanding some of the systemic problems members of our society\u2019s working class face, such as analyzing different forms of employer misconduct and anti-union practices. we gained an intricate understanding of a worker\u2019s rights and legal options, which were broken down into four core categories: the rights to fair pay, health and safety, equality, and unionization. gaining an understanding of these rights proved crucial when developing our employee support score (ess) metric, as companies with a higher ess inherently demonstrated better support of their employees\u2019 fundamental rights. we also encountered many federal resources for the first time, such as the osha guidelines on workplace hazards and state minimum wage databases, and learned to navigate them and understand their uses.\non the technical side, this project served as a deep dive into enterprise-level web development architecture, incorporating both javascript interactive functionality and mongodb integration. since we used a wide variety of programming packages and services in our end product, they often interacted in confusing or unpredictable ways. it was imperative that we properly learned how to manage the integration of the disparate languages, and we dedicated a significant portion of our hackathon time towards achieving mastery over it. traversing the snarls of code gave us a newfound appreciation for backend code developers, as even relatively simple ui elements could end up referencing classes incredibly deep in the file structure.\nwhat's next for union shield\nwe ran out of time to implement the functionality where users can add new companies to the database. currently, users can only choose from our limited list of companies, and there are a few bugs that we need to iron out in selecting a workplace. regardless, there are many future directions for union shield. we were thinking about using the ess to help identify companies that are extremely mistreating their employees and anonymously reporting them to the nlrb for investigation. since many dissatisfied workers are also probably looking for other jobs, we were thinking of adding a job searching aspect to our website where workers can search for new jobs based on work environment-related criteria. in addition, we could also add a forum to allow other discontent workers to unionize, thereby elevating union shield into a tool directly facilitating positive social change.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59506378}, {"Unnamed: 0": 6381, "autor": "Connect", "date": null, "content": "Inspiration\nWith COVID-19, we found it harder to network with others. We wanted to create a way for people to find each other after the event is done.\nWhat it does\nCONNECT allows participants at an event to network with each other. The participant will walk in the event and scan a QR code which will take them to the app. If not already done, they can create a new profile. Then they will share their socials while also seeing the socials of others at the event.\nEvent organizers can use the app to generate the event QR code that will let users join the event through the app.\nHow we built it\nFront-End: React Native & Figma Back-End: QRCode API through Node.js.\nChallenges we ran into\nWe feel that we were inexperienced to accomplish what we envisioned. We also felt that we did not have enough time to complete the project.\nWhat we learned\nWe learned a lot. We learned that Figma is a strong tool for prototyping and collaboration. We even learnt a little bit more about Node.js that we previous had. One of us strengthened our knowledge in React Native. We also learnt how to work with Git.\nWhat's next for Connect\nMaking a functional prototype.\nImplement Twilio (for sending the email with QRCode PDF)\nImplement Database (for social medias)", "link": "https://devpost.com/software/connect-d3o8l2", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwith covid-19, we found it harder to network with others. we wanted to create a way for people to find each other after the event is done.\nwhat it does\nconnect allows participants at an event to network with each other. the participant will walk in the event and scan a qr code which will take them to the app. if not already done, they can create a new profile. then they will share their socials while also seeing the socials of others at the event.\nevent organizers can use the app to generate the event qr code that will let users join the event through the app.\nhow we built it\nfront-end: react native & figma back-end: qrcode api through node.js.\nchallenges we ran into\nwe feel that we were inexperienced to accomplish what we envisioned. we also felt that we did not have enough time to complete the project.\nwhat we learned\nwe learned a lot. we learned that figma is a strong -----> tool !!!  for prototyping and collaboration. we even learnt a little bit more about node.js that we previous had. one of us strengthened our knowledge in react native. we also learnt how to work with git.\nwhat's next for connect\nmaking a functional prototype.\nimplement twilio (for sending the email with qrcode pdf)\nimplement database (for social medias)", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59506381}, {"Unnamed: 0": 6403, "autor": "Y-Budge", "date": null, "content": "Inspiration\nAs freshmen in college for the first time, we all experienced the pain of going over our budgets. It's surprising how much all our transactions add up to. So, we thought about how we could fix this problem and help our users stay within budget. We realized we could only help them stay within budget by discouraging spending and making them see the severe consequences of going over their budget.\nWouldn't you pause for a moment, if you knew that your pristine social media feed would be overtaken by a tool that would call you out for going over your budget and optionally display some of the transactions, that were definitely impulse decisions?\nYep, we leveraged the ultimate 21st-century tool - social media posts to discourage people from going over their budget.\nYou stay within your budget and your feed remains pristine, but exceed that budget and now you're immediately accountable to almost everyone you know. Now, that's a way to encourage people to avoid going over budget!\nWhat it does\nA user signs up for YBudge using Twitter OAuth and then proceeds to input their bank account log-in info and finally enters their maximum budget. YBudge then monitors their transactions and automatically sends out a tweet if they exceed their budget at any time.\nHow we built it\nWe used the MERN stack to build YBudge.\nFor their transaction data, we used the Teller API to allow users to connect their bank accounts and allow us to access their transaction history.\nAdditionally, we interfaced with the Twitter API to allow the user to give YBudge permission to tweet on their behalf through the user's Twitter account if they went over their budget.\nChallenges we ran into\nThe biggest challenge we encountered was implementing Plaid or Teller APIs that would help us get bank account data from users. We would ask the user to sign into their bank account using third-party software such as Plaid or Teller. We encountered several issues while testing and implementing out this feature. We would ask for the front end to show up; however, it would not. In the end, we figured out a way to show up the frontend consistently. Additionally, our feature with Twitter API had trouble running when we called a post request to send a tweet out. After much frustration and troubleshooting, we realized that the issue was with the post request was that it was set to the URL of the frontend portion of the website rather than the backend.\nAccomplishments that we're proud of\nI'm particularly proud of our application's functionality. I'm proud of implementing Twitter Authentication, Twitter API for posting tweets, and Teller API for Banking data. Our team jumped out of our seats when we were able to sign up, get the data from the bank, and post the correct information to Twitter for the first time.\nWhat we learned\nThrough the implementation of Y-budge, we have learned a lot about MERN Stack, Teller, Twitter Authentical API, Twitter API, and MongoDB. We have learned what can be done in 36 hours and the reward of completing a hackathon.\nWhat's next for Y-Budge\nFor Y-budge, we plan to finish implementing the planned features that we brainstormed throughout the event. Some additional functionality that can be implemented is being able to post messages on multiple social media platforms.", "link": "https://devpost.com/software/y-budge", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nas freshmen in college for the first time, we all experienced the pain of going over our budgets. it's surprising how much all our transactions add up to. so, we thought about how we could fix this problem and help our users stay within budget. we realized we could only help them stay within budget by discouraging spending and making them see the severe consequences of going over their budget.\nwouldn't you pause for a moment, if you knew that your pristine social media feed would be overtaken by a -----> tool !!!  that would call you out for going over your budget and optionally display some of the transactions, that were definitely impulse decisions?\nyep, we leveraged the ultimate 21st-century tool - social media posts to discourage people from going over their budget.\nyou stay within your budget and your feed remains pristine, but exceed that budget and now you're immediately accountable to almost everyone you know. now, that's a way to encourage people to avoid going over budget!\nwhat it does\na user signs up for ybudge using twitter oauth and then proceeds to input their bank account log-in info and finally enters their maximum budget. ybudge then monitors their transactions and automatically sends out a tweet if they exceed their budget at any time.\nhow we built it\nwe used the mern stack to build ybudge.\nfor their transaction data, we used the teller api to allow users to connect their bank accounts and allow us to access their transaction history.\nadditionally, we interfaced with the twitter api to allow the user to give ybudge permission to tweet on their behalf through the user's twitter account if they went over their budget.\nchallenges we ran into\nthe biggest challenge we encountered was implementing plaid or teller apis that would help us get bank account data from users. we would ask the user to sign into their bank account using third-party software such as plaid or teller. we encountered several issues while testing and implementing out this feature. we would ask for the front end to show up; however, it would not. in the end, we figured out a way to show up the frontend consistently. additionally, our feature with twitter api had trouble running when we called a post request to send a tweet out. after much frustration and troubleshooting, we realized that the issue was with the post request was that it was set to the url of the frontend portion of the website rather than the backend.\naccomplishments that we're proud of\ni'm particularly proud of our application's functionality. i'm proud of implementing twitter authentication, twitter api for posting tweets, and teller api for banking data. our team jumped out of our seats when we were able to sign up, get the data from the bank, and post the correct information to twitter for the first time.\nwhat we learned\nthrough the implementation of y-budge, we have learned a lot about mern stack, teller, twitter authentical api, twitter api, and mongodb. we have learned what can be done in 36 hours and the reward of completing a hackathon.\nwhat's next for y-budge\nfor y-budge, we plan to finish implementing the planned features that we brainstormed throughout the event. some additional functionality that can be implemented is being able to post messages on multiple social media platforms.", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59506403}, {"Unnamed: 0": 6404, "autor": "Revity", "date": null, "content": "Inspiration\nThis application stemmed from one of Pranay\u2019s close family members falling down and breaking his collarbone, which caused his family to rush to the nearest hospital. There, not only did they receive subpar service, but they were also charged with steep medical bills. Little did they know that the hospital in a nearby sector of town had much better ratings for the doctor but also much more affordable prices. Since Pranay's family was deeply troubled by this, they wanted to reduce people facing a similar situation. Therefore, we brainstormed an intuitive solution to combat similar situations in the real world.\nWhat it does\nRevity is a full-stack web application that shows you the closest hospitals nearby with crowdsourced surgery prices across various insurance providers, optimizing your time and maximizing savings in terms of crisis. It allows you to save time by not having to spend hours researching the best detectors in your area, but also allows you to navigate the most affordable options.\nHow we built it\nOne of our main goals was developing an effective tool that fully leveraged technological solutions to solve our problems. As such, one of these methods was developing a seamless user interface that excited users to try out the application. As such, one of the tools that we used to create the front-end was the platform Figma where we were able to perform wireframing in order to thoroughly plan out the application\u2019s front-end layout and features.\nThe backend architecture of the application consisted of a Python Flask REST API hosted on AWS (ECS, ECR, Copilot, Fargate). Behind our cloud infrastructure were Docker containers utilized to containerize and scale our applications vertically. Our front-end architecture consisted of an HTML/CSS/JSX and ReactJS application with a continuous deployment pipeline via Firebase hosting, allowing us to put greater focus on the user interface and experience while spending less time on managing the infrastructure. Connecting our front-end interfaces and back-end systems, we leveraged Google\u2019s Firestore NoSQL database to allow for a smooth transfer of information from both ends.\nChallenges we ran into\nWe originally had issues with creating a strong UI, but we overcame this challenge through the use of Figma and our approach using wireframing. Furthermore, we were having issues with deploying the CD pipeline, but we were eventually able to make it work after 2 attempts. Another issue that we ran into was difficulty around storing the session memory that needed to be stored in session storage. We were attempting to use Redux to solve this issue but we eventually narrowed down the solution to session storage after thoroughly researching other individuals' problems.\nAccomplishments that we're proud of\nHowever, despite these setbacks, we're very proud that we were able to create a minimal yet effective landing page that will attract users. Furthermore, the collaboration between 3 members using branches on Git often was difficult, but we are glad that we were able to sort those issues out to effectively produce our project.\nWhat we learned\nOverall, we improved our front-end skills significantly through this project, particularly through the use of React components and Axios in order to send information to the application\u2019s Flask-based back-end. We also learned how to integrate the Google Maps API into our web application in order to display the locations and crowdsourced surgery information of different hospitals. Furthermore, our group tackled the learning curve regarding DevOps by deploying a CD pipeline for the project.\nWhat's next for Revity\nWe will focus on information security in the future as well as the reliability of crowdsourced data points for use by the general public. Furthermore, we can improve the scale of the hospitals available on Revity's website by releasing our product to the general public, which will ensure trust between patients and doctors.\nAwards Consideration\nWe would like Revity to be considered for the following awards (General Track): HackGT - Best Social Impact, HackGT - Best Design, HackGT - Most Unique/Out of the Box/Creative, Best Use of Google Cloud, HackGT - Best Overall, BlackRock - Best Overall, Anthem - Best Overall", "link": "https://devpost.com/software/revity", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nthis application stemmed from one of pranay\u2019s close family members falling down and breaking his collarbone, which caused his family to rush to the nearest hospital. there, not only did they receive subpar service, but they were also charged with steep medical bills. little did they know that the hospital in a nearby sector of town had much better ratings for the doctor but also much more affordable prices. since pranay's family was deeply troubled by this, they wanted to reduce people facing a similar situation. therefore, we brainstormed an intuitive solution to combat similar situations in the real world.\nwhat it does\nrevity is a full-stack web application that shows you the closest hospitals nearby with crowdsourced surgery prices across various insurance providers, optimizing your time and maximizing savings in terms of crisis. it allows you to save time by not having to spend hours researching the best detectors in your area, but also allows you to navigate the most affordable options.\nhow we built it\none of our main goals was developing an effective -----> tool !!!  that fully leveraged technological solutions to solve our problems. as such, one of these methods was developing a seamless user interface that excited users to try out the application. as such, one of the tools that we used to create the front-end was the platform figma where we were able to perform wireframing in order to thoroughly plan out the application\u2019s front-end layout and features.\nthe backend architecture of the application consisted of a python flask rest api hosted on aws (ecs, ecr, copilot, fargate). behind our cloud infrastructure were docker containers utilized to containerize and scale our applications vertically. our front-end architecture consisted of an html/css/jsx and reactjs application with a continuous deployment pipeline via firebase hosting, allowing us to put greater focus on the user interface and experience while spending less time on managing the infrastructure. connecting our front-end interfaces and back-end systems, we leveraged google\u2019s firestore nosql database to allow for a smooth transfer of information from both ends.\nchallenges we ran into\nwe originally had issues with creating a strong ui, but we overcame this challenge through the use of figma and our approach using wireframing. furthermore, we were having issues with deploying the cd pipeline, but we were eventually able to make it work after 2 attempts. another issue that we ran into was difficulty around storing the session memory that needed to be stored in session storage. we were attempting to use redux to solve this issue but we eventually narrowed down the solution to session storage after thoroughly researching other individuals' problems.\naccomplishments that we're proud of\nhowever, despite these setbacks, we're very proud that we were able to create a minimal yet effective landing page that will attract users. furthermore, the collaboration between 3 members using branches on git often was difficult, but we are glad that we were able to sort those issues out to effectively produce our project.\nwhat we learned\noverall, we improved our front-end skills significantly through this project, particularly through the use of react components and axios in order to send information to the application\u2019s flask-based back-end. we also learned how to integrate the google maps api into our web application in order to display the locations and crowdsourced surgery information of different hospitals. furthermore, our group tackled the learning curve regarding devops by deploying a cd pipeline for the project.\nwhat's next for revity\nwe will focus on information security in the future as well as the reliability of crowdsourced data points for use by the general public. furthermore, we can improve the scale of the hospitals available on revity's website by releasing our product to the general public, which will ensure trust between patients and doctors.\nawards consideration\nwe would like revity to be considered for the following awards (general track): hackgt - best social impact, hackgt - best design, hackgt - most unique/out of the box/creative, best use of google cloud, hackgt - best overall, blackrock - best overall, anthem - best overall", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59506404}, {"Unnamed: 0": 6411, "autor": "UW Mind Hub", "date": null, "content": "Inspiration\nWe originally brainstormed many ideas for the campus track, ranging from a scheduling tool to connect student\u2019s calendars, creating a virtual map, and even calculating calories for food on campus. All very different things, with one thing in mind: wellness. We wanted to dive deep into the mental health and wellbeing of students.\nWhat it does\nOur application provides three features for users to choose from: Chat, Chill, and Compose.\nChat - Two separate sections of the chat option, one section is praise, where users can throw random praises at each other and receive praises as well. The other one is vent, where users can write down what bothers them, and watch it go into the trash. Then, after choosing an option, the app provides an anonymous chatting option, where users can answer questions about what they like or dislike and get paired up with other users that have the same interest.\nChill - The chill option also provides two sections, one section is music, where the user can choose a genre of music and listen to it while writing down ideas that come into their minds. The other section is travel, where the user can \u201ctravel\u201d to different locations and hear sounds from that location, just like actually traveling to the location.\nCompose - The compose option has three sections, one section is write, it allows users to write down and record their thoughts and feelings, this section provides a small section called calendar, which is a section provided to users who don\u2019t have enough time to write a full entry, but still want to record their emotions. The other section is called letters to strangers. This section is where users can compose a nice letter to a stranger, and another random user will receive this letter as a heartwarming present of the day.\nHow we built it\nWe looked to some meditation and journaling apps for inspiration, and chose to implement minimalist designs with pastel colors to fit our theme. We also split up the general work into 2 groups- graphics and implementation.\nWe decided for the app to adopt a calming color scheme of light purple, and bright yellow, for it is the color scheme of UW and produces a heartwarming effect of community, and relaxes the soul. With a choice of soft shapes and cute icons, this app helps students connect with each other and relieve stress.\nChallenges we ran into\nSome challenges we faced were deciding where to add more details and figuring out the most user-friendly options while designing the platform. We tried to keep the design minimalistic while making sure that the user will understand all the features of our application.\nAccomplishments that we're proud of\nWe are proud of how users using our application could throw random nice words and letters at each other like a sudden gift and recieve equal amounts of support from their peers. We are also proud of the warm atmosphere of our project that we are able to create.\nWhat we learned\nWe learned how to build a product from a problem to an idea to completion. We also learned how to use figma,and most importantly, teamwork.\nWhat's next for UW Mind Hub\nThe next step for UW Mind Hub is extending to arrange advising sessions or meditation sessions for students, and extending our chat feature to include more intresets. Possibly adding a new feature where you can choose lock your journal or choose to destory it.", "link": "https://devpost.com/software/uw-mind-hub", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe originally brainstormed many ideas for the campus track, ranging from a scheduling -----> tool !!!  to connect student\u2019s calendars, creating a virtual map, and even calculating calories for food on campus. all very different things, with one thing in mind: wellness. we wanted to dive deep into the mental health and wellbeing of students.\nwhat it does\nour application provides three features for users to choose from: chat, chill, and compose.\nchat - two separate sections of the chat option, one section is praise, where users can throw random praises at each other and receive praises as well. the other one is vent, where users can write down what bothers them, and watch it go into the trash. then, after choosing an option, the app provides an anonymous chatting option, where users can answer questions about what they like or dislike and get paired up with other users that have the same interest.\nchill - the chill option also provides two sections, one section is music, where the user can choose a genre of music and listen to it while writing down ideas that come into their minds. the other section is travel, where the user can \u201ctravel\u201d to different locations and hear sounds from that location, just like actually traveling to the location.\ncompose - the compose option has three sections, one section is write, it allows users to write down and record their thoughts and feelings, this section provides a small section called calendar, which is a section provided to users who don\u2019t have enough time to write a full entry, but still want to record their emotions. the other section is called letters to strangers. this section is where users can compose a nice letter to a stranger, and another random user will receive this letter as a heartwarming present of the day.\nhow we built it\nwe looked to some meditation and journaling apps for inspiration, and chose to implement minimalist designs with pastel colors to fit our theme. we also split up the general work into 2 groups- graphics and implementation.\nwe decided for the app to adopt a calming color scheme of light purple, and bright yellow, for it is the color scheme of uw and produces a heartwarming effect of community, and relaxes the soul. with a choice of soft shapes and cute icons, this app helps students connect with each other and relieve stress.\nchallenges we ran into\nsome challenges we faced were deciding where to add more details and figuring out the most user-friendly options while designing the platform. we tried to keep the design minimalistic while making sure that the user will understand all the features of our application.\naccomplishments that we're proud of\nwe are proud of how users using our application could throw random nice words and letters at each other like a sudden gift and recieve equal amounts of support from their peers. we are also proud of the warm atmosphere of our project that we are able to create.\nwhat we learned\nwe learned how to build a product from a problem to an idea to completion. we also learned how to use figma,and most importantly, teamwork.\nwhat's next for uw mind hub\nthe next step for uw mind hub is extending to arrange advising sessions or meditation sessions for students, and extending our chat feature to include more intresets. possibly adding a new feature where you can choose lock your journal or choose to destory it.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59506411}, {"Unnamed: 0": 6412, "autor": "Leetcode from Hell", "date": null, "content": "Inspiration\nEvery CS major we've ever talked to has complained about Leetcode at least once. And yet 100% of technical interviews for software engineering jobs and internships are comprised of Leetcode-style problems. So we're forced to try and fail at mastering the \"art\" of Leetcoding in what is a really vicious feedback loop that burns you out.\nWe decided to make Leetcode from Hell, Leetcode's evil-er twin who is (mostly) a troll version of Leetcode that punishes you for lethargy but is also quite funny when thought about on a meta level. With Leetcode from Hell, we hope to alleviate some of the stress and seriousness of Leetcode by illuminating just how ridiculous programming can become when it becomes a robotic exercise instead of the truly beautiful craft it is.\nWhat it does\nThe task at hand is simple and the same as on Leetcode, HackerRank, or any other online programming platform: solve the question by implementing the given method. But there's a catch. If, while writing your solution to the question, you do not type anything for 5 seconds (10 seconds for hards), all your code will be deleted and you will have to start over!\nObviously, the goal of this application is not to gain a perfect score on every question. It's a tongue-in-cheek parody of Leetcode and the discussion surrounding it. Friends who have demo'd the app have already attempted to speedrun solutions to each question while figuring out ways to make their code as concise as possible so they can submit their solution quicker. Just like how on Leetcode discussion pages there are countless users posting barely coherent solutions in hopes of making them as concise and efficient as possible (\"you can actually solve this this huge problem in one line...\"). The app can be cold and ruthless, but also has a playful personality brought through by colorful and funny emojis that hammer home the tongue-in-cheek nature of our project.\nThat being said, solving some of these questions definitely tested how well we knew our algorithms and how competitive we were willing to get to solve the questions, which was really fun!\nHow we built it\nOn the tech side, we used JavaScript, HTML, and CSS to build our application. We used repl.it to host Leetcode from Hell which allowed us to collaborate on committing changes in real time (like Google Drive), a super helpful tool we didn't know repl.it had!\nBeyond technological components, we wanted to have fun this hackathon because of how long it's been since we've been on campus. Creatively, we wanted to create an app that would delete your code if you stopped typing, and after observing the Leetcode stress this interview season, we decided to make it Leetcode-themed. It turned out well!\nChallenges we ran into\nWeb development isn't our forte, but we'd be lying if we said we didn't respect web developers. The amount of cursing and head-banging related just to CSS we've seen in our lives is enough to send us into backend development for an eternity. But even though we struggled, it was really fun! JavaScript, HTML, and CSS is some of the most Googleable CS related content and we couldn't have done this project without the help of StackOverflow and W3Schools.\nAnd the parts we thought would be the hardest (setting a timer up that clears code whenever you don't type, actually processing and running user-generated code and running it against a test suite) ended being some of the easiest. The hardest? Finding questions to use and verifying our solutions against our test cases! You try finding the longest common subsequence in an array at 2 AM when you have to keep typing once every 10 seconds in a language which thinks '5' + 3 = '53' \ud83d\ude12.\nAccomplishments that we're proud of\nIT LOOKS REALLY GOOD!!!! And it works with no bugs! (I've just jinxed us, haven't I.) The questions are diverse, and don't require a large understanding of JavaScript. We know most people are familiar with Java or Python for coding, so we made sure to accommodate for this. And most of all, we're glad we brought a laugh to the faces of those who told us how much they despised Leetcode.\nWhat we learned\nThat web development is as tedious as you make it. But most of the times, it's worth it if you want a better product. We worked hard to smooth around the corners and create an experience worth looking back on using technologies we weren't familiar with. We're proud of what we accomplished and even more excited for what we can build with our newfound knowledge of web development.\nWhat's next for Leetcode from Hell\nAdd more questions!\nThere's Always Better UI To Be Created. We can make our site smooth as butter and clean as a whistle.\nAdd solutions for the questions, MAYBE \ud83d\ude09", "link": "https://devpost.com/software/speed-i-am-speed", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nevery cs major we've ever talked to has complained about leetcode at least once. and yet 100% of technical interviews for software engineering jobs and internships are comprised of leetcode-style problems. so we're forced to try and fail at mastering the \"art\" of leetcoding in what is a really vicious feedback loop that burns you out.\nwe decided to make leetcode from hell, leetcode's evil-er twin who is (mostly) a troll version of leetcode that punishes you for lethargy but is also quite funny when thought about on a meta level. with leetcode from hell, we hope to alleviate some of the stress and seriousness of leetcode by illuminating just how ridiculous programming can become when it becomes a robotic exercise instead of the truly beautiful craft it is.\nwhat it does\nthe task at hand is simple and the same as on leetcode, hackerrank, or any other online programming platform: solve the question by implementing the given method. but there's a catch. if, while writing your solution to the question, you do not type anything for 5 seconds (10 seconds for hards), all your code will be deleted and you will have to start over!\nobviously, the goal of this application is not to gain a perfect score on every question. it's a tongue-in-cheek parody of leetcode and the discussion surrounding it. friends who have demo'd the app have already attempted to speedrun solutions to each question while figuring out ways to make their code as concise as possible so they can submit their solution quicker. just like how on leetcode discussion pages there are countless users posting barely coherent solutions in hopes of making them as concise and efficient as possible (\"you can actually solve this this huge problem in one line...\"). the app can be cold and ruthless, but also has a playful personality brought through by colorful and funny emojis that hammer home the tongue-in-cheek nature of our project.\nthat being said, solving some of these questions definitely tested how well we knew our algorithms and how competitive we were willing to get to solve the questions, which was really fun!\nhow we built it\non the tech side, we used javascript, html, and css to build our application. we used repl.it to host leetcode from hell which allowed us to collaborate on committing changes in real time (like google drive), a super helpful -----> tool !!!  we didn't know repl.it had!\nbeyond technological components, we wanted to have fun this hackathon because of how long it's been since we've been on campus. creatively, we wanted to create an app that would delete your code if you stopped typing, and after observing the leetcode stress this interview season, we decided to make it leetcode-themed. it turned out well!\nchallenges we ran into\nweb development isn't our forte, but we'd be lying if we said we didn't respect web developers. the amount of cursing and head-banging related just to css we've seen in our lives is enough to send us into backend development for an eternity. but even though we struggled, it was really fun! javascript, html, and css is some of the most googleable cs related content and we couldn't have done this project without the help of stackoverflow and w3schools.\nand the parts we thought would be the hardest (setting a timer up that clears code whenever you don't type, actually processing and running user-generated code and running it against a test suite) ended being some of the easiest. the hardest? finding questions to use and verifying our solutions against our test cases! you try finding the longest common subsequence in an array at 2 am when you have to keep typing once every 10 seconds in a language which thinks '5' + 3 = '53' \ud83d\ude12.\naccomplishments that we're proud of\nit looks really good!!!! and it works with no bugs! (i've just jinxed us, haven't i.) the questions are diverse, and don't require a large understanding of javascript. we know most people are familiar with java or python for coding, so we made sure to accommodate for this. and most of all, we're glad we brought a laugh to the faces of those who told us how much they despised leetcode.\nwhat we learned\nthat web development is as tedious as you make it. but most of the times, it's worth it if you want a better product. we worked hard to smooth around the corners and create an experience worth looking back on using technologies we weren't familiar with. we're proud of what we accomplished and even more excited for what we can build with our newfound knowledge of web development.\nwhat's next for leetcode from hell\nadd more questions!\nthere's always better ui to be created. we can make our site smooth as butter and clean as a whistle.\nadd solutions for the questions, maybe \ud83d\ude09", "sortedWord": "None", "removed": "Nan", "score": 6, "comments": 0, "media": null, "medialink": null, "identifyer": 59506412}, {"Unnamed: 0": 6470, "autor": "Logic Learner", "date": null, "content": "Inspiration\nDigital-logic circuitry forms the foundation of almost every modern device we use today. From laptops to calculators to fridges, the brains of our world are powered by digital circuitry.\nUnfortunately, the barrier of entry to learning is incredibly high. If you want to learn about digital logic, you have two options:\n1) Watch a lecture - While there are many wonderful professors out there, most digital logic classes require a very high amount of prerequisite knowledge. On top of that, these classes are usually taught by drawing static diagrams on the board and the professor explaining truth tables of logic gates and circuits. For kids and other beginners, this is very hard to follow and not very intuitive.\n2) Experiment with circuit simulators - If students could start right off by experimenting with actual circuits, they could build up the intuition required to understand these concepts. However, the lack of formal lesson structure combined with extremely complex and unintuitive user interfaces causes these tools to be very difficult to use without prior understanding of the material.\nBecause of this, most people never get the chance to learn how the circuits that run our lives actually work. In fact, even many programmers don't really understand how the CPUs in their machines execute the commands they provide. For students or children who are disadvantaged or have less access to resources, educational resources on these concepts can be almost impossible to reach.\nLogic Learner aims to change that.\nWhat it does\nLogic learner is an educational tool that teaches students or kids concepts about digital logic circuitry in an intuitive, tactile, and accessible way!\nWhat that means, in practice, is that we have an app which teaches students. The app is arranged in a linear set of lessons ranging from simple power sources to full adders (circuits that add numbers together in hardware), latches/flip-flops (computer memory), and more! Each lesson provides an explanation of one, focused new concept or new logic gates. The app also contains challenges the student can solve with the knowledge given by the previously solved lessons.\nNow, here's the fun part: Instead of having to learn a complicated circuitry simulation software or learning by staring at boring, unintuitive logic tables, we have a unique method of education. The student simply draws their circuit on paper in marker and points their camera at it. The app then scans the document and the circuit on it and simulates the hand-drawn circuit before projecting it back onto the paper on the camera screen! Students can easily and intuitively interact with the circuit by tapping or clicking power sources/switches to turn the power on or off, allowing them full control to experiment with their circuit.\nOur app then analyzes whether the circuit solves the lesson. If so, it moves on to the next lesson.\nWhile our app won't teach you how to build a CPU from scratch, it will give you all the tools and foundational knowledge you need to continue learning on your own!\nHow we built it\nThe app itself is a WPF application. The design and UI/UX of the app was designed through a combination of drawing it out on a whiteboard and mocking it up on Figma. Once that was completed, the design was then converted to XAML and connected through WPF's frontend library.\nThe circuitry simulation and document detection aspect was built using Wireform and WireformSketch, two circuit simulation and computer vision libraries built previously by one of our team members using OpenCv.\nWe built a layer on top of these libraries that performs circuit equivalence analysis to check whether or not the student's circuit matches the intended solution. We also built a transformation matrix that transforms clicks/touches on the app's screen to points on the real world circuit which allows the user to tap individual circuit elements on their screen to interact with them.\nWe then designed and wrote each lesson and loaded them into the application.\nChallenges we ran into\nOne of the most significant technical challenges we ran into was the lack of support for EmguCv (A C# wrapper of OpenCv which runs all of WireFormSketch's computer vision code) on Xamarin, our original platform of choice. After debugging and experimenting for hours, we discovered that the Android/IOS version required an expensive commercial license for EmguCv, so we migrated the project over to WPF.\nAnother challenge we ran into was design. When designing an educational application whose target audience includes children, it is very important to have simple, intuitive UX choices. As such, we went through multiple iterations of design in Figma to discover the simplest, most visual solutions to our design goals.\nAccomplishments that we're proud of\nOne of the hardest parts of this project was designing the lessons. Digital Logic is hard. It's abstract and very conceptual, so hooking it in to intuitive understanding through lessons is quite difficult. However, our team experimented with different lesson formats to discover the best way to teach these concepts in a way that is understandable by students, kids, and other beginners. We landed on our final approach: Focus on one, simple concept at a time. Using this model as a framework for our lessons, we were able to reach fairly complex topics in a very intuitive and easy to follow way.\nSimilarly, our team is very proud of the impact our project can have. We deeply believe in the value of education, and are proud to have created a tool which increases the accessibility of a very important field of science. As such, accessibility and intuitiveness were major design goals of our project. Because all that is required is a device, a camera, and paper/pens, our lessons are very accessible for anyone, regardless of background or experience!\nWhat we learned\nBecause this was the first hackathon for many of us, we learned a lot about working under pressure, team organization, and collaborative development. We're very proud of our teamwork this weekend. With all of us living in different places and only being able to connect over zoom, structure and organization were incredibly important. From creating a 10+ page design doc overnight to even distribution and divide-and-conquer approaches to team management, we quickly developed a system of organization that worked. Not only did we create an awesome product, but we had a great time doing it!\nOn the technical side, none of us had significant ios/android app development experience and we also had no prior WPF experience. However, through sharing resources and some long nights watching YouTube videos, we figured learned how to work with tools such as Blend for Visual Studio and how to develop for WPF and XAML-based projects in general.\nThe WPF library itself was a whole other learning experience, especially for a frontend developer who's used to working with HTML and CSS! Navigating through the different screens and passing event handlers up from child components proved to be a tough but manageable challenge. Overall, this was a great learning experience, and we now feel more comfortable writing user interfaces using C# and XAML files.\nWhat's next for Logic Learner\nWhile we created a functional app this hackathon that teaches the basic foundations of digital logic, there's lots of room to grow Logic Learner even more, from platform expansions and content extensions!\nFirstly, we plan to migrate the project back to Xamarin and provide IOS and Android build targets by converting WireFormSketch away from EmguCv. This would require a significant amount of time to do as the library that we were working off of relied on the EmguCv library (which has a $300 commercial mobile development license)! This, of course, was completely inaccessible to us in the scope of this hackathon, and developing a new library for circuit parsing would take way too much time. By cutting out EmguCv, we would no longer be restricted by platform and even programming language for that matter. This would make the app more scalable to different applications, such as a web app!\nSecondly, we plan on adding more complex components and circuit lessons to further advance student's understanding. We can create complex lessons on creating actual CPU components like an ALU, an instruction selectors, RAM, Registers, and more! Logic Learner provides a platform for an unlimited number of lessons. We also thought it would be cool to have professionals and professors contribute to the lessons on this app, whether it be editing/writing lessons or simply providing challenging problems to solve.\nThird, we want this project to be as fun and intuitive for students as possible. This means we plan on adding more polish, more sound effects, more rewards/achievements to incentivize students to keep on learning! As a mobile app, there's a much easier interactive component that can be taken advantage of. This allows for the app to be much more polished than it is at the moment and really feel like an industry-standard learning tool.\nFinally, we plan on further integrating with Google Cloud or CockroachDB to provide a public workplace where students can share custom made lessons or exciting circuits on the public database! This will allow people from all over the world to share their passion for digital logic or provide niche lessons on other interesting circuit components!", "link": "https://devpost.com/software/logic-learner", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ndigital-logic circuitry forms the foundation of almost every modern device we use today. from laptops to calculators to fridges, the brains of our world are powered by digital circuitry.\nunfortunately, the barrier of entry to learning is incredibly high. if you want to learn about digital logic, you have two options:\n1) watch a lecture - while there are many wonderful professors out there, most digital logic classes require a very high amount of prerequisite knowledge. on top of that, these classes are usually taught by drawing static diagrams on the board and the professor explaining truth tables of logic gates and circuits. for kids and other beginners, this is very hard to follow and not very intuitive.\n2) experiment with circuit simulators - if students could start right off by experimenting with actual circuits, they could build up the intuition required to understand these concepts. however, the lack of formal lesson structure combined with extremely complex and unintuitive user interfaces causes these tools to be very difficult to use without prior understanding of the material.\nbecause of this, most people never get the chance to learn how the circuits that run our lives actually work. in fact, even many programmers don't really understand how the cpus in their machines execute the commands they provide. for students or children who are disadvantaged or have less access to resources, educational resources on these concepts can be almost impossible to reach.\nlogic learner aims to change that.\nwhat it does\nlogic learner is an educational -----> tool !!!  that teaches students or kids concepts about digital logic circuitry in an intuitive, tactile, and accessible way!\nwhat that means, in practice, is that we have an app which teaches students. the app is arranged in a linear set of lessons ranging from simple power sources to full adders (circuits that add numbers together in hardware), latches/flip-flops (computer memory), and more! each lesson provides an explanation of one, focused new concept or new logic gates. the app also contains challenges the student can solve with the knowledge given by the previously solved lessons.\nnow, here's the fun part: instead of having to learn a complicated circuitry simulation software or learning by staring at boring, unintuitive logic tables, we have a unique method of education. the student simply draws their circuit on paper in marker and points their camera at it. the app then scans the document and the circuit on it and simulates the hand-drawn circuit before projecting it back onto the paper on the camera screen! students can easily and intuitively interact with the circuit by tapping or clicking power sources/switches to turn the power on or off, allowing them full control to experiment with their circuit.\nour app then analyzes whether the circuit solves the lesson. if so, it moves on to the next lesson.\nwhile our app won't teach you how to build a cpu from scratch, it will give you all the tools and foundational knowledge you need to continue learning on your own!\nhow we built it\nthe app itself is a wpf application. the design and ui/ux of the app was designed through a combination of drawing it out on a whiteboard and mocking it up on figma. once that was completed, the design was then converted to xaml and connected through wpf's frontend library.\nthe circuitry simulation and document detection aspect was built using wireform and wireformsketch, two circuit simulation and computer vision libraries built previously by one of our team members using opencv.\nwe built a layer on top of these libraries that performs circuit equivalence analysis to check whether or not the student's circuit matches the intended solution. we also built a transformation matrix that transforms clicks/touches on the app's screen to points on the real world circuit which allows the user to tap individual circuit elements on their screen to interact with them.\nwe then designed and wrote each lesson and loaded them into the application.\nchallenges we ran into\none of the most significant technical challenges we ran into was the lack of support for emgucv (a c# wrapper of opencv which runs all of wireformsketch's computer vision code) on xamarin, our original platform of choice. after debugging and experimenting for hours, we discovered that the android/ios version required an expensive commercial license for emgucv, so we migrated the project over to wpf.\nanother challenge we ran into was design. when designing an educational application whose target audience includes children, it is very important to have simple, intuitive ux choices. as such, we went through multiple iterations of design in figma to discover the simplest, most visual solutions to our design goals.\naccomplishments that we're proud of\none of the hardest parts of this project was designing the lessons. digital logic is hard. it's abstract and very conceptual, so hooking it in to intuitive understanding through lessons is quite difficult. however, our team experimented with different lesson formats to discover the best way to teach these concepts in a way that is understandable by students, kids, and other beginners. we landed on our final approach: focus on one, simple concept at a time. using this model as a framework for our lessons, we were able to reach fairly complex topics in a very intuitive and easy to follow way.\nsimilarly, our team is very proud of the impact our project can have. we deeply believe in the value of education, and are proud to have created a tool which increases the accessibility of a very important field of science. as such, accessibility and intuitiveness were major design goals of our project. because all that is required is a device, a camera, and paper/pens, our lessons are very accessible for anyone, regardless of background or experience!\nwhat we learned\nbecause this was the first hackathon for many of us, we learned a lot about working under pressure, team organization, and collaborative development. we're very proud of our teamwork this weekend. with all of us living in different places and only being able to connect over zoom, structure and organization were incredibly important. from creating a 10+ page design doc overnight to even distribution and divide-and-conquer approaches to team management, we quickly developed a system of organization that worked. not only did we create an awesome product, but we had a great time doing it!\non the technical side, none of us had significant ios/android app development experience and we also had no prior wpf experience. however, through sharing resources and some long nights watching youtube videos, we figured learned how to work with tools such as blend for visual studio and how to develop for wpf and xaml-based projects in general.\nthe wpf library itself was a whole other learning experience, especially for a frontend developer who's used to working with html and css! navigating through the different screens and passing event handlers up from child components proved to be a tough but manageable challenge. overall, this was a great learning experience, and we now feel more comfortable writing user interfaces using c# and xaml files.\nwhat's next for logic learner\nwhile we created a functional app this hackathon that teaches the basic foundations of digital logic, there's lots of room to grow logic learner even more, from platform expansions and content extensions!\nfirstly, we plan to migrate the project back to xamarin and provide ios and android build targets by converting wireformsketch away from emgucv. this would require a significant amount of time to do as the library that we were working off of relied on the emgucv library (which has a $300 commercial mobile development license)! this, of course, was completely inaccessible to us in the scope of this hackathon, and developing a new library for circuit parsing would take way too much time. by cutting out emgucv, we would no longer be restricted by platform and even programming language for that matter. this would make the app more scalable to different applications, such as a web app!\nsecondly, we plan on adding more complex components and circuit lessons to further advance student's understanding. we can create complex lessons on creating actual cpu components like an alu, an instruction selectors, ram, registers, and more! logic learner provides a platform for an unlimited number of lessons. we also thought it would be cool to have professionals and professors contribute to the lessons on this app, whether it be editing/writing lessons or simply providing challenging problems to solve.\nthird, we want this project to be as fun and intuitive for students as possible. this means we plan on adding more polish, more sound effects, more rewards/achievements to incentivize students to keep on learning! as a mobile app, there's a much easier interactive component that can be taken advantage of. this allows for the app to be much more polished than it is at the moment and really feel like an industry-standard learning tool.\nfinally, we plan on further integrating with google cloud or cockroachdb to provide a public workplace where students can share custom made lessons or exciting circuits on the public database! this will allow people from all over the world to share their passion for digital logic or provide niche lessons on other interesting circuit components!", "sortedWord": "None", "removed": "Nan", "score": 9, "comments": 0, "media": null, "medialink": null, "identifyer": 59506470}, {"Unnamed: 0": 6511, "autor": "Contribute", "date": null, "content": "Inspiration\nDAOs changed our lives. Brought passion, fun, and decentralization to us. But we\u2019ve all been lurking around in too many discords, trying to make ourselves useful and start contributing. We talked with a bunch of DAOs and community members and we tried solving it! Looking at the impressive lineup of new DAO tools; Yearn\u2019s Coordinape, Snapshoots voting, source creed \u2013 we could fill up the gap in between!\nWhat it does\nWe wanted to create a bridge between DAO governance and action. Using the example of governance voting from Tally, we also integrated a subgraph from Snapshop to show you that it works for multiple tools and data of your choice. The DAO gets a bounty/task board that members can contribute to. Get on their discord, talk with the task owner and deliver the project once it's done \u2013 You\u2019ve just started gaining tokens, reputation, and achievements. Browse other DAOs and look at their task board, start getting on-chain credentials and climb the leaderboard.\nHow we built it\nUsing proposal data from Tally as a data source, Standardbounties for the smart contracts to follow best practices and other tools including Next.js, Solidity, Typescript, ethers.js.\nChallenges we ran into\nWe got to sit down with Kevin from Gitcoin who gave us some great feedback. We wanted to create something more sustainable than turning people into bounty hunters with the sole purpose of quick offs. A better way for DAOs to get their members to start contributing and make it easier for the core team and DAO to decide on future core contributors.\nAccomplishments that we're proud of\nWe hope we've created one way of making contributing easier and a more fun way for members of a DAO. Increase the DAOs visibility and a beginning of an on-chain portfolio with validated tasks for the anons.\nWhat we learned\nIt\u2019s a rollercoaster of doubt and feeling that this actually could work! Every DAO works differently, but DAO tools are awesome. Lastly, bounties are more complex than anticipated.\nWhat's next for Contribute\nWe think focusing on DAOs with treasury\u2019s, a defined leadership, and clear docs on how they work are most suitable for this tool.\nOur future vision also includes: Discord integration Staking, if you don\u2019t complete it you get slashed, to create \"skin in the game\" for contributors! Varied degrees of quality and assessment. We want to make it fun, gamified! With additional features to the leaderboard, achievements, and XP. Multi-user claiming Dispute resolution Verifying DAO memberships on-chain (I.e. if you've been paid by the DAO, the DAO can sign off on your membership). Ideally, a good first step to becoming a core contributor.\nWe're making the selection of new core team contributors for DAOs \u2013 easier!", "link": "https://devpost.com/software/contribute-71auhd", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ndaos changed our lives. brought passion, fun, and decentralization to us. but we\u2019ve all been lurking around in too many discords, trying to make ourselves useful and start contributing. we talked with a bunch of daos and community members and we tried solving it! looking at the impressive lineup of new dao tools; yearn\u2019s coordinape, snapshoots voting, source creed \u2013 we could fill up the gap in between!\nwhat it does\nwe wanted to create a bridge between dao governance and action. using the example of governance voting from tally, we also integrated a subgraph from snapshop to show you that it works for multiple tools and data of your choice. the dao gets a bounty/task board that members can contribute to. get on their discord, talk with the task owner and deliver the project once it's done \u2013 you\u2019ve just started gaining tokens, reputation, and achievements. browse other daos and look at their task board, start getting on-chain credentials and climb the leaderboard.\nhow we built it\nusing proposal data from tally as a data source, standardbounties for the smart contracts to follow best practices and other tools including next.js, solidity, typescript, ethers.js.\nchallenges we ran into\nwe got to sit down with kevin from gitcoin who gave us some great feedback. we wanted to create something more sustainable than turning people into bounty hunters with the sole purpose of quick offs. a better way for daos to get their members to start contributing and make it easier for the core team and dao to decide on future core contributors.\naccomplishments that we're proud of\nwe hope we've created one way of making contributing easier and a more fun way for members of a dao. increase the daos visibility and a beginning of an on-chain portfolio with validated tasks for the anons.\nwhat we learned\nit\u2019s a rollercoaster of doubt and feeling that this actually could work! every dao works differently, but dao tools are awesome. lastly, bounties are more complex than anticipated.\nwhat's next for contribute\nwe think focusing on daos with treasury\u2019s, a defined leadership, and clear docs on how they work are most suitable for this -----> tool !!! .\nour future vision also includes: discord integration staking, if you don\u2019t complete it you get slashed, to create \"skin in the game\" for contributors! varied degrees of quality and assessment. we want to make it fun, gamified! with additional features to the leaderboard, achievements, and xp. multi-user claiming dispute resolution verifying dao memberships on-chain (i.e. if you've been paid by the dao, the dao can sign off on your membership). ideally, a good first step to becoming a core contributor.\nwe're making the selection of new core team contributors for daos \u2013 easier!", "sortedWord": "None", "removed": "Nan", "score": 6, "comments": 0, "media": null, "medialink": null, "identifyer": 59506511}, {"Unnamed: 0": 6546, "autor": "DSM(high)V", "date": null, "content": "Inspiration\nWe all have either have known someone struggling with mental health, or have seen its effects within our greater community - especially for college students during the Covid-19 lockdowns last year. We were also disheartened to learn that due to the lockdowns and even still, college students who had incident / recently developed depression were incapable of accessing critical resources to receive a proper diagnosis and care. This inspired us to develop DSM(high)V.\nWhat it does\nDSM(high)V is a self-assessment that determines a user's risk of depression based on their responses to a short list of (10) questions. Each question is on the Montgomery-Asberg Depression Rating Scale (MADRS) scale of 0-6 and, upon answering all of the questions, the user receives a score in the form of their possible risk of depression status. There are 4 categories identified (no risk, mild risk, moderate risk, severe / high risk) for depression. This application is free to use, and depending on the risk assessment, currently provides the user with resources for self-management of depression.\nHow we built it\nWe programmed this application within Rstudio (version 4.1.0), and more specifically with the jquery (version 3.6.0) and shiny (version 1.7.1) R libraries. One of our team members had a background in psychology, so their knowledge on the topic confirmed the validity of the questions we asked in the app, informed by DSM-V diagnosing criteria for depression. While this app does not serve as a diagnostic tool, it is a first step in developing such precision diagnostic technologies.\nChallenges we ran into\nOne of the original ideas for this application was to develop a machine learning algorithm (MLA) to assess a patient's risk for depression, based not only on their survey responses, but also on pre-supplied data for said hypothetical patient (i.e. age, gender identity, college year). Unfortunately, we had limited access to datasets that could provide us with the variables necessary to train and test such an algorithm. While this was the first hackathon that all team members have participated in, our excitement for the topic fueled us to keep going.\nAccomplishments that we're proud of\nWe are proud of the fact that, within our allotted time, we were able to generate a minimal viable product and a user-friendly interface for college students who may be struggling to gain access to mental healthcare. And we were very proud of participating in our first hackathon.\nWhat we learned\nWe learned how to determine which features were most important for a product, and this hackathon was a great exercise to get us thinking from the perspective of both the end users (college students) and customers (universities, insurance companies).\nWhat's next for DSM(high)V\nShould we be unable to obtain access to a dataset containing EHR data (i.e. patient demographics, biomarkers), the next steps would be to create the MLA. Also, we intend to create collaborations with vetted clinical psychologists, and their services would be offered through additional material (hyperlinks) provided through the app for patients to receive affordable mental health care. We also will seek partnerships with university health services , where we will connect directly to college students and test future iterations of this app; we may also utilize email and other virtual channels to reach out to isolated college students.", "link": "https://devpost.com/software/a-tool-for-predicting-depression", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe all have either have known someone struggling with mental health, or have seen its effects within our greater community - especially for college students during the covid-19 lockdowns last year. we were also disheartened to learn that due to the lockdowns and even still, college students who had incident / recently developed depression were incapable of accessing critical resources to receive a proper diagnosis and care. this inspired us to develop dsm(high)v.\nwhat it does\ndsm(high)v is a self-assessment that determines a user's risk of depression based on their responses to a short list of (10) questions. each question is on the montgomery-asberg depression rating scale (madrs) scale of 0-6 and, upon answering all of the questions, the user receives a score in the form of their possible risk of depression status. there are 4 categories identified (no risk, mild risk, moderate risk, severe / high risk) for depression. this application is free to use, and depending on the risk assessment, currently provides the user with resources for self-management of depression.\nhow we built it\nwe programmed this application within rstudio (version 4.1.0), and more specifically with the jquery (version 3.6.0) and shiny (version 1.7.1) r libraries. one of our team members had a background in psychology, so their knowledge on the topic confirmed the validity of the questions we asked in the app, informed by dsm-v diagnosing criteria for depression. while this app does not serve as a diagnostic -----> tool !!! , it is a first step in developing such precision diagnostic technologies.\nchallenges we ran into\none of the original ideas for this application was to develop a machine learning algorithm (mla) to assess a patient's risk for depression, based not only on their survey responses, but also on pre-supplied data for said hypothetical patient (i.e. age, gender identity, college year). unfortunately, we had limited access to datasets that could provide us with the variables necessary to train and test such an algorithm. while this was the first hackathon that all team members have participated in, our excitement for the topic fueled us to keep going.\naccomplishments that we're proud of\nwe are proud of the fact that, within our allotted time, we were able to generate a minimal viable product and a user-friendly interface for college students who may be struggling to gain access to mental healthcare. and we were very proud of participating in our first hackathon.\nwhat we learned\nwe learned how to determine which features were most important for a product, and this hackathon was a great exercise to get us thinking from the perspective of both the end users (college students) and customers (universities, insurance companies).\nwhat's next for dsm(high)v\nshould we be unable to obtain access to a dataset containing ehr data (i.e. patient demographics, biomarkers), the next steps would be to create the mla. also, we intend to create collaborations with vetted clinical psychologists, and their services would be offered through additional material (hyperlinks) provided through the app for patients to receive affordable mental health care. we also will seek partnerships with university health services , where we will connect directly to college students and test future iterations of this app; we may also utilize email and other virtual channels to reach out to isolated college students.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59506546}, {"Unnamed: 0": 6553, "autor": "SLP Manager", "date": null, "content": "Inspiration\nWe run Axie scholarship program one of the most famous NFT game right now. We have more than 50 members in our guild and the problem is when we have to claim SLP token and pay to our scholar.\nIf you are Axie Manager you have to claim token by yourself every 15 days and pay to scholar for everyone who play for your guild. Why can't we just let scholar claim token by themself?\nThere is no feature like that in Axie right now so we decide to create it by ourself.\nWhat it does\nWe would like to introduce SLP Manager, the smart contract that can let scholar claim SLP token by them self without asking their manager which waste their time. They can earn their return in realtime without waiting for 15 days period like before. They can use Metamask Wallet to connect our DApp, check balance and claim their token. This is risk free, save a ton of time and less human error.\nguildMaster have to collateralize SLP token for their scholarship and account SLP token for each member in the guild. guildMaster can freely change percentShare and member in every way they want in less process.\nIn scholar side, all we need is Ronin Wallet Address to get data from Axie Infinity Game and Metamask Wallet to receive their wage.\nHow we built it\nWe use APIConsumer contract to receive and feed all needed data to Blockchain from and account the claimable SLP that our scholar should get and automate API call by using Chainlink keeper to call updateBalance function everyday.\nChallenges we ran into\nHow to create incentive for our team to succeed this hackathon. This is very hard to hack together especially during the Covid pandemic. It hard to work by just talk on internet so we decide to have a have camping together in hotel for 3 days and that is turn out to be the most wonderful hacekathon camp I ever had.\nWe got the first API Consumer for our ever and that is amazing even it hard to find API that actually work from Axie Infinity but we got it. We face many error that we don't understand but with good help from people in discord that can help us overcome it.\nAccomplishments that we're proud of\nWe got the product that can solve our pain and that is so satisfied. The SLP manager is ready to use with a bit of decoration we believe it will reduce a lot of work that we have to manage.\nWe got the solidity contract that work on testnet, we got web3 frontend to interact with and we have experience for using Chainlink product like Chainlink Keeper and any API.\nAnd even we didn't use it right now but we proud to tell you that we already have Chainlink node and we will modify and use it in the future.\nWhat we learned\nWe learn a lot from this hackathon. We create our ever first contarct that can call external API and that's so cool. One of my team finish the web3 frontend even he never write it before.\nWhat's next for SLP Manager\nWe will use it for real production in very soon. We will test our product with only our guild and keep improve so other guild can join this tool then when we confidence with our product we will start to contact Mavis Hub for scale this product to official feature of Axie Infity.\nThis will help a lot of work from every Axie manager and create to utility of SLP single pool on their farm for using as collateralize and earning fee.", "link": "https://devpost.com/software/naga-product-tbd", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe run axie scholarship program one of the most famous nft game right now. we have more than 50 members in our guild and the problem is when we have to claim slp token and pay to our scholar.\nif you are axie manager you have to claim token by yourself every 15 days and pay to scholar for everyone who play for your guild. why can't we just let scholar claim token by themself?\nthere is no feature like that in axie right now so we decide to create it by ourself.\nwhat it does\nwe would like to introduce slp manager, the smart contract that can let scholar claim slp token by them self without asking their manager which waste their time. they can earn their return in realtime without waiting for 15 days period like before. they can use metamask wallet to connect our dapp, check balance and claim their token. this is risk free, save a ton of time and less human error.\nguildmaster have to collateralize slp token for their scholarship and account slp token for each member in the guild. guildmaster can freely change percentshare and member in every way they want in less process.\nin scholar side, all we need is ronin wallet address to get data from axie infinity game and metamask wallet to receive their wage.\nhow we built it\nwe use apiconsumer contract to receive and feed all needed data to blockchain from and account the claimable slp that our scholar should get and automate api call by using chainlink keeper to call updatebalance function everyday.\nchallenges we ran into\nhow to create incentive for our team to succeed this hackathon. this is very hard to hack together especially during the covid pandemic. it hard to work by just talk on internet so we decide to have a have camping together in hotel for 3 days and that is turn out to be the most wonderful hacekathon camp i ever had.\nwe got the first api consumer for our ever and that is amazing even it hard to find api that actually work from axie infinity but we got it. we face many error that we don't understand but with good help from people in discord that can help us overcome it.\naccomplishments that we're proud of\nwe got the product that can solve our pain and that is so satisfied. the slp manager is ready to use with a bit of decoration we believe it will reduce a lot of work that we have to manage.\nwe got the solidity contract that work on testnet, we got web3 frontend to interact with and we have experience for using chainlink product like chainlink keeper and any api.\nand even we didn't use it right now but we proud to tell you that we already have chainlink node and we will modify and use it in the future.\nwhat we learned\nwe learn a lot from this hackathon. we create our ever first contarct that can call external api and that's so cool. one of my team finish the web3 frontend even he never write it before.\nwhat's next for slp manager\nwe will use it for real production in very soon. we will test our product with only our guild and keep improve so other guild can join this -----> tool !!!  then when we confidence with our product we will start to contact mavis hub for scale this product to official feature of axie infity.\nthis will help a lot of work from every axie manager and create to utility of slp single pool on their farm for using as collateralize and earning fee.", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59506553}, {"Unnamed: 0": 6558, "autor": "Razer", "date": null, "content": "Inspiration\nPeople with mouth and jaw injuries find it difficult to have a drink. So we designed a specialized tumbler for them so that they can have their drink comfortably without applying any force.\nWhat it does\nThis tumbler is designed for people who have suffered injuries in face more like jaw injuries. It automatically dispenses water/drink when the patient brings this tumbler right next to their mouth. There is no need to worry that the water/drink is going to fall before the straw is in the mouth. The sensors are calibrated in the right way. As soon as you bring the tumbler right next to your mouth, water pours without the need to sip water from the tumbler.\nHow we built it\nWe used an old water bottle to hold the drink and a straw for the water/drink to flow through it. We used a water pump for the water to dispense from the tumbler on its own. The main part is the IR sensor which tracks the right distance from where water/drink should be dispensed. All these parts are placed in the right way for the patient to use it in ease.\nChallenges we ran into\nThere were several small challenges that we face. But calibrating the sensor properly was the main challenge that we faced. But later we fixed it in place and it worked perfectly!\nAccomplishments that we're proud of\nWe are proud that we were able to make a finished product which can be used in real time to solve someone's problem and make them happy. We are happy that we were able to finish this in a short amount of time. Seeing the tumbler work perfectly is what we are proud of.\nWhat we learned\nWe learnt that every small step leads to the finished product. At first when it didn't make sense assembling each part step by step ended up in the final product. Technology is a very powerful tool which can be used to solve real world problems, small or big.", "link": "https://devpost.com/software/razer", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\npeople with mouth and jaw injuries find it difficult to have a drink. so we designed a specialized tumbler for them so that they can have their drink comfortably without applying any force.\nwhat it does\nthis tumbler is designed for people who have suffered injuries in face more like jaw injuries. it automatically dispenses water/drink when the patient brings this tumbler right next to their mouth. there is no need to worry that the water/drink is going to fall before the straw is in the mouth. the sensors are calibrated in the right way. as soon as you bring the tumbler right next to your mouth, water pours without the need to sip water from the tumbler.\nhow we built it\nwe used an old water bottle to hold the drink and a straw for the water/drink to flow through it. we used a water pump for the water to dispense from the tumbler on its own. the main part is the ir sensor which tracks the right distance from where water/drink should be dispensed. all these parts are placed in the right way for the patient to use it in ease.\nchallenges we ran into\nthere were several small challenges that we face. but calibrating the sensor properly was the main challenge that we faced. but later we fixed it in place and it worked perfectly!\naccomplishments that we're proud of\nwe are proud that we were able to make a finished product which can be used in real time to solve someone's problem and make them happy. we are happy that we were able to finish this in a short amount of time. seeing the tumbler work perfectly is what we are proud of.\nwhat we learned\nwe learnt that every small step leads to the finished product. at first when it didn't make sense assembling each part step by step ended up in the final product. technology is a very powerful -----> tool !!!  which can be used to solve real world problems, small or big.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59506558}, {"Unnamed: 0": 6602, "autor": "LingoAlly", "date": null, "content": "Inspiration\nOver 5% of the world\u2019s population \u2013 432 million adults and 34 million children have impaired hearing. For deaf people, access to sign language is key to breaking down communication barriers and participating in society just like anyone else. The right of deaf people to get equal access for schools, medical treatment, or courts hinges on their opportunity to use their language, and the ability of their loved ones, educators, and hospital staff to provide them equal opportunities majorly depends upon efficient usage of Sign Language. Therefore, an accessible solution available in virtual environment for enabling the hard-of-hearing individuals to learn sign language: a key step towards accessing equal opportunities is needed. Also, As many as 90 percent of deaf children are born to hearing parents, which can make learning sign language a family affair. We wanted to develop a solution to ease this process and make learning ASL a fun team activity. Parents and Guardians who learn ASL along with their child often find it easier to communicate on a deeper level with their deaf child. Today more than ever it\u2019s common for educators to have children who are deaf or hard of hearing in their classroom. Many opt to learn ASL for this reason alone, and LingoAlly provides a hassle-free experience for them to gain proficiency in this language. This inspired us to develop LingoAlly- a tool that not only allows deaf people to learn sign language through features for practicing signs, practicing communication with ASL, quizzes, etc. but also connect and interact with a wider community, learn from their experiences and share their opinions with them. We aim to empower the users financially by making them aware of the jobs so that they never miss out on opportunities, and simultaneously enhance workplace diversity and inclusion and help them to identify their rights by scraping out the best of the blogs from the internet. Users can also volunteer through our website or connect with the volunteers directly!\nWhat it does\nLingoAlly -a complete Solution for the Empowerment of the hard-of-hearing Community!\nThe features of our app include:\nPractice ASL alphabets:\nThe starting point of any language- mastering alphabets. We trained our model using Tensoreflow.js and used HandPose and Fingerpose to provide the user a real-time video interface that detects the letter they are signing and enables them to master letters step by step.\nLearn Essential Everyday Signs\nNot only alphabets but knowing everyday signs is the key to communicating using ASL. This website has a collection of numerous gifs along with step-by-step instructions to help you learn everyday signs and Start Communicating!\nQuizzes!\nNo learning Experience is Complete without a quiz to check your knowledge. Our app provides a short scored quiz to verify what you have learned!\nASL to English Translator\nType using ASL signs and get results in English. Master typing with ASL signs, and translate to English text!\nEnglish to ASL translator\nType in English and get results in ASL immediately!\nTranscribe Audios\nEnter the URL of any audio File and get Transcripts in ASL. This has been achieved using AssemblyAI API.\nSpeech to text!\nSpeak and get ASL transcripts in real-time. The script for this feature is developed in Python.\nCommunicate with ASL:\nSignal any signs at the camera and get results of what you are trying to speak! This helps users master communication with ASL which is an essential part of everyday life!\nSpaces:\nSpaces are a feature to connect All users and help them interact, for groups, share experiences and learn together! The users can create new Spaces, or hop into existing ones. Once inside a space, they can create posts and interact with other users upon a post. Posts can range from doubts to advice! Other users are free to comment on the post and Openly voice their opinions.\nJob Opportunities:\nLingoAlly enables all users to access job opportunities in their Area! This has been achieved using Google Job Results API enabling the deaf community to openly exercise their Independence to work in any Area they like and prove their capabilities.\nBlogs!\nYou can only access your rights if you are aware of them. Read the blogs from anywhere in the world and gain understanding and awareness about opportunities and rights, so that you are never denied the opportunity to grow! Our app lists all the blogs through Google Search API and provides the links to access them!\nVolunteer\nAll volunteers are welcome! Anyone can join our website as a volunteer and offer support and general advice, and form connections with the people in need. Likewise, if anyone wishes to gain guidance, they can connect with any volunteer.\nHow we built it\nWe built our website's Front-end using React.js, with Node.js and Express.js server.\nWe used MongoDB Atlas Google-cloud Database for storing data.\nPassport.js has been used for Authenticating users.\nWe added the practice for static gestures feature using Handpose library for detecting key points on user's hands, Fingerpose for training certain gestures, and finally classified them.\nWe developed the python script for live frame-by-frame detection of sign language using Mediapipe and LSTM. Initially, the key points are detected using Mediapipe which are saved as a Numpy array of 30 frames and 30 samples for each action, during training. Later these samples are provided labels and an LSTM is trained to identify gestures in real-time while predicting the probability of all actions for which model has been trained. For now, we have included 3 gestures but this number can easily be enhanced, by just adding more labels, and training for them.\nPython is used for Real-time speech to ASL translation.\nAssembly API has been used to convert Audio URLs to ASL transcripts. Once a user provides an audio URL, that URL is sent to node-server which later fetches the transcript in English from Assembly-AI API. This transcript can then be converted to ASL using react.\nWe also added animated backgrounds using React particles.\nGoogle Search API has been used to get job search results and blog results.\nGit has been utilized for collaboration and Heroku has been used for deployment.\nChallenges we ran into\nWe faced a lot of challenges as this was the first time we both worked on a Full-stack react Website. Initially, while connecting Node.js API to React front-end, we ran into endless challenges. Also during authentication, we were unable to share logged-in users' credentials across Routes.\nIt was tough for us to connect Python Script to Node-server initially as we had never used the Node.js child process before.\nDuring deployment, we also faced some issues but were ultimately able to resolve them by gaining a deeper understanding of the process.\nWhen we were developing the python script for identifying live actions of sign language, we faced issues as we are new to machine learning. But using LSTM, we were able to accomplish our objective of live and frame-by-frame identification of some of the common signs like - hello, thank you, etc through sign language. We can easily train our model for a wider range of signs.\nAccomplishments that we're proud of\nOur Amazing teamwork and a web app to empower the deaf community!\nWhat we learned\nAs a result of this project, we are comfortable with React.js including React hooks, Props, and React-state. We know how to connect our website to various APIs and achieve perfect results with minimum code!\nNon-technically, I think we came to recognize the challenges faced by deaf individuals worldwide, got ourselves familiar with Sign-language and developed a sense of appreciation towards them!\nWhat's next for LingoAlly\nAccording to us, this project has tremendous potential to grow.\nWe can have a chrome extension which transcribes youtube/online videos to ASL in real-time!\nIntegrating real-time Sign language detection into Google-meet/Zoom meetings.\nPersonalizing the job-search and blogs results using recommendation systems and Machine learning", "link": "https://devpost.com/software/empowerlingo-qledks", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nover 5% of the world\u2019s population \u2013 432 million adults and 34 million children have impaired hearing. for deaf people, access to sign language is key to breaking down communication barriers and participating in society just like anyone else. the right of deaf people to get equal access for schools, medical treatment, or courts hinges on their opportunity to use their language, and the ability of their loved ones, educators, and hospital staff to provide them equal opportunities majorly depends upon efficient usage of sign language. therefore, an accessible solution available in virtual environment for enabling the hard-of-hearing individuals to learn sign language: a key step towards accessing equal opportunities is needed. also, as many as 90 percent of deaf children are born to hearing parents, which can make learning sign language a family affair. we wanted to develop a solution to ease this process and make learning asl a fun team activity. parents and guardians who learn asl along with their child often find it easier to communicate on a deeper level with their deaf child. today more than ever it\u2019s common for educators to have children who are deaf or hard of hearing in their classroom. many opt to learn asl for this reason alone, and lingoally provides a hassle-free experience for them to gain proficiency in this language. this inspired us to develop lingoally- a -----> tool !!!  that not only allows deaf people to learn sign language through features for practicing signs, practicing communication with asl, quizzes, etc. but also connect and interact with a wider community, learn from their experiences and share their opinions with them. we aim to empower the users financially by making them aware of the jobs so that they never miss out on opportunities, and simultaneously enhance workplace diversity and inclusion and help them to identify their rights by scraping out the best of the blogs from the internet. users can also volunteer through our website or connect with the volunteers directly!\nwhat it does\nlingoally -a complete solution for the empowerment of the hard-of-hearing community!\nthe features of our app include:\npractice asl alphabets:\nthe starting point of any language- mastering alphabets. we trained our model using tensoreflow.js and used handpose and fingerpose to provide the user a real-time video interface that detects the letter they are signing and enables them to master letters step by step.\nlearn essential everyday signs\nnot only alphabets but knowing everyday signs is the key to communicating using asl. this website has a collection of numerous gifs along with step-by-step instructions to help you learn everyday signs and start communicating!\nquizzes!\nno learning experience is complete without a quiz to check your knowledge. our app provides a short scored quiz to verify what you have learned!\nasl to english translator\ntype using asl signs and get results in english. master typing with asl signs, and translate to english text!\nenglish to asl translator\ntype in english and get results in asl immediately!\ntranscribe audios\nenter the url of any audio file and get transcripts in asl. this has been achieved using assemblyai api.\nspeech to text!\nspeak and get asl transcripts in real-time. the script for this feature is developed in python.\ncommunicate with asl:\nsignal any signs at the camera and get results of what you are trying to speak! this helps users master communication with asl which is an essential part of everyday life!\nspaces:\nspaces are a feature to connect all users and help them interact, for groups, share experiences and learn together! the users can create new spaces, or hop into existing ones. once inside a space, they can create posts and interact with other users upon a post. posts can range from doubts to advice! other users are free to comment on the post and openly voice their opinions.\njob opportunities:\nlingoally enables all users to access job opportunities in their area! this has been achieved using google job results api enabling the deaf community to openly exercise their independence to work in any area they like and prove their capabilities.\nblogs!\nyou can only access your rights if you are aware of them. read the blogs from anywhere in the world and gain understanding and awareness about opportunities and rights, so that you are never denied the opportunity to grow! our app lists all the blogs through google search api and provides the links to access them!\nvolunteer\nall volunteers are welcome! anyone can join our website as a volunteer and offer support and general advice, and form connections with the people in need. likewise, if anyone wishes to gain guidance, they can connect with any volunteer.\nhow we built it\nwe built our website's front-end using react.js, with node.js and express.js server.\nwe used mongodb atlas google-cloud database for storing data.\npassport.js has been used for authenticating users.\nwe added the practice for static gestures feature using handpose library for detecting key points on user's hands, fingerpose for training certain gestures, and finally classified them.\nwe developed the python script for live frame-by-frame detection of sign language using mediapipe and lstm. initially, the key points are detected using mediapipe which are saved as a numpy array of 30 frames and 30 samples for each action, during training. later these samples are provided labels and an lstm is trained to identify gestures in real-time while predicting the probability of all actions for which model has been trained. for now, we have included 3 gestures but this number can easily be enhanced, by just adding more labels, and training for them.\npython is used for real-time speech to asl translation.\nassembly api has been used to convert audio urls to asl transcripts. once a user provides an audio url, that url is sent to node-server which later fetches the transcript in english from assembly-ai api. this transcript can then be converted to asl using react.\nwe also added animated backgrounds using react particles.\ngoogle search api has been used to get job search results and blog results.\ngit has been utilized for collaboration and heroku has been used for deployment.\nchallenges we ran into\nwe faced a lot of challenges as this was the first time we both worked on a full-stack react website. initially, while connecting node.js api to react front-end, we ran into endless challenges. also during authentication, we were unable to share logged-in users' credentials across routes.\nit was tough for us to connect python script to node-server initially as we had never used the node.js child process before.\nduring deployment, we also faced some issues but were ultimately able to resolve them by gaining a deeper understanding of the process.\nwhen we were developing the python script for identifying live actions of sign language, we faced issues as we are new to machine learning. but using lstm, we were able to accomplish our objective of live and frame-by-frame identification of some of the common signs like - hello, thank you, etc through sign language. we can easily train our model for a wider range of signs.\naccomplishments that we're proud of\nour amazing teamwork and a web app to empower the deaf community!\nwhat we learned\nas a result of this project, we are comfortable with react.js including react hooks, props, and react-state. we know how to connect our website to various apis and achieve perfect results with minimum code!\nnon-technically, i think we came to recognize the challenges faced by deaf individuals worldwide, got ourselves familiar with sign-language and developed a sense of appreciation towards them!\nwhat's next for lingoally\naccording to us, this project has tremendous potential to grow.\nwe can have a chrome extension which transcribes youtube/online videos to asl in real-time!\nintegrating real-time sign language detection into google-meet/zoom meetings.\npersonalizing the job-search and blogs results using recommendation systems and machine learning", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59506602}, {"Unnamed: 0": 6646, "autor": "Yasu", "date": null, "content": "Inspiration\nThe inspiration for the project came from observing misshapen that occurs due to some loopholes in our healthcare system. One of our team member faced this during COVID-19 while getting treatment. As the doctor was not aware of prevailing medications, the person had to face serious post COVID-19 symptoms. Hence, we wanted to change things by bringing something innovative.\nWhat it does\nYasu aspires to modernise the healthcare system by utilising cutting-edge technology to address these gaps in our daily lives. We keep a digital record of a person's medical history, medical appointments, and medications. We want to reduce the danger of such critical data being unavailable during difficult times.\nHow we built it\nFIGMA Figma is the most effective tool for making UI and working over UX part. This makes the software easily understandable and usable.\nVue JS We used VUE JS cuz making this project with HTML and Vanilla JS was pretty difficult and also it will make the code messy, that's why we thought that using a JavaScript framework will be an easier approach.\nSCSS We used SCSS cuz it adds superpowers to Vanilla CSS. We could have used Vanilla CSS too but using it in a big project might makes the code messier and hard to understand. Our team love to use SCSS over Vanilla CSS so that's also a reason why we chose it.\nSupabase Supabase is always our team's go-to backend on hackathons. Not only it's fast and powerful but also very easy to integrate on our project within few clicks unlike firebase, which requires a lot of configuration.\nChallenges we ran into\nFinding way to secure the data\nHow to make it globally accessible?\nGenerating unique QR code\nBusiness strategies for taking project forward\nDeploying and error tests\nAccomplishments that we're proud of\nWe could achieve the expected results by coding in the hackathon hours. Working at such speed was a challenge in itself. However, our team worked hard to bring life to our idea by the end of the deadline.\nWhat we learned\nTeam work\nTime management along with studies\nNew terminologies in the tech stack\nHack this Fall provided many events through which we learned a lot of new stuff.\nUX part for the user\nWhat's next for Yasu\nWe look forward to advance our website and bring this idea into the market so that this can prove beneficial to people around us.", "link": "https://devpost.com/software/yasu", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nthe inspiration for the project came from observing misshapen that occurs due to some loopholes in our healthcare system. one of our team member faced this during covid-19 while getting treatment. as the doctor was not aware of prevailing medications, the person had to face serious post covid-19 symptoms. hence, we wanted to change things by bringing something innovative.\nwhat it does\nyasu aspires to modernise the healthcare system by utilising cutting-edge technology to address these gaps in our daily lives. we keep a digital record of a person's medical history, medical appointments, and medications. we want to reduce the danger of such critical data being unavailable during difficult times.\nhow we built it\nfigma figma is the most effective -----> tool !!!  for making ui and working over ux part. this makes the software easily understandable and usable.\nvue js we used vue js cuz making this project with html and vanilla js was pretty difficult and also it will make the code messy, that's why we thought that using a javascript framework will be an easier approach.\nscss we used scss cuz it adds superpowers to vanilla css. we could have used vanilla css too but using it in a big project might makes the code messier and hard to understand. our team love to use scss over vanilla css so that's also a reason why we chose it.\nsupabase supabase is always our team's go-to backend on hackathons. not only it's fast and powerful but also very easy to integrate on our project within few clicks unlike firebase, which requires a lot of configuration.\nchallenges we ran into\nfinding way to secure the data\nhow to make it globally accessible?\ngenerating unique qr code\nbusiness strategies for taking project forward\ndeploying and error tests\naccomplishments that we're proud of\nwe could achieve the expected results by coding in the hackathon hours. working at such speed was a challenge in itself. however, our team worked hard to bring life to our idea by the end of the deadline.\nwhat we learned\nteam work\ntime management along with studies\nnew terminologies in the tech stack\nhack this fall provided many events through which we learned a lot of new stuff.\nux part for the user\nwhat's next for yasu\nwe look forward to advance our website and bring this idea into the market so that this can prove beneficial to people around us.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59506646}, {"Unnamed: 0": 6653, "autor": "Equal Voices", "date": null, "content": "Figma Prototype | Source Code: GitHub | Demo: APK\nInspiration\nOur Vision is to give people a platform to speak their minds about events that affect us as a community, irrespective of background.\nThere are several examples of structural injustices becoming commonplace, let alone being overlooked - for example, the Untouchables that lasted until India gained independence and the systemic racism of Black and Latino populations in the US that is ongoing today. We disassociate with these horrific events because it seems so far away from us - and those affected by these events oftentimes suffer silently.\nThe first step towards tackling a problem is to talk about it - to gather and engage those who are affected by that problem, to generate awareness about the problem, and brainstorm about how to approach it.\nThis app is also a platform to share untold stories. Many avoid seeking help from Mental Health professionals due to the stigma surrounding it. With more and more voices sounding in support and sharing their experiences, the more comfortable they would be in their own skin.\nWhat it does\nThis app can help bridge the gap between communities. It is a way to bring those from vulnerable minorities into the conversation. It is a great opportunity to see a story from a different perspective.\nSign Up/ Sign In\nHome page with a user profile, notifications\nChat History that shows recent chat rooms and texts\nDiscovery to find new Chat Rooms using topic tags, filter out potentially triggering material using content warning tags, and additional tags to optimize search results\nHow we built it\nFor the front end, we used:\nFigma\nUnsplash\nFeather icons\nIconify\nLottieFiles\nThe app logo was created using FireAlpaca\nFor the back end, we used:\nKotlin\nAndroid Studio\nFirebase\nWhat we learned\nIn the past 36 hours, we took a wireframe from a Prototype to a Minimum Viable Product. Here's what each of us learned a new\nAditi\nCreate UI designs using Figma\nLearned about the Unsplash plugin\nAjeet\nI have stretched myself to build a complete working app in just one day for the first time.\nLearned to bring the Figma designs to reality.\nUsed the latest jetpack compose components in the complete app.\nAkshara\nHow to edit an app demo video\nCreate UI designs using Figma\nAkshaya\nCreated an app interface for the first time in Figma\nUtilized Figma\u2019s prototyping tool to create animations\nHelped create a schematic for storage in a database via Firebase\nWhat\u2019s next for Equal Voices\nA few things that we have in mind:\nGet in touch with people who are concerned about the same issues as you via a randomly matched person in a one to one chat\nIf a chatroom hasn\u2019t been created by an admin yet, the user may send a request for it to be prioritized\nHave extra security by enabling 2FA via phone verification\nAn admin panel to enhance content management", "link": "https://devpost.com/software/equal-voices", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "figma prototype | source code: github | demo: apk\ninspiration\nour vision is to give people a platform to speak their minds about events that affect us as a community, irrespective of background.\nthere are several examples of structural injustices becoming commonplace, let alone being overlooked - for example, the untouchables that lasted until india gained independence and the systemic racism of black and latino populations in the us that is ongoing today. we disassociate with these horrific events because it seems so far away from us - and those affected by these events oftentimes suffer silently.\nthe first step towards tackling a problem is to talk about it - to gather and engage those who are affected by that problem, to generate awareness about the problem, and brainstorm about how to approach it.\nthis app is also a platform to share untold stories. many avoid seeking help from mental health professionals due to the stigma surrounding it. with more and more voices sounding in support and sharing their experiences, the more comfortable they would be in their own skin.\nwhat it does\nthis app can help bridge the gap between communities. it is a way to bring those from vulnerable minorities into the conversation. it is a great opportunity to see a story from a different perspective.\nsign up/ sign in\nhome page with a user profile, notifications\nchat history that shows recent chat rooms and texts\ndiscovery to find new chat rooms using topic tags, filter out potentially triggering material using content warning tags, and additional tags to optimize search results\nhow we built it\nfor the front end, we used:\nfigma\nunsplash\nfeather icons\niconify\nlottiefiles\nthe app logo was created using firealpaca\nfor the back end, we used:\nkotlin\nandroid studio\nfirebase\nwhat we learned\nin the past 36 hours, we took a wireframe from a prototype to a minimum viable product. here's what each of us learned a new\naditi\ncreate ui designs using figma\nlearned about the unsplash plugin\najeet\ni have stretched myself to build a complete working app in just one day for the first time.\nlearned to bring the figma designs to reality.\nused the latest jetpack compose components in the complete app.\nakshara\nhow to edit an app demo video\ncreate ui designs using figma\nakshaya\ncreated an app interface for the first time in figma\nutilized figma\u2019s prototyping -----> tool !!!  to create animations\nhelped create a schematic for storage in a database via firebase\nwhat\u2019s next for equal voices\na few things that we have in mind:\nget in touch with people who are concerned about the same issues as you via a randomly matched person in a one to one chat\nif a chatroom hasn\u2019t been created by an admin yet, the user may send a request for it to be prioritized\nhave extra security by enabling 2fa via phone verification\nan admin panel to enhance content management", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59506653}, {"Unnamed: 0": 6662, "autor": "PrimeDeFi", "date": null, "content": "Introducing Primedefi\nInspiration\nWe were all inspired by the huge potential brought by blockchain technology to transcend the old world into a more transparent, secure, and accessible place for everyone. We came to this Hackathon to meet like-minded souls, brainstorm the next unicorn ideas, and work together towards a better future.\nWhat it does\nPrimedefi is a DeFi protocol operating on smart blockchains for the global finance industry.\nWe envision a decentralized virtual deal room that allows:\nIssuers to tokenize debt instruments and automate issuance/book-building processes; and\nInvestors to bid and invest in new issuances and manage their own walletAll of these are built on a smart chain with real time on-chain settlement and pre-defined condition triggered actions.\nPrimedefi will eliminate the need of traditional middlemen and will revolutionize the capital market fund raising processes for issuers and investors via tokenization and smart contracts.\nHow we built it\nBrainstormed for project idea, overall architectural tech design, user story and flow\nSplit tasks/responsibilities among team members based on individual skill-set\nAgreed on project scope, must-have features, choice of dev tools and environments\nHeld regular team zoom calls for project updates to ensure we met interim project milestone\nLeveraged on discord, email, google drive, and github to discuss issues, ideas, and progresses\nWe chose solidity as smart contract language, ethereum/kovan as our main/test-net, Remix and Truffle as back-end dev environment, Figma for UI/UX design, Moralis & Typescript as front-end build tool & database, Moralis/Infura to serve as connection gateway, Metamask as web3 wallet, Chainlink as oracle and keepers service provider, and OpenZeppelin API for their security libraries\nRe-pivoted along the way when necessary (due to newly discovered tech limitations etc)\nChallenges we ran into\nIt was no easy task to communicate effectively and timely amongst 5 team members who are based in 3 different continents (US west coast, Europe, and Asia)\nIt was also challenging for some team members who have other commitments (family/personal) to commit enough time to this project and deliver assigned task on time\nIt was a steep learning curve for all of our devs who had to pick up new programming skills, platforms and tools such as Moralis and Chainlink\nOur business model for the project also tested the technical design limits of feature/architecture of smart contracts and oracles, on what could be practically done, or what could not be done at this point of time in terms of technology feature availability\nAccomplishments that we're proud of\nNone of our team members have met in person or worked together prior to the start of the Chainlink hackathon. We are glad that the hackathon organizers put this event together to connect the community. We assembled our team through the Discord chat rooms and were motivated to work together. Despite of all the obstacles we encountered along the way, we did our best to deliver a minimum viable product prototype at the end of the 5-week timeline.\nWhat we learned\nWe must ensure continuous communication amounts all team members regarding progress made, issues encountered, re-scoping of task list assignments and timeline/deadlines\nAny problem or issue raised needs to be addressed and resolved without procrastination\nSoftware development is always a Work-In-Progress. There is no final version in real life, just continue improving, expanding and fixing along the way\nWhat's next for PrimeDeFi\nWe intend to continue the development of this platform and potentially raising external funding for commercialization of the protocol", "link": "https://devpost.com/software/primedefi", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "introducing primedefi\ninspiration\nwe were all inspired by the huge potential brought by blockchain technology to transcend the old world into a more transparent, secure, and accessible place for everyone. we came to this hackathon to meet like-minded souls, brainstorm the next unicorn ideas, and work together towards a better future.\nwhat it does\nprimedefi is a defi protocol operating on smart blockchains for the global finance industry.\nwe envision a decentralized virtual deal room that allows:\nissuers to tokenize debt instruments and automate issuance/book-building processes; and\ninvestors to bid and invest in new issuances and manage their own walletall of these are built on a smart chain with real time on-chain settlement and pre-defined condition triggered actions.\nprimedefi will eliminate the need of traditional middlemen and will revolutionize the capital market fund raising processes for issuers and investors via tokenization and smart contracts.\nhow we built it\nbrainstormed for project idea, overall architectural tech design, user story and flow\nsplit tasks/responsibilities among team members based on individual skill-set\nagreed on project scope, must-have features, choice of dev tools and environments\nheld regular team zoom calls for project updates to ensure we met interim project milestone\nleveraged on discord, email, google drive, and github to discuss issues, ideas, and progresses\nwe chose solidity as smart contract language, ethereum/kovan as our main/test-net, remix and truffle as back-end dev environment, figma for ui/ux design, moralis & typescript as front-end build -----> tool !!!  & database, moralis/infura to serve as connection gateway, metamask as web3 wallet, chainlink as oracle and keepers service provider, and openzeppelin api for their security libraries\nre-pivoted along the way when necessary (due to newly discovered tech limitations etc)\nchallenges we ran into\nit was no easy task to communicate effectively and timely amongst 5 team members who are based in 3 different continents (us west coast, europe, and asia)\nit was also challenging for some team members who have other commitments (family/personal) to commit enough time to this project and deliver assigned task on time\nit was a steep learning curve for all of our devs who had to pick up new programming skills, platforms and tools such as moralis and chainlink\nour business model for the project also tested the technical design limits of feature/architecture of smart contracts and oracles, on what could be practically done, or what could not be done at this point of time in terms of technology feature availability\naccomplishments that we're proud of\nnone of our team members have met in person or worked together prior to the start of the chainlink hackathon. we are glad that the hackathon organizers put this event together to connect the community. we assembled our team through the discord chat rooms and were motivated to work together. despite of all the obstacles we encountered along the way, we did our best to deliver a minimum viable product prototype at the end of the 5-week timeline.\nwhat we learned\nwe must ensure continuous communication amounts all team members regarding progress made, issues encountered, re-scoping of task list assignments and timeline/deadlines\nany problem or issue raised needs to be addressed and resolved without procrastination\nsoftware development is always a work-in-progress. there is no final version in real life, just continue improving, expanding and fixing along the way\nwhat's next for primedefi\nwe intend to continue the development of this platform and potentially raising external funding for commercialization of the protocol", "sortedWord": "None", "removed": "Nan", "score": 14, "comments": 2, "media": null, "medialink": null, "identifyer": 59506662}, {"Unnamed: 0": 6663, "autor": "WeCare", "date": null, "content": "Inspiration\nOctober is Breast Cancer Awareness Month. I have been constantly been working on Brain Tumor segmentation for good part of last year.\nI recently read a global report : https://pubmed.ncbi.nlm.nih.gov/33538338/\nWhich pointed out that in 2020, Worldwide, an estimated 19.3 million new cancer cases (18.1 million excluding nonmelanoma skin cancer) and almost 10.0 million cancer deaths (9.9 million excluding nonmelanoma skin cancer) occurred in 2020. Female breast cancer has surpassed lung cancer as the most commonly diagnosed cancer, with an estimated 2.3 million new cases (11.7%), followed by lung (11.4%), colorectal (10.0 %), etc. With 6.9% of all deaths due to breast cancer.\nThat's 700000 breast cancer deaths worldwide in 2020 alone.\nCovid has caused a lot of issues, one being delayed diagnosis of cancer patients. Patients had to stand in long queues to get any kind of reports and that itself caused a lot of lives to end. It made me realize how cutting down the time of diagnosis in any way can save lives. I have lost people to cancer so I wonder if we can help in breast cancer diagnosis using deep learning.\nWhat it does\nI wanted to help with the work of radiologists by providing semi automatic tool that used deep learning to reduce their workload to get better inferences and medical report. I also wanted to spread awareness about breast cancer so I also provided details on self examination.\nThere are two deep learning models that have been employed in this project:\nDeep learning UNet breast cancer segmentor\nThis segmentor provides a semi automatic method of segmenting a breast cancer from\nbreast ultrasound images, which provides the initial mask which can be improved upon by radiologist and saved. This can help better inferences\nDeep learning CNN breast cancer classifier(classes being normal, benign and malignant)\nHow we built it\nFour main components on how I built of the project\nModels: Holds details regarding the models that was trained and other details. Segmentation Model is written in tensorflow(UNET) and used breast cancer ultrasound dataset on kaggle. Classification model is an CNN model which is also written in tensorflow and trained on same dataset. This was the first step.\nModel_Server: Flask server that serves the inference models using REST api. This was the second setup and was relatively easy.\nThere are two endpoints:\npredict : for segmentation model inference purpose\nclassify: for classification model inference purpose\nVectorization_server: NodeJs-Express server that converts Raster images to Vector Image svg path. These svg path are the masks that can can overlaid on the orginal tumor for annotation purpose. This was horribly difficult as no good tools available in Python. This was the third step\nAnnotation_Client: Dash app that provides the front-end to work with the segmentation+classification server and allows for manual annotation after automatic annotation is done by inference model. Also classifies the tumor using the classification model.\nChallenges we ran into\nAfter building the machine learning models, increasing its accuracy was a challenge. Also completing the complete project in 36 hours and doing it by myself was a little challenging. There were also a few errors in integrating the HTML, CSS, and JS smoothly in the beginning so resolving that was challenging as well. Conversion of raster images to svg paths was also an interesting problem to work with plotly graphs\nAccomplishments that we're proud of\nI enjoyed learning about how supercomputers are on the verge of revolutionizing healthcare. Soon, we can get results for any kind of test in a matter of minutes through AI. I am also proud of finishing everything on time and creating an aesthetically pleasing website.\nWhat we learned\nUsing CNN and UNET deep learnings models.\nCreating Dash apps\nUI/UX\nDeployements of tensorflow models\nWhat's next for WeCare.\nAdds more labelling utilities to the project that improves the radiologist's workflow. I also want to improve the UI/UX experiences and also the inference times can be reduced.", "link": "https://devpost.com/software/breast-cancer-tool", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\noctober is breast cancer awareness month. i have been constantly been working on brain tumor segmentation for good part of last year.\ni recently read a global report : https://pubmed.ncbi.nlm.nih.gov/33538338/\nwhich pointed out that in 2020, worldwide, an estimated 19.3 million new cancer cases (18.1 million excluding nonmelanoma skin cancer) and almost 10.0 million cancer deaths (9.9 million excluding nonmelanoma skin cancer) occurred in 2020. female breast cancer has surpassed lung cancer as the most commonly diagnosed cancer, with an estimated 2.3 million new cases (11.7%), followed by lung (11.4%), colorectal (10.0 %), etc. with 6.9% of all deaths due to breast cancer.\nthat's 700000 breast cancer deaths worldwide in 2020 alone.\ncovid has caused a lot of issues, one being delayed diagnosis of cancer patients. patients had to stand in long queues to get any kind of reports and that itself caused a lot of lives to end. it made me realize how cutting down the time of diagnosis in any way can save lives. i have lost people to cancer so i wonder if we can help in breast cancer diagnosis using deep learning.\nwhat it does\ni wanted to help with the work of radiologists by providing semi automatic -----> tool !!!  that used deep learning to reduce their workload to get better inferences and medical report. i also wanted to spread awareness about breast cancer so i also provided details on self examination.\nthere are two deep learning models that have been employed in this project:\ndeep learning unet breast cancer segmentor\nthis segmentor provides a semi automatic method of segmenting a breast cancer from\nbreast ultrasound images, which provides the initial mask which can be improved upon by radiologist and saved. this can help better inferences\ndeep learning cnn breast cancer classifier(classes being normal, benign and malignant)\nhow we built it\nfour main components on how i built of the project\nmodels: holds details regarding the models that was trained and other details. segmentation model is written in tensorflow(unet) and used breast cancer ultrasound dataset on kaggle. classification model is an cnn model which is also written in tensorflow and trained on same dataset. this was the first step.\nmodel_server: flask server that serves the inference models using rest api. this was the second setup and was relatively easy.\nthere are two endpoints:\npredict : for segmentation model inference purpose\nclassify: for classification model inference purpose\nvectorization_server: nodejs-express server that converts raster images to vector image svg path. these svg path are the masks that can can overlaid on the orginal tumor for annotation purpose. this was horribly difficult as no good tools available in python. this was the third step\nannotation_client: dash app that provides the front-end to work with the segmentation+classification server and allows for manual annotation after automatic annotation is done by inference model. also classifies the tumor using the classification model.\nchallenges we ran into\nafter building the machine learning models, increasing its accuracy was a challenge. also completing the complete project in 36 hours and doing it by myself was a little challenging. there were also a few errors in integrating the html, css, and js smoothly in the beginning so resolving that was challenging as well. conversion of raster images to svg paths was also an interesting problem to work with plotly graphs\naccomplishments that we're proud of\ni enjoyed learning about how supercomputers are on the verge of revolutionizing healthcare. soon, we can get results for any kind of test in a matter of minutes through ai. i am also proud of finishing everything on time and creating an aesthetically pleasing website.\nwhat we learned\nusing cnn and unet deep learnings models.\ncreating dash apps\nui/ux\ndeployements of tensorflow models\nwhat's next for wecare.\nadds more labelling utilities to the project that improves the radiologist's workflow. i also want to improve the ui/ux experiences and also the inference times can be reduced.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59506663}, {"Unnamed: 0": 6669, "autor": "UNPLUGGED 2121", "date": null, "content": "Inspiration\nCorruption within the City of Nairobi\nWhat it does\nInfluences crucial decision making Gives much needed insights to prepare the player on tackling/shaping and repositioning themselves into the future.\nHow I built it\nIdentified suitable locations. Each location addresses a single objective, interconnecting with other locations. There are three (3) key Characters who can perform well with respect to multiple locations of interest\nChallenges I ran into\nDue to sensitivity of the project, the players could not be allowed to take photos in certain locations. Demolitions/change of locations due to city development and adjustments. Today I am still being forced to search for new locations.\nAccomplishments that I'm proud of\nOrganized game walks including a launch in Nairobi in November 2019, in corroboration with Games and Politics Exhibition. Also, launched \u201cBUSARA\u201d a Board game at Gamescom 2019, in Cologne Germany. Together with other colleagues from other 14 African countries.\nWhat I learned\nThere is always room for better solutions. Change is much needed which can be achieved through making use of technology as a tool.\nWhat's next for UNPLUGGED 2121\nInvolve more people especially decision makers, develop more solutions for other cities and towns.", "link": "https://devpost.com/software/unplugged-2121-jnz6yw", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ncorruption within the city of nairobi\nwhat it does\ninfluences crucial decision making gives much needed insights to prepare the player on tackling/shaping and repositioning themselves into the future.\nhow i built it\nidentified suitable locations. each location addresses a single objective, interconnecting with other locations. there are three (3) key characters who can perform well with respect to multiple locations of interest\nchallenges i ran into\ndue to sensitivity of the project, the players could not be allowed to take photos in certain locations. demolitions/change of locations due to city development and adjustments. today i am still being forced to search for new locations.\naccomplishments that i'm proud of\norganized game walks including a launch in nairobi in november 2019, in corroboration with games and politics exhibition. also, launched \u201cbusara\u201d a board game at gamescom 2019, in cologne germany. together with other colleagues from other 14 african countries.\nwhat i learned\nthere is always room for better solutions. change is much needed which can be achieved through making use of technology as a -----> tool !!! .\nwhat's next for unplugged 2121\ninvolve more people especially decision makers, develop more solutions for other cities and towns.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59506669}, {"Unnamed: 0": 6673, "autor": "ACT: Accessibility Companion Tool", "date": null, "content": "Inspiration\nWe are inspired by the complexity of this challenge, and how we can create a solution that helps broaden innovation in accessibility across ELC, not just for new products but also the entire collection of existing goods and services. Instead of a single solution, we\u2019re proposing a holistic service that connects employees, customers, and experts in accessibility to rally around human-centered innovations.\nWhat it does\nACT (Accessibility Companion Tool) is a platform to find and evaluate products and services. ACT is easily searched and tagged by disability area, product type, and user.\nChallenges\nThe sheer volume of opportunities in the accessibility space! :-)\nAccomplishments that we're proud of\nOur solution itself is also designed with accessibility in mind. For example, our video has closed captioning and our deck is designed for use with screen readers.\nWhat we learned\nOne of the biggest learnings for our team was to reframe away from a perception of individuals with permanent impairments, and to see the value of expanding our efforts to also include situational disabilities.\nWhat's next for ACT\nWe hope to partner with ELC to build out this tool. We\u2019re ready to ACT!", "link": "https://devpost.com/software/act-accessibility-companion-tool-fbx5vl", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe are inspired by the complexity of this challenge, and how we can create a solution that helps broaden innovation in accessibility across elc, not just for new products but also the entire collection of existing goods and services. instead of a single solution, we\u2019re proposing a holistic service that connects employees, customers, and experts in accessibility to rally around human-centered innovations.\nwhat it does\nact (accessibility companion -----> tool !!! ) is a platform to find and evaluate products and services. act is easily searched and tagged by disability area, product type, and user.\nchallenges\nthe sheer volume of opportunities in the accessibility space! :-)\naccomplishments that we're proud of\nour solution itself is also designed with accessibility in mind. for example, our video has closed captioning and our deck is designed for use with screen readers.\nwhat we learned\none of the biggest learnings for our team was to reframe away from a perception of individuals with permanent impairments, and to see the value of expanding our efforts to also include situational disabilities.\nwhat's next for act\nwe hope to partner with elc to build out this tool. we\u2019re ready to act!", "sortedWord": "None", "removed": "Nan", "score": 23, "comments": 0, "media": null, "medialink": null, "identifyer": 59506673}, {"Unnamed: 0": 6682, "autor": "DO[NFT] - Dynamic Ownable NFT Collection", "date": null, "content": "Inspiration\nAre you tired of your old NFTs but not really into selling them? What about modifying them with our tool?\nWhat it does\nWe made a protocol to modify existing NFTs with the help of neural networks using different filters (adding animation and movements, changing style, background, and many others)\nHow we built it\nWe developed the smart contract that uses the create2 mechanism that provides the technology to allow one NFT to own another. As well we trained a neural network to perform animation transfer of motion from source NFT to another NFT. Later other types of neural networks transformation types can be added and applied\nChallenges we ran into\nIt was quite challenging to run a virtual machine with GPU to perform neural network inference. As well we faced some difficulties with the frontend part (deployment and interaction with a smart contract)\nAccomplishments that we're proud of\nWe succeeded in training a neural network to generate good-quality gifs and created some funny NFTs.\nWhat we learned\nHow to work with Polygon How to mint NFT tokens How to create proxy-contract How to train and deploy neural networks How to run GPU virtual machine How to use Flask\nWhat's next for DO[NFT] - Dynamic Ownable NFT Collection\nWe will try to release our project in mainnet Polygon and start to attract NFT creators", "link": "https://devpost.com/software/do-nft-dynamic-ownable-nft-collection", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nare you tired of your old nfts but not really into selling them? what about modifying them with our -----> tool !!! ?\nwhat it does\nwe made a protocol to modify existing nfts with the help of neural networks using different filters (adding animation and movements, changing style, background, and many others)\nhow we built it\nwe developed the smart contract that uses the create2 mechanism that provides the technology to allow one nft to own another. as well we trained a neural network to perform animation transfer of motion from source nft to another nft. later other types of neural networks transformation types can be added and applied\nchallenges we ran into\nit was quite challenging to run a virtual machine with gpu to perform neural network inference. as well we faced some difficulties with the frontend part (deployment and interaction with a smart contract)\naccomplishments that we're proud of\nwe succeeded in training a neural network to generate good-quality gifs and created some funny nfts.\nwhat we learned\nhow to work with polygon how to mint nft tokens how to create proxy-contract how to train and deploy neural networks how to run gpu virtual machine how to use flask\nwhat's next for do[nft] - dynamic ownable nft collection\nwe will try to release our project in mainnet polygon and start to attract nft creators", "sortedWord": "None", "removed": "Nan", "score": 6, "comments": 0, "media": null, "medialink": null, "identifyer": 59506682}, {"Unnamed: 0": 6692, "autor": "Cidato", "date": null, "content": "Inspiration\nCidato - city data tool is an open source project\nWhat it does\nGet Details of the Cities globally\nHow we built it\nWith Love & HTML/CSS/JS\nChallenges we ran into\nHosting\nAccomplishments that we're proud of\nWhat we learned\nWeb Development\nWhat's next for Cidato", "link": "https://devpost.com/software/plutohacks-hack-1", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ncidato - city data -----> tool !!!  is an open source project\nwhat it does\nget details of the cities globally\nhow we built it\nwith love & html/css/js\nchallenges we ran into\nhosting\naccomplishments that we're proud of\nwhat we learned\nweb development\nwhat's next for cidato", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59506692}, {"Unnamed: 0": 6748, "autor": "EnviromLife", "date": null, "content": "Inspiration\ud83d\udca1\ud83d\udca1\nThe effects of climate change are an inevitable truth. Climate change has already and will continue to noticeably affect many critical aspects of day to day life of many people around the world. At this point, it is necessary to consider the future impacts of climate change in order to have certainty in the security and safety of your geographical location. Our team decided to build a web application that interprets Google Cloud Platform\u2019s Earth Engine datasets captured by satellites to teach people about the impacts climate change may have on their everyday life, if things go unchanged. Such a tool can also improve the health of those considering environmental factors when making a decision of where to live in the future. Additionally, this app can help individuals make plans for future decades, if it is necessary for them to relocate for their safety. Moreover, applications like these highlight the tremendous impact and value space technologies like satellites have on everyday life.\nWhat it does\ud83d\udcc4\ud83d\udcc4\nThe web application takes two entries, a location and a year date. Using these two points, it obtains historical data regarding this region from multiple datasets provided by Google\u2019s Earth Engine (Google Cloud Platform), including precipitation, temperature, changes in local water level, natural disaster threat levels, and other factors like atmospheric analysis of certain compounds. After obtaining data for as far back as the information goes, the application attempts to make a future analysis for up until the user\u2019s specified year date. The application then returns various data charts and an index of how livable the specified area will be in said year.\nHow we built it\u2692\ud83d\udee0\nWe ran the React application on Flask to get user queries from another IP address and respond with information. We used React.js to access the information from the website as well as display the requested information. We then searched Earth Engine\u2019s repositories for datasets we could use to build our project. We used various Python libraries to analyze the data extracted from Earth Engine, including statsmodels to train the machine learning models, and matlib to generate and send images to the frontend.\nChallenges we ran into\ud83d\ude11\ud83d\ude13\nThe foremost issue was determining how to interpret the data from Google\u2019s Earth Engine. There is a vast repository of data regarding hundreds of different aspects of Earth, and this data is stored as a wide variety of differing variables and types. The documentation for each dataset is not as comprehensive as we hoped. We had to learn how to interpret scientific data and then use technology to analyze and accurately extrapolate said data. We also had to learn how to efficiently parse through images to cut down the large amount of time it takes to train the model. In addition, the team had to learn new tools such as Flask and server hosting, both of which are essential to the project. While we would have loved to demonstrate analysis of various tools like pixel analysis of vegetation and atmospheric analysis of various compounds, we realized that the processing time was too great for this demonstration and that a creation of a truly accurate model would take some more time than we have for this event.\nAccomplishments that we're proud of\ud83d\ude0e\ud83d\ude0d\nWe are proud of our ability to have created a web tool that not only will help people make a decision about where to live, but also educate people about the real effects of climate change and about the usefulness of space technologies like satellite sensors and imagery. We are also proud to have accomplished so much in the time period allotted, especially for a team that consisted of members who did not have experience in the technologies used.\nWhat we learned\ud83d\udc68\u200d\ud83d\udd2c\ud83d\ude03\nOur team learned how to efficiently analyze large datasets for useful information and trends, in addition to training a model to make accurate analysis of said data. We learned how to deploy a functional web app over a server. In addition, some members of the team were introduced to libraries like React.js for the first time.\nWhat's next for EnviromLife\ud83e\udd29\ud83e\udd29\nWe hope to continue to add more significant features and analysis to our tool. We realized that there are truly vast amounts of information about the climate and Earth about a wide variety of useful categories. Potential features include better analysis of vegetation, droughts, and natural disasters. We would also look into stronger prediction models more accurate to the specific dataset. We also plan to add this project in a form of an android and ios application.", "link": "https://devpost.com/software/enviroment-life", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ud83d\udca1\ud83d\udca1\nthe effects of climate change are an inevitable truth. climate change has already and will continue to noticeably affect many critical aspects of day to day life of many people around the world. at this point, it is necessary to consider the future impacts of climate change in order to have certainty in the security and safety of your geographical location. our team decided to build a web application that interprets google cloud platform\u2019s earth engine datasets captured by satellites to teach people about the impacts climate change may have on their everyday life, if things go unchanged. such a -----> tool !!!  can also improve the health of those considering environmental factors when making a decision of where to live in the future. additionally, this app can help individuals make plans for future decades, if it is necessary for them to relocate for their safety. moreover, applications like these highlight the tremendous impact and value space technologies like satellites have on everyday life.\nwhat it does\ud83d\udcc4\ud83d\udcc4\nthe web application takes two entries, a location and a year date. using these two points, it obtains historical data regarding this region from multiple datasets provided by google\u2019s earth engine (google cloud platform), including precipitation, temperature, changes in local water level, natural disaster threat levels, and other factors like atmospheric analysis of certain compounds. after obtaining data for as far back as the information goes, the application attempts to make a future analysis for up until the user\u2019s specified year date. the application then returns various data charts and an index of how livable the specified area will be in said year.\nhow we built it\u2692\ud83d\udee0\nwe ran the react application on flask to get user queries from another ip address and respond with information. we used react.js to access the information from the website as well as display the requested information. we then searched earth engine\u2019s repositories for datasets we could use to build our project. we used various python libraries to analyze the data extracted from earth engine, including statsmodels to train the machine learning models, and matlib to generate and send images to the frontend.\nchallenges we ran into\ud83d\ude11\ud83d\ude13\nthe foremost issue was determining how to interpret the data from google\u2019s earth engine. there is a vast repository of data regarding hundreds of different aspects of earth, and this data is stored as a wide variety of differing variables and types. the documentation for each dataset is not as comprehensive as we hoped. we had to learn how to interpret scientific data and then use technology to analyze and accurately extrapolate said data. we also had to learn how to efficiently parse through images to cut down the large amount of time it takes to train the model. in addition, the team had to learn new tools such as flask and server hosting, both of which are essential to the project. while we would have loved to demonstrate analysis of various tools like pixel analysis of vegetation and atmospheric analysis of various compounds, we realized that the processing time was too great for this demonstration and that a creation of a truly accurate model would take some more time than we have for this event.\naccomplishments that we're proud of\ud83d\ude0e\ud83d\ude0d\nwe are proud of our ability to have created a web tool that not only will help people make a decision about where to live, but also educate people about the real effects of climate change and about the usefulness of space technologies like satellite sensors and imagery. we are also proud to have accomplished so much in the time period allotted, especially for a team that consisted of members who did not have experience in the technologies used.\nwhat we learned\ud83d\udc68\u200d\ud83d\udd2c\ud83d\ude03\nour team learned how to efficiently analyze large datasets for useful information and trends, in addition to training a model to make accurate analysis of said data. we learned how to deploy a functional web app over a server. in addition, some members of the team were introduced to libraries like react.js for the first time.\nwhat's next for enviromlife\ud83e\udd29\ud83e\udd29\nwe hope to continue to add more significant features and analysis to our tool. we realized that there are truly vast amounts of information about the climate and earth about a wide variety of useful categories. potential features include better analysis of vegetation, droughts, and natural disasters. we would also look into stronger prediction models more accurate to the specific dataset. we also plan to add this project in a form of an android and ios application.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59506748}, {"Unnamed: 0": 6763, "autor": "AutomationLoop", "date": null, "content": "Q) What is the purpose of this App?\nAns-> It is a very difficult and time taking task to configure 100 and 1000 machines at one time, So here this AutomationLoop's role comes into play through this application, you do not need to write a single piece of code as this application provides a graphical user interface to complete the task.\nQ) What type of data centers can be configured through this app?\nAns->1 Private data centres 2 public data cloud\nQ) What are the Inputs required for the private data center.\nAns-> this app requires certain input like- no of machines to be configured, IP, Authentication details, the software you want to configure\nQ) What are the Inputs required for public cloud machines.\nAns-> this app requires certain input like- virtual machine tags, secret key, Access key\nQ) How this application works?\nAns-> step1- you need to select the type of data center you want to configure.\ns2-choose the software you want to configure(ex-Docker,load balancer,Hadoop,e.t.c)\ns3-enter the number of machines you want to configure\ns4-enter the details of the machine(IP, Username, Password)\ns5-click on the execute button\ns6-machines which are in the process of configuring will be displayed on the shell below\nQ) How this application works for the public cloud?\nAns->step1- you need to select the type of data center you want to configure. s2-give the access key and secret key to this application s3-choose the software you want to configure\nQ) Software that can configure currently by our app\nAns->Doakcer, Web Server(Apache, Nginx), Mail Server(Zimbra), Load Balancer(HAproxy)\nQ) Currently the cloud service we use.\nAns->At this time we use Amazon Web Service but We will also work on Microsoft Azure, a Google cloud platform in the coming future.\nQ)What are the future updates\nAns-> Currently our client has to give their IP details so to overcome this we will integrate an IP scanning tool through which the user has to give only the CIDR value(range). To tackle the same problem there is another solution provided by us in which the user has to provide an excel file with details of IP and authentication through this we can scan this file and maintain our inventory.", "link": "https://devpost.com/software/automationloop", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "q) what is the purpose of this app?\nans-> it is a very difficult and time taking task to configure 100 and 1000 machines at one time, so here this automationloop's role comes into play through this application, you do not need to write a single piece of code as this application provides a graphical user interface to complete the task.\nq) what type of data centers can be configured through this app?\nans->1 private data centres 2 public data cloud\nq) what are the inputs required for the private data center.\nans-> this app requires certain input like- no of machines to be configured, ip, authentication details, the software you want to configure\nq) what are the inputs required for public cloud machines.\nans-> this app requires certain input like- virtual machine tags, secret key, access key\nq) how this application works?\nans-> step1- you need to select the type of data center you want to configure.\ns2-choose the software you want to configure(ex-docker,load balancer,hadoop,e.t.c)\ns3-enter the number of machines you want to configure\ns4-enter the details of the machine(ip, username, password)\ns5-click on the execute button\ns6-machines which are in the process of configuring will be displayed on the shell below\nq) how this application works for the public cloud?\nans->step1- you need to select the type of data center you want to configure. s2-give the access key and secret key to this application s3-choose the software you want to configure\nq) software that can configure currently by our app\nans->doakcer, web server(apache, nginx), mail server(zimbra), load balancer(haproxy)\nq) currently the cloud service we use.\nans->at this time we use amazon web service but we will also work on microsoft azure, a google cloud platform in the coming future.\nq)what are the future updates\nans-> currently our client has to give their ip details so to overcome this we will integrate an ip scanning -----> tool !!!  through which the user has to give only the cidr value(range). to tackle the same problem there is another solution provided by us in which the user has to provide an excel file with details of ip and authentication through this we can scan this file and maintain our inventory.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59506763}, {"Unnamed: 0": 6767, "autor": "Voice For Everybody", "date": null, "content": "Voice For Everybody\nAn Initiative to help the auditory disabled Using AI and Machine Learning\nInspiration\nSince the outbreak of COVID-19, while the rest of the world has moved online, ASL speakers faced even greater inequities making it difficult for so many of them to communicate. However, this has to come to an end. In the pursuit of finding accessibility, I created a tool to empower ASL speakers to speak freely with the help of AI.\nWhat it does\nUses a webcam to translate ASL speech to text.\nHow I built it\nUsed Mediapipe to generate points on hands, then use those points to get training data set. I used Jupyter Notebook to run OpenCV and Mediapipe. Upon running our data in Mediapipe, I was able to get a skeleton map of the body with 22 points for each hand. These points can be mapped in 3-dimension as it contains X, Y, and Z-axis. I processed these features (22 points x 3) by saving them into a spreadsheet. Then I divided the spreadsheet into training and testing data. Using the training set, I were able to create 6 Machine learning models:\n\u2022 Gradient Boost Classifier \u2022 XGBoost Classifier \u2022 Support Vector Machine \u2022 Logistic Regression \u2022 Ridge Classifier \u2022 Random Forest Classifier\nChallenges I ran into\n\u2022 Had to do solo work due to issues with the team \u2022 Time management \u2022 Project management \u2022 Lack of data\nAccomplishments that I'm proud of\nProud of pivoting my original idea and completing this epic hackathon. Also proud of making a useful tool & won a place in the Top 10 best performers in the Digital Education Hack South Asia 2021 and more\nWhat I learned\n\u2022 Time management \u2022 Project management\nWhat's next for Voice4Everyone\n\u2022 More training of data - more classifications \u2022 Phone app + Chrome Extension \u2022 Reverse translation: Converting English Text to ASL \u2022 Cleaner UI \u2022 Add support for the entire ASL dictionary and other sign languages", "link": "https://devpost.com/software/voice-for-everybody", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "voice for everybody\nan initiative to help the auditory disabled using ai and machine learning\ninspiration\nsince the outbreak of covid-19, while the rest of the world has moved online, asl speakers faced even greater inequities making it difficult for so many of them to communicate. however, this has to come to an end. in the pursuit of finding accessibility, i created a -----> tool !!!  to empower asl speakers to speak freely with the help of ai.\nwhat it does\nuses a webcam to translate asl speech to text.\nhow i built it\nused mediapipe to generate points on hands, then use those points to get training data set. i used jupyter notebook to run opencv and mediapipe. upon running our data in mediapipe, i was able to get a skeleton map of the body with 22 points for each hand. these points can be mapped in 3-dimension as it contains x, y, and z-axis. i processed these features (22 points x 3) by saving them into a spreadsheet. then i divided the spreadsheet into training and testing data. using the training set, i were able to create 6 machine learning models:\n\u2022 gradient boost classifier \u2022 xgboost classifier \u2022 support vector machine \u2022 logistic regression \u2022 ridge classifier \u2022 random forest classifier\nchallenges i ran into\n\u2022 had to do solo work due to issues with the team \u2022 time management \u2022 project management \u2022 lack of data\naccomplishments that i'm proud of\nproud of pivoting my original idea and completing this epic hackathon. also proud of making a useful tool & won a place in the top 10 best performers in the digital education hack south asia 2021 and more\nwhat i learned\n\u2022 time management \u2022 project management\nwhat's next for voice4everyone\n\u2022 more training of data - more classifications \u2022 phone app + chrome extension \u2022 reverse translation: converting english text to asl \u2022 cleaner ui \u2022 add support for the entire asl dictionary and other sign languages", "sortedWord": "None", "removed": "Nan", "score": 11, "comments": 0, "media": null, "medialink": null, "identifyer": 59506767}, {"Unnamed: 0": 6777, "autor": "Budgie: An Inclusive Tangible Music Programming Environment", "date": null, "content": "Inspiration\nProgramming is now a part of the primary education curriculum as it becomes a crucial 21st-century skill for various employment fields. Hence, through programming, pupils acquire many vital skills such as creative and computational thinking and gain hands-on abilities such as building electronics and integrating technology into their daily lives. However, commonly used programming tools depend on visual elements, making them relatively inaccessible for visually impaired children. This can lead to an inequality in developing the skills mentioned above for the visually impaired community. To that end, we developed an affordable tangible programming tool for visually impaired children that focuses on creating music with algorithms. We chose music creation because it is one of the most preferred and accessible creative subjects for the visually impaired community.\nWhat it does\nBudgie is an affordable tangible programming tool to create music with algorithms. Our block set consists of twenty-five coding blocks varying in levels of perceptual information (shape and surface detail). These design decisions were made with the guidance of visually impaired developers. The music blocks only require NFC stickers for fast and reliable recognition, and we synthesize the music on Android phones.\nHow we built it\nOur design process consisted of three main steps. First, we conducted participatory design sessions with visually impaired developers. Second, building on this experience, we derived design considerations for our tangible music programming kit. Lastly, we developed our prototype and gathered feedback from mixed-visual ability students. Below we elaborate on our development process further.\nParticipatory design workshops with visually impaired developers:\nWe formulated this research step to identify visually impaired children's needs in a learning environment and derive design considerations. We conducted a workshop session with two visually impaired developers who work in a children-oriented technology company and have been coding for more than two years. They have experience in teaching coding to visually impaired children and know about the obstacles they encounter. In our workshops, we gathered their input on how to design the music blocks.\nFormulating design considerations:\nAs a result of the participatory design workshops, we extracted three primary design considerations for the tangible music kit.\n(1) Avoid the extensive use of the Braille alphabet: The developers stated that the students spend a lot of time trying to read and remember the codes. So, in our design, we only used the Braille alphabet for communicating the musical notes.\n(2) Do not use various feedback modalities to recognize the blocks: Using several feedback types (i.e., audio, haptics) to help children recognize the function of the blocks might overwhelm and confuse the students. We decided to use only shapes as the distinctive feature between the blocks.\n(3) Use abstract and basic shapes rather than detailed icons: Surface engravings need to be easily discernible by touch and conceptually understandable at the same time. For example, a piano icon can be engraved on a tangible surface, but the students might not know what a piano looks like or need extra time to comprehend the engraving itself. Therefore, we adopted simple icons and explored surface information alternatively.\nConceptual Prototype:\nWe designed twenty-five tangible block pieces and developed an Android application. The blocks allow students to design an algorithmic melody with different octave, rhythm, and sound. Tangible blocks are laser-cut cardboards with NFC stickers inside. We used card-board as an affordable and fast solution, but it can be 3D printed or built by any other material that the students are comfortable with. To read the NFC stickers and run the output, we developed an Android application. We didn't use a camera system to recognize the blocks because the system would constrain the working space due to the camera's viewing region. Hence, students would not understand if the block is in viewing the region or identify such a problem easily. To keep track of the blocks on the surface we used a simple rack. Additionally, they can check the program by switching to the info area and then tapping onto the RUN block to hear all the script.\nThe Android application recognizes the blocks with NFC stickers as the coder moves the phone over each block one at a time. As the last step, the play block is scanned and the melody plays from the speakers. The application has two modes: code and info. To create the code and play the melody, students need to be in code mode. If students switch to info mode, they can listen to the functionality of the blocks.\nThe design of the blocks programming language has a very similar style with Processing language. The programmer needs to specify the attributes first, then define the object that will carry the properties. We utilized Processing language as the core of our tangible kit.\nTrying out the conceptual prototype with visually impaired children:\nWe conducted a user study to observe children's interaction with the tangible blocks and the mobile device. Fourteen visually impaired students (7 boys, 7 girls, Mage=12.5) with various socio-economic statuses participated in the study.\nWe designed this user study structure to make participants experience sounds, learn the logic of creating algorithms and learn musical terms such as notes, octave, rhythm, melody, tempo, and sound. One of the most prominent findings of our user study was participants\u2019 enjoyment and engagement with the system. The participants were smiling, talking excitedly, and working very carefully with the system throughout the study. Through this user study, we were also able to identify the challenges of our prototype and the future steps of our project.\nChallenges we ran into\nStarting the design process with the support of visually impaired developers led us to solve most challenges before they occurred. Yet, in the workshop with mixed visual ability students, we encountered a challenge related to distraction. To explain, the groups need to run their code occasionally to hear the creation. Throughout the study, a cacophony of musical sounds emerged, which was slightly distracting. Regardless, the participants were able to maintain their focus on the task. Ideally, children will work in pairs to scale the project and can use headphones to overcome this issue. In this way, many children will be still working in the same room, but only their conversations would create a slight noise.\nWe would like to extend the curricular materials and tool's capability, but reaching the right community (teachers, developers, etc.) to support the development process is a challenge.\nAccomplishments that we're proud of\nOur system is the first algorithmic style- tangible music creation platform for visually impaired children and provides several tangible interaction design considerations for students with low to zero sight.\nBudgie has the potential to support children's engagement and collaboration with each other. We further saw that the design of the blocks was easy to differentiate and use. After the user study, all students stated that they would like to pursue programming education, which indicates our design considerations' ability to support their needs.\nWhat we learned\nDeveloping Budgie with visually impaired developers allowed us to understand the community better. We learned their needs and solved many possible challenges beforehand, which showed the benefits of d their needs and solved many possible challenges beforehand, showing the benefits of a participatory design approach.\nWe explored various interaction strategies, output modalities, and communication styles to enhance the flow of the coding process. We learned structuring a multi-level information flow in interaction and coding is a better way to grasp information.\nWhat's next for Budgie: An Inclusive Tangible Music Programming Environment\nAfter the encouraging findings of our workshops, we will pursue developing our system with the following steps:\nTo fully develop the computational thinking skills of visually impaired children, we aim to introduce additional blocks to use abstraction, decomposition, or pattern recognition skills while creating the algorithm.\nWe will expand our design to meet the programming curriculum standards. For example, we will alter our design so that it allows tinkering and remixing of the projects.\nWe would like to build an inexpensive commercial kit that schools can easily get and start coding.", "link": "https://devpost.com/software/budgie-an-inclusive-tangible-music-programming-environment", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nprogramming is now a part of the primary education curriculum as it becomes a crucial 21st-century skill for various employment fields. hence, through programming, pupils acquire many vital skills such as creative and computational thinking and gain hands-on abilities such as building electronics and integrating technology into their daily lives. however, commonly used programming tools depend on visual elements, making them relatively inaccessible for visually impaired children. this can lead to an inequality in developing the skills mentioned above for the visually impaired community. to that end, we developed an affordable tangible programming -----> tool !!!  for visually impaired children that focuses on creating music with algorithms. we chose music creation because it is one of the most preferred and accessible creative subjects for the visually impaired community.\nwhat it does\nbudgie is an affordable tangible programming tool to create music with algorithms. our block set consists of twenty-five coding blocks varying in levels of perceptual information (shape and surface detail). these design decisions were made with the guidance of visually impaired developers. the music blocks only require nfc stickers for fast and reliable recognition, and we synthesize the music on android phones.\nhow we built it\nour design process consisted of three main steps. first, we conducted participatory design sessions with visually impaired developers. second, building on this experience, we derived design considerations for our tangible music programming kit. lastly, we developed our prototype and gathered feedback from mixed-visual ability students. below we elaborate on our development process further.\nparticipatory design workshops with visually impaired developers:\nwe formulated this research step to identify visually impaired children's needs in a learning environment and derive design considerations. we conducted a workshop session with two visually impaired developers who work in a children-oriented technology company and have been coding for more than two years. they have experience in teaching coding to visually impaired children and know about the obstacles they encounter. in our workshops, we gathered their input on how to design the music blocks.\nformulating design considerations:\nas a result of the participatory design workshops, we extracted three primary design considerations for the tangible music kit.\n(1) avoid the extensive use of the braille alphabet: the developers stated that the students spend a lot of time trying to read and remember the codes. so, in our design, we only used the braille alphabet for communicating the musical notes.\n(2) do not use various feedback modalities to recognize the blocks: using several feedback types (i.e., audio, haptics) to help children recognize the function of the blocks might overwhelm and confuse the students. we decided to use only shapes as the distinctive feature between the blocks.\n(3) use abstract and basic shapes rather than detailed icons: surface engravings need to be easily discernible by touch and conceptually understandable at the same time. for example, a piano icon can be engraved on a tangible surface, but the students might not know what a piano looks like or need extra time to comprehend the engraving itself. therefore, we adopted simple icons and explored surface information alternatively.\nconceptual prototype:\nwe designed twenty-five tangible block pieces and developed an android application. the blocks allow students to design an algorithmic melody with different octave, rhythm, and sound. tangible blocks are laser-cut cardboards with nfc stickers inside. we used card-board as an affordable and fast solution, but it can be 3d printed or built by any other material that the students are comfortable with. to read the nfc stickers and run the output, we developed an android application. we didn't use a camera system to recognize the blocks because the system would constrain the working space due to the camera's viewing region. hence, students would not understand if the block is in viewing the region or identify such a problem easily. to keep track of the blocks on the surface we used a simple rack. additionally, they can check the program by switching to the info area and then tapping onto the run block to hear all the script.\nthe android application recognizes the blocks with nfc stickers as the coder moves the phone over each block one at a time. as the last step, the play block is scanned and the melody plays from the speakers. the application has two modes: code and info. to create the code and play the melody, students need to be in code mode. if students switch to info mode, they can listen to the functionality of the blocks.\nthe design of the blocks programming language has a very similar style with processing language. the programmer needs to specify the attributes first, then define the object that will carry the properties. we utilized processing language as the core of our tangible kit.\ntrying out the conceptual prototype with visually impaired children:\nwe conducted a user study to observe children's interaction with the tangible blocks and the mobile device. fourteen visually impaired students (7 boys, 7 girls, mage=12.5) with various socio-economic statuses participated in the study.\nwe designed this user study structure to make participants experience sounds, learn the logic of creating algorithms and learn musical terms such as notes, octave, rhythm, melody, tempo, and sound. one of the most prominent findings of our user study was participants\u2019 enjoyment and engagement with the system. the participants were smiling, talking excitedly, and working very carefully with the system throughout the study. through this user study, we were also able to identify the challenges of our prototype and the future steps of our project.\nchallenges we ran into\nstarting the design process with the support of visually impaired developers led us to solve most challenges before they occurred. yet, in the workshop with mixed visual ability students, we encountered a challenge related to distraction. to explain, the groups need to run their code occasionally to hear the creation. throughout the study, a cacophony of musical sounds emerged, which was slightly distracting. regardless, the participants were able to maintain their focus on the task. ideally, children will work in pairs to scale the project and can use headphones to overcome this issue. in this way, many children will be still working in the same room, but only their conversations would create a slight noise.\nwe would like to extend the curricular materials and tool's capability, but reaching the right community (teachers, developers, etc.) to support the development process is a challenge.\naccomplishments that we're proud of\nour system is the first algorithmic style- tangible music creation platform for visually impaired children and provides several tangible interaction design considerations for students with low to zero sight.\nbudgie has the potential to support children's engagement and collaboration with each other. we further saw that the design of the blocks was easy to differentiate and use. after the user study, all students stated that they would like to pursue programming education, which indicates our design considerations' ability to support their needs.\nwhat we learned\ndeveloping budgie with visually impaired developers allowed us to understand the community better. we learned their needs and solved many possible challenges beforehand, which showed the benefits of d their needs and solved many possible challenges beforehand, showing the benefits of a participatory design approach.\nwe explored various interaction strategies, output modalities, and communication styles to enhance the flow of the coding process. we learned structuring a multi-level information flow in interaction and coding is a better way to grasp information.\nwhat's next for budgie: an inclusive tangible music programming environment\nafter the encouraging findings of our workshops, we will pursue developing our system with the following steps:\nto fully develop the computational thinking skills of visually impaired children, we aim to introduce additional blocks to use abstraction, decomposition, or pattern recognition skills while creating the algorithm.\nwe will expand our design to meet the programming curriculum standards. for example, we will alter our design so that it allows tinkering and remixing of the projects.\nwe would like to build an inexpensive commercial kit that schools can easily get and start coding.", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59506777}, {"Unnamed: 0": 6823, "autor": "24/7 MD", "date": null, "content": "Inspiration\nWe were pumped up to create a web tool that would bridge the gap between doctors and patients, allowing patients to consult a doctor whenever they wanted. For a healthy life keep your friends, family and doctors close.\nKey Features \ud83d\ude03\n-A full stack Web application\nauthentication and Twilio SMS notification\nWith direct and group chats along with emojis and reaction\nbuilt in gif support\nability to edit and delete messages\nfully responsive with providing functionalities to create as many group channels as you want.\nWe can also share media like images, videos , PDF.\nAlerts user with a new message notification when user is offline\nThe Problem it Solves :\nOur web application creates a friendly and flexible environment between doctors and patients which will solve the issues like\nwaiting in lines for appointment\nnot being able to contact the doctors during emergency\nno late night consultations available.\nChallenges we ran into\nAt some points connectivity between platforms was an issue\nAlso we facing trouble because of mismatch of class names in js and CSS\nProject was frozen because we were running localhost with HTTPS but was rectified by using HTTP\nWe were also stuck for a long time because our channel list and Direct Message list was not visible\nAccomplishments we are proud of\nError resolved:\nWhen we were able to rectify the two major errors we were facing of localhost and channel list\nSuccesses:\nSending and receiving our first message was the best accomplishment\nAlso when server and site were deployed successfully\nWhat's next for 24/7 MD\nIt can further be developed by adding more things like real time video chat, payment option. Also we can connect with doctors, patients and hospital and build a healthy environment with daily health update and health tips direct by experts.", "link": "https://devpost.com/software/24-7-md", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe were pumped up to create a web -----> tool !!!  that would bridge the gap between doctors and patients, allowing patients to consult a doctor whenever they wanted. for a healthy life keep your friends, family and doctors close.\nkey features \ud83d\ude03\n-a full stack web application\nauthentication and twilio sms notification\nwith direct and group chats along with emojis and reaction\nbuilt in gif support\nability to edit and delete messages\nfully responsive with providing functionalities to create as many group channels as you want.\nwe can also share media like images, videos , pdf.\nalerts user with a new message notification when user is offline\nthe problem it solves :\nour web application creates a friendly and flexible environment between doctors and patients which will solve the issues like\nwaiting in lines for appointment\nnot being able to contact the doctors during emergency\nno late night consultations available.\nchallenges we ran into\nat some points connectivity between platforms was an issue\nalso we facing trouble because of mismatch of class names in js and css\nproject was frozen because we were running localhost with https but was rectified by using http\nwe were also stuck for a long time because our channel list and direct message list was not visible\naccomplishments we are proud of\nerror resolved:\nwhen we were able to rectify the two major errors we were facing of localhost and channel list\nsuccesses:\nsending and receiving our first message was the best accomplishment\nalso when server and site were deployed successfully\nwhat's next for 24/7 md\nit can further be developed by adding more things like real time video chat, payment option. also we can connect with doctors, patients and hospital and build a healthy environment with daily health update and health tips direct by experts.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59506823}, {"Unnamed: 0": 6856, "autor": "Solar Powered Water Quality Monitoring", "date": null, "content": "Project Description\n1) Water Quality Management 2) Water Treatment\nAbstract\nAccording to a WHO report, 21% of the total diseases are caused by unsafe water and 77 Million people are suffering due to not having access to safe drinking water. This data clearly raises an alarm on this issue.\nThe water-borne disease continues to occur and is one of the key concerns in developing countries.\n\u2022 In India 3.4 million people, mostly children, die annually from water-related diseases. \u2022 About 2.2 million people die from diarrhea.\n\u2022 90% of these deaths are among children, mostly in developing countries.\n\u2022 More than 1 billion people draw their water from unsafe sources.\n\u2022 The UN has included \u201cSafe Drinking Water\u201d for everyone in Sustainability Development Goals 2030\nThe citizens are not even aware what the quality of the water they are consuming.\nAn interdisciplinary approach is required for providing the best water quality measurement system. The quality of water is detected by considering physical, biological, and chemical characteristics. Today\u2019s system to detect the quality of water, especially for chemical and biological contamination, requires access to lab equipment and sometimes takes 2-3 days. Citizens don\u2019t have access to data about common water quality parameters such as chemical contamination (e.g. amount of Chlorine, CO2, pH value, dissolved oxygen, etc.) and biological contamination (e.g. e- Coli bacteria, etc.) in real-time. In order to ensure the safe supply of drinking water the quality needs to be monitored in real-time. In this paper, we present a design and development of a low-cost solar-powered system for real-time monitoring of water quality in IoT(internet of things). The system consists of several sensors is used to measure the physical and chemical parameters of the water. The parameters such as temperature, PH, turbidity, conductivity, dissolved oxygen of the water can be measured. The measured values from the sensors can be processed by the core controller. The Arduino\nThe Uno model can be used as a core controller. Finally, the sensor data can be viewed on the internet using cloud computing. (ThingSpeak Platform), Here Machine learning algorithms would be applied to address the dependencies between the various parameters of standard levels as described by W.H.O.This will finally be represented as a heat map of a geographical area represented as a web interface.\nBefore moving ahead we must know what are the traditional existing methods of monitoring water quality and try to find some gaps which can be bridged using digitization and the use of IoT technologies.\nOrthodox and Existing methods of water monitoring\nThe traditional methods of water quality monitoring involve the manual collection of water samples from different locations. These water samples were tested in the laboratory using analytical technologies. Such approaches are:\n1.)Time consuming and no longer to be considered efficient\n2.) The current methodologies include analysis of various kinds of parameters of water quality such as physical and chemical\n3.)Complicated methodology\n4.)Long waiting time for results\n5.)Low measurement precision\n6.)High cost\nTherefore, we find this opportunity area that there is a need for continuous monitoring of water quality parameters in real-time\nDevelopment Of Our Model:\nBy focusing on the above issues, we have to develop and design a low-cost water quality monitoring system that can monitor water quality in real-time using an IoT environment. In our proposed system water quality parameters are measured by the different water quality monitoring sensors such as pH, turbidity, conductivity, dissolved oxygen, and temperature. This sensor- values are processed by the microcontroller and these processed values are sent to the core controller remotely using WiFi module IEEE802.11.x protocol. In the proposed system, the IOT module is used to access processed data from the core controller to the cloud. The processed data can be monitored through a browser application using a special IP address. Furthermore, with the help of an IoT environment, we can provide a facility to access data remotely from all over the world.\nObjectives\nThe objectives that we aim to achieve are:\n\u2022 How can we provide real-time water quality monitoring systems at block /\nbuilding complex/household?\n\u2022 How can we allow the administration to quickly and efficiently collect data about water quality in the complete city?\n\u2022 How can we optimize the process of water treatment/ sewage treatment by checking the water quality in real-time?\n\u2022 How can we prevent and hazards or crises due to the mixing of harmful elements in the water supply in real-time, which would help in blocking harmful water in the pipeline supply.\n\u2022 How can we build a self-intelligent system, which can learn from patterns seen previously and judge from the previous dataset to classify water safe or unsafe based on standard learned parameters?\n\u2022 How can the system be trained to predict any signs of water crisis based on machine learning algorithms?\nInnovation\nThe solution can be divided into two phases:-\n1) Phase-1, Measurement and Analysis-\nThe first step will be converting lab type instruments in rugged real time application oriented sensors and designing and calibrating them to measure water quality accurately and cost effectively. The measurement of various parameters as discussed above are needed to be done at various places like at source of water (river, lakes or ponds), filtration plant, pipelines, or at water tank of household-level or building level. As there is no power supply present near the household water tank a low-cost 10W solar panel can be used to power the microcontroller and sensors. This will make the operating cost of the measurement system very low. Then the outputs from various sensors will be sent to an internet cloud service via a wifi module connected to the microcontroller to which all sensors are connected.\nAccording to the real-time data produced the quality of water will be rated on the basic water quality index and home users can see the quality of water which they are drinking in real-time on their LAN(Local Area Network) or on the internet(cloud website).\nIf this kind of system is implemented at most of household water tanks or building complexes then the state water management board or committee can see the water quality data of every household online and they can also derive analysis from the previously recorded data.\nSo the data is being sent to the cloud website in real-time, now the important question comes that how can we judiciously use this stream of data pumping in?\nThe answer lies in the concepts of Machine Learning and Pattern Recognition, we can actually analyze this data and create a training set for our classifier system . This classifier system would basically decide on the quality of water based on the W.H.O standards of certain parameters like turbidity, conductance, pH value etc. Now the trained system would be using some clustering or classification algorithm (here we would be using KNN clustering algorithm) to take a decision on the quality of water.\nThis will be a self-learning system and the accuracy would get enhanced more and more after the addition of more and more data points. In the latest, stage we can represent using the digital platform.\nRepresentation of Data\nThe users can view and control the systems through mobile applications or Web Dashboards. The dashboard will work as a data visualization tool that will display the current status of the quality of water in Real-time to different users after login.\nThe dashboard will consolidate and arrange numbers, metrics, and sometimes performance scorecards on a single screen. The essential features of the dashboard will be to provide a customizable interface and the ability to pull real-time data from multiple sources.\nThe mobile application will be for end-user having less number of uses or appliances (eg. home users) and the web dashboard is for end-users having a large no of uses (eg. Government Water Board) who have to control large water bodies.\nThe Mobile application can be connected via LAN and can give representation and analysis without the internet in Real-time.\n2) Phase-2, Control, and Cleaning Action\nPhase -1 is of no use we don't take the required corrective action for cleaning water and maintain its quality if we detect an unhealthy water supply. Integrating Phase 1 analysis and output into the daily operations of the water management board is also an issue. And for dealing with this an android application can be made for helping water board workers to for doing necessary operations in water treatment and effective supply.\nIf there is bad water present inside any household tank then the measurement system will generate a request to the internet cloud application automatically and the cloud will send the notification to the area manager's mobile app and the necessary action would be taken. From the analysis of Phase 1\nwe can also determine the points from where the water gets polluted in the system during supply or other actions using machine learning. If we also measure the consumption of water in every household from the same device we can also derive the data of total consumption of water from the user and its wastage. We can also do water metering if we want to from the same device only. At the time of the water crisis, we can also regulate the supply for better utilization of the resource.40% of water is wasted at the time of transportation of it. We can find loopholes in the systems and work on them by the analysis provided by the data and decrease the non-revenue water amount from 40% to 5% using this technology. So this idea is a pill for most of the problems in water management.\nSteps of Development:\n1) Application-oriented sensor designing.\n2) Sensors Calibration.\n3) Microcontroller Interfacing. 4) Connecting Microcontroller to the Internet.\n5) Solar Power System Designing. 6) Complete product casing and CAD Simulation.\n7) Cloud Application formation / GSM-based data communication. 8) Algorithm(KNN) to train a classifier system 9)Prediction of Water Crisis based on visible patterns 10) Real-time analysis and processing of data collected from the cloud web platform 11)User interface for both individual and company based usage. 12) Mobile Application formation.\n13) User Integration and Management. *14) Calculation of water utilized per day for the whole city.\n*15) Water Metering (can be done from the same device made in phase 1).\nSteps that are not necessary but can be implemented\nKey Benefits are as follows:\n1) Complete measurement of water quality from household to city level at very low cost.\n2) Real-Time Monitoring of Water Quality and Quality.\n3) Efficient water filtration plant operations and sewage treatment plant operations due to closed-loop control. 4) Energy optimization and eco-friendly as solar power is used.\n5) Non-revenue water can be decreased to 5% from 40%. 6) We can also add the functionality of adding a water billing system to this system.\nFUTURE SCOPE:\nTo give information to whole users who depend on that plant 2. Detecting more parameters for the most secure purposes 3. Increase the parameters by addition of multiple sensors\nBy interfacing relay we control the supply of water\nADVANTAGES :\nDue to automation it will reduce the time to check the parameters. 2. This is economically affordable for common people. 3. Low maintenance. 4. Prevention of water diseases.\nCONCLUSION\nMonitoring of Turbidity, PH & Temperature of Water makes use of water detection sensor with unique advantage and existing GSM network. The system can monitor water quality automatically, and it is low in cost and does not require people on duty. So the water quality testing is likely to be more economical, convenient and fast. The system has good flexibility. Only by replacing the corresponding sensors and changing the relevant software programs, this system can be used to monitor other water quality parameters. The operation is simple. The system can be expanded to monitor hydrologic, air pollution, industrial and agricultural production and so on. It has widespread application and extension value.", "link": "https://devpost.com/software/solar-powered-water-quality-monitoring", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "project description\n1) water quality management 2) water treatment\nabstract\naccording to a who report, 21% of the total diseases are caused by unsafe water and 77 million people are suffering due to not having access to safe drinking water. this data clearly raises an alarm on this issue.\nthe water-borne disease continues to occur and is one of the key concerns in developing countries.\n\u2022 in india 3.4 million people, mostly children, die annually from water-related diseases. \u2022 about 2.2 million people die from diarrhea.\n\u2022 90% of these deaths are among children, mostly in developing countries.\n\u2022 more than 1 billion people draw their water from unsafe sources.\n\u2022 the un has included \u201csafe drinking water\u201d for everyone in sustainability development goals 2030\nthe citizens are not even aware what the quality of the water they are consuming.\nan interdisciplinary approach is required for providing the best water quality measurement system. the quality of water is detected by considering physical, biological, and chemical characteristics. today\u2019s system to detect the quality of water, especially for chemical and biological contamination, requires access to lab equipment and sometimes takes 2-3 days. citizens don\u2019t have access to data about common water quality parameters such as chemical contamination (e.g. amount of chlorine, co2, ph value, dissolved oxygen, etc.) and biological contamination (e.g. e- coli bacteria, etc.) in real-time. in order to ensure the safe supply of drinking water the quality needs to be monitored in real-time. in this paper, we present a design and development of a low-cost solar-powered system for real-time monitoring of water quality in iot(internet of things). the system consists of several sensors is used to measure the physical and chemical parameters of the water. the parameters such as temperature, ph, turbidity, conductivity, dissolved oxygen of the water can be measured. the measured values from the sensors can be processed by the core controller. the arduino\nthe uno model can be used as a core controller. finally, the sensor data can be viewed on the internet using cloud computing. (thingspeak platform), here machine learning algorithms would be applied to address the dependencies between the various parameters of standard levels as described by w.h.o.this will finally be represented as a heat map of a geographical area represented as a web interface.\nbefore moving ahead we must know what are the traditional existing methods of monitoring water quality and try to find some gaps which can be bridged using digitization and the use of iot technologies.\northodox and existing methods of water monitoring\nthe traditional methods of water quality monitoring involve the manual collection of water samples from different locations. these water samples were tested in the laboratory using analytical technologies. such approaches are:\n1.)time consuming and no longer to be considered efficient\n2.) the current methodologies include analysis of various kinds of parameters of water quality such as physical and chemical\n3.)complicated methodology\n4.)long waiting time for results\n5.)low measurement precision\n6.)high cost\ntherefore, we find this opportunity area that there is a need for continuous monitoring of water quality parameters in real-time\ndevelopment of our model:\nby focusing on the above issues, we have to develop and design a low-cost water quality monitoring system that can monitor water quality in real-time using an iot environment. in our proposed system water quality parameters are measured by the different water quality monitoring sensors such as ph, turbidity, conductivity, dissolved oxygen, and temperature. this sensor- values are processed by the microcontroller and these processed values are sent to the core controller remotely using wifi module ieee802.11.x protocol. in the proposed system, the iot module is used to access processed data from the core controller to the cloud. the processed data can be monitored through a browser application using a special ip address. furthermore, with the help of an iot environment, we can provide a facility to access data remotely from all over the world.\nobjectives\nthe objectives that we aim to achieve are:\n\u2022 how can we provide real-time water quality monitoring systems at block /\nbuilding complex/household?\n\u2022 how can we allow the administration to quickly and efficiently collect data about water quality in the complete city?\n\u2022 how can we optimize the process of water treatment/ sewage treatment by checking the water quality in real-time?\n\u2022 how can we prevent and hazards or crises due to the mixing of harmful elements in the water supply in real-time, which would help in blocking harmful water in the pipeline supply.\n\u2022 how can we build a self-intelligent system, which can learn from patterns seen previously and judge from the previous dataset to classify water safe or unsafe based on standard learned parameters?\n\u2022 how can the system be trained to predict any signs of water crisis based on machine learning algorithms?\ninnovation\nthe solution can be divided into two phases:-\n1) phase-1, measurement and analysis-\nthe first step will be converting lab type instruments in rugged real time application oriented sensors and designing and calibrating them to measure water quality accurately and cost effectively. the measurement of various parameters as discussed above are needed to be done at various places like at source of water (river, lakes or ponds), filtration plant, pipelines, or at water tank of household-level or building level. as there is no power supply present near the household water tank a low-cost 10w solar panel can be used to power the microcontroller and sensors. this will make the operating cost of the measurement system very low. then the outputs from various sensors will be sent to an internet cloud service via a wifi module connected to the microcontroller to which all sensors are connected.\naccording to the real-time data produced the quality of water will be rated on the basic water quality index and home users can see the quality of water which they are drinking in real-time on their lan(local area network) or on the internet(cloud website).\nif this kind of system is implemented at most of household water tanks or building complexes then the state water management board or committee can see the water quality data of every household online and they can also derive analysis from the previously recorded data.\nso the data is being sent to the cloud website in real-time, now the important question comes that how can we judiciously use this stream of data pumping in?\nthe answer lies in the concepts of machine learning and pattern recognition, we can actually analyze this data and create a training set for our classifier system . this classifier system would basically decide on the quality of water based on the w.h.o standards of certain parameters like turbidity, conductance, ph value etc. now the trained system would be using some clustering or classification algorithm (here we would be using knn clustering algorithm) to take a decision on the quality of water.\nthis will be a self-learning system and the accuracy would get enhanced more and more after the addition of more and more data points. in the latest, stage we can represent using the digital platform.\nrepresentation of data\nthe users can view and control the systems through mobile applications or web dashboards. the dashboard will work as a data visualization -----> tool !!!  that will display the current status of the quality of water in real-time to different users after login.\nthe dashboard will consolidate and arrange numbers, metrics, and sometimes performance scorecards on a single screen. the essential features of the dashboard will be to provide a customizable interface and the ability to pull real-time data from multiple sources.\nthe mobile application will be for end-user having less number of uses or appliances (eg. home users) and the web dashboard is for end-users having a large no of uses (eg. government water board) who have to control large water bodies.\nthe mobile application can be connected via lan and can give representation and analysis without the internet in real-time.\n2) phase-2, control, and cleaning action\nphase -1 is of no use we don't take the required corrective action for cleaning water and maintain its quality if we detect an unhealthy water supply. integrating phase 1 analysis and output into the daily operations of the water management board is also an issue. and for dealing with this an android application can be made for helping water board workers to for doing necessary operations in water treatment and effective supply.\nif there is bad water present inside any household tank then the measurement system will generate a request to the internet cloud application automatically and the cloud will send the notification to the area manager's mobile app and the necessary action would be taken. from the analysis of phase 1\nwe can also determine the points from where the water gets polluted in the system during supply or other actions using machine learning. if we also measure the consumption of water in every household from the same device we can also derive the data of total consumption of water from the user and its wastage. we can also do water metering if we want to from the same device only. at the time of the water crisis, we can also regulate the supply for better utilization of the resource.40% of water is wasted at the time of transportation of it. we can find loopholes in the systems and work on them by the analysis provided by the data and decrease the non-revenue water amount from 40% to 5% using this technology. so this idea is a pill for most of the problems in water management.\nsteps of development:\n1) application-oriented sensor designing.\n2) sensors calibration.\n3) microcontroller interfacing. 4) connecting microcontroller to the internet.\n5) solar power system designing. 6) complete product casing and cad simulation.\n7) cloud application formation / gsm-based data communication. 8) algorithm(knn) to train a classifier system 9)prediction of water crisis based on visible patterns 10) real-time analysis and processing of data collected from the cloud web platform 11)user interface for both individual and company based usage. 12) mobile application formation.\n13) user integration and management. *14) calculation of water utilized per day for the whole city.\n*15) water metering (can be done from the same device made in phase 1).\nsteps that are not necessary but can be implemented\nkey benefits are as follows:\n1) complete measurement of water quality from household to city level at very low cost.\n2) real-time monitoring of water quality and quality.\n3) efficient water filtration plant operations and sewage treatment plant operations due to closed-loop control. 4) energy optimization and eco-friendly as solar power is used.\n5) non-revenue water can be decreased to 5% from 40%. 6) we can also add the functionality of adding a water billing system to this system.\nfuture scope:\nto give information to whole users who depend on that plant 2. detecting more parameters for the most secure purposes 3. increase the parameters by addition of multiple sensors\nby interfacing relay we control the supply of water\nadvantages :\ndue to automation it will reduce the time to check the parameters. 2. this is economically affordable for common people. 3. low maintenance. 4. prevention of water diseases.\nconclusion\nmonitoring of turbidity, ph & temperature of water makes use of water detection sensor with unique advantage and existing gsm network. the system can monitor water quality automatically, and it is low in cost and does not require people on duty. so the water quality testing is likely to be more economical, convenient and fast. the system has good flexibility. only by replacing the corresponding sensors and changing the relevant software programs, this system can be used to monitor other water quality parameters. the operation is simple. the system can be expanded to monitor hydrologic, air pollution, industrial and agricultural production and so on. it has widespread application and extension value.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59506856}, {"Unnamed: 0": 6870, "autor": "Control Alt Delete", "date": null, "content": "Inspiration\nLike many nonprofits, Control Alt Delete struggles to illustrate how a donation supports their mission of helping women to escape domestic violence. The anonymity that Control Alt Delete maintains is necessary, but hurts their ability to collect grant funding which shifts their focus to fundraising to support their cause. If they are able to make donations more tangible to donors through their website, social media, and even augmented reality, the hope is that they can increase the people they can support through increased donations.\nThe organization's donation mechanism does not deter people from donating as much as the challenge of raising visibility about this issue and encouraging people to donate to help address the issue.\nThe issue is:\n99% of all clients are financially abused. We help with one time assistance with rent/utility/move in payments allowing the women we help to remain housed during this transition to independence.\nThese Survivors have needed financial and logistical help as they take control of their finances and futures.\nThis is why we focused on considering gamification as a way to introduce people to an issue that people shun away from because domestic violence predominately attracts negative news. Introducing people to the issue doesn't necessarily lead to a donation unless there is something \"tangible\" like a feel-good story that they leave with.\nSo how can Control Alt Delete stand out from the crowd of other organiztaions with similar missions?\nSometimes, novelty works. Since few nonprofit organizations use visual novels as a marketing tool, we went this route. Not only is it not used often in the nonprofit world, it also:\nattracts a built-in audience, with people of all ages who like interactive stories and gaming\nlets people experience the journey of some of the survivors Control Alt Delete has helped to create an emotional connection with players, who may become potential donors\ncan be built on an open source engine that is free for commercial use, so the game can be sold as well\nWhat it does\nThe game Control Alt Delete is named after a nonprofit organization called Control Alt Delete, which helps people escape domestic violence. This social impact game is designed to raise social awareness and inspire people to donate to the organization through interactive storytelling by putting the player in the shoes of someone trying to reset their life.\nThis web-based game raises awareness about:\nthe cost of leaving an abusive relationship\nbeing prepared to leave\nsafety\nonline privacy concerns\nHow we built it\nWe built this on Ren'Py, which is \"a visual novel engine \u2013 used by thousands of creators from around the world \u2013 that helps you use words, images, and sounds to tell interactive stories that run on computers and mobile devices.\"\nRen'Py is open source and free for commercial use.\nChallenges we ran into\nDesigning a cohesive story where one event affects another was challenging. There was a lot of back and forth, in collaboration with our nonprofit partner as well, and only so much we can build with time and knowledge. It was the first time working with Ren'Py for all of us.\nAccomplishments that we're proud of\nWe created a full-fledged story and game! It can be published on itch.io or other sites today.\nWhat we learned\nWe learned a lot about what goes into game design, from the game engine we choose to develop on to how we present the story to how we might educate players during the game to how to add elements of surprise and randomization so that every game is going to be different. There will never be a rote set of instructions for how to win - because even with all that you have going for you, there's always a chance that you will lose. That's just life - and that's what we're also teaching.\nWhat's next for Control Alt Delete\nWe look forward to building on this story more with our nonprofit partner Control Alt Delete.", "link": "https://devpost.com/software/control-alt-delete", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nlike many nonprofits, control alt delete struggles to illustrate how a donation supports their mission of helping women to escape domestic violence. the anonymity that control alt delete maintains is necessary, but hurts their ability to collect grant funding which shifts their focus to fundraising to support their cause. if they are able to make donations more tangible to donors through their website, social media, and even augmented reality, the hope is that they can increase the people they can support through increased donations.\nthe organization's donation mechanism does not deter people from donating as much as the challenge of raising visibility about this issue and encouraging people to donate to help address the issue.\nthe issue is:\n99% of all clients are financially abused. we help with one time assistance with rent/utility/move in payments allowing the women we help to remain housed during this transition to independence.\nthese survivors have needed financial and logistical help as they take control of their finances and futures.\nthis is why we focused on considering gamification as a way to introduce people to an issue that people shun away from because domestic violence predominately attracts negative news. introducing people to the issue doesn't necessarily lead to a donation unless there is something \"tangible\" like a feel-good story that they leave with.\nso how can control alt delete stand out from the crowd of other organiztaions with similar missions?\nsometimes, novelty works. since few nonprofit organizations use visual novels as a marketing -----> tool !!! , we went this route. not only is it not used often in the nonprofit world, it also:\nattracts a built-in audience, with people of all ages who like interactive stories and gaming\nlets people experience the journey of some of the survivors control alt delete has helped to create an emotional connection with players, who may become potential donors\ncan be built on an open source engine that is free for commercial use, so the game can be sold as well\nwhat it does\nthe game control alt delete is named after a nonprofit organization called control alt delete, which helps people escape domestic violence. this social impact game is designed to raise social awareness and inspire people to donate to the organization through interactive storytelling by putting the player in the shoes of someone trying to reset their life.\nthis web-based game raises awareness about:\nthe cost of leaving an abusive relationship\nbeing prepared to leave\nsafety\nonline privacy concerns\nhow we built it\nwe built this on ren'py, which is \"a visual novel engine \u2013 used by thousands of creators from around the world \u2013 that helps you use words, images, and sounds to tell interactive stories that run on computers and mobile devices.\"\nren'py is open source and free for commercial use.\nchallenges we ran into\ndesigning a cohesive story where one event affects another was challenging. there was a lot of back and forth, in collaboration with our nonprofit partner as well, and only so much we can build with time and knowledge. it was the first time working with ren'py for all of us.\naccomplishments that we're proud of\nwe created a full-fledged story and game! it can be published on itch.io or other sites today.\nwhat we learned\nwe learned a lot about what goes into game design, from the game engine we choose to develop on to how we present the story to how we might educate players during the game to how to add elements of surprise and randomization so that every game is going to be different. there will never be a rote set of instructions for how to win - because even with all that you have going for you, there's always a chance that you will lose. that's just life - and that's what we're also teaching.\nwhat's next for control alt delete\nwe look forward to building on this story more with our nonprofit partner control alt delete.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59506870}, {"Unnamed: 0": 6878, "autor": "smart retain by dataStacker", "date": null, "content": "Origin Story\nSandip and Chris have spent their careers working for various e-commerce businesses of all sizes and across multiple industries. In our roles, we\u2019ve worked on all aspects of engineering, analytics, and data science. We\u2019ve noticed that it\u2019s hard to do analytics right. Most business analytics tools give their users a deluge of information, charts, and dashboards and then leave it up to the user to glean actionable insights from this information. It takes time, energy and precious headcount for the business owner to take all of this data and know what specific actions they should take. If large companies with teams of highly paid analysts struggle with this, what chance do small businesses have that don\u2019t have the resources to dedicate to a dedicated analytics team? Frustrated by the status quo and having serendipitously come across the Square hackathon, we set out to build dataStacker\nWhat dataStacker does\nWe set out to build dataStacker with the aim of making it :\npowerful for small businesses to get straightforward insights and clear actions from their customer data.\nSmall businesses are facing increasing headwinds, dealing with economic instability, severe challenges due to the pandemic, and an increasingly complex competitive landscape. Given all of this, they don\u2019t have the resources to hire on expensive outside analytics expertise, or spend hours upon hours developing the level of knowledge needed to glean the appropriate actions from an analytics tool. Our goal was to hide the complexity of sophisticated machine learning techniques on the backend, by only surfacing tailored clear-cut actionable recommendations that highlight important trends to power customer retention, loyalty and enabling targeted upselling.\nWe built smart retain by dataStacker to help small businesses understand their customer base. The focus is on understanding the value that each organic customer segment has to the business, and helping them figure out what business actions to take. By leveraging machine learning technology we find complex patterns in our merchant\u2019s sales and customer behavior data to create an action plan for each customer segment. We surface insights such as which customers are most likely to be flight risks, which are the most loyal customers, and identifying customers for upselling opportunities.\nHow we built it\nWe power dataStacker by leveraging the square POS API\u2019s, using the orders, invoices, customers products and loyalty endpoints. In a secure fashion built with cloud security best practices, each small business merchant has multiple custom machine learning models built in the background. These machine learning models are used to discover patterns in the sales and customer behavior data, and then power the actionable business recommendations that dataStacker presents to our users.\nChallenges and accomplishments\nDespite our combined decades of technology experience, this was a challenging project to pull together, especially in such a short period of time. Learning the data points available, nuances of the Square API, and designing a data ingestion process had to be done quickly. In parallel, the development of machine learning models that would find the types of insights we wanted to leverage was challenging, especially without live customer data to work with. This necessitated the development of multiple realistic, synthetic datasets that would challenge our models in different ways. In addition, we also had to develop the technology that could translate complex numerical insights and trends into human readable format, conveying what our models find as interesting into easily understood text. In short, this was a highly complex project leveraging our full technical skillset. In the end, it is rewarding to see a working product that has the ability to change the way millions of potential small businesses operate. We look forward to building on this early iteration and building an even more powerful product.\nWhat's next for smart retain by dataStacker\nOur goal is to provide the same set of powerful tools that Macy's or Gap would use to understand their customers in the hands of small to medium Square e-commerce business owners. We believe that smart retain would make a great and valuable fit into the Square app marketplace ecosystem and look forward to making it a part of the marketplace.", "link": "https://devpost.com/software/smart-retain", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "origin story\nsandip and chris have spent their careers working for various e-commerce businesses of all sizes and across multiple industries. in our roles, we\u2019ve worked on all aspects of engineering, analytics, and data science. we\u2019ve noticed that it\u2019s hard to do analytics right. most business analytics tools give their users a deluge of information, charts, and dashboards and then leave it up to the user to glean actionable insights from this information. it takes time, energy and precious headcount for the business owner to take all of this data and know what specific actions they should take. if large companies with teams of highly paid analysts struggle with this, what chance do small businesses have that don\u2019t have the resources to dedicate to a dedicated analytics team? frustrated by the status quo and having serendipitously come across the square hackathon, we set out to build datastacker\nwhat datastacker does\nwe set out to build datastacker with the aim of making it :\npowerful for small businesses to get straightforward insights and clear actions from their customer data.\nsmall businesses are facing increasing headwinds, dealing with economic instability, severe challenges due to the pandemic, and an increasingly complex competitive landscape. given all of this, they don\u2019t have the resources to hire on expensive outside analytics expertise, or spend hours upon hours developing the level of knowledge needed to glean the appropriate actions from an analytics -----> tool !!! . our goal was to hide the complexity of sophisticated machine learning techniques on the backend, by only surfacing tailored clear-cut actionable recommendations that highlight important trends to power customer retention, loyalty and enabling targeted upselling.\nwe built smart retain by datastacker to help small businesses understand their customer base. the focus is on understanding the value that each organic customer segment has to the business, and helping them figure out what business actions to take. by leveraging machine learning technology we find complex patterns in our merchant\u2019s sales and customer behavior data to create an action plan for each customer segment. we surface insights such as which customers are most likely to be flight risks, which are the most loyal customers, and identifying customers for upselling opportunities.\nhow we built it\nwe power datastacker by leveraging the square pos api\u2019s, using the orders, invoices, customers products and loyalty endpoints. in a secure fashion built with cloud security best practices, each small business merchant has multiple custom machine learning models built in the background. these machine learning models are used to discover patterns in the sales and customer behavior data, and then power the actionable business recommendations that datastacker presents to our users.\nchallenges and accomplishments\ndespite our combined decades of technology experience, this was a challenging project to pull together, especially in such a short period of time. learning the data points available, nuances of the square api, and designing a data ingestion process had to be done quickly. in parallel, the development of machine learning models that would find the types of insights we wanted to leverage was challenging, especially without live customer data to work with. this necessitated the development of multiple realistic, synthetic datasets that would challenge our models in different ways. in addition, we also had to develop the technology that could translate complex numerical insights and trends into human readable format, conveying what our models find as interesting into easily understood text. in short, this was a highly complex project leveraging our full technical skillset. in the end, it is rewarding to see a working product that has the ability to change the way millions of potential small businesses operate. we look forward to building on this early iteration and building an even more powerful product.\nwhat's next for smart retain by datastacker\nour goal is to provide the same set of powerful tools that macy's or gap would use to understand their customers in the hands of small to medium square e-commerce business owners. we believe that smart retain would make a great and valuable fit into the square app marketplace ecosystem and look forward to making it a part of the marketplace.", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59506878}, {"Unnamed: 0": 6887, "autor": "Group 7: Viz", "date": null, "content": "Fast, easy-use, educational visualization tool", "link": "https://devpost.com/software/group-7-viz", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "fast, easy-use, educational visualization -----> tool !!! ", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59506887}, {"Unnamed: 0": 6893, "autor": "Odio", "date": null, "content": "Inspiration\n\"Not everyone can become a great artist, but a great artist can come from anywhere.\" \u2013 Anton Ego, Ratatouille.\nWe often appreciate success that comes from humble origins that are filled with struggles. But rarely do we provide a helping hand in their journey. Everybody has a dream, yet some are not gifted like the rest of us; some might not have the right means or the physical abilities to make their dreams a reality.\nMusic creation requires a lot of natural talent and a lot of hard work. Playing most instruments requires a good amount of practice and only then one can replicate the tune that's been playing in their minds. However being able to hold an instrument in our own hands and being able to play it is something we take for granted. Most specially abled folks might not have the opportunity to be able to play or even hold a music instrument due to physical impairments.\nAs part of this hackathon, we have developed an AI powered Music Studio Tool - ODIO to simplify the process of music creation. This will enable the specially abled ones to create music out of their imagination. This will help even untrained individuals to easily create music in our music studio with the help of AI.\nWhat it does\nODIO is an AI powered Music Studio Tool for Creators.\nIt can make the music in your mind come to life, through a new revolutionary process. You don't need traditional instruments, years of learning, nor a group of five people, to create music.\nYou simply have to Hum and Beat Box into your microphone and ODIO can automatically convert them to how an actual musical instrument would sound.\nIn hindsight, ODIO uses advanced Algorithms and Artificial Intelligence to automatically convert your humming to the tune of a synthetic violin and beat-boxing into actual drum beats! What's more? You can also drag and drop different beats and tunes to arrange them in the studio UI to create a complete music track ready to play!\nThe current version supports the following:\nCreating Drum Beats- Convert human beat-boxing to the corresponding sound of actual drums. We currently support 3 beats viz. bass, snare, and treble.\nCreating Synthetic Violin Tune - Convert human humming to a smooth synthetic violin tune. We are making an effort to make this sound more like a natural violin.\nA Studio UI - to arrange beats and tunes for creating a complete musical track\nHow we built it - \"The Developer Section\"\nThe Pitch detection algorithm is based on a fundamental frequency estimator called the Yin PDA with reference to this paper, with slight improvements to suit our use-case, in which we only need to consider the frequency range of the human voice.\nBeat classification was achieved by training a Deep Neural Network over our custom created dataset with the input comprising of MFCC features instead of raw audio data. Picking these features as the input to the classifier greatly improved model trainability. Apart from classification of an audio sample into a beat category, the beat module also takes care of onset detection for figuring out the start and end timestamps for extracting the audio for classification and whether a sound should be considered a beat.\nFor training the model, we used python's Keras library. Keras.js was then used to load the model and its trained parameters in the UI for in-browser predictions.\nApart from these core algorithms, the WebApp heavily leverages Web Audio API for various components like OscillatorNodes for creating sound of desired frequency, Navigator API for acquiring microphone access, etc. all of which were very critical for building the frontend.\nChallenges we ran into\nThe first and primary challenge we faced, was to get a working prototype ready in a short time frame. Given the lack of expertise towards UI/UX in the team, we tried our best to make a minimalistic UI that accomplished as much as a standard Music Studio WebApp would need. UX will play a major role as we intend to create an App which is convenient and intuitive for everyone. That deserves a project of its own.\nTechnical challenges included precisely extracting samples of the recorded audio, which were part of the same beat, as clips to be fed to the classifier. Data collection was yet another tricky problem for the Beat classifier. Although, we were successfully able to train the model to correctly classify 3 beat categories in our dataset, the model was trained on a small set of users and an environment with minimal background noise. Generating a larger dataset is yet another challenge given the time frame.\nAnother tricky problem we faced was to make the sound of the synthetic violin more natural. We believe we can get there with help from professionals, as we plan to include more instruments in future.\nAccomplishments that we're proud of\nMusic is something which every one of us holds very dear to our heart. Arriving at an idea that could help change the way music is created in the world, makes us very proud. With Odio we intend to grow the community of creators through inclusion.\nWe are also very proud that we could drive the project to its completion. We do want to see the project grow even more, and be embraced by creators as well as others who love music, as a means of personal expression.\nWhat we learned\nIt was a great learning experience, as the project covered various grounds, like implementing an advanced algorithm, training and deploying a deep neural network that yielded considerably impressive results, frontend development that involved learning new APIs core to audio manipulation in the browser, signal processing theory and FFTs that ultimately found use in the discovery of MFCC features that helped improve model trainability, and a myriad of other domain specific as well as generic technical knowledge that we gathered while building this app.\nWhat's next for Odio\nWe tried to create a working prototype of our idea within the duration of the hackathon, as a result of which we could not implement several features and improvements we had in our pipeline. We truly believe in some improvements which can take ODIO to the next level.\nSome of the prominent tasks planned are -\nMake the classifier more robust by improving the dataset to include a variety of human voices and background noise\nUse advanced music theory, for tempo detection and auto-tuning to automatically improve and correct the recorded beats or tunes.\nFeatures to merge and export final music track as an audio file, to be able to add filters and tune correction of recorded clips, and to be able to cut and trim clips recorded in ODIO.\nAdd more instruments - Piano, Guitar, Flutes, different Percussion instruments etc.\nImprovements in UI/UX of ODIO Music Studio.\nNote: Works best on Firefox Web Browser!!", "link": "https://devpost.com/software/odio", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\n\"not everyone can become a great artist, but a great artist can come from anywhere.\" \u2013 anton ego, ratatouille.\nwe often appreciate success that comes from humble origins that are filled with struggles. but rarely do we provide a helping hand in their journey. everybody has a dream, yet some are not gifted like the rest of us; some might not have the right means or the physical abilities to make their dreams a reality.\nmusic creation requires a lot of natural talent and a lot of hard work. playing most instruments requires a good amount of practice and only then one can replicate the tune that's been playing in their minds. however being able to hold an instrument in our own hands and being able to play it is something we take for granted. most specially abled folks might not have the opportunity to be able to play or even hold a music instrument due to physical impairments.\nas part of this hackathon, we have developed an ai powered music studio -----> tool !!!  - odio to simplify the process of music creation. this will enable the specially abled ones to create music out of their imagination. this will help even untrained individuals to easily create music in our music studio with the help of ai.\nwhat it does\nodio is an ai powered music studio tool for creators.\nit can make the music in your mind come to life, through a new revolutionary process. you don't need traditional instruments, years of learning, nor a group of five people, to create music.\nyou simply have to hum and beat box into your microphone and odio can automatically convert them to how an actual musical instrument would sound.\nin hindsight, odio uses advanced algorithms and artificial intelligence to automatically convert your humming to the tune of a synthetic violin and beat-boxing into actual drum beats! what's more? you can also drag and drop different beats and tunes to arrange them in the studio ui to create a complete music track ready to play!\nthe current version supports the following:\ncreating drum beats- convert human beat-boxing to the corresponding sound of actual drums. we currently support 3 beats viz. bass, snare, and treble.\ncreating synthetic violin tune - convert human humming to a smooth synthetic violin tune. we are making an effort to make this sound more like a natural violin.\na studio ui - to arrange beats and tunes for creating a complete musical track\nhow we built it - \"the developer section\"\nthe pitch detection algorithm is based on a fundamental frequency estimator called the yin pda with reference to this paper, with slight improvements to suit our use-case, in which we only need to consider the frequency range of the human voice.\nbeat classification was achieved by training a deep neural network over our custom created dataset with the input comprising of mfcc features instead of raw audio data. picking these features as the input to the classifier greatly improved model trainability. apart from classification of an audio sample into a beat category, the beat module also takes care of onset detection for figuring out the start and end timestamps for extracting the audio for classification and whether a sound should be considered a beat.\nfor training the model, we used python's keras library. keras.js was then used to load the model and its trained parameters in the ui for in-browser predictions.\napart from these core algorithms, the webapp heavily leverages web audio api for various components like oscillatornodes for creating sound of desired frequency, navigator api for acquiring microphone access, etc. all of which were very critical for building the frontend.\nchallenges we ran into\nthe first and primary challenge we faced, was to get a working prototype ready in a short time frame. given the lack of expertise towards ui/ux in the team, we tried our best to make a minimalistic ui that accomplished as much as a standard music studio webapp would need. ux will play a major role as we intend to create an app which is convenient and intuitive for everyone. that deserves a project of its own.\ntechnical challenges included precisely extracting samples of the recorded audio, which were part of the same beat, as clips to be fed to the classifier. data collection was yet another tricky problem for the beat classifier. although, we were successfully able to train the model to correctly classify 3 beat categories in our dataset, the model was trained on a small set of users and an environment with minimal background noise. generating a larger dataset is yet another challenge given the time frame.\nanother tricky problem we faced was to make the sound of the synthetic violin more natural. we believe we can get there with help from professionals, as we plan to include more instruments in future.\naccomplishments that we're proud of\nmusic is something which every one of us holds very dear to our heart. arriving at an idea that could help change the way music is created in the world, makes us very proud. with odio we intend to grow the community of creators through inclusion.\nwe are also very proud that we could drive the project to its completion. we do want to see the project grow even more, and be embraced by creators as well as others who love music, as a means of personal expression.\nwhat we learned\nit was a great learning experience, as the project covered various grounds, like implementing an advanced algorithm, training and deploying a deep neural network that yielded considerably impressive results, frontend development that involved learning new apis core to audio manipulation in the browser, signal processing theory and ffts that ultimately found use in the discovery of mfcc features that helped improve model trainability, and a myriad of other domain specific as well as generic technical knowledge that we gathered while building this app.\nwhat's next for odio\nwe tried to create a working prototype of our idea within the duration of the hackathon, as a result of which we could not implement several features and improvements we had in our pipeline. we truly believe in some improvements which can take odio to the next level.\nsome of the prominent tasks planned are -\nmake the classifier more robust by improving the dataset to include a variety of human voices and background noise\nuse advanced music theory, for tempo detection and auto-tuning to automatically improve and correct the recorded beats or tunes.\nfeatures to merge and export final music track as an audio file, to be able to add filters and tune correction of recorded clips, and to be able to cut and trim clips recorded in odio.\nadd more instruments - piano, guitar, flutes, different percussion instruments etc.\nimprovements in ui/ux of odio music studio.\nnote: works best on firefox web browser!!", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59506893}, {"Unnamed: 0": 6897, "autor": "Migrating to Power BI can be beneficial to your organization", "date": null, "content": "Business Intelligence (BI) reporting refers to the process of receiving or providing information in the form of reports to end-users, organizations, or applications through a BI solution. These reports can be in the form of statistical data, visual charts, and standard textual content.\nBI reporting tools enable businesses to visualize and present data that can be easily accessed and analyzed. The BI market is overcrowded, with most reporting tools offering standard features/similar functionalities like security, production, distribution, deployment, and support. However, while choosing the right BI reporting tool, the differentiating factors include ease of usability, report quality, scalability, and performance.\nBusinesses are quickly beginning to realize the advantages that visualizations have over standard representations of data. Understanding data visualization can radically improve business intelligence, streamline work processes, and, most importantly, help make data-driven business decisions faster. Several data visualization tools are available today, which offer a range of visualizations that cater to different business needs. Some even allow users to customize the look & feel and data representation. One of the leading innovative tools available for data analysis and visualization is Power BI.\nPower BI has become one of the most viable investment options for businesses looking to convert their data into opportunities. Power BI makes visual dashboard creation simple and intuitive with visualization options, also offers the flexibility to enhance its out-of-the-box visualization capabilities.\nCognos\nCognos is a leading enterprise reporting solution that enables users to create reports, analyze their data, and gather valuable business insights. IBM acquired Cognos in 2008.\nBenefits\nCognos architecture is structured and calibrated for optimal performance. This allows enterprises to scale the app to support their growing user base.\nWith Integration of IBM\u2019s SPSS\u00ae, it offers predictive and statistical capabilities to end-users.\nCognos uses an open architecture that leverages XML, SOAP, and WSDL.\nMultilingual capabilities automatically deliver reports in the users\u2019 native language.\nLimitations\nThe initial setup isn't as simple as running the installer and activating it. Adjustments to the server setup may be necessary before it is up and running.\nBrowser support can be tricky. Compatibility is best with Internet Explorer, but speci\ufb01c browser setting changes can cause issues. Other browsers' compatibility can be questionable due to their signi\ufb01cantly more frequent updates.\nIt works best with actual databases and not so well with CSV or other \ufb01le formats. An ODBC or JDBC driver can be used to treat \ufb01les like databases, but native support is somewhat limited.\nUsing external ODBC drivers can be done with 32-bit drivers only.\nHaving a sandbox environment on a developer's machine isn't feasible due to how Cognos installs on a system.\nWhy migrate from Cognos to Power BI Paginated Reports?\nDue to ever-changing business needs and obsolete technologies, organizations are compelled to move from one reporting format to another. There are multiple reasons why enterprises may want to move from Cognos to Power BI Paginated reports. Each suite has its strengths and weaknesses. On balance, there is probably not enough of a rationale on either side to migrate from one to the other, unless addressing specific business needs.\nCheck out this whitepaper to understand some of the reasons businesses may choose to migrate Cognos reports and learn Cognos & Power BI, which gives better ROI?\nConclusion\nThe key differentiating factor between Power BI and Cognos is the cost and availability of trained resources. Licensing for Microsoft is determined on the server instance model, also a user-based model offering flexibility, whereas Cognos licensing is dependent on the number of users. Although the Microsoft BI tool stack offers a quality BI solution, you cannot undermine the enterprise capabilities of IBM Cognos.", "link": "https://devpost.com/software/migrating-to-power-bi-can-be-beneficial-to-your-organization", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "business intelligence (bi) reporting refers to the process of receiving or providing information in the form of reports to end-users, organizations, or applications through a bi solution. these reports can be in the form of statistical data, visual charts, and standard textual content.\nbi reporting tools enable businesses to visualize and present data that can be easily accessed and analyzed. the bi market is overcrowded, with most reporting tools offering standard features/similar functionalities like security, production, distribution, deployment, and support. however, while choosing the right bi reporting -----> tool !!! , the differentiating factors include ease of usability, report quality, scalability, and performance.\nbusinesses are quickly beginning to realize the advantages that visualizations have over standard representations of data. understanding data visualization can radically improve business intelligence, streamline work processes, and, most importantly, help make data-driven business decisions faster. several data visualization tools are available today, which offer a range of visualizations that cater to different business needs. some even allow users to customize the look & feel and data representation. one of the leading innovative tools available for data analysis and visualization is power bi.\npower bi has become one of the most viable investment options for businesses looking to convert their data into opportunities. power bi makes visual dashboard creation simple and intuitive with visualization options, also offers the flexibility to enhance its out-of-the-box visualization capabilities.\ncognos\ncognos is a leading enterprise reporting solution that enables users to create reports, analyze their data, and gather valuable business insights. ibm acquired cognos in 2008.\nbenefits\ncognos architecture is structured and calibrated for optimal performance. this allows enterprises to scale the app to support their growing user base.\nwith integration of ibm\u2019s spss\u00ae, it offers predictive and statistical capabilities to end-users.\ncognos uses an open architecture that leverages xml, soap, and wsdl.\nmultilingual capabilities automatically deliver reports in the users\u2019 native language.\nlimitations\nthe initial setup isn't as simple as running the installer and activating it. adjustments to the server setup may be necessary before it is up and running.\nbrowser support can be tricky. compatibility is best with internet explorer, but speci\ufb01c browser setting changes can cause issues. other browsers' compatibility can be questionable due to their signi\ufb01cantly more frequent updates.\nit works best with actual databases and not so well with csv or other \ufb01le formats. an odbc or jdbc driver can be used to treat \ufb01les like databases, but native support is somewhat limited.\nusing external odbc drivers can be done with 32-bit drivers only.\nhaving a sandbox environment on a developer's machine isn't feasible due to how cognos installs on a system.\nwhy migrate from cognos to power bi paginated reports?\ndue to ever-changing business needs and obsolete technologies, organizations are compelled to move from one reporting format to another. there are multiple reasons why enterprises may want to move from cognos to power bi paginated reports. each suite has its strengths and weaknesses. on balance, there is probably not enough of a rationale on either side to migrate from one to the other, unless addressing specific business needs.\ncheck out this whitepaper to understand some of the reasons businesses may choose to migrate cognos reports and learn cognos & power bi, which gives better roi?\nconclusion\nthe key differentiating factor between power bi and cognos is the cost and availability of trained resources. licensing for microsoft is determined on the server instance model, also a user-based model offering flexibility, whereas cognos licensing is dependent on the number of users. although the microsoft bi tool stack offers a quality bi solution, you cannot undermine the enterprise capabilities of ibm cognos.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59506897}, {"Unnamed: 0": 6924, "autor": "Free Relaxing Music", "date": null, "content": "Free Relaxing music is a powerful tool for relaxation and meditation. Most of us have been using it since we were children, but this simple technique has become a very sophisticated business in the last few years.\nHere's a blog about the best free relaxing music and royalty free relaxing music that you can use in your videos or projects.\nAudio copyright is the practice of protecting the rights of an artist or their heirs to control what people can and cannot do with their music.\nThe copyright holder has the legal power to license recordings, use them for commercial purposes, or allow somebody else to do so. Copyright holders are typically protected by law from unauthorized copying or derivative works.\nIt might be annoying when you're searching for amazing free royalty free music or sound effects free no copyright music to go with your video, but the same regulations that prevent you from using the current top singles as a video's background also safeguard the work you create yourself.\nYou may not be a big artist, but you have the same level of control over your work, along with the same rules.\nTips to select the Best Background Music for Videos\nChoose the type of role music for your video\nSome background music for videos are intended to deliver a sub-perceptual boost. Other music invigorates the audience, while other music compliments the action on screen and sets the tone.\nLearn which genres make the best background music for your videos\nSometimes choosing one single from a seemingly endless list of options seems daunting, then limit your choices to one specific genre. Here are some of the genres that several business videos are known to elicit.\nYou can use Music Intros and Outros as Bookends\nA bookend is considered as a 3-5 sec clip of music, mostly followed by animation or text, which signals the beginning or end of a video. However, if your movie is particularly extensive, it can be used to signify the start and end of each chapter.\nMake Use Of Reference Music\nMost of the time, we already have a song in mind that we can use as a guide. You can listen to it on YouTube and then search for songs with a similar rhythm, pace, progression, instrumentation, or key.", "link": "https://devpost.com/software/free-relaxing-music", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "free relaxing music is a powerful -----> tool !!!  for relaxation and meditation. most of us have been using it since we were children, but this simple technique has become a very sophisticated business in the last few years.\nhere's a blog about the best free relaxing music and royalty free relaxing music that you can use in your videos or projects.\naudio copyright is the practice of protecting the rights of an artist or their heirs to control what people can and cannot do with their music.\nthe copyright holder has the legal power to license recordings, use them for commercial purposes, or allow somebody else to do so. copyright holders are typically protected by law from unauthorized copying or derivative works.\nit might be annoying when you're searching for amazing free royalty free music or sound effects free no copyright music to go with your video, but the same regulations that prevent you from using the current top singles as a video's background also safeguard the work you create yourself.\nyou may not be a big artist, but you have the same level of control over your work, along with the same rules.\ntips to select the best background music for videos\nchoose the type of role music for your video\nsome background music for videos are intended to deliver a sub-perceptual boost. other music invigorates the audience, while other music compliments the action on screen and sets the tone.\nlearn which genres make the best background music for your videos\nsometimes choosing one single from a seemingly endless list of options seems daunting, then limit your choices to one specific genre. here are some of the genres that several business videos are known to elicit.\nyou can use music intros and outros as bookends\na bookend is considered as a 3-5 sec clip of music, mostly followed by animation or text, which signals the beginning or end of a video. however, if your movie is particularly extensive, it can be used to signify the start and end of each chapter.\nmake use of reference music\nmost of the time, we already have a song in mind that we can use as a guide. you can listen to it on youtube and then search for songs with a similar rhythm, pace, progression, instrumentation, or key.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59506924}, {"Unnamed: 0": 6942, "autor": "SpeekUp!", "date": null, "content": "Inspiration\nThe World is changing! And in a constantly changing world, being able to adapt easily and find new solutions is challenging for everyone. This is why we are here to SpeekUp!\nWhat it does\nSpeekUp! is a digital fast access tool created for immediate interjection in digital conversations. Easy to use and always at hand, it becomes the voice of thousands of people with speech impediments who want to actively participate and contribute further in today's business, education, and society.\nHow we built it\nDesigned as a user-friendly interface, that can be accessed from both Desktop and Mobile, SpeekUp! allows you to use already preset buttons in a user-friendly drag-and-drop configuration.\nChallenges we ran into\nThe time constraint came out as the biggest challenge we had to face.\nAccomplishments that we're proud of\nWe succeeded in building a proof of concept containing words and expressions that are mostly used to catch attention when typing is not noticed or more time is needed for typing in other tools.\nWhat we learned\nThis can be your voice! Speekup!\nWhat's next for SpeekUp!\nAn expanded version would allow for voice selection, localization, and button content customization.", "link": "https://devpost.com/software/wip-1kvw4z", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nthe world is changing! and in a constantly changing world, being able to adapt easily and find new solutions is challenging for everyone. this is why we are here to speekup!\nwhat it does\nspeekup! is a digital fast access -----> tool !!!  created for immediate interjection in digital conversations. easy to use and always at hand, it becomes the voice of thousands of people with speech impediments who want to actively participate and contribute further in today's business, education, and society.\nhow we built it\ndesigned as a user-friendly interface, that can be accessed from both desktop and mobile, speekup! allows you to use already preset buttons in a user-friendly drag-and-drop configuration.\nchallenges we ran into\nthe time constraint came out as the biggest challenge we had to face.\naccomplishments that we're proud of\nwe succeeded in building a proof of concept containing words and expressions that are mostly used to catch attention when typing is not noticed or more time is needed for typing in other tools.\nwhat we learned\nthis can be your voice! speekup!\nwhat's next for speekup!\nan expanded version would allow for voice selection, localization, and button content customization.", "sortedWord": "None", "removed": "Nan", "score": 24, "comments": 0, "media": null, "medialink": null, "identifyer": 59506942}, {"Unnamed: 0": 6962, "autor": "Infibridge", "date": null, "content": "Inspiration\nCelo is a fantastic mission driven chain. We believe that it needs a dedicated and easy to use browser extension wallet like Metamask to help users and developers to work with and build on Celo. Presently, interacting with Celo for activities like governance voting or staking requires the use of celo-cli command line tool OR desktop wallets. This takes away from the experience of a wallet like Metamask that is one click away in the browser.\nWhat it does\nInfibridge is an easy to use browser extension wallet designed specifically for Celo Blockchain. It will help Celo DApp developers and users to:\nCreate and manage Celo specific accounts without downloading multiple tools\nInteract with Celo smart contracts, using Celo native assets\nTrack Celo native asset balances (Celo, locked Celo, cUSD and cEUR)\nTransact with Celo native assets, with gas fee payment in either Celo, cUSD or cEUR\nBuy, lock and swap Celo, cUSD and cEUR\nVote for governance proposals\nVote for validator groups\nHow we built it\nWe built it by participating in Celo Camp (Batch 3), talking to users about their pain points and understanding how users interact with DeFi projects.\nChallenges we ran into\nUsing contractKit was a major challenge. We had to write a large portion of the code to interact with Celo smart contracts from the ground up.\nAccomplishments that we're proud of\nBuilding a wallet that is native and completely centered around Celo\nWhat we learned\nCelo is a really fun project to work on. It is pretty simple, easy, and fast.\nWhat's next for Infibridge\nWe are looking forward to integrate the wallet with all the DeFi projects on Celo. Up next is ledger integration and an easier to use staking user experience.", "link": "https://devpost.com/software/infibridge", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ncelo is a fantastic mission driven chain. we believe that it needs a dedicated and easy to use browser extension wallet like metamask to help users and developers to work with and build on celo. presently, interacting with celo for activities like governance voting or staking requires the use of celo-cli command line -----> tool !!!  or desktop wallets. this takes away from the experience of a wallet like metamask that is one click away in the browser.\nwhat it does\ninfibridge is an easy to use browser extension wallet designed specifically for celo blockchain. it will help celo dapp developers and users to:\ncreate and manage celo specific accounts without downloading multiple tools\ninteract with celo smart contracts, using celo native assets\ntrack celo native asset balances (celo, locked celo, cusd and ceur)\ntransact with celo native assets, with gas fee payment in either celo, cusd or ceur\nbuy, lock and swap celo, cusd and ceur\nvote for governance proposals\nvote for validator groups\nhow we built it\nwe built it by participating in celo camp (batch 3), talking to users about their pain points and understanding how users interact with defi projects.\nchallenges we ran into\nusing contractkit was a major challenge. we had to write a large portion of the code to interact with celo smart contracts from the ground up.\naccomplishments that we're proud of\nbuilding a wallet that is native and completely centered around celo\nwhat we learned\ncelo is a really fun project to work on. it is pretty simple, easy, and fast.\nwhat's next for infibridge\nwe are looking forward to integrate the wallet with all the defi projects on celo. up next is ledger integration and an easier to use staking user experience.", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59506962}, {"Unnamed: 0": 6969, "autor": "Remote Bus Tracking", "date": null, "content": "For What\nBus tracking system (BTS) is the technology used to determine the exact location of any bus using different methods like GPS and the other radio navigation systems operation through satellites and ground base stations. The device is used to determine the precise location of a bus, and intimate the position to the concerned authority. The system is an important tool for tracking each vehicle at a given time period of time and now it is becoming popular day by day.\nThe main goal is to find the location of the bus so that the users do not get delayed and got the real time location.", "link": "https://devpost.com/software/remote-bus-tracking", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "for what\nbus tracking system (bts) is the technology used to determine the exact location of any bus using different methods like gps and the other radio navigation systems operation through satellites and ground base stations. the device is used to determine the precise location of a bus, and intimate the position to the concerned authority. the system is an important -----> tool !!!  for tracking each vehicle at a given time period of time and now it is becoming popular day by day.\nthe main goal is to find the location of the bus so that the users do not get delayed and got the real time location.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59506969}, {"Unnamed: 0": 6970, "autor": "Amazing Kitchen", "date": null, "content": "Inspiration\nDoing configuration need an extra time and effort. You can use chef to configure the Operating System with cookbooks. Is possible to automate another level ? The answer of the question yes.\nWhat it does\nwe created a platform to do configuration with few clicks. It's called Amazing Kitchen. Amazing Kitchen will do the configuration in any Operating System from Graphical User Interface\nHow to Configure\nBrowse to Amazing Kitchen\nNavigate to Configuration page\nClick on the configure button at the end of the page\nProvide the required details\nAmazing Kitchen will configure your Operating System\nHow we built it\nTeam Formation\nThe primary step of this project is the same as any other project we formed as a team and we called ourselves Clever Cooks. Team members are from all over India. we are connected over LinkedIn and connected with referrals of friends. The team is a combo of multiple skilled people. So, we divided work\nUser Interface and Experience\nSowmya worked on website development. She created code for the website and the website is informative and has multiple details about Amazing Kitchen.\nBackend\nSridhar worked as a Full Stack Developer. Setup chef server, Written cookbooks for the project. Integrated frontend and backend. Made\nDevOps\nSiva Naik worked on the creation of the pipeline with Frontend and Backend and automated from Development to Production with DevOps tools: Docker, Kubernetes, Git Hub Actions, ArgoCD, Terraform, and Cloud Platforms: Microsoft Azure for AKS\nPipeline\nThe Architecture team/DevOps team has already pushed the GitHub Actions' code in .git/workflows/ directory in both the repositories. For every event of code change, the GitHub Actions will trigger and build a Docker Image and save the image in Docker Hub. The Docker hub credentials are passed from GitHub repository secrets\nThe repositories also have the Helm charts uploaded by the DevOps team in the manifests directory to deploy the application with ArgoCD running on AKS\nThe AKS Cluster is already provisioned and the ArgoCD Helm chart is deployed on top of AKS with Terraform, Infrastructure as a Code tool\nThe DevOps team is doing operations from the Azure cloud shell. They created two ArgoCD projects with declarative manifests the projects will monitor those two repositories for code change and deploy on top AKS when they sync the repositories\nChallenges we ran into\nWe faced a ton of challenges throughout the project :\nIntegration of multiple DevOps tools in the complete pipeline\nFinding the correct hyperparameters when training the Model\nMaking the website responsive\nTo secure the credentials for the client server to do configuration.\nAccomplishments that we're proud of\nIntegrated 10+ tools to automate everything from development to production\nThe Website is designed in such a way it is very user-friendly and simple to navigate\nThe predictions are almost correct.\nWe implemented the project in 3 days before the deadline of the Hackathon\nWhat we learned\nWe understood the power of integrating multiple tools and technologies and solving real-world problems using them. Also as we worked in a team we learned how to work in a team.\nWhat's next for Amazing Kitchen\nAdd some security features to save the pem file with us\nMake it production ready to deliver to customers\nReferences & Further Readings\nChef\nAzure\nAWS\nFlask\nHelm\nGitHub Actions\nDocker\nDockerfile\nKubernetes\nArgoCD\nTerraform", "link": "https://devpost.com/software/automate-configuration", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ndoing configuration need an extra time and effort. you can use chef to configure the operating system with cookbooks. is possible to automate another level ? the answer of the question yes.\nwhat it does\nwe created a platform to do configuration with few clicks. it's called amazing kitchen. amazing kitchen will do the configuration in any operating system from graphical user interface\nhow to configure\nbrowse to amazing kitchen\nnavigate to configuration page\nclick on the configure button at the end of the page\nprovide the required details\namazing kitchen will configure your operating system\nhow we built it\nteam formation\nthe primary step of this project is the same as any other project we formed as a team and we called ourselves clever cooks. team members are from all over india. we are connected over linkedin and connected with referrals of friends. the team is a combo of multiple skilled people. so, we divided work\nuser interface and experience\nsowmya worked on website development. she created code for the website and the website is informative and has multiple details about amazing kitchen.\nbackend\nsridhar worked as a full stack developer. setup chef server, written cookbooks for the project. integrated frontend and backend. made\ndevops\nsiva naik worked on the creation of the pipeline with frontend and backend and automated from development to production with devops tools: docker, kubernetes, git hub actions, argocd, terraform, and cloud platforms: microsoft azure for aks\npipeline\nthe architecture team/devops team has already pushed the github actions' code in .git/workflows/ directory in both the repositories. for every event of code change, the github actions will trigger and build a docker image and save the image in docker hub. the docker hub credentials are passed from github repository secrets\nthe repositories also have the helm charts uploaded by the devops team in the manifests directory to deploy the application with argocd running on aks\nthe aks cluster is already provisioned and the argocd helm chart is deployed on top of aks with terraform, infrastructure as a code -----> tool !!! \nthe devops team is doing operations from the azure cloud shell. they created two argocd projects with declarative manifests the projects will monitor those two repositories for code change and deploy on top aks when they sync the repositories\nchallenges we ran into\nwe faced a ton of challenges throughout the project :\nintegration of multiple devops tools in the complete pipeline\nfinding the correct hyperparameters when training the model\nmaking the website responsive\nto secure the credentials for the client server to do configuration.\naccomplishments that we're proud of\nintegrated 10+ tools to automate everything from development to production\nthe website is designed in such a way it is very user-friendly and simple to navigate\nthe predictions are almost correct.\nwe implemented the project in 3 days before the deadline of the hackathon\nwhat we learned\nwe understood the power of integrating multiple tools and technologies and solving real-world problems using them. also as we worked in a team we learned how to work in a team.\nwhat's next for amazing kitchen\nadd some security features to save the pem file with us\nmake it production ready to deliver to customers\nreferences & further readings\nchef\nazure\naws\nflask\nhelm\ngithub actions\ndocker\ndockerfile\nkubernetes\nargocd\nterraform", "sortedWord": "None", "removed": "Nan", "score": 6, "comments": 0, "media": null, "medialink": null, "identifyer": 59506970}, {"Unnamed: 0": 6974, "autor": "Team 19: MammoDL", "date": null, "content": "Abstract:\nDiagnosing breast cancer remains a subjective process, in which radiologists visually assess mammograms to determine whether a patient needs a more detailed MRI. However, because this judgement does not follow quantitative criteria, medical care can vary considerably based on the radiologist, hospital policy, affluence of the hospital, and insurance coverage. These factors pose issues not only for quality of care but also equity in healthcare. For the millions of women who are screened for breast cancer each year, we want to quantify their risk assessment to enable early, accurate diagnoses. Quantifying this risk assessment will also assist radiologists performing the screenings, standardize hospital procedure, provide more concrete evidence for insurance coverage, and reduce expenses of treating more progressed cancers. More broadly, machine learning in medical research is often restricted to a single hospital or institution\u2019s data. However, patient demographics, including age, race, and socioeconomic status, can vary considerably from hospital to hospital. As a result, models do not necessarily generalize well when only trained on single institution data. To accelerate medical research with machine learning, we propose creating a software wrapper for the Generally Nuanced Deep Learning Framework (GaNDLF) that enables federated learning, a method to iteratively train and update a machine learning model on separate institutions\u2019 datasets, while respecting the privacy and access rights of each institution\u2019s data. MammoDL is a software tool that uses convolutional neural networks to quantitatively assess breast tissue density from mammograms to standardize clinicians\u2019 decisions on prescribing MRIs to patients. \u200b\u200bIt incorporates a federated learning pipeline to enable secure training on datasets across multiple institutions, as this decentralization ensures better performance in comparison to model training on a single server. MammoDL\u2019s user-friendly interface will allow for the tool to be configured to individual clinicians, in order to guide them effectively.", "link": "https://devpost.com/software/mammoai", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "abstract:\ndiagnosing breast cancer remains a subjective process, in which radiologists visually assess mammograms to determine whether a patient needs a more detailed mri. however, because this judgement does not follow quantitative criteria, medical care can vary considerably based on the radiologist, hospital policy, affluence of the hospital, and insurance coverage. these factors pose issues not only for quality of care but also equity in healthcare. for the millions of women who are screened for breast cancer each year, we want to quantify their risk assessment to enable early, accurate diagnoses. quantifying this risk assessment will also assist radiologists performing the screenings, standardize hospital procedure, provide more concrete evidence for insurance coverage, and reduce expenses of treating more progressed cancers. more broadly, machine learning in medical research is often restricted to a single hospital or institution\u2019s data. however, patient demographics, including age, race, and socioeconomic status, can vary considerably from hospital to hospital. as a result, models do not necessarily generalize well when only trained on single institution data. to accelerate medical research with machine learning, we propose creating a software wrapper for the generally nuanced deep learning framework (gandlf) that enables federated learning, a method to iteratively train and update a machine learning model on separate institutions\u2019 datasets, while respecting the privacy and access rights of each institution\u2019s data. mammodl is a software -----> tool !!!  that uses convolutional neural networks to quantitatively assess breast tissue density from mammograms to standardize clinicians\u2019 decisions on prescribing mris to patients. \u200b\u200bit incorporates a federated learning pipeline to enable secure training on datasets across multiple institutions, as this decentralization ensures better performance in comparison to model training on a single server. mammodl\u2019s user-friendly interface will allow for the tool to be configured to individual clinicians, in order to guide them effectively.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59506974}, {"Unnamed: 0": 6975, "autor": "Fixture", "date": null, "content": "Our project objective is to create a tool that can help fantasy soccer league players make the most of their budget and have the highest likelihood of winning the prize money. In order to do so, we have multiple goals for this semester. First, we would like to create a minimum viable product (i.e. the underlying model) using in-depth player statistics to optimize budget choices made by our user based on player\u2019s value at a given time. We also want to incorporate natural language processing, and to do so we plan to scrape Twitter data and apply a sentiment analysis to see how \u201chype\u201d (both positive and negative) affects a player\u2019s perceived value (undervalued/overvalued). We would like our users to make an optimal choice based on statistics and fixtures and not on how someone\u2019s game is being described by a few random people on Twitter. Third, we will be designing a user-friendly US that recommends players based on a given input of positions and budget. Our ultimate goal is to have a dynamic model factoring in upcoming fixtures for weekly transfers of fantasy players as well as any injuries that occur on the pitch in real-time. Not only are we aiming to solve the problem of information asymmetry in the market, but also level the playing field between fantasy players who have a lot of experience versus fantasy players who are just getting into managing a team. We plan to test our model at the end of the semester by comparing it to industry standards and using an objective metric of change in points that we will develop ourselves.", "link": "https://devpost.com/software/winner-winner-chicken-dinner-k7x6y3", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "our project objective is to create a -----> tool !!!  that can help fantasy soccer league players make the most of their budget and have the highest likelihood of winning the prize money. in order to do so, we have multiple goals for this semester. first, we would like to create a minimum viable product (i.e. the underlying model) using in-depth player statistics to optimize budget choices made by our user based on player\u2019s value at a given time. we also want to incorporate natural language processing, and to do so we plan to scrape twitter data and apply a sentiment analysis to see how \u201chype\u201d (both positive and negative) affects a player\u2019s perceived value (undervalued/overvalued). we would like our users to make an optimal choice based on statistics and fixtures and not on how someone\u2019s game is being described by a few random people on twitter. third, we will be designing a user-friendly us that recommends players based on a given input of positions and budget. our ultimate goal is to have a dynamic model factoring in upcoming fixtures for weekly transfers of fantasy players as well as any injuries that occur on the pitch in real-time. not only are we aiming to solve the problem of information asymmetry in the market, but also level the playing field between fantasy players who have a lot of experience versus fantasy players who are just getting into managing a team. we plan to test our model at the end of the semester by comparing it to industry standards and using an objective metric of change in points that we will develop ourselves.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59506975}, {"Unnamed: 0": 6978, "autor": "Wellness Wednesday", "date": null, "content": "Inspiration\nKnowing that positive work culture is essential to long-term business success, companies more and more often implement Booster Breaks, also known as Wellness Wednesdays. It's usually a short break from work, designed to support wellness within an organization, improve employee health and increase productivity. But team building at work can be tricky, especially nowadays when people work remotely or socially distance. At the moment, there is no tool to help find these activities quickly. The process often involves reading articles for hours to finally realize that none of the 50 games described will work in your team.\nWhat it does\nWellness Wednesday lets you quickly find a fun, team bonding activity for your work team. No more time-consuming scrolling in search of this one socially distant 15-minute game. In Wellness Wednesday, you can specify time constraints, needed supply and the character of a game. Mindfulness, Team Bonding or... Creativity? Or maybe you want to do something special for the upcoming Thanksgiving? You can also search by name or description.\nHow I built it\nThe application was built using Node.js, Express, MongoDB, JavaSciprt, JQuery, HTML, CSS and Bootstrap. Wellness Wednesday makes use of Microsoft Azure CosmosDB for MongoDB, Web App, Static Web App and Application Insight.\nWhat's next for Wellness Wednesday\nThe project can be developed further by developing the database and providing helpful materials and instructive videos. Wellness Wednesday could provide a safe space for employees to chat, share pictures and tips that get them through challenging times. In the future, employees could give scores to activities and organizations could keep track of what they have already done, what they want to do next week and activities that don't match their company culture. The application makes use of Microsoft Azure Static Web App, Web App, Cosmos Database and Application Insight.", "link": "https://devpost.com/software/wellness-wednesday-wc75pt", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nknowing that positive work culture is essential to long-term business success, companies more and more often implement booster breaks, also known as wellness wednesdays. it's usually a short break from work, designed to support wellness within an organization, improve employee health and increase productivity. but team building at work can be tricky, especially nowadays when people work remotely or socially distance. at the moment, there is no -----> tool !!!  to help find these activities quickly. the process often involves reading articles for hours to finally realize that none of the 50 games described will work in your team.\nwhat it does\nwellness wednesday lets you quickly find a fun, team bonding activity for your work team. no more time-consuming scrolling in search of this one socially distant 15-minute game. in wellness wednesday, you can specify time constraints, needed supply and the character of a game. mindfulness, team bonding or... creativity? or maybe you want to do something special for the upcoming thanksgiving? you can also search by name or description.\nhow i built it\nthe application was built using node.js, express, mongodb, javasciprt, jquery, html, css and bootstrap. wellness wednesday makes use of microsoft azure cosmosdb for mongodb, web app, static web app and application insight.\nwhat's next for wellness wednesday\nthe project can be developed further by developing the database and providing helpful materials and instructive videos. wellness wednesday could provide a safe space for employees to chat, share pictures and tips that get them through challenging times. in the future, employees could give scores to activities and organizations could keep track of what they have already done, what they want to do next week and activities that don't match their company culture. the application makes use of microsoft azure static web app, web app, cosmos database and application insight.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59506978}, {"Unnamed: 0": 7001, "autor": "BICHEF", "date": null, "content": "Inspiration\nCollaborating with DevOps and interacting with them for creating dashboards, an idea came into my mind that APIs are sources of data. If we can turn the data generated from API into relational data then we can use BI tools to create the dashboards which will be more powerful than the custom ones.\nWhat it does\nBI tools act as a front end to complement Chef Automate dashboards. You can visualize Chef Automate data in beautiful charts and data grids. You can filter data using controls available in BI tools. You can interact with the BI tool just like Chef Automate front end no need to trigger an application to dump data. You get updates sitting within the BI tool.\nEverything you need in one dashboard. Like you can have data from multiple Chef instances in one place. If the analysis dataset is small but highly volatile and you need updates in real-time then BICHEF is for you because you don't have to wait for the regular intervals to see the results.\nBICHEF can scale to support a big user base because we serve data in real-time and we are not piling up the data. Another dimension we can scale in is the API set we cover at little cost.\nBICHEF is a step towards a single source of truth because sometimes developers are logging the same information in multiple systems like two Chef Automate instances. With BICHEF there is no need to do that.\nHow we built it\nA routine (prototype) to convert JSON to relational data. The front end is built in PHP using Codeigniter. The backend is in MariaDB.\nChallenges we ran into\nJSON to relational conversion is challenging however I managed to come up with an implementation that allows cherry-picking JSON nodes and the routine converts Json paths to relational data with some limitations.\nAccomplishments that we're proud of\nWe added major CHEF APIs under our portfolio.\nWhat we learned\nSmart work!\nWhat's next for BICHEF\nTo be the next Big Thing in Software World!", "link": "https://devpost.com/software/bichef", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ncollaborating with devops and interacting with them for creating dashboards, an idea came into my mind that apis are sources of data. if we can turn the data generated from api into relational data then we can use bi tools to create the dashboards which will be more powerful than the custom ones.\nwhat it does\nbi tools act as a front end to complement chef automate dashboards. you can visualize chef automate data in beautiful charts and data grids. you can filter data using controls available in bi tools. you can interact with the bi -----> tool !!!  just like chef automate front end no need to trigger an application to dump data. you get updates sitting within the bi tool.\neverything you need in one dashboard. like you can have data from multiple chef instances in one place. if the analysis dataset is small but highly volatile and you need updates in real-time then bichef is for you because you don't have to wait for the regular intervals to see the results.\nbichef can scale to support a big user base because we serve data in real-time and we are not piling up the data. another dimension we can scale in is the api set we cover at little cost.\nbichef is a step towards a single source of truth because sometimes developers are logging the same information in multiple systems like two chef automate instances. with bichef there is no need to do that.\nhow we built it\na routine (prototype) to convert json to relational data. the front end is built in php using codeigniter. the backend is in mariadb.\nchallenges we ran into\njson to relational conversion is challenging however i managed to come up with an implementation that allows cherry-picking json nodes and the routine converts json paths to relational data with some limitations.\naccomplishments that we're proud of\nwe added major chef apis under our portfolio.\nwhat we learned\nsmart work!\nwhat's next for bichef\nto be the next big thing in software world!", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59507001}, {"Unnamed: 0": 7012, "autor": "Autoscaling JupyterHub", "date": null, "content": "Inspiration\nJupyterHub is a Python application that runs in the cloud or on your own hardware, and makes it possible to serve pre-configured data science containers to multiple users. It is customizable and scalable, and is suitable for small and large teams, academic courses, and large-scale infrastructure.\nHowever, JupyterHub traditionally depends on one of two configurations. Either a vertically-scaled single node deployment (ironically named, \"The Littlest JupyterHub\") or a Kubernetes deployment. For many users, especially in education, both a single node or K8s deployment can quickly become impractical.\nConsider a lab session with 100+ students on a large single node instance, the instance would be deeply underutilized for all but a few hours a week. If the machine was under-provisioned, it may be unable to meet demand during the lab sessions with many concurrent users. K8s presents a viable alternative if you're willing to maintain it, but I propose an alternative solution.\nWhat it does\nThis project uses Terraform, Packer, GHActions, Chef, and Docker Swarm to provision and deploy a JupyterHub instance backed by an EC2 Auto-Scaling Group (ASG) instances with a shared NFS (AWS EFS) file-system.\nAs an added bonus, the system also creates Chef Server, Chef Client, and Chef Workstation AWS AMIs, with recent Ubuntu and Chef Software versions. You can use in other deployments if you'd like, I found the Chef Client AMI very helpful when bootstrapping new auto-scaling nodes quickly.\nThe system has two components. First, a Jupyter Hub instance, this instance is permanent and is the Docker Swarm Manager, all hub requests are routed through this instance. The second is a cluster of ASG instances. As CPU usage increases (could easily add a memory usage rule as well), the new instances join the Chef Server with role \"worker\". From there, each node's recipes configure it to join the Docker Swarm. Once joined, that node can be used as a deployment target for new user's analysis notebook containers.\nBecause we expect ASG nodes to scale in and out with traffic, it's reasonable to purchase these instances from the AWS spot market. This is a risky choice for some use-cases, but for simple exploratory analysis, worth the occasional restart!\nHow we built it\nSee architecture slides above. I used Packer, Terraform, Github Actions, and Chef to deploy the system.\nChef - To configure the cluster's instances, install applications, and manage their state.\nPacker - To create machine images for Chef Workstation, Chef Nodes, and Chef Server. This was important as it allows ASG nodes to come up much quicker because they're pulling an AMI with Chef instead of installing Chef themselves.\nTerraform/AWS - Provision core infrastructure, e.g. VPC, Subnets, Security Groups, IAM profiles, NFS, and EC2 instances.\nGithub Actions - Run the CI jobs for the deployment of the system\nChallenges we ran into / Accomplishments that we're proud of\nProvisioning Chef Servers - Doing this from Terraform was quite difficult at first. Particularly, ensuring that instances came up at the right time and their dependencies user-data scripts had finished successfully. I was able to come up with a temporary solution using SSM Documents to call cloud-init status --wait, but this solution still felt over-engineered for what it was doing. This snag pushed me towards using Packer to pre-build custom Chef AMIs instead.\nJupyterHub Configuration - JupyterHub is very configurable, and this can be confusing even if you've brought up a service like this before. In a real deployment, a researcher/engineer/educator would need to carefully read through the docs to get just the right configuration for them. In this instance though, I just needed a viable demo configuration, which I was able to achieve with a few t3.small and t3.medium nodes.\nDe-registering Nodes - As of writing, both Chef Server and Docker Swarm don't have an intelligent way to de-register ASG nodes when they drop off. This can make management of nodes confusing, especially when there's high churn.\nWhat we learned\nChef - Period. Before this event I hadn't used Chef at all. I work as a data engineer and I provision database clusters with other IAC tools, but I'm glad that I got to have this experience.\nPacker - This was my first time deploying AMIs with Packer, it's a nice tool and I'd love to use it a bit more. I didn't anticipate building any custom AMIs when I started this project, but glad that I ended up here.\nWhat's next for Autoscaling JupyterHub\nHardening - Hardening or security were admittedly not my top priority here. I'd want to go through and audit the choices made and tighten those up. I'd also probably want to integrate Inspec into the CI.\nIsolation of Clusters/Decoupling of Chef and Jupyter Deployment - The way my Terraform and CI is written implies there will be one and only one Jupyter cluster per Chef Server. In practice, only one Chef Server is needed for many jupyterhub clusters (we know Chef is more than capable of managing 100+ nodes). In theory, an engineer should be able to run terraform apply in one module to bring up a Chef Server and Workstation, and terraform apply in another to bring up a JupyterHub cluster (or 10, one for each lecture in the department) backed by an ASG. For this, I'd need to split code into multiple modules and just be mindful about module outputs/variables.\nAdditional Note on Lock-In: There is no reason this deployment needs to be on AWS. At the core of this system is Packer (which can use Linode, GCP, Digital Ocean, etc.) as a target build environment, Terraform (which can provision for most major cloud providers), and Chef (which is of course completely agnostic to what hosting is used. Each of these providers also has object storage, and some sort of concept analogous to AWS' auto-scaling nodes.", "link": "https://devpost.com/software/autoscaling-jupyterhub", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\njupyterhub is a python application that runs in the cloud or on your own hardware, and makes it possible to serve pre-configured data science containers to multiple users. it is customizable and scalable, and is suitable for small and large teams, academic courses, and large-scale infrastructure.\nhowever, jupyterhub traditionally depends on one of two configurations. either a vertically-scaled single node deployment (ironically named, \"the littlest jupyterhub\") or a kubernetes deployment. for many users, especially in education, both a single node or k8s deployment can quickly become impractical.\nconsider a lab session with 100+ students on a large single node instance, the instance would be deeply underutilized for all but a few hours a week. if the machine was under-provisioned, it may be unable to meet demand during the lab sessions with many concurrent users. k8s presents a viable alternative if you're willing to maintain it, but i propose an alternative solution.\nwhat it does\nthis project uses terraform, packer, ghactions, chef, and docker swarm to provision and deploy a jupyterhub instance backed by an ec2 auto-scaling group (asg) instances with a shared nfs (aws efs) file-system.\nas an added bonus, the system also creates chef server, chef client, and chef workstation aws amis, with recent ubuntu and chef software versions. you can use in other deployments if you'd like, i found the chef client ami very helpful when bootstrapping new auto-scaling nodes quickly.\nthe system has two components. first, a jupyter hub instance, this instance is permanent and is the docker swarm manager, all hub requests are routed through this instance. the second is a cluster of asg instances. as cpu usage increases (could easily add a memory usage rule as well), the new instances join the chef server with role \"worker\". from there, each node's recipes configure it to join the docker swarm. once joined, that node can be used as a deployment target for new user's analysis notebook containers.\nbecause we expect asg nodes to scale in and out with traffic, it's reasonable to purchase these instances from the aws spot market. this is a risky choice for some use-cases, but for simple exploratory analysis, worth the occasional restart!\nhow we built it\nsee architecture slides above. i used packer, terraform, github actions, and chef to deploy the system.\nchef - to configure the cluster's instances, install applications, and manage their state.\npacker - to create machine images for chef workstation, chef nodes, and chef server. this was important as it allows asg nodes to come up much quicker because they're pulling an ami with chef instead of installing chef themselves.\nterraform/aws - provision core infrastructure, e.g. vpc, subnets, security groups, iam profiles, nfs, and ec2 instances.\ngithub actions - run the ci jobs for the deployment of the system\nchallenges we ran into / accomplishments that we're proud of\nprovisioning chef servers - doing this from terraform was quite difficult at first. particularly, ensuring that instances came up at the right time and their dependencies user-data scripts had finished successfully. i was able to come up with a temporary solution using ssm documents to call cloud-init status --wait, but this solution still felt over-engineered for what it was doing. this snag pushed me towards using packer to pre-build custom chef amis instead.\njupyterhub configuration - jupyterhub is very configurable, and this can be confusing even if you've brought up a service like this before. in a real deployment, a researcher/engineer/educator would need to carefully read through the docs to get just the right configuration for them. in this instance though, i just needed a viable demo configuration, which i was able to achieve with a few t3.small and t3.medium nodes.\nde-registering nodes - as of writing, both chef server and docker swarm don't have an intelligent way to de-register asg nodes when they drop off. this can make management of nodes confusing, especially when there's high churn.\nwhat we learned\nchef - period. before this event i hadn't used chef at all. i work as a data engineer and i provision database clusters with other iac tools, but i'm glad that i got to have this experience.\npacker - this was my first time deploying amis with packer, it's a nice -----> tool !!!  and i'd love to use it a bit more. i didn't anticipate building any custom amis when i started this project, but glad that i ended up here.\nwhat's next for autoscaling jupyterhub\nhardening - hardening or security were admittedly not my top priority here. i'd want to go through and audit the choices made and tighten those up. i'd also probably want to integrate inspec into the ci.\nisolation of clusters/decoupling of chef and jupyter deployment - the way my terraform and ci is written implies there will be one and only one jupyter cluster per chef server. in practice, only one chef server is needed for many jupyterhub clusters (we know chef is more than capable of managing 100+ nodes). in theory, an engineer should be able to run terraform apply in one module to bring up a chef server and workstation, and terraform apply in another to bring up a jupyterhub cluster (or 10, one for each lecture in the department) backed by an asg. for this, i'd need to split code into multiple modules and just be mindful about module outputs/variables.\nadditional note on lock-in: there is no reason this deployment needs to be on aws. at the core of this system is packer (which can use linode, gcp, digital ocean, etc.) as a target build environment, terraform (which can provision for most major cloud providers), and chef (which is of course completely agnostic to what hosting is used. each of these providers also has object storage, and some sort of concept analogous to aws' auto-scaling nodes.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59507012}, {"Unnamed: 0": 7018, "autor": "Deep Sign Recognition", "date": null, "content": "Inspiration\nAs a doctor, I had seen many people that are struggling due to disabilities. For the particular one the deaf community. They have a lot of problems during their lifetime. We understood that when they starting their start to learn sign language and understand it, most of them do not have any captioning on them. Even during conference meetings, they don't know what exactly the signers are communicating. Captioning on sign language video is very needed in communication, education, and almost all part of their life. We tried on building an API for that purpose, and we built the ** Worlds' first** python API which is integrated into a desktop application, where people can upload sign language videos and get captioned using a state-of-the-art AI model. It doesn't need any fast GPU. Since we optimized the AI model to work on any low-end computer, with CPU power.\nWhat it does\nAn AI based desktop application and python API for automatic sign language recognition and captioning for any sign language including ASL, BSL , ISL, etc. Upload a sign language video and click on Apply and the video get processed using an AI model and the output video with caption is shown, which can be saved for later use.\nHow we built it\nNo available tools or apps are in the market that can recognize dynamic sign language patterns. Every computer vision or AI tool uses a simple object detection model. If we consider the object detection model from TensorFlow, it only takes one frame per inference as input and throws some probabilities of the sign as output. It doesn't consider concatenated frames, which is very needed in the case of real-life signing. So we can't use simple 2-dimensional convolution, we have to use 3D convolution to do the job for us. The I3D model from the deep mind is one of the best video classification models that take bulk frames and applies 3D convolution to get sign output. We used this approach on the Word-Level American Sign Language dataset and converted the model to TensorFlow lite for faster inference. We then made a Desktop application using PyQT5 which can run on both Windows and Ubuntu. The application helps to upload video and add sign language subtitles to it. Since WLASL has 2000 classes, it can cover almost all signs. In the demo we tried the Indian Sign Language, even the model trained on ASL accurately detected the sign from the video. We are building this as an API in the cloud so that it can be attached to meeting applications such as Google meet, or zoom, or android, web, or desktop applications. The API build is almost complete.\nChallenges we ran into\nBuilding a desktop application is very hard for us. PyQt5 dependency issues happened and crashed the app for couple of times during testing.\nConverting the pretrained PyTorch model to ONNX and to TensorFlow Lite for faster inference taken most of our time. Since we need to implement custom ops for TFLite, since most of the conv3D operations doesn't support the builtin operators of TensorFlow.\nBuilding a python API.\nAccomplishments that we're proud of\nBuilding a desktop application that works on both Windows and Linux\nWhat we learned\nBuilding a desktop application\nConverting PyTorch model to TFLite with custom ops\nWhat's next for Deep Sign Recognition\nBuilding a GPU powered API for faster inference, and integrate it to Zoom, Google Meet, android apps and web apps", "link": "https://devpost.com/software/deep-sign-recognition", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nas a doctor, i had seen many people that are struggling due to disabilities. for the particular one the deaf community. they have a lot of problems during their lifetime. we understood that when they starting their start to learn sign language and understand it, most of them do not have any captioning on them. even during conference meetings, they don't know what exactly the signers are communicating. captioning on sign language video is very needed in communication, education, and almost all part of their life. we tried on building an api for that purpose, and we built the ** worlds' first** python api which is integrated into a desktop application, where people can upload sign language videos and get captioned using a state-of-the-art ai model. it doesn't need any fast gpu. since we optimized the ai model to work on any low-end computer, with cpu power.\nwhat it does\nan ai based desktop application and python api for automatic sign language recognition and captioning for any sign language including asl, bsl , isl, etc. upload a sign language video and click on apply and the video get processed using an ai model and the output video with caption is shown, which can be saved for later use.\nhow we built it\nno available tools or apps are in the market that can recognize dynamic sign language patterns. every computer vision or ai -----> tool !!!  uses a simple object detection model. if we consider the object detection model from tensorflow, it only takes one frame per inference as input and throws some probabilities of the sign as output. it doesn't consider concatenated frames, which is very needed in the case of real-life signing. so we can't use simple 2-dimensional convolution, we have to use 3d convolution to do the job for us. the i3d model from the deep mind is one of the best video classification models that take bulk frames and applies 3d convolution to get sign output. we used this approach on the word-level american sign language dataset and converted the model to tensorflow lite for faster inference. we then made a desktop application using pyqt5 which can run on both windows and ubuntu. the application helps to upload video and add sign language subtitles to it. since wlasl has 2000 classes, it can cover almost all signs. in the demo we tried the indian sign language, even the model trained on asl accurately detected the sign from the video. we are building this as an api in the cloud so that it can be attached to meeting applications such as google meet, or zoom, or android, web, or desktop applications. the api build is almost complete.\nchallenges we ran into\nbuilding a desktop application is very hard for us. pyqt5 dependency issues happened and crashed the app for couple of times during testing.\nconverting the pretrained pytorch model to onnx and to tensorflow lite for faster inference taken most of our time. since we need to implement custom ops for tflite, since most of the conv3d operations doesn't support the builtin operators of tensorflow.\nbuilding a python api.\naccomplishments that we're proud of\nbuilding a desktop application that works on both windows and linux\nwhat we learned\nbuilding a desktop application\nconverting pytorch model to tflite with custom ops\nwhat's next for deep sign recognition\nbuilding a gpu powered api for faster inference, and integrate it to zoom, google meet, android apps and web apps", "sortedWord": "None", "removed": "Nan", "score": 12, "comments": 0, "media": null, "medialink": null, "identifyer": 59507018}, {"Unnamed: 0": 7040, "autor": "Unstoppable Auto-caption", "date": null, "content": "Inspiration\nWe developed Unstoppable because the development of accessible products requires accessible tools and with the Unstoppable tool we are making Jira and Confluence accessible for users. In Jira and Confluence, companies across industries use images for a lot of use cases. Accessible users were able to access the text but due to lack of image description, they were not 100% productive hence this idea of image captioning was originated and developed.\nWhat it does\nThere is a famous adage that says, \"A picture is worth a thousand words.\" If this is true, is it true for any modern-day documentation and collaboration workspaces like Confluence or work management tools like Jira? That's where Unstoppable comes into play.\nUnstoppable auto-generates captions for images on Jira issues and Confluence pages. These captions are then read out loud by screen readers that assist accessibility users in understanding the context and content of the images. The image captions are split into two categories:\nDescription of the persons, objects, and their activity in the image. E.g. \"A girl is dancing wearing a blue dress,\" \"Two kids are playing,\" and \"A cycle on the road.\"\nDescription of the text and shapes(graphs, charts etc) in the image. E.g. \"Nutritional Facts\", \"Company Growth Chart\" etc\nThis is done in two steps.\nWe fetch the normal caption for each image as denoted in the first category above.\nIf we find the image requires the caption of the second category, as stated above, then we fetch the OCR caption as well.\nThe typical process works like this:\nOn page load, identify the image, find its URL.\nPass the image URL to an API that checks if the caption is present in DB already.\nIf the caption is present, we return it to the frontend in the API response.\nIf the caption is not present in DB then we go and fetch it through Azure Computer Vision API and save it in the DB for the next call and we return it to the frontend in the API response.\nFrontend appends the caption to the image that is readable by Screen Readers.\nIf the caption is of OCR Type, along with normal caption we show a button to users if they want to fetch more details.\nIf they click on that button, we fetch the OCR description and save it in the DB for the next call and we return it to the frontend in the API response.\nFrontend informs the user that caption is fetched and appended to the image that is read by Screen Reader.\nHow we built it\nWhere does one start with one of these? For us, we started with our Unstoppable for Jira and Confluence app. Unstoppable was already using text-to-speech technology that reads out attributes to the accessibility users for them to take action. This product allowed accessibility users to work more efficiently and effectively within Jira and Confluence. As we were already dealing with text within an and issue and a page, images were a massive gap that we had to fill.\nFirst, we worked internally with one of our visually impaired cohorts to figure out what requirements were needed. We brainstormed together on what made sense from a solution standpoint and a design/user experience standpoint. After going through numerous iterations of that we landed on something that seemed to work. After developing a proof of concept we ran into some issues, which can be read about below, that required us to re-evaluate to get an even better product out the door.\nThe main product is using Javascript/HTML/CSS on the frontend and Java Spring Framework on the backend integration within the Atlassian SDK.\nThen we use the Azure Computer Vision APIs for the actual captioning process of the product.\nChallenges we ran into\nAs always, no plan survives first contact with the enemy. Initially, we tried to implement the solution by passing image URLs to the Azure Computer Vision APIs. It initially worked in our proof of concept with unauthorized images...however, as soon as we integrated with Jira and Confluence it began to fail. We learned this was because the images were authenticated and Azure had no clue about the authentication. Basically, it expected the images to be publicly available.\nWe looked at what we had done and decided to take a step back and change our approach. Instead of passing the images through the Azure Computer Vision APIs, we decided to download the image and convert it into a byte array, which is supported by the Azure Computer Vision APIs, and then send it off to the APIs.\nThe challenge now was how to download the image on the backend. To do that we would need authentication tokens to be passed. To overcome this we used a JSESSIONID for our server apps and a JSESSIONID and Seraph token for our Data Center apps to download the image.\nAfter that hurdle was overcome we had a fully functioning app.\nWhat we learned\nThey say that knowledge is power and this was a huge learning experience. Throughout our process, we learned a number of different things.\nThe first big thing was how fast and reliable the Azure Computer Vision APIs are. It truly is a great tool. The captions are predominately accurate most of the time and the Azure Computer Vision client library helps abstract the complex logic of the integration.\nThe next thing we learned was around all that Image Authentication stuff. When we started this project there were no specific solutions available to the Atlassian community to solve this issue. As this was a new approach we implemented all this through trial and error. *Azure doesn't store any of the images it processes so there are no security concerns.\nAccomplishments that we're proud of\nA lot of fortune 500's are able to create an accessible product with accessible tools.\nUnstoppable Alexa integration was picked by Atlassian for Shipit Live and presented to over 3 thousand people in attendance.\n4 out of Top 25 banks of the world use Unstoppable.\nA visually impaired Scientist at NASA uses Unstoppable for daily activities.\nWhat's next for Unstoppable Auto-caption\nAdding captions into a custom field in Jira to allow the JQL search of issues based on the images.\nWe plan to expand Unstoppable to the source control tools like Bitbucket, Github, Gitlab.\nThe next natural progression is to provide almost real-time feedback to the developer at the time of development to improve accessibility.", "link": "https://devpost.com/software/unstoppable-auto-caption", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe developed unstoppable because the development of accessible products requires accessible tools and with the unstoppable -----> tool !!!  we are making jira and confluence accessible for users. in jira and confluence, companies across industries use images for a lot of use cases. accessible users were able to access the text but due to lack of image description, they were not 100% productive hence this idea of image captioning was originated and developed.\nwhat it does\nthere is a famous adage that says, \"a picture is worth a thousand words.\" if this is true, is it true for any modern-day documentation and collaboration workspaces like confluence or work management tools like jira? that's where unstoppable comes into play.\nunstoppable auto-generates captions for images on jira issues and confluence pages. these captions are then read out loud by screen readers that assist accessibility users in understanding the context and content of the images. the image captions are split into two categories:\ndescription of the persons, objects, and their activity in the image. e.g. \"a girl is dancing wearing a blue dress,\" \"two kids are playing,\" and \"a cycle on the road.\"\ndescription of the text and shapes(graphs, charts etc) in the image. e.g. \"nutritional facts\", \"company growth chart\" etc\nthis is done in two steps.\nwe fetch the normal caption for each image as denoted in the first category above.\nif we find the image requires the caption of the second category, as stated above, then we fetch the ocr caption as well.\nthe typical process works like this:\non page load, identify the image, find its url.\npass the image url to an api that checks if the caption is present in db already.\nif the caption is present, we return it to the frontend in the api response.\nif the caption is not present in db then we go and fetch it through azure computer vision api and save it in the db for the next call and we return it to the frontend in the api response.\nfrontend appends the caption to the image that is readable by screen readers.\nif the caption is of ocr type, along with normal caption we show a button to users if they want to fetch more details.\nif they click on that button, we fetch the ocr description and save it in the db for the next call and we return it to the frontend in the api response.\nfrontend informs the user that caption is fetched and appended to the image that is read by screen reader.\nhow we built it\nwhere does one start with one of these? for us, we started with our unstoppable for jira and confluence app. unstoppable was already using text-to-speech technology that reads out attributes to the accessibility users for them to take action. this product allowed accessibility users to work more efficiently and effectively within jira and confluence. as we were already dealing with text within an and issue and a page, images were a massive gap that we had to fill.\nfirst, we worked internally with one of our visually impaired cohorts to figure out what requirements were needed. we brainstormed together on what made sense from a solution standpoint and a design/user experience standpoint. after going through numerous iterations of that we landed on something that seemed to work. after developing a proof of concept we ran into some issues, which can be read about below, that required us to re-evaluate to get an even better product out the door.\nthe main product is using javascript/html/css on the frontend and java spring framework on the backend integration within the atlassian sdk.\nthen we use the azure computer vision apis for the actual captioning process of the product.\nchallenges we ran into\nas always, no plan survives first contact with the enemy. initially, we tried to implement the solution by passing image urls to the azure computer vision apis. it initially worked in our proof of concept with unauthorized images...however, as soon as we integrated with jira and confluence it began to fail. we learned this was because the images were authenticated and azure had no clue about the authentication. basically, it expected the images to be publicly available.\nwe looked at what we had done and decided to take a step back and change our approach. instead of passing the images through the azure computer vision apis, we decided to download the image and convert it into a byte array, which is supported by the azure computer vision apis, and then send it off to the apis.\nthe challenge now was how to download the image on the backend. to do that we would need authentication tokens to be passed. to overcome this we used a jsessionid for our server apps and a jsessionid and seraph token for our data center apps to download the image.\nafter that hurdle was overcome we had a fully functioning app.\nwhat we learned\nthey say that knowledge is power and this was a huge learning experience. throughout our process, we learned a number of different things.\nthe first big thing was how fast and reliable the azure computer vision apis are. it truly is a great tool. the captions are predominately accurate most of the time and the azure computer vision client library helps abstract the complex logic of the integration.\nthe next thing we learned was around all that image authentication stuff. when we started this project there were no specific solutions available to the atlassian community to solve this issue. as this was a new approach we implemented all this through trial and error. *azure doesn't store any of the images it processes so there are no security concerns.\naccomplishments that we're proud of\na lot of fortune 500's are able to create an accessible product with accessible tools.\nunstoppable alexa integration was picked by atlassian for shipit live and presented to over 3 thousand people in attendance.\n4 out of top 25 banks of the world use unstoppable.\na visually impaired scientist at nasa uses unstoppable for daily activities.\nwhat's next for unstoppable auto-caption\nadding captions into a custom field in jira to allow the jql search of issues based on the images.\nwe plan to expand unstoppable to the source control tools like bitbucket, github, gitlab.\nthe next natural progression is to provide almost real-time feedback to the developer at the time of development to improve accessibility.", "sortedWord": "None", "removed": "Nan", "score": 53, "comments": 0, "media": null, "medialink": null, "identifyer": 59507040}, {"Unnamed: 0": 7043, "autor": "Theia: Object Detection and Depth Perception Using ML", "date": null, "content": "Inspiration\nOur team had a number of inspirations. First and perhaps foremost, our engineer works for a computer vision company that utilizes machine learning (ML) techniques to prevent mass shootings and provide insight to security professionals. He has been involved with the development of a neural inference model that detects weapons in a scene and provides notifications. Building off this knowledge, he was compelled to first consider how computer vision could be used to enhance accessibility.\nOn the business development side, our administrative teammate had previously studied how biomimicry can solve complex human problems. Biomimicry is the emulation of biological phenomena \u2013 or examining nature to find solutions. In the case of visual impairments, animals like bats and dolphins use echolocation to expand their visual abilities. Echolocation is the use of sound to help decipher visual environments. We will expand on this more in the sections below, but our decision to build off the OAK-D system reminds us of echolocation because it fuses stereo depth technology to expand on its video features for improved spatial recognition.\nFinally, we\u2019ve named our project \u201cTheia\u201d after the Greek goddess of sight.\nWhat it does\nTheia allows people with visual impairments to have advance notice of key surroundings, further than a standard mobility cane is able to reach. For example, if a person was approaching our individual, they would receive a spoken notification of the object\u2019s distance, such as \u201c3 meters,\u201d \u201c2 meters,\u201d etc. Theia does this by combining five key components.\nFirst, there is the sensory component from OAK-D. This is hardware that combines a camera with spatial stereos and basic intelligent analytics.\nSecond, there is the software component built by our engineer. Our engineer wrote code to detect, track, and report the distance of objects. The software can handle processing at 30 frames per second.\nThird, there is an audio component. Compatible with both Bluetooth technology and classic wired headphones, this component reports the distance of an object to our user. With these options, our preferred mode of audio is a pair of sunglasses that have Bluetooth speakers on the side of the frame for easy wearability.\nFourth, there is a bridge component. Notably, our software (component 2) does not live on the OAK-D technology that we procured (component 1). In order to bridge the components together, we used a Raspberry Pi 4. This allowed our engineer to create software that could better handle high dimensionality of data, while also maintaining communication between our de-coupled components. This also gave us the freedom to implement more memory-intensive computations, such as storing track IDs as they come in.\nFifth and last is our power source. We used an external battery that can produce a minimum of 20 watts under high load as well as outputting a frequency of at least 10,000 mAMP per second.\nHow we built it\nOur business development teammate first compiled research on existing tools for people with visual impairments. The most common tool is of course the standard mobility cane. The average length of a mobility cane, or white cane, is half of a person\u2019s height. With the average height of adults being roughly between 5 feet and 4 inches to 5 feet and 9 inches, average length of a mobility cane is around 2 to 3 feet \u2013 or just under one meter. Consulting with our engineer teammate, we considered ways to extend this distance to provide information of objects that are further away than the cane can reach. We then wrote our software to only provide insights of objects after this distance to avoid providing redundant information; Our tool does not replace the mobility cane, it enhances it.\nThen our engineer did compiled research on existing, open source tools and appropriate hardware that would allow him to turn our idea into a reality. He invested in the equipment, including the OAK-D technology, Raspberry Pi, and audio component. He then sat down to code the software that could bring Theia to life.\nThe software\u2019s objection detection capacity provides the ability to detect a region within an image and classify it as a particular object. The software\u2019s object tracking capacity is similar to short-term memory in that it can determine if one object is brand new to the region or has been in the region for some time. By combining these two paradigms, we successfully created a system that allows a person with a visual disability to receive real-time alerts when an object is either approaching them or getting further way.\nChallenges we ran into\nWe went back and forth on the audio components, e.g. headphones versus Bluetooth. Our first idea was to use noise-cancelling headphones that could ensure clear, crisp audio for the user. However, we realized that in a real-world scenario, although this would provide sharp insight, it could be overbearing and block out all other stimuli for the wearer. This could make it difficult for Theia users to participate in conversations, or even put them in harm\u2019s way. Thus, we decided to go with the a more standard wired headphone or Bluetooth technology format that could allow the user to use the eyeglasses that contain speakers on the side of the frames (arms), or to simply wear one speaker of the standard headphone. This would allow audio to be played or heard without blocking any other external audio (e.g. traffic, general surrounding noises, etc.). We want Theia users to be able to maintain a balance between our technology and their own environment, without one overbearing the other.\nAccomplishments that we're proud of\nWe are proud that we were able to start with a complex idea, one which had many different routes through which it could be accomplished in terms of hardware and software, and were able to execute it in a fashion that only required accessories that can fit in a pocket or small bag. We believe that in a commercial environment this would allow for more affordability and simplicity for consumers.\nBoth members of our team have past experience with seeking solutions for social problems and/or underrepresented communities. Further, as we are both on the cusp of the Gen-Z and Millennial generations, we recognize that we and our peers are the future of society and feel it is our duty to work on solutions that are accessible to all. There is no better time to do this; As products such as Raspberry Pis become more affordable, software and robotics engineers are unlocking enough horsepower to develop advanced edge computing applications, meaning more portable technological solutions. With recent advancements in technology nearly everything is now possible, but not everything has been applied yet. Being able to dedicate our time to a project that focuses on accessibility, and that could theoretically benefit many people, is rewarding in and of itself.\nWhat we learned\nWe learned a lot about existing hardware and open source tools that are just waiting for engineers to improve and supplement their capacity.\nWhile our engineer has previous experience in computer vision, integrating the audio component was new for him. Inferencing at 30 frames per second means 1,800 possible notifications that could be sent to an audio device. If we sent all of these alerts, then the user would hear sounds 24/7 \u2013 deeming the product useless and maybe even dangerous. He had to teach himself an effective way to pick and choose amongst the frames while maintaining the tool\u2019s integrity to allow for more intelligent analytics, less stimuli and ultimately a better product.\nWhat's next for Theia: Object Detection and Depth Perception Using ML\nWe are interested in seeking solutions to power constraints. In order to power the Raspberry Pi and the camera together, we currently need to use an external battery that can produce a minimum of 20 watts under high load as well as outputting a frequency of at least 10,000 mAMP per second. The program is able to run effectively, but the power discrepancy causes the hardware to slow down in certain circumstances such as when there are many objects in the frame or when objects are moving quickly. In the future, we would like to explore better and/or separate batteries so our components do not have to share a power source.\nAdditionally, due to the onboard audio chip on the Raspberry Pi, we are currently limited to a Bluetooth connection of 3 megabytes per second. Although this is acceptable for an effective initial iteration, if this product were to be used at scale we would prefer a stronger Bluetooth connection with a higher frequency to ensure zero data interruptions while it is being used. Further, on the audio component, we would like to explore how an iPhone app could improve our audio features. We believe an app would allow for de-coupling of several tasks currently performed by the Raspberry Pi and allow for a faster, smoother notification process. This could hypothetically allow us to provide Theia users with more information than the distance at which an object is to them, including identification of the object. For example, instead of telling the user \u201cone meter,\u201d we could tell them \u201cperson in one meter\u201d or \u201ccar in one meter.\u201d Our device already contains the ability to identify several objects, but we are not specifying in the notification due to limited capability of the Raspberry Pi. De-coupling this through use of an iPhone app would allow us to provide longer verbiage to Theia users without slowing down their experience.\nFinally, future iterations should be made even more wearable. We aimed to maximize this in our project by using Bluetooth compatible eyeglasses with speakers in the frames, in addition to setting up an arm for the OAK-D camera that could be placed inside a backpack. However, we found the camera needed adjusting after our user walked a lengthier distance. The user is also required to carry the external battery in their backpack. While our device is portable, the future of Theia should be simplified even further for the ease of consumers.", "link": "https://devpost.com/software/theia-object-detection-and-depth-perception-using-ml", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nour team had a number of inspirations. first and perhaps foremost, our engineer works for a computer vision company that utilizes machine learning (ml) techniques to prevent mass shootings and provide insight to security professionals. he has been involved with the development of a neural inference model that detects weapons in a scene and provides notifications. building off this knowledge, he was compelled to first consider how computer vision could be used to enhance accessibility.\non the business development side, our administrative teammate had previously studied how biomimicry can solve complex human problems. biomimicry is the emulation of biological phenomena \u2013 or examining nature to find solutions. in the case of visual impairments, animals like bats and dolphins use echolocation to expand their visual abilities. echolocation is the use of sound to help decipher visual environments. we will expand on this more in the sections below, but our decision to build off the oak-d system reminds us of echolocation because it fuses stereo depth technology to expand on its video features for improved spatial recognition.\nfinally, we\u2019ve named our project \u201ctheia\u201d after the greek goddess of sight.\nwhat it does\ntheia allows people with visual impairments to have advance notice of key surroundings, further than a standard mobility cane is able to reach. for example, if a person was approaching our individual, they would receive a spoken notification of the object\u2019s distance, such as \u201c3 meters,\u201d \u201c2 meters,\u201d etc. theia does this by combining five key components.\nfirst, there is the sensory component from oak-d. this is hardware that combines a camera with spatial stereos and basic intelligent analytics.\nsecond, there is the software component built by our engineer. our engineer wrote code to detect, track, and report the distance of objects. the software can handle processing at 30 frames per second.\nthird, there is an audio component. compatible with both bluetooth technology and classic wired headphones, this component reports the distance of an object to our user. with these options, our preferred mode of audio is a pair of sunglasses that have bluetooth speakers on the side of the frame for easy wearability.\nfourth, there is a bridge component. notably, our software (component 2) does not live on the oak-d technology that we procured (component 1). in order to bridge the components together, we used a raspberry pi 4. this allowed our engineer to create software that could better handle high dimensionality of data, while also maintaining communication between our de-coupled components. this also gave us the freedom to implement more memory-intensive computations, such as storing track ids as they come in.\nfifth and last is our power source. we used an external battery that can produce a minimum of 20 watts under high load as well as outputting a frequency of at least 10,000 mamp per second.\nhow we built it\nour business development teammate first compiled research on existing tools for people with visual impairments. the most common -----> tool !!!  is of course the standard mobility cane. the average length of a mobility cane, or white cane, is half of a person\u2019s height. with the average height of adults being roughly between 5 feet and 4 inches to 5 feet and 9 inches, average length of a mobility cane is around 2 to 3 feet \u2013 or just under one meter. consulting with our engineer teammate, we considered ways to extend this distance to provide information of objects that are further away than the cane can reach. we then wrote our software to only provide insights of objects after this distance to avoid providing redundant information; our tool does not replace the mobility cane, it enhances it.\nthen our engineer did compiled research on existing, open source tools and appropriate hardware that would allow him to turn our idea into a reality. he invested in the equipment, including the oak-d technology, raspberry pi, and audio component. he then sat down to code the software that could bring theia to life.\nthe software\u2019s objection detection capacity provides the ability to detect a region within an image and classify it as a particular object. the software\u2019s object tracking capacity is similar to short-term memory in that it can determine if one object is brand new to the region or has been in the region for some time. by combining these two paradigms, we successfully created a system that allows a person with a visual disability to receive real-time alerts when an object is either approaching them or getting further way.\nchallenges we ran into\nwe went back and forth on the audio components, e.g. headphones versus bluetooth. our first idea was to use noise-cancelling headphones that could ensure clear, crisp audio for the user. however, we realized that in a real-world scenario, although this would provide sharp insight, it could be overbearing and block out all other stimuli for the wearer. this could make it difficult for theia users to participate in conversations, or even put them in harm\u2019s way. thus, we decided to go with the a more standard wired headphone or bluetooth technology format that could allow the user to use the eyeglasses that contain speakers on the side of the frames (arms), or to simply wear one speaker of the standard headphone. this would allow audio to be played or heard without blocking any other external audio (e.g. traffic, general surrounding noises, etc.). we want theia users to be able to maintain a balance between our technology and their own environment, without one overbearing the other.\naccomplishments that we're proud of\nwe are proud that we were able to start with a complex idea, one which had many different routes through which it could be accomplished in terms of hardware and software, and were able to execute it in a fashion that only required accessories that can fit in a pocket or small bag. we believe that in a commercial environment this would allow for more affordability and simplicity for consumers.\nboth members of our team have past experience with seeking solutions for social problems and/or underrepresented communities. further, as we are both on the cusp of the gen-z and millennial generations, we recognize that we and our peers are the future of society and feel it is our duty to work on solutions that are accessible to all. there is no better time to do this; as products such as raspberry pis become more affordable, software and robotics engineers are unlocking enough horsepower to develop advanced edge computing applications, meaning more portable technological solutions. with recent advancements in technology nearly everything is now possible, but not everything has been applied yet. being able to dedicate our time to a project that focuses on accessibility, and that could theoretically benefit many people, is rewarding in and of itself.\nwhat we learned\nwe learned a lot about existing hardware and open source tools that are just waiting for engineers to improve and supplement their capacity.\nwhile our engineer has previous experience in computer vision, integrating the audio component was new for him. inferencing at 30 frames per second means 1,800 possible notifications that could be sent to an audio device. if we sent all of these alerts, then the user would hear sounds 24/7 \u2013 deeming the product useless and maybe even dangerous. he had to teach himself an effective way to pick and choose amongst the frames while maintaining the tool\u2019s integrity to allow for more intelligent analytics, less stimuli and ultimately a better product.\nwhat's next for theia: object detection and depth perception using ml\nwe are interested in seeking solutions to power constraints. in order to power the raspberry pi and the camera together, we currently need to use an external battery that can produce a minimum of 20 watts under high load as well as outputting a frequency of at least 10,000 mamp per second. the program is able to run effectively, but the power discrepancy causes the hardware to slow down in certain circumstances such as when there are many objects in the frame or when objects are moving quickly. in the future, we would like to explore better and/or separate batteries so our components do not have to share a power source.\nadditionally, due to the onboard audio chip on the raspberry pi, we are currently limited to a bluetooth connection of 3 megabytes per second. although this is acceptable for an effective initial iteration, if this product were to be used at scale we would prefer a stronger bluetooth connection with a higher frequency to ensure zero data interruptions while it is being used. further, on the audio component, we would like to explore how an iphone app could improve our audio features. we believe an app would allow for de-coupling of several tasks currently performed by the raspberry pi and allow for a faster, smoother notification process. this could hypothetically allow us to provide theia users with more information than the distance at which an object is to them, including identification of the object. for example, instead of telling the user \u201cone meter,\u201d we could tell them \u201cperson in one meter\u201d or \u201ccar in one meter.\u201d our device already contains the ability to identify several objects, but we are not specifying in the notification due to limited capability of the raspberry pi. de-coupling this through use of an iphone app would allow us to provide longer verbiage to theia users without slowing down their experience.\nfinally, future iterations should be made even more wearable. we aimed to maximize this in our project by using bluetooth compatible eyeglasses with speakers in the frames, in addition to setting up an arm for the oak-d camera that could be placed inside a backpack. however, we found the camera needed adjusting after our user walked a lengthier distance. the user is also required to carry the external battery in their backpack. while our device is portable, the future of theia should be simplified even further for the ease of consumers.", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59507043}, {"Unnamed: 0": 7048, "autor": "A Comprehensive Guide to CNC Machining Service In Chennai", "date": null, "content": "What is CNC Machining?\nCNC Machining is a process wherein we instruct the plant machinery and tools to perform machining operations in a specific sequence. We provide the instructions through a computer program. This process is a successor to NC Machining wherein we provide instructions through punched cards or magnetic tapes. CNC Machining process involves subtractive manufacturing in which the layers of material are removed from blank/workpiece to produce customized parts.\nFramework of the process\nHow do we achieve this process ?\nThe framework of this process involves two steps :\nDeveloping the Computer Program : We need to prepare a set of instructions for the machine. Languages understood by machine are G-Code and M-code. Inserting the Program in CNC Machine : CNC Machine reads the instructions from left to right and from top to bottom and then manufactures the product. Let us understand each step in detail. We will start with the first step.\nComputer Program\nWhat is G-Code and M-Code ?\nG-Code and M-Codes are AlphaNumeric in nature.\nG-Code :\nG stands for Geometry. G-Code covers the Geometric and Cutting aspect for the workpiece. G-code tells how the tool/workpiece will perform motion, where it will move. By using G-Code we can specify coordinate systems, we can tell the machine which units to use i.e. English or Metric, we can specify the positioning system i.e. Absolute or Incremental, etc.\nBy using G-Code, we can achieve the desired geometry from a workpiece.\nM-Code :\nM stands for Miscellaneous / Machine. M-Code instructs machines to perform Non-Cutting / Non-Geometric operations like - Coolant ON/OFF, Tool Change, Spindle rotation CW/CCW, etc.\nOther AlphaNumerals used in CNC Program : G-Code also includes alphanumerals that start with alphabets apart from G.\nExample of the Computer Program :\nLet us take an example of drilling a hole. The workpiece is of square shape with dimensions 100 mm x 100 mm x 5mm.As shown in Fig 1. a hole of diameter 10 mm has to be drilled at the center of the workpiece. The coordinates of the workpiece and tool are shown in Fig 1. The workpiece and the tool are at the same height Z=0.\nHow does a CNC machine work?\nMCU converts the entered program into electrical signals and sends it to the amplifier to drive the motor. The motor operates the machine tool components to perform the cutting actions and other auxiliary functions. Feedback systems monitor the actual output and it sends the data to MCU. MCU calculates the error. If there is any error, MCU generates electrical signals for the motor and eliminates the error.\nFactors affecting cost of CNC Machining\nRaw materials: It includes the cost of raw material itself. The availability of raw material is also a factor that affects cost. Hard to find materials will be more expensive than materials that are easily available. Machinists buy material in blocks and they will charge for the whole use of a block. Customers must try to fit as many parts as they can in a block. Use of fewer blocks can save money in the long run. Machinability: Materials that have low machinability will take more time and effort to machine. Apart from time and effort, these materials also consume cutting fluids, electricity and cutting tools. Materials that have high machinability can be machined faster.", "link": "https://devpost.com/software/a-comprehensive-guide-to-cnc-machining-service-in-chennai", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what is cnc machining?\ncnc machining is a process wherein we instruct the plant machinery and tools to perform machining operations in a specific sequence. we provide the instructions through a computer program. this process is a successor to nc machining wherein we provide instructions through punched cards or magnetic tapes. cnc machining process involves subtractive manufacturing in which the layers of material are removed from blank/workpiece to produce customized parts.\nframework of the process\nhow do we achieve this process ?\nthe framework of this process involves two steps :\ndeveloping the computer program : we need to prepare a set of instructions for the machine. languages understood by machine are g-code and m-code. inserting the program in cnc machine : cnc machine reads the instructions from left to right and from top to bottom and then manufactures the product. let us understand each step in detail. we will start with the first step.\ncomputer program\nwhat is g-code and m-code ?\ng-code and m-codes are alphanumeric in nature.\ng-code :\ng stands for geometry. g-code covers the geometric and cutting aspect for the workpiece. g-code tells how the -----> tool !!! /workpiece will perform motion, where it will move. by using g-code we can specify coordinate systems, we can tell the machine which units to use i.e. english or metric, we can specify the positioning system i.e. absolute or incremental, etc.\nby using g-code, we can achieve the desired geometry from a workpiece.\nm-code :\nm stands for miscellaneous / machine. m-code instructs machines to perform non-cutting / non-geometric operations like - coolant on/off, tool change, spindle rotation cw/ccw, etc.\nother alphanumerals used in cnc program : g-code also includes alphanumerals that start with alphabets apart from g.\nexample of the computer program :\nlet us take an example of drilling a hole. the workpiece is of square shape with dimensions 100 mm x 100 mm x 5mm.as shown in fig 1. a hole of diameter 10 mm has to be drilled at the center of the workpiece. the coordinates of the workpiece and tool are shown in fig 1. the workpiece and the tool are at the same height z=0.\nhow does a cnc machine work?\nmcu converts the entered program into electrical signals and sends it to the amplifier to drive the motor. the motor operates the machine tool components to perform the cutting actions and other auxiliary functions. feedback systems monitor the actual output and it sends the data to mcu. mcu calculates the error. if there is any error, mcu generates electrical signals for the motor and eliminates the error.\nfactors affecting cost of cnc machining\nraw materials: it includes the cost of raw material itself. the availability of raw material is also a factor that affects cost. hard to find materials will be more expensive than materials that are easily available. machinists buy material in blocks and they will charge for the whole use of a block. customers must try to fit as many parts as they can in a block. use of fewer blocks can save money in the long run. machinability: materials that have low machinability will take more time and effort to machine. apart from time and effort, these materials also consume cutting fluids, electricity and cutting tools. materials that have high machinability can be machined faster.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59507048}, {"Unnamed: 0": 7060, "autor": "[Hacktahon 2021] Open-Sport", "date": null, "content": "Inspiration\nWe all have a favorite sport in our group and we noticed that it was difficult to obtain information about the competitions once the pandemia calmed down. That problem made us realize that the communication between the sports federation and the club was quite difficult and ineffective. To communicate the competition information, each federation has their own resources and each club has to adapt in their own way.\nWhat it does\nOur goal was to provide a simple tool that allows clubs and federations to communicate easily and efficiently. With this project, we open the sport federation competition database, and we retrieve those info with another architecture for sport club which desire those data. We added a little notification system as well, so that web user can get notified for competitions they are interested in.\nHow we built it\nFirst we had a design phase where we thought more deeply about how we would implement our solution and build the architecture. This phase improves our capabilities to link real use cases and technical solutions. We also had issues to collect data and find a suitable format for all of them without penalizing any sport and allow every federation to add any kind of information they may need. We set up different databases via the free trial in mongoDB Atlas. The second phase was obviously the implementation phase where we followed and adapted the specifications made in Anypoint Platform Design Center. We set up a domain to ease the development as well.\nChallenges we ran into\nThe main challenge of our team was the short time we had to get the project done with a very large difference of level inside the team. Some of our members already have experience with mulesoft and others were beginners.\nWhat we learned\nThanks to this Hackathon, we all became quite knowledgeable in Mulesoft and Project Management\nThen we had to choose which technology we could pair with Mulesoft for the database and the front-end interface. During this phase we had to make a choice between all the connectors Mulesoft already provides. For connecting our database we use realm for the federation mocked database via API and mongodb connectors for the mocked club database.\nWhat's next for [Hacktahon 2021] Open-Sport\nDuring all the project we thought about many other functionality we could add with more time. Like calculate distance between club and the competition location, put the database in dockers or send email directly to the club client.", "link": "https://devpost.com/software/hacktahon-2021-sport-competition-notification", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe all have a favorite sport in our group and we noticed that it was difficult to obtain information about the competitions once the pandemia calmed down. that problem made us realize that the communication between the sports federation and the club was quite difficult and ineffective. to communicate the competition information, each federation has their own resources and each club has to adapt in their own way.\nwhat it does\nour goal was to provide a simple -----> tool !!!  that allows clubs and federations to communicate easily and efficiently. with this project, we open the sport federation competition database, and we retrieve those info with another architecture for sport club which desire those data. we added a little notification system as well, so that web user can get notified for competitions they are interested in.\nhow we built it\nfirst we had a design phase where we thought more deeply about how we would implement our solution and build the architecture. this phase improves our capabilities to link real use cases and technical solutions. we also had issues to collect data and find a suitable format for all of them without penalizing any sport and allow every federation to add any kind of information they may need. we set up different databases via the free trial in mongodb atlas. the second phase was obviously the implementation phase where we followed and adapted the specifications made in anypoint platform design center. we set up a domain to ease the development as well.\nchallenges we ran into\nthe main challenge of our team was the short time we had to get the project done with a very large difference of level inside the team. some of our members already have experience with mulesoft and others were beginners.\nwhat we learned\nthanks to this hackathon, we all became quite knowledgeable in mulesoft and project management\nthen we had to choose which technology we could pair with mulesoft for the database and the front-end interface. during this phase we had to make a choice between all the connectors mulesoft already provides. for connecting our database we use realm for the federation mocked database via api and mongodb connectors for the mocked club database.\nwhat's next for [hacktahon 2021] open-sport\nduring all the project we thought about many other functionality we could add with more time. like calculate distance between club and the competition location, put the database in dockers or send email directly to the club client.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59507060}, {"Unnamed: 0": 7064, "autor": "downloading...", "date": null, "content": "Concept\nEvery machine learning starts from downloading...\nInspiration\nWe are young, dumb, and free, so we want to get ourself busy.\nWe are a group of students who are interested in applying Machine Learning into what we are studying (Material Sciences and Chemistry). This year, TMLCC is focused on predicting CO2 absorption by Metal Organic Frameworks (MOFs) .\nIt was a good chance to apply chemical intuition into determining the CO2 capture capability of MOF-based materials\nIt gonna be fun.\nHow we built it\nThe approach taken in our group can be divided into 2 parts: Features Extraction and Modeling\nFeatures Extraction\nThere are feature extraction based on CIFs and machine learning model predicting the CO2 working capacity of Metal-Organic Frameworks (MOFs)\nOriginal - volume [A^3], weight [u], void_volume [cm^3/g], CO2/N2_selectivity, heat_adsorption_CO2_P0.15bar_T298K [kcal/mol], all Linkers\nFrom .cif file - Canonical SMILES, Surface Area, VSA, Topology, Pore Area, Void Fraction, Overlapping sphere, Grid point approach, Pore size distribution (These were calculate by supercomputer from the National Supercomputing Centre, Singapore)\nFrom Coulomb Matrix - Functional group 1 eigen 1-3, Functional group 2 eigen 1-3, Metal linkers 1 eigen 1-10, Organic linkers 1 eigen 1-10, Organic linkers 1 eigen 1-10.\nModels\nThe most exciting part, maybe?\nBig thanks to these libraries\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\n# Find the imposter\nimport torch\nimport tensorflow as tf\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom sklearn.preprocessing import MinMaxScaler\nfrom torch.utils.data import DataLoader, TensorDataset\nUsed a tool called lazy prediction to save time while experimenting with various regression techniques. We also tried to perform the regression using a neural network. As a result, LightGBM and Catboost offered the most accurate prediction.\nFound the most important feature out of all of them. We discovered 8 more features that have an influence on our regression model.\nSome delicious codes\ntrain_data['poreV_OSEp2'] = train_data['poreV_OSE']**2\ntrain_data['head_selectivity'] = train_data['CO2/N2_selectivity'] / train_data['heat_adsorption_CO2_P0.15bar_T298K [kcal/mol]']\ntrain_data['inverse_heat_adsorption'] = 1 / train_data['heat_adsorption_CO2_P0.15bar_T298K [kcal/mol]']\ntrain_data['specific_vol'] = 1 / train_data[\"density\"]\ntrain_data['mof_vol'] = train_data['specific_vol'] / train_data['void_volume [cm^3/g]']\ntrain_data['selectivity_heat'] = train_data['heat_adsorption_CO2_P0.15bar_T298K [kcal/mol]'] / train_data['CO2/N2_selectivity']\ntrain_data['sum_mo2'] = train_data['metal_e1'] + train_data['orlink1_e1'] + train_data['orlink2_e1']\nAnother technique that was inspired from the form of the molecules. As we can see, the molecule can be seen as node and edge. From our original data, it was not reasonable to use a 2D convolutional network without extracting the 2D data. So, we used the canonical smiles to find the molecular fingerprint, adjacency matrix, and molecular size. And train the Graph Neural Network model or GNN.\nGraph Convolutional Networks or GCN. Like GNN, we extracted the features out of canonical smiles and used the Convolutional Neural Networks to train to model inorder to receive the information about neighborhood relationship.\nMolecular Attention Transformers or MAT, which were inspired by the previous two approaches, were also tested in the data. Because this model includes an attention layer that captures the locational region and uses position-wise feed forward to highlight the area of the molecule that is critical to the prediction.\nSummary We use both original and generated features to the boosting technique for finding the important features. And then recode the GNN, GCN, and MAT from scratch with multiple network inputs of extracted smiles (2D, 3D features) and our features. And train in both Colab Pro + and Deepnote Pro Machine.\nWe hope these models will make good results. But everything wasn't gone as plan...\nBut at least we have tried!\nAt that time the model is already learned. Use the output of the training models as one of the features in LightGBM and CatBoost.\nChallenges we ran into\nLaziness\nFeatures extraction is too long to be done serially with provided resources.\nOn this note, we faced an issue implementing multiprocessing modules in parallelization as few samples are skipped with no error given and thus resulting in missing points once the result is exported. We applied a band-aid solution to this issue by recalculating missing points serially.\nThe calculation of dipole moments and polarizability, hypothetically relates to van der Waals binding between surface and nonpolar molecules such as CO2, is based on DFT, which are too computationally expensive to be done, and thus are dropped from our project.\n\u0e17\u0e49\u0e2d\u0e41\u0e17\u0e49 \u0e17\u0e33\u0e44\u0e21\u0e48\u0e44\u0e14\u0e49\nDeepnote cut out the run time, so... sad, and go back to above bullet point. Moving on in circles.\nAccomplishments that we're proud of\nEverything\nYou know, the moment when your codes were all working. One step accomplishment was there.\nWhat we learned\nEverything about this competition was new. So that's what we learned.\nWhat's next for downloading...\nGood sleep\nWrite the medium in order to be some knowledge to the world. (Woah!)\nMiscellaneous\nReferences\nBucior, B. J.; Rosen, A. S.; Haranczyk, M.; Yao, Z.; Ziebel, M. E.; Farha, O. K.; Hupp, J. T.; Siepmann, J. I.; Aspuru-Guzik, A.; Snurr, R. Q., Identification Schemes for Metal\u2013Organic Frameworks To Enable Rapid Search and Cheminformatics Analysis. Crystal Growth & Design 2019, 19 (11), 6682-6697.\nTrepte, K.; Schwalbe, S., porE: A code for deterministic and systematic analyses of porosities. Journal of Computational Chemistry 2021, 42 (9), 630-643.\nMy everything\nhttps://paperswithcode.com/\nThe medium\nhttps://link.medium.com/4GwJ7dBfXjb\nhttps://link.medium.com/eUF7679eXjb\nhttps://link.medium.com/qSnpMe3eXjb\nhttps://www.nature.com/articles/s41524-021-00554-0.pdf\nhttps://chemintelligence.com/blog/machine-learning-descriptors-molecules\nGithub link\nhttps://github.com/cherkung/TMLCC-downloading-public\nGNN, GCN, Several MOFS\nhttps://github.com/thunlp/GNNPapers\nhttps://github.com/masashitsubaki/molecularGNN_3Dstructure\nhttps://github.com/divelab/MoleculeX/blob/molx/Molecule3D/build_yours.ipynb\nhttps://github.com/deepchem/deepchem/blob/master/deepchem/models/torch_models/gcn.py\nhttps://github.com/scidatasoft/mof/blob/master/matfeaturizers.py\nhttps://github.com/arosen93/QMOF\nTransformers\nhttps://github.com/ardigen/MAT\nhttps://arxiv.org/abs/2002.08264\nhttps://pubs.acs.org/doi/10.1021/acscentsci.9b00576\nAcknowledgements\nThe computational work for this article was partially performed on resources of the National Supercomputing Centre, Singapore (https://www.nscc.sg)\nGoogle Colab Pro +\nDeepnote Pro Machine\nMore details\nIn the video with our text-to-speech friend! Enjoy!", "link": "https://devpost.com/software/downloading-udh9qy", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "concept\nevery machine learning starts from downloading...\ninspiration\nwe are young, dumb, and free, so we want to get ourself busy.\nwe are a group of students who are interested in applying machine learning into what we are studying (material sciences and chemistry). this year, tmlcc is focused on predicting co2 absorption by metal organic frameworks (mofs) .\nit was a good chance to apply chemical intuition into determining the co2 capture capability of mof-based materials\nit gonna be fun.\nhow we built it\nthe approach taken in our group can be divided into 2 parts: features extraction and modeling\nfeatures extraction\nthere are feature extraction based on cifs and machine learning model predicting the co2 working capacity of metal-organic frameworks (mofs)\noriginal - volume [a^3], weight [u], void_volume [cm^3/g], co2/n2_selectivity, heat_adsorption_co2_p0.15bar_t298k [kcal/mol], all linkers\nfrom .cif file - canonical smiles, surface area, vsa, topology, pore area, void fraction, overlapping sphere, grid point approach, pore size distribution (these were calculate by supercomputer from the national supercomputing centre, singapore)\nfrom coulomb matrix - functional group 1 eigen 1-3, functional group 2 eigen 1-3, metal linkers 1 eigen 1-10, organic linkers 1 eigen 1-10, organic linkers 1 eigen 1-10.\nmodels\nthe most exciting part, maybe?\nbig thanks to these libraries\nfrom lightgbm import lgbmregressor\nfrom catboost import catboostregressor\n# find the imposter\nimport torch\nimport tensorflow as tf\nimport torch.nn as nn\nimport torch.nn.functional as f\nfrom sklearn.preprocessing import minmaxscaler\nfrom torch.utils.data import dataloader, tensordataset\nused a -----> tool !!!  called lazy prediction to save time while experimenting with various regression techniques. we also tried to perform the regression using a neural network. as a result, lightgbm and catboost offered the most accurate prediction.\nfound the most important feature out of all of them. we discovered 8 more features that have an influence on our regression model.\nsome delicious codes\ntrain_data['porev_osep2'] = train_data['porev_ose']**2\ntrain_data['head_selectivity'] = train_data['co2/n2_selectivity'] / train_data['heat_adsorption_co2_p0.15bar_t298k [kcal/mol]']\ntrain_data['inverse_heat_adsorption'] = 1 / train_data['heat_adsorption_co2_p0.15bar_t298k [kcal/mol]']\ntrain_data['specific_vol'] = 1 / train_data[\"density\"]\ntrain_data['mof_vol'] = train_data['specific_vol'] / train_data['void_volume [cm^3/g]']\ntrain_data['selectivity_heat'] = train_data['heat_adsorption_co2_p0.15bar_t298k [kcal/mol]'] / train_data['co2/n2_selectivity']\ntrain_data['sum_mo2'] = train_data['metal_e1'] + train_data['orlink1_e1'] + train_data['orlink2_e1']\nanother technique that was inspired from the form of the molecules. as we can see, the molecule can be seen as node and edge. from our original data, it was not reasonable to use a 2d convolutional network without extracting the 2d data. so, we used the canonical smiles to find the molecular fingerprint, adjacency matrix, and molecular size. and train the graph neural network model or gnn.\ngraph convolutional networks or gcn. like gnn, we extracted the features out of canonical smiles and used the convolutional neural networks to train to model inorder to receive the information about neighborhood relationship.\nmolecular attention transformers or mat, which were inspired by the previous two approaches, were also tested in the data. because this model includes an attention layer that captures the locational region and uses position-wise feed forward to highlight the area of the molecule that is critical to the prediction.\nsummary we use both original and generated features to the boosting technique for finding the important features. and then recode the gnn, gcn, and mat from scratch with multiple network inputs of extracted smiles (2d, 3d features) and our features. and train in both colab pro + and deepnote pro machine.\nwe hope these models will make good results. but everything wasn't gone as plan...\nbut at least we have tried!\nat that time the model is already learned. use the output of the training models as one of the features in lightgbm and catboost.\nchallenges we ran into\nlaziness\nfeatures extraction is too long to be done serially with provided resources.\non this note, we faced an issue implementing multiprocessing modules in parallelization as few samples are skipped with no error given and thus resulting in missing points once the result is exported. we applied a band-aid solution to this issue by recalculating missing points serially.\nthe calculation of dipole moments and polarizability, hypothetically relates to van der waals binding between surface and nonpolar molecules such as co2, is based on dft, which are too computationally expensive to be done, and thus are dropped from our project.\n\u0e17\u0e49\u0e2d\u0e41\u0e17\u0e49 \u0e17\u0e33\u0e44\u0e21\u0e48\u0e44\u0e14\u0e49\ndeepnote cut out the run time, so... sad, and go back to above bullet point. moving on in circles.\naccomplishments that we're proud of\neverything\nyou know, the moment when your codes were all working. one step accomplishment was there.\nwhat we learned\neverything about this competition was new. so that's what we learned.\nwhat's next for downloading...\ngood sleep\nwrite the medium in order to be some knowledge to the world. (woah!)\nmiscellaneous\nreferences\nbucior, b. j.; rosen, a. s.; haranczyk, m.; yao, z.; ziebel, m. e.; farha, o. k.; hupp, j. t.; siepmann, j. i.; aspuru-guzik, a.; snurr, r. q., identification schemes for metal\u2013organic frameworks to enable rapid search and cheminformatics analysis. crystal growth & design 2019, 19 (11), 6682-6697.\ntrepte, k.; schwalbe, s., pore: a code for deterministic and systematic analyses of porosities. journal of computational chemistry 2021, 42 (9), 630-643.\nmy everything\nhttps://paperswithcode.com/\nthe medium\nhttps://link.medium.com/4gwj7dbfxjb\nhttps://link.medium.com/euf7679exjb\nhttps://link.medium.com/qsnpme3exjb\nhttps://www.nature.com/articles/s41524-021-00554-0.pdf\nhttps://chemintelligence.com/blog/machine-learning-descriptors-molecules\ngithub link\nhttps://github.com/cherkung/tmlcc-downloading-public\ngnn, gcn, several mofs\nhttps://github.com/thunlp/gnnpapers\nhttps://github.com/masashitsubaki/moleculargnn_3dstructure\nhttps://github.com/divelab/moleculex/blob/molx/molecule3d/build_yours.ipynb\nhttps://github.com/deepchem/deepchem/blob/master/deepchem/models/torch_models/gcn.py\nhttps://github.com/scidatasoft/mof/blob/master/matfeaturizers.py\nhttps://github.com/arosen93/qmof\ntransformers\nhttps://github.com/ardigen/mat\nhttps://arxiv.org/abs/2002.08264\nhttps://pubs.acs.org/doi/10.1021/acscentsci.9b00576\nacknowledgements\nthe computational work for this article was partially performed on resources of the national supercomputing centre, singapore (https://www.nscc.sg)\ngoogle colab pro +\ndeepnote pro machine\nmore details\nin the video with our text-to-speech friend! enjoy!", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59507064}, {"Unnamed: 0": 7171, "autor": "The World of Wreckage: Analyzing Salvaged/Damaged Vehicles", "date": null, "content": "Inspiration\nWe noticed that although there is a lot of information readily available on a majority of vehicles in the market, there is a blind spot regarding salvaged vehicles, so we decided to help fill the void.\nWhat it does\nThis project extracts and compiles data in a structured format. It allows for ease of access and ease of readability, so that way anyone could understand what the data is about.\nHow we built it\nUsing Python, we created a web scraper that collects price, damage, odometer reading and other information on salvaged cars from Salvage Bid that would then be placed into a .csv file.\nChallenges we ran into\nA major issue we ran into during the project is the computing resources needed to collect and implement the large data sets using a web scraping tool.\nAccomplishments that we're proud of\nWe are proud of the successful implementation of a web scraper for collecting data from Salvage Bid, since we were all new to the realm of web scraping.\nWhat we learned\nThis project gave us more insights into web scraping, and how we could compile and analyze useful data in a readable format. This data could then be implemented in various ways; furthermore, we discovered how computing resources are a major factor in how much data a web scraper can collect.\nWhat's next for The World of Wreckage: Analyzing Salvaged/Damaged Vehicles\nThe next step would be to move the web scraper from operating and compiling the collected data on a physical host system to operating on a cloud-based system. This would raise the headroom the web scraper has on the amount of data the web scraper collects. Furthermore, we would like to implement a machine learning program to analyze the factors in the data from the web scraper such as odometer reading and the type of damage that would predict the sale of the car and allow people who flip cars to then determine the amount of potential profit they would make from the flip.", "link": "https://devpost.com/software/the-world-of-wreckage-analyzing-salvaged-damaged-vehicles", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe noticed that although there is a lot of information readily available on a majority of vehicles in the market, there is a blind spot regarding salvaged vehicles, so we decided to help fill the void.\nwhat it does\nthis project extracts and compiles data in a structured format. it allows for ease of access and ease of readability, so that way anyone could understand what the data is about.\nhow we built it\nusing python, we created a web scraper that collects price, damage, odometer reading and other information on salvaged cars from salvage bid that would then be placed into a .csv file.\nchallenges we ran into\na major issue we ran into during the project is the computing resources needed to collect and implement the large data sets using a web scraping -----> tool !!! .\naccomplishments that we're proud of\nwe are proud of the successful implementation of a web scraper for collecting data from salvage bid, since we were all new to the realm of web scraping.\nwhat we learned\nthis project gave us more insights into web scraping, and how we could compile and analyze useful data in a readable format. this data could then be implemented in various ways; furthermore, we discovered how computing resources are a major factor in how much data a web scraper can collect.\nwhat's next for the world of wreckage: analyzing salvaged/damaged vehicles\nthe next step would be to move the web scraper from operating and compiling the collected data on a physical host system to operating on a cloud-based system. this would raise the headroom the web scraper has on the amount of data the web scraper collects. furthermore, we would like to implement a machine learning program to analyze the factors in the data from the web scraper such as odometer reading and the type of damage that would predict the sale of the car and allow people who flip cars to then determine the amount of potential profit they would make from the flip.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59507171}, {"Unnamed: 0": 7184, "autor": "Celonis Challenge: Delivery Process Analytics", "date": null, "content": "Inspiration\nUsing Celonis to apply Process Mining for discovering, monitoring, and improving business processes in an accurate and analytical methods for Pizzeria Mama Mia Delivery System.\nWhat it does\nProcess Mining gives objective, fact-based insights in delivery process to find relationship between customer satisfactory rating and pizza sale throughput time to identify process issues, reduce anomalies and maximize profit.\nHow we built it\nWith data sets provided by Celonis, Pizzeria Mama Mia's Case, Event, and Customer tables were uploaded to Celonis Data Pools, then using Data Modeling tool, we were able to connect different data set in relational dataset. Data was then loaded into the workspace, and we implemented different components, process variations, and KPI's to identify and investigate delivery process.\nChallenges we ran into\nDifferent challenges were faced when learning the different components of Celonis and digging deeper into PQL to implement more insightful and complex KPI's and unique components to our process findings.\nAccomplishments that we're proud of\nLearn Celonis Data Processing techniques and tools to put meaning into data by enabling business owners and managers to fully understand their business process and efficient approaches with automation.\nWhat we learned\nProcess Mining, Celonis Technology, PQL, and Business Performance and Analytics.\nWhat's next for Celonis Challenge: Delivery Process Analytics\nObtain more details about the phone and online service of the pizzeria. For example, what type of training their employees go through since we noticed a lot of delay and call backs in the online and phone orders. More details like the condition of their scooters could also helps us understand how we can improve the transportation to see if there's also a delay here.", "link": "https://devpost.com/software/celonis-challenge-delivery-process-analytics", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nusing celonis to apply process mining for discovering, monitoring, and improving business processes in an accurate and analytical methods for pizzeria mama mia delivery system.\nwhat it does\nprocess mining gives objective, fact-based insights in delivery process to find relationship between customer satisfactory rating and pizza sale throughput time to identify process issues, reduce anomalies and maximize profit.\nhow we built it\nwith data sets provided by celonis, pizzeria mama mia's case, event, and customer tables were uploaded to celonis data pools, then using data modeling -----> tool !!! , we were able to connect different data set in relational dataset. data was then loaded into the workspace, and we implemented different components, process variations, and kpi's to identify and investigate delivery process.\nchallenges we ran into\ndifferent challenges were faced when learning the different components of celonis and digging deeper into pql to implement more insightful and complex kpi's and unique components to our process findings.\naccomplishments that we're proud of\nlearn celonis data processing techniques and tools to put meaning into data by enabling business owners and managers to fully understand their business process and efficient approaches with automation.\nwhat we learned\nprocess mining, celonis technology, pql, and business performance and analytics.\nwhat's next for celonis challenge: delivery process analytics\nobtain more details about the phone and online service of the pizzeria. for example, what type of training their employees go through since we noticed a lot of delay and call backs in the online and phone orders. more details like the condition of their scooters could also helps us understand how we can improve the transportation to see if there's also a delay here.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59507184}, {"Unnamed: 0": 7216, "autor": "Impacts of Environment on US Stock Price", "date": null, "content": "Inspiration\nThe stock market has always been a dynamic presentation of both economic activities and energy consumption across countries and economic sectors. Stock market activities are hence expected to have a significant effect on environment indicators such as CO2 emissions, greenhouse gas emissions etc.\nMany studies find that weather has a close relationship with human\u2019s mood and behavior. Hence, weather factors might also have an impact on stock return and trading volume.\nWe are interested in exploring how environment and weather indicators and stock markets are correlated and tied together. Our results can enable us to build models to predict risk events and discover investment opportunities based on environmental variations\nWhat you learned:\nWorking with a new data warehouse platform and new data. None of us has worked with Snowflake before so it was a hurdle at the beginning to get used to the platform and extract data we need for our analysis. Yet it was a great learning experience as we all learned so much from looking into it together. Representatives from Goldman Sachs were very approachable and helpful with that too (Thanks Janeen!). We also built up our domain knowledge when researching the problem to understand and analyze the given financial and environmental data.\nWork with big data and collaborative coding\nCollaboration! Our team never met before and it was a bit challenging to communicate given different time zones and virtual-in-person situation. Yet we made a great team and helped each other learn a lot about technical knowledge, project management, and communication.\nHow you built your project\nGet used to the data set and research the problem, define our hypothesis\nGet familiar with the Snowflake platform and unload data to Python for analysis\nClean data, EDA, deciding on visualizations\nBuilt the interactive programming tool\nTest product\nFurther details are presented in the slide deck attached\nChallenges\nCommunication online/ in-person, plus (3) different timezones :)!\nUnfamiliarity with new data storage platform\nDeal with limitations in the dataset:\nLack of data documentation in the data sources made it difficult to understand the variables and interpret the analysis results. We missed a few instances of data cleaning that were quite obvious to us late during the project. For example, there were some instances where the \u201cHigh\u201d in a single day was in the millions. Cross checking with the history online showed this to be obviously false. Similarly, when comparing to some of the indicators, some features like \u201cPrice\u201d are averaged over entire years. This is a gross oversimplification of a sensitive market such as the stock market, but this is helpful in seeing a general trend.\nEnvironment indicators data were only available on a yearly estimates, while stock prices are volatile by day/hours. This makes it more challenging to correctly identify the correlation of these two factors.", "link": "https://devpost.com/software/impacts-of-environment-on-us-stock-price", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nthe stock market has always been a dynamic presentation of both economic activities and energy consumption across countries and economic sectors. stock market activities are hence expected to have a significant effect on environment indicators such as co2 emissions, greenhouse gas emissions etc.\nmany studies find that weather has a close relationship with human\u2019s mood and behavior. hence, weather factors might also have an impact on stock return and trading volume.\nwe are interested in exploring how environment and weather indicators and stock markets are correlated and tied together. our results can enable us to build models to predict risk events and discover investment opportunities based on environmental variations\nwhat you learned:\nworking with a new data warehouse platform and new data. none of us has worked with snowflake before so it was a hurdle at the beginning to get used to the platform and extract data we need for our analysis. yet it was a great learning experience as we all learned so much from looking into it together. representatives from goldman sachs were very approachable and helpful with that too (thanks janeen!). we also built up our domain knowledge when researching the problem to understand and analyze the given financial and environmental data.\nwork with big data and collaborative coding\ncollaboration! our team never met before and it was a bit challenging to communicate given different time zones and virtual-in-person situation. yet we made a great team and helped each other learn a lot about technical knowledge, project management, and communication.\nhow you built your project\nget used to the data set and research the problem, define our hypothesis\nget familiar with the snowflake platform and unload data to python for analysis\nclean data, eda, deciding on visualizations\nbuilt the interactive programming -----> tool !!! \ntest product\nfurther details are presented in the slide deck attached\nchallenges\ncommunication online/ in-person, plus (3) different timezones :)!\nunfamiliarity with new data storage platform\ndeal with limitations in the dataset:\nlack of data documentation in the data sources made it difficult to understand the variables and interpret the analysis results. we missed a few instances of data cleaning that were quite obvious to us late during the project. for example, there were some instances where the \u201chigh\u201d in a single day was in the millions. cross checking with the history online showed this to be obviously false. similarly, when comparing to some of the indicators, some features like \u201cprice\u201d are averaged over entire years. this is a gross oversimplification of a sensitive market such as the stock market, but this is helpful in seeing a general trend.\nenvironment indicators data were only available on a yearly estimates, while stock prices are volatile by day/hours. this makes it more challenging to correctly identify the correlation of these two factors.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59507216}, {"Unnamed: 0": 7237, "autor": "Heal Map", "date": null, "content": "Inspiration\nThe whole project was inspired not only from my academic focus on machine learning, but also cooperation with geographers recently to work on research projects.\nWhat it does\nThis helps people find locations of institutions that benefit their well-being, such as hospitals, clinics, pharmacies, and fitness centers. There are two (2) main parts. The first part simply asks for a location in text (city is enough) and displays an interactive map with found locations marked down. A distance radar chart of the top 5 closest locations is also displayed for filtering. The other part does the same thing conceptually, but this time an NLP model is used to recognize organization and location entities from potentially longer texts e.g. memo.\nHow we built it\nThe most important components were several Google Cloud APIs: Geocoding, Places, Distance Matrix.\nChallenges we ran into\nUsing so many different tools and libraries.\nAccomplishments that we're proud of\nThe app works without complications!\nWhat we learned\nGoogle Cloud APIs for geography and the gcloud CLI tool.\nWhat's next for Heal Map\nAdd more advanced markers with metadata such as website and hours of operations.", "link": "https://devpost.com/software/heal-map", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nthe whole project was inspired not only from my academic focus on machine learning, but also cooperation with geographers recently to work on research projects.\nwhat it does\nthis helps people find locations of institutions that benefit their well-being, such as hospitals, clinics, pharmacies, and fitness centers. there are two (2) main parts. the first part simply asks for a location in text (city is enough) and displays an interactive map with found locations marked down. a distance radar chart of the top 5 closest locations is also displayed for filtering. the other part does the same thing conceptually, but this time an nlp model is used to recognize organization and location entities from potentially longer texts e.g. memo.\nhow we built it\nthe most important components were several google cloud apis: geocoding, places, distance matrix.\nchallenges we ran into\nusing so many different tools and libraries.\naccomplishments that we're proud of\nthe app works without complications!\nwhat we learned\ngoogle cloud apis for geography and the gcloud cli -----> tool !!! .\nwhat's next for heal map\nadd more advanced markers with metadata such as website and hours of operations.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59507237}, {"Unnamed: 0": 7266, "autor": "The Generally Hard(but fun!) General Motors Challenge!", "date": null, "content": "Inspiration\nI've decided on this particular project because it's the closest to my interest, as I am enrolled in mechatronics engineering, and this project will help me get more experienced in my craft.\nWhat it does\nThe goal was to estimate the position of a bolt in 3d space in order for a robot arm to reach it. This came in 2 parts, pose-estimation, and robot control.\nHow we built it\nThis project provided various functions, data, and resources that greatly reduced the load on this project. The first step of the project was to turn the depth image of the bolt head into a point cloud. The point cloud allows us to easily manipulate the points on the object to rotate and move it around. The point cloud was translated to the origin to prepare for the next step. I then used the segment_plane function to locate the flat side of the bolt head. This was the most crucial part of the code because it allows us to find the plane equation of this flat side. From this equation, I can find the rotation of the plane respective to the x-axis. Now that we have the angles, we can rotate the object to face the x-axis, and now find the real translation as opposed to just the center of the untransformed object. We now know the angle of rotation and the translation to get to the bolt head in 3d space!\nChallenges we ran into\nThis project was pretty heavy but thought-provoking, which lead to many challenges throughout. This was my first time working with point clouds and open3d, so I had a lot of learning to do. I spent a lot of time trying to figure out where to start, as stated, I have never done this before. My first plan was to try to get two clusters of data, one for the bolt head, and one for the threaded end. I had no idea how to do that, so on the next plan. I then tried to get the closest point, representing the bolt head, and the furthest point, the threaded end, on the x-axis. With these two points, I tried to find the rough angle of the bolt, as close as I could. This lead to many issues as I often got very inconsistent results. I then delve further into research, as I found the answer staring at me in the face... a segmented plane. This quickly solved many issues I've had before, and I was able to find the answer after that. Unfortunately, I was unable to complete the robot control portion, as I am a one-man team, and I spent most of my time doing pose_esimation. I believe with enough time and research, I could have figured that out too!\nAccomplishments that we're proud of\nThe best thing that came to me was figuring out using the segmented plane function. As stated before, this was lurking the time while doing this project, as it was the most obvious solution, and it was such a rush of eureka when I finally figured it out.\nWhat we learned\nThe open3d library and matrix transformations. Open3d is such a powerful tool and library, which if I had known about a lot sooner, I could have come up with a better solution with more elegant code. Another thing was these transformations. When first getting into this, these concepts went way over my head. Fortunately, throughout the project, things kept clicking in my head as I was better understanding what I was doing, and more importantly, what the code was doing.\nWhat's next for General Motors Challenge\nI hope to improve my codebase to get a better score with a better translation. This was a very challenging project to work on, and I'm glad I got this experience to further improve myself!\nName: Aaron Bell Discord: YoungBreezy", "link": "https://devpost.com/software/general-motors-challenge", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ni've decided on this particular project because it's the closest to my interest, as i am enrolled in mechatronics engineering, and this project will help me get more experienced in my craft.\nwhat it does\nthe goal was to estimate the position of a bolt in 3d space in order for a robot arm to reach it. this came in 2 parts, pose-estimation, and robot control.\nhow we built it\nthis project provided various functions, data, and resources that greatly reduced the load on this project. the first step of the project was to turn the depth image of the bolt head into a point cloud. the point cloud allows us to easily manipulate the points on the object to rotate and move it around. the point cloud was translated to the origin to prepare for the next step. i then used the segment_plane function to locate the flat side of the bolt head. this was the most crucial part of the code because it allows us to find the plane equation of this flat side. from this equation, i can find the rotation of the plane respective to the x-axis. now that we have the angles, we can rotate the object to face the x-axis, and now find the real translation as opposed to just the center of the untransformed object. we now know the angle of rotation and the translation to get to the bolt head in 3d space!\nchallenges we ran into\nthis project was pretty heavy but thought-provoking, which lead to many challenges throughout. this was my first time working with point clouds and open3d, so i had a lot of learning to do. i spent a lot of time trying to figure out where to start, as stated, i have never done this before. my first plan was to try to get two clusters of data, one for the bolt head, and one for the threaded end. i had no idea how to do that, so on the next plan. i then tried to get the closest point, representing the bolt head, and the furthest point, the threaded end, on the x-axis. with these two points, i tried to find the rough angle of the bolt, as close as i could. this lead to many issues as i often got very inconsistent results. i then delve further into research, as i found the answer staring at me in the face... a segmented plane. this quickly solved many issues i've had before, and i was able to find the answer after that. unfortunately, i was unable to complete the robot control portion, as i am a one-man team, and i spent most of my time doing pose_esimation. i believe with enough time and research, i could have figured that out too!\naccomplishments that we're proud of\nthe best thing that came to me was figuring out using the segmented plane function. as stated before, this was lurking the time while doing this project, as it was the most obvious solution, and it was such a rush of eureka when i finally figured it out.\nwhat we learned\nthe open3d library and matrix transformations. open3d is such a powerful -----> tool !!!  and library, which if i had known about a lot sooner, i could have come up with a better solution with more elegant code. another thing was these transformations. when first getting into this, these concepts went way over my head. fortunately, throughout the project, things kept clicking in my head as i was better understanding what i was doing, and more importantly, what the code was doing.\nwhat's next for general motors challenge\ni hope to improve my codebase to get a better score with a better translation. this was a very challenging project to work on, and i'm glad i got this experience to further improve myself!\nname: aaron bell discord: youngbreezy", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59507266}, {"Unnamed: 0": 7274, "autor": "ASL Speech to Text", "date": null, "content": "Inspiration\nWe wanted to provide an easy-to-use tool for purposes such as learning ASL or translating from audio to a form an ASL speaker can better understand.\nWhat it does\nThe tool allows one to record their voice or other audio sources, and once one stops recording, it will use AssemblyAI's API to generate a transcript of that recording. It then displays ASL hand signs along with that transcript, with the option to cycle between 3 different dictionaries of interpreters. It also performs an ML-based sentiment analysis to predict what emotions were felt by the speaker.\nHow we built it\nThe audio input and connection to the AssemblyAI API were done through javascript. The text received from the AssemblyAI API will be sent to the sentiment analysis API, built by flask, hosted by Heroku.\nChallenges we ran into\nConnecting the various resources we had together was the main challenge. It would have been simpler with each part separated: a \"record and save your voice\" button, an \"upload your voice to AssemblyAI\" button, some links that say \"click here to go to someone's ASL dictionary site\" and \"click here to try out a sentiment analysis AI\". It was quite a tedious set up communication between these different systems, exacerbated by the short deadline and made worse still by the time zone disparities within our team. I don't think there was so much as an hour when all 4 of us were talking together. For sentiment analysis, we've run into overfitting problems and an imbalanced dataset which lots of times had been wasted. However, we managed to clean up the dataset and boost up the accuracy to >83%.\nAccomplishments that we're proud of\nThe pride of an accomplishment goes hand-in-hand with its difficulty. We managed to bring all the features that challenged us together into a single page. It significantly reduces the hassle for a user compared to what there would be without putting in the effort to streamline those features.\nWhat we learned\nTheo: I came into this without having used javascript before, so that was a learning experience for me. While working on it, using the AssemblyAI API seemed difficult, but in retrospect, I was able to get it working within a few hours of work, and in a programming language I hadn't used before at that.\nAlex: I discovered that web development is actually really cool and rewarding to do. I thought it would be a bore, but seeing it work in the end: using our product felt great!\nHub: I've never hosted a rest API on the cloud before and had little experience with NLP. However, I managed to train the NLP model in time and host it.\nZeaan: I worked on the frontend of the website and helped on hosting the flask sentiment analysis api, and built on my past experience with those to explore new options.\nWhat's next for ASL Speech to Text\nCreate our own dataset of hearing-impaired people signing and implement grammar in the algorithm.\nImprove sentiment analysis model to multi-class classification which includes anger, happy, sad, etc.", "link": "https://devpost.com/software/asl-speech-to-text", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe wanted to provide an easy-to-use -----> tool !!!  for purposes such as learning asl or translating from audio to a form an asl speaker can better understand.\nwhat it does\nthe tool allows one to record their voice or other audio sources, and once one stops recording, it will use assemblyai's api to generate a transcript of that recording. it then displays asl hand signs along with that transcript, with the option to cycle between 3 different dictionaries of interpreters. it also performs an ml-based sentiment analysis to predict what emotions were felt by the speaker.\nhow we built it\nthe audio input and connection to the assemblyai api were done through javascript. the text received from the assemblyai api will be sent to the sentiment analysis api, built by flask, hosted by heroku.\nchallenges we ran into\nconnecting the various resources we had together was the main challenge. it would have been simpler with each part separated: a \"record and save your voice\" button, an \"upload your voice to assemblyai\" button, some links that say \"click here to go to someone's asl dictionary site\" and \"click here to try out a sentiment analysis ai\". it was quite a tedious set up communication between these different systems, exacerbated by the short deadline and made worse still by the time zone disparities within our team. i don't think there was so much as an hour when all 4 of us were talking together. for sentiment analysis, we've run into overfitting problems and an imbalanced dataset which lots of times had been wasted. however, we managed to clean up the dataset and boost up the accuracy to >83%.\naccomplishments that we're proud of\nthe pride of an accomplishment goes hand-in-hand with its difficulty. we managed to bring all the features that challenged us together into a single page. it significantly reduces the hassle for a user compared to what there would be without putting in the effort to streamline those features.\nwhat we learned\ntheo: i came into this without having used javascript before, so that was a learning experience for me. while working on it, using the assemblyai api seemed difficult, but in retrospect, i was able to get it working within a few hours of work, and in a programming language i hadn't used before at that.\nalex: i discovered that web development is actually really cool and rewarding to do. i thought it would be a bore, but seeing it work in the end: using our product felt great!\nhub: i've never hosted a rest api on the cloud before and had little experience with nlp. however, i managed to train the nlp model in time and host it.\nzeaan: i worked on the frontend of the website and helped on hosting the flask sentiment analysis api, and built on my past experience with those to explore new options.\nwhat's next for asl speech to text\ncreate our own dataset of hearing-impaired people signing and implement grammar in the algorithm.\nimprove sentiment analysis model to multi-class classification which includes anger, happy, sad, etc.", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59507274}, {"Unnamed: 0": 7279, "autor": "Trackzu", "date": null, "content": "Inspiration:\nDuring the pandemic, many people have been struggling to maintain structure and adjust to constant change. With Trackzu, we hope to encourage users to develop good habits and let go of bad ones to improve their mental health. Unlike other progress tracking apps, we wanted to add elements that would keep the user accountable once they started tracking a habit (nothing too high-stakes as that would create more stress). We chose the octopus for our logo since it has 8 arms, and therefore is symbolic of the ability to juggle multiple activities.\nWhat it does:\nTrackzu achieves its goal by prompting the user to select which habits they want to develop/get rid of. Then, they're asked to select a virtual octopus friend whose happiness is dependent on the user's consistency when tracking their progress on Trackzu. We also have a Tinder-style community tab where users can interact and get support from people working on the same habits. If a user chooses to make their profile public for a certain habit, they can then interact with others working on the same habit. We included a journal tab for users to record their thoughts, their process, or reminders of why they are doing this.\nHow we built it:\nBefore designing our app, we conducted a survey with 19 test users regarding what they find to be helpful when working on a habit. We found that productive habits help improve daily life which improves a person's mental health. You can use the following link to help us develop Trackzu further: https://docs.google.com/forms/d/e/1FAIpQLSeGJJfPLV6T5WaGNAuMkvXh7NNusSd8qD3GeK5D47QwDAr-eg/viewform\nWe used Figma to design the interface of our app. We began with a simple wireframe then added to it based on the features we wanted to include.\nChallenges we ran into:\nMost of the team had never worked with Figma before, so it was difficult to learn how to use a new tool and create a functioning prototype within the limited timeframe. Unfortunately, a team member had to leave early due to personal reasons. It was a bit challenging to split up work since they did not expect to leave so early.\nAccomplishments that we're proud of:\nThankfully, we communicated a lot as a team so it was easier than expected to account for road bumps and conflicting schedules. We put all of our skills together and helped each other out to fill in any knowledge gaps.\nWhat we learned:\nOur team learned how to use Figma to create a prototype. We also learned how to direct a demo video, make scripts, and outline a project from start to finish.\nWhat's next for Trackzu:\nWe hope to work on developing a prototype that allows the user to interact with their octopi more, such as growing it instead and being able to see it grow from a baby to an adult. Another option would be to include a tab allowing users to receive additional support by connecting them to professionals or connecting the app to smartwatches to make the tracking for certain habits easier (i.e running, sleeping, etc).", "link": "https://devpost.com/software/trackzu", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration:\nduring the pandemic, many people have been struggling to maintain structure and adjust to constant change. with trackzu, we hope to encourage users to develop good habits and let go of bad ones to improve their mental health. unlike other progress tracking apps, we wanted to add elements that would keep the user accountable once they started tracking a habit (nothing too high-stakes as that would create more stress). we chose the octopus for our logo since it has 8 arms, and therefore is symbolic of the ability to juggle multiple activities.\nwhat it does:\ntrackzu achieves its goal by prompting the user to select which habits they want to develop/get rid of. then, they're asked to select a virtual octopus friend whose happiness is dependent on the user's consistency when tracking their progress on trackzu. we also have a tinder-style community tab where users can interact and get support from people working on the same habits. if a user chooses to make their profile public for a certain habit, they can then interact with others working on the same habit. we included a journal tab for users to record their thoughts, their process, or reminders of why they are doing this.\nhow we built it:\nbefore designing our app, we conducted a survey with 19 test users regarding what they find to be helpful when working on a habit. we found that productive habits help improve daily life which improves a person's mental health. you can use the following link to help us develop trackzu further: https://docs.google.com/forms/d/e/1faipqlsegjjfplv6t5wagnaumkvxh7nnussd8qd3gek5d47qwdar-eg/viewform\nwe used figma to design the interface of our app. we began with a simple wireframe then added to it based on the features we wanted to include.\nchallenges we ran into:\nmost of the team had never worked with figma before, so it was difficult to learn how to use a new -----> tool !!!  and create a functioning prototype within the limited timeframe. unfortunately, a team member had to leave early due to personal reasons. it was a bit challenging to split up work since they did not expect to leave so early.\naccomplishments that we're proud of:\nthankfully, we communicated a lot as a team so it was easier than expected to account for road bumps and conflicting schedules. we put all of our skills together and helped each other out to fill in any knowledge gaps.\nwhat we learned:\nour team learned how to use figma to create a prototype. we also learned how to direct a demo video, make scripts, and outline a project from start to finish.\nwhat's next for trackzu:\nwe hope to work on developing a prototype that allows the user to interact with their octopi more, such as growing it instead and being able to see it grow from a baby to an adult. another option would be to include a tab allowing users to receive additional support by connecting them to professionals or connecting the app to smartwatches to make the tracking for certain habits easier (i.e running, sleeping, etc).", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59507279}, {"Unnamed: 0": 7285, "autor": "CrisisDetector", "date": null, "content": "Inspiration\nOver 90% of college students in the US have experienced some level of negative mental health symptoms according to a BestColleges.com survey. This has been aggravated by COVID-19 and the advent of online education, increasing mental health concerns in the student fraternity. Often, students have a hard time expressing their mental state to their family and friends, and thus their condition goes undiagnosed which can lead to concerning outcomes.\nWhat it does\nWith this in mind, we wanted to create a tool that can detect concerning trends in social media posts and take appropriate action if the concern levels are high. We believe implementing these detectors on different social media platforms can act as a proactive tool for detecting early signs of mental illnesses.\nHow we built it\nWe wrote a python code using beautiful-soup and tweepy to scrape Twitter and Mental Health Forum for posts. Using those posts, we created a bag of words that are frequent in posts that show clear indications of mental health issues. Then, using the cosine similarity index we wrote a code to detect similarity in new posts to look for indications of severe mental health issues that could show suicidal tendencies.\nChallenges we ran into\nWith no experience in web scrapping, we were looking for teammates with some experience in web scrapping. However, we were not able to find anyone who is willing to join our team. Hence,...\nWe also tried crawling, but the code was taking much time to run\u2014with no errors ;)\u2014which we thought is not suitable for the scope of our project.\nOkay, well, websites do not like being scrapped\u2014I don\u2019t think anyone would\u2014which was difficult to get around because often we were required to verify that we\u2019re not bots.\nOne important feature we tried to add to our tool is the geo-location of the tweet or post, which will help in sending the OP\u2019s information to schools and colleges in that area to contact them. This will allow schools and universities to take proactive action in aiding students with mental health issues. It is very common that students rather post about their issues online than seeking help with their counseling centers.\nWe tried to create a flask server to link our code to HTML, but, unfortunately, time was of the essence and we could not do it.\nAccomplishments that we're proud of\nWe are very proud of the work we were able to accomplish especially in the area of web scraping. We, literally, learned web scraping in a few hours. None of us had any previous experience in web scrapping\u2014our expertise is limited to data analysis. We were able to generate ORIGINAL, UNIQUE data sets via scrapping Twitter and Mental Health Forum. We were able to scrape over 300 pages from the Mental Health Forum, which was time-consuming (40 mins!), with around 8,000 rows of data about posts. From Twitter, we were able to generate around 10,000 rows of data that is updated every time we run the code with real-time data.\nWhat we learned\nWe learned about web scraping, its limitations, usefulness, and verification. It is vital to have a human element in verifying the validity of natural language processing models because machine learning often cannot grasp the context of posts and might produce false positives or false negatives.\nWhat's next for CrisisDetectors\nOne important feature that could further enhance our tool is incorporating geolocation detection of posts. This will allow us to detect the location of the posts, and we can send this kind of information to schools and universities in the area. Schools should start searching for those usernames in their databases and reach out to help them\u2014being proactive.", "link": "https://devpost.com/software/xx-w3lcg8", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nover 90% of college students in the us have experienced some level of negative mental health symptoms according to a bestcolleges.com survey. this has been aggravated by covid-19 and the advent of online education, increasing mental health concerns in the student fraternity. often, students have a hard time expressing their mental state to their family and friends, and thus their condition goes undiagnosed which can lead to concerning outcomes.\nwhat it does\nwith this in mind, we wanted to create a -----> tool !!!  that can detect concerning trends in social media posts and take appropriate action if the concern levels are high. we believe implementing these detectors on different social media platforms can act as a proactive tool for detecting early signs of mental illnesses.\nhow we built it\nwe wrote a python code using beautiful-soup and tweepy to scrape twitter and mental health forum for posts. using those posts, we created a bag of words that are frequent in posts that show clear indications of mental health issues. then, using the cosine similarity index we wrote a code to detect similarity in new posts to look for indications of severe mental health issues that could show suicidal tendencies.\nchallenges we ran into\nwith no experience in web scrapping, we were looking for teammates with some experience in web scrapping. however, we were not able to find anyone who is willing to join our team. hence,...\nwe also tried crawling, but the code was taking much time to run\u2014with no errors ;)\u2014which we thought is not suitable for the scope of our project.\nokay, well, websites do not like being scrapped\u2014i don\u2019t think anyone would\u2014which was difficult to get around because often we were required to verify that we\u2019re not bots.\none important feature we tried to add to our tool is the geo-location of the tweet or post, which will help in sending the op\u2019s information to schools and colleges in that area to contact them. this will allow schools and universities to take proactive action in aiding students with mental health issues. it is very common that students rather post about their issues online than seeking help with their counseling centers.\nwe tried to create a flask server to link our code to html, but, unfortunately, time was of the essence and we could not do it.\naccomplishments that we're proud of\nwe are very proud of the work we were able to accomplish especially in the area of web scraping. we, literally, learned web scraping in a few hours. none of us had any previous experience in web scrapping\u2014our expertise is limited to data analysis. we were able to generate original, unique data sets via scrapping twitter and mental health forum. we were able to scrape over 300 pages from the mental health forum, which was time-consuming (40 mins!), with around 8,000 rows of data about posts. from twitter, we were able to generate around 10,000 rows of data that is updated every time we run the code with real-time data.\nwhat we learned\nwe learned about web scraping, its limitations, usefulness, and verification. it is vital to have a human element in verifying the validity of natural language processing models because machine learning often cannot grasp the context of posts and might produce false positives or false negatives.\nwhat's next for crisisdetectors\none important feature that could further enhance our tool is incorporating geolocation detection of posts. this will allow us to detect the location of the posts, and we can send this kind of information to schools and universities in the area. schools should start searching for those usernames in their databases and reach out to help them\u2014being proactive.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59507285}, {"Unnamed: 0": 7313, "autor": "OpenSea NFT Sales Tracker", "date": null, "content": "Inspiration\nOur team consist of Blockchain Student Society members who are passionate about crypto, blockchain, and especially NFTs. We have members who hold, trade, and mint NFTs and because of this, our enthusiasm for NFTs led to the inspiration of this project. OpenSea is the world\u2019s first and largest NFT marketplace, yet it doesn\u2019t allow users to easily see real time transactions in a digestible way. It does allow us to view some NFT statistics, but the shortest time frame is just 24 hours. Trying to identify market trends on such a long time horizon in this rapid & volatile space will only guarantee missed opportunities and unnecessary FOMO (fear of missing out).\nWhat it does\nWe have created an active database of real time OpenSea NFT Sales, otherwise known as The OpenSea NFT Sales Tracker. This program documents every NFT sale that goes through OpenSea in real time and converts them into easily digestible visuals such as tables and graphs to help the user front run the opportunity. We have two examples that demonstrate our program. The csv is a database proof of concept in that it shows how the program pulls real world data from the OpenSea API. It records every NFT sale that goes through OpenSea and the graph represents the total purchases within a time frame per NFT collection. The google sheet is a real time dataset visualization of concept that shows active NFT sales on OpenSea. Values are updated every 5 seconds and the graph represents the top 10 NFT collections by sales within a given time frame. We verified our results by comparing it to OpenSea\u2019s recent transactions history and used block explorers like Etherscan to further clarify some questionable transactions. With this tool, retail consumers can formulate their own short term strategies to front run the opportunity and help them get ahead. This data set is to help users digest large quantities of information to help them strategize their short term NFT investments.\nHow we built it\nWe started off the development searching for ways to incorporate the Opensea api into our project dataset. After determining the type of information we began implementing the api into our python project, we used http GET requests in order to query their server and converting the response to a json format. After parsing through the json data for useful information we landed on studying successful sales in nft transactions. We target the successful sales and organized them by collection. We created a function to make many calls per second, checking for a successful transactions, this function used dictionaries in order to keep track of the time they were found and included a counter for how long ago they were found. We formatted the dataset by intervals of time and tested them against the time found in order to decrement upon passing each interval. For the real time data, we remove the instance of the collection counters in order to make room for another to take its place in the visualization. We used pandas in order to create a transferrable dataframe so we could export to many different types of files, including CSV, Excel, and Google Sheets (which is where our real time graph lies).\nChallenges we ran into\nWe had the challenge of excluding certain sales such as ERC-1159, OPENSTORE transactions, and duplicate transaction. However, our largest challenge that was most time consuming was deciding which data from the json responses from the OpenSea API was appropriate to our vision. This was solved through a process of trial and error to determine which data type was appropriate to help the end user have an idea of market momentum.\nAccomplishments that we're proud of\nWe got to experience for the first time using https request to access an API, parsing a json file to access certain information, creating a real time information database, and writing the real time data to multiple platforms for visualization.\nWhat we learned\nWe learned that developing a general vision as a template for a direction of our project will increase productivity and efficiency. Attempts at figuring out every detail before hand will only slow down processes. We also learned how to translate real time data into a spreadsheet database that will also update as the code runs.\nWhat's next for OpenSea NFT Sales Tracker\nFuture plans for OpenSea NFT Sales Tracker would be to implement a percent change in sales with color coordination in respect to the positive/negative changes. We would also want to develop a stable webservice where users around the world can connect to and use our program.", "link": "https://devpost.com/software/nft-sales-tracker", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nour team consist of blockchain student society members who are passionate about crypto, blockchain, and especially nfts. we have members who hold, trade, and mint nfts and because of this, our enthusiasm for nfts led to the inspiration of this project. opensea is the world\u2019s first and largest nft marketplace, yet it doesn\u2019t allow users to easily see real time transactions in a digestible way. it does allow us to view some nft statistics, but the shortest time frame is just 24 hours. trying to identify market trends on such a long time horizon in this rapid & volatile space will only guarantee missed opportunities and unnecessary fomo (fear of missing out).\nwhat it does\nwe have created an active database of real time opensea nft sales, otherwise known as the opensea nft sales tracker. this program documents every nft sale that goes through opensea in real time and converts them into easily digestible visuals such as tables and graphs to help the user front run the opportunity. we have two examples that demonstrate our program. the csv is a database proof of concept in that it shows how the program pulls real world data from the opensea api. it records every nft sale that goes through opensea and the graph represents the total purchases within a time frame per nft collection. the google sheet is a real time dataset visualization of concept that shows active nft sales on opensea. values are updated every 5 seconds and the graph represents the top 10 nft collections by sales within a given time frame. we verified our results by comparing it to opensea\u2019s recent transactions history and used block explorers like etherscan to further clarify some questionable transactions. with this -----> tool !!! , retail consumers can formulate their own short term strategies to front run the opportunity and help them get ahead. this data set is to help users digest large quantities of information to help them strategize their short term nft investments.\nhow we built it\nwe started off the development searching for ways to incorporate the opensea api into our project dataset. after determining the type of information we began implementing the api into our python project, we used http get requests in order to query their server and converting the response to a json format. after parsing through the json data for useful information we landed on studying successful sales in nft transactions. we target the successful sales and organized them by collection. we created a function to make many calls per second, checking for a successful transactions, this function used dictionaries in order to keep track of the time they were found and included a counter for how long ago they were found. we formatted the dataset by intervals of time and tested them against the time found in order to decrement upon passing each interval. for the real time data, we remove the instance of the collection counters in order to make room for another to take its place in the visualization. we used pandas in order to create a transferrable dataframe so we could export to many different types of files, including csv, excel, and google sheets (which is where our real time graph lies).\nchallenges we ran into\nwe had the challenge of excluding certain sales such as erc-1159, openstore transactions, and duplicate transaction. however, our largest challenge that was most time consuming was deciding which data from the json responses from the opensea api was appropriate to our vision. this was solved through a process of trial and error to determine which data type was appropriate to help the end user have an idea of market momentum.\naccomplishments that we're proud of\nwe got to experience for the first time using https request to access an api, parsing a json file to access certain information, creating a real time information database, and writing the real time data to multiple platforms for visualization.\nwhat we learned\nwe learned that developing a general vision as a template for a direction of our project will increase productivity and efficiency. attempts at figuring out every detail before hand will only slow down processes. we also learned how to translate real time data into a spreadsheet database that will also update as the code runs.\nwhat's next for opensea nft sales tracker\nfuture plans for opensea nft sales tracker would be to implement a percent change in sales with color coordination in respect to the positive/negative changes. we would also want to develop a stable webservice where users around the world can connect to and use our program.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59507313}, {"Unnamed: 0": 7325, "autor": "Celonis 2021 Challenge | TAMU Datathon", "date": null, "content": "We are a team of highly motivated industrial engineers looking to solve real world problems where we can cut costs and improve profitability. This particular problem was a kaizen blitz lean management problem and we wanted to do this challenge. We used Celonis as a tool to analyse the order, event and customer data dump to pull process insights. We identified certain breakdowns and bottlenecks and analyzed the business effects of them. We performed root cause analysis, compared our planned vs solution model and identified the metrics that were performing poorly. We have given our suggestions in the document uploaded.", "link": "https://devpost.com/software/celonis-2021-challenge-tamu-datathon", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "we are a team of highly motivated industrial engineers looking to solve real world problems where we can cut costs and improve profitability. this particular problem was a kaizen blitz lean management problem and we wanted to do this challenge. we used celonis as a -----> tool !!!  to analyse the order, event and customer data dump to pull process insights. we identified certain breakdowns and bottlenecks and analyzed the business effects of them. we performed root cause analysis, compared our planned vs solution model and identified the metrics that were performing poorly. we have given our suggestions in the document uploaded.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59507325}, {"Unnamed: 0": 7327, "autor": "Robotorque Regression", "date": null, "content": "Inspiration\nWe were inspired by the demo General Motors showed us of the Robotorque application in real life.\nWhat it does\nOur solution first gets the robot translation and rotation from a depth image using pointclouds. We find the plane which represents the face of the head of the bolt and use the normal vector to that plane to calculate the angle which the bolt is oriented. We then use the points which lie on the plane's equation which we have found to find a center point which we use to get the bolts translation from the origin.\nAfter we have the translation and rotation of the bolt, we focus on moving the robot arm to track the bolt head over time. Here, we use the last locations of the bolt to create a function of best fit, in our case going up to a two degree polynomial using least squares regression. We estimate where the bolt will be by extrapolating this function, and move our robot arm based on this predicted position.\nThrough these two steps we are able to track the position of a bolt as it changes using our robot arm and depth information from a LIDAR camera.\nHow we built it\nWe built our application in python, making special use of open3d and numpy.\nFrom open3d, we used the PointCloud object to make finding a representative plane for the bolt efficient. Using this PointCloud object, we were able to sample thousands of test planes on the bolt and see which test plane contained the most points using a method of random sampling. Since the bolt head is the flattest part of the bolt with the most points lying on the plane which is flush with it, the ouput of this algorithm will be a plane which contains all the points on the head of the bolt.\nWe also used numpy to make our regression analysis of the bolt translation data more accurate. The polynomial fit tools included in numpy allowed us to effectively create a model for the bolt location over time and use that model to predict future locations of the bolt which follow a physics based swinging motion.\nChallenges we ran into\nOne of the challenges we ran into was finding an accurate modeling function to predict the future location of the bolt. We used many different methods, starting with a simple formula of taking the first location of the bolt, taking the last location of the bolt, and drawing a line through those points to predict the next location of the bolt. While this was pretty accurate for situations in which the bolts translation was generally either decreasing or increasing, in situations where the general motion of the bolt oscillated it did not perform as well.\nWe also tried using the last four locations of the bolt to create a linear regression model for where the next location would end up. While this modeled general function behavior well, it did not seem to be able to get the peaks and troughs of the bolts oscillation well enough for the reward function.\nWe then tried numpy polynomial regression with a degree of 3. We found both this regression model to work the best for most of the cases which arise in this problem. The model takes into account all the previous points, and is able to capture the motion of the bolt well in most situations, though we still think there may be some trouble in cases where the general behavior of the function includes many local extrema, like if the bolt follows some quartic or quintic function which cannot be modeled in full detail by our third degree polynomial.\nWe also tried using the scipy package spline interpolation tool, which had the potential to be very good at modeling the oscillating short-term pattern of the bolt while also keeping up with long-term function behavior. Unfortunately, the short-term oscillations of our model overshot the actual oscillations of the bolt head, and so our values were less accurate than with the numpy method.\nAccomplishments that we're proud of\nWe are proud of the method we used to find the translation and rotation of the bolt for the first segement of this problem. The solution is very elegant once we find the plane which represents the face of the head of the bolt. We first create a PointCloud from the depth image of the bolt. Then we find the representative plane by trying thousands of planes which intersect the points in the PointCloud, selecting the plane which contains the most points. We can then find a normal vector to this plane, which will point in the same direction as the bolt. After this, we just have to do some vector operations to get the angle between that normal vector and the one representing the board the bolt is on. To find the translation of the bolt, we again use our PointCloud and can average the x, y, and z values of every point which lies in the cloud, giving us the center of our bolt head.\nWhat we learned\nWe learned a lot about matrix and vector math, especially as it used to model translations and rotations; the process of grabbing features from 3d images (like 3d depth images) using the PointCloud tool in open3d; and different regression techniques to model real-life movements of objects (like the oscillating motion of the bolt).\nWhat's next for Robotorque\nWe would like to improve our modeling function techniqe, perhaps adding a lower and upper bounding function for the location of the bolt over time. We could then alternate between favoring the upper and lower bound function to simulate the type of oscillating behavior which we found hard to capture using a single regression function.", "link": "https://devpost.com/software/robotorque", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe were inspired by the demo general motors showed us of the robotorque application in real life.\nwhat it does\nour solution first gets the robot translation and rotation from a depth image using pointclouds. we find the plane which represents the face of the head of the bolt and use the normal vector to that plane to calculate the angle which the bolt is oriented. we then use the points which lie on the plane's equation which we have found to find a center point which we use to get the bolts translation from the origin.\nafter we have the translation and rotation of the bolt, we focus on moving the robot arm to track the bolt head over time. here, we use the last locations of the bolt to create a function of best fit, in our case going up to a two degree polynomial using least squares regression. we estimate where the bolt will be by extrapolating this function, and move our robot arm based on this predicted position.\nthrough these two steps we are able to track the position of a bolt as it changes using our robot arm and depth information from a lidar camera.\nhow we built it\nwe built our application in python, making special use of open3d and numpy.\nfrom open3d, we used the pointcloud object to make finding a representative plane for the bolt efficient. using this pointcloud object, we were able to sample thousands of test planes on the bolt and see which test plane contained the most points using a method of random sampling. since the bolt head is the flattest part of the bolt with the most points lying on the plane which is flush with it, the ouput of this algorithm will be a plane which contains all the points on the head of the bolt.\nwe also used numpy to make our regression analysis of the bolt translation data more accurate. the polynomial fit tools included in numpy allowed us to effectively create a model for the bolt location over time and use that model to predict future locations of the bolt which follow a physics based swinging motion.\nchallenges we ran into\none of the challenges we ran into was finding an accurate modeling function to predict the future location of the bolt. we used many different methods, starting with a simple formula of taking the first location of the bolt, taking the last location of the bolt, and drawing a line through those points to predict the next location of the bolt. while this was pretty accurate for situations in which the bolts translation was generally either decreasing or increasing, in situations where the general motion of the bolt oscillated it did not perform as well.\nwe also tried using the last four locations of the bolt to create a linear regression model for where the next location would end up. while this modeled general function behavior well, it did not seem to be able to get the peaks and troughs of the bolts oscillation well enough for the reward function.\nwe then tried numpy polynomial regression with a degree of 3. we found both this regression model to work the best for most of the cases which arise in this problem. the model takes into account all the previous points, and is able to capture the motion of the bolt well in most situations, though we still think there may be some trouble in cases where the general behavior of the function includes many local extrema, like if the bolt follows some quartic or quintic function which cannot be modeled in full detail by our third degree polynomial.\nwe also tried using the scipy package spline interpolation -----> tool !!! , which had the potential to be very good at modeling the oscillating short-term pattern of the bolt while also keeping up with long-term function behavior. unfortunately, the short-term oscillations of our model overshot the actual oscillations of the bolt head, and so our values were less accurate than with the numpy method.\naccomplishments that we're proud of\nwe are proud of the method we used to find the translation and rotation of the bolt for the first segement of this problem. the solution is very elegant once we find the plane which represents the face of the head of the bolt. we first create a pointcloud from the depth image of the bolt. then we find the representative plane by trying thousands of planes which intersect the points in the pointcloud, selecting the plane which contains the most points. we can then find a normal vector to this plane, which will point in the same direction as the bolt. after this, we just have to do some vector operations to get the angle between that normal vector and the one representing the board the bolt is on. to find the translation of the bolt, we again use our pointcloud and can average the x, y, and z values of every point which lies in the cloud, giving us the center of our bolt head.\nwhat we learned\nwe learned a lot about matrix and vector math, especially as it used to model translations and rotations; the process of grabbing features from 3d images (like 3d depth images) using the pointcloud tool in open3d; and different regression techniques to model real-life movements of objects (like the oscillating motion of the bolt).\nwhat's next for robotorque\nwe would like to improve our modeling function techniqe, perhaps adding a lower and upper bounding function for the location of the bolt over time. we could then alternate between favoring the upper and lower bound function to simulate the type of oscillating behavior which we found hard to capture using a single regression function.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59507327}, {"Unnamed: 0": 7355, "autor": "EnviroLife", "date": null, "content": "Inspiration\nThe effects of climate change are an inevitable truth. Climate change has already and will continue to noticeably affect many critical aspects of day to day life of many people around the world. At this point, it is necessary to consider the future impacts of climate change in order to have certainty in the security and safety of your geographical location. Our team decided to build a web application that interprets Google Cloud Platform\u2019s Earth Engine datasets captured by satellites to teach people about the impacts climate change may have on their everyday life, if things go unchanged. Such a tool can also improve the health of those considering environmental factors when making a decision of where to live in the future. Additionally, this app can help individuals make plans for future decades, if it is necessary for them to relocate for their safety. Moreover, applications like these highlight the tremendous impact and value space technologies like satellites have on everyday life.\nWhat it does\nThe web application takes two entries, a location and a year date. Using these two points, it obtains historical data regarding this region from multiple datasets provided by Google\u2019s Earth Engine (Google Cloud Platform), including precipitation, temperature, changes in local water level, natural disaster threat levels, and other factors like atmospheric analysis of certain compounds. After obtaining data for as far back as the information goes, the application attempts to make a future analysis for up until the user\u2019s specified year date. The application then returns various data charts and an index of how livable the specified area will be in said year.\nHow we built it\nWe ran the React application on Flask to get user queries from another IP address and respond with information. We used React.js to access the information from the website as well as display the requested information. We then searched Earth Engine\u2019s repositories for datasets we could use to build our project. We used various Python libraries to analyze the data extracted from Earth Engine, including statsmodels to train the machine learning models, and matlib to generate and send images to the frontend.\nChallenges we ran into\nThe foremost issue was determining how to interpret the data from Google\u2019s Earth Engine. There is a vast repository of data regarding hundreds of different aspects of Earth, and this data is stored as a wide variety of differing variables and types. The documentation for each dataset is not as comprehensive as we hoped. We had to learn how to interpret scientific data and then use technology to analyze and accurately extrapolate said data. We also had to learn how to efficiently parse through images to cut down the large amount of time it takes to train the model. In addition, the team had to learn new tools such as Flask and server hosting, both of which are essential to the project. While we would have loved to demonstrate analysis of various tools like pixel analysis of vegetation and atmospheric analysis of various compounds, we realized that the processing time was too great for this demonstration and that a creation of a truly accurate model would take some more time than we have for this event.\nAccomplishments that we're proud of\nWe are proud of our ability to have created a web tool that not only will help people make a decision about where to live, but also educate people about the real effects of climate change and about the usefulness of space technologies like satellite sensors and imagery. We are also proud to have accomplished so much in the time period allotted, especially for a team that consisted of members who did not have experience in the technologies used.\nWhat we learned\nOur team learned how to efficiently analyze large datasets for useful information and trends, in addition to training a model to make accurate analysis of said data. We learned how to deploy a functional web app over a server. In addition, some members of the team were introduced to libraries like React.js for the first time.\nWhat's next for EnviroLife\nWe hope to continue to add more significant features and analysis to our tool. We realized that there are truly vast amounts of information about the climate and Earth about a wide variety of useful categories. Potential features include better analysis of vegetation, droughts, and natural disasters. We would also look into stronger prediction models more accurate to the specific dataset.", "link": "https://devpost.com/software/envirolife", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nthe effects of climate change are an inevitable truth. climate change has already and will continue to noticeably affect many critical aspects of day to day life of many people around the world. at this point, it is necessary to consider the future impacts of climate change in order to have certainty in the security and safety of your geographical location. our team decided to build a web application that interprets google cloud platform\u2019s earth engine datasets captured by satellites to teach people about the impacts climate change may have on their everyday life, if things go unchanged. such a -----> tool !!!  can also improve the health of those considering environmental factors when making a decision of where to live in the future. additionally, this app can help individuals make plans for future decades, if it is necessary for them to relocate for their safety. moreover, applications like these highlight the tremendous impact and value space technologies like satellites have on everyday life.\nwhat it does\nthe web application takes two entries, a location and a year date. using these two points, it obtains historical data regarding this region from multiple datasets provided by google\u2019s earth engine (google cloud platform), including precipitation, temperature, changes in local water level, natural disaster threat levels, and other factors like atmospheric analysis of certain compounds. after obtaining data for as far back as the information goes, the application attempts to make a future analysis for up until the user\u2019s specified year date. the application then returns various data charts and an index of how livable the specified area will be in said year.\nhow we built it\nwe ran the react application on flask to get user queries from another ip address and respond with information. we used react.js to access the information from the website as well as display the requested information. we then searched earth engine\u2019s repositories for datasets we could use to build our project. we used various python libraries to analyze the data extracted from earth engine, including statsmodels to train the machine learning models, and matlib to generate and send images to the frontend.\nchallenges we ran into\nthe foremost issue was determining how to interpret the data from google\u2019s earth engine. there is a vast repository of data regarding hundreds of different aspects of earth, and this data is stored as a wide variety of differing variables and types. the documentation for each dataset is not as comprehensive as we hoped. we had to learn how to interpret scientific data and then use technology to analyze and accurately extrapolate said data. we also had to learn how to efficiently parse through images to cut down the large amount of time it takes to train the model. in addition, the team had to learn new tools such as flask and server hosting, both of which are essential to the project. while we would have loved to demonstrate analysis of various tools like pixel analysis of vegetation and atmospheric analysis of various compounds, we realized that the processing time was too great for this demonstration and that a creation of a truly accurate model would take some more time than we have for this event.\naccomplishments that we're proud of\nwe are proud of our ability to have created a web tool that not only will help people make a decision about where to live, but also educate people about the real effects of climate change and about the usefulness of space technologies like satellite sensors and imagery. we are also proud to have accomplished so much in the time period allotted, especially for a team that consisted of members who did not have experience in the technologies used.\nwhat we learned\nour team learned how to efficiently analyze large datasets for useful information and trends, in addition to training a model to make accurate analysis of said data. we learned how to deploy a functional web app over a server. in addition, some members of the team were introduced to libraries like react.js for the first time.\nwhat's next for envirolife\nwe hope to continue to add more significant features and analysis to our tool. we realized that there are truly vast amounts of information about the climate and earth about a wide variety of useful categories. potential features include better analysis of vegetation, droughts, and natural disasters. we would also look into stronger prediction models more accurate to the specific dataset.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 2, "media": null, "medialink": null, "identifyer": 59507355}, {"Unnamed: 0": 7365, "autor": "Logglytics", "date": null, "content": "Inspiration\nWe use many applications and these apps are fused with our day-to-day life... Did you know our apps are responding to our actions? Recently Instagram has changed its primary action to reels. How Instagram knows this?. They got an insight about Instagram that the Reels options are used often than sharing posts.\nWe tried to create a tool to fetch insight like that.\nWhat it does\nIt will help developers to fetch and visualize insights, logs from applications that are running on production mode.\nWe can use it on any platform.\nHow we built it\nWe chose to React for Dashboard, Nodejs, and MongoDB for backend & APIs.\nChallenges we ran into\nThe main challenge we faced is while writing APIs we often get \"cors\" errors. :( But we managed to solve that... We are happy that we learned new things.\nAccomplishments that we're proud of\nWe have built a dashboard UI for devs to visualize the Events and logs. Build a Nodejs NPM package for using this API\nWhat we learned\nBuilding APIs, creating NPM packages, and many more\nWhat's next for Logglytics\nOur future plan is to build SDKs for other platforms like Android, iOS, etc.. :)", "link": "https://devpost.com/software/logglytics-62ginf", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe use many applications and these apps are fused with our day-to-day life... did you know our apps are responding to our actions? recently instagram has changed its primary action to reels. how instagram knows this?. they got an insight about instagram that the reels options are used often than sharing posts.\nwe tried to create a -----> tool !!!  to fetch insight like that.\nwhat it does\nit will help developers to fetch and visualize insights, logs from applications that are running on production mode.\nwe can use it on any platform.\nhow we built it\nwe chose to react for dashboard, nodejs, and mongodb for backend & apis.\nchallenges we ran into\nthe main challenge we faced is while writing apis we often get \"cors\" errors. :( but we managed to solve that... we are happy that we learned new things.\naccomplishments that we're proud of\nwe have built a dashboard ui for devs to visualize the events and logs. build a nodejs npm package for using this api\nwhat we learned\nbuilding apis, creating npm packages, and many more\nwhat's next for logglytics\nour future plan is to build sdks for other platforms like android, ios, etc.. :)", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59507365}, {"Unnamed: 0": 7403, "autor": "Text Summarizer", "date": null, "content": "Inspiration\nHow often does it occur that we find ourselves with an urgent need to look up something on the web for a reference to some article, to clear a doubt encountered while working on an assignment, etc and we are ambushed by these long cumbersome articles that may or may not contain the piece of information that we are probing for.\nWhat it does\nTextSummarizer is a tool that encounters this problem and helps us to gather relevant and contextual information from the websites that we are looking at, for the information that we are looking for.\nHow we built it\nWe built our project on NLP using the LexRank algorithm and formed eigenvectors using similarity matrix on Lemmatized words of the sentences and deployed using Flask on a WebApp\nChallenges we ran into\nDeployment on web app hosting platforms like Heroku, AWS etc\nAccomplishments that we're proud of\nGreat Accuracy, Responsive web app\nWhat we learned\nNLP, Responsive web design\nWhat's next for Text Summarizer\nDeploying on a Webapp Hosting Platform", "link": "https://devpost.com/software/text-summarizer-5pljnt", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nhow often does it occur that we find ourselves with an urgent need to look up something on the web for a reference to some article, to clear a doubt encountered while working on an assignment, etc and we are ambushed by these long cumbersome articles that may or may not contain the piece of information that we are probing for.\nwhat it does\ntextsummarizer is a -----> tool !!!  that encounters this problem and helps us to gather relevant and contextual information from the websites that we are looking at, for the information that we are looking for.\nhow we built it\nwe built our project on nlp using the lexrank algorithm and formed eigenvectors using similarity matrix on lemmatized words of the sentences and deployed using flask on a webapp\nchallenges we ran into\ndeployment on web app hosting platforms like heroku, aws etc\naccomplishments that we're proud of\ngreat accuracy, responsive web app\nwhat we learned\nnlp, responsive web design\nwhat's next for text summarizer\ndeploying on a webapp hosting platform", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59507403}, {"Unnamed: 0": 7406, "autor": "Fishtype", "date": null, "content": "Project Fishtype\nGithub: https://github.com/AKil47/FishType\nProblem: Cybercrime is more rampant than ever\nToday we live in a world where technology is advancing, rapidly simplifying our lives while making us utterly dependent on computers and digital networks. At the same time, data breaches and complex hacks have also become daily news, threatening our wealth, privacy, and security. These modern times mean that any and all measures of reinforcing our security are more vital than ever before.\nSolution: Keystroke Dynamics - Applying Gait Analysis to Typing\nGait Analysis is the study of analyzing a person's walking habits to determine their identity. Although real-time systems like the gif shown above don't fully exist, it has been scientifically proven that each human's gait has a set of uniqueness that is akin to a fingerprint.\nOur Idea is to take gait analysis from the feet and see if the same holds true for the fingers Spoiler Alert: It kind of does!\nCollecting the Data\nTo date, there is only a single dataset on Kaggle that has anything related to keystroke data and identity. It has a whopping five rows where each user entered a string five characters long. Our goal was to create an open dataset that has enough volume and reliability to help Data Scientists develop accurate models to solve the problems listed above\nWe designed a custom keystroke surveying tool called fishtype (bc we're freshman) to collect the data. The code is on our GitHub, so check it out!\nThe website had users type the phrase \"The quick brown fox jumped over the lazy dog\" five different times into a text box. If the users entered the phrase incorrectly, they were asked to type it again so that the dataset could remain consistent.\nWe went across the Hall of Champions and asked every single person that we saw to fill out our survey. We also sent the link to our friends and family members.\nBy asking the user to enter the data multiple times and repeat the process until they did not make any mistakes, we ensured that our data would be clean and consistent from the collection stage itself.\nThe data was then uploaded to Google's Firebase Cloud Storage platform as JSON files that we could download for further analysis on our local machine.\nAnalyzing the Data\nAt the end of our surveying and cleaning, we were able to collect 63 users with 5 trials each worth of data yielding a total of 315 typing samples. Each sample logged 90 different key events per trial.\nFeature Generation\nHere is a sample of our raw data: Our raw data consisted of timestamps for every keypress and key release a user made in a given trial. On its own, this data is not very useful. In order to provide this data some meaning, we extracted two main features from the dataset, seek time and duration.\nIn gait analysis with feet, analysts focus on the time it takes to switch legs, and the time a person spends on a single leg. We took the same concepts and applied them to typing to get our two main features.\nSeek Time - The time a user spends switching from one key to the following key\nDuration - The time a user hovers on a single key\nWe were able to compute both of the variables by simply subtracting the relevant timestamps from the raw data.\nCleaning the Data\nWe designed the experiment to promote data cleanliness from the get-go but overlooked one crucial factor: capital letters. On a keyboard, capital letters can be inputted using the shift key or the Caps Lock key. To ensure that each of the records was exactly the same , we pruned all records where participants used the Caps Lock key instead of the shift key. This was an unfortunate setback that impacted a small percentage of our dataset.\nValidating Our Theory (With Pictures ! )\nWe collected data based on a gut instinct. The most important question is, were we right? Well, we think so, and here's why:\nNote: Each point on the x-axis is one part of the sentence \"The quick brown fox jumped over the lazy dog\" We omitted the text labels on the axis for readability sake.\nConclusion 1: Humans do, in fact, have a habitual manner of typing\nThe first thing we confirmed was that typing data is not just a random blob. Here is a look at a sample trial for a single user's duration and seek. Each color represents an individual trial. As you can see, the lines are relatively close together and show slight variance aside from some outliers.\nTo further confirm this, we calculated the coefficient of variance (fancy DS term that means the standard deviation/mean) for each data point across multiple trials for each user. Any CV value that is less than one means that the data variance is statistically small. The following two plots show the CV's for each user for each letter in the sentence where each color is a user.\nThat's a lot of dots! The plot shows that most CV values are well under 0.5 and are thus highly similar! Wow!\nThe plots indicate that seek data varies slightly more than duration, so perhaps that might not be as important of a feature to use in models.\nConclusion 2: Human typing is distinguishable\nThe easiest way to prove that human typing is distinguishable is to build a model that has a high accuracy. So that's exactly what we did.\nAfter a long process of hyperparameter tuning, we were able to get a RandomForestClassifier to predict a user with an accuracy of 88.2%! To get this accuracy value without overfitting, we split the data into 67% training and 33% testing data.\nMore importantly, we created a confusion matrix to determine the False Positive and False Negative Rates. That is, when we accidentally let a bad guy login vs when we accidentally stop a good guy from logging in. The results speak for themselves.....\nThe false positive rate is infinitesemly small while the false negative rate is about 12%. This is very good because it means that the model will always keep the bad guy out and wrongly stop the good guy only 12% of the time when logging in.\nPotential Use Cases\nThis dataset has the potential to impact multiple industries to promote security and integrity.\nIndustry Use Case Description\nIndustry:\nCybersecurity Detect unauthorized login attempts The standard use case is detecting whenever an unauthorized user tries to access an account. Say that an attacker obtains your password. When they try to access your web app, the site will ask them extra verification questions because their typing habits were different than what was expected.\nIndustry:\nData Providers Detect bot activity vs. human activity The dataset indicates that there are many trends that all humans follow. Perhaps these can be used to build a classifier that can distinguish between human and bot activity and mitigate web scraping without intrusive CAPTCHAs.\nIndustry:\nAcademia Detect students impersonating others on exams When students are taking an online exam, the proctoring software could cross-check typing samples with other students to ensure that another student is not taking the test on someone else's behalf.\nReflections\nGoogle Firebase is excellent for quickly gathering data into a bucket and downloading it. Data scientists that are building quick surveys should definitely consider using it\nA significant and very important step of Data Science is properly cleaning the dataset. We spent hours debugging our analysis code when the problems were caused by a failure to correctly sanitize all of our input. Survey data from humans is tough to analyze\nCoding is good to know for data science, but math is way more important.\nThis dataset was only with the people at the datathon. I believe that it will become more valuable if we keep the website open and continue to gather data.", "link": "https://devpost.com/software/fishtype", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "project fishtype\ngithub: https://github.com/akil47/fishtype\nproblem: cybercrime is more rampant than ever\ntoday we live in a world where technology is advancing, rapidly simplifying our lives while making us utterly dependent on computers and digital networks. at the same time, data breaches and complex hacks have also become daily news, threatening our wealth, privacy, and security. these modern times mean that any and all measures of reinforcing our security are more vital than ever before.\nsolution: keystroke dynamics - applying gait analysis to typing\ngait analysis is the study of analyzing a person's walking habits to determine their identity. although real-time systems like the gif shown above don't fully exist, it has been scientifically proven that each human's gait has a set of uniqueness that is akin to a fingerprint.\nour idea is to take gait analysis from the feet and see if the same holds true for the fingers spoiler alert: it kind of does!\ncollecting the data\nto date, there is only a single dataset on kaggle that has anything related to keystroke data and identity. it has a whopping five rows where each user entered a string five characters long. our goal was to create an open dataset that has enough volume and reliability to help data scientists develop accurate models to solve the problems listed above\nwe designed a custom keystroke surveying -----> tool !!!  called fishtype (bc we're freshman) to collect the data. the code is on our github, so check it out!\nthe website had users type the phrase \"the quick brown fox jumped over the lazy dog\" five different times into a text box. if the users entered the phrase incorrectly, they were asked to type it again so that the dataset could remain consistent.\nwe went across the hall of champions and asked every single person that we saw to fill out our survey. we also sent the link to our friends and family members.\nby asking the user to enter the data multiple times and repeat the process until they did not make any mistakes, we ensured that our data would be clean and consistent from the collection stage itself.\nthe data was then uploaded to google's firebase cloud storage platform as json files that we could download for further analysis on our local machine.\nanalyzing the data\nat the end of our surveying and cleaning, we were able to collect 63 users with 5 trials each worth of data yielding a total of 315 typing samples. each sample logged 90 different key events per trial.\nfeature generation\nhere is a sample of our raw data: our raw data consisted of timestamps for every keypress and key release a user made in a given trial. on its own, this data is not very useful. in order to provide this data some meaning, we extracted two main features from the dataset, seek time and duration.\nin gait analysis with feet, analysts focus on the time it takes to switch legs, and the time a person spends on a single leg. we took the same concepts and applied them to typing to get our two main features.\nseek time - the time a user spends switching from one key to the following key\nduration - the time a user hovers on a single key\nwe were able to compute both of the variables by simply subtracting the relevant timestamps from the raw data.\ncleaning the data\nwe designed the experiment to promote data cleanliness from the get-go but overlooked one crucial factor: capital letters. on a keyboard, capital letters can be inputted using the shift key or the caps lock key. to ensure that each of the records was exactly the same , we pruned all records where participants used the caps lock key instead of the shift key. this was an unfortunate setback that impacted a small percentage of our dataset.\nvalidating our theory (with pictures ! )\nwe collected data based on a gut instinct. the most important question is, were we right? well, we think so, and here's why:\nnote: each point on the x-axis is one part of the sentence \"the quick brown fox jumped over the lazy dog\" we omitted the text labels on the axis for readability sake.\nconclusion 1: humans do, in fact, have a habitual manner of typing\nthe first thing we confirmed was that typing data is not just a random blob. here is a look at a sample trial for a single user's duration and seek. each color represents an individual trial. as you can see, the lines are relatively close together and show slight variance aside from some outliers.\nto further confirm this, we calculated the coefficient of variance (fancy ds term that means the standard deviation/mean) for each data point across multiple trials for each user. any cv value that is less than one means that the data variance is statistically small. the following two plots show the cv's for each user for each letter in the sentence where each color is a user.\nthat's a lot of dots! the plot shows that most cv values are well under 0.5 and are thus highly similar! wow!\nthe plots indicate that seek data varies slightly more than duration, so perhaps that might not be as important of a feature to use in models.\nconclusion 2: human typing is distinguishable\nthe easiest way to prove that human typing is distinguishable is to build a model that has a high accuracy. so that's exactly what we did.\nafter a long process of hyperparameter tuning, we were able to get a randomforestclassifier to predict a user with an accuracy of 88.2%! to get this accuracy value without overfitting, we split the data into 67% training and 33% testing data.\nmore importantly, we created a confusion matrix to determine the false positive and false negative rates. that is, when we accidentally let a bad guy login vs when we accidentally stop a good guy from logging in. the results speak for themselves.....\nthe false positive rate is infinitesemly small while the false negative rate is about 12%. this is very good because it means that the model will always keep the bad guy out and wrongly stop the good guy only 12% of the time when logging in.\npotential use cases\nthis dataset has the potential to impact multiple industries to promote security and integrity.\nindustry use case description\nindustry:\ncybersecurity detect unauthorized login attempts the standard use case is detecting whenever an unauthorized user tries to access an account. say that an attacker obtains your password. when they try to access your web app, the site will ask them extra verification questions because their typing habits were different than what was expected.\nindustry:\ndata providers detect bot activity vs. human activity the dataset indicates that there are many trends that all humans follow. perhaps these can be used to build a classifier that can distinguish between human and bot activity and mitigate web scraping without intrusive captchas.\nindustry:\nacademia detect students impersonating others on exams when students are taking an online exam, the proctoring software could cross-check typing samples with other students to ensure that another student is not taking the test on someone else's behalf.\nreflections\ngoogle firebase is excellent for quickly gathering data into a bucket and downloading it. data scientists that are building quick surveys should definitely consider using it\na significant and very important step of data science is properly cleaning the dataset. we spent hours debugging our analysis code when the problems were caused by a failure to correctly sanitize all of our input. survey data from humans is tough to analyze\ncoding is good to know for data science, but math is way more important.\nthis dataset was only with the people at the datathon. i believe that it will become more valuable if we keep the website open and continue to gather data.", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59507406}, {"Unnamed: 0": 7427, "autor": "EmotionSwitch", "date": null, "content": "Inspiration\nAccording to reports, there are 200 different kinds of underlying mental disorders that cannot be easily detected out of which Depression, Anxiety Disorders and Dementia are the most common. Shockingly, 1 in 5 people in the US is affected with some sort of mental disorder where 25 million people are solely affected by only depression! An even shocking fact is that 7% of people who experience depression commit suicide \ud83d\ude25. And that's what we are trying to say that Mental disorders are big and Mental health matters...\nAfter researching a lot we found an accurate identifier that will partially help in tackling this problem and it is Emotion. Emotions play an important role in our life, Apart from just expressing ourselves, they do a lot more. According to Harvard, Emotions are the best representation and revelation of one\u2019s mental health. Emotions are also the easiest way to find about someone\u2019s mental conditions. Using the inter-relation between emotions and mental health we developed an app and we called it EmotionSwitch\nWhat it does\nEmotionSwitch is a machine-learning-powered tool that uses powerful and complex algorithms to keep track of your emotions and mood swings to detect a probable underlying mental disorder before it's too late. All of this is done just from your webcam video feed..\nAll you need to do is visit the website and start a session after allowing your webcam to be captured then you can carry on with your other tasks while the web app keeps a real-time track of your emotions, you can also see your emotions varying on a realtime graph that is visible on the web app. After a suitable time, you can end the session when you feel like (an average good session is 15 minutes). After ending the session, the app will generate a detailed report with a lot of content including a potential mental disorder detection and some visual content including a graph and a pie chart. The app also provides a downloadable transcript which you can share with your therapist later on.\nHow we built it\nChallenges we ran into\nAccomplishments that we're proud of\nWe are very proud of the following things:\nCompleting such a complex app within 36 hours\nVery proud of our modelling approach, that's what makes us different from existing solutions\nIntegrating a video-based ml model without WebSockets directly into the flask\nHaving a nice UI\nMaking a real-time graph\nWhat we learned\nWe learned a heck of a lot of stuff during the hackathon and we're extremely thankful for that, some of the major stuff we learned:\nWe played with and learned a lot about flask and compiling a full-stack app\nWe mastered frontend skills and using frameworks to speed up the development process\nIntegrating a video-based machine learning model\nHosting complex and heavy full-stack apps online.\nTeam coordination skills\nWhat's next for EmotionSwitch\nWe had a lot planned for EmotionSwitch but we couldn't finish all of it in 36 hours. So we will cover that parts here:\nImproving the interface and design of the app\nAdding Medical records to the system and securing them.\nHaving it more personalized and saving all previous sessions.\nTest prototype with a psychologist/psychiatrist\nWe really had fun making this app. All of the code was written 100% by us", "link": "https://devpost.com/software/emotionswitch", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\naccording to reports, there are 200 different kinds of underlying mental disorders that cannot be easily detected out of which depression, anxiety disorders and dementia are the most common. shockingly, 1 in 5 people in the us is affected with some sort of mental disorder where 25 million people are solely affected by only depression! an even shocking fact is that 7% of people who experience depression commit suicide \ud83d\ude25. and that's what we are trying to say that mental disorders are big and mental health matters...\nafter researching a lot we found an accurate identifier that will partially help in tackling this problem and it is emotion. emotions play an important role in our life, apart from just expressing ourselves, they do a lot more. according to harvard, emotions are the best representation and revelation of one\u2019s mental health. emotions are also the easiest way to find about someone\u2019s mental conditions. using the inter-relation between emotions and mental health we developed an app and we called it emotionswitch\nwhat it does\nemotionswitch is a machine-learning-powered -----> tool !!!  that uses powerful and complex algorithms to keep track of your emotions and mood swings to detect a probable underlying mental disorder before it's too late. all of this is done just from your webcam video feed..\nall you need to do is visit the website and start a session after allowing your webcam to be captured then you can carry on with your other tasks while the web app keeps a real-time track of your emotions, you can also see your emotions varying on a realtime graph that is visible on the web app. after a suitable time, you can end the session when you feel like (an average good session is 15 minutes). after ending the session, the app will generate a detailed report with a lot of content including a potential mental disorder detection and some visual content including a graph and a pie chart. the app also provides a downloadable transcript which you can share with your therapist later on.\nhow we built it\nchallenges we ran into\naccomplishments that we're proud of\nwe are very proud of the following things:\ncompleting such a complex app within 36 hours\nvery proud of our modelling approach, that's what makes us different from existing solutions\nintegrating a video-based ml model without websockets directly into the flask\nhaving a nice ui\nmaking a real-time graph\nwhat we learned\nwe learned a heck of a lot of stuff during the hackathon and we're extremely thankful for that, some of the major stuff we learned:\nwe played with and learned a lot about flask and compiling a full-stack app\nwe mastered frontend skills and using frameworks to speed up the development process\nintegrating a video-based machine learning model\nhosting complex and heavy full-stack apps online.\nteam coordination skills\nwhat's next for emotionswitch\nwe had a lot planned for emotionswitch but we couldn't finish all of it in 36 hours. so we will cover that parts here:\nimproving the interface and design of the app\nadding medical records to the system and securing them.\nhaving it more personalized and saving all previous sessions.\ntest prototype with a psychologist/psychiatrist\nwe really had fun making this app. all of the code was written 100% by us", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59507427}, {"Unnamed: 0": 7429, "autor": "BIEVERY", "date": null, "content": "Inspiration\nWorking on the backend always thought about how can have a quick dashboard without much effort. An idea came into my mind that APIs are sources of data. If we can turn the data generated from API into relational data then we can use BI tools to connect to RDBMS.\nWhat it does\nBI tools act as a front end for data viewing. You can visualize data in beautiful charts and data grids. You can filter data using controls available in BI tools. You can interact with the BI tool just like a front end no need to trigger an application to dump data. You get updates sitting within the BI tool.\nEverything you need in one dashboard. Like you can have data from multiple Jira, Teamwork, Pivotal Tracker instances in one place. If the analysis dataset is small but highly volatile and you need updates in real-time then BIEVERY is for you because you don't have to wait for the regular intervals to see the results.\nBIEVERY can scale to support a big user base because we serve data in real-time and we are not piling up the data. Another dimension we can scale in is the API set we cover at little cost.\nBIEVERY is a step towards a single source of truth because sometimes developers are logging the same information in multiple systems like Jira and Teamwork. With BIEVERY there is no need to do that.\nHow we built it\nTo be generic written a routine (prototype) to convert JSON to relational data. The front end is built in PHP using Codeigniter. Backend is MariaDB\nChallenges we ran into\nJSON to relational conversion is challenging however I managed to come up with an implementation that allows cherry-picking JSON nodes and the routine converts Json paths to relational data with some limitations\nAccomplishments that we're proud of\nWe added a complete list of Jira, Tempo, Pivotal Tracker, and Teamwork under our portfolio\nWhat we learned\nSmart work!\nWhat's next for BIEVERY\nTo be the next Big Thing in Software World!", "link": "https://devpost.com/software/bievery-d13mer", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nworking on the backend always thought about how can have a quick dashboard without much effort. an idea came into my mind that apis are sources of data. if we can turn the data generated from api into relational data then we can use bi tools to connect to rdbms.\nwhat it does\nbi tools act as a front end for data viewing. you can visualize data in beautiful charts and data grids. you can filter data using controls available in bi tools. you can interact with the bi -----> tool !!!  just like a front end no need to trigger an application to dump data. you get updates sitting within the bi tool.\neverything you need in one dashboard. like you can have data from multiple jira, teamwork, pivotal tracker instances in one place. if the analysis dataset is small but highly volatile and you need updates in real-time then bievery is for you because you don't have to wait for the regular intervals to see the results.\nbievery can scale to support a big user base because we serve data in real-time and we are not piling up the data. another dimension we can scale in is the api set we cover at little cost.\nbievery is a step towards a single source of truth because sometimes developers are logging the same information in multiple systems like jira and teamwork. with bievery there is no need to do that.\nhow we built it\nto be generic written a routine (prototype) to convert json to relational data. the front end is built in php using codeigniter. backend is mariadb\nchallenges we ran into\njson to relational conversion is challenging however i managed to come up with an implementation that allows cherry-picking json nodes and the routine converts json paths to relational data with some limitations\naccomplishments that we're proud of\nwe added a complete list of jira, tempo, pivotal tracker, and teamwork under our portfolio\nwhat we learned\nsmart work!\nwhat's next for bievery\nto be the next big thing in software world!", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59507429}, {"Unnamed: 0": 7447, "autor": "MountainDew_Celonis_2021_Challenge", "date": null, "content": "Inspiration\nThe inspiration was to leverage the power of data visualizations to provide accurate recommendations for the end user just by looking at some of the process data.\nWhat it does\nThe project contains charts that can be provided to the owner of the Pizzeria that show various metrics that are relevant to the improvement in the rating, positive feedback, and increase revenue.\nHow we built it\nWe have built a model that conforms with the dataset of Mamma Mia Pizzeria with a conformity rate of 76%. The datasets were analyzed by making various charts and graphs using Celonis as well as python in local machine. Once the analysis was done, the model was built using the provided model as a reference. Using the mining tool of Celonis, the model was updated.\nChallenges we ran into\nBuilding the initial graphs and charts and doing the initial analysis with the small dataset was a challenge. Since PQL is a new language for us, writing the queries took more time than expected.\nAccomplishments that we're proud of\nWe were able to get a conformity rate of 76%, also reducing the total number of violations from 39 to 22.\nWhat we learned\nWe learned a new tool as well as a new query language. We also gained insight into the power of process data mining and how it can be used to improve processes.\nWhat's next for MountainDew_Celonis_2021_Challenge\nIt would be interesting the impact of recommendations learned from the various visualizations after performing process mining to see its real impact. This will help in gauging the impact of the process.", "link": "https://devpost.com/software/mountaindew_celonis_2021_challenge", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nthe inspiration was to leverage the power of data visualizations to provide accurate recommendations for the end user just by looking at some of the process data.\nwhat it does\nthe project contains charts that can be provided to the owner of the pizzeria that show various metrics that are relevant to the improvement in the rating, positive feedback, and increase revenue.\nhow we built it\nwe have built a model that conforms with the dataset of mamma mia pizzeria with a conformity rate of 76%. the datasets were analyzed by making various charts and graphs using celonis as well as python in local machine. once the analysis was done, the model was built using the provided model as a reference. using the mining -----> tool !!!  of celonis, the model was updated.\nchallenges we ran into\nbuilding the initial graphs and charts and doing the initial analysis with the small dataset was a challenge. since pql is a new language for us, writing the queries took more time than expected.\naccomplishments that we're proud of\nwe were able to get a conformity rate of 76%, also reducing the total number of violations from 39 to 22.\nwhat we learned\nwe learned a new tool as well as a new query language. we also gained insight into the power of process data mining and how it can be used to improve processes.\nwhat's next for mountaindew_celonis_2021_challenge\nit would be interesting the impact of recommendations learned from the various visualizations after performing process mining to see its real impact. this will help in gauging the impact of the process.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59507447}, {"Unnamed: 0": 7459, "autor": "Flight Booking Hacks", "date": null, "content": "How to Book the Cheapest Flight Possible to Anywhere\n1.Keep your searches top secret You're not crazy for thinking that a flight price has changed after searching it a few times in your web browser. Based on the cookies in your browser, flight prices do increase when a particular route is repeatedly searched, as the site wants to scare you into booking the flight quickly before prices get even higher. Always search for flights in incognito or private browsing mode to see the lowest prices. In Google Chrome or Safari, incognito is enabled by hitting Command (or \u201cControl\u201d if using PC), Shift, \u201cN\u201d. For Mozilla Firefox or Internet Explorer, hit Command (or \u201cControl\u201d if using a PC), Shift, \u201cP\u201d. This will open a new browser window where your information is not tracked, thus not inflating prices as you search. Note: if you're using an older version of OS X, open Safari then click \u201cSafari\u201d in the menu bar, and select \u201cPrivate Browsing\u201d.\nYour cookies are reset each time you re-open an incognito window. So if you want to start with a clean slate for each flight search (so your previous searches aren't \u201cremembered\u201d, potentially inflating costs), close all your incognito windows, open a new one, and then perform your flight search.\n1.Use the best flight search engines All search engines have inflated flight costs as part of taking a cut from the airlines. Some search engines (e.g. Expedia) consistently inflate much higher than others (listed below). It pays to familiarise yourself with sites that offer the best prices. As of 2020, most budget airlines will now appear in the broad search engines we are recommending (but NOT Southwest). If you want to be 100% sure though, you can do an additional search for regional budget airlines (we've listed as many we know of further below).\nFinally, no single search engine is consistently perfect (though we typically find the cheapest price on Skyscanner or Momondo). As such, you may need to try a combination of search engines to ensure you're not missing any results. There doesn't seem to be one that gets the cheapest flight 100% of the time.\n1.Identify the cheapest day to fly out While many theories exist around booking specifically on a Tuesday to save money, the reality is there is no consistent truth to exactly which days are cheapest to fly. Most of the time it is cheaper to leave on a weekday, though this isn't always the case. Your best strategy is to get a\nquick visual of prices for a whole month to see what days are cheapest for your specific route. Here's how: Step 1. Hop on the Skyscanner website or download the app\nStep 2. Enter your departure & arrival cities\nStep 3. Select \u201cone-way\u201d (even if flying round trip \u2013 you're just determining the cheapest days to fly out first)\nStep 4. On a computer: click \u201cDepart\u201d but instead of entering a date, select \u201cWhole Month\u201d, and then \u201cCheapest Month\u201d so you can browse all dates to see which is cheapest. Hit \u201cSearch flights\u201d to easily see which date is cheapest.\nIn the mobile app: Tap the departure date, then change the view to \u201cChart\u201d. You can easily swipe left and right to find the cheapest date, and tap on one of the bars to see the price.\nRepeat these steps for your return flight if applicable. You can still book a round trip in one booking, but doing these steps first will let you see which dates are generally cheaper to fly in/out on for your round trip.\nKiwi.com and Google Flights work similarly to Skyscanner, plus they have map views as well, so you can see where the airport is. For tracking when and where is cheapest to fly, Hopper also offer price analysis and track fluctuations (i.e. when is best to fly).\n1.Fly for free with points The cheapest flight you can book is free, and the easiest way to do that is with points. If you don't have any, don't fret \u2013 signing up for just one travel rewards credit card and hitting the minimum spend can land you a major points bonus, often enough for a free flight! You can sign up for an airline rewards card that earns points with a particular airline, or a more general travel rewards card that lets you redeem points across a variety of airlines. As a beginner card, we recommend the Chase Sapphire Preferred Card. You can earn up to 60,000 in bonus points after spending $4,000 in the first 3 months after your account opening. That's worth $750 when redeemed through Chase Ultimate Rewards, and more than enough for an international flight. And since it includes trip cancelation insurance, car rental collision damage waiver, lost luggage insurance, and no foreign transaction fees, it's the perfect all-rounder travel credit card. You can compare this card with other top credit cards.\n1.Befriend budget airlines Budget airlines offer significantly cheaper tickets than their full-service counterparts. It should be obvious, but this comes with compromises such as less leg room and no \u201cfree\u201d food/drink on-board (which by the way, is normally covered in your higher-priced ticket with full-service airlines). If you are considering flying budget, which in our opinion is a great way to save, I'd highly recommend reading our guide on 7 must-know facts about budget airlines.\nBelow is a comprehensive list of budget airlines around the globe. Red indicates true budget airlines while blue implies the cheapest companies available where budget airlines don't exist or aren't plentiful.\n1.Search for airline error and sale fares Airlines sometimes make mistakes when posting their fares, leading to seriously discounted flights. This can happen for various reasons \u2013 currency conversion mishaps, technical glitches, or human error. If you're in the know-how on where to find airline error fares, you can save yourself hundreds of dollars on a ticket. One of the most popular sites for tracking sale and error fares is Scott's Cheap Flights (or if you're in the UK, check out Jack's Flight Club). The website has a dedicated team that scours the internet for flight deals every day. Just make an account and enter your departure airport, and you'll start getting flight deals sent to your inbox. There's also a premium plan for only $49/year (a 14-day free trial is available), which gives you access to even more flight deals with even deeper discounts. Considering members save an average of $550 per ticket, this is well worth the cost!\nOther sites like Airfarewatchdog, Kiwi.com Deals, and Secret Flying are great resources to stalk for finding mistake and sale pricing, as they conglomerate slashed ticket rates all in one spot. You can also read our detailed how-to guide which spells out how to find mistake fares on your own. One great approach is to search for flights for an entire month using Skyscanner (Tip #3 in this article). This will allow you to easily spot a significantly reduced fare against what's displayed that month, and has twice helped us stumble on error fares ourselves.\n1.Book connecting flights yourself for less If you're flying somewhere that involves a transfer, say from Canada to Australia which typically involves Canada to LA, then LA to Australia, consider that it may be cheaper to book these two legs separately on your own by adding another destination to your trip. It should go without saying that in doing this, you should not book tight layovers. I repeat: do not book layovers that are hours apart! This approach is for those who want to create an additional destination of a few days or more, before catching their next flight. The one exception is when booking with Kiwi.com, who offer their own guarantee on making connecting flights even when not with the same partner airlines. First, do your research: are there budget airlines unique to the country you're flying out of and where you're headed to? Booking with a budget Australian airline (Jetstar) from Sydney to Honolulu, then an American one from Honolulu to Montreal saved us over $400 each when flying back from Australia to Canada earlier this year. This allowed us to create a thrifty five-day stopover in Hawaii on our way back, which was less exhausting and a lot cheaper! Kiwi.com and AirWander are both great search engine for revealing cheaper routes like this that involve multiple airlines.\nYou can even book your own multi-day layovers, essentially allowing you to see 2 destinations for the price of 1. Rather than spend a day sitting in the airport, you can spend multiple days exploring the city you are laying over in. AirWander is a specialized search engine for doing exactly this. Put in your origin, final destination, and number of days you want to stopover. AirWander will return a list of places you can visit on your stopover, often even cheaper than a regular flight search engine! To learn how to do this, read our guide on How to Get Free Extended Layovers & Hack One Trip Into Two.\n1.Find the cheapest place to fly Whether you know exactly where you're going or you just want to find to the cheapest possible country to fly into, Kiwi.com is a great tool to get the wanderlust going and save some big bucks. Hop on their site and enter your departure city, then select a date range to fly. Approximate costs then appear over hundreds of countries around the globe from your departure point, while the list of destinations is sorted by price, allowing you to see the most cost-effective place you can fly.\n2.Mix and match airlines Where Kiwi.com really shines is the way in which it mixes and matches airlines in order to find the cheapest price. For example, maybe you want to go to Rome, Italy from Washington, USA. A typical flight search engine will only suggest routes coming from a single airline and its partners. An example search on Expedia shows the cheapest route as $631.20 USD via TAP Portugal.\nKiwi.com, on the other hand, will mix and match airlines (including budget airlines) in order to find you the very cheapest route. For long-haul flights especially, this can make a huge difference. The same search on Kiwi.com returns a route at $459.80 USD via JetBlue, Norwegian Air, and Vueling. That's a savings of $171.40 USD, and the travel time is even shorter!\nTypically, booking a whole trip with different airlines would be risky. For example, let's say your first flight with JetBlue was delayed, and you missed your connecting flight with Norwegian Air. Because the airlines have no association with each other, Norwegian Air has no obligation to reschedule your flight for free, so you would just lose your money. Kiwi.com instead offers their own guarantee, which covers schedule changes, flight delays, and cancellations. As long as you contact Kiwi.com as soon as you're aware of the delay, they will provide you with an alternate connecting flight, or a full refund, at your discretion. We haven't used this guarantee ourselves, but it certainly sounds like an appealing way to take the risk out of a thrifty flight hack!\n1.Consider hidden city ticketing Travellers long ago discovered a trick known now as \u201chidden city\u201d flights. In a nutshell, sometimes a flight that connects in a city you want to go to is cheaper than flying directly to it. So instead, you book that cheap flight which connects in your desired city and hop off there, not taking the ongoing connection. It should go without saying this is risky for many reasons. Here are some factors to consider:\nIf your luggage is checked, it may go on to the final destination. For this reason, with hidden city ticketing it's best to have carry-on luggage only. You may not be allowed off the plane (if the same plane is continuing onward to its final destination). Airlines may detect that you did not take your connection. The consequences of this are hard to say, especially since people miss flights all the time. There is now a web search engine which finds hidden city tickets for you. It's called Skiplagged and was even sued by United Airlines who were angry about this debatable hack. We have never used the hidden city approach, but I know several who have, and they've scored some thrifty savings. Use at your own risk!\n1.Buy flights in bulk As a general rule, you will get better value on your flight tickets if you buy more at the same time, but only if they are with the same airline, or airlines in the same alliance (e.g. Star Alliance). For example, flying a return ticket from New York City to London with United Airlines will be cheaper than 2 one-way tickets. You can also take advantage of this on multi-city flights. For example, Virgin Atlantic has routes from New York City to London, London to Shanghai, and Shanghai to New York City. If you book all these at the same time using the multi-city search function on Skyscanner, you'll save some serious cash.\nAnother way to bulk buy flights for less is with round the world tickets and regional passes. These are special tickets offered by airline alliances that let you go around the world, a continent, or a country at a discounted rate. To learn how to book these, be sure to check out our ultimate guide to round the world tickets.\nAll this said, sometimes a few separately booked flights with budget airlines is still cheaper than what partner airlines can offer. The only way to be sure is to use a site like Skyscanner, punch in your destinations, search as multi-city or return vs multiple one-way bookings, and compare yourself.\n1.Don't forget about local airlines While the above search engines are great, they do not always include small airlines, especially in less popularly booked routes and/or in remote regions. If you're flying somewhere obscure, Google search and ask around if there exists a local airline. While in South America we learnt that the LADE Air in Argentina (run by the military) has cheap flights to Patagonia, which is of course not listed in mass search engines online. When you do find small airlines, even if they are listed in a search engine results, it often pays to check the company site which may reveal exclusive online offers not found in a regular search engine. For example, when flying in Western Canada, I found that Hawk Air, a small and local company offers weekly deals on certain days. Be sure to double check!\n1.If you know when and where you're going, don't wait to book Rarely ever do airline tickets get cheaper as your departure date approaches, especially if you need to fly on a certain date. Budget airlines typically offer low rates as a baseline price, and as these tickets sell, the remaining ones increase in cost. This is very typical in Europe and Australia. If you know when and where you're going, don't wait on an unknown sale. More often than not, your biggest savings come from booking far ahead when you can.\n2.Check if it's cheaper to pay in other currencies Before booking a flight, consider if the rate is cheaper if paid in another currency. Often budget airlines will make you pay in the currency of the country you're departing from, but this isn't always the case. An important note when doing this: make sure you're using a credit card that is free of foreign-transaction fees, such as the Chase Sapphire Preferred Card, otherwise your attempts to save money doing this will be lost! Our article on money matters for world travellers can help steer you in the right direction for the best credit cards for travel.", "link": "https://devpost.com/software/flight-booking-hacks-hs9dpl", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "how to book the cheapest flight possible to anywhere\n1.keep your searches top secret you're not crazy for thinking that a flight price has changed after searching it a few times in your web browser. based on the cookies in your browser, flight prices do increase when a particular route is repeatedly searched, as the site wants to scare you into booking the flight quickly before prices get even higher. always search for flights in incognito or private browsing mode to see the lowest prices. in google chrome or safari, incognito is enabled by hitting command (or \u201ccontrol\u201d if using pc), shift, \u201cn\u201d. for mozilla firefox or internet explorer, hit command (or \u201ccontrol\u201d if using a pc), shift, \u201cp\u201d. this will open a new browser window where your information is not tracked, thus not inflating prices as you search. note: if you're using an older version of os x, open safari then click \u201csafari\u201d in the menu bar, and select \u201cprivate browsing\u201d.\nyour cookies are reset each time you re-open an incognito window. so if you want to start with a clean slate for each flight search (so your previous searches aren't \u201cremembered\u201d, potentially inflating costs), close all your incognito windows, open a new one, and then perform your flight search.\n1.use the best flight search engines all search engines have inflated flight costs as part of taking a cut from the airlines. some search engines (e.g. expedia) consistently inflate much higher than others (listed below). it pays to familiarise yourself with sites that offer the best prices. as of 2020, most budget airlines will now appear in the broad search engines we are recommending (but not southwest). if you want to be 100% sure though, you can do an additional search for regional budget airlines (we've listed as many we know of further below).\nfinally, no single search engine is consistently perfect (though we typically find the cheapest price on skyscanner or momondo). as such, you may need to try a combination of search engines to ensure you're not missing any results. there doesn't seem to be one that gets the cheapest flight 100% of the time.\n1.identify the cheapest day to fly out while many theories exist around booking specifically on a tuesday to save money, the reality is there is no consistent truth to exactly which days are cheapest to fly. most of the time it is cheaper to leave on a weekday, though this isn't always the case. your best strategy is to get a\nquick visual of prices for a whole month to see what days are cheapest for your specific route. here's how: step 1. hop on the skyscanner website or download the app\nstep 2. enter your departure & arrival cities\nstep 3. select \u201cone-way\u201d (even if flying round trip \u2013 you're just determining the cheapest days to fly out first)\nstep 4. on a computer: click \u201cdepart\u201d but instead of entering a date, select \u201cwhole month\u201d, and then \u201ccheapest month\u201d so you can browse all dates to see which is cheapest. hit \u201csearch flights\u201d to easily see which date is cheapest.\nin the mobile app: tap the departure date, then change the view to \u201cchart\u201d. you can easily swipe left and right to find the cheapest date, and tap on one of the bars to see the price.\nrepeat these steps for your return flight if applicable. you can still book a round trip in one booking, but doing these steps first will let you see which dates are generally cheaper to fly in/out on for your round trip.\nkiwi.com and google flights work similarly to skyscanner, plus they have map views as well, so you can see where the airport is. for tracking when and where is cheapest to fly, hopper also offer price analysis and track fluctuations (i.e. when is best to fly).\n1.fly for free with points the cheapest flight you can book is free, and the easiest way to do that is with points. if you don't have any, don't fret \u2013 signing up for just one travel rewards credit card and hitting the minimum spend can land you a major points bonus, often enough for a free flight! you can sign up for an airline rewards card that earns points with a particular airline, or a more general travel rewards card that lets you redeem points across a variety of airlines. as a beginner card, we recommend the chase sapphire preferred card. you can earn up to 60,000 in bonus points after spending $4,000 in the first 3 months after your account opening. that's worth $750 when redeemed through chase ultimate rewards, and more than enough for an international flight. and since it includes trip cancelation insurance, car rental collision damage waiver, lost luggage insurance, and no foreign transaction fees, it's the perfect all-rounder travel credit card. you can compare this card with other top credit cards.\n1.befriend budget airlines budget airlines offer significantly cheaper tickets than their full-service counterparts. it should be obvious, but this comes with compromises such as less leg room and no \u201cfree\u201d food/drink on-board (which by the way, is normally covered in your higher-priced ticket with full-service airlines). if you are considering flying budget, which in our opinion is a great way to save, i'd highly recommend reading our guide on 7 must-know facts about budget airlines.\nbelow is a comprehensive list of budget airlines around the globe. red indicates true budget airlines while blue implies the cheapest companies available where budget airlines don't exist or aren't plentiful.\n1.search for airline error and sale fares airlines sometimes make mistakes when posting their fares, leading to seriously discounted flights. this can happen for various reasons \u2013 currency conversion mishaps, technical glitches, or human error. if you're in the know-how on where to find airline error fares, you can save yourself hundreds of dollars on a ticket. one of the most popular sites for tracking sale and error fares is scott's cheap flights (or if you're in the uk, check out jack's flight club). the website has a dedicated team that scours the internet for flight deals every day. just make an account and enter your departure airport, and you'll start getting flight deals sent to your inbox. there's also a premium plan for only $49/year (a 14-day free trial is available), which gives you access to even more flight deals with even deeper discounts. considering members save an average of $550 per ticket, this is well worth the cost!\nother sites like airfarewatchdog, kiwi.com deals, and secret flying are great resources to stalk for finding mistake and sale pricing, as they conglomerate slashed ticket rates all in one spot. you can also read our detailed how-to guide which spells out how to find mistake fares on your own. one great approach is to search for flights for an entire month using skyscanner (tip #3 in this article). this will allow you to easily spot a significantly reduced fare against what's displayed that month, and has twice helped us stumble on error fares ourselves.\n1.book connecting flights yourself for less if you're flying somewhere that involves a transfer, say from canada to australia which typically involves canada to la, then la to australia, consider that it may be cheaper to book these two legs separately on your own by adding another destination to your trip. it should go without saying that in doing this, you should not book tight layovers. i repeat: do not book layovers that are hours apart! this approach is for those who want to create an additional destination of a few days or more, before catching their next flight. the one exception is when booking with kiwi.com, who offer their own guarantee on making connecting flights even when not with the same partner airlines. first, do your research: are there budget airlines unique to the country you're flying out of and where you're headed to? booking with a budget australian airline (jetstar) from sydney to honolulu, then an american one from honolulu to montreal saved us over $400 each when flying back from australia to canada earlier this year. this allowed us to create a thrifty five-day stopover in hawaii on our way back, which was less exhausting and a lot cheaper! kiwi.com and airwander are both great search engine for revealing cheaper routes like this that involve multiple airlines.\nyou can even book your own multi-day layovers, essentially allowing you to see 2 destinations for the price of 1. rather than spend a day sitting in the airport, you can spend multiple days exploring the city you are laying over in. airwander is a specialized search engine for doing exactly this. put in your origin, final destination, and number of days you want to stopover. airwander will return a list of places you can visit on your stopover, often even cheaper than a regular flight search engine! to learn how to do this, read our guide on how to get free extended layovers & hack one trip into two.\n1.find the cheapest place to fly whether you know exactly where you're going or you just want to find to the cheapest possible country to fly into, kiwi.com is a great -----> tool !!!  to get the wanderlust going and save some big bucks. hop on their site and enter your departure city, then select a date range to fly. approximate costs then appear over hundreds of countries around the globe from your departure point, while the list of destinations is sorted by price, allowing you to see the most cost-effective place you can fly.\n2.mix and match airlines where kiwi.com really shines is the way in which it mixes and matches airlines in order to find the cheapest price. for example, maybe you want to go to rome, italy from washington, usa. a typical flight search engine will only suggest routes coming from a single airline and its partners. an example search on expedia shows the cheapest route as $631.20 usd via tap portugal.\nkiwi.com, on the other hand, will mix and match airlines (including budget airlines) in order to find you the very cheapest route. for long-haul flights especially, this can make a huge difference. the same search on kiwi.com returns a route at $459.80 usd via jetblue, norwegian air, and vueling. that's a savings of $171.40 usd, and the travel time is even shorter!\ntypically, booking a whole trip with different airlines would be risky. for example, let's say your first flight with jetblue was delayed, and you missed your connecting flight with norwegian air. because the airlines have no association with each other, norwegian air has no obligation to reschedule your flight for free, so you would just lose your money. kiwi.com instead offers their own guarantee, which covers schedule changes, flight delays, and cancellations. as long as you contact kiwi.com as soon as you're aware of the delay, they will provide you with an alternate connecting flight, or a full refund, at your discretion. we haven't used this guarantee ourselves, but it certainly sounds like an appealing way to take the risk out of a thrifty flight hack!\n1.consider hidden city ticketing travellers long ago discovered a trick known now as \u201chidden city\u201d flights. in a nutshell, sometimes a flight that connects in a city you want to go to is cheaper than flying directly to it. so instead, you book that cheap flight which connects in your desired city and hop off there, not taking the ongoing connection. it should go without saying this is risky for many reasons. here are some factors to consider:\nif your luggage is checked, it may go on to the final destination. for this reason, with hidden city ticketing it's best to have carry-on luggage only. you may not be allowed off the plane (if the same plane is continuing onward to its final destination). airlines may detect that you did not take your connection. the consequences of this are hard to say, especially since people miss flights all the time. there is now a web search engine which finds hidden city tickets for you. it's called skiplagged and was even sued by united airlines who were angry about this debatable hack. we have never used the hidden city approach, but i know several who have, and they've scored some thrifty savings. use at your own risk!\n1.buy flights in bulk as a general rule, you will get better value on your flight tickets if you buy more at the same time, but only if they are with the same airline, or airlines in the same alliance (e.g. star alliance). for example, flying a return ticket from new york city to london with united airlines will be cheaper than 2 one-way tickets. you can also take advantage of this on multi-city flights. for example, virgin atlantic has routes from new york city to london, london to shanghai, and shanghai to new york city. if you book all these at the same time using the multi-city search function on skyscanner, you'll save some serious cash.\nanother way to bulk buy flights for less is with round the world tickets and regional passes. these are special tickets offered by airline alliances that let you go around the world, a continent, or a country at a discounted rate. to learn how to book these, be sure to check out our ultimate guide to round the world tickets.\nall this said, sometimes a few separately booked flights with budget airlines is still cheaper than what partner airlines can offer. the only way to be sure is to use a site like skyscanner, punch in your destinations, search as multi-city or return vs multiple one-way bookings, and compare yourself.\n1.don't forget about local airlines while the above search engines are great, they do not always include small airlines, especially in less popularly booked routes and/or in remote regions. if you're flying somewhere obscure, google search and ask around if there exists a local airline. while in south america we learnt that the lade air in argentina (run by the military) has cheap flights to patagonia, which is of course not listed in mass search engines online. when you do find small airlines, even if they are listed in a search engine results, it often pays to check the company site which may reveal exclusive online offers not found in a regular search engine. for example, when flying in western canada, i found that hawk air, a small and local company offers weekly deals on certain days. be sure to double check!\n1.if you know when and where you're going, don't wait to book rarely ever do airline tickets get cheaper as your departure date approaches, especially if you need to fly on a certain date. budget airlines typically offer low rates as a baseline price, and as these tickets sell, the remaining ones increase in cost. this is very typical in europe and australia. if you know when and where you're going, don't wait on an unknown sale. more often than not, your biggest savings come from booking far ahead when you can.\n2.check if it's cheaper to pay in other currencies before booking a flight, consider if the rate is cheaper if paid in another currency. often budget airlines will make you pay in the currency of the country you're departing from, but this isn't always the case. an important note when doing this: make sure you're using a credit card that is free of foreign-transaction fees, such as the chase sapphire preferred card, otherwise your attempts to save money doing this will be lost! our article on money matters for world travellers can help steer you in the right direction for the best credit cards for travel.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59507459}, {"Unnamed: 0": 7481, "autor": "seneSENSE - HackTheValley", "date": null, "content": "Inspiration\nPromoting human longevity and the overall healthcare system through hacking.\nWhat does it do?\nThe seneSENSE tool aids in monitoring senescent cells more frequently for patients. This can help healthcare providers identify whether a patient is at risk for developing age-related skin diseases, as excessive senescent cells can be harmful for one's wellbeing.\nHow we built it\nPython - Tensorflow, matplotlib, Flask, HTML, CSS\nChallenges we ran into\nThe connection between all the components of the project (HTML to Flask to Tensorflow).\nWhat we learned\nThe implementation of a machine learning back-end on an HTML front-end.", "link": "https://devpost.com/software/senesense-hackthevalley", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\npromoting human longevity and the overall healthcare system through hacking.\nwhat does it do?\nthe senesense -----> tool !!!  aids in monitoring senescent cells more frequently for patients. this can help healthcare providers identify whether a patient is at risk for developing age-related skin diseases, as excessive senescent cells can be harmful for one's wellbeing.\nhow we built it\npython - tensorflow, matplotlib, flask, html, css\nchallenges we ran into\nthe connection between all the components of the project (html to flask to tensorflow).\nwhat we learned\nthe implementation of a machine learning back-end on an html front-end.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59507481}, {"Unnamed: 0": 7509, "autor": "Accurate", "date": null, "content": "Note\nWe were compelled to abandon the Twitter portion of our project after the \"rapid unplanned disassembly\" of the backend engineer's computer's disc. At the moment, it's just a tool for interacting with a backend that specialises in sentiment analysis. We've left the README in its current state below so you can see what we were close to accomplishing if it weren't for Rocky Mountain Power (thats right, I called them out. Fight me).\nThe reason why we don\u2019t have a full-fledged website isn\u2019t because of a faulty backend or a faulty front end but rather it\u2019s a connectivity issue between the two.\nCross-Origin Resource Sharing (CORS). CORS is a standardization of HTTP headers that specifies the origin of the sender (e.g. the domain). The purpose of CORS is to let the browser know if it should permit the loading resource. CORS only works over HTTP (unencrypted HTTPS). The website is HTTPS, but the API is not (HTTP).\nCORS disallows the loading of HTTP resources from HTTPS, which means we can\u2019t use our API (fully-working) from our frontend. There are three ways we could get around this:\nDisable browser safety and protection, which is is impossible for obvious reasons.\nGet the API an SSL certificate (thus allowing for HTTPS). Also impossible (it would require changing IP routing tables over SSH on a remote server; synonym for a bad idea).\nGet the API and the Frontend under the same domain. The last idea, get them under the same domain. We cannot do this because we do not own the domain we use (we use Netlify).\nOur API: http://166.70.232.119:443/ (single endpoint, query, with a single variable q. Example: http://166.70.232.119:443/query?q=my%20cat%20killed%20eighty-seven%20door%20to%20door%20salemen)\nInspiration\nWe were motivated to create a tool to try to discern the difference between what is trustworthy and what is not after observing the widespread misuse of social media and the spread of fake news via social media. Your Aunt Velma will no longer be able to claim that Bonobos are using 5G to spread poisons into millions of people's drinking water! Doesn't it sound fantastic...?\nWhat it does\nAccurate is a tool that can help you figure out which online sources are the least trustworthy. This is accomplished by gathering information on which sources are related to which social media posts (in this case, Twitter). After this information is gathered, NLP is used to identify basic intent, and the result is skewed by the popularity of the source. Following that, the data would be traced back to the social media site, and some factors might be linked to identifiers (e.g. hashtags).\nData Collection:\nA bot will be used to start with one post and then go on to others, collecting information along the way. After that, sources are gathered, and all data is converted to a format that the Data Analyzing engine can understand.\nData Analysing:\nUsing Natural Language Processing to first determine a source's purpose, then biassing it with metadata acquired during data collection. Data is looped back to social media, where identifiers (hashtags) are assigned a trustworthiness grade.\nData Visualisation:\nWe've also built a web application to enable users to engage with our tool, which provides the user with accuracy of specific posts and hashtags, as well as posts related to the user's post in a visually appealing manner.\nHow we built it\nThis project is made of a few different components:\nCrawler: Rust, Asynchronous processing (Tokio + Hyper), Web scraping Twitter, JSON\nAI & NLP: Natural Language Sentiment Analysis, Python (Tensorflow, Pandas, NLTK), R (ggplot), Lots of homegrown Data\nBackend: Rust, Asynchronous processing (Tokio + Hyper), Data aggregation, REST Arch.\nFrontend: HTML, CSS, Javascript, JSON\nPitch Deck: Figma\nCloning repo\ngit clone https://github.com/kern-3/accurate.git\ncd accurate\nCrawler\nThis is the peice that grabs social media posts and sources, and bundles them into a file that the NLP software. The crawler is built in Rust, so you can just use the cargo utility that we all know and love :)\nRust\nAsynchronous processing (Tokio + Hyper)\nWeb scraping Twitter\nJSON\ncd crawler\ncargo build --release\n# Find the binary where it normally is (target/release/accurate-crawler)!\n# View the help message for invocation.\nAI & NLP (natural language processing)\nThe Natural Language Processing aspect of this project is one of the most critical parts. It should only be needed to run once per group of crawler output data. This is built in Python, so you can run it as normal (but download the dependencies first!).\nNatural Language Sentiment Analysis\nR (ggplot)\nPython (Tensorflow, Pandas, NLTK)\nLots of homegrown data\nsudo pip3 -r requirements.txt\npython3 accurate-nlp.py\n# View the help message for invocation.\nBackend\nThe view from the outside world! This peice assembles crawler and NLP data into a final product, which is then accessible via a REST HTTP server. This is also built in Rust, so build it the same way as the crawler!\nRust\nAsynchronous processing (Tokio + Hyper)\nData aggregation\nREST architecture\ncd backend\ncargo build --release\n# Find the binary where it normally is (target/release/accurate-backend)\n# View the help message for invocation.\nFrontend\nWhats the point of a backend without a front end\u203d This is hosted on a server that the website domain points to. To host it on your local computer, do this!\nHTML\nCSS\nJavascript\nJSON\nFigma\ncd frontend\n# Open the index.html file\nChallenges we ran into\nOne of the most significant challenges we faced during the hackathon was a 13-hour power outage at one of our teammates' location, which prevented us from progressing because we needed data that he had to provide for further predicting the outcomes from that data and aggregating it, as well as connecting the frontend, backend, and model, but it's something no one could avoid.\nAlso one of the issues we encountered was obtaining the enormous amount of data we required. We concluded that using already gathered data would be against the spirit of the competition, so we opted to collect our own. We discovered that Web Scraping Twitter was the ideal tool for the job because of a combination of incredibly fast networking rates and a highly parallel processor.\nAccomplishments that we're proud of\nThis is our (kern3\u2019s) first project ! As a result, we're pleased to be capable of doing anything like this. We're also pleased with how we divided the tasks among ourselves which turned out to be very efficient.\nWe successfully finished with the model and were able to achieve 92% model accuracy on test data, which we were pleased with given the obstacles and time constraints.\nPleased with the website's visual design.\nWe were ecstatic since we were able to work with a variety of languages and technologies from various fields and link them together to create Accurate.\nWhat we learned\nWe got a lot of experience in how to aggregate data, and how to connect different systems together, especially with so many different languages interacting with each other.\nWe all learned a lot about how to interact in reasonably large and complicated projects because this was our first/second hackathon, and we had a great time working in our team.\nWhat's next for Accurate\nGiven the time (and possible financial backing), we would love to package this technology into a browser extension, for easier, inline, and informed social media interaction.\nWe'd also like to expand our reach to various social media sites and check for the accuracy of the information shared there too.", "link": "https://devpost.com/software/accurate", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "note\nwe were compelled to abandon the twitter portion of our project after the \"rapid unplanned disassembly\" of the backend engineer's computer's disc. at the moment, it's just a -----> tool !!!  for interacting with a backend that specialises in sentiment analysis. we've left the readme in its current state below so you can see what we were close to accomplishing if it weren't for rocky mountain power (thats right, i called them out. fight me).\nthe reason why we don\u2019t have a full-fledged website isn\u2019t because of a faulty backend or a faulty front end but rather it\u2019s a connectivity issue between the two.\ncross-origin resource sharing (cors). cors is a standardization of http headers that specifies the origin of the sender (e.g. the domain). the purpose of cors is to let the browser know if it should permit the loading resource. cors only works over http (unencrypted https). the website is https, but the api is not (http).\ncors disallows the loading of http resources from https, which means we can\u2019t use our api (fully-working) from our frontend. there are three ways we could get around this:\ndisable browser safety and protection, which is is impossible for obvious reasons.\nget the api an ssl certificate (thus allowing for https). also impossible (it would require changing ip routing tables over ssh on a remote server; synonym for a bad idea).\nget the api and the frontend under the same domain. the last idea, get them under the same domain. we cannot do this because we do not own the domain we use (we use netlify).\nour api: http://166.70.232.119:443/ (single endpoint, query, with a single variable q. example: http://166.70.232.119:443/query?q=my%20cat%20killed%20eighty-seven%20door%20to%20door%20salemen)\ninspiration\nwe were motivated to create a tool to try to discern the difference between what is trustworthy and what is not after observing the widespread misuse of social media and the spread of fake news via social media. your aunt velma will no longer be able to claim that bonobos are using 5g to spread poisons into millions of people's drinking water! doesn't it sound fantastic...?\nwhat it does\naccurate is a tool that can help you figure out which online sources are the least trustworthy. this is accomplished by gathering information on which sources are related to which social media posts (in this case, twitter). after this information is gathered, nlp is used to identify basic intent, and the result is skewed by the popularity of the source. following that, the data would be traced back to the social media site, and some factors might be linked to identifiers (e.g. hashtags).\ndata collection:\na bot will be used to start with one post and then go on to others, collecting information along the way. after that, sources are gathered, and all data is converted to a format that the data analyzing engine can understand.\ndata analysing:\nusing natural language processing to first determine a source's purpose, then biassing it with metadata acquired during data collection. data is looped back to social media, where identifiers (hashtags) are assigned a trustworthiness grade.\ndata visualisation:\nwe've also built a web application to enable users to engage with our tool, which provides the user with accuracy of specific posts and hashtags, as well as posts related to the user's post in a visually appealing manner.\nhow we built it\nthis project is made of a few different components:\ncrawler: rust, asynchronous processing (tokio + hyper), web scraping twitter, json\nai & nlp: natural language sentiment analysis, python (tensorflow, pandas, nltk), r (ggplot), lots of homegrown data\nbackend: rust, asynchronous processing (tokio + hyper), data aggregation, rest arch.\nfrontend: html, css, javascript, json\npitch deck: figma\ncloning repo\ngit clone https://github.com/kern-3/accurate.git\ncd accurate\ncrawler\nthis is the peice that grabs social media posts and sources, and bundles them into a file that the nlp software. the crawler is built in rust, so you can just use the cargo utility that we all know and love :)\nrust\nasynchronous processing (tokio + hyper)\nweb scraping twitter\njson\ncd crawler\ncargo build --release\n# find the binary where it normally is (target/release/accurate-crawler)!\n# view the help message for invocation.\nai & nlp (natural language processing)\nthe natural language processing aspect of this project is one of the most critical parts. it should only be needed to run once per group of crawler output data. this is built in python, so you can run it as normal (but download the dependencies first!).\nnatural language sentiment analysis\nr (ggplot)\npython (tensorflow, pandas, nltk)\nlots of homegrown data\nsudo pip3 -r requirements.txt\npython3 accurate-nlp.py\n# view the help message for invocation.\nbackend\nthe view from the outside world! this peice assembles crawler and nlp data into a final product, which is then accessible via a rest http server. this is also built in rust, so build it the same way as the crawler!\nrust\nasynchronous processing (tokio + hyper)\ndata aggregation\nrest architecture\ncd backend\ncargo build --release\n# find the binary where it normally is (target/release/accurate-backend)\n# view the help message for invocation.\nfrontend\nwhats the point of a backend without a front end\u203d this is hosted on a server that the website domain points to. to host it on your local computer, do this!\nhtml\ncss\njavascript\njson\nfigma\ncd frontend\n# open the index.html file\nchallenges we ran into\none of the most significant challenges we faced during the hackathon was a 13-hour power outage at one of our teammates' location, which prevented us from progressing because we needed data that he had to provide for further predicting the outcomes from that data and aggregating it, as well as connecting the frontend, backend, and model, but it's something no one could avoid.\nalso one of the issues we encountered was obtaining the enormous amount of data we required. we concluded that using already gathered data would be against the spirit of the competition, so we opted to collect our own. we discovered that web scraping twitter was the ideal tool for the job because of a combination of incredibly fast networking rates and a highly parallel processor.\naccomplishments that we're proud of\nthis is our (kern3\u2019s) first project ! as a result, we're pleased to be capable of doing anything like this. we're also pleased with how we divided the tasks among ourselves which turned out to be very efficient.\nwe successfully finished with the model and were able to achieve 92% model accuracy on test data, which we were pleased with given the obstacles and time constraints.\npleased with the website's visual design.\nwe were ecstatic since we were able to work with a variety of languages and technologies from various fields and link them together to create accurate.\nwhat we learned\nwe got a lot of experience in how to aggregate data, and how to connect different systems together, especially with so many different languages interacting with each other.\nwe all learned a lot about how to interact in reasonably large and complicated projects because this was our first/second hackathon, and we had a great time working in our team.\nwhat's next for accurate\ngiven the time (and possible financial backing), we would love to package this technology into a browser extension, for easier, inline, and informed social media interaction.\nwe'd also like to expand our reach to various social media sites and check for the accuracy of the information shared there too.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59507509}, {"Unnamed: 0": 7515, "autor": "Email Verifier", "date": null, "content": "What is Email Verification?\nEmail verification involves checking the deliverability of the email addresses you are collecting thanks to your lead generation initiatives. Having a lot of emails in your database is worthless unless they\u2019ll receive your messages.\nOur free email verifier doesn\u2019t simply check if an email looks valid. It actually performs several checks to ensure that an email can be delivered to a specific email address. Email verification includes checking the domain name exists, if it has a correctly configured email server, and, finally, if the server will accept the email address provided.\nThe email checker makes sure that the emails are genuine and are likely to connect you to a real person (or, at the least, a real email account of an organization.)\nWhat is the purpose of Email verification? An email verifier, or email checker, is a tool to validate that an email address exists and can receive emails. Verifying is important before contacting new recipients to avoid bounces and protect your sender reputation.\nHunter's Email Verifier does a complete check of email addresses, verifies if it appears publicly on the web and returns the sources.\nHow we built it\nWe built it with the help a set of python codes.\nChallenges we ran into\nAfter 3 - 4 times changes and re-execution we are able built a prefect set of codes.\nWhat we learned\nWe learned how to create a Email Verification API.", "link": "https://devpost.com/software/email-verifier-83d7fj", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what is email verification?\nemail verification involves checking the deliverability of the email addresses you are collecting thanks to your lead generation initiatives. having a lot of emails in your database is worthless unless they\u2019ll receive your messages.\nour free email verifier doesn\u2019t simply check if an email looks valid. it actually performs several checks to ensure that an email can be delivered to a specific email address. email verification includes checking the domain name exists, if it has a correctly configured email server, and, finally, if the server will accept the email address provided.\nthe email checker makes sure that the emails are genuine and are likely to connect you to a real person (or, at the least, a real email account of an organization.)\nwhat is the purpose of email verification? an email verifier, or email checker, is a -----> tool !!!  to validate that an email address exists and can receive emails. verifying is important before contacting new recipients to avoid bounces and protect your sender reputation.\nhunter's email verifier does a complete check of email addresses, verifies if it appears publicly on the web and returns the sources.\nhow we built it\nwe built it with the help a set of python codes.\nchallenges we ran into\nafter 3 - 4 times changes and re-execution we are able built a prefect set of codes.\nwhat we learned\nwe learned how to create a email verification api.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59507515}, {"Unnamed: 0": 7518, "autor": "Flight Booking Hacks", "date": null, "content": "How to Book the Cheapest Flight Possible to Anywhere\nKeep your searches top secret You're not crazy for thinking that a flight price has changed after searching it a few times in your web browser. Based on the cookies in your browser, flight prices do increase when a particular route is repeatedly searched, as the site wants to scare you into booking the flight quickly before prices get even higher. Always search for flights in incognito or private browsing mode to see the lowest prices.\nIn Google Chrome or Safari, incognito is enabled by hitting Command (or \u201cControl\u201d if using PC), Shift, \u201cN\u201d. For Mozilla Firefox or Internet Explorer, hit Command (or \u201cControl\u201d if using a PC), Shift, \u201cP\u201d. This will open a new browser window where your information is not tracked, thus not inflating prices as you search. Note: if you're using an older version of OS X, open Safari then click \u201cSafari\u201d in the menu bar, and select \u201cPrivate Browsing\u201d.\nYour cookies are reset each time you re-open an incognito window. So if you want to start with a clean slate for each flight search (so your previous searches aren't \u201cremembered\u201d, potentially inflating costs), close all your incognito windows, open a new one, and then perform your flight search.\nUse the best flight search engines All search engines have inflated flight costs as part of taking a cut from the airlines. Some search engines (e.g. Expedia) consistently inflate much higher than others (listed below). It pays to familiarise yourself with sites that offer the best prices.\nAs of 2020, most budget airlines will now appear in the broad search engines we are recommending (but NOT Southwest). If you want to be 100% sure though, you can do an additional search for regional budget airlines (we've listed as many we know of further below).\nFinally, no single search engine is consistently perfect (though we typically find the cheapest price on Skyscanner or Momondo). As such, you may need to try a combination of search engines to ensure you're not missing any results. There doesn't seem to be one that gets the cheapest flight 100% of the time.\nIdentify the cheapest day to fly out While many theories exist around booking specifically on a Tuesday to save money, the reality is there is no consistent truth to exactly which days are cheapest to fly. Most of the time it is cheaper to leave on a weekday, though this isn't always the case. Your best strategy is to get a quick visual of prices for a whole month to see what days are cheapest for your specific route. Here's how:\nStep 1. Hop on the Skyscanner website or download the app\nStep 2. Enter your departure & arrival cities\nStep 3. Select \u201cone-way\u201d (even if flying round trip \u2013 you're just determining the cheapest days to fly out first)\nStep 4. On a computer: click \u201cDepart\u201d but instead of entering a date, select \u201cWhole Month\u201d, and then \u201cCheapest Month\u201d so you can browse all dates to see which is cheapest. Hit \u201cSearch flights\u201d to easily see which date is cheapest.\nIn the mobile app: Tap the departure date, then change the view to \u201cChart\u201d. You can easily swipe left and right to find the cheapest date, and tap on one of the bars to see the price.\nRepeat these steps for your return flight if applicable. You can still book a round trip in one booking, but doing these steps first will let you see which dates are generally cheaper to fly in/out on for your round trip.\nKiwi.com and Google Flights work similarly to Skyscanner, plus they have map views as well, so you can see where the airport is. For tracking when and where is cheapest to fly, Hopper also offer price analysis and track fluctuations (i.e. when is best to fly).\nFly for free with points The cheapest flight you can book is free, and the easiest way to do that is with points. If you don't have any, don't fret \u2013 signing up for just one travel rewards credit card and hitting the minimum spend can land you a major points bonus, often enough for a free flight!\nYou can sign up for an airline rewards card that earns points with a particular airline, or a more general travel rewards card that lets you redeem points across a variety of airlines. As a beginner card, we recommend the Chase Sapphire Preferred Card. You can earn up to 60,000 in bonus points after spending $4,000 in the first 3 months after your account opening. That's worth $750 when redeemed through Chase Ultimate Rewards, and more than enough for an international flight. And since it includes trip cancelation insurance, car rental collision damage waiver, lost luggage insurance, and no foreign transaction fees, it's the perfect all-rounder travel credit card. You can compare this card with other top credit cards.\nBefriend budget airlines Budget airlines offer significantly cheaper tickets than their full-service counterparts. It should be obvious, but this comes with compromises such as less leg room and no \u201cfree\u201d food/drink on-board (which by the way, is normally covered in your higher-priced ticket with full-service airlines).\nIf you are considering flying budget, which in our opinion is a great way to save, I'd highly recommend reading our guide on 7 must-know facts about budget airlines.\nBelow is a comprehensive list of budget airlines around the globe. Red indicates true budget airlines while blue implies the cheapest companies available where budget airlines don't exist or aren't plentiful.\nSearch for airline error and sale fares Airlines sometimes make mistakes when posting their fares, leading to seriously discounted flights. This can happen for various reasons \u2013 currency conversion mishaps, technical glitches, or human error. If you're in the know-how on where to find airline error fares, you can save yourself hundreds of dollars on a ticket.\nOne of the most popular sites for tracking sale and error fares is Scott's Cheap Flights (or if you're in the UK, check out Jack's Flight Club). The website has a dedicated team that scours the internet for flight deals every day. Just make an account and enter your departure airport, and you'll start getting flight deals sent to your inbox. There's also a premium plan for only $49/year (a 14-day free trial is available), which gives you access to even more flight deals with even deeper discounts. Considering members save an average of $550 per ticket, this is well worth the cost!\nOther sites like Airfarewatchdog, Kiwi.com Deals, and Secret Flying are great resources to stalk for finding mistake and sale pricing, as they conglomerate slashed ticket rates all in one spot. You can also read our detailed how-to guide which spells out how to find mistake fares on your own. One great approach is to search for flights for an entire month using Skyscanner (Tip #3 in this article). This will allow you to easily spot a significantly reduced fare against what's displayed that month, and has twice helped us stumble on error fares ourselves.\nBook connecting flights yourself for less If you're flying somewhere that involves a transfer, say from Canada to Australia which typically involves Canada to LA, then LA to Australia, consider that it may be cheaper to book these two legs separately on your own by adding another destination to your trip. It should go without saying that in doing this, you should not book tight layovers. I repeat: do not book layovers that are hours apart! This approach is for those who want to create an additional destination of a few days or more, before catching their next flight. The one exception is when booking with Kiwi.com, who offer their own guarantee on making connecting flights even when not with the same partner airlines.\nFirst, do your research: are there budget airlines unique to the country you're flying out of and where you're headed to? Booking with a budget Australian airline (Jetstar) from Sydney to Honolulu, then an American one from Honolulu to Montreal saved us over $400 each when flying back from Australia to Canada earlier this year. This allowed us to create a thrifty five-day stopover in Hawaii on our way back, which was less exhausting and a lot cheaper! Kiwi.com and AirWander are both great search engine for revealing cheaper routes like this that involve multiple airlines.\nYou can even book your own multi-day layovers, essentially allowing you to see 2 destinations for the price of 1. Rather than spend a day sitting in the airport, you can spend multiple days exploring the city you are laying over in. AirWander is a specialized search engine for doing exactly this. Put in your origin, final destination, and number of days you want to stopover. AirWander will return a list of places you can visit on your stopover, often even cheaper than a regular flight search engine! To learn how to do this, read our guide on How to Get Free Extended Layovers & Hack One Trip Into Two.\nFind the cheapest place to fly Whether you know exactly where you're going or you just want to find to the cheapest possible country to fly into, Kiwi.com is a great tool to get the wanderlust going and save some big bucks. Hop on their site and enter your departure city, then select a date range to fly. Approximate costs then appear over hundreds of countries around the globe from your departure point, while the list of destinations is sorted by price, allowing you to see the most cost-effective place you can fly.\nMix and match airlines Where Kiwi.com really shines is the way in which it mixes and matches airlines in order to find the cheapest price. For example, maybe you want to go to Rome, Italy from Washington, USA. A typical flight search engine will only suggest routes coming from a single airline and its partners. An example search on Expedia shows the cheapest route as $631.20 USD via TAP Portugal.\nKiwi.com, on the other hand, will mix and match airlines (including budget airlines) in order to find you the very cheapest route. For long-haul flights especially, this can make a huge difference. The same search on Kiwi.com returns a route at $459.80 USD via JetBlue, Norwegian Air, and Vueling. That's a savings of $171.40 USD, and the travel time is even shorter!\nTypically, booking a whole trip with different airlines would be risky. For example, let's say your first flight with JetBlue was delayed, and you missed your connecting flight with Norwegian Air. Because the airlines have no association with each other, Norwegian Air has no obligation to reschedule your flight for free, so you would just lose your money. Kiwi.com instead offers their own guarantee, which covers schedule changes, flight delays, and cancellations. As long as you contact Kiwi.com as soon as you're aware of the delay, they will provide you with an alternate connecting flight, or a full refund, at your discretion. We haven't used this guarantee ourselves, but it certainly sounds like an appealing way to take the risk out of a thrifty flight hack!\nConsider hidden city ticketing Travellers long ago discovered a trick known now as \u201chidden city\u201d flights. In a nutshell, sometimes a flight that connects in a city you want to go to is cheaper than flying directly to it. So instead, you book that cheap flight which connects in your desired city and hop off there, not taking the ongoing connection.\nIt should go without saying this is risky for many reasons. Here are some factors to consider:\nIf your luggage is checked, it may go on to the final destination. For this reason, with hidden city ticketing it's best to have carry-on luggage only. You may not be allowed off the plane (if the same plane is continuing onward to its final destination). Airlines may detect that you did not take your connection. The consequences of this are hard to say, especially since people miss flights all the time. There is now a web search engine which finds hidden city tickets for you. It's called Skiplagged and was even sued by United Airlines who were angry about this debatable hack. We have never used the hidden city approach, but I know several who have, and they've scored some thrifty savings. Use at your own risk!\nBuy flights in bulk As a general rule, you will get better value on your flight tickets if you buy more at the same time, but only if they are with the same airline, or airlines in the same alliance (e.g. Star Alliance). For example, flying a return ticket from New York City to London with United Airlines will be cheaper than 2 one-way tickets.\nYou can also take advantage of this on multi-city flights. For example, Virgin Atlantic has routes from New York City to London, London to Shanghai, and Shanghai to New York City. If you book all these at the same time using the multi-city search function on Skyscanner, you'll save some serious cash.\nAnother way to bulk buy flights for less is with round the world tickets and regional passes. These are special tickets offered by airline alliances that let you go around the world, a continent, or a country at a discounted rate. To learn how to book these, be sure to check out our ultimate guide to round the world tickets.\nAll this said, sometimes a few separately booked flights with budget airlines is still cheaper than what partner airlines can offer. The only way to be sure is to use a site like Skyscanner, punch in your destinations, search as multi-city or return vs multiple one-way bookings, and compare yourself.\nDon't forget about local airlines While the above search engines are great, they do not always include small airlines, especially in less popularly booked routes and/or in remote regions. If you're flying somewhere obscure, Google search and ask around if there exists a local airline. While in South America we learnt that the LADE Air in Argentina (run by the military) has cheap flights to Patagonia, which is of course not listed in mass search engines online.\nWhen you do find small airlines, even if they are listed in a search engine results, it often pays to check the company site which may reveal exclusive online offers not found in a regular search engine. For example, when flying in Western Canada, I found that Hawk Air, a small and local company offers weekly deals on certain days. Be sure to double check!\nIf you know when and where you're going, don't wait to book Rarely ever do airline tickets get cheaper as your departure date approaches, especially if you need to fly on a certain date. Budget airlines typically offer low rates as a baseline price, and as these tickets sell, the remaining ones increase in cost. This is very typical in Europe and Australia. If you know when and where you're going, don't wait on an unknown sale. More often than not, your biggest savings come from booking far ahead when you can.\nCheck if it's cheaper to pay in other currencies Before booking a flight, consider if the rate is cheaper if paid in another currency. Often budget airlines will make you pay in the currency of the country you're departing from, but this isn't always the case. An important note when doing this: make sure you're using a credit card that is free of foreign-transaction fees, such as the Chase Sapphire Preferred Card, otherwise your attempts to save money doing this will be lost! Our article on money matters for world travellers can help steer you in the right direction for the best credit cards for travel.", "link": "https://devpost.com/software/flight-booking-hacks", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "how to book the cheapest flight possible to anywhere\nkeep your searches top secret you're not crazy for thinking that a flight price has changed after searching it a few times in your web browser. based on the cookies in your browser, flight prices do increase when a particular route is repeatedly searched, as the site wants to scare you into booking the flight quickly before prices get even higher. always search for flights in incognito or private browsing mode to see the lowest prices.\nin google chrome or safari, incognito is enabled by hitting command (or \u201ccontrol\u201d if using pc), shift, \u201cn\u201d. for mozilla firefox or internet explorer, hit command (or \u201ccontrol\u201d if using a pc), shift, \u201cp\u201d. this will open a new browser window where your information is not tracked, thus not inflating prices as you search. note: if you're using an older version of os x, open safari then click \u201csafari\u201d in the menu bar, and select \u201cprivate browsing\u201d.\nyour cookies are reset each time you re-open an incognito window. so if you want to start with a clean slate for each flight search (so your previous searches aren't \u201cremembered\u201d, potentially inflating costs), close all your incognito windows, open a new one, and then perform your flight search.\nuse the best flight search engines all search engines have inflated flight costs as part of taking a cut from the airlines. some search engines (e.g. expedia) consistently inflate much higher than others (listed below). it pays to familiarise yourself with sites that offer the best prices.\nas of 2020, most budget airlines will now appear in the broad search engines we are recommending (but not southwest). if you want to be 100% sure though, you can do an additional search for regional budget airlines (we've listed as many we know of further below).\nfinally, no single search engine is consistently perfect (though we typically find the cheapest price on skyscanner or momondo). as such, you may need to try a combination of search engines to ensure you're not missing any results. there doesn't seem to be one that gets the cheapest flight 100% of the time.\nidentify the cheapest day to fly out while many theories exist around booking specifically on a tuesday to save money, the reality is there is no consistent truth to exactly which days are cheapest to fly. most of the time it is cheaper to leave on a weekday, though this isn't always the case. your best strategy is to get a quick visual of prices for a whole month to see what days are cheapest for your specific route. here's how:\nstep 1. hop on the skyscanner website or download the app\nstep 2. enter your departure & arrival cities\nstep 3. select \u201cone-way\u201d (even if flying round trip \u2013 you're just determining the cheapest days to fly out first)\nstep 4. on a computer: click \u201cdepart\u201d but instead of entering a date, select \u201cwhole month\u201d, and then \u201ccheapest month\u201d so you can browse all dates to see which is cheapest. hit \u201csearch flights\u201d to easily see which date is cheapest.\nin the mobile app: tap the departure date, then change the view to \u201cchart\u201d. you can easily swipe left and right to find the cheapest date, and tap on one of the bars to see the price.\nrepeat these steps for your return flight if applicable. you can still book a round trip in one booking, but doing these steps first will let you see which dates are generally cheaper to fly in/out on for your round trip.\nkiwi.com and google flights work similarly to skyscanner, plus they have map views as well, so you can see where the airport is. for tracking when and where is cheapest to fly, hopper also offer price analysis and track fluctuations (i.e. when is best to fly).\nfly for free with points the cheapest flight you can book is free, and the easiest way to do that is with points. if you don't have any, don't fret \u2013 signing up for just one travel rewards credit card and hitting the minimum spend can land you a major points bonus, often enough for a free flight!\nyou can sign up for an airline rewards card that earns points with a particular airline, or a more general travel rewards card that lets you redeem points across a variety of airlines. as a beginner card, we recommend the chase sapphire preferred card. you can earn up to 60,000 in bonus points after spending $4,000 in the first 3 months after your account opening. that's worth $750 when redeemed through chase ultimate rewards, and more than enough for an international flight. and since it includes trip cancelation insurance, car rental collision damage waiver, lost luggage insurance, and no foreign transaction fees, it's the perfect all-rounder travel credit card. you can compare this card with other top credit cards.\nbefriend budget airlines budget airlines offer significantly cheaper tickets than their full-service counterparts. it should be obvious, but this comes with compromises such as less leg room and no \u201cfree\u201d food/drink on-board (which by the way, is normally covered in your higher-priced ticket with full-service airlines).\nif you are considering flying budget, which in our opinion is a great way to save, i'd highly recommend reading our guide on 7 must-know facts about budget airlines.\nbelow is a comprehensive list of budget airlines around the globe. red indicates true budget airlines while blue implies the cheapest companies available where budget airlines don't exist or aren't plentiful.\nsearch for airline error and sale fares airlines sometimes make mistakes when posting their fares, leading to seriously discounted flights. this can happen for various reasons \u2013 currency conversion mishaps, technical glitches, or human error. if you're in the know-how on where to find airline error fares, you can save yourself hundreds of dollars on a ticket.\none of the most popular sites for tracking sale and error fares is scott's cheap flights (or if you're in the uk, check out jack's flight club). the website has a dedicated team that scours the internet for flight deals every day. just make an account and enter your departure airport, and you'll start getting flight deals sent to your inbox. there's also a premium plan for only $49/year (a 14-day free trial is available), which gives you access to even more flight deals with even deeper discounts. considering members save an average of $550 per ticket, this is well worth the cost!\nother sites like airfarewatchdog, kiwi.com deals, and secret flying are great resources to stalk for finding mistake and sale pricing, as they conglomerate slashed ticket rates all in one spot. you can also read our detailed how-to guide which spells out how to find mistake fares on your own. one great approach is to search for flights for an entire month using skyscanner (tip #3 in this article). this will allow you to easily spot a significantly reduced fare against what's displayed that month, and has twice helped us stumble on error fares ourselves.\nbook connecting flights yourself for less if you're flying somewhere that involves a transfer, say from canada to australia which typically involves canada to la, then la to australia, consider that it may be cheaper to book these two legs separately on your own by adding another destination to your trip. it should go without saying that in doing this, you should not book tight layovers. i repeat: do not book layovers that are hours apart! this approach is for those who want to create an additional destination of a few days or more, before catching their next flight. the one exception is when booking with kiwi.com, who offer their own guarantee on making connecting flights even when not with the same partner airlines.\nfirst, do your research: are there budget airlines unique to the country you're flying out of and where you're headed to? booking with a budget australian airline (jetstar) from sydney to honolulu, then an american one from honolulu to montreal saved us over $400 each when flying back from australia to canada earlier this year. this allowed us to create a thrifty five-day stopover in hawaii on our way back, which was less exhausting and a lot cheaper! kiwi.com and airwander are both great search engine for revealing cheaper routes like this that involve multiple airlines.\nyou can even book your own multi-day layovers, essentially allowing you to see 2 destinations for the price of 1. rather than spend a day sitting in the airport, you can spend multiple days exploring the city you are laying over in. airwander is a specialized search engine for doing exactly this. put in your origin, final destination, and number of days you want to stopover. airwander will return a list of places you can visit on your stopover, often even cheaper than a regular flight search engine! to learn how to do this, read our guide on how to get free extended layovers & hack one trip into two.\nfind the cheapest place to fly whether you know exactly where you're going or you just want to find to the cheapest possible country to fly into, kiwi.com is a great -----> tool !!!  to get the wanderlust going and save some big bucks. hop on their site and enter your departure city, then select a date range to fly. approximate costs then appear over hundreds of countries around the globe from your departure point, while the list of destinations is sorted by price, allowing you to see the most cost-effective place you can fly.\nmix and match airlines where kiwi.com really shines is the way in which it mixes and matches airlines in order to find the cheapest price. for example, maybe you want to go to rome, italy from washington, usa. a typical flight search engine will only suggest routes coming from a single airline and its partners. an example search on expedia shows the cheapest route as $631.20 usd via tap portugal.\nkiwi.com, on the other hand, will mix and match airlines (including budget airlines) in order to find you the very cheapest route. for long-haul flights especially, this can make a huge difference. the same search on kiwi.com returns a route at $459.80 usd via jetblue, norwegian air, and vueling. that's a savings of $171.40 usd, and the travel time is even shorter!\ntypically, booking a whole trip with different airlines would be risky. for example, let's say your first flight with jetblue was delayed, and you missed your connecting flight with norwegian air. because the airlines have no association with each other, norwegian air has no obligation to reschedule your flight for free, so you would just lose your money. kiwi.com instead offers their own guarantee, which covers schedule changes, flight delays, and cancellations. as long as you contact kiwi.com as soon as you're aware of the delay, they will provide you with an alternate connecting flight, or a full refund, at your discretion. we haven't used this guarantee ourselves, but it certainly sounds like an appealing way to take the risk out of a thrifty flight hack!\nconsider hidden city ticketing travellers long ago discovered a trick known now as \u201chidden city\u201d flights. in a nutshell, sometimes a flight that connects in a city you want to go to is cheaper than flying directly to it. so instead, you book that cheap flight which connects in your desired city and hop off there, not taking the ongoing connection.\nit should go without saying this is risky for many reasons. here are some factors to consider:\nif your luggage is checked, it may go on to the final destination. for this reason, with hidden city ticketing it's best to have carry-on luggage only. you may not be allowed off the plane (if the same plane is continuing onward to its final destination). airlines may detect that you did not take your connection. the consequences of this are hard to say, especially since people miss flights all the time. there is now a web search engine which finds hidden city tickets for you. it's called skiplagged and was even sued by united airlines who were angry about this debatable hack. we have never used the hidden city approach, but i know several who have, and they've scored some thrifty savings. use at your own risk!\nbuy flights in bulk as a general rule, you will get better value on your flight tickets if you buy more at the same time, but only if they are with the same airline, or airlines in the same alliance (e.g. star alliance). for example, flying a return ticket from new york city to london with united airlines will be cheaper than 2 one-way tickets.\nyou can also take advantage of this on multi-city flights. for example, virgin atlantic has routes from new york city to london, london to shanghai, and shanghai to new york city. if you book all these at the same time using the multi-city search function on skyscanner, you'll save some serious cash.\nanother way to bulk buy flights for less is with round the world tickets and regional passes. these are special tickets offered by airline alliances that let you go around the world, a continent, or a country at a discounted rate. to learn how to book these, be sure to check out our ultimate guide to round the world tickets.\nall this said, sometimes a few separately booked flights with budget airlines is still cheaper than what partner airlines can offer. the only way to be sure is to use a site like skyscanner, punch in your destinations, search as multi-city or return vs multiple one-way bookings, and compare yourself.\ndon't forget about local airlines while the above search engines are great, they do not always include small airlines, especially in less popularly booked routes and/or in remote regions. if you're flying somewhere obscure, google search and ask around if there exists a local airline. while in south america we learnt that the lade air in argentina (run by the military) has cheap flights to patagonia, which is of course not listed in mass search engines online.\nwhen you do find small airlines, even if they are listed in a search engine results, it often pays to check the company site which may reveal exclusive online offers not found in a regular search engine. for example, when flying in western canada, i found that hawk air, a small and local company offers weekly deals on certain days. be sure to double check!\nif you know when and where you're going, don't wait to book rarely ever do airline tickets get cheaper as your departure date approaches, especially if you need to fly on a certain date. budget airlines typically offer low rates as a baseline price, and as these tickets sell, the remaining ones increase in cost. this is very typical in europe and australia. if you know when and where you're going, don't wait on an unknown sale. more often than not, your biggest savings come from booking far ahead when you can.\ncheck if it's cheaper to pay in other currencies before booking a flight, consider if the rate is cheaper if paid in another currency. often budget airlines will make you pay in the currency of the country you're departing from, but this isn't always the case. an important note when doing this: make sure you're using a credit card that is free of foreign-transaction fees, such as the chase sapphire preferred card, otherwise your attempts to save money doing this will be lost! our article on money matters for world travellers can help steer you in the right direction for the best credit cards for travel.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59507518}, {"Unnamed: 0": 7520, "autor": "Celonis 2021 Challenge", "date": null, "content": "Inspiration\nWe wanted to see what causes Pizzeria Mamma Mia's low customer satisfaction\nWhat it does\nBy looking at the pizza-making process data and doing process mining to figure out the problem\nHow we built it\nWe used Celonis's process mining tool to generate the procedure variants by uploading timestamp data.\nChallenges we ran into\nWhich feature is important to customer satisfaction, eventually, we used the XGBoost to plot the feature importance.\nAccomplishments that we're proud of\nWe learned the concept of process mining, and we are able to combine some machine learning algorithms with Celonis's tool to analyze problems.\nWhat's next for Celonis 2021 Challenge\nGetting more comments of low satisfaction, and using this information to build a more productive and useful solution for Giovanni.", "link": "https://devpost.com/software/celonis-2021-challenge", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe wanted to see what causes pizzeria mamma mia's low customer satisfaction\nwhat it does\nby looking at the pizza-making process data and doing process mining to figure out the problem\nhow we built it\nwe used celonis's process mining -----> tool !!!  to generate the procedure variants by uploading timestamp data.\nchallenges we ran into\nwhich feature is important to customer satisfaction, eventually, we used the xgboost to plot the feature importance.\naccomplishments that we're proud of\nwe learned the concept of process mining, and we are able to combine some machine learning algorithms with celonis's tool to analyze problems.\nwhat's next for celonis 2021 challenge\ngetting more comments of low satisfaction, and using this information to build a more productive and useful solution for giovanni.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59507520}, {"Unnamed: 0": 7551, "autor": "EdEasy", "date": null, "content": "Inspiration\nEven the education industry is strongly impacted by today's health dangers associated with traveling to public spaces. Although online institutions have been available for a few years, face-to-face studying remains the preferred method. As a result, most of us are naive when it comes to online learning, often known as eLearning. Google Classroom is one such eLearning tool.\nWhat it does\n\u2022 It saves time and paper, and makes it easy to create classes, distribute assignments, communicate and stay organized! No more lost homework excuses! \u2022 Teachers can quickly see who has or hasn't completed the work. \u2022 It creates one central place where you can post resources, assignments, and other class information. \u2022 It's simple to set up the platform. Teachers only have to set up the classroom, invite co-teachers and students, provide content, make announcements, assign homework, and answer questions.\nHow we built it\nWe built its frontend using Html, CSS, JavaScript, Redux, Reactjs, Material UI, Bootstrap, and Reactstrap. We built its backend using Nodejs and Express with JWT authentication. For storing data we used MongoDB and for storing files we used Firebase cloud storage.\nChallenges we ran into\nThe biggest challenge for us was time management. We had to think, design, and develop the website along with backend integrations and file storage within the time span of 48 hours. We also faced difficulties in storing the files because we were thinking of deploying our application on the Heroku server and it gives only 1GB storage so for more scalability we chose firebase cloud storage which gives 10GB of file storage. And we made that decision at the last moment so that was also a big risk.\nAccomplishments that we're proud of\nThe biggest accomplishment for us was proper management of time and our team's collaboration. We were able to make an application with all the functionalities that we thought of and we built great understanding among ourselves.\nWhat we learned\nWe learned how to develop an application as a team and how to understand each other's thought processes and bring our ideas into reality with proper management of resources and time.\nWhat's next for EdEasy\nWe will try to improve the UI/UX of EdEasy even more and we will add some more useful functionalities in our project.", "link": "https://devpost.com/software/edeasy-0jspq3", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\neven the education industry is strongly impacted by today's health dangers associated with traveling to public spaces. although online institutions have been available for a few years, face-to-face studying remains the preferred method. as a result, most of us are naive when it comes to online learning, often known as elearning. google classroom is one such elearning -----> tool !!! .\nwhat it does\n\u2022 it saves time and paper, and makes it easy to create classes, distribute assignments, communicate and stay organized! no more lost homework excuses! \u2022 teachers can quickly see who has or hasn't completed the work. \u2022 it creates one central place where you can post resources, assignments, and other class information. \u2022 it's simple to set up the platform. teachers only have to set up the classroom, invite co-teachers and students, provide content, make announcements, assign homework, and answer questions.\nhow we built it\nwe built its frontend using html, css, javascript, redux, reactjs, material ui, bootstrap, and reactstrap. we built its backend using nodejs and express with jwt authentication. for storing data we used mongodb and for storing files we used firebase cloud storage.\nchallenges we ran into\nthe biggest challenge for us was time management. we had to think, design, and develop the website along with backend integrations and file storage within the time span of 48 hours. we also faced difficulties in storing the files because we were thinking of deploying our application on the heroku server and it gives only 1gb storage so for more scalability we chose firebase cloud storage which gives 10gb of file storage. and we made that decision at the last moment so that was also a big risk.\naccomplishments that we're proud of\nthe biggest accomplishment for us was proper management of time and our team's collaboration. we were able to make an application with all the functionalities that we thought of and we built great understanding among ourselves.\nwhat we learned\nwe learned how to develop an application as a team and how to understand each other's thought processes and bring our ideas into reality with proper management of resources and time.\nwhat's next for edeasy\nwe will try to improve the ui/ux of edeasy even more and we will add some more useful functionalities in our project.", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59507551}, {"Unnamed: 0": 7575, "autor": "Natural Event Tracker", "date": null, "content": "Inspiration\nWith the effects of climate change becoming more and more apparent, we wanted to make a tool that allows users to stay informed on current climate events and stay safe by being warned of nearby climate warnings.\nWhat it does\nOur web app has two functions. One of the functions is to show a map of the entire world that displays markers on locations of current climate events like hurricanes, wildfires, etc. The other function allows users to submit their phone numbers to us, which subscribes the user to regular SMS updates through Twilio if there are any dangerous climate events in their vicinity. This SMS update is sent regardless of whether the user has the app open or not, allowing users to be sure that they will get the latest updates in case of any severe or dangerous weather patterns.\nHow we built it\nWe used Angular to build our frontend. With that, we used the Google Maps API to show the world map along with markers, with information we got from our server. The server gets this climate data from the NASA EONET API. The server also uses Twilio along with Google Firebase to allow users to sign up and receive text message updates about severe climate events in their vicinity (within 50km).\nChallenges we ran into\nFor the front end, one of the biggest challenges was the markers on the map. Not only, did we need to place markers on many different climate event locations, but we wanted the markers to have different icons based on weather events. We also wanted to be able to filter the marker types for a better user experience. For the back end, we had challenges to figure out Twilio to be able to text users, Google firebase for user sign-in, and MongoDB for database operation. Using these tools was a challenge at first because this was our first time using these tools. We also ran into problems trying to accurately calculate a user's vicinity to current events due to the complex nature of geographical math, but after a lot of number crunching, and the use of a helpful library, we were accurately able to determine if any given event is within 50km of a users position based solely on the coordiantes.\nAccomplishments that we're proud of\nWe are really proud to make an app that not only informs users but can also help them in dangerous situations. We are also proud of ourselves for finding solutions to the tough technical challenges we ran into.\nWhat we learned\nWe learned how to use all the different tools that we used for the first time while making this project. We also refined our front-end and back-end experience and knowledge.\nWhat's next for Natural Event Tracker\nWe want to perhaps make the map run faster and have more features for the user, like more information, etc. We also are interested in finding more ways to help our users stay safer during future climate events that they may experience.", "link": "https://devpost.com/software/natural-event-tracker", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwith the effects of climate change becoming more and more apparent, we wanted to make a -----> tool !!!  that allows users to stay informed on current climate events and stay safe by being warned of nearby climate warnings.\nwhat it does\nour web app has two functions. one of the functions is to show a map of the entire world that displays markers on locations of current climate events like hurricanes, wildfires, etc. the other function allows users to submit their phone numbers to us, which subscribes the user to regular sms updates through twilio if there are any dangerous climate events in their vicinity. this sms update is sent regardless of whether the user has the app open or not, allowing users to be sure that they will get the latest updates in case of any severe or dangerous weather patterns.\nhow we built it\nwe used angular to build our frontend. with that, we used the google maps api to show the world map along with markers, with information we got from our server. the server gets this climate data from the nasa eonet api. the server also uses twilio along with google firebase to allow users to sign up and receive text message updates about severe climate events in their vicinity (within 50km).\nchallenges we ran into\nfor the front end, one of the biggest challenges was the markers on the map. not only, did we need to place markers on many different climate event locations, but we wanted the markers to have different icons based on weather events. we also wanted to be able to filter the marker types for a better user experience. for the back end, we had challenges to figure out twilio to be able to text users, google firebase for user sign-in, and mongodb for database operation. using these tools was a challenge at first because this was our first time using these tools. we also ran into problems trying to accurately calculate a user's vicinity to current events due to the complex nature of geographical math, but after a lot of number crunching, and the use of a helpful library, we were accurately able to determine if any given event is within 50km of a users position based solely on the coordiantes.\naccomplishments that we're proud of\nwe are really proud to make an app that not only informs users but can also help them in dangerous situations. we are also proud of ourselves for finding solutions to the tough technical challenges we ran into.\nwhat we learned\nwe learned how to use all the different tools that we used for the first time while making this project. we also refined our front-end and back-end experience and knowledge.\nwhat's next for natural event tracker\nwe want to perhaps make the map run faster and have more features for the user, like more information, etc. we also are interested in finding more ways to help our users stay safer during future climate events that they may experience.", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59507575}, {"Unnamed: 0": 7585, "autor": "Emotional.AI", "date": null, "content": "Inspiration\nWe express emotions in our everyday lives when we communicate with our loved ones, our neighbors, our friends, our local Loblaw store customer service, our doctors or therapists. These emotions can be examined by cues such as gesture, text and facial expressions. The goal of Emotional.AI is to provide a tool for businesses (customer service, etc), or doctors/therapists to identify emotions and enhance their services.\nWhat it does\nUses natural language processing (from audio transcription via Assembly AI) and computer vision to determine emotion of people.\nHow we built it\nNatural Language processing\nFirst we took emotion classified data from public sources (Kaggle and research studies).\nWe preprocessed, cleaned, transformed, created features, and performed light EDA on the dataset.\nUsed TF-IDF tokenizer to deal with numbers, punctuation marks, non letter symbols, etc.\nScaled the data using Robust Scaler and made 7 models. (MNB, Linear Regression, KNN, SVM, Decision Tree, Random Forrest, XGB)\nComputer Vision\nUsed Mediapipe to generate points on face, then use those points to get training data set. We used Jupyter Notebook to run OpenCV and Mediapipe. Upon running our data in Mediapipe, we were able to get a skeleton map of the face with 468 points. These points can be mapped in 3-dimension as it contains X, Y, and Z axis. We processed these features (468 points x 3) by saving them into a spreadsheet. Then we divided the spreadsheet into training and testing data. Using the training set, we were able to create 6 Machine learning models and choose the best one.\nAssembly AI\nWe converted video/audio from recordings (whether it\u2019s a therapy session or customer service audio from 1000s of Loblaws customers \ud83d\ude09) to text using Assembly API.\nAmazon Web Services\nWe used the S3 services to host the video files uploaded by the user. These video files were then sent the Assembly AI Api.\nDCP\nFor Computing (ML)\nChallenges we ran into\nCollaborating virtually is challenging\nDeep learning training takes a lot of computing power and time\nConnecting our front-end with back-end (and ML)\nTime management\nWorking with react + flask server\nConfiguring amazon buckets and users to make the app work with the s3 services\nAccomplishments that we're proud of\nApart from completing this hack, we persevered through each challenge as a team and succeeded in what we put ourselves up to.\nWhat we learned\nWorking as a team\nConfiguration management\nWorking with Flask\nWhat's next for Emotional.AI\nWe hope to have a more refined application with cleaner UI.\nWe want to train our models further with more data and have more classifications.\nWe want to make a platform for therapists to connect with their clients and use our tech.\nMake our solution work in real-time.", "link": "https://devpost.com/software/emotional-ai", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe express emotions in our everyday lives when we communicate with our loved ones, our neighbors, our friends, our local loblaw store customer service, our doctors or therapists. these emotions can be examined by cues such as gesture, text and facial expressions. the goal of emotional.ai is to provide a -----> tool !!!  for businesses (customer service, etc), or doctors/therapists to identify emotions and enhance their services.\nwhat it does\nuses natural language processing (from audio transcription via assembly ai) and computer vision to determine emotion of people.\nhow we built it\nnatural language processing\nfirst we took emotion classified data from public sources (kaggle and research studies).\nwe preprocessed, cleaned, transformed, created features, and performed light eda on the dataset.\nused tf-idf tokenizer to deal with numbers, punctuation marks, non letter symbols, etc.\nscaled the data using robust scaler and made 7 models. (mnb, linear regression, knn, svm, decision tree, random forrest, xgb)\ncomputer vision\nused mediapipe to generate points on face, then use those points to get training data set. we used jupyter notebook to run opencv and mediapipe. upon running our data in mediapipe, we were able to get a skeleton map of the face with 468 points. these points can be mapped in 3-dimension as it contains x, y, and z axis. we processed these features (468 points x 3) by saving them into a spreadsheet. then we divided the spreadsheet into training and testing data. using the training set, we were able to create 6 machine learning models and choose the best one.\nassembly ai\nwe converted video/audio from recordings (whether it\u2019s a therapy session or customer service audio from 1000s of loblaws customers \ud83d\ude09) to text using assembly api.\namazon web services\nwe used the s3 services to host the video files uploaded by the user. these video files were then sent the assembly ai api.\ndcp\nfor computing (ml)\nchallenges we ran into\ncollaborating virtually is challenging\ndeep learning training takes a lot of computing power and time\nconnecting our front-end with back-end (and ml)\ntime management\nworking with react + flask server\nconfiguring amazon buckets and users to make the app work with the s3 services\naccomplishments that we're proud of\napart from completing this hack, we persevered through each challenge as a team and succeeded in what we put ourselves up to.\nwhat we learned\nworking as a team\nconfiguration management\nworking with flask\nwhat's next for emotional.ai\nwe hope to have a more refined application with cleaner ui.\nwe want to train our models further with more data and have more classifications.\nwe want to make a platform for therapists to connect with their clients and use our tech.\nmake our solution work in real-time.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59507585}, {"Unnamed: 0": 7591, "autor": "Environment vs. Stock Price", "date": null, "content": "Inspiration\nWe were curious on how COVID-19 impacted the stock price and CO2 emissions within the United States and wanted to investigate if there existed any relationships. We continued to focus on how the stock price of a oil/gas company is correlated to the CO2 emissions of the United States.\nWhat it does\nWe have found a relationship between CO2 emissions and the stock price of Conoco Phillips and Exxon Mobile. We also investigated the relationships between various restaurants and CO2 emissions of the United States but could not find a strong correlation between the two.\nHow we built it\nWe have created charts to display these relationships. We've gathered the data from the available resources from Snowflake. We connected the Snowflake service to Alteryx data analysis tool.\nChallenges we ran into\nThe largest challenge we ran into was the difficulty of finding datasets that were correlated with each other. Since there were over 100 tables to search in, each table containing different value types, it became a large search problem.\nAccomplishments that we're proud of\nWe are proud to have found a correlation to explain! And the fact that most of us were able to learn a new tool analyze data.\nWhat we learned\nWe learned how to comb through large amounts of data and think critically about the data we were looking at. We also learned how to use Snowflake and Alteryx software.\nWhat's next for Goldman Sachs Challenge Submission\nI think something we can improve on is to find a better algorithm to search through the datasets and create mathematical relationships between CO2 emissions and stock prices.", "link": "https://devpost.com/software/goldman-sachs-challenge-submission", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe were curious on how covid-19 impacted the stock price and co2 emissions within the united states and wanted to investigate if there existed any relationships. we continued to focus on how the stock price of a oil/gas company is correlated to the co2 emissions of the united states.\nwhat it does\nwe have found a relationship between co2 emissions and the stock price of conoco phillips and exxon mobile. we also investigated the relationships between various restaurants and co2 emissions of the united states but could not find a strong correlation between the two.\nhow we built it\nwe have created charts to display these relationships. we've gathered the data from the available resources from snowflake. we connected the snowflake service to alteryx data analysis -----> tool !!! .\nchallenges we ran into\nthe largest challenge we ran into was the difficulty of finding datasets that were correlated with each other. since there were over 100 tables to search in, each table containing different value types, it became a large search problem.\naccomplishments that we're proud of\nwe are proud to have found a correlation to explain! and the fact that most of us were able to learn a new tool analyze data.\nwhat we learned\nwe learned how to comb through large amounts of data and think critically about the data we were looking at. we also learned how to use snowflake and alteryx software.\nwhat's next for goldman sachs challenge submission\ni think something we can improve on is to find a better algorithm to search through the datasets and create mathematical relationships between co2 emissions and stock prices.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59507591}, {"Unnamed: 0": 7641, "autor": "Vigil", "date": null, "content": "Vigil\nWhat it does\nVigil is an AI Social Distancing Detection Tool that helps to enforce COVID-19 Safe Distancing Measures. Given any video feed, Vigil helps to identify individuals who have gathered in groups larger than the permissible size and alerts the user about the rule flouters.\nThis eliminated the need for safe distancing ambassadors to patrol crowded areas as a single Safe Distancing Manager can monitor these areas with the help of surveillance cameras. Managers can then deploy their ambassadors to warn rule flouters upon alerts by Vigil.\nHow we built it\nWe built Vigil using the OpenCV library and the pre-trained object detection model, Yolov3. We conducted multiple tests to fine-tune Vigil, allowing it to accurately identify instances of close contact.\nWhat Inspired Us\nAs the COVID-19 pandemic spread across the world, many countries implemented Social Distancing Measures (SDM) to tackle the infectivity of the virus. In Singapore, Social Distancing Ambassadors (SDA) have been introduced to help enforce these rules and warn those who fail to adhere to them. Currently, these SDAs have to patrol crowded areas and manually identify SDM breaches.\nNDSM feels that this is an inefficient use of manpower as too many SDAs are required for the enforcement of SDMs, especially as Singapore pivots to treat COVID-19 as endemic. Furthermore, SDAs also face constant exposure risk as their job requires them to patrol areas with high human traffic.\nThus, we decided to build an object detection tool, which can aid SDAs in their everyday job, and serves as a central management system at malls.\nChallenges Faced\nWhen starting this project, we knew nothing about machine learning or deep convolutional neural network. While googling and researching, we also stumbled upon code that we did not understand and models that were unsuitable for our use. It took us quite some time to find a suitable object detection model called Yolov3. Further, documentation for Yolov3 was not very comprehensive, so we had to figure out how to implement it and which parameters to change to get the program working.", "link": "https://devpost.com/software/vigil-evxidn", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "vigil\nwhat it does\nvigil is an ai social distancing detection -----> tool !!!  that helps to enforce covid-19 safe distancing measures. given any video feed, vigil helps to identify individuals who have gathered in groups larger than the permissible size and alerts the user about the rule flouters.\nthis eliminated the need for safe distancing ambassadors to patrol crowded areas as a single safe distancing manager can monitor these areas with the help of surveillance cameras. managers can then deploy their ambassadors to warn rule flouters upon alerts by vigil.\nhow we built it\nwe built vigil using the opencv library and the pre-trained object detection model, yolov3. we conducted multiple tests to fine-tune vigil, allowing it to accurately identify instances of close contact.\nwhat inspired us\nas the covid-19 pandemic spread across the world, many countries implemented social distancing measures (sdm) to tackle the infectivity of the virus. in singapore, social distancing ambassadors (sda) have been introduced to help enforce these rules and warn those who fail to adhere to them. currently, these sdas have to patrol crowded areas and manually identify sdm breaches.\nndsm feels that this is an inefficient use of manpower as too many sdas are required for the enforcement of sdms, especially as singapore pivots to treat covid-19 as endemic. furthermore, sdas also face constant exposure risk as their job requires them to patrol areas with high human traffic.\nthus, we decided to build an object detection tool, which can aid sdas in their everyday job, and serves as a central management system at malls.\nchallenges faced\nwhen starting this project, we knew nothing about machine learning or deep convolutional neural network. while googling and researching, we also stumbled upon code that we did not understand and models that were unsuitable for our use. it took us quite some time to find a suitable object detection model called yolov3. further, documentation for yolov3 was not very comprehensive, so we had to figure out how to implement it and which parameters to change to get the program working.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59507641}, {"Unnamed: 0": 7643, "autor": "Detect Neurological problems", "date": null, "content": "Inspiration\nA single traumatic brain injury caused by a car accidents, falls, concussions, etc can cause Dementia or Schizophrenia which go un-detected and grow worse over time. Dementia is a general term for loss of memory, language, problem-solving and other thinking abilities, moreover it includes Alzheimer's disease and Parkinson's disease. People with dementia have problems with thinking and remembering that affects their ability to manage their daily life. During uncertain times like these where people avoid going to the hospital, we present a simple a web-app tool to detect neurological problems such as Alzheimer and Schizophrenia with maximum accuracy.\nWhat it does\nNow you must be wondering, how can we detect neurological problems without MRI and some brains scans ? The answer simple, you just have to draw a clock face.\nThe user just needs to draw a clock-face on our web-app, or scan and upload one and our machine learning model will predict whether they need medical attention or not. We use the famous clock-drawing test to screen people for signs of neurological problems, such as Alzheimer\u2019s and other dementias. The clock-drawing test is often used in combination with other, more thorough screening tests, but even when used by itself, it can provide helpful insight into a person\u2019s cognitive ability.\nHealthy-Drawing\nUnHealthy-Drawing\nHow we built it\nUsed Angular (1.x) for the website\nMade our dataset from scratch for clock-faces\nUsed CNN to make our Machine Learning model\nWe used Flask API to connect the website with our model\nCNN model\nFor the detection we had used CNN architecture, which can be summarised as:\nAccuracy\nThe following was the accuracy vs epoch graph:\nPart of web-app screenshots\nHigh Risk - Drawn on our web-app\nNo Risk - Uploaded scanned image from device\nChallenges we ran into\nCreating the dataset was the main challenge, there were no already datasets of hand drawn clock-faces, so we had to create dataset of our own which was extremely time consuming. Since we had a small dataset, we ran into trouble to train our model, and gain high accuracy. However, after hours of grinding we were finally able to create a good enough model with accuracy that is sufficient for the incubation stage.\nAccomplishments that we're proud of\nGenerating a non-existing dataset\nCreating a ML model with optimum accuracy\nCreating a way for users to draw directly on our web-app\nDoing all this in 2 days\nWhat's next for Detect Neurological problems\nWe want to improve our model accuracy and achieve the best result we can. Maybe we will work with other diseases and make it an online diagnosis for multiple diseases.", "link": "https://devpost.com/software/detect-neurological-problems", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\na single traumatic brain injury caused by a car accidents, falls, concussions, etc can cause dementia or schizophrenia which go un-detected and grow worse over time. dementia is a general term for loss of memory, language, problem-solving and other thinking abilities, moreover it includes alzheimer's disease and parkinson's disease. people with dementia have problems with thinking and remembering that affects their ability to manage their daily life. during uncertain times like these where people avoid going to the hospital, we present a simple a web-app -----> tool !!!  to detect neurological problems such as alzheimer and schizophrenia with maximum accuracy.\nwhat it does\nnow you must be wondering, how can we detect neurological problems without mri and some brains scans ? the answer simple, you just have to draw a clock face.\nthe user just needs to draw a clock-face on our web-app, or scan and upload one and our machine learning model will predict whether they need medical attention or not. we use the famous clock-drawing test to screen people for signs of neurological problems, such as alzheimer\u2019s and other dementias. the clock-drawing test is often used in combination with other, more thorough screening tests, but even when used by itself, it can provide helpful insight into a person\u2019s cognitive ability.\nhealthy-drawing\nunhealthy-drawing\nhow we built it\nused angular (1.x) for the website\nmade our dataset from scratch for clock-faces\nused cnn to make our machine learning model\nwe used flask api to connect the website with our model\ncnn model\nfor the detection we had used cnn architecture, which can be summarised as:\naccuracy\nthe following was the accuracy vs epoch graph:\npart of web-app screenshots\nhigh risk - drawn on our web-app\nno risk - uploaded scanned image from device\nchallenges we ran into\ncreating the dataset was the main challenge, there were no already datasets of hand drawn clock-faces, so we had to create dataset of our own which was extremely time consuming. since we had a small dataset, we ran into trouble to train our model, and gain high accuracy. however, after hours of grinding we were finally able to create a good enough model with accuracy that is sufficient for the incubation stage.\naccomplishments that we're proud of\ngenerating a non-existing dataset\ncreating a ml model with optimum accuracy\ncreating a way for users to draw directly on our web-app\ndoing all this in 2 days\nwhat's next for detect neurological problems\nwe want to improve our model accuracy and achieve the best result we can. maybe we will work with other diseases and make it an online diagnosis for multiple diseases.", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59507643}, {"Unnamed: 0": 7646, "autor": "Hand Signals", "date": null, "content": "Inspiration\nAI/ML has always been something that we wanted to incorporate into a project and we though this would be the perfect opportunity to experiment with this new tool in Computer Science.\nWhat it does\nHand Signal uses an image recognition AI to read different signals and gestures created with your right hand. With its image recognition, it then labels the hand in the frame to an assigned label and is then put into a text to speech to speak out what the hand sign is.\nHow we built it\nWe implemented the TensorFlow library and the MediaPipe google api to create the image recognition tool. Then we fed it data for each of the hand signals we wanted to have in our project into a TensorFlow model we trained and built from scratch. To start the actual text to speech and reading of hand signs, we assigned the motion of palming open your hand from a fist. We then use the recognition built in and implement it into the text to speech to assign what the voice says.\nChallenges we ran into\nOriginal implementation of the data and the use of TensorFlow was a bit tricky at first because of the lack of experience in our group. We had to watch a lot of different videos and google up tutorials to familiarize ourselves with the database and its uses. The main challenge was creating the image recognition AI itself. Because of our previously mentioned experience level and lack of familiarity, we didn't know where to start exactly with the image recognition. However, after looking into other models and similar AIs, we were able to push through it.\nAccomplishments that we're proud of\nWe are proud that we were able to create an hand recognition AI and use it with the text to speech editor. That was the original concept for the project and our proudest accomplishment is the fact that we accomplished our mission statement.\nWhat we learned\nHow to use TensorFlow and MediaPipe. We also began scratching the surface into developing a neural network and its applications into real life uses.\nWhat's next for Hand Signals\nWe would like to develop our image recognition into other uses, such as developing an app for remote LED control. Our future project involves using the hand signal AI, but instead of a text to speech, it will send signals over to the Raspberry Pi and take control of the LEDs. The goal is to use hand signals to remotely change the colors and intensity of LED strips.", "link": "https://devpost.com/software/hand-signals", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nai/ml has always been something that we wanted to incorporate into a project and we though this would be the perfect opportunity to experiment with this new -----> tool !!!  in computer science.\nwhat it does\nhand signal uses an image recognition ai to read different signals and gestures created with your right hand. with its image recognition, it then labels the hand in the frame to an assigned label and is then put into a text to speech to speak out what the hand sign is.\nhow we built it\nwe implemented the tensorflow library and the mediapipe google api to create the image recognition tool. then we fed it data for each of the hand signals we wanted to have in our project into a tensorflow model we trained and built from scratch. to start the actual text to speech and reading of hand signs, we assigned the motion of palming open your hand from a fist. we then use the recognition built in and implement it into the text to speech to assign what the voice says.\nchallenges we ran into\noriginal implementation of the data and the use of tensorflow was a bit tricky at first because of the lack of experience in our group. we had to watch a lot of different videos and google up tutorials to familiarize ourselves with the database and its uses. the main challenge was creating the image recognition ai itself. because of our previously mentioned experience level and lack of familiarity, we didn't know where to start exactly with the image recognition. however, after looking into other models and similar ais, we were able to push through it.\naccomplishments that we're proud of\nwe are proud that we were able to create an hand recognition ai and use it with the text to speech editor. that was the original concept for the project and our proudest accomplishment is the fact that we accomplished our mission statement.\nwhat we learned\nhow to use tensorflow and mediapipe. we also began scratching the surface into developing a neural network and its applications into real life uses.\nwhat's next for hand signals\nwe would like to develop our image recognition into other uses, such as developing an app for remote led control. our future project involves using the hand signal ai, but instead of a text to speech, it will send signals over to the raspberry pi and take control of the leds. the goal is to use hand signals to remotely change the colors and intensity of led strips.", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 1, "media": null, "medialink": null, "identifyer": 59507646}, {"Unnamed: 0": 7655, "autor": "Pizzeria Order Fulfillment Process Analysis", "date": null, "content": "Inspiration\nWe really enjoyed the ability to perform complex data mining techniques without hard coding. This gave some of our inexperienced team members the ability to participate and make meaningful contributions without coding.\nWhat it does\nThis dashboard provides an interactive process analysis for the Pizzeria owners. With this tool, the owners can examine how their processes might be inhibiting their profits and causing lower customer ratings.\nHow we built it\nWe started by thoroughly examining the reason why Pizzeria Mamma Mia is seeking help: low customer ratings, and negative profits on deliveries. When considering these problems, we brainstormed potential factors that might be contributing to the problems from both the perspective of the customers and the business owners.\nFactors potentially contributing to low ratings:\ntaste of food\ntemp of food\nspeed of service\ncost of meal\nconvenience of ordering\ndelivery times\nFactors potentially contributing to low profits\nhigh cost / cost factor\nlow sales\nday of week\ncustomer type (maybe older folks order more than teenagers?)\nautomation of processes\nFrom here, we were able see that many of these factors could stem from the overall ordering process. This is where we focused our analysis.\nChallenges we ran into\nUsing this new software posed some challenges. There was a large learning curve to understand the results of our input. We did feel that we could have hard-coded some of the analysis to provide the Pizzeria with a more robust analysis. Even so, we chose to keep our analysis maintained within the software provided in order to avoid challenges with integration of outside work.\nAccomplishments that we're proud of\nWe are really proud of the interactivity of our project. There are four clickable buttons that link to our written findings and recommendations as well as three other process analyses we created using Celonis software. Additionally, we added a selection dropdown menu that allows the user to filter on process variants to examine how each of the processes are different.\nWhat we learned\nThis project gave us an opportunity to explore process mining without the need for hard coding machine learning techniques. We learned that business processes can often be messy, and this can cause other aspects of the business to suffer. Optimizing processes is the first step in improving customer satisfaction and profits.\nWhat's next for Pizzeria Order Fulfillment Process Analysis\nIn the future, we would like to take what we've accomplished using the Celonis software and hard code these findings. This way, we can understand the backend processes used by the Celonis software to perform the machine learning tasks that give actionable insights.", "link": "https://devpost.com/software/pizzeria-order-fulfillment-process-analysis", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe really enjoyed the ability to perform complex data mining techniques without hard coding. this gave some of our inexperienced team members the ability to participate and make meaningful contributions without coding.\nwhat it does\nthis dashboard provides an interactive process analysis for the pizzeria owners. with this -----> tool !!! , the owners can examine how their processes might be inhibiting their profits and causing lower customer ratings.\nhow we built it\nwe started by thoroughly examining the reason why pizzeria mamma mia is seeking help: low customer ratings, and negative profits on deliveries. when considering these problems, we brainstormed potential factors that might be contributing to the problems from both the perspective of the customers and the business owners.\nfactors potentially contributing to low ratings:\ntaste of food\ntemp of food\nspeed of service\ncost of meal\nconvenience of ordering\ndelivery times\nfactors potentially contributing to low profits\nhigh cost / cost factor\nlow sales\nday of week\ncustomer type (maybe older folks order more than teenagers?)\nautomation of processes\nfrom here, we were able see that many of these factors could stem from the overall ordering process. this is where we focused our analysis.\nchallenges we ran into\nusing this new software posed some challenges. there was a large learning curve to understand the results of our input. we did feel that we could have hard-coded some of the analysis to provide the pizzeria with a more robust analysis. even so, we chose to keep our analysis maintained within the software provided in order to avoid challenges with integration of outside work.\naccomplishments that we're proud of\nwe are really proud of the interactivity of our project. there are four clickable buttons that link to our written findings and recommendations as well as three other process analyses we created using celonis software. additionally, we added a selection dropdown menu that allows the user to filter on process variants to examine how each of the processes are different.\nwhat we learned\nthis project gave us an opportunity to explore process mining without the need for hard coding machine learning techniques. we learned that business processes can often be messy, and this can cause other aspects of the business to suffer. optimizing processes is the first step in improving customer satisfaction and profits.\nwhat's next for pizzeria order fulfillment process analysis\nin the future, we would like to take what we've accomplished using the celonis software and hard code these findings. this way, we can understand the backend processes used by the celonis software to perform the machine learning tasks that give actionable insights.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59507655}, {"Unnamed: 0": 7664, "autor": "HDB Price Estimator", "date": null, "content": "Inspiration\nTired of not being able to find accurate quotes for your HDB apartment? Tired of property agents jacking up the prices artificially just to siphon more of your net worth into their own pockets? Soon, your nightmares will come to an end because the HDB Price Estimator is here to save your day!\nWhat it does\nThe HDB Price Estimator application is an essential tool for both buyers and sellers. The HDB Price Estimator provides fair quotes based on the factors that the buyer or seller enters. The factors are put through our machine learning model that was trained on past HDB sales data since 2012.\nHow we built it\nWe used HDB sales data from Data.gov.sg\nData Preparation:\nCleaning out the NULL value\nAdding new features - Geocoding. By using the street name in raw data and geocode the longitude and latitude of each listing, we\u2019re able to add in more features to improves the accuracy of the model:\nAdd distance to closest MRT\nAdd distance to closest Mall\nAdd distance to CBD\nAdding new features - Economic Environment.\nGDP per capita for the past 10 years (quarterly)\nSIBOR (Singapore Interbank Offered Rate) for the past 10 years (quarterly)\nModeling:\nHandling categorical data in Regression\nOne-Hot Encoding\nFeatures Engineering:\nTransform categorical data into numeric ones by assigning numeric values based on the deviation from the median of the resale price of a specific category (see featuring engineering in Modeling)\nAssigning boolean value to every category of each categorical feature\nML techniques\nLinear Regression\nGradientBoosting Regression (ensemble model)\nWe also made a Flutter application to bring this feature to the end-user. This application is multi-paged and fully interactive. The application sends API requests to a Node.js webserver that we deployed, then the webserver processes the user input by running it through a Python child-process that runs a pickled version of our machine learning model. The Python script then returns the estimated HDB value to the webserver to be sent back to the application.\nChallenges we ran into\nThe data provided by HDB did not include postal codes but only street addresses and block numbers. In order to find the longitude and latitude of every block, we wrote an algorithm to use OneMap's API to retrieve the postal codes for every address. However, there were almost 200,000 rows of data that we had to update, hence this process took about 8 hours just to retrieve and update the postal codes. In order to avoid losing data during this time-consuming operation if something fails, we stored all relevant API responses into a .csv file. This is done to allow easy resumption after a potential crash or error and would save us from waiting for the algorithm to run from the start.\nData analysis and machine learning are often done through Python's Jupyter Notebook. However, our webserver is written in Node.js, hence due to the language difference, we had to figure out a method for the Node.js backend to be interoperable with the Python machine learning model that we created. In the end, we pickled our Jupyter Notebook containing the model and passed it as a byte stream to another Python script that handles the communication between the webserver and the ML model.\nAccomplishments that we're proud of\n\u201cData scientists spend 45% of the time prepping the data\u201d (according to research conducted by Anaconda).\nThis is what happened to us while trying to deal with almost 200000 entries from real governmental datasets, and even trying to geocode and adding in new features like distance to the closest MRT station or shopping mall, or data repressing economy states like GDP per capita and SIBOR based on literature review.\nTo examine and experiment with different ways (Feature Engineering and One-Hot Encoding) of handling categorical data while both getting accurate results with gradient boosting regression (95% and 96%) was also not an easy task.\nLast but definitely not least, it was difficult to fit the extracted model into the User Interface with correct output in a speedy fashion. But our team was able to complete a basically done UI with accurate and fast response in a short period of time given.\nThese additional efforts allow us to build more accurate models while actually deploying in real life to assist more young adults (or anyone who wishes to purchase an HDB) to make a wise purchase decision.\nWhat we learned\nOther than the accomplishments stated above that definitely help us develop more ML skills and stimulate our creativity, the model itself brought up some interesting facts.\nAs we built 5 different ML models for 5 different room types, there are some interesting differences. For example, the weight of each variable in the regression model can be quite different based on the room-type of HDB, e.g. while SIBOR only accounts for around 1% weighting in the resale price for Executive room type, it accounts for nearly 20% weighting when trying to predict the resale price of 4_room type.\nMoreover, it was also surprising to see some of the features that seem intuitively minor at the beginning turned out to be important features or, in some instances, the opposite. For example, SIBOR (an indicator of current interest rate) was actually an important feature, while the floor area wasn\u2019t really significant in most of the models.\nWhat's next for HDB Price Estimator\nFirst of all, a more complete UI interface can be built. We wish to return a more fully founded result to the user, not only the suggested pricing but reasonable pricing in a given area and visualization of release price history trend in the fast few months, etc.\nMoreover, as the relative importance of each variable is known from different models, combining with some more features like supply and demand of HDB release market or other factors, prediction of release price in the future is possible with sufficient data given. In this case, we can advance our model into a forward-looking \u201cpredictor\u201d that can even better assist the user with the purchase/investing decision, e.g. to purchase BTO or HDB at the moment by comparing the given price of BTO (from the government) and our prediction of HDB of a certain type.", "link": "https://devpost.com/software/hdb-price-estimator", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ntired of not being able to find accurate quotes for your hdb apartment? tired of property agents jacking up the prices artificially just to siphon more of your net worth into their own pockets? soon, your nightmares will come to an end because the hdb price estimator is here to save your day!\nwhat it does\nthe hdb price estimator application is an essential -----> tool !!!  for both buyers and sellers. the hdb price estimator provides fair quotes based on the factors that the buyer or seller enters. the factors are put through our machine learning model that was trained on past hdb sales data since 2012.\nhow we built it\nwe used hdb sales data from data.gov.sg\ndata preparation:\ncleaning out the null value\nadding new features - geocoding. by using the street name in raw data and geocode the longitude and latitude of each listing, we\u2019re able to add in more features to improves the accuracy of the model:\nadd distance to closest mrt\nadd distance to closest mall\nadd distance to cbd\nadding new features - economic environment.\ngdp per capita for the past 10 years (quarterly)\nsibor (singapore interbank offered rate) for the past 10 years (quarterly)\nmodeling:\nhandling categorical data in regression\none-hot encoding\nfeatures engineering:\ntransform categorical data into numeric ones by assigning numeric values based on the deviation from the median of the resale price of a specific category (see featuring engineering in modeling)\nassigning boolean value to every category of each categorical feature\nml techniques\nlinear regression\ngradientboosting regression (ensemble model)\nwe also made a flutter application to bring this feature to the end-user. this application is multi-paged and fully interactive. the application sends api requests to a node.js webserver that we deployed, then the webserver processes the user input by running it through a python child-process that runs a pickled version of our machine learning model. the python script then returns the estimated hdb value to the webserver to be sent back to the application.\nchallenges we ran into\nthe data provided by hdb did not include postal codes but only street addresses and block numbers. in order to find the longitude and latitude of every block, we wrote an algorithm to use onemap's api to retrieve the postal codes for every address. however, there were almost 200,000 rows of data that we had to update, hence this process took about 8 hours just to retrieve and update the postal codes. in order to avoid losing data during this time-consuming operation if something fails, we stored all relevant api responses into a .csv file. this is done to allow easy resumption after a potential crash or error and would save us from waiting for the algorithm to run from the start.\ndata analysis and machine learning are often done through python's jupyter notebook. however, our webserver is written in node.js, hence due to the language difference, we had to figure out a method for the node.js backend to be interoperable with the python machine learning model that we created. in the end, we pickled our jupyter notebook containing the model and passed it as a byte stream to another python script that handles the communication between the webserver and the ml model.\naccomplishments that we're proud of\n\u201cdata scientists spend 45% of the time prepping the data\u201d (according to research conducted by anaconda).\nthis is what happened to us while trying to deal with almost 200000 entries from real governmental datasets, and even trying to geocode and adding in new features like distance to the closest mrt station or shopping mall, or data repressing economy states like gdp per capita and sibor based on literature review.\nto examine and experiment with different ways (feature engineering and one-hot encoding) of handling categorical data while both getting accurate results with gradient boosting regression (95% and 96%) was also not an easy task.\nlast but definitely not least, it was difficult to fit the extracted model into the user interface with correct output in a speedy fashion. but our team was able to complete a basically done ui with accurate and fast response in a short period of time given.\nthese additional efforts allow us to build more accurate models while actually deploying in real life to assist more young adults (or anyone who wishes to purchase an hdb) to make a wise purchase decision.\nwhat we learned\nother than the accomplishments stated above that definitely help us develop more ml skills and stimulate our creativity, the model itself brought up some interesting facts.\nas we built 5 different ml models for 5 different room types, there are some interesting differences. for example, the weight of each variable in the regression model can be quite different based on the room-type of hdb, e.g. while sibor only accounts for around 1% weighting in the resale price for executive room type, it accounts for nearly 20% weighting when trying to predict the resale price of 4_room type.\nmoreover, it was also surprising to see some of the features that seem intuitively minor at the beginning turned out to be important features or, in some instances, the opposite. for example, sibor (an indicator of current interest rate) was actually an important feature, while the floor area wasn\u2019t really significant in most of the models.\nwhat's next for hdb price estimator\nfirst of all, a more complete ui interface can be built. we wish to return a more fully founded result to the user, not only the suggested pricing but reasonable pricing in a given area and visualization of release price history trend in the fast few months, etc.\nmoreover, as the relative importance of each variable is known from different models, combining with some more features like supply and demand of hdb release market or other factors, prediction of release price in the future is possible with sufficient data given. in this case, we can advance our model into a forward-looking \u201cpredictor\u201d that can even better assist the user with the purchase/investing decision, e.g. to purchase bto or hdb at the moment by comparing the given price of bto (from the government) and our prediction of hdb of a certain type.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59507664}, {"Unnamed: 0": 7665, "autor": "Artificial Inteligence", "date": null, "content": "What we learned\nArtificial intelligence systems ingest large amounts of labeled training data, analyze the data for correlations and patterns, and use these patterns to predict future states. If a chatbot is fed examples of text messages, it will learn to produce lifelike exchanges with people, or an image recognition tool will learn to identify and describe images by reviewing millions of examples. There are four types of artificial intelligence 1- Reactive machines. During the 1990s, Deep Blue defeated Garry Kasparov. The Deep Blue algorithm can identify pieces on a chessboard and make predictions, but because it has no memory, it cannot use past experiences to inform future decisions.\n2: Limited memory. These artificial intelligence systems are capable of learning from their past experience to make future decisions. In self-driving cars, some of the decision-making functions are designed in this way.\nTheory of mind. This is a psychology term. If applied to AI, it means the system will be able to understand emotions. In order for AI systems to become integral members of human teams, they need to be able to infer human intentions and predict behavior.\n4- Self-awareness. These AI systems possess a sense of self, which gives them consciousness. Self-aware machines recognize their own situation as it is. Such AI does not yet exist.\n##Examples of AI technology\nAutomation. Combined with AI technologies, automation tools can increase the volume and types of tasks performed. RPA, Machine learning. The science of getting a computer to act without programming. In very simple terms, deep learning can be described as the automation of predictive analytics", "link": "https://devpost.com/software/artificial-inteligence", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what we learned\nartificial intelligence systems ingest large amounts of labeled training data, analyze the data for correlations and patterns, and use these patterns to predict future states. if a chatbot is fed examples of text messages, it will learn to produce lifelike exchanges with people, or an image recognition -----> tool !!!  will learn to identify and describe images by reviewing millions of examples. there are four types of artificial intelligence 1- reactive machines. during the 1990s, deep blue defeated garry kasparov. the deep blue algorithm can identify pieces on a chessboard and make predictions, but because it has no memory, it cannot use past experiences to inform future decisions.\n2: limited memory. these artificial intelligence systems are capable of learning from their past experience to make future decisions. in self-driving cars, some of the decision-making functions are designed in this way.\ntheory of mind. this is a psychology term. if applied to ai, it means the system will be able to understand emotions. in order for ai systems to become integral members of human teams, they need to be able to infer human intentions and predict behavior.\n4- self-awareness. these ai systems possess a sense of self, which gives them consciousness. self-aware machines recognize their own situation as it is. such ai does not yet exist.\n##examples of ai technology\nautomation. combined with ai technologies, automation tools can increase the volume and types of tasks performed. rpa, machine learning. the science of getting a computer to act without programming. in very simple terms, deep learning can be described as the automation of predictive analytics", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59507665}, {"Unnamed: 0": 7689, "autor": "Semi-Automatic Brain Tumor Segment Annotation Tool using DL", "date": null, "content": "Inspiration\nBeing part of brain tumor segmentation team at university, this idea has been lingering behind in my head for reasonable amount of time to create a semi automated tool for annotation purposes in medical field in which a rough work is done by the computer and only the precision work is done by human experts.\nWhat it does\nIt provides a computer aided brain tumor segmentation annotation tool for radiologists to reduce the errors and help with their work.\nHow we built it\nFour main components on how I built of the project\nModel: Holds details regarding the model that was trained and other details. Model is written in pytorch(UNET) and used figshare dataset. This was the first step.\nsegment_server: Flask server that serves the inference model using REST api. This was the second setup and was relatively easy.\nvectorization_server: NodeJs-Express server that converts Raster images to Vector Image svg path. These svg path are the masks that can can overlaid on the orginal tumor for annotation purpose. This was horribly difficult as no good tools available in Python. This was the third step\nannotation_client: Dash app that provides the front-end to work with the segmentation server and allows for manual annotation after automatic annotation is done by inference model.\nChallenges we ran into\nBy far, the biggest was conversion of raster images to svg paths with no support available in python or julia (languages I'm comfortable in). That easily eat up half my time, then I had to create a vectorization_server rest api for conversion from raster images to vector images using potrace.js to get the svg paths in a way that was acceptable to dash app and plotly graphs.\nAccomplishments that we're proud of and What we learned\nI never did Flask server, Express server and not even Dash server ever before. I knew about brain tumor segmentation but not enough about these topics but I think I did fairly well with my deployements.\nWhat's next for DL supported Brain Annotation Tool\nImprovements in UI,UX is biggest one and to make it more accessible. Then availabilty of other models and improvements in accuracy. There is lot of space for improvement in speed of the process.", "link": "https://devpost.com/software/dl-supported-brain-annotation-tool", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nbeing part of brain tumor segmentation team at university, this idea has been lingering behind in my head for reasonable amount of time to create a semi automated -----> tool !!!  for annotation purposes in medical field in which a rough work is done by the computer and only the precision work is done by human experts.\nwhat it does\nit provides a computer aided brain tumor segmentation annotation tool for radiologists to reduce the errors and help with their work.\nhow we built it\nfour main components on how i built of the project\nmodel: holds details regarding the model that was trained and other details. model is written in pytorch(unet) and used figshare dataset. this was the first step.\nsegment_server: flask server that serves the inference model using rest api. this was the second setup and was relatively easy.\nvectorization_server: nodejs-express server that converts raster images to vector image svg path. these svg path are the masks that can can overlaid on the orginal tumor for annotation purpose. this was horribly difficult as no good tools available in python. this was the third step\nannotation_client: dash app that provides the front-end to work with the segmentation server and allows for manual annotation after automatic annotation is done by inference model.\nchallenges we ran into\nby far, the biggest was conversion of raster images to svg paths with no support available in python or julia (languages i'm comfortable in). that easily eat up half my time, then i had to create a vectorization_server rest api for conversion from raster images to vector images using potrace.js to get the svg paths in a way that was acceptable to dash app and plotly graphs.\naccomplishments that we're proud of and what we learned\ni never did flask server, express server and not even dash server ever before. i knew about brain tumor segmentation but not enough about these topics but i think i did fairly well with my deployements.\nwhat's next for dl supported brain annotation tool\nimprovements in ui,ux is biggest one and to make it more accessible. then availabilty of other models and improvements in accuracy. there is lot of space for improvement in speed of the process.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59507689}, {"Unnamed: 0": 7700, "autor": "[TOOLS AND APPS] RMRKspyBot and Rarity/Estimation tool", "date": null, "content": "Inspiration\nI like Telegram and RMRK, so why not? :)\nWhat it does\nhttps://t.me/RMRKspyBot\n1) Rarity tool for Birds and Items (/estimate_full ) (Done)\n2) Birds and Items price estimation (/estimate ) (Done)\n3) Telegram bot that reports RMRKv1,2 sales above a certain threshold. (Done)\n4) Monitoring cheap items and birds listed for sale (Done)\n5) Tracking NFT statuses Buy, Sell, Send, Burn (Done)\nBonus Telegram Channels:\nAll Kanaria sales https://t.me/kanariasales\nAll Singular sales https://t.me/singularsales\nDaily/weekly/monthly record sales https://t.me/RMRKtopSales (new day starts at 00:00:00 UTC)\nHow we built it\nrmrk2psql dump parser + python3 + telegram api\nChallenges we ran into\nmetadata has different format\nAccomplishments that we're proud of\nIt works\nWhat we learned\nCrazy people buy pictures for big money!\nWhat's next for [TOOLS AND APPS] RMRKspyBot\nComment code", "link": "https://devpost.com/software/tools-and-apps-rmrkspybot", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ni like telegram and rmrk, so why not? :)\nwhat it does\nhttps://t.me/rmrkspybot\n1) rarity -----> tool !!!  for birds and items (/estimate_full ) (done)\n2) birds and items price estimation (/estimate ) (done)\n3) telegram bot that reports rmrkv1,2 sales above a certain threshold. (done)\n4) monitoring cheap items and birds listed for sale (done)\n5) tracking nft statuses buy, sell, send, burn (done)\nbonus telegram channels:\nall kanaria sales https://t.me/kanariasales\nall singular sales https://t.me/singularsales\ndaily/weekly/monthly record sales https://t.me/rmrktopsales (new day starts at 00:00:00 utc)\nhow we built it\nrmrk2psql dump parser + python3 + telegram api\nchallenges we ran into\nmetadata has different format\naccomplishments that we're proud of\nit works\nwhat we learned\ncrazy people buy pictures for big money!\nwhat's next for [tools and apps] rmrkspybot\ncomment code", "sortedWord": "None", "removed": "Nan", "score": 13, "comments": 3, "media": null, "medialink": null, "identifyer": 59507700}, {"Unnamed: 0": 7712, "autor": "Bushier", "date": null, "content": "Inspiration\nIt gets rather hot in our room in the afternoon. While using the aircon was an option, it was not economically feasible in the long run. We wondered if using plants as a natural cooling option was possible, and if so, where can we place it on the walls outside the building to reduce the heat from coming in.\nWhat it does\nbushier is a visual advisory tool for building owners to identify optimal vertical greenery solutions for energy and cost saving measures through ML visual overlays. Use bushier to capture / upload building profiles to our ML models, to obtain optimal overlay of vertical greenery, potential energy and cost savings.\nHow we built it\nWe built a segmentation model (HRnet + OCR head) trained on various building profile datasets, ranging from LabelMeFacade, eTRIMS to CMP Facade. Then, used IMU, location data, and other web-scrapped data to predict build temperature, incident radiation, vertical garden placement, temperature reduction, energy and cost savings. We also explored multiple ways to predict building profile area, ranging from depth estimation methods to conventional geometrical methods.\nChallenges we ran into\nGetting the ML model to produce meaningful results (i.e. being able to train properly on the training sets)\nConnecting our middle end Flask with front end React Native, and other integration hell\nFinding good features / predictors of building temperature / incident radiation\nAccomplishments that we're proud of\nEventually the ML model worked and produced a sensible set of results\nWe linked up the frontend with the ML model and backend through Flask\nPrediction of building temperature on a range of inputs seems sensible\nAR output seems rather attractive, compared to we expect initially\nWhat we learned\nToo many things. The last 48 hours was a whirlwind as we faced all kinds of issues and had to debug them, even such as git pull/merge issues. However, every problem we faced was a learning opportunity. Our biggest takeaway was that it takes time to learn and having failures along the way are normal. Just don't give up.\nWhat's next for Cooling Singapore Sustainably\nWe hope to make the app more robust, to be able to take in more kinds of building surfaces, and produce a greater range of metrics that can be useful in encouraging vertical greenery solutions in Singapore\nAppendix\n1) Yuan. Y, X. Cheng, X. Chen, J. Wang, Segmentation Transformer: Object-Contextual Representations for Semantic Segmentation, ECCV 2020, https://arxiv.org/abs/1909.11065\n2) Ke et. al. High-Resolution Representations for Labeling Pixels and Regions, CVPR 2020, https://arxiv.org/abs/1904.04514\n3) B. Frohlich, E. Rodner, J. Denzler, A Fast Approach for Pixelwise Labeling of Fa\u00e7ade Images, ICPR 2010, https://www.researchgate.net/publication/220929012_A_Fast_Approach_for_Pixelwise_Labeling_of_Facade_Images\n4) Holm, D. (1989). Thermal improvement by means of leaf cover on external walls \u2014 a simulation model. Energy and Buildings, 14(1), 19\u201330. https://doi.org/10.1016/0378-7788(89)90025-x\n5) Canadarma, W. W., Juniwati, A., & Kristanto, L. (2006). Effect Of Orientation On Indoor Temperature Case Study: Yekape Penjaringansari Housing in Surabaya. INTA Conference 2006 - Harmony in Culture and Nature.\n6) P\u00e9rez, G., Coma, J., Martorell, I., & Cabeza, L. F. (2014). Vertical Greenery Systems (VGS) for energy saving in buildings: A Review. Renewable and Sustainable Energy Reviews, 39, 139\u2013165. https://doi.org/10.1016/j.rser.2014.07.055\n7) Pan, J., Tang, J., Caniza, M., Heraud, J.-M., Koay, E., Lee, H. K., Lee, C. K., Li, Y., Ruiz, A. N., Santillan-Salas, C. F., & Marr, L. (2021). Correlating indoor and outdoor temperature and humidity in a sample of buildings in tropical climates. https://doi.org/10.31224/osf.io/6ap5q\n8) Wong, N. H., Kwang Tan, A. Y., Chen, Y., Sekar, K., Tan, P. Y., Chan, D., Chiang, K., & Wong, N. C. (2010). Thermal evaluation of vertical greenery systems for building walls. Building and Environment, 45(3), 663\u2013672. https://doi.org/10.1016/j.buildenv.2009.08.005", "link": "https://devpost.com/software/cooling-singapore-sustainably", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nit gets rather hot in our room in the afternoon. while using the aircon was an option, it was not economically feasible in the long run. we wondered if using plants as a natural cooling option was possible, and if so, where can we place it on the walls outside the building to reduce the heat from coming in.\nwhat it does\nbushier is a visual advisory -----> tool !!!  for building owners to identify optimal vertical greenery solutions for energy and cost saving measures through ml visual overlays. use bushier to capture / upload building profiles to our ml models, to obtain optimal overlay of vertical greenery, potential energy and cost savings.\nhow we built it\nwe built a segmentation model (hrnet + ocr head) trained on various building profile datasets, ranging from labelmefacade, etrims to cmp facade. then, used imu, location data, and other web-scrapped data to predict build temperature, incident radiation, vertical garden placement, temperature reduction, energy and cost savings. we also explored multiple ways to predict building profile area, ranging from depth estimation methods to conventional geometrical methods.\nchallenges we ran into\ngetting the ml model to produce meaningful results (i.e. being able to train properly on the training sets)\nconnecting our middle end flask with front end react native, and other integration hell\nfinding good features / predictors of building temperature / incident radiation\naccomplishments that we're proud of\neventually the ml model worked and produced a sensible set of results\nwe linked up the frontend with the ml model and backend through flask\nprediction of building temperature on a range of inputs seems sensible\nar output seems rather attractive, compared to we expect initially\nwhat we learned\ntoo many things. the last 48 hours was a whirlwind as we faced all kinds of issues and had to debug them, even such as git pull/merge issues. however, every problem we faced was a learning opportunity. our biggest takeaway was that it takes time to learn and having failures along the way are normal. just don't give up.\nwhat's next for cooling singapore sustainably\nwe hope to make the app more robust, to be able to take in more kinds of building surfaces, and produce a greater range of metrics that can be useful in encouraging vertical greenery solutions in singapore\nappendix\n1) yuan. y, x. cheng, x. chen, j. wang, segmentation transformer: object-contextual representations for semantic segmentation, eccv 2020, https://arxiv.org/abs/1909.11065\n2) ke et. al. high-resolution representations for labeling pixels and regions, cvpr 2020, https://arxiv.org/abs/1904.04514\n3) b. frohlich, e. rodner, j. denzler, a fast approach for pixelwise labeling of fa\u00e7ade images, icpr 2010, https://www.researchgate.net/publication/220929012_a_fast_approach_for_pixelwise_labeling_of_facade_images\n4) holm, d. (1989). thermal improvement by means of leaf cover on external walls \u2014 a simulation model. energy and buildings, 14(1), 19\u201330. https://doi.org/10.1016/0378-7788(89)90025-x\n5) canadarma, w. w., juniwati, a., & kristanto, l. (2006). effect of orientation on indoor temperature case study: yekape penjaringansari housing in surabaya. inta conference 2006 - harmony in culture and nature.\n6) p\u00e9rez, g., coma, j., martorell, i., & cabeza, l. f. (2014). vertical greenery systems (vgs) for energy saving in buildings: a review. renewable and sustainable energy reviews, 39, 139\u2013165. https://doi.org/10.1016/j.rser.2014.07.055\n7) pan, j., tang, j., caniza, m., heraud, j.-m., koay, e., lee, h. k., lee, c. k., li, y., ruiz, a. n., santillan-salas, c. f., & marr, l. (2021). correlating indoor and outdoor temperature and humidity in a sample of buildings in tropical climates. https://doi.org/10.31224/osf.io/6ap5q\n8) wong, n. h., kwang tan, a. y., chen, y., sekar, k., tan, p. y., chan, d., chiang, k., & wong, n. c. (2010). thermal evaluation of vertical greenery systems for building walls. building and environment, 45(3), 663\u2013672. https://doi.org/10.1016/j.buildenv.2009.08.005", "sortedWord": "None", "removed": "Nan", "score": 9, "comments": 1, "media": null, "medialink": null, "identifyer": 59507712}, {"Unnamed: 0": 7757, "autor": "Adventi", "date": null, "content": "Inspiration\nOur goal is to help everyone have a safe adventure and have all the tools they need in one app!\nWhat it does\nOur app has many useful features:\nTrip Advisor - Search up vacations and places to go.\nBreadcrumbs - Fear getting lost? Not to fret, this feature logs your current location every x seconds. This will create a direct path to the trail you are on. If you get lost, either find your way back or the maps will point to the closest ranger station to find more help.\nPlant Identifier - A feature that helps users identify flora in the wild.\nPacking Checklist - Plans everything from To-dos to items recommended to pack.\nAdvisory - Find out the weather of the places you are going to make sure it is safe.\nHow we built it\nWe used\nReact Native\nJavascript\nMaps\nPlant.id API\nReddit API\nTrip Advisor API\nChallenges we ran into\nAt one point while working on the plant identifier, we could not get the camera to work. Eventually our whole app stopped working. Luckily, we had a backup of the app, which we used, getting our app set back up again.\nAccomplishments that we're proud of\nGetting the Plant Identifier to work\nGetting the Breadcrumbs map tool to work\nWeather displaying\nAmazing intuitive UI throughout our whole app\nWhat we learned\nHow to use the camera package in react native\nHow to use APIs\nTime management\nWhat's next for Adventi\nImproving existing features.\nPublishing it on the App Store and/or the Google Play store (if we win, to help the most people)!\nDownloading our app!\nTo download the project, you will have to download the GitHub repository and use the expo client to run it on your phone. (Instructions below if you need help.)\nDownloading the project:\nDownload the GitHub repository then unzip.\nOpen the folder in your code editor (eg. VSCode).\nDownload npm if not already installed and run npm install in the command prompt inside the project folder.\nRun npm install --global expo-cli in the command prompt in the folder.\nDownload Expo Go on your mobile phone.\nRun expo start in the command prompt in the folder, scan the QR Code, and open it.\nThe app should open in the Expo Go app!\nPresentation link\nGitHub Link\nDownload Website for Adventi", "link": "https://devpost.com/software/adventi", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nour goal is to help everyone have a safe adventure and have all the tools they need in one app!\nwhat it does\nour app has many useful features:\ntrip advisor - search up vacations and places to go.\nbreadcrumbs - fear getting lost? not to fret, this feature logs your current location every x seconds. this will create a direct path to the trail you are on. if you get lost, either find your way back or the maps will point to the closest ranger station to find more help.\nplant identifier - a feature that helps users identify flora in the wild.\npacking checklist - plans everything from to-dos to items recommended to pack.\nadvisory - find out the weather of the places you are going to make sure it is safe.\nhow we built it\nwe used\nreact native\njavascript\nmaps\nplant.id api\nreddit api\ntrip advisor api\nchallenges we ran into\nat one point while working on the plant identifier, we could not get the camera to work. eventually our whole app stopped working. luckily, we had a backup of the app, which we used, getting our app set back up again.\naccomplishments that we're proud of\ngetting the plant identifier to work\ngetting the breadcrumbs map -----> tool !!!  to work\nweather displaying\namazing intuitive ui throughout our whole app\nwhat we learned\nhow to use the camera package in react native\nhow to use apis\ntime management\nwhat's next for adventi\nimproving existing features.\npublishing it on the app store and/or the google play store (if we win, to help the most people)!\ndownloading our app!\nto download the project, you will have to download the github repository and use the expo client to run it on your phone. (instructions below if you need help.)\ndownloading the project:\ndownload the github repository then unzip.\nopen the folder in your code editor (eg. vscode).\ndownload npm if not already installed and run npm install in the command prompt inside the project folder.\nrun npm install --global expo-cli in the command prompt in the folder.\ndownload expo go on your mobile phone.\nrun expo start in the command prompt in the folder, scan the qr code, and open it.\nthe app should open in the expo go app!\npresentation link\ngithub link\ndownload website for adventi", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59507757}, {"Unnamed: 0": 7762, "autor": "006_405 Found - Chatbot service for senior citizens", "date": null, "content": "Inspiration\nInspired by the chatbot in Telegram and Siri, as well as the aging population in Asia, we wanted to come up with a tool that can enable effective communication with seniors to know their needs, help them if needed.\nWhat it does\nThis chatbot will focus on four major sections of conversations that are common in real life: greeting, entertaining, help and mental health. When the user initiates a conversation, we will try to provide relevant answers for the users. This would, psychologically, help the seniors.\nHow we built it\nStep 1: convert all English words into one-hot representation Step 2: Convert one-hot vectors into embedded word vectors Step 3: Parse the embedded word vectors into the encoder section Step 4: Generate the output word vectors in embedded format sequentially. Step 5: Output word vectors fed into the Dense Layer, converted back to one-hot representation Step 6: Identify the respective English words output\nChallenges we ran into\nThere are not much NLP-related dataset on the internet, which puts us in the scenario where we have to generate the dataset (write English conversations) on ourselves Time is limited, so we do not have too much time to write a dataset that covers a broad spectrum of conversational topics Debugging skills and developing a workable model in a short period of time, which is very challenging\nAccomplishments that we're proud of\nThe structure of the video and the functionality of the chatbot, especially when the input prompts contents relating to psychology and mental health which returns answers that are fulfilling.\nWhat we learned\nHow to convert our previous theoretical knowledge into a product that solves real-life challenges. This often requires a lot of fine-tuning and adaptation of our model to fit in to the particular challenge we are facing. In this project, we observe how the breadth and quality of our dataset (the English conversations that we write ourselves) will affect the generated sentence output. The reason that our chatbot is capable of returning fulfilling answers to psychological or mental health related contents is because we deliberately input the relevant English conversational topics into our dataset, which affects the final outcome.\nWhat's next for 006_405 Found - Chatbot service for senior citizens\nWe will try to add more features on this chatbot: bring internet of things, Health +, multi-language accessibility and outreach. If they are successfully done, we would enlarge our target audience and therefore help more seniors in need.", "link": "https://devpost.com/software/006_405-found", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ninspired by the chatbot in telegram and siri, as well as the aging population in asia, we wanted to come up with a -----> tool !!!  that can enable effective communication with seniors to know their needs, help them if needed.\nwhat it does\nthis chatbot will focus on four major sections of conversations that are common in real life: greeting, entertaining, help and mental health. when the user initiates a conversation, we will try to provide relevant answers for the users. this would, psychologically, help the seniors.\nhow we built it\nstep 1: convert all english words into one-hot representation step 2: convert one-hot vectors into embedded word vectors step 3: parse the embedded word vectors into the encoder section step 4: generate the output word vectors in embedded format sequentially. step 5: output word vectors fed into the dense layer, converted back to one-hot representation step 6: identify the respective english words output\nchallenges we ran into\nthere are not much nlp-related dataset on the internet, which puts us in the scenario where we have to generate the dataset (write english conversations) on ourselves time is limited, so we do not have too much time to write a dataset that covers a broad spectrum of conversational topics debugging skills and developing a workable model in a short period of time, which is very challenging\naccomplishments that we're proud of\nthe structure of the video and the functionality of the chatbot, especially when the input prompts contents relating to psychology and mental health which returns answers that are fulfilling.\nwhat we learned\nhow to convert our previous theoretical knowledge into a product that solves real-life challenges. this often requires a lot of fine-tuning and adaptation of our model to fit in to the particular challenge we are facing. in this project, we observe how the breadth and quality of our dataset (the english conversations that we write ourselves) will affect the generated sentence output. the reason that our chatbot is capable of returning fulfilling answers to psychological or mental health related contents is because we deliberately input the relevant english conversational topics into our dataset, which affects the final outcome.\nwhat's next for 006_405 found - chatbot service for senior citizens\nwe will try to add more features on this chatbot: bring internet of things, health +, multi-language accessibility and outreach. if they are successfully done, we would enlarge our target audience and therefore help more seniors in need.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59507762}, {"Unnamed: 0": 7766, "autor": "[Team 71-Onsen] Project Cactus", "date": null, "content": "Project Cactus\nA cross-platform AI Fake News Detector.\nWeb Application \u00b7 GitHub Repository \u00b7 Report Bugs \u00b7 Request Features\nInspiration \ud83d\udca1\nThe trend of taking ivermectin despite its dubious purported health benefits has perplexed and enraged health experts and doctors struggling to control Covid-19. Self-medicating with ivermectin has been widely reported in the U.S., despite medical professionals advising against it. However, problems generally arise when people consume the version of ivermectin that is only fit for animals.\nUnfortunately, this \"fake news made its way to Singapore\", where Vanessa Koh Wan-Ling, her mother's daughter, fell victim to false information, leading to her comatose state.\nVenessa's story shook us to the core, as it showed to us, that not only was fake news more prevalent than we thought, its consequences were deadly serious. Furthermore, we were taken aback by how fake news can reinforce people's perceptions and notions of things or ideas, thereby creating an echo chamber, which consequently creates a negative feedback loop. In addition, we were shocked at how fake news can spread like wildfire \ud83d\udd25.\nAs a result, our team was inspired to apply our knowledge and skills by combining software and frontend skills with Artificial Intelligence to combat the spread of fake news. We wanted the community to have a second opinion on whether it was fake news before they shared, spread and digest it.\nWhat it does \ud83d\udcaa\nProject Cactus is a cross-platform web app and extension that allows the community to verify their news with the help of a machine learning model. Moreover, both avenues allow users to flag out fake news as well!\nThe web app allows users to copy and paste the headline of news articles suspected to be fake news, upon which the user will receive feedback from the model on how confident it is that the article may contain fake news. Our users can then use the app's sharing functionality to share the model predictions to their friends and family on platforms like WhatsApp.\nOn the other hand, the web extension works automatically on social media platforms such as Twitter. It analyzes posts and articles in the user's feed while providing feedback for any that contain potentially misleading information. If the headline of news articles exceed a certain threshold, the users will receive feedbacks in red to proceed carefully. Otherwise, feedbacks are in green signalling the article looks safe. It encourages the community to be wary of any articles before proceeding so.\nIn addition, suppose you came across a provocative news article, that comes from a source you're not familar with. Or perhaps your parents have just shared a suspicous news article with you via WhatsApp. Before you share the article, you could choose to send the article to our model, which could tell you if the article is potentially fake news. This will let you make a more informed decision about whether or not to share the article, and you could even send the results of the query to your family, warning them of the misleading article.\nAnd what if you came across a piece of fake news that wasn't flagged? You can simply use our reporting system to report the article. This will help out in future updates to the model, allowing the model to stay abrest of fake news trends.\nHow we built it \ud83c\udfd7\ufe0f\nProject Cactus was built from the ground up by a group of technology enthusiasts. Each group member was responsible for a certain task to ensure fast development times.\nThis is a brief overview of how Project Cactus was built:\nData Curation \ud83d\udcda\nThe model is trained with Open Source Dataset with over 62k data points(And More!). Refer to our Reference page for links to the fake news datasets.\nThe Machine Learning Model \ud83e\udd16\nPowering our app is a Bidirectional Long Short Term Memory (LSTM) network, built-in Keras. It has been trained using over 62k news articles and makes use of pre-trained GloVe Word Embeddings for word representation.\nWe also did additional pre-processing on the data, such as removing stop words from input sentences, before tokenizing our inputs.\nThe model was trained using Google Colab, with the final model, evaluated on 15k news articles (7.5k for validation during training, 7.5k for the independent test set), obtaining 99% accuracy on the independent test set.\nModel Deployment & Backend API \ud83d\udcbb\nThe fully-trained model is deployed using Google Cloud AI Platform, running prediction mode accelerated with a Single NVIDIA TESLA T4 GPU.\nAn additional Node.js Backend API is then hosted with Heroku, to provide accessible API endpoints for web clients.\nWeb Extension \ud83e\udde9\nThe web extension uses JavaScript. It works by injecting a script into social media platforms before it loads, which then analyzes the posts. Does not store any user inputs or posts into a database unless manually reported by the users themselves.\nWeb App \ud83d\udd78\ufe0f\nThe web app, hosted on Firebase, is powered by Vue.js.\nChallenges we ran into \ud83e\uddf1\nSaving the model turned out to be rather difficult as the use of pre-trained word embeddings and a custom text pre-processing functions made it difficult to directly save the model. We discovered, after hours of training,that we had to serialize our pre-processing function before we could successfully load our saved model. In addition, the size of our model meant that our compute instance would have a tendency to crash due to a lack of memory.\nAccomplishments that we're proud of \ud83e\udd9a\nIt\u2019s a Trusted and Convenient Tool for Fake News Validation\nMinimalistic and Easy Interface\nCompleted targeted minimum viable product within very short span of time\nMulti-platform solution with seamless integration API server\nWhat we learned \ud83c\udfeb\nParsing data from Twitter was initially very confusing as Twitter uses a lazy loader. As such, fetching all the new nodes from the frontend was made hard. In addition, handling JavaScript asynchronous issues was a challenge for us as well as we were beginners.\nWe also learned how to export a trained Tensorflow Keras model, and deploy it on Google Cloud and Heroku, making use of Google Credentials.\nFinally, we learnt to work together, and distribute tasks such as to complete our project within a very short time span.\nWhat's next for [Team 71-Onsen] Project Cactus \u231b\nThere are several improvements we want to make to our project, which is summarised below.\nExtension support for other social media platforms for a wider reach (e.g. Reddit, Facebook and Instagram)\nSupport for other languages (e.g. Chinese, Tamil and Malay)\nImprovements on the AI model\nImprovements on inference speed via weight pruning and model quantization\nImprovements on network architecture for even better predictions\nAbility for Cactus to suggest trustworthy sources related to a given fake news article", "link": "https://devpost.com/software/team-71-onsen-project-cactus", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "project cactus\na cross-platform ai fake news detector.\nweb application \u00b7 github repository \u00b7 report bugs \u00b7 request features\ninspiration \ud83d\udca1\nthe trend of taking ivermectin despite its dubious purported health benefits has perplexed and enraged health experts and doctors struggling to control covid-19. self-medicating with ivermectin has been widely reported in the u.s., despite medical professionals advising against it. however, problems generally arise when people consume the version of ivermectin that is only fit for animals.\nunfortunately, this \"fake news made its way to singapore\", where vanessa koh wan-ling, her mother's daughter, fell victim to false information, leading to her comatose state.\nvenessa's story shook us to the core, as it showed to us, that not only was fake news more prevalent than we thought, its consequences were deadly serious. furthermore, we were taken aback by how fake news can reinforce people's perceptions and notions of things or ideas, thereby creating an echo chamber, which consequently creates a negative feedback loop. in addition, we were shocked at how fake news can spread like wildfire \ud83d\udd25.\nas a result, our team was inspired to apply our knowledge and skills by combining software and frontend skills with artificial intelligence to combat the spread of fake news. we wanted the community to have a second opinion on whether it was fake news before they shared, spread and digest it.\nwhat it does \ud83d\udcaa\nproject cactus is a cross-platform web app and extension that allows the community to verify their news with the help of a machine learning model. moreover, both avenues allow users to flag out fake news as well!\nthe web app allows users to copy and paste the headline of news articles suspected to be fake news, upon which the user will receive feedback from the model on how confident it is that the article may contain fake news. our users can then use the app's sharing functionality to share the model predictions to their friends and family on platforms like whatsapp.\non the other hand, the web extension works automatically on social media platforms such as twitter. it analyzes posts and articles in the user's feed while providing feedback for any that contain potentially misleading information. if the headline of news articles exceed a certain threshold, the users will receive feedbacks in red to proceed carefully. otherwise, feedbacks are in green signalling the article looks safe. it encourages the community to be wary of any articles before proceeding so.\nin addition, suppose you came across a provocative news article, that comes from a source you're not familar with. or perhaps your parents have just shared a suspicous news article with you via whatsapp. before you share the article, you could choose to send the article to our model, which could tell you if the article is potentially fake news. this will let you make a more informed decision about whether or not to share the article, and you could even send the results of the query to your family, warning them of the misleading article.\nand what if you came across a piece of fake news that wasn't flagged? you can simply use our reporting system to report the article. this will help out in future updates to the model, allowing the model to stay abrest of fake news trends.\nhow we built it \ud83c\udfd7\ufe0f\nproject cactus was built from the ground up by a group of technology enthusiasts. each group member was responsible for a certain task to ensure fast development times.\nthis is a brief overview of how project cactus was built:\ndata curation \ud83d\udcda\nthe model is trained with open source dataset with over 62k data points(and more!). refer to our reference page for links to the fake news datasets.\nthe machine learning model \ud83e\udd16\npowering our app is a bidirectional long short term memory (lstm) network, built-in keras. it has been trained using over 62k news articles and makes use of pre-trained glove word embeddings for word representation.\nwe also did additional pre-processing on the data, such as removing stop words from input sentences, before tokenizing our inputs.\nthe model was trained using google colab, with the final model, evaluated on 15k news articles (7.5k for validation during training, 7.5k for the independent test set), obtaining 99% accuracy on the independent test set.\nmodel deployment & backend api \ud83d\udcbb\nthe fully-trained model is deployed using google cloud ai platform, running prediction mode accelerated with a single nvidia tesla t4 gpu.\nan additional node.js backend api is then hosted with heroku, to provide accessible api endpoints for web clients.\nweb extension \ud83e\udde9\nthe web extension uses javascript. it works by injecting a script into social media platforms before it loads, which then analyzes the posts. does not store any user inputs or posts into a database unless manually reported by the users themselves.\nweb app \ud83d\udd78\ufe0f\nthe web app, hosted on firebase, is powered by vue.js.\nchallenges we ran into \ud83e\uddf1\nsaving the model turned out to be rather difficult as the use of pre-trained word embeddings and a custom text pre-processing functions made it difficult to directly save the model. we discovered, after hours of training,that we had to serialize our pre-processing function before we could successfully load our saved model. in addition, the size of our model meant that our compute instance would have a tendency to crash due to a lack of memory.\naccomplishments that we're proud of \ud83e\udd9a\nit\u2019s a trusted and convenient -----> tool !!!  for fake news validation\nminimalistic and easy interface\ncompleted targeted minimum viable product within very short span of time\nmulti-platform solution with seamless integration api server\nwhat we learned \ud83c\udfeb\nparsing data from twitter was initially very confusing as twitter uses a lazy loader. as such, fetching all the new nodes from the frontend was made hard. in addition, handling javascript asynchronous issues was a challenge for us as well as we were beginners.\nwe also learned how to export a trained tensorflow keras model, and deploy it on google cloud and heroku, making use of google credentials.\nfinally, we learnt to work together, and distribute tasks such as to complete our project within a very short time span.\nwhat's next for [team 71-onsen] project cactus \u231b\nthere are several improvements we want to make to our project, which is summarised below.\nextension support for other social media platforms for a wider reach (e.g. reddit, facebook and instagram)\nsupport for other languages (e.g. chinese, tamil and malay)\nimprovements on the ai model\nimprovements on inference speed via weight pruning and model quantization\nimprovements on network architecture for even better predictions\nability for cactus to suggest trustworthy sources related to a given fake news article", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59507766}, {"Unnamed: 0": 7773, "autor": "Businesses Run Together", "date": null, "content": "Inspiration\nSmall business owners and sole proprietors have experienced sharp revenue drops after Covid-19 resulting in many businesses having to make cuts or close down. This has resulted in an increase in unemployment which in turn severely impacts the general well being of the population. In addition, it is these distinctive small businesses that give New York City its personality and attract tourists. The adverse impact on businesses is toppling the economy, all the while the larger franchises and monopolies are making huge increases in their own profits and putting the locals out of business.\nWhat it does\nWe propose an app where businesses in the same neighborhood with similar domains are matched (using a similarity learning ML model). They can then connect and establish partnerships forming a collaborative ecosystem focused on collective growth, rather than fostering a competitive environment where businesses' trample on each other to succeed. In addition, the app will connect the businesses to potential customers and help them get more reach. The app will also help the business owners streamline an online ordering system should they wish to expand their reach via e-commerce. A dashboard in the app will provide insights into the business's profits and sale trends, impacts of promotions they ran, and other data analytics, as well as propose strategies they can implement to grow and evolve.\nThe app will be marketed to customers as an e-commerce assistant which they can use as a search tool to see what local businesses within their specified budget and distance range offer the products/services they are looking for and are interested in. The app will also let the customers directly order from the business if they offer a delivery service.\nThe app brings together small businesses and consumers in a discovery platform. Based on the consumer\u2019s previous spend (from their linked bank data) and location, the app recommends small businesses in the vicinity via the app. Consumers can click to learn more about the businesses, and leave reviews after they have visited the business, or used their services.\nHow we built it\nWe established what the community of small businesses could be better served thinking past the COVID-19 pandemic after the botched stimulus checks that were designated to go to them but larger businesses snagged them. From there, we laid out a few basic panels on Figma establishing the low fidelity wireframing that could serve as templates on how a potential user or small business owner would navigate through the app.\nSimplicity was a high priority of the development as any confusion or ambiguity would leave the users less likely to utilize the app. With both the business owners and customers having different interfaces, we wanted to have the app be the best place for business owners to both have an idea on their customers through an analytics dashboard and connect with other owners.\nAlgorithmic Ideas\nTo match businesses for potential partnerships we need a distance metric that takes into account the geographical location and the domain, services and product categories of two business and outputs a value indicating their similarity. Using this metric, we can suggest 3-5 other businesses for that the owner can partner with. This can be done using a Similarity Learning ML model that is typically used in recommendation systems and rankings.\nChallenges we ran into\nBeing able to shrink down the scope of our idea to a place where we could complete what we wanted to do. Lack of programming knowledge to implement the idea to the fullest extent. Starting later than most groups. Learning how to use Figma to create the panels.\nAccomplishments that we're proud of\nComing up with many ideas and narrowing it down with the help of our mentors to create a hack in the allotted time frame. We learned about UI/UX design, and how to use Figma. Being able to establish our pitch on how to present our finalized idea in a concise manner. Thank you to our mentors, Fiona and Mansi for your help! :)\nWhat we learned\nWe learned about low fidelity and high fidelity wireframing in Figma with the help of our mentor Fiona. Along with learning how to create a prototype and link together the frames with mentor Mansi. Additionally, we had a better understanding on how much small businesses have been affected over the last two years with the importance they put on society.\nWhat's next for Businesses Run Together\nAn analytics dashboard backed by programmed data, consumer pathway and other pathways. A Google map tracking pop up when you leave the store to leave a review, and establishing a marketing plan with QR codes to download in local established areas", "link": "https://devpost.com/software/businesses-run-together", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nsmall business owners and sole proprietors have experienced sharp revenue drops after covid-19 resulting in many businesses having to make cuts or close down. this has resulted in an increase in unemployment which in turn severely impacts the general well being of the population. in addition, it is these distinctive small businesses that give new york city its personality and attract tourists. the adverse impact on businesses is toppling the economy, all the while the larger franchises and monopolies are making huge increases in their own profits and putting the locals out of business.\nwhat it does\nwe propose an app where businesses in the same neighborhood with similar domains are matched (using a similarity learning ml model). they can then connect and establish partnerships forming a collaborative ecosystem focused on collective growth, rather than fostering a competitive environment where businesses' trample on each other to succeed. in addition, the app will connect the businesses to potential customers and help them get more reach. the app will also help the business owners streamline an online ordering system should they wish to expand their reach via e-commerce. a dashboard in the app will provide insights into the business's profits and sale trends, impacts of promotions they ran, and other data analytics, as well as propose strategies they can implement to grow and evolve.\nthe app will be marketed to customers as an e-commerce assistant which they can use as a search -----> tool !!!  to see what local businesses within their specified budget and distance range offer the products/services they are looking for and are interested in. the app will also let the customers directly order from the business if they offer a delivery service.\nthe app brings together small businesses and consumers in a discovery platform. based on the consumer\u2019s previous spend (from their linked bank data) and location, the app recommends small businesses in the vicinity via the app. consumers can click to learn more about the businesses, and leave reviews after they have visited the business, or used their services.\nhow we built it\nwe established what the community of small businesses could be better served thinking past the covid-19 pandemic after the botched stimulus checks that were designated to go to them but larger businesses snagged them. from there, we laid out a few basic panels on figma establishing the low fidelity wireframing that could serve as templates on how a potential user or small business owner would navigate through the app.\nsimplicity was a high priority of the development as any confusion or ambiguity would leave the users less likely to utilize the app. with both the business owners and customers having different interfaces, we wanted to have the app be the best place for business owners to both have an idea on their customers through an analytics dashboard and connect with other owners.\nalgorithmic ideas\nto match businesses for potential partnerships we need a distance metric that takes into account the geographical location and the domain, services and product categories of two business and outputs a value indicating their similarity. using this metric, we can suggest 3-5 other businesses for that the owner can partner with. this can be done using a similarity learning ml model that is typically used in recommendation systems and rankings.\nchallenges we ran into\nbeing able to shrink down the scope of our idea to a place where we could complete what we wanted to do. lack of programming knowledge to implement the idea to the fullest extent. starting later than most groups. learning how to use figma to create the panels.\naccomplishments that we're proud of\ncoming up with many ideas and narrowing it down with the help of our mentors to create a hack in the allotted time frame. we learned about ui/ux design, and how to use figma. being able to establish our pitch on how to present our finalized idea in a concise manner. thank you to our mentors, fiona and mansi for your help! :)\nwhat we learned\nwe learned about low fidelity and high fidelity wireframing in figma with the help of our mentor fiona. along with learning how to create a prototype and link together the frames with mentor mansi. additionally, we had a better understanding on how much small businesses have been affected over the last two years with the importance they put on society.\nwhat's next for businesses run together\nan analytics dashboard backed by programmed data, consumer pathway and other pathways. a google map tracking pop up when you leave the store to leave a review, and establishing a marketing plan with qr codes to download in local established areas", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 3, "media": null, "medialink": null, "identifyer": 59507773}, {"Unnamed: 0": 8031, "autor": "Flask", "date": null, "content": "Flask (source code) is a Python web framework built with a small core and easy-to-extend philosophy.\nWhy is Flask a good web framework choice?\nFlask is considered more Pythonic than the Django web framework because in common situations the equivalent Flask web application is more explicit. Flask is also easy to get started with as a beginner because there is little boilerplate code for getting a simple app up and running.\nHow does Flask relate to the Pallets Projects?\nFlask was originally designed and developed by Armin Ronacher as an April Fool's Day joke in 2010. Despite the origin as a joke, the Flask framework became wildly popular as an alternative to Django projects with their monolithic structure and dependencies.\nFlask's success created a lot of additional work in issue tickets and pull requests. Armin eventually created The Pallets Projects collection of open source code libraries after he had been managing Flask under his own GitHub account for several years. The Pallets Project now serves as the community-driven organization that handles Flask and other related Python libraries such as Lektor, Jinja and several others.\nFlask's lack of standard boilerplate via a commandline interface for setting up your project structure is a double edged sword. When you get started with Flask you will have to figure out how to scale the files and modules for the code in your application.\nOpen source code for learning Flask\nThere are many open source projects that rely on Flask to operate. One of the best ways to learn how to use this framework is to read how other projects use it in real-world code. This section lists these code examples by class and method in Flask.\nHow can we built it?\nStep 1: Flask Installation and Server Setup We are assuming that you have already installed Python, and it\u2019s up to date. So let\u2019s set up our project and set up a virtual environment.\nWhy do we need a virtual environment?\nA virtual environment is used to create an isolated Python environment for different projects. We create virtual environments because different projects have different dependencies. Also, it helps to keep the global packages folder clean.\nSo now open the terminal type the below command:\npip3 install virtualenv Once it\u2019s installed, let\u2019s create a directory or folder for your project. We are using flask-test as the folder name, but you can pick any name for your project.\nmkdir flask-test Then, change the directory:\ncd flask-test Now it\u2019s time to create a virtual environment for our project so that the dependencies don\u2019t mess up the global package folder.\nExecute the below command now:\nvirtualenv . And then,\nsource bin/activate Explanation: What we are doing here is we are telling the module that the current folder can be used for the virtual environment, and then it is activating the virtual environment in the second step.\nNow once you have activated the virtual environment, let\u2019s install the Flask package in that environment.\nNow run the below command:\nPython3 -m pip install Flask So far, we have created our project folder, installed and created a virtual environment for our project, and installed Flask in the environment. Let\u2019s head towards Step 2.\nStep 2: Let\u2019s Write Some Code Now create a file app.py and paste the below code:\nfrom flask import Flask app = Flask(name)\n@app.route('/') def hello_world(): return 'This is my first API call!' Make sure to save your app.py file to the current directory.\nCode Explanation: First, we are importing the flask module into our application and then defining the API route. The @ is used for defining the API route. We\u2019re passing /, which means this is our base route.\nStep 3: Running the Server and Making the First API Call Once you\u2019re done with the coding part, it\u2019s time to run our Flask server and make our first API call.\nTo run the server, execute the below command:\nflask run You should see the below output on the terminal:\n*Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. *Debug mode: off *Running on http://127.0.0.1:5000/ (Press CTRL+C to quit) Now open your favorite API testing tool. We\u2019ll be using Postman in the tutorial below, but you can pick any of the tools mentioned here. Now copy and paste the URL printed on the terminal and make a GET request. You should see the below output:\nTADA! Congratulations, you have made your first API call. Now let\u2019s make some APIs based on different methods like POST.\nStep 4: POST APIs Flask makes it very easy to program different commands for various HTTP methods like POST, GET, PUT, etc. In the above code, you can see there\u2019s the function route. The second parameter passed to this function is actually the method of the route. If nothing is passed then, it is GET.\nBut we also have to import two additional modules named request and jsonify used to fetch the params and JSON conversion.\nNow lets define another route for our POST requests. So open the app.py file and replace the existing code with the below code:\nfrom flask import Flask, request, jsonify\napp = Flask(name)\n@app.route('/') def hello_world(): return 'This is my first API call!'\n@app.route('/post', methods=[\"POST\"]) def testpost(): input_json = request.get_json(force=True) dictToReturn = {'text':input_json['text']} return jsonify(dictToReturn) Code Explanation: Here, we have imported some more modules from Flask, like request and jsonify. We are using request to get the data which the user is sending, and we\u2019re using jsonify for converting dictionaries to JSON. We have added one more route that is /post and also passing POST as a list and returning back what the user is sending in the parameters.\nNow once again head over to the API testing tool and hit URL: http://127.0.0.1:5000/post with parameters:\n{ \"text\": \"Post Example\" } Since we edited app.py, before you run a POST API call, you will need to restart the virtual server. To do so, (Press CTRL+C to quit), and then enter flask run into the Terminal again.", "link": "https://devpost.com/software/flask-w8vrot", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "flask (source code) is a python web framework built with a small core and easy-to-extend philosophy.\nwhy is flask a good web framework choice?\nflask is considered more pythonic than the django web framework because in common situations the equivalent flask web application is more explicit. flask is also easy to get started with as a beginner because there is little boilerplate code for getting a simple app up and running.\nhow does flask relate to the pallets projects?\nflask was originally designed and developed by armin ronacher as an april fool's day joke in 2010. despite the origin as a joke, the flask framework became wildly popular as an alternative to django projects with their monolithic structure and dependencies.\nflask's success created a lot of additional work in issue tickets and pull requests. armin eventually created the pallets projects collection of open source code libraries after he had been managing flask under his own github account for several years. the pallets project now serves as the community-driven organization that handles flask and other related python libraries such as lektor, jinja and several others.\nflask's lack of standard boilerplate via a commandline interface for setting up your project structure is a double edged sword. when you get started with flask you will have to figure out how to scale the files and modules for the code in your application.\nopen source code for learning flask\nthere are many open source projects that rely on flask to operate. one of the best ways to learn how to use this framework is to read how other projects use it in real-world code. this section lists these code examples by class and method in flask.\nhow can we built it?\nstep 1: flask installation and server setup we are assuming that you have already installed python, and it\u2019s up to date. so let\u2019s set up our project and set up a virtual environment.\nwhy do we need a virtual environment?\na virtual environment is used to create an isolated python environment for different projects. we create virtual environments because different projects have different dependencies. also, it helps to keep the global packages folder clean.\nso now open the terminal type the below command:\npip3 install virtualenv once it\u2019s installed, let\u2019s create a directory or folder for your project. we are using flask-test as the folder name, but you can pick any name for your project.\nmkdir flask-test then, change the directory:\ncd flask-test now it\u2019s time to create a virtual environment for our project so that the dependencies don\u2019t mess up the global package folder.\nexecute the below command now:\nvirtualenv . and then,\nsource bin/activate explanation: what we are doing here is we are telling the module that the current folder can be used for the virtual environment, and then it is activating the virtual environment in the second step.\nnow once you have activated the virtual environment, let\u2019s install the flask package in that environment.\nnow run the below command:\npython3 -m pip install flask so far, we have created our project folder, installed and created a virtual environment for our project, and installed flask in the environment. let\u2019s head towards step 2.\nstep 2: let\u2019s write some code now create a file app.py and paste the below code:\nfrom flask import flask app = flask(name)\n@app.route('/') def hello_world(): return 'this is my first api call!' make sure to save your app.py file to the current directory.\ncode explanation: first, we are importing the flask module into our application and then defining the api route. the @ is used for defining the api route. we\u2019re passing /, which means this is our base route.\nstep 3: running the server and making the first api call once you\u2019re done with the coding part, it\u2019s time to run our flask server and make our first api call.\nto run the server, execute the below command:\nflask run you should see the below output on the terminal:\n*environment: production warning: this is a development server. do not use it in a production deployment. use a production wsgi server instead. *debug mode: off *running on http://127.0.0.1:5000/ (press ctrl+c to quit) now open your favorite api testing -----> tool !!! . we\u2019ll be using postman in the tutorial below, but you can pick any of the tools mentioned here. now copy and paste the url printed on the terminal and make a get request. you should see the below output:\ntada! congratulations, you have made your first api call. now let\u2019s make some apis based on different methods like post.\nstep 4: post apis flask makes it very easy to program different commands for various http methods like post, get, put, etc. in the above code, you can see there\u2019s the function route. the second parameter passed to this function is actually the method of the route. if nothing is passed then, it is get.\nbut we also have to import two additional modules named request and jsonify used to fetch the params and json conversion.\nnow lets define another route for our post requests. so open the app.py file and replace the existing code with the below code:\nfrom flask import flask, request, jsonify\napp = flask(name)\n@app.route('/') def hello_world(): return 'this is my first api call!'\n@app.route('/post', methods=[\"post\"]) def testpost(): input_json = request.get_json(force=true) dicttoreturn = {'text':input_json['text']} return jsonify(dicttoreturn) code explanation: here, we have imported some more modules from flask, like request and jsonify. we are using request to get the data which the user is sending, and we\u2019re using jsonify for converting dictionaries to json. we have added one more route that is /post and also passing post as a list and returning back what the user is sending in the parameters.\nnow once again head over to the api testing tool and hit url: http://127.0.0.1:5000/post with parameters:\n{ \"text\": \"post example\" } since we edited app.py, before you run a post api call, you will need to restart the virtual server. to do so, (press ctrl+c to quit), and then enter flask run into the terminal again.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59508031}, {"Unnamed: 0": 8129, "autor": "Flask", "date": null, "content": "Flask (source code) is a Python web framework built with a small core and easy-to-extend philosophy.\nWhy is Flask a good web framework choice?\nFlask is considered more Pythonic than the Django web framework because in common situations the equivalent Flask web application is more explicit. Flask is also easy to get started with as a beginner because there is little boilerplate code for getting a simple app up and running.\nHow does Flask relate to the Pallets Projects?\nFlask was originally designed and developed by Armin Ronacher as an April Fool's Day joke in 2010. Despite the origin as a joke, the Flask framework became wildly popular as an alternative to Django projects with their monolithic structure and dependencies.\nFlask's success created a lot of additional work in issue tickets and pull requests. Armin eventually created The Pallets Projects collection of open source code libraries after he had been managing Flask under his own GitHub account for several years. The Pallets Project now serves as the community-driven organization that handles Flask and other related Python libraries such as Lektor, Jinja and several others.\nFlask's lack of standard boilerplate via a commandline interface for setting up your project structure is a double edged sword. When you get started with Flask you will have to figure out how to scale the files and modules for the code in your application.\nOpen source code for learning Flask\nThere are many open source projects that rely on Flask to operate. One of the best ways to learn how to use this framework is to read how other projects use it in real-world code. This section lists these code examples by class and method in Flask.\nHow can we built it?\nStep 1: Flask Installation and Server Setup We are assuming that you have already installed Python, and it\u2019s up to date. So let\u2019s set up our project and set up a virtual environment.\nWhy do we need a virtual environment?\nA virtual environment is used to create an isolated Python environment for different projects. We create virtual environments because different projects have different dependencies. Also, it helps to keep the global packages folder clean.\nSo now open the terminal type the below command:\npip3 install virtualenv Once it\u2019s installed, let\u2019s create a directory or folder for your project. We are using flask-test as the folder name, but you can pick any name for your project.\nmkdir flask-test Then, change the directory:\ncd flask-test Now it\u2019s time to create a virtual environment for our project so that the dependencies don\u2019t mess up the global package folder.\nExecute the below command now:\nvirtualenv . And then,\nsource bin/activate Explanation: What we are doing here is we are telling the module that the current folder can be used for the virtual environment, and then it is activating the virtual environment in the second step.\nNow once you have activated the virtual environment, let\u2019s install the Flask package in that environment.\nNow run the below command:\nPython3 -m pip install Flask So far, we have created our project folder, installed and created a virtual environment for our project, and installed Flask in the environment. Let\u2019s head towards Step 2.\nStep 2: Let\u2019s Write Some Code Now create a file app.py and paste the below code:\nfrom flask import Flask app = Flask(name)\n@app.route('/') def hello_world(): return 'This is my first API call!' Make sure to save your app.py file to the current directory.\nCode Explanation: First, we are importing the flask module into our application and then defining the API route. The @ is used for defining the API route. We\u2019re passing /, which means this is our base route.\nStep 3: Running the Server and Making the First API Call Once you\u2019re done with the coding part, it\u2019s time to run our Flask server and make our first API call.\nTo run the server, execute the below command:\nflask run You should see the below output on the terminal:\nEnvironment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\nDebug mode: off\nRunning on http://127.0.0.1:5000/ (Press CTRL+C to quit) Now open your favorite API testing tool. We\u2019ll be using Postman in the tutorial below, but you can pick any of the tools mentioned here.\nNow copy and paste the URL printed on the terminal and make a GET request. You should see the below output:\nTADA! Congratulations, you have made your first API call. Now let\u2019s make some APIs based on different methods like POST.\nStep 4: POST APIs Flask makes it very easy to program different commands for various HTTP methods like POST, GET, PUT, etc. In the above code, you can see there\u2019s the function route. The second parameter passed to this function is actually the method of the route. If nothing is passed then, it is GET.\nBut we also have to import two additional modules named request and jsonify used to fetch the params and JSON conversion.\nNow lets define another route for our POST requests. So open the app.py file and replace the existing code with the below code:\nfrom flask import Flask, request, jsonify\napp = Flask(name)\n@app.route('/') def hello_world(): return 'This is my first API call!'\n@app.route('/post', methods=[\"POST\"]) def testpost(): input_json = request.get_json(force=True) dictToReturn = {'text':input_json['text']} return jsonify(dictToReturn) Code Explanation: Here, we have imported some more modules from Flask, like request and jsonify. We are using request to get the data which the user is sending, and we\u2019re using jsonify for converting dictionaries to JSON. We have added one more route that is /post and also passing POST as a list and returning back what the user is sending in the parameters.\nNow once again head over to the API testing tool and hit URL: http://127.0.0.1:5000/post with parameters:\n{ \"text\": \"Post Example\" } Since we edited app.py, before you run a POST API call, you will need to restart the virtual server. To do so, (Press CTRL+C to quit), and then enter flask run into the Terminal again.", "link": "https://devpost.com/software/flask-a6zuce", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "flask (source code) is a python web framework built with a small core and easy-to-extend philosophy.\nwhy is flask a good web framework choice?\nflask is considered more pythonic than the django web framework because in common situations the equivalent flask web application is more explicit. flask is also easy to get started with as a beginner because there is little boilerplate code for getting a simple app up and running.\nhow does flask relate to the pallets projects?\nflask was originally designed and developed by armin ronacher as an april fool's day joke in 2010. despite the origin as a joke, the flask framework became wildly popular as an alternative to django projects with their monolithic structure and dependencies.\nflask's success created a lot of additional work in issue tickets and pull requests. armin eventually created the pallets projects collection of open source code libraries after he had been managing flask under his own github account for several years. the pallets project now serves as the community-driven organization that handles flask and other related python libraries such as lektor, jinja and several others.\nflask's lack of standard boilerplate via a commandline interface for setting up your project structure is a double edged sword. when you get started with flask you will have to figure out how to scale the files and modules for the code in your application.\nopen source code for learning flask\nthere are many open source projects that rely on flask to operate. one of the best ways to learn how to use this framework is to read how other projects use it in real-world code. this section lists these code examples by class and method in flask.\nhow can we built it?\nstep 1: flask installation and server setup we are assuming that you have already installed python, and it\u2019s up to date. so let\u2019s set up our project and set up a virtual environment.\nwhy do we need a virtual environment?\na virtual environment is used to create an isolated python environment for different projects. we create virtual environments because different projects have different dependencies. also, it helps to keep the global packages folder clean.\nso now open the terminal type the below command:\npip3 install virtualenv once it\u2019s installed, let\u2019s create a directory or folder for your project. we are using flask-test as the folder name, but you can pick any name for your project.\nmkdir flask-test then, change the directory:\ncd flask-test now it\u2019s time to create a virtual environment for our project so that the dependencies don\u2019t mess up the global package folder.\nexecute the below command now:\nvirtualenv . and then,\nsource bin/activate explanation: what we are doing here is we are telling the module that the current folder can be used for the virtual environment, and then it is activating the virtual environment in the second step.\nnow once you have activated the virtual environment, let\u2019s install the flask package in that environment.\nnow run the below command:\npython3 -m pip install flask so far, we have created our project folder, installed and created a virtual environment for our project, and installed flask in the environment. let\u2019s head towards step 2.\nstep 2: let\u2019s write some code now create a file app.py and paste the below code:\nfrom flask import flask app = flask(name)\n@app.route('/') def hello_world(): return 'this is my first api call!' make sure to save your app.py file to the current directory.\ncode explanation: first, we are importing the flask module into our application and then defining the api route. the @ is used for defining the api route. we\u2019re passing /, which means this is our base route.\nstep 3: running the server and making the first api call once you\u2019re done with the coding part, it\u2019s time to run our flask server and make our first api call.\nto run the server, execute the below command:\nflask run you should see the below output on the terminal:\nenvironment: production warning: this is a development server. do not use it in a production deployment. use a production wsgi server instead.\ndebug mode: off\nrunning on http://127.0.0.1:5000/ (press ctrl+c to quit) now open your favorite api testing -----> tool !!! . we\u2019ll be using postman in the tutorial below, but you can pick any of the tools mentioned here.\nnow copy and paste the url printed on the terminal and make a get request. you should see the below output:\ntada! congratulations, you have made your first api call. now let\u2019s make some apis based on different methods like post.\nstep 4: post apis flask makes it very easy to program different commands for various http methods like post, get, put, etc. in the above code, you can see there\u2019s the function route. the second parameter passed to this function is actually the method of the route. if nothing is passed then, it is get.\nbut we also have to import two additional modules named request and jsonify used to fetch the params and json conversion.\nnow lets define another route for our post requests. so open the app.py file and replace the existing code with the below code:\nfrom flask import flask, request, jsonify\napp = flask(name)\n@app.route('/') def hello_world(): return 'this is my first api call!'\n@app.route('/post', methods=[\"post\"]) def testpost(): input_json = request.get_json(force=true) dicttoreturn = {'text':input_json['text']} return jsonify(dicttoreturn) code explanation: here, we have imported some more modules from flask, like request and jsonify. we are using request to get the data which the user is sending, and we\u2019re using jsonify for converting dictionaries to json. we have added one more route that is /post and also passing post as a list and returning back what the user is sending in the parameters.\nnow once again head over to the api testing tool and hit url: http://127.0.0.1:5000/post with parameters:\n{ \"text\": \"post example\" } since we edited app.py, before you run a post api call, you will need to restart the virtual server. to do so, (press ctrl+c to quit), and then enter flask run into the terminal again.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59508129}, {"Unnamed: 0": 8191, "autor": "Uchoose", "date": null, "content": "Inspiration\nThe project started this fall at the definition hackathon. The main idea of the project is to allow NFT buyers to directly participate in the creation of the collections, and to give artists a convenient tool to lay the foundation for the new collections.\nWhat it does\nUchoose is a dApp that allows artists to create programmatically generated collections without writing any code.\nArtists can initiate and upload the set of parameters which would be used for NFT generation in the collection.\nMoreover, with the help of our dApp buyers control the appearance of the created NFT and release it by choosing its parameters from a set (the new NFT generated only after the purchase of the previous one in collection).\nHow we built it\nProject divided into two main parts. Backend and frontend.\nBackend build upon popular stack of technologies: .NET Core (+ different opensource libraries such as MediatR, AutoMapper, Serilog, Seq, FluentValidation and others) and PostgreSQL.\nThe C # Solnet SDK is used as the SDK for interacting with Solana on the backend.\nFrontend part is build on React.js library.\nWe use integration with Ceramic network for creating and changing Ceramic streams with mutable metadata.\nChallenges we ran into\nWe've faced some issues with integration of different libraries into project:\nStarter pack provided for Next.js framework was not buildable/compilable, that's why we've decided to switch to React.\nSolnet.Metaplex is steel on dev stage and not published on nuget packages repository.\nAccomplishments that we're proud of\nWe learned many new technologies related with blockchain. We became winners of the definition hackathon (3rd place from Ceramic).\nWhat we learned\nLearned how to create smart contracts in Solana, create Ceramic streams and store dynamic data in them and more other. In addition, we delved deeper into the world of DeFi and blockchain.\nWhat's next for Uchoose\nCommunity Expansion\nIntegration with marketplaces\nAdd other popular networks\nIncrease our team\nRelease Beta\nAnd more other... (see the roadmap in our GitHub page)", "link": "https://devpost.com/software/uchoose", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nthe project started this fall at the definition hackathon. the main idea of the project is to allow nft buyers to directly participate in the creation of the collections, and to give artists a convenient -----> tool !!!  to lay the foundation for the new collections.\nwhat it does\nuchoose is a dapp that allows artists to create programmatically generated collections without writing any code.\nartists can initiate and upload the set of parameters which would be used for nft generation in the collection.\nmoreover, with the help of our dapp buyers control the appearance of the created nft and release it by choosing its parameters from a set (the new nft generated only after the purchase of the previous one in collection).\nhow we built it\nproject divided into two main parts. backend and frontend.\nbackend build upon popular stack of technologies: .net core (+ different opensource libraries such as mediatr, automapper, serilog, seq, fluentvalidation and others) and postgresql.\nthe c # solnet sdk is used as the sdk for interacting with solana on the backend.\nfrontend part is build on react.js library.\nwe use integration with ceramic network for creating and changing ceramic streams with mutable metadata.\nchallenges we ran into\nwe've faced some issues with integration of different libraries into project:\nstarter pack provided for next.js framework was not buildable/compilable, that's why we've decided to switch to react.\nsolnet.metaplex is steel on dev stage and not published on nuget packages repository.\naccomplishments that we're proud of\nwe learned many new technologies related with blockchain. we became winners of the definition hackathon (3rd place from ceramic).\nwhat we learned\nlearned how to create smart contracts in solana, create ceramic streams and store dynamic data in them and more other. in addition, we delved deeper into the world of defi and blockchain.\nwhat's next for uchoose\ncommunity expansion\nintegration with marketplaces\nadd other popular networks\nincrease our team\nrelease beta\nand more other... (see the roadmap in our github page)", "sortedWord": "None", "removed": "Nan", "score": 18, "comments": 3, "media": null, "medialink": null, "identifyer": 59508191}, {"Unnamed: 0": 8195, "autor": "My MedicAid", "date": null, "content": "\ud83d\udca1 Inspiration \ud83d\udca1\nWe call them heroes, but the support we give them is equal to the one of a slave. Because of the COVID-19 pandemic, a lot of medics have to keep track of their patient's history, symptoms, and possible diseases. However, we've talked with a lot of medics, and almost all of them share the same problem when tracking the patients: Their software is either clunky and bad for productivity, or too expensive to use on a bigger scale. Most of the time, there is a lot of unnecessary management that needs to be done to get a patient on the record.\nMoreover, the software can even get the clinician so tired they have a risk of burnout, which makes their disease predictions even worse the more they work, and with the average computer-assisted interview lasting more than 20 minutes and a medic having more than 30 patients on average a day, the risk is even worse. That's where we introduce My MedicAid. With our AI-assisted patient tracker, we reduce this time frame from 20 minutes to only 5 minutes. This platform is easy to use and focused on giving the medics the ultimate productivity tool for patient tracking.\n\u2753 What it does \u2753\nMy MedicAid gets rid of all of the unnecessary management that is unfortunately common in the medical software industry. With My MedicAid, medics can track their patients by different categories and even get help for their disease predictions using an AI-assisted engine to guide them towards the urgency of the symptoms and the probable dangers that the patient is exposed to. With all of the enhancements and our platform being easy to use, we give the user (medic) a 50-75% productivity enhancement compared to the older, expensive, and clunky patient tracking software.\n\ud83c\udfd7\ufe0f How we built it \ud83c\udfd7\ufe0f\nThe patient's symptoms get tracked through an AI-assisted symptom checker, which uses APIMedic to process all of the symptoms and quickly return the danger of them and any probable diseases to help the medic take a decision quickly without having to ask for the symptoms by themselves. This completely removes the process of having to ask the patient how they feel and speeds up the process for the medic to predict what disease their patient might have since they already have some possible diseases that were returned by the API. We used Tailwind CSS and Next JS for the Frontend, MongoDB for the patient tracking database, and Express JS for the Backend.\n\ud83d\udea7 Challenges we ran into \ud83d\udea7\nWe had never used APIMedic before, so going through their documentation and getting to implement it was one of the biggest challenges. However, we're happy that we now have experience with more 3rd party APIs, and this API is of great use, especially with this project. Integrating the backend and frontend was another one of the challenges.\n\u2705 Accomplishments that we're proud of \u2705\nThe accomplishment that we're the proudest of would probably be the fact that we got the management system and the 3rd party API working correctly. This opens the door to work further on this project in the future and get to fully deploy it to tackle its main objective, especially since this is of great importance in the pandemic, where a lot of patient management needs to be done.\n\ud83d\ude4b\u200d\u2642\ufe0f What we learned \ud83d\ude4b\u200d\u2642\ufe0f\nWe learned a lot about CRUD APIs and the usage of 3rd party APIs in personal projects. We also learned a lot about the field of medical software by talking to medics in the field who have way more experience than us. However, we hope that this tool helps them in their productivity and to remove their burnout, which is something critical, especially in this pandemic.\n\ud83d\udcad What's next for My MedicAid \ud83d\udcad\nWe plan on implementing an NLP-based service to make it easier for the medics to just type what the patient is feeling like a text prompt, and detect the possible diseases just from that prompt. We also plan on implementing a private 1-on-1 chat between the patient and the medic to resolve any complaints that the patient might have, and for the medic to use if they need more info from the patient.", "link": "https://devpost.com/software/medicaid-3f2er5", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "\ud83d\udca1 inspiration \ud83d\udca1\nwe call them heroes, but the support we give them is equal to the one of a slave. because of the covid-19 pandemic, a lot of medics have to keep track of their patient's history, symptoms, and possible diseases. however, we've talked with a lot of medics, and almost all of them share the same problem when tracking the patients: their software is either clunky and bad for productivity, or too expensive to use on a bigger scale. most of the time, there is a lot of unnecessary management that needs to be done to get a patient on the record.\nmoreover, the software can even get the clinician so tired they have a risk of burnout, which makes their disease predictions even worse the more they work, and with the average computer-assisted interview lasting more than 20 minutes and a medic having more than 30 patients on average a day, the risk is even worse. that's where we introduce my medicaid. with our ai-assisted patient tracker, we reduce this time frame from 20 minutes to only 5 minutes. this platform is easy to use and focused on giving the medics the ultimate productivity -----> tool !!!  for patient tracking.\n\u2753 what it does \u2753\nmy medicaid gets rid of all of the unnecessary management that is unfortunately common in the medical software industry. with my medicaid, medics can track their patients by different categories and even get help for their disease predictions using an ai-assisted engine to guide them towards the urgency of the symptoms and the probable dangers that the patient is exposed to. with all of the enhancements and our platform being easy to use, we give the user (medic) a 50-75% productivity enhancement compared to the older, expensive, and clunky patient tracking software.\n\ud83c\udfd7\ufe0f how we built it \ud83c\udfd7\ufe0f\nthe patient's symptoms get tracked through an ai-assisted symptom checker, which uses apimedic to process all of the symptoms and quickly return the danger of them and any probable diseases to help the medic take a decision quickly without having to ask for the symptoms by themselves. this completely removes the process of having to ask the patient how they feel and speeds up the process for the medic to predict what disease their patient might have since they already have some possible diseases that were returned by the api. we used tailwind css and next js for the frontend, mongodb for the patient tracking database, and express js for the backend.\n\ud83d\udea7 challenges we ran into \ud83d\udea7\nwe had never used apimedic before, so going through their documentation and getting to implement it was one of the biggest challenges. however, we're happy that we now have experience with more 3rd party apis, and this api is of great use, especially with this project. integrating the backend and frontend was another one of the challenges.\n\u2705 accomplishments that we're proud of \u2705\nthe accomplishment that we're the proudest of would probably be the fact that we got the management system and the 3rd party api working correctly. this opens the door to work further on this project in the future and get to fully deploy it to tackle its main objective, especially since this is of great importance in the pandemic, where a lot of patient management needs to be done.\n\ud83d\ude4b\u200d\u2642\ufe0f what we learned \ud83d\ude4b\u200d\u2642\ufe0f\nwe learned a lot about crud apis and the usage of 3rd party apis in personal projects. we also learned a lot about the field of medical software by talking to medics in the field who have way more experience than us. however, we hope that this tool helps them in their productivity and to remove their burnout, which is something critical, especially in this pandemic.\n\ud83d\udcad what's next for my medicaid \ud83d\udcad\nwe plan on implementing an nlp-based service to make it easier for the medics to just type what the patient is feeling like a text prompt, and detect the possible diseases just from that prompt. we also plan on implementing a private 1-on-1 chat between the patient and the medic to resolve any complaints that the patient might have, and for the medic to use if they need more info from the patient.", "sortedWord": "None", "removed": "Nan", "score": 6, "comments": 0, "media": null, "medialink": null, "identifyer": 59508195}, {"Unnamed: 0": 8196, "autor": "SIGILLUM", "date": null, "content": "Inspiration\nDeFi ecosystem, user experience and the simplicity are the focus of our projects. Every day we listen about exchanges's scams, hight fees, hight interests, hidden and complex intermediary systems that works only to improve their conditions without thinking to users. We love programming, we love blockchain, we love the DeFi ideology but we also love make our future better than now using technologies and our skills. Many challanges are very difficult to face up, however we do our best to overcome hardles and a plus is the continuous improving of our skills and teamworks. People who use their skills (strong or not) to build a better future for our society are our inspiration.\nWhat it does\nAs you can see from image gallery, our idea concerns on receive a token (it\u2019s name is verified wallet) that will be required to access to specific services like decentralised exchanges. The token will be send from a whitelister authority, such as regulated entities, through Sigillum website, only to the accounts that have been successfully completed Know Your Customer procedure on the platform.\nAlgorand offers the infrastructure on which token deliveries are recorded. ASAs offer an excellent tool for representing a token that will be used to identify users belonging to the whitelist. In particular, if a user has a token it means he will be whitelisted and if a user is whitelisted then he will be offered numerous opportunities and benefits from DeFi projects such as: decentralised exchanges, decentralised estate plataforms, decentralised social media, etc etc . . . . Therefore, our token allow to understand if an account is verified or not. Every account must receive at most one token each, after opt-in for it. The opt-in procedure required to compile the Know Your Customer form.\nWe think that, especially when it comes to managing the \"public thing\", transparency is fundamental. Through the Algo Explorer platform it is possible to view all the transactions that take place on the blockchain, verifying who owns a specific token or not.\nSigillum's platform provide three main sections:\nUser section;\nAdmin panel;\nDecentralised demo marketplace.\nUser section allows users to compile Know Your Customer form and opt-in for our token. Admin panel allows an regulated entity to whitelist a specific user sending him verified token, using the wallet address of the user. In addition, admin can see in a table the following information:\nWho send all information or not and data that are loaded successfully or not;\nWho has (or not) received the token, yet;\nDemo marketplace is an exaple of decentralised platform that required our tokens to login. In this way, only users that have been completed successfully KYC and opt-in procedure could login to DeFi platforms (will be explain better in the following sections).\nHow we built it\nAccording to our inspirations and user experience, we build our project following the 10 Usability Heuristics for User Interface Design by Nielsen and Molich. We made our platform as simple as possible in such a way regulated entities and users could trust us.\nUsing Django framework we have developed back-end and a database to store information about the users who made KYC procedure and Algorand\u2019s wallets addresses. Data are docupled using hash functions, so it won\u2019t be possible to retrieve fiscal code knowing the wallet address or vice versa. Nobody can know what is the relation between fiscal code and wallet addresses, ensuring the protection of sensitive data (according to GDPR).\nUsing Python, we have developed the logic of back-end who will comunicate with Algorand's blockchain because of user-admin actions. We designed a connect Algorand's wallet feature, in this way user could use our services using, for example, Algorand wallet mobile app or myAlgo browser plugin. Login and use of our logic will be fast and secure.\nChallenges we ran into\nWe ran into many difficult challenges such as:\nAlgorand Wallet connecting features. Because of it using React framwork (we didn't know it) it was difficult to integrate this function. We tried to workaround using Pipeline UI or AlgoPay but there was little time to study their documentation and, at the same time, design our project, developing front-end and back-end logic. Anyways, we do of our best.\nTime management. Because of our University's engagements we have to organize our days to maximise our objective function: continue studying by following our university's lectures and make the best project that we could have done for this hackathon.\nProgramming languages. We didn't realize a lot of projects in our career and our knowledge is limited. We use this opportunity to improve our skills.\nFinally, we come across many difficult challenges such as design choices and bugs, but we tried to realize something we belived in, useful and simple.\nAccomplishments that we're proud of\nWe are proud that we have realised something that works and could be use from users and regulated entities beacause of its simplicity. We are proud of solve a problem with an innovative, secure, trasparent solution that is really operable in everyday life. Finally, we are proud of done everithing and every key concept/idea that we have designed in the initial part of the project without omitting user experience and feasibility.\nWhat we learned\nThis Hackathon was a good opportunity to learn team work and programming skills. To coordinate our work, we use Trello, and we have conciliate ideas of team's members. This type of challenges allow us to improve our friendship because we have to work together to the solution to specific problem. We are really happy of that. We improve our skills and knowledge of Python, Django, PostgreSQL, HTML, CSS, Javascript, Bootstrap, time management and, obviously, we learn more about Algorand and its potential.\nWhat's next for SIGILLUM\nImprove the Algorand wallect feature. Implement algorithms that detect incorrect behavior. Improvements of Know Your Customer procedure according to national legislation implementing the European directive such as the verification of:\nidentification data;\nemail addres;\nphone number;;\ngeolocation;\ncard identity;\nphoto in real time;", "link": "https://devpost.com/software/defi-authorized", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ndefi ecosystem, user experience and the simplicity are the focus of our projects. every day we listen about exchanges's scams, hight fees, hight interests, hidden and complex intermediary systems that works only to improve their conditions without thinking to users. we love programming, we love blockchain, we love the defi ideology but we also love make our future better than now using technologies and our skills. many challanges are very difficult to face up, however we do our best to overcome hardles and a plus is the continuous improving of our skills and teamworks. people who use their skills (strong or not) to build a better future for our society are our inspiration.\nwhat it does\nas you can see from image gallery, our idea concerns on receive a token (it\u2019s name is verified wallet) that will be required to access to specific services like decentralised exchanges. the token will be send from a whitelister authority, such as regulated entities, through sigillum website, only to the accounts that have been successfully completed know your customer procedure on the platform.\nalgorand offers the infrastructure on which token deliveries are recorded. asas offer an excellent -----> tool !!!  for representing a token that will be used to identify users belonging to the whitelist. in particular, if a user has a token it means he will be whitelisted and if a user is whitelisted then he will be offered numerous opportunities and benefits from defi projects such as: decentralised exchanges, decentralised estate plataforms, decentralised social media, etc etc . . . . therefore, our token allow to understand if an account is verified or not. every account must receive at most one token each, after opt-in for it. the opt-in procedure required to compile the know your customer form.\nwe think that, especially when it comes to managing the \"public thing\", transparency is fundamental. through the algo explorer platform it is possible to view all the transactions that take place on the blockchain, verifying who owns a specific token or not.\nsigillum's platform provide three main sections:\nuser section;\nadmin panel;\ndecentralised demo marketplace.\nuser section allows users to compile know your customer form and opt-in for our token. admin panel allows an regulated entity to whitelist a specific user sending him verified token, using the wallet address of the user. in addition, admin can see in a table the following information:\nwho send all information or not and data that are loaded successfully or not;\nwho has (or not) received the token, yet;\ndemo marketplace is an exaple of decentralised platform that required our tokens to login. in this way, only users that have been completed successfully kyc and opt-in procedure could login to defi platforms (will be explain better in the following sections).\nhow we built it\naccording to our inspirations and user experience, we build our project following the 10 usability heuristics for user interface design by nielsen and molich. we made our platform as simple as possible in such a way regulated entities and users could trust us.\nusing django framework we have developed back-end and a database to store information about the users who made kyc procedure and algorand\u2019s wallets addresses. data are docupled using hash functions, so it won\u2019t be possible to retrieve fiscal code knowing the wallet address or vice versa. nobody can know what is the relation between fiscal code and wallet addresses, ensuring the protection of sensitive data (according to gdpr).\nusing python, we have developed the logic of back-end who will comunicate with algorand's blockchain because of user-admin actions. we designed a connect algorand's wallet feature, in this way user could use our services using, for example, algorand wallet mobile app or myalgo browser plugin. login and use of our logic will be fast and secure.\nchallenges we ran into\nwe ran into many difficult challenges such as:\nalgorand wallet connecting features. because of it using react framwork (we didn't know it) it was difficult to integrate this function. we tried to workaround using pipeline ui or algopay but there was little time to study their documentation and, at the same time, design our project, developing front-end and back-end logic. anyways, we do of our best.\ntime management. because of our university's engagements we have to organize our days to maximise our objective function: continue studying by following our university's lectures and make the best project that we could have done for this hackathon.\nprogramming languages. we didn't realize a lot of projects in our career and our knowledge is limited. we use this opportunity to improve our skills.\nfinally, we come across many difficult challenges such as design choices and bugs, but we tried to realize something we belived in, useful and simple.\naccomplishments that we're proud of\nwe are proud that we have realised something that works and could be use from users and regulated entities beacause of its simplicity. we are proud of solve a problem with an innovative, secure, trasparent solution that is really operable in everyday life. finally, we are proud of done everithing and every key concept/idea that we have designed in the initial part of the project without omitting user experience and feasibility.\nwhat we learned\nthis hackathon was a good opportunity to learn team work and programming skills. to coordinate our work, we use trello, and we have conciliate ideas of team's members. this type of challenges allow us to improve our friendship because we have to work together to the solution to specific problem. we are really happy of that. we improve our skills and knowledge of python, django, postgresql, html, css, javascript, bootstrap, time management and, obviously, we learn more about algorand and its potential.\nwhat's next for sigillum\nimprove the algorand wallect feature. implement algorithms that detect incorrect behavior. improvements of know your customer procedure according to national legislation implementing the european directive such as the verification of:\nidentification data;\nemail addres;\nphone number;;\ngeolocation;\ncard identity;\nphoto in real time;", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59508196}, {"Unnamed: 0": 8210, "autor": "solana-nft-explorer", "date": null, "content": "Inspiration\nThere is no one and easy way to search and explore all the NFTs ever created on Solana.\nWhat it does\nYou can search and filter all Solana NFTs.\nHow we built it\nThe Solana NFT explorer was built using my Solana Go SDK and my Go Metaplex client.\nChallenges we ran into\nIt was all actually a smooth process.\nAccomplishments that we're proud of\nSearches are blazing fast.\nWhat we learned\nSearch is fun.\nWhat's next for solana-nft-explorer\nAfter the NFTs are found, the tool could allow to do things like make an offer to the current owner, subscribe to listing, and view ownership history.", "link": "https://devpost.com/software/solana-nft-explorer", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nthere is no one and easy way to search and explore all the nfts ever created on solana.\nwhat it does\nyou can search and filter all solana nfts.\nhow we built it\nthe solana nft explorer was built using my solana go sdk and my go metaplex client.\nchallenges we ran into\nit was all actually a smooth process.\naccomplishments that we're proud of\nsearches are blazing fast.\nwhat we learned\nsearch is fun.\nwhat's next for solana-nft-explorer\nafter the nfts are found, the -----> tool !!!  could allow to do things like make an offer to the current owner, subscribe to listing, and view ownership history.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59508210}, {"Unnamed: 0": 8212, "autor": "MetroEZ", "date": null, "content": "Inspiration\nImagine it\u2019s a rainy day, you rush to the subway station to get the next express train to school. You run down the steps of the station and notice that the steps are very slippery and are starting to flood... As you wait for the train you hear a commotion. Someone seemed to have fallen down the stairs\u2026The train screeches into the station. You get jostled by all the people pushing by you. You try to push through the crowd to get onto the train, but the doors close before you can get on.\nNow imagine it\u2019s that same rainy day. You rush to the subway station to get the next express train to school. What's different? You check the Metro EZ app on your phone before walking down into the subway station. On the app, you see that the next train is relatively empty, so there\u2019s a good chance you can get a seat. You run down the steps of the station and notice that the steps are very slippery and are starting to flood. Before swiping your MetroCard you snap a picture of the flooded steps and post it onto Metro EZ. Your picture is viewed by an elderly app user. He sees the slippery steps and decides to go to another station that has an elevator. With Metro EZ, you\u2019ll get real-time information on station traffic and conditions, taking the guesswork out of commuting.\nWhat it does\nOur mobile and web app helps New Yorkers and tourists effectively navigate the subway system with our easy-to-use interface. By downloading our app or using our website, the user can check a regularly updated heat map of the New York City subway system to see live location updates of how crowded a particular train is, the condition of the station and train, and if there is an issue with a train or station. As the MTA has limited resources, they will be able to use the data that MetroEZ collects to determine which stations have the most foot-traffic, and invest in maintenance and accessibility in the stations that need it the most. Our app will also allow users to upload pictures or leave comments on the condition of the train or station. This feed will make it easier for the MTA to get information in real-time of which stations have problems that will impede accessibility, such as flooding, debris, and broken elevators. We designed this app with New York City in mind, but it can be easily expanded to any public transportation system.\nHow we built it\nWe outlined the front-end of the project by building the basic user interface for the MetroEZ tool. From there, the front-end was created in React, Javascript, HTML/CSS. The backend was created using Python, Firebase, and Node.js.\nWe created a database using Firestore/Firebase to capture user inputted information about station conditions.\nFor the backend we decided what we wanted to work on - using bluetooth low energy to estimate the number of devices (and people) in an area. We had to do a lot of research to determine which programming language would be the best suited to capturing this data. We decided that node.js was the most efficient way, and researched other code that used similar implementations.\nChallenges we ran into\nIt was difficult for us to find a way to sync together the front end and back end. None of us have had to create a webpage or app that uses both front end and back end code. We decided to work with Firebase, which was also new to all of us. We also had to work with programming languages that we had not used before, or were not comfortable with.\nAccomplishments that we're proud of\nThis was the first hackathon for many of our teammates and the project was built with new tools and languages we did not have prior experience in. Despite the steep learning curve, we were able to create the launching point for a robust application. Even though we didn\u2019t know how to use Firebase, or certain programming languages that were necessary to create a functioning project, we were able to pull together and learn the basics about the new tools that were necessary for our ambitious project.\nWhat we learned\nWe learned just how interconnected everything is. Through research, we discovered how to use Bluetooth enabled devices to count and estimate the number of people in a given area. We also learned how APIs are necessary to integrate the front end and back end.\nWhat's next for MetroEZ\nWe definitely would like to continue working on our app, by adding new features that we weren\u2019t able to in this limited amount of time. For example adding a login for users and a picture upload function. We would also like to improve the MetroEZ\u2019s security and monitoring capabilities.", "link": "https://devpost.com/software/metro-infrastructure", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nimagine it\u2019s a rainy day, you rush to the subway station to get the next express train to school. you run down the steps of the station and notice that the steps are very slippery and are starting to flood... as you wait for the train you hear a commotion. someone seemed to have fallen down the stairs\u2026the train screeches into the station. you get jostled by all the people pushing by you. you try to push through the crowd to get onto the train, but the doors close before you can get on.\nnow imagine it\u2019s that same rainy day. you rush to the subway station to get the next express train to school. what's different? you check the metro ez app on your phone before walking down into the subway station. on the app, you see that the next train is relatively empty, so there\u2019s a good chance you can get a seat. you run down the steps of the station and notice that the steps are very slippery and are starting to flood. before swiping your metrocard you snap a picture of the flooded steps and post it onto metro ez. your picture is viewed by an elderly app user. he sees the slippery steps and decides to go to another station that has an elevator. with metro ez, you\u2019ll get real-time information on station traffic and conditions, taking the guesswork out of commuting.\nwhat it does\nour mobile and web app helps new yorkers and tourists effectively navigate the subway system with our easy-to-use interface. by downloading our app or using our website, the user can check a regularly updated heat map of the new york city subway system to see live location updates of how crowded a particular train is, the condition of the station and train, and if there is an issue with a train or station. as the mta has limited resources, they will be able to use the data that metroez collects to determine which stations have the most foot-traffic, and invest in maintenance and accessibility in the stations that need it the most. our app will also allow users to upload pictures or leave comments on the condition of the train or station. this feed will make it easier for the mta to get information in real-time of which stations have problems that will impede accessibility, such as flooding, debris, and broken elevators. we designed this app with new york city in mind, but it can be easily expanded to any public transportation system.\nhow we built it\nwe outlined the front-end of the project by building the basic user interface for the metroez -----> tool !!! . from there, the front-end was created in react, javascript, html/css. the backend was created using python, firebase, and node.js.\nwe created a database using firestore/firebase to capture user inputted information about station conditions.\nfor the backend we decided what we wanted to work on - using bluetooth low energy to estimate the number of devices (and people) in an area. we had to do a lot of research to determine which programming language would be the best suited to capturing this data. we decided that node.js was the most efficient way, and researched other code that used similar implementations.\nchallenges we ran into\nit was difficult for us to find a way to sync together the front end and back end. none of us have had to create a webpage or app that uses both front end and back end code. we decided to work with firebase, which was also new to all of us. we also had to work with programming languages that we had not used before, or were not comfortable with.\naccomplishments that we're proud of\nthis was the first hackathon for many of our teammates and the project was built with new tools and languages we did not have prior experience in. despite the steep learning curve, we were able to create the launching point for a robust application. even though we didn\u2019t know how to use firebase, or certain programming languages that were necessary to create a functioning project, we were able to pull together and learn the basics about the new tools that were necessary for our ambitious project.\nwhat we learned\nwe learned just how interconnected everything is. through research, we discovered how to use bluetooth enabled devices to count and estimate the number of people in a given area. we also learned how apis are necessary to integrate the front end and back end.\nwhat's next for metroez\nwe definitely would like to continue working on our app, by adding new features that we weren\u2019t able to in this limited amount of time. for example adding a login for users and a picture upload function. we would also like to improve the metroez\u2019s security and monitoring capabilities.", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59508212}, {"Unnamed: 0": 8266, "autor": "flowboat - A Node Based Machine Learning App", "date": null, "content": "Inspiration\nWe were inspired by learning visualization tools related to ML and wanted to create our own.\nWhat it does\nFlowboat is a web application that allows anyone to experience Machine Learning algorithms without having to code! Using flowboat, users can drag-and-drop input data, functions, and visualization outputs to see what ML is really like beyond lines of code.\nFrom the node pocket, users can choose from an abundance of nodes to add to their flow. Nodes interact through their connectors. As you add more nodes, you can see how your initial data transforms through your flow, into something quite beautiful.\nHow we built it\nWe use React.js as a Front-end framework and built our editor on the top of React Flow (https://reactflow.dev/). We implemented material components from the MUI React library. As a front-end development tool we used Storybook, so that we can easily preview each of our components. Python and Flask as the Back-end was utilized for building machine learning models and transferring data between nodes.\nChallenges we ran into\nWe had difficulty linking the apps backend and were unable to implement all the features we planned at the start of the hackathon but we are still proud of what we accomplished.\nAccomplishments that we're proud of\nWe created a intuitive and easy to understand UI that allows people with no machine learning background to understand how linear regression models work. The app also allows the users to play around with their own datasets and gives them creative freedom.\nWhat we learned\nWe worked with many new libraries during this project and even used frameworks we haven't used before like Flask. We gained valuable experience working the MUI React library to create clean front-end components.\nWhats next for flowboat\nWe wish to complete the features we have yet to do (such as the share feature) and perhaps add more types of machine learning models in the future. Implementing hotkeys to add nodes would also create a more efficient user experience.", "link": "https://devpost.com/software/flowboat-a-node-based-machine-learning-app", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe were inspired by learning visualization tools related to ml and wanted to create our own.\nwhat it does\nflowboat is a web application that allows anyone to experience machine learning algorithms without having to code! using flowboat, users can drag-and-drop input data, functions, and visualization outputs to see what ml is really like beyond lines of code.\nfrom the node pocket, users can choose from an abundance of nodes to add to their flow. nodes interact through their connectors. as you add more nodes, you can see how your initial data transforms through your flow, into something quite beautiful.\nhow we built it\nwe use react.js as a front-end framework and built our editor on the top of react flow (https://reactflow.dev/). we implemented material components from the mui react library. as a front-end development -----> tool !!!  we used storybook, so that we can easily preview each of our components. python and flask as the back-end was utilized for building machine learning models and transferring data between nodes.\nchallenges we ran into\nwe had difficulty linking the apps backend and were unable to implement all the features we planned at the start of the hackathon but we are still proud of what we accomplished.\naccomplishments that we're proud of\nwe created a intuitive and easy to understand ui that allows people with no machine learning background to understand how linear regression models work. the app also allows the users to play around with their own datasets and gives them creative freedom.\nwhat we learned\nwe worked with many new libraries during this project and even used frameworks we haven't used before like flask. we gained valuable experience working the mui react library to create clean front-end components.\nwhats next for flowboat\nwe wish to complete the features we have yet to do (such as the share feature) and perhaps add more types of machine learning models in the future. implementing hotkeys to add nodes would also create a more efficient user experience.", "sortedWord": "None", "removed": "Nan", "score": 15, "comments": 0, "media": null, "medialink": null, "identifyer": 59508266}, {"Unnamed: 0": 8301, "autor": "Tiles@Maphub", "date": null, "content": "Inspiration\nAs you may know, in the US right now (Oct 2021) there is ongoing debate about investment in public infrastructure. It's important to acknowledge that the state of the nation's infrastructure does not effect everyone uniformly. Individuals who are dependent on public transit shoulder the brunt of this burden, and the shortcomings in public transit are particularly devastating to individuals who have mobility, visual, or auditory disabilities. In short, planning trips can become a nightmare if one cannot get access to information about the public services that they need to get from point A to point B.\nAs a data engineer I often find myself working with a wide variety of geospatial sources from the very simple (e.g. points, polygons, roads) to the complex (e.g. LIDAR, raster, GTFS). Few of them unify different use-cases in the way that Open Street Map (OSM) does. Earlier this month I found myself looking through OSM and noticed a particular subway station that had an amazing amount of information associated with it (internet access, seating descriptions, notes on elevator repair status, flights of stairs, etc.).\nWhen I cross referenced this particular station with Google, Apple, and the local transit authority's data, I found that not only was OSM in agreement with these major sources on basic information, it surpassed them on station metadata.\nBecause OSM is crowdsourced, it offers data from thousands of individuals who've elected to contribute data to help others understand physical spaces without being present there themselves.\nFor the Estee Lauder Hack4llY Hackathon, I undertook this project as an opportunity to build a service on top of OSM data that could allow individuals to derive insights into the accessibility of their public spaces and transit systems.\nFor disability and transit advocates, urban planners, policymakers, etc., this is a tool that can be used as a starting point to develop targeted advocacy plans on infrastructure. Map layers are shareable via link, example - NYC MTA B68 - Shelter Status of Stops for easier communication between parties.\nFor all users, this can be a tool that helps with understanding the state of their local transit system and planning trips. Once users get the hang of how to make custom queries (which I admit, can take some time!) they can craft targeted requests that can help with their own trip planning or targeted advocacy efforts. Finally, Tiles has international coverage that goes beyond just transit, it considers all elements of the built world, this can include the physical state of roads and sidewalks, businesses, and (more generally) any building's accessibility characteristics\nWhat it does\nThe project's backend services allow users to query through ~500GB of OpenStreetMap (OSM) data and request custom vector layers from this data (e.g. \"All stations with an elevator AND underground\"). For this event, I am only able to host North and South America, but the architectural principles will hold on a larger database (the OSM world extract is ~1.3-1.4TB at time of writing)\nThe webpage offers a convenient graphic interface to explore and visualize custom layers. A user guide is available here as well.\nHow I built it\nThe means by which the application does all this is called a dynamic tileserver. As the user pans an area, the area in view is requested from a backend service. This service fetches nodes from our OSM replica DB that are in view, packages them into a tile, and then returns them to be rendered in the browser.\nI launched an ECS cluster with my API, a cache for recently seen tiles, as well as some monitoring sidecar containers (xray, cw-log-agent). A tileserver works just like any other API, but instead of a (more standard) json response, we return a binary payload. The webpage's client library understands this payload as a map tile and can render at the correct portion of the screen.\nBecause the DB does a lot of the service's heavy lifting, the most important infrastructure and provisioning decisions were DB related, right now I am running the PostGIS DB on a m6.large EC2 instance, but for stability's sake, may migrate to RDS when I'm able to polish the application.\nThe full architecture of the project is available here\nChallenges I ran into\nFor this event I wanted to challenge myself to develop a webpage that (at least) met Mozilla's accessibility guidelines. This meant solving problems that I've never had to solve as a developer before re: accessibility on the web such as:\nDescriptive alt text values assigned to all images\nMaking all text zoomable up to 200%\nMaking the default color palette functional for individuals with red, green, blue, color, or contrast loss\nEnsuring the website is navigable by keyboard\nRendering in browser. Depending on the browser used, request ordering/rendering behaves variably. Requests to the baselayer (a 3rd party static tileserver, Carto Free Basemaps) take precedence over my requests which leads to a less than ideal user experience.\nAccomplishments that I'm proud of\nSee Challenges point 1\nI wanted to make a webpage that respected users' data constraints (e.g. a webpage that automatically loads 30MB of content can be quite annoying). Map tiles can be large, but the addition of Nginx allowed me to compress the data that gets sent over to the user by a factor of 2-3x.\nWhat I learned\nQuite a bit about frontend development, webpage accessibility standards, and UX\nA good bit about networking tasks on ECS\nWhat's next for Tiles@Maphub\nThere are a lot of gaps that need to be plugged to make this an MVP.\nThere are infrastructure and system design considerations (DB tuning, request sharding, caching to disk via Nginx vs Redis to save cluster memory, tidying the core application to use a proper config!), but the most significant will be in UX and user testing.\nEven as the developer of this app, It's clear to me the JSON query syntax is quite confusing, a much cleaner user interface for layer creation is possible. Before releasing this as a product I'd want to turn that into a drag and drop interface.\nThe application has been tested on Mozilla Firefox >=v92.0, Chrome v94, and Safari >13.1.x, other browsers may not display w/ live map interaction properly, for an MVP, I would want compatibility with all common browsers", "link": "https://devpost.com/software/tiles-maphub", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nas you may know, in the us right now (oct 2021) there is ongoing debate about investment in public infrastructure. it's important to acknowledge that the state of the nation's infrastructure does not effect everyone uniformly. individuals who are dependent on public transit shoulder the brunt of this burden, and the shortcomings in public transit are particularly devastating to individuals who have mobility, visual, or auditory disabilities. in short, planning trips can become a nightmare if one cannot get access to information about the public services that they need to get from point a to point b.\nas a data engineer i often find myself working with a wide variety of geospatial sources from the very simple (e.g. points, polygons, roads) to the complex (e.g. lidar, raster, gtfs). few of them unify different use-cases in the way that open street map (osm) does. earlier this month i found myself looking through osm and noticed a particular subway station that had an amazing amount of information associated with it (internet access, seating descriptions, notes on elevator repair status, flights of stairs, etc.).\nwhen i cross referenced this particular station with google, apple, and the local transit authority's data, i found that not only was osm in agreement with these major sources on basic information, it surpassed them on station metadata.\nbecause osm is crowdsourced, it offers data from thousands of individuals who've elected to contribute data to help others understand physical spaces without being present there themselves.\nfor the estee lauder hack4lly hackathon, i undertook this project as an opportunity to build a service on top of osm data that could allow individuals to derive insights into the accessibility of their public spaces and transit systems.\nfor disability and transit advocates, urban planners, policymakers, etc., this is a -----> tool !!!  that can be used as a starting point to develop targeted advocacy plans on infrastructure. map layers are shareable via link, example - nyc mta b68 - shelter status of stops for easier communication between parties.\nfor all users, this can be a tool that helps with understanding the state of their local transit system and planning trips. once users get the hang of how to make custom queries (which i admit, can take some time!) they can craft targeted requests that can help with their own trip planning or targeted advocacy efforts. finally, tiles has international coverage that goes beyond just transit, it considers all elements of the built world, this can include the physical state of roads and sidewalks, businesses, and (more generally) any building's accessibility characteristics\nwhat it does\nthe project's backend services allow users to query through ~500gb of openstreetmap (osm) data and request custom vector layers from this data (e.g. \"all stations with an elevator and underground\"). for this event, i am only able to host north and south america, but the architectural principles will hold on a larger database (the osm world extract is ~1.3-1.4tb at time of writing)\nthe webpage offers a convenient graphic interface to explore and visualize custom layers. a user guide is available here as well.\nhow i built it\nthe means by which the application does all this is called a dynamic tileserver. as the user pans an area, the area in view is requested from a backend service. this service fetches nodes from our osm replica db that are in view, packages them into a tile, and then returns them to be rendered in the browser.\ni launched an ecs cluster with my api, a cache for recently seen tiles, as well as some monitoring sidecar containers (xray, cw-log-agent). a tileserver works just like any other api, but instead of a (more standard) json response, we return a binary payload. the webpage's client library understands this payload as a map tile and can render at the correct portion of the screen.\nbecause the db does a lot of the service's heavy lifting, the most important infrastructure and provisioning decisions were db related, right now i am running the postgis db on a m6.large ec2 instance, but for stability's sake, may migrate to rds when i'm able to polish the application.\nthe full architecture of the project is available here\nchallenges i ran into\nfor this event i wanted to challenge myself to develop a webpage that (at least) met mozilla's accessibility guidelines. this meant solving problems that i've never had to solve as a developer before re: accessibility on the web such as:\ndescriptive alt text values assigned to all images\nmaking all text zoomable up to 200%\nmaking the default color palette functional for individuals with red, green, blue, color, or contrast loss\nensuring the website is navigable by keyboard\nrendering in browser. depending on the browser used, request ordering/rendering behaves variably. requests to the baselayer (a 3rd party static tileserver, carto free basemaps) take precedence over my requests which leads to a less than ideal user experience.\naccomplishments that i'm proud of\nsee challenges point 1\ni wanted to make a webpage that respected users' data constraints (e.g. a webpage that automatically loads 30mb of content can be quite annoying). map tiles can be large, but the addition of nginx allowed me to compress the data that gets sent over to the user by a factor of 2-3x.\nwhat i learned\nquite a bit about frontend development, webpage accessibility standards, and ux\na good bit about networking tasks on ecs\nwhat's next for tiles@maphub\nthere are a lot of gaps that need to be plugged to make this an mvp.\nthere are infrastructure and system design considerations (db tuning, request sharding, caching to disk via nginx vs redis to save cluster memory, tidying the core application to use a proper config!), but the most significant will be in ux and user testing.\neven as the developer of this app, it's clear to me the json query syntax is quite confusing, a much cleaner user interface for layer creation is possible. before releasing this as a product i'd want to turn that into a drag and drop interface.\nthe application has been tested on mozilla firefox >=v92.0, chrome v94, and safari >13.1.x, other browsers may not display w/ live map interaction properly, for an mvp, i would want compatibility with all common browsers", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59508301}, {"Unnamed: 0": 8334, "autor": "Twitter Headers Creator", "date": null, "content": "Inspiration\nLast week I bought some Solana NFTs. Changed my profile picture, but didn't know what to put on the header. After doing a research, didn't found any header from the projects I bought the NFTs. Then a figured out that this huge space on the Twitter header was unexploited by most of Solana communities. So, I decided to build a easy-to-use tool for the whole Solana community.\nWhat it does\nIt generate a header image for Twitter based on NFTs you want. All you need is write the IDs of your NFTs and choose the collection. Then you can add some extra elements like monkeys and skeleton. Easy. All running client side.\nHow we built it\nInitially, I thought that this would be easy, but then figured out it's not. I needed to pull all data from blockchain to create the database of IDs (names) of images. Then pull metadata from arweave. Also found a lot of fake NFTs that I needed to clean from DB. After that deployed an API to do reverse search (Collection + ID to find the corresponding Token Public Key).\nFor frontend I built a easy to use website with some header templates and add-ons. Linked with the API and a CDN (to host NFT thumbs). The major bottleneck here was designing the UI and dealing with CORS authorization when showing images, that's why I had to setup a CDN.\nChallenges we ran into\nI learnt how to identify fake NFTs. Cleaning Missed/wrong metadata.\nAccomplishments that we're proud of\nCreate a useful tool for the Solana NFT community. I received some interesting feedback and I already have a list of interesting features for the next version. Some of them are: Sync the header with a wallet, so when any new NFT arrives, it will automatically update the header. Link with Bonfida Name service to create a vanity URL and show a custom gallery, etc.\nWhat we learned\nSolana tech is awesome. The community is growing faster.\nWhat's next for Twitter Headers Creator\nWe launched a beta version, built in 10 days, from zero. The next step is to allow anyone put their wallet address, pull nfts, and create a Header with their nfts. Also, allow NFT projects build custom templates for their projects.", "link": "https://devpost.com/software/twitter-headers-creator", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nlast week i bought some solana nfts. changed my profile picture, but didn't know what to put on the header. after doing a research, didn't found any header from the projects i bought the nfts. then a figured out that this huge space on the twitter header was unexploited by most of solana communities. so, i decided to build a easy-to-use -----> tool !!!  for the whole solana community.\nwhat it does\nit generate a header image for twitter based on nfts you want. all you need is write the ids of your nfts and choose the collection. then you can add some extra elements like monkeys and skeleton. easy. all running client side.\nhow we built it\ninitially, i thought that this would be easy, but then figured out it's not. i needed to pull all data from blockchain to create the database of ids (names) of images. then pull metadata from arweave. also found a lot of fake nfts that i needed to clean from db. after that deployed an api to do reverse search (collection + id to find the corresponding token public key).\nfor frontend i built a easy to use website with some header templates and add-ons. linked with the api and a cdn (to host nft thumbs). the major bottleneck here was designing the ui and dealing with cors authorization when showing images, that's why i had to setup a cdn.\nchallenges we ran into\ni learnt how to identify fake nfts. cleaning missed/wrong metadata.\naccomplishments that we're proud of\ncreate a useful tool for the solana nft community. i received some interesting feedback and i already have a list of interesting features for the next version. some of them are: sync the header with a wallet, so when any new nft arrives, it will automatically update the header. link with bonfida name service to create a vanity url and show a custom gallery, etc.\nwhat we learned\nsolana tech is awesome. the community is growing faster.\nwhat's next for twitter headers creator\nwe launched a beta version, built in 10 days, from zero. the next step is to allow anyone put their wallet address, pull nfts, and create a header with their nfts. also, allow nft projects build custom templates for their projects.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59508334}, {"Unnamed: 0": 8372, "autor": "Iris", "date": null, "content": "Inspiration\nAfter reading Vogue's Business 'Beauty Weak Spot: People with disabilities' and Allure's 'Blindness & Beauty, we realized that there is such a need to create an inclusive and accessible environment for all. Just thinking about how we struggle to find the right shade products online, we all realized that this challenge is magnified tenfold for those that are visually impaired. Iris levels the playing field and allows all shoppers regardless of their physical impairments to get access to a personalized beautician in the palm of their hand.\nWhat it does\nFirst we created a fully accessible website for Iris, with React, html, and css interacting with a django backend python server via REST API endpoints. This website includes alt text, a contrast ratio above 7:1, easy to read font, and no pop ups to confuse a screen reader.\nYou can start your journey here by finding makeup that fits your skin tone. If you have any trouble identifying what makeup fits you, Iris has a face recognition computer vision model which interfaces with a color segmentation model to identify your skin tone and hair color. After collecting this information, Iris gives you a full list of makeup recommendations that personally fit you and then you can simply add the products to your cart and check out.\nAfter you order your products, you can use Iris every morning to get ready. You can either view saved products like shown below or simply scan a product. Iris uses a deep convolutional neural network to scan and identify products that you have in your hand. This comes in handy for bottles that feel similar, trying to see different shades of lipstick, or differentiating between highlighter, blush, and bronzer containers. After Iris identifies your product, she reads out what the product is used for, how to apply it, and even adds in a few fun makeup tricks related to that product.\nFor anyone who would rather talk through the process instead of navigating the webpage, all of these functionalities can be completed through Iris\u2019s AI text to speech chatbot.\nHow we built it\nFirst we created a fully accessible website for Iris, with React, html, and css interacting with a django backend python server via REST API endpoints. We also built a face recognition computer vision model which interfaces with a color segmentation model to identify your skin tone and hair color and\nChallenges we ran into\nWe ran into issues fine tuning the skin tone model and having to gather various points of data. We also struggled with getting the AI chatbot to be responsive to various command and then be able to convert text to speech. We also struggled deploying into Azure for the first time.\nAccomplishments that we're proud of\nWe were able to develop a product that fully transforms the at home beauty experience for not just the disabled community, but for everyone. We were able to think in a new way to create a dully accessible environment. We were able to integrate various technologies and also a variety of skill sets across the team.\nWhat we learned\nWe learned that accessible engineering should be prioritized and the center of a design rather than a later thought. We learned that creating a fully accessible experience removes pain points for everyone and can create a better and more convenient experience for all and a deep convolutional neural network to scan and identify products that you have in your hand. Lastly, we developed an AI chatbot to be able to guide everyone through a personalized experience.\nWhat's next for Iris\nWe hope to improve model accuracy for both scanning skin tone and products under various conditions and be able to interface this as a tool within the Estee Lauder sales website to be able to closely intertwine the experience and enhance personalization.", "link": "https://devpost.com/software/iris-lkwm2c", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nafter reading vogue's business 'beauty weak spot: people with disabilities' and allure's 'blindness & beauty, we realized that there is such a need to create an inclusive and accessible environment for all. just thinking about how we struggle to find the right shade products online, we all realized that this challenge is magnified tenfold for those that are visually impaired. iris levels the playing field and allows all shoppers regardless of their physical impairments to get access to a personalized beautician in the palm of their hand.\nwhat it does\nfirst we created a fully accessible website for iris, with react, html, and css interacting with a django backend python server via rest api endpoints. this website includes alt text, a contrast ratio above 7:1, easy to read font, and no pop ups to confuse a screen reader.\nyou can start your journey here by finding makeup that fits your skin tone. if you have any trouble identifying what makeup fits you, iris has a face recognition computer vision model which interfaces with a color segmentation model to identify your skin tone and hair color. after collecting this information, iris gives you a full list of makeup recommendations that personally fit you and then you can simply add the products to your cart and check out.\nafter you order your products, you can use iris every morning to get ready. you can either view saved products like shown below or simply scan a product. iris uses a deep convolutional neural network to scan and identify products that you have in your hand. this comes in handy for bottles that feel similar, trying to see different shades of lipstick, or differentiating between highlighter, blush, and bronzer containers. after iris identifies your product, she reads out what the product is used for, how to apply it, and even adds in a few fun makeup tricks related to that product.\nfor anyone who would rather talk through the process instead of navigating the webpage, all of these functionalities can be completed through iris\u2019s ai text to speech chatbot.\nhow we built it\nfirst we created a fully accessible website for iris, with react, html, and css interacting with a django backend python server via rest api endpoints. we also built a face recognition computer vision model which interfaces with a color segmentation model to identify your skin tone and hair color and\nchallenges we ran into\nwe ran into issues fine tuning the skin tone model and having to gather various points of data. we also struggled with getting the ai chatbot to be responsive to various command and then be able to convert text to speech. we also struggled deploying into azure for the first time.\naccomplishments that we're proud of\nwe were able to develop a product that fully transforms the at home beauty experience for not just the disabled community, but for everyone. we were able to think in a new way to create a dully accessible environment. we were able to integrate various technologies and also a variety of skill sets across the team.\nwhat we learned\nwe learned that accessible engineering should be prioritized and the center of a design rather than a later thought. we learned that creating a fully accessible experience removes pain points for everyone and can create a better and more convenient experience for all and a deep convolutional neural network to scan and identify products that you have in your hand. lastly, we developed an ai chatbot to be able to guide everyone through a personalized experience.\nwhat's next for iris\nwe hope to improve model accuracy for both scanning skin tone and products under various conditions and be able to interface this as a -----> tool !!!  within the estee lauder sales website to be able to closely intertwine the experience and enhance personalization.", "sortedWord": "None", "removed": "Nan", "score": 8, "comments": 0, "media": null, "medialink": null, "identifyer": 59508372}, {"Unnamed: 0": 8484, "autor": "Art", "date": null, "content": "Generative art refers to art that in whole or in part has been created with the use of an autonomous system. An autonomous system in this context is generally one that is non-human and can independently determine features of an artwork that would otherwise require decisions made directly by the artist. In some cases the human creator may claim that the generative system represents their own artistic idea, and in others that the system takes on the role of the creator.\n\"Generative art\" often refers to algorithmic art (algorithmically determined computer generated artwork) and synthetic media (general term for any algorithmically-generated media), but artists can also make it using systems of chemistry, biology, mechanics and robotics, smart materials, manual randomization, mathematics, data mapping, symmetry, tiling, and more.\nTypes\nMusic Johann Philipp Kirnberger's \"Musikalisches W\u00fcrfelspiel\" (Musical Dice Game) 1757 is considered an early example of a generative system based on randomness. Dice were used to select musical sequences from a numbered pool of previously composed phrases. This system provided a balance of order and disorder. The structure was based on an element of order on one hand, and disorder on the other.\nThe fugues of J.S. Bach could be considered generative, in that there is a strict underlying process that is followed by the composer. Similarly, serialism follows strict procedures which, in some cases, can be set up to generate entire compositions with limited human intervention.\nComposers such as John Cage,: 13\u201315 Farmers Manual, and Brian Eno: 133 have used generative systems in their works.\nVisual art The artist Ellsworth Kelly created paintings by using chance operations to assign colors in a grid. He also created works on paper that he then cut into strips or squares and reassembled using chance operations to determine placement.\nArtists such as Hans Haacke have explored processes of physical and social systems in artistic context. Fran\u00e7ois Morellet has used both highly ordered and highly disordered systems in his artwork. Some of his paintings feature regular systems of radial or parallel lines to create Moir\u00e9 Patterns. In other works he has used chance operations to determine the coloration of grids. Sol LeWitt created generative art in the form of systems expressed in natural language and systems of geometric permutation. Harold Cohen's AARON system is a longstanding project combining software artificial intelligence with robotic painting devices to create physical artifacts. Steina and Woody Vasulka are video art pioneers who used analog video feedback to create generative art. Video feedback is now cited as an example of deterministic chaos, and the early explorations by the Vasulkas anticipated contemporary science by many years. Software systems exploiting evolutionary computing to create visual form include those created by Scott Draves and Karl Sims. The digital artist Joseph Nechvatal has exploited models of viral contagion. Autopoiesis by Ken Rinaldo includes fifteen musical and robotic sculptures that interact with the public and modify their behaviors based on both the presence of the participants and each other.: 144\u2013145 Jean-Pierre Hebert and Roman Verostko are founding members of the Algorists, a group of artists who create their own algorithms to create art. A. Michael Noll, of Bell Telephone Laboratories, Incorporated, programmed computer art using mathematical equations and programmed randomness, starting in 1962. The French artist Jean-Max Albert, beside environmental sculptures like Iapetus, and O=C=O, developed a project dedicated to the vegetation itself, in terms of biological activity. The Calmoduline Monument project is based on the property of a protein, calmodulin, to bond selectively to calcium. Exterior physical constraints (wind, rain, etc.) modify the electric potential of the cellular membranes of a plant and consequently the flux of calcium. However, the calcium controls the expression of the calmoduline gene. The plant can thus, when there is a stimulus, modify its \u00ab typical \u00bb growth pattern. So the basic principle of this monumental sculpture is that to the extent that they could be picked up and transported, these signals could be enlarged, translated into colors and shapes, and show the plant's \u00ab decisions \u00bb suggesting a level of fundamental biological activity.\nMaurizio Bolognini works with generative machines to address conceptual and social concerns. Mark Napier is a pioneer in data mapping, creating works based on the streams of zeros and ones in ethernet traffic, as part of the \"Carnivore\" project. Martin Wattenberg pushed this theme further, transforming \"data sets\" as diverse as musical scores and Wikipedia edits into dramatic visual compositions. The Canadian artist San Base developed a \"Dynamic Painting\" algorithm in 2002. Using computer algorithms as \"brush strokes,\" Base creates sophisticated imagery that evolves over time to produce a fluid, never-repeating artwork.\nSince 1996 there have been ambigram generators that auto generate ambigrams.\nItalian composer Pietro Grossi, pioneer of computer music since 1986, he extended his experiments to images, (same procedure used in his musical work) precisely to computer graphics, writing programs with specific auto-decisions, and developing the concept of HomeArt, presented for the first time in the exhibition New Atlantis: the continent of electronic music organized by the Venice Biennale in 1986.\nSoftware art For some artists, graphic user interfaces and computer code have become an independent art form in themselves. Adrian Ward created Auto-Illustrator as a commentary on software and generative methods applied to art and design.\nArchitecture In 1987 Celestino Soddu created the artificial DNA of Italian Medieval towns able to generate endless 3D models of cities identifiable as belonging to the idea.\nIn 2010, Michael Hansmeyer generated architectural columns in a project called \"Subdivided Columns \u2013 A New Order (2010)\". The piece explored how the simple process of repeated subdivision can create elaborate architectural patterns. Rather than designing any columns directly, Hansmeyer designed a process that produced columns automatically. The process could be run again and again with different parameters to create endless permutations. Endless permutations could be considered a hallmark of generative design.\nLiterature Further information: Oulipo, The Eureka, Electronic literature, Spam Lit, Informationist poetry, Language game, and Prehistoric Digital Poetry Writers such as Tristan Tzara, Brion Gysin, and William Burroughs used the cut-up technique to introduce randomization to literature as a generative system. Jackson Mac Low produced computer-assisted poetry and used algorithms to generate texts; Philip M. Parker has written software to automatically generate entire books. Jason Nelson used generative methods with speech-to-text software to create a series of digital poems from movies, television and other audio sources.\nLive coding Generative systems may be modified while they operate, for example by using interactive programming environments such as SuperCollider, Fluxus and TidalCycles, including patching environments such as Max/MSP, Pure Data and vvvv. This is a standard approach to programming by artists, but may also be used to create live music and/or video by manipulating generative systems on stage, a performance practice that has become known as live coding. As with many examples of software art, because live coding emphasises human authorship rather than autonomy, it may be considered in opposition to generative art.\nHow to create a generative art\nThe tool can be found here: https://www.acrylicode.com/acrylicodes-drawing-tool/\nOnce you go there you should see this.\nNow you should be able to draw with your mouse by moving it while pressing inside the drawing area. Start with a simple shape (with very complex shapes the experience may be a little slow). Something like this:\nAfter you draw the shape you see that the \u201cHarmonize\u201d button is enabled and then you can click it to get into the harmonization part.\nOnce you click the \u201cHarmonize\u201d button you should see a couple of sliders on the Left. The most important slider is the \u201cNumber of Shapes\u201d, you should increment this first. After incrementing it, you should see something like this :\nAfter that, experiment with the other sliders to create different variations. If you want to delete everything and start over click on the \u201cDraw\u201d button. This will take you to the first step and then you can start again with your own custom shape. You can experiment with multiple small shapes to get different results. And once you are happy with the result, click on \u201cSave SVG\u201d to save the file. SVGs files are suitable for pen plotting. If you create something and want to publish it on Instagram, please tag us @acrylicode.berlin and share the link to the tool so others can also experiment themselves. I hope you enjoyed this short tutorial and hope you have a lot of fun with the tool.\nWe learned how to create art.", "link": "https://devpost.com/software/art-b7c6wf", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "generative art refers to art that in whole or in part has been created with the use of an autonomous system. an autonomous system in this context is generally one that is non-human and can independently determine features of an artwork that would otherwise require decisions made directly by the artist. in some cases the human creator may claim that the generative system represents their own artistic idea, and in others that the system takes on the role of the creator.\n\"generative art\" often refers to algorithmic art (algorithmically determined computer generated artwork) and synthetic media (general term for any algorithmically-generated media), but artists can also make it using systems of chemistry, biology, mechanics and robotics, smart materials, manual randomization, mathematics, data mapping, symmetry, tiling, and more.\ntypes\nmusic johann philipp kirnberger's \"musikalisches w\u00fcrfelspiel\" (musical dice game) 1757 is considered an early example of a generative system based on randomness. dice were used to select musical sequences from a numbered pool of previously composed phrases. this system provided a balance of order and disorder. the structure was based on an element of order on one hand, and disorder on the other.\nthe fugues of j.s. bach could be considered generative, in that there is a strict underlying process that is followed by the composer. similarly, serialism follows strict procedures which, in some cases, can be set up to generate entire compositions with limited human intervention.\ncomposers such as john cage,: 13\u201315 farmers manual, and brian eno: 133 have used generative systems in their works.\nvisual art the artist ellsworth kelly created paintings by using chance operations to assign colors in a grid. he also created works on paper that he then cut into strips or squares and reassembled using chance operations to determine placement.\nartists such as hans haacke have explored processes of physical and social systems in artistic context. fran\u00e7ois morellet has used both highly ordered and highly disordered systems in his artwork. some of his paintings feature regular systems of radial or parallel lines to create moir\u00e9 patterns. in other works he has used chance operations to determine the coloration of grids. sol lewitt created generative art in the form of systems expressed in natural language and systems of geometric permutation. harold cohen's aaron system is a longstanding project combining software artificial intelligence with robotic painting devices to create physical artifacts. steina and woody vasulka are video art pioneers who used analog video feedback to create generative art. video feedback is now cited as an example of deterministic chaos, and the early explorations by the vasulkas anticipated contemporary science by many years. software systems exploiting evolutionary computing to create visual form include those created by scott draves and karl sims. the digital artist joseph nechvatal has exploited models of viral contagion. autopoiesis by ken rinaldo includes fifteen musical and robotic sculptures that interact with the public and modify their behaviors based on both the presence of the participants and each other.: 144\u2013145 jean-pierre hebert and roman verostko are founding members of the algorists, a group of artists who create their own algorithms to create art. a. michael noll, of bell telephone laboratories, incorporated, programmed computer art using mathematical equations and programmed randomness, starting in 1962. the french artist jean-max albert, beside environmental sculptures like iapetus, and o=c=o, developed a project dedicated to the vegetation itself, in terms of biological activity. the calmoduline monument project is based on the property of a protein, calmodulin, to bond selectively to calcium. exterior physical constraints (wind, rain, etc.) modify the electric potential of the cellular membranes of a plant and consequently the flux of calcium. however, the calcium controls the expression of the calmoduline gene. the plant can thus, when there is a stimulus, modify its \u00ab typical \u00bb growth pattern. so the basic principle of this monumental sculpture is that to the extent that they could be picked up and transported, these signals could be enlarged, translated into colors and shapes, and show the plant's \u00ab decisions \u00bb suggesting a level of fundamental biological activity.\nmaurizio bolognini works with generative machines to address conceptual and social concerns. mark napier is a pioneer in data mapping, creating works based on the streams of zeros and ones in ethernet traffic, as part of the \"carnivore\" project. martin wattenberg pushed this theme further, transforming \"data sets\" as diverse as musical scores and wikipedia edits into dramatic visual compositions. the canadian artist san base developed a \"dynamic painting\" algorithm in 2002. using computer algorithms as \"brush strokes,\" base creates sophisticated imagery that evolves over time to produce a fluid, never-repeating artwork.\nsince 1996 there have been ambigram generators that auto generate ambigrams.\nitalian composer pietro grossi, pioneer of computer music since 1986, he extended his experiments to images, (same procedure used in his musical work) precisely to computer graphics, writing programs with specific auto-decisions, and developing the concept of homeart, presented for the first time in the exhibition new atlantis: the continent of electronic music organized by the venice biennale in 1986.\nsoftware art for some artists, graphic user interfaces and computer code have become an independent art form in themselves. adrian ward created auto-illustrator as a commentary on software and generative methods applied to art and design.\narchitecture in 1987 celestino soddu created the artificial dna of italian medieval towns able to generate endless 3d models of cities identifiable as belonging to the idea.\nin 2010, michael hansmeyer generated architectural columns in a project called \"subdivided columns \u2013 a new order (2010)\". the piece explored how the simple process of repeated subdivision can create elaborate architectural patterns. rather than designing any columns directly, hansmeyer designed a process that produced columns automatically. the process could be run again and again with different parameters to create endless permutations. endless permutations could be considered a hallmark of generative design.\nliterature further information: oulipo, the eureka, electronic literature, spam lit, informationist poetry, language game, and prehistoric digital poetry writers such as tristan tzara, brion gysin, and william burroughs used the cut-up technique to introduce randomization to literature as a generative system. jackson mac low produced computer-assisted poetry and used algorithms to generate texts; philip m. parker has written software to automatically generate entire books. jason nelson used generative methods with speech-to-text software to create a series of digital poems from movies, television and other audio sources.\nlive coding generative systems may be modified while they operate, for example by using interactive programming environments such as supercollider, fluxus and tidalcycles, including patching environments such as max/msp, pure data and vvvv. this is a standard approach to programming by artists, but may also be used to create live music and/or video by manipulating generative systems on stage, a performance practice that has become known as live coding. as with many examples of software art, because live coding emphasises human authorship rather than autonomy, it may be considered in opposition to generative art.\nhow to create a generative art\nthe -----> tool !!!  can be found here: https://www.acrylicode.com/acrylicodes-drawing-tool/\nonce you go there you should see this.\nnow you should be able to draw with your mouse by moving it while pressing inside the drawing area. start with a simple shape (with very complex shapes the experience may be a little slow). something like this:\nafter you draw the shape you see that the \u201charmonize\u201d button is enabled and then you can click it to get into the harmonization part.\nonce you click the \u201charmonize\u201d button you should see a couple of sliders on the left. the most important slider is the \u201cnumber of shapes\u201d, you should increment this first. after incrementing it, you should see something like this :\nafter that, experiment with the other sliders to create different variations. if you want to delete everything and start over click on the \u201cdraw\u201d button. this will take you to the first step and then you can start again with your own custom shape. you can experiment with multiple small shapes to get different results. and once you are happy with the result, click on \u201csave svg\u201d to save the file. svgs files are suitable for pen plotting. if you create something and want to publish it on instagram, please tag us @acrylicode.berlin and share the link to the tool so others can also experiment themselves. i hope you enjoyed this short tutorial and hope you have a lot of fun with the tool.\nwe learned how to create art.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59508484}, {"Unnamed: 0": 8529, "autor": "Fingame", "date": null, "content": "Fingame\nAn application to educate young adults about financial planning and investing\nInspiration\nThe idea came to my mind when I saw my friends and me having very low knowledge about financial investments in our life. I decided to build a web tool in a gamified way so that it looks interactive and one can learn things by doing it. Earlier the traditional way of understanding finance was reading big boring books which can be tackled via this tool.\nWhat it does\nI decided to create an interactive web app from which people can understand their finances and get to know about different investment options available to them. People can invest their money in a better and organized manner. To make the process more interesting I gamified the app.\nHow I built it\nI used the concept of WEBD and crawled some of the articles, research papers to conclude the idea of developing this tool. Further experiments and errors lead to me build this successful web tool. Exploring the data and stocks in the market with different pattern changes in the growth of the economy made me understood to create a loss/gain factor on different investments options.\nChallenges ran into\nA lot can be improved in the app. We can build a better frontend and responsive site.\nMany investing options are yet to be added.\nMaking the predictions using APIs or a Machine learning backend engine.\nAccomplishments that I am proud of\nWell, this app is one kind that helps people understand their finances in a fun manner. Earlier the traditional way of understanding finance was reading big boring books which everyone avoided doing. Making games from finance will also encourage the young to take it seriously and help them to make more effective decisions.\nWhat I learned\nA lot from different tech stacks to innovative and progressive skills. It was my pleasure to participate in this hackathon.", "link": "https://devpost.com/software/fingame", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "fingame\nan application to educate young adults about financial planning and investing\ninspiration\nthe idea came to my mind when i saw my friends and me having very low knowledge about financial investments in our life. i decided to build a web -----> tool !!!  in a gamified way so that it looks interactive and one can learn things by doing it. earlier the traditional way of understanding finance was reading big boring books which can be tackled via this tool.\nwhat it does\ni decided to create an interactive web app from which people can understand their finances and get to know about different investment options available to them. people can invest their money in a better and organized manner. to make the process more interesting i gamified the app.\nhow i built it\ni used the concept of webd and crawled some of the articles, research papers to conclude the idea of developing this tool. further experiments and errors lead to me build this successful web tool. exploring the data and stocks in the market with different pattern changes in the growth of the economy made me understood to create a loss/gain factor on different investments options.\nchallenges ran into\na lot can be improved in the app. we can build a better frontend and responsive site.\nmany investing options are yet to be added.\nmaking the predictions using apis or a machine learning backend engine.\naccomplishments that i am proud of\nwell, this app is one kind that helps people understand their finances in a fun manner. earlier the traditional way of understanding finance was reading big boring books which everyone avoided doing. making games from finance will also encourage the young to take it seriously and help them to make more effective decisions.\nwhat i learned\na lot from different tech stacks to innovative and progressive skills. it was my pleasure to participate in this hackathon.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59508529}, {"Unnamed: 0": 8582, "autor": "Bitslimes", "date": null, "content": "Inspiration\nA major inspiration for this project is old JRPGs from the mid 2000's. The slime is a classic monster from this style of game and manages to invoke strong nostalgia in those who are familiar with these games. I strongly believe that NFTs will be a major game changer for video games, and I want to experiment with this new approach to handling collectibles in video games.\nWhat it does\nPlayers can create a monster on the Bitslimes platform using their own NFT. They are then able to take that monster and assign it abilities based on its level. Once abilities are assigned, players can enter fights with other player's monsters. All combat will be resolved on chain.\nWe are looking to build a world inspired by NES/SNES era pixel art, the main characters of our world are our algorithmically generated Bitslimes. We are looking to give owners of our Bitslimes the ability to vote on future features and developments of our game.\nHow we built it\nThe game was designed in Rust using the Anchor framework. The game is also built to work with the Metaplex program ecosystem as well.\nThe frontend was created using React.js + solana/web3 and anchor libraries.\nThe art for our NFT's were not built with layers. Our hand drawn traits were converted into coordinates via a Python script for a paintbrush tool to draw on a blank canvas. We used this approach because we found it much faster during generation than a typical layer-based implementation.\nChallenges we ran into\nThere was a large skill curve to learning how to develop on Solana and learning how to use Rust. I found a lot of help in the discord servers dedicated to Anchor and Metaplex. Going from very little blockchain experience to being able to have a strong understanding of the technology and being able to build a unique product was a great learning experience.\nLearning how to generate the art was difficult, as previously I had very little art experience; but it was a great experience and ended up enjoying it quite a bit.\nBuilding a community for the first time was also a new experience that I wasn't used to; I made a lot of new friends and was glad to have the opportunity to get involved in the Solana + NFT community.\nAccomplishments that we're proud of\nGoing from zero Solana, rust, and art experience to having a full product built during the span of the hackathon.\nWhat we learned\nHow to generate art procedurally, how to use IPFS and Arweave, how to market effectively, the power of a strong community, how to code in Rust, becoming comfortable with bytecode, how to use VPS services, how to calculate rarities for NFTs\nWhat's next for Bitslimes\nI would love to continue working on this project! I would like to add much deeper mechanics to the combat, as it is very simple in its current implementation due to time constraints.\nI would love to implement a DAO mechanic where owners of the NFTs are able to vote on what game features/mechanics they want implemented.\nBetting mechanics, where two players can make bets on the outcome of their battle.\nEvolution, I want to implement a feature where users can evolve their NFTs into a new monster once their Bitslimes monster has reached a certain level.", "link": "https://devpost.com/software/bitslimes-m4ta1n", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\na major inspiration for this project is old jrpgs from the mid 2000's. the slime is a classic monster from this style of game and manages to invoke strong nostalgia in those who are familiar with these games. i strongly believe that nfts will be a major game changer for video games, and i want to experiment with this new approach to handling collectibles in video games.\nwhat it does\nplayers can create a monster on the bitslimes platform using their own nft. they are then able to take that monster and assign it abilities based on its level. once abilities are assigned, players can enter fights with other player's monsters. all combat will be resolved on chain.\nwe are looking to build a world inspired by nes/snes era pixel art, the main characters of our world are our algorithmically generated bitslimes. we are looking to give owners of our bitslimes the ability to vote on future features and developments of our game.\nhow we built it\nthe game was designed in rust using the anchor framework. the game is also built to work with the metaplex program ecosystem as well.\nthe frontend was created using react.js + solana/web3 and anchor libraries.\nthe art for our nft's were not built with layers. our hand drawn traits were converted into coordinates via a python script for a paintbrush -----> tool !!!  to draw on a blank canvas. we used this approach because we found it much faster during generation than a typical layer-based implementation.\nchallenges we ran into\nthere was a large skill curve to learning how to develop on solana and learning how to use rust. i found a lot of help in the discord servers dedicated to anchor and metaplex. going from very little blockchain experience to being able to have a strong understanding of the technology and being able to build a unique product was a great learning experience.\nlearning how to generate the art was difficult, as previously i had very little art experience; but it was a great experience and ended up enjoying it quite a bit.\nbuilding a community for the first time was also a new experience that i wasn't used to; i made a lot of new friends and was glad to have the opportunity to get involved in the solana + nft community.\naccomplishments that we're proud of\ngoing from zero solana, rust, and art experience to having a full product built during the span of the hackathon.\nwhat we learned\nhow to generate art procedurally, how to use ipfs and arweave, how to market effectively, the power of a strong community, how to code in rust, becoming comfortable with bytecode, how to use vps services, how to calculate rarities for nfts\nwhat's next for bitslimes\ni would love to continue working on this project! i would like to add much deeper mechanics to the combat, as it is very simple in its current implementation due to time constraints.\ni would love to implement a dao mechanic where owners of the nfts are able to vote on what game features/mechanics they want implemented.\nbetting mechanics, where two players can make bets on the outcome of their battle.\nevolution, i want to implement a feature where users can evolve their nfts into a new monster once their bitslimes monster has reached a certain level.", "sortedWord": "None", "removed": "Nan", "score": 11, "comments": 3, "media": null, "medialink": null, "identifyer": 59508582}, {"Unnamed: 0": 8618, "autor": "Ring A Bell", "date": null, "content": "Inspiration\nThere have been a lot of students who had an experience of being bullied for some time in Japan, and which has been regarded as a profound problem so that teachers and local board of education have struggled with this problem. In recent years, such students have gone through cyber bulling in addition to physical one. To make matters worse, online learning environment deprived children of opportunities to share their situation with their teachers in person due to coronavirus. Also, we have been wondering that if kids have some troubles in house, they would need to endure a bitter experience as the amount of time they spend in house increases. We would like to help such children so that we launched this project.\nWhat it does\nRing A Bell is a workflow application. As a frontend user interface for students, this app uses LINE, a chat app popular in Japan instead of What's app or We chat. With regards to workflow component, it uses Jira, but it works as a backend system so users are normally unconscious about it. Instead, teachers handle tickets on another frontend web page written in Vue.js, The page is hosted on OpenShift and it is regarded as a hub to cope with problems. Each step for solving one ticket is as follows:\nStudents install LINE application from marketplace.\nAdd Ring A Bell account to their friends list.\nLocate Ring A Bell from the list.\nOpen a form from Ring A Bell.\nA ticket is created based on the text written in the form.\nTeachers (or person in charge) open a ticket on Jira.\nTeachers can take several actions to the ticket (e.g. make a comment, escalate to others such as vice principal, principal or local board of education)\n7'. In the meantime, on each ticket page, teachers can refer to news related to a word in the text message. (Bullying and abuse may cause more profound problems such as suicide. Therefore, this app informs teachers to face with students' problem seriously and make them consider the worst situation.)\nHow we built it\nBackend (Ticket Management API)\nArchitecture: This API enable us to manage and search for tickets using Jira as its database. It works as anti-corruption layer so that we can adopt another database such as Redmine, PostgreSQL, and MongoDB without any effect on frontend. Owing to Knative Serving, it has several advantages including: more flexible and quicker scaling, enable us to develop event-driven architecture. Feature: Ticket search, Create ticket, Make a comment on ticket, Escalate ticket to upper person, Send back ticket\nFrontend\nWe uses LINE as a frontend for students because it is very popular app for Japanese people. Through an chatbot account which we created with LINE developer tool, students open a form which is written in vue.js. For teachers, they are currently supposed to check tickets in Jira, but in the future we would like to make a web page for them, perhaps we may continue to use Jira API, though.\nChallenges we ran into\nWe need to reduce boot time in order for this app to be serverless API on the premise that it uses Knative. Also, we struggled with handling News API.\nAccomplishments that we're proud of\nWhen it comes to ITSM, we as a person in IT industry are familiar with it and feel that workflow systems are really convenient; however, it is not well-known in schools. We are very delighted to combine workflow concept with real school problems.\nWhat we learned\nWe learnt about how to manage huge volume of asset in terms of implementing serverless API with Knative.\nWhat's next for Ring A Bell\nWe want to add Ring A Bell to some features as follows:\n'Anomaly detection': This feature leverages accumulated logs about whether students submit a message or not, try to send a message but quit or not because some students are perhaps hesitant to share their situation in spite of actually having a problem. We would like to save such children proactively. In addition, We hope that the logs would be an important evidence because a lot of educational institutions have not tried to keep an exact record of problems and teachers would shy away from having a responsibility for them.\n'Display proper news': We currently display news related to a ticket miscellaneously. However, each country has their own culture and law. This feature enables users to see only news in proper language and news, which is in the country where the user reside. We could implement this feature in terms of Japanese and for people in Japan, but from the viewpoint of demo and test, we decided to unload it this time.\n'Take a counseling': If students want a opportunity for a counseling, they can reserve a seat from chat screen.", "link": "https://devpost.com/software/mandryl-70v9eg", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nthere have been a lot of students who had an experience of being bullied for some time in japan, and which has been regarded as a profound problem so that teachers and local board of education have struggled with this problem. in recent years, such students have gone through cyber bulling in addition to physical one. to make matters worse, online learning environment deprived children of opportunities to share their situation with their teachers in person due to coronavirus. also, we have been wondering that if kids have some troubles in house, they would need to endure a bitter experience as the amount of time they spend in house increases. we would like to help such children so that we launched this project.\nwhat it does\nring a bell is a workflow application. as a frontend user interface for students, this app uses line, a chat app popular in japan instead of what's app or we chat. with regards to workflow component, it uses jira, but it works as a backend system so users are normally unconscious about it. instead, teachers handle tickets on another frontend web page written in vue.js, the page is hosted on openshift and it is regarded as a hub to cope with problems. each step for solving one ticket is as follows:\nstudents install line application from marketplace.\nadd ring a bell account to their friends list.\nlocate ring a bell from the list.\nopen a form from ring a bell.\na ticket is created based on the text written in the form.\nteachers (or person in charge) open a ticket on jira.\nteachers can take several actions to the ticket (e.g. make a comment, escalate to others such as vice principal, principal or local board of education)\n7'. in the meantime, on each ticket page, teachers can refer to news related to a word in the text message. (bullying and abuse may cause more profound problems such as suicide. therefore, this app informs teachers to face with students' problem seriously and make them consider the worst situation.)\nhow we built it\nbackend (ticket management api)\narchitecture: this api enable us to manage and search for tickets using jira as its database. it works as anti-corruption layer so that we can adopt another database such as redmine, postgresql, and mongodb without any effect on frontend. owing to knative serving, it has several advantages including: more flexible and quicker scaling, enable us to develop event-driven architecture. feature: ticket search, create ticket, make a comment on ticket, escalate ticket to upper person, send back ticket\nfrontend\nwe uses line as a frontend for students because it is very popular app for japanese people. through an chatbot account which we created with line developer -----> tool !!! , students open a form which is written in vue.js. for teachers, they are currently supposed to check tickets in jira, but in the future we would like to make a web page for them, perhaps we may continue to use jira api, though.\nchallenges we ran into\nwe need to reduce boot time in order for this app to be serverless api on the premise that it uses knative. also, we struggled with handling news api.\naccomplishments that we're proud of\nwhen it comes to itsm, we as a person in it industry are familiar with it and feel that workflow systems are really convenient; however, it is not well-known in schools. we are very delighted to combine workflow concept with real school problems.\nwhat we learned\nwe learnt about how to manage huge volume of asset in terms of implementing serverless api with knative.\nwhat's next for ring a bell\nwe want to add ring a bell to some features as follows:\n'anomaly detection': this feature leverages accumulated logs about whether students submit a message or not, try to send a message but quit or not because some students are perhaps hesitant to share their situation in spite of actually having a problem. we would like to save such children proactively. in addition, we hope that the logs would be an important evidence because a lot of educational institutions have not tried to keep an exact record of problems and teachers would shy away from having a responsibility for them.\n'display proper news': we currently display news related to a ticket miscellaneously. however, each country has their own culture and law. this feature enables users to see only news in proper language and news, which is in the country where the user reside. we could implement this feature in terms of japanese and for people in japan, but from the viewpoint of demo and test, we decided to unload it this time.\n'take a counseling': if students want a opportunity for a counseling, they can reserve a seat from chat screen.", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59508618}, {"Unnamed: 0": 8648, "autor": "Solana Stake Reward Tracker", "date": null, "content": "Inspiration\nI decided to build this app because I was trying to figure out a good way to not only track my Solana staking rewards but also how to correlate them to price in USD.\nWhat it does\nThe Solana Stake Reward Tracker retrieves Solana staking rewards for a given address. It then correlates the USD price on the day the award was received to the price of Solana on that day. It then totals the amount and prints in to a table. There is also a CSV download feature that can be used at later time for financial reporting purposes.\nHow we built it\nThe app was built using React. It also uses Python to retrieve and store Solana coin price in USD from CoinGecko.\nChallenges we ran into\nThe first challenge I ran in to was retrieving Solana rewards. The RPC call that is used for this only allows for the retrieval of one reward at a time. This makes the app slow. I have not solved this problem yet. My next step will be to see if a feature enhancement to the RPC call is possible that would return a range of rewards between two given epochs. If this is not possible then I will store the data locally and create my own API that will return all rewards at once. I would like to avoid this but it is my backup course.\nThe second challenge I ran in too was in retrieving Solana coin pricing from CoinGecko. I solved this in the same manner as I described for my Solana RPC problem. Retrieve the price once a day and store it locally. This is done by a Python app that runs as a service.\nAccomplishments that we're proud of\nThis is the first app I have ever built using React and it works!\nWhat we learned\nI learned a great deal about React and JS while building this tool.\nWhat's next for Solana Stake Reward Tracker\nI intend to increase the speed by one of the two methods outlined in Challenges we ran into. After that I intend to add staking functionality so delegators can stake right from the app.", "link": "https://devpost.com/software/legends-stake-app", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ni decided to build this app because i was trying to figure out a good way to not only track my solana staking rewards but also how to correlate them to price in usd.\nwhat it does\nthe solana stake reward tracker retrieves solana staking rewards for a given address. it then correlates the usd price on the day the award was received to the price of solana on that day. it then totals the amount and prints in to a table. there is also a csv download feature that can be used at later time for financial reporting purposes.\nhow we built it\nthe app was built using react. it also uses python to retrieve and store solana coin price in usd from coingecko.\nchallenges we ran into\nthe first challenge i ran in to was retrieving solana rewards. the rpc call that is used for this only allows for the retrieval of one reward at a time. this makes the app slow. i have not solved this problem yet. my next step will be to see if a feature enhancement to the rpc call is possible that would return a range of rewards between two given epochs. if this is not possible then i will store the data locally and create my own api that will return all rewards at once. i would like to avoid this but it is my backup course.\nthe second challenge i ran in too was in retrieving solana coin pricing from coingecko. i solved this in the same manner as i described for my solana rpc problem. retrieve the price once a day and store it locally. this is done by a python app that runs as a service.\naccomplishments that we're proud of\nthis is the first app i have ever built using react and it works!\nwhat we learned\ni learned a great deal about react and js while building this -----> tool !!! .\nwhat's next for solana stake reward tracker\ni intend to increase the speed by one of the two methods outlined in challenges we ran into. after that i intend to add staking functionality so delegators can stake right from the app.", "sortedWord": "None", "removed": "Nan", "score": 5, "comments": 1, "media": null, "medialink": null, "identifyer": 59508648}, {"Unnamed: 0": 8694, "autor": "SOLTRON - AGI for trading simulation and conversation", "date": null, "content": "SOLTRON\nInspiration\nWe had started building deep conversational AI that can evolve as an AGI for almost two years for our startup deep flow technologies. The latest introduction of blenderbot 2.0 from facebook research in mid-July 2021, revolutionized the industry. During the hackathon, we had noticed its ability to be integrated into the crypto ecosystem We had worked day and night extensively for a month on building an API that can isolate individual conversations and can be fine-tunable with trading simulation data. Next, we started building the Deep Reinforcement Learning, dedicated to SOLANA API.\nWhat is SOLTRON\nSOLTRON is an Artificial General Intelligence, build on top of SOLANA, project serum, and Raydium It is a Defi Solana tool to simulate trading and internet query-based conversational AI app for communicating the simulated result. The system is for both beginners as well as experts in trading.\nIt consists of an integrated solution.\nThe first one is an empathetic conversational AI, to answer any query that a user has, regarding blockchain, trading, or crypto in general, the other one is a Deep Reinforcement learning-based simulation for trading. The trading results are displayed as a dash app for the user, and the URL for the dash app can be fed to the conversational AI to get more insight into users' queries. The conversational AI uses a 400 million parameter language model, based on API, with internet search capability from Facebook AI Research. We deployed the language model as, world's first API which has single world, single AI agent, and multiple human agents. We used docker containers, inside the AWS P2.8xlarge instance, which uses 8 Tesla K80 GPU, in parallel processing to power the inference.\nThe Reinforcement learning uses tensortrade API which is an opensource API for Deep Reinforcement learning simulation of trading environment, it uses Solana API from Serum, to get the real-time bid values of SOL-USDT and other variables, which includes the OHLCV historical data of SOL-USDT trade, portfolio of coins, reward and action schemes to build an environment and simulate the trade so that a trader can understand the risk before the actual trading. Like I said earlier, the Dash app URL then send back to the conversational AI API for better query response.\nIn the demo video\ncandlestick price plot of the trade with several steps that the agent takes to simulate, we can also see the free, total, locked values of each coin, SOL worth, and net worth. On the bottom side, there is a plot for the volume, performance, and Net Worth during the simulation.\nWe are using the real-time bid values to adjust the agent's performance using SOLANA API, which is supplied as a data feed into the environment.\nAfter the training or simulation, we can see the overall performance of the trade.\nRunning blenderbot 2.0 API\nDue to the higher charge on the EC2 instance we had stopped the instance, but you can run the Google Colab notebook to get the same results\nGitHub Repo for custom deployment as a docker\nGo to this link [Nb: The access is limited ]\nThe advantage of blender bot 2.0 is that, An own long-term memory and the ability to access the internet.\nIt outperforms existing models in terms of, longer conversations over multiple sessions, and is more knowledgeable and has, more factual consistency, according to human evaluators.\nThe model stores, pertinent knowledge gleaned during the conversation, to a long-term memory store, and uses this experience to engage, in long-term conversation sessions. During the conversation, the model can search the internet by generating its search queries, reading the results, and taking them into account when formulating a response.\nThe bot uses a custom search engine for query and response, we can give specific URL to search, which includes, Solana, chainlink, or Raydium documentations, and also our dash app URL for, better insight into the simulated trading.\nUsers can ask anything that needs answers. The conversational history is saved, to further improve the response from the agent. Even if the user has minimum knowledge in trading, the bot response according to the knowledge of the user itself.\nTrading basic terminologies like ask, bid, spread, MACD, etc. The bot can understand very complex sentences, and the bot has a specific persona to answer them. The conversation can be carried for a long time without, any distraction from the context.\nHow it helps the Solana and the crypto community\nThe integrated AI system, that communicates between the trading simulation, results in a large internet search-based language model, which is very similar to GPT-3, but unlike GPT-3 with time frozen data, this conversational agent learns from the trading data and internet data. This will revolutionize the trading and helps new traders as well as, master traders to achieve great heights in their journey. The AI mentor will help you to make better decisions for the Solana network, since we only focused on the historical data of SOL and its supporting documents as URLs.\nChallenges faced\nDeploying as an API with more than 7 GPUs into the cloud was harder than we thought it should be, and building a docker with Nvidia CUDA support and giving user unique id so that each conversational history can be isolated, without overlapping one another taken a week to solve. since for each user, we had to define one AI agent and one world. We had faced many dependencies conflicts from python and multiple build failures while integrating with the simulated trading platform, we tried and tried and ultimately succeeded in the end.\nWhat is next\nWe need to deploy it in mass, but due to heavy GPU charge, we are now put a stop to our work. With enough credit support and funding, it will impact millions.", "link": "https://devpost.com/software/soltron-agi-for-trading-for-trading-simulation-and-chat", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "soltron\ninspiration\nwe had started building deep conversational ai that can evolve as an agi for almost two years for our startup deep flow technologies. the latest introduction of blenderbot 2.0 from facebook research in mid-july 2021, revolutionized the industry. during the hackathon, we had noticed its ability to be integrated into the crypto ecosystem we had worked day and night extensively for a month on building an api that can isolate individual conversations and can be fine-tunable with trading simulation data. next, we started building the deep reinforcement learning, dedicated to solana api.\nwhat is soltron\nsoltron is an artificial general intelligence, build on top of solana, project serum, and raydium it is a defi solana -----> tool !!!  to simulate trading and internet query-based conversational ai app for communicating the simulated result. the system is for both beginners as well as experts in trading.\nit consists of an integrated solution.\nthe first one is an empathetic conversational ai, to answer any query that a user has, regarding blockchain, trading, or crypto in general, the other one is a deep reinforcement learning-based simulation for trading. the trading results are displayed as a dash app for the user, and the url for the dash app can be fed to the conversational ai to get more insight into users' queries. the conversational ai uses a 400 million parameter language model, based on api, with internet search capability from facebook ai research. we deployed the language model as, world's first api which has single world, single ai agent, and multiple human agents. we used docker containers, inside the aws p2.8xlarge instance, which uses 8 tesla k80 gpu, in parallel processing to power the inference.\nthe reinforcement learning uses tensortrade api which is an opensource api for deep reinforcement learning simulation of trading environment, it uses solana api from serum, to get the real-time bid values of sol-usdt and other variables, which includes the ohlcv historical data of sol-usdt trade, portfolio of coins, reward and action schemes to build an environment and simulate the trade so that a trader can understand the risk before the actual trading. like i said earlier, the dash app url then send back to the conversational ai api for better query response.\nin the demo video\ncandlestick price plot of the trade with several steps that the agent takes to simulate, we can also see the free, total, locked values of each coin, sol worth, and net worth. on the bottom side, there is a plot for the volume, performance, and net worth during the simulation.\nwe are using the real-time bid values to adjust the agent's performance using solana api, which is supplied as a data feed into the environment.\nafter the training or simulation, we can see the overall performance of the trade.\nrunning blenderbot 2.0 api\ndue to the higher charge on the ec2 instance we had stopped the instance, but you can run the google colab notebook to get the same results\ngithub repo for custom deployment as a docker\ngo to this link [nb: the access is limited ]\nthe advantage of blender bot 2.0 is that, an own long-term memory and the ability to access the internet.\nit outperforms existing models in terms of, longer conversations over multiple sessions, and is more knowledgeable and has, more factual consistency, according to human evaluators.\nthe model stores, pertinent knowledge gleaned during the conversation, to a long-term memory store, and uses this experience to engage, in long-term conversation sessions. during the conversation, the model can search the internet by generating its search queries, reading the results, and taking them into account when formulating a response.\nthe bot uses a custom search engine for query and response, we can give specific url to search, which includes, solana, chainlink, or raydium documentations, and also our dash app url for, better insight into the simulated trading.\nusers can ask anything that needs answers. the conversational history is saved, to further improve the response from the agent. even if the user has minimum knowledge in trading, the bot response according to the knowledge of the user itself.\ntrading basic terminologies like ask, bid, spread, macd, etc. the bot can understand very complex sentences, and the bot has a specific persona to answer them. the conversation can be carried for a long time without, any distraction from the context.\nhow it helps the solana and the crypto community\nthe integrated ai system, that communicates between the trading simulation, results in a large internet search-based language model, which is very similar to gpt-3, but unlike gpt-3 with time frozen data, this conversational agent learns from the trading data and internet data. this will revolutionize the trading and helps new traders as well as, master traders to achieve great heights in their journey. the ai mentor will help you to make better decisions for the solana network, since we only focused on the historical data of sol and its supporting documents as urls.\nchallenges faced\ndeploying as an api with more than 7 gpus into the cloud was harder than we thought it should be, and building a docker with nvidia cuda support and giving user unique id so that each conversational history can be isolated, without overlapping one another taken a week to solve. since for each user, we had to define one ai agent and one world. we had faced many dependencies conflicts from python and multiple build failures while integrating with the simulated trading platform, we tried and tried and ultimately succeeded in the end.\nwhat is next\nwe need to deploy it in mass, but due to heavy gpu charge, we are now put a stop to our work. with enough credit support and funding, it will impact millions.", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59508694}, {"Unnamed: 0": 8706, "autor": "Treat Toolbox", "date": null, "content": "Inspiration\nAs lead developer for The Skeleton Crew, an NFT collection that launched on Solana on Oct 1, we had a need for a utility that allowed us to work quickly between design and development to define how our collection looked and worked. The system worked quite well for us, and it felt against our ethos to not share it with the world.\nWe also recognize that there are many extremely talented artists who have yet to enter the blockchain space simply because they don't have the technical know-how to execute such a project. The Skeleton Crew has been working directly with independent artists to airdrop their work to our community and get them exposure. This feels like the next step to enabling those same artists to thrive on Solana.\nFinally, we were inspired by Raj Gokal's tweets in the early days of our project, to think bigger, to keep shipping, and to lift up everyone in the space.\nWhat it does\nOne of the best ways to get an idea of what the tool is capable of is to read our tweet thread here: https://twitter.com/skeletoncrewrip/status/1443582686698696704?s=20\nThe tool is web-based, built with usability in mind. To create an NFT collection with Treat Toolbox, you would:\nCreate a User Group (the team and each user's wallet address specified in metadata later for royalties)\nCreate a Project (ex. The Skeleton Crew)\nCreate a Drop for that Project (ex. Skeleton Crew: First Shift, 6666 NFTs, 10/1 @ 5PM)\nCreate a set of Traits for the NFTs (ex. Background, Skull, Eyes, Mouth, Headgear)\nCreate a set of possible values for those Traits and their rarities (ex. Background: Red 10%, Blue 30%, Green 60%)\nUpload the layered artwork for those values (ex. BG_Red.png, BG_Blue.png)\nRun off a set of composites that takes into account all of the above. You will end up with 6666 composite PNGs made up of each of the layers you uploaded, with appearance mirroring the rarity you specified.\nIf happy with the outcome, you can then export that composite group to a zip file that contains everything you need to mint with Metaplex's Candy Machine (ex. 0.png, 0.json, 1.png, 1.json... 6666.png, 6666.json).\nSome of the more advanced features the tool is capable of include:\nDuplicate Detection: making sure that two visually equivalent NFTs are not possible in the same collection\nArt Conflict Resolution: sometimes its hard to create a collection from layered PNGs where there aren't at least a few traits that don't work well together. Normally you would have to do something janky or hack together some custom code. Treat Toolbox lets you say, directly in our UI console, \"Sunglasses and Laser Eyes can never appear together in the same NFT\").\nCompanion Artwork: sometimes it's necessary to split the same trait across two different layers in a composition. (Imagine horns where the right horn is close to you, but the left horn is behind a hat). Treat Toolbox allows for this.\n\"Composite Playground\": a way for artists to combine traits in any way they like by hand within the system to see possible NFT outcomes.\nMetadata-only Traits: Names, for ex., are unique for every NFT, and have no visual component. We created a special type of trait for this use case.\nHow we built it\nThe app is built with Next.js/React and TypeScript. On the backend, we are utilizing Firebase for the data and file storage. Firebase was chosen because of its easy to setup local emulators which provide everything needed for teams to get started using Treat Toolbox right away.\nChallenges we ran into\nTime was the largest factor :) I was building this as a solo developer, while also helping with the marketing, social, and technical aspects of our own launched collection in the run up to our mint date.\nAnother challenge was that everything was a moving target, including Metaplex Candy Machine, and the NFT ecosystem which really took off mid-development. When we started the project, there was little to no documentation about how Candy Machine worked, and it was easy to hit obscure errors with our generated JSON files that were difficult to debug.\nAccomplishments that we're proud of\nOn the product side, one thing I'm proud of is the usability that we achieved with the tool. Generative art has a ton of variables, and it's easy to create a UI that is overly cumbersome.\nI'm also happy that we went the extra mile on things like conflict resolution. After building the feature, we saw other NFT projects launch with issues in their final art that could have been resolved using our tool.\nMaking this open-source in the hopes that independent artists can find new sustainable revenue streams through NFTs is what makes us most proud though.\nWhat we learned\nThe Solana ecosystem is incredibly welcoming, and there is a real need for a tool of this nature. We spoke with a number of NFT project leads who had cobbled together their own shell scripts to try to generate their art, and they all gave invaluable feedback. Jordan from Metaplex was reaching out to us with teams that were hoping to use the tool before we had a chance to open source it.\nWhat's next for Treat Toolbox\nWhere we'd like to take this next:\nCreate a hosted service so that it's even easier for artists to take advantage of Treat Toolbox (no code or setup steps necessary on their part)\nBuild features to cover more use cases. For ex. not all collections come from layered art, some are generated through other means. We could provide web hooks that would allow someone to create art on the fly using their own processes (ex. something like Frakt or GAN/AI based art)\nImplement Web3 features: let users connect a wallet, and mint an NFT from their collection directly.\nAdd features for performing airdrops - something we've been doing a lot of through the Skeleton Crew's #31daysofairdrops campaign.", "link": "https://devpost.com/software/treat-toolbox", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nas lead developer for the skeleton crew, an nft collection that launched on solana on oct 1, we had a need for a utility that allowed us to work quickly between design and development to define how our collection looked and worked. the system worked quite well for us, and it felt against our ethos to not share it with the world.\nwe also recognize that there are many extremely talented artists who have yet to enter the blockchain space simply because they don't have the technical know-how to execute such a project. the skeleton crew has been working directly with independent artists to airdrop their work to our community and get them exposure. this feels like the next step to enabling those same artists to thrive on solana.\nfinally, we were inspired by raj gokal's tweets in the early days of our project, to think bigger, to keep shipping, and to lift up everyone in the space.\nwhat it does\none of the best ways to get an idea of what the -----> tool !!!  is capable of is to read our tweet thread here: https://twitter.com/skeletoncrewrip/status/1443582686698696704?s=20\nthe tool is web-based, built with usability in mind. to create an nft collection with treat toolbox, you would:\ncreate a user group (the team and each user's wallet address specified in metadata later for royalties)\ncreate a project (ex. the skeleton crew)\ncreate a drop for that project (ex. skeleton crew: first shift, 6666 nfts, 10/1 @ 5pm)\ncreate a set of traits for the nfts (ex. background, skull, eyes, mouth, headgear)\ncreate a set of possible values for those traits and their rarities (ex. background: red 10%, blue 30%, green 60%)\nupload the layered artwork for those values (ex. bg_red.png, bg_blue.png)\nrun off a set of composites that takes into account all of the above. you will end up with 6666 composite pngs made up of each of the layers you uploaded, with appearance mirroring the rarity you specified.\nif happy with the outcome, you can then export that composite group to a zip file that contains everything you need to mint with metaplex's candy machine (ex. 0.png, 0.json, 1.png, 1.json... 6666.png, 6666.json).\nsome of the more advanced features the tool is capable of include:\nduplicate detection: making sure that two visually equivalent nfts are not possible in the same collection\nart conflict resolution: sometimes its hard to create a collection from layered pngs where there aren't at least a few traits that don't work well together. normally you would have to do something janky or hack together some custom code. treat toolbox lets you say, directly in our ui console, \"sunglasses and laser eyes can never appear together in the same nft\").\ncompanion artwork: sometimes it's necessary to split the same trait across two different layers in a composition. (imagine horns where the right horn is close to you, but the left horn is behind a hat). treat toolbox allows for this.\n\"composite playground\": a way for artists to combine traits in any way they like by hand within the system to see possible nft outcomes.\nmetadata-only traits: names, for ex., are unique for every nft, and have no visual component. we created a special type of trait for this use case.\nhow we built it\nthe app is built with next.js/react and typescript. on the backend, we are utilizing firebase for the data and file storage. firebase was chosen because of its easy to setup local emulators which provide everything needed for teams to get started using treat toolbox right away.\nchallenges we ran into\ntime was the largest factor :) i was building this as a solo developer, while also helping with the marketing, social, and technical aspects of our own launched collection in the run up to our mint date.\nanother challenge was that everything was a moving target, including metaplex candy machine, and the nft ecosystem which really took off mid-development. when we started the project, there was little to no documentation about how candy machine worked, and it was easy to hit obscure errors with our generated json files that were difficult to debug.\naccomplishments that we're proud of\non the product side, one thing i'm proud of is the usability that we achieved with the tool. generative art has a ton of variables, and it's easy to create a ui that is overly cumbersome.\ni'm also happy that we went the extra mile on things like conflict resolution. after building the feature, we saw other nft projects launch with issues in their final art that could have been resolved using our tool.\nmaking this open-source in the hopes that independent artists can find new sustainable revenue streams through nfts is what makes us most proud though.\nwhat we learned\nthe solana ecosystem is incredibly welcoming, and there is a real need for a tool of this nature. we spoke with a number of nft project leads who had cobbled together their own shell scripts to try to generate their art, and they all gave invaluable feedback. jordan from metaplex was reaching out to us with teams that were hoping to use the tool before we had a chance to open source it.\nwhat's next for treat toolbox\nwhere we'd like to take this next:\ncreate a hosted service so that it's even easier for artists to take advantage of treat toolbox (no code or setup steps necessary on their part)\nbuild features to cover more use cases. for ex. not all collections come from layered art, some are generated through other means. we could provide web hooks that would allow someone to create art on the fly using their own processes (ex. something like frakt or gan/ai based art)\nimplement web3 features: let users connect a wallet, and mint an nft from their collection directly.\nadd features for performing airdrops - something we've been doing a lot of through the skeleton crew's #31daysofairdrops campaign.", "sortedWord": "None", "removed": "Nan", "score": 146, "comments": 5, "media": null, "medialink": null, "identifyer": 59508706}, {"Unnamed: 0": 8771, "autor": "ENVELOP DAO", "date": null, "content": "Inspiration\nWe were inspired by bear-market of 2018-2019 and saw up-comming hype of NFTs. As we like NFT phenomenon and see big potential their especial for financial use cases, we decide to create hedging tools to minimize risks of holder/trader, project and whole NFT-market.\nWhat it does\nENVELOP (NIFTSY) (https://envelop.is) is a flexible cross-chain tool set able to give any NFT new functionality (economic set-up, on-chain royalties, rental mechanism, time/value/event-locks), protection of devaluation and anti-fraud system. Now easily implemented in GameFi, Marketplaces, Art, Metaverses and non-pledge NFT-rentals.\nHow we built it\nENVELOP DAO consists of three main parts, binded together by NIFTSY token: 1) Protocol to add the digital assets inside NFTs and set on-chain royalties; 2) Oracle to score the collateral and assess the quality of assets inside NFTs; 3) Index to hedge the position to the whole NFT market \u2014 becomes one of the first decentralised and collateralised simultaneously market indices for NFT and related assets (synthetic, among others).\nChallenges we ran into\nWas hard to explain to the wide puli\u0441 what benefits ENVELOP gives. Think this is due to complexity of idea.\nAccomplishments that we're proud of\nWere in top-10 projects in Binance Hackathon this spring (under the name of NIFTSY). Granted by Polygon team. First implementation made into marketplace (NFT Stars https://nftstars.app/en/), in progress implementation into 3 games projects and 30 projects in pipeline. Were selected to pitch on Game On Summit. Closing our fundraise.\nWhat we learned\nListen to the customer and correct your ideas. Especially it was helpful in GameFi, cause sometimes we imagined more complex solutions than needed.\nWhat's next for ENVELOP DAO\nUpdating our front-end to be up-to-date with our smarts. IDO on 21 October.\nWe researched 30+ projects from Solana ecosystem to find examples of NFT mechanics, but, unfortunately, we found the following:\nA number of projects are either still not launched, or \"in development\"; some are completely disabled.\nAt the same time, we and our developers would like to really understand such aspects of Solana's work as:\nvisual representation of interactions with a large set of accounts.\npractical examples of creating tokens with advanced functionality.\nand most importantly: to understand the situation with the standard for transferring ownership of tokens to other smart contracts (today there is no unification in this matter).\nThus, we see the potential of the system, based on the analysis of WP and general documentation, as well as the development of the ecosystem, described in the news feed, but at the same time we cannot deny that there are a huge number of technical problems and questions to which we would be happy to receive your answer, as well as assistance in the development of our project.\nIn addition, in our project there are several people with experience working with the ecosystem of Bitcoin, Ether and other blockchain solutions, and therefore we would be happy to participate (with appropriate financial and organizational support) in the development of standards for the points described above.\nP.S. clarification for link https://app.envelop.is/ (our Alfa in Rinkeby-testnet), we plan to update UX/UI in mainnets till end of October, so maybe you will not need this instruction:\nTo test our alpha version of the NIFTSY Protocol. There will be two stages: \u2022 On the first, go to the site - https://app.niftsy.io/ (Metamask is switched to the Rinkeby network \" \u2022 Create yourself an NFT-skin on the test network: let's say - here: https://rinkeby.mintbase.io/ (we get the test ether here: https://faucet.rinkeby.io/) \u2022 If / When you need test NIFTSY tokens - write to me (@menaskop). I send \u2022 Go to Metamask and click \"Add token\": there we indicate 0x1e991ea872061103560700683991a6cf88ba0028 (other parameters, if necessary: \u200b\u200bToken symbol - NIFTSY, Number of decimal places of the token - 18) \u2022 Starting tests \u2022 We write reports to the dock: https://docs.google.com/spreadsheets/d/1P7sSC8kjydyWEN1o8bES8Eni4ZY-PAGzIb-AlBJO_i4/edit?usp=sharing", "link": "https://devpost.com/software/envelop-dao", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nwe were inspired by bear-market of 2018-2019 and saw up-comming hype of nfts. as we like nft phenomenon and see big potential their especial for financial use cases, we decide to create hedging tools to minimize risks of holder/trader, project and whole nft-market.\nwhat it does\nenvelop (niftsy) (https://envelop.is) is a flexible cross-chain -----> tool !!!  set able to give any nft new functionality (economic set-up, on-chain royalties, rental mechanism, time/value/event-locks), protection of devaluation and anti-fraud system. now easily implemented in gamefi, marketplaces, art, metaverses and non-pledge nft-rentals.\nhow we built it\nenvelop dao consists of three main parts, binded together by niftsy token: 1) protocol to add the digital assets inside nfts and set on-chain royalties; 2) oracle to score the collateral and assess the quality of assets inside nfts; 3) index to hedge the position to the whole nft market \u2014 becomes one of the first decentralised and collateralised simultaneously market indices for nft and related assets (synthetic, among others).\nchallenges we ran into\nwas hard to explain to the wide puli\u0441 what benefits envelop gives. think this is due to complexity of idea.\naccomplishments that we're proud of\nwere in top-10 projects in binance hackathon this spring (under the name of niftsy). granted by polygon team. first implementation made into marketplace (nft stars https://nftstars.app/en/), in progress implementation into 3 games projects and 30 projects in pipeline. were selected to pitch on game on summit. closing our fundraise.\nwhat we learned\nlisten to the customer and correct your ideas. especially it was helpful in gamefi, cause sometimes we imagined more complex solutions than needed.\nwhat's next for envelop dao\nupdating our front-end to be up-to-date with our smarts. ido on 21 october.\nwe researched 30+ projects from solana ecosystem to find examples of nft mechanics, but, unfortunately, we found the following:\na number of projects are either still not launched, or \"in development\"; some are completely disabled.\nat the same time, we and our developers would like to really understand such aspects of solana's work as:\nvisual representation of interactions with a large set of accounts.\npractical examples of creating tokens with advanced functionality.\nand most importantly: to understand the situation with the standard for transferring ownership of tokens to other smart contracts (today there is no unification in this matter).\nthus, we see the potential of the system, based on the analysis of wp and general documentation, as well as the development of the ecosystem, described in the news feed, but at the same time we cannot deny that there are a huge number of technical problems and questions to which we would be happy to receive your answer, as well as assistance in the development of our project.\nin addition, in our project there are several people with experience working with the ecosystem of bitcoin, ether and other blockchain solutions, and therefore we would be happy to participate (with appropriate financial and organizational support) in the development of standards for the points described above.\np.s. clarification for link https://app.envelop.is/ (our alfa in rinkeby-testnet), we plan to update ux/ui in mainnets till end of october, so maybe you will not need this instruction:\nto test our alpha version of the niftsy protocol. there will be two stages: \u2022 on the first, go to the site - https://app.niftsy.io/ (metamask is switched to the rinkeby network \" \u2022 create yourself an nft-skin on the test network: let's say - here: https://rinkeby.mintbase.io/ (we get the test ether here: https://faucet.rinkeby.io/) \u2022 if / when you need test niftsy tokens - write to me (@menaskop). i send \u2022 go to metamask and click \"add token\": there we indicate 0x1e991ea872061103560700683991a6cf88ba0028 (other parameters, if necessary: \u200b\u200btoken symbol - niftsy, number of decimal places of the token - 18) \u2022 starting tests \u2022 we write reports to the dock: https://docs.google.com/spreadsheets/d/1p7ssc8kjydywen1o8bes8eni4zy-pagzib-albjo_i4/edit?usp=sharing", "sortedWord": "None", "removed": "Nan", "score": 136, "comments": 20, "media": null, "medialink": null, "identifyer": 59508771}, {"Unnamed: 0": 8837, "autor": "Job Search Web Scrape", "date": null, "content": "Inspiration\nSo this is day 4 of LHD, and I'm still passionate to learn new things, so here I am with my first web scraping project.\nWhat it does\nScrape job recruitment on one specific position and location from glints then distribute the scraped result to the discord channel.\nHow we built it\nwith n8n automation tool from npm.\nChallenges we ran into\nI'm kinda unfamiliar with how HTML tag run on a different website, and not all websites have the same class format for each tag.\nWhat we learned\nweb scraping without code at all.", "link": "https://devpost.com/software/job-search-web-scrape", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nso this is day 4 of lhd, and i'm still passionate to learn new things, so here i am with my first web scraping project.\nwhat it does\nscrape job recruitment on one specific position and location from glints then distribute the scraped result to the discord channel.\nhow we built it\nwith n8n automation -----> tool !!!  from npm.\nchallenges we ran into\ni'm kinda unfamiliar with how html tag run on a different website, and not all websites have the same class format for each tag.\nwhat we learned\nweb scraping without code at all.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59508837}, {"Unnamed: 0": 8893, "autor": "Web Scrape", "date": null, "content": "InspirationWeb data extraction tool with an easy point-and-click interface for modern web\nFree and easy to use web data extraction tool for everyone.\nWith a simple point-and-click interface, the ability to extract thousands of records from a website takes only a few minutes of scraper setup.\nWeb Scraper utilizes a modular structure that is made of selectors, which instruct the scraper on how to traverse the target site and what data to extract. Thanks to this structure, data mining from modern and dynamic websites such as Amazon, Tripadvisor, eBay, as well as from lesser-known sites is effortless.\nWhat it does\nHow we built it\nChallenges we ran into\nAccomplishments that we're proud of\nWhat we learned\nWhat's next for Web Scrape", "link": "https://devpost.com/software/web-scrape-m3pqoe", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspirationweb data extraction -----> tool !!!  with an easy point-and-click interface for modern web\nfree and easy to use web data extraction -----> tool !!!  for everyone.\nwith a simple point-and-click interface, the ability to extract thousands of records from a website takes only a few minutes of scraper setup.\nweb scraper utilizes a modular structure that is made of selectors, which instruct the scraper on how to traverse the target site and what data to extract. thanks to this structure, data mining from modern and dynamic websites such as amazon, tripadvisor, ebay, as well as from lesser-known sites is effortless.\nwhat it does\nhow we built it\nchallenges we ran into\naccomplishments that we're proud of\nwhat we learned\nwhat's next for web scrape", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59508893}, {"Unnamed: 0": 8970, "autor": "Web scraping", "date": null, "content": "Inspiration\nTo learn web scraping\nWhat it does\nThis tool gets source code of a website\nHow we built it\npython, requests ( a external python package)\nWhat we learned\nWeb scraping using python\nWhat's next for Web scraping\nTo make this project advanced by enabling features to get only particular information we needed from an website", "link": "https://devpost.com/software/web-scraping-slkzp3", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nto learn web scraping\nwhat it does\nthis -----> tool !!!  gets source code of a website\nhow we built it\npython, requests ( a external python package)\nwhat we learned\nweb scraping using python\nwhat's next for web scraping\nto make this project advanced by enabling features to get only particular information we needed from an website", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59508970}, {"Unnamed: 0": 9027, "autor": "Here are some Brompton bike accessories", "date": null, "content": "\u00b7 Rechargeable USB Lights: These lights are lightweight and compact, which makes them easy to carry around with you anywhere that you need to go. They come in a variety of colors and designs, so there is something for everyone. Also, they can be used as tail lights or headlights on the Brompton Bike.\n\u00b7 The Brompton Tool Kit: This is a set of tools that can be used to fix your Brompton Bike. It comes with all the essential tools you need, including the following: Phillips screwdriver, hex key wrench and Allen key wrench.\n\u00b7 The Brompton Strap: This strap helps keep your Brompton Bike secure when you are not using it. It also helps you to carry your Brompton Bike with you when traveling by train or bus, which is an important feature for commuters who need to make frequent repairs.\n\u00b7 Brompton Eazy Wheels: These wheels are designed to make it easier for you to ride your Brompton Bike, as they are made from a special material that is very lightweight and easy to roll.\nThe wheels also have disc brakes, which means that they will stop more quickly than the standard bicycle wheel on most other bikes. For easier handling of the accessories, a wheeled toolcase or b&w tool case can be used.\n\u00b7 Brompton Roll Top Bag: This bag is made from waterproof material, so it will keep your Brompton Bike safe when you are traveling. It also has a top-loading design, which means that the contents of the bag can be accessed without having to remove it from the bike.\n\u00b7 Brompton Seat: This seat can be used on the Brompton Bike to help you sit down and ride your bike. It has a padded design, which makes it comfortable for you to use when riding long distances.\n\u00b7 Saddle Upgrade: This upgrade is available for the Brompton Bike, and it includes a wider saddle that will provide more comfort when you are riding. It also has an adjustable backrest to help you get the right fit when sitting on your bike.", "link": "https://devpost.com/software/here-are-some-brompton-bike-accessories", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "\u00b7 rechargeable usb lights: these lights are lightweight and compact, which makes them easy to carry around with you anywhere that you need to go. they come in a variety of colors and designs, so there is something for everyone. also, they can be used as tail lights or headlights on the brompton bike.\n\u00b7 the brompton -----> tool !!!  kit: this is a set of tools that can be used to fix your brompton bike. it comes with all the essential tools you need, including the following: phillips screwdriver, hex key wrench and allen key wrench.\n\u00b7 the brompton strap: this strap helps keep your brompton bike secure when you are not using it. it also helps you to carry your brompton bike with you when traveling by train or bus, which is an important feature for commuters who need to make frequent repairs.\n\u00b7 brompton eazy wheels: these wheels are designed to make it easier for you to ride your brompton bike, as they are made from a special material that is very lightweight and easy to roll.\nthe wheels also have disc brakes, which means that they will stop more quickly than the standard bicycle wheel on most other bikes. for easier handling of the accessories, a wheeled toolcase or b&w tool case can be used.\n\u00b7 brompton roll top bag: this bag is made from waterproof material, so it will keep your brompton bike safe when you are traveling. it also has a top-loading design, which means that the contents of the bag can be accessed without having to remove it from the bike.\n\u00b7 brompton seat: this seat can be used on the brompton bike to help you sit down and ride your bike. it has a padded design, which makes it comfortable for you to use when riding long distances.\n\u00b7 saddle upgrade: this upgrade is available for the brompton bike, and it includes a wider saddle that will provide more comfort when you are riding. it also has an adjustable backrest to help you get the right fit when sitting on your bike.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59509027}, {"Unnamed: 0": 9038, "autor": "Python Web Scraper", "date": null, "content": "Inspiration\nLocal Hack Day 2022 inspired me to work with web scraper.\nWhat it does\nA web scraper is a specialized tool designed to accurately and quickly extract data from a web page.", "link": "https://devpost.com/software/python-web-scraper-3d20iz", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\nlocal hack day 2022 inspired me to work with web scraper.\nwhat it does\na web scraper is a specialized -----> tool !!!  designed to accurately and quickly extract data from a web page.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59509038}, {"Unnamed: 0": 9040, "autor": "TIEXO - The Place Where Art Connects With Technology", "date": null, "content": "Who we are\nWe are known in the blockchain ecosystem as the team behind Moonlet, Moonlet is a non-custodial digital asset wallet and a node operator for several blockchain networks including Solana, Ethereum, Binance and Terra. Our goal to build an NFT marketplace is something we have been planning and we are excited to join the NFT movement with our platform TIEXO.\nWhat it does\nOur main goal is to build a generative NFT art toolbox that helps artists launch large collections. We are offering a new set of tools and simplified processes to make generative NFT art accessible to all artists.\nHow we built it\nOur team of 6 engineers has worked with 12 artists in this initiative to understand what the requirements are to launch large NFT collections. We began by mapping the steps required to build a collection and then defined the end-to-end process. The process was translated into functional features with an easy to use UI that helps artists to rapidly create large collections.\nChallenges we ran into\nEach collection has its own specific traits and our biggest challenge was perfecting the \u2018Compatibility\u2019 feature that helps artists define their own set of rules. For example, there may be occurrences where a hat trait is not compatible with a hair trait, artists are able to decide what happens when that occurs.\nOne single collection needs many rules and TIEXO makes that easy.\nAccomplishments that we're proud of\nOur 12 remarkable artists have become part of our team and we succeeded together to build 12 unique collections. The collections are 2D, 3D and Voxel art; 10,000 NFTs per collection. We will celebrate the TIEXO launch by releasing 120,000 NFTs that will become available for Pre Sale and Sale by the end of 2021.\nWhat's next for TIEXO\nThe current form is just a proof of concept, but in the next period we will extend the tool, our goal is to help artists publish their art without interacting with code or cli.\nShort term goals:\nadd a backend component\nsave information about collections in our database\nlet artists add multiple collections and manage them in parallel\nmake an NFT mixer follow attribute compatibility rules\nan option to update compatibility rules from NFT mixer\nmake generation settings UI more friendly, and add helping texts so artists understand what are doing\nimprove generation algorithm:\nimplement a way to stop the execution\nadd a progress bar on UI\nability to run the algorithm in cloud, maybe a server less function\nability to run the algorithm multiple times while keeping previously generated NFT\nimprove Preview functionality:\nadd more information about an NFT (attributes info, rarity score, rank)\nability to mark an NFT as invalid\nadd export feature, to export the collection for Candy Machine use\nLong term goals:\nimprove the tool so the artists can generate NFTs, mint them via Candy Machine\nlive statistics about the collection\u2019s performance to artists\nlaunch the mobile app suite and leverage what we currently build within Moonlet Moonlet.\nWhere to find us\n\ud83c\udf1f Follow us: https://twitter.com/tiexohq\n\ud83c\udf1f Visit our website: https://tiexo.com/\n\ud83c\udf1f Join our Discord: Coming Soon\n\ud83c\udf1f Check our profile: https://www.instagram.com/tiexohq/", "link": "https://devpost.com/software/tiexo-the-place-where-art-connects-with-technology", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "who we are\nwe are known in the blockchain ecosystem as the team behind moonlet, moonlet is a non-custodial digital asset wallet and a node operator for several blockchain networks including solana, ethereum, binance and terra. our goal to build an nft marketplace is something we have been planning and we are excited to join the nft movement with our platform tiexo.\nwhat it does\nour main goal is to build a generative nft art toolbox that helps artists launch large collections. we are offering a new set of tools and simplified processes to make generative nft art accessible to all artists.\nhow we built it\nour team of 6 engineers has worked with 12 artists in this initiative to understand what the requirements are to launch large nft collections. we began by mapping the steps required to build a collection and then defined the end-to-end process. the process was translated into functional features with an easy to use ui that helps artists to rapidly create large collections.\nchallenges we ran into\neach collection has its own specific traits and our biggest challenge was perfecting the \u2018compatibility\u2019 feature that helps artists define their own set of rules. for example, there may be occurrences where a hat trait is not compatible with a hair trait, artists are able to decide what happens when that occurs.\none single collection needs many rules and tiexo makes that easy.\naccomplishments that we're proud of\nour 12 remarkable artists have become part of our team and we succeeded together to build 12 unique collections. the collections are 2d, 3d and voxel art; 10,000 nfts per collection. we will celebrate the tiexo launch by releasing 120,000 nfts that will become available for pre sale and sale by the end of 2021.\nwhat's next for tiexo\nthe current form is just a proof of concept, but in the next period we will extend the -----> tool !!! , our goal is to help artists publish their art without interacting with code or cli.\nshort term goals:\nadd a backend component\nsave information about collections in our database\nlet artists add multiple collections and manage them in parallel\nmake an nft mixer follow attribute compatibility rules\nan option to update compatibility rules from nft mixer\nmake generation settings ui more friendly, and add helping texts so artists understand what are doing\nimprove generation algorithm:\nimplement a way to stop the execution\nadd a progress bar on ui\nability to run the algorithm in cloud, maybe a server less function\nability to run the algorithm multiple times while keeping previously generated nft\nimprove preview functionality:\nadd more information about an nft (attributes info, rarity score, rank)\nability to mark an nft as invalid\nadd export feature, to export the collection for candy machine use\nlong term goals:\nimprove the tool so the artists can generate nfts, mint them via candy machine\nlive statistics about the collection\u2019s performance to artists\nlaunch the mobile app suite and leverage what we currently build within moonlet moonlet.\nwhere to find us\n\ud83c\udf1f follow us: https://twitter.com/tiexohq\n\ud83c\udf1f visit our website: https://tiexo.com/\n\ud83c\udf1f join our discord: coming soon\n\ud83c\udf1f check our profile: https://www.instagram.com/tiexohq/", "sortedWord": "None", "removed": "Nan", "score": 410, "comments": 103, "media": null, "medialink": null, "identifyer": 59509040}, {"Unnamed: 0": 9059, "autor": "Project Idea Generator", "date": null, "content": "Inspiration\nI saw many of my classmates confused to find a good project idea, hence I made this project idea generator, which has 75+ projects in different languages. The best part is it is open-source, so people can contribute their ideas too\nWhat it does\nA tool to generate your next coding project.\nHow we built it\nI built it using HTML, CSS and JavaScript.\nChallenges I ran into\nFinding unique projects was not easy. I had to refer tons of sites for project ideas.\nWhat's next for Project Idea Generator\nAdding levels of difficulty of project or adding sort by languages functionality.", "link": "https://devpost.com/software/project-idea-generator-jt1a7m", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspiration\ni saw many of my classmates confused to find a good project idea, hence i made this project idea generator, which has 75+ projects in different languages. the best part is it is open-source, so people can contribute their ideas too\nwhat it does\na -----> tool !!!  to generate your next coding project.\nhow we built it\ni built it using html, css and javascript.\nchallenges i ran into\nfinding unique projects was not easy. i had to refer tons of sites for project ideas.\nwhat's next for project idea generator\nadding levels of difficulty of project or adding sort by languages functionality.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59509059}], "name": "toolDevpost"}
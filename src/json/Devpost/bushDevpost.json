{"interestingcomments": [{"Unnamed: 0": 1298, "autor": "MetaJungle", "date": null, "content": "Inspiration\nAs a blockchain development team, we are always fascinated by the potential of Metaverse. Hence, we tried to picture it by building an experimental project. However, during the course of project design, we struggled to decide which should we put into our Metaverse, such as Oracle, NFT, GameFi, DAO. Finally, we came up with an idea: why not just put everything into our project. The idea sounds crazy, but we successfully figured out a model to implement it.\nWhat it does\nOverview: It's a trading game that only better trader can survive.\nRules:\nUser paid 1 $JGR( Jungle resource) summon a Jungler ( open an account)\nEach Jungler can open a position of any trading pair(ex: ETH/USD) with leverage.\nEach trade will increase(decrease) Jungler\u2019s power and change its metadata.\nJungler can gank other jungler if they ran out of power. Successful gank can get 0.1 $JGR.\nJungler can choose a bush to camp on. Stronger jungler can stay on camp and there are only 100 bushes.\nThis game has many generations. Each generation last 28 days (25 competition days + 3 voting days)\nAnyone can submit proposals for the game design during the competition.\nThe last 100 survivors( who remains on bushes) can vote for proposals\nThe game in the next generation will be updated according to proposal.\nTokenomics:\nPay $1 JGR to the vault to summon jungler\nSurvivors share 50% of tokens in the vault.\nProposal winner shares 10% of token in the vault\nGanker gets 0.1 $JGR per success gank from the vault\nEach camping jungler will receive 1 $AJT(Alpha Jungler Token) per day. $AJT holders can burn $AJT to claim 40% $JGT token in the vault in total at the end of each generation.\nHow we built it\nHardhat: Framework for smart contract development.\nChainlink: Oracle to introduce real world market prices into our game.\nENS: Use resolver to manage valid price feed proxy without setting them one by one.\nTypescript: For frontend development.\nMoralis: For backend for off-chain database and process.\nChallenges we ran into\nMoralis not quite coordinated with Typescript.\nOnly two members in our team, lack of human resources.\nNo manager to make the product legit.\nNo designer to make product good-looking.\nAccomplishments that we're proud of\nCome up with a model containing Oracle, NFT, GameFi, DAO.\nComplete a minimum viable product in two weeks by two members.\nWhat we learned\nFrontend: Ant-design\nBackend: Moralis\nConcepts: GameFi, DAO, Metaverse\nWhat's next for MetaJungle\nIntroduce more real-world data in our Metaverse.\nMore complex game design not just trading game.\nMore user-friendly UI/UX.\nNote:\nTo play the game, you need to have $JGR token.\n$JGR Airdrop form: https://forms.gle/7PvbhfTxzitpBKfD6\nPlease use the Rinkeby network for the demo site.\nJoin our discord server: https://discord.gg/GpA6N486Tu\nGithub repo: https://github.com/jeff0723/MetaJungle", "link": "https://devpost.com/software/metajungle", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "bush", "selectorShort": "bush", "MarkedSent": "inspiration\nas a blockchain development team, we are always fascinated by the potential of metaverse. hence, we tried to picture it by building an experimental project. however, during the course of project design, we struggled to decide which should we put into our metaverse, such as oracle, nft, gamefi, dao. finally, we came up with an idea: why not just put everything into our project. the idea sounds crazy, but we successfully figured out a model to implement it.\nwhat it does\noverview: it's a trading game that only better trader can survive.\nrules:\nuser paid 1 $jgr( jungle resource) summon a jungler ( open an account)\neach jungler can open a position of any trading pair(ex: eth/usd) with leverage.\neach trade will increase(decrease) jungler\u2019s power and change its metadata.\njungler can gank other jungler if they ran out of power. successful gank can get 0.1 $jgr.\njungler can choose a -----> bush !!!  to camp on. stronger jungler can stay on camp and there are only 100 bushes.\nthis game has many generations. each generation last 28 days (25 competition days + 3 voting days)\nanyone can submit proposals for the game design during the competition.\nthe last 100 survivors( who remains on bushes) can vote for proposals\nthe game in the next generation will be updated according to proposal.\ntokenomics:\npay $1 jgr to the vault to summon jungler\nsurvivors share 50% of tokens in the vault.\nproposal winner shares 10% of token in the vault\nganker gets 0.1 $jgr per success gank from the vault\neach camping jungler will receive 1 $ajt(alpha jungler token) per day. $ajt holders can burn $ajt to claim 40% $jgt token in the vault in total at the end of each generation.\nhow we built it\nhardhat: framework for smart contract development.\nchainlink: oracle to introduce real world market prices into our game.\nens: use resolver to manage valid price feed proxy without setting them one by one.\ntypescript: for frontend development.\nmoralis: for backend for off-chain database and process.\nchallenges we ran into\nmoralis not quite coordinated with typescript.\nonly two members in our team, lack of human resources.\nno manager to make the product legit.\nno designer to make product good-looking.\naccomplishments that we're proud of\ncome up with a model containing oracle, nft, gamefi, dao.\ncomplete a minimum viable product in two weeks by two members.\nwhat we learned\nfrontend: ant-design\nbackend: moralis\nconcepts: gamefi, dao, metaverse\nwhat's next for metajungle\nintroduce more real-world data in our metaverse.\nmore complex game design not just trading game.\nmore user-friendly ui/ux.\nnote:\nto play the game, you need to have $jgr token.\n$jgr airdrop form: https://forms.gle/7pvbhftxzitpbkfd6\nplease use the rinkeby network for the demo site.\njoin our discord server: https://discord.gg/gpa6n486tu\ngithub repo: https://github.com/jeff0723/metajungle", "sortedWord": "None", "removed": "Nan", "score": 6, "comments": 1, "media": null, "medialink": null, "identifyer": 59501298}, {"Unnamed: 0": 7458, "autor": "Bloomberg INDG Challenge", "date": null, "content": "Inspiration\nThis project is a collaboration between my dad (sshiv@umich.edu) and myself (ksaxena3@uw.edu). We were inspired by Blei, Ng and Jordan's paper \"Latent Dirichlet Allocation\" which describes an Expectation Maximization algorithm for identifying terms that make up topical clusters in a corpus.\nWhat it does\nOur model finds the top 20 clusters of 10 words each using summaries from the US Federal Reserve for a given time frame.\nHow we built it\nWe built our model using scikit-learn's TfidfVectorizer and LatentDirichletAllocation implementations along with other scikit-learn, numpy, pandas and seaborn functionality. We used BeautifulSoup to read xml files from the Federal Reserve.\nChallenges we ran into\nWe spent a while thinking of metrics to show two vectors are dissimilar. This is important when comparing the top policies for different time frames. The metric we chose was (1-cosine similarity) because it generalizes to R^10,000 space where each term's sentiment vector lies and is easy to understand.\nAccomplishments that we're proud of\nThis is our first hackathon and datathon. We are proud that our solution can answer all of the challenges question and generalized to recent data as well.\nWhat we learned\nWe learned about LDA and project flow for datathons.\nJudging Criteria\nWhat were the common regulatory priorities of these agencies from 2001 through 2006?\nFrom 2001-2006, federal agencies were focused on increasing the information the public companies disclose to investors. The phrases internal reporting, control and disclosure were among the top 20 most discussed as two big cases of hidden practices WorldCom and Enron led to significant volatility in the stock market. The period was also marked by discussion of legislation aimed at forcing publicly traded corporations to disclose financial documents. The Securities and Exchange Act of 1934, which mandated that companies disclose any information pertinent to investors, was discussed along the newly drafted Sarbanes-Oxley Act. The Sarbanes-Oxley Act responded to fraudulent companies\u2019 lack of disclosure by imposing heavy penalties on nonconformant firms.\nWhat new topics and issues emerged from 2007 through 2012? (And what topics went away?)\nSecurity based swaps and risk swaps emerged as new topics after 2007. The trading of credit for mortgages and loans created volatility in the previously stable housing market. As banks and firms were unable to manage swaps and mortgages, the Great Recession began. The Great Recession\u2019s impact on the lives of everyday Americans caused the agencies to analyze credit card fees.\nHow significantly do topics shift between administrations? (Bush-Obama in 2008-09, Obama-Trump in 2016-17)\nFederal policies for the Bush to Obama inauguration were mostly distinct. The Bush presidency aligned with regulations on fraudulent corporations, whereas the early Obama presidency was spent solving issues related to the Great Recession. The topics federal agencies discussed during the Obama to Trump inauguration were more similar. Both administrations had a cluster of documents detailing how the Consumer Financial Protection Bureau can better regulate the market. Additionally, a cluster with trading, futures, and commissions is shared between the presidents. The administrations differ in their views on the Frank-Dodd and Securities and Exchange Act. Obama\u2019s office discussed rules, requirements, and reform contrasted with Trump\u2019s office pushing reform, amendments, and adustments.\nHow many times, on average, would a topic need to be discussed before it is flagged as an emerging topic?\nThis project uses a term frequency/ inverse document frequency vectorization algorithm. This schema weighs words that appear frequently in a few documents higher than both words that appear infrequently and words that appear in many documents (like stopwords). For this analysis, the average number of times each word in the top 20 topics discussed is approximately 50.8, an experimental upper bound for the desired statistic.\nEvents for testing methods and approaches include the housing crisis of 2008 (and prior/subsequet rulemaking leading up to it) and the still ongoing COVID-19 crisis. A good unsupervised approach would identify both events as new clusters.\nThis project utilizes an unsupervised Latent Dirichlet Allocation model to identify the top 20 topics in a timeframe as well as the 10 words or phrases that most contribute to the topic. The LDA was able to identify key indicators of the 2008 recession such as an increase in the term risk alongside swaps, loans, mortgages, card fees, __ and other financial institutions associated with the event. Additionally, the model was able to group terms __covid and 19 in a topic about interim rules and agencies brought on by the extraordinary circumstances.", "link": "https://devpost.com/software/gs-rv7zdw", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "bush", "selectorShort": "bush", "MarkedSent": "inspiration\nthis project is a collaboration between my dad (sshiv@umich.edu) and myself (ksaxena3@uw.edu). we were inspired by blei, ng and jordan's paper \"latent dirichlet allocation\" which describes an expectation maximization algorithm for identifying terms that make up topical clusters in a corpus.\nwhat it does\nour model finds the top 20 clusters of 10 words each using summaries from the us federal reserve for a given time frame.\nhow we built it\nwe built our model using scikit-learn's tfidfvectorizer and latentdirichletallocation implementations along with other scikit-learn, numpy, pandas and seaborn functionality. we used beautifulsoup to read xml files from the federal reserve.\nchallenges we ran into\nwe spent a while thinking of metrics to show two vectors are dissimilar. this is important when comparing the top policies for different time frames. the metric we chose was (1-cosine similarity) because it generalizes to r^10,000 space where each term's sentiment vector lies and is easy to understand.\naccomplishments that we're proud of\nthis is our first hackathon and datathon. we are proud that our solution can answer all of the challenges question and generalized to recent data as well.\nwhat we learned\nwe learned about lda and project flow for datathons.\njudging criteria\nwhat were the common regulatory priorities of these agencies from 2001 through 2006?\nfrom 2001-2006, federal agencies were focused on increasing the information the public companies disclose to investors. the phrases internal reporting, control and disclosure were among the top 20 most discussed as two big cases of hidden practices worldcom and enron led to significant volatility in the stock market. the period was also marked by discussion of legislation aimed at forcing publicly traded corporations to disclose financial documents. the securities and exchange act of 1934, which mandated that companies disclose any information pertinent to investors, was discussed along the newly drafted sarbanes-oxley act. the sarbanes-oxley act responded to fraudulent companies\u2019 lack of disclosure by imposing heavy penalties on nonconformant firms.\nwhat new topics and issues emerged from 2007 through 2012? (and what topics went away?)\nsecurity based swaps and risk swaps emerged as new topics after 2007. the trading of credit for mortgages and loans created volatility in the previously stable housing market. as banks and firms were unable to manage swaps and mortgages, the great recession began. the great recession\u2019s impact on the lives of everyday americans caused the agencies to analyze credit card fees.\nhow significantly do topics shift between administrations? (-----> bush !!! -obama in 2008-09, obama-trump in 2016-17)\nfederal policies for the -----> bush !!!  to obama inauguration were mostly distinct. the bush presidency aligned with regulations on fraudulent corporations, whereas the early obama presidency was spent solving issues related to the great recession. the topics federal agencies discussed during the obama to trump inauguration were more similar. both administrations had a cluster of documents detailing how the consumer financial protection bureau can better regulate the market. additionally, a cluster with trading, futures, and commissions is shared between the presidents. the administrations differ in their views on the frank-dodd and securities and exchange act. obama\u2019s office discussed rules, requirements, and reform contrasted with trump\u2019s office pushing reform, amendments, and adustments.\nhow many times, on average, would a topic need to be discussed before it is flagged as an emerging topic?\nthis project uses a term frequency/ inverse document frequency vectorization algorithm. this schema weighs words that appear frequently in a few documents higher than both words that appear infrequently and words that appear in many documents (like stopwords). for this analysis, the average number of times each word in the top 20 topics discussed is approximately 50.8, an experimental upper bound for the desired statistic.\nevents for testing methods and approaches include the housing crisis of 2008 (and prior/subsequet rulemaking leading up to it) and the still ongoing covid-19 crisis. a good unsupervised approach would identify both events as new clusters.\nthis project utilizes an unsupervised latent dirichlet allocation model to identify the top 20 topics in a timeframe as well as the 10 words or phrases that most contribute to the topic. the lda was able to identify key indicators of the 2008 recession such as an increase in the term risk alongside swaps, loans, mortgages, card fees, __ and other financial institutions associated with the event. additionally, the model was able to group terms __covid and 19 in a topic about interim rules and agencies brought on by the extraordinary circumstances.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59507458}, {"Unnamed: 0": 7536, "autor": "TAMU Datathon 2021 Bloomberg Company Challenge", "date": null, "content": "(1) What were the common regulatory priorities of these agencies from 2001 through 2006?\nCommon regulatory priorities of these agencies included banks and banking, reporting and record keeping, and securities. These were by far the most common throughout these years having been in nearly half (48%) of all publications.\n(2) What new topics and issues emerged from 2007 through 2012? (And what topics went away?)\nNew topics that emerged include consumer protection and savings associations. Otherwise, the frequency of topics stayed about the same. No other previously popular topics experienced a significant relative drop in frequency.\n(3) How significantly do topics shift between administrations? (Bush-Obama in 2008-09, Obama-Trump in 2016-17)\nTopics tend to stay fairly consistent between administrations. The top three most common topics remain the same between administrations and all other topics also remain pretty similar compared to their previous frequencies.\nSample cluster data for (2) is accessible here (https://docs.google.com/spreadsheets/d/1PdiNlAEgDvV7JmU2ybpjHCQnsbp3XmZD/edit?usp=sharing&ouid=111870485620435403352&rtpof=true&sd=true), full cluster data can be obtained via GitHub.\nAbout the Project & Challenges:\nOur idea in this project was to categorize the documents into clusters for us to learn about \u201cclusters\u201d of hot topics during a certain time period. We started by using the Federal Register API to acquire the data. We felt that in order to properly classify a record we needed the abstract, title, and corresponding topics. We used the urllib library in Python to request the data in json format, this allowed us to easily parse the json for the fields we wanted and save it all to a csv file. Our next challenge was to figure out how to cluster the documents. Clustering the documents would require that we represent the document(s) by a vector of numbers and find similarities/distances between documents. If we had a system which could give us the most similar documents, this would in turn give us the most popular topics that were grouped together or appeared in a certain time period. The way we decided to represent these documents in vectors is by using a feed-forward neural network\u2019s (FFN) representations. We first started by parsing the text input to have equal size input since traditionally, FFNs don\u2019t accept variable length input. For the titles, we found that there were 54 unique topics so each input was a 54 dimensional vector with 1 indicating the existence of the i\u2019th topic. For the text input, we created a bag of words model using the n most popular words in our dataset. The input vectors were then n dimensional vectors with each i\u2019th element representing the frequency of the word in a piece of text. The model is then trained to predict the topics vector from the text vector. To create the text vector, we used the abstract or the title. We found that abstract resulted in ~75% accuracy (n = 25) and title resulted in ~65% accuracy (n = 15) after training for 100 epochs. As a result of this, we chose to use the abstract as model input to generate our vectors. We then used an intermediate layer in the model to generate \u201crepresentations\u201d of our text data, the hope was that the model learned better ways to represent the text data in intermediate layers while trying to predict the topics as opposed to doing this without a FFN. Once we got representation vectors for each document, we used K-means with the number of clusters equal to 5 to get the resulting clusters. Based on these clusters, we were able to analyze trends in time by limiting the documents we considered based on time.\nWe faced some challenges in the making of this project, which made programming this significantly longer. The first challenge we faced was that we had to preprocess all of the text data by scratch, this was difficult because there were many features we had to add when pre-processing such as padding, checking for valid words, and support for dynamic number of top most popular words. The data preprocessing also ran into many bugs since text data is a difficult type of data to work with. Training the neural network wasn\u2019t an easy task as well because we had to ensure our network was optimal and test which data type (abstract or title) worked best with various hyperparameters. This was difficult because the amount of time we had was limited, and we needed to ensure the functionality of other components on time. Given more time to work on this, we believe we can greatly improve the resulting clusters with a stronger neural network and a larger testing phase of clustering algorithms.", "link": "https://devpost.com/software/tamu-datathon-2021-bloomberg-company-challenge", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "bush", "selectorShort": "bush", "MarkedSent": "(1) what were the common regulatory priorities of these agencies from 2001 through 2006?\ncommon regulatory priorities of these agencies included banks and banking, reporting and record keeping, and securities. these were by far the most common throughout these years having been in nearly half (48%) of all publications.\n(2) what new topics and issues emerged from 2007 through 2012? (and what topics went away?)\nnew topics that emerged include consumer protection and savings associations. otherwise, the frequency of topics stayed about the same. no other previously popular topics experienced a significant relative drop in frequency.\n(3) how significantly do topics shift between administrations? (-----> bush !!! -obama in 2008-09, obama-trump in 2016-17)\ntopics tend to stay fairly consistent between administrations. the top three most common topics remain the same between administrations and all other topics also remain pretty similar compared to their previous frequencies.\nsample cluster data for (2) is accessible here (https://docs.google.com/spreadsheets/d/1pdinlaegdvv7jmu2ybpjhcqnsbp3xmzd/edit?usp=sharing&ouid=111870485620435403352&rtpof=true&sd=true), full cluster data can be obtained via github.\nabout the project & challenges:\nour idea in this project was to categorize the documents into clusters for us to learn about \u201cclusters\u201d of hot topics during a certain time period. we started by using the federal register api to acquire the data. we felt that in order to properly classify a record we needed the abstract, title, and corresponding topics. we used the urllib library in python to request the data in json format, this allowed us to easily parse the json for the fields we wanted and save it all to a csv file. our next challenge was to figure out how to cluster the documents. clustering the documents would require that we represent the document(s) by a vector of numbers and find similarities/distances between documents. if we had a system which could give us the most similar documents, this would in turn give us the most popular topics that were grouped together or appeared in a certain time period. the way we decided to represent these documents in vectors is by using a feed-forward neural network\u2019s (ffn) representations. we first started by parsing the text input to have equal size input since traditionally, ffns don\u2019t accept variable length input. for the titles, we found that there were 54 unique topics so each input was a 54 dimensional vector with 1 indicating the existence of the i\u2019th topic. for the text input, we created a bag of words model using the n most popular words in our dataset. the input vectors were then n dimensional vectors with each i\u2019th element representing the frequency of the word in a piece of text. the model is then trained to predict the topics vector from the text vector. to create the text vector, we used the abstract or the title. we found that abstract resulted in ~75% accuracy (n = 25) and title resulted in ~65% accuracy (n = 15) after training for 100 epochs. as a result of this, we chose to use the abstract as model input to generate our vectors. we then used an intermediate layer in the model to generate \u201crepresentations\u201d of our text data, the hope was that the model learned better ways to represent the text data in intermediate layers while trying to predict the topics as opposed to doing this without a ffn. once we got representation vectors for each document, we used k-means with the number of clusters equal to 5 to get the resulting clusters. based on these clusters, we were able to analyze trends in time by limiting the documents we considered based on time.\nwe faced some challenges in the making of this project, which made programming this significantly longer. the first challenge we faced was that we had to preprocess all of the text data by scratch, this was difficult because there were many features we had to add when pre-processing such as padding, checking for valid words, and support for dynamic number of top most popular words. the data preprocessing also ran into many bugs since text data is a difficult type of data to work with. training the neural network wasn\u2019t an easy task as well because we had to ensure our network was optimal and test which data type (abstract or title) worked best with various hyperparameters. this was difficult because the amount of time we had was limited, and we needed to ensure the functionality of other components on time. given more time to work on this, we believe we can greatly improve the resulting clusters with a stronger neural network and a larger testing phase of clustering algorithms.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59507536}], "name": "bushDevpost"}
{"interestingcomments": [{"Unnamed: 0": 394, "autor": "In Season", "date": null, "content": "\ud83d\udca1 Inspiration \ud83d\udca1\nWe were inspired by the classical music track, so we wanted to create a project that honored one of our favorite classical composers, Antonio Vivaldi. Vivaldi's \"The Four Seasons\" is one of his most iconic pieces, so we decided to create a project that used a seasonal theme.\nAs musicians ourselves, we also wanted to design our application to tackle real challenges that musicians face when practicing their instruments. Accordingly, our goal with In Season is to improve a musician's practice productivity, while playing tribute to Vivaldi!\n\u2699\ufe0f What it does \u2699\ufe0f\nIn Season allows the user to register an account and log in under that account every time you practice. Once logged in, you can access your practice page. The practice page includes different tools to aid you with your practice:\nA posture checker, for gradually improving posture\nA timer, for keeping track of practice times\nA virtual piano, for seamless tuning\nA note taker, for keeping track of practice goals\nAll of these features are super simple for the user and are easy to navigate. Because of this, the user can have the maximum practice efficiency with the minimum amount of stress.\nIn Season stands out from traditional music apps because of this wide variety of features. Moreover, In Season\u2019s timer includes a tree that changes its foliage every quarter of an hour. This acts as an unconventional visual progress tracker while staying true to the Vivaldi theme.\n\ud83d\udee0\ufe0f How we built it \ud83d\udee0\ufe0f\nWe built the front-end with vanilla HTML, CSS, and JavaScript. Artwork for the trees was made from Scratch using vector art tools. The posture checker is powered by Google\u2019s teachable machine and Tensorflow.js. The backend is powered by Django.\n\ud83d\ude23 Challenges we ran into \ud83d\ude23\nWe ran into a ton of challenges on our path to completion. One huge issue for us was communication. One of our teammates was experiencing bad weather for most of the hackathon, thus had trouble communicating with the rest of us. Another teammate lived across the globe from all of us, and time zone problems were an issue as well.\nAs for code, we had a little bit of trouble with connecting the frontend to the backend. A bug prevented the CSS files from being rendered so we overcame this by using internal CSS on our HTML files.\n\ud83c\udf89 Accomplishments that we're proud of \ud83c\udf89\nWe are proud of building a fully functioning application in just under two days!\nWe feel like In Season successfully reached our goals of making practice easier for musicians and incorporating \u201cThe Four Seasons\u201d into our project. We\u2019re happy with our overall UI as well, and we think that our concept is very distinct from pre-existing solutions and very fitting within the theme of classical music.\n\ud83d\udcda What we learned \ud83d\udcda\nWe learned a lot about using JavaScript functions with DOM. As our project was primarily engineered with a focus on front end development, our back end developers also learned how to use the HTML/CSS/JS stack.\nWith our notes feature, we also discovered how to use local storage in order to save the user's notes as new entries were recorded.\n\u23ed\ufe0f What's next for In Season \u23ed\ufe0f\nThe next \"season\" of In Season will include more customization features, so users can style their UI between spring, summer, fall, and winter themes.\nWe also want to continue developing our account feature so users' unique practice data can be saved and revisited later on.\n\"Practicing is always In Season!\"", "link": "https://devpost.com/software/in-season", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "\ud83d\udca1 inspiration \ud83d\udca1\nwe were inspired by the classical music track, so we wanted to create a project that honored one of our favorite classical composers, antonio vivaldi. vivaldi's \"the four seasons\" is one of his most iconic pieces, so we decided to create a project that used a seasonal theme.\nas musicians ourselves, we also wanted to design our application to tackle real challenges that musicians face when practicing their instruments. accordingly, our goal with in season is to improve a musician's practice productivity, while playing tribute to vivaldi!\n\u2699\ufe0f what it does \u2699\ufe0f\nin season allows the user to register an account and log in under that account every time you practice. once logged in, you can access your practice page. the practice page includes different tools to aid you with your practice:\na posture checker, for gradually improving posture\na timer, for keeping track of practice times\na virtual piano, for seamless tuning\na note taker, for keeping track of practice goals\nall of these features are super simple for the user and are easy to navigate. because of this, the user can have the maximum practice efficiency with the minimum amount of stress.\nin season stands out from traditional music apps because of this wide variety of features. moreover, in season\u2019s timer includes a -----> tree !!!  that changes its foliage every quarter of an hour. this acts as an unconventional visual progress tracker while staying true to the vivaldi theme.\n\ud83d\udee0\ufe0f how we built it \ud83d\udee0\ufe0f\nwe built the front-end with vanilla html, css, and javascript. artwork for the trees was made from scratch using vector art tools. the posture checker is powered by google\u2019s teachable machine and tensorflow.js. the backend is powered by django.\n\ud83d\ude23 challenges we ran into \ud83d\ude23\nwe ran into a ton of challenges on our path to completion. one huge issue for us was communication. one of our teammates was experiencing bad weather for most of the hackathon, thus had trouble communicating with the rest of us. another teammate lived across the globe from all of us, and time zone problems were an issue as well.\nas for code, we had a little bit of trouble with connecting the frontend to the backend. a bug prevented the css files from being rendered so we overcame this by using internal css on our html files.\n\ud83c\udf89 accomplishments that we're proud of \ud83c\udf89\nwe are proud of building a fully functioning application in just under two days!\nwe feel like in season successfully reached our goals of making practice easier for musicians and incorporating \u201cthe four seasons\u201d into our project. we\u2019re happy with our overall ui as well, and we think that our concept is very distinct from pre-existing solutions and very fitting within the theme of classical music.\n\ud83d\udcda what we learned \ud83d\udcda\nwe learned a lot about using javascript functions with dom. as our project was primarily engineered with a focus on front end development, our back end developers also learned how to use the html/css/js stack.\nwith our notes feature, we also discovered how to use local storage in order to save the user's notes as new entries were recorded.\n\u23ed\ufe0f what's next for in season \u23ed\ufe0f\nthe next \"season\" of in season will include more customization features, so users can style their ui between spring, summer, fall, and winter themes.\nwe also want to continue developing our account feature so users' unique practice data can be saved and revisited later on.\n\"practicing is always in season!\"", "sortedWord": "None", "removed": "Nan", "score": 8, "comments": 2, "media": null, "medialink": null, "identifyer": 59500394}, {"Unnamed: 0": 682, "autor": "NFText", "date": null, "content": "Inspiration\nThe core Idea Is to build decentralized studio where people can develop 3D characters and environment together.\nWhat it does\nThis is seed to make the core Idea life. Where anybody, even If they can't paint or do 3D stuff could start from character (or anything) description text, then artist add a drawing based on this text and the 3D artists add 3D models, skeletons, animation loops e.t.c.\nHow we built it\nI used Cosmos Archway because of It's core rewarding developers. It allow to make a model when developers would be motivated develop contracts and interface and make such a big Idea life.\nChallenges we ran into\nAt this moment It's a bit hard to deploy wasm sm-contract to Archway directly from front-end.\nAccomplishments that we're proud of\nThe Ui reflects the core Idea of the project and could be the seed of the big tree.\nWhat we learned\nI leaned a lot about web 3.0, how It's powerful and cool and how It could change the world. At the technical side I learned a lot about react, next.js, rust, ipfs, smart-contracts types (as cw721 * cw20) and more.\nWhat's next for NFText\n1 - With Archway team found a way how to deploy wasm sm-contracts directly from front-end. 2 - Write a new smart-contract based on cw721 with multi-owner ability. 3 - Polish the platform and Invite people for collaborative content creation. 4 - Write an mobile app based on Unity to allow people to interact with the content created at NFText. and beyond ...", "link": "https://devpost.com/software/nftcollab", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\nthe core idea is to build decentralized studio where people can develop 3d characters and environment together.\nwhat it does\nthis is seed to make the core idea life. where anybody, even if they can't paint or do 3d stuff could start from character (or anything) description text, then artist add a drawing based on this text and the 3d artists add 3d models, skeletons, animation loops e.t.c.\nhow we built it\ni used cosmos archway because of it's core rewarding developers. it allow to make a model when developers would be motivated develop contracts and interface and make such a big idea life.\nchallenges we ran into\nat this moment it's a bit hard to deploy wasm sm-contract to archway directly from front-end.\naccomplishments that we're proud of\nthe ui reflects the core idea of the project and could be the seed of the big -----> tree !!! .\nwhat we learned\ni leaned a lot about web 3.0, how it's powerful and cool and how it could change the world. at the technical side i learned a lot about react, next.js, rust, ipfs, smart-contracts types (as cw721 * cw20) and more.\nwhat's next for nftext\n1 - with archway team found a way how to deploy wasm sm-contracts directly from front-end. 2 - write a new smart-contract based on cw721 with multi-owner ability. 3 - polish the platform and invite people for collaborative content creation. 4 - write an mobile app based on unity to allow people to interact with the content created at nftext. and beyond ...", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59500682}, {"Unnamed: 0": 809, "autor": "Skiing Game", "date": null, "content": "Inspiration \ud83d\udca1\nAs we all know, we all enjoy skiing in the winter and always want to go skiing. So, inspired by skiing, I created this game with the added twist of skiing down from Mount Everest owing to an approaching avalanche.\nWhat it does \u2753\nYou must ski down the mountain while avoiding avalanche-created snowballs as well as trees in this game. Your health score will be reduced if you collide with snowballs or trees. Catch flags to get flag points!\nKEYBOARD COMMANDS:\n[<] = ski left\n[<<] = accelerate left\n[>] = ski right\n[>>] = accelerate right\nHOW TO PLAY:\nBegin with a health score of 100\nMove player left and right to avoid trees and snowballs.\nCatch flags for flag points\nThe game re-starts when your health score reaches 0\nHow we built it \ud83d\udd27\ud83d\udd28\nI have use Python framework Pygame and arcade to code this game.\nI created the game without utilizing Classes at first. This allowed me to rapidly practice basic animation ideas, get the game up and running, and see how crucial variables interacted.\nConverting the code to classes, on the other hand, took a long time and required a lot of debugging. Because the ski symbols I saw online were copyrighted, I had to make my own.\nChallenges we ran into \ud83c\udfc3\u200d\u2642\ufe0f\nThere were many challenges which I have faced during making this project\nselecting the collision with the trees was the biggest one and then reduce the health score of the player.\nLike the random generation of snowballs to attack the player\nThe score calculator when the player hits the snowball and tree\nSynchronization of the song in the game\nAccomplishments that we're proud of \ud83c\udfc6\nI am pleased with myself for completing this project inside the deadline. It was quite a tough time to complete this project, but I am really happy with my end results and hope to keep on making games like this in the future.\nWhat we learned \ud83e\udde0\nOverall, learning Pygame took a lot of work, but it was also a lot of fun and fulfilling. Learning about animation, fiddling with the code, creating the icons, finding a good music, and putting it all together was a lot of fun for me. I'm hoping that by sharing my idea, people may be inspired to learn Pygame. In addition, I had a lot of fun working on this, especially the part where I got to play the game!\nWhat's next for Everest Ski \u23ed\nIn the future, I want to improve my game by adding more levels and will try to make it multiplayer so that more players can play it with their friend to compete with each other and have fun.", "link": "https://devpost.com/software/skiing-game", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration \ud83d\udca1\nas we all know, we all enjoy skiing in the winter and always want to go skiing. so, inspired by skiing, i created this game with the added twist of skiing down from mount everest owing to an approaching avalanche.\nwhat it does \u2753\nyou must ski down the mountain while avoiding avalanche-created snowballs as well as trees in this game. your health score will be reduced if you collide with snowballs or trees. catch flags to get flag points!\nkeyboard commands:\n[<] = ski left\n[<<] = accelerate left\n[>] = ski right\n[>>] = accelerate right\nhow to play:\nbegin with a health score of 100\nmove player left and right to avoid trees and snowballs.\ncatch flags for flag points\nthe game re-starts when your health score reaches 0\nhow we built it \ud83d\udd27\ud83d\udd28\ni have use python framework pygame and arcade to code this game.\ni created the game without utilizing classes at first. this allowed me to rapidly practice basic animation ideas, get the game up and running, and see how crucial variables interacted.\nconverting the code to classes, on the other hand, took a long time and required a lot of debugging. because the ski symbols i saw online were copyrighted, i had to make my own.\nchallenges we ran into \ud83c\udfc3\u200d\u2642\ufe0f\nthere were many challenges which i have faced during making this project\nselecting the collision with the trees was the biggest one and then reduce the health score of the player.\nlike the random generation of snowballs to attack the player\nthe score calculator when the player hits the snowball and -----> tree !!! \nsynchronization of the song in the game\naccomplishments that we're proud of \ud83c\udfc6\ni am pleased with myself for completing this project inside the deadline. it was quite a tough time to complete this project, but i am really happy with my end results and hope to keep on making games like this in the future.\nwhat we learned \ud83e\udde0\noverall, learning pygame took a lot of work, but it was also a lot of fun and fulfilling. learning about animation, fiddling with the code, creating the icons, finding a good music, and putting it all together was a lot of fun for me. i'm hoping that by sharing my idea, people may be inspired to learn pygame. in addition, i had a lot of fun working on this, especially the part where i got to play the game!\nwhat's next for everest ski \u23ed\nin the future, i want to improve my game by adding more levels and will try to make it multiplayer so that more players can play it with their friend to compete with each other and have fun.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59500809}, {"Unnamed: 0": 928, "autor": "Summit Restoration, LLC", "date": null, "content": "While other restoration companies shy away from working with insurance providers, Summit Restoration will work with you and your insurance company. Our team is comprised of local technicians who specialize in treating the following types of structural damage: water and flood damage, fire and smoke damage, storm and wind damage, fallen tree removal, mold removal, asbestos abatement, and crime and trauma scene clean up.\n#disastercleanup #expertrestoration #floodcleanup #24/7waterdamagecleanup #disasterrestoration #watercleanup #Emergencywaterdamage #floodcleanup #MoldRemediation #firerestoration\nSummit Restoration, LLC - Location\nAddress :- 13894 S. Bangerter Parkway, Suite 200, Draper UT 84020\nPhone :- 801-679-4111\nServices We Offer\nWater damage restoration service\ndisaster cleanup services\nflood restoration services\nmold removal services\nOur Other Links\n24/7 water damage clean up Draper - https://www.summitut.com/water-flood-damage/\nfire restoration Draper - https://www.summitut.com/fire-smoke-damage/\nmold removal draprer - https://www.summitut.com/mold-removal-company/\nSummit Restoration, LLC - Social Profile\nYoutube - https://www.youtube.com/channel/UCd9iLOxMmy_C7fppgLEHx6w/about\nPinterest - https://www.pinterest.com/SummitRestorationLLC\nTwitter - https://twitter.com/SummitRestorat5\nfacebook.com - https://www.facebook.com/SummitRestorationUT\ninstagram.com - https://www.instagram.com/summitrestorationut/\nlinkedin.com - https://www.linkedin.com/in/summitut/", "link": "https://devpost.com/software/summit-restoration-llc", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "while other restoration companies shy away from working with insurance providers, summit restoration will work with you and your insurance company. our team is comprised of local technicians who specialize in treating the following types of structural damage: water and flood damage, fire and smoke damage, storm and wind damage, fallen -----> tree !!!  removal, mold removal, asbestos abatement, and crime and trauma scene clean up.\n#disastercleanup #expertrestoration #floodcleanup #24/7waterdamagecleanup #disasterrestoration #watercleanup #emergencywaterdamage #floodcleanup #moldremediation #firerestoration\nsummit restoration, llc - location\naddress :- 13894 s. bangerter parkway, suite 200, draper ut 84020\nphone :- 801-679-4111\nservices we offer\nwater damage restoration service\ndisaster cleanup services\nflood restoration services\nmold removal services\nour other links\n24/7 water damage clean up draper - https://www.summitut.com/water-flood-damage/\nfire restoration draper - https://www.summitut.com/fire-smoke-damage/\nmold removal draprer - https://www.summitut.com/mold-removal-company/\nsummit restoration, llc - social profile\nyoutube - https://www.youtube.com/channel/ucd9iloxmmy_c7fppglehx6w/about\npinterest - https://www.pinterest.com/summitrestorationllc\ntwitter - https://twitter.com/summitrestorat5\nfacebook.com - https://www.facebook.com/summitrestorationut\ninstagram.com - https://www.instagram.com/summitrestorationut/\nlinkedin.com - https://www.linkedin.com/in/summitut/", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59500928}, {"Unnamed: 0": 1055, "autor": "Code Scaffolding in Starport with AST Analysis and Mutation", "date": null, "content": "Inspiration\nKnowing how hard it can be to edit code in place, I chose to use two AST libraries written in golang to integrate into starport that allows walking an AST (analysis) and insert new AST nodes into the tree (mutation)\nWhat it does\nInstead of relying on a regular expression matcher, or specially worded comments, we manually walk the AST of a file, looking for functions that we wish to edit, then insert nodes into the tree before writing them back out to disk\nHow we built it\nWrapping the golang DST and protobuf proto libraries, I was able to construct AST builders to make working with each language's AST almost painless.\nChallenges we ran into\nAs a team of one, there was only so much I could get done in the time allotted. Additionally, a large amount of pre-existing code is only slightly different from other \"typed\" templates, and replacing them all would have been long and tedious at first until a final solution with ease of use was divised.\nAdditionally it's been difficult to strike a balance between \"expert level code\" and \"something anyone can pick up and use right away\"\nThere was also the issue of naming conventions and code generation changing in the base starport repository, even up until a few days before the competition ended. At some point I had to make a decision to stop rebasing on top of upstream.\nLastly, there is not silver bullet parsing, querying, and modifying library for golang and protobuf.\nAccomplishments that we're proud of\nBuilding the AST nodes in a quick and concise manner with a nearly WYSIWYG approach to the code (when using the golang AST API is much more prone to errors, and there is no builtin way to do this in protobuf) was very great to have working, especially when it worked for the first time. The approach this project takes can be reused everywhere in starport, but could also be used for general project scaffolding and could possibly even lead to a replacement for an API like yeoman, a generic javascript based code scaffolding tool\nWhat we learned\nThe current way of editing templates with placeholders is easier to see at a glance, however it breaks if someone removes comments from a generated file. With the AST approach, only the expected functions have to exist.\nWhat's next for Code Scaffolding in Starport with AST Analysis and Mutation\nIn addition to helping replace the rest of the AST nodes in scaffold map, singleton, etc. I would like to break out the gocode and protocode packages into a library, as well as fork the two main packages I relied on, integrate them into said library, and also write a way to allow quickly walking and finding items in an AST by using CSS selector style syntax. (NOTE: While there are several libraries that work on the builtin golang compiler's AST library, none of them work on the DST library, which preserves comments)", "link": "https://devpost.com/software/code-scaffolding-in-starport-with-ast-analysis-and-mutation", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\nknowing how hard it can be to edit code in place, i chose to use two ast libraries written in golang to integrate into starport that allows walking an ast (analysis) and insert new ast nodes into the -----> tree !!!  (mutation)\nwhat it does\ninstead of relying on a regular expression matcher, or specially worded comments, we manually walk the ast of a file, looking for functions that we wish to edit, then insert nodes into the tree before writing them back out to disk\nhow we built it\nwrapping the golang dst and protobuf proto libraries, i was able to construct ast builders to make working with each language's ast almost painless.\nchallenges we ran into\nas a team of one, there was only so much i could get done in the time allotted. additionally, a large amount of pre-existing code is only slightly different from other \"typed\" templates, and replacing them all would have been long and tedious at first until a final solution with ease of use was divised.\nadditionally it's been difficult to strike a balance between \"expert level code\" and \"something anyone can pick up and use right away\"\nthere was also the issue of naming conventions and code generation changing in the base starport repository, even up until a few days before the competition ended. at some point i had to make a decision to stop rebasing on top of upstream.\nlastly, there is not silver bullet parsing, querying, and modifying library for golang and protobuf.\naccomplishments that we're proud of\nbuilding the ast nodes in a quick and concise manner with a nearly wysiwyg approach to the code (when using the golang ast api is much more prone to errors, and there is no builtin way to do this in protobuf) was very great to have working, especially when it worked for the first time. the approach this project takes can be reused everywhere in starport, but could also be used for general project scaffolding and could possibly even lead to a replacement for an api like yeoman, a generic javascript based code scaffolding tool\nwhat we learned\nthe current way of editing templates with placeholders is easier to see at a glance, however it breaks if someone removes comments from a generated file. with the ast approach, only the expected functions have to exist.\nwhat's next for code scaffolding in starport with ast analysis and mutation\nin addition to helping replace the rest of the ast nodes in scaffold map, singleton, etc. i would like to break out the gocode and protocode packages into a library, as well as fork the two main packages i relied on, integrate them into said library, and also write a way to allow quickly walking and finding items in an ast by using css selector style syntax. (note: while there are several libraries that work on the builtin golang compiler's ast library, none of them work on the dst library, which preserves comments)", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59501055}, {"Unnamed: 0": 1196, "autor": "SimpliSmart", "date": null, "content": "Inspiration\nMachine learning has a steep learning curve, and even those with experience spend a lot of time prototyping and executing their ideas. Being machine learning engineers, we realized building a custom model pipeline is a gruesome, redundant, and time taking task.\nAfter talking to some senior data scientists, we realized how much of a blocker it is to deploy and test every single model every time they experiment with A/B testing, ensembling, or chaining of machine learning models. There is a necessity to eliminate these redundant tasks for the people working in the industry, so they can focus on what's more important. Even for experienced engineers/data scientists, it's not an easy task to build model pipelines and a near-impossible task for people without any experience in computer science. This leaves a huge void in the industry and academia where educators, students, researchers, entrepreneurs, and enterprises can come in with great ideas but can not, right now, because of the high barrier of entry. So we came up with the idea of building a platform that lets you build your machine learning recipes without writing a single line of code.\nWhat it does\nSimpliSmart is a universal platform that allows anyone to create their machine learning recipes. The platform lets you combine machine learning models into complex data pipelines within minutes, without code, to fulfil your machine learning needs.\nSimpliSmart allows users to build and execute machine learning pipelines for use cases, including but not limited to ensemble modelling, model A/B testing, chaining machine learning models to get desired results. One example could be getting the summary and keywords from the photographs of a book. SimpliSmart creates a Directed Acyclic Graph of tasks in real-time according to the user specification that can be scheduled or manually triggered by the user. This is done by taking the user specifications, breaking them into node tasks, creating a dependency graph, and topologically sorting it to produce the final DAG.\nHow we built it\nWe used Angular to build the UI and Python/Django for the backend server. We leveraged Modzy's machine learning models and SDK to provide a framework for model pipeline creation.\nWe wrote adapters to expose a universal interface for all Modzy machine learning models in our platform. These models can then further be used for dynamic model pipeline generation. We built an easy-to-use UI that makes it intuitive for the end-users to build and visualize complex pipelines irrespective of their level of expertise in machine learning.\nWe used airflow to build, generate, and manage the model pipeline. The unique thing we do here is to build the DAGs (Directed Acyclic Graph) at runtime based on the inputs received by the server. Every task node has three abstract tasks that can be parameterized. First, the node pulls the required data from Airflow XCOM (Airflow cross-communication data store). Second, it processes the required operations/computations on the data. Finally pushes it back to Airflow XCOM. This makes the platform very robust, extensible, and adaptive for any pipeline specifications it may receive.\nChallenges we ran into\nConcretising the nebulous concept of dynamic model pipeline generation into scalable, extensible modules and designing the architecture for the same was an interesting challenge. We had to ideate on the theory of node interaction, dependency generation, and graph creation to make the entire system as robust and resilient as possible.\nOne of the major challenges that we ran into was to build an adaptive and extensible interface to incorporate all the machine learning models available on the Modzy platform and at the same time make it very easy to use and intuitive for the end-user. The solution we came up with was to fetch sample requests/responses for all the available models and represent the dependent parameters in a tree structure to make the chaining of models more intuitive.\nGenerating raw code dynamically in real-time according to the user specifications and pipeline requirements was another challenge that was a critical feature to implement. This alone took a significant amount of time to ideate and implement since this was one of the most sensitive parts of DAG generation and had to be highly robust and stable.\nThe lack of computation resources (single-core machine with 1GB memory) for such a computationally heavy platform proved to be a big challenge for us. We approached this challenge by optimizing configurations for the Airflow task scheduler and webserver. By managing its workers, parallel threads, scheduler heartbeat among other parameters, we improved the performance of the system. We also did some OS-level optimizations in the machine by utilizing disk space as swap memory to compensate for the low volatile-memory.\nWe came across this hackathon only a week ago and were eager to implement our idea in the best possible way. It left us with very little time to ideate, design and execute our idea.\nAccomplishments that we're proud of\nWe are proud to have built:\nA general-purpose platform that lets users easily create model pipelines within a few minutes to best suit their machine learning needs, that would take a good machine learning engineer a few days to build.\nAn easy to use platform that significantly reduces the barrier of entry for creating and using machine learning pipelines, especially for the users not having a strong background in machine learning.\nAn adaptive and extensible platform that already incorporates all the models available on Modzy and can easily consume any new models or their versions when need be.\nA robust and resilient system that can consume any valid user specification, and generate a stable and optimized model pipeline.\nA highly scalable containerized system that can be easily deployed and horizontally/vertically scaled.\nWhat we learned\nWe learned:\nHow to use and leverage Modzy's platform and SDK to provide our users with a diverse range of best-in-class machine learning models.\nThe intricacies of the python interpreter, local context management, variable scoping to dynamically generate resilient code blocks in real-time.\nAirflow in-depth - such as DAG management, task cross-communication and the intricacies of dynamically generating DAGs from a single file.\nHow to leverage traditional graph theory and algorithms for inter-dependency generation and resolution of the actionable tasks.\nHow to make the most out of a minimal computation unit by optimizing OS and web server configurations.\nWhat's next for SimpliSmart\nWe plan to:\nExtend the platform to support custom model training pipelines. This would allow our users to just as easily create their models using their data and leverage them in our inference pipeline generation system.\nAdd extensions for data lakes and make the code of the nodes editable. Introducing this feature will also require a more active validation of the pipelines created by the user.\nMake the UI more intuitive and simpler. Rather than expecting the end-user to learn the platform, we plan to learn from the user's textual description of the pipeline they wish to create. It would require extracting the action items and generating a plan which can be consumed by the system to further generate model pipelines.", "link": "https://devpost.com/software/simplismart", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\nmachine learning has a steep learning curve, and even those with experience spend a lot of time prototyping and executing their ideas. being machine learning engineers, we realized building a custom model pipeline is a gruesome, redundant, and time taking task.\nafter talking to some senior data scientists, we realized how much of a blocker it is to deploy and test every single model every time they experiment with a/b testing, ensembling, or chaining of machine learning models. there is a necessity to eliminate these redundant tasks for the people working in the industry, so they can focus on what's more important. even for experienced engineers/data scientists, it's not an easy task to build model pipelines and a near-impossible task for people without any experience in computer science. this leaves a huge void in the industry and academia where educators, students, researchers, entrepreneurs, and enterprises can come in with great ideas but can not, right now, because of the high barrier of entry. so we came up with the idea of building a platform that lets you build your machine learning recipes without writing a single line of code.\nwhat it does\nsimplismart is a universal platform that allows anyone to create their machine learning recipes. the platform lets you combine machine learning models into complex data pipelines within minutes, without code, to fulfil your machine learning needs.\nsimplismart allows users to build and execute machine learning pipelines for use cases, including but not limited to ensemble modelling, model a/b testing, chaining machine learning models to get desired results. one example could be getting the summary and keywords from the photographs of a book. simplismart creates a directed acyclic graph of tasks in real-time according to the user specification that can be scheduled or manually triggered by the user. this is done by taking the user specifications, breaking them into node tasks, creating a dependency graph, and topologically sorting it to produce the final dag.\nhow we built it\nwe used angular to build the ui and python/django for the backend server. we leveraged modzy's machine learning models and sdk to provide a framework for model pipeline creation.\nwe wrote adapters to expose a universal interface for all modzy machine learning models in our platform. these models can then further be used for dynamic model pipeline generation. we built an easy-to-use ui that makes it intuitive for the end-users to build and visualize complex pipelines irrespective of their level of expertise in machine learning.\nwe used airflow to build, generate, and manage the model pipeline. the unique thing we do here is to build the dags (directed acyclic graph) at runtime based on the inputs received by the server. every task node has three abstract tasks that can be parameterized. first, the node pulls the required data from airflow xcom (airflow cross-communication data store). second, it processes the required operations/computations on the data. finally pushes it back to airflow xcom. this makes the platform very robust, extensible, and adaptive for any pipeline specifications it may receive.\nchallenges we ran into\nconcretising the nebulous concept of dynamic model pipeline generation into scalable, extensible modules and designing the architecture for the same was an interesting challenge. we had to ideate on the theory of node interaction, dependency generation, and graph creation to make the entire system as robust and resilient as possible.\none of the major challenges that we ran into was to build an adaptive and extensible interface to incorporate all the machine learning models available on the modzy platform and at the same time make it very easy to use and intuitive for the end-user. the solution we came up with was to fetch sample requests/responses for all the available models and represent the dependent parameters in a -----> tree !!!  structure to make the chaining of models more intuitive.\ngenerating raw code dynamically in real-time according to the user specifications and pipeline requirements was another challenge that was a critical feature to implement. this alone took a significant amount of time to ideate and implement since this was one of the most sensitive parts of dag generation and had to be highly robust and stable.\nthe lack of computation resources (single-core machine with 1gb memory) for such a computationally heavy platform proved to be a big challenge for us. we approached this challenge by optimizing configurations for the airflow task scheduler and webserver. by managing its workers, parallel threads, scheduler heartbeat among other parameters, we improved the performance of the system. we also did some os-level optimizations in the machine by utilizing disk space as swap memory to compensate for the low volatile-memory.\nwe came across this hackathon only a week ago and were eager to implement our idea in the best possible way. it left us with very little time to ideate, design and execute our idea.\naccomplishments that we're proud of\nwe are proud to have built:\na general-purpose platform that lets users easily create model pipelines within a few minutes to best suit their machine learning needs, that would take a good machine learning engineer a few days to build.\nan easy to use platform that significantly reduces the barrier of entry for creating and using machine learning pipelines, especially for the users not having a strong background in machine learning.\nan adaptive and extensible platform that already incorporates all the models available on modzy and can easily consume any new models or their versions when need be.\na robust and resilient system that can consume any valid user specification, and generate a stable and optimized model pipeline.\na highly scalable containerized system that can be easily deployed and horizontally/vertically scaled.\nwhat we learned\nwe learned:\nhow to use and leverage modzy's platform and sdk to provide our users with a diverse range of best-in-class machine learning models.\nthe intricacies of the python interpreter, local context management, variable scoping to dynamically generate resilient code blocks in real-time.\nairflow in-depth - such as dag management, task cross-communication and the intricacies of dynamically generating dags from a single file.\nhow to leverage traditional graph theory and algorithms for inter-dependency generation and resolution of the actionable tasks.\nhow to make the most out of a minimal computation unit by optimizing os and web server configurations.\nwhat's next for simplismart\nwe plan to:\nextend the platform to support custom model training pipelines. this would allow our users to just as easily create their models using their data and leverage them in our inference pipeline generation system.\nadd extensions for data lakes and make the code of the nodes editable. introducing this feature will also require a more active validation of the pipelines created by the user.\nmake the ui more intuitive and simpler. rather than expecting the end-user to learn the platform, we plan to learn from the user's textual description of the pipeline they wish to create. it would require extracting the action items and generating a plan which can be consumed by the system to further generate model pipelines.", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 3, "media": null, "medialink": null, "identifyer": 59501196}, {"Unnamed: 0": 1310, "autor": "NFT Trees", "date": null, "content": "Inspiration\nThere are many NFT projects that bring the community together. Not so many directly for a \"good cause\".\nWhat it does\nAllows people to invest funds into a NFT represented by a tree illustration. These funds are then invested in a DeFi application which accrue interest over time. The owner after a given period of time is allowed to withdraw funds and or donate to a list of approved 501 non-profit organizations.\nHow we built it\nRemix for the smart contracts with Moralis for the frontend. BentoBox handles the DeFi aspects. Connect to Polygon network. Moralis is hosting the images.\nChallenges we ran into\nConnecting BentoBox with our NFT account holders.\nAccomplishments that we're proud of\nConnecting to Opensea and seeing the artwork in the marketplace.\nWhat we learned\nLearning all the features of the ERC-1155 token\nWhat's next for NFT Trees\nCreating custom artwork and connecting BentoBox correctly.", "link": "https://devpost.com/software/nft-trees-ojnv54", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\nthere are many nft projects that bring the community together. not so many directly for a \"good cause\".\nwhat it does\nallows people to invest funds into a nft represented by a -----> tree !!!  illustration. these funds are then invested in a defi application which accrue interest over time. the owner after a given period of time is allowed to withdraw funds and or donate to a list of approved 501 non-profit organizations.\nhow we built it\nremix for the smart contracts with moralis for the frontend. bentobox handles the defi aspects. connect to polygon network. moralis is hosting the images.\nchallenges we ran into\nconnecting bentobox with our nft account holders.\naccomplishments that we're proud of\nconnecting to opensea and seeing the artwork in the marketplace.\nwhat we learned\nlearning all the features of the erc-1155 token\nwhat's next for nft trees\ncreating custom artwork and connecting bentobox correctly.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59501310}, {"Unnamed: 0": 1523, "autor": "Predicting Heart Disease", "date": null, "content": "Inspiration\nFrom the beginning of the course, I very well understood the huge impact that ML and AI could have on the medical field. My grandfather is a heavy smoker and since I was small, we always experienced those tragic moments of fear where he rushed him to the hospital due to having a heart malfunction. Each time, the doctors would explain that what happens was a heart disease and that quarterly checkups need to be made on his heart to ensure stability of the condition. However, the process was hard to follow, was extremely costly, and contained a lot of human error (due to doctors not able to detect heart disease). Thus, I decided that I wanted to create a ML project that would provide a somewhat adequate solution to this issue.\nWhat it does\nThe aim of the project is to be able to predict whether a patient has heart disease or not. The project is intended to be used by doctors as most of the inputs consist of the medical data of the patient that are observed by a doctor following having a patient\u2019s checkup report. There are many types of heart disease, and sometimes detecting them given the patient report and his symptoms may be troublesome for the doctor without further scans and reports that are mostly very costly. Even though the program is not 100% accurate, it is able to give a somewhat close to true prediction of the result of the patient.\nHow we built it\nI used the UCI Heart Disease dataset from kaggle in order to train and test my model. I started off by loading the data and preparing the data to be used in the models I will be testing. I attempted to solve this problem using two different approaches, one was random forest classifiers and the other was decision trees with bagging. For both models, I performed hyperparameter tuning using 5-fold cross validation, which helped reduce overfitting and improve generalization accuracy. Both models were performing extremely well with both achieving 85%+ accuracy in most runs, but I decided to use the random forest classifier as it was achieving better accuracies and better stability during all runs on average. Another reason I selected the random forest is because it gives a better bias-variance tradeoff than bagging decision tree, and with faster computation time. The randomness in the forests decreases the correlation between decision trees in the forest, which results in decreasing the effects of overfitting and noise in the data.\nChallenges we ran into\nOne of the challenges I faced was that two columns in my dataset had about 30% of their values to be N/A, and the possibility of removing such columns was not an option as after creating a correlation matrix between those columns and the final prediction, it was clear to see that there was a noticeable correlation between those columns and the label to be predicted. The way I solved this issue was by filling the N/A values with averages/modes (based on the column if it was categorical or continuous) of the column values we have for each stage of heart disease. The result of this step was substantially improving the accuracy results for the Random Forest model while keeping the correlation between the columns and the data the nearly similar.\nAccomplishments that we're proud of\nI am very proud that I was able to get a very high accuracy result in predicting heart disease. I was able to achieve a maximum accuracy of 90% on a test set that consists of 10% of the training data. This result was achieved by the random forest classifier model that I trained.\nWhat we learned\nThis project was an extremely fruitful experience and is surely one of the most important building blocks in my ML career. This project helped me understand the way I should approach any task that has minimal instructions/restrictions, and how a ML project is always a systematic process that starts with data processing and ends with training and testing a model. In addition, I was able to appreciate the true meaning of data and be able to understand and utilize some ideas like correlation, reproducibility, etc. I truly understood what data is and how I could assess the quality of the data I am using in my model. In addition, I got a very powerful insight on how to make the choice of which model to use, and when it is best to utilize a certain ML model.\nWhat's next for Predicting Heart Disease\nAfter looking at the confusion matrix of the training data, I recognized that the model was doing a very good job in predicting no disease, stage 1, and stage 2 heart disease. However, the model is not always consistent with the stages 3 and 4 and often the accuracy of those classes deviates. This is because classes 0 (no heart disease), stage 1, and stage 2 have lot of instances in the training and test set, however those from classes 3 and 4 are very less frequent in both sets as they are more rare cases in patients. So one way I would look into to improve the project is to find a way to solve the issue of having class imbalance in the training data, which would hopefully solve the issue we are facing. Also, the other way I look into improving the results is through introducing a neural network model and experiment with the results I will obtain from such a model.", "link": "https://devpost.com/software/predicting-heart-disease", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\nfrom the beginning of the course, i very well understood the huge impact that ml and ai could have on the medical field. my grandfather is a heavy smoker and since i was small, we always experienced those tragic moments of fear where he rushed him to the hospital due to having a heart malfunction. each time, the doctors would explain that what happens was a heart disease and that quarterly checkups need to be made on his heart to ensure stability of the condition. however, the process was hard to follow, was extremely costly, and contained a lot of human error (due to doctors not able to detect heart disease). thus, i decided that i wanted to create a ml project that would provide a somewhat adequate solution to this issue.\nwhat it does\nthe aim of the project is to be able to predict whether a patient has heart disease or not. the project is intended to be used by doctors as most of the inputs consist of the medical data of the patient that are observed by a doctor following having a patient\u2019s checkup report. there are many types of heart disease, and sometimes detecting them given the patient report and his symptoms may be troublesome for the doctor without further scans and reports that are mostly very costly. even though the program is not 100% accurate, it is able to give a somewhat close to true prediction of the result of the patient.\nhow we built it\ni used the uci heart disease dataset from kaggle in order to train and test my model. i started off by loading the data and preparing the data to be used in the models i will be testing. i attempted to solve this problem using two different approaches, one was random forest classifiers and the other was decision trees with bagging. for both models, i performed hyperparameter tuning using 5-fold cross validation, which helped reduce overfitting and improve generalization accuracy. both models were performing extremely well with both achieving 85%+ accuracy in most runs, but i decided to use the random forest classifier as it was achieving better accuracies and better stability during all runs on average. another reason i selected the random forest is because it gives a better bias-variance tradeoff than bagging decision -----> tree !!! , and with faster computation time. the randomness in the forests decreases the correlation between decision trees in the forest, which results in decreasing the effects of overfitting and noise in the data.\nchallenges we ran into\none of the challenges i faced was that two columns in my dataset had about 30% of their values to be n/a, and the possibility of removing such columns was not an option as after creating a correlation matrix between those columns and the final prediction, it was clear to see that there was a noticeable correlation between those columns and the label to be predicted. the way i solved this issue was by filling the n/a values with averages/modes (based on the column if it was categorical or continuous) of the column values we have for each stage of heart disease. the result of this step was substantially improving the accuracy results for the random forest model while keeping the correlation between the columns and the data the nearly similar.\naccomplishments that we're proud of\ni am very proud that i was able to get a very high accuracy result in predicting heart disease. i was able to achieve a maximum accuracy of 90% on a test set that consists of 10% of the training data. this result was achieved by the random forest classifier model that i trained.\nwhat we learned\nthis project was an extremely fruitful experience and is surely one of the most important building blocks in my ml career. this project helped me understand the way i should approach any task that has minimal instructions/restrictions, and how a ml project is always a systematic process that starts with data processing and ends with training and testing a model. in addition, i was able to appreciate the true meaning of data and be able to understand and utilize some ideas like correlation, reproducibility, etc. i truly understood what data is and how i could assess the quality of the data i am using in my model. in addition, i got a very powerful insight on how to make the choice of which model to use, and when it is best to utilize a certain ml model.\nwhat's next for predicting heart disease\nafter looking at the confusion matrix of the training data, i recognized that the model was doing a very good job in predicting no disease, stage 1, and stage 2 heart disease. however, the model is not always consistent with the stages 3 and 4 and often the accuracy of those classes deviates. this is because classes 0 (no heart disease), stage 1, and stage 2 have lot of instances in the training and test set, however those from classes 3 and 4 are very less frequent in both sets as they are more rare cases in patients. so one way i would look into to improve the project is to find a way to solve the issue of having class imbalance in the training data, which would hopefully solve the issue we are facing. also, the other way i look into improving the results is through introducing a neural network model and experiment with the results i will obtain from such a model.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59501523}, {"Unnamed: 0": 1605, "autor": "Cozy Reef", "date": null, "content": "Inspiration\nCozy Reef is a NFT project inspired by Squid Game x Happy Tree Friends where players compete in seasonal elimination games in order to join the Cozy Reef Killers, the organization that runs the games.\nThe team, with several decades of combined game industry experience, is excited about bringing unique and engaging gaming experiences that fully embrace the advantages of blockchain. Our current understanding is that many blockchain games heavily rely on centralized components, and fully on-chain games are commonly defi protocols with UI and art overlays.\nWe see an opportunity to develop gaming experiences that are truly decentralized, reward players for their achievements, and focus on the joys of gaming above all else. We recognize that Chainlink oracles play a key role in truly decentralizing game state and management, potentially becoming part of the industry standard technology stack for blockchain game development.\nWhat it does\nCheckout the white paper to learn more about the project.\nThe core loop for the Cozy Reef project is as follows:\nPlayers mint a 1 of 1 unique NFT from the Cozy Reef ERC721 contract on Ethereum, containing 10,000 tokens.\nPlayers then mint an equivalent one time game pass ERC721 token on Polygon through a proof of reserves contract call to validate token ownership that is powered by Chainlink oracles.\nPlayers gain access to Cozy Reef games that are deployed on Polygon, and play through a web client and Metamask. The game runner is driven by Chainlink Keepers and certain game outcomes are driven by Chainlink VRF.\nPlayers who survive the entire game season receive a 1 of 1 Cozy Reef Killer mask NFT. Players can choose to use the mask and the base NFT to mint a Cozy Reef Killer or sell the mask.\nDevelopment of Cozy Reef began at the start of the hackathon. At the time of the hackathon submission, the current state of the project is as follows:\n[Live] Website - landing page with information on the project, links to the white paper, blog, and social media accounts.\n[Live] Honorary NFTs - Honorary NFT ERC721 contract deployed to Ethereum mainnet with several tokens minted to thank project advisors.\n[Live] white paper - whitepaper that discusses project direction, challenges, and future work.\n[Alpha] Slingshot Sailors - Slingshot Sailors, the first elimination game of Cozy Reef season one. We ran two public Alpha play tests with 25 players. We will continue to run playtests as we continue to improve the game. ( Alpha 1 contract). (Help Guide)\n[No e2e test] 10k NFT Art Project - art generation from hundreds of attributes, contract, and supporting toolchain.\n[Demo] NFT Proof of Reserves - Contracts and Chainlink infrastructure to validate token ownership on Ethereum to mint an equivalent token on Polygon.\nHow we built it\nWebsite\nThe website is built as a React app in Typescript, using Github Actions as our CI/CD pipeline and deployed in AWS. All infrastructure is managed within a Terraform project.\nHonorary NFTs\nThe Honorary NFTs are ERC721 tokens on Ethereum mainnet pointing to metadata and art hosted on Arweave. The development and testing of the contract is done through Hardhat and Ethers.js. The project state is automatically written to a metadata.json file with extensive validations in the toolchain to ensure each NFT is only minted once and contains the correct image/metadata/recipient.\nSlingshot Sailors\nThe contracts for Slingshot Sailors are written in Solidity, with Metamask integration on the web app. Contracts use OpenZeppelin base contracts and Hardhat / Chai for development and testing. The game UI is built in React with custom art assets with state fully managed on-chain and Chainlink Keepers as the game runner. Specific values shown on the game UI are derived through lazy evaluation of state calls through view functions.\nNFT Art Pipeline\nWe built a configurable Javascript tool to generate 10,000 unique and random NFT art over the hundreds of features drawn by our artist. This allows us to quickly iterate on the art as we hone in on the look and feel of Cozy Reef.\nNFT Proof of Reserves\nWe built a prototype of a proof of reserves service to validate NFT token ownership on Ethereum and mint an equivalent one-time game token on Polygon. The core service is a Chainlink oracle with a View-Function external adapter, with dummy NFT and custodian request contracts for testing. The token owner calls verify to the custodian contract on Polygon with the tokenId they own on Ethereum and wish to mint, and a direct request Chainlink job event is emitted. The Chainlink oracle triggers the View-Function external adapter to call ownerOf on the ERC721 contract on Ethereum, and validates the token ownership in order for the custodian contract to mint a one-time game token on Polygon.\nChallenges we ran into\nStoring game state without exorbitant gas prices\nOptimizing CPU and memory performance is commonplace in game development, however these optimizations become critical to the viability of a game when developing a fully on-chain experience. Players cannot be expected to pay dollars, let alone 10s to 100s of dollars for a single action. Similarly, keeper upkeep must be kept to a reasonable cost to ensure the game is cost effective to run. Storing less state is an option, but cutting state often means cutting features, which can lead to an uninteresting game loop.\nFun game experiences\nDesigning games for blockchain comes with its fair share of constraints.\nStoring State is expensive, any mechanic that requires additional storage, has to be carefully designed to ensure the game remains within our gas cost budgets.\nPlayers can\u2019t communicate with the game authority (ie. the chain) in real time, nor will players receive information about state changes at the same time. This means that game mechanics can\u2019t rely on fast user action, or any quick-time events.\nNo internal state on the authority can be hidden from other players without extra logic (and increased gas costs). A simple game of rock-paper-scissors isn\u2019t so simple when you can\u2019t hide the first person\u2019s choice from the second. Games either have to be designed to handle each player having full knowledge of the state, or pay the complexity and gas cost of adding hidden state.\nIntegrating Chainlink technology\nChainlink technology is critical in the Cozy Reef platform, with VRFs and Keepers acting as the driving force behind game state and management. We also leverage External Adapters to do cross chain proof of reserves. We ran into several challenges setting up Chainlink infrastructure, mainly with finding very specific documentation as well as trouble shooting bugs with limited logs. Some examples of issues we ran into:\nSetting the ORACLE_PAYMENT values and not understanding the implications of MINIMUM_CONTRACT_PAYMNENT_LINK_JUELS (job does not run)\nTroubleshooting silent VRF failures, later realizing that we exceeded the gas limit\nTroubleshooting Keepers not polling our contract and learning that we need to add ownership rights to the Keeper addresses .\nAccomplishments that we're proud of\nAlpha Tests\nWe ran two Alpha tests in the final week of the Hackathon and played \u201cSlingshot Sailors\u201d with 25 players around the world. We received positive and encouraging feedback, as well as many suggestions we plan to implement as we work towards beta. Slingshot Sailors uses Chainlink VRF to determine outcomes of the slings and Chainlink Keepers as the game runner.\nNFT Project\nWe\u2019re really excited about the duality of the Cozy Reef guest and the Cozy Reef Killer, along with their respective art direction. In addition to having all the assets to generate 10,000 unique characters, we deployed the honorary contract to mainnet and began minting custom NFTs to members of the community who are advising and spreading the word on.\nWhat we learned\nComing into this hackathon, most of the engineers on the team had little or no experience with the blockchain technical ecosystem, so everything we\u2019ve done has been new for us. Everything from setting up our dapp to integrate with metamask, creating a quick iteration loop locally with hardhat and waffle for testing our game and contract, and integrating with Chainlink technology all introduced complexities that we\u2019ve dealt with over the course of the hackathon.\nNevertheless, here are some of the key insights that we\u2019ve gained after working in this space for the past month. Feel free to check out our whitepaper for a much more in depth breakdown.\nGetting around high gas usage\nOne of the largest challenges around building games on chain is dealing with high gas usage and high gas prices. To get around high gas prices, the only thing we can do is move to a cheaper side chain such as Polygon. However, reducing gas usage is also important to decrease costs in general.\nWe try to have each transaction in our game run with a space complexity of O(1). In order to avoid scaling gas fees, we avoid storing player state and opt to store player inputs. Calculating player state dynamically after the fact allows us to avoid state update transactions that scale linearly according to the number of players, which significantly reduces our gas usage.\nDesigning fun blockchain games\nWe\u2019ve listed some blockchain game limitations in the section above covering challenges we\u2019ve faced. In general, we try to follow these core tenants:\nGames must allow high player agency.\nGames must require skill to win over pure luck.\nPlayer to player interactions must be high, since all information is public and fast paced interactions do not exist.\nIn general, what we\u2019ve learned is that the best on chain games at the moment must be turn based. In addition, games that rely on perfect information work best, which means that the best blockchain based games mimic common board games. The advantages that blockchains provide is a decentralized backend for these games, reducing technical complexity in areas such as scaling and distribution by limiting the capabilities of what the game can achieve.\nFast Iteration Loops\nSeveral tools help us create fast iteration loops and made development of our dapp and game faster:\nHardhat + waffle provided a solid testing framework for our contracts and enabled a lot of scripting that allowed us to quickly bootstrap our dapp with a local blockchain for development\nTypescript + typechain + language servers made the entire development experience much easier, reducing entire classes of errors that one can make with raw javascript such as calling contract methods with the right data types.\nmocking out Chainlink contracts properly allowed us to test things such as VRFs and Keepers to make sure everything looks right locally.\nNextJS, GitHub actions, and Terraform removed nearly all the boilerplate around developing and deploying a React based app. NextJS provided all the tooling to compile and optimize our application. GitHub actions produced an efficient and free CI/CD pipeline. Terraform made producing new environments very quick and easy, allowing us to focus on application development.\nGetting a roadmap out early\nWe got a roadmap out early and publicly on our website. This not only pushed us forward to deliver but also kept us honest about what work we had left. This seems like a trivial thing to do but was an important part to keeping us moving forward even when faced with difficult challenges throughout the hackathon.\nWhat's next for Cozy Reef\nSlingshot Sailors Beta\nThe next milestone for the project is developing the beta based off of the feedback we received from the Alpha test. The main improvements we plan to make and iterate over with our project supporters:\nMusic and SFX - We will add music and sound effects to the game to create a more engaging environment and provide audio cues on state transitions and round outcomes.\nUI polish - we will improve our UI for more intuitive visual indicators. The most requested addition to the UI is a scoreboard, which we will explore showing with ENS names. Other UI improvements we intend to add are animations in the game.\nStun mechanic - currently if the slingshot a player is on snaps, they are unable to make a move for a full round. We\u2019ve received enough feedback that this is frustrating, and are going to revise the penalty system that still encourages players to engage.\nERC20 Game Token\nWe plan to release an ERC20 game token (\u201cCozy Coins\u201d) as part of the Cozy Reef ecosystem. While the ultimate reward is for players to survive a season to an end to join the Cozy Reef Killers, players will earn Cozy Coins based on how well they do in each game. This will reward players for their achievements and engagement while setting the foundations for us to build out a long lasting game ecosystem.\nLaunch the Cozy Reef Project\nWhen we are confident in \u201cSlingshot Sailors\u201d, the first game of the season, we will launch the Cozy Reef project, consisting of 10,000 ERC721 tokens on Ethereum to mint. Shortly after the minting event we will launch \u201cSlingshot Sailors\u201d on Polygon, along with the proof of reserves for players to validate their ownership for a game token.\nSeason One\nWe intend to run at least three elimination games for season one of Cozy Reef. We\u2019ve begun ideating the remaining games beyond Slingshot Sailors that explore new game mechanics that take advantage of blockchain and Chainlink technology. We will take our learnings and reusable components from Slingshot Sailors to bring the next series of games to players sooner.\nCozy Reef Killers\nPlayers who survive the entire season of games will be airdropped a unique Cozy Reef Killer mask (ERC721). They can then choose to sell the mask, or use the mask and a Cozy Reef NFT to mint a Cozy Reef Killer NFT. We already have begun ideation on the art for the masks and will need to do additional work on the Cozy Reef Killers NFT contract to support the minting process.", "link": "https://devpost.com/software/cozy-reef", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\ncozy reef is a nft project inspired by squid game x happy -----> tree !!!  friends where players compete in seasonal elimination games in order to join the cozy reef killers, the organization that runs the games.\nthe team, with several decades of combined game industry experience, is excited about bringing unique and engaging gaming experiences that fully embrace the advantages of blockchain. our current understanding is that many blockchain games heavily rely on centralized components, and fully on-chain games are commonly defi protocols with ui and art overlays.\nwe see an opportunity to develop gaming experiences that are truly decentralized, reward players for their achievements, and focus on the joys of gaming above all else. we recognize that chainlink oracles play a key role in truly decentralizing game state and management, potentially becoming part of the industry standard technology stack for blockchain game development.\nwhat it does\ncheckout the white paper to learn more about the project.\nthe core loop for the cozy reef project is as follows:\nplayers mint a 1 of 1 unique nft from the cozy reef erc721 contract on ethereum, containing 10,000 tokens.\nplayers then mint an equivalent one time game pass erc721 token on polygon through a proof of reserves contract call to validate token ownership that is powered by chainlink oracles.\nplayers gain access to cozy reef games that are deployed on polygon, and play through a web client and metamask. the game runner is driven by chainlink keepers and certain game outcomes are driven by chainlink vrf.\nplayers who survive the entire game season receive a 1 of 1 cozy reef killer mask nft. players can choose to use the mask and the base nft to mint a cozy reef killer or sell the mask.\ndevelopment of cozy reef began at the start of the hackathon. at the time of the hackathon submission, the current state of the project is as follows:\n[live] website - landing page with information on the project, links to the white paper, blog, and social media accounts.\n[live] honorary nfts - honorary nft erc721 contract deployed to ethereum mainnet with several tokens minted to thank project advisors.\n[live] white paper - whitepaper that discusses project direction, challenges, and future work.\n[alpha] slingshot sailors - slingshot sailors, the first elimination game of cozy reef season one. we ran two public alpha play tests with 25 players. we will continue to run playtests as we continue to improve the game. ( alpha 1 contract). (help guide)\n[no e2e test] 10k nft art project - art generation from hundreds of attributes, contract, and supporting toolchain.\n[demo] nft proof of reserves - contracts and chainlink infrastructure to validate token ownership on ethereum to mint an equivalent token on polygon.\nhow we built it\nwebsite\nthe website is built as a react app in typescript, using github actions as our ci/cd pipeline and deployed in aws. all infrastructure is managed within a terraform project.\nhonorary nfts\nthe honorary nfts are erc721 tokens on ethereum mainnet pointing to metadata and art hosted on arweave. the development and testing of the contract is done through hardhat and ethers.js. the project state is automatically written to a metadata.json file with extensive validations in the toolchain to ensure each nft is only minted once and contains the correct image/metadata/recipient.\nslingshot sailors\nthe contracts for slingshot sailors are written in solidity, with metamask integration on the web app. contracts use openzeppelin base contracts and hardhat / chai for development and testing. the game ui is built in react with custom art assets with state fully managed on-chain and chainlink keepers as the game runner. specific values shown on the game ui are derived through lazy evaluation of state calls through view functions.\nnft art pipeline\nwe built a configurable javascript tool to generate 10,000 unique and random nft art over the hundreds of features drawn by our artist. this allows us to quickly iterate on the art as we hone in on the look and feel of cozy reef.\nnft proof of reserves\nwe built a prototype of a proof of reserves service to validate nft token ownership on ethereum and mint an equivalent one-time game token on polygon. the core service is a chainlink oracle with a view-function external adapter, with dummy nft and custodian request contracts for testing. the token owner calls verify to the custodian contract on polygon with the tokenid they own on ethereum and wish to mint, and a direct request chainlink job event is emitted. the chainlink oracle triggers the view-function external adapter to call ownerof on the erc721 contract on ethereum, and validates the token ownership in order for the custodian contract to mint a one-time game token on polygon.\nchallenges we ran into\nstoring game state without exorbitant gas prices\noptimizing cpu and memory performance is commonplace in game development, however these optimizations become critical to the viability of a game when developing a fully on-chain experience. players cannot be expected to pay dollars, let alone 10s to 100s of dollars for a single action. similarly, keeper upkeep must be kept to a reasonable cost to ensure the game is cost effective to run. storing less state is an option, but cutting state often means cutting features, which can lead to an uninteresting game loop.\nfun game experiences\ndesigning games for blockchain comes with its fair share of constraints.\nstoring state is expensive, any mechanic that requires additional storage, has to be carefully designed to ensure the game remains within our gas cost budgets.\nplayers can\u2019t communicate with the game authority (ie. the chain) in real time, nor will players receive information about state changes at the same time. this means that game mechanics can\u2019t rely on fast user action, or any quick-time events.\nno internal state on the authority can be hidden from other players without extra logic (and increased gas costs). a simple game of rock-paper-scissors isn\u2019t so simple when you can\u2019t hide the first person\u2019s choice from the second. games either have to be designed to handle each player having full knowledge of the state, or pay the complexity and gas cost of adding hidden state.\nintegrating chainlink technology\nchainlink technology is critical in the cozy reef platform, with vrfs and keepers acting as the driving force behind game state and management. we also leverage external adapters to do cross chain proof of reserves. we ran into several challenges setting up chainlink infrastructure, mainly with finding very specific documentation as well as trouble shooting bugs with limited logs. some examples of issues we ran into:\nsetting the oracle_payment values and not understanding the implications of minimum_contract_paymnent_link_juels (job does not run)\ntroubleshooting silent vrf failures, later realizing that we exceeded the gas limit\ntroubleshooting keepers not polling our contract and learning that we need to add ownership rights to the keeper addresses .\naccomplishments that we're proud of\nalpha tests\nwe ran two alpha tests in the final week of the hackathon and played \u201cslingshot sailors\u201d with 25 players around the world. we received positive and encouraging feedback, as well as many suggestions we plan to implement as we work towards beta. slingshot sailors uses chainlink vrf to determine outcomes of the slings and chainlink keepers as the game runner.\nnft project\nwe\u2019re really excited about the duality of the cozy reef guest and the cozy reef killer, along with their respective art direction. in addition to having all the assets to generate 10,000 unique characters, we deployed the honorary contract to mainnet and began minting custom nfts to members of the community who are advising and spreading the word on.\nwhat we learned\ncoming into this hackathon, most of the engineers on the team had little or no experience with the blockchain technical ecosystem, so everything we\u2019ve done has been new for us. everything from setting up our dapp to integrate with metamask, creating a quick iteration loop locally with hardhat and waffle for testing our game and contract, and integrating with chainlink technology all introduced complexities that we\u2019ve dealt with over the course of the hackathon.\nnevertheless, here are some of the key insights that we\u2019ve gained after working in this space for the past month. feel free to check out our whitepaper for a much more in depth breakdown.\ngetting around high gas usage\none of the largest challenges around building games on chain is dealing with high gas usage and high gas prices. to get around high gas prices, the only thing we can do is move to a cheaper side chain such as polygon. however, reducing gas usage is also important to decrease costs in general.\nwe try to have each transaction in our game run with a space complexity of o(1). in order to avoid scaling gas fees, we avoid storing player state and opt to store player inputs. calculating player state dynamically after the fact allows us to avoid state update transactions that scale linearly according to the number of players, which significantly reduces our gas usage.\ndesigning fun blockchain games\nwe\u2019ve listed some blockchain game limitations in the section above covering challenges we\u2019ve faced. in general, we try to follow these core tenants:\ngames must allow high player agency.\ngames must require skill to win over pure luck.\nplayer to player interactions must be high, since all information is public and fast paced interactions do not exist.\nin general, what we\u2019ve learned is that the best on chain games at the moment must be turn based. in addition, games that rely on perfect information work best, which means that the best blockchain based games mimic common board games. the advantages that blockchains provide is a decentralized backend for these games, reducing technical complexity in areas such as scaling and distribution by limiting the capabilities of what the game can achieve.\nfast iteration loops\nseveral tools help us create fast iteration loops and made development of our dapp and game faster:\nhardhat + waffle provided a solid testing framework for our contracts and enabled a lot of scripting that allowed us to quickly bootstrap our dapp with a local blockchain for development\ntypescript + typechain + language servers made the entire development experience much easier, reducing entire classes of errors that one can make with raw javascript such as calling contract methods with the right data types.\nmocking out chainlink contracts properly allowed us to test things such as vrfs and keepers to make sure everything looks right locally.\nnextjs, github actions, and terraform removed nearly all the boilerplate around developing and deploying a react based app. nextjs provided all the tooling to compile and optimize our application. github actions produced an efficient and free ci/cd pipeline. terraform made producing new environments very quick and easy, allowing us to focus on application development.\ngetting a roadmap out early\nwe got a roadmap out early and publicly on our website. this not only pushed us forward to deliver but also kept us honest about what work we had left. this seems like a trivial thing to do but was an important part to keeping us moving forward even when faced with difficult challenges throughout the hackathon.\nwhat's next for cozy reef\nslingshot sailors beta\nthe next milestone for the project is developing the beta based off of the feedback we received from the alpha test. the main improvements we plan to make and iterate over with our project supporters:\nmusic and sfx - we will add music and sound effects to the game to create a more engaging environment and provide audio cues on state transitions and round outcomes.\nui polish - we will improve our ui for more intuitive visual indicators. the most requested addition to the ui is a scoreboard, which we will explore showing with ens names. other ui improvements we intend to add are animations in the game.\nstun mechanic - currently if the slingshot a player is on snaps, they are unable to make a move for a full round. we\u2019ve received enough feedback that this is frustrating, and are going to revise the penalty system that still encourages players to engage.\nerc20 game token\nwe plan to release an erc20 game token (\u201ccozy coins\u201d) as part of the cozy reef ecosystem. while the ultimate reward is for players to survive a season to an end to join the cozy reef killers, players will earn cozy coins based on how well they do in each game. this will reward players for their achievements and engagement while setting the foundations for us to build out a long lasting game ecosystem.\nlaunch the cozy reef project\nwhen we are confident in \u201cslingshot sailors\u201d, the first game of the season, we will launch the cozy reef project, consisting of 10,000 erc721 tokens on ethereum to mint. shortly after the minting event we will launch \u201cslingshot sailors\u201d on polygon, along with the proof of reserves for players to validate their ownership for a game token.\nseason one\nwe intend to run at least three elimination games for season one of cozy reef. we\u2019ve begun ideating the remaining games beyond slingshot sailors that explore new game mechanics that take advantage of blockchain and chainlink technology. we will take our learnings and reusable components from slingshot sailors to bring the next series of games to players sooner.\ncozy reef killers\nplayers who survive the entire season of games will be airdropped a unique cozy reef killer mask (erc721). they can then choose to sell the mask, or use the mask and a cozy reef nft to mint a cozy reef killer nft. we already have begun ideation on the art for the masks and will need to do additional work on the cozy reef killers nft contract to support the minting process.", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59501605}, {"Unnamed: 0": 1648, "autor": "Crypto Plants: Save the planet with awesome NFTs!", "date": null, "content": "The problem Crypto Plants: Save the planet with awesome NFTs! solves\nCryptoPlants is a crowd-funding platform for environmental and social causes. It works towards SDG 13- partnerships for the goals. Any organization working on SDG 1-17 (food, health, education, climate action, etc) can join our DAO.\nUsers earn CryptoPlants by buying 'seeds' from listed NGOs. Each seed sponsors a specific cause like food donations or aforestation. CryptoPlants are unique NFTs that can be gifted, traded or sold. The idea is to gamify donations and turn them into status and currency.\nBenefits\nFor users\nShow off your CryptoPlants and environmental impact to the world\nCollect rare trees and resell. Every plant is unique\nOffset company carbon emissions\nTransparency on fund usage\nFor NGOs\n1 . NGOs spend 10-15% of their budgets on donation-seeking ads. Conversion rate is low since donations are philantropic acts. We will bring better returns by turning donations into cypto-collectables. People splurge thousands on digital kittens\nJoin the CryptoPlants DAO and get decision making power\nFuture scope\nTop donors' leaderboard.\nBecome a gifting platform. Many hackathons give away free t-shirts, but many users don't need them (we ourselves have some 15 swag shirts). Shipping shirts across continents is not very environment friendly. Instead let them opt for a free CryptoPlant! HacktoberFest did this, letting users opt for planting a real tree on tree-nation.org", "link": "https://devpost.com/software/crypto-plants-save-the-planet-with-awesome-nfts", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "the problem crypto plants: save the planet with awesome nfts! solves\ncryptoplants is a crowd-funding platform for environmental and social causes. it works towards sdg 13- partnerships for the goals. any organization working on sdg 1-17 (food, health, education, climate action, etc) can join our dao.\nusers earn cryptoplants by buying 'seeds' from listed ngos. each seed sponsors a specific cause like food donations or aforestation. cryptoplants are unique nfts that can be gifted, traded or sold. the idea is to gamify donations and turn them into status and currency.\nbenefits\nfor users\nshow off your cryptoplants and environmental impact to the world\ncollect rare trees and resell. every plant is unique\noffset company carbon emissions\ntransparency on fund usage\nfor ngos\n1 . ngos spend 10-15% of their budgets on donation-seeking ads. conversion rate is low since donations are philantropic acts. we will bring better returns by turning donations into cypto-collectables. people splurge thousands on digital kittens\njoin the cryptoplants dao and get decision making power\nfuture scope\ntop donors' leaderboard.\nbecome a gifting platform. many hackathons give away free t-shirts, but many users don't need them (we ourselves have some 15 swag shirts). shipping shirts across continents is not very environment friendly. instead let them opt for a free cryptoplant! hacktoberfest did this, letting users opt for planting a real -----> tree !!!  on tree-nation.org", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59501648}, {"Unnamed: 0": 1723, "autor": "Last Christmas I forgot the date", "date": null, "content": "Predicting Christmas using Data Analysis\nIntroduction\nThroughout the history of humankind, civilizations have used all kinds of methods to keep track of time and created different kinds of calendars, many of them long lost to time. Calendars allow us to arrange meetings, and generally keep track of time in a way that everyone agrees on. Important events or holidays are oftentimes closely connected to the way these calendars are structured and usually celebrated on the same date every year.\nBut isn't there a better way? What makes for example the 25th December so special, that we should celebrate Christmas on that day? Why can't we just celebrate Christmas when we feel like it?\nProblem statement\nThat is the issue I sought out to solve. Finding a better date for Christmas, which isn't determined by arbitrary astronomical periods or historic reasons, but rather by when society feels Christmas should be.\nWhy can't we just celebrate Christmas when we feel like it?\nBut how can we determine when people feel like Christmas? Is there anything, that is so closely linked to our emotional state, that we can measure?\nThe Solution\nMusic! What else reflects human emotions as accurately as art and music? Especially Christmas is characterized by the obnoxious number of Christmas songs that play on loop during the season. Nothing shouts Christmas more than Jingle Bells, Feliz Navidad, and Wham!'s Last Christmas on repeat!\nBy analyzing the current charts we should easily be able to determine how christmasie people feel! And using some basic data analysis and mathematical models, we can then extrapolate the ideal Christmas date to celebrate!\nMethods\nFor the historical analysis, I used Python with the popular data science libraries. The dataset I'm working on is Spotify's historical chart data.\nFirst I looked for the most frequently played Christmas songs, determined by how often they appear in the charts around Christmas. By using this method, one-hit wonders and songs that just happen to be trending in December are not included (as often). Then I filtered out all the songs that appear in the charts during other times of the year, to eliminate some remaining outliers (since those can't be solid Christmas indicators if they are e.g. trending in August).\nThe next step was finding a general way to determine if a song is a Christmas song. For known historic songs that's easy - just look up the song in the list of known Christmas songs, and return one if it has been found. But what about new Christmas hits? To detect those, I created a scoring function, that weights the words used in the track title based on how commonly they appear in the titles of known Christmas songs. This way new songs that might get released, like \"snow rock, Christmas tree version\" or \"Santas jolly baby\" will be scored highly, while other songs that aren't Christmas themed will score poorly.\nIf we now multiply the songs christmasness scores with their last week's number of streams and set that in relation to the total number of Spotify streams of all songs in that week, we have a \"mood score\" that will be high, when people feel like Christmas and low during other times of the year. By fitting a mathematical function to the historic Christmas mood data, we obtain a model that allows us to predict the following Christmas holidays based on the current charts.\nPlugging in today's (25 Nov 2021) global Spotify charts, the model returns 04 Feb 2022 as recommended date for next Christmas. I personally agree that that would be a much better date for Christmas, instead of next month, giving me enough time to shop for presents sometime in January.\nClosing thoughts\nEvery revolutionary idea will be met with resistance, but I am convinced, that selecting a Christmas date based on when people feel like it instead of a fixed date, would have great benefits for society and is worth pursuing. During the making, I got lots of feedback on the project, some of which have been posted here.\nA friend:\nI don't know how much time you spent on this, but I know it was too much.\nMy colleagues:\nLol\nWhoever made this is an idiot. Wait you did? Why???\nSomeone online:\nWow, that's useless!\nI hope to be able to do some more polishing and potentially develop an improved mathematical model before handing in my proposed redefinition for Christmas to the consortium. Thanks for reading, if you have feedback or want to support the project, leave a comment below.", "link": "https://devpost.com/software/last-christmas-i-forgot-the-date", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "predicting christmas using data analysis\nintroduction\nthroughout the history of humankind, civilizations have used all kinds of methods to keep track of time and created different kinds of calendars, many of them long lost to time. calendars allow us to arrange meetings, and generally keep track of time in a way that everyone agrees on. important events or holidays are oftentimes closely connected to the way these calendars are structured and usually celebrated on the same date every year.\nbut isn't there a better way? what makes for example the 25th december so special, that we should celebrate christmas on that day? why can't we just celebrate christmas when we feel like it?\nproblem statement\nthat is the issue i sought out to solve. finding a better date for christmas, which isn't determined by arbitrary astronomical periods or historic reasons, but rather by when society feels christmas should be.\nwhy can't we just celebrate christmas when we feel like it?\nbut how can we determine when people feel like christmas? is there anything, that is so closely linked to our emotional state, that we can measure?\nthe solution\nmusic! what else reflects human emotions as accurately as art and music? especially christmas is characterized by the obnoxious number of christmas songs that play on loop during the season. nothing shouts christmas more than jingle bells, feliz navidad, and wham!'s last christmas on repeat!\nby analyzing the current charts we should easily be able to determine how christmasie people feel! and using some basic data analysis and mathematical models, we can then extrapolate the ideal christmas date to celebrate!\nmethods\nfor the historical analysis, i used python with the popular data science libraries. the dataset i'm working on is spotify's historical chart data.\nfirst i looked for the most frequently played christmas songs, determined by how often they appear in the charts around christmas. by using this method, one-hit wonders and songs that just happen to be trending in december are not included (as often). then i filtered out all the songs that appear in the charts during other times of the year, to eliminate some remaining outliers (since those can't be solid christmas indicators if they are e.g. trending in august).\nthe next step was finding a general way to determine if a song is a christmas song. for known historic songs that's easy - just look up the song in the list of known christmas songs, and return one if it has been found. but what about new christmas hits? to detect those, i created a scoring function, that weights the words used in the track title based on how commonly they appear in the titles of known christmas songs. this way new songs that might get released, like \"snow rock, christmas -----> tree !!!  version\" or \"santas jolly baby\" will be scored highly, while other songs that aren't christmas themed will score poorly.\nif we now multiply the songs christmasness scores with their last week's number of streams and set that in relation to the total number of spotify streams of all songs in that week, we have a \"mood score\" that will be high, when people feel like christmas and low during other times of the year. by fitting a mathematical function to the historic christmas mood data, we obtain a model that allows us to predict the following christmas holidays based on the current charts.\nplugging in today's (25 nov 2021) global spotify charts, the model returns 04 feb 2022 as recommended date for next christmas. i personally agree that that would be a much better date for christmas, instead of next month, giving me enough time to shop for presents sometime in january.\nclosing thoughts\nevery revolutionary idea will be met with resistance, but i am convinced, that selecting a christmas date based on when people feel like it instead of a fixed date, would have great benefits for society and is worth pursuing. during the making, i got lots of feedback on the project, some of which have been posted here.\na friend:\ni don't know how much time you spent on this, but i know it was too much.\nmy colleagues:\nlol\nwhoever made this is an idiot. wait you did? why???\nsomeone online:\nwow, that's useless!\ni hope to be able to do some more polishing and potentially develop an improved mathematical model before handing in my proposed redefinition for christmas to the consortium. thanks for reading, if you have feedback or want to support the project, leave a comment below.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59501723}, {"Unnamed: 0": 1952, "autor": "Save the Seeds to Save the World", "date": null, "content": "According to the law of attraction, whatever we dream in our mind, it automatically comes true. Still, the dreams of most human beings do not come true, because of a world-wide misconception among scientists and scholars, humans have understood their brain as their mind. In this misconception, those seeds are often destroyed where our mind really lies. If you want to make your desired dreams come true naturally, then you have to understand that our mind is in our seeds and our seeds work subconsciously to make our mind\u2019s dreams come true. Just as the whole tree is hidden in a seed, similarly the whole human is hidden in a seed. That is, our mind is in the DNA of our seeds. The red strand of DNA is our soul as the subconscious mind and the blue strand of DNA is our mind as the conscious mind. Thus, we think through the mind located in the DNA of our seeds, and our brain wirelessly processes the genetic codes of the DNA of the seeds.\nThe dream we see with our seed-like mind, that dream becomes the fruit of our seeds. It is a natural law that the seed only makes the fruit. All plants, animals and human beings in nature work on the same principle of getting fruit from seed. A seed first makes the tree, and then many seeds on that tree make many fruits. Since a human is also born from the seed, therefore the same process works for humans as well. A seed first makes our body, and then many seeds in the body give the fruits of many successes. To live a successful life we have to understand, Natural Science of Success - Save-Seed. Since our mind is in our seeds, therefore the law of attraction works only when Save-Seed is followed lifelong.\nMost human beings dream both desire and lustful dreams in their mind. Our mind writes both types of dreams on the DNA of the seeds with the help of the soul. Then the seeds by their natural instincts work subconsciously to get the fruits of those dreams. Our seeds can work only for one kind of dream, because it is necessary to save the seeds in the body for the permanent bliss of desires, while for the transient pleasure of lust the seeds have to be destroyed. Since most of the human beings destroy their seeds for the fulfillment of lust, that is why their dreams of lust come true, but due to the destruction of the seeds, their fruits of desire are also destroyed, so their dreams of desires are not fulfilled. On the contrary, the destruction of seeds definitely brings disease, poverty, infamy and anonymity in life.\nSave-Seed is the greatest and simplest education in the world. All animals get lifelong good health without any practical knowledge and all plants get lifelong fruit wealth without any physical working, because they all follow nature's only law of Save-Seed throughout their life. Whereas in spite of working hard with all the knowledge of science and spirituality, today most of the human beings are living life in disease and poverty, because they do not follow Save-Seed. If Save-Seed is not followed in life, then even the biggest education has no importance. If Save-Seed is followed, then despite normal education, sure success is achieved in life. All successful persons of the world also knowingly or unknowingly follow Save-Seed lifelong. If you want to achieve sure success in life, then follow Save-Seed lifelong by understanding the working procedure of your seeds.\nTo follow Save-Seed, maintain complete purity in mind, body and deeds while having a relationship with your spouse only. If you are unmarried, then keep complete restraint till the marriage takes place. By following Save-Seed lifelong, one gets good health, wealth, name and fame naturally. Our brain processes the genetic code of our seed DNA through the wireless nerve system for the success of the mind's dreams. Watch the detailed video presentation on 'Save-Seed' to understand this wireless processing of seed and brain on a scientific basis. This video presentation is available for free on the website www.anarvam.com. For a healthy, prosperous and secure future for yourself and your future generations, you must watch this detailed video presentation and seriously understand the importance of the only law of nature 'Save-Seed'. Follow Save-Seed lifelong and inspire others to follow Save-Seed.\nFull English Presentation https://youtu.be/OOpzHKfV5uM\nFull Hindi Presentation https://youtu.be/IDZ7TSDJnPU\nWebsite https://www.anarvam.com\ne-Mail anarvam@anarvam.com", "link": "https://devpost.com/software/save-the-seeds-to-save-the-world", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "according to the law of attraction, whatever we dream in our mind, it automatically comes true. still, the dreams of most human beings do not come true, because of a world-wide misconception among scientists and scholars, humans have understood their brain as their mind. in this misconception, those seeds are often destroyed where our mind really lies. if you want to make your desired dreams come true naturally, then you have to understand that our mind is in our seeds and our seeds work subconsciously to make our mind\u2019s dreams come true. just as the whole -----> tree !!!  is hidden in a seed, similarly the whole human is hidden in a seed. that is, our mind is in the dna of our seeds. the red strand of dna is our soul as the subconscious mind and the blue strand of dna is our mind as the conscious mind. thus, we think through the mind located in the dna of our seeds, and our brain wirelessly processes the genetic codes of the dna of the seeds.\nthe dream we see with our seed-like mind, that dream becomes the fruit of our seeds. it is a natural law that the seed only makes the fruit. all plants, animals and human beings in nature work on the same principle of getting fruit from seed. a seed first makes the tree, and then many seeds on that tree make many fruits. since a human is also born from the seed, therefore the same process works for humans as well. a seed first makes our body, and then many seeds in the body give the fruits of many successes. to live a successful life we have to understand, natural science of success - save-seed. since our mind is in our seeds, therefore the law of attraction works only when save-seed is followed lifelong.\nmost human beings dream both desire and lustful dreams in their mind. our mind writes both types of dreams on the dna of the seeds with the help of the soul. then the seeds by their natural instincts work subconsciously to get the fruits of those dreams. our seeds can work only for one kind of dream, because it is necessary to save the seeds in the body for the permanent bliss of desires, while for the transient pleasure of lust the seeds have to be destroyed. since most of the human beings destroy their seeds for the fulfillment of lust, that is why their dreams of lust come true, but due to the destruction of the seeds, their fruits of desire are also destroyed, so their dreams of desires are not fulfilled. on the contrary, the destruction of seeds definitely brings disease, poverty, infamy and anonymity in life.\nsave-seed is the greatest and simplest education in the world. all animals get lifelong good health without any practical knowledge and all plants get lifelong fruit wealth without any physical working, because they all follow nature's only law of save-seed throughout their life. whereas in spite of working hard with all the knowledge of science and spirituality, today most of the human beings are living life in disease and poverty, because they do not follow save-seed. if save-seed is not followed in life, then even the biggest education has no importance. if save-seed is followed, then despite normal education, sure success is achieved in life. all successful persons of the world also knowingly or unknowingly follow save-seed lifelong. if you want to achieve sure success in life, then follow save-seed lifelong by understanding the working procedure of your seeds.\nto follow save-seed, maintain complete purity in mind, body and deeds while having a relationship with your spouse only. if you are unmarried, then keep complete restraint till the marriage takes place. by following save-seed lifelong, one gets good health, wealth, name and fame naturally. our brain processes the genetic code of our seed dna through the wireless nerve system for the success of the mind's dreams. watch the detailed video presentation on 'save-seed' to understand this wireless processing of seed and brain on a scientific basis. this video presentation is available for free on the website www.anarvam.com. for a healthy, prosperous and secure future for yourself and your future generations, you must watch this detailed video presentation and seriously understand the importance of the only law of nature 'save-seed'. follow save-seed lifelong and inspire others to follow save-seed.\nfull english presentation https://youtu.be/oopzhkfv5um\nfull hindi presentation https://youtu.be/idz7tsdjnpu\nwebsite https://www.anarvam.com\ne-mail anarvam@anarvam.com", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59501952}, {"Unnamed: 0": 2002, "autor": "Famille - Helping Alzheimer affected humans", "date": null, "content": "18+ Age Group Category\nInspiration\nThe underlying inspiration for the project was my grandfather who recently passed away and had Alzheimer's. Being really close to him exposed me to the troubles and difficulties for people with Alzheimer's. I saw an opportunity for a web app that could help a lot of people by assisting them through a platform fueled by family and memories.\nWhat it does\nFamille stores information about your family and display a family tree where the user is highlighted so that they can see where they are in their family tree. It supports images and nicknames and \"partner\" and \"parent\" relations in terms of the tree. Famille also stores memories and uses Google Cloud Vision API with Face Annotation, Emotion Detection & Label Detection to give context on images as well as detect emotions so one can filter for only happy images. It also provides questions from memories and family information as a \"daily check in\" to help people remember more about their memories and families and encourage them to remember more. The platform is really meant to be used by the person with Alzheimer's and their family. Example: the page to add family should ideally done by a family member.\nHow we built it\nThe frontend is built with VueJS. The backend is a micro-services architecture with containerization of two NodeJS services. I used Postgres as a database for storing all the relational data. I also used the google cloud vision API. I used docker for dockerization.\nChallenges we ran into\nI think one of the challenges was to interact, filter and understand the backend logic for the cloud vision API. I also had many crashes of the express server and found ways to mitigate that. I also had to learn VueJS as I wasn't familiar with it. Doing everything myself was also a big challenge considering I had an ambitious goal. I think another big challenge was to understand how to store family relations as well trying to figure out how to code a family tree.\nAccomplishments that we're proud of\nFamille I think has a future in terms of helping people with Alzheimer's. I am really proud of the impact it cold have. I am also proud that I could get something working together in 24 hours and working just my by myself.\nWhat we learned\nI learned a lot about building my technical skills such as VueJS, docker. I also learned about thinking about social good and how technology can help in that area.\nWhat's next for Famille - Helping Alzheimer affected humans\nI hope to continue working on Famille as I feel it can make a genuine impact on people's lives. Some next steps would be to clean out some code, add some new features and work on deployment.", "link": "https://devpost.com/software/famille-helping-alzheimer-affected-humans", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "18+ age group category\ninspiration\nthe underlying inspiration for the project was my grandfather who recently passed away and had alzheimer's. being really close to him exposed me to the troubles and difficulties for people with alzheimer's. i saw an opportunity for a web app that could help a lot of people by assisting them through a platform fueled by family and memories.\nwhat it does\nfamille stores information about your family and display a family -----> tree !!!  where the user is highlighted so that they can see where they are in their family -----> tree !!! . it supports images and nicknames and \"partner\" and \"parent\" relations in terms of the tree. famille also stores memories and uses google cloud vision api with face annotation, emotion detection & label detection to give context on images as well as detect emotions so one can filter for only happy images. it also provides questions from memories and family information as a \"daily check in\" to help people remember more about their memories and families and encourage them to remember more. the platform is really meant to be used by the person with alzheimer's and their family. example: the page to add family should ideally done by a family member.\nhow we built it\nthe frontend is built with vuejs. the backend is a micro-services architecture with containerization of two nodejs services. i used postgres as a database for storing all the relational data. i also used the google cloud vision api. i used docker for dockerization.\nchallenges we ran into\ni think one of the challenges was to interact, filter and understand the backend logic for the cloud vision api. i also had many crashes of the express server and found ways to mitigate that. i also had to learn vuejs as i wasn't familiar with it. doing everything myself was also a big challenge considering i had an ambitious goal. i think another big challenge was to understand how to store family relations as well trying to figure out how to code a family tree.\naccomplishments that we're proud of\nfamille i think has a future in terms of helping people with alzheimer's. i am really proud of the impact it cold have. i am also proud that i could get something working together in 24 hours and working just my by myself.\nwhat we learned\ni learned a lot about building my technical skills such as vuejs, docker. i also learned about thinking about social good and how technology can help in that area.\nwhat's next for famille - helping alzheimer affected humans\ni hope to continue working on famille as i feel it can make a genuine impact on people's lives. some next steps would be to clean out some code, add some new features and work on deployment.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59502002}, {"Unnamed: 0": 2099, "autor": "Giving Tree", "date": null, "content": "Inspiration\nWe have a problem! We have a new generation of broke philanthropists.\nThe majority of students do not have a lot of spare cash so it can be challenging for them to choose between investing in their own future or the causes that they believe in to build a better future for others.\nOn the other hand, large companies have the capital needed to make sizeable donations but many of these acts go unnoticed or quickly forgotten.\nWhat it does\nWhat if I told you that there is a way to support your favourite charities while also saving money? Students no longer need to choose between investing and donating!\nGiving tree changes how we think about investing. Giving tree focuses on a charity driven investment model providing the ability to indulge in philanthropy while still supporting your future financially.\nWe created a platform that connects students to companies that make donations to the charities that they are interested in. Students will be able to support charities they believe in by investing in companies that are driven to make donations to such causes.\nOur mission is to encourage students to invest in companies that financially support the same causes they believe in. Students will be able to not only learn more about financial planning but also help support various charities and services.\nHow we built it\nBackend\nThe backend of this application was built using python. In the backend, we were able to overcome one of our largest obstacles, that this concept has never been done before! We really struggled finding a database or API that would provide us with information on what companies were donating to which charities.\nSo, how did we overcome this? We wanted to avoid having to manually input the data we needed as this was not a sustainable solution. Additionally, we needed a way to get data dynamically. As time passes, companies will continue to donate and we needed recent and topical data.\nGiving Tree overcomes these obstacles using a 4 step process:\nUsing a google search API, search for articles about companies donating to a specified category or charity.\nIdentify all the nouns in the header of the search result.\nUsing the nouns, look for companies with data in Yahoo Finance that have a strong likeness to the noun.\nGet the financial data of the company mentioned in the article and return the financial data to the user.\nThis was one of our greatest accomplishments of this project. We were able to overcome and obstacle that almost made us want to do a different project. Although the algorithm can occasionally produce false positives, it works more often than not and allows for us to have a self-sustaining platform to build off of.\nFlask\n```shell script $ touch application.py\nfrom flask import Flask application = Flask(name) @application.route('/') def hello_world(): return 'Hello World'\n```shell script\n$ export FLASK_APP=\"application.py\"\n$ flask run\nNow runs locally:\nhttp://127.0.0.1:5000/\nAWS Elastic Beanstalk\nCreate a Web Server Environment: ```shell script AWS -> Services -> Elastic beanstalk Create New Application called hack-western-8 using Python Create New Environment called hack-western-8-env using Web Server Environment\n### AWS CodePipeline\nLink to Github for Continuous Deployment:\n```shell script\nServices -> Developer Tools -> CodePipeline\nCreate Pipeline called hack-western-8\nGitHub Version 2 -> Connect to Github\nConnection Name -> Install a New App -> Choose Repo Name -> Skip Build Stage -> Deploy to AWS Elastic Beanstalk\nThis link is no longer local:\nhttp://hack-western-8-env.eba-a5injkhs.us-east-1.elasticbeanstalk.com/\nAWS Route 53\nRegister a Domain: ```shell script Route 53 -> Registered Domains -> Register Domain -> hack-western-8.com -> Check Route 53 -> Hosted zones -> Create Record -> Route Traffic to IPv4 Address -> Alias -> Elastic Beanstalk -> hack-western-8-env -> Create Records Create another record but with alias www.\nNow we can load the website using:<br/>\n[hack-western-8.com](http://hack-western-8.com)<br/>\nwww.hack-western-8.com<br/>\nhttp://hack-western-8.com<br/>\nhttp://www.hack-western-8.com<br/>\nNote that it says \"Not Secure\" beside the link<br/>\n### AWS Certificate Manager\nAdd SSL to use HTTPS:\n```shell script\nAWS Certificate Manager -> Request a Public Certificate -> Domain Name \"hack-western-8.com\" and \"*.hack-western-8.com\" -> DNS validation -> Request\n$ dig +short CNAME -> No Output? -> Certificate -> Domains -> Create Records in Route 53\nElastic Beanstalk -> Environments -> Configuration -> Capacity -> Enable Load Balancing\nLoad balancer -> Add listener -> Port 443 -> Protocol HTTPS -> SSL certificate -> Save -> Apply\nNow we can load the website using:\nhttps://hack-western-8.com\nhttps://www.hack-western-8.com\nNote that there is a lock icon beside the link to indicate that we are using a SSL certificate so we are secure\nChallenges we ran into\nThe most challenging part of the project was connecting the charities to the companies. We allowed the user to either type the charity name or choose a category that they would like to support. Once we knew what charity they are interested in, we could use this query to scrape information concerning donations from various companies and then display the stock information related to those companies. We were able to successfully complete this query and we can display the donations made by various companies in the command line, however further work would need to be done in order to display all of this information on the website. Despite these challenges, the current website is a great prototype and proof of concept!\nAccomplishments that we're proud of\nWe were able to successfully use the charity name or category to scrape information concerning donations from various companies. We not only tested our code locally, but also deployed this website on AWS using Elastic Beanstalk. We created a unique domain for the website and we made it secure through a SSL certificate.\nWhat we learned\nWe learned how to connect Flask to AWS, how to design an eye-catching website, how to create a logo using Photoshop and how to scrape information using APIs.\nWe also learned about thinking outside the box. To find the data we needed we approached the problem from several different angles. We looked for ways to see what companies were giving to charities, where charities were receiving their money, how to minimize false positives in our search algorithm, and how to overcome seemingly impossible obstacles.\nWhat's next for Giving Tree\nCurrently, students have 6 categories they can choose from, in the future we would be able to divide them into more specific sub-categories in order to get a better query and find charities that more closely align with their interests.\nHealth - Medical Research - Mental Health - Physical Health - Infectious Diseases\nEnvironment - Ocean Conservation - Disaster Relief - Natural Resources - Rainforest Sustainability - Global Warming\nHuman Rights - Women's Rights - Children\nCommunity Development - Housing - Poverty - Water - Sanitation - Hunger\nEducation - Literacy - After School Programs - Scholarships\nAnimals - Animal Cruelty - Animal Health - Wildlife Habitats\nWe would also want to connect the front and back end.", "link": "https://devpost.com/software/giving-tree-dzw95j", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\nwe have a problem! we have a new generation of broke philanthropists.\nthe majority of students do not have a lot of spare cash so it can be challenging for them to choose between investing in their own future or the causes that they believe in to build a better future for others.\non the other hand, large companies have the capital needed to make sizeable donations but many of these acts go unnoticed or quickly forgotten.\nwhat it does\nwhat if i told you that there is a way to support your favourite charities while also saving money? students no longer need to choose between investing and donating!\ngiving -----> tree !!!  changes how we think about investing. giving tree focuses on a charity driven investment model providing the ability to indulge in philanthropy while still supporting your future financially.\nwe created a platform that connects students to companies that make donations to the charities that they are interested in. students will be able to support charities they believe in by investing in companies that are driven to make donations to such causes.\nour mission is to encourage students to invest in companies that financially support the same causes they believe in. students will be able to not only learn more about financial planning but also help support various charities and services.\nhow we built it\nbackend\nthe backend of this application was built using python. in the backend, we were able to overcome one of our largest obstacles, that this concept has never been done before! we really struggled finding a database or api that would provide us with information on what companies were donating to which charities.\nso, how did we overcome this? we wanted to avoid having to manually input the data we needed as this was not a sustainable solution. additionally, we needed a way to get data dynamically. as time passes, companies will continue to donate and we needed recent and topical data.\ngiving tree overcomes these obstacles using a 4 step process:\nusing a google search api, search for articles about companies donating to a specified category or charity.\nidentify all the nouns in the header of the search result.\nusing the nouns, look for companies with data in yahoo finance that have a strong likeness to the noun.\nget the financial data of the company mentioned in the article and return the financial data to the user.\nthis was one of our greatest accomplishments of this project. we were able to overcome and obstacle that almost made us want to do a different project. although the algorithm can occasionally produce false positives, it works more often than not and allows for us to have a self-sustaining platform to build off of.\nflask\n```shell script $ touch application.py\nfrom flask import flask application = flask(name) @application.route('/') def hello_world(): return 'hello world'\n```shell script\n$ export flask_app=\"application.py\"\n$ flask run\nnow runs locally:\nhttp://127.0.0.1:5000/\naws elastic beanstalk\ncreate a web server environment: ```shell script aws -> services -> elastic beanstalk create new application called hack-western-8 using python create new environment called hack-western-8-env using web server environment\n### aws codepipeline\nlink to github for continuous deployment:\n```shell script\nservices -> developer tools -> codepipeline\ncreate pipeline called hack-western-8\ngithub version 2 -> connect to github\nconnection name -> install a new app -> choose repo name -> skip build stage -> deploy to aws elastic beanstalk\nthis link is no longer local:\nhttp://hack-western-8-env.eba-a5injkhs.us-east-1.elasticbeanstalk.com/\naws route 53\nregister a domain: ```shell script route 53 -> registered domains -> register domain -> hack-western-8.com -> check route 53 -> hosted zones -> create record -> route traffic to ipv4 address -> alias -> elastic beanstalk -> hack-western-8-env -> create records create another record but with alias www.\nnow we can load the website using:<br/>\n[hack-western-8.com](http://hack-western-8.com)<br/>\nwww.hack-western-8.com<br/>\nhttp://hack-western-8.com<br/>\nhttp://www.hack-western-8.com<br/>\nnote that it says \"not secure\" beside the link<br/>\n### aws certificate manager\nadd ssl to use https:\n```shell script\naws certificate manager -> request a public certificate -> domain name \"hack-western-8.com\" and \"*.hack-western-8.com\" -> dns validation -> request\n$ dig +short cname -> no output? -> certificate -> domains -> create records in route 53\nelastic beanstalk -> environments -> configuration -> capacity -> enable load balancing\nload balancer -> add listener -> port 443 -> protocol https -> ssl certificate -> save -> apply\nnow we can load the website using:\nhttps://hack-western-8.com\nhttps://www.hack-western-8.com\nnote that there is a lock icon beside the link to indicate that we are using a ssl certificate so we are secure\nchallenges we ran into\nthe most challenging part of the project was connecting the charities to the companies. we allowed the user to either type the charity name or choose a category that they would like to support. once we knew what charity they are interested in, we could use this query to scrape information concerning donations from various companies and then display the stock information related to those companies. we were able to successfully complete this query and we can display the donations made by various companies in the command line, however further work would need to be done in order to display all of this information on the website. despite these challenges, the current website is a great prototype and proof of concept!\naccomplishments that we're proud of\nwe were able to successfully use the charity name or category to scrape information concerning donations from various companies. we not only tested our code locally, but also deployed this website on aws using elastic beanstalk. we created a unique domain for the website and we made it secure through a ssl certificate.\nwhat we learned\nwe learned how to connect flask to aws, how to design an eye-catching website, how to create a logo using photoshop and how to scrape information using apis.\nwe also learned about thinking outside the box. to find the data we needed we approached the problem from several different angles. we looked for ways to see what companies were giving to charities, where charities were receiving their money, how to minimize false positives in our search algorithm, and how to overcome seemingly impossible obstacles.\nwhat's next for giving tree\ncurrently, students have 6 categories they can choose from, in the future we would be able to divide them into more specific sub-categories in order to get a better query and find charities that more closely align with their interests.\nhealth - medical research - mental health - physical health - infectious diseases\nenvironment - ocean conservation - disaster relief - natural resources - rainforest sustainability - global warming\nhuman rights - women's rights - children\ncommunity development - housing - poverty - water - sanitation - hunger\neducation - literacy - after school programs - scholarships\nanimals - animal cruelty - animal health - wildlife habitats\nwe would also want to connect the front and back end.", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59502099}, {"Unnamed: 0": 2175, "autor": "Treeify", "date": null, "content": "Inspiration\nThere\u2019s no doubt that both small companies and the environment have been negatively impacted by the pandemic: so, why not try to help both?\nWhat it does\nTreeify is an app that allows users to review their favorite businesses, restaurants, and locations while helping the community. Treeify works by its users leaving reviews and giving companies the spotlight if they make substantial efforts to be eco-friendly. Once companies get a certain amount of rewards, we plant a tree on their behalf!\nHow we built it\nWe built Treeify using React, Firebase, Toast, and kebabCase from Lodash.\nChallenges we ran into\nA lot of backend troubles, but after hours of experimenting with code and researching we were able to create a finished product. We also had some poor communication at the beginning of the hackathon but we tried to discuss more and our progress was a lot more smooth.\nAccomplishments that we're proud of\nLearn more about Firebase, the aesthetics of the project, and many others! Especially our artificial intelligence system that helps track the eco-friendliness of a specific company based on large-scale user input.\nWhat we learned\nHow to build a web app using react. How to collaborate more efficiently and work together in a team environment online. In addition, we learned how to implement complex queries and algorithms to our database to filter out the information we need.\nWhat's next for Treeify\nIn the future, we plan on putting pictures of a tree planted for the company on the website to show our appreciation for the companies that decide to be eco-friendly.", "link": "https://devpost.com/software/treeify", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\nthere\u2019s no doubt that both small companies and the environment have been negatively impacted by the pandemic: so, why not try to help both?\nwhat it does\ntreeify is an app that allows users to review their favorite businesses, restaurants, and locations while helping the community. treeify works by its users leaving reviews and giving companies the spotlight if they make substantial efforts to be eco-friendly. once companies get a certain amount of rewards, we plant a -----> tree !!!  on their behalf!\nhow we built it\nwe built treeify using react, firebase, toast, and kebabcase from lodash.\nchallenges we ran into\na lot of backend troubles, but after hours of experimenting with code and researching we were able to create a finished product. we also had some poor communication at the beginning of the hackathon but we tried to discuss more and our progress was a lot more smooth.\naccomplishments that we're proud of\nlearn more about firebase, the aesthetics of the project, and many others! especially our artificial intelligence system that helps track the eco-friendliness of a specific company based on large-scale user input.\nwhat we learned\nhow to build a web app using react. how to collaborate more efficiently and work together in a team environment online. in addition, we learned how to implement complex queries and algorithms to our database to filter out the information we need.\nwhat's next for treeify\nin the future, we plan on putting pictures of a tree planted for the company on the website to show our appreciation for the companies that decide to be eco-friendly.", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 2, "media": null, "medialink": null, "identifyer": 59502175}, {"Unnamed: 0": 2334, "autor": "smARtree", "date": null, "content": "Inspiration\nTo make future learning easier and interactive for kids we planned to make something smart solution for kids using Augmented Reality.\nWhat it does\nIt introduces kids to different types of trees of different areas. If you click on bring to reality, it'll show you the 3D texture of the tree to help kids to understand structure of trees of that areas.\nHow we built it\nUsed HTML, CSS, JS for frontend. echoAR for AR models\nChallenges we ran into\nChoosing the AR models as per the the tree divisions was quite challenging\nAccomplishments that we're proud of\nIt was pretty tough schedule for ideation with team and bringing that idea to reality within short time. But happy to accomplish.\nWhat's next for smARtree\nWe plan to add more categories of trees to help the kids to learn. Here goes the allover idea. But couldn't add much categories for limited time and less hands.", "link": "https://devpost.com/software/smartree", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\nto make future learning easier and interactive for kids we planned to make something smart solution for kids using augmented reality.\nwhat it does\nit introduces kids to different types of trees of different areas. if you click on bring to reality, it'll show you the 3d texture of the -----> tree !!!  to help kids to understand structure of trees of that areas.\nhow we built it\nused html, css, js for frontend. echoar for ar models\nchallenges we ran into\nchoosing the ar models as per the the tree divisions was quite challenging\naccomplishments that we're proud of\nit was pretty tough schedule for ideation with team and bringing that idea to reality within short time. but happy to accomplish.\nwhat's next for smartree\nwe plan to add more categories of trees to help the kids to learn. here goes the allover idea. but couldn't add much categories for limited time and less hands.", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59502334}, {"Unnamed: 0": 2653, "autor": "\u00f3ptimo", "date": null, "content": "Inspiration\nWe are are passionate about sustainability and the environment, as well as a healthy nutritional lifestyle. Therefore it is imperative to educate the younger generation about healthy and sustainable shopping choices. We are combining state-of-the-art artificial intelligence, with cutting a edge gaming framework that gamifies the shopping experience for a better future.\nWhat it does\n\u00f3ptimo provides a gaming experience for for users, in which they can upload higher order grocery articles they want to buy for a specific time period. Using advanced artificial intellgence and realistic data simulations we return a recommended shopping list that is sustainable in terms of CO2 emissions and healthy for the consumer, while satisfying price and size constraints of the shopper.\nHow we built it\nWe wrote the platform on Java, utilizing the game library libGDX while seamlessly integrating a custom Monte Carlo Tree Search library written in Kotlin, mctreesearch4j. The software libraries are both operating in the JVM ecosystem.\nChallenges we ran into\nHaving created this app in 1 day, we ran into many issues, particularily involving the Intellig IDE, gradle functionality, and integration efforts between Java and Kotlin. But the key challenge was time pressure. Also as we had a multi-national team, we ran into time zone conflicts.\nAccomplishments that we're proud of\nThis is a unique a cross platform application, and working prototype, that's fully functional on the JVM, incorporating advanced AI.\nWhat we learned\nWe learned collaborate as multi-national and multi-disciplinary team with teammate's back ranging from software engineering to management consulting. We considered the sustainability problem from multiple perspectives.\nWhat's next for \u00f3ptimo\nIf possible please provide some Series A please.", "link": "https://devpost.com/software/optimo-d60x1h", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\nwe are are passionate about sustainability and the environment, as well as a healthy nutritional lifestyle. therefore it is imperative to educate the younger generation about healthy and sustainable shopping choices. we are combining state-of-the-art artificial intelligence, with cutting a edge gaming framework that gamifies the shopping experience for a better future.\nwhat it does\n\u00f3ptimo provides a gaming experience for for users, in which they can upload higher order grocery articles they want to buy for a specific time period. using advanced artificial intellgence and realistic data simulations we return a recommended shopping list that is sustainable in terms of co2 emissions and healthy for the consumer, while satisfying price and size constraints of the shopper.\nhow we built it\nwe wrote the platform on java, utilizing the game library libgdx while seamlessly integrating a custom monte carlo -----> tree !!!  search library written in kotlin, mctreesearch4j. the software libraries are both operating in the jvm ecosystem.\nchallenges we ran into\nhaving created this app in 1 day, we ran into many issues, particularily involving the intellig ide, gradle functionality, and integration efforts between java and kotlin. but the key challenge was time pressure. also as we had a multi-national team, we ran into time zone conflicts.\naccomplishments that we're proud of\nthis is a unique a cross platform application, and working prototype, that's fully functional on the jvm, incorporating advanced ai.\nwhat we learned\nwe learned collaborate as multi-national and multi-disciplinary team with teammate's back ranging from software engineering to management consulting. we considered the sustainability problem from multiple perspectives.\nwhat's next for \u00f3ptimo\nif possible please provide some series a please.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59502653}, {"Unnamed: 0": 3141, "autor": "ActionApp", "date": null, "content": "Inspiration\nI am very concerned about the environment, and with the help of my family, I decided on this as a way to help! I think that this is a very important topic as it affects everyone.\nWhat it does\nMy app is called ActionApp, and it generates ways that you can benefit the environment, whether its by singing a petition, donating to an environmental organization, or planting a tree.\nHow we built it\nI built this app in flutter, as I have developed apps with flutter in the past, and I had a lot of fun.\nChallenges we ran into\nI had some issues starting my project, and committing it to github, but in the end everything worked out.\nAccomplishments that we're proud of\nI am proud of the fact that my app functions exactly how I wanted it to.\nWhat we learned\nI learned a lot about dart and flutter, as well as a lot about how hackathons work.\nWhat's next for ActionApp\nI want to add to it by giving more variety and quantity in the ways you can help.", "link": "https://devpost.com/software/actionapp", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\ni am very concerned about the environment, and with the help of my family, i decided on this as a way to help! i think that this is a very important topic as it affects everyone.\nwhat it does\nmy app is called actionapp, and it generates ways that you can benefit the environment, whether its by singing a petition, donating to an environmental organization, or planting a -----> tree !!! .\nhow we built it\ni built this app in flutter, as i have developed apps with flutter in the past, and i had a lot of fun.\nchallenges we ran into\ni had some issues starting my project, and committing it to github, but in the end everything worked out.\naccomplishments that we're proud of\ni am proud of the fact that my app functions exactly how i wanted it to.\nwhat we learned\ni learned a lot about dart and flutter, as well as a lot about how hackathons work.\nwhat's next for actionapp\ni want to add to it by giving more variety and quantity in the ways you can help.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59503141}, {"Unnamed: 0": 3317, "autor": "Heart Failure Predictor", "date": null, "content": "Inspiration\nHeart attack is a common cause of death nowadays, not only old people but younger ones have also been victims of this. Although we have specialized doctors in this, yet the doctor to patient ratio is huge making our medical infrastructure inefficient. This is where Machine Learning comes in, this model will predict the chances of heart attack of a person based on various parameters. ##With an accuracy of 83.51 % with an precision of 84.31% .\nWhat it does\nOn giving the required parameters the model predicts the possibility of heart attack. I have deployed the model on the internet.\nHow I built it\nI tried with multiple Machine Learning algorithms like KNN , Logistic Regression , Decision Tree and Random Forest Classifier and looking at the initial performance, I tried to improve the model performance by hyper tuning the parameter, feature selection of Random Forest Classifier based model.\nRandom Forest Classifier Algorithm is a supervised algorithm classification method. In this algorithm, some trees form a forest. All trees in the Random Forest give the expected value of the class, and the class with the most votes is the model prediction. The three common methods are: Forest RI (random input selection). Forest RC (random mix). Combination of forest RI and forest RC. It can be used for both classification and regression problems, but good for dealing with classification problems and overcoming missing values. Further, after selecting the best model I deployed it using flask and Heroku.\nChallenges I ran into\nThe biggest challenge was selecting the most precise and accurate algorithm for our model.\nAccomplishments that we're proud of\nI am quite proud that our model shows an accuracy of 83.51 % with an precision of *84.31% *. And it is very easy to use as it is deployed globally on the internet.\nWhat's next for Heart Attack Predictor\nI am planning to make it more interactive by integrating with various medical APIs to make the website more useful and to make it a complete website and not only a detector one. I also plan to create a mobile application too to make it more feasible for users to interact and keep track of their heart condition.\nDiscord usernames of the team member:\nKumari Astha- ikumariastha#8822", "link": "https://devpost.com/software/heart-failure-prediction-etvnkp", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\nheart attack is a common cause of death nowadays, not only old people but younger ones have also been victims of this. although we have specialized doctors in this, yet the doctor to patient ratio is huge making our medical infrastructure inefficient. this is where machine learning comes in, this model will predict the chances of heart attack of a person based on various parameters. ##with an accuracy of 83.51 % with an precision of 84.31% .\nwhat it does\non giving the required parameters the model predicts the possibility of heart attack. i have deployed the model on the internet.\nhow i built it\ni tried with multiple machine learning algorithms like knn , logistic regression , decision -----> tree !!!  and random forest classifier and looking at the initial performance, i tried to improve the model performance by hyper tuning the parameter, feature selection of random forest classifier based model.\nrandom forest classifier algorithm is a supervised algorithm classification method. in this algorithm, some trees form a forest. all trees in the random forest give the expected value of the class, and the class with the most votes is the model prediction. the three common methods are: forest ri (random input selection). forest rc (random mix). combination of forest ri and forest rc. it can be used for both classification and regression problems, but good for dealing with classification problems and overcoming missing values. further, after selecting the best model i deployed it using flask and heroku.\nchallenges i ran into\nthe biggest challenge was selecting the most precise and accurate algorithm for our model.\naccomplishments that we're proud of\ni am quite proud that our model shows an accuracy of 83.51 % with an precision of *84.31% *. and it is very easy to use as it is deployed globally on the internet.\nwhat's next for heart attack predictor\ni am planning to make it more interactive by integrating with various medical apis to make the website more useful and to make it a complete website and not only a detector one. i also plan to create a mobile application too to make it more feasible for users to interact and keep track of their heart condition.\ndiscord usernames of the team member:\nkumari astha- ikumariastha#8822", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59503317}, {"Unnamed: 0": 3383, "autor": "Terrain", "date": null, "content": "I hope this project will help lots of people in their journey for learning.\nI was inspired by the CommonApp, a site that connects you to multiple colleges. I wanted to create a miniature version but for programs and internships. I know a lot of people don't have resources like this or don't know where to look (I started out that way too!), so I developed this website in the pursuits to be that certain guiding hand.\nThis was actually my first time learning how to code for a website (HTML, CSS, JS), meaning I had to learn all of this on the spot. I'm not sure whether I like torturing myself or I like the challenge. Basically, not having any prior knowledge was the main problem. I also worked alone (due to my insecurities of not knowing anything)- although that might've added onto the stress.\nI built this through html, css, and javascript (javascript was most definitely the hardest one).\nAfter this hackathon is over, I will be scouring the internet for programs and internships and hopefully my tree-map of opportunities will be full of lights (if you watch the video, you'll understand). I will be implementing a calendar as well- in order to make deadlines more clear.", "link": "https://devpost.com/software/terrain", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "i hope this project will help lots of people in their journey for learning.\ni was inspired by the commonapp, a site that connects you to multiple colleges. i wanted to create a miniature version but for programs and internships. i know a lot of people don't have resources like this or don't know where to look (i started out that way too!), so i developed this website in the pursuits to be that certain guiding hand.\nthis was actually my first time learning how to code for a website (html, css, js), meaning i had to learn all of this on the spot. i'm not sure whether i like torturing myself or i like the challenge. basically, not having any prior knowledge was the main problem. i also worked alone (due to my insecurities of not knowing anything)- although that might've added onto the stress.\ni built this through html, css, and javascript (javascript was most definitely the hardest one).\nafter this hackathon is over, i will be scouring the internet for programs and internships and hopefully my -----> tree !!! -map of opportunities will be full of lights (if you watch the video, you'll understand). i will be implementing a calendar as well- in order to make deadlines more clear.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59503383}, {"Unnamed: 0": 3455, "autor": "Diagnosis at Home", "date": null, "content": "Inspiration\nNovember 14 marks World Diabetes Day. It is a group of diseases that result in too much sugar in the blood. The Center of Disease Control and Prevention states that there are two types: Type 1 and Type 2 diabetes. Type 2 Diabetes chronic conditions can be prevented by staying active and eating healthy food.\nWhat it does\nThis is Automatic Confirmatory System identifies if a person has diabetes based on medical input provided by using Machine Learning. Based on the results the person gets tips on becoming fitter.\nNote: This project is not meant to be replacement for medical diagnosis from authorised professionals.\nHow we built it\nWe build backend and frontend in the project. Backend: Exploring, Cleaning and Transforming dataset from UCI Repository. Trained Machine Learning classification models using Decision Tree Classifier, Random Forest, Gradient Boosting classifier Support Vector and K Nearest Number algorithms. We tuned each model to reduce overfitting. Then we test the models and found out that the Random Forest classifier model gave maximum accuracy. This trained model is saved and made an API endpoint to connect with front end of the website.\nFrontend:\nChallenges we ran into\nWe started the project late in the hackathon and had difficulty integrating the front end with the back end. We overcame these challenges by helping each other and learning new skills.\nAccomplishments that we're proud of\nLearning machine learning and implementing it for the first time. Onboarding beginners in our team and helping them contribute to the hackathon.\nWhat we learned\nLearned to collaborate on our project with strangers. Learned to integrate different technologies: Python and HTML, CSS and JavaScript.\nWhat's next for Deal with Diabetes\nExtend our functionality", "link": "https://devpost.com/software/deal-with-diabetes", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\nnovember 14 marks world diabetes day. it is a group of diseases that result in too much sugar in the blood. the center of disease control and prevention states that there are two types: type 1 and type 2 diabetes. type 2 diabetes chronic conditions can be prevented by staying active and eating healthy food.\nwhat it does\nthis is automatic confirmatory system identifies if a person has diabetes based on medical input provided by using machine learning. based on the results the person gets tips on becoming fitter.\nnote: this project is not meant to be replacement for medical diagnosis from authorised professionals.\nhow we built it\nwe build backend and frontend in the project. backend: exploring, cleaning and transforming dataset from uci repository. trained machine learning classification models using decision -----> tree !!!  classifier, random forest, gradient boosting classifier support vector and k nearest number algorithms. we tuned each model to reduce overfitting. then we test the models and found out that the random forest classifier model gave maximum accuracy. this trained model is saved and made an api endpoint to connect with front end of the website.\nfrontend:\nchallenges we ran into\nwe started the project late in the hackathon and had difficulty integrating the front end with the back end. we overcame these challenges by helping each other and learning new skills.\naccomplishments that we're proud of\nlearning machine learning and implementing it for the first time. onboarding beginners in our team and helping them contribute to the hackathon.\nwhat we learned\nlearned to collaborate on our project with strangers. learned to integrate different technologies: python and html, css and javascript.\nwhat's next for deal with diabetes\nextend our functionality", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59503455}, {"Unnamed: 0": 3750, "autor": "Related Domain Experience Replay (ReDER)", "date": null, "content": "REDER (Related Domain Experience Replay)\nMembers\nName cslogin\nRaymond Dai rdai4\nRichard Tang rtang26\nAkash Singirikonda asingir1\nKenta Yoshii kyoshii\nFinal Technical Report\nLink to a Google Drive folder with the demo video and the write up. Check out some cool gifs of our model in action!\nIntroduction\nArtificial Intelligence has been used to train models on increasingly complicated games, from Chess to Go to Starcraft 2. These models have mainly been trained on a specific game, using techniques such as adversarial search and deep reinforcement learning (RL) with Monte Carlo Tree Search (MCTS) to optimize performance on that single board game. While successful, these models essentially train on exorbitant amounts of data, and must re-learn novel games from scratch.\nWe hypothesize that while games have different rules, their underlying mechanics are largely similar, so training on one game should theoretically improve performance across other games. Specifically, using shared experience replay, we hope to employ RL to learn a general structure of similar games\nResults\nRegular Dueling DDQN\nDueling DDQN with Joint Experience Replay (our model)\nThe top gameplay is by the Dueling DDQN. The bottom two gameplays are by the Dueling DDQN with Joint Experience Replay. Due to time limitations, we only trained each model for certain number of steps. Based on this comparison, following points can be made:\nThe regular Dueling DDQN model knew to shoot the bonus point red ship whenever it comes out. In contrast, our model pretty much ignored it.\nThis could be explained by the fact that since it is also trained on DemonAttack by means of Joint Experience Replay, the importance of the red ship (which does not exist in the latter) did not get much importance (experience did not get shared).\nDueling DDQN with Joint Experience Replay model could be interpreted as a little more \"aggressive\" than the baseline model as we can see from how it shoots off the objects from the get-go and by looking at the end result suggests.\nFinally, as we can see from the two gameplays, the Doueling DDQN with joint experience replay does okay on both games. However, when comparing DemonAttack and SpaceInvaders, it becomes clear that it performs better on the latter. This might be attributed to the fact that some experiences are unequally prioritized over others.\nRelated Work\nPlay Atari with Deep Reinforcement Learning\nHuman-level Control through Deep Reinforcement Learning\nUnsupervised State Representation Learning in Atari\nData\nSince REDER is a reinforcement model without any teacher enforcing, there is no need for us to collect data for this project. One thing to note is that we will be using OpenAI Gym to get access to the simulated environments for each game we will be using. With these simulated environments, our group is planning on training multiple models and comparing their performance to see how our REDER performs with respect to other existing models. Thankfully, OpenAI has baseline implementation entirely open to access, making the process of comparing a lot more simple. As for the actual games we are going to use to train our model, we will be using SpaceInvaders-v0 and DemonAttack-v0.\nSpaceInvaders-v0\nDemonAttack-v0\nOpenAI Gym\nOpenAI Baseline\nMethods\nWe used Dueling Double Q-Network(DDQN) for this project. Two streams, advantage and value functions, are aggregated at the end to produce an estimate of state-action value function Q. Since in games, certain actions taken place in certain states could possibly carry less value than others. By decoupling the estimation, intuitively our Dueling DDQN can learn which states are (or are not) valuable without having to learn the effect of each action at each state.\nThe above model serves as our general model structure. To realize our goal of creating a model that is extensible among similar atari games, we first tried implementing a GAN as our shared buffer. However, this approach turned out to be infeasible since GAN required us to have even more weights and added significant amount of time for our model training (running for 1 epoch took us 10 minutes). Instead of using GAN, we followed a naive approach where we first initialized the experience buffer with randomly generated experiences from each game (Half from each). We then alternate playing each game by frame while training, storing each stack of frames in the experience buffer. We hypothesize that the q function would learn to identify similar situations in both games and use that experience to learn faster and avoid local minima. SpaceInvaders becomes similar to DemonAttack when the shields disappear.\nFinally, we test our model on each game to see how well the model perform when compared with the OpenAI Baseline model.\nMetrics\nWe will consider OpenAI baseline implementation as our standard and will compare our model against it.\nA success constitutes an experimental architecture training correctly to convergence, even if it might not perform as well.\nThe training and testing environments will be chosen from OpenAI Gym.\nAccuracy does not apply; rather having an agent maximize its reward quickly as possible is an important metric to track in RL.\nBase goals: Implement DDQN and a standard RL model. Our model should be able to learn from SpaceInvaders-v0 and reach the score of 300 per episode.\nTarget Goals: Implemented Dueling DDQN with naive state representation shows moderate signs of learning from SpaceInvaders-v0 and DemonAttack-v0. This model then should be able to achieve 500 per episode on SpaceInvaders-v0.\nStretch Goals: Implemented Dueling DDQN with complex state representation shows strong signs of learning from SpaceInvaders-v0 and DemonAttack-v0. This model then should be able to achieve 700 per episode on SpaceInvaders-v0\nEthics\nWhy is Deep Learning a good approach to this problem?\nIn games, environment, rules, and operations are relatively complicated. In order to achieve the goal of letting the machine play the game, we need to make it perceive the screen display or information of the game, understand the rules of the game, and be able to find a way to correctly lead to higher scores in practice. The deep convolutional neural network in deep learning is very suitable for processing visual information and can solve the problem of visual information perception. Deep neural networks can be used to fit most functions, including the rules of the game. Deep reinforcement learning has the ability to learn all kinds of behaviors needed to play a game well. Compared with other methods, deep learning methods can also reduce the dependence on a large number of manually set action rules. Based on the above reasons, we believe that deep learning is a very good choice for machines to play games.\nHow are you planning to quantify or measure error or success? What implications does your quantification have?\nIn the field of games, we usually use a score to mark the achievements and progress of game players. The reinforcement learning model we use should be able to describe the actual situation corresponding to the score, and make the correct response and action to maximize the score. Under normal circumstances, the game will set a rule and a total score. Whenever the player completes a certain behavior, the player's score will increase. Of course, there are many complicated situations. In some games, the game will have multiple scores. And even in some cases these scores will be used and reduced. In this case, we need to set some importance, upper and lower bound limits for these different scores. For example, some games have the value of money, and players can choose to consume a certain amount of money in exchange for other scores or changes the game content. We need to set a more refined reinforcement learning reward function to accomplish these things. Some games do not have a clear and fixed score to indicate the current state of the game, or the state is not clear before the end of the game, such as chess. For this, we need to use other deep learning methods to describe the state. In general, for the games we are studying, we can use a single score to quantify our success and failure. The higher the single score, the better and more successful our method is.\nDivision of Labor\nRichard:\nBase model research and implementation of the Dueling DDQN Model\nShared experience buffer research and ideation\nOpenAI Gym environment pre-processing\nPython environment setup, user command-line integration, model checkpoint saving/loading\nEpisodic visualization, metrics logging\nModel training data generation\nFinal technical report write-up\nRaymond:\nShared experience buffer and state representation research, ideation, and implementation\nState-to-State Translation, Delta (with Time) Mapping, VAE + GAN latent space implementation\nModel training data generation\nMetrics (loss, Q-values, rewards, episodes) visualization\nModel metrics analysis\nKenta:\nBase model research and implementation of DQN Model\nShared experience buffer research and ideation\nGoogle Cloud Compute Engine integration\nModel training data generation\nPoster presentation, DevPost write-up, Final Video editing\nModel visualization\nAkash:\nShared experience buffer and state representation research, ideation, and implementation\nImplementation of joint naive shared experience Dueling DDQN Model\nGoogle Cloud Compute Engine integration\nModel training data generation\nEpisodic visualization\nUseful Links\nCheckPoint2\nFinal Technical Report\nLink to a Google Drive folder with the demo video and the write up. Check out some cool gifs of our model in action!", "link": "https://devpost.com/software/mid-diff", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "reder (related domain experience replay)\nmembers\nname cslogin\nraymond dai rdai4\nrichard tang rtang26\nakash singirikonda asingir1\nkenta yoshii kyoshii\nfinal technical report\nlink to a google drive folder with the demo video and the write up. check out some cool gifs of our model in action!\nintroduction\nartificial intelligence has been used to train models on increasingly complicated games, from chess to go to starcraft 2. these models have mainly been trained on a specific game, using techniques such as adversarial search and deep reinforcement learning (rl) with monte carlo -----> tree !!!  search (mcts) to optimize performance on that single board game. while successful, these models essentially train on exorbitant amounts of data, and must re-learn novel games from scratch.\nwe hypothesize that while games have different rules, their underlying mechanics are largely similar, so training on one game should theoretically improve performance across other games. specifically, using shared experience replay, we hope to employ rl to learn a general structure of similar games\nresults\nregular dueling ddqn\ndueling ddqn with joint experience replay (our model)\nthe top gameplay is by the dueling ddqn. the bottom two gameplays are by the dueling ddqn with joint experience replay. due to time limitations, we only trained each model for certain number of steps. based on this comparison, following points can be made:\nthe regular dueling ddqn model knew to shoot the bonus point red ship whenever it comes out. in contrast, our model pretty much ignored it.\nthis could be explained by the fact that since it is also trained on demonattack by means of joint experience replay, the importance of the red ship (which does not exist in the latter) did not get much importance (experience did not get shared).\ndueling ddqn with joint experience replay model could be interpreted as a little more \"aggressive\" than the baseline model as we can see from how it shoots off the objects from the get-go and by looking at the end result suggests.\nfinally, as we can see from the two gameplays, the doueling ddqn with joint experience replay does okay on both games. however, when comparing demonattack and spaceinvaders, it becomes clear that it performs better on the latter. this might be attributed to the fact that some experiences are unequally prioritized over others.\nrelated work\nplay atari with deep reinforcement learning\nhuman-level control through deep reinforcement learning\nunsupervised state representation learning in atari\ndata\nsince reder is a reinforcement model without any teacher enforcing, there is no need for us to collect data for this project. one thing to note is that we will be using openai gym to get access to the simulated environments for each game we will be using. with these simulated environments, our group is planning on training multiple models and comparing their performance to see how our reder performs with respect to other existing models. thankfully, openai has baseline implementation entirely open to access, making the process of comparing a lot more simple. as for the actual games we are going to use to train our model, we will be using spaceinvaders-v0 and demonattack-v0.\nspaceinvaders-v0\ndemonattack-v0\nopenai gym\nopenai baseline\nmethods\nwe used dueling double q-network(ddqn) for this project. two streams, advantage and value functions, are aggregated at the end to produce an estimate of state-action value function q. since in games, certain actions taken place in certain states could possibly carry less value than others. by decoupling the estimation, intuitively our dueling ddqn can learn which states are (or are not) valuable without having to learn the effect of each action at each state.\nthe above model serves as our general model structure. to realize our goal of creating a model that is extensible among similar atari games, we first tried implementing a gan as our shared buffer. however, this approach turned out to be infeasible since gan required us to have even more weights and added significant amount of time for our model training (running for 1 epoch took us 10 minutes). instead of using gan, we followed a naive approach where we first initialized the experience buffer with randomly generated experiences from each game (half from each). we then alternate playing each game by frame while training, storing each stack of frames in the experience buffer. we hypothesize that the q function would learn to identify similar situations in both games and use that experience to learn faster and avoid local minima. spaceinvaders becomes similar to demonattack when the shields disappear.\nfinally, we test our model on each game to see how well the model perform when compared with the openai baseline model.\nmetrics\nwe will consider openai baseline implementation as our standard and will compare our model against it.\na success constitutes an experimental architecture training correctly to convergence, even if it might not perform as well.\nthe training and testing environments will be chosen from openai gym.\naccuracy does not apply; rather having an agent maximize its reward quickly as possible is an important metric to track in rl.\nbase goals: implement ddqn and a standard rl model. our model should be able to learn from spaceinvaders-v0 and reach the score of 300 per episode.\ntarget goals: implemented dueling ddqn with naive state representation shows moderate signs of learning from spaceinvaders-v0 and demonattack-v0. this model then should be able to achieve 500 per episode on spaceinvaders-v0.\nstretch goals: implemented dueling ddqn with complex state representation shows strong signs of learning from spaceinvaders-v0 and demonattack-v0. this model then should be able to achieve 700 per episode on spaceinvaders-v0\nethics\nwhy is deep learning a good approach to this problem?\nin games, environment, rules, and operations are relatively complicated. in order to achieve the goal of letting the machine play the game, we need to make it perceive the screen display or information of the game, understand the rules of the game, and be able to find a way to correctly lead to higher scores in practice. the deep convolutional neural network in deep learning is very suitable for processing visual information and can solve the problem of visual information perception. deep neural networks can be used to fit most functions, including the rules of the game. deep reinforcement learning has the ability to learn all kinds of behaviors needed to play a game well. compared with other methods, deep learning methods can also reduce the dependence on a large number of manually set action rules. based on the above reasons, we believe that deep learning is a very good choice for machines to play games.\nhow are you planning to quantify or measure error or success? what implications does your quantification have?\nin the field of games, we usually use a score to mark the achievements and progress of game players. the reinforcement learning model we use should be able to describe the actual situation corresponding to the score, and make the correct response and action to maximize the score. under normal circumstances, the game will set a rule and a total score. whenever the player completes a certain behavior, the player's score will increase. of course, there are many complicated situations. in some games, the game will have multiple scores. and even in some cases these scores will be used and reduced. in this case, we need to set some importance, upper and lower bound limits for these different scores. for example, some games have the value of money, and players can choose to consume a certain amount of money in exchange for other scores or changes the game content. we need to set a more refined reinforcement learning reward function to accomplish these things. some games do not have a clear and fixed score to indicate the current state of the game, or the state is not clear before the end of the game, such as chess. for this, we need to use other deep learning methods to describe the state. in general, for the games we are studying, we can use a single score to quantify our success and failure. the higher the single score, the better and more successful our method is.\ndivision of labor\nrichard:\nbase model research and implementation of the dueling ddqn model\nshared experience buffer research and ideation\nopenai gym environment pre-processing\npython environment setup, user command-line integration, model checkpoint saving/loading\nepisodic visualization, metrics logging\nmodel training data generation\nfinal technical report write-up\nraymond:\nshared experience buffer and state representation research, ideation, and implementation\nstate-to-state translation, delta (with time) mapping, vae + gan latent space implementation\nmodel training data generation\nmetrics (loss, q-values, rewards, episodes) visualization\nmodel metrics analysis\nkenta:\nbase model research and implementation of dqn model\nshared experience buffer research and ideation\ngoogle cloud compute engine integration\nmodel training data generation\nposter presentation, devpost write-up, final video editing\nmodel visualization\nakash:\nshared experience buffer and state representation research, ideation, and implementation\nimplementation of joint naive shared experience dueling ddqn model\ngoogle cloud compute engine integration\nmodel training data generation\nepisodic visualization\nuseful links\ncheckpoint2\nfinal technical report\nlink to a google drive folder with the demo video and the write up. check out some cool gifs of our model in action!", "sortedWord": "None", "removed": "Nan", "score": 5, "comments": 11, "media": null, "medialink": null, "identifyer": 59503750}, {"Unnamed: 0": 3794, "autor": "Modzy.NET", "date": null, "content": "Inspiration\nThere aren't any official .NET clients for Modzy as yet. .NET is one of the most popular enterprise technologies today and increasingly .NET developers wish to easily incorporate machine learning into both their traditional desktop and web applications, as well as into Jupyter notebooks running C# and F# kernels. With .NET 6 and C# 10 released I wanted to create a Modzy interface for .NET developers too that could take advantage of some best practices and the latest language and platform features. At the same time I noticed some missing features with the official Modzy CLI, the main one being the inability to quickly submit model jobs with data and view the results which is an essential function when testing Modzy models. As a developer who uses the command-line extensively I wanted to create a modern developer-focused CLI that provided the fastest way to perform model inference with the Modzy API as a complement to the admin-focused official CLI.\nWhat it does\nModzy.NET is an unofficial .NET SDK for Modzy. It tries to offer the same features as the official Python SDK in a statically-typed library designed to be used by .NET developers. Modzy.NET also features a developer-oriented CLI which in addition to common admin operations allows you to run model inference on file and text data from the command line. The Modzy.NET CLI is a modern Unicode-based CLI with status bar, tree and progress widgets that is the fastest and easiest way for .NET developers to work with Modzy models and inference without requiring installation of Go or Python. The Modzy.NET library can be installed from NuGet into traditional desktop and web .NET applications as well as Jupyter notebooks using the .NET interactive C# and F# kernels.\nHow we built it\nI studied the official Python SDK to understand the Modzy REST API and created models for the incoming and outgoing JSON data. I used .NET asynchronous I/O throughout the library and used multithreading for performance where it made sense like fetching a list of all models. For the CLI I used libraries like SpecteConsole to create a sophisticated console user interface that doesn't sacrifice usability compared to a web interface.\nAccomplishments that we're proud of\nI was able to learn a lot about the Modzy API in a short timr.", "link": "https://devpost.com/software/modzy-net", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\nthere aren't any official .net clients for modzy as yet. .net is one of the most popular enterprise technologies today and increasingly .net developers wish to easily incorporate machine learning into both their traditional desktop and web applications, as well as into jupyter notebooks running c# and f# kernels. with .net 6 and c# 10 released i wanted to create a modzy interface for .net developers too that could take advantage of some best practices and the latest language and platform features. at the same time i noticed some missing features with the official modzy cli, the main one being the inability to quickly submit model jobs with data and view the results which is an essential function when testing modzy models. as a developer who uses the command-line extensively i wanted to create a modern developer-focused cli that provided the fastest way to perform model inference with the modzy api as a complement to the admin-focused official cli.\nwhat it does\nmodzy.net is an unofficial .net sdk for modzy. it tries to offer the same features as the official python sdk in a statically-typed library designed to be used by .net developers. modzy.net also features a developer-oriented cli which in addition to common admin operations allows you to run model inference on file and text data from the command line. the modzy.net cli is a modern unicode-based cli with status bar, -----> tree !!!  and progress widgets that is the fastest and easiest way for .net developers to work with modzy models and inference without requiring installation of go or python. the modzy.net library can be installed from nuget into traditional desktop and web .net applications as well as jupyter notebooks using the .net interactive c# and f# kernels.\nhow we built it\ni studied the official python sdk to understand the modzy rest api and created models for the incoming and outgoing json data. i used .net asynchronous i/o throughout the library and used multithreading for performance where it made sense like fetching a list of all models. for the cli i used libraries like specteconsole to create a sophisticated console user interface that doesn't sacrifice usability compared to a web interface.\naccomplishments that we're proud of\ni was able to learn a lot about the modzy api in a short timr.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59503794}, {"Unnamed: 0": 3923, "autor": "(AST)Walking around starport", "date": null, "content": "Inspiration\nI wanted to further understand the cosmos ecosystem. Starting with starport, which is the first step in the process of creating a new blockchain, seemed like the perfect first step.\nWhat it does\nIt builds ASTs for both the protobuf language files and the go files and manipulates them in order to insert the elements as required. One can either create completely new nodes and insert them in specific positions relative to other elements or mutate currently existing nodes as required.\nHow we built it\nProtocol buffers\nFor the protocol buffer language, support for parsing is available with emicklei/proto and is currently already used in starport (for the protopackages package). Since the protocol buffer language is small and simple, the project creates an abstraction package over it that allows creation of nodes via a higher-level API. In addition to that, functionality similar to astutil is implemented (in the form of a Cursor and Apply) in order to allow precise manipulation of positioning and early exit from the tree traversal (things not offered by the proto packages Walk functionality).\nCurrently protofmt is used in order to output the AST back into source file. Since this is relatively opinionated, though, a different formater is currently written and being tested that can be tailored to have as little opinions as possible.\nA new package, protoutil, has been created inside the pkg/protoanalysis folder.\nGolang\nFor go, most of the tools are readily available. Go's built in ast/parser/astutil give you everything you need in order to manipulate go elements. This includes parsing, manipulation and outputting of ASTs back to source files via printer.Fprint. Due to some shortcomings (bad support for comment parsing) of the ast package, though, Decorated Syntax Trees are used which provide the same set of tools.\nSince the language is generally large and more complex than protocol buffers, creating new nodes on the fly is not available via a more explicit high level package. Instead, strings are directly supplied to the parser which is tasked with creating new nodes.\nHelper/utility functions have been added directly in the goanalysis package.\nChallenges we ran into\nThe first challenge was understanding the project, how it operates, how to test it and how to augment it. The more I worked on it, the more clear things became.\nThe second main challenge involved understanding the Cursor objects as provided for astutil/dstutil and how to translate that over to protocol buffers. After panicking many times with reflect, I believe I've got a solid understanding of how it operates.\nAccomplishments that we're proud of\nI personally liked how the wrapping of the ASTs in the proto package was done. It made it easy to create new nodes and mutate files.\nWhat we learned\nMainly two things:\nThe codebase of starport and how it works\nThe structure of go/protobuf source files and the way in which to manipulate them.\nWhat's next for (AST)Walking around starport\nA couple of improvements/enhancements I can think off:\nRefactoring! There's a number of places where common code is used, these can be further consilidated in smaller utility functions that are shared.\nWhat to do when certain elements don't exist. For example, if a const () doesn't exist in certain files, the code doesn't currently try and create one. It could easily do this instead of failing.\nThe creation of go nodes could be more structured (instead of creating them from strings).\nAll in all, I believe this generally just requires another iteration of refactoring to get it to a much cleaner state.\nSee the project's README.md for additional notes!", "link": "https://devpost.com/software/ast-walking-around-starport", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\ni wanted to further understand the cosmos ecosystem. starting with starport, which is the first step in the process of creating a new blockchain, seemed like the perfect first step.\nwhat it does\nit builds asts for both the protobuf language files and the go files and manipulates them in order to insert the elements as required. one can either create completely new nodes and insert them in specific positions relative to other elements or mutate currently existing nodes as required.\nhow we built it\nprotocol buffers\nfor the protocol buffer language, support for parsing is available with emicklei/proto and is currently already used in starport (for the protopackages package). since the protocol buffer language is small and simple, the project creates an abstraction package over it that allows creation of nodes via a higher-level api. in addition to that, functionality similar to astutil is implemented (in the form of a cursor and apply) in order to allow precise manipulation of positioning and early exit from the -----> tree !!!  traversal (things not offered by the proto packages walk functionality).\ncurrently protofmt is used in order to output the ast back into source file. since this is relatively opinionated, though, a different formater is currently written and being tested that can be tailored to have as little opinions as possible.\na new package, protoutil, has been created inside the pkg/protoanalysis folder.\ngolang\nfor go, most of the tools are readily available. go's built in ast/parser/astutil give you everything you need in order to manipulate go elements. this includes parsing, manipulation and outputting of asts back to source files via printer.fprint. due to some shortcomings (bad support for comment parsing) of the ast package, though, decorated syntax trees are used which provide the same set of tools.\nsince the language is generally large and more complex than protocol buffers, creating new nodes on the fly is not available via a more explicit high level package. instead, strings are directly supplied to the parser which is tasked with creating new nodes.\nhelper/utility functions have been added directly in the goanalysis package.\nchallenges we ran into\nthe first challenge was understanding the project, how it operates, how to test it and how to augment it. the more i worked on it, the more clear things became.\nthe second main challenge involved understanding the cursor objects as provided for astutil/dstutil and how to translate that over to protocol buffers. after panicking many times with reflect, i believe i've got a solid understanding of how it operates.\naccomplishments that we're proud of\ni personally liked how the wrapping of the asts in the proto package was done. it made it easy to create new nodes and mutate files.\nwhat we learned\nmainly two things:\nthe codebase of starport and how it works\nthe structure of go/protobuf source files and the way in which to manipulate them.\nwhat's next for (ast)walking around starport\na couple of improvements/enhancements i can think off:\nrefactoring! there's a number of places where common code is used, these can be further consilidated in smaller utility functions that are shared.\nwhat to do when certain elements don't exist. for example, if a const () doesn't exist in certain files, the code doesn't currently try and create one. it could easily do this instead of failing.\nthe creation of go nodes could be more structured (instead of creating them from strings).\nall in all, i believe this generally just requires another iteration of refactoring to get it to a much cleaner state.\nsee the project's readme.md for additional notes!", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59503923}, {"Unnamed: 0": 4065, "autor": "Bias Identification of Political Media", "date": null, "content": "Conservative and Liberal Bias Detection in Political Media using Natural Language Processing and Neural Networks\nEliza Berman (eberman3) and Maia Mongado (mmongado)\nRead our full written report here.\nIntroduction:\nBiases infiltrate many aspects of spoken and written language. In the sphere of politics, media outlets, as well as individual journalists, often have strong political motivations that can lead to biased sentences in their published works. In our current polarized political climate, the quantity of media being produced from both conservative and liberal outlets has risen greatly. This has created an incredibly useful challenge for NLP researchers to automate the detection of political bias. In the paper that we are implementing, Political News Bias Detection using Machine Learning, the author \u200b\u200bMinh Vu describes the objectives of building a MultiLayer Perceptron model that characterizes articles by the percentages of liberal, conservative, and neutral sentences. This is an opinion classification problem. We believe this is a nontrivial area of research as the political biases that we are fed can greatly influence our own internalized biases moving forward, which can, in turn, cyclically feed us more political media with those biases. Therefore, it is important to be able to acknowledge and recognize biases present in the content we consume.\nRelated Work:\nWe are basing our work off of the paper, Political News Bias Detection using Machine Learning. This article details an implementation that takes as input a political article and outputs the percentages of liberal, conservative, and neutral sentences in the article.\nBeyond this paper, which we are re-implementing in part, there are a number of related works relevant to this topic. For example, in the paper, Political Ideology Detection Using Recursive Neural Networks, the authors design a framework that uses RNNs (as opposed to location-invariant approaches such as bag of words) to determine the political ideology at the sentence level. Additionally, in the paper, Detecting Political Bias in News Articles Using Headline Attention, the authors use an attention-based approach to predict political ideology based on headline. By using an attention-based approach, the authors are able to pay more attention to \u201ccritical content.\u201d These papers demonstrate different approaches to prediction, through RNNs, attention, MLPs and CNNs.\nData:\nOur dataset is the Ideological Books Corpus (IBC), a 2013 dataset of 4062 sentences annotated with political ideology (conservative, liberal, neutral). It has 2025 liberal sentences, 1071 conservative, and 655 neutral. It was crowdsourced through Crowdflower. Each sentence is represented by a parse tree where annotated nodes have a label {conservative, neutral, liberal}. So, it will require a bit of preprocessing to get it into a form ready to be passed into our neural network - Iyyer et al., 2014 includes a Python script on how to access the sentences, phrases, and annotations, so we will most likely be making various modifications to this script.\nMethodology:\nWe have found various papers on different neural networks trying to detect political biases. We are basing our work primarily off one that uses an MLP (rather than the traditional RNN for language learning) and fastText (similar to word2vec in that it obtains vector representations of words but has higher success with rarer words). It turns the words/sentences into vector representations with fastText then passes it to the MLP, which then outputs the classification of the sentence. The best results (81% accuracy) came with parameters (hidden_layer_sizes=(500, 20, 20, 20), max_iter=500, batch_size=32, warm_start=True, early_stopping=True), so we will most likely experiment with those in our architecture. We believe the hardest part about implementing our model will be determining MLP architecture - the paper we are primarily working with does not have too much specific info on what the actual MLP architecture they used is, so that will likely take a lot of experimenting with to determine how many layers we will use.\nMetrics:\nWe plan to run experiments on our multilayer perceptron model by tuning the hyperparameters to see which combination yields the highest accuracy (measured by precision, recall, and F1 score).\nIn the context of our project, the notion of accuracy refers to the number of sentences that were correctly labeled with the appropriate political ideology. We will use precision, recall, and F1 score as metrics for measuring the accuracy of our model. Precision is the total correctly predicted positives divided by the total number of positives. Recall is the total correctly predicted positives divided by the total number of observations. F1 score is the weighted average of precision and recall.\nThe authors of the existing project planned to use the metrics that they collected during experiments to optimize the hyperparameters of the MLP. The parameters that they optimized were: \u200b\u200bhidden_layer_size, max_iter, batch_size, and early_stopping. They also used precision, recall, and F1 score.\nOur base goal is to create a working architecture for our model that generates the desired metrics and correctly preprocesses and utilizes our data set. Our target goal is to achieve 70% accuracy on the multilayer perceptron. Our stretch goal is to implement RNN in addition to the multilayer perceptron so that we can compare performance across two different types of deep learning models.\nEthics:\nWhat broader societal issues are relevant to your chosen problem space? In the US today (and internationally) politics affects every sector of society. More specifically, skewed information and bias has become a very pressing issue; most people depend on the news to get basic information about most national and international events, but the agenda and truthfulness of various news outlets are now constantly being called into question. This can lead people to cast misinformed votes, reaffirm historical prejudices, and generally plays a major role in the pipeline to political extremism - when people cannot trust their sources to be neutral, paranoia and hostility quickly set in. So, identifying bias in news sources is of the utmost importance to societal issues such as the right-left divide in this country, the growing prominence of the alt-right, and in general all major political issues. What is your dataset? Are there any concerns about how it was collected, or labeled? Is it representative? What kind of underlying historical or societal biases might it contain? Our dataset is the Ideological Books Corpus (IBC), a 2013 dataset of 4062 sentences annotated with political ideology (conservative, liberal, neutral). It has 2025 liberal sentences, 1071 conservative, and 655 neutral. So, right away, it seems to be skewed in the liberal direction, which could pose a problem in detecting conservative/neutral biases. Also, political ideology itself is tough to pin down - what is considered a conservative standpoint now might have been different even just eight years ago when the IBC was originally annotated. So it could have biases from the political environment in 2013 in its annotation. The annotations were also crowdsourced through Crowdflower, so the annotations might be inconsistent - after all, what seems like a moderately conservative viewpoint to one person may seem moderately liberal to another.\nDivision of labor:\nMaia - I will be responsible for obtaining the dataset, possibly finding additional datasets, and preprocessing it to get in a form ready to pass through the fastText then MLP model. Eliza - I will be responsible for finding compute resources for the neural network and testing it on political news articles. Both - We will both be responsible for setting up the initial training pipeline, tuning hyperparameters and architecture, and doing the final research paper.", "link": "https://devpost.com/software/bias-identification-of-political-media", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "conservative and liberal bias detection in political media using natural language processing and neural networks\neliza berman (eberman3) and maia mongado (mmongado)\nread our full written report here.\nintroduction:\nbiases infiltrate many aspects of spoken and written language. in the sphere of politics, media outlets, as well as individual journalists, often have strong political motivations that can lead to biased sentences in their published works. in our current polarized political climate, the quantity of media being produced from both conservative and liberal outlets has risen greatly. this has created an incredibly useful challenge for nlp researchers to automate the detection of political bias. in the paper that we are implementing, political news bias detection using machine learning, the author \u200b\u200bminh vu describes the objectives of building a multilayer perceptron model that characterizes articles by the percentages of liberal, conservative, and neutral sentences. this is an opinion classification problem. we believe this is a nontrivial area of research as the political biases that we are fed can greatly influence our own internalized biases moving forward, which can, in turn, cyclically feed us more political media with those biases. therefore, it is important to be able to acknowledge and recognize biases present in the content we consume.\nrelated work:\nwe are basing our work off of the paper, political news bias detection using machine learning. this article details an implementation that takes as input a political article and outputs the percentages of liberal, conservative, and neutral sentences in the article.\nbeyond this paper, which we are re-implementing in part, there are a number of related works relevant to this topic. for example, in the paper, political ideology detection using recursive neural networks, the authors design a framework that uses rnns (as opposed to location-invariant approaches such as bag of words) to determine the political ideology at the sentence level. additionally, in the paper, detecting political bias in news articles using headline attention, the authors use an attention-based approach to predict political ideology based on headline. by using an attention-based approach, the authors are able to pay more attention to \u201ccritical content.\u201d these papers demonstrate different approaches to prediction, through rnns, attention, mlps and cnns.\ndata:\nour dataset is the ideological books corpus (ibc), a 2013 dataset of 4062 sentences annotated with political ideology (conservative, liberal, neutral). it has 2025 liberal sentences, 1071 conservative, and 655 neutral. it was crowdsourced through crowdflower. each sentence is represented by a parse -----> tree !!!  where annotated nodes have a label {conservative, neutral, liberal}. so, it will require a bit of preprocessing to get it into a form ready to be passed into our neural network - iyyer et al., 2014 includes a python script on how to access the sentences, phrases, and annotations, so we will most likely be making various modifications to this script.\nmethodology:\nwe have found various papers on different neural networks trying to detect political biases. we are basing our work primarily off one that uses an mlp (rather than the traditional rnn for language learning) and fasttext (similar to word2vec in that it obtains vector representations of words but has higher success with rarer words). it turns the words/sentences into vector representations with fasttext then passes it to the mlp, which then outputs the classification of the sentence. the best results (81% accuracy) came with parameters (hidden_layer_sizes=(500, 20, 20, 20), max_iter=500, batch_size=32, warm_start=true, early_stopping=true), so we will most likely experiment with those in our architecture. we believe the hardest part about implementing our model will be determining mlp architecture - the paper we are primarily working with does not have too much specific info on what the actual mlp architecture they used is, so that will likely take a lot of experimenting with to determine how many layers we will use.\nmetrics:\nwe plan to run experiments on our multilayer perceptron model by tuning the hyperparameters to see which combination yields the highest accuracy (measured by precision, recall, and f1 score).\nin the context of our project, the notion of accuracy refers to the number of sentences that were correctly labeled with the appropriate political ideology. we will use precision, recall, and f1 score as metrics for measuring the accuracy of our model. precision is the total correctly predicted positives divided by the total number of positives. recall is the total correctly predicted positives divided by the total number of observations. f1 score is the weighted average of precision and recall.\nthe authors of the existing project planned to use the metrics that they collected during experiments to optimize the hyperparameters of the mlp. the parameters that they optimized were: \u200b\u200bhidden_layer_size, max_iter, batch_size, and early_stopping. they also used precision, recall, and f1 score.\nour base goal is to create a working architecture for our model that generates the desired metrics and correctly preprocesses and utilizes our data set. our target goal is to achieve 70% accuracy on the multilayer perceptron. our stretch goal is to implement rnn in addition to the multilayer perceptron so that we can compare performance across two different types of deep learning models.\nethics:\nwhat broader societal issues are relevant to your chosen problem space? in the us today (and internationally) politics affects every sector of society. more specifically, skewed information and bias has become a very pressing issue; most people depend on the news to get basic information about most national and international events, but the agenda and truthfulness of various news outlets are now constantly being called into question. this can lead people to cast misinformed votes, reaffirm historical prejudices, and generally plays a major role in the pipeline to political extremism - when people cannot trust their sources to be neutral, paranoia and hostility quickly set in. so, identifying bias in news sources is of the utmost importance to societal issues such as the right-left divide in this country, the growing prominence of the alt-right, and in general all major political issues. what is your dataset? are there any concerns about how it was collected, or labeled? is it representative? what kind of underlying historical or societal biases might it contain? our dataset is the ideological books corpus (ibc), a 2013 dataset of 4062 sentences annotated with political ideology (conservative, liberal, neutral). it has 2025 liberal sentences, 1071 conservative, and 655 neutral. so, right away, it seems to be skewed in the liberal direction, which could pose a problem in detecting conservative/neutral biases. also, political ideology itself is tough to pin down - what is considered a conservative standpoint now might have been different even just eight years ago when the ibc was originally annotated. so it could have biases from the political environment in 2013 in its annotation. the annotations were also crowdsourced through crowdflower, so the annotations might be inconsistent - after all, what seems like a moderately conservative viewpoint to one person may seem moderately liberal to another.\ndivision of labor:\nmaia - i will be responsible for obtaining the dataset, possibly finding additional datasets, and preprocessing it to get in a form ready to pass through the fasttext then mlp model. eliza - i will be responsible for finding compute resources for the neural network and testing it on political news articles. both - we will both be responsible for setting up the initial training pipeline, tuning hyperparameters and architecture, and doing the final research paper.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 6, "media": null, "medialink": null, "identifyer": 59504065}, {"Unnamed: 0": 4340, "autor": "TORCHWOOD", "date": null, "content": "Inspiration\n\ud83d\ude80TORCHWOOD\ud83d\udd75\ufe0f\u200d\u2640\ufe0f \u2728 Welcome to TorchWood\u2728 This project is target to a very regular problem which grew larger and came to notice during covid situations. People are too bored and struggling with severe mental issues. Our application tries to overcome this boredom and mental health issue with the help of some easy and fun tasks. Helping them contribute to social issues during such hard times.\n\ud83d\udccc Briefing of the project : We have been into these tough situations for now over 2 years. People are restricted to stay at home and socialize less. There have been many cases reported where it is seen that people are suffering through severe mental health issues like stress, anxiety, depressive symptoms, insomnia, denial, anger and fear. People were not ready for such a sudden change in their routine and lifestyle. As the world has changed and many conventional entertainment sources are no more helpful. Here comes our application. Our application is specially targeted to help people overcome the above stated problems. So what does the app do? The app is like a social platform where people can join and the will get few weekly/monthly tasks to be done. Tasks would look something like Planting tree/watering tree/Donating food/Donating blood/Taking vaccination etc. In this way people can help out other people in socity and get entertained as well. After completion of every task the user will receive some points which they can redeem in many ways as they want or if they want they can donate the points to the application where we (team) can convert these points into monitory things (food, clothes) etc or donate it to a NGO. This application is totally target for helping society and it's issues dureing the covid time :)\n\ud83d\udccc Technologies used ReactJS (Frontend) NodeJS/ExpressJS (Backend) DataStax AstraDB (DataBase) Twilio-Chat (Communication)\nChallenges we ran into\nReact Router weren't working properly plus the database query was showing some error.\nAccomplishments that we're proud of\nThis game will help many to explore things.", "link": "https://devpost.com/software/torchwood", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\n\ud83d\ude80torchwood\ud83d\udd75\ufe0f\u200d\u2640\ufe0f \u2728 welcome to torchwood\u2728 this project is target to a very regular problem which grew larger and came to notice during covid situations. people are too bored and struggling with severe mental issues. our application tries to overcome this boredom and mental health issue with the help of some easy and fun tasks. helping them contribute to social issues during such hard times.\n\ud83d\udccc briefing of the project : we have been into these tough situations for now over 2 years. people are restricted to stay at home and socialize less. there have been many cases reported where it is seen that people are suffering through severe mental health issues like stress, anxiety, depressive symptoms, insomnia, denial, anger and fear. people were not ready for such a sudden change in their routine and lifestyle. as the world has changed and many conventional entertainment sources are no more helpful. here comes our application. our application is specially targeted to help people overcome the above stated problems. so what does the app do? the app is like a social platform where people can join and the will get few weekly/monthly tasks to be done. tasks would look something like planting -----> tree !!! /watering -----> tree !!! /donating food/donating blood/taking vaccination etc. in this way people can help out other people in socity and get entertained as well. after completion of every task the user will receive some points which they can redeem in many ways as they want or if they want they can donate the points to the application where we (team) can convert these points into monitory things (food, clothes) etc or donate it to a ngo. this application is totally target for helping society and it's issues dureing the covid time :)\n\ud83d\udccc technologies used reactjs (frontend) nodejs/expressjs (backend) datastax astradb (database) twilio-chat (communication)\nchallenges we ran into\nreact router weren't working properly plus the database query was showing some error.\naccomplishments that we're proud of\nthis game will help many to explore things.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59504340}, {"Unnamed: 0": 4535, "autor": "UCR Requisite Map", "date": null, "content": "Inspiration\nI've always wanted to build a website with a data visualization every since I saw a website that used D3.js to show potential career pathways.\nWhat it does\nThis website helps visualize prerequisites that need to be completed before registering for a course.\nHow we built it\nI wrote a Python script that queries UCR's registration website for requisite info with rate limited requests. This prerequires are parsed from their weird format into something that can be stored in a JSON file. I used Svelte and Chart.js to create the frontend SPA which reads the data from the JSON file then creates a tree on screen.\nChallenges we ran into\nThe most difficult part of the entire project was trying to learn and get D3.js to work correctly. D3.js is an industry standard library used for all sorts of data visualization needs and would have been perfect for this project. However, I couldn't get D3.js to work after many hours of effort and settles on using Chart.js instead. The second most difficulty part was trying to parse the prerequisites since the format is really weird:\n(\nCourse or Test: Anthropology 001\nMinimum Grade of D-\nMay not be taken concurrently.\n)\nor\n(\nCourse or Test: Anthropology 001H\nMinimum Grade of D-\nMay not be taken concurrently.\n)\nor\n(\nCourse or Test: Anthropology 001W\nMinimum Grade of D-\nMay not be taken concurrently.\n)\nand\n(\nCourse or Test: Anthropology 003\nMinimum Grade of D-\nMay not be taken concurrently.\n)\nor\n(\nCourse or Test: Anthropology 005\nMinimum Grade of D-\nMay not be taken concurrently.\n)\nAccomplishments that we're proud of\nI thought about giving up after I couldn't get D3.js to work but I couldn't give up the idea so I'm proud that I actually finished it.\nWhat we learned\nD3.js is pain\nWhat's next for UCR Requisite Map\nThough the basic website is finished, there are still some features I want to implement:\nAdd support for corequisites\nPrevent nodes from overlapping each other\nDifferentiate between prerequisites that are mutually exclusive (OR vs AND prerequisites)\nAdd feature to click on prerequisite to turn that course into the root node\nAdd more branches for prerequisites of prerequisites (e.g. if also show prerequisites of B if asked for prerequisites of A)", "link": "https://devpost.com/software/ucr-requisite-map", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\ni've always wanted to build a website with a data visualization every since i saw a website that used d3.js to show potential career pathways.\nwhat it does\nthis website helps visualize prerequisites that need to be completed before registering for a course.\nhow we built it\ni wrote a python script that queries ucr's registration website for requisite info with rate limited requests. this prerequires are parsed from their weird format into something that can be stored in a json file. i used svelte and chart.js to create the frontend spa which reads the data from the json file then creates a -----> tree !!!  on screen.\nchallenges we ran into\nthe most difficult part of the entire project was trying to learn and get d3.js to work correctly. d3.js is an industry standard library used for all sorts of data visualization needs and would have been perfect for this project. however, i couldn't get d3.js to work after many hours of effort and settles on using chart.js instead. the second most difficulty part was trying to parse the prerequisites since the format is really weird:\n(\ncourse or test: anthropology 001\nminimum grade of d-\nmay not be taken concurrently.\n)\nor\n(\ncourse or test: anthropology 001h\nminimum grade of d-\nmay not be taken concurrently.\n)\nor\n(\ncourse or test: anthropology 001w\nminimum grade of d-\nmay not be taken concurrently.\n)\nand\n(\ncourse or test: anthropology 003\nminimum grade of d-\nmay not be taken concurrently.\n)\nor\n(\ncourse or test: anthropology 005\nminimum grade of d-\nmay not be taken concurrently.\n)\naccomplishments that we're proud of\ni thought about giving up after i couldn't get d3.js to work but i couldn't give up the idea so i'm proud that i actually finished it.\nwhat we learned\nd3.js is pain\nwhat's next for ucr requisite map\nthough the basic website is finished, there are still some features i want to implement:\nadd support for corequisites\nprevent nodes from overlapping each other\ndifferentiate between prerequisites that are mutually exclusive (or vs and prerequisites)\nadd feature to click on prerequisite to turn that course into the root node\nadd more branches for prerequisites of prerequisites (e.g. if also show prerequisites of b if asked for prerequisites of a)", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 1, "media": null, "medialink": null, "identifyer": 59504535}, {"Unnamed: 0": 4608, "autor": "Heart Attack Predictor", "date": null, "content": "Inspiration\nHeart attack is a common cause of death nowadays, not only old people but younger ones have also been victims of this. Although we have specialized doctors in this, yet the doctor to patient ratio is huge making our medical infrastructure inefficient. This is where Machine Learning comes in, our model will predict the chances of heart attack of a person based on various parameters. With an accuracy of 83.51 % with an precision of 84.31% .\nWhat it does\nOn giving the required parameters our model predicts the possibility of heart attack. We have deployed our model on the internet.\nHow we built it\nWe tried with multiple Machine Learning algorithms like KNN , Logistic Regression , Decision Tree and Random Forest Classifier and looking at the initial performance, we tried to improve the model performance by hyper tuning the parameter, feature selection of Random Forest Classifier based model.\nRandom Forest Classifier Algorithm is a supervised algorithm classification method. In this algorithm, some trees form a forest. All trees in the Random Forest give the expected value of the class, and the class with the most votes is the model prediction. The three common methods are: Forest RI (random input selection). Forest RC (random mix). Combination of forest RI and forest RC. It can be used for both classification and regression problems, but good for dealing with classification problems and overcoming missing values. Further, after selecting the best model we deployed it using flask and Heroku.\nChallenges we ran into\nThe biggest challenge was selecting the most precise and accurate algorithm for our model.\nAccomplishments that we're proud of\nWe are quite proud that our model shows an accuracy of 83.51 % with an precision of *84.31% *. And it is very easy to use as it is deployed globally on the internet.\nWhat's next for Heart Attack Predictor\nWe are planning to make it more interactive by integrating with various medical APIs to make the website more useful and to make it a complete website and not only a detector one. We also plan to create a mobile application too to make it more feasible for users to interact and keep track of their heart condition.\nDiscord usernames of the team member:\nAyush Sinha- A Y U S H#9289\nKumari Astha- ikumariastha#8822", "link": "https://devpost.com/software/heart-attack-predictor", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\nheart attack is a common cause of death nowadays, not only old people but younger ones have also been victims of this. although we have specialized doctors in this, yet the doctor to patient ratio is huge making our medical infrastructure inefficient. this is where machine learning comes in, our model will predict the chances of heart attack of a person based on various parameters. with an accuracy of 83.51 % with an precision of 84.31% .\nwhat it does\non giving the required parameters our model predicts the possibility of heart attack. we have deployed our model on the internet.\nhow we built it\nwe tried with multiple machine learning algorithms like knn , logistic regression , decision -----> tree !!!  and random forest classifier and looking at the initial performance, we tried to improve the model performance by hyper tuning the parameter, feature selection of random forest classifier based model.\nrandom forest classifier algorithm is a supervised algorithm classification method. in this algorithm, some trees form a forest. all trees in the random forest give the expected value of the class, and the class with the most votes is the model prediction. the three common methods are: forest ri (random input selection). forest rc (random mix). combination of forest ri and forest rc. it can be used for both classification and regression problems, but good for dealing with classification problems and overcoming missing values. further, after selecting the best model we deployed it using flask and heroku.\nchallenges we ran into\nthe biggest challenge was selecting the most precise and accurate algorithm for our model.\naccomplishments that we're proud of\nwe are quite proud that our model shows an accuracy of 83.51 % with an precision of *84.31% *. and it is very easy to use as it is deployed globally on the internet.\nwhat's next for heart attack predictor\nwe are planning to make it more interactive by integrating with various medical apis to make the website more useful and to make it a complete website and not only a detector one. we also plan to create a mobile application too to make it more feasible for users to interact and keep track of their heart condition.\ndiscord usernames of the team member:\nayush sinha- a y u s h#9289\nkumari astha- ikumariastha#8822", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59504608}, {"Unnamed: 0": 4658, "autor": "Einflussfaktoren Mietpreise", "date": null, "content": "Inspiration\nWir wollten herausfinden, welche Faktoren einen wie starken Einfluss auf den Mietpreis f\u00fcr Wohnungen in der Stadt St. Gallen haben. Aus diesen Erkenntnissen wollten wir dann ein Modell f\u00fcr die Vorhersage von Mietpreisen aus den relevanten Faktoren konstruieren.\nWhat it does\nDas erstellte Modell kann aus den Faktoren Wohnfl\u00e4che, Baujahr und Anzahl nicht-Lebensmittell\u00e4den in einem 500m Radius um die Wohnung den Mietpreis einer Wohnung mit einem Fehler von ca. 400 Fr. vorhersagen.\nHow we built it\nDas Modell basiert auf ordinary least squares linearer Regression. Die Trainingsdaten wurden von Cividi \u00fcbernommen und durch eigens beschaffte Daten angereichert. Es wurden folgende Daten pro Wohnung erg\u00e4nzt:\nAnzahl Superm\u00e4rkte im 500m Radius\nAnzahl Erholungsorte im 500m Radius\nAnzahl nicht-Lebensmittell\u00e4den im 500m Radius\nDistanz zum n\u00e4chsten Bahnhof\nDistanz zum Hauptbahnhof\nH\u00f6he der Wohnung (m. \u00fc. M.)\nBei allen beschafften Daten wurde analysiert, ob sie einen signifikanten Einfluss auf den Mietpreis haben. Falls ein signifikanter Einfluss vorhanden war, wurde das Attribut in das Trainingsset aufgenommen.\nChallenges we ran into\nDas zur Verf\u00fcgung gestellte Datenset ist zu klein, um es gewinnbringend auf komplexere Machine Learning Modelle wie bspw. KNN Regression oder Decision Tree Regression anzuwenden.\nAccomplishments that we're proud of\nWir sind stolz darauf, dass wir sehr viele Faktoren in das bestehende Datenset integrieren konnten.\nWhat we learned\nBei geringer Datenqualit\u00e4t kann es schwierig sein korrekte Einflussfaktoren f\u00fcr die Mietpreise zu ermitteln.\nGeodaten von OSM k\u00f6nnen mittels Overpass Turbo relativ einfach extrahiert werden.\nBevor man Modelle trainiert, sollten die gelabelten Daten inspiziert werden, um einen \u00dcberblick zu erhalten.", "link": "https://devpost.com/software/einflussfaktoren-mietpreise", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\nwir wollten herausfinden, welche faktoren einen wie starken einfluss auf den mietpreis f\u00fcr wohnungen in der stadt st. gallen haben. aus diesen erkenntnissen wollten wir dann ein modell f\u00fcr die vorhersage von mietpreisen aus den relevanten faktoren konstruieren.\nwhat it does\ndas erstellte modell kann aus den faktoren wohnfl\u00e4che, baujahr und anzahl nicht-lebensmittell\u00e4den in einem 500m radius um die wohnung den mietpreis einer wohnung mit einem fehler von ca. 400 fr. vorhersagen.\nhow we built it\ndas modell basiert auf ordinary least squares linearer regression. die trainingsdaten wurden von cividi \u00fcbernommen und durch eigens beschaffte daten angereichert. es wurden folgende daten pro wohnung erg\u00e4nzt:\nanzahl superm\u00e4rkte im 500m radius\nanzahl erholungsorte im 500m radius\nanzahl nicht-lebensmittell\u00e4den im 500m radius\ndistanz zum n\u00e4chsten bahnhof\ndistanz zum hauptbahnhof\nh\u00f6he der wohnung (m. \u00fc. m.)\nbei allen beschafften daten wurde analysiert, ob sie einen signifikanten einfluss auf den mietpreis haben. falls ein signifikanter einfluss vorhanden war, wurde das attribut in das trainingsset aufgenommen.\nchallenges we ran into\ndas zur verf\u00fcgung gestellte datenset ist zu klein, um es gewinnbringend auf komplexere machine learning modelle wie bspw. knn regression oder decision -----> tree !!!  regression anzuwenden.\naccomplishments that we're proud of\nwir sind stolz darauf, dass wir sehr viele faktoren in das bestehende datenset integrieren konnten.\nwhat we learned\nbei geringer datenqualit\u00e4t kann es schwierig sein korrekte einflussfaktoren f\u00fcr die mietpreise zu ermitteln.\ngeodaten von osm k\u00f6nnen mittels overpass turbo relativ einfach extrahiert werden.\nbevor man modelle trainiert, sollten die gelabelten daten inspiziert werden, um einen \u00fcberblick zu erhalten.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59504658}, {"Unnamed: 0": 4714, "autor": "Altruistic Agent", "date": null, "content": "\u2728Inspiration\u2728\nBATMAN. Besides being the world\u2019s greatest detective, he does everything he can to make the world a better place. Batman runs multiple charitable organizations as Bruce Wayne and becomes a hero saving countless lives in Gotham City. We created a web-app that helps YOU be like batman.\n\u2699\ufe0fWhat it does\u2699\ufe0f\nThe web-app is essentially a mission directory for special agents like you, who want to become just like Batman. These missions are carefully selected to benefit society. An agent receives a message on the homepage after logging in. The agent can choose to view the mission directory, which takes him to a new page in our application. Over there, clicking a button generates a mission, which the agent has to complete. Some of our missions are \u2014 Donate Blood, Volunteer at a Soup Kitchen, Plant a Tree etc. The resources tab includes a list of organizations that would help the agent on their way along with a map.\n\ud83c\udfd7How we built it\ud83c\udfd7\nWe used TailwindCSS, VanillaJS and HTML to build the application. The Unsplash API lets us show images of our missions.\nWe also did full justice to tools like Google Cloud, Linode and GoDaddy provided by Major League Hacking this weekend. Here\u2019s how we used them:\n\ud83d\udfe1Use of Google Cloud - https://high-acre-325503.web.app/ \ud83d\udfe1\nWe built Altruistic Agent's authentication system with Google Cloud's Firebase. We chose this because we wanted to make an application that was VERY VERY SECURE \u2014 something Batman himself would use.\nWe learned how robust & fast Google Cloud services are and seeing that Firebase had a free plan that was great for us student hackers, using Google Cloud was kind of a no-brainer.\nAdditionally, Firebase Authentication provided a backend service, easy-to-use SDKs, and ready-made UI libraries, and the ability to authenticate using passwords, phone numbers, Google, Facebook and Twitter, and the like. Thus, implementation was easy and we are pretty sure we made it incredibly convenient for our users while keeping it secure.\nThe site is also hosted on Firebase (google cloud).\nWe used Google Maps API for the map component in our hack.\n\ud83d\udfe2Use of Linode Cloud\ud83d\udfe2\nWe tried to utilize Linode for its hosting and data storage through Cloudaways. Linode is one of the top IaaS providers and is incredibly easy to use. Besides, we had a great experience using Linode twice previously, and the free Linode credit from MLH for us to learn and build on Linode was the cherry on the cake! Linode is fast, flexible, and reliable, and we truly enjoyed using it, and we can say that Linode truly took our hack to the next level.\n\ud83d\udea7Challenges we ran into\ud83d\udea7\nUnsplash API kept giving us irrelevant images: We fixed this by changing our keywords.\nOur site looked dull and boring: We fixed it by using Tailwind CSS Framework and Tailwind UI\n\ud83c\udfc6Accomplishments that we're proud of\ud83c\udfc6\nHaving completed a project this weekend with new hackers among us.\n\ud83d\udcdaWhat we learned\ud83d\udcda\nWe learned a lot about Firebase and Linode: we went through their neat documentation and we were glad to be able to use their services in our hackathon project.\nWe learned the importance of collaboration.\nThis was Samar\u2019s first of hackathons and he learnt a lot of HTML, CSS and JS this weekend.\n\ud83d\udd2eWhat's next for Altruistic Agent\ud83d\udd2e\nWe plan to refine the user experience, add more missions and improve our service based on the feedback of agents around the world.", "link": "https://devpost.com/software/altruistic-agent", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "\u2728inspiration\u2728\nbatman. besides being the world\u2019s greatest detective, he does everything he can to make the world a better place. batman runs multiple charitable organizations as bruce wayne and becomes a hero saving countless lives in gotham city. we created a web-app that helps you be like batman.\n\u2699\ufe0fwhat it does\u2699\ufe0f\nthe web-app is essentially a mission directory for special agents like you, who want to become just like batman. these missions are carefully selected to benefit society. an agent receives a message on the homepage after logging in. the agent can choose to view the mission directory, which takes him to a new page in our application. over there, clicking a button generates a mission, which the agent has to complete. some of our missions are \u2014 donate blood, volunteer at a soup kitchen, plant a -----> tree !!!  etc. the resources tab includes a list of organizations that would help the agent on their way along with a map.\n\ud83c\udfd7how we built it\ud83c\udfd7\nwe used tailwindcss, vanillajs and html to build the application. the unsplash api lets us show images of our missions.\nwe also did full justice to tools like google cloud, linode and godaddy provided by major league hacking this weekend. here\u2019s how we used them:\n\ud83d\udfe1use of google cloud - https://high-acre-325503.web.app/ \ud83d\udfe1\nwe built altruistic agent's authentication system with google cloud's firebase. we chose this because we wanted to make an application that was very very secure \u2014 something batman himself would use.\nwe learned how robust & fast google cloud services are and seeing that firebase had a free plan that was great for us student hackers, using google cloud was kind of a no-brainer.\nadditionally, firebase authentication provided a backend service, easy-to-use sdks, and ready-made ui libraries, and the ability to authenticate using passwords, phone numbers, google, facebook and twitter, and the like. thus, implementation was easy and we are pretty sure we made it incredibly convenient for our users while keeping it secure.\nthe site is also hosted on firebase (google cloud).\nwe used google maps api for the map component in our hack.\n\ud83d\udfe2use of linode cloud\ud83d\udfe2\nwe tried to utilize linode for its hosting and data storage through cloudaways. linode is one of the top iaas providers and is incredibly easy to use. besides, we had a great experience using linode twice previously, and the free linode credit from mlh for us to learn and build on linode was the cherry on the cake! linode is fast, flexible, and reliable, and we truly enjoyed using it, and we can say that linode truly took our hack to the next level.\n\ud83d\udea7challenges we ran into\ud83d\udea7\nunsplash api kept giving us irrelevant images: we fixed this by changing our keywords.\nour site looked dull and boring: we fixed it by using tailwind css framework and tailwind ui\n\ud83c\udfc6accomplishments that we're proud of\ud83c\udfc6\nhaving completed a project this weekend with new hackers among us.\n\ud83d\udcdawhat we learned\ud83d\udcda\nwe learned a lot about firebase and linode: we went through their neat documentation and we were glad to be able to use their services in our hackathon project.\nwe learned the importance of collaboration.\nthis was samar\u2019s first of hackathons and he learnt a lot of html, css and js this weekend.\n\ud83d\udd2ewhat's next for altruistic agent\ud83d\udd2e\nwe plan to refine the user experience, add more missions and improve our service based on the feedback of agents around the world.", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59504714}, {"Unnamed: 0": 5098, "autor": "TreatMap", "date": null, "content": "TreatMap\nArjun Sarao, Bram Ogus, Connor Wilson\n\ud83d\udca1 Inspiration\nCreating some tools to maximize candy collection on Halloween has been a dream of ours since childhood.\n\ud83d\udcf1 What it does\nTreatMap gamifies the process of collecting candies. TreatMap aggregates user reports on what candy is being given out at specific houses. The user then decides on what candy they would like to collect and TreatMap automagically routes them to the closest location where said candy is available.\n\ud83d\udee0 How we built it\nAdobe XD: We designed the mockups in Adobe XD to figure out the UI/UX\nFlutter: We built the frontend in Flutter\nGO: We created the backend in GO\nFirebase: We also made our backend with firebase\nApple Map Kit: This renders the actual map and displays the locations where candy is found\nGoogle Directions API: As the name suggests, we used this API to find directions to candy\n\ud83d\uded1 Challenges we ran into\nIssues with GO syntax\nMaking our GO backend communicate with the frontend and firebase\nA tree fell onto one of our teammates' houses, taking out his internet for half a day\nThere was a scheduled outage at another teammates house, leaving him without internet for 5 hours\n\u2705 Accomplishments that we're proud of\nWriting our own from-scratch GO backend, this was the first time our team used GO in a hackathon project\nBeing able to finish this app despite internet and electricity shortages\n\ud83d\udcd6 What we learned\nGin web framework for GO, allows you to run an API on the web\nUsing types structs in GO\nMaking GO communicate with Firebase\nGetting direction coordinates from the maps API in Flutter\n\ud83e\udd14 What's next for TreatMap\nAdd different themes\nAdding scores based on how much candy you collect, the weight of candy, and the number of user reports\nAdd geotagging to automagically add candies to your score\nAdd teams so you can compete alongside your friends and family\nDynamically allocating point value to candies to incentivizes collecting rarer types of candy\nML, duh\nTech Stack\nFlutter\nGOLANG\nFirebase / Google Cloud\n\ud83d\udcbb Lines of Code\n1118 for the flutter app and 137 for the GO backend\n\ud83d\ude47\u200d\u2642\ufe0f Acknowledgements\nWe would like to thank Hack-o-lantern for the opportunity to create and develop our idea.", "link": "https://devpost.com/software/treatmap", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "treatmap\narjun sarao, bram ogus, connor wilson\n\ud83d\udca1 inspiration\ncreating some tools to maximize candy collection on halloween has been a dream of ours since childhood.\n\ud83d\udcf1 what it does\ntreatmap gamifies the process of collecting candies. treatmap aggregates user reports on what candy is being given out at specific houses. the user then decides on what candy they would like to collect and treatmap automagically routes them to the closest location where said candy is available.\n\ud83d\udee0 how we built it\nadobe xd: we designed the mockups in adobe xd to figure out the ui/ux\nflutter: we built the frontend in flutter\ngo: we created the backend in go\nfirebase: we also made our backend with firebase\napple map kit: this renders the actual map and displays the locations where candy is found\ngoogle directions api: as the name suggests, we used this api to find directions to candy\n\ud83d\uded1 challenges we ran into\nissues with go syntax\nmaking our go backend communicate with the frontend and firebase\na -----> tree !!!  fell onto one of our teammates' houses, taking out his internet for half a day\nthere was a scheduled outage at another teammates house, leaving him without internet for 5 hours\n\u2705 accomplishments that we're proud of\nwriting our own from-scratch go backend, this was the first time our team used go in a hackathon project\nbeing able to finish this app despite internet and electricity shortages\n\ud83d\udcd6 what we learned\ngin web framework for go, allows you to run an api on the web\nusing types structs in go\nmaking go communicate with firebase\ngetting direction coordinates from the maps api in flutter\n\ud83e\udd14 what's next for treatmap\nadd different themes\nadding scores based on how much candy you collect, the weight of candy, and the number of user reports\nadd geotagging to automagically add candies to your score\nadd teams so you can compete alongside your friends and family\ndynamically allocating point value to candies to incentivizes collecting rarer types of candy\nml, duh\ntech stack\nflutter\ngolang\nfirebase / google cloud\n\ud83d\udcbb lines of code\n1118 for the flutter app and 137 for the go backend\n\ud83d\ude47\u200d\u2642\ufe0f acknowledgements\nwe would like to thank hack-o-lantern for the opportunity to create and develop our idea.", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59505098}, {"Unnamed: 0": 5154, "autor": "Campus Mapper", "date": null, "content": "Inspiration\nWhat inspired me to make this project was my friend asking me if I would be late to class. I told her \"of course not, it's a six minute walk.\" Then I wanted to prove it.\nWhat it does\nThis program takes location names, their latitude and longitude, and the time it takes to reach them from a previous point to construct a map of potential routes to academic buildings then calculate the fastest route\nHow we built it\nThis program started on Friday, Oct 29th as I walked around campus in the rain with Under Armour's MapMyWalk app running on my phone collecting data on how long it took for me to walk between various points on campus. That evening, after HackUMBC had started, I began by going through the documentation for Streamlit's Map and Pandas' DataFrame to construct a visual representation of my data. On the following day, I built up functions to take this data and construct nodes which could be represented as a tree through which to determine a., which paths were possible to take from one point to another, and b., the paths that yielded the fastest time. In the wee hours of Halloween, I tidied up the code, removed non-functioning implementation, and fixed an error with the array indexing until I was happy with my demonstration.\nChallenges we ran into\nThe way by which the routes were originally meant to be calculated was a tree with DFS applied to it. However, I hit a wall with the node object attributes being read in as string values. Since I had committed to using the Streamlit framework, and thus Python, I decided to represent this tree as an array of nodes and to evaluate my pathways from there. Due to this setback, the csv file that was supposed to accompany this project was turned into a string so as to save time implementing loading and reading in the file.\nAccomplishments that we're proud of\nI am proud of myself for pushing myself outside my comfort zone of C++ and tackling API for the first time in my academic career. The rigorous documentation Streamlit provides for their library gave me a framework to build off of that led to an interface for what I had originally thought of as being only back-end code.\nWhat we learned\nI learned how to get back to the basics with arrays with the enhancements of my current knowledge that enabled me to overcome difficulties in coding practices between languages.\nWhat's next for Campus Mapper\nTo add more data points! Due to the rain on Friday, data collection was far more limited than I'd hoped for and going forward I would like to have the entire UMBC campus mapped out. In addition, streamlining of all data inputs, including the addition of a csv file for data, and DataFrames to give the program a sleeker look.", "link": "https://devpost.com/software/campus-mapper", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\nwhat inspired me to make this project was my friend asking me if i would be late to class. i told her \"of course not, it's a six minute walk.\" then i wanted to prove it.\nwhat it does\nthis program takes location names, their latitude and longitude, and the time it takes to reach them from a previous point to construct a map of potential routes to academic buildings then calculate the fastest route\nhow we built it\nthis program started on friday, oct 29th as i walked around campus in the rain with under armour's mapmywalk app running on my phone collecting data on how long it took for me to walk between various points on campus. that evening, after hackumbc had started, i began by going through the documentation for streamlit's map and pandas' dataframe to construct a visual representation of my data. on the following day, i built up functions to take this data and construct nodes which could be represented as a -----> tree !!!  through which to determine a., which paths were possible to take from one point to another, and b., the paths that yielded the fastest time. in the wee hours of halloween, i tidied up the code, removed non-functioning implementation, and fixed an error with the array indexing until i was happy with my demonstration.\nchallenges we ran into\nthe way by which the routes were originally meant to be calculated was a tree with dfs applied to it. however, i hit a wall with the node object attributes being read in as string values. since i had committed to using the streamlit framework, and thus python, i decided to represent this tree as an array of nodes and to evaluate my pathways from there. due to this setback, the csv file that was supposed to accompany this project was turned into a string so as to save time implementing loading and reading in the file.\naccomplishments that we're proud of\ni am proud of myself for pushing myself outside my comfort zone of c++ and tackling api for the first time in my academic career. the rigorous documentation streamlit provides for their library gave me a framework to build off of that led to an interface for what i had originally thought of as being only back-end code.\nwhat we learned\ni learned how to get back to the basics with arrays with the enhancements of my current knowledge that enabled me to overcome difficulties in coding practices between languages.\nwhat's next for campus mapper\nto add more data points! due to the rain on friday, data collection was far more limited than i'd hoped for and going forward i would like to have the entire umbc campus mapped out. in addition, streamlining of all data inputs, including the addition of a csv file for data, and dataframes to give the program a sleeker look.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59505154}, {"Unnamed: 0": 5163, "autor": "Halloween Costume Recommender", "date": null, "content": "The prospect of choosing a costume is always daunting; there are so many to choose from. Many of us choose costumes based on certain characteristics we liked, so we designed an app that chooses costumes for us using the same criteria.\nOur app asks the user for adjectives describing their ideal costume. We then map these adjectives to descriptions of costumes using natural language processing. Based on which descriptions have the best fit, the app then recommends costumes.\nWe built the app using the Vue framework.\nOne of the challenges we ran into was developing a presentable U.I. We needed it to convey the information in an aesthetic manner. A lot of time was spent tweaking and editing it.\nWe're proud of how the logic turned out. It was pretty accurate, giving us insightful recommendations for each of our inputs.\nWe learned a lot about natural language processing.\nNext, we would add a candy recommender to the app. It would recommend candies through a decision tree asking about whether the user likes chocolate, gum, and other factors.", "link": "https://devpost.com/software/halloween-costume-recommender-z5gm08", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "the prospect of choosing a costume is always daunting; there are so many to choose from. many of us choose costumes based on certain characteristics we liked, so we designed an app that chooses costumes for us using the same criteria.\nour app asks the user for adjectives describing their ideal costume. we then map these adjectives to descriptions of costumes using natural language processing. based on which descriptions have the best fit, the app then recommends costumes.\nwe built the app using the vue framework.\none of the challenges we ran into was developing a presentable u.i. we needed it to convey the information in an aesthetic manner. a lot of time was spent tweaking and editing it.\nwe're proud of how the logic turned out. it was pretty accurate, giving us insightful recommendations for each of our inputs.\nwe learned a lot about natural language processing.\nnext, we would add a candy recommender to the app. it would recommend candies through a decision -----> tree !!!  asking about whether the user likes chocolate, gum, and other factors.", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59505163}, {"Unnamed: 0": 5272, "autor": "ChadBot", "date": null, "content": "Inspiration\nWith obesity rates rising and COVID forcing people to a more sedentary life, we wanted to build something that would encourage working out.\nWhat it does\nChadBot is an AI virtual assistant that will learn where the user is in their fitness journey, how much time they can put into the gym, and what their goals are to present a workout plan in an encouraging and casual conversational manner. Additionally, we implemented a body fat percentage predictor that uses facial landmarks and detected features.\nHow we built it\nBody Fat Predictor We collected and labeled hundreds of facial pictures in a tree folder structure (for ease and fast collection) to train our Random Forrest Algorithm model. To preprocess our image data, we used inorder traversal through our tree and ran our facial points detection module on each image to get a normalized array of x, y, and z. coordinates for 468 different facial landmarks.\nChadBot We created an expansive JSON file (that we added to our Cockroach DB as a JSON object to allow for serializable isolation in the case we may need additional JSON files later) with multiple patterns in user responses each associated with a tag that represents the user's intent. We developed the path of the conversation to depend on the context built up through the user's responses and the response's corresponding tags. We then integrated our Body Fat Percentage Predictor into our bot using python modules.\nChallenges we ran into\nData collection and labeling in such a short time was an issue. We developed an approach of creating a tree folder structure where the folder names contain the labels. This allowed us to simply drag and drop images and label them in our python code based on file location within the tree.\nAccomplishments that we're proud of\nWe are proud of coming up with unique and clever solutions in moments of stress when we were backed into a corner. We are proud of building the product we had in mind at the start and not having to cut corners despite the time pressure. We are proud of all the team members for completing their roles in a timely manner.\nWhat we learned\nWe learned how to architect a big project with a team. We learned a lot about Deep Learning and NLP. It was cool to see how a program can be taught how to understand natural language.\nWhat's next for ChadBot\nWe plan on building this into a mobile application with additional features such as progress tracking and sharing. We plan on refining the features that are passed into the ML model and collecting a larger dataset to improve our predictions.", "link": "https://devpost.com/software/chadbot-5f9vik", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\nwith obesity rates rising and covid forcing people to a more sedentary life, we wanted to build something that would encourage working out.\nwhat it does\nchadbot is an ai virtual assistant that will learn where the user is in their fitness journey, how much time they can put into the gym, and what their goals are to present a workout plan in an encouraging and casual conversational manner. additionally, we implemented a body fat percentage predictor that uses facial landmarks and detected features.\nhow we built it\nbody fat predictor we collected and labeled hundreds of facial pictures in a -----> tree !!!  folder structure (for ease and fast collection) to train our random forrest algorithm model. to preprocess our image data, we used inorder traversal through our tree and ran our facial points detection module on each image to get a normalized array of x, y, and z. coordinates for 468 different facial landmarks.\nchadbot we created an expansive json file (that we added to our cockroach db as a json object to allow for serializable isolation in the case we may need additional json files later) with multiple patterns in user responses each associated with a tag that represents the user's intent. we developed the path of the conversation to depend on the context built up through the user's responses and the response's corresponding tags. we then integrated our body fat percentage predictor into our bot using python modules.\nchallenges we ran into\ndata collection and labeling in such a short time was an issue. we developed an approach of creating a tree folder structure where the folder names contain the labels. this allowed us to simply drag and drop images and label them in our python code based on file location within the tree.\naccomplishments that we're proud of\nwe are proud of coming up with unique and clever solutions in moments of stress when we were backed into a corner. we are proud of building the product we had in mind at the start and not having to cut corners despite the time pressure. we are proud of all the team members for completing their roles in a timely manner.\nwhat we learned\nwe learned how to architect a big project with a team. we learned a lot about deep learning and nlp. it was cool to see how a program can be taught how to understand natural language.\nwhat's next for chadbot\nwe plan on building this into a mobile application with additional features such as progress tracking and sharing. we plan on refining the features that are passed into the ml model and collecting a larger dataset to improve our predictions.", "sortedWord": "None", "removed": "Nan", "score": 5, "comments": 0, "media": null, "medialink": null, "identifyer": 59505272}, {"Unnamed: 0": 5289, "autor": "Trick or Tree", "date": null, "content": "Inspiration\nInspiration came from the Discord chatbot, which assists us in many ways especially in Hackathon events. in this Covid situation, people are skeptical to visit and interact. But Halloween is there and we want to give candies maintaining the social distance. Also, these days we are hurting our environment badly. The least we could do is plant as many trees as possible. So, to make children/youngsters/everyone encourage the planting of trees.\nWhat it does\nWe can give the WhatsApp chatbot number outside the door. Anyone can scan the code and play trick or treat with a chatbot on their phones. The chatbot will ask Trick or Treat. Then it goes through multiple steps like giving the user a quest to solve. And have an option to select from a treat or Tree. If a user selects tree, then he or she will be given a plant along with candies(A bumper prize hidden). Owner of this game will receive a message with name of person who has selected tree or treat. Then Owner can create candies and tress and have a name on top of it. This can be placed outside the house, keeping social distance and people can come and check for their treats.\nHow we built it\nWe build it using Twillio studio\nChallenges we ran into\nCreating a WhatsApp Sandbox. Using Twillo studio.\nAccomplishments that we're proud of\nWorking Chatbot\nWhat we learned\nUsing Twillo Message APIs to send messages using REST APIs. Creating a chatbot using Twilio. Using a function call within Twilio.\nWhat's next for Trick or Tree\nlinking of database to keep a record of all the users who opted for treats or trees and more analytical for this Chatbot. Creating more quests by linking with backend databases.", "link": "https://devpost.com/software/trick-or-tree-6l0ja1", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\ninspiration came from the discord chatbot, which assists us in many ways especially in hackathon events. in this covid situation, people are skeptical to visit and interact. but halloween is there and we want to give candies maintaining the social distance. also, these days we are hurting our environment badly. the least we could do is plant as many trees as possible. so, to make children/youngsters/everyone encourage the planting of trees.\nwhat it does\nwe can give the whatsapp chatbot number outside the door. anyone can scan the code and play trick or treat with a chatbot on their phones. the chatbot will ask trick or treat. then it goes through multiple steps like giving the user a quest to solve. and have an option to select from a treat or -----> tree !!! . if a user selects tree, then he or she will be given a plant along with candies(a bumper prize hidden). owner of this game will receive a message with name of person who has selected tree or treat. then owner can create candies and tress and have a name on top of it. this can be placed outside the house, keeping social distance and people can come and check for their treats.\nhow we built it\nwe build it using twillio studio\nchallenges we ran into\ncreating a whatsapp sandbox. using twillo studio.\naccomplishments that we're proud of\nworking chatbot\nwhat we learned\nusing twillo message apis to send messages using rest apis. creating a chatbot using twilio. using a function call within twilio.\nwhat's next for trick or tree\nlinking of database to keep a record of all the users who opted for treats or trees and more analytical for this chatbot. creating more quests by linking with backend databases.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59505289}, {"Unnamed: 0": 5454, "autor": "[KANARIA] Battle of Eternity", "date": null, "content": "Players will have to take a side in the infinite war of darkness and light and fight for supremacy over the Tree of Light. Participants should make their way from the roots of the Tree to the top and slay the Dragon of Oblivion. The winning side takes the benefits and sacred knowledge of the astral union.\nInspiration\nWe want to create a thrilling and non-intrusive game so that the Birds fulfill their potential to the full extent. In the game every player will have the ability to contribute and lead its team to victory! The supremacy over the Tree of Life will depend on each bird.\nThe Game\nWe create an interesting game for all bird owners! Interesting game mechanics will allow each participant to choose the team of darkness or light and help his/her team to occupy the top of the Tree and dominate there for two weeks. Participants of the Battle will gain experience and level up their fighting birds! The higher the level each bird achieved, the more chances the team has to win! Each bird can become a leader and lift its team to a win. The first team to slay the final boss, the Dragon of Oblivion, wins. The winning team receives valuable prizes and bonuses.\nHow we build it\nIn order to create our game on smart contracts, we need to write a bridge to MOVR to be able to transfer our birds to the MOVR network. We will then need to write a smart contract for the game on the MOVR blockchain.\nChallenges we face\nWe need to develop the architecture of the contract for the game and make it intercommunicate with the game module. Lots of tests, experiments, and maximum of safety!\nWhat we learned\nWe learned how to deal with smart contracts, how to transfer NTF safely and how to use it then for games.\nTargets\nAs a result, we will create a Bridge and a Game which will supplement the wide functionality of NFT on RMRK. We will also learn to knit together the game module with a smart contract and communicate with it through the game interface.\nWhat's next\nIntroduction of the elements. Elemental monsters (fire, water, air, earth), which will get additional damage if the attacking bird owns a counter-element;\nBird\u2019s skills. A skill tree which adds bonuses for the birds;\nIntermediate Mini Bosses;\nPvP. A system of duels against players of an opposing team;\nDebuffs for the opposing team;\nTeaming up to attack Mini Bosses;\nMagic wwoww ~~!", "link": "https://devpost.com/software/battle-of-eternity", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "players will have to take a side in the infinite war of darkness and light and fight for supremacy over the -----> tree !!!  of light. participants should make their way from the roots of the tree to the top and slay the dragon of oblivion. the winning side takes the benefits and sacred knowledge of the astral union.\ninspiration\nwe want to create a thrilling and non-intrusive game so that the birds fulfill their potential to the full extent. in the game every player will have the ability to contribute and lead its team to victory! the supremacy over the tree of life will depend on each bird.\nthe game\nwe create an interesting game for all bird owners! interesting game mechanics will allow each participant to choose the team of darkness or light and help his/her team to occupy the top of the tree and dominate there for two weeks. participants of the battle will gain experience and level up their fighting birds! the higher the level each bird achieved, the more chances the team has to win! each bird can become a leader and lift its team to a win. the first team to slay the final boss, the dragon of oblivion, wins. the winning team receives valuable prizes and bonuses.\nhow we build it\nin order to create our game on smart contracts, we need to write a bridge to movr to be able to transfer our birds to the movr network. we will then need to write a smart contract for the game on the movr blockchain.\nchallenges we face\nwe need to develop the architecture of the contract for the game and make it intercommunicate with the game module. lots of tests, experiments, and maximum of safety!\nwhat we learned\nwe learned how to deal with smart contracts, how to transfer ntf safely and how to use it then for games.\ntargets\nas a result, we will create a bridge and a game which will supplement the wide functionality of nft on rmrk. we will also learn to knit together the game module with a smart contract and communicate with it through the game interface.\nwhat's next\nintroduction of the elements. elemental monsters (fire, water, air, earth), which will get additional damage if the attacking bird owns a counter-element;\nbird\u2019s skills. a skill tree which adds bonuses for the birds;\nintermediate mini bosses;\npvp. a system of duels against players of an opposing team;\ndebuffs for the opposing team;\nteaming up to attack mini bosses;\nmagic wwoww ~~!", "sortedWord": "None", "removed": "Nan", "score": 13, "comments": 6, "media": null, "medialink": null, "identifyer": 59505454}, {"Unnamed: 0": 5846, "autor": "Smart Picks", "date": null, "content": "Inspiration\nOur team strongly feels that decentralized betting platforms are the next logical step for DeFi. Our goal was to create something that we could see ourselves using, and increase the adoption rate of blockchain and cryptocurrencies.\nPer Visual Capitalist's research, we know that online sports betting is a burgeoning industry. We see an opportunity to contribute to this space and believe that a substantial part of this market would move to trustless blockchain solutions.\nWhen the idea for Smart Picks came up, we knew immediately that we wanted to do it. We knew this would be an amazing, fun thing to create, with plenty of room for us to learn and grow as engineers. Now that Smart Picks is built, we want to continue to broaden its use cases and bring it to market in time for the 2022 March Madness Tournament.\nWhat it does\nSmart Picks is a dApp that lets users create and enter into NCAA March Madness betting pools. The UI allows users to build and save brackets, with an easy-to-use interface. Each pool is a smart contract, which uses Chainlink price feeds to provide the up-to-date price in $ETH, $MATIC, or $AVAX. We use SportsDataIO for our Oracle\u2019s data source and, to maintain proper decentralization, a keeper contract will close all of the pools 24 hours after the tournament has ended.\nHow we built it\nFront End\nThe front end is a React app that uses Material UI. We used Moralis to connect to our smart contracts, and to allow us to switch between our supported chains. Metamask is used to facilitate all transactions, as well as handle authentication.\nSmart Contracts\nEach chain has its own Pool Factory smart contract, through which users can generate their own pool. All smart contracts maintain strict rules for buy-in price, number of entrants allowed, and one bracket per user.\nOracle\nWe deployed Chainlink\u2019s Oracle smart contract and pointed it at our node. We hosted our Chainlink node and related jobs on LinkPool's NaaS.\nDeploying\nOur smart contracts were deployed using Hardhat scripts, and our dApp is deployed on IPFS via fleek.\nChallenges we ran into\nThe first challenge we faced was setting up our smart contracts. We spent a lot of time reading through Solidity documentation and testing things out on Remix before we finally got everything set up as we wanted.\nThe biggest challenge we had was setting up our oracle node to close pools. Not only did we have to learn how to set up a .toml job, but also we had to build an API call to SportsDataIO.\nAccomplishments that we're proud of\nSmart Contracts\nBoth of us are new to Solidity, and so we are especially proud of the smart contracts that we built. We\u2019re very happy that we were able to learn Solidity in such a short period of time and get everything working as we had envisioned.\nBracket Selection\nWe\u2019re very pleased with how the bracket selection UI turned out. We managed to get the games to re-render appropriately from a binary tree structure.\nFirst Hackathon\nThis is Cole\u2019s first hackathon! I (Beckylee) have done a handful before, and I am incredibly proud of how well my partner has done. It\u2019s been wonderful to pair with him and watch him make enormous code contributions to our project.\nWhat we learned\nBeckylee\nI am completely new to the crypto space, so it was very interesting to learn about DeFi and blockchain. I now feel like I have a good sense of the tech and how impactful it all is, across all industries.\nTo be more specific, I learned Solidity, how to use MetaMask, Moralis, Hardhat and IPFS. A month ago, I had never heard of any of them, and now I can have a full conversation about implementation!\nCole\nI learned Solidity, and wrote my first smart contracts. I love that I now know details and quirks of the language, and feel confident in my ability to write more. Also, I had only ever used class components in React, so this was my first time working with functional components.\nI\u2019ve been a user of DeFi for many years, so learning the technical side of these things completely solidified my belief that it will gain mainstream adoption.\nWhat's next for Smart Picks\nIn the future, we would like to expand support to more blockchains, allow more customization on pools, and allow pools and brackets for other sporting events. We\u2019d also like to bolster our payouts with Superfluid, and set up an ENS for our dApp.\nWe plan to continue work on Smart Picks after the hackathon ends, as it\u2019s been so fun to work on and we both want to see it through all of the features we\u2019ve discussed.", "link": "https://devpost.com/software/smart-picks", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\nour team strongly feels that decentralized betting platforms are the next logical step for defi. our goal was to create something that we could see ourselves using, and increase the adoption rate of blockchain and cryptocurrencies.\nper visual capitalist's research, we know that online sports betting is a burgeoning industry. we see an opportunity to contribute to this space and believe that a substantial part of this market would move to trustless blockchain solutions.\nwhen the idea for smart picks came up, we knew immediately that we wanted to do it. we knew this would be an amazing, fun thing to create, with plenty of room for us to learn and grow as engineers. now that smart picks is built, we want to continue to broaden its use cases and bring it to market in time for the 2022 march madness tournament.\nwhat it does\nsmart picks is a dapp that lets users create and enter into ncaa march madness betting pools. the ui allows users to build and save brackets, with an easy-to-use interface. each pool is a smart contract, which uses chainlink price feeds to provide the up-to-date price in $eth, $matic, or $avax. we use sportsdataio for our oracle\u2019s data source and, to maintain proper decentralization, a keeper contract will close all of the pools 24 hours after the tournament has ended.\nhow we built it\nfront end\nthe front end is a react app that uses material ui. we used moralis to connect to our smart contracts, and to allow us to switch between our supported chains. metamask is used to facilitate all transactions, as well as handle authentication.\nsmart contracts\neach chain has its own pool factory smart contract, through which users can generate their own pool. all smart contracts maintain strict rules for buy-in price, number of entrants allowed, and one bracket per user.\noracle\nwe deployed chainlink\u2019s oracle smart contract and pointed it at our node. we hosted our chainlink node and related jobs on linkpool's naas.\ndeploying\nour smart contracts were deployed using hardhat scripts, and our dapp is deployed on ipfs via fleek.\nchallenges we ran into\nthe first challenge we faced was setting up our smart contracts. we spent a lot of time reading through solidity documentation and testing things out on remix before we finally got everything set up as we wanted.\nthe biggest challenge we had was setting up our oracle node to close pools. not only did we have to learn how to set up a .toml job, but also we had to build an api call to sportsdataio.\naccomplishments that we're proud of\nsmart contracts\nboth of us are new to solidity, and so we are especially proud of the smart contracts that we built. we\u2019re very happy that we were able to learn solidity in such a short period of time and get everything working as we had envisioned.\nbracket selection\nwe\u2019re very pleased with how the bracket selection ui turned out. we managed to get the games to re-render appropriately from a binary -----> tree !!!  structure.\nfirst hackathon\nthis is cole\u2019s first hackathon! i (beckylee) have done a handful before, and i am incredibly proud of how well my partner has done. it\u2019s been wonderful to pair with him and watch him make enormous code contributions to our project.\nwhat we learned\nbeckylee\ni am completely new to the crypto space, so it was very interesting to learn about defi and blockchain. i now feel like i have a good sense of the tech and how impactful it all is, across all industries.\nto be more specific, i learned solidity, how to use metamask, moralis, hardhat and ipfs. a month ago, i had never heard of any of them, and now i can have a full conversation about implementation!\ncole\ni learned solidity, and wrote my first smart contracts. i love that i now know details and quirks of the language, and feel confident in my ability to write more. also, i had only ever used class components in react, so this was my first time working with functional components.\ni\u2019ve been a user of defi for many years, so learning the technical side of these things completely solidified my belief that it will gain mainstream adoption.\nwhat's next for smart picks\nin the future, we would like to expand support to more blockchains, allow more customization on pools, and allow pools and brackets for other sporting events. we\u2019d also like to bolster our payouts with superfluid, and set up an ens for our dapp.\nwe plan to continue work on smart picks after the hackathon ends, as it\u2019s been so fun to work on and we both want to see it through all of the features we\u2019ve discussed.", "sortedWord": "None", "removed": "Nan", "score": 20, "comments": 4, "media": null, "medialink": null, "identifyer": 59505846}, {"Unnamed: 0": 5918, "autor": "COVID Detection using portable ML", "date": null, "content": "Inspiration\nIt would help a lot if the average person is able to get an indication of their chance of already being infected. This would help them to make the choice of coming out of isolation and taking the risk of going out and getting consulted\nWhat it does\nThis portable ML code uses Decision Tree algorithm to learn from an existing large COVID dataset, the dependence of the chance of being infected, on 8 fundamental factors.\nAccomplishments that we're proud of\nThis code is a very simple implementation that involves minimum processing overhead and yet is able to indicate the possibility of a being infected with an accuracy of 85%, using just 8 simple questions.\nWhat's next for COVID Detection using portable ML\nNext in line is to embed the code into a mobile application so that it is easily available for the average person.", "link": "https://devpost.com/software/covid-detection-using-portable-ml", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\nit would help a lot if the average person is able to get an indication of their chance of already being infected. this would help them to make the choice of coming out of isolation and taking the risk of going out and getting consulted\nwhat it does\nthis portable ml code uses decision -----> tree !!!  algorithm to learn from an existing large covid dataset, the dependence of the chance of being infected, on 8 fundamental factors.\naccomplishments that we're proud of\nthis code is a very simple implementation that involves minimum processing overhead and yet is able to indicate the possibility of a being infected with an accuracy of 85%, using just 8 simple questions.\nwhat's next for covid detection using portable ml\nnext in line is to embed the code into a mobile application so that it is easily available for the average person.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59505918}, {"Unnamed: 0": 5919, "autor": "InstaPresent", "date": null, "content": "Inspiration\nBoth of us were tired of making presentations for almost every single class. Also inspired by the 'in-game advertising' episode on the show Silicon Valley(S06E02).\nWhat it does\nInstaPresent uses your computer microphone to generate content to appear on your screen in a presentation in real-time. It can retrieve images and graphs and summarize your words into bullet points.\nHow we built it\nFORMATTING ENGINE\nTo know how to adjust the slide content when a new bullet point or image needs to be added, we had to build a formatting engine. This engine uses flex-boxes to distribute space between text and images and has custom Javascript to resize images based on aspect ratio and fit and to switch between the multiple slide types.\nVOICE-TO-SPEECH\nWe use Google\u2019s Text To Speech API to process audio on the microphone of the laptop. The Text To Speech is captured whenever a user records their audio, and when they let go the aggregated text is sent to the server over WebSockets to be processed.\nTOPIC ANALYSIS\nFundamentally we needed a way to determine whether a given sentence included a request to an image or not. So we gathered a repository of sample sentences from news articles for \u201cno\u201d examples, and manually curated a list of \u201cyes\u201d examples. We then used Facebook\u2019s Deep Learning text classification library, FastText, to train a custom neural network that could perform text classification.\nIMAGE SCRAPING\nOnce we have a sentence that the neural network classifies as a request for an image, such as \u201cand here you can see a picture of a dachshund\u201d, we use part of speech tagging and some tree theory rules to extract the subject, \u201cdachshund\u201d, and scrape Bing for pictures of the Weiner dog. These image URLs are then rendered on the screen.\nGRAPH GENERATION\nOnce the backend detects that the user specifically wants a graph that demonstrates their point, we used matplotlib code to generate the graphs. These graphs are then added to the presentation in real-time.\nSENTENCE SEGMENTATION\nWhen we receive the text back from the google text to speech API, it doesn\u2019t naturally add periods when we pause in our speech. This can give more conventional NLP analysis (like part-of-speech analysis), some trouble because the text is grammatically incorrect. We use a sequence to sequence transformer architecture, seq2seq, and transfer learned a new head that was capable of classifying the borders between sentences. This was then able to add punctuation back into the text before the rest of the processing pipeline.\nTEXT TITLE-IFICATION\nUsing Part-of-speech analysis, we determine which parts of a sentence (or sentences) would best serve as a title to a new slide. We do this by searching through sentence dependency trees to find short sub-phrases (1-5 words optimally) which contain important words and verbs. If the user is signaling the clicker that it needs a new slide, this function is run on their text until a suitable sub-phrase is found. When it is, a new slide is created using that sub-phrase as a title.\nTEXT SUMMARIZATION\nWhen the user is talking \u201cnormally,\u201d and not signaling for a new slide, image, or graph, we attempt to summarize their speech into bullet points. This summarization is performed using custom Part-of-speech analysis, which starts at verbs with many dependencies and works its way outward in the dependency tree, pruning branches of the sentence that are superfluous.\nINTERNAL SOCKET COMMUNICATION\nIn addition to the WebSockets portion of our project, we had to use internal socket communications to do the actual text analysis. Unfortunately, the machine learning prediction could not be run within the web app itself, so we had to put it into its process and thread and send the information over regular sockets so that the website would work. When the server receives a relevant WebSockets message, it creates a connection to our socket server running the machine learning model and sends information about what the user has been saying to the model. Once it receives the details back from the model, it broadcasts the new elements that need to be added to the slides and the front-end JavaScript adds the content to the slides.\nChallenges We ran into\nText summarization is very difficult - there may be powerful algorithms to turn articles into paragraph summaries, there is essentially nothing on shortening sentences into bullet points. We ended up developing a custom pipeline for bullet-point generation based on 'Part-of-speech' and 'Dependency analysis'. We couldn't explore the APIs of other services like Auth0, Twilio, etc. We also had plans of making an Android app for the same, but couldn't because of limited team members and time constraints. But despite our challenges, we enjoyed the opportunity and are grateful for that.\nAccomplishments that we're proud of\nMaking a web application, with a variety of machine learning and non-machine learning techniques. Working on an unsolved machine learning problem (sentence simplification) Real-time text analysis to determine new elements\nWhat's next for InstaPresent\nPredict what the user intends to say next Improving text summarization with word reordering", "link": "https://devpost.com/software/instapresent", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\nboth of us were tired of making presentations for almost every single class. also inspired by the 'in-game advertising' episode on the show silicon valley(s06e02).\nwhat it does\ninstapresent uses your computer microphone to generate content to appear on your screen in a presentation in real-time. it can retrieve images and graphs and summarize your words into bullet points.\nhow we built it\nformatting engine\nto know how to adjust the slide content when a new bullet point or image needs to be added, we had to build a formatting engine. this engine uses flex-boxes to distribute space between text and images and has custom javascript to resize images based on aspect ratio and fit and to switch between the multiple slide types.\nvoice-to-speech\nwe use google\u2019s text to speech api to process audio on the microphone of the laptop. the text to speech is captured whenever a user records their audio, and when they let go the aggregated text is sent to the server over websockets to be processed.\ntopic analysis\nfundamentally we needed a way to determine whether a given sentence included a request to an image or not. so we gathered a repository of sample sentences from news articles for \u201cno\u201d examples, and manually curated a list of \u201cyes\u201d examples. we then used facebook\u2019s deep learning text classification library, fasttext, to train a custom neural network that could perform text classification.\nimage scraping\nonce we have a sentence that the neural network classifies as a request for an image, such as \u201cand here you can see a picture of a dachshund\u201d, we use part of speech tagging and some -----> tree !!!  theory rules to extract the subject, \u201cdachshund\u201d, and scrape bing for pictures of the weiner dog. these image urls are then rendered on the screen.\ngraph generation\nonce the backend detects that the user specifically wants a graph that demonstrates their point, we used matplotlib code to generate the graphs. these graphs are then added to the presentation in real-time.\nsentence segmentation\nwhen we receive the text back from the google text to speech api, it doesn\u2019t naturally add periods when we pause in our speech. this can give more conventional nlp analysis (like part-of-speech analysis), some trouble because the text is grammatically incorrect. we use a sequence to sequence transformer architecture, seq2seq, and transfer learned a new head that was capable of classifying the borders between sentences. this was then able to add punctuation back into the text before the rest of the processing pipeline.\ntext title-ification\nusing part-of-speech analysis, we determine which parts of a sentence (or sentences) would best serve as a title to a new slide. we do this by searching through sentence dependency trees to find short sub-phrases (1-5 words optimally) which contain important words and verbs. if the user is signaling the clicker that it needs a new slide, this function is run on their text until a suitable sub-phrase is found. when it is, a new slide is created using that sub-phrase as a title.\ntext summarization\nwhen the user is talking \u201cnormally,\u201d and not signaling for a new slide, image, or graph, we attempt to summarize their speech into bullet points. this summarization is performed using custom part-of-speech analysis, which starts at verbs with many dependencies and works its way outward in the dependency tree, pruning branches of the sentence that are superfluous.\ninternal socket communication\nin addition to the websockets portion of our project, we had to use internal socket communications to do the actual text analysis. unfortunately, the machine learning prediction could not be run within the web app itself, so we had to put it into its process and thread and send the information over regular sockets so that the website would work. when the server receives a relevant websockets message, it creates a connection to our socket server running the machine learning model and sends information about what the user has been saying to the model. once it receives the details back from the model, it broadcasts the new elements that need to be added to the slides and the front-end javascript adds the content to the slides.\nchallenges we ran into\ntext summarization is very difficult - there may be powerful algorithms to turn articles into paragraph summaries, there is essentially nothing on shortening sentences into bullet points. we ended up developing a custom pipeline for bullet-point generation based on 'part-of-speech' and 'dependency analysis'. we couldn't explore the apis of other services like auth0, twilio, etc. we also had plans of making an android app for the same, but couldn't because of limited team members and time constraints. but despite our challenges, we enjoyed the opportunity and are grateful for that.\naccomplishments that we're proud of\nmaking a web application, with a variety of machine learning and non-machine learning techniques. working on an unsolved machine learning problem (sentence simplification) real-time text analysis to determine new elements\nwhat's next for instapresent\npredict what the user intends to say next improving text summarization with word reordering", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59505919}, {"Unnamed: 0": 5940, "autor": "FurnitureExp.VR", "date": null, "content": "Inspiration\nHow can one reduce their carbon footprint in furniture shopping, while social distancing, but still get a 'feel' for what furniture they are buying?\nWhat it does\nA VR living room that uses 3D models from Wayfair's website to interact with, see pricing info, as well as suggest which furniture items to consider next given current furniture selection.\nHow we built it\nUtilized Unity's engine for creating the VR space, mid-end interfaces with javascript, utilized Jupyter Notebooks for data collection and analysis.\nChallenges we ran into\nHaving models correctly mesh, retrieving 3D models from Wayfair, web scraping congruous data.\nAccomplishments that we're proud of\nMaking a working VR platform.\nWhat we learned\nIt is best to first build with the minimum amount of features needed.\nWhat's next for FurnitureExp.VR\nFleshing out the decision tree suggesting, importing more 3D models.", "link": "https://devpost.com/software/furnitureexp-vr", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\nhow can one reduce their carbon footprint in furniture shopping, while social distancing, but still get a 'feel' for what furniture they are buying?\nwhat it does\na vr living room that uses 3d models from wayfair's website to interact with, see pricing info, as well as suggest which furniture items to consider next given current furniture selection.\nhow we built it\nutilized unity's engine for creating the vr space, mid-end interfaces with javascript, utilized jupyter notebooks for data collection and analysis.\nchallenges we ran into\nhaving models correctly mesh, retrieving 3d models from wayfair, web scraping congruous data.\naccomplishments that we're proud of\nmaking a working vr platform.\nwhat we learned\nit is best to first build with the minimum amount of features needed.\nwhat's next for furnitureexp.vr\nfleshing out the decision -----> tree !!!  suggesting, importing more 3d models.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59505940}, {"Unnamed: 0": 6183, "autor": "Discere Waterfall", "date": null, "content": "Inspiration\nMinecraft, Where's my Water?\nWhat it does\nBuilds and selects puzzles for logic game that users can access through their browser\nHow we built it\nUsing JavaScript, integrated website and game engine\nChallenges we ran into\nImplementing tree search algorithm, user interface\nAccomplishments that we're proud of\nOptimizing algorithm, Front-End product\nWhat we learned\nJavaScript, Front-End development, algorithm optimization\nWhat's next for Discere Waterfall\nIntegrate current game to social system in which users can create and share problem", "link": "https://devpost.com/software/discere-waterfall", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\nminecraft, where's my water?\nwhat it does\nbuilds and selects puzzles for logic game that users can access through their browser\nhow we built it\nusing javascript, integrated website and game engine\nchallenges we ran into\nimplementing -----> tree !!!  search algorithm, user interface\naccomplishments that we're proud of\noptimizing algorithm, front-end product\nwhat we learned\njavascript, front-end development, algorithm optimization\nwhat's next for discere waterfall\nintegrate current game to social system in which users can create and share problem", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59506183}, {"Unnamed: 0": 6384, "autor": "CyberLife (Autonomous NFTs)", "date": null, "content": "Inspiration\nThe CyberLife project is inspired by the mission to bring to market a way for NFT to bring fundamental value to emerging industries by driving down costs in production for creators (e.g., game developers) and eliminate functional market inefficiencies (e.g., automated arbitration).\nIn the ETHLisbon Hackathon, we explore the use case of autonomous NFTs (aNFTs) in the nascent on-chain gaming industry.\nThe problem we solve is - how can autonomous NFT be deployed to automate creation of non-playable characters in the metaverse, which make up >75% of player interactions in the world's most popular games. Basically, how can NFTs actually provide real usability in the metaverse in an easy and scalable way, not just for looks or speculation.\nWhat it does\nThe aNPC use case presented in the ETHLisbon Hackathon is built on top of Autonomy Network\u2019s money lego brick infrastructure that allows for automation of interconnecting autonomous upgradable NFTs in auto generative manner.\naNFTs have random-determined free will and are pseudo-deterministically bound to the blockchain to act upon other aNFTs in order to procreate, feed and eliminate one-another.\nHow we built it\nFirst, the team built a global grid contract to handle movements and interacterability among characters.\nOur engineering team built the architecture and modified library for the upgradeable proxy pattern aNFTs smart contracts use, trying to be as modular as possible and at the same time not running into inheritance issues or scope conflicts between all contracts. First the contract spawn network (Quantum & QuantumFactory) was built, then we had QuantumLogic for a barebones logic, QuantumLogicAlpha for a more complex one and QuantumLogicBravo for a more balanced one.\nWe then used the ERC721 for both wolf-like and sheep-like characters with extra functions such as eat, graze, breed, move. Properties explored were Health points, Attack points, Wisdom, Fertility & Resistance, all determined on-chain by interacting with the hub world and other characters.\nThen, we built the AI speech interface for aNPCs to engage with the blockchain layer, as well as built the Family tree representation of the current wolves and sheep on-chain. We also modeled the flow for breeding (cloning) new NFTs\nChallenges we ran into\nComplexities encountered along the coding sprints were manifold. Along the most pressing unforeseen questions that emerged when entering into the build phase were following:\nThe existing infrastructure for OpenZeppelin Upgrades didn't fit our use case directly so we had to adapt proxies to have a bit more autonomy and better work with the logic implementations.\nFor this POC we had to deploy a lot of contracts at once to test and verify. Tooling proved to be a challenge as Brownie does not appear to be fit for so much data being verified on-chain.\nLogic for aNFTs can be really complex, so we came up with a total of three different implementations, one more complete than the other.\nHow to allocate enough resources to both code development, testing, as well as complimentary build of marketing strategy and assets proved to be a very serious teamwork/multitask/parallel work challenge.\nAccomplishments that we're proud of\nPushing out more than 1500 lines of working Solidity code during the hackathon without compromising code quality nor architecture effectiveness is something we're really proud of. Next to only producing sleek code, we also allocated sufficient resources to vital non-code elements of delivering a holistic project. This includes creation of marketing & brand assets, communication strategy for launch of aNFT campaign. Video looks really great too.\nWe believe technology and UX are what ultimately pushes the aNFTs usability to the next level and the balance we developed internally during ETHLisbon was really seamless, so we really hope we can show CyberLife to more people outside the niche.\nWhat we learned\nSome of the major learnings include: Plan enough time for testing. Plan more time for testing. Get your non-tech team members onboarded as soon as possible so they can also support in giving invaluable feedback to product creation. It\u2019s fun to work In interdisciplinary teams. Clear communication is key.\nCreating autonomous little creatures inside a blockchain can get really hard really fast. We had to strip down the logic a good number of times to fit within the hackathon timeframe, but those more complex implementations ultimately ended up serving as good lessons and fundamentals towards building leaner ones. Integrating OpenAI with Web3 is quite a ride as well, as latency can be a problem and end up making the whole flow a little unreliable.\nWhat's next for CyberLife (Autonomous NFTs)\nThe team is working to prepare for a launch of the inaugural genesis CyberLife aNFT drop containing 8 aNFTs on Rarible platform working with influencers from the space, such as gmoney. These 8 aNFTs can expand indefinitely and end up in thousands of offspring over just a few days, fueling the secondary market in the POC so we can better tweak the many parameters of the contracts. New hub worlds can be deployed future alongside a clear dashboard where owners can visualize the interactions on-chain.", "link": "https://devpost.com/software/anft-autonomous-nfts", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\nthe cyberlife project is inspired by the mission to bring to market a way for nft to bring fundamental value to emerging industries by driving down costs in production for creators (e.g., game developers) and eliminate functional market inefficiencies (e.g., automated arbitration).\nin the ethlisbon hackathon, we explore the use case of autonomous nfts (anfts) in the nascent on-chain gaming industry.\nthe problem we solve is - how can autonomous nft be deployed to automate creation of non-playable characters in the metaverse, which make up >75% of player interactions in the world's most popular games. basically, how can nfts actually provide real usability in the metaverse in an easy and scalable way, not just for looks or speculation.\nwhat it does\nthe anpc use case presented in the ethlisbon hackathon is built on top of autonomy network\u2019s money lego brick infrastructure that allows for automation of interconnecting autonomous upgradable nfts in auto generative manner.\nanfts have random-determined free will and are pseudo-deterministically bound to the blockchain to act upon other anfts in order to procreate, feed and eliminate one-another.\nhow we built it\nfirst, the team built a global grid contract to handle movements and interacterability among characters.\nour engineering team built the architecture and modified library for the upgradeable proxy pattern anfts smart contracts use, trying to be as modular as possible and at the same time not running into inheritance issues or scope conflicts between all contracts. first the contract spawn network (quantum & quantumfactory) was built, then we had quantumlogic for a barebones logic, quantumlogicalpha for a more complex one and quantumlogicbravo for a more balanced one.\nwe then used the erc721 for both wolf-like and sheep-like characters with extra functions such as eat, graze, breed, move. properties explored were health points, attack points, wisdom, fertility & resistance, all determined on-chain by interacting with the hub world and other characters.\nthen, we built the ai speech interface for anpcs to engage with the blockchain layer, as well as built the family -----> tree !!!  representation of the current wolves and sheep on-chain. we also modeled the flow for breeding (cloning) new nfts\nchallenges we ran into\ncomplexities encountered along the coding sprints were manifold. along the most pressing unforeseen questions that emerged when entering into the build phase were following:\nthe existing infrastructure for openzeppelin upgrades didn't fit our use case directly so we had to adapt proxies to have a bit more autonomy and better work with the logic implementations.\nfor this poc we had to deploy a lot of contracts at once to test and verify. tooling proved to be a challenge as brownie does not appear to be fit for so much data being verified on-chain.\nlogic for anfts can be really complex, so we came up with a total of three different implementations, one more complete than the other.\nhow to allocate enough resources to both code development, testing, as well as complimentary build of marketing strategy and assets proved to be a very serious teamwork/multitask/parallel work challenge.\naccomplishments that we're proud of\npushing out more than 1500 lines of working solidity code during the hackathon without compromising code quality nor architecture effectiveness is something we're really proud of. next to only producing sleek code, we also allocated sufficient resources to vital non-code elements of delivering a holistic project. this includes creation of marketing & brand assets, communication strategy for launch of anft campaign. video looks really great too.\nwe believe technology and ux are what ultimately pushes the anfts usability to the next level and the balance we developed internally during ethlisbon was really seamless, so we really hope we can show cyberlife to more people outside the niche.\nwhat we learned\nsome of the major learnings include: plan enough time for testing. plan more time for testing. get your non-tech team members onboarded as soon as possible so they can also support in giving invaluable feedback to product creation. it\u2019s fun to work in interdisciplinary teams. clear communication is key.\ncreating autonomous little creatures inside a blockchain can get really hard really fast. we had to strip down the logic a good number of times to fit within the hackathon timeframe, but those more complex implementations ultimately ended up serving as good lessons and fundamentals towards building leaner ones. integrating openai with web3 is quite a ride as well, as latency can be a problem and end up making the whole flow a little unreliable.\nwhat's next for cyberlife (autonomous nfts)\nthe team is working to prepare for a launch of the inaugural genesis cyberlife anft drop containing 8 anfts on rarible platform working with influencers from the space, such as gmoney. these 8 anfts can expand indefinitely and end up in thousands of offspring over just a few days, fueling the secondary market in the poc so we can better tweak the many parameters of the contracts. new hub worlds can be deployed future alongside a clear dashboard where owners can visualize the interactions on-chain.", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59506384}, {"Unnamed: 0": 6401, "autor": "CoverMe", "date": null, "content": "Inspiration\nAs medical students taking a Population Health course at Pitt Med, we were struck by just how unnecessarily convoluted picking health insurance can be - it was almost as if it was designed to be that way. Our inspiration came from asking ourselves...how on earth could a single mother or working-class father, who is pinching pennies, navigate this system and find health insurance that doesn\u2019t waste money and truly works for them and their kids?\nWhat it does\nWe designed a health insurance optimization platform - CoverMe - that takes the guesswork out of picking health insurance. It compares personal health information with data from a Medical Panel Expenditure Survey in identifying a customized UPMC plan that best addresses one\u2019s projected health care costs.\nHow we built it\nWe started with a large sample of healthcare data for 30,000 individuals from a self-reported survey, which included their background, health, and cost of care in 2019. Our data came from the Medical Expenditure Panel Survey, which is popularly cited among economic studies. Using this data set we compared different machine learning models to determine which could best estimate the total cost an individual had for the year. Our two best models were random tree regression and support vector machine, with random tree regression overall proving to have the highest fidelity.\nUsing the generated model, we predicted the users health care cost based on the information they provided. From the cost of the care over the year, we compared the four UPMC health plans available to employees to determine which would be the most cost effective while maintaining the best possible health for our users.\nChallenges we ran into\nFinding compiled health data was extremely challenging. We spent nearly the entire first day trying to find a trustworthy source that included the wide variety of data/metrics that we hoped to incorporate into our model.\nLearning how to implement a machine learning (ML) model was also challenging. Our team is composed of medical students with limited ML exposure. We spent a significant amount of time watching YouTube videos, reading forums, and trying GitHub code.\nAccomplishments that we're proud of\nEven though our group initially had a difficult time finding health data, we are proud of the fact that we eventually were able to locate a fairly comprehensive Medical Panel Expenditure Survey to use as the basis for our optimization platform.\nWhat we learned\nUltimately, we learned that in the long term, the data available from the insurance companies (in this case UPMC Health Plan) will be more appropriate for the model. Importantly, this data will be more reliable because it is more granular and is not self reported. We also think that by coming directly from the insurer there will be even more information available for the model; info like past medical/surgical history, more granular spending categorized by inpatient stays, medications, procedures, etc; and all of this on a broader list of patients.\nWhat's next for CoverMe\nWe will be looking to broaden CoverMe\u2019s scope, for example by adding the ability to keep out-of-network providers, expanding it beyond the walls of UPMC, and further improving the platform so that it can incorporate spouse/family health metrics and parse Statements of Benefits to automate data collections.", "link": "https://devpost.com/software/coverme-nr6e0v", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\nas medical students taking a population health course at pitt med, we were struck by just how unnecessarily convoluted picking health insurance can be - it was almost as if it was designed to be that way. our inspiration came from asking ourselves...how on earth could a single mother or working-class father, who is pinching pennies, navigate this system and find health insurance that doesn\u2019t waste money and truly works for them and their kids?\nwhat it does\nwe designed a health insurance optimization platform - coverme - that takes the guesswork out of picking health insurance. it compares personal health information with data from a medical panel expenditure survey in identifying a customized upmc plan that best addresses one\u2019s projected health care costs.\nhow we built it\nwe started with a large sample of healthcare data for 30,000 individuals from a self-reported survey, which included their background, health, and cost of care in 2019. our data came from the medical expenditure panel survey, which is popularly cited among economic studies. using this data set we compared different machine learning models to determine which could best estimate the total cost an individual had for the year. our two best models were random -----> tree !!!  regression and support vector machine, with random -----> tree !!!  regression overall proving to have the highest fidelity.\nusing the generated model, we predicted the users health care cost based on the information they provided. from the cost of the care over the year, we compared the four upmc health plans available to employees to determine which would be the most cost effective while maintaining the best possible health for our users.\nchallenges we ran into\nfinding compiled health data was extremely challenging. we spent nearly the entire first day trying to find a trustworthy source that included the wide variety of data/metrics that we hoped to incorporate into our model.\nlearning how to implement a machine learning (ml) model was also challenging. our team is composed of medical students with limited ml exposure. we spent a significant amount of time watching youtube videos, reading forums, and trying github code.\naccomplishments that we're proud of\neven though our group initially had a difficult time finding health data, we are proud of the fact that we eventually were able to locate a fairly comprehensive medical panel expenditure survey to use as the basis for our optimization platform.\nwhat we learned\nultimately, we learned that in the long term, the data available from the insurance companies (in this case upmc health plan) will be more appropriate for the model. importantly, this data will be more reliable because it is more granular and is not self reported. we also think that by coming directly from the insurer there will be even more information available for the model; info like past medical/surgical history, more granular spending categorized by inpatient stays, medications, procedures, etc; and all of this on a broader list of patients.\nwhat's next for coverme\nwe will be looking to broaden coverme\u2019s scope, for example by adding the ability to keep out-of-network providers, expanding it beyond the walls of upmc, and further improving the platform so that it can incorporate spouse/family health metrics and parse statements of benefits to automate data collections.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59506401}, {"Unnamed: 0": 6456, "autor": "Krypton", "date": null, "content": "Inspiration\nEarlier this year, Michael wanted to develop a DeFi asset management protocol that would implement simple systematic strategies at virtually zero cost. The trustless nature of public blockchain ecosystems would ensure that the funds are actually there (no Bernie Madoff type issues) and that they are being invested according to the stated portfolio rules.\nBut the public nature of the blockchain meant that sophisticated traders would be able to predict the protocol\u2019s trades by analyzing the smart contract\u2019s rebalancing logic. This issue is especially stark with the current iteration of DEX protocols that are mostly based on the AMM concept. In AMMs, a passive side stands ready to buy or sell at prices determined via a simple formulaic approach such as x*y=k. On Uniswap, for example, 25 basis points out of the 30 basis point fee paid by active traders go to compensate liquidity providers for systematic losses at the hand of arbitrageurs who adversely select the LP pool using publicly available information! The rise of Flashbots in 2021 means that front-running is now being offered as a service by miners to the highest bidder, so each and every order will now be squeezed for its maximal extractable value in sandwich attacks and other schemes. This leads to slippage losses amounting to another 50 basis points for a total cost of 80 basis points or 0.8%. The reduction in transaction fees due to rollups, sidechains, and the imminent arrival of Ethereum 2.0 means that ever smaller trades can be profitably front-run by sophisticated trading operations.\nAll of this meant that before Michael could realize his vision of a universally accessible, trustless, and cost-efficient asset management protocol, he would first have to develop a trading protocol that solves the issue of toxic trading. This is where Krypton comes in.\nKrypton is a new way of trading that is resistant to toxic trading schemes such as front-running, adverse selection based on short-term information advantages, and miner extractable value.\nTo make it a reality, Michael wrote down his rationale for Krypton and its design in a white paper in October of 2021 and enlisted the help of his brother Andreas and his former student Nathan to implement the protocol during the Chainlink hackathon this Fall.\nWhat it does\nKrypton is a novel peer-to-peer DeFi exchange protocol that solves the issue of miner extractable value in the context of trading. It is fully decentralized, resistant to front-running and to short-term adverse selection. This is achieved by executing orders as continuous streams over time at finite trading speeds in contrast to concentrating trade execution to a distinct instant of time at infinite speed. The protocol features price discovery. Rather than relating price to quantity, traders submit demand and supply schedules relating price to trading speed. This change in perspective affords enormous conceptual clarity, predictability, and simplicity: The market clearing equilibrium is simply defined as the intersection of the aggregate demand and supply curves in price/trading-speed space. An actor in possession of short-term information that needs to be monetized prior to becoming public will need to request a high trading speed, which is limited by the maximum aggregate trading speed offered by the opposite side and increases the price. The unilateral price and trading speed jump induced by a fast trade signals the presence of asymmetric information and gives uninformed traders time to respond by adjusting their quotes. In infinite speed trading systems such as automated market makers (AMMs) and central limit order books (CLOBs), the maximum tradable quantity is exchanged in the very same moment the order that signals the presence of superior information arrives, making it too late for everyone else to react and opening the door for toxic trading schemes. These toxic trading schemes are responsible for the existence of bid-ask spreads in limit order books and price impact in AMMs.\nHow we built it\nKrypton is built on top of a diverse open source software stack.\nWe are using Solidity to implement EVM code and Brownie for development and deployment.\nThe entire KryptonCompute off-chain framework is implemented in Python. PyCharm is Michael\u2019s development platform of choice, whereas Nathan prefers to work in vim, and Andreas uses VSCode.\nThe oracle is built using Chainlink technology, which we run in a Docker container.\nWe use Avalanche as our blockchain of choice. We explain in the section on challenges below how Avalanche\u2019s high performance and low cost nature works synergistically with our Chainlink-based scaling solution to improve capital efficiency on oracle nodes.\nThe web frontend is built as a single page application using Quasar, a Node.js framework based on Vue. We use Plotly.js to produce the order book graph. All of the REST APIs are implemented using Flask.\nThe communication with IPFS as a tamper-resistant software delivery platform for oracle nodes and keepers is accomplished using the ipfshttpclient package for Python.\nWe have used Linux as our primary development platform.\nChallenges we ran into\nThe biggest challenge overcome is the severe limitation of on-chain computation available in today's smart contract ecosystems. We used a combination of keepers and Chainlink external adapters to minimize on-chain storage and compute requirements, thereby reducing cost and eliminating the barrier of computational complexity in decentralized systems. This was achieved by developing a KryptonComputable contract that manages update cycles and integrates on-chain with trustless off-chain computation. A KryptonComputable internally relies on the Chainlink oracle network where nodes run an external adapter implementing the Krypton Merkle tree framework (see the modules krypton.merkle_tree and krypton.calculation.calculation for details). The oracles determine the work that the smart contract needs to perform in each update cycle.In the present trading context, the work consists of ERC-20 token transfers for trade settlement, the update of order structures on-chain to reflect executed quantities, and the inactivation of orders that have finished executing. The work is divided into chunks small enough to fit the transaction gas limit of the blockchain. These chunks constitute the leaves of the Merkle tree. The oracle computes the hash of the Merkle tree root and returns it as 256 bit scalar to the KryptonComputable contract. A keeper carries out the exact same calculation which results in the exact same Merkle tree and submits leaves to the smart contract one by one along with the hashes of adjacent nodes. The smart contract computes the hash of each leaf and combines it with the adjacent node hashes to verify that it matches the root hash provided by the oracle network. After validation and implementation, the smart contract stores the hash of the leaf on-chain to prevent replay attacks.\nOur framework is a general approach and can be used in other contexts to expand the computational envelope of decentralized systems using Chainlink technology. Protocols using it simply need to\nInherit from KryptonComputable.\nImplement the methods construct_from_hashables, empty_batch_hashable_function, and solidity_keccak for each type of work to be done in a smart contract. See the Transfer class in the krypton.structs module for example.\nWrite a function that implements the work on the smart contract for each type of work. See the contract function implementTransfers in Krypton.sol for example.\nWhile Krypton\u2019s off-chain compute approach is compatible with every EVM compliant blockchain that integrates with Chainlink, a high-performance and cost efficient blockchain such as Avalanche works synergistically with Krypton. The shorter the settlement window, the higher the number of update cycles an order is involved in. With a settlement window of 10 minutes, an order that continuously trades over 5 minutes will be part of at most two settlement cycles. Increasing the update frequency to once per minute will feature the order in up to 6 settlement cycles which results in a multiplication of on-chain work for token transfers, order structure updates, and order inactivations. A more cost efficient blockchain will allow more frequent settlement cycles while keeping the cost of on-chain transactions minimal. The obvious and direct effect is an improvement of user experience as it makes the protocol less expensive and shortens the wait time for token transfers for parts of orders that have been executed. A second, less obvious but more profound, effect is an improvement of the capital efficiency of nodes in the oracle network. Chainlink 2.0 features a staking model which guarantees a crypto-economic security level that is quadratic in the number of nodes. The level of security required for Krypton is dictated by peak trading volume (i.e. maximum amount of value transferred) in a single settlement window. A shorter settlement window enabled by the use of a high-performance blockchain ecosystem such as Avalanche reduces the amount of crypto-economic security required to secure a given window at the exact same trading activity on Krypton. This improves capital employment on oracle nodes.\nAccomplishments that we're proud of\nOn a technical level, we are thrilled to have developed a novel decentralized scaling approach that ultimately enables our economic vision and can be used to power the next generation of Web3 protocols.\nOn an economic level, we are excited to be presenting a fully working trading solution that is resistant to the toxic trading patterns that are pervasive in both traditional finance and crypto. These practices have become pertinent in DeFi with the rise of Flasbots over the last year. Since DeFi\u2019s inception, the computational limitation of smart contract ecosystems has limited the protocol design space to simple peer-to-pool mechanisms such as AMMs which have suffered from severe economic inefficiencies as a passive side needed to be compensated via high fees for systematic losses to toxic traders. As such the economic efficiency of decentralized finance has lagged behind that of its centralized counterpart. Krypton makes DeFi significantly more economically efficient than any exchange mechanism available in CeFi or DeFi to this day. We hope that Krypton will create an incentive for unsophisticated market participants such as pension funds, mutual funds, and retail investors that have traditionally been on the losing end of market shenanigans practiced by high-frequency traders and hedge funds to move into DeFi.\nWhat we learned\nWe have built Krypton using a number of technologies none of us have used before. We were impressed with the quality of the open source stack that we built upon and their developer friendliness. Kudos to the Chainlink organizers and sponsors for the hours of video content to get us started with their tech.\nWhat's next for Krypton\nWe are thrilled to be part of the Chainlink hackathon and hope that this event will help publicize the idea that a financial ecosystem where the majority does not systematically lose to a small, technically well-versed, well-informed, and sophisticated minority is not only a remote possibility but feasible with technology available today as proven by our implementation. The next steps will be to conduct security audits and publicly launch the protocol next year.", "link": "https://devpost.com/software/krypton-1gy0c5", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\nearlier this year, michael wanted to develop a defi asset management protocol that would implement simple systematic strategies at virtually zero cost. the trustless nature of public blockchain ecosystems would ensure that the funds are actually there (no bernie madoff type issues) and that they are being invested according to the stated portfolio rules.\nbut the public nature of the blockchain meant that sophisticated traders would be able to predict the protocol\u2019s trades by analyzing the smart contract\u2019s rebalancing logic. this issue is especially stark with the current iteration of dex protocols that are mostly based on the amm concept. in amms, a passive side stands ready to buy or sell at prices determined via a simple formulaic approach such as x*y=k. on uniswap, for example, 25 basis points out of the 30 basis point fee paid by active traders go to compensate liquidity providers for systematic losses at the hand of arbitrageurs who adversely select the lp pool using publicly available information! the rise of flashbots in 2021 means that front-running is now being offered as a service by miners to the highest bidder, so each and every order will now be squeezed for its maximal extractable value in sandwich attacks and other schemes. this leads to slippage losses amounting to another 50 basis points for a total cost of 80 basis points or 0.8%. the reduction in transaction fees due to rollups, sidechains, and the imminent arrival of ethereum 2.0 means that ever smaller trades can be profitably front-run by sophisticated trading operations.\nall of this meant that before michael could realize his vision of a universally accessible, trustless, and cost-efficient asset management protocol, he would first have to develop a trading protocol that solves the issue of toxic trading. this is where krypton comes in.\nkrypton is a new way of trading that is resistant to toxic trading schemes such as front-running, adverse selection based on short-term information advantages, and miner extractable value.\nto make it a reality, michael wrote down his rationale for krypton and its design in a white paper in october of 2021 and enlisted the help of his brother andreas and his former student nathan to implement the protocol during the chainlink hackathon this fall.\nwhat it does\nkrypton is a novel peer-to-peer defi exchange protocol that solves the issue of miner extractable value in the context of trading. it is fully decentralized, resistant to front-running and to short-term adverse selection. this is achieved by executing orders as continuous streams over time at finite trading speeds in contrast to concentrating trade execution to a distinct instant of time at infinite speed. the protocol features price discovery. rather than relating price to quantity, traders submit demand and supply schedules relating price to trading speed. this change in perspective affords enormous conceptual clarity, predictability, and simplicity: the market clearing equilibrium is simply defined as the intersection of the aggregate demand and supply curves in price/trading-speed space. an actor in possession of short-term information that needs to be monetized prior to becoming public will need to request a high trading speed, which is limited by the maximum aggregate trading speed offered by the opposite side and increases the price. the unilateral price and trading speed jump induced by a fast trade signals the presence of asymmetric information and gives uninformed traders time to respond by adjusting their quotes. in infinite speed trading systems such as automated market makers (amms) and central limit order books (clobs), the maximum tradable quantity is exchanged in the very same moment the order that signals the presence of superior information arrives, making it too late for everyone else to react and opening the door for toxic trading schemes. these toxic trading schemes are responsible for the existence of bid-ask spreads in limit order books and price impact in amms.\nhow we built it\nkrypton is built on top of a diverse open source software stack.\nwe are using solidity to implement evm code and brownie for development and deployment.\nthe entire kryptoncompute off-chain framework is implemented in python. pycharm is michael\u2019s development platform of choice, whereas nathan prefers to work in vim, and andreas uses vscode.\nthe oracle is built using chainlink technology, which we run in a docker container.\nwe use avalanche as our blockchain of choice. we explain in the section on challenges below how avalanche\u2019s high performance and low cost nature works synergistically with our chainlink-based scaling solution to improve capital efficiency on oracle nodes.\nthe web frontend is built as a single page application using quasar, a node.js framework based on vue. we use plotly.js to produce the order book graph. all of the rest apis are implemented using flask.\nthe communication with ipfs as a tamper-resistant software delivery platform for oracle nodes and keepers is accomplished using the ipfshttpclient package for python.\nwe have used linux as our primary development platform.\nchallenges we ran into\nthe biggest challenge overcome is the severe limitation of on-chain computation available in today's smart contract ecosystems. we used a combination of keepers and chainlink external adapters to minimize on-chain storage and compute requirements, thereby reducing cost and eliminating the barrier of computational complexity in decentralized systems. this was achieved by developing a kryptoncomputable contract that manages update cycles and integrates on-chain with trustless off-chain computation. a kryptoncomputable internally relies on the chainlink oracle network where nodes run an external adapter implementing the krypton merkle -----> tree !!!  framework (see the modules krypton.merkle_tree and krypton.calculation.calculation for details). the oracles determine the work that the smart contract needs to perform in each update cycle.in the present trading context, the work consists of erc-20 token transfers for trade settlement, the update of order structures on-chain to reflect executed quantities, and the inactivation of orders that have finished executing. the work is divided into chunks small enough to fit the transaction gas limit of the blockchain. these chunks constitute the leaves of the merkle tree. the oracle computes the hash of the merkle tree root and returns it as 256 bit scalar to the kryptoncomputable contract. a keeper carries out the exact same calculation which results in the exact same merkle tree and submits leaves to the smart contract one by one along with the hashes of adjacent nodes. the smart contract computes the hash of each leaf and combines it with the adjacent node hashes to verify that it matches the root hash provided by the oracle network. after validation and implementation, the smart contract stores the hash of the leaf on-chain to prevent replay attacks.\nour framework is a general approach and can be used in other contexts to expand the computational envelope of decentralized systems using chainlink technology. protocols using it simply need to\ninherit from kryptoncomputable.\nimplement the methods construct_from_hashables, empty_batch_hashable_function, and solidity_keccak for each type of work to be done in a smart contract. see the transfer class in the krypton.structs module for example.\nwrite a function that implements the work on the smart contract for each type of work. see the contract function implementtransfers in krypton.sol for example.\nwhile krypton\u2019s off-chain compute approach is compatible with every evm compliant blockchain that integrates with chainlink, a high-performance and cost efficient blockchain such as avalanche works synergistically with krypton. the shorter the settlement window, the higher the number of update cycles an order is involved in. with a settlement window of 10 minutes, an order that continuously trades over 5 minutes will be part of at most two settlement cycles. increasing the update frequency to once per minute will feature the order in up to 6 settlement cycles which results in a multiplication of on-chain work for token transfers, order structure updates, and order inactivations. a more cost efficient blockchain will allow more frequent settlement cycles while keeping the cost of on-chain transactions minimal. the obvious and direct effect is an improvement of user experience as it makes the protocol less expensive and shortens the wait time for token transfers for parts of orders that have been executed. a second, less obvious but more profound, effect is an improvement of the capital efficiency of nodes in the oracle network. chainlink 2.0 features a staking model which guarantees a crypto-economic security level that is quadratic in the number of nodes. the level of security required for krypton is dictated by peak trading volume (i.e. maximum amount of value transferred) in a single settlement window. a shorter settlement window enabled by the use of a high-performance blockchain ecosystem such as avalanche reduces the amount of crypto-economic security required to secure a given window at the exact same trading activity on krypton. this improves capital employment on oracle nodes.\naccomplishments that we're proud of\non a technical level, we are thrilled to have developed a novel decentralized scaling approach that ultimately enables our economic vision and can be used to power the next generation of web3 protocols.\non an economic level, we are excited to be presenting a fully working trading solution that is resistant to the toxic trading patterns that are pervasive in both traditional finance and crypto. these practices have become pertinent in defi with the rise of flasbots over the last year. since defi\u2019s inception, the computational limitation of smart contract ecosystems has limited the protocol design space to simple peer-to-pool mechanisms such as amms which have suffered from severe economic inefficiencies as a passive side needed to be compensated via high fees for systematic losses to toxic traders. as such the economic efficiency of decentralized finance has lagged behind that of its centralized counterpart. krypton makes defi significantly more economically efficient than any exchange mechanism available in cefi or defi to this day. we hope that krypton will create an incentive for unsophisticated market participants such as pension funds, mutual funds, and retail investors that have traditionally been on the losing end of market shenanigans practiced by high-frequency traders and hedge funds to move into defi.\nwhat we learned\nwe have built krypton using a number of technologies none of us have used before. we were impressed with the quality of the open source stack that we built upon and their developer friendliness. kudos to the chainlink organizers and sponsors for the hours of video content to get us started with their tech.\nwhat's next for krypton\nwe are thrilled to be part of the chainlink hackathon and hope that this event will help publicize the idea that a financial ecosystem where the majority does not systematically lose to a small, technically well-versed, well-informed, and sophisticated minority is not only a remote possibility but feasible with technology available today as proven by our implementation. the next steps will be to conduct security audits and publicly launch the protocol next year.", "sortedWord": "None", "removed": "Nan", "score": 10, "comments": 2, "media": null, "medialink": null, "identifyer": 59506456}, {"Unnamed: 0": 6526, "autor": "Chatify", "date": null, "content": "Inspiration\nOur team for sure wanted to create something that could help in some shape or form towards global issues. We bounced through several ideas, ranging from carpool tracking to tree planting. Chatify is a combination of all our ideas, in which users are able to discuss in small groups about a topic pertaining to a global issue. Through socially tough times like Covid-19, meeting new people will no longer be a challenge!\nWhat it does\nChoose from numerous different chat rooms, where unique topics have been assigned. The first to join a room is able to set a topic, which will be seen on the main page. Up to 10 people can join each room, and then, let the debate begin! There\u2019s no right or wrong answer, it\u2019s just an opportunity to share your thoughts and hear from everyone else.\nHow we built it\nThis site is built with Bootstrap and React, while the chat rooms are made with Firebase. It is a simple react app designed with Bootstrap, and when users click on a room, they\u2019ll be redirected to a new page where different hosts can message one another.\nChallenges we ran into\nIt took us a few hours to come up with a solid idea of what kind of project we wanted to make. We had a lot of different ideas, but we weren\u2019t sure how practical they were to realize them. As well, since not everyone on the team is familiar with the same technology, it was difficult to modify certain aspects of the project that we weren\u2019t a part of. There was a lot of new learning, especially with Firebase since only one member of the team was familiar with it.\nAccomplishments that we're proud of\nIt works!! Although not all aspects of the project are completed, the general structure and framework is functioning. Users are able to first log in via Google account, and then choose a chat room that has a topic of interest to them. They can chat away! It was not easy to complete a project with a team that\u2019s remote from each other, and that made the communication between ourselves difficult.\nWhat we learned\nReflecting on past hackathons, this one was a lot less stressful because we had come up with the idea beforehand and already arranged what technology we wanted to incorporate. We divided the team into two halves, which worked on separate ends of the project to lessen the burden on each person. Ultimately, we learned that it\u2019s important to rely on each other for help, since everyone\u2019s able to bring something to the table.\nWhat's next for Chatify\nDynamism!! Our project is built to only support 12 chatrooms at once, but further improvements could bump this number to as many as the public desires. As well, Chatify is very straightforward right now and doesn\u2019t offer much besides a messaging API. We can improve our project by adding more features, such as \u201cshorts\u201d (similar to Tiktok) or video services (similar to Omegle).", "link": "https://devpost.com/software/chatify-42wgsp", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\nour team for sure wanted to create something that could help in some shape or form towards global issues. we bounced through several ideas, ranging from carpool tracking to -----> tree !!!  planting. chatify is a combination of all our ideas, in which users are able to discuss in small groups about a topic pertaining to a global issue. through socially tough times like covid-19, meeting new people will no longer be a challenge!\nwhat it does\nchoose from numerous different chat rooms, where unique topics have been assigned. the first to join a room is able to set a topic, which will be seen on the main page. up to 10 people can join each room, and then, let the debate begin! there\u2019s no right or wrong answer, it\u2019s just an opportunity to share your thoughts and hear from everyone else.\nhow we built it\nthis site is built with bootstrap and react, while the chat rooms are made with firebase. it is a simple react app designed with bootstrap, and when users click on a room, they\u2019ll be redirected to a new page where different hosts can message one another.\nchallenges we ran into\nit took us a few hours to come up with a solid idea of what kind of project we wanted to make. we had a lot of different ideas, but we weren\u2019t sure how practical they were to realize them. as well, since not everyone on the team is familiar with the same technology, it was difficult to modify certain aspects of the project that we weren\u2019t a part of. there was a lot of new learning, especially with firebase since only one member of the team was familiar with it.\naccomplishments that we're proud of\nit works!! although not all aspects of the project are completed, the general structure and framework is functioning. users are able to first log in via google account, and then choose a chat room that has a topic of interest to them. they can chat away! it was not easy to complete a project with a team that\u2019s remote from each other, and that made the communication between ourselves difficult.\nwhat we learned\nreflecting on past hackathons, this one was a lot less stressful because we had come up with the idea beforehand and already arranged what technology we wanted to incorporate. we divided the team into two halves, which worked on separate ends of the project to lessen the burden on each person. ultimately, we learned that it\u2019s important to rely on each other for help, since everyone\u2019s able to bring something to the table.\nwhat's next for chatify\ndynamism!! our project is built to only support 12 chatrooms at once, but further improvements could bump this number to as many as the public desires. as well, chatify is very straightforward right now and doesn\u2019t offer much besides a messaging api. we can improve our project by adding more features, such as \u201cshorts\u201d (similar to tiktok) or video services (similar to omegle).", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59506526}, {"Unnamed: 0": 6566, "autor": "Eco-lution", "date": null, "content": "Inspiration\nEcosystem, the fabric of life on which we all depend, is declining rapidly because of human actions. Tropical forests across the world were destroyed at an alarming rate in 2020, despite the raging Coronavirus and the lockdowns as natural forces took over. India lost nearly 38.5 thousand hectares (Kha) of tropical forest between 2019 and 2020 making up nearly 14 per cent loss of its tree cover. Millions of tons of food go unused in India every year. India faces major environmental challenges associated with waste generation and inadequate waste collection, transport, treatment and disposal. Current systems in India cannot cope with the volumes of waste generated by an increasing urban population, and this impacts on the environment and public health.\nWhat it does\nThis app contains certain levels which can be achieved by performing certain eco-friendly activities and after achieving certain levels the users can claim rewards and badges.\nWe have a certain number of registered non-profit plant organizations which plant trees. The user can connect through the organization through the app and plant a tree verified by the organization the points can be gained.\nThere are many organisations which recycle the dry as well as organic waste and produce some useful products out of them.\nWe can connect with organisations that recycle dry waste like bottles, cans, clothing, plastic, wood, glass, metals and paper. People can send their waste by packaging it to the organisation\u2019s address and help contribute towards recycling it.\nUsers can also see various cleanliness drives happening in their vicinity and can participate in them and gain rewards.\nUsers can also participate in marathons and register them through the app and gain points. These marathons take place for raising funds for social causes .\nUsers can also see the contributions made by other people in the nearby region.\nHow we built it\nWe used React and Material UI for designing and building the web application. We made a list of all the organisations that work for these causes and them had them listed in the dashboard so the users can interact with them in a few clicks\nChallenges we ran into\nUnderstanding Material UI and other libraries in React and building components with them.\nAccomplishments that we're proud of\nOne of the themes of the hackathon was building hacks on the environment and climate change. We got a chance to work on this theme and explore many more options about how we can contribute towards the environment. The app currently contains some well known organisations working tirelessly in their respective fields and interacting with them or participating in their events is one click\nWhat we learned\nWorking in a team, teaming up with individuals having different skills and experience, understanding each other's views and coming up with the solution was also a worthwhile experience\nGetting to learn the applications of technologies we were already aware of, like react.\nWe learned more about the organisations themselves and how they work\nWhat's next for Eco-lution\nWe have a number of ideas of features that could be added in the future, here are some of them:\nUsers can team up with other users and participate in various drives.\nThey can visit other users Profile and connect with them. They can also post things related to eco-friendly activities.\nUsers can earn various rewards and offers if they have a high enough score or if they complete a specific achievement.\nThey can get filtered tasks for their locality and help make their surrounding more cleaner and greener\nWe can partner up with organisations that recycle dry waste like bottles, cans, clothing, plastic, wood, glass, metals and paper. People can send their waste by packaging it to the organisation\u2019s address and help contribute towards recycling it.", "link": "https://devpost.com/software/eco-lution-nigp74", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\necosystem, the fabric of life on which we all depend, is declining rapidly because of human actions. tropical forests across the world were destroyed at an alarming rate in 2020, despite the raging coronavirus and the lockdowns as natural forces took over. india lost nearly 38.5 thousand hectares (kha) of tropical forest between 2019 and 2020 making up nearly 14 per cent loss of its -----> tree !!!  cover. millions of tons of food go unused in india every year. india faces major environmental challenges associated with waste generation and inadequate waste collection, transport, treatment and disposal. current systems in india cannot cope with the volumes of waste generated by an increasing urban population, and this impacts on the environment and public health.\nwhat it does\nthis app contains certain levels which can be achieved by performing certain eco-friendly activities and after achieving certain levels the users can claim rewards and badges.\nwe have a certain number of registered non-profit plant organizations which plant trees. the user can connect through the organization through the app and plant a tree verified by the organization the points can be gained.\nthere are many organisations which recycle the dry as well as organic waste and produce some useful products out of them.\nwe can connect with organisations that recycle dry waste like bottles, cans, clothing, plastic, wood, glass, metals and paper. people can send their waste by packaging it to the organisation\u2019s address and help contribute towards recycling it.\nusers can also see various cleanliness drives happening in their vicinity and can participate in them and gain rewards.\nusers can also participate in marathons and register them through the app and gain points. these marathons take place for raising funds for social causes .\nusers can also see the contributions made by other people in the nearby region.\nhow we built it\nwe used react and material ui for designing and building the web application. we made a list of all the organisations that work for these causes and them had them listed in the dashboard so the users can interact with them in a few clicks\nchallenges we ran into\nunderstanding material ui and other libraries in react and building components with them.\naccomplishments that we're proud of\none of the themes of the hackathon was building hacks on the environment and climate change. we got a chance to work on this theme and explore many more options about how we can contribute towards the environment. the app currently contains some well known organisations working tirelessly in their respective fields and interacting with them or participating in their events is one click\nwhat we learned\nworking in a team, teaming up with individuals having different skills and experience, understanding each other's views and coming up with the solution was also a worthwhile experience\ngetting to learn the applications of technologies we were already aware of, like react.\nwe learned more about the organisations themselves and how they work\nwhat's next for eco-lution\nwe have a number of ideas of features that could be added in the future, here are some of them:\nusers can team up with other users and participate in various drives.\nthey can visit other users profile and connect with them. they can also post things related to eco-friendly activities.\nusers can earn various rewards and offers if they have a high enough score or if they complete a specific achievement.\nthey can get filtered tasks for their locality and help make their surrounding more cleaner and greener\nwe can partner up with organisations that recycle dry waste like bottles, cans, clothing, plastic, wood, glass, metals and paper. people can send their waste by packaging it to the organisation\u2019s address and help contribute towards recycling it.", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59506566}, {"Unnamed: 0": 6589, "autor": "ZKP NFT drop", "date": null, "content": "Project overview\nGenerate a random number for fair NFT distribution using a multi-party ZKP commit-reveal scheme such that only 1 minter has to be honest for the randomness to be unbiased.\nDiscord contact username: ismaeldm#6744\nSLIDES\nInspiration\nWe observed that some NFT projects face a problem where they use on-chain randomness to distribute NFTs with traits of varying rarity. Yet, on-chain randomness is a poor source of entropy. Miners who collude with an attacker may easily manipulate the random value used to distribute NFTs and give the attacker the most rare NFTs, which is unfair to other market participants. Another attack is to submit and cancel mint transactions until one mints an ultra-rare NFT.\nOne solution commonly proposed, such as by Paradigm, is to use a third-party Verifiable Random Function service (such as Chainlink VRF) to obtain this random value. We believe, however, that the community deserves an alternative that does not rely on a third party. Even if there is a good third-party solution, an NFT team may not wish to use it. In such a case, whatever their reasons, they should be able to choose a different method to generate this random value.\nWe also observed that zero-knowledge proofs (ZKPs) on Ethereum are a very useful skill and we are motivated to use this hackathon to learn how to write ZKPs and smart contracts that use them.\nWhat it does\nOur approach is to have the team and each minter commit to a secret value, and reveal it later. We use a ZKP to prove the correctness of multiple revealed values in a single transaction, allowing users and the team to save gas. Only one minter needs be honest for our approach to work.\nOur scheme is fully described in this document.\nHow we built it\nWe used a variety of languages and developer tools:\nName What it was used for\nSolidity Ethereum smart contracts\nHardhat Smart contract development and testing\ncircom v2 ZK circuits\ncircom-helper Circuit development and testing\nsnarkjs / rapidsnark Proof generation\nzkey-manager Proving key generation\nWe adopted code from open-source projects including the following:\nProject Components\nMinimum Anti-Collusion Infrastructure Incremental Merkle Tree contract, circuits, and library\nChallenges we ran into\nZKPs are complex and a require a lot of deep domain knowledge. As such, we prepared for the hackathon by practicing how to write and test circom circuits (e.g. writing a circuit which allows a prover to prove that a list of input signals is sorted).\nWhen figuring out how our system should work, we spent a lot of time finding and addressing potential vulnerabilities. For instance, we had to ensure that the last NFT minter would not be able to influence the random number. To prevent this attack, we added a randomness submission deadline such that only one minter needs to withhold their secret random number until after the deadline, so the last minter has no way to unduly bias the result.\nWe had tried to address the last-minter attack by applying a block hash not known in advance, but decided against this approach. The reason for this is that if all minters collude, there is no point randomising the distribution as they can simply redistribute the NFTs among themselves. Furthermore, we wanted to keep the project simple and manageable for this hackathon, so we removed this mechanism from our scheme.\nAccomplishments that we're proud of\nFinished project in time, writing all circuits, contracts and tests\nManaged to create a script that demonstrates the functionality\nKept a cheerful and motivating environment all the time\nWe were mindful of our time and energy and planned accordingly\nWhat we learned\nHow to write circuits in circom, generate proofs, and verify them in Ethereum.\nThe utility of Merkle trees as a mechanism to prove knowledge of many values without the high gas cost of passing them into the verifier contract function as public inputs.\nWhat's next for ZK-boyz\nWe believe ZKPs will have a key role in the future of Ethereum and we're motivated to keep finding problems that can be solved with them", "link": "https://devpost.com/software/zkpboyz", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "project overview\ngenerate a random number for fair nft distribution using a multi-party zkp commit-reveal scheme such that only 1 minter has to be honest for the randomness to be unbiased.\ndiscord contact username: ismaeldm#6744\nslides\ninspiration\nwe observed that some nft projects face a problem where they use on-chain randomness to distribute nfts with traits of varying rarity. yet, on-chain randomness is a poor source of entropy. miners who collude with an attacker may easily manipulate the random value used to distribute nfts and give the attacker the most rare nfts, which is unfair to other market participants. another attack is to submit and cancel mint transactions until one mints an ultra-rare nft.\none solution commonly proposed, such as by paradigm, is to use a third-party verifiable random function service (such as chainlink vrf) to obtain this random value. we believe, however, that the community deserves an alternative that does not rely on a third party. even if there is a good third-party solution, an nft team may not wish to use it. in such a case, whatever their reasons, they should be able to choose a different method to generate this random value.\nwe also observed that zero-knowledge proofs (zkps) on ethereum are a very useful skill and we are motivated to use this hackathon to learn how to write zkps and smart contracts that use them.\nwhat it does\nour approach is to have the team and each minter commit to a secret value, and reveal it later. we use a zkp to prove the correctness of multiple revealed values in a single transaction, allowing users and the team to save gas. only one minter needs be honest for our approach to work.\nour scheme is fully described in this document.\nhow we built it\nwe used a variety of languages and developer tools:\nname what it was used for\nsolidity ethereum smart contracts\nhardhat smart contract development and testing\ncircom v2 zk circuits\ncircom-helper circuit development and testing\nsnarkjs / rapidsnark proof generation\nzkey-manager proving key generation\nwe adopted code from open-source projects including the following:\nproject components\nminimum anti-collusion infrastructure incremental merkle -----> tree !!!  contract, circuits, and library\nchallenges we ran into\nzkps are complex and a require a lot of deep domain knowledge. as such, we prepared for the hackathon by practicing how to write and test circom circuits (e.g. writing a circuit which allows a prover to prove that a list of input signals is sorted).\nwhen figuring out how our system should work, we spent a lot of time finding and addressing potential vulnerabilities. for instance, we had to ensure that the last nft minter would not be able to influence the random number. to prevent this attack, we added a randomness submission deadline such that only one minter needs to withhold their secret random number until after the deadline, so the last minter has no way to unduly bias the result.\nwe had tried to address the last-minter attack by applying a block hash not known in advance, but decided against this approach. the reason for this is that if all minters collude, there is no point randomising the distribution as they can simply redistribute the nfts among themselves. furthermore, we wanted to keep the project simple and manageable for this hackathon, so we removed this mechanism from our scheme.\naccomplishments that we're proud of\nfinished project in time, writing all circuits, contracts and tests\nmanaged to create a script that demonstrates the functionality\nkept a cheerful and motivating environment all the time\nwe were mindful of our time and energy and planned accordingly\nwhat we learned\nhow to write circuits in circom, generate proofs, and verify them in ethereum.\nthe utility of merkle trees as a mechanism to prove knowledge of many values without the high gas cost of passing them into the verifier contract function as public inputs.\nwhat's next for zk-boyz\nwe believe zkps will have a key role in the future of ethereum and we're motivated to keep finding problems that can be solved with them", "sortedWord": "None", "removed": "Nan", "score": 4, "comments": 0, "media": null, "medialink": null, "identifyer": 59506589}, {"Unnamed: 0": 6611, "autor": "SheerAI", "date": null, "content": "Inspiration \ud83d\udca1\nDeveloping countries across the globe had already been facing shortage of resources and adequately skilled healthcare workers before pandemics began. COVID-19 pandemics exacerbated the situation exponentially for both healthcare seekers and givers. Because we believe the power modern-day technologies have to help people to live a decent life, we developed SheerAI. It is an application, empowered by AI, that helps healthcare workers to deliver the best care. workers before pandemics began. COVID-19 pandemics exacerbated the situation exponentially for both healthcare seekers and givers.\nBecause we believe the power modern-day technologies have to help people to live a decent life, we developed SheerAI. It is an application, empowered by AI, that helps healthcare workers to deliver quick, accurate, and lasting health care.\nWhat it does \ud83e\udd14\nVoice to Text Note-taking, because it is more convenient!\nSymptom Identification from Notes\nMedical History Identification\nPredict Possible Diagnosis\nSecure Data Storage which is decentralized\nHow we built it \ud83c\udfd7\nFirst and foremost, it is Crafted with \ud83d\udc99. For the front-end, we\u2019ve used React.js & Tailwind as CSS framework. The Authentication (OAuth) has been done by Firebase & we\u2019re also using the Cloudstore database for storing user logs. We\u2019re using Python as the root language for creating the ML model which is fed via a few different libraries such as Numpy, Pandas & Keras. An image of the same is deployed on Docker for Debug purposes & the same is hosted on a free dyno of Heroku. For preserving the privacy of those docs/audio-files, we are keeping track of all of those stuff by deploying the media on IPFS which makes all of those immutable & tamperproof.\nFor the ML model, we have used a very popular dataset to extract the information from those tables & columns & created our own model to predict diseases from given inputs (via keyword extraction from the audio-note). Then there are 4 different classification algorithms (Decision Tree, KNN, Naive Bayes, Random Forest) which we are using to classify, analyze & predict the disease using that dataset. At the end, we do a cosine-average and return the true value of the predicted disease.\nChallenges We ran into \ud83e\uddf1\nThere were lots of challenges on our way. First, because we are all online and spread around the globe, it was somewhat difficult for us to be communicating during the process. We also spent a great deal of time discussing ideas for the project. We have reached a final decision on what to include in our project after we had a couple of calls with mentors. After we settled on the idea, we have separated the work according to everyone's skills. Shradha was the product manager. She deisgned outlook and prepared our product for promotion. Aziz was primarily working on the Front-end, while Irenna set up integrations & backend. Besides, Pratyay worked on building the ML model & cooked the API. We faced most challenges when we tried to allocate segregated chunks into one project.\nDesign\nWe were heavily inspired by the revised version of Double Diamond design process developed by UK Research Council, which not only includes visual design, but a full-fledged research cycle in which you must discover and define your problem before tackling your solution.\nDiscover: a deep dive into the problem we are trying to solve.\nDefine: synthesizing the information from the discovery phase into a problem definition.\nDevelop: think up solutions to the problem.\nDeliver: pick the best solution and build that.\nThis time went for the minimalist Material UI design. We utilized design tools like Figma, Photoshop & Illustrator to prototype our designs before doing any coding. Through this, we are able to get iterative feedback so that we spend less time re-writing code.\nResearch \ud83d\udcda\nResearch is the key to empathizing with users: we found our specific user group early and that paves the way for our whole project. Here are few of the resources that were helpful to us \u2014\nThe role of medical data in efficient patient care delivery: a review : https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6486797/\nLack of medical workers plagues developing world : https://www.reuters.com/article/us-braindrain-idINTRE49001E20081001\nThere is a global shortage of nurses. COVID-19 is making it worse. : https://www.clintonhealthaccess.org/there-is-a-global-shortage-of-nurses-covid-19-is-making-it-worse/\nDisease Prediction : https://www.irjet.net/archives/V6/i12/IRJET-V6I12122.pdf\n\u2663 Dataset used :- https://bit.ly/30KYQ4r\nCREDITS\nDesign Resources : Freepik\nIcons : Icons8\nFont : Lufga / Recoleta / Manrope / Montserrat / Roboto\nTakeways\nAccomplishments that we're proud of \ud83d\ude4c\nA fully working prototype! This has been an intense yet insightful 36 hours. We are very proud to have designed and build an application within such a short timeframe.\nLearning how to collaborate on GitHub! Not all of us were familiar with making branches or making a PR and merging. This hackathon has fast-tracked the learning process and we are all now very comfortable in using GitHub!\nLearning new technology (like Tailwind CSS, routing in React, training ML model, implementing sophisticated design features) meeting new people, debugging, debugging, and more debugging!\nOptimizing Hyperparameters of the ML model in such a short span was really a big dare that we made.\nThe idea of helping health workers with burnout and making positive changes in our community.\nWhat we learned \ud83d\ude4c\nStaying hydrated was our motto for completing this impactful and complicated project on time. We have learned how great wins are accomplished by working together. For the technical part, we did face some issues when we were merging front-end and backend. We also gave our level best to make the UI/UX look minimalistic and useful! Not to mention, documentations and help of google for technologies we used (be it react components libraries, ML, IPFS, API calls) were exteremely useful!\nWhat's next for SheerAI \ud83d\udcc3\nWe believe that Sheer AI is an app with a great potential. Since all four of us are very passionate in tackling the issue of burnout in health workers, it's easy to come up with a lot of ideas for new features (like we did in the beginning of this hackathon!). However, we now have learnt the importance of focusing on a single feature at a time and making sure that feature works flawlessly before designing a new feature! \u2728\nThis includes:\nMoving forward and making all the storage system decentralized\nRefractor our code; because there's so much we can do under 36 hours\nDoing many, many testing (another thing we lack on during the past 36 hours). We want to understand all the nitty gritty details on whether the app flow is intuitive or how the speech-to-text feature behave on a noisy background, or if there is anything we can do to make the user experience better.\nOverall, we hope that one day this project can be widely used globally to help health workers in their day-to-day job.\nNote \u2014 API credentials have been revoked. If you want to run the same on your local, use your own credentials.", "link": "https://devpost.com/software/sheerai", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration \ud83d\udca1\ndeveloping countries across the globe had already been facing shortage of resources and adequately skilled healthcare workers before pandemics began. covid-19 pandemics exacerbated the situation exponentially for both healthcare seekers and givers. because we believe the power modern-day technologies have to help people to live a decent life, we developed sheerai. it is an application, empowered by ai, that helps healthcare workers to deliver the best care. workers before pandemics began. covid-19 pandemics exacerbated the situation exponentially for both healthcare seekers and givers.\nbecause we believe the power modern-day technologies have to help people to live a decent life, we developed sheerai. it is an application, empowered by ai, that helps healthcare workers to deliver quick, accurate, and lasting health care.\nwhat it does \ud83e\udd14\nvoice to text note-taking, because it is more convenient!\nsymptom identification from notes\nmedical history identification\npredict possible diagnosis\nsecure data storage which is decentralized\nhow we built it \ud83c\udfd7\nfirst and foremost, it is crafted with \ud83d\udc99. for the front-end, we\u2019ve used react.js & tailwind as css framework. the authentication (oauth) has been done by firebase & we\u2019re also using the cloudstore database for storing user logs. we\u2019re using python as the root language for creating the ml model which is fed via a few different libraries such as numpy, pandas & keras. an image of the same is deployed on docker for debug purposes & the same is hosted on a free dyno of heroku. for preserving the privacy of those docs/audio-files, we are keeping track of all of those stuff by deploying the media on ipfs which makes all of those immutable & tamperproof.\nfor the ml model, we have used a very popular dataset to extract the information from those tables & columns & created our own model to predict diseases from given inputs (via keyword extraction from the audio-note). then there are 4 different classification algorithms (decision -----> tree !!! , knn, naive bayes, random forest) which we are using to classify, analyze & predict the disease using that dataset. at the end, we do a cosine-average and return the true value of the predicted disease.\nchallenges we ran into \ud83e\uddf1\nthere were lots of challenges on our way. first, because we are all online and spread around the globe, it was somewhat difficult for us to be communicating during the process. we also spent a great deal of time discussing ideas for the project. we have reached a final decision on what to include in our project after we had a couple of calls with mentors. after we settled on the idea, we have separated the work according to everyone's skills. shradha was the product manager. she deisgned outlook and prepared our product for promotion. aziz was primarily working on the front-end, while irenna set up integrations & backend. besides, pratyay worked on building the ml model & cooked the api. we faced most challenges when we tried to allocate segregated chunks into one project.\ndesign\nwe were heavily inspired by the revised version of double diamond design process developed by uk research council, which not only includes visual design, but a full-fledged research cycle in which you must discover and define your problem before tackling your solution.\ndiscover: a deep dive into the problem we are trying to solve.\ndefine: synthesizing the information from the discovery phase into a problem definition.\ndevelop: think up solutions to the problem.\ndeliver: pick the best solution and build that.\nthis time went for the minimalist material ui design. we utilized design tools like figma, photoshop & illustrator to prototype our designs before doing any coding. through this, we are able to get iterative feedback so that we spend less time re-writing code.\nresearch \ud83d\udcda\nresearch is the key to empathizing with users: we found our specific user group early and that paves the way for our whole project. here are few of the resources that were helpful to us \u2014\nthe role of medical data in efficient patient care delivery: a review : https://www.ncbi.nlm.nih.gov/pmc/articles/pmc6486797/\nlack of medical workers plagues developing world : https://www.reuters.com/article/us-braindrain-idintre49001e20081001\nthere is a global shortage of nurses. covid-19 is making it worse. : https://www.clintonhealthaccess.org/there-is-a-global-shortage-of-nurses-covid-19-is-making-it-worse/\ndisease prediction : https://www.irjet.net/archives/v6/i12/irjet-v6i12122.pdf\n\u2663 dataset used :- https://bit.ly/30kyq4r\ncredits\ndesign resources : freepik\nicons : icons8\nfont : lufga / recoleta / manrope / montserrat / roboto\ntakeways\naccomplishments that we're proud of \ud83d\ude4c\na fully working prototype! this has been an intense yet insightful 36 hours. we are very proud to have designed and build an application within such a short timeframe.\nlearning how to collaborate on github! not all of us were familiar with making branches or making a pr and merging. this hackathon has fast-tracked the learning process and we are all now very comfortable in using github!\nlearning new technology (like tailwind css, routing in react, training ml model, implementing sophisticated design features) meeting new people, debugging, debugging, and more debugging!\noptimizing hyperparameters of the ml model in such a short span was really a big dare that we made.\nthe idea of helping health workers with burnout and making positive changes in our community.\nwhat we learned \ud83d\ude4c\nstaying hydrated was our motto for completing this impactful and complicated project on time. we have learned how great wins are accomplished by working together. for the technical part, we did face some issues when we were merging front-end and backend. we also gave our level best to make the ui/ux look minimalistic and useful! not to mention, documentations and help of google for technologies we used (be it react components libraries, ml, ipfs, api calls) were exteremely useful!\nwhat's next for sheerai \ud83d\udcc3\nwe believe that sheer ai is an app with a great potential. since all four of us are very passionate in tackling the issue of burnout in health workers, it's easy to come up with a lot of ideas for new features (like we did in the beginning of this hackathon!). however, we now have learnt the importance of focusing on a single feature at a time and making sure that feature works flawlessly before designing a new feature! \u2728\nthis includes:\nmoving forward and making all the storage system decentralized\nrefractor our code; because there's so much we can do under 36 hours\ndoing many, many testing (another thing we lack on during the past 36 hours). we want to understand all the nitty gritty details on whether the app flow is intuitive or how the speech-to-text feature behave on a noisy background, or if there is anything we can do to make the user experience better.\noverall, we hope that one day this project can be widely used globally to help health workers in their day-to-day job.\nnote \u2014 api credentials have been revoked. if you want to run the same on your local, use your own credentials.", "sortedWord": "None", "removed": "Nan", "score": 6, "comments": 0, "media": null, "medialink": null, "identifyer": 59506611}, {"Unnamed: 0": 6698, "autor": "SellOTree", "date": null, "content": "Inspiration\nDue to massive commercialization, we are somewhere neglecting our mother Earth. Environment's CO2 level is increasing at an alarming rate. So, to deal with it we can perform our roles as a concerned citizen and take initiative towards making contributions for a better future.\nWhat it does\nThis website aims towards raising funds for tree plantation and generates hope for the future. Every 20 Rupees contributed counts as a tree planted. Users can donate and make their contributions as well as they can also participate in the ongoing drives where we would link them to the various plantation drives. They can be involved individually or as a business group that may include flexible programs to fit company goals at reduced planting costs or even the children and teachers can get involved to increase awareness and educating children about the enormous impact that trees have on our planet. A separate contact us section is added so that users can directly connect in case of queries. They can also subscribe to the regular newsletter so that the latest drives and contributions could be spread as much as possible in turn boosting awareness. The main feature that this website includes is the donate section where the users can donate the amount that they see fit accordingly. A payment gateway integration would be added via which they can make a seamless payment to plant a tree.\nHow we built it\nWe created a website which would act as a web app for fund raising. The languages which we used were HTML, CSS and JavaScript.\nChallenges we ran into\nSetting the appropriate amount for each tree plantation cost. But we did our research and analyzed certain surveys which helped to conclude the price. Also communicating with the team members remotely was a little bit challenging in the beginning but by the end we found our way.\nAccomplishments that we're proud of\nCombining all the required sections to create a decent fund raiser platform where individuals/collaborators can make their contributions to preserve the future.\nWhat we learned\nThe technicalities of payment gateway integration.\nWhat's next for SellOTree\nEstablishing our own (SellOTree's) plantation drives and starting with the collaboration for generating nurseries. Selling biodegradable products in future is also in scope.", "link": "https://devpost.com/software/sellotree", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\ndue to massive commercialization, we are somewhere neglecting our mother earth. environment's co2 level is increasing at an alarming rate. so, to deal with it we can perform our roles as a concerned citizen and take initiative towards making contributions for a better future.\nwhat it does\nthis website aims towards raising funds for -----> tree !!!  plantation and generates hope for the future. every 20 rupees contributed counts as a tree planted. users can donate and make their contributions as well as they can also participate in the ongoing drives where we would link them to the various plantation drives. they can be involved individually or as a business group that may include flexible programs to fit company goals at reduced planting costs or even the children and teachers can get involved to increase awareness and educating children about the enormous impact that trees have on our planet. a separate contact us section is added so that users can directly connect in case of queries. they can also subscribe to the regular newsletter so that the latest drives and contributions could be spread as much as possible in turn boosting awareness. the main feature that this website includes is the donate section where the users can donate the amount that they see fit accordingly. a payment gateway integration would be added via which they can make a seamless payment to plant a tree.\nhow we built it\nwe created a website which would act as a web app for fund raising. the languages which we used were html, css and javascript.\nchallenges we ran into\nsetting the appropriate amount for each tree plantation cost. but we did our research and analyzed certain surveys which helped to conclude the price. also communicating with the team members remotely was a little bit challenging in the beginning but by the end we found our way.\naccomplishments that we're proud of\ncombining all the required sections to create a decent fund raiser platform where individuals/collaborators can make their contributions to preserve the future.\nwhat we learned\nthe technicalities of payment gateway integration.\nwhat's next for sellotree\nestablishing our own (sellotree's) plantation drives and starting with the collaboration for generating nurseries. selling biodegradable products in future is also in scope.", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59506698}, {"Unnamed: 0": 6700, "autor": "Eligible", "date": null, "content": "Inspiration\nDefi ecosystem grows fast and appears a lot of projects and most of them distribute tokens in some way, and while projects strive to find the community that will suit them, that's not the case for major instances, mainly projects couldn't control whom they distribute their tokens and questioning themself: are they distributing tokens to the right hands? On the other hand, usually, there are a lot of applicants missing the sale that can be beneficial for the project. Usually, there is no proper selection, investors receive their allocations depending on how fast they are, or even randomly, or via whitelists based on some off-chain data, such as KYC, Social Networks activity, and other bullshit. We are thinking that your allocation should depend on your on-chain activity.\nWhat it does\nFirst of all, the project provides information about planning a fundraising campaign and some selection criteria for investors. Then anyone can provide his address to participate in token-sale and commit the amount, that he wants to invest. All applicants registered via Eligible smart contract. After that our Degen Guardians start their screening and Kick out flippers, week hands, and other not eligible guys. After that, they provide Merkle proof into Eligible smart contract,\nHow we built it\nOur solution is based on:\nThe Graph to collect on-chain data for further analysis\nIn Eligible we use The Graph for querying criteria eligibility data (liquidity provision and POAP criteria). Cases covered by now include Aave subgraph, POAP subgraph, Uniswap subgraph (V2 and V3), and Curve subgraph.\nOur own addresses scoring system\nWe are using a system that assigns scores for each applicant based on his on-chain activity.\nWalletConnect/Metamask as a convenient way to authenticate users and their addresses, to apply for token-sale.\nSmart contracts on Solidity The smart contract allows to set up sales and receive deposits. When the sale is over the seller reveals eligible addresses. The seller can reveal an array of eligible addresses from the degen Guardian or provide a Merkle tree that includes all eligible addresses and their allocations\nPOAP's to identify applicants interest based on meetups, which he visited\nChallenges we ran into\nFirst of all, it's not an easy challenge to arrive in Lisbon from Moscow and bring to the hackathon all our friends without tickets - that's a joke, obviously(maybe not). Also, we was in Lisbon since 16th October and it's a big challenge to participate in a hackathon after being high every day during all week.\nAccomplishments that we're proud of\nWe are really proud that we went through all of these challenges successfully and managed to develop cute UI/UX, developed robust smart-contracts and our own scoring mechanism for applicant's screening.\nWhat we learned\nDuring the hackathon, we improved our skills in creating Node.js backend applications and interacting with The Graph. Also learned how to use WalletConnect to authenticate users.\nWhat's next for Eligible\nNext step we are planning to create Eligible DAO", "link": "https://devpost.com/software/heimdall-d9iczu", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\ndefi ecosystem grows fast and appears a lot of projects and most of them distribute tokens in some way, and while projects strive to find the community that will suit them, that's not the case for major instances, mainly projects couldn't control whom they distribute their tokens and questioning themself: are they distributing tokens to the right hands? on the other hand, usually, there are a lot of applicants missing the sale that can be beneficial for the project. usually, there is no proper selection, investors receive their allocations depending on how fast they are, or even randomly, or via whitelists based on some off-chain data, such as kyc, social networks activity, and other bullshit. we are thinking that your allocation should depend on your on-chain activity.\nwhat it does\nfirst of all, the project provides information about planning a fundraising campaign and some selection criteria for investors. then anyone can provide his address to participate in token-sale and commit the amount, that he wants to invest. all applicants registered via eligible smart contract. after that our degen guardians start their screening and kick out flippers, week hands, and other not eligible guys. after that, they provide merkle proof into eligible smart contract,\nhow we built it\nour solution is based on:\nthe graph to collect on-chain data for further analysis\nin eligible we use the graph for querying criteria eligibility data (liquidity provision and poap criteria). cases covered by now include aave subgraph, poap subgraph, uniswap subgraph (v2 and v3), and curve subgraph.\nour own addresses scoring system\nwe are using a system that assigns scores for each applicant based on his on-chain activity.\nwalletconnect/metamask as a convenient way to authenticate users and their addresses, to apply for token-sale.\nsmart contracts on solidity the smart contract allows to set up sales and receive deposits. when the sale is over the seller reveals eligible addresses. the seller can reveal an array of eligible addresses from the degen guardian or provide a merkle -----> tree !!!  that includes all eligible addresses and their allocations\npoap's to identify applicants interest based on meetups, which he visited\nchallenges we ran into\nfirst of all, it's not an easy challenge to arrive in lisbon from moscow and bring to the hackathon all our friends without tickets - that's a joke, obviously(maybe not). also, we was in lisbon since 16th october and it's a big challenge to participate in a hackathon after being high every day during all week.\naccomplishments that we're proud of\nwe are really proud that we went through all of these challenges successfully and managed to develop cute ui/ux, developed robust smart-contracts and our own scoring mechanism for applicant's screening.\nwhat we learned\nduring the hackathon, we improved our skills in creating node.js backend applications and interacting with the graph. also learned how to use walletconnect to authenticate users.\nwhat's next for eligible\nnext step we are planning to create eligible dao", "sortedWord": "None", "removed": "Nan", "score": 5, "comments": 2, "media": null, "medialink": null, "identifyer": 59506700}, {"Unnamed: 0": 6712, "autor": "climate updater", "date": null, "content": "Inspiration\nthe climate change all around us and keeping a update of it by just a window application\nWhat it does\nbasically anybody who is using this windows app can upload the weather in their area and by using a server connection and database connectivity people far away using this app can know what is the weather in that particular area.\nHow we built it\nwith the help of python tkinter library we can develop these kind of simple projects of database connectivity to add and update data.\nChallenges we ran into\nchallenge is the main functions building for accessing the data at the time of presenting on the tree view\nAccomplishments that we're proud of\ni made it in 3 hrs which is my current record\nWhat we learned\ni learned how to manage deadline and complete the project with no bugs in it.\nWhat's next for climate updater\nin next climate updater we will see new features like MAP showing temperature and weather of different places at the same time.", "link": "https://devpost.com/software/climate-updater", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\nthe climate change all around us and keeping a update of it by just a window application\nwhat it does\nbasically anybody who is using this windows app can upload the weather in their area and by using a server connection and database connectivity people far away using this app can know what is the weather in that particular area.\nhow we built it\nwith the help of python tkinter library we can develop these kind of simple projects of database connectivity to add and update data.\nchallenges we ran into\nchallenge is the main functions building for accessing the data at the time of presenting on the -----> tree !!!  view\naccomplishments that we're proud of\ni made it in 3 hrs which is my current record\nwhat we learned\ni learned how to manage deadline and complete the project with no bugs in it.\nwhat's next for climate updater\nin next climate updater we will see new features like map showing temperature and weather of different places at the same time.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59506712}, {"Unnamed: 0": 6770, "autor": "ecoNet", "date": null, "content": "try out this project github repo\nInspiration\nWe were inspired by social networks and the power of communities\nWhat it does\nIt is basically a social media platform that helps people to share there memories with nature.\nHow we built it\nWe used firebase for backend, react for frontend along with SAWO Authentication\nChallenges we ran into\nWe all were beginners hence we faced multiple challenges\nAccomplishments that we're proud of\nThis was our first hackathon which we took seriously and we enjoyed the process I think that's the biggest accomplishment for us.\nWhat we learned\nWe learned a lot of things from our mistakes in terms of tech and the most important thing that we learnt was teamwork.\nWhat's next for ecoNet\nWe are planning to add more exciting features to ecoNet such as location tagging so that users can track their plants. Tree Maps etc. I think location tagging along with full follow/friend requests liketrees will be agame changer as in just imagine, we plant trees but often forget where we had planted it and eventually forget about it but with location tagging people would be able to find their trees even after 5-10 years and share it with their family and friends.", "link": "https://devpost.com/software/econet", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "try out this project github repo\ninspiration\nwe were inspired by social networks and the power of communities\nwhat it does\nit is basically a social media platform that helps people to share there memories with nature.\nhow we built it\nwe used firebase for backend, react for frontend along with sawo authentication\nchallenges we ran into\nwe all were beginners hence we faced multiple challenges\naccomplishments that we're proud of\nthis was our first hackathon which we took seriously and we enjoyed the process i think that's the biggest accomplishment for us.\nwhat we learned\nwe learned a lot of things from our mistakes in terms of tech and the most important thing that we learnt was teamwork.\nwhat's next for econet\nwe are planning to add more exciting features to econet such as location tagging so that users can track their plants. -----> tree !!!  maps etc. i think location tagging along with full follow/friend requests liketrees will be agame changer as in just imagine, we plant trees but often forget where we had planted it and eventually forget about it but with location tagging people would be able to find their trees even after 5-10 years and share it with their family and friends.", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59506770}, {"Unnamed: 0": 6793, "autor": "Tree Surgeon", "date": null, "content": "Inspiration\nWe wanted to create something to spread awareness about climate change in our respective communities. Having an international team from across the globe, with greenhouse gases hotspots such as London and Chennai being our home cities, we decided on prototyping a handy device which helped nurture something cheap and practical like a plant, and turn it into a therapeutic mindfulness exercise that also helps to make the world a bit more green!\nWhat it does\nTree surgeon allows you to monitor your plant on the go! Sensor values are sent to our website, additionally a push notification can alert you from your phone.\nHow we built it\nAn ESP-32 development board was used to connect the sensor to WIFI. We then create a website and used API to send push emails.\nChallenges we ran into\nDuring coding, due to faults in the implementation of the ESP-32 library to Arduino, many hours were lost trying to locate missing library headers such as \"AsyncTCP.h\". Certain libraries were catered specifically to ESP8266, which also caused problems. Some libraries also required dependencies that were hard to discover. Moreover, we changed board from a Mega 2560 to an ESP-32 as the Wi-Fi and Bluetooth capabilities proved superior.\nAccomplishments that we're proud of\nDespite coming into the competition having never met before, never worked on sending data over the internet, never sent emails out to recipients and having it be three of our members first hackathon, we managed to produce a project we are very proud of.\nWhat we learned\nWe learnt the importance of setting time goals, assigning roles and also how to be flexible. As we lived in different time zones, (Very drastic ones) we had to work around each other schedule. But we also learnt how much dedication pays off, even though we had progress checks at awkward timings (to say the least) we still managed to produce a (semi)-working website and hardware component.\nWhat's next for the Tree Huggers\nWe want to complete certain features that we didn't have time to fully implement like adding the email pushing to the frontend of our website. We also had plans to add a 'Find-Your-Plant feature' just in case your plant likes to go for walkies.", "link": "https://devpost.com/software/name-placeholder-fv3omr", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\nwe wanted to create something to spread awareness about climate change in our respective communities. having an international team from across the globe, with greenhouse gases hotspots such as london and chennai being our home cities, we decided on prototyping a handy device which helped nurture something cheap and practical like a plant, and turn it into a therapeutic mindfulness exercise that also helps to make the world a bit more green!\nwhat it does\n-----> tree !!!  surgeon allows you to monitor your plant on the go! sensor values are sent to our website, additionally a push notification can alert you from your phone.\nhow we built it\nan esp-32 development board was used to connect the sensor to wifi. we then create a website and used api to send push emails.\nchallenges we ran into\nduring coding, due to faults in the implementation of the esp-32 library to arduino, many hours were lost trying to locate missing library headers such as \"asynctcp.h\". certain libraries were catered specifically to esp8266, which also caused problems. some libraries also required dependencies that were hard to discover. moreover, we changed board from a mega 2560 to an esp-32 as the wi-fi and bluetooth capabilities proved superior.\naccomplishments that we're proud of\ndespite coming into the competition having never met before, never worked on sending data over the internet, never sent emails out to recipients and having it be three of our members first hackathon, we managed to produce a project we are very proud of.\nwhat we learned\nwe learnt the importance of setting time goals, assigning roles and also how to be flexible. as we lived in different time zones, (very drastic ones) we had to work around each other schedule. but we also learnt how much dedication pays off, even though we had progress checks at awkward timings (to say the least) we still managed to produce a (semi)-working website and hardware component.\nwhat's next for the tree huggers\nwe want to complete certain features that we didn't have time to fully implement like adding the email pushing to the frontend of our website. we also had plans to add a 'find-your-plant feature' just in case your plant likes to go for walkies.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59506793}, {"Unnamed: 0": 6817, "autor": "NFTrees", "date": null, "content": "Disclaimer\nwe wrote this on a windows machine. unfortunately to build the UI it has to go through prettier which disallows carriage returns. we weren\u2019t able to create the UI after trying to replace the carriage returns and disabling prettier, so the video only goes through the codebase. sorry about that.\nInspiration\nWe need to Plant more and more Trees to be planted and so to encourage people and make it exciting we decided to bring it over an NFTs Platform\nWhat it does\nThe app mints NFTs according to a serial number and sends it to the agoric wallet as an Amount. 7433 unique NFTrees are randomly generated and all are unique. Using the concept of \u201cNFT drops,\u201d users don\u2019t know the serial number or item until they mint. Each NFT belongs to a different collection (genus) of varying sizes, affecting their value. The idea is that for each tree purchased, it is also planted in real life.\nHow we built it\nWe build it using Agoric Blockchain. The wallet access and UI is based off of dapp-card-store, and the contract is based off of dapp-nft-drop.\nChallenges we ran into\nIt's a bit difficult to install Agoric Blockchain and as all of us were windows user we took time to that. Zoe Smart Contracts are very new to us and it was our first time to work with it so we got lot of bugs and still with lot of efforts we were not able to compile it successfully.\nAccomplishments that we're proud of\nWe at the end not only had Agoric smart contract on our machines but also were able to create an app on it.\nWhat we learned\nWe learn about decentralized application and also about the working of agoric blockchain working and zoe smart contracts.\nWhat's next for NFTrees\nWe right now have a very basic Front-end and so we have decided to improve that and wanted to shift the application in android to make it available to more people.", "link": "https://devpost.com/software/nft-trees", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "disclaimer\nwe wrote this on a windows machine. unfortunately to build the ui it has to go through prettier which disallows carriage returns. we weren\u2019t able to create the ui after trying to replace the carriage returns and disabling prettier, so the video only goes through the codebase. sorry about that.\ninspiration\nwe need to plant more and more trees to be planted and so to encourage people and make it exciting we decided to bring it over an nfts platform\nwhat it does\nthe app mints nfts according to a serial number and sends it to the agoric wallet as an amount. 7433 unique nftrees are randomly generated and all are unique. using the concept of \u201cnft drops,\u201d users don\u2019t know the serial number or item until they mint. each nft belongs to a different collection (genus) of varying sizes, affecting their value. the idea is that for each -----> tree !!!  purchased, it is also planted in real life.\nhow we built it\nwe build it using agoric blockchain. the wallet access and ui is based off of dapp-card-store, and the contract is based off of dapp-nft-drop.\nchallenges we ran into\nit's a bit difficult to install agoric blockchain and as all of us were windows user we took time to that. zoe smart contracts are very new to us and it was our first time to work with it so we got lot of bugs and still with lot of efforts we were not able to compile it successfully.\naccomplishments that we're proud of\nwe at the end not only had agoric smart contract on our machines but also were able to create an app on it.\nwhat we learned\nwe learn about decentralized application and also about the working of agoric blockchain working and zoe smart contracts.\nwhat's next for nftrees\nwe right now have a very basic front-end and so we have decided to improve that and wanted to shift the application in android to make it available to more people.", "sortedWord": "None", "removed": "Nan", "score": 2, "comments": 0, "media": null, "medialink": null, "identifyer": 59506817}, {"Unnamed: 0": 6829, "autor": "PLANTIFY", "date": null, "content": "Inspiration\nThere is a website called mama earth which supplies cosmetics products and whenever you buys a product they plant a tree and give you the location of that tree and information that what type of tree they planted and where they have planted.\nWhat it does\nThis project can help people and can contribute to nature at the same time. Sounds Interesting right let us now jump into the process of how this project works first there are two categories one is a planter and the other is donor so let us understand the role of planter here He can log in to our website and choose a role as a planter after that he has to plant a tree and provide us the description of that tree and make a video while planting the tree and submit it to the website after reviewing phase He is verified and can take the money. now where did the money come from Donors are the persons who donate money to planters for this good work for society\nHow we built it\nWe build the Frontend through HTML, CSS, JavaScript, React ,BOOTSTRAP API's used:- SWAO API we used for the authentication which is Passwordless authentications. GEOLOCATION API we used to get the location of the planter. REACT MEDIA RECORDER we used to record video and submit it to the website for review FIREBASE we used it to connect with the backend and store data in firebase.\nChallenges we ran into\nThere are a lot of challenges we faced during this project First to make the website Responsive we had to change a lot of changes and While using Geolocation Api we also got stuck it wasn't working and we lost most of our time during implementing Firebase Api but we can't resolve the issue. while publishing the site through the Go Daddy domain we also got stuck so in the end we had to publish it through the Versel platform.\nAccomplishments that we're proud of\nWe Got to achieve a lot in this Hackathon We made the full-fledged responsive website with complete Frontend and implemented successfully all APIs after a lot of hard work.\nWhat we learned\nThere are a lot of things that we learned in this Hackathon:-\nHow to make the website responsive\nHow to implement API's successfully\nyes, of course, We learned a lot about How Github works\nWE learned about how to host a website\nWhat's next for PLANTIFY\nWe have a lot of plans related to Plantify in the future like We have to implement the backend to store Data and successfully get the data of planters' video and location for verification purposes.\nWe can also provide some QR codes to people to stick to plants. *We want to automate the process of reviewing through Using AI and Machine learning", "link": "https://devpost.com/software/plantify-bz8aik", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\nthere is a website called mama earth which supplies cosmetics products and whenever you buys a product they plant a -----> tree !!!  and give you the location of that -----> tree !!!  and information that what type of -----> tree !!!  they planted and where they have planted.\nwhat it does\nthis project can help people and can contribute to nature at the same time. sounds interesting right let us now jump into the process of how this project works first there are two categories one is a planter and the other is donor so let us understand the role of planter here he can log in to our website and choose a role as a planter after that he has to plant a tree and provide us the description of that tree and make a video while planting the tree and submit it to the website after reviewing phase he is verified and can take the money. now where did the money come from donors are the persons who donate money to planters for this good work for society\nhow we built it\nwe build the frontend through html, css, javascript, react ,bootstrap api's used:- swao api we used for the authentication which is passwordless authentications. geolocation api we used to get the location of the planter. react media recorder we used to record video and submit it to the website for review firebase we used it to connect with the backend and store data in firebase.\nchallenges we ran into\nthere are a lot of challenges we faced during this project first to make the website responsive we had to change a lot of changes and while using geolocation api we also got stuck it wasn't working and we lost most of our time during implementing firebase api but we can't resolve the issue. while publishing the site through the go daddy domain we also got stuck so in the end we had to publish it through the versel platform.\naccomplishments that we're proud of\nwe got to achieve a lot in this hackathon we made the full-fledged responsive website with complete frontend and implemented successfully all apis after a lot of hard work.\nwhat we learned\nthere are a lot of things that we learned in this hackathon:-\nhow to make the website responsive\nhow to implement api's successfully\nyes, of course, we learned a lot about how github works\nwe learned about how to host a website\nwhat's next for plantify\nwe have a lot of plans related to plantify in the future like we have to implement the backend to store data and successfully get the data of planters' video and location for verification purposes.\nwe can also provide some qr codes to people to stick to plants. *we want to automate the process of reviewing through using ai and machine learning", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59506829}, {"Unnamed: 0": 6945, "autor": "PyTree", "date": null, "content": "Inspiration\nNatural language processing is an extremely prolific research field. The rise of deep learning witnessed the apparition of distinct architectures. The efficiency of such architecture is crucial. Slow or under-optimized implementations might constraint the parameter size or the research of hyperparameters. Therefore, the popularity of architectures is, to some extend, bounded by the efficiency of their implementation. For example, transformers mainly depend on attention and matrix multiplication which can be performed extremely fast on GPUs or TPUs. Nonetheless, other less efficient architectures might be worth exploring. The latter might be more intelligible or exhibit some specific properties.\nTree-structured networks are of special interest for NLP applications. Language is indeed often associated with a recursive structure. PyTorch is really convenient to implement recursive neural networks. Indeed, the computation graph is dynamically computed for each input. As trees might present distinct structures and shapes, this makes it easy to adapt to a variety of inputs. However, it is more difficult to compute a whole batch of distinct trees at one.\nWhat it does\nFor the hackathon, we implemented tree-structured neural networks in PyTorch. The package, called PyTree, provides highly generic recursive neural networks implementations as well as efficient batching methods. Recursive neural networks are notoriously hard to implement and deploy and although many custom implementations exist, they lack unity. The goal of our implementation is to be as most straightforward as possible: simplify the format of the inputs and outputs and align them with other popular architectures, such as Transformers or LSTM.\nHow we built it\nThe package is built in pure PyTorch and is designed to work with the entire PyTorch stack. Dataset, Dataloader, Datacollator, or nn.Module. As a result, it is compatible with every PyTorch-friendly project. For example, our demonstration uses a Trainer module from another Open Source library.\nChallenges we ran into\nWe aimed at implementing multiple models with each specific characteristics and architecture specificities. We had to redefine our intermediate variables or functions multiple times in order iteratively to improve the implementation power of generalization.\nAccomplishments that we're proud of\nWe were proud to be able to reproduce some paper results, which validated our implementation.\nWhat we learned\nWe learned a lot about PyTorch build-in functions and develop our intuition about dimensions in Tensors. Eventually, we also had to deep dive into our implementation to fix some bugs. From our point of view, debugging is one of the main strengths of PyTorch. You just have to use the standard Python debugger in the IDE of your choice and you can execute your code step by step to control every line of code. This feature makes it really convenient to debug.\nWhat's next for PyTree\nWe like to keep improving the library. In particular, we expect to add other recursive models such as other encoders or decoders. We also aim to extend the number of tutorials and examples, for NLP or other fields.", "link": "https://devpost.com/software/pytree", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\nnatural language processing is an extremely prolific research field. the rise of deep learning witnessed the apparition of distinct architectures. the efficiency of such architecture is crucial. slow or under-optimized implementations might constraint the parameter size or the research of hyperparameters. therefore, the popularity of architectures is, to some extend, bounded by the efficiency of their implementation. for example, transformers mainly depend on attention and matrix multiplication which can be performed extremely fast on gpus or tpus. nonetheless, other less efficient architectures might be worth exploring. the latter might be more intelligible or exhibit some specific properties.\n-----> tree !!! -structured networks are of special interest for nlp applications. language is indeed often associated with a recursive structure. pytorch is really convenient to implement recursive neural networks. indeed, the computation graph is dynamically computed for each input. as trees might present distinct structures and shapes, this makes it easy to adapt to a variety of inputs. however, it is more difficult to compute a whole batch of distinct trees at one.\nwhat it does\nfor the hackathon, we implemented tree-structured neural networks in pytorch. the package, called pytree, provides highly generic recursive neural networks implementations as well as efficient batching methods. recursive neural networks are notoriously hard to implement and deploy and although many custom implementations exist, they lack unity. the goal of our implementation is to be as most straightforward as possible: simplify the format of the inputs and outputs and align them with other popular architectures, such as transformers or lstm.\nhow we built it\nthe package is built in pure pytorch and is designed to work with the entire pytorch stack. dataset, dataloader, datacollator, or nn.module. as a result, it is compatible with every pytorch-friendly project. for example, our demonstration uses a trainer module from another open source library.\nchallenges we ran into\nwe aimed at implementing multiple models with each specific characteristics and architecture specificities. we had to redefine our intermediate variables or functions multiple times in order iteratively to improve the implementation power of generalization.\naccomplishments that we're proud of\nwe were proud to be able to reproduce some paper results, which validated our implementation.\nwhat we learned\nwe learned a lot about pytorch build-in functions and develop our intuition about dimensions in tensors. eventually, we also had to deep dive into our implementation to fix some bugs. from our point of view, debugging is one of the main strengths of pytorch. you just have to use the standard python debugger in the ide of your choice and you can execute your code step by step to control every line of code. this feature makes it really convenient to debug.\nwhat's next for pytree\nwe like to keep improving the library. in particular, we expect to add other recursive models such as other encoders or decoders. we also aim to extend the number of tutorials and examples, for nlp or other fields.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59506945}, {"Unnamed: 0": 7019, "autor": "Sustainable Street Light", "date": null, "content": "Inspiration\nInspired by the mechanism of the forest. As seeing many streetlights on the road, I am thinking can I develop the streetlights into more sustainable streetlights that act like a tree. It can generate electricity and help to reduce CO2 in the air.\nWhat it does\nTree-like Street Light is designed to reduce the risk of flooding and turn the excess water to generate electricity and reduce the possibility of water scarcity happens in the selected area in the future. It also has a water system that is built to manage the flow of excess water during a flood and prevent waste of water during the flood as the water collected will be reused after water is treated.\nTree-like Street light is inspired by the mechanism of the forest. When heavy rain, rainwater will be collected through the funnel-shaped street light and the drain channel under the street light. Some water will be absorbed by the tube of the street light that contains orbeez that use to provide water to the plant that is planted on the street light. After the water enters the concrete drain underneath the street light, rubbish, oil, grease and debris will be trapped in the catch basin that contains the drain insert. The water from a small sewage system nearby and catch basin will flow through the micro-hydro turbine to generate electricity. The funnel-shaped street light has a solar panel on it to generate electricity to light up the street at night. The water collected from the street light will later flow to the small sewage treatment system to be purified. When there is excess water during a flood, the water will flow to the river. however, when the water level of the river is high, there will be a water gate that will close due to water pressure and the excess water will flow to nearby lakes, ponds and another water treatment system. The purified water be used to provide continuous water to micro-hydro turbine to generate electricity and be drank by residents. Only clean recycled water will be supply to the resident.\nHow I built it\n-Have not to do a prototype but have done a draft in 3d form\nChallenges I ran into\n-The rubbish will affect the water flow to generate electricity. -Have not to do experiments to see its workability\nWhat I learned\n-Keep improving Tree-like Street Light to a more sustainable and affordable street light -Try to challenge more Sustainable Development Goal\nWhat's next for Tree-like Sustainable Street Light\n-Semi-permeable road pavement that can generate electricity through osmosis", "link": "https://devpost.com/software/sustainable-street-light", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\ninspired by the mechanism of the forest. as seeing many streetlights on the road, i am thinking can i develop the streetlights into more sustainable streetlights that act like a -----> tree !!! . it can generate electricity and help to reduce co2 in the air.\nwhat it does\ntree-like street light is designed to reduce the risk of flooding and turn the excess water to generate electricity and reduce the possibility of water scarcity happens in the selected area in the future. it also has a water system that is built to manage the flow of excess water during a flood and prevent waste of water during the flood as the water collected will be reused after water is treated.\ntree-like street light is inspired by the mechanism of the forest. when heavy rain, rainwater will be collected through the funnel-shaped street light and the drain channel under the street light. some water will be absorbed by the tube of the street light that contains orbeez that use to provide water to the plant that is planted on the street light. after the water enters the concrete drain underneath the street light, rubbish, oil, grease and debris will be trapped in the catch basin that contains the drain insert. the water from a small sewage system nearby and catch basin will flow through the micro-hydro turbine to generate electricity. the funnel-shaped street light has a solar panel on it to generate electricity to light up the street at night. the water collected from the street light will later flow to the small sewage treatment system to be purified. when there is excess water during a flood, the water will flow to the river. however, when the water level of the river is high, there will be a water gate that will close due to water pressure and the excess water will flow to nearby lakes, ponds and another water treatment system. the purified water be used to provide continuous water to micro-hydro turbine to generate electricity and be drank by residents. only clean recycled water will be supply to the resident.\nhow i built it\n-have not to do a prototype but have done a draft in 3d form\nchallenges i ran into\n-the rubbish will affect the water flow to generate electricity. -have not to do experiments to see its workability\nwhat i learned\n-keep improving tree-like street light to a more sustainable and affordable street light -try to challenge more sustainable development goal\nwhat's next for tree-like sustainable street light\n-semi-permeable road pavement that can generate electricity through osmosis", "sortedWord": "None", "removed": "Nan", "score": 1, "comments": 0, "media": null, "medialink": null, "identifyer": 59507019}, {"Unnamed: 0": 7055, "autor": "\u0e27\u0e31\u0e22\u0e23\u0e38\u0e48\u0e19\u0e22\u0e38\u0e04\u0e43\u0e2b\u0e21\u0e48\u0e43\u0e2a\u0e48\u0e43\u0e08\u0e1b\u0e31\u0e0d\u0e0d\u0e32\u0e1b\u0e23\u0e30\u0e14\u0e34\u0e29\u0e10\u0e4c", "date": null, "content": "Inspiration\n\u0e2a\u0e21\u0e32\u0e0a\u0e34\u0e01\u0e43\u0e19\u0e01\u0e25\u0e38\u0e48\u0e21\u0e41\u0e15\u0e48\u0e25\u0e30\u0e04\u0e19\u0e44\u0e14\u0e49\u0e0a\u0e48\u0e27\u0e22\u0e01\u0e31\u0e19\u0e28\u0e36\u0e01\u0e29\u0e32\u0e0a\u0e19\u0e34\u0e14\u0e41\u0e25\u0e30\u0e1b\u0e23\u0e30\u0e2a\u0e34\u0e17\u0e18\u0e34\u0e20\u0e32\u0e1e\u0e02\u0e2d\u0e07 machine learning model \u0e41\u0e25\u0e30 features representation \u0e43\u0e19\u0e23\u0e39\u0e1b\u0e41\u0e1a\u0e1a\u0e15\u0e48\u0e32\u0e07 \u0e46 \u0e08\u0e32\u0e01\u0e07\u0e32\u0e19\u0e27\u0e34\u0e08\u0e31\u0e22\u0e17\u0e35\u0e48\u0e40\u0e01\u0e35\u0e48\u0e22\u0e27\u0e02\u0e49\u0e2d\u0e07 \u0e41\u0e25\u0e30\u0e19\u0e33\u0e21\u0e32\u0e1b\u0e23\u0e30\u0e22\u0e38\u0e01\u0e15\u0e4c\u0e43\u0e19\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e43\u0e0a\u0e49\u0e01\u0e31\u0e1a representation \u0e15\u0e48\u0e32\u0e07 \u0e46 \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e2b\u0e32 machine learning model \u0e41\u0e25\u0e30 features representation \u0e17\u0e35\u0e48\u0e21\u0e35\u0e04\u0e48\u0e32 LMAE \u0e08\u0e32\u0e01\u0e01\u0e32\u0e23\u0e17\u0e33\u0e19\u0e32\u0e22 working capacity \u0e02\u0e2d\u0e07 MOF \u0e41\u0e15\u0e48\u0e25\u0e30\u0e15\u0e31\u0e27\u0e19\u0e49\u0e2d\u0e22\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14\nWhat we do\n\u0e01\u0e25\u0e38\u0e48\u0e21\u0e02\u0e2d\u0e07\u0e40\u0e23\u0e32\u0e1b\u0e23\u0e30\u0e22\u0e38\u0e01\u0e15\u0e4c algorithm \u0e17\u0e35\u0e48\u0e21\u0e35\u0e2d\u0e22\u0e39\u0e48\u0e01\u0e48\u0e2d\u0e19\u0e2b\u0e19\u0e49\u0e32\u0e41\u0e25\u0e30\u0e2a\u0e23\u0e49\u0e32\u0e07 Representation \u0e02\u0e2d\u0e07\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e14\u0e49\u0e27\u0e22\u0e40\u0e17\u0e04\u0e19\u0e34\u0e04\u0e15\u0e48\u0e32\u0e07 \u0e46 \u0e44\u0e14\u0e49\u0e41\u0e01\u0e48\npreprocessing \u0e42\u0e14\u0e22\u0e01\u0e33\u0e2b\u0e19\u0e14\u0e04\u0e48\u0e32\u0e15\u0e31\u0e27\u0e40\u0e25\u0e02\u0e43\u0e2b\u0e49 metal group, metal element, functional group \u0e41\u0e25\u0e30 organic linkers \u0e02\u0e2d\u0e07 MOF \u0e41\u0e15\u0e48\u0e25\u0e30\u0e15\u0e31\u0e27\nbinary matrix \u0e02\u0e2d\u0e07 functional group\nword embedding \u0e02\u0e2d\u0e07 functional group\nPCA \u0e02\u0e2d\u0e07 linear AP-RDF \u0e41\u0e25\u0e30 PCA \u0e02\u0e2d\u0e07 constant AP-RDF\nPCA \u0e02\u0e2d\u0e07 coordinates\nPCA \u0e02\u0e2d\u0e07 Bag of Atoms\n\u0e42\u0e14\u0e22\u0e40\u0e25\u0e37\u0e2d\u0e01 features \u0e17\u0e35\u0e48\u0e19\u0e48\u0e32\u0e08\u0e30\u0e21\u0e35\u0e1c\u0e25\u0e15\u0e48\u0e2d working capacity \u0e21\u0e32\u0e01\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14\u0e21\u0e32\u0e2a\u0e23\u0e49\u0e32\u0e07 representation \u0e2a\u0e38\u0e14\u0e17\u0e49\u0e32\u0e22\nMachine Learning model \u0e43\u0e19\u0e23\u0e39\u0e1b\u0e41\u0e1a\u0e1a\u0e15\u0e48\u0e32\u0e07 \u0e46 \u0e44\u0e14\u0e49\u0e41\u0e01\u0e48\nCatBoostRegressor\nXGBRegressor\nLightGBMRegressor\nRandomForestRegressor\nExtraTreesRegressor\nGradientBoostingRegressor\nAdaBoostingRegressor\nNeural Networks\n\u0e23\u0e27\u0e21\u0e44\u0e1b\u0e16\u0e36\u0e07\u0e01\u0e32\u0e23\u0e1b\u0e23\u0e30\u0e22\u0e38\u0e01\u0e15\u0e4c\u0e40\u0e17\u0e04\u0e19\u0e34\u0e04 Ensemble Method \u0e43\u0e19\u0e01\u0e32\u0e23\u0e23\u0e27\u0e21\u0e2b\u0e25\u0e32\u0e22 model \u0e21\u0e32\u0e43\u0e0a\u0e49\u0e43\u0e19\u0e01\u0e32\u0e23\u0e17\u0e33\u0e19\u0e32\u0e22 working capacity \u0e02\u0e2d\u0e07 MOF \u0e41\u0e15\u0e48\u0e25\u0e30\u0e15\u0e31\u0e27 \u0e42\u0e14\u0e22\u0e42\u0e21\u0e40\u0e14\u0e25\u0e41\u0e15\u0e48\u0e25\u0e30\u0e0a\u0e19\u0e34\u0e14\u0e01\u0e25\u0e38\u0e48\u0e21\u0e02\u0e2d\u0e07\u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e1b\u0e23\u0e31\u0e1a parameter \u0e02\u0e2d\u0e07\u0e41\u0e15\u0e48\u0e25\u0e30\u0e42\u0e21\u0e40\u0e14\u0e25\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e1b\u0e23\u0e30\u0e22\u0e38\u0e01\u0e15\u0e4c\u0e40\u0e17\u0e04\u0e19\u0e34\u0e04 GridSearch \u0e43\u0e19\u0e01\u0e32\u0e23\u0e2b\u0e32 parameter \u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e1c\u0e25\u0e14\u0e35\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14 \u0e19\u0e2d\u0e01\u0e08\u0e32\u0e01\u0e19\u0e35\u0e49 \u0e01\u0e25\u0e38\u0e48\u0e21\u0e02\u0e2d\u0e07\u0e40\u0e23\u0e32\u0e43\u0e0a\u0e49 Optuna \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e1b\u0e23\u0e31\u0e1a\u0e04\u0e48\u0e32 hyperparameter \u0e17\u0e35\u0e48\u0e14\u0e35\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14\u0e43\u0e19\u0e01\u0e32\u0e23\u0e17\u0e33\u0e19\u0e32\u0e22\u0e1c\u0e25\u0e40\u0e1e\u0e34\u0e48\u0e21\u0e40\u0e15\u0e34\u0e21\u0e2d\u0e35\u0e01\u0e14\u0e49\u0e27\u0e22\n\u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e19\u0e33\u0e42\u0e21\u0e40\u0e14\u0e25\u0e15\u0e48\u0e32\u0e07 \u0e46 \u0e21\u0e32\u0e2b\u0e32\u0e04\u0e48\u0e32\u0e04\u0e27\u0e32\u0e21\u0e04\u0e25\u0e32\u0e14\u0e40\u0e04\u0e25\u0e37\u0e48\u0e2d\u0e19 (LMAE) \u0e08\u0e32\u0e01\u0e01\u0e32\u0e23\u0e17\u0e33\u0e19\u0e32\u0e22 working capacity \u0e02\u0e2d\u0e07 MOF \u0e41\u0e15\u0e48\u0e25\u0e30\u0e15\u0e31\u0e27 \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49 representation \u0e02\u0e2d\u0e07 features \u0e17\u0e35\u0e48\u0e15\u0e48\u0e32\u0e07\u0e01\u0e31\u0e19\n\u0e17\u0e31\u0e49\u0e07\u0e19\u0e35\u0e49 \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e01\u0e32\u0e23\u0e17\u0e14\u0e25\u0e2d\u0e07\u0e19\u0e33 representation \u0e15\u0e48\u0e32\u0e07 \u0e46 \u0e21\u0e32\u0e08\u0e31\u0e1a\u0e04\u0e39\u0e48\u0e01\u0e31\u0e1a\u0e42\u0e21\u0e40\u0e14\u0e25\u0e15\u0e48\u0e32\u0e07 \u0e46 \u0e40\u0e1b\u0e47\u0e19\u0e44\u0e1b\u0e44\u0e14\u0e49\u0e07\u0e48\u0e32\u0e22\u0e02\u0e36\u0e49\u0e19 \u0e40\u0e23\u0e32\u0e17\u0e33\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e42\u0e1b\u0e23\u0e41\u0e01\u0e23\u0e21\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e01\u0e32\u0e23\u0e17\u0e14\u0e25\u0e2d\u0e07\u0e02\u0e36\u0e49\u0e19 (\u0e14\u0e39\u0e17\u0e35\u0e48\u0e25\u0e34\u0e07\u0e01\u0e4c\u0e19\u0e35\u0e49) \u0e0b\u0e36\u0e48\u0e07\u0e17\u0e38\u0e01\u0e04\u0e19\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e40\u0e02\u0e49\u0e32\u0e44\u0e1b\u0e17\u0e14\u0e25\u0e2d\u0e07\u0e01\u0e31\u0e1a representation \u0e2b\u0e23\u0e37\u0e2d\u0e42\u0e21\u0e40\u0e14\u0e25\u0e02\u0e2d\u0e07\u0e15\u0e31\u0e27\u0e40\u0e2d\u0e07\u0e44\u0e14\u0e49\u0e14\u0e49\u0e27\u0e22\u0e40\u0e0a\u0e48\u0e19\u0e01\u0e31\u0e19 \u0e15\u0e32\u0e21\u0e27\u0e34\u0e18\u0e35\u0e17\u0e35\u0e48\u0e2d\u0e18\u0e34\u0e1a\u0e32\u0e22\u0e44\u0e27\u0e49\u0e2d\u0e22\u0e39\u0e48\u0e43\u0e19\u0e2b\u0e19\u0e49\u0e32\u0e40\u0e27\u0e47\u0e1a\u0e02\u0e2d\u0e07 repository \u0e0b\u0e36\u0e48\u0e07\u0e2b\u0e27\u0e31\u0e07\u0e27\u0e48\u0e32\u0e40\u0e04\u0e23\u0e37\u0e48\u0e2d\u0e07\u0e21\u0e37\u0e2d\u0e19\u0e35\u0e49\u0e08\u0e30\u0e17\u0e33\u0e43\u0e2b\u0e49\u0e01\u0e32\u0e23\u0e04\u0e49\u0e19\u0e04\u0e27\u0e49\u0e32\u0e2b\u0e32 representation \u0e41\u0e25\u0e30\u0e42\u0e21\u0e40\u0e14\u0e25\u0e15\u0e48\u0e32\u0e07 \u0e46 \u0e21\u0e35\u0e1b\u0e23\u0e30\u0e2a\u0e34\u0e17\u0e18\u0e34\u0e20\u0e32\u0e1e\u0e22\u0e34\u0e48\u0e07\u0e02\u0e36\u0e49\u0e19\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e17\u0e38\u0e01 \u0e46 \u0e04\u0e19\u0e43\u0e19\u0e2d\u0e19\u0e32\u0e04\u0e15\n\u0e15\u0e31\u0e27\u0e2d\u0e22\u0e48\u0e32\u0e07\u0e01\u0e32\u0e23\u0e23\u0e31\u0e19\u0e42\u0e04\u0e49\u0e14\u0e17\u0e35\u0e48\u0e17\u0e33\u0e01\u0e32\u0e23 train XGBRegressor \u0e41\u0e1a\u0e1a\u0e43\u0e0a\u0e49 GridSearchCV \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49 representation \u0e41\u0e1a\u0e1a preprocessed \u0e01\u0e31\u0e1a linear AP-RDF \u0e23\u0e48\u0e27\u0e21\u0e01\u0e31\u0e19 (\u0e2b\u0e32\u0e01\u0e15\u0e49\u0e2d\u0e07\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49\u0e21\u0e32\u0e01\u0e01\u0e27\u0e48\u0e32\u0e19\u0e35\u0e49 \u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e40\u0e1e\u0e34\u0e48\u0e21 argument \u0e15\u0e48\u0e2d\u0e08\u0e32\u0e01 linearAP-RDF \u0e44\u0e14\u0e49\u0e40\u0e25\u0e22) \u0e0b\u0e36\u0e48\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25\u0e17\u0e35\u0e48 train \u0e40\u0e23\u0e35\u0e22\u0e1a\u0e23\u0e49\u0e2d\u0e22\u0e41\u0e25\u0e49\u0e27\u0e08\u0e30\u0e16\u0e39\u0e01\u0e1a\u0e31\u0e19\u0e17\u0e36\u0e01\u0e44\u0e27\u0e49\u0e43\u0e19\u0e42\u0e1f\u0e25\u0e40\u0e14\u0e2d\u0e23\u0e4c results \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e43\u0e0a\u0e49\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e17\u0e33\u0e19\u0e32\u0e22 test set \u0e15\u0e48\u0e2d\u0e44\u0e1b\npython main.py --reps preprocessed linearAP-RDF --model xgb --directory results --grid_search\nHow we built\n\u0e17\u0e38\u0e01\u0e42\u0e1b\u0e23\u0e41\u0e01\u0e23\u0e21\u0e17\u0e35\u0e48\u0e43\u0e0a\u0e49\u0e2a\u0e23\u0e49\u0e32\u0e07 representation \u0e41\u0e25\u0e30 machine learning model \u0e16\u0e39\u0e01\u0e40\u0e02\u0e35\u0e22\u0e19\u0e14\u0e49\u0e27\u0e22\u0e20\u0e32\u0e29\u0e32 Python \u0e42\u0e14\u0e22\u0e19\u0e33 library \u0e2b\u0e25\u0e32\u0e22\u0e0a\u0e19\u0e34\u0e14\u0e21\u0e32\u0e1b\u0e23\u0e30\u0e22\u0e38\u0e01\u0e15\u0e4c\u0e43\u0e0a\u0e49 \u0e40\u0e0a\u0e48\u0e19\nnumpy: \u0e43\u0e0a\u0e49\u0e08\u0e31\u0e14\u0e01\u0e32\u0e23\u0e01\u0e32\u0e23\u0e04\u0e33\u0e19\u0e27\u0e13\u0e15\u0e31\u0e27\u0e40\u0e25\u0e02\u0e41\u0e25\u0e30 matrix transformation\npandas: \u0e43\u0e0a\u0e49\u0e08\u0e31\u0e14\u0e01\u0e32\u0e23 dataframe\noctadist: \u0e43\u0e0a\u0e49\u0e28\u0e36\u0e01\u0e29\u0e32\u0e42\u0e04\u0e23\u0e07\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e02\u0e2d\u0e07\u0e2a\u0e32\u0e23\u0e1b\u0e23\u0e30\u0e01\u0e2d\u0e1a\u0e42\u0e25\u0e2b\u0e30\u0e17\u0e23\u0e07\u0e41\u0e1b\u0e14\u0e2b\u0e19\u0e49\u0e32 (octahedron)\nopenbabel: \u0e43\u0e0a\u0e49\u0e41\u0e1b\u0e25\u0e07 .cif \u0e21\u0e32\u0e40\u0e1b\u0e47\u0e19 smile\nsklearn: \u0e43\u0e0a\u0e49\u0e08\u0e31\u0e14\u0e01\u0e32\u0e23 machine learning model\ntensorflow: \u0e43\u0e0a\u0e49\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e42\u0e21\u0e40\u0e14\u0e25 neural network\nxgboost, lightgbm, catboost, nn: machine learning model \u0e17\u0e35\u0e48\u0e43\u0e0a\u0e49\u0e17\u0e33\u0e19\u0e32\u0e22 working capacity \u0e02\u0e2d\u0e07 CO2\naltair: \u0e43\u0e0a\u0e49\u0e2a\u0e23\u0e49\u0e32\u0e07 heatmap \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e19\u0e33\u0e40\u0e2a\u0e19\u0e2d\u0e04\u0e48\u0e32\u0e04\u0e27\u0e32\u0e21\u0e04\u0e32\u0e14\u0e40\u0e04\u0e25\u0e37\u0e48\u0e2d\u0e19 LMAE \u0e02\u0e2d\u0e07\u0e01\u0e32\u0e23\u0e17\u0e33\u0e19\u0e32\u0e22 Working Capacity \u0e02\u0e2d\u0e07 MOF\ngensim: \u0e43\u0e0a\u0e49\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e40\u0e27\u0e01\u0e40\u0e15\u0e2d\u0e23\u0e4c\u0e02\u0e2d\u0e07\u0e04\u0e33\u0e43\u0e19\u0e01\u0e32\u0e23 represent feature \u0e41\u0e1a\u0e1a word embedding\nResults and Conclusion\n\u0e08\u0e32\u0e01\u0e01\u0e32\u0e23\u0e17\u0e14\u0e25\u0e2d\u0e07 \u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e1e\u0e1a\u0e27\u0e48\u0e32 \u0e0a\u0e19\u0e34\u0e14\u0e02\u0e2d\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25 Machine Learning \u0e17\u0e35\u0e48\u0e43\u0e0a\u0e49\u0e41\u0e25\u0e30\u0e23\u0e39\u0e1b\u0e41\u0e1a\u0e1a\u0e01\u0e32\u0e23\u0e19\u0e33\u0e40\u0e2a\u0e19\u0e2d\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25 (representation) \u0e17\u0e35\u0e48\u0e15\u0e48\u0e32\u0e07\u0e01\u0e31\u0e19\u0e2d\u0e2d\u0e01\u0e44\u0e1b\u0e21\u0e35\u0e1c\u0e25\u0e01\u0e31\u0e1a\u0e04\u0e48\u0e32\u0e04\u0e27\u0e32\u0e21\u0e04\u0e25\u0e32\u0e14\u0e40\u0e04\u0e25\u0e37\u0e48\u0e2d\u0e19 (LMAE) \u0e02\u0e2d\u0e07\u0e01\u0e32\u0e23\u0e17\u0e33\u0e19\u0e32\u0e22 Working Capacity \u0e02\u0e2d\u0e07 MOF \u0e15\u0e48\u0e32\u0e07 \u0e46 \u0e2d\u0e22\u0e48\u0e32\u0e07\u0e40\u0e2b\u0e47\u0e19\u0e44\u0e14\u0e49\u0e0a\u0e31\u0e14 \u0e42\u0e14\u0e22\u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e19\u0e33\u0e40\u0e2a\u0e19\u0e2d\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25 LMAE \u0e02\u0e2d\u0e07\u0e41\u0e15\u0e48\u0e25\u0e30\u0e42\u0e21\u0e40\u0e14\u0e25\u0e41\u0e25\u0e30 representation \u0e17\u0e35\u0e48\u0e15\u0e48\u0e32\u0e07\u0e01\u0e31\u0e19\u0e2d\u0e2d\u0e01\u0e44\u0e1b\u0e43\u0e19\u0e23\u0e39\u0e1b\u0e41\u0e1a\u0e1a\u0e02\u0e2d\u0e07 Heatmap \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49 Altair python library \u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e02\u0e49\u0e2d\u0e2a\u0e23\u0e38\u0e1b\u0e15\u0e48\u0e32\u0e07 \u0e46 \u0e14\u0e31\u0e07\u0e19\u0e35\u0e49\n\u0e42\u0e21\u0e40\u0e14\u0e25\u0e08\u0e33\u0e1e\u0e27\u0e01 Decision Tree \u0e40\u0e0a\u0e48\u0e19 CatBoostRegressor, XGBRegressor, \u0e41\u0e25\u0e30 LightGBMRegressor \u0e40\u0e1b\u0e47\u0e19\u0e42\u0e21\u0e40\u0e14\u0e25\u0e17\u0e35\u0e48\u0e43\u0e0a\u0e49\u0e44\u0e14\u0e49\u0e14\u0e35\u0e01\u0e31\u0e1a\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e15\u0e48\u0e32\u0e07 \u0e46 \u0e17\u0e35\u0e48\u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32\u0e21\u0e35\u0e2d\u0e22\u0e39\u0e48\nRepresentation \u0e17\u0e35\u0e48\u0e21\u0e35\u0e0a\u0e37\u0e48\u0e2d\u0e27\u0e48\u0e32 preprocessed \u0e41\u0e25\u0e30 preprocessed2 \u0e40\u0e1b\u0e47\u0e19 representation \u0e17\u0e35\u0e48\u0e19\u0e33\u0e04\u0e48\u0e32\u0e15\u0e48\u0e32\u0e07 \u0e46 \u0e08\u0e32\u0e01\u0e44\u0e1f\u0e25\u0e4c .csv \u0e21\u0e32\u0e43\u0e0a\u0e49\u0e42\u0e14\u0e22\u0e15\u0e23\u0e07 \u0e08\u0e30\u0e40\u0e2b\u0e47\u0e19\u0e44\u0e14\u0e49\u0e27\u0e48\u0e32\u0e04\u0e48\u0e32 \u200b\u200b\u200b\u200b\u200b\u200bLMAE \u0e02\u0e2d\u0e07 representation \u0e2a\u0e2d\u0e07\u0e15\u0e31\u0e27\u0e19\u0e35\u0e49\u0e2b\u0e23\u0e37\u0e2d\u0e01\u0e32\u0e23\u0e23\u0e27\u0e21\u0e01\u0e31\u0e19\u0e17\u0e35\u0e48\u0e21\u0e35 representation \u0e2d\u0e31\u0e19\u0e43\u0e14\u0e2d\u0e31\u0e19\u0e2b\u0e19\u0e36\u0e48\u0e07\u0e43\u0e19\u0e19\u0e35\u0e49\u0e08\u0e30\u0e43\u0e2b\u0e49\u0e04\u0e48\u0e32 LMAE \u0e17\u0e35\u0e48\u0e19\u0e49\u0e2d\u0e22\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14 \u0e42\u0e14\u0e22\u0e08\u0e30\u0e2d\u0e22\u0e39\u0e48\u0e43\u0e19\u0e0a\u0e48\u0e27\u0e07 1.23 - 1.29\n\u0e19\u0e2d\u0e01\u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e1e\u0e31\u0e12\u0e19\u0e32\u0e04\u0e27\u0e32\u0e21\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e02\u0e2d\u0e07 ML \u0e2a\u0e32\u0e21\u0e15\u0e31\u0e27\u0e14\u0e31\u0e07\u0e01\u0e25\u0e48\u0e32\u0e27 \u0e40\u0e23\u0e32\u0e43\u0e0a\u0e49 Optuna \u0e0b\u0e36\u0e48\u0e07\u0e40\u0e1b\u0e47\u0e19 library \u0e17\u0e35\u0e48\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e1a\u0e19 python \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e1b\u0e23\u0e31\u0e1a hyperparameter \u0e43\u0e19\u0e41\u0e15\u0e48\u0e25\u0e30\u0e42\u0e21\u0e40\u0e14\u0e25 \u0e40\u0e23\u0e32\u0e04\u0e49\u0e19\u0e1e\u0e1a\u0e27\u0e48\u0e32\u0e04\u0e48\u0e32\u0e04\u0e27\u0e32\u0e21\u0e04\u0e25\u0e32\u0e14\u0e40\u0e04\u0e25\u0e37\u0e48\u0e2d\u0e19 (\u200bLMAE) \u0e25\u0e14\u0e25\u0e07\u0e2d\u0e22\u0e48\u0e32\u0e07\u0e21\u0e35\u0e19\u0e31\u0e22\u0e2a\u0e33\u0e04\u0e31\u0e0d \u0e40\u0e21\u0e37\u0e48\u0e2d\u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e17\u0e14\u0e2a\u0e2d\u0e1a\u0e01\u0e31\u0e1a\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e17\u0e35\u0e48\u0e21\u0e35 preprocessed \u0e41\u0e25\u0e30 preprocessed2 \u0e04\u0e48\u0e32 LMAE \u0e08\u0e30\u0e25\u0e14\u0e25\u0e07\u0e2d\u0e22\u0e39\u0e48\u0e43\u0e19\u0e0a\u0e48\u0e27\u0e07 1.229 - 1.240\n\u0e2a\u0e38\u0e14\u0e17\u0e49\u0e32\u0e22\u0e19\u0e35\u0e49 \u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e17\u0e14\u0e25\u0e2d\u0e07\u0e43\u0e0a\u0e49 voting ensemble \u0e43\u0e19\u0e01\u0e32\u0e23\u0e40\u0e09\u0e25\u0e35\u0e48\u0e22\u0e1c\u0e25\u0e25\u0e31\u0e1e\u0e18\u0e4c\u0e08\u0e32\u0e01\u0e01\u0e32\u0e23\u0e17\u0e33\u0e19\u0e32\u0e22 working capacity \u0e02\u0e2d\u0e07 MOF \u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e08\u0e32\u0e01\u0e42\u0e21\u0e40\u0e14\u0e25 3 \u0e15\u0e31\u0e27\u0e17\u0e35\u0e48\u0e14\u0e35\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14 \u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e1e\u0e1a\u0e27\u0e48\u0e32\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49 ensemble \u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e25\u0e14\u0e01\u0e32\u0e23 overfitting \u0e02\u0e2d\u0e07\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e44\u0e14\u0e49\u0e41\u0e25\u0e30\u0e43\u0e2b\u0e49\u0e04\u0e48\u0e32 LMAE \u0e2d\u0e22\u0e39\u0e48\u0e17\u0e35\u0e48 1.222 \u0e0b\u0e36\u0e48\u0e07\u0e40\u0e1b\u0e47\u0e19\u0e1c\u0e25\u0e25\u0e31\u0e1e\u0e18\u0e4c\u0e17\u0e35\u0e48\u0e14\u0e35\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14\u0e02\u0e2d\u0e07\u0e17\u0e35\u0e21\nChallenges we ran into\n\u0e40\u0e19\u0e37\u0e48\u0e2d\u0e07\u0e08\u0e32\u0e01\u0e2a\u0e21\u0e32\u0e0a\u0e34\u0e01\u0e43\u0e19\u0e17\u0e35\u0e21\u0e41\u0e15\u0e48\u0e25\u0e30\u0e04\u0e19\u0e21\u0e35\u0e1e\u0e37\u0e49\u0e19\u0e10\u0e32\u0e19\u0e17\u0e32\u0e07\u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49\u0e17\u0e35\u0e48\u0e41\u0e15\u0e01\u0e15\u0e48\u0e32\u0e07\u0e01\u0e31\u0e19\u0e21\u0e32\u0e01 \u0e40\u0e23\u0e32\u0e41\u0e01\u0e49\u0e44\u0e02\u0e1b\u0e31\u0e0d\u0e2b\u0e32\u0e19\u0e35\u0e49\u0e42\u0e14\u0e22\u0e01\u0e32\u0e23\u0e2d\u0e18\u0e34\u0e1a\u0e32\u0e22\u0e2b\u0e25\u0e31\u0e01\u0e01\u0e32\u0e23\u0e17\u0e32\u0e07\u0e40\u0e04\u0e21\u0e35 \u0e41\u0e25\u0e01\u0e40\u0e1b\u0e25\u0e35\u0e48\u0e22\u0e19\u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49 \u0e2a\u0e2d\u0e19\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19\u0e42\u0e1b\u0e23\u0e41\u0e01\u0e23\u0e21\u0e40\u0e1a\u0e37\u0e49\u0e2d\u0e07\u0e15\u0e49\u0e19 \u0e25\u0e14\u0e04\u0e27\u0e32\u0e21\u0e0b\u0e31\u0e1a\u0e0b\u0e49\u0e2d\u0e19\u0e02\u0e2d\u0e07\u0e42\u0e1b\u0e23\u0e41\u0e01\u0e23\u0e21 \u0e41\u0e25\u0e30 \u0e21\u0e35\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e38\u0e1b\u0e04\u0e27\u0e32\u0e21\u0e04\u0e37\u0e1a\u0e2b\u0e19\u0e49\u0e32\u0e04\u0e23\u0e48\u0e32\u0e27 \u0e46 \u0e43\u0e19\u0e17\u0e38\u0e01\u0e04\u0e23\u0e31\u0e49\u0e07\u0e02\u0e2d\u0e07\u0e01\u0e32\u0e23\u0e1b\u0e23\u0e30\u0e0a\u0e38\u0e21 \u0e19\u0e2d\u0e01\u0e08\u0e32\u0e01\u0e19\u0e35\u0e49\u0e0a\u0e48\u0e27\u0e07\u0e40\u0e27\u0e25\u0e32\u0e17\u0e35\u0e48\u0e2a\u0e30\u0e14\u0e27\u0e01\u0e02\u0e2d\u0e07\u0e2a\u0e21\u0e32\u0e0a\u0e34\u0e01\u0e43\u0e19\u0e17\u0e35\u0e21\u0e44\u0e21\u0e48\u0e15\u0e23\u0e07\u0e01\u0e31\u0e19\u0e40\u0e19\u0e37\u0e48\u0e2d\u0e07\u0e08\u0e32\u0e01\u0e01\u0e32\u0e23\u0e2a\u0e2d\u0e1a\u0e43\u0e19\u0e21\u0e2b\u0e32\u0e27\u0e34\u0e17\u0e22\u0e32\u0e25\u0e31\u0e22\u0e41\u0e25\u0e30 timezone \u0e02\u0e2d\u0e07\u0e41\u0e15\u0e48\u0e25\u0e30\u0e04\u0e19 \u0e40\u0e19\u0e37\u0e48\u0e2d\u0e07\u0e08\u0e32\u0e01\u0e01\u0e32\u0e23\u0e40\u0e02\u0e49\u0e32\u0e43\u0e08\u0e2d\u0e38\u0e1b\u0e2a\u0e23\u0e23\u0e04\u0e02\u0e2d\u0e07\u0e2a\u0e21\u0e32\u0e0a\u0e34\u0e01\u0e41\u0e15\u0e48\u0e25\u0e30\u0e04\u0e19\u0e1e\u0e23\u0e49\u0e2d\u0e21\u0e01\u0e31\u0e1a\u0e04\u0e27\u0e32\u0e21\u0e15\u0e31\u0e49\u0e07\u0e43\u0e08\u0e43\u0e19\u0e01\u0e32\u0e23\u0e41\u0e02\u0e48\u0e07\u0e02\u0e31\u0e19\u0e04\u0e23\u0e31\u0e49\u0e07\u0e19\u0e35\u0e49 \u0e17\u0e35\u0e21\u0e02\u0e2d\u0e07\u0e40\u0e23\u0e32\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e01\u0e49\u0e32\u0e27\u0e02\u0e49\u0e32\u0e21\u0e41\u0e25\u0e30\u0e1b\u0e23\u0e30\u0e2a\u0e32\u0e19\u0e07\u0e32\u0e19\u0e44\u0e14\u0e49\u0e23\u0e32\u0e1a\u0e23\u0e37\u0e48\u0e19\nAccomplishments that our group proud of\n\u0e08\u0e32\u0e01\u0e01\u0e32\u0e23\u0e41\u0e25\u0e01\u0e40\u0e1b\u0e25\u0e35\u0e48\u0e22\u0e19\u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49\u0e41\u0e25\u0e30\u0e01\u0e32\u0e23\u0e44\u0e14\u0e49\u0e1d\u0e36\u0e01\u0e1d\u0e19\u0e01\u0e32\u0e23\u0e40\u0e02\u0e35\u0e22\u0e19\u0e42\u0e1b\u0e23\u0e41\u0e01\u0e23\u0e21 \u0e2a\u0e21\u0e32\u0e0a\u0e34\u0e01\u0e43\u0e19\u0e17\u0e35\u0e21\u0e44\u0e14\u0e49\u0e23\u0e31\u0e1a\u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49\u0e41\u0e25\u0e30\u0e1b\u0e23\u0e30\u0e2a\u0e1a\u0e01\u0e32\u0e23\u0e13\u0e4c\u0e08\u0e32\u0e01\u0e01\u0e32\u0e23\u0e41\u0e02\u0e48\u0e07\u0e02\u0e31\u0e19\u0e04\u0e23\u0e31\u0e49\u0e07\u0e19\u0e35\u0e49\u0e0b\u0e36\u0e48\u0e07\u0e08\u0e30\u0e0a\u0e48\u0e27\u0e22\u0e43\u0e2b\u0e49\u0e01\u0e32\u0e23\u0e17\u0e33\u0e07\u0e32\u0e19\u0e17\u0e32\u0e07\u0e14\u0e49\u0e32\u0e19 coding \u0e41\u0e25\u0e30 representation \u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e43\u0e19\u0e2d\u0e19\u0e32\u0e04\u0e15\u0e07\u0e48\u0e32\u0e22\u0e21\u0e32\u0e01\u0e02\u0e36\u0e49\u0e19 \u0e23\u0e27\u0e21\u0e16\u0e36\u0e07\u0e01\u0e32\u0e23\u0e1b\u0e23\u0e30\u0e22\u0e38\u0e01\u0e15\u0e4c\u0e43\u0e0a\u0e49\u0e42\u0e1b\u0e23\u0e41\u0e01\u0e23\u0e21 ML \u0e01\u0e31\u0e1a\u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49\u0e41\u0e02\u0e19\u0e07\u0e2d\u0e37\u0e48\u0e19 \u0e46\nWhat we have learnt\n\u0e1b\u0e23\u0e30\u0e2a\u0e1a\u0e01\u0e32\u0e23\u0e13\u0e4c\u0e40\u0e01\u0e35\u0e48\u0e22\u0e27\u0e01\u0e31\u0e1a\u0e01\u0e32\u0e23\u0e19\u0e33 ML \u0e21\u0e32\u0e1b\u0e23\u0e30\u0e22\u0e38\u0e01\u0e15\u0e4c\u0e43\u0e0a\u0e49\u0e43\u0e19\u0e42\u0e08\u0e17\u0e22\u0e4c\u0e40\u0e04\u0e21\u0e35\n\u0e01\u0e32\u0e23\u0e19\u0e33\u0e40\u0e2a\u0e19\u0e2d\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e17\u0e32\u0e07\u0e40\u0e04\u0e21\u0e35\u0e43\u0e19\u0e23\u0e39\u0e1b\u0e41\u0e1a\u0e1a\u0e17\u0e35\u0e48\u0e04\u0e2d\u0e21\u0e1e\u0e34\u0e27\u0e40\u0e15\u0e2d\u0e23\u0e4c\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e40\u0e02\u0e49\u0e32\u0e43\u0e08\u0e41\u0e25\u0e30\u0e19\u0e33\u0e44\u0e1b\u0e43\u0e0a\u0e49\u0e15\u0e48\u0e2d\u0e44\u0e14\u0e49\n\u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49\u0e40\u0e01\u0e35\u0e48\u0e22\u0e27\u0e01\u0e31\u0e1a Metal Organic Framework\nDeepnote \u0e01\u0e31\u0e1a tensorflow \u0e21\u0e35 negative synergy \u0e01\u0e31\u0e19 \u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19\u0e2d\u0e32\u0e08\u0e22\u0e31\u0e07\u0e44\u0e21\u0e48\u0e40\u0e2a\u0e16\u0e35\u0e22\u0e23\u0e21\u0e32\u0e01\u0e19\u0e31\u0e01", "link": "https://devpost.com/software/project-6mw3fgs9j7uo", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\n\u0e2a\u0e21\u0e32\u0e0a\u0e34\u0e01\u0e43\u0e19\u0e01\u0e25\u0e38\u0e48\u0e21\u0e41\u0e15\u0e48\u0e25\u0e30\u0e04\u0e19\u0e44\u0e14\u0e49\u0e0a\u0e48\u0e27\u0e22\u0e01\u0e31\u0e19\u0e28\u0e36\u0e01\u0e29\u0e32\u0e0a\u0e19\u0e34\u0e14\u0e41\u0e25\u0e30\u0e1b\u0e23\u0e30\u0e2a\u0e34\u0e17\u0e18\u0e34\u0e20\u0e32\u0e1e\u0e02\u0e2d\u0e07 machine learning model \u0e41\u0e25\u0e30 features representation \u0e43\u0e19\u0e23\u0e39\u0e1b\u0e41\u0e1a\u0e1a\u0e15\u0e48\u0e32\u0e07 \u0e46 \u0e08\u0e32\u0e01\u0e07\u0e32\u0e19\u0e27\u0e34\u0e08\u0e31\u0e22\u0e17\u0e35\u0e48\u0e40\u0e01\u0e35\u0e48\u0e22\u0e27\u0e02\u0e49\u0e2d\u0e07 \u0e41\u0e25\u0e30\u0e19\u0e33\u0e21\u0e32\u0e1b\u0e23\u0e30\u0e22\u0e38\u0e01\u0e15\u0e4c\u0e43\u0e19\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e43\u0e0a\u0e49\u0e01\u0e31\u0e1a representation \u0e15\u0e48\u0e32\u0e07 \u0e46 \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e2b\u0e32 machine learning model \u0e41\u0e25\u0e30 features representation \u0e17\u0e35\u0e48\u0e21\u0e35\u0e04\u0e48\u0e32 lmae \u0e08\u0e32\u0e01\u0e01\u0e32\u0e23\u0e17\u0e33\u0e19\u0e32\u0e22 working capacity \u0e02\u0e2d\u0e07 mof \u0e41\u0e15\u0e48\u0e25\u0e30\u0e15\u0e31\u0e27\u0e19\u0e49\u0e2d\u0e22\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14\nwhat we do\n\u0e01\u0e25\u0e38\u0e48\u0e21\u0e02\u0e2d\u0e07\u0e40\u0e23\u0e32\u0e1b\u0e23\u0e30\u0e22\u0e38\u0e01\u0e15\u0e4c algorithm \u0e17\u0e35\u0e48\u0e21\u0e35\u0e2d\u0e22\u0e39\u0e48\u0e01\u0e48\u0e2d\u0e19\u0e2b\u0e19\u0e49\u0e32\u0e41\u0e25\u0e30\u0e2a\u0e23\u0e49\u0e32\u0e07 representation \u0e02\u0e2d\u0e07\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e14\u0e49\u0e27\u0e22\u0e40\u0e17\u0e04\u0e19\u0e34\u0e04\u0e15\u0e48\u0e32\u0e07 \u0e46 \u0e44\u0e14\u0e49\u0e41\u0e01\u0e48\npreprocessing \u0e42\u0e14\u0e22\u0e01\u0e33\u0e2b\u0e19\u0e14\u0e04\u0e48\u0e32\u0e15\u0e31\u0e27\u0e40\u0e25\u0e02\u0e43\u0e2b\u0e49 metal group, metal element, functional group \u0e41\u0e25\u0e30 organic linkers \u0e02\u0e2d\u0e07 mof \u0e41\u0e15\u0e48\u0e25\u0e30\u0e15\u0e31\u0e27\nbinary matrix \u0e02\u0e2d\u0e07 functional group\nword embedding \u0e02\u0e2d\u0e07 functional group\npca \u0e02\u0e2d\u0e07 linear ap-rdf \u0e41\u0e25\u0e30 pca \u0e02\u0e2d\u0e07 constant ap-rdf\npca \u0e02\u0e2d\u0e07 coordinates\npca \u0e02\u0e2d\u0e07 bag of atoms\n\u0e42\u0e14\u0e22\u0e40\u0e25\u0e37\u0e2d\u0e01 features \u0e17\u0e35\u0e48\u0e19\u0e48\u0e32\u0e08\u0e30\u0e21\u0e35\u0e1c\u0e25\u0e15\u0e48\u0e2d working capacity \u0e21\u0e32\u0e01\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14\u0e21\u0e32\u0e2a\u0e23\u0e49\u0e32\u0e07 representation \u0e2a\u0e38\u0e14\u0e17\u0e49\u0e32\u0e22\nmachine learning model \u0e43\u0e19\u0e23\u0e39\u0e1b\u0e41\u0e1a\u0e1a\u0e15\u0e48\u0e32\u0e07 \u0e46 \u0e44\u0e14\u0e49\u0e41\u0e01\u0e48\ncatboostregressor\nxgbregressor\nlightgbmregressor\nrandomforestregressor\nextratreesregressor\ngradientboostingregressor\nadaboostingregressor\nneural networks\n\u0e23\u0e27\u0e21\u0e44\u0e1b\u0e16\u0e36\u0e07\u0e01\u0e32\u0e23\u0e1b\u0e23\u0e30\u0e22\u0e38\u0e01\u0e15\u0e4c\u0e40\u0e17\u0e04\u0e19\u0e34\u0e04 ensemble method \u0e43\u0e19\u0e01\u0e32\u0e23\u0e23\u0e27\u0e21\u0e2b\u0e25\u0e32\u0e22 model \u0e21\u0e32\u0e43\u0e0a\u0e49\u0e43\u0e19\u0e01\u0e32\u0e23\u0e17\u0e33\u0e19\u0e32\u0e22 working capacity \u0e02\u0e2d\u0e07 mof \u0e41\u0e15\u0e48\u0e25\u0e30\u0e15\u0e31\u0e27 \u0e42\u0e14\u0e22\u0e42\u0e21\u0e40\u0e14\u0e25\u0e41\u0e15\u0e48\u0e25\u0e30\u0e0a\u0e19\u0e34\u0e14\u0e01\u0e25\u0e38\u0e48\u0e21\u0e02\u0e2d\u0e07\u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e1b\u0e23\u0e31\u0e1a parameter \u0e02\u0e2d\u0e07\u0e41\u0e15\u0e48\u0e25\u0e30\u0e42\u0e21\u0e40\u0e14\u0e25\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e1b\u0e23\u0e30\u0e22\u0e38\u0e01\u0e15\u0e4c\u0e40\u0e17\u0e04\u0e19\u0e34\u0e04 gridsearch \u0e43\u0e19\u0e01\u0e32\u0e23\u0e2b\u0e32 parameter \u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e1c\u0e25\u0e14\u0e35\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14 \u0e19\u0e2d\u0e01\u0e08\u0e32\u0e01\u0e19\u0e35\u0e49 \u0e01\u0e25\u0e38\u0e48\u0e21\u0e02\u0e2d\u0e07\u0e40\u0e23\u0e32\u0e43\u0e0a\u0e49 optuna \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e1b\u0e23\u0e31\u0e1a\u0e04\u0e48\u0e32 hyperparameter \u0e17\u0e35\u0e48\u0e14\u0e35\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14\u0e43\u0e19\u0e01\u0e32\u0e23\u0e17\u0e33\u0e19\u0e32\u0e22\u0e1c\u0e25\u0e40\u0e1e\u0e34\u0e48\u0e21\u0e40\u0e15\u0e34\u0e21\u0e2d\u0e35\u0e01\u0e14\u0e49\u0e27\u0e22\n\u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e19\u0e33\u0e42\u0e21\u0e40\u0e14\u0e25\u0e15\u0e48\u0e32\u0e07 \u0e46 \u0e21\u0e32\u0e2b\u0e32\u0e04\u0e48\u0e32\u0e04\u0e27\u0e32\u0e21\u0e04\u0e25\u0e32\u0e14\u0e40\u0e04\u0e25\u0e37\u0e48\u0e2d\u0e19 (lmae) \u0e08\u0e32\u0e01\u0e01\u0e32\u0e23\u0e17\u0e33\u0e19\u0e32\u0e22 working capacity \u0e02\u0e2d\u0e07 mof \u0e41\u0e15\u0e48\u0e25\u0e30\u0e15\u0e31\u0e27 \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49 representation \u0e02\u0e2d\u0e07 features \u0e17\u0e35\u0e48\u0e15\u0e48\u0e32\u0e07\u0e01\u0e31\u0e19\n\u0e17\u0e31\u0e49\u0e07\u0e19\u0e35\u0e49 \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e01\u0e32\u0e23\u0e17\u0e14\u0e25\u0e2d\u0e07\u0e19\u0e33 representation \u0e15\u0e48\u0e32\u0e07 \u0e46 \u0e21\u0e32\u0e08\u0e31\u0e1a\u0e04\u0e39\u0e48\u0e01\u0e31\u0e1a\u0e42\u0e21\u0e40\u0e14\u0e25\u0e15\u0e48\u0e32\u0e07 \u0e46 \u0e40\u0e1b\u0e47\u0e19\u0e44\u0e1b\u0e44\u0e14\u0e49\u0e07\u0e48\u0e32\u0e22\u0e02\u0e36\u0e49\u0e19 \u0e40\u0e23\u0e32\u0e17\u0e33\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e42\u0e1b\u0e23\u0e41\u0e01\u0e23\u0e21\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e01\u0e32\u0e23\u0e17\u0e14\u0e25\u0e2d\u0e07\u0e02\u0e36\u0e49\u0e19 (\u0e14\u0e39\u0e17\u0e35\u0e48\u0e25\u0e34\u0e07\u0e01\u0e4c\u0e19\u0e35\u0e49) \u0e0b\u0e36\u0e48\u0e07\u0e17\u0e38\u0e01\u0e04\u0e19\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e40\u0e02\u0e49\u0e32\u0e44\u0e1b\u0e17\u0e14\u0e25\u0e2d\u0e07\u0e01\u0e31\u0e1a representation \u0e2b\u0e23\u0e37\u0e2d\u0e42\u0e21\u0e40\u0e14\u0e25\u0e02\u0e2d\u0e07\u0e15\u0e31\u0e27\u0e40\u0e2d\u0e07\u0e44\u0e14\u0e49\u0e14\u0e49\u0e27\u0e22\u0e40\u0e0a\u0e48\u0e19\u0e01\u0e31\u0e19 \u0e15\u0e32\u0e21\u0e27\u0e34\u0e18\u0e35\u0e17\u0e35\u0e48\u0e2d\u0e18\u0e34\u0e1a\u0e32\u0e22\u0e44\u0e27\u0e49\u0e2d\u0e22\u0e39\u0e48\u0e43\u0e19\u0e2b\u0e19\u0e49\u0e32\u0e40\u0e27\u0e47\u0e1a\u0e02\u0e2d\u0e07 repository \u0e0b\u0e36\u0e48\u0e07\u0e2b\u0e27\u0e31\u0e07\u0e27\u0e48\u0e32\u0e40\u0e04\u0e23\u0e37\u0e48\u0e2d\u0e07\u0e21\u0e37\u0e2d\u0e19\u0e35\u0e49\u0e08\u0e30\u0e17\u0e33\u0e43\u0e2b\u0e49\u0e01\u0e32\u0e23\u0e04\u0e49\u0e19\u0e04\u0e27\u0e49\u0e32\u0e2b\u0e32 representation \u0e41\u0e25\u0e30\u0e42\u0e21\u0e40\u0e14\u0e25\u0e15\u0e48\u0e32\u0e07 \u0e46 \u0e21\u0e35\u0e1b\u0e23\u0e30\u0e2a\u0e34\u0e17\u0e18\u0e34\u0e20\u0e32\u0e1e\u0e22\u0e34\u0e48\u0e07\u0e02\u0e36\u0e49\u0e19\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e17\u0e38\u0e01 \u0e46 \u0e04\u0e19\u0e43\u0e19\u0e2d\u0e19\u0e32\u0e04\u0e15\n\u0e15\u0e31\u0e27\u0e2d\u0e22\u0e48\u0e32\u0e07\u0e01\u0e32\u0e23\u0e23\u0e31\u0e19\u0e42\u0e04\u0e49\u0e14\u0e17\u0e35\u0e48\u0e17\u0e33\u0e01\u0e32\u0e23 train xgbregressor \u0e41\u0e1a\u0e1a\u0e43\u0e0a\u0e49 gridsearchcv \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49 representation \u0e41\u0e1a\u0e1a preprocessed \u0e01\u0e31\u0e1a linear ap-rdf \u0e23\u0e48\u0e27\u0e21\u0e01\u0e31\u0e19 (\u0e2b\u0e32\u0e01\u0e15\u0e49\u0e2d\u0e07\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49\u0e21\u0e32\u0e01\u0e01\u0e27\u0e48\u0e32\u0e19\u0e35\u0e49 \u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e40\u0e1e\u0e34\u0e48\u0e21 argument \u0e15\u0e48\u0e2d\u0e08\u0e32\u0e01 linearap-rdf \u0e44\u0e14\u0e49\u0e40\u0e25\u0e22) \u0e0b\u0e36\u0e48\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25\u0e17\u0e35\u0e48 train \u0e40\u0e23\u0e35\u0e22\u0e1a\u0e23\u0e49\u0e2d\u0e22\u0e41\u0e25\u0e49\u0e27\u0e08\u0e30\u0e16\u0e39\u0e01\u0e1a\u0e31\u0e19\u0e17\u0e36\u0e01\u0e44\u0e27\u0e49\u0e43\u0e19\u0e42\u0e1f\u0e25\u0e40\u0e14\u0e2d\u0e23\u0e4c results \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e43\u0e0a\u0e49\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e17\u0e33\u0e19\u0e32\u0e22 test set \u0e15\u0e48\u0e2d\u0e44\u0e1b\npython main.py --reps preprocessed linearap-rdf --model xgb --directory results --grid_search\nhow we built\n\u0e17\u0e38\u0e01\u0e42\u0e1b\u0e23\u0e41\u0e01\u0e23\u0e21\u0e17\u0e35\u0e48\u0e43\u0e0a\u0e49\u0e2a\u0e23\u0e49\u0e32\u0e07 representation \u0e41\u0e25\u0e30 machine learning model \u0e16\u0e39\u0e01\u0e40\u0e02\u0e35\u0e22\u0e19\u0e14\u0e49\u0e27\u0e22\u0e20\u0e32\u0e29\u0e32 python \u0e42\u0e14\u0e22\u0e19\u0e33 library \u0e2b\u0e25\u0e32\u0e22\u0e0a\u0e19\u0e34\u0e14\u0e21\u0e32\u0e1b\u0e23\u0e30\u0e22\u0e38\u0e01\u0e15\u0e4c\u0e43\u0e0a\u0e49 \u0e40\u0e0a\u0e48\u0e19\nnumpy: \u0e43\u0e0a\u0e49\u0e08\u0e31\u0e14\u0e01\u0e32\u0e23\u0e01\u0e32\u0e23\u0e04\u0e33\u0e19\u0e27\u0e13\u0e15\u0e31\u0e27\u0e40\u0e25\u0e02\u0e41\u0e25\u0e30 matrix transformation\npandas: \u0e43\u0e0a\u0e49\u0e08\u0e31\u0e14\u0e01\u0e32\u0e23 dataframe\noctadist: \u0e43\u0e0a\u0e49\u0e28\u0e36\u0e01\u0e29\u0e32\u0e42\u0e04\u0e23\u0e07\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e02\u0e2d\u0e07\u0e2a\u0e32\u0e23\u0e1b\u0e23\u0e30\u0e01\u0e2d\u0e1a\u0e42\u0e25\u0e2b\u0e30\u0e17\u0e23\u0e07\u0e41\u0e1b\u0e14\u0e2b\u0e19\u0e49\u0e32 (octahedron)\nopenbabel: \u0e43\u0e0a\u0e49\u0e41\u0e1b\u0e25\u0e07 .cif \u0e21\u0e32\u0e40\u0e1b\u0e47\u0e19 smile\nsklearn: \u0e43\u0e0a\u0e49\u0e08\u0e31\u0e14\u0e01\u0e32\u0e23 machine learning model\ntensorflow: \u0e43\u0e0a\u0e49\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e42\u0e21\u0e40\u0e14\u0e25 neural network\nxgboost, lightgbm, catboost, nn: machine learning model \u0e17\u0e35\u0e48\u0e43\u0e0a\u0e49\u0e17\u0e33\u0e19\u0e32\u0e22 working capacity \u0e02\u0e2d\u0e07 co2\naltair: \u0e43\u0e0a\u0e49\u0e2a\u0e23\u0e49\u0e32\u0e07 heatmap \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e19\u0e33\u0e40\u0e2a\u0e19\u0e2d\u0e04\u0e48\u0e32\u0e04\u0e27\u0e32\u0e21\u0e04\u0e32\u0e14\u0e40\u0e04\u0e25\u0e37\u0e48\u0e2d\u0e19 lmae \u0e02\u0e2d\u0e07\u0e01\u0e32\u0e23\u0e17\u0e33\u0e19\u0e32\u0e22 working capacity \u0e02\u0e2d\u0e07 mof\ngensim: \u0e43\u0e0a\u0e49\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e40\u0e27\u0e01\u0e40\u0e15\u0e2d\u0e23\u0e4c\u0e02\u0e2d\u0e07\u0e04\u0e33\u0e43\u0e19\u0e01\u0e32\u0e23 represent feature \u0e41\u0e1a\u0e1a word embedding\nresults and conclusion\n\u0e08\u0e32\u0e01\u0e01\u0e32\u0e23\u0e17\u0e14\u0e25\u0e2d\u0e07 \u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e1e\u0e1a\u0e27\u0e48\u0e32 \u0e0a\u0e19\u0e34\u0e14\u0e02\u0e2d\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25 machine learning \u0e17\u0e35\u0e48\u0e43\u0e0a\u0e49\u0e41\u0e25\u0e30\u0e23\u0e39\u0e1b\u0e41\u0e1a\u0e1a\u0e01\u0e32\u0e23\u0e19\u0e33\u0e40\u0e2a\u0e19\u0e2d\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25 (representation) \u0e17\u0e35\u0e48\u0e15\u0e48\u0e32\u0e07\u0e01\u0e31\u0e19\u0e2d\u0e2d\u0e01\u0e44\u0e1b\u0e21\u0e35\u0e1c\u0e25\u0e01\u0e31\u0e1a\u0e04\u0e48\u0e32\u0e04\u0e27\u0e32\u0e21\u0e04\u0e25\u0e32\u0e14\u0e40\u0e04\u0e25\u0e37\u0e48\u0e2d\u0e19 (lmae) \u0e02\u0e2d\u0e07\u0e01\u0e32\u0e23\u0e17\u0e33\u0e19\u0e32\u0e22 working capacity \u0e02\u0e2d\u0e07 mof \u0e15\u0e48\u0e32\u0e07 \u0e46 \u0e2d\u0e22\u0e48\u0e32\u0e07\u0e40\u0e2b\u0e47\u0e19\u0e44\u0e14\u0e49\u0e0a\u0e31\u0e14 \u0e42\u0e14\u0e22\u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e19\u0e33\u0e40\u0e2a\u0e19\u0e2d\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25 lmae \u0e02\u0e2d\u0e07\u0e41\u0e15\u0e48\u0e25\u0e30\u0e42\u0e21\u0e40\u0e14\u0e25\u0e41\u0e25\u0e30 representation \u0e17\u0e35\u0e48\u0e15\u0e48\u0e32\u0e07\u0e01\u0e31\u0e19\u0e2d\u0e2d\u0e01\u0e44\u0e1b\u0e43\u0e19\u0e23\u0e39\u0e1b\u0e41\u0e1a\u0e1a\u0e02\u0e2d\u0e07 heatmap \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49 altair python library \u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e02\u0e49\u0e2d\u0e2a\u0e23\u0e38\u0e1b\u0e15\u0e48\u0e32\u0e07 \u0e46 \u0e14\u0e31\u0e07\u0e19\u0e35\u0e49\n\u0e42\u0e21\u0e40\u0e14\u0e25\u0e08\u0e33\u0e1e\u0e27\u0e01 decision -----> tree !!!  \u0e40\u0e0a\u0e48\u0e19 catboostregressor, xgbregressor, \u0e41\u0e25\u0e30 lightgbmregressor \u0e40\u0e1b\u0e47\u0e19\u0e42\u0e21\u0e40\u0e14\u0e25\u0e17\u0e35\u0e48\u0e43\u0e0a\u0e49\u0e44\u0e14\u0e49\u0e14\u0e35\u0e01\u0e31\u0e1a\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e15\u0e48\u0e32\u0e07 \u0e46 \u0e17\u0e35\u0e48\u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32\u0e21\u0e35\u0e2d\u0e22\u0e39\u0e48\nrepresentation \u0e17\u0e35\u0e48\u0e21\u0e35\u0e0a\u0e37\u0e48\u0e2d\u0e27\u0e48\u0e32 preprocessed \u0e41\u0e25\u0e30 preprocessed2 \u0e40\u0e1b\u0e47\u0e19 representation \u0e17\u0e35\u0e48\u0e19\u0e33\u0e04\u0e48\u0e32\u0e15\u0e48\u0e32\u0e07 \u0e46 \u0e08\u0e32\u0e01\u0e44\u0e1f\u0e25\u0e4c .csv \u0e21\u0e32\u0e43\u0e0a\u0e49\u0e42\u0e14\u0e22\u0e15\u0e23\u0e07 \u0e08\u0e30\u0e40\u0e2b\u0e47\u0e19\u0e44\u0e14\u0e49\u0e27\u0e48\u0e32\u0e04\u0e48\u0e32 \u200b\u200b\u200b\u200b\u200b\u200blmae \u0e02\u0e2d\u0e07 representation \u0e2a\u0e2d\u0e07\u0e15\u0e31\u0e27\u0e19\u0e35\u0e49\u0e2b\u0e23\u0e37\u0e2d\u0e01\u0e32\u0e23\u0e23\u0e27\u0e21\u0e01\u0e31\u0e19\u0e17\u0e35\u0e48\u0e21\u0e35 representation \u0e2d\u0e31\u0e19\u0e43\u0e14\u0e2d\u0e31\u0e19\u0e2b\u0e19\u0e36\u0e48\u0e07\u0e43\u0e19\u0e19\u0e35\u0e49\u0e08\u0e30\u0e43\u0e2b\u0e49\u0e04\u0e48\u0e32 lmae \u0e17\u0e35\u0e48\u0e19\u0e49\u0e2d\u0e22\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14 \u0e42\u0e14\u0e22\u0e08\u0e30\u0e2d\u0e22\u0e39\u0e48\u0e43\u0e19\u0e0a\u0e48\u0e27\u0e07 1.23 - 1.29\n\u0e19\u0e2d\u0e01\u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e1e\u0e31\u0e12\u0e19\u0e32\u0e04\u0e27\u0e32\u0e21\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e02\u0e2d\u0e07 ml \u0e2a\u0e32\u0e21\u0e15\u0e31\u0e27\u0e14\u0e31\u0e07\u0e01\u0e25\u0e48\u0e32\u0e27 \u0e40\u0e23\u0e32\u0e43\u0e0a\u0e49 optuna \u0e0b\u0e36\u0e48\u0e07\u0e40\u0e1b\u0e47\u0e19 library \u0e17\u0e35\u0e48\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e1a\u0e19 python \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e1b\u0e23\u0e31\u0e1a hyperparameter \u0e43\u0e19\u0e41\u0e15\u0e48\u0e25\u0e30\u0e42\u0e21\u0e40\u0e14\u0e25 \u0e40\u0e23\u0e32\u0e04\u0e49\u0e19\u0e1e\u0e1a\u0e27\u0e48\u0e32\u0e04\u0e48\u0e32\u0e04\u0e27\u0e32\u0e21\u0e04\u0e25\u0e32\u0e14\u0e40\u0e04\u0e25\u0e37\u0e48\u0e2d\u0e19 (\u200blmae) \u0e25\u0e14\u0e25\u0e07\u0e2d\u0e22\u0e48\u0e32\u0e07\u0e21\u0e35\u0e19\u0e31\u0e22\u0e2a\u0e33\u0e04\u0e31\u0e0d \u0e40\u0e21\u0e37\u0e48\u0e2d\u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e17\u0e14\u0e2a\u0e2d\u0e1a\u0e01\u0e31\u0e1a\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e17\u0e35\u0e48\u0e21\u0e35 preprocessed \u0e41\u0e25\u0e30 preprocessed2 \u0e04\u0e48\u0e32 lmae \u0e08\u0e30\u0e25\u0e14\u0e25\u0e07\u0e2d\u0e22\u0e39\u0e48\u0e43\u0e19\u0e0a\u0e48\u0e27\u0e07 1.229 - 1.240\n\u0e2a\u0e38\u0e14\u0e17\u0e49\u0e32\u0e22\u0e19\u0e35\u0e49 \u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e17\u0e14\u0e25\u0e2d\u0e07\u0e43\u0e0a\u0e49 voting ensemble \u0e43\u0e19\u0e01\u0e32\u0e23\u0e40\u0e09\u0e25\u0e35\u0e48\u0e22\u0e1c\u0e25\u0e25\u0e31\u0e1e\u0e18\u0e4c\u0e08\u0e32\u0e01\u0e01\u0e32\u0e23\u0e17\u0e33\u0e19\u0e32\u0e22 working capacity \u0e02\u0e2d\u0e07 mof \u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e08\u0e32\u0e01\u0e42\u0e21\u0e40\u0e14\u0e25 3 \u0e15\u0e31\u0e27\u0e17\u0e35\u0e48\u0e14\u0e35\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14 \u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e1e\u0e1a\u0e27\u0e48\u0e32\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49 ensemble \u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e25\u0e14\u0e01\u0e32\u0e23 overfitting \u0e02\u0e2d\u0e07\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e44\u0e14\u0e49\u0e41\u0e25\u0e30\u0e43\u0e2b\u0e49\u0e04\u0e48\u0e32 lmae \u0e2d\u0e22\u0e39\u0e48\u0e17\u0e35\u0e48 1.222 \u0e0b\u0e36\u0e48\u0e07\u0e40\u0e1b\u0e47\u0e19\u0e1c\u0e25\u0e25\u0e31\u0e1e\u0e18\u0e4c\u0e17\u0e35\u0e48\u0e14\u0e35\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14\u0e02\u0e2d\u0e07\u0e17\u0e35\u0e21\nchallenges we ran into\n\u0e40\u0e19\u0e37\u0e48\u0e2d\u0e07\u0e08\u0e32\u0e01\u0e2a\u0e21\u0e32\u0e0a\u0e34\u0e01\u0e43\u0e19\u0e17\u0e35\u0e21\u0e41\u0e15\u0e48\u0e25\u0e30\u0e04\u0e19\u0e21\u0e35\u0e1e\u0e37\u0e49\u0e19\u0e10\u0e32\u0e19\u0e17\u0e32\u0e07\u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49\u0e17\u0e35\u0e48\u0e41\u0e15\u0e01\u0e15\u0e48\u0e32\u0e07\u0e01\u0e31\u0e19\u0e21\u0e32\u0e01 \u0e40\u0e23\u0e32\u0e41\u0e01\u0e49\u0e44\u0e02\u0e1b\u0e31\u0e0d\u0e2b\u0e32\u0e19\u0e35\u0e49\u0e42\u0e14\u0e22\u0e01\u0e32\u0e23\u0e2d\u0e18\u0e34\u0e1a\u0e32\u0e22\u0e2b\u0e25\u0e31\u0e01\u0e01\u0e32\u0e23\u0e17\u0e32\u0e07\u0e40\u0e04\u0e21\u0e35 \u0e41\u0e25\u0e01\u0e40\u0e1b\u0e25\u0e35\u0e48\u0e22\u0e19\u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49 \u0e2a\u0e2d\u0e19\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19\u0e42\u0e1b\u0e23\u0e41\u0e01\u0e23\u0e21\u0e40\u0e1a\u0e37\u0e49\u0e2d\u0e07\u0e15\u0e49\u0e19 \u0e25\u0e14\u0e04\u0e27\u0e32\u0e21\u0e0b\u0e31\u0e1a\u0e0b\u0e49\u0e2d\u0e19\u0e02\u0e2d\u0e07\u0e42\u0e1b\u0e23\u0e41\u0e01\u0e23\u0e21 \u0e41\u0e25\u0e30 \u0e21\u0e35\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e38\u0e1b\u0e04\u0e27\u0e32\u0e21\u0e04\u0e37\u0e1a\u0e2b\u0e19\u0e49\u0e32\u0e04\u0e23\u0e48\u0e32\u0e27 \u0e46 \u0e43\u0e19\u0e17\u0e38\u0e01\u0e04\u0e23\u0e31\u0e49\u0e07\u0e02\u0e2d\u0e07\u0e01\u0e32\u0e23\u0e1b\u0e23\u0e30\u0e0a\u0e38\u0e21 \u0e19\u0e2d\u0e01\u0e08\u0e32\u0e01\u0e19\u0e35\u0e49\u0e0a\u0e48\u0e27\u0e07\u0e40\u0e27\u0e25\u0e32\u0e17\u0e35\u0e48\u0e2a\u0e30\u0e14\u0e27\u0e01\u0e02\u0e2d\u0e07\u0e2a\u0e21\u0e32\u0e0a\u0e34\u0e01\u0e43\u0e19\u0e17\u0e35\u0e21\u0e44\u0e21\u0e48\u0e15\u0e23\u0e07\u0e01\u0e31\u0e19\u0e40\u0e19\u0e37\u0e48\u0e2d\u0e07\u0e08\u0e32\u0e01\u0e01\u0e32\u0e23\u0e2a\u0e2d\u0e1a\u0e43\u0e19\u0e21\u0e2b\u0e32\u0e27\u0e34\u0e17\u0e22\u0e32\u0e25\u0e31\u0e22\u0e41\u0e25\u0e30 timezone \u0e02\u0e2d\u0e07\u0e41\u0e15\u0e48\u0e25\u0e30\u0e04\u0e19 \u0e40\u0e19\u0e37\u0e48\u0e2d\u0e07\u0e08\u0e32\u0e01\u0e01\u0e32\u0e23\u0e40\u0e02\u0e49\u0e32\u0e43\u0e08\u0e2d\u0e38\u0e1b\u0e2a\u0e23\u0e23\u0e04\u0e02\u0e2d\u0e07\u0e2a\u0e21\u0e32\u0e0a\u0e34\u0e01\u0e41\u0e15\u0e48\u0e25\u0e30\u0e04\u0e19\u0e1e\u0e23\u0e49\u0e2d\u0e21\u0e01\u0e31\u0e1a\u0e04\u0e27\u0e32\u0e21\u0e15\u0e31\u0e49\u0e07\u0e43\u0e08\u0e43\u0e19\u0e01\u0e32\u0e23\u0e41\u0e02\u0e48\u0e07\u0e02\u0e31\u0e19\u0e04\u0e23\u0e31\u0e49\u0e07\u0e19\u0e35\u0e49 \u0e17\u0e35\u0e21\u0e02\u0e2d\u0e07\u0e40\u0e23\u0e32\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e01\u0e49\u0e32\u0e27\u0e02\u0e49\u0e32\u0e21\u0e41\u0e25\u0e30\u0e1b\u0e23\u0e30\u0e2a\u0e32\u0e19\u0e07\u0e32\u0e19\u0e44\u0e14\u0e49\u0e23\u0e32\u0e1a\u0e23\u0e37\u0e48\u0e19\naccomplishments that our group proud of\n\u0e08\u0e32\u0e01\u0e01\u0e32\u0e23\u0e41\u0e25\u0e01\u0e40\u0e1b\u0e25\u0e35\u0e48\u0e22\u0e19\u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49\u0e41\u0e25\u0e30\u0e01\u0e32\u0e23\u0e44\u0e14\u0e49\u0e1d\u0e36\u0e01\u0e1d\u0e19\u0e01\u0e32\u0e23\u0e40\u0e02\u0e35\u0e22\u0e19\u0e42\u0e1b\u0e23\u0e41\u0e01\u0e23\u0e21 \u0e2a\u0e21\u0e32\u0e0a\u0e34\u0e01\u0e43\u0e19\u0e17\u0e35\u0e21\u0e44\u0e14\u0e49\u0e23\u0e31\u0e1a\u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49\u0e41\u0e25\u0e30\u0e1b\u0e23\u0e30\u0e2a\u0e1a\u0e01\u0e32\u0e23\u0e13\u0e4c\u0e08\u0e32\u0e01\u0e01\u0e32\u0e23\u0e41\u0e02\u0e48\u0e07\u0e02\u0e31\u0e19\u0e04\u0e23\u0e31\u0e49\u0e07\u0e19\u0e35\u0e49\u0e0b\u0e36\u0e48\u0e07\u0e08\u0e30\u0e0a\u0e48\u0e27\u0e22\u0e43\u0e2b\u0e49\u0e01\u0e32\u0e23\u0e17\u0e33\u0e07\u0e32\u0e19\u0e17\u0e32\u0e07\u0e14\u0e49\u0e32\u0e19 coding \u0e41\u0e25\u0e30 representation \u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e43\u0e19\u0e2d\u0e19\u0e32\u0e04\u0e15\u0e07\u0e48\u0e32\u0e22\u0e21\u0e32\u0e01\u0e02\u0e36\u0e49\u0e19 \u0e23\u0e27\u0e21\u0e16\u0e36\u0e07\u0e01\u0e32\u0e23\u0e1b\u0e23\u0e30\u0e22\u0e38\u0e01\u0e15\u0e4c\u0e43\u0e0a\u0e49\u0e42\u0e1b\u0e23\u0e41\u0e01\u0e23\u0e21 ml \u0e01\u0e31\u0e1a\u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49\u0e41\u0e02\u0e19\u0e07\u0e2d\u0e37\u0e48\u0e19 \u0e46\nwhat we have learnt\n\u0e1b\u0e23\u0e30\u0e2a\u0e1a\u0e01\u0e32\u0e23\u0e13\u0e4c\u0e40\u0e01\u0e35\u0e48\u0e22\u0e27\u0e01\u0e31\u0e1a\u0e01\u0e32\u0e23\u0e19\u0e33 ml \u0e21\u0e32\u0e1b\u0e23\u0e30\u0e22\u0e38\u0e01\u0e15\u0e4c\u0e43\u0e0a\u0e49\u0e43\u0e19\u0e42\u0e08\u0e17\u0e22\u0e4c\u0e40\u0e04\u0e21\u0e35\n\u0e01\u0e32\u0e23\u0e19\u0e33\u0e40\u0e2a\u0e19\u0e2d\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e17\u0e32\u0e07\u0e40\u0e04\u0e21\u0e35\u0e43\u0e19\u0e23\u0e39\u0e1b\u0e41\u0e1a\u0e1a\u0e17\u0e35\u0e48\u0e04\u0e2d\u0e21\u0e1e\u0e34\u0e27\u0e40\u0e15\u0e2d\u0e23\u0e4c\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e40\u0e02\u0e49\u0e32\u0e43\u0e08\u0e41\u0e25\u0e30\u0e19\u0e33\u0e44\u0e1b\u0e43\u0e0a\u0e49\u0e15\u0e48\u0e2d\u0e44\u0e14\u0e49\n\u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49\u0e40\u0e01\u0e35\u0e48\u0e22\u0e27\u0e01\u0e31\u0e1a metal organic framework\ndeepnote \u0e01\u0e31\u0e1a tensorflow \u0e21\u0e35 negative synergy \u0e01\u0e31\u0e19 \u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19\u0e2d\u0e32\u0e08\u0e22\u0e31\u0e07\u0e44\u0e21\u0e48\u0e40\u0e2a\u0e16\u0e35\u0e22\u0e23\u0e21\u0e32\u0e01\u0e19\u0e31\u0e01", "sortedWord": "None", "removed": "Nan", "score": 7, "comments": 0, "media": null, "medialink": null, "identifyer": 59507055}, {"Unnamed: 0": 7120, "autor": "MS365", "date": null, "content": "\ud83d\udd39 \u0e41\u0e23\u0e07\u0e1a\u0e31\u0e19\u0e14\u0e32\u0e25\u0e43\u0e08\u0e02\u0e2d\u0e07\u0e40\u0e23\u0e32\n\u0e17\u0e35\u0e21\u0e02\u0e2d\u0e07\u0e40\u0e23\u0e32\u0e21\u0e35\u0e04\u0e27\u0e32\u0e21\u0e2a\u0e19\u0e43\u0e08\u0e43\u0e19\u0e01\u0e32\u0e23\u0e1b\u0e23\u0e30\u0e22\u0e38\u0e01\u0e15\u0e4c\u0e43\u0e0a\u0e49\u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49\u0e17\u0e32\u0e07\u0e14\u0e49\u0e32\u0e19 Machine learning (ML) \u0e23\u0e48\u0e27\u0e21\u0e01\u0e31\u0e1a\u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49\u0e27\u0e34\u0e17\u0e22\u0e32\u0e28\u0e32\u0e2a\u0e15\u0e23\u0e4c\u0e43\u0e19\u0e2a\u0e32\u0e02\u0e32\u0e15\u0e48\u0e32\u0e07 \u0e46 \u0e40\u0e23\u0e32\u0e08\u0e36\u0e07\u0e40\u0e02\u0e49\u0e32\u0e23\u0e48\u0e27\u0e21\u0e01\u0e32\u0e23\u0e41\u0e02\u0e48\u0e07\u0e02\u0e31\u0e19 TMLCC \u0e0b\u0e36\u0e48\u0e07\u0e40\u0e1b\u0e34\u0e14\u0e42\u0e2d\u0e01\u0e32\u0e2a\u0e43\u0e2b\u0e49\u0e40\u0e23\u0e32\u0e43\u0e0a\u0e49 ML \u0e21\u0e32\u0e17\u0e33\u0e19\u0e32\u0e22\u0e04\u0e27\u0e32\u0e21\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e43\u0e19\u0e01\u0e32\u0e23\u0e14\u0e39\u0e14\u0e0b\u0e31\u0e1a\u0e01\u0e4a\u0e32\u0e0b\u0e02\u0e2d\u0e07 Metal-Organic Frameworks (MOFs)\n\ud83d\udd39 Features \u0e17\u0e35\u0e48\u0e40\u0e23\u0e32\u0e43\u0e0a\u0e49\n\u25fc Energy-based features\nRdfpy: Radial basis distribution function (mean and max value) ref.\n\u25fc Chemical features\nMOFfeatures: Total degree of unsaturation, Metallic percentage, Nitrogen to oxygen, etc. ref.\nMatminer: BandCenter, Stoichiometry, Meredig ref.\nDeepchem: ElementNet features (a portion of each element in each formula) ref.\n\u25fc Geometrical features\nPorE: Pore distribution (max value), # pores, Porosity etc. ref.\nZeo++: Largest cavity diameter (LCD) and Pore limiting diameter (PLD) ref.\niRASPA: GSA, VSA, Void fraction ref.\ncif file: Cell length, number of atoms, and coordinates.\n\u25fc Remapped features\nFunctional group remap: Map to the new group (based on intermolecular forces), Map to number of functional groups ref.\nMetal linker remap: Map to weight\n\u25fc Crossed features Density x CO2/N2 selectivity, Surface area x CO2/N2 selectivity, Pore size max x Heat adsorption, etc.\n\ud83d\udd39 \u0e42\u0e21\u0e40\u0e14\u0e25\u0e02\u0e2d\u0e07\u0e40\u0e23\u0e32\n\u0e40\u0e23\u0e32\u0e43\u0e0a\u0e49\u0e42\u0e21\u0e40\u0e14\u0e25\u0e2b\u0e25\u0e31\u0e01 \u0e46 2 \u0e01\u0e25\u0e38\u0e48\u0e21 \u0e44\u0e14\u0e49\u0e41\u0e01\u0e48\n\u25fc Gradient Boosting Tree\nXGB Regressor\nCat Boost Regressor\nLGBM Regressor\nHistogram-based Gradient Boosting Regressor\nStacked generalized model of 1) to 4) with meta-model being Ridge regression\n\u25fc Neural Networks (NNs)\nSmall NNs: TabNet ref, and DeepInsight by Squeezenet1_1 ref.\nBig NNs: TabTransformer ref., 1D-CNN ref.\n\ud83d\udccc Final prediction\nFinal prediction = weighted average \u0e23\u0e30\u0e2b\u0e27\u0e48\u0e32\u0e07 Gradient Boosting Tree (1-5) + DeepInsight + NN contributions\nWeights \u0e16\u0e39\u0e01\u0e04\u0e33\u0e19\u0e27\u0e13\u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49 Hyperopt (tree-based black-box optimization algorithm)\nNN contributions = 0.5*TabNet + 0.25*TabTransformer + 0.25 * 1D-CNN\n\ud83d\udd39 \u0e04\u0e27\u0e32\u0e21\u0e17\u0e49\u0e32\u0e17\u0e32\u0e22 \u0e41\u0e25\u0e30\u0e04\u0e27\u0e32\u0e21\u0e20\u0e32\u0e04\u0e20\u0e39\u0e21\u0e34\u0e43\u0e08\u0e02\u0e2d\u0e07\u0e40\u0e23\u0e32\n\u25fc \u0e04\u0e27\u0e32\u0e21\u0e17\u0e49\u0e32\u0e17\u0e32\u0e22 1: \u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25 Surface area, Void fraction \u0e41\u0e25\u0e30 Void volume | \u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e40\u0e2b\u0e25\u0e48\u0e32\u0e19\u0e35\u0e49\u0e1a\u0e32\u0e07\u0e2a\u0e48\u0e27\u0e19\u0e21\u0e35\u0e04\u0e48\u0e32\u0e40\u0e1b\u0e47\u0e19 0 \u0e2b\u0e23\u0e37\u0e2d -1 \u0e41\u0e25\u0e30\u0e40\u0e23\u0e32\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e08\u0e31\u0e14\u0e01\u0e32\u0e23\u0e01\u0e31\u0e1a\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e17\u0e35\u0e48\u0e21\u0e35\u0e1b\u0e31\u0e0d\u0e2b\u0e32\u0e40\u0e2b\u0e25\u0e48\u0e32\u0e19\u0e35\u0e49\u0e44\u0e14\u0e49\u0e43\u0e19\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14 \u0e2b\u0e25\u0e31\u0e07\u0e08\u0e32\u0e01\u0e1e\u0e22\u0e32\u0e22\u0e32\u0e21\u0e21\u0e32\u0e2b\u0e25\u0e32\u0e01\u0e2b\u0e25\u0e32\u0e22\u0e27\u0e34\u0e18\u0e35 \u0e08\u0e19\u0e40\u0e01\u0e47\u0e1a\u0e40\u0e2d\u0e32\u0e44\u0e1b\u0e1d\u0e31\u0e19 \ud83d\udc7b \u0e2a\u0e38\u0e14\u0e17\u0e49\u0e32\u0e22\u0e40\u0e23\u0e32\u0e04\u0e33\u0e19\u0e27\u0e13\u0e44\u0e14\u0e49\u0e08\u0e32\u0e01 iRASPA software \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49 Command-line Utility \u0e04\u0e49\u0e19\u0e1e\u0e1a\u0e42\u0e14\u0e22\u0e01\u0e32\u0e23\u0e25\u0e2d\u0e07\u0e01\u0e14\u0e17\u0e38\u0e01\u0e1b\u0e38\u0e48\u0e21\u0e43\u0e19\u0e42\u0e1b\u0e23\u0e41\u0e01\u0e23\u0e21\n\u25fc \u0e04\u0e27\u0e32\u0e21\u0e17\u0e49\u0e32\u0e17\u0e32\u0e22 2: \u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49\u0e17\u0e32\u0e07\u0e14\u0e49\u0e32\u0e19\u0e1f\u0e34\u0e2a\u0e34\u0e01\u0e2a\u0e4c \u0e41\u0e25\u0e30\u0e40\u0e04\u0e21\u0e35\u0e17\u0e35\u0e48\u0e08\u0e33\u0e01\u0e31\u0e14 | \u0e40\u0e23\u0e32\u0e44\u0e21\u0e48\u0e44\u0e14\u0e49\u0e40\u0e23\u0e35\u0e22\u0e19\u0e17\u0e32\u0e07\u0e14\u0e49\u0e32\u0e19\u0e1f\u0e34\u0e2a\u0e34\u0e01\u0e2a\u0e4c \u0e2b\u0e23\u0e37\u0e2d\u0e40\u0e04\u0e21\u0e35\u0e21\u0e32\u0e42\u0e14\u0e22\u0e40\u0e09\u0e1e\u0e32\u0e30 \u0e0b\u0e49\u0e33\u0e22\u0e31\u0e07\u0e40\u0e04\u0e22\u0e15\u0e34\u0e14\u0e28\u0e39\u0e19\u0e22\u0e4c \ud83d\ude2d \u0e17\u0e33\u0e43\u0e2b\u0e49\u0e01\u0e32\u0e23\u0e17\u0e33\u0e04\u0e27\u0e32\u0e21\u0e40\u0e02\u0e49\u0e32\u0e43\u0e08\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25 \u0e41\u0e25\u0e30\u0e17\u0e24\u0e29\u0e0e\u0e35\u0e15\u0e48\u0e32\u0e07 \u0e46 \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e2b\u0e32\u0e04\u0e27\u0e32\u0e21\u0e40\u0e0a\u0e37\u0e48\u0e2d\u0e21\u0e42\u0e22\u0e07\u0e02\u0e2d\u0e07 Features \u0e2b\u0e23\u0e37\u0e2d\u0e2a\u0e23\u0e49\u0e32\u0e07 Features \u0e43\u0e2b\u0e21\u0e48 \u0e46 \u0e40\u0e1b\u0e47\u0e19\u0e40\u0e23\u0e37\u0e48\u0e2d\u0e07\u0e22\u0e32\u0e01\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e17\u0e35\u0e21\u0e40\u0e23\u0e32 \u0e41\u0e15\u0e48\u0e40\u0e23\u0e32\u0e01\u0e47\u0e43\u0e0a\u0e49\u0e04\u0e27\u0e32\u0e21\u0e1e\u0e22\u0e32\u0e22\u0e32\u0e21\u0e43\u0e19\u0e01\u0e32\u0e23\u0e2b\u0e32\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e08\u0e19\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e17\u0e33\u0e42\u0e21\u0e40\u0e14\u0e25\u0e43\u0e19\u0e01\u0e32\u0e23\u0e17\u0e33\u0e19\u0e32\u0e22 CO2 working capacity \u0e44\u0e14\u0e49\u0e2a\u0e33\u0e40\u0e23\u0e47\u0e08\n\u25fc \u0e04\u0e27\u0e32\u0e21\u0e17\u0e49\u0e32\u0e17\u0e32\u0e22 3: Performance machine \u0e02\u0e2d\u0e07 Deepnote \u0e14\u0e31\u0e1a\u0e1a\u0e48\u0e2d\u0e22 \u0e17\u0e33\u0e43\u0e2b\u0e49\u0e01\u0e32\u0e23\u0e04\u0e33\u0e19\u0e27\u0e13 features \u0e15\u0e48\u0e32\u0e07 \u0e46 \u0e43\u0e0a\u0e49\u0e40\u0e27\u0e25\u0e32\u0e19\u0e32\u0e19\u0e01\u0e27\u0e48\u0e32\u0e17\u0e35\u0e48\u0e04\u0e27\u0e23\u0e08\u0e30\u0e40\u0e1b\u0e47\u0e19 \u0e41\u0e15\u0e48\u0e40\u0e23\u0e32\u0e44\u0e21\u0e48\u0e22\u0e48\u0e2d\u0e17\u0e49\u0e2d \u0e21\u0e35\u0e04\u0e27\u0e32\u0e21\u0e21\u0e38\u0e21\u0e32\u0e19\u0e30 \u0e19\u0e31\u0e48\u0e07\u0e01\u0e14\u0e23\u0e31\u0e19\u0e43\u0e2b\u0e21\u0e48\u0e15\u0e25\u0e2d\u0e14\u0e08\u0e19\u0e01\u0e27\u0e48\u0e32\u0e08\u0e30\u0e40\u0e2a\u0e23\u0e47\u0e08 \ud83e\udd18\n\u25fc \u0e04\u0e27\u0e32\u0e21\u0e17\u0e49\u0e32\u0e17\u0e32\u0e22 4: \u0e44\u0e21\u0e48\u0e21\u0e35 GPU \u0e40\u0e1b\u0e47\u0e19\u0e02\u0e2d\u0e07\u0e15\u0e31\u0e27\u0e40\u0e2d\u0e07 (\u0e40\u0e23\u0e32\u0e02\u0e2d resource \u0e08\u0e32\u0e01 Deepnote \u0e44\u0e21\u0e48\u0e17\u0e31\u0e19\u0e40\u0e27\u0e25\u0e32) \u0e17\u0e33\u0e43\u0e2b\u0e49\u0e44\u0e21\u0e48\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e40\u0e17\u0e23\u0e19\u0e42\u0e21\u0e40\u0e14\u0e25\u0e17\u0e35\u0e48\u0e0b\u0e31\u0e1a\u0e0b\u0e49\u0e2d\u0e19\u0e44\u0e14\u0e49 \u0e41\u0e15\u0e48\u0e40\u0e23\u0e32\u0e01\u0e47\u0e44\u0e21\u0e48\u0e17\u0e49\u0e2d\u0e40\u0e0a\u0e48\u0e19\u0e01\u0e31\u0e19 \u0e23\u0e31\u0e19\u0e42\u0e21\u0e40\u0e14\u0e25\u0e43\u0e19 Macbook \u0e08\u0e19\u0e04\u0e27\u0e31\u0e19\u0e02\u0e36\u0e49\u0e19 \ud83d\udd25\n\ud83d\udd39 \u0e2a\u0e34\u0e48\u0e07\u0e17\u0e35\u0e48\u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e40\u0e23\u0e35\u0e22\u0e19\u0e23\u0e39\u0e49\n\u25fc \u0e44\u0e14\u0e49\u0e40\u0e23\u0e35\u0e22\u0e19\u0e23\u0e39\u0e49\u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49\u0e1e\u0e37\u0e49\u0e19\u0e10\u0e32\u0e19\u0e02\u0e2d\u0e07 MOFs \u0e40\u0e0a\u0e48\u0e19 \u0e42\u0e04\u0e23\u0e07\u0e2a\u0e23\u0e49\u0e32\u0e07 \u0e04\u0e38\u0e13\u0e2a\u0e21\u0e1a\u0e31\u0e15\u0e34\u0e17\u0e32\u0e07\u0e01\u0e32\u0e22\u0e20\u0e32\u0e1e \u0e41\u0e25\u0e30\u0e40\u0e04\u0e21\u0e35\u0e15\u0e48\u0e32\u0e07 \u0e46 \u0e02\u0e2d\u0e07 MOFs\n\u25fc \u0e44\u0e14\u0e49\u0e40\u0e23\u0e35\u0e22\u0e19\u0e23\u0e39\u0e49\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49 Library \u0e15\u0e48\u0e32\u0e07 \u0e46 \u0e17\u0e32\u0e07\u0e40\u0e04\u0e21\u0e35 \u0e40\u0e0a\u0e48\u0e19 Rdkit, Deepchem, zeo++ \u0e40\u0e1b\u0e47\u0e19\u0e15\u0e49\u0e19\n\ud83d\udd39 Contact\npongpisit.tha@gmail.com\nsupitcha.suks@gmail.com", "link": "https://devpost.com/software/ms365", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "\ud83d\udd39 \u0e41\u0e23\u0e07\u0e1a\u0e31\u0e19\u0e14\u0e32\u0e25\u0e43\u0e08\u0e02\u0e2d\u0e07\u0e40\u0e23\u0e32\n\u0e17\u0e35\u0e21\u0e02\u0e2d\u0e07\u0e40\u0e23\u0e32\u0e21\u0e35\u0e04\u0e27\u0e32\u0e21\u0e2a\u0e19\u0e43\u0e08\u0e43\u0e19\u0e01\u0e32\u0e23\u0e1b\u0e23\u0e30\u0e22\u0e38\u0e01\u0e15\u0e4c\u0e43\u0e0a\u0e49\u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49\u0e17\u0e32\u0e07\u0e14\u0e49\u0e32\u0e19 machine learning (ml) \u0e23\u0e48\u0e27\u0e21\u0e01\u0e31\u0e1a\u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49\u0e27\u0e34\u0e17\u0e22\u0e32\u0e28\u0e32\u0e2a\u0e15\u0e23\u0e4c\u0e43\u0e19\u0e2a\u0e32\u0e02\u0e32\u0e15\u0e48\u0e32\u0e07 \u0e46 \u0e40\u0e23\u0e32\u0e08\u0e36\u0e07\u0e40\u0e02\u0e49\u0e32\u0e23\u0e48\u0e27\u0e21\u0e01\u0e32\u0e23\u0e41\u0e02\u0e48\u0e07\u0e02\u0e31\u0e19 tmlcc \u0e0b\u0e36\u0e48\u0e07\u0e40\u0e1b\u0e34\u0e14\u0e42\u0e2d\u0e01\u0e32\u0e2a\u0e43\u0e2b\u0e49\u0e40\u0e23\u0e32\u0e43\u0e0a\u0e49 ml \u0e21\u0e32\u0e17\u0e33\u0e19\u0e32\u0e22\u0e04\u0e27\u0e32\u0e21\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e43\u0e19\u0e01\u0e32\u0e23\u0e14\u0e39\u0e14\u0e0b\u0e31\u0e1a\u0e01\u0e4a\u0e32\u0e0b\u0e02\u0e2d\u0e07 metal-organic frameworks (mofs)\n\ud83d\udd39 features \u0e17\u0e35\u0e48\u0e40\u0e23\u0e32\u0e43\u0e0a\u0e49\n\u25fc energy-based features\nrdfpy: radial basis distribution function (mean and max value) ref.\n\u25fc chemical features\nmoffeatures: total degree of unsaturation, metallic percentage, nitrogen to oxygen, etc. ref.\nmatminer: bandcenter, stoichiometry, meredig ref.\ndeepchem: elementnet features (a portion of each element in each formula) ref.\n\u25fc geometrical features\npore: pore distribution (max value), # pores, porosity etc. ref.\nzeo++: largest cavity diameter (lcd) and pore limiting diameter (pld) ref.\niraspa: gsa, vsa, void fraction ref.\ncif file: cell length, number of atoms, and coordinates.\n\u25fc remapped features\nfunctional group remap: map to the new group (based on intermolecular forces), map to number of functional groups ref.\nmetal linker remap: map to weight\n\u25fc crossed features density x co2/n2 selectivity, surface area x co2/n2 selectivity, pore size max x heat adsorption, etc.\n\ud83d\udd39 \u0e42\u0e21\u0e40\u0e14\u0e25\u0e02\u0e2d\u0e07\u0e40\u0e23\u0e32\n\u0e40\u0e23\u0e32\u0e43\u0e0a\u0e49\u0e42\u0e21\u0e40\u0e14\u0e25\u0e2b\u0e25\u0e31\u0e01 \u0e46 2 \u0e01\u0e25\u0e38\u0e48\u0e21 \u0e44\u0e14\u0e49\u0e41\u0e01\u0e48\n\u25fc gradient boosting -----> tree !!! \nxgb regressor\ncat boost regressor\nlgbm regressor\nhistogram-based gradient boosting regressor\nstacked generalized model of 1) to 4) with meta-model being ridge regression\n\u25fc neural networks (nns)\nsmall nns: tabnet ref, and deepinsight by squeezenet1_1 ref.\nbig nns: tabtransformer ref., 1d-cnn ref.\n\ud83d\udccc final prediction\nfinal prediction = weighted average \u0e23\u0e30\u0e2b\u0e27\u0e48\u0e32\u0e07 gradient boosting tree (1-5) + deepinsight + nn contributions\nweights \u0e16\u0e39\u0e01\u0e04\u0e33\u0e19\u0e27\u0e13\u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49 hyperopt (tree-based black-box optimization algorithm)\nnn contributions = 0.5*tabnet + 0.25*tabtransformer + 0.25 * 1d-cnn\n\ud83d\udd39 \u0e04\u0e27\u0e32\u0e21\u0e17\u0e49\u0e32\u0e17\u0e32\u0e22 \u0e41\u0e25\u0e30\u0e04\u0e27\u0e32\u0e21\u0e20\u0e32\u0e04\u0e20\u0e39\u0e21\u0e34\u0e43\u0e08\u0e02\u0e2d\u0e07\u0e40\u0e23\u0e32\n\u25fc \u0e04\u0e27\u0e32\u0e21\u0e17\u0e49\u0e32\u0e17\u0e32\u0e22 1: \u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25 surface area, void fraction \u0e41\u0e25\u0e30 void volume | \u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e40\u0e2b\u0e25\u0e48\u0e32\u0e19\u0e35\u0e49\u0e1a\u0e32\u0e07\u0e2a\u0e48\u0e27\u0e19\u0e21\u0e35\u0e04\u0e48\u0e32\u0e40\u0e1b\u0e47\u0e19 0 \u0e2b\u0e23\u0e37\u0e2d -1 \u0e41\u0e25\u0e30\u0e40\u0e23\u0e32\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e08\u0e31\u0e14\u0e01\u0e32\u0e23\u0e01\u0e31\u0e1a\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e17\u0e35\u0e48\u0e21\u0e35\u0e1b\u0e31\u0e0d\u0e2b\u0e32\u0e40\u0e2b\u0e25\u0e48\u0e32\u0e19\u0e35\u0e49\u0e44\u0e14\u0e49\u0e43\u0e19\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14 \u0e2b\u0e25\u0e31\u0e07\u0e08\u0e32\u0e01\u0e1e\u0e22\u0e32\u0e22\u0e32\u0e21\u0e21\u0e32\u0e2b\u0e25\u0e32\u0e01\u0e2b\u0e25\u0e32\u0e22\u0e27\u0e34\u0e18\u0e35 \u0e08\u0e19\u0e40\u0e01\u0e47\u0e1a\u0e40\u0e2d\u0e32\u0e44\u0e1b\u0e1d\u0e31\u0e19 \ud83d\udc7b \u0e2a\u0e38\u0e14\u0e17\u0e49\u0e32\u0e22\u0e40\u0e23\u0e32\u0e04\u0e33\u0e19\u0e27\u0e13\u0e44\u0e14\u0e49\u0e08\u0e32\u0e01 iraspa software \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49 command-line utility \u0e04\u0e49\u0e19\u0e1e\u0e1a\u0e42\u0e14\u0e22\u0e01\u0e32\u0e23\u0e25\u0e2d\u0e07\u0e01\u0e14\u0e17\u0e38\u0e01\u0e1b\u0e38\u0e48\u0e21\u0e43\u0e19\u0e42\u0e1b\u0e23\u0e41\u0e01\u0e23\u0e21\n\u25fc \u0e04\u0e27\u0e32\u0e21\u0e17\u0e49\u0e32\u0e17\u0e32\u0e22 2: \u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49\u0e17\u0e32\u0e07\u0e14\u0e49\u0e32\u0e19\u0e1f\u0e34\u0e2a\u0e34\u0e01\u0e2a\u0e4c \u0e41\u0e25\u0e30\u0e40\u0e04\u0e21\u0e35\u0e17\u0e35\u0e48\u0e08\u0e33\u0e01\u0e31\u0e14 | \u0e40\u0e23\u0e32\u0e44\u0e21\u0e48\u0e44\u0e14\u0e49\u0e40\u0e23\u0e35\u0e22\u0e19\u0e17\u0e32\u0e07\u0e14\u0e49\u0e32\u0e19\u0e1f\u0e34\u0e2a\u0e34\u0e01\u0e2a\u0e4c \u0e2b\u0e23\u0e37\u0e2d\u0e40\u0e04\u0e21\u0e35\u0e21\u0e32\u0e42\u0e14\u0e22\u0e40\u0e09\u0e1e\u0e32\u0e30 \u0e0b\u0e49\u0e33\u0e22\u0e31\u0e07\u0e40\u0e04\u0e22\u0e15\u0e34\u0e14\u0e28\u0e39\u0e19\u0e22\u0e4c \ud83d\ude2d \u0e17\u0e33\u0e43\u0e2b\u0e49\u0e01\u0e32\u0e23\u0e17\u0e33\u0e04\u0e27\u0e32\u0e21\u0e40\u0e02\u0e49\u0e32\u0e43\u0e08\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25 \u0e41\u0e25\u0e30\u0e17\u0e24\u0e29\u0e0e\u0e35\u0e15\u0e48\u0e32\u0e07 \u0e46 \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e2b\u0e32\u0e04\u0e27\u0e32\u0e21\u0e40\u0e0a\u0e37\u0e48\u0e2d\u0e21\u0e42\u0e22\u0e07\u0e02\u0e2d\u0e07 features \u0e2b\u0e23\u0e37\u0e2d\u0e2a\u0e23\u0e49\u0e32\u0e07 features \u0e43\u0e2b\u0e21\u0e48 \u0e46 \u0e40\u0e1b\u0e47\u0e19\u0e40\u0e23\u0e37\u0e48\u0e2d\u0e07\u0e22\u0e32\u0e01\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e17\u0e35\u0e21\u0e40\u0e23\u0e32 \u0e41\u0e15\u0e48\u0e40\u0e23\u0e32\u0e01\u0e47\u0e43\u0e0a\u0e49\u0e04\u0e27\u0e32\u0e21\u0e1e\u0e22\u0e32\u0e22\u0e32\u0e21\u0e43\u0e19\u0e01\u0e32\u0e23\u0e2b\u0e32\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e08\u0e19\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e17\u0e33\u0e42\u0e21\u0e40\u0e14\u0e25\u0e43\u0e19\u0e01\u0e32\u0e23\u0e17\u0e33\u0e19\u0e32\u0e22 co2 working capacity \u0e44\u0e14\u0e49\u0e2a\u0e33\u0e40\u0e23\u0e47\u0e08\n\u25fc \u0e04\u0e27\u0e32\u0e21\u0e17\u0e49\u0e32\u0e17\u0e32\u0e22 3: performance machine \u0e02\u0e2d\u0e07 deepnote \u0e14\u0e31\u0e1a\u0e1a\u0e48\u0e2d\u0e22 \u0e17\u0e33\u0e43\u0e2b\u0e49\u0e01\u0e32\u0e23\u0e04\u0e33\u0e19\u0e27\u0e13 features \u0e15\u0e48\u0e32\u0e07 \u0e46 \u0e43\u0e0a\u0e49\u0e40\u0e27\u0e25\u0e32\u0e19\u0e32\u0e19\u0e01\u0e27\u0e48\u0e32\u0e17\u0e35\u0e48\u0e04\u0e27\u0e23\u0e08\u0e30\u0e40\u0e1b\u0e47\u0e19 \u0e41\u0e15\u0e48\u0e40\u0e23\u0e32\u0e44\u0e21\u0e48\u0e22\u0e48\u0e2d\u0e17\u0e49\u0e2d \u0e21\u0e35\u0e04\u0e27\u0e32\u0e21\u0e21\u0e38\u0e21\u0e32\u0e19\u0e30 \u0e19\u0e31\u0e48\u0e07\u0e01\u0e14\u0e23\u0e31\u0e19\u0e43\u0e2b\u0e21\u0e48\u0e15\u0e25\u0e2d\u0e14\u0e08\u0e19\u0e01\u0e27\u0e48\u0e32\u0e08\u0e30\u0e40\u0e2a\u0e23\u0e47\u0e08 \ud83e\udd18\n\u25fc \u0e04\u0e27\u0e32\u0e21\u0e17\u0e49\u0e32\u0e17\u0e32\u0e22 4: \u0e44\u0e21\u0e48\u0e21\u0e35 gpu \u0e40\u0e1b\u0e47\u0e19\u0e02\u0e2d\u0e07\u0e15\u0e31\u0e27\u0e40\u0e2d\u0e07 (\u0e40\u0e23\u0e32\u0e02\u0e2d resource \u0e08\u0e32\u0e01 deepnote \u0e44\u0e21\u0e48\u0e17\u0e31\u0e19\u0e40\u0e27\u0e25\u0e32) \u0e17\u0e33\u0e43\u0e2b\u0e49\u0e44\u0e21\u0e48\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e40\u0e17\u0e23\u0e19\u0e42\u0e21\u0e40\u0e14\u0e25\u0e17\u0e35\u0e48\u0e0b\u0e31\u0e1a\u0e0b\u0e49\u0e2d\u0e19\u0e44\u0e14\u0e49 \u0e41\u0e15\u0e48\u0e40\u0e23\u0e32\u0e01\u0e47\u0e44\u0e21\u0e48\u0e17\u0e49\u0e2d\u0e40\u0e0a\u0e48\u0e19\u0e01\u0e31\u0e19 \u0e23\u0e31\u0e19\u0e42\u0e21\u0e40\u0e14\u0e25\u0e43\u0e19 macbook \u0e08\u0e19\u0e04\u0e27\u0e31\u0e19\u0e02\u0e36\u0e49\u0e19 \ud83d\udd25\n\ud83d\udd39 \u0e2a\u0e34\u0e48\u0e07\u0e17\u0e35\u0e48\u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e40\u0e23\u0e35\u0e22\u0e19\u0e23\u0e39\u0e49\n\u25fc \u0e44\u0e14\u0e49\u0e40\u0e23\u0e35\u0e22\u0e19\u0e23\u0e39\u0e49\u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49\u0e1e\u0e37\u0e49\u0e19\u0e10\u0e32\u0e19\u0e02\u0e2d\u0e07 mofs \u0e40\u0e0a\u0e48\u0e19 \u0e42\u0e04\u0e23\u0e07\u0e2a\u0e23\u0e49\u0e32\u0e07 \u0e04\u0e38\u0e13\u0e2a\u0e21\u0e1a\u0e31\u0e15\u0e34\u0e17\u0e32\u0e07\u0e01\u0e32\u0e22\u0e20\u0e32\u0e1e \u0e41\u0e25\u0e30\u0e40\u0e04\u0e21\u0e35\u0e15\u0e48\u0e32\u0e07 \u0e46 \u0e02\u0e2d\u0e07 mofs\n\u25fc \u0e44\u0e14\u0e49\u0e40\u0e23\u0e35\u0e22\u0e19\u0e23\u0e39\u0e49\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49 library \u0e15\u0e48\u0e32\u0e07 \u0e46 \u0e17\u0e32\u0e07\u0e40\u0e04\u0e21\u0e35 \u0e40\u0e0a\u0e48\u0e19 rdkit, deepchem, zeo++ \u0e40\u0e1b\u0e47\u0e19\u0e15\u0e49\u0e19\n\ud83d\udd39 contact\npongpisit.tha@gmail.com\nsupitcha.suks@gmail.com", "sortedWord": "None", "removed": "Nan", "score": 3, "comments": 0, "media": null, "medialink": null, "identifyer": 59507120}, {"Unnamed: 0": 7283, "autor": "Wonderland", "date": null, "content": "\u0e04\u0e33\u0e40\u0e15\u0e37\u0e2d\u0e19 \u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\u0e2d\u0e48\u0e32\u0e19\u0e40\u0e2d\u0e32\u0e21\u0e31\u0e19!!!!!\n\u0e41\u0e23\u0e07\u0e1a\u0e31\u0e19\u0e14\u0e32\u0e25\u0e43\u0e08\n\u0e17\u0e32\u0e07\u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e21\u0e35\u0e04\u0e27\u0e32\u0e21\u0e2a\u0e19\u0e43\u0e08\u0e01\u0e32\u0e23\u0e1b\u0e23\u0e30\u0e22\u0e38\u0e04\u0e43\u0e0a\u0e49 ML \u0e2b\u0e23\u0e37\u0e2d Machine learning \u0e23\u0e48\u0e27\u0e21\u0e01\u0e31\u0e1a \u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e1b\u0e23\u0e30\u0e40\u0e20\u0e17\u0e15\u0e48\u0e32\u0e07\u0e46 \u0e01\u0e32\u0e23\u0e17\u0e35\u0e48\u0e21\u0e35 TMLCC \u0e40\u0e01\u0e34\u0e14\u0e02\u0e36\u0e49\u0e19\u0e01\u0e47\u0e40\u0e2b\u0e21\u0e37\u0e2d\u0e19\u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e17\u0e48\u0e2d\u0e07\u0e22\u0e38\u0e17\u0e18\u0e08\u0e31\u0e01\u0e23 \u0e40\u0e1b\u0e34\u0e14\u0e2b\u0e39\u0e40\u0e1b\u0e34\u0e14\u0e15\u0e32\u0e23\u0e31\u0e1a\u0e23\u0e39\u0e49\u0e27\u0e48\u0e32 MOFs \u0e21\u0e31\u0e19\u0e04\u0e37\u0e2d\u0e2d\u0e30\u0e44\u0e23 \u0e41\u0e25\u0e30 \u0e40\u0e23\u0e32\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e1b\u0e23\u0e30\u0e22\u0e38\u0e04 ML\u0e43\u0e0a\u0e49\u0e43\u0e19\u0e2a\u0e48\u0e27\u0e19\u0e44\u0e2b\u0e19\u0e44\u0e14\u0e49\u0e1a\u0e49\u0e32\u0e07\n\u0e17\u0e33\u0e2d\u0e30\u0e44\u0e23\u0e1a\u0e49\u0e32\u0e07\n\u0e40\u0e2d\u0e32\u0e08\u0e23\u0e34\u0e07\u0e46\u0e21\u0e31\u0e19\u0e01\u0e47 data science Process \u0e18\u0e23\u0e23\u0e21\u0e14\u0e32\u0e44\u0e21\u0e48\u0e2b\u0e27\u0e37\u0e2d\u0e2b\u0e27\u0e32\u0e2d\u0e30\u0e44\u0e23 \u0e41\u0e15\u0e48\u0e17\u0e35\u0e48\u0e2a\u0e33\u0e04\u0e31\u0e0d\u0e15\u0e49\u0e2d\u0e07\u0e23\u0e39\u0e49\u0e27\u0e48\u0e32 \u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e21\u0e32\u0e40\u0e1b\u0e47\u0e19\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e41\u0e1a\u0e1a\u0e44\u0e2b\u0e19 \u0e40\u0e01\u0e34\u0e14 loss \u0e44\u0e2b\u0e21 correlation \u0e2b\u0e23\u0e37\u0e2d \u0e2a\u0e31\u0e21\u0e1e\u0e31\u0e19\u0e18\u0e4c\u0e01\u0e31\u0e19\u0e22\u0e31\u0e07\u0e44\u0e07 \u0e01\u0e48\u0e2d\u0e19\u0e40\u0e02\u0e49\u0e32 model \u0e43\u0e0a\u0e48\u0e1b\u0e30 \u0e15\u0e2d\u0e19\u0e40\u0e25\u0e37\u0e2d\u0e01\u0e42\u0e21\u0e40\u0e14\u0e25 \u0e04\u0e34\u0e14\u0e44\u0e23\u0e2d\u0e2d\u0e01\u0e01\u0e47 run \u0e44\u0e1b\u0e01\u0e48\u0e2d\u0e19 \u0e41\u0e25\u0e49\u0e27\u0e16\u0e49\u0e32\u0e04\u0e34\u0e14\u0e27\u0e48\u0e32\u0e21\u0e31\u0e19\u0e14\u0e35\u0e01\u0e47\n\u201c Tune \u0e2a\u0e34\u0e04\u0e23\u0e31\u0e1a\u0e1e\u0e35\u0e48\u0e19\u0e49\u0e2d\u0e07\u201d\n\u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e01\u0e47\u0e44\u0e1b Validation \u0e04\u0e23\u0e31\u0e1a \u0e14\u0e35\u0e01\u0e47\u0e2a\u0e48\u0e07\u0e44\u0e21\u0e48\u0e14\u0e35 \u0e01\u0e47 tune \u0e15\u0e48\u0e2d\u0e41\u0e2b\u0e21\u0e48 \u2026.. \u0e41\u0e25\u0e49\u0e27 \u0e15\u0e2d\u0e19\u0e2a\u0e48\u0e07\u0e01\u0e47\u0e41\u0e2b\u0e21\u0e48 \"Failed \u0e40\u0e1b\u0e47\u0e19\u0e15\u0e31\u0e1a\u0e08\u0e49\u0e32\"\n\u0e42\u0e14\u0e22\u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32\u0e17\u0e33\u0e01\u0e32\u0e23\u0e19\u0e31\u0e48\u0e07\u0e19\u0e36\u0e01\u0e27\u0e48\u0e32\u0e43\u0e0a\u0e49 Model \u0e2d\u0e30\u0e44\u0e23\u0e14\u0e35 \u0e08\u0e19\u0e01\u0e23\u0e30\u0e17\u0e31\u0e48\u0e07\u0e04\u0e34\u0e14\u0e46\u0e27\u0e48\u0e32\u0e17\u0e33\u0e46\u0e44\u0e1b\u0e15\u0e32\u0e21\u0e04\u0e27\u0e32\u0e21\u0e16\u0e19\u0e31\u0e14\u0e19\u0e35\u0e48\u0e41\u0e2b\u0e25\u0e30 \u0e04\u0e37\u0e2d\nXGBoost: \u0e42\u0e14\u0e22\u0e2a\u0e48\u0e27\u0e19\u0e15\u0e31\u0e27 Tune \u0e07\u0e48\u0e32\u0e22 \u0e2b\u0e32\u0e04\u0e33\u0e15\u0e2d\u0e1a\u0e40\u0e23\u0e47\u0e27\u0e40\u0e23\u0e48\u0e07\u0e40\u0e04\u0e23\u0e37\u0e48\u0e2d\u0e07\u0e44\u0e14\u0e49\u0e14\u0e35 \u0e23\u0e39\u0e49\u0e27\u0e48\u0e32 Feature \u0e44\u0e2b\u0e19\u0e2a\u0e33\u0e04\u0e31\u0e0d\u0e44\u0e21\u0e48\u0e2a\u0e33\u0e04\u0e31\u0e0d\nRandom forest: \u0e21\u0e31\u0e19\u0e04\u0e37\u0e2d\u0e01\u0e32\u0e23\u0e2a\u0e38\u0e48\u0e21\u0e15\u0e49\u0e19\u0e44\u0e21\u0e49 (Decision Tree) \u0e15\u0e32\u0e21\u0e08\u0e33\u0e19\u0e27\u0e19\u0e17\u0e35\u0e48\u0e15\u0e49\u0e2d\u0e07\u0e01\u0e32\u0e23 \u0e2a\u0e38\u0e48\u0e21\u0e2d\u0e22\u0e39\u0e48\u0e19\u0e31\u0e48\u0e19\u0e41\u0e2b\u0e25\u0e30\u0e08\u0e19\u0e40\u0e15\u0e47\u0e21\u0e1b\u0e48\u0e32 (\u0e15\u0e32\u0e21\u0e0a\u0e37\u0e48\u0e2d) \u0e41\u0e25\u0e49\u0e27\u0e01\u0e47\u0e40\u0e2d\u0e32\u0e15\u0e49\u0e19\u0e40\u0e17\u0e1e\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14\u0e43\u0e19\u0e1b\u0e48\u0e32\u0e2d\u0e2d\u0e01\u0e21\u0e32 \u2026. \u201c\u0e19\u0e35\u0e48\u0e2a\u0e34\u0e19\u0e30\u0e17\u0e35\u0e48\u0e40\u0e23\u0e35\u0e22\u0e01\u0e27\u0e48\u0e32 \u0e2a\u0e38\u0e14\u0e43\u0e19\u0e1b\u0e48\u0e32\u201d\nNeural Network: \u0e42\u0e21\u0e40\u0e14\u0e25\u0e23\u0e38\u0e48\u0e19\u0e43\u0e2b\u0e21\u0e48 \u0e44\u0e17\u0e22\u0e19\u0e34\u0e22\u0e21 (\u0e15\u0e48\u0e32\u0e07\u0e1b\u0e23\u0e30\u0e40\u0e17\u0e28\u0e01\u0e47\u0e19\u0e34\u0e22\u0e21) \u0e41\u0e15\u0e48\u0e27\u0e48\u0e32\u0e15\u0e49\u0e2d\u0e07\u0e43\u0e2b\u0e49\u0e40\u0e27\u0e25\u0e32\u0e21\u0e31\u0e19\u0e40\u0e22\u0e2d\u0e30\u0e2b\u0e19\u0e48\u0e2d\u0e22\u0e16\u0e49\u0e32\u0e21\u0e31\u0e19 tune \u0e42\u0e04\u0e23\u0e07\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e40\u0e08\u0e2d\u0e19\u0e30 ML \u0e18\u0e23\u0e23\u0e21\u0e14\u0e32 \"tune 3 \u0e0a\u0e32\u0e15\u0e34\u0e01\u0e47\u0e44\u0e21\u0e48\u0e2a\u0e39\u0e49\"\nSymbolic regression (Genetic programming):\u0e42\u0e21\u0e40\u0e14\u0e25\u0e19\u0e35\u0e49\u0e21\u0e31\u0e19\u0e40\u0e0b\u0e47\u0e17 Rules \u0e2b\u0e23\u0e37\u0e2d \u0e2a\u0e21\u0e01\u0e32\u0e23 \u0e21\u0e32\u0e01\u0e48\u0e2d\u0e19 \u0e17\u0e33\u0e43\u0e2b\u0e49\u0e40\u0e23\u0e32\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e23\u0e39\u0e49\u0e44\u0e14\u0e49\u0e27\u0e48\u0e32 Rules \u0e17\u0e35\u0e48\u0e43\u0e2b\u0e49\u0e44\u0e1b\u0e21\u0e31\u0e19 sig \u0e44\u0e2b\u0e21 \u0e01\u0e31\u0e1a Data\nVoting Regressor: \u0e07\u0e48\u0e32\u0e22\u0e46\u0e21\u0e31\u0e19\u0e04\u0e37\u0e2d\u0e01\u0e32\u0e23\u0e40\u0e2d\u0e32 Model \u0e17\u0e35\u0e48\u0e04\u0e34\u0e14\u0e27\u0e48\u0e32 \"\u0e08\u0e30\u0e41\u0e21\u0e48\u0e19\" \u0e43\u0e19\u0e17\u0e35\u0e48\u0e19\u0e35\u0e49\u0e40\u0e23\u0e32\u0e40\u0e25\u0e37\u0e2d\u0e01\u0e43\u0e0a\u0e49 Gradient boosting, Random Forest, and Linear regression \u0e21\u0e32\u0e17\u0e33\u0e01\u0e32\u0e23 Vote \u0e27\u0e48\u0e32 model \u0e44\u0e2b\u0e19\u0e21\u0e31\u0e19\u0e04\u0e48\u0e32\u0e43\u0e01\u0e25\u0e49\u0e46\u0e01\u0e31\u0e1a\u0e0a\u0e38\u0e14\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e01\u0e47\u0e43\u0e2b\u0e49\u0e19\u0e49\u0e33\u0e2b\u0e19\u0e31\u0e01\u0e2b\u0e23\u0e37\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e2a\u0e33\u0e04\u0e31\u0e0dmodel\u0e19\u0e31\u0e49\u0e19\u0e21\u0e32\u0e01\u0e2b\u0e19\u0e48\u0e2d\u0e22 \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e01\u0e47\u0e40\u0e2d\u0e32\u0e04\u0e48\u0e32\u0e17\u0e35\u0e48\u0e17\u0e32\u0e22\u0e43\u0e19 model \u0e2d\u0e37\u0e48\u0e19\u0e46\u0e21\u0e32 \u0e40\u0e09\u0e25\u0e35\u0e48\u0e22\u0e16\u0e48\u0e27\u0e07\u0e19\u0e49\u0e33\u0e2b\u0e19\u0e31\u0e01\u0e01\u0e31\u0e19\u0e08\u0e19\u0e44\u0e14\u0e49\u0e04\u0e48\u0e32 \u0e17\u0e33\u0e19\u0e32\u0e22\u0e43\u0e2b\u0e21\u0e48\n\u0e04\u0e27\u0e32\u0e21\u0e17\u0e49\u0e32\u0e17\u0e32\u0e22\u0e17\u0e35\u0e48\u0e40\u0e23\u0e32\u0e1e\u0e1a\u0e40\u0e08\u0e2d\n\u0e14\u0e49\u0e27\u0e22\u0e40\u0e27\u0e25\u0e32\u0e17\u0e35\u0e48\u0e01\u0e25\u0e38\u0e48\u0e21\u0e19\u0e31\u0e49\u0e19\u0e17\u0e33\u0e07\u0e32\u0e19\u0e1b\u0e23\u0e30\u0e08\u0e33\u0e01\u0e31\u0e19\u0e40\u0e1b\u0e47\u0e19\u0e2a\u0e48\u0e27\u0e19\u0e43\u0e2b\u0e0d\u0e48 (\u0e41\u0e15\u0e48\u0e1c\u0e39\u0e49\u0e40\u0e02\u0e35\u0e22\u0e19 \u0e40\u0e1b\u0e47\u0e19 freelance \u0e2d\u0e22\u0e39\u0e48\u0e19\u0e30 \u201c\u0e08\u0e49\u0e32\u0e07\u0e44\u0e14\u0e49\u0e09\u0e31\u0e19\u0e2b\u0e34\u0e27\u201d) \u0e17\u0e33\u0e43\u0e2b\u0e49\u0e1b\u0e23\u0e30\u0e0a\u0e38\u0e21\u0e01\u0e31\u0e19\u0e19\u0e49\u0e2d\u0e22 \u0e17\u0e33\u0e43\u0e2b\u0e49\u0e21\u0e35\u0e40\u0e27\u0e25\u0e32\u0e43\u0e19\u0e01\u0e32\u0e23\u0e17\u0e33\u0e19\u0e49\u0e2d\u0e22 \u0e1e\u0e2d\u0e23\u0e39\u0e49\u0e27\u0e48\u0e32\u0e43\u0e01\u0e25\u0e49\u0e2b\u0e21\u0e14\u0e0a\u0e48\u0e27\u0e07\u0e40\u0e27\u0e25\u0e32\u0e41\u0e02\u0e48\u0e07\u0e01\u0e47\u0e04\u0e48\u0e2d\u0e22\u0e21\u0e32\u0e14\u0e39\u0e04\u0e25\u0e34\u0e1b\u0e22\u0e49\u0e2d\u0e19\u0e2b\u0e25\u0e31\u0e07\u0e17\u0e33\u0e43\u0e2b\u0e49\u0e23\u0e39\u0e49\u0e27\u0e48\u0e32\u0e21\u0e31\u0e19\u0e40\u0e22\u0e2d\u0e30\u0e21\u0e32\u0e01\u0e01\u0e01\u0e01\u0e01 \u0e1e\u0e2d\u0e44\u0e1f\u0e40\u0e23\u0e34\u0e48\u0e21\u0e25\u0e32\u0e21\u0e41\u0e25\u0e49\u0e27\u0e40\u0e23\u0e32\u0e08\u0e36\u0e07\u0e21\u0e35\u0e44\u0e1f\u0e43\u0e19\u0e01\u0e32\u0e23\u0e17\u0e33\u0e17\u0e31\u0e19\u0e17\u0e35 submit \u0e17\u0e33\u0e43\u0e2b\u0e49\u0e21\u0e35\u0e40\u0e27\u0e25\u0e32\u0e17\u0e33 feature selection \u0e19\u0e49\u0e2d\u0e22\u0e21\u0e32\u0e01\u0e08\u0e23\u0e34\u0e07\u0e46\u0e41\u0e25\u0e49\u0e27\u0e40\u0e23\u0e32\u0e21\u0e35\u0e2d\u0e30\u0e44\u0e23\u0e2b\u0e25\u0e32\u0e22\u0e46\u0e2d\u0e22\u0e48\u0e32\u0e07\u0e2d\u0e22\u0e32\u0e01\u0e43\u0e2a\u0e48\u0e40\u0e02\u0e49\u0e32\u0e44\u0e1b\u0e2d\u0e35\u0e01\u0e40\u0e22\u0e2d\u0e30\u0e41\u0e15\u0e48\u0e01\u0e47\u0e01\u0e25\u0e31\u0e27\u0e44\u0e21\u0e48\u0e17\u0e31\u0e19\u0e40\u0e1e\u0e23\u0e32\u0e30\u0e27\u0e48\u0e32 \u0e17\u0e35\u0e48 run model \u0e40\u0e04\u0e23\u0e37\u0e48\u0e2d\u0e07\u0e01\u0e47 \u0e23\u0e49\u0e2d\u0e07\u0e42\u0e2d\u0e14\u0e23\u0e49\u0e2d\u0e07\u0e42\u0e2d\u0e22\u0e41\u0e25\u0e49\u0e27\n\u0e2a\u0e34\u0e48\u0e07\u0e17\u0e35\u0e48\u0e20\u0e39\u0e21\u0e34\u0e43\u0e08\n\u0e40\u0e23\u0e32\u0e43\u0e0a\u0e49\u0e40\u0e27\u0e25\u0e32\u0e17\u0e33 Model \u0e23\u0e27\u0e21\u0e16\u0e36\u0e07 Clean data \u0e1e\u0e23\u0e49\u0e2d\u0e21\u0e17\u0e31\u0e49\u0e07 Tune \u0e40\u0e2a\u0e23\u0e47\u0e08\u0e14\u0e49\u0e27\u0e22 \u0e20\u0e32\u0e22\u0e43\u0e19\u0e40\u0e27\u0e25\u0e32 1 \u0e27\u0e31\u0e19 \u0e19\u0e34\u0e14\u0e46 \u0e42\u0e14\u0e22\u0e1b\u0e01\u0e15\u0e34\u0e17\u0e33\u0e19\u0e32\u0e19\u0e01\u0e27\u0e48\u0e32\u0e19\u0e35\u0e49\n\u0e08\u0e32\u0e01\u0e04\u0e27\u0e32\u0e21\u0e16\u0e49\u0e32\u0e17\u0e32\u0e22\u0e40\u0e23\u0e37\u0e48\u0e2d\u0e07 Feature \u0e43\u0e19\u0e14\u0e49\u0e32\u0e19 MOF \u0e40\u0e1b\u0e47\u0e19 0 \u0e17\u0e33\u0e43\u0e2b\u0e49 \u0e44\u0e21\u0e48\u0e23\u0e39\u0e49\u0e27\u0e48\u0e32\u0e15\u0e49\u0e2d\u0e07\u0e15\u0e31\u0e14\u0e15\u0e31\u0e27\u0e44\u0e2b\u0e19\u0e2b\u0e23\u0e37\u0e2d\u0e43\u0e0a\u0e48\u0e04\u0e48\u0e32\u0e44\u0e2b\u0e19\u0e01\u0e31\u0e19\u0e41\u0e19\u0e48 \u0e40\u0e23\u0e32\u0e08\u0e36\u0e07\u0e04\u0e49\u0e19\u0e04\u0e27\u0e49\u0e32\u0e21\u0e31\u0e19\u0e40\u0e1e\u0e34\u0e48\u0e21\u0e40\u0e15\u0e34\u0e21\n\u0e2a\u0e34\u0e48\u0e07\u0e17\u0e35\u0e48\u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e40\u0e23\u0e35\u0e22\u0e19\u0e23\u0e39\u0e49\n\u0e23\u0e31\u0e1a\u0e23\u0e39\u0e49\u0e40\u0e23\u0e37\u0e48\u0e2d\u0e07\u0e01\u0e32\u0e23\u0e21\u0e35\u0e2d\u0e22\u0e39\u0e48\u0e02\u0e2d\u0e07 MOF (\u0e44\u0e21\u0e48\u0e43\u0e0a\u0e48 UFO \u0e19\u0e30 \u0e14\u0e39\u0e1c\u0e48\u0e32\u0e19\u0e46\u0e21\u0e31\u0e19\u0e01\u0e47\u0e40\u0e2b\u0e21\u0e37\u0e2d\u0e19\u0e2d\u0e22\u0e39\u0e48\u0e19\u0e30)\n\u0e44\u0e14\u0e49\u0e41\u0e25\u0e01\u0e40\u0e1b\u0e25\u0e35\u0e48\u0e22\u0e19\u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49\u0e43\u0e19\u0e17\u0e35\u0e21\n\u0e44\u0e14\u0e49\u0e43\u0e0a\u0e49\u0e40\u0e27\u0e25\u0e32\u0e17\u0e35\u0e48\u0e40\u0e2b\u0e25\u0e37\u0e2d\u0e2d\u0e22\u0e39\u0e48\u0e2d\u0e22\u0e48\u0e32\u0e07\u0e04\u0e38\u0e49\u0e21\u0e04\u0e48\u0e32\n\u0e44\u0e14\u0e49\u0e43\u0e0a\u0e49\u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49\u0e17\u0e35\u0e48\u0e21\u0e35\u0e21\u0e32\u0e17\u0e14\u0e2a\u0e2d\u0e1a\u0e2a\u0e19\u0e32\u0e21\n\u0e44\u0e14\u0e49\u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49\u0e08\u0e32\u0e01 \" \u0e17\u0e35\u0e21\u0e2d\u0e37\u0e48\u0e19 \" 555\n\u0e17\u0e35\u0e21\u0e02\u0e2d\u0e07\u0e40\u0e23\u0e32\u0e08\u0e30\u0e17\u0e33\u0e2d\u0e30\u0e44\u0e23\u0e15\u0e48\u0e2d\n\u0e08\u0e23\u0e34\u0e07\u0e46\u0e17\u0e35\u0e48\u0e40\u0e02\u0e35\u0e22\u0e19\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\u0e40\u0e1e\u0e23\u0e32\u0e30\u0e27\u0e48\u0e32\u0e2d\u0e22\u0e32\u0e01\u0e43\u0e2b\u0e49\u0e2d\u0e48\u0e32\u0e19\u0e01\u0e31\u0e19\u0e07\u0e48\u0e32\u0e22\u0e46 \u0e41\u0e25\u0e49\u0e27 in \u0e44\u0e1b\u0e01\u0e31\u0e1a\u0e01\u0e32\u0e23\u0e41\u0e02\u0e48\u0e07\u0e02\u0e31\u0e19\u0e19\u0e30\u0e04\u0e23\u0e31\u0e1a \u0e2a\u0e48\u0e27\u0e19\u0e2a\u0e34\u0e48\u0e07\u0e17\u0e35\u0e48\u0e08\u0e30\u0e17\u0e33\u0e15\u0e48\u0e2d\u0e44\u0e1b\u0e04\u0e37\u0e2d \u0e19\u0e2d\u0e19\u0e43\u0e2b\u0e49\u0e1e\u0e2d\u0e01\u0e34\u0e19 \u0e2d\u0e32\u0e2b\u0e32\u0e23\u0e43\u0e2b\u0e49\u0e04\u0e23\u0e1a 5 \u0e2b\u0e21\u0e39\u0e48\u0e04\u0e23\u0e31\u0e1a\nOFFICIAL\nInspiration\nIt cannot be denied that global warming issue is one of the biggest trends these days. With higher technology & bigger data people have, the higher chance of finding ways to help delaying world temperature. Carbon capture through Metal Organic Framework (MOF) are one of the technologies that can help this situation. We believe that by using Machine Learning with suitable method can help us understand and predict carbon capture capability of each MOF to find the most effective compound.\nWhat it does\nThe created model will predict CO2 working capacity based on given parameters by following methods:\nXGBoost: the method will correct model itself every data shuffle; therefore, fitting data can be done quickly.\nRandom forest: the method will random decision tree and vote the best solution to reduce over-fitting issue.\nNeural Network: the method is to simplify the complexity of correlation and select related parameters to create the model.\nSymbolic regression (Genetic programming): the method will create empirical equation from raw data which helps to explain in mathematical perspective.\nVoting Regressor: the method will vote the best method that we selected (Gradient boosting, Random Forest, and Linear regression) to apply on each situation to find the best outcomes.\nHow we built it\nExploratory data Analysis\nTo check data type & Missing value.\nCorrelation\nData cleaning & preparation\nDrop missing value and incorrect data.\nFeatures selection (Combining both correlation and chemistry theoretical knowledge)\nTransform category data by One-Hot encoder\nModelling\nSplit data to be train & test set\nTraining model with various algorithm/technique such as XGBoost, Randomforest, Neural Network, Genetic programming and Voting Regressor (Gradient boosting, Random Forest and Linear regression).\nModel validation by using R2 (linear regression) & Log of mean absolute error (MAE)\nModel selection\n-Choose model by Evaluate from mean square error\nChallenges we ran into\nOur team member has no background in pure chemistry and MOFs. However, resources provided by the competition organizer is sufficient but time consuming in a short period. Thus, we\u2019ve separated responsibility for our team members who has background in chemical engineering and data science to be domain knowledge understanding about MOFs and machine learning modelling respectively.\nDue to our time constraint (Most of our team member are working full time), thus we could manage to have a group discussion about 1-2 times a week while we see working as a team is more powerful and effective.\nModelling\n3.1 Features selection: we managed to apply both chemistry of MOFs and statistical knowledge to understand how each feature affect CO2 working capacity.\n3.2 Slow find tune model because there is limited time.\nAccomplishments that we're proud of\nwhat to be proud of\nIt took us 1 day to create and tune the model. Usually, it takes longer time for this process.\nHow can we know if any parameters should be selected as features for our model with little(zero) understanding about MOF? So, we had researched to understand it more (at least better than zero \ud83d\ude0a and we are more confident).\nWhat we learned\nWe have learned a lot about MOFs and opportunity for applying machine learning in chemistry.\nExchanging knowledge in the team\nMake the most of your remaining time.\nUse the knowledge gained to test the field.\nGain knowledge from other teams.\nWhat's next for Wonderland\nParticipating in this competition helps our team to learn more about the MOF in Chemistry point of view as well as Machine learning research opportunity. As the competition goal is to find the best prediction of CO2 working capacity, \u201cdata cleansing\u201d is a crucial step to filter out unrelated parameters from the model. This process requires high-level knowledge in Chemistry to make sure that the model is applicable per theoretical reference. In our case, conduct an analysis on functional group and found that some of them have high correlation with CO2 working capacity while others have not. Thus, we strongly believe that statical analysis can help to identify potential parameters (both chemical & physical properties) which impact CO2 working capacity. After achieving competition goal, we saw an opportunity to use Machine learning to design MOF in variety of ways through selection of organic likers, metal node, topology and functional group since all input are provided.", "link": "https://devpost.com/software/wonderland-6hl953", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "\u0e04\u0e33\u0e40\u0e15\u0e37\u0e2d\u0e19 \u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\u0e2d\u0e48\u0e32\u0e19\u0e40\u0e2d\u0e32\u0e21\u0e31\u0e19!!!!!\n\u0e41\u0e23\u0e07\u0e1a\u0e31\u0e19\u0e14\u0e32\u0e25\u0e43\u0e08\n\u0e17\u0e32\u0e07\u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e21\u0e35\u0e04\u0e27\u0e32\u0e21\u0e2a\u0e19\u0e43\u0e08\u0e01\u0e32\u0e23\u0e1b\u0e23\u0e30\u0e22\u0e38\u0e04\u0e43\u0e0a\u0e49 ml \u0e2b\u0e23\u0e37\u0e2d machine learning \u0e23\u0e48\u0e27\u0e21\u0e01\u0e31\u0e1a \u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e1b\u0e23\u0e30\u0e40\u0e20\u0e17\u0e15\u0e48\u0e32\u0e07\u0e46 \u0e01\u0e32\u0e23\u0e17\u0e35\u0e48\u0e21\u0e35 tmlcc \u0e40\u0e01\u0e34\u0e14\u0e02\u0e36\u0e49\u0e19\u0e01\u0e47\u0e40\u0e2b\u0e21\u0e37\u0e2d\u0e19\u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e17\u0e48\u0e2d\u0e07\u0e22\u0e38\u0e17\u0e18\u0e08\u0e31\u0e01\u0e23 \u0e40\u0e1b\u0e34\u0e14\u0e2b\u0e39\u0e40\u0e1b\u0e34\u0e14\u0e15\u0e32\u0e23\u0e31\u0e1a\u0e23\u0e39\u0e49\u0e27\u0e48\u0e32 mofs \u0e21\u0e31\u0e19\u0e04\u0e37\u0e2d\u0e2d\u0e30\u0e44\u0e23 \u0e41\u0e25\u0e30 \u0e40\u0e23\u0e32\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e1b\u0e23\u0e30\u0e22\u0e38\u0e04 ml\u0e43\u0e0a\u0e49\u0e43\u0e19\u0e2a\u0e48\u0e27\u0e19\u0e44\u0e2b\u0e19\u0e44\u0e14\u0e49\u0e1a\u0e49\u0e32\u0e07\n\u0e17\u0e33\u0e2d\u0e30\u0e44\u0e23\u0e1a\u0e49\u0e32\u0e07\n\u0e40\u0e2d\u0e32\u0e08\u0e23\u0e34\u0e07\u0e46\u0e21\u0e31\u0e19\u0e01\u0e47 data science process \u0e18\u0e23\u0e23\u0e21\u0e14\u0e32\u0e44\u0e21\u0e48\u0e2b\u0e27\u0e37\u0e2d\u0e2b\u0e27\u0e32\u0e2d\u0e30\u0e44\u0e23 \u0e41\u0e15\u0e48\u0e17\u0e35\u0e48\u0e2a\u0e33\u0e04\u0e31\u0e0d\u0e15\u0e49\u0e2d\u0e07\u0e23\u0e39\u0e49\u0e27\u0e48\u0e32 \u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e21\u0e32\u0e40\u0e1b\u0e47\u0e19\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e41\u0e1a\u0e1a\u0e44\u0e2b\u0e19 \u0e40\u0e01\u0e34\u0e14 loss \u0e44\u0e2b\u0e21 correlation \u0e2b\u0e23\u0e37\u0e2d \u0e2a\u0e31\u0e21\u0e1e\u0e31\u0e19\u0e18\u0e4c\u0e01\u0e31\u0e19\u0e22\u0e31\u0e07\u0e44\u0e07 \u0e01\u0e48\u0e2d\u0e19\u0e40\u0e02\u0e49\u0e32 model \u0e43\u0e0a\u0e48\u0e1b\u0e30 \u0e15\u0e2d\u0e19\u0e40\u0e25\u0e37\u0e2d\u0e01\u0e42\u0e21\u0e40\u0e14\u0e25 \u0e04\u0e34\u0e14\u0e44\u0e23\u0e2d\u0e2d\u0e01\u0e01\u0e47 run \u0e44\u0e1b\u0e01\u0e48\u0e2d\u0e19 \u0e41\u0e25\u0e49\u0e27\u0e16\u0e49\u0e32\u0e04\u0e34\u0e14\u0e27\u0e48\u0e32\u0e21\u0e31\u0e19\u0e14\u0e35\u0e01\u0e47\n\u201c tune \u0e2a\u0e34\u0e04\u0e23\u0e31\u0e1a\u0e1e\u0e35\u0e48\u0e19\u0e49\u0e2d\u0e07\u201d\n\u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e01\u0e47\u0e44\u0e1b validation \u0e04\u0e23\u0e31\u0e1a \u0e14\u0e35\u0e01\u0e47\u0e2a\u0e48\u0e07\u0e44\u0e21\u0e48\u0e14\u0e35 \u0e01\u0e47 tune \u0e15\u0e48\u0e2d\u0e41\u0e2b\u0e21\u0e48 \u2026.. \u0e41\u0e25\u0e49\u0e27 \u0e15\u0e2d\u0e19\u0e2a\u0e48\u0e07\u0e01\u0e47\u0e41\u0e2b\u0e21\u0e48 \"failed \u0e40\u0e1b\u0e47\u0e19\u0e15\u0e31\u0e1a\u0e08\u0e49\u0e32\"\n\u0e42\u0e14\u0e22\u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32\u0e17\u0e33\u0e01\u0e32\u0e23\u0e19\u0e31\u0e48\u0e07\u0e19\u0e36\u0e01\u0e27\u0e48\u0e32\u0e43\u0e0a\u0e49 model \u0e2d\u0e30\u0e44\u0e23\u0e14\u0e35 \u0e08\u0e19\u0e01\u0e23\u0e30\u0e17\u0e31\u0e48\u0e07\u0e04\u0e34\u0e14\u0e46\u0e27\u0e48\u0e32\u0e17\u0e33\u0e46\u0e44\u0e1b\u0e15\u0e32\u0e21\u0e04\u0e27\u0e32\u0e21\u0e16\u0e19\u0e31\u0e14\u0e19\u0e35\u0e48\u0e41\u0e2b\u0e25\u0e30 \u0e04\u0e37\u0e2d\nxgboost: \u0e42\u0e14\u0e22\u0e2a\u0e48\u0e27\u0e19\u0e15\u0e31\u0e27 tune \u0e07\u0e48\u0e32\u0e22 \u0e2b\u0e32\u0e04\u0e33\u0e15\u0e2d\u0e1a\u0e40\u0e23\u0e47\u0e27\u0e40\u0e23\u0e48\u0e07\u0e40\u0e04\u0e23\u0e37\u0e48\u0e2d\u0e07\u0e44\u0e14\u0e49\u0e14\u0e35 \u0e23\u0e39\u0e49\u0e27\u0e48\u0e32 feature \u0e44\u0e2b\u0e19\u0e2a\u0e33\u0e04\u0e31\u0e0d\u0e44\u0e21\u0e48\u0e2a\u0e33\u0e04\u0e31\u0e0d\nrandom forest: \u0e21\u0e31\u0e19\u0e04\u0e37\u0e2d\u0e01\u0e32\u0e23\u0e2a\u0e38\u0e48\u0e21\u0e15\u0e49\u0e19\u0e44\u0e21\u0e49 (decision -----> tree !!! ) \u0e15\u0e32\u0e21\u0e08\u0e33\u0e19\u0e27\u0e19\u0e17\u0e35\u0e48\u0e15\u0e49\u0e2d\u0e07\u0e01\u0e32\u0e23 \u0e2a\u0e38\u0e48\u0e21\u0e2d\u0e22\u0e39\u0e48\u0e19\u0e31\u0e48\u0e19\u0e41\u0e2b\u0e25\u0e30\u0e08\u0e19\u0e40\u0e15\u0e47\u0e21\u0e1b\u0e48\u0e32 (\u0e15\u0e32\u0e21\u0e0a\u0e37\u0e48\u0e2d) \u0e41\u0e25\u0e49\u0e27\u0e01\u0e47\u0e40\u0e2d\u0e32\u0e15\u0e49\u0e19\u0e40\u0e17\u0e1e\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14\u0e43\u0e19\u0e1b\u0e48\u0e32\u0e2d\u0e2d\u0e01\u0e21\u0e32 \u2026. \u201c\u0e19\u0e35\u0e48\u0e2a\u0e34\u0e19\u0e30\u0e17\u0e35\u0e48\u0e40\u0e23\u0e35\u0e22\u0e01\u0e27\u0e48\u0e32 \u0e2a\u0e38\u0e14\u0e43\u0e19\u0e1b\u0e48\u0e32\u201d\nneural network: \u0e42\u0e21\u0e40\u0e14\u0e25\u0e23\u0e38\u0e48\u0e19\u0e43\u0e2b\u0e21\u0e48 \u0e44\u0e17\u0e22\u0e19\u0e34\u0e22\u0e21 (\u0e15\u0e48\u0e32\u0e07\u0e1b\u0e23\u0e30\u0e40\u0e17\u0e28\u0e01\u0e47\u0e19\u0e34\u0e22\u0e21) \u0e41\u0e15\u0e48\u0e27\u0e48\u0e32\u0e15\u0e49\u0e2d\u0e07\u0e43\u0e2b\u0e49\u0e40\u0e27\u0e25\u0e32\u0e21\u0e31\u0e19\u0e40\u0e22\u0e2d\u0e30\u0e2b\u0e19\u0e48\u0e2d\u0e22\u0e16\u0e49\u0e32\u0e21\u0e31\u0e19 tune \u0e42\u0e04\u0e23\u0e07\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e40\u0e08\u0e2d\u0e19\u0e30 ml \u0e18\u0e23\u0e23\u0e21\u0e14\u0e32 \"tune 3 \u0e0a\u0e32\u0e15\u0e34\u0e01\u0e47\u0e44\u0e21\u0e48\u0e2a\u0e39\u0e49\"\nsymbolic regression (genetic programming):\u0e42\u0e21\u0e40\u0e14\u0e25\u0e19\u0e35\u0e49\u0e21\u0e31\u0e19\u0e40\u0e0b\u0e47\u0e17 rules \u0e2b\u0e23\u0e37\u0e2d \u0e2a\u0e21\u0e01\u0e32\u0e23 \u0e21\u0e32\u0e01\u0e48\u0e2d\u0e19 \u0e17\u0e33\u0e43\u0e2b\u0e49\u0e40\u0e23\u0e32\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e23\u0e39\u0e49\u0e44\u0e14\u0e49\u0e27\u0e48\u0e32 rules \u0e17\u0e35\u0e48\u0e43\u0e2b\u0e49\u0e44\u0e1b\u0e21\u0e31\u0e19 sig \u0e44\u0e2b\u0e21 \u0e01\u0e31\u0e1a data\nvoting regressor: \u0e07\u0e48\u0e32\u0e22\u0e46\u0e21\u0e31\u0e19\u0e04\u0e37\u0e2d\u0e01\u0e32\u0e23\u0e40\u0e2d\u0e32 model \u0e17\u0e35\u0e48\u0e04\u0e34\u0e14\u0e27\u0e48\u0e32 \"\u0e08\u0e30\u0e41\u0e21\u0e48\u0e19\" \u0e43\u0e19\u0e17\u0e35\u0e48\u0e19\u0e35\u0e49\u0e40\u0e23\u0e32\u0e40\u0e25\u0e37\u0e2d\u0e01\u0e43\u0e0a\u0e49 gradient boosting, random forest, and linear regression \u0e21\u0e32\u0e17\u0e33\u0e01\u0e32\u0e23 vote \u0e27\u0e48\u0e32 model \u0e44\u0e2b\u0e19\u0e21\u0e31\u0e19\u0e04\u0e48\u0e32\u0e43\u0e01\u0e25\u0e49\u0e46\u0e01\u0e31\u0e1a\u0e0a\u0e38\u0e14\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e01\u0e47\u0e43\u0e2b\u0e49\u0e19\u0e49\u0e33\u0e2b\u0e19\u0e31\u0e01\u0e2b\u0e23\u0e37\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e2a\u0e33\u0e04\u0e31\u0e0dmodel\u0e19\u0e31\u0e49\u0e19\u0e21\u0e32\u0e01\u0e2b\u0e19\u0e48\u0e2d\u0e22 \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e01\u0e47\u0e40\u0e2d\u0e32\u0e04\u0e48\u0e32\u0e17\u0e35\u0e48\u0e17\u0e32\u0e22\u0e43\u0e19 model \u0e2d\u0e37\u0e48\u0e19\u0e46\u0e21\u0e32 \u0e40\u0e09\u0e25\u0e35\u0e48\u0e22\u0e16\u0e48\u0e27\u0e07\u0e19\u0e49\u0e33\u0e2b\u0e19\u0e31\u0e01\u0e01\u0e31\u0e19\u0e08\u0e19\u0e44\u0e14\u0e49\u0e04\u0e48\u0e32 \u0e17\u0e33\u0e19\u0e32\u0e22\u0e43\u0e2b\u0e21\u0e48\n\u0e04\u0e27\u0e32\u0e21\u0e17\u0e49\u0e32\u0e17\u0e32\u0e22\u0e17\u0e35\u0e48\u0e40\u0e23\u0e32\u0e1e\u0e1a\u0e40\u0e08\u0e2d\n\u0e14\u0e49\u0e27\u0e22\u0e40\u0e27\u0e25\u0e32\u0e17\u0e35\u0e48\u0e01\u0e25\u0e38\u0e48\u0e21\u0e19\u0e31\u0e49\u0e19\u0e17\u0e33\u0e07\u0e32\u0e19\u0e1b\u0e23\u0e30\u0e08\u0e33\u0e01\u0e31\u0e19\u0e40\u0e1b\u0e47\u0e19\u0e2a\u0e48\u0e27\u0e19\u0e43\u0e2b\u0e0d\u0e48 (\u0e41\u0e15\u0e48\u0e1c\u0e39\u0e49\u0e40\u0e02\u0e35\u0e22\u0e19 \u0e40\u0e1b\u0e47\u0e19 freelance \u0e2d\u0e22\u0e39\u0e48\u0e19\u0e30 \u201c\u0e08\u0e49\u0e32\u0e07\u0e44\u0e14\u0e49\u0e09\u0e31\u0e19\u0e2b\u0e34\u0e27\u201d) \u0e17\u0e33\u0e43\u0e2b\u0e49\u0e1b\u0e23\u0e30\u0e0a\u0e38\u0e21\u0e01\u0e31\u0e19\u0e19\u0e49\u0e2d\u0e22 \u0e17\u0e33\u0e43\u0e2b\u0e49\u0e21\u0e35\u0e40\u0e27\u0e25\u0e32\u0e43\u0e19\u0e01\u0e32\u0e23\u0e17\u0e33\u0e19\u0e49\u0e2d\u0e22 \u0e1e\u0e2d\u0e23\u0e39\u0e49\u0e27\u0e48\u0e32\u0e43\u0e01\u0e25\u0e49\u0e2b\u0e21\u0e14\u0e0a\u0e48\u0e27\u0e07\u0e40\u0e27\u0e25\u0e32\u0e41\u0e02\u0e48\u0e07\u0e01\u0e47\u0e04\u0e48\u0e2d\u0e22\u0e21\u0e32\u0e14\u0e39\u0e04\u0e25\u0e34\u0e1b\u0e22\u0e49\u0e2d\u0e19\u0e2b\u0e25\u0e31\u0e07\u0e17\u0e33\u0e43\u0e2b\u0e49\u0e23\u0e39\u0e49\u0e27\u0e48\u0e32\u0e21\u0e31\u0e19\u0e40\u0e22\u0e2d\u0e30\u0e21\u0e32\u0e01\u0e01\u0e01\u0e01\u0e01 \u0e1e\u0e2d\u0e44\u0e1f\u0e40\u0e23\u0e34\u0e48\u0e21\u0e25\u0e32\u0e21\u0e41\u0e25\u0e49\u0e27\u0e40\u0e23\u0e32\u0e08\u0e36\u0e07\u0e21\u0e35\u0e44\u0e1f\u0e43\u0e19\u0e01\u0e32\u0e23\u0e17\u0e33\u0e17\u0e31\u0e19\u0e17\u0e35 submit \u0e17\u0e33\u0e43\u0e2b\u0e49\u0e21\u0e35\u0e40\u0e27\u0e25\u0e32\u0e17\u0e33 feature selection \u0e19\u0e49\u0e2d\u0e22\u0e21\u0e32\u0e01\u0e08\u0e23\u0e34\u0e07\u0e46\u0e41\u0e25\u0e49\u0e27\u0e40\u0e23\u0e32\u0e21\u0e35\u0e2d\u0e30\u0e44\u0e23\u0e2b\u0e25\u0e32\u0e22\u0e46\u0e2d\u0e22\u0e48\u0e32\u0e07\u0e2d\u0e22\u0e32\u0e01\u0e43\u0e2a\u0e48\u0e40\u0e02\u0e49\u0e32\u0e44\u0e1b\u0e2d\u0e35\u0e01\u0e40\u0e22\u0e2d\u0e30\u0e41\u0e15\u0e48\u0e01\u0e47\u0e01\u0e25\u0e31\u0e27\u0e44\u0e21\u0e48\u0e17\u0e31\u0e19\u0e40\u0e1e\u0e23\u0e32\u0e30\u0e27\u0e48\u0e32 \u0e17\u0e35\u0e48 run model \u0e40\u0e04\u0e23\u0e37\u0e48\u0e2d\u0e07\u0e01\u0e47 \u0e23\u0e49\u0e2d\u0e07\u0e42\u0e2d\u0e14\u0e23\u0e49\u0e2d\u0e07\u0e42\u0e2d\u0e22\u0e41\u0e25\u0e49\u0e27\n\u0e2a\u0e34\u0e48\u0e07\u0e17\u0e35\u0e48\u0e20\u0e39\u0e21\u0e34\u0e43\u0e08\n\u0e40\u0e23\u0e32\u0e43\u0e0a\u0e49\u0e40\u0e27\u0e25\u0e32\u0e17\u0e33 model \u0e23\u0e27\u0e21\u0e16\u0e36\u0e07 clean data \u0e1e\u0e23\u0e49\u0e2d\u0e21\u0e17\u0e31\u0e49\u0e07 tune \u0e40\u0e2a\u0e23\u0e47\u0e08\u0e14\u0e49\u0e27\u0e22 \u0e20\u0e32\u0e22\u0e43\u0e19\u0e40\u0e27\u0e25\u0e32 1 \u0e27\u0e31\u0e19 \u0e19\u0e34\u0e14\u0e46 \u0e42\u0e14\u0e22\u0e1b\u0e01\u0e15\u0e34\u0e17\u0e33\u0e19\u0e32\u0e19\u0e01\u0e27\u0e48\u0e32\u0e19\u0e35\u0e49\n\u0e08\u0e32\u0e01\u0e04\u0e27\u0e32\u0e21\u0e16\u0e49\u0e32\u0e17\u0e32\u0e22\u0e40\u0e23\u0e37\u0e48\u0e2d\u0e07 feature \u0e43\u0e19\u0e14\u0e49\u0e32\u0e19 mof \u0e40\u0e1b\u0e47\u0e19 0 \u0e17\u0e33\u0e43\u0e2b\u0e49 \u0e44\u0e21\u0e48\u0e23\u0e39\u0e49\u0e27\u0e48\u0e32\u0e15\u0e49\u0e2d\u0e07\u0e15\u0e31\u0e14\u0e15\u0e31\u0e27\u0e44\u0e2b\u0e19\u0e2b\u0e23\u0e37\u0e2d\u0e43\u0e0a\u0e48\u0e04\u0e48\u0e32\u0e44\u0e2b\u0e19\u0e01\u0e31\u0e19\u0e41\u0e19\u0e48 \u0e40\u0e23\u0e32\u0e08\u0e36\u0e07\u0e04\u0e49\u0e19\u0e04\u0e27\u0e49\u0e32\u0e21\u0e31\u0e19\u0e40\u0e1e\u0e34\u0e48\u0e21\u0e40\u0e15\u0e34\u0e21\n\u0e2a\u0e34\u0e48\u0e07\u0e17\u0e35\u0e48\u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e40\u0e23\u0e35\u0e22\u0e19\u0e23\u0e39\u0e49\n\u0e23\u0e31\u0e1a\u0e23\u0e39\u0e49\u0e40\u0e23\u0e37\u0e48\u0e2d\u0e07\u0e01\u0e32\u0e23\u0e21\u0e35\u0e2d\u0e22\u0e39\u0e48\u0e02\u0e2d\u0e07 mof (\u0e44\u0e21\u0e48\u0e43\u0e0a\u0e48 ufo \u0e19\u0e30 \u0e14\u0e39\u0e1c\u0e48\u0e32\u0e19\u0e46\u0e21\u0e31\u0e19\u0e01\u0e47\u0e40\u0e2b\u0e21\u0e37\u0e2d\u0e19\u0e2d\u0e22\u0e39\u0e48\u0e19\u0e30)\n\u0e44\u0e14\u0e49\u0e41\u0e25\u0e01\u0e40\u0e1b\u0e25\u0e35\u0e48\u0e22\u0e19\u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49\u0e43\u0e19\u0e17\u0e35\u0e21\n\u0e44\u0e14\u0e49\u0e43\u0e0a\u0e49\u0e40\u0e27\u0e25\u0e32\u0e17\u0e35\u0e48\u0e40\u0e2b\u0e25\u0e37\u0e2d\u0e2d\u0e22\u0e39\u0e48\u0e2d\u0e22\u0e48\u0e32\u0e07\u0e04\u0e38\u0e49\u0e21\u0e04\u0e48\u0e32\n\u0e44\u0e14\u0e49\u0e43\u0e0a\u0e49\u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49\u0e17\u0e35\u0e48\u0e21\u0e35\u0e21\u0e32\u0e17\u0e14\u0e2a\u0e2d\u0e1a\u0e2a\u0e19\u0e32\u0e21\n\u0e44\u0e14\u0e49\u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49\u0e08\u0e32\u0e01 \" \u0e17\u0e35\u0e21\u0e2d\u0e37\u0e48\u0e19 \" 555\n\u0e17\u0e35\u0e21\u0e02\u0e2d\u0e07\u0e40\u0e23\u0e32\u0e08\u0e30\u0e17\u0e33\u0e2d\u0e30\u0e44\u0e23\u0e15\u0e48\u0e2d\n\u0e08\u0e23\u0e34\u0e07\u0e46\u0e17\u0e35\u0e48\u0e40\u0e02\u0e35\u0e22\u0e19\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\u0e40\u0e1e\u0e23\u0e32\u0e30\u0e27\u0e48\u0e32\u0e2d\u0e22\u0e32\u0e01\u0e43\u0e2b\u0e49\u0e2d\u0e48\u0e32\u0e19\u0e01\u0e31\u0e19\u0e07\u0e48\u0e32\u0e22\u0e46 \u0e41\u0e25\u0e49\u0e27 in \u0e44\u0e1b\u0e01\u0e31\u0e1a\u0e01\u0e32\u0e23\u0e41\u0e02\u0e48\u0e07\u0e02\u0e31\u0e19\u0e19\u0e30\u0e04\u0e23\u0e31\u0e1a \u0e2a\u0e48\u0e27\u0e19\u0e2a\u0e34\u0e48\u0e07\u0e17\u0e35\u0e48\u0e08\u0e30\u0e17\u0e33\u0e15\u0e48\u0e2d\u0e44\u0e1b\u0e04\u0e37\u0e2d \u0e19\u0e2d\u0e19\u0e43\u0e2b\u0e49\u0e1e\u0e2d\u0e01\u0e34\u0e19 \u0e2d\u0e32\u0e2b\u0e32\u0e23\u0e43\u0e2b\u0e49\u0e04\u0e23\u0e1a 5 \u0e2b\u0e21\u0e39\u0e48\u0e04\u0e23\u0e31\u0e1a\nofficial\ninspiration\nit cannot be denied that global warming issue is one of the biggest trends these days. with higher technology & bigger data people have, the higher chance of finding ways to help delaying world temperature. carbon capture through metal organic framework (mof) are one of the technologies that can help this situation. we believe that by using machine learning with suitable method can help us understand and predict carbon capture capability of each mof to find the most effective compound.\nwhat it does\nthe created model will predict co2 working capacity based on given parameters by following methods:\nxgboost: the method will correct model itself every data shuffle; therefore, fitting data can be done quickly.\nrandom forest: the method will random decision tree and vote the best solution to reduce over-fitting issue.\nneural network: the method is to simplify the complexity of correlation and select related parameters to create the model.\nsymbolic regression (genetic programming): the method will create empirical equation from raw data which helps to explain in mathematical perspective.\nvoting regressor: the method will vote the best method that we selected (gradient boosting, random forest, and linear regression) to apply on each situation to find the best outcomes.\nhow we built it\nexploratory data analysis\nto check data type & missing value.\ncorrelation\ndata cleaning & preparation\ndrop missing value and incorrect data.\nfeatures selection (combining both correlation and chemistry theoretical knowledge)\ntransform category data by one-hot encoder\nmodelling\nsplit data to be train & test set\ntraining model with various algorithm/technique such as xgboost, randomforest, neural network, genetic programming and voting regressor (gradient boosting, random forest and linear regression).\nmodel validation by using r2 (linear regression) & log of mean absolute error (mae)\nmodel selection\n-choose model by evaluate from mean square error\nchallenges we ran into\nour team member has no background in pure chemistry and mofs. however, resources provided by the competition organizer is sufficient but time consuming in a short period. thus, we\u2019ve separated responsibility for our team members who has background in chemical engineering and data science to be domain knowledge understanding about mofs and machine learning modelling respectively.\ndue to our time constraint (most of our team member are working full time), thus we could manage to have a group discussion about 1-2 times a week while we see working as a team is more powerful and effective.\nmodelling\n3.1 features selection: we managed to apply both chemistry of mofs and statistical knowledge to understand how each feature affect co2 working capacity.\n3.2 slow find tune model because there is limited time.\naccomplishments that we're proud of\nwhat to be proud of\nit took us 1 day to create and tune the model. usually, it takes longer time for this process.\nhow can we know if any parameters should be selected as features for our model with little(zero) understanding about mof? so, we had researched to understand it more (at least better than zero \ud83d\ude0a and we are more confident).\nwhat we learned\nwe have learned a lot about mofs and opportunity for applying machine learning in chemistry.\nexchanging knowledge in the team\nmake the most of your remaining time.\nuse the knowledge gained to test the field.\ngain knowledge from other teams.\nwhat's next for wonderland\nparticipating in this competition helps our team to learn more about the mof in chemistry point of view as well as machine learning research opportunity. as the competition goal is to find the best prediction of co2 working capacity, \u201cdata cleansing\u201d is a crucial step to filter out unrelated parameters from the model. this process requires high-level knowledge in chemistry to make sure that the model is applicable per theoretical reference. in our case, conduct an analysis on functional group and found that some of them have high correlation with co2 working capacity while others have not. thus, we strongly believe that statical analysis can help to identify potential parameters (both chemical & physical properties) which impact co2 working capacity. after achieving competition goal, we saw an opportunity to use machine learning to design mof in variety of ways through selection of organic likers, metal node, topology and functional group since all input are provided.", "sortedWord": "None", "removed": "Nan", "score": 26, "comments": 5, "media": null, "medialink": null, "identifyer": 59507283}, {"Unnamed: 0": 7367, "autor": "Plantara", "date": null, "content": "Inspiration\nI was inspired as I love animals and spending time in nature and this idea made me realize and research the different problems ongoing in the world that are hurting these animals and life on Earth. So, I wanted to contribute something on my behalf to help a cause that is directly connected with what's affecting them as I saw many people hurting the environment, but also countless saving it and I wanted to be a part of them.\nWhat it does\nPlantara is a way to derive clean and green energy for the world's use as it could help the many environmental concerns that exist today especially the carbon emission and pollution causes. It uses the microbial fuel cell technology to gain energy from a plant or tree and convert it into electricity without affecting the plant's growth in any way.\nHow we built it\nPlantara is a solution that can surely encourage change. This is how it works. This innovation is heavily dependent on the concept of organic matter. Organic matter refers to the large source of carbon-based compounds that come from the remains of plants, animals, and their waste products in the environment. Plants excrete organic matter into the soil, which is then broken down by bacteria. During this breakdown process, electrons are being released as a waste product of the bacteria living around plant roots. It is possible to harvest those electrons using inert electrodes to turn them into electricity, without affecting the plant\u2019s growth. Precious metals, mercury, and carbon are typically used as inert electrodes. For example, zinc could be the anode and copper could be the cathode. These two materials would be connected with wires to run the current of electricity, and that would make up the microbial fuel cell part within Plantara. Next, the energy harvester would take that energy and convert it into electricity suitable for use. An example of an energy harvester that could be used as a part for Plantara would be the Microgen energy harvesting converter. The boost converter connected underneath the energy harvester would be used to \u201cstep up\u201d the voltage of the input current to its output current. A boost converter is a DC to DC (which is a voltage source) converter with an output voltage greater than the source voltage. This is a very important part of the design as the increase in voltage can allow an object that requires more power to be used. For the boost converter to work, though, an energy storage element needs to be directly connected. In this case, that is the supercapacitor. A supercapacitor is similar to a battery except it is faster when transferring the current to an external source like a low-power sensor. Not only that, but they charge and store energy at a faster rate than batteries and have a longer lifespan. This means that if Plantara were to be used throughout the world, then around 3 trillion trees could produce approximately 75 billion kWh of electricity per day, which could cover the energy consumption of about 1.2 billion people. This would be a major step toward the era of cleaner energy and advanced technologies. To make this more scalable, if a country like Russia, which contains about 21% of the world\u2019s trees with about 642 billion trees, were to use this technology on as many trees as possible it could cover about 5% of the entire world\u2019s energy needs.\nChallenges we ran into\nI ran into challenges finding the right metal for the electrodes and doubting whether this method would provide enough electricity to actually run something or it would be too light.\nAccomplishments that we're proud of\nI am proud that we tried to contribute to the environment in some way and achieve a method that could be the part of the future of clean and green energy.\nWhat we learned\nI learned a lot about the renewable energy industry and how necessary it is for us to make a transition because the Earth is suffering in so many ways and it is our job to protect our and many other living things' home.\nWhat's next for Plantara\nI would love to take this idea further to build a prototype and research further on how to implement this idea, so it could, as soon as possible, start encouraging change.", "link": "https://devpost.com/software/plantara-5ecb0r", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\ni was inspired as i love animals and spending time in nature and this idea made me realize and research the different problems ongoing in the world that are hurting these animals and life on earth. so, i wanted to contribute something on my behalf to help a cause that is directly connected with what's affecting them as i saw many people hurting the environment, but also countless saving it and i wanted to be a part of them.\nwhat it does\nplantara is a way to derive clean and green energy for the world's use as it could help the many environmental concerns that exist today especially the carbon emission and pollution causes. it uses the microbial fuel cell technology to gain energy from a plant or -----> tree !!!  and convert it into electricity without affecting the plant's growth in any way.\nhow we built it\nplantara is a solution that can surely encourage change. this is how it works. this innovation is heavily dependent on the concept of organic matter. organic matter refers to the large source of carbon-based compounds that come from the remains of plants, animals, and their waste products in the environment. plants excrete organic matter into the soil, which is then broken down by bacteria. during this breakdown process, electrons are being released as a waste product of the bacteria living around plant roots. it is possible to harvest those electrons using inert electrodes to turn them into electricity, without affecting the plant\u2019s growth. precious metals, mercury, and carbon are typically used as inert electrodes. for example, zinc could be the anode and copper could be the cathode. these two materials would be connected with wires to run the current of electricity, and that would make up the microbial fuel cell part within plantara. next, the energy harvester would take that energy and convert it into electricity suitable for use. an example of an energy harvester that could be used as a part for plantara would be the microgen energy harvesting converter. the boost converter connected underneath the energy harvester would be used to \u201cstep up\u201d the voltage of the input current to its output current. a boost converter is a dc to dc (which is a voltage source) converter with an output voltage greater than the source voltage. this is a very important part of the design as the increase in voltage can allow an object that requires more power to be used. for the boost converter to work, though, an energy storage element needs to be directly connected. in this case, that is the supercapacitor. a supercapacitor is similar to a battery except it is faster when transferring the current to an external source like a low-power sensor. not only that, but they charge and store energy at a faster rate than batteries and have a longer lifespan. this means that if plantara were to be used throughout the world, then around 3 trillion trees could produce approximately 75 billion kwh of electricity per day, which could cover the energy consumption of about 1.2 billion people. this would be a major step toward the era of cleaner energy and advanced technologies. to make this more scalable, if a country like russia, which contains about 21% of the world\u2019s trees with about 642 billion trees, were to use this technology on as many trees as possible it could cover about 5% of the entire world\u2019s energy needs.\nchallenges we ran into\ni ran into challenges finding the right metal for the electrodes and doubting whether this method would provide enough electricity to actually run something or it would be too light.\naccomplishments that we're proud of\ni am proud that we tried to contribute to the environment in some way and achieve a method that could be the part of the future of clean and green energy.\nwhat we learned\ni learned a lot about the renewable energy industry and how necessary it is for us to make a transition because the earth is suffering in so many ways and it is our job to protect our and many other living things' home.\nwhat's next for plantara\ni would love to take this idea further to build a prototype and research further on how to implement this idea, so it could, as soon as possible, start encouraging change.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59507367}, {"Unnamed: 0": 7760, "autor": "b2f2", "date": null, "content": "Inspiration\n\u0e17\u0e35\u0e21\u0e02\u0e2d\u0e07\u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e21\u0e35\u0e04\u0e27\u0e32\u0e21\u0e2a\u0e19\u0e43\u0e08\u0e43\u0e19\u0e01\u0e32\u0e23\u0e41\u0e02\u0e48\u0e07\u0e02\u0e31\u0e19 TMLCC \u0e0b\u0e36\u0e48\u0e07\u0e42\u0e08\u0e17\u0e22\u0e4c\u0e43\u0e19\u0e1b\u0e35\u0e19\u0e35\u0e49\u0e04\u0e37\u0e2d\u0e01\u0e32\u0e23\u0e17\u0e33\u0e19\u0e32\u0e22\u0e04\u0e27\u0e32\u0e21\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e43\u0e19\u0e01\u0e32\u0e23\u0e14\u0e39\u0e14\u0e0b\u0e31\u0e1a\u0e01\u0e4a\u0e32\u0e0b\u0e02\u0e2d\u0e07 Metal-Organic Frameworks (MOFs)\n\u0e40\u0e23\u0e32\u0e44\u0e21\u0e48\u0e44\u0e14\u0e49\u0e04\u0e32\u0e14\u0e2b\u0e27\u0e31\u0e07\u0e23\u0e32\u0e07\u0e27\u0e31\u0e25\u0e2b\u0e23\u0e37\u0e2d\u0e2d\u0e30\u0e44\u0e23 \u0e2b\u0e32\u0e01\u0e41\u0e15\u0e48\u0e40\u0e1b\u0e47\u0e19\u0e40\u0e1e\u0e35\u0e22\u0e07\u0e01\u0e32\u0e23\u0e2b\u0e32\u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49 \u0e41\u0e25\u0e30\u0e2b\u0e32\u0e01\u0e34\u0e08\u0e01\u0e23\u0e23\u0e21\u0e17\u0e33\u0e23\u0e48\u0e27\u0e21\u0e01\u0e31\u0e1a\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e19\u0e46\u0e17\u0e35\u0e48\u0e1e\u0e36\u0e48\u0e07\u0e08\u0e1a\u0e21.\u0e1b\u0e25\u0e32\u0e22\u0e01\u0e31\u0e19\u0e21\u0e32\nHow we built it\n\u0e17\u0e35\u0e21\u0e40\u0e23\u0e32\u0e43\u0e0a\u0e49 python \u0e40\u0e02\u0e35\u0e22\u0e19\u0e40\u0e1b\u0e47\u0e19 .ipynb \u0e1a\u0e19 GPU machine \u0e02\u0e2d\u0e07 deepnote \u0e2a\u0e25\u0e31\u0e1a\u0e01\u0e31\u0e1a\u0e23\u0e31\u0e19\u0e1a\u0e19\u0e04\u0e2d\u0e21\u0e1e\u0e34\u0e27\u0e40\u0e15\u0e2d\u0e23\u0e4c\u0e2a\u0e48\u0e27\u0e19\u0e15\u0e31\u0e27 \u0e15\u0e25\u0e2d\u0e14\u0e01\u0e32\u0e23\u0e17\u0e33\u0e42\u0e1b\u0e23\u0e40\u0e08\u0e04\u0e04\u0e23\u0e31\u0e49\u0e07\u0e19\u0e35\u0e49\nAlong the pathway\n\u0e40\u0e19\u0e37\u0e48\u0e2d\u0e07\u0e08\u0e32\u0e01\u0e42\u0e08\u0e17\u0e22\u0e4c\u0e21\u0e35\u0e23\u0e39\u0e1b\u0e41\u0e1a\u0e1a\u0e44\u0e1b\u0e17\u0e32\u0e07 Regression model \u0e40\u0e17\u0e04\u0e19\u0e34\u0e04\u0e17\u0e32\u0e07 Machine Learning \u0e17\u0e35\u0e48\u0e40\u0e23\u0e32\u0e21\u0e2d\u0e07\u0e40\u0e1b\u0e47\u0e19\u0e2d\u0e31\u0e19\u0e14\u0e31\u0e1a\u0e41\u0e23\u0e01\u0e46\u0e08\u0e36\u0e07\u0e40\u0e1b\u0e47\u0e19 Regression Tree \u0e41\u0e15\u0e48\u0e27\u0e48\u0e32 dataset \u0e40\u0e23\u0e34\u0e48\u0e21\u0e15\u0e49\u0e19\u0e17\u0e35\u0e48\u0e43\u0e2b\u0e49\u0e21\u0e32 \u0e21\u0e35\u0e17\u0e31\u0e49\u0e07 numerical feature \u0e41\u0e25\u0e30 categorical feature\n\u0e40\u0e23\u0e32\u0e08\u0e36\u0e07\u0e44\u0e14\u0e49\u0e21\u0e2d\u0e07\u0e2b\u0e32\u0e40\u0e17\u0e04\u0e19\u0e34\u0e04\u0e17\u0e35\u0e48\u0e21\u0e35\u0e04\u0e27\u0e32\u0e21\u0e40\u0e2b\u0e21\u0e32\u0e30\u0e2a\u0e21\u0e41\u0e25\u0e30\u0e16\u0e39\u0e01\u0e2d\u0e2d\u0e01\u0e41\u0e1a\u0e1a\u0e21\u0e32\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e2a\u0e34\u0e48\u0e07\u0e40\u0e2b\u0e25\u0e48\u0e32\u0e19\u0e31\u0e49\u0e19\u0e21\u0e32\u0e01\u0e01\u0e27\u0e48\u0e32 \u0e2d\u0e48\u0e32\u0e19\u0e40\u0e1b\u0e40\u0e1b\u0e2d\u0e23\u0e4c\u0e01\u0e31\u0e19\u0e23\u0e31\u0e27\u0e46 \u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e17\u0e14\u0e25\u0e2d\u0e07\u0e44\u0e21\u0e48\u0e27\u0e48\u0e32\u0e08\u0e30\u0e40\u0e1b\u0e47\u0e19 Random Forest, Gradient Boost, Extreme Gradient Boost, \u0e41\u0e25\u0e30 Neural Network \u0e2b\u0e32\u0e42\u0e21\u0e40\u0e14\u0e25\u0e17\u0e35\u0e48\u0e21\u0e35\u0e1b\u0e23\u0e30\u0e2a\u0e34\u0e17\u0e18\u0e34\u0e20\u0e32\u0e1e\u0e2a\u0e39\u0e07.\nSupposedly the best we can do\n\u0e43\u0e19\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14\u0e01\u0e47\u0e44\u0e14\u0e49\u0e40\u0e1b\u0e47\u0e19 Light Gradient Boost Machine Regressor \u0e41\u0e25\u0e30 Catboost Regressor \u0e17\u0e35\u0e48\u0e21\u0e35\u0e1b\u0e23\u0e30\u0e2a\u0e34\u0e17\u0e18\u0e34\u0e20\u0e32\u0e1e\u0e2a\u0e39\u0e07\u0e43\u0e01\u0e25\u0e49\u0e40\u0e04\u0e35\u0e22\u0e07\u0e01\u0e31\u0e19\n\u0e40\u0e21\u0e37\u0e48\u0e2d\u0e44\u0e14\u0e49 model \u0e17\u0e35\u0e48\u0e21\u0e35\u0e1b\u0e23\u0e30\u0e2a\u0e34\u0e17\u0e18\u0e34\u0e20\u0e32\u0e1e\u0e41\u0e25\u0e49\u0e27 \u0e40\u0e23\u0e32\u0e01\u0e47\u0e44\u0e14\u0e49 optimize hyperparameter \u0e1e\u0e37\u0e49\u0e19\u0e10\u0e32\u0e19\u0e15\u0e48\u0e32\u0e07\u0e46 \u0e23\u0e31\u0e19\u0e01\u0e31\u0e19\u0e02\u0e49\u0e32\u0e21\u0e27\u0e31\u0e19\u0e02\u0e49\u0e32\u0e21\u0e04\u0e37\u0e19\u0e08\u0e19\u0e44\u0e14\u0e49\u0e42\u0e21\u0e40\u0e14\u0e25\u0e17\u0e35\u0e48\u0e40\u0e23\u0e32\u0e40\u0e0a\u0e37\u0e48\u0e2d\u0e27\u0e48\u0e32\u0e21\u0e35\u0e1b\u0e23\u0e30\u0e2a\u0e34\u0e17\u0e18\u0e34\u0e20\u0e32\u0e1e\u0e21\u0e32\u0e01\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14\nImprove Improve Improve\n\u0e2b\u0e25\u0e31\u0e07\u0e08\u0e32\u0e01\u0e15\u0e31\u0e19\u0e2d\u0e22\u0e39\u0e48\u0e1e\u0e31\u0e01\u0e2b\u0e19\u0e36\u0e48\u0e07 \u0e40\u0e23\u0e32\u0e01\u0e47\u0e21\u0e35\u0e44\u0e2d\u0e40\u0e14\u0e35\u0e22\u0e27\u0e48\u0e32\u0e1a\u0e32\u0e07\u0e17\u0e35\u0e01\u0e32\u0e23\u0e2a\u0e01\u0e31\u0e14\u0e2b\u0e32\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e40\u0e1e\u0e34\u0e48\u0e21\u0e40\u0e15\u0e34\u0e21\u0e21\u0e32\u0e08\u0e32\u0e01 file .cif \u0e2d\u0e32\u0e08\u0e44\u0e14\u0e49 feature \u0e17\u0e35\u0e48\u0e17\u0e33\u0e43\u0e2b\u0e49 model \u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e04\u0e32\u0e14\u0e01\u0e32\u0e23\u0e13\u0e4c\u0e44\u0e14\u0e49\u0e14\u0e35\u0e02\u0e36\u0e49\u0e19 \u0e08\u0e36\u0e07\u0e25\u0e2d\u0e07\u0e2b\u0e32\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e40\u0e01\u0e35\u0e48\u0e22\u0e27\u0e01\u0e31\u0e1a\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49\u0e44\u0e1f\u0e25\u0e4c .cif\n\u0e08\u0e32\u0e01\u0e01\u0e32\u0e23\u0e25\u0e2d\u0e07\u0e2d\u0e48\u0e32\u0e19\u0e07\u0e32\u0e19\u0e27\u0e34\u0e08\u0e31\u0e22\u0e40\u0e01\u0e35\u0e48\u0e22\u0e27\u0e01\u0e31\u0e1a MOFs machime learning \u0e40\u0e23\u0e32\u0e1e\u0e1a\u0e27\u0e48\u0e32\u0e21\u0e35 open source \u0e17\u0e35\u0e48\u0e19\u0e48\u0e32\u0e2a\u0e19\u0e43\u0e08\u0e21\u0e32\u0e01\u0e41\u0e01\u0e48\u0e01\u0e32\u0e23\u0e19\u0e33\u0e21\u0e32\u0e43\u0e0a\u0e49\u0e01\u0e31\u0e1a\u0e07\u0e32\u0e19\u0e02\u0e2d\u0e07\u0e40\u0e23\u0e32 \u0e19\u0e31\u0e48\u0e19\u0e04\u0e37\u0e2d pywindow \u0e41\u0e25\u0e30 zeo++ \u0e17\u0e35\u0e48\u0e40\u0e1b\u0e47\u0e19\u0e17\u0e35\u0e48\u0e43\u0e0a\u0e49\u0e01\u0e31\u0e19\u0e2d\u0e22\u0e48\u0e32\u0e07\u0e2b\u0e25\u0e32\u0e01\u0e2b\u0e25\u0e32\u0e22 \u0e40\u0e23\u0e32\u0e08\u0e36\u0e07\u0e23\u0e31\u0e19\u0e01\u0e31\u0e19\u0e2d\u0e35\u0e01\u0e02\u0e49\u0e32\u0e21\u0e27\u0e31\u0e19\u0e02\u0e49\u0e32\u0e21\u0e04\u0e37\u0e19\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e40\u0e1e\u0e34\u0e48\u0e21 feature \u0e43\u0e2b\u0e49\u0e01\u0e31\u0e1a dataset \u0e02\u0e2d\u0e07\u0e40\u0e23\u0e32 \u0e2d\u0e22\u0e48\u0e32\u0e07\u0e44\u0e23\u0e01\u0e47\u0e15\u0e32\u0e21\u0e40\u0e23\u0e32\u0e1e\u0e1a\u0e27\u0e48\u0e32 data \u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e40\u0e1e\u0e34\u0e48\u0e21\u0e21\u0e32\u0e01\u0e25\u0e31\u0e1a\u0e17\u0e33\u0e43\u0e2b\u0e49\u0e1c\u0e25\u0e25\u0e31\u0e1e\u0e18\u0e4c\u0e01\u0e32\u0e23\u0e04\u0e32\u0e14\u0e40\u0e14\u0e32\u0e41\u0e22\u0e48\u0e25\u0e07 \u0e41\u0e25\u0e30\u0e44\u0e21\u0e48\u0e27\u0e48\u0e32\u0e08\u0e30\u0e1b\u0e23\u0e31\u0e1a\u0e2d\u0e22\u0e48\u0e32\u0e07\u0e44\u0e23\u0e01\u0e47\u0e44\u0e21\u0e48\u0e21\u0e35\u0e17\u0e35\u0e17\u0e48\u0e32\u0e27\u0e48\u0e32\u0e08\u0e30\u0e02\u0e36\u0e49\u0e19 T_T \u0e2d\u0e32\u0e08\u0e08\u0e30\u0e40\u0e1b\u0e47\u0e19\u0e40\u0e1e\u0e23\u0e32\u0e30 data \u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e21\u0e32\u0e40\u0e1e\u0e34\u0e48\u0e21\u0e44\u0e21\u0e48\u0e15\u0e23\u0e07\u0e01\u0e31\u0e1a\u0e17\u0e35\u0e48\u0e43\u0e0a\u0e49\u0e43\u0e19 simulation \u0e14\u0e49\u0e27\u0e22\n\u0e40\u0e23\u0e32\u0e08\u0e36\u0e07\u0e15\u0e31\u0e14\u0e2a\u0e34\u0e19\u0e43\u0e08\u0e43\u0e0a\u0e49 dataset \u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e23\u0e31\u0e1a\u0e21\u0e32\u0e43\u0e19\u0e15\u0e2d\u0e19\u0e41\u0e23\u0e01 \u0e40\u0e1b\u0e47\u0e19\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e2b\u0e25\u0e31\u0e01\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e01\u0e32\u0e23\u0e23\u0e31\u0e19\u0e42\u0e21\u0e40\u0e14\u0e25\u0e02\u0e2d\u0e07\u0e40\u0e23\u0e32\u0e40\u0e2b\u0e21\u0e37\u0e2d\u0e19\u0e40\u0e14\u0e34\u0e21\u0e15\u0e48\u0e2d\u0e44\u0e1b\nLast 15 mins (Accomplishments that we're proud of)\n\u0e40\u0e23\u0e32\u0e1e\u0e22\u0e32\u0e22\u0e32\u0e21\u0e1b\u0e23\u0e31\u0e1a parameter \u0e41\u0e25\u0e30\u0e17\u0e14\u0e25\u0e2d\u0e07\u0e08\u0e31\u0e14 data \u0e43\u0e2b\u0e21\u0e48\u0e2d\u0e35\u0e01\u0e2b\u0e25\u0e32\u0e22\u0e04\u0e23\u0e31\u0e49\u0e07\u0e08\u0e19\u0e0a\u0e31\u0e48\u0e27\u0e42\u0e21\u0e07\u0e2a\u0e38\u0e14\u0e17\u0e49\u0e32\u0e22 \u0e01\u0e47\u0e22\u0e31\u0e07\u0e44\u0e21\u0e48\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e17\u0e33\u0e43\u0e2b\u0e49\u0e04\u0e30\u0e41\u0e19\u0e19\u0e02\u0e36\u0e49\u0e19\u0e44\u0e14\u0e49\u0e2d\u0e35\u0e01 \u0e08\u0e19\u0e40\u0e23\u0e32\u0e40\u0e01\u0e37\u0e2d\u0e1a\u0e08\u0e30\u0e15\u0e31\u0e14\u0e2a\u0e34\u0e19\u0e43\u0e08\u0e01\u0e31\u0e19\u0e27\u0e48\u0e32 \u0e2a\u0e48\u0e07\u0e2a\u0e34\u0e48\u0e07\u0e17\u0e35\u0e48\u0e21\u0e35\u0e2d\u0e22\u0e39\u0e48\u0e2d\u0e31\u0e19\u0e25\u0e48\u0e32\u0e2a\u0e38\u0e14\u0e44\u0e1b\u0e41\u0e2b\u0e25\u0e30 \u0e08\u0e32\u0e01\u0e40\u0e14\u0e47\u0e01\u0e1e\u0e36\u0e48\u0e07\u0e08\u0e1a\u0e21.\u0e1b\u0e25\u0e32\u0e22\u0e17\u0e35\u0e48\u0e40\u0e02\u0e35\u0e22\u0e19 ML \u0e44\u0e21\u0e48\u0e40\u0e1b\u0e47\u0e19\u0e40\u0e25\u0e22\u0e08\u0e19\u0e15\u0e34\u0e14 scoreboard \u0e17\u0e35\u0e21\u0e17\u0e35\u0e48\u0e2a\u0e34\u0e1a\u0e01\u0e47\u0e40\u0e01\u0e48\u0e07\u0e01\u0e31\u0e19\u0e21\u0e32\u0e01\u0e41\u0e25\u0e49\u0e27\n\u0e41\u0e15\u0e48\u0e1a\u0e31\u0e07\u0e40\u0e2d\u0e34\u0e4a\u0e0d \u0e42\u0e14\u0e19\u0e17\u0e35\u0e21\u0e17\u0e35\u0e48 11 \u0e41\u0e0b\u0e07\u0e2b\u0e19\u0e49\u0e32\u0e02\u0e36\u0e49\u0e19\u0e21\u0e32\n.\n.\n\u201c\u0e2d\u0e22\u0e32\u0e01\u0e15\u0e34\u0e14\u0e17\u0e49\u0e2d\u0e1b\u0e2a\u0e34\u0e1a\u0e27\u0e48\u0e30\u201d \u0e40\u0e2a\u0e35\u0e22\u0e07\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e19\u0e04\u0e19\u0e2b\u0e19\u0e35\u0e48\u0e07\u0e14\u0e31\u0e07\u0e2d\u0e2d\u0e01\u0e21\u0e32\u0e08\u0e32\u0e01\u0e04\u0e2d\u0e25\u0e01\u0e25\u0e38\u0e48\u0e21 \u0e40\u0e23\u0e32\u0e17\u0e38\u0e01\u0e04\u0e19\u0e21\u0e2d\u0e07\u0e2b\u0e19\u0e49\u0e32\u0e01\u0e31\u0e19 \u0e2b\u0e31\u0e19\u0e40\u0e02\u0e49\u0e32\u0e04\u0e2d\u0e21 \u0e41\u0e25\u0e30\u0e40\u0e1b\u0e34\u0e14 code \u0e02\u0e36\u0e49\u0e19\u0e21\u0e32\u0e25\u0e2d\u0e07\u0e41\u0e01\u0e49\u0e01\u0e31\u0e19\u0e15\u0e48\u0e2d\u0e14\u0e49\u0e27\u0e22\u0e04\u0e27\u0e32\u0e21\u0e2b\u0e27\u0e31\u0e07\u0e41\u0e25\u0e30\u0e41\u0e23\u0e07\u0e40\u0e2e\u0e37\u0e2d\u0e01\u0e2a\u0e38\u0e14\u0e17\u0e49\u0e32\u0e22\n.\n\u0e2a\u0e34\u0e1a\u0e2b\u0e49\u0e32\u0e19\u0e32\u0e17\u0e35\u0e2a\u0e38\u0e14\u0e17\u0e49\u0e32\u0e22\u0e40\u0e23\u0e32\u0e01\u0e47\u0e22\u0e31\u0e07\u0e04\u0e07\u0e40\u0e1b\u0e47\u0e19\u0e17\u0e35\u0e48 11 \u0e2d\u0e22\u0e39\u0e48 \u0e04\u0e27\u0e32\u0e21\u0e2b\u0e27\u0e31\u0e07\u0e2a\u0e38\u0e14\u0e17\u0e49\u0e32\u0e22\u0e2b\u0e21\u0e48\u0e19\u0e2b\u0e21\u0e2d\u0e07\u0e25\u0e07\u0e17\u0e38\u0e01\u0e17\u0e35\n\u201c\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e19 \u0e2d\u0e22\u0e32\u0e01\u0e25\u0e2d\u0e07\u0e2d\u0e30\u0e44\u0e23\u0e2b\u0e19\u0e48\u0e2d\u0e22\u201d \u0e2a\u0e32\u0e22 math \u0e04\u0e19\u0e2a\u0e38\u0e14\u0e17\u0e49\u0e32\u0e22\u0e17\u0e35\u0e48\u0e22\u0e31\u0e07\u0e1e\u0e2d\u0e08\u0e30\u0e21\u0e35\u0e04\u0e27\u0e32\u0e21\u0e2b\u0e27\u0e31\u0e07\u0e22\u0e31\u0e07\u0e1e\u0e39\u0e14\u0e2d\u0e2d\u0e01\u0e21\u0e32 \u0e17\u0e38\u0e01\u0e04\u0e19\u0e44\u0e21\u0e48\u0e21\u0e35\u0e44\u0e2d\u0e40\u0e14\u0e35\u0e22\u0e41\u0e25\u0e30\u0e41\u0e23\u0e07\u0e08\u0e30\u0e17\u0e14\u0e25\u0e2d\u0e07\u0e2d\u0e30\u0e44\u0e23\u0e41\u0e25\u0e49\u0e27\u0e40\u0e25\u0e22\u0e1b\u0e25\u0e48\u0e2d\u0e22\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e19\u0e25\u0e2d\u0e07\u0e44\u0e1b\u0e15\u0e32\u0e21\u0e2a\u0e1a\u0e32\u0e22\n.\n\u201c\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e19\u0e46 \u0e04\u0e30\u0e41\u0e19\u0e19\u0e02\u0e36\u0e49\u0e19!!!!\u201d \u0e17\u0e38\u0e01\u0e04\u0e19\u0e15\u0e01\u0e43\u0e08\u0e01\u0e31\u0e19\u0e2b\u0e21\u0e14 \u0e15\u0e48\u0e32\u0e07\u0e16\u0e32\u0e21\u0e01\u0e31\u0e19\u0e22\u0e01\u0e43\u0e2b\u0e0d\u0e48\u0e27\u0e48\u0e32\u0e17\u0e33\u0e44\u0e14\u0e49\u0e14\u0e49\u0e27\u0e22\u0e27\u0e34\u0e18\u0e35\u0e2d\u0e30\u0e44\u0e23\n\u201c\u0e08\u0e31\u0e1a\u0e21\u0e32\u0e2b\u0e32 median\u201d \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e19\u0e43\u0e19\u0e01\u0e25\u0e38\u0e48\u0e21\u0e07\u0e07\u0e01\u0e31\u0e19\u0e2b\u0e21\u0e14 \u0e44\u0e21\u0e48\u0e40\u0e02\u0e49\u0e32\u0e43\u0e08\u0e27\u0e48\u0e32\u0e44\u0e2d\u0e40\u0e14\u0e35\u0e22\u0e21\u0e32\u0e08\u0e32\u0e01\u0e44\u0e2b\u0e19 \u0e04\u0e30\u0e41\u0e19\u0e19\u0e02\u0e36\u0e49\u0e19\u0e44\u0e14\u0e49\u0e22\u0e31\u0e07\u0e44\u0e07 \u0e41\u0e15\u0e48\u0e01\u0e47\u0e44\u0e21\u0e48\u0e40\u0e1b\u0e47\u0e19\u0e44\u0e23\u0e40\u0e2b\u0e25\u0e37\u0e2d\u0e2d\u0e35\u0e01\u0e41\u0e04\u0e48 5 \u0e19\u0e32\u0e17\u0e35\u0e41\u0e25\u0e49\u0e27 \u0e17\u0e38\u0e01\u0e04\u0e19\u0e01\u0e47\u0e23\u0e48\u0e27\u0e21\u0e43\u0e08\u0e01\u0e31\u0e19\u0e2a\u0e48\u0e07\u0e1c\u0e25\u0e17\u0e35\u0e48\u0e14\u0e35\u0e46\u0e17\u0e31\u0e49\u0e07\u0e2b\u0e21\u0e14\u0e23\u0e27\u0e21 11 set \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e44\u0e1b\u0e2b\u0e32\u0e04\u0e48\u0e32 median \u0e02\u0e2d\u0e07 MOF \u0e41\u0e15\u0e48\u0e25\u0e30\u0e15\u0e31\u0e27 \u0e41\u0e25\u0e30\u0e19\u0e33\u0e44\u0e1b\u0e2a\u0e48\u0e07\u0e40\u0e1b\u0e47\u0e19\u0e23\u0e2d\u0e1a\u0e2a\u0e38\u0e14\u0e17\u0e49\u0e32\u0e22\n\u0e41\u0e25\u0e30\u0e19\u0e31\u0e48\u0e19\u0e01\u0e47\u0e40\u0e1b\u0e47\u0e19 submission \u0e2a\u0e38\u0e14\u0e17\u0e49\u0e32\u0e22\u0e02\u0e2d\u0e07 competition \u0e02\u0e2d\u0e07\u0e40\u0e23\u0e32\u0e17\u0e35\u0e48\u0e17\u0e33\u0e43\u0e2b\u0e49\u0e17\u0e35\u0e21\u0e40\u0e23\u0e32\u0e01\u0e25\u0e31\u0e1a\u0e21\u0e32\u0e40\u0e2b\u0e22\u0e35\u0e22\u0e1a\u0e15\u0e33\u0e41\u0e2b\u0e19\u0e48\u0e07 top 10 \u0e2d\u0e35\u0e01\u0e04\u0e23\u0e31\u0e49\u0e07\n\u0e16\u0e36\u0e07\u0e41\u0e21\u0e49\u0e08\u0e30\u0e40\u0e1b\u0e47\u0e19\u0e41\u0e04\u0e48 unofficial leaderboard \u0e01\u0e47\u0e15\u0e32\u0e21 \u0e08\u0e32\u0e01\u0e21\u0e38\u0e21\u0e21\u0e2d\u0e07\u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32\u0e41\u0e25\u0e49\u0e27 \u0e19\u0e35\u0e48\u0e40\u0e1b\u0e47\u0e19 achievement \u0e04\u0e23\u0e31\u0e49\u0e07\u0e2a\u0e33\u0e04\u0e31\u0e0d\u0e17\u0e35\u0e48\u0e08\u0e30\u0e04\u0e2d\u0e22\u0e40\u0e1b\u0e47\u0e19\u0e41\u0e23\u0e07\u0e1c\u0e25\u0e31\u0e01\u0e14\u0e31\u0e19\u0e43\u0e2b\u0e49\u0e40\u0e23\u0e32\u0e21\u0e38\u0e48\u0e07\u0e21\u0e31\u0e48\u0e19\u0e15\u0e48\u0e2d\u0e44\u0e1b\u0e2d\u0e22\u0e48\u0e32\u0e07\u0e41\u0e19\u0e48\u0e19\u0e2d\u0e19", "link": "https://devpost.com/software/b2f2", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\n\u0e17\u0e35\u0e21\u0e02\u0e2d\u0e07\u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e21\u0e35\u0e04\u0e27\u0e32\u0e21\u0e2a\u0e19\u0e43\u0e08\u0e43\u0e19\u0e01\u0e32\u0e23\u0e41\u0e02\u0e48\u0e07\u0e02\u0e31\u0e19 tmlcc \u0e0b\u0e36\u0e48\u0e07\u0e42\u0e08\u0e17\u0e22\u0e4c\u0e43\u0e19\u0e1b\u0e35\u0e19\u0e35\u0e49\u0e04\u0e37\u0e2d\u0e01\u0e32\u0e23\u0e17\u0e33\u0e19\u0e32\u0e22\u0e04\u0e27\u0e32\u0e21\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e43\u0e19\u0e01\u0e32\u0e23\u0e14\u0e39\u0e14\u0e0b\u0e31\u0e1a\u0e01\u0e4a\u0e32\u0e0b\u0e02\u0e2d\u0e07 metal-organic frameworks (mofs)\n\u0e40\u0e23\u0e32\u0e44\u0e21\u0e48\u0e44\u0e14\u0e49\u0e04\u0e32\u0e14\u0e2b\u0e27\u0e31\u0e07\u0e23\u0e32\u0e07\u0e27\u0e31\u0e25\u0e2b\u0e23\u0e37\u0e2d\u0e2d\u0e30\u0e44\u0e23 \u0e2b\u0e32\u0e01\u0e41\u0e15\u0e48\u0e40\u0e1b\u0e47\u0e19\u0e40\u0e1e\u0e35\u0e22\u0e07\u0e01\u0e32\u0e23\u0e2b\u0e32\u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49 \u0e41\u0e25\u0e30\u0e2b\u0e32\u0e01\u0e34\u0e08\u0e01\u0e23\u0e23\u0e21\u0e17\u0e33\u0e23\u0e48\u0e27\u0e21\u0e01\u0e31\u0e1a\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e19\u0e46\u0e17\u0e35\u0e48\u0e1e\u0e36\u0e48\u0e07\u0e08\u0e1a\u0e21.\u0e1b\u0e25\u0e32\u0e22\u0e01\u0e31\u0e19\u0e21\u0e32\nhow we built it\n\u0e17\u0e35\u0e21\u0e40\u0e23\u0e32\u0e43\u0e0a\u0e49 python \u0e40\u0e02\u0e35\u0e22\u0e19\u0e40\u0e1b\u0e47\u0e19 .ipynb \u0e1a\u0e19 gpu machine \u0e02\u0e2d\u0e07 deepnote \u0e2a\u0e25\u0e31\u0e1a\u0e01\u0e31\u0e1a\u0e23\u0e31\u0e19\u0e1a\u0e19\u0e04\u0e2d\u0e21\u0e1e\u0e34\u0e27\u0e40\u0e15\u0e2d\u0e23\u0e4c\u0e2a\u0e48\u0e27\u0e19\u0e15\u0e31\u0e27 \u0e15\u0e25\u0e2d\u0e14\u0e01\u0e32\u0e23\u0e17\u0e33\u0e42\u0e1b\u0e23\u0e40\u0e08\u0e04\u0e04\u0e23\u0e31\u0e49\u0e07\u0e19\u0e35\u0e49\nalong the pathway\n\u0e40\u0e19\u0e37\u0e48\u0e2d\u0e07\u0e08\u0e32\u0e01\u0e42\u0e08\u0e17\u0e22\u0e4c\u0e21\u0e35\u0e23\u0e39\u0e1b\u0e41\u0e1a\u0e1a\u0e44\u0e1b\u0e17\u0e32\u0e07 regression model \u0e40\u0e17\u0e04\u0e19\u0e34\u0e04\u0e17\u0e32\u0e07 machine learning \u0e17\u0e35\u0e48\u0e40\u0e23\u0e32\u0e21\u0e2d\u0e07\u0e40\u0e1b\u0e47\u0e19\u0e2d\u0e31\u0e19\u0e14\u0e31\u0e1a\u0e41\u0e23\u0e01\u0e46\u0e08\u0e36\u0e07\u0e40\u0e1b\u0e47\u0e19 regression -----> tree !!!  \u0e41\u0e15\u0e48\u0e27\u0e48\u0e32 dataset \u0e40\u0e23\u0e34\u0e48\u0e21\u0e15\u0e49\u0e19\u0e17\u0e35\u0e48\u0e43\u0e2b\u0e49\u0e21\u0e32 \u0e21\u0e35\u0e17\u0e31\u0e49\u0e07 numerical feature \u0e41\u0e25\u0e30 categorical feature\n\u0e40\u0e23\u0e32\u0e08\u0e36\u0e07\u0e44\u0e14\u0e49\u0e21\u0e2d\u0e07\u0e2b\u0e32\u0e40\u0e17\u0e04\u0e19\u0e34\u0e04\u0e17\u0e35\u0e48\u0e21\u0e35\u0e04\u0e27\u0e32\u0e21\u0e40\u0e2b\u0e21\u0e32\u0e30\u0e2a\u0e21\u0e41\u0e25\u0e30\u0e16\u0e39\u0e01\u0e2d\u0e2d\u0e01\u0e41\u0e1a\u0e1a\u0e21\u0e32\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e2a\u0e34\u0e48\u0e07\u0e40\u0e2b\u0e25\u0e48\u0e32\u0e19\u0e31\u0e49\u0e19\u0e21\u0e32\u0e01\u0e01\u0e27\u0e48\u0e32 \u0e2d\u0e48\u0e32\u0e19\u0e40\u0e1b\u0e40\u0e1b\u0e2d\u0e23\u0e4c\u0e01\u0e31\u0e19\u0e23\u0e31\u0e27\u0e46 \u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e17\u0e14\u0e25\u0e2d\u0e07\u0e44\u0e21\u0e48\u0e27\u0e48\u0e32\u0e08\u0e30\u0e40\u0e1b\u0e47\u0e19 random forest, gradient boost, extreme gradient boost, \u0e41\u0e25\u0e30 neural network \u0e2b\u0e32\u0e42\u0e21\u0e40\u0e14\u0e25\u0e17\u0e35\u0e48\u0e21\u0e35\u0e1b\u0e23\u0e30\u0e2a\u0e34\u0e17\u0e18\u0e34\u0e20\u0e32\u0e1e\u0e2a\u0e39\u0e07.\nsupposedly the best we can do\n\u0e43\u0e19\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14\u0e01\u0e47\u0e44\u0e14\u0e49\u0e40\u0e1b\u0e47\u0e19 light gradient boost machine regressor \u0e41\u0e25\u0e30 catboost regressor \u0e17\u0e35\u0e48\u0e21\u0e35\u0e1b\u0e23\u0e30\u0e2a\u0e34\u0e17\u0e18\u0e34\u0e20\u0e32\u0e1e\u0e2a\u0e39\u0e07\u0e43\u0e01\u0e25\u0e49\u0e40\u0e04\u0e35\u0e22\u0e07\u0e01\u0e31\u0e19\n\u0e40\u0e21\u0e37\u0e48\u0e2d\u0e44\u0e14\u0e49 model \u0e17\u0e35\u0e48\u0e21\u0e35\u0e1b\u0e23\u0e30\u0e2a\u0e34\u0e17\u0e18\u0e34\u0e20\u0e32\u0e1e\u0e41\u0e25\u0e49\u0e27 \u0e40\u0e23\u0e32\u0e01\u0e47\u0e44\u0e14\u0e49 optimize hyperparameter \u0e1e\u0e37\u0e49\u0e19\u0e10\u0e32\u0e19\u0e15\u0e48\u0e32\u0e07\u0e46 \u0e23\u0e31\u0e19\u0e01\u0e31\u0e19\u0e02\u0e49\u0e32\u0e21\u0e27\u0e31\u0e19\u0e02\u0e49\u0e32\u0e21\u0e04\u0e37\u0e19\u0e08\u0e19\u0e44\u0e14\u0e49\u0e42\u0e21\u0e40\u0e14\u0e25\u0e17\u0e35\u0e48\u0e40\u0e23\u0e32\u0e40\u0e0a\u0e37\u0e48\u0e2d\u0e27\u0e48\u0e32\u0e21\u0e35\u0e1b\u0e23\u0e30\u0e2a\u0e34\u0e17\u0e18\u0e34\u0e20\u0e32\u0e1e\u0e21\u0e32\u0e01\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14\nimprove improve improve\n\u0e2b\u0e25\u0e31\u0e07\u0e08\u0e32\u0e01\u0e15\u0e31\u0e19\u0e2d\u0e22\u0e39\u0e48\u0e1e\u0e31\u0e01\u0e2b\u0e19\u0e36\u0e48\u0e07 \u0e40\u0e23\u0e32\u0e01\u0e47\u0e21\u0e35\u0e44\u0e2d\u0e40\u0e14\u0e35\u0e22\u0e27\u0e48\u0e32\u0e1a\u0e32\u0e07\u0e17\u0e35\u0e01\u0e32\u0e23\u0e2a\u0e01\u0e31\u0e14\u0e2b\u0e32\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e40\u0e1e\u0e34\u0e48\u0e21\u0e40\u0e15\u0e34\u0e21\u0e21\u0e32\u0e08\u0e32\u0e01 file .cif \u0e2d\u0e32\u0e08\u0e44\u0e14\u0e49 feature \u0e17\u0e35\u0e48\u0e17\u0e33\u0e43\u0e2b\u0e49 model \u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e04\u0e32\u0e14\u0e01\u0e32\u0e23\u0e13\u0e4c\u0e44\u0e14\u0e49\u0e14\u0e35\u0e02\u0e36\u0e49\u0e19 \u0e08\u0e36\u0e07\u0e25\u0e2d\u0e07\u0e2b\u0e32\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e40\u0e01\u0e35\u0e48\u0e22\u0e27\u0e01\u0e31\u0e1a\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49\u0e44\u0e1f\u0e25\u0e4c .cif\n\u0e08\u0e32\u0e01\u0e01\u0e32\u0e23\u0e25\u0e2d\u0e07\u0e2d\u0e48\u0e32\u0e19\u0e07\u0e32\u0e19\u0e27\u0e34\u0e08\u0e31\u0e22\u0e40\u0e01\u0e35\u0e48\u0e22\u0e27\u0e01\u0e31\u0e1a mofs machime learning \u0e40\u0e23\u0e32\u0e1e\u0e1a\u0e27\u0e48\u0e32\u0e21\u0e35 open source \u0e17\u0e35\u0e48\u0e19\u0e48\u0e32\u0e2a\u0e19\u0e43\u0e08\u0e21\u0e32\u0e01\u0e41\u0e01\u0e48\u0e01\u0e32\u0e23\u0e19\u0e33\u0e21\u0e32\u0e43\u0e0a\u0e49\u0e01\u0e31\u0e1a\u0e07\u0e32\u0e19\u0e02\u0e2d\u0e07\u0e40\u0e23\u0e32 \u0e19\u0e31\u0e48\u0e19\u0e04\u0e37\u0e2d pywindow \u0e41\u0e25\u0e30 zeo++ \u0e17\u0e35\u0e48\u0e40\u0e1b\u0e47\u0e19\u0e17\u0e35\u0e48\u0e43\u0e0a\u0e49\u0e01\u0e31\u0e19\u0e2d\u0e22\u0e48\u0e32\u0e07\u0e2b\u0e25\u0e32\u0e01\u0e2b\u0e25\u0e32\u0e22 \u0e40\u0e23\u0e32\u0e08\u0e36\u0e07\u0e23\u0e31\u0e19\u0e01\u0e31\u0e19\u0e2d\u0e35\u0e01\u0e02\u0e49\u0e32\u0e21\u0e27\u0e31\u0e19\u0e02\u0e49\u0e32\u0e21\u0e04\u0e37\u0e19\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e40\u0e1e\u0e34\u0e48\u0e21 feature \u0e43\u0e2b\u0e49\u0e01\u0e31\u0e1a dataset \u0e02\u0e2d\u0e07\u0e40\u0e23\u0e32 \u0e2d\u0e22\u0e48\u0e32\u0e07\u0e44\u0e23\u0e01\u0e47\u0e15\u0e32\u0e21\u0e40\u0e23\u0e32\u0e1e\u0e1a\u0e27\u0e48\u0e32 data \u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e40\u0e1e\u0e34\u0e48\u0e21\u0e21\u0e32\u0e01\u0e25\u0e31\u0e1a\u0e17\u0e33\u0e43\u0e2b\u0e49\u0e1c\u0e25\u0e25\u0e31\u0e1e\u0e18\u0e4c\u0e01\u0e32\u0e23\u0e04\u0e32\u0e14\u0e40\u0e14\u0e32\u0e41\u0e22\u0e48\u0e25\u0e07 \u0e41\u0e25\u0e30\u0e44\u0e21\u0e48\u0e27\u0e48\u0e32\u0e08\u0e30\u0e1b\u0e23\u0e31\u0e1a\u0e2d\u0e22\u0e48\u0e32\u0e07\u0e44\u0e23\u0e01\u0e47\u0e44\u0e21\u0e48\u0e21\u0e35\u0e17\u0e35\u0e17\u0e48\u0e32\u0e27\u0e48\u0e32\u0e08\u0e30\u0e02\u0e36\u0e49\u0e19 t_t \u0e2d\u0e32\u0e08\u0e08\u0e30\u0e40\u0e1b\u0e47\u0e19\u0e40\u0e1e\u0e23\u0e32\u0e30 data \u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e21\u0e32\u0e40\u0e1e\u0e34\u0e48\u0e21\u0e44\u0e21\u0e48\u0e15\u0e23\u0e07\u0e01\u0e31\u0e1a\u0e17\u0e35\u0e48\u0e43\u0e0a\u0e49\u0e43\u0e19 simulation \u0e14\u0e49\u0e27\u0e22\n\u0e40\u0e23\u0e32\u0e08\u0e36\u0e07\u0e15\u0e31\u0e14\u0e2a\u0e34\u0e19\u0e43\u0e08\u0e43\u0e0a\u0e49 dataset \u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e23\u0e31\u0e1a\u0e21\u0e32\u0e43\u0e19\u0e15\u0e2d\u0e19\u0e41\u0e23\u0e01 \u0e40\u0e1b\u0e47\u0e19\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e2b\u0e25\u0e31\u0e01\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e01\u0e32\u0e23\u0e23\u0e31\u0e19\u0e42\u0e21\u0e40\u0e14\u0e25\u0e02\u0e2d\u0e07\u0e40\u0e23\u0e32\u0e40\u0e2b\u0e21\u0e37\u0e2d\u0e19\u0e40\u0e14\u0e34\u0e21\u0e15\u0e48\u0e2d\u0e44\u0e1b\nlast 15 mins (accomplishments that we're proud of)\n\u0e40\u0e23\u0e32\u0e1e\u0e22\u0e32\u0e22\u0e32\u0e21\u0e1b\u0e23\u0e31\u0e1a parameter \u0e41\u0e25\u0e30\u0e17\u0e14\u0e25\u0e2d\u0e07\u0e08\u0e31\u0e14 data \u0e43\u0e2b\u0e21\u0e48\u0e2d\u0e35\u0e01\u0e2b\u0e25\u0e32\u0e22\u0e04\u0e23\u0e31\u0e49\u0e07\u0e08\u0e19\u0e0a\u0e31\u0e48\u0e27\u0e42\u0e21\u0e07\u0e2a\u0e38\u0e14\u0e17\u0e49\u0e32\u0e22 \u0e01\u0e47\u0e22\u0e31\u0e07\u0e44\u0e21\u0e48\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e17\u0e33\u0e43\u0e2b\u0e49\u0e04\u0e30\u0e41\u0e19\u0e19\u0e02\u0e36\u0e49\u0e19\u0e44\u0e14\u0e49\u0e2d\u0e35\u0e01 \u0e08\u0e19\u0e40\u0e23\u0e32\u0e40\u0e01\u0e37\u0e2d\u0e1a\u0e08\u0e30\u0e15\u0e31\u0e14\u0e2a\u0e34\u0e19\u0e43\u0e08\u0e01\u0e31\u0e19\u0e27\u0e48\u0e32 \u0e2a\u0e48\u0e07\u0e2a\u0e34\u0e48\u0e07\u0e17\u0e35\u0e48\u0e21\u0e35\u0e2d\u0e22\u0e39\u0e48\u0e2d\u0e31\u0e19\u0e25\u0e48\u0e32\u0e2a\u0e38\u0e14\u0e44\u0e1b\u0e41\u0e2b\u0e25\u0e30 \u0e08\u0e32\u0e01\u0e40\u0e14\u0e47\u0e01\u0e1e\u0e36\u0e48\u0e07\u0e08\u0e1a\u0e21.\u0e1b\u0e25\u0e32\u0e22\u0e17\u0e35\u0e48\u0e40\u0e02\u0e35\u0e22\u0e19 ml \u0e44\u0e21\u0e48\u0e40\u0e1b\u0e47\u0e19\u0e40\u0e25\u0e22\u0e08\u0e19\u0e15\u0e34\u0e14 scoreboard \u0e17\u0e35\u0e21\u0e17\u0e35\u0e48\u0e2a\u0e34\u0e1a\u0e01\u0e47\u0e40\u0e01\u0e48\u0e07\u0e01\u0e31\u0e19\u0e21\u0e32\u0e01\u0e41\u0e25\u0e49\u0e27\n\u0e41\u0e15\u0e48\u0e1a\u0e31\u0e07\u0e40\u0e2d\u0e34\u0e4a\u0e0d \u0e42\u0e14\u0e19\u0e17\u0e35\u0e21\u0e17\u0e35\u0e48 11 \u0e41\u0e0b\u0e07\u0e2b\u0e19\u0e49\u0e32\u0e02\u0e36\u0e49\u0e19\u0e21\u0e32\n.\n.\n\u201c\u0e2d\u0e22\u0e32\u0e01\u0e15\u0e34\u0e14\u0e17\u0e49\u0e2d\u0e1b\u0e2a\u0e34\u0e1a\u0e27\u0e48\u0e30\u201d \u0e40\u0e2a\u0e35\u0e22\u0e07\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e19\u0e04\u0e19\u0e2b\u0e19\u0e35\u0e48\u0e07\u0e14\u0e31\u0e07\u0e2d\u0e2d\u0e01\u0e21\u0e32\u0e08\u0e32\u0e01\u0e04\u0e2d\u0e25\u0e01\u0e25\u0e38\u0e48\u0e21 \u0e40\u0e23\u0e32\u0e17\u0e38\u0e01\u0e04\u0e19\u0e21\u0e2d\u0e07\u0e2b\u0e19\u0e49\u0e32\u0e01\u0e31\u0e19 \u0e2b\u0e31\u0e19\u0e40\u0e02\u0e49\u0e32\u0e04\u0e2d\u0e21 \u0e41\u0e25\u0e30\u0e40\u0e1b\u0e34\u0e14 code \u0e02\u0e36\u0e49\u0e19\u0e21\u0e32\u0e25\u0e2d\u0e07\u0e41\u0e01\u0e49\u0e01\u0e31\u0e19\u0e15\u0e48\u0e2d\u0e14\u0e49\u0e27\u0e22\u0e04\u0e27\u0e32\u0e21\u0e2b\u0e27\u0e31\u0e07\u0e41\u0e25\u0e30\u0e41\u0e23\u0e07\u0e40\u0e2e\u0e37\u0e2d\u0e01\u0e2a\u0e38\u0e14\u0e17\u0e49\u0e32\u0e22\n.\n\u0e2a\u0e34\u0e1a\u0e2b\u0e49\u0e32\u0e19\u0e32\u0e17\u0e35\u0e2a\u0e38\u0e14\u0e17\u0e49\u0e32\u0e22\u0e40\u0e23\u0e32\u0e01\u0e47\u0e22\u0e31\u0e07\u0e04\u0e07\u0e40\u0e1b\u0e47\u0e19\u0e17\u0e35\u0e48 11 \u0e2d\u0e22\u0e39\u0e48 \u0e04\u0e27\u0e32\u0e21\u0e2b\u0e27\u0e31\u0e07\u0e2a\u0e38\u0e14\u0e17\u0e49\u0e32\u0e22\u0e2b\u0e21\u0e48\u0e19\u0e2b\u0e21\u0e2d\u0e07\u0e25\u0e07\u0e17\u0e38\u0e01\u0e17\u0e35\n\u201c\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e19 \u0e2d\u0e22\u0e32\u0e01\u0e25\u0e2d\u0e07\u0e2d\u0e30\u0e44\u0e23\u0e2b\u0e19\u0e48\u0e2d\u0e22\u201d \u0e2a\u0e32\u0e22 math \u0e04\u0e19\u0e2a\u0e38\u0e14\u0e17\u0e49\u0e32\u0e22\u0e17\u0e35\u0e48\u0e22\u0e31\u0e07\u0e1e\u0e2d\u0e08\u0e30\u0e21\u0e35\u0e04\u0e27\u0e32\u0e21\u0e2b\u0e27\u0e31\u0e07\u0e22\u0e31\u0e07\u0e1e\u0e39\u0e14\u0e2d\u0e2d\u0e01\u0e21\u0e32 \u0e17\u0e38\u0e01\u0e04\u0e19\u0e44\u0e21\u0e48\u0e21\u0e35\u0e44\u0e2d\u0e40\u0e14\u0e35\u0e22\u0e41\u0e25\u0e30\u0e41\u0e23\u0e07\u0e08\u0e30\u0e17\u0e14\u0e25\u0e2d\u0e07\u0e2d\u0e30\u0e44\u0e23\u0e41\u0e25\u0e49\u0e27\u0e40\u0e25\u0e22\u0e1b\u0e25\u0e48\u0e2d\u0e22\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e19\u0e25\u0e2d\u0e07\u0e44\u0e1b\u0e15\u0e32\u0e21\u0e2a\u0e1a\u0e32\u0e22\n.\n\u201c\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e19\u0e46 \u0e04\u0e30\u0e41\u0e19\u0e19\u0e02\u0e36\u0e49\u0e19!!!!\u201d \u0e17\u0e38\u0e01\u0e04\u0e19\u0e15\u0e01\u0e43\u0e08\u0e01\u0e31\u0e19\u0e2b\u0e21\u0e14 \u0e15\u0e48\u0e32\u0e07\u0e16\u0e32\u0e21\u0e01\u0e31\u0e19\u0e22\u0e01\u0e43\u0e2b\u0e0d\u0e48\u0e27\u0e48\u0e32\u0e17\u0e33\u0e44\u0e14\u0e49\u0e14\u0e49\u0e27\u0e22\u0e27\u0e34\u0e18\u0e35\u0e2d\u0e30\u0e44\u0e23\n\u201c\u0e08\u0e31\u0e1a\u0e21\u0e32\u0e2b\u0e32 median\u201d \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e19\u0e43\u0e19\u0e01\u0e25\u0e38\u0e48\u0e21\u0e07\u0e07\u0e01\u0e31\u0e19\u0e2b\u0e21\u0e14 \u0e44\u0e21\u0e48\u0e40\u0e02\u0e49\u0e32\u0e43\u0e08\u0e27\u0e48\u0e32\u0e44\u0e2d\u0e40\u0e14\u0e35\u0e22\u0e21\u0e32\u0e08\u0e32\u0e01\u0e44\u0e2b\u0e19 \u0e04\u0e30\u0e41\u0e19\u0e19\u0e02\u0e36\u0e49\u0e19\u0e44\u0e14\u0e49\u0e22\u0e31\u0e07\u0e44\u0e07 \u0e41\u0e15\u0e48\u0e01\u0e47\u0e44\u0e21\u0e48\u0e40\u0e1b\u0e47\u0e19\u0e44\u0e23\u0e40\u0e2b\u0e25\u0e37\u0e2d\u0e2d\u0e35\u0e01\u0e41\u0e04\u0e48 5 \u0e19\u0e32\u0e17\u0e35\u0e41\u0e25\u0e49\u0e27 \u0e17\u0e38\u0e01\u0e04\u0e19\u0e01\u0e47\u0e23\u0e48\u0e27\u0e21\u0e43\u0e08\u0e01\u0e31\u0e19\u0e2a\u0e48\u0e07\u0e1c\u0e25\u0e17\u0e35\u0e48\u0e14\u0e35\u0e46\u0e17\u0e31\u0e49\u0e07\u0e2b\u0e21\u0e14\u0e23\u0e27\u0e21 11 set \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e44\u0e1b\u0e2b\u0e32\u0e04\u0e48\u0e32 median \u0e02\u0e2d\u0e07 mof \u0e41\u0e15\u0e48\u0e25\u0e30\u0e15\u0e31\u0e27 \u0e41\u0e25\u0e30\u0e19\u0e33\u0e44\u0e1b\u0e2a\u0e48\u0e07\u0e40\u0e1b\u0e47\u0e19\u0e23\u0e2d\u0e1a\u0e2a\u0e38\u0e14\u0e17\u0e49\u0e32\u0e22\n\u0e41\u0e25\u0e30\u0e19\u0e31\u0e48\u0e19\u0e01\u0e47\u0e40\u0e1b\u0e47\u0e19 submission \u0e2a\u0e38\u0e14\u0e17\u0e49\u0e32\u0e22\u0e02\u0e2d\u0e07 competition \u0e02\u0e2d\u0e07\u0e40\u0e23\u0e32\u0e17\u0e35\u0e48\u0e17\u0e33\u0e43\u0e2b\u0e49\u0e17\u0e35\u0e21\u0e40\u0e23\u0e32\u0e01\u0e25\u0e31\u0e1a\u0e21\u0e32\u0e40\u0e2b\u0e22\u0e35\u0e22\u0e1a\u0e15\u0e33\u0e41\u0e2b\u0e19\u0e48\u0e07 top 10 \u0e2d\u0e35\u0e01\u0e04\u0e23\u0e31\u0e49\u0e07\n\u0e16\u0e36\u0e07\u0e41\u0e21\u0e49\u0e08\u0e30\u0e40\u0e1b\u0e47\u0e19\u0e41\u0e04\u0e48 unofficial leaderboard \u0e01\u0e47\u0e15\u0e32\u0e21 \u0e08\u0e32\u0e01\u0e21\u0e38\u0e21\u0e21\u0e2d\u0e07\u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32\u0e41\u0e25\u0e49\u0e27 \u0e19\u0e35\u0e48\u0e40\u0e1b\u0e47\u0e19 achievement \u0e04\u0e23\u0e31\u0e49\u0e07\u0e2a\u0e33\u0e04\u0e31\u0e0d\u0e17\u0e35\u0e48\u0e08\u0e30\u0e04\u0e2d\u0e22\u0e40\u0e1b\u0e47\u0e19\u0e41\u0e23\u0e07\u0e1c\u0e25\u0e31\u0e01\u0e14\u0e31\u0e19\u0e43\u0e2b\u0e49\u0e40\u0e23\u0e32\u0e21\u0e38\u0e48\u0e07\u0e21\u0e31\u0e48\u0e19\u0e15\u0e48\u0e2d\u0e44\u0e1b\u0e2d\u0e22\u0e48\u0e32\u0e07\u0e41\u0e19\u0e48\u0e19\u0e2d\u0e19", "sortedWord": "None", "removed": "Nan", "score": 6, "comments": 2, "media": null, "medialink": null, "identifyer": 59507760}, {"Unnamed: 0": 7842, "autor": "thunder storm", "date": null, "content": "Inspiration\n\u0e17\u0e35\u0e21\u0e02\u0e2d\u0e07\u0e40\u0e23\u0e32\u0e2a\u0e19\u0e43\u0e08\u0e01\u0e32\u0e23\u0e41\u0e02\u0e48\u0e07\u0e02\u0e31\u0e19\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07 Machine Learning \u0e42\u0e14\u0e22\u0e44\u0e14\u0e49\u0e21\u0e32\u0e40\u0e08\u0e2d\u0e01\u0e32\u0e23\u0e41\u0e02\u0e48\u0e07\u0e02\u0e31\u0e19 TMLCC \u0e08\u0e36\u0e07\u0e2a\u0e19\u0e43\u0e08\u0e40\u0e1b\u0e47\u0e19\u0e2d\u0e22\u0e48\u0e32\u0e07\u0e21\u0e32\u0e01 \u0e0b\u0e36\u0e48\u0e07\u0e42\u0e08\u0e17\u0e22\u0e4c\u0e1b\u0e35\u0e19\u0e35\u0e49 \u0e04\u0e37\u0e2d \u0e2a\u0e23\u0e49\u0e32\u0e07\u0e15\u0e31\u0e27\u0e41\u0e1a\u0e1a\u0e43\u0e19\u0e01\u0e32\u0e23\u0e17\u0e33\u0e19\u0e32\u0e22\u0e1b\u0e23\u0e34\u0e21\u0e32\u0e13\u0e01\u0e4a\u0e32\u0e0b\u0e04\u0e32\u0e23\u0e4c\u0e1a\u0e2d\u0e19\u0e44\u0e14\u0e2d\u0e2d\u0e01\u0e44\u0e0b\u0e14\u0e4c (CO2) \u0e17\u0e35\u0e48 MOFs \u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e14\u0e39\u0e14\u0e0b\u0e31\u0e1a\u0e2b\u0e23\u0e37\u0e2d\u0e01\u0e31\u0e01\u0e40\u0e01\u0e47\u0e1a\u0e44\u0e27\u0e49\u0e44\u0e14\u0e49 \u0e0b\u0e36\u0e48\u0e07\u0e01\u0e4a\u0e32\u0e0b\u0e04\u0e32\u0e23\u0e4c\u0e1a\u0e2d\u0e19\u0e44\u0e14\u0e2d\u0e2d\u0e01\u0e44\u0e0b\u0e14\u0e4c (CO2) \u0e40\u0e1b\u0e47\u0e19\u0e2a\u0e32\u0e40\u0e2b\u0e15\u0e38\u0e17\u0e35\u0e48\u0e17\u0e33\u0e43\u0e2b\u0e49\u0e40\u0e01\u0e34\u0e14\u0e20\u0e32\u0e27\u0e30\u0e01\u0e32\u0e23\u0e40\u0e1b\u0e25\u0e35\u0e48\u0e22\u0e19\u0e41\u0e1b\u0e25\u0e07\u0e20\u0e39\u0e21\u0e34\u0e2d\u0e32\u0e01\u0e32\u0e28\u0e02\u0e2d\u0e07\u0e42\u0e25\u0e01 \u0e42\u0e14\u0e22\u0e19\u0e33\u0e40\u0e17\u0e04\u0e19\u0e34\u0e04\u0e01\u0e32\u0e23\u0e40\u0e23\u0e35\u0e22\u0e19\u0e23\u0e39\u0e49\u0e02\u0e2d\u0e07\u0e40\u0e04\u0e23\u0e37\u0e48\u0e2d\u0e07 (Machine Learning)\nModel explaination\n\u0e17\u0e33\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25 Neural Network \u0e17\u0e35\u0e48\u0e41\u0e15\u0e01\u0e15\u0e48\u0e32\u0e07\u0e01\u0e31\u0e19 3 \u0e42\u0e21\u0e40\u0e14\u0e25 \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e40\u0e2d\u0e32\u0e04\u0e48\u0e32\u0e1e\u0e22\u0e32\u0e01\u0e23\u0e13\u0e4c\u0e17\u0e35\u0e48\u0e17\u0e33\u0e19\u0e32\u0e22\u0e44\u0e14\u0e49\u0e41\u0e15\u0e48\u0e43\u0e19\u0e25\u0e30\u0e42\u0e21\u0e40\u0e14\u0e25\u0e21\u0e32 weight \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49\u0e19\u0e49\u0e33\u0e2b\u0e19\u0e31\u0e01\u0e40\u0e17\u0e48\u0e32\u0e01\u0e31\u0e1a 0.7 0.2 \u0e41\u0e25\u0e30 0.1\n\u0e17\u0e33\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25\u0e41\u0e23\u0e01\u0e04\u0e37\u0e2d Ensemble \u0e08\u0e32\u0e01 3 \u0e42\u0e21\u0e40\u0e14\u0e25 \u0e44\u0e14\u0e49\u0e41\u0e01\u0e48 AdaBoost \u0e17\u0e35\u0e48\u0e21\u0e35 base estimator \u0e04\u0e37\u0e2d Decision tree \u0e08\u0e32\u0e01\u0e42\u0e21\u0e40\u0e14\u0e25 Bagging \u0e17\u0e35\u0e48\u0e21\u0e35 base estimator \u0e04\u0e37\u0e2d Decision tree \u0e41\u0e25\u0e30\u0e08\u0e32\u0e01\u0e42\u0e21\u0e40\u0e14\u0e25 Random Forest \u0e2a\u0e48\u0e27\u0e19\u0e42\u0e21\u0e40\u0e14\u0e25\u0e17\u0e35\u0e48\u0e2a\u0e2d\u0e07 \u0e04\u0e37\u0e2d AdaBoost \u0e17\u0e35\u0e48\u0e21\u0e35 base estimator \u0e04\u0e37\u0e2d Decision tree \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e17\u0e33\u0e01\u0e32\u0e23\u0e17\u0e33\u0e19\u0e32\u0e22\u0e04\u0e48\u0e32\u0e08\u0e32\u0e01\u0e17\u0e31\u0e49\u0e07 2 \u0e42\u0e21\u0e40\u0e14\u0e25 \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e19\u0e33\u0e04\u0e48\u0e32\u0e1e\u0e22\u0e32\u0e01\u0e23\u0e13\u0e4c\u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e08\u0e32\u0e01\u0e17\u0e31\u0e49\u0e07 2 \u0e42\u0e21\u0e40\u0e14\u0e25 \u0e41\u0e25\u0e30\u0e04\u0e48\u0e32\u0e1e\u0e22\u0e32\u0e01\u0e23\u0e13\u0e4c\u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e08\u0e32\u0e01\u0e02\u0e49\u0e2d 1 \u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e01\u0e25\u0e48\u0e32\u0e27\u0e21\u0e32 \u0e19\u0e33\u0e21\u0e32 weight \u0e2b\u0e32\u0e04\u0e48\u0e32\u0e1e\u0e22\u0e32\u0e01\u0e23\u0e13\u0e4c \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49\u0e19\u0e49\u0e33\u0e2b\u0e19\u0e31\u0e01 \u0e04\u0e37\u0e2d 0.1 0.25 \u0e41\u0e25\u0e30 0.65 \u0e15\u0e32\u0e21\u0e25\u0e33\u0e14\u0e31\u0e1a\n\u0e17\u0e33\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25 AdaBoost \u0e17\u0e35\u0e48\u0e21\u0e35 base estimator \u0e04\u0e37\u0e2d Decision tree \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e17\u0e33\u0e01\u0e32\u0e23\u0e1e\u0e22\u0e32\u0e01\u0e23\u0e13\u0e4c\u0e04\u0e48\u0e32 \u0e19\u0e33\u0e04\u0e48\u0e32\u0e1e\u0e22\u0e32\u0e01\u0e23\u0e13\u0e4c\u0e17\u0e35\u0e48\u0e44\u0e14\u0e49 \u0e21\u0e32 weight \u0e01\u0e31\u0e1a\u0e04\u0e48\u0e32\u0e1e\u0e22\u0e32\u0e01\u0e23\u0e13\u0e4c\u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e08\u0e32\u0e01\u0e02\u0e49\u0e2d 1 \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49\u0e19\u0e49\u0e33\u0e2b\u0e19\u0e31\u0e01 \u0e04\u0e37\u0e2d 0.3 \u0e41\u0e25\u0e30 0.7 \u0e15\u0e32\u0e21\u0e25\u0e33\u0e14\u0e31\u0e1a\n\u0e17\u0e33\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25\u0e41\u0e23\u0e01\u0e04\u0e37\u0e2d Ensemble \u0e08\u0e32\u0e01 3 \u0e42\u0e21\u0e40\u0e14\u0e25 \u0e44\u0e14\u0e49\u0e41\u0e01\u0e48 AdaBoost \u0e17\u0e35\u0e48\u0e21\u0e35 base estimator \u0e04\u0e37\u0e2d Decision tree \u0e08\u0e32\u0e01\u0e42\u0e21\u0e40\u0e14\u0e25 Bagging \u0e17\u0e35\u0e48\u0e21\u0e35 base estimator \u0e04\u0e37\u0e2d Decision tree \u0e41\u0e25\u0e30\u0e08\u0e32\u0e01\u0e42\u0e21\u0e40\u0e14\u0e25 Random Forest \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e19\u0e33\u0e42\u0e21\u0e40\u0e14\u0e25 Ensemble \u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e21\u0e32\u0e1e\u0e22\u0e32\u0e01\u0e23\u0e13\u0e4c\u0e04\u0e48\u0e32 \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e19\u0e33\u0e04\u0e48\u0e32\u0e1e\u0e22\u0e32\u0e01\u0e23\u0e13\u0e4c\u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e21\u0e32 weight \u0e01\u0e31\u0e1a\u0e04\u0e48\u0e32\u0e1e\u0e22\u0e32\u0e01\u0e23\u0e13\u0e4c\u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e08\u0e32\u0e01\u0e02\u0e49\u0e2d 1 \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49\u0e19\u0e49\u0e33\u0e2b\u0e19\u0e31\u0e01 \u0e04\u0e37\u0e2d 0.09 \u0e41\u0e25\u0e30 0.91 \u0e15\u0e32\u0e21\u0e25\u0e33\u0e14\u0e31\u0e1a\n\u0e17\u0e33\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25\u0e41\u0e23\u0e01\u0e04\u0e37\u0e2d \u0e01\u0e32\u0e23\u0e17\u0e33 Ensemble 3 \u0e42\u0e21\u0e40\u0e14\u0e25 \u0e08\u0e32\u0e01 AdaBoost \u0e17\u0e35\u0e48\u0e21\u0e35 base estimator \u0e04\u0e37\u0e2d Decision tree \u0e08\u0e32\u0e01\u0e42\u0e21\u0e40\u0e14\u0e25 Bagging \u0e17\u0e35\u0e48\u0e21\u0e35 base estimator \u0e04\u0e37\u0e2d Decision tree \u0e41\u0e25\u0e30\u0e08\u0e32\u0e01\u0e42\u0e21\u0e40\u0e14\u0e25 Random Forest \u0e2a\u0e48\u0e27\u0e19\u0e42\u0e21\u0e40\u0e14\u0e25\u0e17\u0e35\u0e48\u0e2a\u0e2d\u0e07 \u0e04\u0e37\u0e2d AdaBoost \u0e17\u0e35\u0e48\u0e21\u0e35 base estimator \u0e04\u0e37\u0e2d Decision tree \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e17\u0e33\u0e01\u0e32\u0e23\u0e17\u0e33\u0e19\u0e32\u0e22\u0e04\u0e48\u0e32\u0e08\u0e32\u0e01\u0e17\u0e31\u0e49\u0e07 2 \u0e42\u0e21\u0e40\u0e14\u0e25 \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e19\u0e33\u0e04\u0e48\u0e32\u0e1e\u0e22\u0e32\u0e01\u0e23\u0e13\u0e4c\u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e08\u0e32\u0e01\u0e17\u0e31\u0e49\u0e07 2 \u0e42\u0e21\u0e40\u0e14\u0e25 \u0e41\u0e25\u0e30\u0e04\u0e48\u0e32\u0e1e\u0e22\u0e32\u0e01\u0e23\u0e13\u0e4c\u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e08\u0e32\u0e01\u0e02\u0e49\u0e2d 1 \u0e19\u0e33\u0e21\u0e32 weight \u0e2b\u0e32\u0e04\u0e48\u0e32\u0e1e\u0e22\u0e32\u0e01\u0e23\u0e13\u0e4c \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49\u0e19\u0e49\u0e33\u0e2b\u0e19\u0e31\u0e01 \u0e04\u0e37\u0e2d 0.1 0.2 \u0e41\u0e25\u0e30 0.7 \u0e15\u0e32\u0e21\u0e25\u0e33\u0e14\u0e31\u0e1a\nHow we built it\n\u0e43\u0e19\u0e02\u0e31\u0e49\u0e19\u0e15\u0e2d\u0e19\u0e41\u0e23\u0e01 \u0e08\u0e30\u0e17\u0e33\u0e01\u0e32\u0e23\u0e2a\u0e33\u0e23\u0e27\u0e08\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e23\u0e31\u0e1a\u0e21\u0e32 \u0e40\u0e23\u0e32\u0e1e\u0e1a\u0e27\u0e48\u0e32\u0e21\u0e35\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e17\u0e35\u0e48\u0e1c\u0e34\u0e14\u0e1b\u0e01\u0e15\u0e34\u0e04\u0e37\u0e2d \u0e04\u0e2d\u0e25\u0e31\u0e21\u0e19\u0e4c surface area, volume \u0e41\u0e25\u0e30 void fraction \u0e17\u0e35\u0e48\u0e08\u0e30\u0e21\u0e35\u0e04\u0e48\u0e32 0,-1 \u0e41\u0e25\u0e30 \u0e04\u0e2d\u0e25\u0e31\u0e21\u0e19\u0e4c heat adsorption \u0e21\u0e35\u0e04\u0e48\u0e32\u0e40\u0e1b\u0e47\u0e19 infinity \u0e41\u0e15\u0e48\u0e17\u0e31\u0e49\u0e07\u0e04\u0e2d\u0e25\u0e31\u0e21\u0e19\u0e4c volume void fraction \u0e41\u0e25\u0e30 heat absorption\u0e21\u0e35\u0e08\u0e33\u0e19\u0e27\u0e19\u0e17\u0e35\u0e48\u0e1c\u0e34\u0e14\u0e1b\u0e01\u0e15\u0e34\u0e44\u0e21\u0e48\u0e21\u0e32\u0e01 \u0e40\u0e23\u0e32\u0e40\u0e25\u0e22\u0e43\u0e0a\u0e49\u0e27\u0e34\u0e18\u0e35\u0e15\u0e31\u0e14\u0e2d\u0e2d\u0e01 \u0e41\u0e15\u0e48 surface area \u0e21\u0e35\u0e08\u0e33\u0e19\u0e27\u0e19\u0e1c\u0e34\u0e14\u0e1b\u0e01\u0e15\u0e34\u0e21\u0e32\u0e01\u0e16\u0e36\u0e07 10000\u0e01\u0e27\u0e48\u0e32 record \u0e17\u0e33\u0e43\u0e2b\u0e49\u0e16\u0e49\u0e32\u0e15\u0e31\u0e14\u0e2d\u0e2d\u0e01 \u0e08\u0e30\u0e40\u0e2b\u0e25\u0e37\u0e2d\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e19\u0e49\u0e2d\u0e22\u0e25\u0e07\u0e44\u0e1b\u0e40\u0e22\u0e2d\u0e30 \u0e40\u0e23\u0e32\u0e40\u0e25\u0e22\u0e04\u0e34\u0e14\u0e27\u0e34\u0e18\u0e35\u0e41\u0e01\u0e49\u0e44\u0e02\u0e14\u0e31\u0e07\u0e19\u0e35\u0e49\n\u0e15\u0e31\u0e14\u0e04\u0e2d\u0e25\u0e31\u0e21\u0e19\u0e4c surface area \u0e2d\u0e2d\u0e01\nImpute surface area \u0e14\u0e49\u0e27\u0e22\u0e04\u0e48\u0e32\u0e15\u0e48\u0e32\u0e07\u0e46\n\u0e19\u0e33\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e02\u0e2d\u0e07 mofs \u0e21\u0e32\u0e04\u0e33\u0e19\u0e27\u0e13\u0e40\u0e1b\u0e47\u0e19 surface area\n\u0e15\u0e31\u0e14 surface area record \u0e17\u0e35\u0e48\u0e1c\u0e34\u0e14\u0e1b\u0e01\u0e15\u0e34\u0e2d\u0e2d\u0e01\n\u0e0b\u0e36\u0e48\u0e07\u0e27\u0e34\u0e18\u0e35\u0e17\u0e35\u0e48 1 \u0e01\u0e31\u0e1a 4 \u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e25\u0e2d\u0e07\u0e17\u0e33\u0e41\u0e25\u0e49\u0e27 \u0e1e\u0e1a\u0e27\u0e48\u0e32\u0e1c\u0e25\u0e25\u0e31\u0e1e\u0e18\u0e4c\u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e08\u0e32\u0e01\u0e42\u0e21\u0e40\u0e14\u0e25\u0e41\u0e22\u0e48\u0e25\u0e07\u0e21\u0e32\u0e01 \u0e40\u0e23\u0e32\u0e08\u0e36\u0e07\u0e04\u0e34\u0e14\u0e27\u0e48\u0e32 \u0e04\u0e27\u0e23\u0e19\u0e33\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e02\u0e2d\u0e07 mofs\u0e41\u0e15\u0e48\u0e25\u0e30\u0e15\u0e31\u0e27\u0e21\u0e32\u0e04\u0e33\u0e19\u0e27\u0e13\u0e40\u0e1b\u0e47\u0e19 surface area \u0e41\u0e15\u0e48\u0e40\u0e19\u0e37\u0e48\u0e2d\u0e07\u0e08\u0e32\u0e01\u0e01\u0e32\u0e23\u0e14\u0e36\u0e07\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e02\u0e2d\u0e07mofs \u0e21\u0e32\u0e04\u0e33\u0e19\u0e27\u0e13\u0e43\u0e0a\u0e49\u0e40\u0e27\u0e25\u0e32\u0e19\u0e32\u0e19\u0e21\u0e32\u0e01 \u0e23\u0e30\u0e2b\u0e27\u0e48\u0e32\u0e07\u0e19\u0e31\u0e49\u0e19\u0e40\u0e23\u0e32\u0e08\u0e36\u0e07\u0e17\u0e14\u0e25\u0e2d\u0e07\u0e43\u0e0a\u0e49\u0e27\u0e34\u0e18\u0e35 impute surface area \u0e14\u0e49\u0e27\u0e22 linear regression \u0e41\u0e25\u0e30 stochastic \u0e0b\u0e36\u0e48\u0e07\u0e1c\u0e25\u0e2d\u0e2d\u0e01\u0e21\u0e32\u0e27\u0e48\u0e32 linear \u0e44\u0e14\u0e49\u0e1c\u0e25\u0e17\u0e35\u0e48\u0e14\u0e35\u0e01\u0e27\u0e48\u0e32\u0e40\u0e23\u0e32\u0e08\u0e36\u0e07\u0e43\u0e0a\u0e49\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e40\u0e0b\u0e47\u0e17\u0e19\u0e35\u0e49\u0e43\u0e19\u0e01\u0e32\u0e23\u0e17\u0e33\u0e42\u0e21\u0e40\u0e14\u0e25\u0e43\u0e19\u0e23\u0e30\u0e2b\u0e27\u0e48\u0e32\u0e07\u0e17\u0e35\u0e48\u0e23\u0e2d\u0e01\u0e32\u0e23\u0e14\u0e36\u0e07\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e08\u0e23\u0e34\u0e07\u0e46\u0e02\u0e2d\u0e07 mofs\n\u0e2b\u0e25\u0e31\u0e07\u0e08\u0e32\u0e01\u0e17\u0e35\u0e48\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e14\u0e36\u0e07\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e21\u0e32\u0e44\u0e14\u0e49\u0e08\u0e19\u0e04\u0e23\u0e1a\u0e41\u0e25\u0e49\u0e27\u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e17\u0e33\u0e01\u0e32\u0e23\u0e40\u0e17\u0e35\u0e22\u0e1a\u0e27\u0e48\u0e32 gsa \u0e2b\u0e23\u0e37\u0e2d vsa \u0e17\u0e35\u0e48\u0e21\u0e35\u0e04\u0e27\u0e32\u0e21\u0e43\u0e01\u0e25\u0e49\u0e40\u0e04\u0e35\u0e22\u0e07\u0e01\u0e31\u0e1a surface area \u0e17\u0e35\u0e48\u0e21\u0e35\u0e2d\u0e22\u0e39\u0e48\u0e43\u0e19data set\u0e21\u0e32\u0e01\u0e01\u0e27\u0e48\u0e32\u0e01\u0e31\u0e19 \u0e1e\u0e1a\u0e27\u0e48\u0e32 vsa \u0e21\u0e35\u0e04\u0e27\u0e32\u0e21\u0e43\u0e01\u0e25\u0e49\u0e40\u0e04\u0e35\u0e22\u0e07\u0e01\u0e27\u0e48\u0e32\u0e21\u0e32\u0e01 \u0e40\u0e23\u0e32\u0e08\u0e36\u0e07\u0e15\u0e31\u0e14\u0e2a\u0e34\u0e19\u0e43\u0e08\u0e40\u0e25\u0e37\u0e2d\u0e01 vsa \u0e43\u0e19\u0e01\u0e32\u0e23 impute \u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25 surface area\u0e17\u0e35\u0e48\u0e1c\u0e34\u0e14\u0e1b\u0e01\u0e15\u0e34\n\u0e43\u0e19\u0e15\u0e2d\u0e19\u0e41\u0e23\u0e01 \u0e40\u0e23\u0e32\u0e43\u0e0a\u0e49\u0e41\u0e04\u0e48 feature \u0e17\u0e35\u0e48\u0e21\u0e35\u0e2d\u0e22\u0e39\u0e48\u0e43\u0e19 data set \u0e44\u0e21\u0e48\u0e44\u0e14\u0e49\u0e21\u0e35\u0e01\u0e32\u0e23\u0e2b\u0e32 feature \u0e40\u0e1e\u0e34\u0e48\u0e21 \u0e1e\u0e1a\u0e27\u0e48\u0e32 feature\u0e17\u0e35\u0e48\u0e21\u0e35\u0e2d\u0e22\u0e39\u0e48\u0e19\u0e49\u0e2d\u0e22\u0e40\u0e01\u0e34\u0e19\u0e44\u0e1b \u0e17\u0e33\u0e43\u0e2b\u0e49\u0e1c\u0e25\u0e25\u0e31\u0e1e\u0e18\u0e4c\u0e08\u0e32\u0e01\u0e01\u0e32\u0e23\u0e17\u0e33\u0e42\u0e21\u0e40\u0e14\u0e25\u0e44\u0e21\u0e48\u0e14\u0e35\u0e40\u0e17\u0e48\u0e32\u0e17\u0e35\u0e48\u0e04\u0e27\u0e23 \u0e40\u0e23\u0e32\u0e08\u0e36\u0e07\u0e2b\u0e32 feature \u0e40\u0e1e\u0e34\u0e48\u0e21\u0e41\u0e15\u0e48\u0e01\u0e32\u0e23\u0e08\u0e30\u0e2b\u0e32 feature \u0e40\u0e1e\u0e34\u0e48\u0e21\u0e44\u0e14\u0e49 \u0e15\u0e49\u0e2d\u0e07\u0e43\u0e0a\u0e49 smiles \u0e41\u0e15\u0e48\u0e40\u0e19\u0e37\u0e48\u0e2d\u0e07\u0e08\u0e32\u0e01\u0e40\u0e23\u0e32\u0e44\u0e21\u0e48\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e2b\u0e32 library\u0e17\u0e35\u0e48\u0e08\u0e30\u0e41\u0e1b\u0e25\u0e07 functional group \u0e40\u0e1b\u0e47\u0e19 smiles \u0e44\u0e14\u0e49 \u0e40\u0e23\u0e32\u0e08\u0e36\u0e07\u0e40\u0e02\u0e35\u0e22\u0e19\u0e1f\u0e31\u0e07\u0e01\u0e4c\u0e0a\u0e31\u0e48\u0e19\u0e43\u0e19 python \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e41\u0e1b\u0e25\u0e07 functional group \u0e44\u0e1b\u0e40\u0e1b\u0e47\u0e19 smiles \u0e40\u0e2d\u0e07 \u0e42\u0e14\u0e22\u0e2d\u0e34\u0e07\u0e2b\u0e25\u0e31\u0e01\u0e01\u0e32\u0e23\u0e08\u0e32\u0e01\u0e27\u0e34\u0e18\u0e35\u0e41\u0e1b\u0e25\u0e07 functional group \u0e40\u0e1b\u0e47\u0e19 smiles \u0e40\u0e0a\u0e48\u0e19 OMe \u0e21\u0e35 smile \u0e04\u0e37\u0e2d CO- \u0e41\u0e25\u0e30 COOH\u0e21\u0e35 smile \u0e04\u0e37\u0e2d -C(=O)(O)\u0e40\u0e21\u0e37\u0e48\u0e2d\u0e19\u0e33\u0e21\u0e32\u0e23\u0e27\u0e21\u0e01\u0e31\u0e19 OMe-COOH \u0e08\u0e30\u0e21\u0e35 smile \u0e04\u0e37\u0e2d COC(=O)(O) \u0e40\u0e1b\u0e47\u0e19\u0e15\u0e49\u0e19 \u0e40\u0e21\u0e37\u0e48\u0e2d\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e2b\u0e32 smiles \u0e02\u0e2d\u0e07MOFs \u0e17\u0e38\u0e01\u0e15\u0e31\u0e27\u0e44\u0e14\u0e49\u0e41\u0e25\u0e49\u0e27 \u0e40\u0e23\u0e32\u0e08\u0e36\u0e07\u0e2b\u0e32feature\u0e40\u0e1e\u0e34\u0e48\u0e21 \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49library \u0e17\u0e32\u0e07\u0e40\u0e04\u0e21\u0e35\u0e15\u0e48\u0e32\u0e07\u0e46 \u0e14\u0e31\u0e07\u0e19\u0e35\u0e49 library rdkit \u0e44\u0e14\u0e49\u0e1f\u0e35\u0e40\u0e08\u0e2d\u0e23\u0e4c MolWt, MolLogP, NumValenceElectrons, HeavyAtomCount, NOCount, NumAliphaticRings, NumAromaticRings ,NumHAcceptors, NumHDonors, NumHeteroatoms, NumRotatableBonds\nLibrary pubchempy \u0e44\u0e14\u0e49\u0e1f\u0e35\u0e40\u0e08\u0e2d\u0e23\u0e4c \u0e14\u0e31\u0e07\u0e19\u0e35\u0e49 MolecularFormula, MolecularWeight, CanonicalSMILES, IsomericSMILES, InChI, InChIKey, IUPACName, XLogP, ExactMass, MonoisotopicMass, TPSA, Complexity, Charge, HBondDonorCount, HBondAcceptorCount, RotatableBondCount, IsotopeAtomCount, AtomStereoCount, DefinedAtomStereoCount, UndefinedAtomStereoCount, BondStereoCount, DefinedBondStereoCount, UndefinedBondStereoCount, CovalentUnitCount\n\u0e40\u0e21\u0e37\u0e48\u0e2d\u0e25\u0e2d\u0e07\u0e19\u0e33\u0e21\u0e32\u0e17\u0e33\u0e42\u0e21\u0e40\u0e14\u0e25 \u0e42\u0e14\u0e22\u0e17\u0e35\u0e48 drop \u0e15\u0e31\u0e27\u0e41\u0e1b\u0e23\u0e17\u0e35\u0e48\u0e40\u0e1b\u0e47\u0e19\u0e0a\u0e37\u0e48\u0e2d\u0e15\u0e48\u0e32\u0e07\u0e46\u0e2d\u0e2d\u0e01 \u0e17\u0e33\u0e43\u0e2b\u0e49\u0e1c\u0e25\u0e25\u0e31\u0e1e\u0e18\u0e4c\u0e02\u0e2d\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25\u0e14\u0e35\u0e02\u0e36\u0e49\u0e19\u0e21\u0e32\u0e01\n\u0e43\u0e19\u0e2a\u0e48\u0e27\u0e19\u0e02\u0e2d\u0e07\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25 \u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e41\u0e1a\u0e48\u0e07\u0e01\u0e31\u0e19\u0e44\u0e1b\u0e17\u0e33\u0e42\u0e21\u0e40\u0e14\u0e25\u0e15\u0e48\u0e32\u0e07\u0e46\u0e14\u0e31\u0e07\u0e19\u0e35\u0e49\n\u0e17\u0e33\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25 Deep neural network \u0e43\u0e0a\u0e49 hyperband, random search \u0e43\u0e19\u0e01\u0e32\u0e23\u0e17\u0e33 hyperparameter tuning \u0e40\u0e21\u0e37\u0e48\u0e2d\u0e44\u0e14\u0e49 hyperparameter \u0e17\u0e35\u0e48\u0e14\u0e35\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14\u0e08\u0e32\u0e01 search \u0e21\u0e32\u0e41\u0e25\u0e49\u0e27 \u0e40\u0e23\u0e32\u0e01\u0e47\u0e44\u0e14\u0e49\u0e21\u0e32\u0e25\u0e2d\u0e07\u0e40\u0e1e\u0e34\u0e48\u0e21 learning rate schedule, drop out , regularization \u0e41\u0e25\u0e30\u0e25\u0e2d\u0e07\u0e1b\u0e23\u0e31\u0e1a node, layer \u0e15\u0e48\u0e32\u0e07\u0e46 \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e41\u0e01\u0e49 overfitting \u0e41\u0e25\u0e30\u0e17\u0e33\u0e43\u0e2b\u0e49\u0e42\u0e21\u0e40\u0e14\u0e25\u0e14\u0e35\u0e02\u0e36\u0e49\u0e19 \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e25\u0e2d\u0e07\u0e17\u0e33 functional api \u0e43\u0e19\u0e01\u0e32\u0e23\u0e23\u0e27\u0e21 2 \u0e42\u0e21\u0e40\u0e14\u0e25\u0e40\u0e02\u0e49\u0e32\u0e14\u0e49\u0e27\u0e22\u0e01\u0e31\u0e19 \u0e0b\u0e36\u0e48\u0e07\u0e1c\u0e25\u0e2d\u0e2d\u0e01\u0e21\u0e32\u0e22\u0e31\u0e07\u0e44\u0e21\u0e48\u0e14\u0e35\u0e40\u0e17\u0e48\u0e32\u0e01\u0e31\u0e1a\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49 weight \u0e23\u0e30\u0e2b\u0e27\u0e48\u0e32\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25\u0e42\u0e14\u0e22\u0e15\u0e23\u0e07\n\u0e17\u0e33\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25 Tree-based \u0e42\u0e14\u0e22\u0e42\u0e21\u0e40\u0e14\u0e25\u0e17\u0e35\u0e48\u0e2a\u0e23\u0e49\u0e32\u0e07 \u0e44\u0e14\u0e49\u0e41\u0e01\u0e48 Decision tree, Random Forest, XGBoost, Gradient Boosting, Bagging \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49 base estimator \u0e17\u0e35\u0e48\u0e2b\u0e25\u0e32\u0e01\u0e2b\u0e25\u0e32\u0e22, AdaBoost \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49 base estimator \u0e17\u0e35\u0e48\u0e2b\u0e25\u0e32\u0e01\u0e2b\u0e25\u0e32\u0e22, Ensemble \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49\u0e08\u0e32\u0e01 Adaboost \u0e17\u0e35\u0e48\u0e43\u0e0a\u0e49 base estimator \u0e04\u0e37\u0e2d Decision Tree \u0e08\u0e32\u0e01 Bagging \u0e17\u0e35\u0e48\u0e43\u0e0a\u0e49 base estimator \u0e04\u0e37\u0e2d Decision Tree \u0e41\u0e25\u0e30\u0e08\u0e32\u0e01 Random Forest \u0e41\u0e25\u0e30\u0e44\u0e14\u0e49\u0e21\u0e35\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49 Grid Search \u0e41\u0e25\u0e30 Randomized Search \u0e43\u0e19\u0e43\u0e19\u0e01\u0e32\u0e23\u0e2b\u0e32\u0e1e\u0e32\u0e23\u0e32\u0e21\u0e34\u0e40\u0e15\u0e2d\u0e23\u0e4c\u0e17\u0e35\u0e48\u0e40\u0e2b\u0e21\u0e32\u0e30\u0e2a\u0e21\n\u0e17\u0e33\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25 KNN-Regressor \u0e42\u0e14\u0e22\u0e17\u0e33\u0e01\u0e32\u0e23\u0e2a\u0e40\u0e01\u0e25\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e43\u0e2b\u0e49\u0e2d\u0e22\u0e39\u0e48\u0e43\u0e19\u0e0a\u0e48\u0e27\u0e07 0-1 \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e2b\u0e32\u0e04\u0e48\u0e32 k \u0e17\u0e35\u0e48\u0e17\u0e33\u0e43\u0e2b\u0e49 Root-squared mean error (rsme) \u0e21\u0e35\u0e04\u0e48\u0e32\u0e19\u0e49\u0e2d\u0e22\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14 \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19 \u0e19\u0e33\u0e04\u0e48\u0e32 k \u0e21\u0e32\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e15\u0e31\u0e27\u0e41\u0e1a\u0e1a \u0e41\u0e25\u0e30\u0e1b\u0e23\u0e31\u0e1a\u0e1e\u0e32\u0e23\u0e32\u0e21\u0e34\u0e40\u0e15\u0e2d\u0e23\u0e4c \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e43\u0e2b\u0e49\u0e04\u0e48\u0e32 mean absolute error (mae) \u0e21\u0e35\u0e04\u0e48\u0e32\u0e15\u0e48\u0e33\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14 \u0e41\u0e25\u0e30\u0e17\u0e33\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25 Ensemble \u0e42\u0e14\u0e22\u0e40\u0e25\u0e37\u0e2d\u0e01\u0e08\u0e32\u0e01 Linear regression, AdaBoostRegressor, XGBoost, Randomforest, K-neighbors Regression \u0e21\u0e32 3 \u0e42\u0e21\u0e40\u0e14\u0e25 \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e19\u0e33\u0e21\u0e32\u0e1b\u0e23\u0e30\u0e01\u0e2d\u0e1a\u0e01\u0e31\u0e19 \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e17\u0e33\u0e01\u0e32\u0e23\u0e40\u0e09\u0e25\u0e35\u0e48\u0e22\u0e04\u0e48\u0e32\u0e17\u0e35\u0e48 predict \u0e44\u0e14\u0e49\n\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25 Light GBM Regressor \u0e42\u0e14\u0e22\u0e17\u0e33\u0e01\u0e32\u0e23\u0e2a\u0e40\u0e01\u0e25\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e43\u0e2b\u0e49\u0e2d\u0e22\u0e39\u0e48\u0e43\u0e19\u0e0a\u0e48\u0e27\u0e07 0-1 \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e2b\u0e32 \u0e08\u0e39\u0e19\u0e1e\u0e32\u0e23\u0e32\u0e21\u0e34\u0e40\u0e15\u0e2d\u0e23\u0e4c\u0e17\u0e35\u0e48\u0e08\u0e30\u0e17\u0e33\u0e43\u0e2b\u0e49 Root-squared mean error (rsme) \u0e21\u0e35\u0e04\u0e48\u0e32\u0e19\u0e49\u0e2d\u0e22\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14\n\u0e28\u0e36\u0e01\u0e29\u0e32\u0e43\u0e19\u0e2a\u0e48\u0e27\u0e19\u0e02\u0e2d\u0e07 Library Ensemble \u0e08\u0e32\u0e01 sklearn \u0e41\u0e25\u0e30\u0e17\u0e14\u0e25\u0e2d\u0e07\u0e43\u0e0a\u0e49\u0e01\u0e31\u0e1a\u0e42\u0e21\u0e40\u0e14\u0e25\u0e17\u0e35\u0e48\u0e17\u0e35\u0e21\u0e44\u0e14\u0e49\u0e17\u0e33\u0e21\u0e32\u0e1b\u0e23\u0e30\u0e01\u0e2d\u0e1a\u0e01\u0e31\u0e19\u0e43\u0e19\u0e2b\u0e25\u0e32\u0e22\u0e46\u0e23\u0e39\u0e1b\u0e41\u0e1a\u0e1a\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e2b\u0e32\u0e42\u0e21\u0e40\u0e14\u0e25\u0e17\u0e35\u0e48\u0e14\u0e35\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14\u0e08\u0e32\u0e01\u0e01\u0e32\u0e23 ensemble model \u0e41\u0e25\u0e30\u0e44\u0e14\u0e49\u0e44\u0e1b\u0e28\u0e36\u0e01\u0e29\u0e32\u0e40\u0e01\u0e35\u0e48\u0e22\u0e27\u0e01\u0e31\u0e1a Library \u0e17\u0e35\u0e48\u0e40\u0e01\u0e35\u0e48\u0e22\u0e27\u0e01\u0e31\u0e1a automl \u0e42\u0e14\u0e22\u0e44\u0e1b\u0e28\u0e36\u0e01\u0e29\u0e32\u0e14\u0e39 benchmark \u0e43\u0e19 Library \u0e15\u0e48\u0e32\u0e07\u0e46\u0e41\u0e25\u0e30\u0e44\u0e14\u0e49\u0e17\u0e14\u0e25\u0e2d\u0e07\u0e19\u0e33\u0e21\u0e32\u0e43\u0e0a\u0e49\u0e01\u0e31\u0e1a\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e40\u0e0a\u0e48\u0e19 Library autokeras, autosklearn, FLAML, h2o\n\u0e08\u0e32\u0e01\u0e17\u0e35\u0e48\u0e01\u0e25\u0e48\u0e32\u0e27\u0e21\u0e32\u0e02\u0e49\u0e32\u0e07\u0e15\u0e49\u0e19 \u0e42\u0e21\u0e40\u0e14\u0e25\u0e17\u0e35\u0e48\u0e41\u0e15\u0e48\u0e25\u0e30\u0e04\u0e19\u0e44\u0e14\u0e49\u0e17\u0e33\u0e01\u0e32\u0e23\u0e1e\u0e22\u0e32\u0e01\u0e23\u0e13\u0e4c\u0e04\u0e48\u0e32 CO2 \u0e21\u0e32 \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e40\u0e23\u0e32\u0e19\u0e33\u0e42\u0e21\u0e40\u0e14\u0e25\u0e17\u0e35\u0e48\u0e14\u0e35\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14\u0e02\u0e2d\u0e07\u0e41\u0e15\u0e48\u0e25\u0e30\u0e04\u0e19\u0e21\u0e32\u0e17\u0e33\u0e01\u0e32\u0e23 weight \u0e19\u0e49\u0e33\u0e2b\u0e19\u0e31\u0e01\u0e02\u0e2d\u0e07\u0e04\u0e48\u0e32\u0e1e\u0e22\u0e32\u0e01\u0e23\u0e13\u0e4c \u0e40\u0e0a\u0e48\u0e19 \u0e2a\u0e21\u0e32\u0e0a\u0e34\u0e01\u0e04\u0e19\u0e41\u0e23\u0e01\u0e44\u0e14\u0e49\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25\u0e17\u0e33\u0e19\u0e32\u0e22\u0e04\u0e48\u0e32 \u0e44\u0e14\u0e49\u0e04\u0e48\u0e32 log(MAE) = 1.23 \u0e2a\u0e21\u0e32\u0e0a\u0e34\u0e01\u0e04\u0e19\u0e17\u0e35\u0e48 2 \u0e2a\u0e23\u0e49\u0e32\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25\u0e17\u0e33\u0e19\u0e32\u0e22\u0e04\u0e48\u0e32 \u0e44\u0e14\u0e49\u0e04\u0e48\u0e32 log(MAE) = 1.24 \u0e41\u0e25\u0e30\u0e2a\u0e21\u0e32\u0e0a\u0e34\u0e01\u0e04\u0e19\u0e17\u0e35\u0e48 3 \u0e2a\u0e23\u0e49\u0e32\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25\u0e17\u0e33\u0e19\u0e32\u0e22\u0e04\u0e48\u0e32 \u0e44\u0e14\u0e49\u0e04\u0e48\u0e32 log(MAE) = 1.25 \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e40\u0e23\u0e32\u0e17\u0e33\u0e01\u0e32\u0e23 weight \u0e04\u0e48\u0e32\u0e1e\u0e22\u0e32\u0e01\u0e23\u0e13\u0e4c\u0e02\u0e2d\u0e07\u0e41\u0e15\u0e48\u0e42\u0e21\u0e40\u0e14\u0e25 \u0e2d\u0e32\u0e17\u0e34 0.65 0.25 0.1 \u0e42\u0e14\u0e22\u0e01\u0e32\u0e23\u0e17\u0e35\u0e48\u0e40\u0e23\u0e32\u0e17\u0e33 weight \u0e04\u0e48\u0e32\u0e1e\u0e22\u0e32\u0e01\u0e23\u0e13\u0e4c\u0e41\u0e15\u0e48\u0e25\u0e30\u0e42\u0e21\u0e40\u0e14\u0e25\u0e19\u0e35\u0e49 \u0e17\u0e33\u0e43\u0e2b\u0e49\u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e04\u0e48\u0e32 log(MAE) \u0e17\u0e35\u0e48\u0e19\u0e49\u0e2d\u0e22\u0e25\u0e07 \u0e40\u0e23\u0e32\u0e08\u0e36\u0e07\u0e19\u0e33\u0e27\u0e34\u0e18\u0e35\u0e01\u0e32\u0e23\u0e19\u0e35\u0e49\u0e21\u0e32\u0e2b\u0e32\u0e04\u0e48\u0e32\u0e17\u0e33\u0e19\u0e32\u0e22 CO2\nChallenges we ran into\n\u0e40\u0e19\u0e37\u0e48\u0e2d\u0e07\u0e08\u0e32\u0e01\u0e40\u0e1b\u0e47\u0e19\u0e01\u0e32\u0e23\u0e41\u0e02\u0e48\u0e07\u0e02\u0e31\u0e19\u0e14\u0e49\u0e32\u0e19 ML \u0e17\u0e35\u0e48\u0e01\u0e25\u0e38\u0e48\u0e21\u0e40\u0e23\u0e32\u0e23\u0e48\u0e27\u0e21\u0e41\u0e02\u0e48\u0e07\u0e01\u0e31\u0e19\u0e01\u0e31\u0e19\u0e40\u0e1b\u0e47\u0e19\u0e04\u0e23\u0e31\u0e49\u0e07\u0e41\u0e23\u0e01 \u0e2d\u0e35\u0e01\u0e17\u0e31\u0e49\u0e07\u0e40\u0e1b\u0e47\u0e19\u0e40\u0e23\u0e37\u0e48\u0e2d\u0e07\u0e40\u0e01\u0e35\u0e48\u0e22\u0e27\u0e01\u0e31\u0e1a\u0e17\u0e32\u0e07\u0e14\u0e49\u0e32\u0e19\u0e40\u0e04\u0e21\u0e35\u0e17\u0e35\u0e48\u0e01\u0e25\u0e38\u0e48\u0e21\u0e40\u0e23\u0e32\u0e44\u0e21\u0e48\u0e04\u0e48\u0e2d\u0e22\u0e21\u0e35\u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49\u0e21\u0e32\u0e01\u0e19\u0e31\u0e01 \u0e40\u0e19\u0e37\u0e48\u0e2d\u0e07\u0e08\u0e32\u0e01\u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32\u0e40\u0e23\u0e35\u0e22\u0e19\u0e21\u0e32\u0e17\u0e32\u0e07\u0e14\u0e49\u0e32\u0e19\u0e2a\u0e16\u0e34\u0e15\u0e34 \u0e08\u0e36\u0e07\u0e15\u0e49\u0e2d\u0e07\u0e04\u0e49\u0e19\u0e2b\u0e32\u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49\u0e17\u0e32\u0e07\u0e14\u0e49\u0e32\u0e19\u0e40\u0e04\u0e21\u0e35 \u0e40\u0e0a\u0e48\u0e19 \u0e01\u0e32\u0e23\u0e41\u0e1b\u0e25\u0e07 functional group \u0e40\u0e1b\u0e47\u0e19 smiles \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e2b\u0e32 feature \u0e40\u0e1e\u0e34\u0e48\u0e21 \u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e44\u0e1b\u0e04\u0e49\u0e19\u0e2b\u0e32\u0e14\u0e39\u0e27\u0e48\u0e32\u0e21\u0e35\u0e1f\u0e31\u0e07\u0e01\u0e4c\u0e0a\u0e31\u0e19\u0e43\u0e19 python \u0e2d\u0e30\u0e44\u0e23\u0e1a\u0e49\u0e32\u0e07\u0e17\u0e35\u0e48\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e41\u0e1b\u0e25\u0e07 functional group \u0e40\u0e1b\u0e47\u0e19 smile \u0e44\u0e14\u0e49 \u0e0b\u0e36\u0e48\u0e07\u0e40\u0e23\u0e32\u0e44\u0e21\u0e48\u0e1e\u0e1a \u0e40\u0e23\u0e32\u0e08\u0e36\u0e07\u0e41\u0e01\u0e49\u0e1b\u0e31\u0e0d\u0e2b\u0e32\u0e14\u0e49\u0e27\u0e22\u0e01\u0e32\u0e23\u0e2b\u0e32\u0e2b\u0e25\u0e31\u0e01\u0e01\u0e32\u0e23\u0e41\u0e1b\u0e25\u0e07 functional group \u0e40\u0e1b\u0e47\u0e19 smiles \u0e41\u0e25\u0e30\u0e40\u0e02\u0e35\u0e22\u0e19\u0e1f\u0e31\u0e07\u0e01\u0e4c\u0e0a\u0e31\u0e48\u0e19\u0e43\u0e19\u0e01\u0e32\u0e23\u0e41\u0e1b\u0e25\u0e07\u0e40\u0e2d\u0e07\u0e42\u0e14\u0e22\u0e2d\u0e34\u0e07\u0e08\u0e32\u0e01\u0e2b\u0e25\u0e31\u0e01\u0e01\u0e32\u0e23\u0e17\u0e35\u0e48\u0e01\u0e25\u0e48\u0e32\u0e27\u0e44\u0e1b\u0e02\u0e49\u0e32\u0e07\u0e15\u0e49\u0e19 \u0e17\u0e33\u0e43\u0e2b\u0e49\u0e40\u0e23\u0e32\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e41\u0e1b\u0e25\u0e07 functional group \u0e40\u0e1b\u0e47\u0e19 smiles \u0e44\u0e14\u0e49\u0e2a\u0e33\u0e40\u0e23\u0e47\u0e08\u0e41\u0e25\u0e30\u0e17\u0e33\u0e43\u0e2b\u0e49\u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e15\u0e31\u0e27\u0e41\u0e1b\u0e23\u0e2d\u0e37\u0e48\u0e19\u0e40\u0e1e\u0e34\u0e48\u0e21\u0e02\u0e36\u0e49\u0e19\u0e40\u0e22\u0e2d\u0e30\u0e21\u0e32\u0e01 \u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e19\u0e33\u0e15\u0e31\u0e27\u0e41\u0e1b\u0e23\u0e17\u0e35\u0e48\u0e40\u0e1e\u0e34\u0e48\u0e21\u0e21\u0e32\u0e1e\u0e31\u0e12\u0e19\u0e32\u0e42\u0e21\u0e40\u0e14\u0e25\u0e44\u0e14\u0e49\u0e21\u0e35\u0e1b\u0e23\u0e30\u0e2a\u0e34\u0e17\u0e18\u0e34\u0e20\u0e32\u0e1e\u0e21\u0e32\u0e01\u0e02\u0e36\u0e49\u0e19 \u0e19\u0e2d\u0e01\u0e08\u0e32\u0e01\u0e19\u0e35\u0e49 \u0e22\u0e31\u0e07\u0e1e\u0e1a\u0e1b\u0e31\u0e0d\u0e2b\u0e32\u0e43\u0e19\u0e01\u0e32\u0e23\u0e04\u0e33\u0e19\u0e27\u0e13 surface \u0e08\u0e32\u0e01\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25 MOFs \u0e0b\u0e36\u0e48\u0e07\u0e21\u0e35\u0e1b\u0e23\u0e34\u0e21\u0e32\u0e13\u0e21\u0e32\u0e01\u0e41\u0e25\u0e30\u0e15\u0e49\u0e2d\u0e07\u0e43\u0e0a\u0e49\u0e40\u0e27\u0e25\u0e32\u0e19\u0e32\u0e19 \u0e17\u0e38\u0e01\u0e04\u0e19\u0e43\u0e19\u0e17\u0e35\u0e21\u0e08\u0e36\u0e07\u0e41\u0e1a\u0e48\u0e07\u0e07\u0e32\u0e19\u0e01\u0e31\u0e19\u0e23\u0e31\u0e19\u0e2b\u0e32 surface \u0e17\u0e33\u0e43\u0e2b\u0e49\u0e43\u0e0a\u0e49\u0e40\u0e27\u0e25\u0e32\u0e19\u0e49\u0e2d\u0e22\u0e25\u0e07\nAccomplishments that we're proud of\n\u0e20\u0e32\u0e04\u0e20\u0e39\u0e21\u0e34\u0e43\u0e08\u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e25\u0e2d\u0e07\u0e2b\u0e32\u0e2a\u0e34\u0e48\u0e07\u0e43\u0e2b\u0e21\u0e48 \u0e46 \u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49\u0e43\u0e2b\u0e21\u0e48 \u0e46 \u0e19\u0e2d\u0e01\u0e2b\u0e49\u0e2d\u0e07\u0e40\u0e23\u0e35\u0e22\u0e19 \u0e44\u0e14\u0e49\u0e25\u0e2d\u0e07\u0e44\u0e14\u0e49\u0e21\u0e32\u0e40\u0e08\u0e2d\u0e1b\u0e31\u0e0d\u0e2b\u0e32\u0e08\u0e23\u0e34\u0e07 \u0e44\u0e14\u0e49\u0e25\u0e2d\u0e07\u0e21\u0e32\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25\u0e08\u0e23\u0e34\u0e07 \u0e40\u0e19\u0e37\u0e48\u0e2d\u0e07\u0e08\u0e32\u0e01\u0e01\u0e32\u0e23\u0e41\u0e02\u0e48\u0e07\u0e04\u0e23\u0e31\u0e49\u0e07\u0e19\u0e35\u0e49\u0e40\u0e1b\u0e47\u0e19\u0e01\u0e32\u0e23\u0e41\u0e02\u0e48\u0e07\u0e02\u0e31\u0e19\u0e14\u0e49\u0e32\u0e19 machine learning \u0e04\u0e23\u0e31\u0e49\u0e07\u0e41\u0e23\u0e01\u0e02\u0e2d\u0e07\u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32 \u0e16\u0e36\u0e07\u0e41\u0e21\u0e49\u0e01\u0e48\u0e2d\u0e19\u0e2b\u0e19\u0e49\u0e32\u0e19\u0e35\u0e49\u0e08\u0e30\u0e21\u0e35\u0e42\u0e2d\u0e01\u0e32\u0e2a\u0e44\u0e14\u0e49\u0e25\u0e2d\u0e07\u0e17\u0e33\u0e41\u0e1a\u0e1a\u0e1d\u0e36\u0e01\u0e2b\u0e31\u0e14\u0e40\u0e23\u0e37\u0e48\u0e2d\u0e07 machine learning \u0e43\u0e19\u0e2b\u0e49\u0e2d\u0e07\u0e40\u0e23\u0e35\u0e22\u0e19\u0e21\u0e32\u0e1a\u0e49\u0e32\u0e07 \u0e41\u0e15\u0e48\u0e01\u0e47\u0e22\u0e31\u0e07\u0e15\u0e49\u0e2d\u0e07\u0e28\u0e36\u0e01\u0e29\u0e32\u0e40\u0e1e\u0e34\u0e48\u0e21\u0e40\u0e15\u0e34\u0e21\u0e41\u0e25\u0e30\u0e1e\u0e31\u0e12\u0e19\u0e32\u0e17\u0e31\u0e01\u0e29\u0e30\u0e40\u0e1e\u0e34\u0e48\u0e21\u0e21\u0e32\u0e01\u0e02\u0e36\u0e49\u0e19 \u0e43\u0e19\u0e01\u0e32\u0e23\u0e41\u0e02\u0e48\u0e07\u0e02\u0e31\u0e19\u0e04\u0e23\u0e31\u0e49\u0e07\u0e19\u0e35\u0e49\u0e17\u0e33\u0e43\u0e2b\u0e49\u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e21\u0e35\u0e1b\u0e23\u0e30\u0e2a\u0e1a\u0e01\u0e32\u0e23\u0e13\u0e4c\u0e25\u0e07\u0e21\u0e37\u0e2d\u0e17\u0e33\u0e08\u0e23\u0e34\u0e07\u0e41\u0e25\u0e30\u0e44\u0e14\u0e49\u0e04\u0e49\u0e19\u0e04\u0e27\u0e49\u0e32\u0e2b\u0e32\u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49\u0e40\u0e1e\u0e34\u0e48\u0e21\u0e40\u0e15\u0e34\u0e21 \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e21\u0e32\u0e1e\u0e31\u0e12\u0e19\u0e32\u0e42\u0e21\u0e40\u0e14\u0e25 \u0e2d\u0e35\u0e01\u0e2b\u0e19\u0e36\u0e48\u0e07\u0e04\u0e27\u0e32\u0e21\u0e20\u0e32\u0e04\u0e20\u0e39\u0e21\u0e34\u0e43\u0e08 \u0e04\u0e37\u0e2d \u0e04\u0e27\u0e32\u0e21\u0e2a\u0e32\u0e21\u0e31\u0e04\u0e04\u0e35\u0e43\u0e19\u0e01\u0e32\u0e23\u0e41\u0e02\u0e48\u0e07\u0e02\u0e31\u0e19\u0e02\u0e2d\u0e07\u0e04\u0e19\u0e43\u0e19\u0e17\u0e35\u0e21\u0e17\u0e35\u0e48\u0e17\u0e33\u0e43\u0e2b\u0e49\u0e01\u0e32\u0e23\u0e41\u0e02\u0e48\u0e07\u0e02\u0e31\u0e19\u0e19\u0e35\u0e49\u0e40\u0e2a\u0e23\u0e47\u0e08\u0e25\u0e38\u0e25\u0e48\u0e27\u0e07 \u0e16\u0e36\u0e07\u0e41\u0e21\u0e49\u0e08\u0e30\u0e40\u0e08\u0e2d\u0e1b\u0e31\u0e0d\u0e2b\u0e32\u0e41\u0e25\u0e30\u0e2d\u0e38\u0e1b\u0e2a\u0e23\u0e23\u0e04\u0e15\u0e48\u0e32\u0e07 \u0e46 \u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32\u0e01\u0e47\u0e44\u0e21\u0e48\u0e22\u0e48\u0e2d\u0e17\u0e49\u0e2d\u0e15\u0e48\u0e2d\u0e1b\u0e31\u0e0d\u0e2b\u0e32\u0e17\u0e35\u0e48\u0e40\u0e01\u0e34\u0e14\u0e02\u0e36\u0e49\u0e19 \u0e43\u0e19\u0e23\u0e30\u0e22\u0e30\u0e40\u0e27\u0e25\u0e32 1 \u0e40\u0e14\u0e37\u0e2d\u0e19\u0e17\u0e35\u0e48\u0e1c\u0e48\u0e32\u0e19\u0e21\u0e32 \u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32\u0e2a\u0e39\u0e49\u0e08\u0e19\u0e16\u0e36\u0e07\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14 \u0e0a\u0e48\u0e27\u0e22\u0e01\u0e31\u0e19\u0e1f\u0e31\u0e19\u0e1d\u0e48\u0e32\u0e2d\u0e38\u0e1b\u0e2a\u0e23\u0e23\u0e04\u0e08\u0e19\u0e16\u0e36\u0e07\u0e40\u0e2a\u0e49\u0e19\u0e0a\u0e31\u0e22\u0e41\u0e2b\u0e48\u0e07\u0e04\u0e27\u0e32\u0e21\u0e1e\u0e22\u0e32\u0e22\u0e32\u0e21\nWhat we learned\n\u0e08\u0e32\u0e01\u0e01\u0e32\u0e23\u0e44\u0e14\u0e49\u0e25\u0e07\u0e41\u0e02\u0e48\u0e07\u0e43\u0e19\u0e04\u0e23\u0e31\u0e49\u0e07\u0e19\u0e35\u0e49 \u0e17\u0e32\u0e07\u0e01\u0e25\u0e38\u0e48\u0e21\u0e02\u0e2d\u0e07\u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49\u0e17\u0e32\u0e07\u0e40\u0e04\u0e21\u0e35\u0e40\u0e1e\u0e34\u0e48\u0e21\u0e21\u0e32\u0e01\u0e02\u0e36\u0e49\u0e19 \u0e42\u0e14\u0e22\u0e40\u0e09\u0e1e\u0e32\u0e30\u0e40\u0e23\u0e37\u0e48\u0e2d\u0e07 Metal-organic frameworks (MOFs) \u0e44\u0e14\u0e49\u0e23\u0e39\u0e49\u0e27\u0e34\u0e18\u0e35\u0e40\u0e02\u0e35\u0e22\u0e19\u0e42\u0e1b\u0e23\u0e41\u0e01\u0e23\u0e21\u0e41\u0e1a\u0e1a\u0e43\u0e2b\u0e21\u0e48\u0e46 library\u0e15\u0e48\u0e32\u0e07\u0e46 \u0e17\u0e31\u0e49\u0e07\u0e17\u0e32\u0e07 modeling \u0e41\u0e25\u0e30\u0e17\u0e32\u0e07\u0e40\u0e04\u0e21\u0e35 \u0e21\u0e35\u0e17\u0e31\u0e01\u0e29\u0e30\u0e43\u0e19\u0e01\u0e32\u0e23 Explore \u0e41\u0e25\u0e30 Clean \u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e17\u0e35\u0e48\u0e08\u0e30\u0e19\u0e33\u0e44\u0e1b\u0e43\u0e0a\u0e49\u0e43\u0e19\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25\u0e17\u0e33\u0e19\u0e32\u0e22\u0e1c\u0e25\u0e43\u0e2b\u0e49\u0e21\u0e35\u0e1b\u0e23\u0e30\u0e2a\u0e34\u0e17\u0e18\u0e34\u0e20\u0e32\u0e1e\u0e21\u0e32\u0e01\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14 \u0e44\u0e14\u0e49\u0e23\u0e39\u0e49\u0e08\u0e31\u0e01\u0e15\u0e31\u0e27\u0e41\u0e1a\u0e1a\u0e2b\u0e23\u0e37\u0e2d\u0e40\u0e17\u0e04\u0e19\u0e34\u0e04\u0e43\u0e19\u0e01\u0e32\u0e23\u0e17\u0e33\u0e43\u0e2b\u0e21\u0e48\u0e46 \u0e19\u0e2d\u0e01\u0e08\u0e32\u0e01\u0e19\u0e35\u0e49\u0e22\u0e31\u0e07\u0e44\u0e14\u0e49\u0e17\u0e31\u0e01\u0e29\u0e30\u0e43\u0e19\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e41\u0e25\u0e30\u0e1e\u0e31\u0e12\u0e19\u0e32\u0e15\u0e31\u0e27\u0e41\u0e1a\u0e1a\u0e40\u0e1e\u0e34\u0e48\u0e21\u0e02\u0e36\u0e49\u0e19\u0e2d\u0e35\u0e01\u0e14\u0e49\u0e27\u0e22\nWhat's next for thunder storm\n\u0e1e\u0e31\u0e12\u0e19\u0e32\u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49\u0e43\u0e19\u0e14\u0e49\u0e32\u0e19 machine learning \u0e15\u0e48\u0e2d\u0e41\u0e25\u0e30\u0e40\u0e02\u0e49\u0e32\u0e23\u0e48\u0e27\u0e21\u0e01\u0e32\u0e23\u0e41\u0e02\u0e48\u0e07\u0e02\u0e31\u0e19\u0e2d\u0e37\u0e48\u0e19\u0e46\u0e2d\u0e35\u0e01", "link": "https://devpost.com/software/thunder-storm", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\n\u0e17\u0e35\u0e21\u0e02\u0e2d\u0e07\u0e40\u0e23\u0e32\u0e2a\u0e19\u0e43\u0e08\u0e01\u0e32\u0e23\u0e41\u0e02\u0e48\u0e07\u0e02\u0e31\u0e19\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07 machine learning \u0e42\u0e14\u0e22\u0e44\u0e14\u0e49\u0e21\u0e32\u0e40\u0e08\u0e2d\u0e01\u0e32\u0e23\u0e41\u0e02\u0e48\u0e07\u0e02\u0e31\u0e19 tmlcc \u0e08\u0e36\u0e07\u0e2a\u0e19\u0e43\u0e08\u0e40\u0e1b\u0e47\u0e19\u0e2d\u0e22\u0e48\u0e32\u0e07\u0e21\u0e32\u0e01 \u0e0b\u0e36\u0e48\u0e07\u0e42\u0e08\u0e17\u0e22\u0e4c\u0e1b\u0e35\u0e19\u0e35\u0e49 \u0e04\u0e37\u0e2d \u0e2a\u0e23\u0e49\u0e32\u0e07\u0e15\u0e31\u0e27\u0e41\u0e1a\u0e1a\u0e43\u0e19\u0e01\u0e32\u0e23\u0e17\u0e33\u0e19\u0e32\u0e22\u0e1b\u0e23\u0e34\u0e21\u0e32\u0e13\u0e01\u0e4a\u0e32\u0e0b\u0e04\u0e32\u0e23\u0e4c\u0e1a\u0e2d\u0e19\u0e44\u0e14\u0e2d\u0e2d\u0e01\u0e44\u0e0b\u0e14\u0e4c (co2) \u0e17\u0e35\u0e48 mofs \u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e14\u0e39\u0e14\u0e0b\u0e31\u0e1a\u0e2b\u0e23\u0e37\u0e2d\u0e01\u0e31\u0e01\u0e40\u0e01\u0e47\u0e1a\u0e44\u0e27\u0e49\u0e44\u0e14\u0e49 \u0e0b\u0e36\u0e48\u0e07\u0e01\u0e4a\u0e32\u0e0b\u0e04\u0e32\u0e23\u0e4c\u0e1a\u0e2d\u0e19\u0e44\u0e14\u0e2d\u0e2d\u0e01\u0e44\u0e0b\u0e14\u0e4c (co2) \u0e40\u0e1b\u0e47\u0e19\u0e2a\u0e32\u0e40\u0e2b\u0e15\u0e38\u0e17\u0e35\u0e48\u0e17\u0e33\u0e43\u0e2b\u0e49\u0e40\u0e01\u0e34\u0e14\u0e20\u0e32\u0e27\u0e30\u0e01\u0e32\u0e23\u0e40\u0e1b\u0e25\u0e35\u0e48\u0e22\u0e19\u0e41\u0e1b\u0e25\u0e07\u0e20\u0e39\u0e21\u0e34\u0e2d\u0e32\u0e01\u0e32\u0e28\u0e02\u0e2d\u0e07\u0e42\u0e25\u0e01 \u0e42\u0e14\u0e22\u0e19\u0e33\u0e40\u0e17\u0e04\u0e19\u0e34\u0e04\u0e01\u0e32\u0e23\u0e40\u0e23\u0e35\u0e22\u0e19\u0e23\u0e39\u0e49\u0e02\u0e2d\u0e07\u0e40\u0e04\u0e23\u0e37\u0e48\u0e2d\u0e07 (machine learning)\nmodel explaination\n\u0e17\u0e33\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25 neural network \u0e17\u0e35\u0e48\u0e41\u0e15\u0e01\u0e15\u0e48\u0e32\u0e07\u0e01\u0e31\u0e19 3 \u0e42\u0e21\u0e40\u0e14\u0e25 \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e40\u0e2d\u0e32\u0e04\u0e48\u0e32\u0e1e\u0e22\u0e32\u0e01\u0e23\u0e13\u0e4c\u0e17\u0e35\u0e48\u0e17\u0e33\u0e19\u0e32\u0e22\u0e44\u0e14\u0e49\u0e41\u0e15\u0e48\u0e43\u0e19\u0e25\u0e30\u0e42\u0e21\u0e40\u0e14\u0e25\u0e21\u0e32 weight \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49\u0e19\u0e49\u0e33\u0e2b\u0e19\u0e31\u0e01\u0e40\u0e17\u0e48\u0e32\u0e01\u0e31\u0e1a 0.7 0.2 \u0e41\u0e25\u0e30 0.1\n\u0e17\u0e33\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25\u0e41\u0e23\u0e01\u0e04\u0e37\u0e2d ensemble \u0e08\u0e32\u0e01 3 \u0e42\u0e21\u0e40\u0e14\u0e25 \u0e44\u0e14\u0e49\u0e41\u0e01\u0e48 adaboost \u0e17\u0e35\u0e48\u0e21\u0e35 base estimator \u0e04\u0e37\u0e2d decision -----> tree !!!  \u0e08\u0e32\u0e01\u0e42\u0e21\u0e40\u0e14\u0e25 bagging \u0e17\u0e35\u0e48\u0e21\u0e35 base estimator \u0e04\u0e37\u0e2d decision -----> tree !!!  \u0e41\u0e25\u0e30\u0e08\u0e32\u0e01\u0e42\u0e21\u0e40\u0e14\u0e25 random forest \u0e2a\u0e48\u0e27\u0e19\u0e42\u0e21\u0e40\u0e14\u0e25\u0e17\u0e35\u0e48\u0e2a\u0e2d\u0e07 \u0e04\u0e37\u0e2d adaboost \u0e17\u0e35\u0e48\u0e21\u0e35 base estimator \u0e04\u0e37\u0e2d decision -----> tree !!!  \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e17\u0e33\u0e01\u0e32\u0e23\u0e17\u0e33\u0e19\u0e32\u0e22\u0e04\u0e48\u0e32\u0e08\u0e32\u0e01\u0e17\u0e31\u0e49\u0e07 2 \u0e42\u0e21\u0e40\u0e14\u0e25 \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e19\u0e33\u0e04\u0e48\u0e32\u0e1e\u0e22\u0e32\u0e01\u0e23\u0e13\u0e4c\u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e08\u0e32\u0e01\u0e17\u0e31\u0e49\u0e07 2 \u0e42\u0e21\u0e40\u0e14\u0e25 \u0e41\u0e25\u0e30\u0e04\u0e48\u0e32\u0e1e\u0e22\u0e32\u0e01\u0e23\u0e13\u0e4c\u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e08\u0e32\u0e01\u0e02\u0e49\u0e2d 1 \u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e01\u0e25\u0e48\u0e32\u0e27\u0e21\u0e32 \u0e19\u0e33\u0e21\u0e32 weight \u0e2b\u0e32\u0e04\u0e48\u0e32\u0e1e\u0e22\u0e32\u0e01\u0e23\u0e13\u0e4c \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49\u0e19\u0e49\u0e33\u0e2b\u0e19\u0e31\u0e01 \u0e04\u0e37\u0e2d 0.1 0.25 \u0e41\u0e25\u0e30 0.65 \u0e15\u0e32\u0e21\u0e25\u0e33\u0e14\u0e31\u0e1a\n\u0e17\u0e33\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25 adaboost \u0e17\u0e35\u0e48\u0e21\u0e35 base estimator \u0e04\u0e37\u0e2d decision -----> tree !!!  \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e17\u0e33\u0e01\u0e32\u0e23\u0e1e\u0e22\u0e32\u0e01\u0e23\u0e13\u0e4c\u0e04\u0e48\u0e32 \u0e19\u0e33\u0e04\u0e48\u0e32\u0e1e\u0e22\u0e32\u0e01\u0e23\u0e13\u0e4c\u0e17\u0e35\u0e48\u0e44\u0e14\u0e49 \u0e21\u0e32 weight \u0e01\u0e31\u0e1a\u0e04\u0e48\u0e32\u0e1e\u0e22\u0e32\u0e01\u0e23\u0e13\u0e4c\u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e08\u0e32\u0e01\u0e02\u0e49\u0e2d 1 \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49\u0e19\u0e49\u0e33\u0e2b\u0e19\u0e31\u0e01 \u0e04\u0e37\u0e2d 0.3 \u0e41\u0e25\u0e30 0.7 \u0e15\u0e32\u0e21\u0e25\u0e33\u0e14\u0e31\u0e1a\n\u0e17\u0e33\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25\u0e41\u0e23\u0e01\u0e04\u0e37\u0e2d ensemble \u0e08\u0e32\u0e01 3 \u0e42\u0e21\u0e40\u0e14\u0e25 \u0e44\u0e14\u0e49\u0e41\u0e01\u0e48 adaboost \u0e17\u0e35\u0e48\u0e21\u0e35 base estimator \u0e04\u0e37\u0e2d decision tree \u0e08\u0e32\u0e01\u0e42\u0e21\u0e40\u0e14\u0e25 bagging \u0e17\u0e35\u0e48\u0e21\u0e35 base estimator \u0e04\u0e37\u0e2d decision tree \u0e41\u0e25\u0e30\u0e08\u0e32\u0e01\u0e42\u0e21\u0e40\u0e14\u0e25 random forest \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e19\u0e33\u0e42\u0e21\u0e40\u0e14\u0e25 ensemble \u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e21\u0e32\u0e1e\u0e22\u0e32\u0e01\u0e23\u0e13\u0e4c\u0e04\u0e48\u0e32 \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e19\u0e33\u0e04\u0e48\u0e32\u0e1e\u0e22\u0e32\u0e01\u0e23\u0e13\u0e4c\u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e21\u0e32 weight \u0e01\u0e31\u0e1a\u0e04\u0e48\u0e32\u0e1e\u0e22\u0e32\u0e01\u0e23\u0e13\u0e4c\u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e08\u0e32\u0e01\u0e02\u0e49\u0e2d 1 \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49\u0e19\u0e49\u0e33\u0e2b\u0e19\u0e31\u0e01 \u0e04\u0e37\u0e2d 0.09 \u0e41\u0e25\u0e30 0.91 \u0e15\u0e32\u0e21\u0e25\u0e33\u0e14\u0e31\u0e1a\n\u0e17\u0e33\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25\u0e41\u0e23\u0e01\u0e04\u0e37\u0e2d \u0e01\u0e32\u0e23\u0e17\u0e33 ensemble 3 \u0e42\u0e21\u0e40\u0e14\u0e25 \u0e08\u0e32\u0e01 adaboost \u0e17\u0e35\u0e48\u0e21\u0e35 base estimator \u0e04\u0e37\u0e2d decision tree \u0e08\u0e32\u0e01\u0e42\u0e21\u0e40\u0e14\u0e25 bagging \u0e17\u0e35\u0e48\u0e21\u0e35 base estimator \u0e04\u0e37\u0e2d decision tree \u0e41\u0e25\u0e30\u0e08\u0e32\u0e01\u0e42\u0e21\u0e40\u0e14\u0e25 random forest \u0e2a\u0e48\u0e27\u0e19\u0e42\u0e21\u0e40\u0e14\u0e25\u0e17\u0e35\u0e48\u0e2a\u0e2d\u0e07 \u0e04\u0e37\u0e2d adaboost \u0e17\u0e35\u0e48\u0e21\u0e35 base estimator \u0e04\u0e37\u0e2d decision tree \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e17\u0e33\u0e01\u0e32\u0e23\u0e17\u0e33\u0e19\u0e32\u0e22\u0e04\u0e48\u0e32\u0e08\u0e32\u0e01\u0e17\u0e31\u0e49\u0e07 2 \u0e42\u0e21\u0e40\u0e14\u0e25 \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e19\u0e33\u0e04\u0e48\u0e32\u0e1e\u0e22\u0e32\u0e01\u0e23\u0e13\u0e4c\u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e08\u0e32\u0e01\u0e17\u0e31\u0e49\u0e07 2 \u0e42\u0e21\u0e40\u0e14\u0e25 \u0e41\u0e25\u0e30\u0e04\u0e48\u0e32\u0e1e\u0e22\u0e32\u0e01\u0e23\u0e13\u0e4c\u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e08\u0e32\u0e01\u0e02\u0e49\u0e2d 1 \u0e19\u0e33\u0e21\u0e32 weight \u0e2b\u0e32\u0e04\u0e48\u0e32\u0e1e\u0e22\u0e32\u0e01\u0e23\u0e13\u0e4c \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49\u0e19\u0e49\u0e33\u0e2b\u0e19\u0e31\u0e01 \u0e04\u0e37\u0e2d 0.1 0.2 \u0e41\u0e25\u0e30 0.7 \u0e15\u0e32\u0e21\u0e25\u0e33\u0e14\u0e31\u0e1a\nhow we built it\n\u0e43\u0e19\u0e02\u0e31\u0e49\u0e19\u0e15\u0e2d\u0e19\u0e41\u0e23\u0e01 \u0e08\u0e30\u0e17\u0e33\u0e01\u0e32\u0e23\u0e2a\u0e33\u0e23\u0e27\u0e08\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e23\u0e31\u0e1a\u0e21\u0e32 \u0e40\u0e23\u0e32\u0e1e\u0e1a\u0e27\u0e48\u0e32\u0e21\u0e35\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e17\u0e35\u0e48\u0e1c\u0e34\u0e14\u0e1b\u0e01\u0e15\u0e34\u0e04\u0e37\u0e2d \u0e04\u0e2d\u0e25\u0e31\u0e21\u0e19\u0e4c surface area, volume \u0e41\u0e25\u0e30 void fraction \u0e17\u0e35\u0e48\u0e08\u0e30\u0e21\u0e35\u0e04\u0e48\u0e32 0,-1 \u0e41\u0e25\u0e30 \u0e04\u0e2d\u0e25\u0e31\u0e21\u0e19\u0e4c heat adsorption \u0e21\u0e35\u0e04\u0e48\u0e32\u0e40\u0e1b\u0e47\u0e19 infinity \u0e41\u0e15\u0e48\u0e17\u0e31\u0e49\u0e07\u0e04\u0e2d\u0e25\u0e31\u0e21\u0e19\u0e4c volume void fraction \u0e41\u0e25\u0e30 heat absorption\u0e21\u0e35\u0e08\u0e33\u0e19\u0e27\u0e19\u0e17\u0e35\u0e48\u0e1c\u0e34\u0e14\u0e1b\u0e01\u0e15\u0e34\u0e44\u0e21\u0e48\u0e21\u0e32\u0e01 \u0e40\u0e23\u0e32\u0e40\u0e25\u0e22\u0e43\u0e0a\u0e49\u0e27\u0e34\u0e18\u0e35\u0e15\u0e31\u0e14\u0e2d\u0e2d\u0e01 \u0e41\u0e15\u0e48 surface area \u0e21\u0e35\u0e08\u0e33\u0e19\u0e27\u0e19\u0e1c\u0e34\u0e14\u0e1b\u0e01\u0e15\u0e34\u0e21\u0e32\u0e01\u0e16\u0e36\u0e07 10000\u0e01\u0e27\u0e48\u0e32 record \u0e17\u0e33\u0e43\u0e2b\u0e49\u0e16\u0e49\u0e32\u0e15\u0e31\u0e14\u0e2d\u0e2d\u0e01 \u0e08\u0e30\u0e40\u0e2b\u0e25\u0e37\u0e2d\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e19\u0e49\u0e2d\u0e22\u0e25\u0e07\u0e44\u0e1b\u0e40\u0e22\u0e2d\u0e30 \u0e40\u0e23\u0e32\u0e40\u0e25\u0e22\u0e04\u0e34\u0e14\u0e27\u0e34\u0e18\u0e35\u0e41\u0e01\u0e49\u0e44\u0e02\u0e14\u0e31\u0e07\u0e19\u0e35\u0e49\n\u0e15\u0e31\u0e14\u0e04\u0e2d\u0e25\u0e31\u0e21\u0e19\u0e4c surface area \u0e2d\u0e2d\u0e01\nimpute surface area \u0e14\u0e49\u0e27\u0e22\u0e04\u0e48\u0e32\u0e15\u0e48\u0e32\u0e07\u0e46\n\u0e19\u0e33\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e02\u0e2d\u0e07 mofs \u0e21\u0e32\u0e04\u0e33\u0e19\u0e27\u0e13\u0e40\u0e1b\u0e47\u0e19 surface area\n\u0e15\u0e31\u0e14 surface area record \u0e17\u0e35\u0e48\u0e1c\u0e34\u0e14\u0e1b\u0e01\u0e15\u0e34\u0e2d\u0e2d\u0e01\n\u0e0b\u0e36\u0e48\u0e07\u0e27\u0e34\u0e18\u0e35\u0e17\u0e35\u0e48 1 \u0e01\u0e31\u0e1a 4 \u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e25\u0e2d\u0e07\u0e17\u0e33\u0e41\u0e25\u0e49\u0e27 \u0e1e\u0e1a\u0e27\u0e48\u0e32\u0e1c\u0e25\u0e25\u0e31\u0e1e\u0e18\u0e4c\u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e08\u0e32\u0e01\u0e42\u0e21\u0e40\u0e14\u0e25\u0e41\u0e22\u0e48\u0e25\u0e07\u0e21\u0e32\u0e01 \u0e40\u0e23\u0e32\u0e08\u0e36\u0e07\u0e04\u0e34\u0e14\u0e27\u0e48\u0e32 \u0e04\u0e27\u0e23\u0e19\u0e33\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e02\u0e2d\u0e07 mofs\u0e41\u0e15\u0e48\u0e25\u0e30\u0e15\u0e31\u0e27\u0e21\u0e32\u0e04\u0e33\u0e19\u0e27\u0e13\u0e40\u0e1b\u0e47\u0e19 surface area \u0e41\u0e15\u0e48\u0e40\u0e19\u0e37\u0e48\u0e2d\u0e07\u0e08\u0e32\u0e01\u0e01\u0e32\u0e23\u0e14\u0e36\u0e07\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e02\u0e2d\u0e07mofs \u0e21\u0e32\u0e04\u0e33\u0e19\u0e27\u0e13\u0e43\u0e0a\u0e49\u0e40\u0e27\u0e25\u0e32\u0e19\u0e32\u0e19\u0e21\u0e32\u0e01 \u0e23\u0e30\u0e2b\u0e27\u0e48\u0e32\u0e07\u0e19\u0e31\u0e49\u0e19\u0e40\u0e23\u0e32\u0e08\u0e36\u0e07\u0e17\u0e14\u0e25\u0e2d\u0e07\u0e43\u0e0a\u0e49\u0e27\u0e34\u0e18\u0e35 impute surface area \u0e14\u0e49\u0e27\u0e22 linear regression \u0e41\u0e25\u0e30 stochastic \u0e0b\u0e36\u0e48\u0e07\u0e1c\u0e25\u0e2d\u0e2d\u0e01\u0e21\u0e32\u0e27\u0e48\u0e32 linear \u0e44\u0e14\u0e49\u0e1c\u0e25\u0e17\u0e35\u0e48\u0e14\u0e35\u0e01\u0e27\u0e48\u0e32\u0e40\u0e23\u0e32\u0e08\u0e36\u0e07\u0e43\u0e0a\u0e49\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e40\u0e0b\u0e47\u0e17\u0e19\u0e35\u0e49\u0e43\u0e19\u0e01\u0e32\u0e23\u0e17\u0e33\u0e42\u0e21\u0e40\u0e14\u0e25\u0e43\u0e19\u0e23\u0e30\u0e2b\u0e27\u0e48\u0e32\u0e07\u0e17\u0e35\u0e48\u0e23\u0e2d\u0e01\u0e32\u0e23\u0e14\u0e36\u0e07\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e08\u0e23\u0e34\u0e07\u0e46\u0e02\u0e2d\u0e07 mofs\n\u0e2b\u0e25\u0e31\u0e07\u0e08\u0e32\u0e01\u0e17\u0e35\u0e48\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e14\u0e36\u0e07\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e21\u0e32\u0e44\u0e14\u0e49\u0e08\u0e19\u0e04\u0e23\u0e1a\u0e41\u0e25\u0e49\u0e27\u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e17\u0e33\u0e01\u0e32\u0e23\u0e40\u0e17\u0e35\u0e22\u0e1a\u0e27\u0e48\u0e32 gsa \u0e2b\u0e23\u0e37\u0e2d vsa \u0e17\u0e35\u0e48\u0e21\u0e35\u0e04\u0e27\u0e32\u0e21\u0e43\u0e01\u0e25\u0e49\u0e40\u0e04\u0e35\u0e22\u0e07\u0e01\u0e31\u0e1a surface area \u0e17\u0e35\u0e48\u0e21\u0e35\u0e2d\u0e22\u0e39\u0e48\u0e43\u0e19data set\u0e21\u0e32\u0e01\u0e01\u0e27\u0e48\u0e32\u0e01\u0e31\u0e19 \u0e1e\u0e1a\u0e27\u0e48\u0e32 vsa \u0e21\u0e35\u0e04\u0e27\u0e32\u0e21\u0e43\u0e01\u0e25\u0e49\u0e40\u0e04\u0e35\u0e22\u0e07\u0e01\u0e27\u0e48\u0e32\u0e21\u0e32\u0e01 \u0e40\u0e23\u0e32\u0e08\u0e36\u0e07\u0e15\u0e31\u0e14\u0e2a\u0e34\u0e19\u0e43\u0e08\u0e40\u0e25\u0e37\u0e2d\u0e01 vsa \u0e43\u0e19\u0e01\u0e32\u0e23 impute \u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25 surface area\u0e17\u0e35\u0e48\u0e1c\u0e34\u0e14\u0e1b\u0e01\u0e15\u0e34\n\u0e43\u0e19\u0e15\u0e2d\u0e19\u0e41\u0e23\u0e01 \u0e40\u0e23\u0e32\u0e43\u0e0a\u0e49\u0e41\u0e04\u0e48 feature \u0e17\u0e35\u0e48\u0e21\u0e35\u0e2d\u0e22\u0e39\u0e48\u0e43\u0e19 data set \u0e44\u0e21\u0e48\u0e44\u0e14\u0e49\u0e21\u0e35\u0e01\u0e32\u0e23\u0e2b\u0e32 feature \u0e40\u0e1e\u0e34\u0e48\u0e21 \u0e1e\u0e1a\u0e27\u0e48\u0e32 feature\u0e17\u0e35\u0e48\u0e21\u0e35\u0e2d\u0e22\u0e39\u0e48\u0e19\u0e49\u0e2d\u0e22\u0e40\u0e01\u0e34\u0e19\u0e44\u0e1b \u0e17\u0e33\u0e43\u0e2b\u0e49\u0e1c\u0e25\u0e25\u0e31\u0e1e\u0e18\u0e4c\u0e08\u0e32\u0e01\u0e01\u0e32\u0e23\u0e17\u0e33\u0e42\u0e21\u0e40\u0e14\u0e25\u0e44\u0e21\u0e48\u0e14\u0e35\u0e40\u0e17\u0e48\u0e32\u0e17\u0e35\u0e48\u0e04\u0e27\u0e23 \u0e40\u0e23\u0e32\u0e08\u0e36\u0e07\u0e2b\u0e32 feature \u0e40\u0e1e\u0e34\u0e48\u0e21\u0e41\u0e15\u0e48\u0e01\u0e32\u0e23\u0e08\u0e30\u0e2b\u0e32 feature \u0e40\u0e1e\u0e34\u0e48\u0e21\u0e44\u0e14\u0e49 \u0e15\u0e49\u0e2d\u0e07\u0e43\u0e0a\u0e49 smiles \u0e41\u0e15\u0e48\u0e40\u0e19\u0e37\u0e48\u0e2d\u0e07\u0e08\u0e32\u0e01\u0e40\u0e23\u0e32\u0e44\u0e21\u0e48\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e2b\u0e32 library\u0e17\u0e35\u0e48\u0e08\u0e30\u0e41\u0e1b\u0e25\u0e07 functional group \u0e40\u0e1b\u0e47\u0e19 smiles \u0e44\u0e14\u0e49 \u0e40\u0e23\u0e32\u0e08\u0e36\u0e07\u0e40\u0e02\u0e35\u0e22\u0e19\u0e1f\u0e31\u0e07\u0e01\u0e4c\u0e0a\u0e31\u0e48\u0e19\u0e43\u0e19 python \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e41\u0e1b\u0e25\u0e07 functional group \u0e44\u0e1b\u0e40\u0e1b\u0e47\u0e19 smiles \u0e40\u0e2d\u0e07 \u0e42\u0e14\u0e22\u0e2d\u0e34\u0e07\u0e2b\u0e25\u0e31\u0e01\u0e01\u0e32\u0e23\u0e08\u0e32\u0e01\u0e27\u0e34\u0e18\u0e35\u0e41\u0e1b\u0e25\u0e07 functional group \u0e40\u0e1b\u0e47\u0e19 smiles \u0e40\u0e0a\u0e48\u0e19 ome \u0e21\u0e35 smile \u0e04\u0e37\u0e2d co- \u0e41\u0e25\u0e30 cooh\u0e21\u0e35 smile \u0e04\u0e37\u0e2d -c(=o)(o)\u0e40\u0e21\u0e37\u0e48\u0e2d\u0e19\u0e33\u0e21\u0e32\u0e23\u0e27\u0e21\u0e01\u0e31\u0e19 ome-cooh \u0e08\u0e30\u0e21\u0e35 smile \u0e04\u0e37\u0e2d coc(=o)(o) \u0e40\u0e1b\u0e47\u0e19\u0e15\u0e49\u0e19 \u0e40\u0e21\u0e37\u0e48\u0e2d\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e2b\u0e32 smiles \u0e02\u0e2d\u0e07mofs \u0e17\u0e38\u0e01\u0e15\u0e31\u0e27\u0e44\u0e14\u0e49\u0e41\u0e25\u0e49\u0e27 \u0e40\u0e23\u0e32\u0e08\u0e36\u0e07\u0e2b\u0e32feature\u0e40\u0e1e\u0e34\u0e48\u0e21 \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49library \u0e17\u0e32\u0e07\u0e40\u0e04\u0e21\u0e35\u0e15\u0e48\u0e32\u0e07\u0e46 \u0e14\u0e31\u0e07\u0e19\u0e35\u0e49 library rdkit \u0e44\u0e14\u0e49\u0e1f\u0e35\u0e40\u0e08\u0e2d\u0e23\u0e4c molwt, mollogp, numvalenceelectrons, heavyatomcount, nocount, numaliphaticrings, numaromaticrings ,numhacceptors, numhdonors, numheteroatoms, numrotatablebonds\nlibrary pubchempy \u0e44\u0e14\u0e49\u0e1f\u0e35\u0e40\u0e08\u0e2d\u0e23\u0e4c \u0e14\u0e31\u0e07\u0e19\u0e35\u0e49 molecularformula, molecularweight, canonicalsmiles, isomericsmiles, inchi, inchikey, iupacname, xlogp, exactmass, monoisotopicmass, tpsa, complexity, charge, hbonddonorcount, hbondacceptorcount, rotatablebondcount, isotopeatomcount, atomstereocount, definedatomstereocount, undefinedatomstereocount, bondstereocount, definedbondstereocount, undefinedbondstereocount, covalentunitcount\n\u0e40\u0e21\u0e37\u0e48\u0e2d\u0e25\u0e2d\u0e07\u0e19\u0e33\u0e21\u0e32\u0e17\u0e33\u0e42\u0e21\u0e40\u0e14\u0e25 \u0e42\u0e14\u0e22\u0e17\u0e35\u0e48 drop \u0e15\u0e31\u0e27\u0e41\u0e1b\u0e23\u0e17\u0e35\u0e48\u0e40\u0e1b\u0e47\u0e19\u0e0a\u0e37\u0e48\u0e2d\u0e15\u0e48\u0e32\u0e07\u0e46\u0e2d\u0e2d\u0e01 \u0e17\u0e33\u0e43\u0e2b\u0e49\u0e1c\u0e25\u0e25\u0e31\u0e1e\u0e18\u0e4c\u0e02\u0e2d\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25\u0e14\u0e35\u0e02\u0e36\u0e49\u0e19\u0e21\u0e32\u0e01\n\u0e43\u0e19\u0e2a\u0e48\u0e27\u0e19\u0e02\u0e2d\u0e07\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25 \u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e41\u0e1a\u0e48\u0e07\u0e01\u0e31\u0e19\u0e44\u0e1b\u0e17\u0e33\u0e42\u0e21\u0e40\u0e14\u0e25\u0e15\u0e48\u0e32\u0e07\u0e46\u0e14\u0e31\u0e07\u0e19\u0e35\u0e49\n\u0e17\u0e33\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25 deep neural network \u0e43\u0e0a\u0e49 hyperband, random search \u0e43\u0e19\u0e01\u0e32\u0e23\u0e17\u0e33 hyperparameter tuning \u0e40\u0e21\u0e37\u0e48\u0e2d\u0e44\u0e14\u0e49 hyperparameter \u0e17\u0e35\u0e48\u0e14\u0e35\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14\u0e08\u0e32\u0e01 search \u0e21\u0e32\u0e41\u0e25\u0e49\u0e27 \u0e40\u0e23\u0e32\u0e01\u0e47\u0e44\u0e14\u0e49\u0e21\u0e32\u0e25\u0e2d\u0e07\u0e40\u0e1e\u0e34\u0e48\u0e21 learning rate schedule, drop out , regularization \u0e41\u0e25\u0e30\u0e25\u0e2d\u0e07\u0e1b\u0e23\u0e31\u0e1a node, layer \u0e15\u0e48\u0e32\u0e07\u0e46 \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e41\u0e01\u0e49 overfitting \u0e41\u0e25\u0e30\u0e17\u0e33\u0e43\u0e2b\u0e49\u0e42\u0e21\u0e40\u0e14\u0e25\u0e14\u0e35\u0e02\u0e36\u0e49\u0e19 \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e25\u0e2d\u0e07\u0e17\u0e33 functional api \u0e43\u0e19\u0e01\u0e32\u0e23\u0e23\u0e27\u0e21 2 \u0e42\u0e21\u0e40\u0e14\u0e25\u0e40\u0e02\u0e49\u0e32\u0e14\u0e49\u0e27\u0e22\u0e01\u0e31\u0e19 \u0e0b\u0e36\u0e48\u0e07\u0e1c\u0e25\u0e2d\u0e2d\u0e01\u0e21\u0e32\u0e22\u0e31\u0e07\u0e44\u0e21\u0e48\u0e14\u0e35\u0e40\u0e17\u0e48\u0e32\u0e01\u0e31\u0e1a\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49 weight \u0e23\u0e30\u0e2b\u0e27\u0e48\u0e32\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25\u0e42\u0e14\u0e22\u0e15\u0e23\u0e07\n\u0e17\u0e33\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25 tree-based \u0e42\u0e14\u0e22\u0e42\u0e21\u0e40\u0e14\u0e25\u0e17\u0e35\u0e48\u0e2a\u0e23\u0e49\u0e32\u0e07 \u0e44\u0e14\u0e49\u0e41\u0e01\u0e48 decision tree, random forest, xgboost, gradient boosting, bagging \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49 base estimator \u0e17\u0e35\u0e48\u0e2b\u0e25\u0e32\u0e01\u0e2b\u0e25\u0e32\u0e22, adaboost \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49 base estimator \u0e17\u0e35\u0e48\u0e2b\u0e25\u0e32\u0e01\u0e2b\u0e25\u0e32\u0e22, ensemble \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49\u0e08\u0e32\u0e01 adaboost \u0e17\u0e35\u0e48\u0e43\u0e0a\u0e49 base estimator \u0e04\u0e37\u0e2d decision tree \u0e08\u0e32\u0e01 bagging \u0e17\u0e35\u0e48\u0e43\u0e0a\u0e49 base estimator \u0e04\u0e37\u0e2d decision tree \u0e41\u0e25\u0e30\u0e08\u0e32\u0e01 random forest \u0e41\u0e25\u0e30\u0e44\u0e14\u0e49\u0e21\u0e35\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49 grid search \u0e41\u0e25\u0e30 randomized search \u0e43\u0e19\u0e43\u0e19\u0e01\u0e32\u0e23\u0e2b\u0e32\u0e1e\u0e32\u0e23\u0e32\u0e21\u0e34\u0e40\u0e15\u0e2d\u0e23\u0e4c\u0e17\u0e35\u0e48\u0e40\u0e2b\u0e21\u0e32\u0e30\u0e2a\u0e21\n\u0e17\u0e33\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25 knn-regressor \u0e42\u0e14\u0e22\u0e17\u0e33\u0e01\u0e32\u0e23\u0e2a\u0e40\u0e01\u0e25\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e43\u0e2b\u0e49\u0e2d\u0e22\u0e39\u0e48\u0e43\u0e19\u0e0a\u0e48\u0e27\u0e07 0-1 \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e2b\u0e32\u0e04\u0e48\u0e32 k \u0e17\u0e35\u0e48\u0e17\u0e33\u0e43\u0e2b\u0e49 root-squared mean error (rsme) \u0e21\u0e35\u0e04\u0e48\u0e32\u0e19\u0e49\u0e2d\u0e22\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14 \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19 \u0e19\u0e33\u0e04\u0e48\u0e32 k \u0e21\u0e32\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e15\u0e31\u0e27\u0e41\u0e1a\u0e1a \u0e41\u0e25\u0e30\u0e1b\u0e23\u0e31\u0e1a\u0e1e\u0e32\u0e23\u0e32\u0e21\u0e34\u0e40\u0e15\u0e2d\u0e23\u0e4c \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e43\u0e2b\u0e49\u0e04\u0e48\u0e32 mean absolute error (mae) \u0e21\u0e35\u0e04\u0e48\u0e32\u0e15\u0e48\u0e33\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14 \u0e41\u0e25\u0e30\u0e17\u0e33\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25 ensemble \u0e42\u0e14\u0e22\u0e40\u0e25\u0e37\u0e2d\u0e01\u0e08\u0e32\u0e01 linear regression, adaboostregressor, xgboost, randomforest, k-neighbors regression \u0e21\u0e32 3 \u0e42\u0e21\u0e40\u0e14\u0e25 \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e19\u0e33\u0e21\u0e32\u0e1b\u0e23\u0e30\u0e01\u0e2d\u0e1a\u0e01\u0e31\u0e19 \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e17\u0e33\u0e01\u0e32\u0e23\u0e40\u0e09\u0e25\u0e35\u0e48\u0e22\u0e04\u0e48\u0e32\u0e17\u0e35\u0e48 predict \u0e44\u0e14\u0e49\n\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25 light gbm regressor \u0e42\u0e14\u0e22\u0e17\u0e33\u0e01\u0e32\u0e23\u0e2a\u0e40\u0e01\u0e25\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e43\u0e2b\u0e49\u0e2d\u0e22\u0e39\u0e48\u0e43\u0e19\u0e0a\u0e48\u0e27\u0e07 0-1 \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e2b\u0e32 \u0e08\u0e39\u0e19\u0e1e\u0e32\u0e23\u0e32\u0e21\u0e34\u0e40\u0e15\u0e2d\u0e23\u0e4c\u0e17\u0e35\u0e48\u0e08\u0e30\u0e17\u0e33\u0e43\u0e2b\u0e49 root-squared mean error (rsme) \u0e21\u0e35\u0e04\u0e48\u0e32\u0e19\u0e49\u0e2d\u0e22\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14\n\u0e28\u0e36\u0e01\u0e29\u0e32\u0e43\u0e19\u0e2a\u0e48\u0e27\u0e19\u0e02\u0e2d\u0e07 library ensemble \u0e08\u0e32\u0e01 sklearn \u0e41\u0e25\u0e30\u0e17\u0e14\u0e25\u0e2d\u0e07\u0e43\u0e0a\u0e49\u0e01\u0e31\u0e1a\u0e42\u0e21\u0e40\u0e14\u0e25\u0e17\u0e35\u0e48\u0e17\u0e35\u0e21\u0e44\u0e14\u0e49\u0e17\u0e33\u0e21\u0e32\u0e1b\u0e23\u0e30\u0e01\u0e2d\u0e1a\u0e01\u0e31\u0e19\u0e43\u0e19\u0e2b\u0e25\u0e32\u0e22\u0e46\u0e23\u0e39\u0e1b\u0e41\u0e1a\u0e1a\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e2b\u0e32\u0e42\u0e21\u0e40\u0e14\u0e25\u0e17\u0e35\u0e48\u0e14\u0e35\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14\u0e08\u0e32\u0e01\u0e01\u0e32\u0e23 ensemble model \u0e41\u0e25\u0e30\u0e44\u0e14\u0e49\u0e44\u0e1b\u0e28\u0e36\u0e01\u0e29\u0e32\u0e40\u0e01\u0e35\u0e48\u0e22\u0e27\u0e01\u0e31\u0e1a library \u0e17\u0e35\u0e48\u0e40\u0e01\u0e35\u0e48\u0e22\u0e27\u0e01\u0e31\u0e1a automl \u0e42\u0e14\u0e22\u0e44\u0e1b\u0e28\u0e36\u0e01\u0e29\u0e32\u0e14\u0e39 benchmark \u0e43\u0e19 library \u0e15\u0e48\u0e32\u0e07\u0e46\u0e41\u0e25\u0e30\u0e44\u0e14\u0e49\u0e17\u0e14\u0e25\u0e2d\u0e07\u0e19\u0e33\u0e21\u0e32\u0e43\u0e0a\u0e49\u0e01\u0e31\u0e1a\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e40\u0e0a\u0e48\u0e19 library autokeras, autosklearn, flaml, h2o\n\u0e08\u0e32\u0e01\u0e17\u0e35\u0e48\u0e01\u0e25\u0e48\u0e32\u0e27\u0e21\u0e32\u0e02\u0e49\u0e32\u0e07\u0e15\u0e49\u0e19 \u0e42\u0e21\u0e40\u0e14\u0e25\u0e17\u0e35\u0e48\u0e41\u0e15\u0e48\u0e25\u0e30\u0e04\u0e19\u0e44\u0e14\u0e49\u0e17\u0e33\u0e01\u0e32\u0e23\u0e1e\u0e22\u0e32\u0e01\u0e23\u0e13\u0e4c\u0e04\u0e48\u0e32 co2 \u0e21\u0e32 \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e40\u0e23\u0e32\u0e19\u0e33\u0e42\u0e21\u0e40\u0e14\u0e25\u0e17\u0e35\u0e48\u0e14\u0e35\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14\u0e02\u0e2d\u0e07\u0e41\u0e15\u0e48\u0e25\u0e30\u0e04\u0e19\u0e21\u0e32\u0e17\u0e33\u0e01\u0e32\u0e23 weight \u0e19\u0e49\u0e33\u0e2b\u0e19\u0e31\u0e01\u0e02\u0e2d\u0e07\u0e04\u0e48\u0e32\u0e1e\u0e22\u0e32\u0e01\u0e23\u0e13\u0e4c \u0e40\u0e0a\u0e48\u0e19 \u0e2a\u0e21\u0e32\u0e0a\u0e34\u0e01\u0e04\u0e19\u0e41\u0e23\u0e01\u0e44\u0e14\u0e49\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25\u0e17\u0e33\u0e19\u0e32\u0e22\u0e04\u0e48\u0e32 \u0e44\u0e14\u0e49\u0e04\u0e48\u0e32 log(mae) = 1.23 \u0e2a\u0e21\u0e32\u0e0a\u0e34\u0e01\u0e04\u0e19\u0e17\u0e35\u0e48 2 \u0e2a\u0e23\u0e49\u0e32\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25\u0e17\u0e33\u0e19\u0e32\u0e22\u0e04\u0e48\u0e32 \u0e44\u0e14\u0e49\u0e04\u0e48\u0e32 log(mae) = 1.24 \u0e41\u0e25\u0e30\u0e2a\u0e21\u0e32\u0e0a\u0e34\u0e01\u0e04\u0e19\u0e17\u0e35\u0e48 3 \u0e2a\u0e23\u0e49\u0e32\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25\u0e17\u0e33\u0e19\u0e32\u0e22\u0e04\u0e48\u0e32 \u0e44\u0e14\u0e49\u0e04\u0e48\u0e32 log(mae) = 1.25 \u0e08\u0e32\u0e01\u0e19\u0e31\u0e49\u0e19\u0e40\u0e23\u0e32\u0e17\u0e33\u0e01\u0e32\u0e23 weight \u0e04\u0e48\u0e32\u0e1e\u0e22\u0e32\u0e01\u0e23\u0e13\u0e4c\u0e02\u0e2d\u0e07\u0e41\u0e15\u0e48\u0e42\u0e21\u0e40\u0e14\u0e25 \u0e2d\u0e32\u0e17\u0e34 0.65 0.25 0.1 \u0e42\u0e14\u0e22\u0e01\u0e32\u0e23\u0e17\u0e35\u0e48\u0e40\u0e23\u0e32\u0e17\u0e33 weight \u0e04\u0e48\u0e32\u0e1e\u0e22\u0e32\u0e01\u0e23\u0e13\u0e4c\u0e41\u0e15\u0e48\u0e25\u0e30\u0e42\u0e21\u0e40\u0e14\u0e25\u0e19\u0e35\u0e49 \u0e17\u0e33\u0e43\u0e2b\u0e49\u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e04\u0e48\u0e32 log(mae) \u0e17\u0e35\u0e48\u0e19\u0e49\u0e2d\u0e22\u0e25\u0e07 \u0e40\u0e23\u0e32\u0e08\u0e36\u0e07\u0e19\u0e33\u0e27\u0e34\u0e18\u0e35\u0e01\u0e32\u0e23\u0e19\u0e35\u0e49\u0e21\u0e32\u0e2b\u0e32\u0e04\u0e48\u0e32\u0e17\u0e33\u0e19\u0e32\u0e22 co2\nchallenges we ran into\n\u0e40\u0e19\u0e37\u0e48\u0e2d\u0e07\u0e08\u0e32\u0e01\u0e40\u0e1b\u0e47\u0e19\u0e01\u0e32\u0e23\u0e41\u0e02\u0e48\u0e07\u0e02\u0e31\u0e19\u0e14\u0e49\u0e32\u0e19 ml \u0e17\u0e35\u0e48\u0e01\u0e25\u0e38\u0e48\u0e21\u0e40\u0e23\u0e32\u0e23\u0e48\u0e27\u0e21\u0e41\u0e02\u0e48\u0e07\u0e01\u0e31\u0e19\u0e01\u0e31\u0e19\u0e40\u0e1b\u0e47\u0e19\u0e04\u0e23\u0e31\u0e49\u0e07\u0e41\u0e23\u0e01 \u0e2d\u0e35\u0e01\u0e17\u0e31\u0e49\u0e07\u0e40\u0e1b\u0e47\u0e19\u0e40\u0e23\u0e37\u0e48\u0e2d\u0e07\u0e40\u0e01\u0e35\u0e48\u0e22\u0e27\u0e01\u0e31\u0e1a\u0e17\u0e32\u0e07\u0e14\u0e49\u0e32\u0e19\u0e40\u0e04\u0e21\u0e35\u0e17\u0e35\u0e48\u0e01\u0e25\u0e38\u0e48\u0e21\u0e40\u0e23\u0e32\u0e44\u0e21\u0e48\u0e04\u0e48\u0e2d\u0e22\u0e21\u0e35\u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49\u0e21\u0e32\u0e01\u0e19\u0e31\u0e01 \u0e40\u0e19\u0e37\u0e48\u0e2d\u0e07\u0e08\u0e32\u0e01\u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32\u0e40\u0e23\u0e35\u0e22\u0e19\u0e21\u0e32\u0e17\u0e32\u0e07\u0e14\u0e49\u0e32\u0e19\u0e2a\u0e16\u0e34\u0e15\u0e34 \u0e08\u0e36\u0e07\u0e15\u0e49\u0e2d\u0e07\u0e04\u0e49\u0e19\u0e2b\u0e32\u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49\u0e17\u0e32\u0e07\u0e14\u0e49\u0e32\u0e19\u0e40\u0e04\u0e21\u0e35 \u0e40\u0e0a\u0e48\u0e19 \u0e01\u0e32\u0e23\u0e41\u0e1b\u0e25\u0e07 functional group \u0e40\u0e1b\u0e47\u0e19 smiles \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e2b\u0e32 feature \u0e40\u0e1e\u0e34\u0e48\u0e21 \u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e44\u0e1b\u0e04\u0e49\u0e19\u0e2b\u0e32\u0e14\u0e39\u0e27\u0e48\u0e32\u0e21\u0e35\u0e1f\u0e31\u0e07\u0e01\u0e4c\u0e0a\u0e31\u0e19\u0e43\u0e19 python \u0e2d\u0e30\u0e44\u0e23\u0e1a\u0e49\u0e32\u0e07\u0e17\u0e35\u0e48\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e41\u0e1b\u0e25\u0e07 functional group \u0e40\u0e1b\u0e47\u0e19 smile \u0e44\u0e14\u0e49 \u0e0b\u0e36\u0e48\u0e07\u0e40\u0e23\u0e32\u0e44\u0e21\u0e48\u0e1e\u0e1a \u0e40\u0e23\u0e32\u0e08\u0e36\u0e07\u0e41\u0e01\u0e49\u0e1b\u0e31\u0e0d\u0e2b\u0e32\u0e14\u0e49\u0e27\u0e22\u0e01\u0e32\u0e23\u0e2b\u0e32\u0e2b\u0e25\u0e31\u0e01\u0e01\u0e32\u0e23\u0e41\u0e1b\u0e25\u0e07 functional group \u0e40\u0e1b\u0e47\u0e19 smiles \u0e41\u0e25\u0e30\u0e40\u0e02\u0e35\u0e22\u0e19\u0e1f\u0e31\u0e07\u0e01\u0e4c\u0e0a\u0e31\u0e48\u0e19\u0e43\u0e19\u0e01\u0e32\u0e23\u0e41\u0e1b\u0e25\u0e07\u0e40\u0e2d\u0e07\u0e42\u0e14\u0e22\u0e2d\u0e34\u0e07\u0e08\u0e32\u0e01\u0e2b\u0e25\u0e31\u0e01\u0e01\u0e32\u0e23\u0e17\u0e35\u0e48\u0e01\u0e25\u0e48\u0e32\u0e27\u0e44\u0e1b\u0e02\u0e49\u0e32\u0e07\u0e15\u0e49\u0e19 \u0e17\u0e33\u0e43\u0e2b\u0e49\u0e40\u0e23\u0e32\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e41\u0e1b\u0e25\u0e07 functional group \u0e40\u0e1b\u0e47\u0e19 smiles \u0e44\u0e14\u0e49\u0e2a\u0e33\u0e40\u0e23\u0e47\u0e08\u0e41\u0e25\u0e30\u0e17\u0e33\u0e43\u0e2b\u0e49\u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e15\u0e31\u0e27\u0e41\u0e1b\u0e23\u0e2d\u0e37\u0e48\u0e19\u0e40\u0e1e\u0e34\u0e48\u0e21\u0e02\u0e36\u0e49\u0e19\u0e40\u0e22\u0e2d\u0e30\u0e21\u0e32\u0e01 \u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e19\u0e33\u0e15\u0e31\u0e27\u0e41\u0e1b\u0e23\u0e17\u0e35\u0e48\u0e40\u0e1e\u0e34\u0e48\u0e21\u0e21\u0e32\u0e1e\u0e31\u0e12\u0e19\u0e32\u0e42\u0e21\u0e40\u0e14\u0e25\u0e44\u0e14\u0e49\u0e21\u0e35\u0e1b\u0e23\u0e30\u0e2a\u0e34\u0e17\u0e18\u0e34\u0e20\u0e32\u0e1e\u0e21\u0e32\u0e01\u0e02\u0e36\u0e49\u0e19 \u0e19\u0e2d\u0e01\u0e08\u0e32\u0e01\u0e19\u0e35\u0e49 \u0e22\u0e31\u0e07\u0e1e\u0e1a\u0e1b\u0e31\u0e0d\u0e2b\u0e32\u0e43\u0e19\u0e01\u0e32\u0e23\u0e04\u0e33\u0e19\u0e27\u0e13 surface \u0e08\u0e32\u0e01\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25 mofs \u0e0b\u0e36\u0e48\u0e07\u0e21\u0e35\u0e1b\u0e23\u0e34\u0e21\u0e32\u0e13\u0e21\u0e32\u0e01\u0e41\u0e25\u0e30\u0e15\u0e49\u0e2d\u0e07\u0e43\u0e0a\u0e49\u0e40\u0e27\u0e25\u0e32\u0e19\u0e32\u0e19 \u0e17\u0e38\u0e01\u0e04\u0e19\u0e43\u0e19\u0e17\u0e35\u0e21\u0e08\u0e36\u0e07\u0e41\u0e1a\u0e48\u0e07\u0e07\u0e32\u0e19\u0e01\u0e31\u0e19\u0e23\u0e31\u0e19\u0e2b\u0e32 surface \u0e17\u0e33\u0e43\u0e2b\u0e49\u0e43\u0e0a\u0e49\u0e40\u0e27\u0e25\u0e32\u0e19\u0e49\u0e2d\u0e22\u0e25\u0e07\naccomplishments that we're proud of\n\u0e20\u0e32\u0e04\u0e20\u0e39\u0e21\u0e34\u0e43\u0e08\u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e25\u0e2d\u0e07\u0e2b\u0e32\u0e2a\u0e34\u0e48\u0e07\u0e43\u0e2b\u0e21\u0e48 \u0e46 \u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49\u0e43\u0e2b\u0e21\u0e48 \u0e46 \u0e19\u0e2d\u0e01\u0e2b\u0e49\u0e2d\u0e07\u0e40\u0e23\u0e35\u0e22\u0e19 \u0e44\u0e14\u0e49\u0e25\u0e2d\u0e07\u0e44\u0e14\u0e49\u0e21\u0e32\u0e40\u0e08\u0e2d\u0e1b\u0e31\u0e0d\u0e2b\u0e32\u0e08\u0e23\u0e34\u0e07 \u0e44\u0e14\u0e49\u0e25\u0e2d\u0e07\u0e21\u0e32\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25\u0e08\u0e23\u0e34\u0e07 \u0e40\u0e19\u0e37\u0e48\u0e2d\u0e07\u0e08\u0e32\u0e01\u0e01\u0e32\u0e23\u0e41\u0e02\u0e48\u0e07\u0e04\u0e23\u0e31\u0e49\u0e07\u0e19\u0e35\u0e49\u0e40\u0e1b\u0e47\u0e19\u0e01\u0e32\u0e23\u0e41\u0e02\u0e48\u0e07\u0e02\u0e31\u0e19\u0e14\u0e49\u0e32\u0e19 machine learning \u0e04\u0e23\u0e31\u0e49\u0e07\u0e41\u0e23\u0e01\u0e02\u0e2d\u0e07\u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32 \u0e16\u0e36\u0e07\u0e41\u0e21\u0e49\u0e01\u0e48\u0e2d\u0e19\u0e2b\u0e19\u0e49\u0e32\u0e19\u0e35\u0e49\u0e08\u0e30\u0e21\u0e35\u0e42\u0e2d\u0e01\u0e32\u0e2a\u0e44\u0e14\u0e49\u0e25\u0e2d\u0e07\u0e17\u0e33\u0e41\u0e1a\u0e1a\u0e1d\u0e36\u0e01\u0e2b\u0e31\u0e14\u0e40\u0e23\u0e37\u0e48\u0e2d\u0e07 machine learning \u0e43\u0e19\u0e2b\u0e49\u0e2d\u0e07\u0e40\u0e23\u0e35\u0e22\u0e19\u0e21\u0e32\u0e1a\u0e49\u0e32\u0e07 \u0e41\u0e15\u0e48\u0e01\u0e47\u0e22\u0e31\u0e07\u0e15\u0e49\u0e2d\u0e07\u0e28\u0e36\u0e01\u0e29\u0e32\u0e40\u0e1e\u0e34\u0e48\u0e21\u0e40\u0e15\u0e34\u0e21\u0e41\u0e25\u0e30\u0e1e\u0e31\u0e12\u0e19\u0e32\u0e17\u0e31\u0e01\u0e29\u0e30\u0e40\u0e1e\u0e34\u0e48\u0e21\u0e21\u0e32\u0e01\u0e02\u0e36\u0e49\u0e19 \u0e43\u0e19\u0e01\u0e32\u0e23\u0e41\u0e02\u0e48\u0e07\u0e02\u0e31\u0e19\u0e04\u0e23\u0e31\u0e49\u0e07\u0e19\u0e35\u0e49\u0e17\u0e33\u0e43\u0e2b\u0e49\u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e21\u0e35\u0e1b\u0e23\u0e30\u0e2a\u0e1a\u0e01\u0e32\u0e23\u0e13\u0e4c\u0e25\u0e07\u0e21\u0e37\u0e2d\u0e17\u0e33\u0e08\u0e23\u0e34\u0e07\u0e41\u0e25\u0e30\u0e44\u0e14\u0e49\u0e04\u0e49\u0e19\u0e04\u0e27\u0e49\u0e32\u0e2b\u0e32\u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49\u0e40\u0e1e\u0e34\u0e48\u0e21\u0e40\u0e15\u0e34\u0e21 \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e21\u0e32\u0e1e\u0e31\u0e12\u0e19\u0e32\u0e42\u0e21\u0e40\u0e14\u0e25 \u0e2d\u0e35\u0e01\u0e2b\u0e19\u0e36\u0e48\u0e07\u0e04\u0e27\u0e32\u0e21\u0e20\u0e32\u0e04\u0e20\u0e39\u0e21\u0e34\u0e43\u0e08 \u0e04\u0e37\u0e2d \u0e04\u0e27\u0e32\u0e21\u0e2a\u0e32\u0e21\u0e31\u0e04\u0e04\u0e35\u0e43\u0e19\u0e01\u0e32\u0e23\u0e41\u0e02\u0e48\u0e07\u0e02\u0e31\u0e19\u0e02\u0e2d\u0e07\u0e04\u0e19\u0e43\u0e19\u0e17\u0e35\u0e21\u0e17\u0e35\u0e48\u0e17\u0e33\u0e43\u0e2b\u0e49\u0e01\u0e32\u0e23\u0e41\u0e02\u0e48\u0e07\u0e02\u0e31\u0e19\u0e19\u0e35\u0e49\u0e40\u0e2a\u0e23\u0e47\u0e08\u0e25\u0e38\u0e25\u0e48\u0e27\u0e07 \u0e16\u0e36\u0e07\u0e41\u0e21\u0e49\u0e08\u0e30\u0e40\u0e08\u0e2d\u0e1b\u0e31\u0e0d\u0e2b\u0e32\u0e41\u0e25\u0e30\u0e2d\u0e38\u0e1b\u0e2a\u0e23\u0e23\u0e04\u0e15\u0e48\u0e32\u0e07 \u0e46 \u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32\u0e01\u0e47\u0e44\u0e21\u0e48\u0e22\u0e48\u0e2d\u0e17\u0e49\u0e2d\u0e15\u0e48\u0e2d\u0e1b\u0e31\u0e0d\u0e2b\u0e32\u0e17\u0e35\u0e48\u0e40\u0e01\u0e34\u0e14\u0e02\u0e36\u0e49\u0e19 \u0e43\u0e19\u0e23\u0e30\u0e22\u0e30\u0e40\u0e27\u0e25\u0e32 1 \u0e40\u0e14\u0e37\u0e2d\u0e19\u0e17\u0e35\u0e48\u0e1c\u0e48\u0e32\u0e19\u0e21\u0e32 \u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32\u0e2a\u0e39\u0e49\u0e08\u0e19\u0e16\u0e36\u0e07\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14 \u0e0a\u0e48\u0e27\u0e22\u0e01\u0e31\u0e19\u0e1f\u0e31\u0e19\u0e1d\u0e48\u0e32\u0e2d\u0e38\u0e1b\u0e2a\u0e23\u0e23\u0e04\u0e08\u0e19\u0e16\u0e36\u0e07\u0e40\u0e2a\u0e49\u0e19\u0e0a\u0e31\u0e22\u0e41\u0e2b\u0e48\u0e07\u0e04\u0e27\u0e32\u0e21\u0e1e\u0e22\u0e32\u0e22\u0e32\u0e21\nwhat we learned\n\u0e08\u0e32\u0e01\u0e01\u0e32\u0e23\u0e44\u0e14\u0e49\u0e25\u0e07\u0e41\u0e02\u0e48\u0e07\u0e43\u0e19\u0e04\u0e23\u0e31\u0e49\u0e07\u0e19\u0e35\u0e49 \u0e17\u0e32\u0e07\u0e01\u0e25\u0e38\u0e48\u0e21\u0e02\u0e2d\u0e07\u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32\u0e44\u0e14\u0e49\u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49\u0e17\u0e32\u0e07\u0e40\u0e04\u0e21\u0e35\u0e40\u0e1e\u0e34\u0e48\u0e21\u0e21\u0e32\u0e01\u0e02\u0e36\u0e49\u0e19 \u0e42\u0e14\u0e22\u0e40\u0e09\u0e1e\u0e32\u0e30\u0e40\u0e23\u0e37\u0e48\u0e2d\u0e07 metal-organic frameworks (mofs) \u0e44\u0e14\u0e49\u0e23\u0e39\u0e49\u0e27\u0e34\u0e18\u0e35\u0e40\u0e02\u0e35\u0e22\u0e19\u0e42\u0e1b\u0e23\u0e41\u0e01\u0e23\u0e21\u0e41\u0e1a\u0e1a\u0e43\u0e2b\u0e21\u0e48\u0e46 library\u0e15\u0e48\u0e32\u0e07\u0e46 \u0e17\u0e31\u0e49\u0e07\u0e17\u0e32\u0e07 modeling \u0e41\u0e25\u0e30\u0e17\u0e32\u0e07\u0e40\u0e04\u0e21\u0e35 \u0e21\u0e35\u0e17\u0e31\u0e01\u0e29\u0e30\u0e43\u0e19\u0e01\u0e32\u0e23 explore \u0e41\u0e25\u0e30 clean \u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e17\u0e35\u0e48\u0e08\u0e30\u0e19\u0e33\u0e44\u0e1b\u0e43\u0e0a\u0e49\u0e43\u0e19\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e42\u0e21\u0e40\u0e14\u0e25\u0e17\u0e33\u0e19\u0e32\u0e22\u0e1c\u0e25\u0e43\u0e2b\u0e49\u0e21\u0e35\u0e1b\u0e23\u0e30\u0e2a\u0e34\u0e17\u0e18\u0e34\u0e20\u0e32\u0e1e\u0e21\u0e32\u0e01\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14 \u0e44\u0e14\u0e49\u0e23\u0e39\u0e49\u0e08\u0e31\u0e01\u0e15\u0e31\u0e27\u0e41\u0e1a\u0e1a\u0e2b\u0e23\u0e37\u0e2d\u0e40\u0e17\u0e04\u0e19\u0e34\u0e04\u0e43\u0e19\u0e01\u0e32\u0e23\u0e17\u0e33\u0e43\u0e2b\u0e21\u0e48\u0e46 \u0e19\u0e2d\u0e01\u0e08\u0e32\u0e01\u0e19\u0e35\u0e49\u0e22\u0e31\u0e07\u0e44\u0e14\u0e49\u0e17\u0e31\u0e01\u0e29\u0e30\u0e43\u0e19\u0e01\u0e32\u0e23\u0e2a\u0e23\u0e49\u0e32\u0e07\u0e41\u0e25\u0e30\u0e1e\u0e31\u0e12\u0e19\u0e32\u0e15\u0e31\u0e27\u0e41\u0e1a\u0e1a\u0e40\u0e1e\u0e34\u0e48\u0e21\u0e02\u0e36\u0e49\u0e19\u0e2d\u0e35\u0e01\u0e14\u0e49\u0e27\u0e22\nwhat's next for thunder storm\n\u0e1e\u0e31\u0e12\u0e19\u0e32\u0e04\u0e27\u0e32\u0e21\u0e23\u0e39\u0e49\u0e43\u0e19\u0e14\u0e49\u0e32\u0e19 machine learning \u0e15\u0e48\u0e2d\u0e41\u0e25\u0e30\u0e40\u0e02\u0e49\u0e32\u0e23\u0e48\u0e27\u0e21\u0e01\u0e32\u0e23\u0e41\u0e02\u0e48\u0e07\u0e02\u0e31\u0e19\u0e2d\u0e37\u0e48\u0e19\u0e46\u0e2d\u0e35\u0e01", "sortedWord": "None", "removed": "Nan", "score": 6, "comments": 0, "media": null, "medialink": null, "identifyer": 59507842}, {"Unnamed: 0": 8309, "autor": "Tree-like Street Light", "date": null, "content": "Inspiration\nInspired by the mechanism of forest. As seeing many streetlights on the road, I am thinking can I develop the streetlights to more sustainable streetlights that acts like a tree. It can generate electricity and help to reduce CO2 in the air. It also can help to solve flood risk and water scarcity that act like a tree.\nWhat it does\nTree-like Street Light is designed to reduce the risk of flooding and turn the excess water to generate electricity and reduce the possibility of water scarcity happens in the selected area in the future. It also have a water system that be build to manage the flow of excess water during flood and prevent waste of water during flood as the water collected will be reuse after water be treated.\nTree-like Street light is inspired by the mechanism of forest. To achieve zero floods , we come out with tree-like street light that inspired by the mechanism of the forest. Funnel-shaped street lights can collect more rainwater even when the wind direction is different due to the larger surface area. The upper side of the street light is made of solar panels while underneath the funnel is LED light that will light up at night.\nEpiphytes and bryophytes will be grown on the street light. Rainwater will be absorbed by a tube of orbeez that has plants to provide water to the plants. The plants will purify the air and monitor the quality of the water and air. This can help to achieve urban greening.\nThe funnel part and the holes surrounded under the street light are the entrance for the water to enter. The height of the holes is lower than the road. Filters are installed in the holes to block waste from entering the piping system.\nAfter the water enters the concrete drain, the water will flow along until it flows to the mesh filter. Some water will flow through the mesh filter to the next tree-like street lights while most of the water will flow down to the catch basin due to gravity. The rubbish and debris will trap in the drain inlet that consists of small holes to allow more water flow during heavy rain. The missed trapped rubbish and debris will be blocked by another mesh filter on the exit of the catch basin and settle down. Oil and grease will trap in the drain inlet insert. Water will then flow to the micro-hydro turbine located underneath each street light.\nThe electricity generated by micro-hydro turbines will operate a small sewage system nearby. If there is excess electricity from solar panels, the electricity will help to work the small sewage system. The micro-hydro turbine creates a sustainable energy resource to power the small sewage system and reverses osmosis.\nSome water collected from the street light and a small sewage treatment plant will be stored in the underground storage tank, while the excess water will flow out to ponds, rivers, another water treatment system, and underground during the flood. Referring to the ancient city Ganzhou, a water window will be inserted on the exit to nearby rivers. When the river level is high, the water window automatically closes due to river pressure and vice versa. During a flood, ponds and lakes will be used to collect and store trapped water when floods.\nThe black water, grey water, and the water collected during a flood will undergo reserve osmosis then supply drinking water to the resident when there is no water supply. The black water will flow in a specific pipe to the sewage treatment system while greywater and water collected when the rain will be the continuous supply for the micro-hydro turbine. The water that does not undergo reserve osmosis will not allow entering the piping system to residents. The water will flow along the loop of the piping system underneath the street light to guarantee only clean recycled water supply to the resident during the flood and pledge a continuous water supply to the micro-hydro turbine.\nThe loop will continue day-to-day but when flood, excess water will be distributed.\nHow we built it\n-Have not do prototype but have done draft in 3d form\nChallenges we ran into\n-The rubbish will affect the water flow to generate electricity. -Have not do experiment to see its workability\nAccomplishments that we're proud of\n-Gold Award in SiCEx COMPETITION USM 2021 -Silver Award in I2CreaTE-2021 -Gold medal for The 6th International Invention Innovation Competition in Canada, iCAN 2021 -3rd Place Pitch in the Envirothon -Wolfram Award for top 10 teams\nWhat we learned\n-Keep improve Tree-like Street Light to a more sustainable and affordable street light -Try to challenge more Sustainable Development Goal\nWhat's next for Tree-like Street Light\n-Semi-permeable road pavement that can generate electricity through osmosis -Tesla Turbine can be study to be inserted to increase electricity in the future -Advance filter can be inserted in the street light after water enters the pipe to prevent clogging and mechanical damage such as DSD Separator from bioclean company -People can drink water that condense from the air from the street light. -More recycled materials or sustainable materials be used to construct this product and system to reduce carbon footprint. We plan to use recycled plastic to be the main material to create the street light. 3D Printing will be used to cut down some costs. Recycled solar panel will be used for each street light .We also hope to use a more sustainable cement to build the concrete drain in the future", "link": "https://devpost.com/software/tree-like-street-light-yksfvn", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\ninspired by the mechanism of forest. as seeing many streetlights on the road, i am thinking can i develop the streetlights to more sustainable streetlights that acts like a -----> tree !!! . it can generate electricity and help to reduce co2 in the air. it also can help to solve flood risk and water scarcity that act like a tree.\nwhat it does\ntree-like street light is designed to reduce the risk of flooding and turn the excess water to generate electricity and reduce the possibility of water scarcity happens in the selected area in the future. it also have a water system that be build to manage the flow of excess water during flood and prevent waste of water during flood as the water collected will be reuse after water be treated.\ntree-like street light is inspired by the mechanism of forest. to achieve zero floods , we come out with tree-like street light that inspired by the mechanism of the forest. funnel-shaped street lights can collect more rainwater even when the wind direction is different due to the larger surface area. the upper side of the street light is made of solar panels while underneath the funnel is led light that will light up at night.\nepiphytes and bryophytes will be grown on the street light. rainwater will be absorbed by a tube of orbeez that has plants to provide water to the plants. the plants will purify the air and monitor the quality of the water and air. this can help to achieve urban greening.\nthe funnel part and the holes surrounded under the street light are the entrance for the water to enter. the height of the holes is lower than the road. filters are installed in the holes to block waste from entering the piping system.\nafter the water enters the concrete drain, the water will flow along until it flows to the mesh filter. some water will flow through the mesh filter to the next tree-like street lights while most of the water will flow down to the catch basin due to gravity. the rubbish and debris will trap in the drain inlet that consists of small holes to allow more water flow during heavy rain. the missed trapped rubbish and debris will be blocked by another mesh filter on the exit of the catch basin and settle down. oil and grease will trap in the drain inlet insert. water will then flow to the micro-hydro turbine located underneath each street light.\nthe electricity generated by micro-hydro turbines will operate a small sewage system nearby. if there is excess electricity from solar panels, the electricity will help to work the small sewage system. the micro-hydro turbine creates a sustainable energy resource to power the small sewage system and reverses osmosis.\nsome water collected from the street light and a small sewage treatment plant will be stored in the underground storage tank, while the excess water will flow out to ponds, rivers, another water treatment system, and underground during the flood. referring to the ancient city ganzhou, a water window will be inserted on the exit to nearby rivers. when the river level is high, the water window automatically closes due to river pressure and vice versa. during a flood, ponds and lakes will be used to collect and store trapped water when floods.\nthe black water, grey water, and the water collected during a flood will undergo reserve osmosis then supply drinking water to the resident when there is no water supply. the black water will flow in a specific pipe to the sewage treatment system while greywater and water collected when the rain will be the continuous supply for the micro-hydro turbine. the water that does not undergo reserve osmosis will not allow entering the piping system to residents. the water will flow along the loop of the piping system underneath the street light to guarantee only clean recycled water supply to the resident during the flood and pledge a continuous water supply to the micro-hydro turbine.\nthe loop will continue day-to-day but when flood, excess water will be distributed.\nhow we built it\n-have not do prototype but have done draft in 3d form\nchallenges we ran into\n-the rubbish will affect the water flow to generate electricity. -have not do experiment to see its workability\naccomplishments that we're proud of\n-gold award in sicex competition usm 2021 -silver award in i2create-2021 -gold medal for the 6th international invention innovation competition in canada, ican 2021 -3rd place pitch in the envirothon -wolfram award for top 10 teams\nwhat we learned\n-keep improve tree-like street light to a more sustainable and affordable street light -try to challenge more sustainable development goal\nwhat's next for tree-like street light\n-semi-permeable road pavement that can generate electricity through osmosis -tesla turbine can be study to be inserted to increase electricity in the future -advance filter can be inserted in the street light after water enters the pipe to prevent clogging and mechanical damage such as dsd separator from bioclean company -people can drink water that condense from the air from the street light. -more recycled materials or sustainable materials be used to construct this product and system to reduce carbon footprint. we plan to use recycled plastic to be the main material to create the street light. 3d printing will be used to cut down some costs. recycled solar panel will be used for each street light .we also hope to use a more sustainable cement to build the concrete drain in the future", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59508309}, {"Unnamed: 0": 8328, "autor": "Started learning Ruby on Rails !", "date": null, "content": "Workflow for Creating Rails Applications\nA recommended work flow for creating Rails Application is as follows \u2212\nUse the rails command to create the basic skeleton of the application.\nCreate a database on the PostgreSQL server to hold your data.\nConfigure the application to know where your database is located and the login credentials for it.\nCreate Rails Active Records (Models), because they are the business objects you'll be working with in your controllers.\nGenerate Migrations that simplify the creating and maintaining of database tables and columns.\nWrite Controller Code to put a life in your application.\nCreate Views to present your data through User Interface.\nSo, let us start with creating our library application.\nCreating an Empty Rails Web Application Rails is both a runtime web application framework and a set of helper scripts that automate many of the things you do when developing a web application. In this step, we will use one such helper script to create the entire directory structure and the initial set of files to start our Library System application.\nGo into ruby installation directory to create your application.\nRun the following command to create a skeleton for library application. It will create the directory structure in the current directory.\ntp> rails new library This will create a subdirectory for the library application containing a complete directory tree of folders and files for an empty Rails application. Check a complete directory structure of the application. Check Rails Directory Structure for more detail.\nMost of our development work will be creating and editing files in the library/app subdirectories. Here's a quick run down of how to use them \u2212\nThe controllers subdirectory is where Rails looks to find controller classes. A controller handles a web request from the user.\nThe views subdirectory holds the display templates to fill in with data from our application, convert to HTML, and return to the user's browser.\nThe models subdirectory holds the classes that model and wrap the data stored in our application's database. In most frameworks, this part of the application can grow pretty messy, tedious, verbose, and error-prone. Rails makes it dead simple.\nThe helpers subdirectory holds any helper classes used to assist the model, view, and controller classes. This helps to keep the model, view, and controller code small, focused, and uncluttered.\nStarting Web Server Rails web application can run under virtually any web server, but the most convenient way to develop a Rails web application is to use the built-in WEBrick web server. Let's start this web server and then browse to our empty library application \u2212\nThis server will be started from the application directory as follows. It runs on port number 3000.\ntp> cd ruby\\library tp\\ruby\\library\\> Rails server\nNow open your browser and browse to http://127.0.0.1:3000. If everything is gone fine, then you should see a greeting message from WEBrick.", "link": "https://devpost.com/software/started-learning-ruby-on-rails", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "workflow for creating rails applications\na recommended work flow for creating rails application is as follows \u2212\nuse the rails command to create the basic skeleton of the application.\ncreate a database on the postgresql server to hold your data.\nconfigure the application to know where your database is located and the login credentials for it.\ncreate rails active records (models), because they are the business objects you'll be working with in your controllers.\ngenerate migrations that simplify the creating and maintaining of database tables and columns.\nwrite controller code to put a life in your application.\ncreate views to present your data through user interface.\nso, let us start with creating our library application.\ncreating an empty rails web application rails is both a runtime web application framework and a set of helper scripts that automate many of the things you do when developing a web application. in this step, we will use one such helper script to create the entire directory structure and the initial set of files to start our library system application.\ngo into ruby installation directory to create your application.\nrun the following command to create a skeleton for library application. it will create the directory structure in the current directory.\ntp> rails new library this will create a subdirectory for the library application containing a complete directory -----> tree !!!  of folders and files for an empty rails application. check a complete directory structure of the application. check rails directory structure for more detail.\nmost of our development work will be creating and editing files in the library/app subdirectories. here's a quick run down of how to use them \u2212\nthe controllers subdirectory is where rails looks to find controller classes. a controller handles a web request from the user.\nthe views subdirectory holds the display templates to fill in with data from our application, convert to html, and return to the user's browser.\nthe models subdirectory holds the classes that model and wrap the data stored in our application's database. in most frameworks, this part of the application can grow pretty messy, tedious, verbose, and error-prone. rails makes it dead simple.\nthe helpers subdirectory holds any helper classes used to assist the model, view, and controller classes. this helps to keep the model, view, and controller code small, focused, and uncluttered.\nstarting web server rails web application can run under virtually any web server, but the most convenient way to develop a rails web application is to use the built-in webrick web server. let's start this web server and then browse to our empty library application \u2212\nthis server will be started from the application directory as follows. it runs on port number 3000.\ntp> cd ruby\\library tp\\ruby\\library\\> rails server\nnow open your browser and browse to http://127.0.0.1:3000. if everything is gone fine, then you should see a greeting message from webrick.", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59508328}, {"Unnamed: 0": 8997, "autor": "LEET Protect", "date": null, "content": "Inspiration\nGlobally, we lose about 8 billion trees per year, releasing 6 billion tonnes CO2 annually in addition to affecting 2 billion people living dependently on forests. As scientists and tech entrepreneurs, we strongly believe we can address these problems effectively at speed and scale. We need to stop the loss for our future generations and planet.\nWhat it does\nOur tech uses machine learning algorithms to detect trees every 5-16 days. Tree owners and the public are connected to the trees and incentives for tree protection and planting are provided in forms of NFTs and carbon sales.\nHow we built it\nLEET Carbon is built on iOS and Android for tree and human connection. LEET Protect is the next version of LEET Carbon to be built using blockchain technology for transparency, fairness, and success.\nChallenges we ran into\nWe are the top experts in the field of forest carbon management and technovation for sustainability. We understand the problems of tree loss, deforestation, and forest degradation better than the average experts in the field. Still, we need data scientists and blockchain engineers to scale up the project or seed funding to hire them for the sake of our planet.\nAccomplishments that we're proud of\nWe organized many professional workshops and published high-impact peer-reviewed journal articles based on our algorithms and methods. APP of LEET Carbon is expected by end of October.\nWhat we learned\nTeamwork and people management are important toward success\nWhat's next for LEET Protect\nWe plan to introduce blockchain technology for LEET Protect APP, where tree owners are connected to trees and allocated NFTs as incentives for tree protection and more planting.", "link": "https://devpost.com/software/leet-protect", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "inspiration\nglobally, we lose about 8 billion trees per year, releasing 6 billion tonnes co2 annually in addition to affecting 2 billion people living dependently on forests. as scientists and tech entrepreneurs, we strongly believe we can address these problems effectively at speed and scale. we need to stop the loss for our future generations and planet.\nwhat it does\nour tech uses machine learning algorithms to detect trees every 5-16 days. -----> tree !!!  owners and the public are connected to the trees and incentives for -----> tree !!!  protection and planting are provided in forms of nfts and carbon sales.\nhow we built it\nleet carbon is built on ios and android for tree and human connection. leet protect is the next version of leet carbon to be built using blockchain technology for transparency, fairness, and success.\nchallenges we ran into\nwe are the top experts in the field of forest carbon management and technovation for sustainability. we understand the problems of tree loss, deforestation, and forest degradation better than the average experts in the field. still, we need data scientists and blockchain engineers to scale up the project or seed funding to hire them for the sake of our planet.\naccomplishments that we're proud of\nwe organized many professional workshops and published high-impact peer-reviewed journal articles based on our algorithms and methods. app of leet carbon is expected by end of october.\nwhat we learned\nteamwork and people management are important toward success\nwhat's next for leet protect\nwe plan to introduce blockchain technology for leet protect app, where tree owners are connected to trees and allocated nfts as incentives for tree protection and more planting.", "sortedWord": "None", "removed": "Nan", "score": 62, "comments": 0, "media": null, "medialink": null, "identifyer": 59508997}], "name": "treeDevpost"}
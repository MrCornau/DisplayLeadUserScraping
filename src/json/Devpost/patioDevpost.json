{"interestingcomments": [{"Unnamed: 0": 6595, "autor": "Architectural Grilles & Sunshades, Inc.", "date": null, "content": "Architectural Grilles & Sunshades, Inc. aluminum trellises are excellent for limiting heat gain and providing shade for sun-soaked patio and outdoor eating areas. Depending on your building\u2019s design, they can also be an integral part of its architecture. We offer aluminum trellises, aluminum garden trellis, aluminum wall trellis, pergolas, outdoor trellis, aluminium trellis panels & systems built to your specific design.\n#customwindowsunshades #customsunshades #customsunshades #customexteriorsunshades #customoutdoorsunshades #customexteriorshades #customoutdoorshades\nArchitectural Grilles & Sunshades, Inc. - Location\nAddress :- 22442 S West Fey Drive Frankfort IL 60423\nPhone :- (708) 479-9458\nServices We Offer\nvertical window sunshades\naluminum trellis panels\ntrellis panels\ncustom window sunshades\ncustom sunshades\ndecorative grilles\nornamental grilles\ndecorative metal grilles\nornamental grill\ndecorative wall grilles\nlight shelves\nvertical window sunshades\nvertical sun shades exterior\nvertical sun shades\nvertical sunshades\noutdoor vertical shade\nOur Other Links\naluminum trellis panels - https://agsshade.com/trellises/\nairfoil window sunshades - https://agsshade.com/airfoil-shades/\ncustom window sunshades - https://agsshade.com/custom-shades/\noutdoor sun shades - https://agsshade.com/sunshades/\ndecorative grilles - https://agsshade.com/grilles/\nArchitectural Grilles & Sunshades, Inc. - Social Profile\nyoutube.com - https://www.youtube.com/channel/UC3u0ql3u2L8vDDs7ehc7GsQ\npinterest.com - https://www.pinterest.com/architecturalgrilles1449/\ntwitter.com - https://twitter.com/agsshade\nfacebook.com - https://www.facebook.com/agssunshades/", "link": "https://devpost.com/software/architectural-grilles-sunshades-inc", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "patio", "selectorShort": "patio", "MarkedSent": "architectural grilles & sunshades, inc. aluminum trellises are excellent for limiting heat gain and providing shade for sun-soaked -----> patio !!!  and outdoor eating areas. depending on your building\u2019s design, they can also be an integral part of its architecture. we offer aluminum trellises, aluminum garden trellis, aluminum wall trellis, pergolas, outdoor trellis, aluminium trellis panels & systems built to your specific design.\n#customwindowsunshades #customsunshades #customsunshades #customexteriorsunshades #customoutdoorsunshades #customexteriorshades #customoutdoorshades\narchitectural grilles & sunshades, inc. - location\naddress :- 22442 s west fey drive frankfort il 60423\nphone :- (708) 479-9458\nservices we offer\nvertical window sunshades\naluminum trellis panels\ntrellis panels\ncustom window sunshades\ncustom sunshades\ndecorative grilles\nornamental grilles\ndecorative metal grilles\nornamental grill\ndecorative wall grilles\nlight shelves\nvertical window sunshades\nvertical sun shades exterior\nvertical sun shades\nvertical sunshades\noutdoor vertical shade\nour other links\naluminum trellis panels - https://agsshade.com/trellises/\nairfoil window sunshades - https://agsshade.com/airfoil-shades/\ncustom window sunshades - https://agsshade.com/custom-shades/\noutdoor sun shades - https://agsshade.com/sunshades/\ndecorative grilles - https://agsshade.com/grilles/\narchitectural grilles & sunshades, inc. - social profile\nyoutube.com - https://www.youtube.com/channel/uc3u0ql3u2l8vdds7ehc7gsq\npinterest.com - https://www.pinterest.com/architecturalgrilles1449/\ntwitter.com - https://twitter.com/agsshade\nfacebook.com - https://www.facebook.com/agssunshades/", "sortedWord": "None", "removed": "Nan", "score": 0, "comments": 0, "media": null, "medialink": null, "identifyer": 59506595}, {"Unnamed: 0": 7475, "autor": "KLARITY", "date": null, "content": "KLARITY\nDataset Link\nhttps://github.com/philipk19238/klarity/tree/master/data\nInspiration\nMany members of our team enjoy thrifting & buying used products but we've oftentimes had to bargain & haggle to acquire an item. This process oftentimes results in one party paying too much/too little and is overall an unpleasant experience. We thought it would be cool to build a solution that allows buyers to know if they're paying too much for a used good and inform sellers of the going market rate so they can correctly price their items.\nWhat really solidified our decision is that we discovered that the used goods market is growing at a rapid pace and we believe our solution can encourage more people to \"renew, rather than by new.\"\nWhat it does\nKlarity is an advanced search engine that tracks the price of used goods over time. Feel free to enter any query and Klarity will visualize the price trend results to you instantly!\nThe reason we are able to provide such data is because it is powered by a powerful web scraper that uses multiprocessing to scrape thousands of websites in mere minutes. Furthermore, we built an advanced labeller & tagger that goes through our scraped results and adds additional tags to the data to provide our users with additional context.\nHow we built it\nArchitecture\nThe Klarity backend is comprised into three main sections - scraper, labeller, and API.\nScraper\nhttps://github.com/philipk19238/klarity/tree/master/api/app/scraper\nOur scraper uses a two step process to parse websites. First, it seeds our Mongo database with tens of thousands of \"scrape-able\" links using a DFS algorithm. This process is designed to be a background job that scrapes new links for our scraper to process and will stop once it detects a link it already scraped. Afterwards, we paginate over those results and utlize multiprocessing & caching to quickly scrape, label, and save the data into our database. We originally designed this scraper to be asynchronous but could not implement due to time constraints\nLabeller\nhttps://github.com/philipk19238/klarity/tree/master/api/app/labeller\nOur labeller also uses a two step process that cleans & adds additional tags to our dataset. The cleaning process consists of four steps:\nTokenize the input sentences into individual words\nConvert everything to lower case\nFilter punctuation\nFilter out stop words (ex: the, is, how, etc)\nAfterwards, it uses a Trie to search for unique labels. We initialize our Trie with precoded constants with tags that represent the majority of the data that we scrape. We would then iterate the title & the description of the used goods post over the Trie to label the data.\nAPI\nhttps://github.com/philipk19238/klarity/tree/master/api/app/api\nOur API consists of two main endpoints:\n/api/export\n/api/query\nExport\nOur export endpoint converts the data inside our database into a CSV file and streams the bytes to the user for download\nQuery\nOur query endpoint accepts a parameter of type mean, median, min, and max as a metric to visualize the price trend and accepts the following query parameters:\ntype\ncondition (new, like new, good, etc)\nmaterial\nhome_location (dining, living, patio, etc)\ncolor\nmattress_size (king, queen, full, twin, etc)\nIt will then filter our database and group results by date. We planned on adding more visualizations such as item comparisons and location clustering but ran out of time. This information is visualized on our React.js frontend.\nTechnologies Used\nDatabase\nMongoDB\nBackend API\nFlask + Swagger\nFrontend Framework\nReact.js\nLabeller\nnltk + custom Trie\nDevops\nDocker, Makefile\nScraping\nrequests + BeautifulSoup + multiprocessing\nChallenges we ran into\nWe ran into a lot of difficulties with scraping data. Our scraper processed links too quickly and would oftentimes trigger anti-scraper measures. We did our best to adapt by switching user agents constantly and randomizing time between requests. We were able to scrape ~10,000 rows of data but can definitely scrape a lot more if we didn't have such limitations.\nAccomplishments that we're proud of\nWe're proud of finishing such a holistic project in a short amount of time. Furthermore, our team mainly consisted of business majors with little technical experience so it was a great eye opener to the world of software. Finally, we're proud of being able to fight through many roadblocks, hurdles, and weird bugs without getting discouraged and managing to submit a project.\nWhat we learned\nWe learned a ton about web scraping, database technologies, and visualization methods. We also improved our teamwork, collaboration, and commmunication skills.\nWhat's next for KLARITY\nWe plan on building out more of the visualizations and eventually hosting the website online so everyone can use it. Furthermore, if we schedule nightly scraping jobs, we can build up a large collection of used goods data. The North Star would be to create a free chrome extension so everyone can compare used good prices for free!", "link": "https://devpost.com/software/klarity", "origin": "Devpost", "suborigin": "Devpost", "result": true, "Selector": "patio", "selectorShort": "patio", "MarkedSent": "klarity\ndataset link\nhttps://github.com/philipk19238/klarity/tree/master/data\ninspiration\nmany members of our team enjoy thrifting & buying used products but we've oftentimes had to bargain & haggle to acquire an item. this process oftentimes results in one party paying too much/too little and is overall an unpleasant experience. we thought it would be cool to build a solution that allows buyers to know if they're paying too much for a used good and inform sellers of the going market rate so they can correctly price their items.\nwhat really solidified our decision is that we discovered that the used goods market is growing at a rapid pace and we believe our solution can encourage more people to \"renew, rather than by new.\"\nwhat it does\nklarity is an advanced search engine that tracks the price of used goods over time. feel free to enter any query and klarity will visualize the price trend results to you instantly!\nthe reason we are able to provide such data is because it is powered by a powerful web scraper that uses multiprocessing to scrape thousands of websites in mere minutes. furthermore, we built an advanced labeller & tagger that goes through our scraped results and adds additional tags to the data to provide our users with additional context.\nhow we built it\narchitecture\nthe klarity backend is comprised into three main sections - scraper, labeller, and api.\nscraper\nhttps://github.com/philipk19238/klarity/tree/master/api/app/scraper\nour scraper uses a two step process to parse websites. first, it seeds our mongo database with tens of thousands of \"scrape-able\" links using a dfs algorithm. this process is designed to be a background job that scrapes new links for our scraper to process and will stop once it detects a link it already scraped. afterwards, we paginate over those results and utlize multiprocessing & caching to quickly scrape, label, and save the data into our database. we originally designed this scraper to be asynchronous but could not implement due to time constraints\nlabeller\nhttps://github.com/philipk19238/klarity/tree/master/api/app/labeller\nour labeller also uses a two step process that cleans & adds additional tags to our dataset. the cleaning process consists of four steps:\ntokenize the input sentences into individual words\nconvert everything to lower case\nfilter punctuation\nfilter out stop words (ex: the, is, how, etc)\nafterwards, it uses a trie to search for unique labels. we initialize our trie with precoded constants with tags that represent the majority of the data that we scrape. we would then iterate the title & the description of the used goods post over the trie to label the data.\napi\nhttps://github.com/philipk19238/klarity/tree/master/api/app/api\nour api consists of two main endpoints:\n/api/export\n/api/query\nexport\nour export endpoint converts the data inside our database into a csv file and streams the bytes to the user for download\nquery\nour query endpoint accepts a parameter of type mean, median, min, and max as a metric to visualize the price trend and accepts the following query parameters:\ntype\ncondition (new, like new, good, etc)\nmaterial\nhome_location (dining, living, -----> patio !!! , etc)\ncolor\nmattress_size (king, queen, full, twin, etc)\nit will then filter our database and group results by date. we planned on adding more visualizations such as item comparisons and location clustering but ran out of time. this information is visualized on our react.js frontend.\ntechnologies used\ndatabase\nmongodb\nbackend api\nflask + swagger\nfrontend framework\nreact.js\nlabeller\nnltk + custom trie\ndevops\ndocker, makefile\nscraping\nrequests + beautifulsoup + multiprocessing\nchallenges we ran into\nwe ran into a lot of difficulties with scraping data. our scraper processed links too quickly and would oftentimes trigger anti-scraper measures. we did our best to adapt by switching user agents constantly and randomizing time between requests. we were able to scrape ~10,000 rows of data but can definitely scrape a lot more if we didn't have such limitations.\naccomplishments that we're proud of\nwe're proud of finishing such a holistic project in a short amount of time. furthermore, our team mainly consisted of business majors with little technical experience so it was a great eye opener to the world of software. finally, we're proud of being able to fight through many roadblocks, hurdles, and weird bugs without getting discouraged and managing to submit a project.\nwhat we learned\nwe learned a ton about web scraping, database technologies, and visualization methods. we also improved our teamwork, collaboration, and commmunication skills.\nwhat's next for klarity\nwe plan on building out more of the visualizations and eventually hosting the website online so everyone can use it. furthermore, if we schedule nightly scraping jobs, we can build up a large collection of used goods data. the north star would be to create a free chrome extension so everyone can compare used good prices for free!", "sortedWord": "None", "removed": "Nan", "score": 22, "comments": 0, "media": null, "medialink": null, "identifyer": 59507475}], "name": "patioDevpost"}
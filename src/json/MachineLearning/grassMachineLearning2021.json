{"interestingcomments": [{"autor": "bigattichouse", "date": 1625158949000, "content": "[D] Some thoughts on enhancing neural networks using vectorspaces /!/ Originally published at [https://bigattichouse.medium.com/shape-of-thought-enhancing-neural-networks-with-vectorspaces-7b0ebb8f5375](https://bigattichouse.medium.com/shape-of-thought-enhancing-neural-networks-with-vectorspaces-7b0ebb8f5375)  \nTerm vectorspaces were created in the 1960s as a way of representing documents in a mathematical way.  This technique could then be used as a way to find the similarity  between two or more documents. As computing power grew, this technique  could be automated to search and organize millions of documents. Term  vectorspaces became the foundation for many modern search engines. These  vectorspaces don\u2019t care what their dimensions are tracking, whether  arbitrarily assigned, words, or properties of an object \u2014 they can be  used to find the similarity between ANY two definitions of objects, as  long as there is a numerical value to the dimensions. The vectors also  have no limit of dimensionality, only space and computing power matter.  Because of this property, we can exploit the nature of how neural  networks are stored to learn more about how the networks change and grow  over time, and how they are related. These neural network vectors  become \u201cpoints\u201d in space, and multiple points in space can become a  shape or trajectory, for which we can test new outcomes even when we  don\u2019t have complete test data. We can also determine if some test data  is relevant to the task, and set it aside to create a new network or for  further scrutiny.\n\nVectorspaces  work by mapping a document to a point in space, these points can then  be compared using a number of techniques, the simplest of which is the  provided by a combination of the Pythagorean theorem sqrt(a\u00b2+b\u00b2=c\u00b2) and  the dot product to give us something called the \u201ccosine similarity\u201d. If  you were standing at 0,0 (the origin) and looking out into a field of  documents, any two documents you see nearly lined up will have a very  high cosine similarity (similarity=1.0). \u201cMy dog has fleas\u201d and \u201cMy cat  has fleas\u201d differ by 25%, a single word. I\u2019m Ignoring other techniques  to make this more efficient to explain the idea. Instead of the normal  X/Y dimensions of a flat graph, we have to use four, one for each unique  word. \u201cMy house has flies\u201d would only match on two words, and be less  similar. But wait! flies and fleas \\*sound\\* similar. Maybe we could use  something like soundex to help with matching! We can. We can use  anything we want, as long as there\u2019s a number associated with a  dimension. Flies Soundex=F420 and Fleas Soundex=F420! So we could just  add another set of dimensions with the soundex result, and find  documents that not only match words, but \\*sound\\* similar. Incidentally,  in these examples using each word as a dimension means word order ends  up begin ignored, so \u201cFleas my dog has\u201d would match \u201cMy dog has fleas\u201d  100%. The ways to mix and match term vectorspace search are well known.  We set some threshold and limit our output to the items that meet that  threshold of similarity.\n\nBut  term vectorspaces are not limited to text documents. In programming, we  frequently run into \u201cobjects\u201d, or instances of grouped properties.  \u201cColor:red, Wheels:4, seats:2\u201d might describe a sports car, while  \u201cColor:brown, Wheels:4, seats:6\u201d might be an SUV. On a dating website,  we might search for common interests with such a vectorspace. On a  shopping website, we might use it to narrow down requirements for  related items. We could use the properties of a book (Young Adult,  Vampires, Author, page count, etc) to recommend other books that the  reader might enjoy. We could also build a profile of a shopper or reader  or customer based on their previous choices and use that mess of  options as a recommendation. Deep down, if we just convert each  dimension to some kind of code, say a hashed value, then it really  doesn\u2019t matter as long as the dimensions are unique. It\u2019s just a list of  hashed names and float values.\n\nBut  normal vectorspace models ignore something important: Choices are  stories, and vectors are points. Mashing all your music selections into  one big mass ignores the possibility you were in a mood for something  different, or perhaps someone else was choosing songs that day \u2014 it  misses the story, the flow of time. We can, however, group vectors  together and treat them like dimensions. Imagine a cow in a field. At  each moment, internal state and the world around them could be  considered vectors. \u201cI\u2019m hungry, I\u2019m lonely, I\u2019m bored\u201d could be  dimensions of the internal state vector, while \u201cHey, grass! or Some  water over there\u201d could be the external state vector. If you package  those vectors up into a group, a \u201cshape\u201d in space, we have a moment in  time (Time = 4pm, internal state = Hungry,Lonely, external state=grass  nearby, water far away). We can then capture multiple moments and form a  story! Using a technique similar to our cosine similarity, we can look  for vector matches and story order, and come up with a way to match the  similarity between two stories. Most humans can easily determine the  similarity between the nursery rhyme *Jack and Jill*  and \u201cJack and Jill went down the street, to fetch a cup of coffee.\u201d With  a simple vectorspace we could determine the general similarity, with  the \u201cstory\u201d (or I call it an \u201carc vector\u201d \u2014 representing a story arc),  we could split the story into clauses and determine which parts might be  different or out of place.\n\nBut  the \u201cstory\u201d the \u201carc vector\u201d really doesn\u2019t have to be related to time.  In a way, any grouping of vectors is a shape. So each moment in the arc  vector is just a shape, and the arc can either be a series of these  shapes, or can be a single shape that changes over time. You can track  the cow\u2019s journey from hunger to satiety \u2014 or you could track how an  organization might grow over time, adding employees and assets, covering  physical space, tracking ebb and flow of finances. The arc tracks  discrete changes in state, in what can be time. Those states can be  individual vectors representing points in space. In simple terms, an arc  moment could be a vector with three points, a triangle. In the next  moment, a square, in the next a star.. etc. By adding a timestamp  vector, we gain a dimension of time to the changes and can watch our  shape grow, or even build an animation.\n\nSome code I wrote almost 15 years ago is available that explores these concepts and is available on github [https://github.com/bigattichouse/vectorspace](https://github.com/bigattichouse/vectorspace)  I\u2019ve noticed some compilation issues with the service since C++ has  changed quite a bit since I wrote it. The project has most of the major  pieces, but you can see the internal workings in vs\\_core.c \u2014 I\u2019ll work  to add my original binaries for the tests, the github project is an  attempt to organize and get the code compiling again. It also tweaks the  cosine similarity using magnitude, a thesaurus for dimensions, as well  as \u201cmeta vectors\u201d \u2014 an early attempt at the arc vector that represents  that shape in space.\n\nThis  is the foundation for where I see vectorspaces being used in the  future, as way to see what neural networks have learned, what they can  learn, pre-suppose test data, and even anticipate trajectories. We can  take a vector, and use each moment in the story to reflect the addition  of something new \u2014 \u201cWhat if we added this test data to this vector, how  does it change\u201d. The story then becomes \u201cWhat If?\u201d instead of \u201cWhat  Was?\u201d\n\nNeural  networks are a method of machine learning that, while useful, is  frankly a bit opaque. Input neurons lead to inner hidden layers of  layers of neurons which lead to output layers, replicating a similar  structure in biological nervous systems. Each neuron has a number of  connections to the next layer, and neurons become activated based on  those connections and activation functions. Some value reaches a  threshold, and activates a neuron down the line. Data is trained with  feedback mechanisms, or pure modified monte carlo simulations that test  the resulting outputs against expected results. Use enough neurons, and  enough training data, and you can recognize a smiling face or sort candy  bars to meet manufacturing standards.\n\nBut  the learning is tedious, and expensive. Test data is loaded with  expected outcomes, feedback or failures continue loading data with  tweaks \u2014 round and round until you get a good result. This can take a  very long time to meet thresholds. There\u2019s been a lot of work lately in  the field, even with \u201cAntogonist\u201d networks that are trained to \u201ctrick\u201d  the primary system into false positives, which then retrain the primary  as an example of negative data. It\u2019s a thriving field, and someone  probably has even proposed what I\u2019ve been thinking about for a while: A  neural network is a vector.\n\nA  simple neural network could be represented by three nodes: A, B, and C.  A is our input layer, B our hidden layer, and C our output layer. We  could probably make a simple NOT logic gate with this configuration.  But, the magic of a neural network isn\u2019t necessarily the values of the  nodes, but the associated values of the connections. AB and BC , and the  associated activation functions of each node are a major part of the  how the network operates. We could then transform this network into a  simple \u201cblank\u201d vector: (A=0,B=0,C=0,AB=0,BC=0) we might even add a  \u201cSigmoidActivation=1\u201d for each neuron to indicate how we want that  neuron to activate, the number 1 could represent some threshold or just  that the chosen function exists.\n\nWhen  we train this network, these values will all change. We will arrive at  some new vector in space. We can then see how far we\u2019ve come. After each  cycle, we can add a new dimension \u201cFitness=0.2\u201d, or even multiple  fitness criteria \u201cFitnessA=0.2, FitnessB=5.0\u201d. We train, we measure, we  see resulting shapes developing. At some point, we will have a number of  vectors pointing toward a Fitness Goal. If we consider these points a  \u201ctrajectory\u201d over time, we can then extrapolate a new point, that  doesn\u2019t have training data, and test that against our dataset. We can  create \u201csick\u201d versions of our network that have failed adversarial  attacks, and use them to prune examples. If a potential network falls in  a \u201csick\u201d zone, ignore it or test it and add it to the sick shapes.\n\nWe  then develop a neural network as the shape of the results of its  training data, figure out early which paths are less likely to be  fruitful when building networks, and provide a sense of \u201cdirection\u201d  toward successful networks \u2014 by treating each network as a point in  space, and part of a process of change instead of as a discrete object.\n\nIn  a way, by taking values to extremes and medians for connection values  in initial dataset loading, we can likely find some outer boundaries to  possible successful shapes in the early stages of loading test data. and  use those bounds to limit tested connection values and speed overall  testing time by limiting to more likely outcomes in the space. Then, we  can go back and do smaller tests to expand the known space outside this  basic shape for better results.\n\nAdditionally,  if we add a data item to our network that drastically places the  network outside of our trajectory, we can set that item aside for  special analysis. I\u2019m hoping this idea might be able to help identify  implicit bias in networks, by calling out items to the developer  \u201cSomething is very different here\u201d, but that\u2019s purely conjecture.", "link": "https://www.reddit.com/r/MachineLearning/comments/obq5tt/d_some_thoughts_on_enhancing_neural_networks/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "grass", "selectorShort": "grass", "MarkedSent": "[d] some thoughts on enhancing neural networks using vectorspaces /!/ originally published at [https://bigattichouse.medium.com/shape-of-thought-enhancing-neural-networks-with-vectorspaces-7b0ebb8f5375](https://bigattichouse.medium.com/shape-of-thought-enhancing-neural-networks-with-vectorspaces-7b0ebb8f5375)  \nterm vectorspaces were created in the 1960s as a way of representing documents in a mathematical way.  this technique could then be used as a way to find the similarity  between two or more documents. as computing power grew, this technique  could be automated to search and organize millions of documents. term  vectorspaces became the foundation for many modern search engines. these  vectorspaces don\u2019t care what their dimensions are tracking, whether  arbitrarily assigned, words, or properties of an object \u2014 they can be  used to find the similarity between any two definitions of objects, as  long as there is a numerical value to the dimensions. the vectors also  have no limit of dimensionality, only space and computing power matter.  because of this property, we can exploit the nature of how neural  networks are stored to learn more about how the networks change and grow  over time, and how they are related. these neural network vectors  become \u201cpoints\u201d in space, and multiple points in space can become a  shape or trajectory, for which we can test new outcomes even when we  don\u2019t have complete test data. we can also determine if some test data  is relevant to the task, and set it aside to create a new network or for  further scrutiny.\n\nvectorspaces  work by mapping a document to a point in space, these points can then  be compared using a number of techniques, the simplest of which is the  provided by a combination of the pythagorean theorem sqrt(a\u00b2+b\u00b2=c\u00b2) and  the dot product to give us something called the \u201ccosine similarity\u201d. if  you were standing at 0,0 (the origin) and looking out into a field of  documents, any two documents you see nearly lined up will have a very  high cosine similarity (similarity=1.0). \u201cmy dog has fleas\u201d and \u201cmy cat  has fleas\u201d differ by 25%, a single word. i\u2019m ignoring other techniques  to make this more efficient to explain the idea. instead of the normal  x/y dimensions of a flat graph, we have to use four, one for each unique  word. \u201cmy house has flies\u201d would only match on two words, and be less  similar. but wait! flies and fleas \\*sound\\* similar. maybe we could use  something like soundex to help with matching! we can. we can use  anything we want, as long as there\u2019s a number associated with a  dimension. flies soundex=f420 and fleas soundex=f420! so we could just  add another set of dimensions with the soundex result, and find  documents that not only match words, but \\*sound\\* similar. incidentally,  in these examples using each word as a dimension means word order ends  up begin ignored, so \u201cfleas my dog has\u201d would match \u201cmy dog has fleas\u201d  100%. the ways to mix and match term vectorspace search are well known.  we set some threshold and limit our output to the items that meet that  threshold of similarity.\n\nbut  term vectorspaces are not limited to text documents. in programming, we  frequently run into \u201cobjects\u201d, or instances of grouped properties.  \u201ccolor:red, wheels:4, seats:2\u201d might describe a sports car, while  \u201ccolor:brown, wheels:4, seats:6\u201d might be an suv. on a dating website,  we might search for common interests with such a vectorspace. on a  shopping website, we might use it to narrow down requirements for  related items. we could use the properties of a book (young adult,  vampires, author, page count, etc) to recommend other books that the  reader might enjoy. we could also build a profile of a shopper or reader  or customer based on their previous choices and use that mess of  options as a recommendation. deep down, if we just convert each  dimension to some kind of code, say a hashed value, then it really  doesn\u2019t matter as long as the dimensions are unique. it\u2019s just a list of  hashed names and float values.\n\nbut  normal vectorspace models ignore something important: choices are  stories, and vectors are points. mashing all your music selections into  one big mass ignores the possibility you were in a mood for something  different, or perhaps someone else was choosing songs that day \u2014 it  misses the story, the flow of time. we can, however, group vectors  together and treat them like dimensions. imagine a cow in a field. at  each moment, internal state and the world around them could be  considered vectors. \u201ci\u2019m hungry, i\u2019m lonely, i\u2019m bored\u201d could be  dimensions of the internal state vector, while \u201chey, -----> grass !!! ! or some  water over there\u201d could be the external state vector. if you package  those vectors up into a group, a \u201cshape\u201d in space, we have a moment in  time (time = 4pm, internal state = hungry,lonely, external state=grass  nearby, water far away). we can then capture multiple moments and form a  story! using a technique similar to our cosine similarity, we can look  for vector matches and story order, and come up with a way to match the  similarity between two stories. most humans can easily determine the  similarity between the nursery rhyme *jack and jill*  and \u201cjack and jill went down the street, to fetch a cup of coffee.\u201d with  a simple vectorspace we could determine the general similarity, with  the \u201cstory\u201d (or i call it an \u201carc vector\u201d \u2014 representing a story arc),  we could split the story into clauses and determine which parts might be  different or out of place.\n\nbut  the \u201cstory\u201d the \u201carc vector\u201d really doesn\u2019t have to be related to time.  in a way, any grouping of vectors is a shape. so each moment in the arc  vector is just a shape, and the arc can either be a series of these  shapes, or can be a single shape that changes over time. you can track  the cow\u2019s journey from hunger to satiety \u2014 or you could track how an  organization might grow over time, adding employees and assets, covering  physical space, tracking ebb and flow of finances. the arc tracks  discrete changes in state, in what can be time. those states can be  individual vectors representing points in space. in simple terms, an arc  moment could be a vector with three points, a triangle. in the next  moment, a square, in the next a star.. etc. by adding a timestamp  vector, we gain a dimension of time to the changes and can watch our  shape grow, or even build an animation.\n\nsome code i wrote almost 15 years ago is available that explores these concepts and is available on github [https://github.com/bigattichouse/vectorspace](https://github.com/bigattichouse/vectorspace)  i\u2019ve noticed some compilation issues with the service since c++ has  changed quite a bit since i wrote it. the project has most of the major  pieces, but you can see the internal workings in vs\\_core.c \u2014 i\u2019ll work  to add my original binaries for the tests, the github project is an  attempt to organize and get the code compiling again. it also tweaks the  cosine similarity using magnitude, a thesaurus for dimensions, as well  as \u201cmeta vectors\u201d \u2014 an early attempt at the arc vector that represents  that shape in space.\n\nthis  is the foundation for where i see vectorspaces being used in the  future, as way to see what neural networks have learned, what they can  learn, pre-suppose test data, and even anticipate trajectories. we can  take a vector, and use each moment in the story to reflect the addition  of something new \u2014 \u201cwhat if we added this test data to this vector, how  does it change\u201d. the story then becomes \u201cwhat if?\u201d instead of \u201cwhat  was?\u201d\n\nneural  networks are a method of machine learning that, while useful, is  frankly a bit opaque. input neurons lead to inner hidden layers of  layers of neurons which lead to output layers, replicating a similar  structure in biological nervous systems. each neuron has a number of  connections to the next layer, and neurons become activated based on  those connections and activation functions. some value reaches a  threshold, and activates a neuron down the line. data is trained with  feedback mechanisms, or pure modified monte carlo simulations that test  the resulting outputs against expected results. use enough neurons, and  enough training data, and you can recognize a smiling face or sort candy  bars to meet manufacturing standards.\n\nbut  the learning is tedious, and expensive. test data is loaded with  expected outcomes, feedback or failures continue loading data with  tweaks \u2014 round and round until you get a good result. this can take a  very long time to meet thresholds. there\u2019s been a lot of work lately in  the field, even with \u201cantogonist\u201d networks that are trained to \u201ctrick\u201d  the primary system into false positives, which then retrain the primary  as an example of negative data. it\u2019s a thriving field, and someone  probably has even proposed what i\u2019ve been thinking about for a while: a  neural network is a vector.\n\na  simple neural network could be represented by three nodes: a, b, and c.  a is our input layer, b our hidden layer, and c our output layer. we  could probably make a simple not logic gate with this configuration.  but, the magic of a neural network isn\u2019t necessarily the values of the  nodes, but the associated values of the connections. ab and bc , and the  associated activation functions of each node are a major part of the  how the network operates. we could then transform this network into a  simple \u201cblank\u201d vector: (a=0,b=0,c=0,ab=0,bc=0) we might even add a  \u201csigmoidactivation=1\u201d for each neuron to indicate how we want that  neuron to activate, the number 1 could represent some threshold or just  that the chosen function exists.\n\nwhen  we train this network, these values will all change. we will arrive at  some new vector in space. we can then see how far we\u2019ve come. after each  cycle, we can add a new dimension \u201cfitness=0.2\u201d, or even multiple  fitness criteria \u201cfitnessa=0.2, fitnessb=5.0\u201d. we train, we measure, we  see resulting shapes developing. at some point, we will have a number of  vectors pointing toward a fitness goal. if we consider these points a  \u201ctrajectory\u201d over time, we can then extrapolate a new point, that  doesn\u2019t have training data, and test that against our dataset. we can  create \u201csick\u201d versions of our network that have failed adversarial  attacks, and use them to prune examples. if a potential network falls in  a \u201csick\u201d zone, ignore it or test it and add it to the sick shapes.\n\nwe  then develop a neural network as the shape of the results of its  training data, figure out early which paths are less likely to be  fruitful when building networks, and provide a sense of \u201cdirection\u201d  toward successful networks \u2014 by treating each network as a point in  space, and part of a process of change instead of as a discrete object.\n\nin  a way, by taking values to extremes and medians for connection values  in initial dataset loading, we can likely find some outer boundaries to  possible successful shapes in the early stages of loading test data. and  use those bounds to limit tested connection values and speed overall  testing time by limiting to more likely outcomes in the space. then, we  can go back and do smaller tests to expand the known space outside this  basic shape for better results.\n\nadditionally,  if we add a data item to our network that drastically places the  network outside of our trajectory, we can set that item aside for  special analysis. i\u2019m hoping this idea might be able to help identify  implicit bias in networks, by calling out items to the developer  \u201csomething is very different here\u201d, but that\u2019s purely conjecture.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/obq5tt/d_some_thoughts_on_enhancing_neural_networks/',)", "identifyer": 5721972, "year": "2021"}], "name": "grassMachineLearning2021"}
{"interestingcomments": [{"autor": "arbueticos", "date": 1614707211000, "content": "[D] Your favourite tool for data extraction from web? /!/ This is an idea gathering on what is your favorite tool to extract data from the web in a general-purpose fashion.  \nWould be great to have name, how it works and why you think what you use is the best! :)", "link": "https://www.reddit.com/r/MachineLearning/comments/lw7ze2/d_your_favourite_tool_for_data_extraction_from_web/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] your favourite -----> tool !!!  for data extraction from web? /!/ this is an idea gathering on what is your favorite tool to extract data from the web in a general-purpose fashion.  \nwould be great to have name, how it works and why you think what you use is the best! :)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/lw7ze2/d_your_favourite_tool_for_data_extraction_from_web/',)", "identifyer": 5721427, "year": "2021"}, {"autor": "arbueticos", "date": 1614707099000, "content": "[Brainstorming] Your favourite tool for data extraction from web? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/lw7xxz/brainstorming_your_favourite_tool_for_data/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[brainstorming] your favourite -----> tool !!!  for data extraction from web? /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/lw7xxz/brainstorming_your_favourite_tool_for_data/',)", "identifyer": 5721428, "year": "2021"}, {"autor": "ploomber-io", "date": 1617292408000, "content": "[D] Incremental builds for ML pipelines /!/ Hi everyone, I'd like to get some perspective on incremental builds. When developing an ML pipeline, I often have to revisit a processing step  (e.g., update SQL or Python script). Since the output is now outdated, I  have to rerun the pipeline, but I can skip unaffected tasks to save some time; as the pipeline grows, this has a significant impact. The most common example is Make. Incremental builds are a must for me since it allows me to modify something and bring everything up-to-date quickly.\n\nA few orchestrators have this feature: [Ploomber](https://github.com/ploomber/ploomber)  (which I'm developing), DVC, drake (in R), but others don't: dagster,  kedro, prefect. Surprisingly, users from the latter group do not seem to miss (or maybe be aware of) that feature.\n\nMy only guess is that people who mostly work in Deep Learning don't see much of a benefit of a make-like tool because there are fewer pre-processing steps.\n\nAm I missing anything? How do you ensure all tasks are using the most recent data?  And more importantly, how do you quickly get all outputs up-to-date?", "link": "https://www.reddit.com/r/MachineLearning/comments/mhy97j/d_incremental_builds_for_ml_pipelines/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] incremental builds for ml pipelines /!/ hi everyone, i'd like to get some perspective on incremental builds. when developing an ml pipeline, i often have to revisit a processing step  (e.g., update sql or python script). since the output is now outdated, i  have to rerun the pipeline, but i can skip unaffected tasks to save some time; as the pipeline grows, this has a significant impact. the most common example is make. incremental builds are a must for me since it allows me to modify something and bring everything up-to-date quickly.\n\na few orchestrators have this feature: [ploomber](https://github.com/ploomber/ploomber)  (which i'm developing), dvc, drake (in r), but others don't: dagster,  kedro, prefect. surprisingly, users from the latter group do not seem to miss (or maybe be aware of) that feature.\n\nmy only guess is that people who mostly work in deep learning don't see much of a benefit of a make-like -----> tool !!!  because there are fewer pre-processing steps.\n\nam i missing anything? how do you ensure all tasks are using the most recent data?  and more importantly, how do you quickly get all outputs up-to-date?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 8, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/mhy97j/d_incremental_builds_for_ml_pipelines/',)", "identifyer": 5721571, "year": "2021"}, {"autor": "sci-genie", "date": 1612045026000, "content": "[P] Sci-Genie: Search Engine on CS ArXiv /!/ TLDR; *Check out the tool I built:* [*https://sci-genie.com/*](https://sci-genie.com/)*. It's a search engine over CS ArXiv for quickly finding new papers on ArXiv.* \n\nHello Redditors,\n\nThe past year with the pandemic has been grueling. I was fortunate enough to be in grad school and it gave me a lot of time to analyze and study new AI/ML and deep learning research. To make my life easier while navigating through the landscape I created a tool that indexed ArXiv papers and their content into a search engine.\n\nWhy Arxiv? Well, too many papers that I found really cool came up on ArXiv before getting accepted to something Like NeuRIPs. One of my favorite examples is [Gradient Surgery](https://arxiv.org/abs/2001.06782). Which was in NeuRIPs 2020 while it was first published on ArXiv in January 2020.  \n\nI built this tool because ArXiv is flooding with new research for the past few years and tools like [arxiv-sanity](http://www.arxiv-sanity.com/) is a very good place to start when finding what's trending but not that useful when doing research that intersects ML and AI in outside domains of Language and Vision. So using the baseline built by Karapath, I started building a tool of my own during the pandemic to quickly help me find new research. Its build using Python, React, FastAPI, Elasticsearch and uses this [model](https://github.com/angelosalatino/cso-classifier) for keyword annotations. Big shout to [https://cso.kmi.open.ac.uk/](https://cso.kmi.open.ac.uk/) for making such a good model and constantly working on it and updating it. \n\nThe tool is still in its infancy and this is V0.1 of the tool. This is still subject to changes as time/interest progresses. I haven't even added features like sorting by citations/page-rank and conference publications etc as this is the start version but I am working towards adding those too.  \n\nI use this tool myself quite frequently when looking for research on some topics or doing literature reviews for school-related research work. I would appreciate any feedback on the tool and suggestions on any improvement that would be useful to you. \n\nA cute fun fact I found via this tool: CS researchers seem to love using \"All you need\" in the title of a paper. Nearly every couple of months there is a paper with an \"All you need\" title since its first introduction :P\n\n*Processing img i0l3pntmhje61...*\n\nHope you find it useful !.\n\nPS. \n\nWould also love any help on improving this. DM me if interested.", "link": "https://www.reddit.com/r/MachineLearning/comments/l8x8by/p_scigenie_search_engine_on_cs_arxiv/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] sci-genie: search engine on cs arxiv /!/ tldr; *check out the -----> tool !!!  i built:* [*https://sci-genie.com/*](https://sci-genie.com/)*. it's a search engine over cs arxiv for quickly finding new papers on arxiv.* \n\nhello redditors,\n\nthe past year with the pandemic has been grueling. i was fortunate enough to be in grad school and it gave me a lot of time to analyze and study new ai/ml and deep learning research. to make my life easier while navigating through the landscape i created a tool that indexed arxiv papers and their content into a search engine.\n\nwhy arxiv? well, too many papers that i found really cool came up on arxiv before getting accepted to something like neurips. one of my favorite examples is [gradient surgery](https://arxiv.org/abs/2001.06782). which was in neurips 2020 while it was first published on arxiv in january 2020.  \n\ni built this tool because arxiv is flooding with new research for the past few years and tools like [arxiv-sanity](http://www.arxiv-sanity.com/) is a very good place to start when finding what's trending but not that useful when doing research that intersects ml and ai in outside domains of language and vision. so using the baseline built by karapath, i started building a tool of my own during the pandemic to quickly help me find new research. its build using python, react, fastapi, elasticsearch and uses this [model](https://github.com/angelosalatino/cso-classifier) for keyword annotations. big shout to [https://cso.kmi.open.ac.uk/](https://cso.kmi.open.ac.uk/) for making such a good model and constantly working on it and updating it. \n\nthe tool is still in its infancy and this is v0.1 of the tool. this is still subject to changes as time/interest progresses. i haven't even added features like sorting by citations/page-rank and conference publications etc as this is the start version but i am working towards adding those too.  \n\ni use this tool myself quite frequently when looking for research on some topics or doing literature reviews for school-related research work. i would appreciate any feedback on the tool and suggestions on any improvement that would be useful to you. \n\na cute fun fact i found via this tool: cs researchers seem to love using \"all you need\" in the title of a paper. nearly every couple of months there is a paper with an \"all you need\" title since its first introduction :p\n\n*processing img i0l3pntmhje61...*\n\nhope you find it useful !.\n\nps. \n\nwould also love any help on improving this. dm me if interested.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 8, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/l8x8by/p_scigenie_search_engine_on_cs_arxiv/',)", "identifyer": 5721652, "year": "2021"}, {"autor": "danapo", "date": 1619946586000, "content": "[N] Free Online Event \"Your Ultimate Data Annotation Experience for Deep Learning\" /!/ Dear Machine Learning Enthusiasts,\n\nSoon there is an online event at Linkedin titled\n\n\u201c[Your Ultimate Data Annotation Experience for Deep Learning](https://www.linkedin.com/events/yourultimatedataannotationexper6786631495491682304/about/)\u201d with this description:\n\nAre You Ready For Speeding Up Your Deep Learning Process?\n\n\\-It all starts with danaXa's CEO Dana Pordel\u2019s keynote, where he'll unveil the future of annotation and so much more.\n\n\\-Stephan Gould, an Amazon Scholar and professor at Australian National University, will present \"Ikea Assembly and Other Experiences in Dataset Labelling\". Professor Gould has PhD from Stanford University and worked extensively with different research projects.\n\n\\-This is our first of the series of events where we discuss the progress in data annotation. It promises to be full of exciting practical announcements\u2014and some Q/A with Experts in the field\u2014so don\u2019t miss out.\n\n\\-This is a free event that will be limited to only 100 members. Hopefully, in future, we can host thousands of members.\n\nThis event is an excellent opportunity to see what the machine learning community needs as a tool for data annotation and the future of image/video annotation. I suggest you sign up and, within the Q/A time, ask about the challenges you face, especially if you are dealing with massive data for deep learning.\n\nHere is the [event link](https://www.linkedin.com/events/yourultimatedataannotationexper6786631495491682304/about/).", "link": "https://www.reddit.com/r/MachineLearning/comments/n32nfu/n_free_online_event_your_ultimate_data_annotation/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[n] free online event \"your ultimate data annotation experience for deep learning\" /!/ dear machine learning enthusiasts,\n\nsoon there is an online event at linkedin titled\n\n\u201c[your ultimate data annotation experience for deep learning](https://www.linkedin.com/events/yourultimatedataannotationexper6786631495491682304/about/)\u201d with this description:\n\nare you ready for speeding up your deep learning process?\n\n\\-it all starts with danaxa's ceo dana pordel\u2019s keynote, where he'll unveil the future of annotation and so much more.\n\n\\-stephan gould, an amazon scholar and professor at australian national university, will present \"ikea assembly and other experiences in dataset labelling\". professor gould has phd from stanford university and worked extensively with different research projects.\n\n\\-this is our first of the series of events where we discuss the progress in data annotation. it promises to be full of exciting practical announcements\u2014and some q/a with experts in the field\u2014so don\u2019t miss out.\n\n\\-this is a free event that will be limited to only 100 members. hopefully, in future, we can host thousands of members.\n\nthis event is an excellent opportunity to see what the machine learning community needs as a -----> tool !!!  for data annotation and the future of image/video annotation. i suggest you sign up and, within the q/a time, ask about the challenges you face, especially if you are dealing with massive data for deep learning.\n\nhere is the [event link](https://www.linkedin.com/events/yourultimatedataannotationexper6786631495491682304/about/).", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/n32nfu/n_free_online_event_your_ultimate_data_annotation/',)", "identifyer": 5721736, "year": "2021"}, {"autor": "jayalammar", "date": 1622545207000, "content": "[D] Probing Classifiers: A Gentle Intro (Explainable AI for Deep Learning) [Video] /!/ &amp;#x200B;\n\n[https://www.youtube.com/watch?v=HJn-OTNLnoE](https://www.youtube.com/watch?v=HJn-OTNLnoE)\n\n&amp;#x200B;\n\nProbing Classifiers are an Explainable AI tool used to make sense of the representations that deep neural networks learn for their inputs. They allow us to understand if the numeric representation at the end (or in the middle) of the model encodes certain properties of the input which we are interested in (for example, whether an input token is a verb or a noun). Using probes, machine learning researchers gained a better understanding of the difference between models and between the various layers of a single model.\n\n&amp;#x200B;\n\n**Outline:**  \nIntroductions (0:00)   \nMotivation for probes in Machine Translation (0:40)   \nProbing sentence encoders (3:42)   \nHow a probe is trained (5:32)   \nProbing token representations (8:08)   \nSize of probes (9:15)   \nBetter metrics using Control Tasks (9:48)   \nConclusion (10:32)\n\n&amp;#x200B;\n\n**Papers:**  \nWhat you can cram into a single $&amp;!#\\* vector: Probing sentence embeddings for linguistic properties  \n[https://www.aclweb.org/anthology/P18-1198/](https://www.aclweb.org/anthology/P18-1198/)\n\nDiagnostic classifiers: revealing how neural networks process hierarchical structure   \n[http://ceur-ws.org/Vol-1773/CoCoNIPS\\_2016\\_paper6.pdf](http://ceur-ws.org/Vol-1773/CoCoNIPS_2016_paper6.pdf)\n\nDesigning and Interpreting Probes with Control Tasks   \n[https://www.aclweb.org/anthology/D19-1275/](https://www.aclweb.org/anthology/D19-1275/)  \n\nFine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks [https://arxiv.org/abs/1608.04207](https://arxiv.org/abs/1608.04207)  \n\nWhat do you learn from context? Probing for sentence structure in contextualized word representations   \n[https://arxiv.org/abs/1905.06316](https://arxiv.org/abs/1905.06316)", "link": "https://www.reddit.com/r/MachineLearning/comments/npraw7/d_probing_classifiers_a_gentle_intro_explainable/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] probing classifiers: a gentle intro (explainable ai for deep learning) [video] /!/ &amp;#x200b;\n\n[https://www.youtube.com/watch?v=hjn-otnlnoe](https://www.youtube.com/watch?v=hjn-otnlnoe)\n\n&amp;#x200b;\n\nprobing classifiers are an explainable ai -----> tool !!!  used to make sense of the representations that deep neural networks learn for their inputs. they allow us to understand if the numeric representation at the end (or in the middle) of the model encodes certain properties of the input which we are interested in (for example, whether an input token is a verb or a noun). using probes, machine learning researchers gained a better understanding of the difference between models and between the various layers of a single model.\n\n&amp;#x200b;\n\n**outline:**  \nintroductions (0:00)   \nmotivation for probes in machine translation (0:40)   \nprobing sentence encoders (3:42)   \nhow a probe is trained (5:32)   \nprobing token representations (8:08)   \nsize of probes (9:15)   \nbetter metrics using control tasks (9:48)   \nconclusion (10:32)\n\n&amp;#x200b;\n\n**papers:**  \nwhat you can cram into a single $&amp;!#\\* vector: probing sentence embeddings for linguistic properties  \n[https://www.aclweb.org/anthology/p18-1198/](https://www.aclweb.org/anthology/p18-1198/)\n\ndiagnostic classifiers: revealing how neural networks process hierarchical structure   \n[http://ceur-ws.org/vol-1773/coconips\\_2016\\_paper6.pdf](http://ceur-ws.org/vol-1773/coconips_2016_paper6.pdf)\n\ndesigning and interpreting probes with control tasks   \n[https://www.aclweb.org/anthology/d19-1275/](https://www.aclweb.org/anthology/d19-1275/)  \n\nfine-grained analysis of sentence embeddings using auxiliary prediction tasks [https://arxiv.org/abs/1608.04207](https://arxiv.org/abs/1608.04207)  \n\nwhat do you learn from context? probing for sentence structure in contextualized word representations   \n[https://arxiv.org/abs/1905.06316](https://arxiv.org/abs/1905.06316)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/npraw7/d_probing_classifiers_a_gentle_intro_explainable/',)", "identifyer": 5721891, "year": "2021"}, {"autor": "HenHanna", "date": 1625168373000, "content": "[D] Which of Two poets A, B wrote this Mystery-Poem? ----- Is there a Web site for Neural-Nets, or Deep-Learning, etc ? (or a Python TOOL or package?) /!/ - There are two poets A, B and enough samples from each.\n\n- There is this Mystery-Poem (3K characters), and I want to find out which of A or B wrote the poem.\n\n________________\n________________\n\n-    Is there a Web site that can give me an answer using Neural-Nets, or Deep-Learning, etc ?\n\n- -------- (or a Python (Tool or) package?)\n\n________________\n________________\n\nThis seems such a common problem (template) that it seems that there should be a Ready-Made Website for it.\n\n---------- i have done Web-searches but was unable to find answers to my questions.", "link": "https://www.reddit.com/r/MachineLearning/comments/obtg9g/d_which_of_two_poets_a_b_wrote_this_mysterypoem/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] which of two poets a, b wrote this mystery-poem? ----- is there a web site for neural-nets, or deep-learning, etc ? (or a python -----> tool !!!  or package?) /!/ - there are two poets a, b and enough samples from each.\n\n- there is this mystery-poem (3k characters), and i want to find out which of a or b wrote the poem.\n\n________________\n________________\n\n-    is there a web site that can give me an answer using neural-nets, or deep-learning, etc ?\n\n- -------- (or a python (tool or) package?)\n\n________________\n________________\n\nthis seems such a common problem (template) that it seems that there should be a ready-made website for it.\n\n---------- i have done web-searches but was unable to find answers to my questions.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/obtg9g/d_which_of_two_poets_a_b_wrote_this_mysterypoem/',)", "identifyer": 5721963, "year": "2021"}, {"autor": "meldiwin", "date": 1625138262000, "content": "[R] What Are The Tools I can use for designing this figure /!/ &amp;#x200B;\n\nHello,\n\nI am still new to the field and would like to know what kind of tool I can use for creating figure like that?\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nhttps://preview.redd.it/s5klx8qv4l871.png?width=982&amp;format=png&amp;auto=webp&amp;s=724588b4dda3737db647261cd418d01047d9c588", "link": "https://www.reddit.com/r/MachineLearning/comments/objnpv/r_what_are_the_tools_i_can_use_for_designing_this/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[r] what are the tools i can use for designing this figure /!/ &amp;#x200b;\n\nhello,\n\ni am still new to the field and would like to know what kind of -----> tool !!!  i can use for creating figure like that?\n\n&amp;#x200b;\n\n&amp;#x200b;\n\nhttps://preview.redd.it/s5klx8qv4l871.png?width=982&amp;format=png&amp;auto=webp&amp;s=724588b4dda3737db647261cd418d01047d9c588", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/objnpv/r_what_are_the_tools_i_can_use_for_designing_this/',)", "identifyer": 5721998, "year": "2021"}, {"autor": "Ok-Craft-9908", "date": 1625133684000, "content": "[Project] The metaproject: Using ML to learn ML at https://www.learney.me/ /!/ [Prototype knowledge graph for ML topics](https://preview.redd.it/h39mq14wqk871.png?width=1495&amp;format=png&amp;auto=webp&amp;s=993d52dd6ad910d39f84996982f8fe6ee2fa57d0)\n\nMotivation for proejct: I got frustrated going through online courses and the like and covering irrelevant content / stuff way above my head or that I already knew. Thought I could do something a bit meta: use ML to learn ML better. \n\nThe aim is to build a tool that estimates your knowledge level to tailor the best questions and content to you. Think this would be a fun application of ML, thinking of using transformers or an RNN for this but still playing around! (something like this: [https://stanford.edu/\\~cpiech/bio/papers/deepKnowledgeTracing.pdf](https://stanford.edu/~cpiech/bio/papers/deepKnowledgeTracing.pdf))\n\nThink we need to build a graph of ML concepts and dependencies first - built a prototype here: [https://app.learney.me/](https://app.learney.me/). Can also sign up here to stay in touch with project: [https://www.learney.me/](https://www.learney.me/)\n\nAlso need a decent question bank - have started to build this but could do with help from community!", "link": "https://www.reddit.com/r/MachineLearning/comments/obiksd/project_the_metaproject_using_ml_to_learn_ml_at/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[project] the metaproject: using ml to learn ml at https://www.learney.me/ /!/ [prototype knowledge graph for ml topics](https://preview.redd.it/h39mq14wqk871.png?width=1495&amp;format=png&amp;auto=webp&amp;s=993d52dd6ad910d39f84996982f8fe6ee2fa57d0)\n\nmotivation for proejct: i got frustrated going through online courses and the like and covering irrelevant content / stuff way above my head or that i already knew. thought i could do something a bit meta: use ml to learn ml better. \n\nthe aim is to build a -----> tool !!!  that estimates your knowledge level to tailor the best questions and content to you. think this would be a fun application of ml, thinking of using transformers or an rnn for this but still playing around! (something like this: [https://stanford.edu/\\~cpiech/bio/papers/deepknowledgetracing.pdf](https://stanford.edu/~cpiech/bio/papers/deepknowledgetracing.pdf))\n\nthink we need to build a graph of ml concepts and dependencies first - built a prototype here: [https://app.learney.me/](https://app.learney.me/). can also sign up here to stay in touch with project: [https://www.learney.me/](https://www.learney.me/)\n\nalso need a decent question bank - have started to build this but could do with help from community!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/obiksd/project_the_metaproject_using_ml_to_learn_ml_at/',)", "identifyer": 5722001, "year": "2021"}, {"autor": "dataqa_ai", "date": 1633084770000, "content": "[P] Using rules to speed up labelling by 2x /!/ I have developed a tool to label documents using rules, and ran some experiments on a multi-class problem where I have 3000 Amazon product descriptions that belong to 24 product categories. I show how using a combination of rules and random labelling is much less time consuming and achieves better results than just performing random labelling.\n\nhttps://preview.redd.it/l9gdt47qhtq71.png?width=864&amp;format=png&amp;auto=webp&amp;s=4addc2254cb1785b969d6c3e340448a6b6df32f9\n\nFor more info: [https://dataqa.ai/case-study/](https://dataqa.ai/case-study/)\n\nThe tool I developed and used for this problem: [https://github.com/dataqa/dataqa](https://github.com/dataqa/dataqa)", "link": "https://www.reddit.com/r/MachineLearning/comments/pz5c0r/p_using_rules_to_speed_up_labelling_by_2x/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] using rules to speed up labelling by 2x /!/ i have developed a -----> tool !!!  to label documents using rules, and ran some experiments on a multi-class problem where i have 3000 amazon product descriptions that belong to 24 product categories. i show how using a combination of rules and random labelling is much less time consuming and achieves better results than just performing random labelling.\n\nhttps://preview.redd.it/l9gdt47qhtq71.png?width=864&amp;format=png&amp;auto=webp&amp;s=4addc2254cb1785b969d6c3e340448a6b6df32f9\n\nfor more info: [https://dataqa.ai/case-study/](https://dataqa.ai/case-study/)\n\nthe tool i developed and used for this problem: [https://github.com/dataqa/dataqa](https://github.com/dataqa/dataqa)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/pz5c0r/p_using_rules_to_speed_up_labelling_by_2x/',)", "identifyer": 5722238, "year": "2021"}, {"autor": "elcric_krej", "date": 1633081718000, "content": "[D] Small, neglected, complex datasets for benchmarking automatic machine learning /!/ I've been working on a benchmark suite for testing automatic ML (basically libraries where you only have to input the target id and the data in a structured). Think autosklearn, auto gluoan, auto h2o, pycaret, etc.\n\nI'm doing this for selfish reasons (benchmarking of an automatic ml tool I'm working on). However, I'm open-sourcing every single dataset for which I can find an open license and the runtime environment. The RE is pretty dumb, it just does on-machine parallelism based on the nr of GPUs, but in principle, I should be able to implement multi-machine parallelism soon.\n\nAt any rate, the issue I have with other such benchmarks (e.g. openML) is that they are too myopic in terms of problem-space. They boil down to classification, or at most classification and regression, based on categorical and numerical features.\n\nThere is indeed a problem with a large benchmark suite, in that it can't run very large problems due to compute limitations (e.g. image net, translation tasks with an associated learning corpus, protein folding, DICOM classification, etc). However, I don't feel like that should limit it to only the most basics of tasks that can fit in a standard CSV.\n\nIn my current implementation I've added:\n- Datasets with text inputs (mixed with date, numeric and categorical inputs)\n- Datasets with image inputs (mixed with categorical labels, think cifrar100 but feed the superclass + image and have it predict the class)\n- Timeseries data (where there's an ordering of and relation between all rows of the dataset)\n\nOn the other hand, this seems to just be scratching the surface of what auto ml should be doing and I'm curious to find more datasets that represent complex problems *but* are not so large as to be computationally unviable to run with a startup budget (or with an individual researcher budget, since that's my goal for this suite).\n\nHence why I'd like to focus on \"neglected\" areas first, since I assume they might actually harbour a lot of datasets for which auto ml techniques could provide real help. As opposed to, say, image classification, which is a nice point of comparison, but realistically you're competing against teams of dozens, building models that run across million-dollar clusters, so you're never touching the edge.\n\nSome ideas that come to mind, but for which I haven't found datasets:\n- Equation solving (text to text or text to number)\n- Images augmented with text (e.g. x-ray + doctor diagnosis)\n- Predicting numbers/classes from niche \"imaging\" formats where the typical network might not work well (e.g. electron diffraction patterns, or something else where the proximity of pixels doesn't \"mean\" what it means in macroscopic photography)\n- Any sort of mixed numeric/categoric and multimedia format (e.g. song recognition identification *but* with the genre or name of the band as an input)\n- Self-referential / graph datasets (e.g. text with a lot of hyperlinks to other parts of the page)\n- Non-text, non-multimedia sequences of arbitrary size and high-variance magnitude as either inputs or output\n- \"Typical\" classification or regression problems where \"traditional\" methods (e.g. boosting, random forests, regressions, plain TDs, clustering, SVMs or even normal MLPs) work poorly for some poorly understood reasons, not being able to outperform humans or not being able to outperform hand-crafted equations\n- Equation optimization aka parameter finding/tweaking (in cases where just running SGD on the equation themselves is impossible or practically impossible)\n\nBut again, this is me spit-balling and I haven't really found good examples of such datasets.\n\nAlso, I might be completely missing the mark as to what's \"interesting\" for generic supervised machine learning to tackle, what classes of problems this would work best for. So I'm curious what other people would think are the interesting niches I should focus my benchmarking on.\n\n\nNote for the pednatic: I get that \"computational budget\" varies 1000x based on the method being used and when you call quits (i.e. assume the model has converged to the best solution it's likely to obtain best on some validation data), but still, input size and a number of rows matter (although, again, I get that even those can be relative due to how you prepare/pre-encode/encode the data and because you can do sampling). But I think the framing still \"makes sense\" even if it has this inherent nebulosity.", "link": "https://www.reddit.com/r/MachineLearning/comments/pz4ny2/d_small_neglected_complex_datasets_for/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] small, neglected, complex datasets for benchmarking automatic machine learning /!/ i've been working on a benchmark suite for testing automatic ml (basically libraries where you only have to input the target id and the data in a structured). think autosklearn, auto gluoan, auto h2o, pycaret, etc.\n\ni'm doing this for selfish reasons (benchmarking of an automatic ml -----> tool !!!  i'm working on). however, i'm open-sourcing every single dataset for which i can find an open license and the runtime environment. the re is pretty dumb, it just does on-machine parallelism based on the nr of gpus, but in principle, i should be able to implement multi-machine parallelism soon.\n\nat any rate, the issue i have with other such benchmarks (e.g. openml) is that they are too myopic in terms of problem-space. they boil down to classification, or at most classification and regression, based on categorical and numerical features.\n\nthere is indeed a problem with a large benchmark suite, in that it can't run very large problems due to compute limitations (e.g. image net, translation tasks with an associated learning corpus, protein folding, dicom classification, etc). however, i don't feel like that should limit it to only the most basics of tasks that can fit in a standard csv.\n\nin my current implementation i've added:\n- datasets with text inputs (mixed with date, numeric and categorical inputs)\n- datasets with image inputs (mixed with categorical labels, think cifrar100 but feed the superclass + image and have it predict the class)\n- timeseries data (where there's an ordering of and relation between all rows of the dataset)\n\non the other hand, this seems to just be scratching the surface of what auto ml should be doing and i'm curious to find more datasets that represent complex problems *but* are not so large as to be computationally unviable to run with a startup budget (or with an individual researcher budget, since that's my goal for this suite).\n\nhence why i'd like to focus on \"neglected\" areas first, since i assume they might actually harbour a lot of datasets for which auto ml techniques could provide real help. as opposed to, say, image classification, which is a nice point of comparison, but realistically you're competing against teams of dozens, building models that run across million-dollar clusters, so you're never touching the edge.\n\nsome ideas that come to mind, but for which i haven't found datasets:\n- equation solving (text to text or text to number)\n- images augmented with text (e.g. x-ray + doctor diagnosis)\n- predicting numbers/classes from niche \"imaging\" formats where the typical network might not work well (e.g. electron diffraction patterns, or something else where the proximity of pixels doesn't \"mean\" what it means in macroscopic photography)\n- any sort of mixed numeric/categoric and multimedia format (e.g. song recognition identification *but* with the genre or name of the band as an input)\n- self-referential / graph datasets (e.g. text with a lot of hyperlinks to other parts of the page)\n- non-text, non-multimedia sequences of arbitrary size and high-variance magnitude as either inputs or output\n- \"typical\" classification or regression problems where \"traditional\" methods (e.g. boosting, random forests, regressions, plain tds, clustering, svms or even normal mlps) work poorly for some poorly understood reasons, not being able to outperform humans or not being able to outperform hand-crafted equations\n- equation optimization aka parameter finding/tweaking (in cases where just running sgd on the equation themselves is impossible or practically impossible)\n\nbut again, this is me spit-balling and i haven't really found good examples of such datasets.\n\nalso, i might be completely missing the mark as to what's \"interesting\" for generic supervised machine learning to tackle, what classes of problems this would work best for. so i'm curious what other people would think are the interesting niches i should focus my benchmarking on.\n\n\nnote for the pednatic: i get that \"computational budget\" varies 1000x based on the method being used and when you call quits (i.e. assume the model has converged to the best solution it's likely to obtain best on some validation data), but still, input size and a number of rows matter (although, again, i get that even those can be relative due to how you prepare/pre-encode/encode the data and because you can do sampling). but i think the framing still \"makes sense\" even if it has this inherent nebulosity.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 8, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/pz4ny2/d_small_neglected_complex_datasets_for/',)", "identifyer": 5722241, "year": "2021"}, {"autor": "Sheev_For_Senate", "date": 1632941041000, "content": "Looking for batch image cleaning tool /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/py1tqk/looking_for_batch_image_cleaning_tool/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "looking for batch image cleaning -----> tool !!!  /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/py1tqk/looking_for_batch_image_cleaning_tool/',)", "identifyer": 5722316, "year": "2021"}, {"autor": "B_Ray18", "date": 1632938315000, "content": "I spent 6 months on a personal project: create an AI/ML-based animation tool that allows me to make an original animated video in one click. I was successful. Here\u2019s the story of how I cloned myself using Machine Learning.", "link": "https://www.reddit.com/r/MachineLearning/comments/py0wwe/i_spent_6_months_on_a_personal_project_create_an/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "i spent 6 months on a personal project: create an ai/ml-based animation -----> tool !!!  that allows me to make an original animated video in one click. i was successful. here\u2019s the story of how i cloned myself using machine learning.", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('rich:video',)", "medialink": "('https://youtu.be/40fYijeAyKo',)", "identifyer": 5722318, "year": "2021"}, {"autor": "alexid95", "date": 1635698317000, "content": "Training a model to recognize that a source/citation is needed in a text [R] /!/ I'm looking into creating a tool that will analyze text and recognize where a source is needed for the claim or fact.\n\nSome examples could be:\n\"Amazon was founded in 1997\"\n\"Avarage conversion rates for eCommerce is 2%\"\n\"Gold is up 20% in May 2021\"\n\nMy question is if it would be possible to train a NLP model to understand this?", "link": "https://www.reddit.com/r/MachineLearning/comments/qjtapm/training_a_model_to_recognize_that_a/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "training a model to recognize that a source/citation is needed in a text [r] /!/ i'm looking into creating a -----> tool !!!  that will analyze text and recognize where a source is needed for the claim or fact.\n\nsome examples could be:\n\"amazon was founded in 1997\"\n\"avarage conversion rates for ecommerce is 2%\"\n\"gold is up 20% in may 2021\"\n\nmy question is if it would be possible to train a nlp model to understand this?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/qjtapm/training_a_model_to_recognize_that_a/',)", "identifyer": 5722338, "year": "2021"}, {"autor": "alexid95", "date": 1635697984000, "content": "Training a model to recognize where a citation/source is needed in a text /!/ I'm looking into creating a tool that will analyze text and recognize where a source is needed for the claim or fact.\n\nSome examples could be:\n\"Amazon was founded in 1997\"\n\"Avarage conversion rates for eCommerce is 2%\"\n\"Gold is up 20% in May 2021\"\n\nMy question is if it would be possible to train a NLP model to understand this?", "link": "https://www.reddit.com/r/MachineLearning/comments/qjt6h8/training_a_model_to_recognize_where_a/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "training a model to recognize where a citation/source is needed in a text /!/ i'm looking into creating a -----> tool !!!  that will analyze text and recognize where a source is needed for the claim or fact.\n\nsome examples could be:\n\"amazon was founded in 1997\"\n\"avarage conversion rates for ecommerce is 2%\"\n\"gold is up 20% in may 2021\"\n\nmy question is if it would be possible to train a nlp model to understand this?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/qjt6h8/training_a_model_to_recognize_where_a/',)", "identifyer": 5722339, "year": "2021"}, {"autor": "cpierse", "date": 1613178130000, "content": "[P] Transformers Interpret - Explainable AI for transformer models. /!/ Hey, \n\nI've just released an early version of [Transformers Interpret](https://github.com/cdpierse/transformers-interpret) which is a model explainability tool designed to work exclusively with huggingface transformers.\n\nIn line with the philosophy of the transformers package tranformers interpret allows any transformers model's output to be explained in just two lines. \n\nThe package also supports visualizations in both notebooks and as savable html files. Right now the package is in its infancy and currently only provides explanations and visualizations for all classification models. I'll be adding support for question answering and NER models in the near future.\n\nThe package is built upon [Pytorch Captum](https://github.com/pytorch/captum) which is a really great project focussed on delivering state of the art explainable AI and interpretability into the pytorch ecosystem. I'd highly recommend check that out too. \n\nI'd love to hear some feedback and thoughts on the project, thanks !", "link": "https://www.reddit.com/r/MachineLearning/comments/lipl90/p_transformers_interpret_explainable_ai_for/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] transformers interpret - explainable ai for transformer models. /!/ hey, \n\ni've just released an early version of [transformers interpret](https://github.com/cdpierse/transformers-interpret) which is a model explainability -----> tool !!!  designed to work exclusively with huggingface transformers.\n\nin line with the philosophy of the transformers package tranformers interpret allows any transformers model's output to be explained in just two lines. \n\nthe package also supports visualizations in both notebooks and as savable html files. right now the package is in its infancy and currently only provides explanations and visualizations for all classification models. i'll be adding support for question answering and ner models in the near future.\n\nthe package is built upon [pytorch captum](https://github.com/pytorch/captum) which is a really great project focussed on delivering state of the art explainable ai and interpretability into the pytorch ecosystem. i'd highly recommend check that out too. \n\ni'd love to hear some feedback and thoughts on the project, thanks !", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/lipl90/p_transformers_interpret_explainable_ai_for/',)", "identifyer": 5722450, "year": "2021"}, {"autor": "demegir", "date": 1613158346000, "content": "[D] Why is Google Colab free? /!/ Colab has become the go-to tool for beginners, prototyping and small projects. But why does Google still provide hundreds or thousands of good GPU's (P100, T4..) for free? Surely it isn't for the 'betterment of the AI community'. And they probably are not gaining enough money in Colab Pro to balance the losses in the free version. What do you think?", "link": "https://www.reddit.com/r/MachineLearning/comments/liiqxr/d_why_is_google_colab_free/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] why is google colab free? /!/ colab has become the go-to -----> tool !!!  for beginners, prototyping and small projects. but why does google still provide hundreds or thousands of good gpu's (p100, t4..) for free? surely it isn't for the 'betterment of the ai community'. and they probably are not gaining enough money in colab pro to balance the losses in the free version. what do you think?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 136, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/liiqxr/d_why_is_google_colab_free/',)", "identifyer": 5722473, "year": "2021"}, {"autor": "rajsharma2020", "date": 1613130912000, "content": "[D] Pattern Classification methods /!/ I want to create 9 groups of patterns based on thousands of patterns and each pattern fitting in either of the 9 groups based on similarity.\n\nAny referance to library or tool that does it beautifully?\n\nNote: I do not want to create the groups myself, I need the groups to be auto-generated.", "link": "https://www.reddit.com/r/MachineLearning/comments/li9kzh/d_pattern_classification_methods/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] pattern classification methods /!/ i want to create 9 groups of patterns based on thousands of patterns and each pattern fitting in either of the 9 groups based on similarity.\n\nany referance to library or -----> tool !!!  that does it beautifully?\n\nnote: i do not want to create the groups myself, i need the groups to be auto-generated.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/li9kzh/d_pattern_classification_methods/',)", "identifyer": 5722500, "year": "2021"}, {"autor": "michael_htx", "date": 1615916431000, "content": "[P] Label Studio v1.0 \u2013 an open source data labeling tool that helps you prepare ML training data and improve the quality of your datasets, works for computer vision, NLP, speech processing, time series analysis, and more /!/  Hi r/MachineLearning,\n\nExcited to post about this, Label Studio \u2013 open source data labeling tool we've started working on more than a year ago is hitting v1.0 with a lot of new goodies. Some features are:\n\n* **Multi-user labeling** \u2013 users can work off of the same datasets and each user\u2019s annotations are tied to their account\n* **Label Studio Projects** \u2013 streamline managing and working on different datasets, can be shared with other users, and can be reused for similar projects in the future\n* **Data manager** \u2013 filter and visualize everything you have in your dataset\n\n[Label Studio Projects \u2013 manage all your datasets in one place](https://preview.redd.it/iqwjic8kd7n61.png?width=1440&amp;format=png&amp;auto=webp&amp;s=ecd28ccdde5bf912f2e8516df0fddc30188027d2)\n\nYou can use label studio to improve the quality of your training datasets and get more accurate models as a result. Read more in [the announcement](https://labelstud.io/blog/release-100.html) and let us know what you think in the comments below or in\u00a0[our Slack community](https://label-studio.slack.com/join/shared_invite/zt-cr8b7ygm-6L45z7biEBw4HXa5A2b5pw#/)!", "link": "https://www.reddit.com/r/MachineLearning/comments/m6eoru/p_label_studio_v10_an_open_source_data_labeling/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] label studio v1.0 \u2013 an open source data labeling -----> tool !!!  that helps you prepare ml training data and improve the quality of your datasets, works for computer vision, nlp, speech processing, time series analysis, and more /!/  hi r/machinelearning,\n\nexcited to post about this, label studio \u2013 open source data labeling tool we've started working on more than a year ago is hitting v1.0 with a lot of new goodies. some features are:\n\n* **multi-user labeling** \u2013 users can work off of the same datasets and each user\u2019s annotations are tied to their account\n* **label studio projects** \u2013 streamline managing and working on different datasets, can be shared with other users, and can be reused for similar projects in the future\n* **data manager** \u2013 filter and visualize everything you have in your dataset\n\n[label studio projects \u2013 manage all your datasets in one place](https://preview.redd.it/iqwjic8kd7n61.png?width=1440&amp;format=png&amp;auto=webp&amp;s=ecd28ccdde5bf912f2e8516df0fddc30188027d2)\n\nyou can use label studio to improve the quality of your training datasets and get more accurate models as a result. read more in [the announcement](https://labelstud.io/blog/release-100.html) and let us know what you think in the comments below or in\u00a0[our slack community](https://label-studio.slack.com/join/shared_invite/zt-cr8b7ygm-6l45z7biebw4hxa5a2b5pw#/)!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 20, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/m6eoru/p_label_studio_v10_an_open_source_data_labeling/',)", "identifyer": 5722691, "year": "2021"}, {"autor": "ccrbltscm", "date": 1615916320000, "content": "[P] A paper-knowledge mapping for \"Towards Causal Representation Learning\" - feedback appreciated /!/ My co-workers and I are experimenting with a new research paper graph tool to help ML researchers easily explore important knowledge areas and research papers related to notable new publications in the field. We first created a static graph that maps out the 10 most relevant research papers (with videos), key knowledge areas, and their connections to the core paper **Towards Causal Representation Learning** lead-authored by Bernhard Sch\u00f6lkopf.  [You can find more details here](https://blog.crossminds.ai/post/ai-research-graph-towards-causal-representation-learning). \n\nWe are in the process to make the graph interactive and plan to include a more complete list of papers (with &amp; without videos). I am sharing the first version here to get your feedback. Is this helpful? Any other improvements to make it more helpful for exploring &amp; understanding new papers? Thank you for your time!\n\nhttps://preview.redd.it/mlfw2ujwafn61.jpg?width=1200&amp;format=pjpg&amp;auto=webp&amp;s=ad6ba6232687073d797f2257281bf575c73c075b", "link": "https://www.reddit.com/r/MachineLearning/comments/m6en9c/p_a_paperknowledge_mapping_for_towards_causal/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] a paper-knowledge mapping for \"towards causal representation learning\" - feedback appreciated /!/ my co-workers and i are experimenting with a new research paper graph -----> tool !!!  to help ml researchers easily explore important knowledge areas and research papers related to notable new publications in the field. we first created a static graph that maps out the 10 most relevant research papers (with videos), key knowledge areas, and their connections to the core paper **towards causal representation learning** lead-authored by bernhard sch\u00f6lkopf.  [you can find more details here](https://blog.crossminds.ai/post/ai-research-graph-towards-causal-representation-learning). \n\nwe are in the process to make the graph interactive and plan to include a more complete list of papers (with &amp; without videos). i am sharing the first version here to get your feedback. is this helpful? any other improvements to make it more helpful for exploring &amp; understanding new papers? thank you for your time!\n\nhttps://preview.redd.it/mlfw2ujwafn61.jpg?width=1200&amp;format=pjpg&amp;auto=webp&amp;s=ad6ba6232687073d797f2257281bf575c73c075b", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/m6en9c/p_a_paperknowledge_mapping_for_towards_causal/',)", "identifyer": 5722692, "year": "2021"}, {"autor": "__Julia", "date": 1615895979000, "content": "[D] Speech Annotation Tools /!/ Hi,\n\nI am trying to annotate audio files in order to work on speech models with customized datasets that we intend to annotate. When I work on NLP problems, I use some tools such [https://prodi.gy/](https://prodi.gy/). \n\nI found this one [https://archive.mpi.nl/tla/elan](https://archive.mpi.nl/tla/elan)\n\nI don't seem to find \"relatively easy-to-use\" annotation tools for speech-to-text and text-to-speech systems.  Is there any  Audio transcription Annotation Tool?. If you don't rely on open datasets to benchmark your models, how do you build such datasets?.", "link": "https://www.reddit.com/r/MachineLearning/comments/m67f6t/d_speech_annotation_tools/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] speech annotation tools /!/ hi,\n\ni am trying to annotate audio files in order to work on speech models with customized datasets that we intend to annotate. when i work on nlp problems, i use some tools such [https://prodi.gy/](https://prodi.gy/). \n\ni found this one [https://archive.mpi.nl/tla/elan](https://archive.mpi.nl/tla/elan)\n\ni don't seem to find \"relatively easy-to-use\" annotation tools for speech-to-text and text-to-speech systems.  is there any  audio transcription annotation -----> tool !!! ?. if you don't rely on open datasets to benchmark your models, how do you build such datasets?.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/m67f6t/d_speech_annotation_tools/',)", "identifyer": 5722709, "year": "2021"}, {"autor": "jhuntinator27", "date": 1617235871000, "content": "[D] Biggest limitations to Artificial Intelligence /!/ I was thinking about something that might just currently be the biggest limitation to artificial intelligence, but I am wondering what other people think.\n\nThis may be a bit more skeptical than what you usually hear regarding AI, but my belief is that we are severely limited by computer architecture, and I don't really know if this will be amenable for quite some time, if at all.\n\nHere is what made me think about it, and I'd love some feedback.\n\nCurrently, I work at UPS, and I like to compare everything they do to what a computer does. The boxes themselves are the data / electrons, while the workers are more of the computer itself. In this scenario, you can see why AI itself won't be able to compete based on the architecture of a computer vs the structure of a company.\n\nThere are managers, trainers, oversight, executive boards, and many more things that fundamentally don't exist within a computer's architecture. This chain of command within a company has redundancies, quality assurance, statistical models baked into it via the people who work there, so all these things are still required to be done by a human being.\n\nIn essence, this chain of command would hypothetically be like a fractal, or branching architecture that may already exist in a limited scope, but I'm quite skeptical about whether or not this would be able to be mimicked by a machine.\n\nThis would explain why AI has become far superior to humans at doing one specific task, but could never possibly take the role of a warehouse worker in it's current state.\n\nPumping more transistors, more data, and more of what we currently have only works to converge these unit based tasks that much more quickly or strongly, but until computers have a much more in depth hierarchy of decision making, the tasks completed by AI will remain quite simple, or better stated, limited.\n\nI'm wondering what others think of this, or if you have a better understanding of just this concept that will help me better form my understanding of it, but I'm thinking AI is almost a misnomer. It's more of a tool that requires a very strong understanding of it to be able to use it and develop it to solve new tasks. This requires a larger human \"architecture\" or hierarchy, to maintain its use and development.\n\nThis is why I believe AI is creating far more jobs than it is destroying (if it's destroying any at all). This is because it's only actually intelligence in the way that a book is intelligence. It's actually more of a tool.", "link": "https://www.reddit.com/r/MachineLearning/comments/mhja1o/d_biggest_limitations_to_artificial_intelligence/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] biggest limitations to artificial intelligence /!/ i was thinking about something that might just currently be the biggest limitation to artificial intelligence, but i am wondering what other people think.\n\nthis may be a bit more skeptical than what you usually hear regarding ai, but my belief is that we are severely limited by computer architecture, and i don't really know if this will be amenable for quite some time, if at all.\n\nhere is what made me think about it, and i'd love some feedback.\n\ncurrently, i work at ups, and i like to compare everything they do to what a computer does. the boxes themselves are the data / electrons, while the workers are more of the computer itself. in this scenario, you can see why ai itself won't be able to compete based on the architecture of a computer vs the structure of a company.\n\nthere are managers, trainers, oversight, executive boards, and many more things that fundamentally don't exist within a computer's architecture. this chain of command within a company has redundancies, quality assurance, statistical models baked into it via the people who work there, so all these things are still required to be done by a human being.\n\nin essence, this chain of command would hypothetically be like a fractal, or branching architecture that may already exist in a limited scope, but i'm quite skeptical about whether or not this would be able to be mimicked by a machine.\n\nthis would explain why ai has become far superior to humans at doing one specific task, but could never possibly take the role of a warehouse worker in it's current state.\n\npumping more transistors, more data, and more of what we currently have only works to converge these unit based tasks that much more quickly or strongly, but until computers have a much more in depth hierarchy of decision making, the tasks completed by ai will remain quite simple, or better stated, limited.\n\ni'm wondering what others think of this, or if you have a better understanding of just this concept that will help me better form my understanding of it, but i'm thinking ai is almost a misnomer. it's more of a -----> tool !!!  that requires a very strong understanding of it to be able to use it and develop it to solve new tasks. this requires a larger human \"architecture\" or hierarchy, to maintain its use and development.\n\nthis is why i believe ai is creating far more jobs than it is destroying (if it's destroying any at all). this is because it's only actually intelligence in the way that a book is intelligence. it's actually more of a tool.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/mhja1o/d_biggest_limitations_to_artificial_intelligence/',)", "identifyer": 5722728, "year": "2021"}, {"autor": "fripperML", "date": 1617172367000, "content": "[D] What\u2019s the simplest, most lightweight but complete and 100% open source MLOps toolkit? -&gt; MY OWN CONCLUSIONS /!/ Although I have posted this summary in the [thread](https://www.reddit.com/r/MachineLearning/comments/mfca0p/d_whats_the_simplest_most_lightweight_but/), most people won't find it, so to make it more visible I post it as another thread. \n\nFirst of all, I have to thank the reddit ML community in general and each of you in particular for the detailed, insightful and interesting answers I have received in the past few days. I have learnt a lot and the picture in my head is now clearer. Now, I am posting a summary with the things that, for me, make more sense (it's my opinion and will serve as our guideline for making the decision, so it's not just a bare summary).\n\n**General advice**\n\nWe should start with a reduced set of tools, the most useful ones, in order to have the flexibility to change or adapt our projects to a new infrastructure a provider could offer us. This is something that could happen.\n\n**End-to-end solutions**\n\nThere are mainly two solutions that are 100% open source and free to install and use, and that may solve most of the requirements of ML practitioners: [Hopsworks](https://hopsworks.readthedocs.io/en/stable/) and [ClearML](https://allegro.ai/clearml/docs/). Among this two, if I had to chose one right now, it will be ClearML. Hopsworks might be much more complete, but ClearML seems to have a bigger community behind it and to be easier to install and use. So ClearML will be something to take a look at in case we go for an all-in-one package. I also like the idea of having a platform with an UI with all our projects.\n\n**Python Programming**\n\n[Flake8](https://flake8.pycqa.org/en/latest/) (including flake8-docstrings), [MyPy](http://mypy-lang.org/) and [Black](https://black.readthedocs.io/en/stable/) are hugely recommended. [Google style guide](https://google.github.io/styleguide/pyguide.html) is something to take a look at too.\n\nThis morning I have found this [guide](https://cjolowicz.github.io/posts/hypermodern-python-01-setup/) that might be worth it, as it covers many good practices. Also this [article](https://martinheinz.dev/blog/14).\n\nRegarding the IDE, VSCode is not the same as Visual Studio, the most recommended one is VSCode.\n\n[Poetry](https://python-poetry.org/) is also something to consider.\n\n**CI and Deployment**\n\nJenkins is a good tool. Docker might not be totally needed, but is hugely recommended as it is becoming a standard, and even many of the libraries rely on it (for example, ClearML does). In addition, it works very well with Jenkins.\n\nWe should switch from SVN to git (strongly recommended). [Gitlab](https://about.gitlab.com/) is a good option.\n\n**Project Scaffolding**\n\n[CookieCutter](https://cookiecutter.readthedocs.io/en/1.7.2/) or [Kedro](https://kedro.readthedocs.io/en/stable/) are the winners. I still think we will stick to Kedro template, because it offers extra functionality, and I like to think of each project as a set of pipelines to be run. Anyway, some cookiecutter templates are very good, like this [one](https://github.com/TezRomacH/python-package-template). In case we use both Kedro and ClearML, we'll have to figure out how to integrate its pipelines with ClearML tasks. But in the slack channel of ClearML there are other teams doing the same, so at least it's possible.\n\n**Documentation**\n\n[Sphinx](https://www.sphinx-doc.org/en/master/index.html) for the documentation is totally recommended (Google style docstrings). [Napoleon](https://www.sphinx-doc.org/en/master/usage/extensions/napoleon.html) can be very useful for helping with that.\n\n**Project registry**\n\nClearML if we finally chose it. Otherwise, we migth use an internal wiki or just the repository with a clear documentation.\n\n**Data Exploration and Preparation**\n\nWe should use PySpark when things go \"big\", and Pandas when things fit in memory.\n\n**Tests**\n\nI expected Great Expectations library to be recommended, but nobody told anything. Instead, unit testing and/or smoke tests using [pytest](https://docs.pytest.org/en/stable/). And checking them with Jenkins. Anyway, if Kedro ends up being our project template, I'll keep an eye on the [plugin](https://github.com/tamsanh/kedro-great) with [Great Expectations](https://github.com/great-expectations/great_expectations).\n\n**Feature Store, Data Versioning**\n\nMaybe not so important in the beginning. [DVC](https://dvc.org/doc) looks good, but it's not easy to use.\n\n**Model registry**\n\nNot essential.\n\n**Experimenting**\n\nIt's an important piece. If we use ClearML, this will be solved. Otherwise, we might try [MLFlow](https://www.mlflow.org/docs/latest/index.html) using Kedro-MLFlow or [PipelineX](https://pipelinex.readthedocs.io/en/latest/).\n\n[Hydra](https://hydra.cc/docs/intro/) can be an interesting addition to define configurations, although Kedro does have a nice way too.\n\n**Training**\n\nApart from the \"classical\" libraries, in case of DL for simplicity [PyTorch Lighting](https://www.pytorchlightning.ai/) will be our first option.\n\n**Model serving**\n\n[FastAPI](https://fastapi.tiangolo.com/). Or even simpler: [DL4J](https://deeplearning4j.org/), to be used in Java when we need to communicate with the rest of the applications in real time.\n\n**Visualization**\n\nWe should take a look at [voila](https://voila.readthedocs.io/en/stable/using.html) and [streamlit](https://streamlit.io/).\n\n**Model monitoring**\n\nWe could use Jenkins pipelines or ad-hoc scheduled processed. We don't need a tool for that.", "link": "https://www.reddit.com/r/MachineLearning/comments/mgzvt2/d_whats_the_simplest_most_lightweight_but/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] what\u2019s the simplest, most lightweight but complete and 100% open source mlops toolkit? -&gt; my own conclusions /!/ although i have posted this summary in the [thread](https://www.reddit.com/r/machinelearning/comments/mfca0p/d_whats_the_simplest_most_lightweight_but/), most people won't find it, so to make it more visible i post it as another thread. \n\nfirst of all, i have to thank the reddit ml community in general and each of you in particular for the detailed, insightful and interesting answers i have received in the past few days. i have learnt a lot and the picture in my head is now clearer. now, i am posting a summary with the things that, for me, make more sense (it's my opinion and will serve as our guideline for making the decision, so it's not just a bare summary).\n\n**general advice**\n\nwe should start with a reduced set of tools, the most useful ones, in order to have the flexibility to change or adapt our projects to a new infrastructure a provider could offer us. this is something that could happen.\n\n**end-to-end solutions**\n\nthere are mainly two solutions that are 100% open source and free to install and use, and that may solve most of the requirements of ml practitioners: [hopsworks](https://hopsworks.readthedocs.io/en/stable/) and [clearml](https://allegro.ai/clearml/docs/). among this two, if i had to chose one right now, it will be clearml. hopsworks might be much more complete, but clearml seems to have a bigger community behind it and to be easier to install and use. so clearml will be something to take a look at in case we go for an all-in-one package. i also like the idea of having a platform with an ui with all our projects.\n\n**python programming**\n\n[flake8](https://flake8.pycqa.org/en/latest/) (including flake8-docstrings), [mypy](http://mypy-lang.org/) and [black](https://black.readthedocs.io/en/stable/) are hugely recommended. [google style guide](https://google.github.io/styleguide/pyguide.html) is something to take a look at too.\n\nthis morning i have found this [guide](https://cjolowicz.github.io/posts/hypermodern-python-01-setup/) that might be worth it, as it covers many good practices. also this [article](https://martinheinz.dev/blog/14).\n\nregarding the ide, vscode is not the same as visual studio, the most recommended one is vscode.\n\n[poetry](https://python-poetry.org/) is also something to consider.\n\n**ci and deployment**\n\njenkins is a good -----> tool !!! . docker might not be totally needed, but is hugely recommended as it is becoming a standard, and even many of the libraries rely on it (for example, clearml does). in addition, it works very well with jenkins.\n\nwe should switch from svn to git (strongly recommended). [gitlab](https://about.gitlab.com/) is a good option.\n\n**project scaffolding**\n\n[cookiecutter](https://cookiecutter.readthedocs.io/en/1.7.2/) or [kedro](https://kedro.readthedocs.io/en/stable/) are the winners. i still think we will stick to kedro template, because it offers extra functionality, and i like to think of each project as a set of pipelines to be run. anyway, some cookiecutter templates are very good, like this [one](https://github.com/tezromach/python-package-template). in case we use both kedro and clearml, we'll have to figure out how to integrate its pipelines with clearml tasks. but in the slack channel of clearml there are other teams doing the same, so at least it's possible.\n\n**documentation**\n\n[sphinx](https://www.sphinx-doc.org/en/master/index.html) for the documentation is totally recommended (google style docstrings). [napoleon](https://www.sphinx-doc.org/en/master/usage/extensions/napoleon.html) can be very useful for helping with that.\n\n**project registry**\n\nclearml if we finally chose it. otherwise, we migth use an internal wiki or just the repository with a clear documentation.\n\n**data exploration and preparation**\n\nwe should use pyspark when things go \"big\", and pandas when things fit in memory.\n\n**tests**\n\ni expected great expectations library to be recommended, but nobody told anything. instead, unit testing and/or smoke tests using [pytest](https://docs.pytest.org/en/stable/). and checking them with jenkins. anyway, if kedro ends up being our project template, i'll keep an eye on the [plugin](https://github.com/tamsanh/kedro-great) with [great expectations](https://github.com/great-expectations/great_expectations).\n\n**feature store, data versioning**\n\nmaybe not so important in the beginning. [dvc](https://dvc.org/doc) looks good, but it's not easy to use.\n\n**model registry**\n\nnot essential.\n\n**experimenting**\n\nit's an important piece. if we use clearml, this will be solved. otherwise, we might try [mlflow](https://www.mlflow.org/docs/latest/index.html) using kedro-mlflow or [pipelinex](https://pipelinex.readthedocs.io/en/latest/).\n\n[hydra](https://hydra.cc/docs/intro/) can be an interesting addition to define configurations, although kedro does have a nice way too.\n\n**training**\n\napart from the \"classical\" libraries, in case of dl for simplicity [pytorch lighting](https://www.pytorchlightning.ai/) will be our first option.\n\n**model serving**\n\n[fastapi](https://fastapi.tiangolo.com/). or even simpler: [dl4j](https://deeplearning4j.org/), to be used in java when we need to communicate with the rest of the applications in real time.\n\n**visualization**\n\nwe should take a look at [voila](https://voila.readthedocs.io/en/stable/using.html) and [streamlit](https://streamlit.io/).\n\n**model monitoring**\n\nwe could use jenkins pipelines or ad-hoc scheduled processed. we don't need a tool for that.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 73, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/mgzvt2/d_whats_the_simplest_most_lightweight_but/',)", "identifyer": 5722786, "year": "2021"}, {"autor": "ProfJasonCorso", "date": 1610662883000, "content": "[D] Feedback Requested: FiftyOne: new open-source ML tool for data quality and model analysis /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/kxgbdh/d_feedback_requested_fiftyone_new_opensource_ml/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] feedback requested: fiftyone: new open-source ml -----> tool !!!  for data quality and model analysis /!/ [removed]", "sortedWord": "None", "removed": "('reddit',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/kxgbdh/d_feedback_requested_fiftyone_new_opensource_ml/',)", "identifyer": 5722877, "year": "2021"}, {"autor": "ProfJasonCorso", "date": 1610662563000, "content": "Feedback Requested: FiftyOne: a new open-source ML tool for data quality and model analysis /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/kxg7g1/feedback_requested_fiftyone_a_new_opensource_ml/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "feedback requested: fiftyone: a new open-source ml -----> tool !!!  for data quality and model analysis /!/ [removed]", "sortedWord": "None", "removed": "('reddit',)", "score": 1, "comments": 2, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/kxg7g1/feedback_requested_fiftyone_a_new_opensource_ml/',)", "identifyer": 5722878, "year": "2021"}, {"autor": "HahaBear221", "date": 1610625331000, "content": "[D]What if drop highest few weights in a network only for erroneous simples in the training set when we dropout /!/   It's renowned that dropout is akin to a kind of ensemble method, thereby it is a great tool to reduce variance. However, compared to the numerous ways people invented for ensembling models (Adaboost, gradient boost, voting, etc), the way to perform dropout seems to be mono-toned. How about we mimic the principal in Adaboost to let the model drop more for samples with wrong predictions, and make it more likely to drop higher weight (borrowed from dropconnect and L1/L2 regularization)?\n\nI can surely imagine this random idea of mine is already invented, implemented &amp; published in a year like 2014 or 2015. But given people are so good at inventing cool names for their tricks in this emerging field I don't think I can easily find the literature. Has anybody done an experiment with this idea?\n\nCould you tell me any fancy dropout technique which has a sensible theory as well as a reproducible project? I know the layer dropout. Thank you!", "link": "https://www.reddit.com/r/MachineLearning/comments/kx49c7/dwhat_if_drop_highest_few_weights_in_a_network/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d]what if drop highest few weights in a network only for erroneous simples in the training set when we dropout /!/   it's renowned that dropout is akin to a kind of ensemble method, thereby it is a great -----> tool !!!  to reduce variance. however, compared to the numerous ways people invented for ensembling models (adaboost, gradient boost, voting, etc), the way to perform dropout seems to be mono-toned. how about we mimic the principal in adaboost to let the model drop more for samples with wrong predictions, and make it more likely to drop higher weight (borrowed from dropconnect and l1/l2 regularization)?\n\ni can surely imagine this random idea of mine is already invented, implemented &amp; published in a year like 2014 or 2015. but given people are so good at inventing cool names for their tricks in this emerging field i don't think i can easily find the literature. has anybody done an experiment with this idea?\n\ncould you tell me any fancy dropout technique which has a sensible theory as well as a reproducible project? i know the layer dropout. thank you!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/kx49c7/dwhat_if_drop_highest_few_weights_in_a_network/',)", "identifyer": 5722921, "year": "2021"}, {"autor": "kg_unist", "date": 1610624424000, "content": "[D] Benchmarking on REAL mobiles devices? /!/ Hello everyone,\nThis question is for people who deploy Tflite models on mobile devices.\n\nMe and my friends working on one project and interested to understand the needs for measuring the speed of your Tflite models on mobile devices.\n\n1. How often do you benchmark Tflite models?\n2. What is the average time you spend?\n3. How difficult is to analyze the speed and compare your models?\n4. If we can provide easy to measure tool with full of mobiles device infrastructure and speed visualization dashboard, would you be interested?\n\nPlease let me know your opinion. Thank you.\n\n[Analyze](https://edgebenchmark.com)", "link": "https://www.reddit.com/r/MachineLearning/comments/kx42c5/d_benchmarking_on_real_mobiles_devices/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] benchmarking on real mobiles devices? /!/ hello everyone,\nthis question is for people who deploy tflite models on mobile devices.\n\nme and my friends working on one project and interested to understand the needs for measuring the speed of your tflite models on mobile devices.\n\n1. how often do you benchmark tflite models?\n2. what is the average time you spend?\n3. how difficult is to analyze the speed and compare your models?\n4. if we can provide easy to measure -----> tool !!!  with full of mobiles device infrastructure and speed visualization dashboard, would you be interested?\n\nplease let me know your opinion. thank you.\n\n[analyze](https://edgebenchmark.com)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/kx42c5/d_benchmarking_on_real_mobiles_devices/',)", "identifyer": 5722924, "year": "2021"}, {"autor": "universome", "date": 1618499655000, "content": "[P] Aligning Latent and Image Spaces to Connect the Unconnectable /!/ Hi! Wanted to share our latest project on infinite image generation: [http://universome.github.io/alis](http://universome.github.io/alis)\n\n*Processing video 53apkx1asct61...*\n\nBasically, it works the following way. We put each latent code at some position of the coordinates grid (where image pixels are located) and compute each pixel from an interpolation of the nearby latent codes. During training, we generate frames from random positions of this coordinates grid and feed them to the discriminator. At test time, this allows us to produce images from any position of the infinite plane which stitch seamlessly with one another.\n\nOur generator computes an image through independent patches (like [CocoGAN](https://arxiv.org/abs/1904.00284) does), which is a less extreme version of [INR-GAN](https://arxiv.org/abs/2011.12026)/[CIPS](https://arxiv.org/abs/2011.13775), that have independence at the pixel level. This (+some technical tweaking of the coordinate embeddings) makes it (periodically) spatially equivariant: when you shift the coordinates, the output image shifts accordingly and their pixel values at common coordinates are equal (up to numerical precision), as illustrated below.\n\n&amp;#x200B;\n\n*Processing img 4wbquwfgsct61...*\n\nA surprising thing is that our approach works not only on the dataset of nature landscapes (which has spatially invariant image statistics), but also (to some extent) on LSUN bedroom, which is a very \"difficult\" dataset for infinite image generation because of the walls and close-by objects. They make it hard or impossible to extrapolate an image to the left or to the right since the dataset does not have any pictures with walls or close-by objects in the middle, so the model has nowhere to learn such knowledge from.\n\n&amp;#x200B;\n\n*Processing img ugyrknposct61...*\n\nBesides, we also collected/preprocessed a high quality dataset of 90k landscape images (Landscapes HQ) from Unsplash and Flickr and will release it soon.\n\n**Drawbacks of the approach:**\n\n* Generating patches completely independently limits the generation quality (by \\\\\\~30% in our experiments). Moreover, due to patchwise generation the model learnt to ignore noise injection which is a powerful tool for image editing (this also happens for INR-GAN/CIPS).\n* Not all the datasets are \"connectable\". For LSUN bedroom, it worked only after we filtered away images which do not contain spatially invariant statistics (see Sec 3.3 and Appendix C). For FFHQ or ImageNet, I cannot imagine it to work at all (see the \"study\" on this in tables 3 &amp; 4 in Appendix C)\n\nProject page: [https://universome.github.io/alis](https://universome.github.io/alis)\n\nCode: [https://github.com/universome/alis](https://github.com/universome/alis)\n\nPaper: [https://arxiv.org/abs/2104.06954](https://arxiv.org/abs/2104.06954)", "link": "https://www.reddit.com/r/MachineLearning/comments/mrgrdn/p_aligning_latent_and_image_spaces_to_connect_the/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] aligning latent and image spaces to connect the unconnectable /!/ hi! wanted to share our latest project on infinite image generation: [http://universome.github.io/alis](http://universome.github.io/alis)\n\n*processing video 53apkx1asct61...*\n\nbasically, it works the following way. we put each latent code at some position of the coordinates grid (where image pixels are located) and compute each pixel from an interpolation of the nearby latent codes. during training, we generate frames from random positions of this coordinates grid and feed them to the discriminator. at test time, this allows us to produce images from any position of the infinite plane which stitch seamlessly with one another.\n\nour generator computes an image through independent patches (like [cocogan](https://arxiv.org/abs/1904.00284) does), which is a less extreme version of [inr-gan](https://arxiv.org/abs/2011.12026)/[cips](https://arxiv.org/abs/2011.13775), that have independence at the pixel level. this (+some technical tweaking of the coordinate embeddings) makes it (periodically) spatially equivariant: when you shift the coordinates, the output image shifts accordingly and their pixel values at common coordinates are equal (up to numerical precision), as illustrated below.\n\n&amp;#x200b;\n\n*processing img 4wbquwfgsct61...*\n\na surprising thing is that our approach works not only on the dataset of nature landscapes (which has spatially invariant image statistics), but also (to some extent) on lsun bedroom, which is a very \"difficult\" dataset for infinite image generation because of the walls and close-by objects. they make it hard or impossible to extrapolate an image to the left or to the right since the dataset does not have any pictures with walls or close-by objects in the middle, so the model has nowhere to learn such knowledge from.\n\n&amp;#x200b;\n\n*processing img ugyrknposct61...*\n\nbesides, we also collected/preprocessed a high quality dataset of 90k landscape images (landscapes hq) from unsplash and flickr and will release it soon.\n\n**drawbacks of the approach:**\n\n* generating patches completely independently limits the generation quality (by \\\\\\~30% in our experiments). moreover, due to patchwise generation the model learnt to ignore noise injection which is a powerful -----> tool !!!  for image editing (this also happens for inr-gan/cips).\n* not all the datasets are \"connectable\". for lsun bedroom, it worked only after we filtered away images which do not contain spatially invariant statistics (see sec 3.3 and appendix c). for ffhq or imagenet, i cannot imagine it to work at all (see the \"study\" on this in tables 3 &amp; 4 in appendix c)\n\nproject page: [https://universome.github.io/alis](https://universome.github.io/alis)\n\ncode: [https://github.com/universome/alis](https://github.com/universome/alis)\n\npaper: [https://arxiv.org/abs/2104.06954](https://arxiv.org/abs/2104.06954)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/mrgrdn/p_aligning_latent_and_image_spaces_to_connect_the/',)", "identifyer": 5722987, "year": "2021"}, {"autor": "universome", "date": 1618494966000, "content": "[P] Aligning Latent and Image Spaces to Connect the Unconnectable /!/ Hi! Wanted to share our latest project on infinite image generation: [https://universome.github.io/alis](https://universome.github.io/alis)\n\n![video](exo3parq5ct61 \"The method works without any conditioning and learns from a dataset of unrelated square images\")\n\nBasically, it works the following way. We put each latent code at some position of the coordinates grid (where image pixels are located) and compute each pixel from an interpolation of the nearby latent codes. During training, we generate frames from random positions of this coordinates grid and feed them to the discriminator. At test time, this allows us to produce images from any position of the infinite plane which stitch seamlessly with one another.\n\nOur generator computes an image through independent patches (like [CocoGAN](https://arxiv.org/abs/1904.00284) does), which is a less extreme version of [INR-GAN](https://arxiv.org/abs/2011.12026)/[CIPS](https://arxiv.org/abs/2011.13775), which have independence at the pixel level. This (+some technical tweaking of the coordinate embeddings) makes it (periodically) spatially equivariant: when you shift the coordinates, the output image shifts accordingly and their pixel values at common coordinates are equal (up to numerical precision), as illustrated below.\n\n&amp;#x200B;\n\n![img](7zhszqjoact61 \"\ud835\udeff is the value of the coordinates shift. Pixel values inside circles are equal for different generations (up to numerical precision)\")\n\nA surprising thing is that our approach works not only on the dataset of nature landscapes (which has spatially invariant image statistics), but also (to some extent) on LSUN bedroom, which is a very \"difficult\" dataset for infinite image generation because of the walls and close-by objects. They make it hard or impossible to extrapolate an image to the left or to the right since the dataset does not have any pictures with walls or close-by objects in the middle, so the model has nowhere to learn such knowledge from.\n\n![img](qqhseppqbct61 \"Results on LSUN bedroom\")\n\nBesides, we also collected/preprocessed a high quality dataset of 90k landscape images (Landscapes HQ) from Unsplash and Flickr and will release it soon.\n\n**Drawbacks of the approach:**\n\n* Generating patches completely independently limits the generation quality (by \\~30% in our experiments). Moreover, due to patchwise generation the model learnt to ignore noise injection which is a powerful tool for image editing (this also happens for INR-GAN/CIPS).\n* Not all the datasets are \"connectable\". For LSUN bedroom, it worked only after we filtered away images which do not contain spatially invariant statistics (see Sec 3.3 and Appendix C). For FFHQ or ImageNet, I cannot imagine it to work at all (see the \"study\" on this in tables 3 &amp; 4 in Appendix C)\n\nProject page: [https://universome.github.io/alis](https://universome.github.io/alis)\n\nCode: [https://github.com/universome/alis](https://github.com/universome/alis)\n\nPaper: [https://arxiv.org/abs/2104.06954](https://arxiv.org/abs/2104.06954)", "link": "https://www.reddit.com/r/MachineLearning/comments/mrf6op/p_aligning_latent_and_image_spaces_to_connect_the/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] aligning latent and image spaces to connect the unconnectable /!/ hi! wanted to share our latest project on infinite image generation: [https://universome.github.io/alis](https://universome.github.io/alis)\n\n![video](exo3parq5ct61 \"the method works without any conditioning and learns from a dataset of unrelated square images\")\n\nbasically, it works the following way. we put each latent code at some position of the coordinates grid (where image pixels are located) and compute each pixel from an interpolation of the nearby latent codes. during training, we generate frames from random positions of this coordinates grid and feed them to the discriminator. at test time, this allows us to produce images from any position of the infinite plane which stitch seamlessly with one another.\n\nour generator computes an image through independent patches (like [cocogan](https://arxiv.org/abs/1904.00284) does), which is a less extreme version of [inr-gan](https://arxiv.org/abs/2011.12026)/[cips](https://arxiv.org/abs/2011.13775), which have independence at the pixel level. this (+some technical tweaking of the coordinate embeddings) makes it (periodically) spatially equivariant: when you shift the coordinates, the output image shifts accordingly and their pixel values at common coordinates are equal (up to numerical precision), as illustrated below.\n\n&amp;#x200b;\n\n![img](7zhszqjoact61 \"\ud835\udeff is the value of the coordinates shift. pixel values inside circles are equal for different generations (up to numerical precision)\")\n\na surprising thing is that our approach works not only on the dataset of nature landscapes (which has spatially invariant image statistics), but also (to some extent) on lsun bedroom, which is a very \"difficult\" dataset for infinite image generation because of the walls and close-by objects. they make it hard or impossible to extrapolate an image to the left or to the right since the dataset does not have any pictures with walls or close-by objects in the middle, so the model has nowhere to learn such knowledge from.\n\n![img](qqhseppqbct61 \"results on lsun bedroom\")\n\nbesides, we also collected/preprocessed a high quality dataset of 90k landscape images (landscapes hq) from unsplash and flickr and will release it soon.\n\n**drawbacks of the approach:**\n\n* generating patches completely independently limits the generation quality (by \\~30% in our experiments). moreover, due to patchwise generation the model learnt to ignore noise injection which is a powerful -----> tool !!!  for image editing (this also happens for inr-gan/cips).\n* not all the datasets are \"connectable\". for lsun bedroom, it worked only after we filtered away images which do not contain spatially invariant statistics (see sec 3.3 and appendix c). for ffhq or imagenet, i cannot imagine it to work at all (see the \"study\" on this in tables 3 &amp; 4 in appendix c)\n\nproject page: [https://universome.github.io/alis](https://universome.github.io/alis)\n\ncode: [https://github.com/universome/alis](https://github.com/universome/alis)\n\npaper: [https://arxiv.org/abs/2104.06954](https://arxiv.org/abs/2104.06954)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/mrf6op/p_aligning_latent_and_image_spaces_to_connect_the/',)", "identifyer": 5723000, "year": "2021"}, {"autor": "SeaworthinessNo8761", "date": 1619682934000, "content": "I made a simple tool to detect wheather a person wearing a mask", "link": "https://www.reddit.com/r/MachineLearning/comments/n0zjbs/i_made_a_simple_tool_to_detect_wheather_a_person/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "i made a simple -----> tool !!!  to detect wheather a person wearing a mask", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('rich:video',)", "medialink": "('https://youtu.be/kPharXwcerE',)", "identifyer": 5723217, "year": "2021"}, {"autor": "palashio111", "date": 1611912901000, "content": "[D] I built an automated machine learning tool like no other. Integrate machine learning into your team with ease. Modify what you want and let us handle the rest. /!/ [https://www.producthunt.com/posts/paraglide](https://www.producthunt.com/posts/paraglide)", "link": "https://www.reddit.com/r/MachineLearning/comments/l7ogwn/d_i_built_an_automated_machine_learning_tool_like/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] i built an automated machine learning -----> tool !!!  like no other. integrate machine learning into your team with ease. modify what you want and let us handle the rest. /!/ [https://www.producthunt.com/posts/paraglide](https://www.producthunt.com/posts/paraglide)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/l7ogwn/d_i_built_an_automated_machine_learning_tool_like/',)", "identifyer": 5723291, "year": "2021"}, {"autor": "palashio111", "date": 1611912712000, "content": "I built an automated machine learning tool like no other. Integrate machine learning into your team with ease. Modify what you want and let us handle the rest. /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/l7of7e/i_built_an_automated_machine_learning_tool_like/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "i built an automated machine learning -----> tool !!!  like no other. integrate machine learning into your team with ease. modify what you want and let us handle the rest. /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/l7of7e/i_built_an_automated_machine_learning_tool_like/',)", "identifyer": 5723292, "year": "2021"}, {"autor": "That_ai21", "date": 1627574948000, "content": "[P] Introducing TensorBay - free, Git-like unstructured data management /!/ Hey everyone! I\u2019m part of the product team at [Graviti](https://graviti.com/?utm_medium=0730_reddit), and we are creating products that will enhance and ease your ML development experience. Our current project is [TensorBay](https://gas.graviti.com/open-datasets/?utm_medium=0730_reddit), which is essentially a management tool for unstructured data. We know that developers are wasting too much time dealing with data, so TensorBay has made things much easier for you by providing Git-like version control, collaboration, visualization tools, and other features that you are already used to elsewhere but have not seen in the ML space.\n\nBesides TensorBay, we\u2019ve also worked with some of the best unstructured dataset providers out there and made [Open Datasets](https://graviti.com/tensorBay/?utm_medium=0730_reddit), a platform that helps you acquire publicly licensed datasets. Right now, we\u2019ve got amazing open datasets like nuScenes, MNIST, and KITTI, and we\u2019re working on adding more.\n\nThese products are still in development, so I would love to hear all your thoughts and feedback!  \nHomepage: [https://graviti.com/?utm\\_medium=0730\\_reddit](https://graviti.com/?utm_medium=0730_reddit)\n\n&amp;#x200B;\n\n[TensorBay in action](https://i.redd.it/d3bbpiwce6e71.gif)", "link": "https://www.reddit.com/r/MachineLearning/comments/ou0h5e/p_introducing_tensorbay_free_gitlike_unstructured/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] introducing tensorbay - free, git-like unstructured data management /!/ hey everyone! i\u2019m part of the product team at [graviti](https://graviti.com/?utm_medium=0730_reddit), and we are creating products that will enhance and ease your ml development experience. our current project is [tensorbay](https://gas.graviti.com/open-datasets/?utm_medium=0730_reddit), which is essentially a management -----> tool !!!  for unstructured data. we know that developers are wasting too much time dealing with data, so tensorbay has made things much easier for you by providing git-like version control, collaboration, visualization tools, and other features that you are already used to elsewhere but have not seen in the ml space.\n\nbesides tensorbay, we\u2019ve also worked with some of the best unstructured dataset providers out there and made [open datasets](https://graviti.com/tensorbay/?utm_medium=0730_reddit), a platform that helps you acquire publicly licensed datasets. right now, we\u2019ve got amazing open datasets like nuscenes, mnist, and kitti, and we\u2019re working on adding more.\n\nthese products are still in development, so i would love to hear all your thoughts and feedback!  \nhomepage: [https://graviti.com/?utm\\_medium=0730\\_reddit](https://graviti.com/?utm_medium=0730_reddit)\n\n&amp;#x200b;\n\n[tensorbay in action](https://i.redd.it/d3bbpiwce6e71.gif)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/ou0h5e/p_introducing_tensorbay_free_gitlike_unstructured/',)", "identifyer": 5723355, "year": "2021"}, {"autor": "nirav_diu", "date": 1627512747000, "content": "[N] Announcing the XVIEW 3 Prize Challenge: DARKVESSELS - Computer Vision for Illegal, Unreported, and Unregulated Fishing /!/  \n\nhttps://preview.redd.it/hsjqhjl791e71.png?width=3252&amp;format=png&amp;auto=webp&amp;s=edc08ab4e596b1773982ccaffd8ca6c2f7b7190b\n\nThe [Defense Innovation Unit](https://www.diu.mil/) (DIU) and [Global Fishing Watch](https://globalfishingwatch.org/) (GFW) recently (on 7/22/21) announced the [xView3 Challenge](https://iuu.xview.us/) prize competition to identify the best computer vision algorithms to advance the fight against illegal, unreported and unregulated (IUU) fishing. \n\nThe xView3 Challenge is a global call for skilled developers to harness satellite-based synthetic aperture radar (SAR) data to detect dark vessels. The $150,000 prize purse for this Challenge is provided by DIU and GFW, and will be awarded to U.S. and international participants (individuals and companies) with the best performing algorithms. Winning machine learning algorithms must be able to automatically detect and characterize dark vessels\u2014vessels that do not publicly broadcast their location or appear in public monitoring systems\u2014in these SAR images. SAR technology can penetrate clouds and darkness to reveal these dark vessels\u2019 location and activity.\n\nIUU fishing is a threat to food security, marine ecosystem health, and geopolitical stability. According to the Food and Agriculture Organization of the United Nations, IUU fishing accounts for 20 percent of the global seafood catch and up to 50 percent in some areas. The Department of Defense (DoD), partners and allies, and major nonprofits are interested in rapid detection of IUU fishing activity to mitigate these damages. \n\n\u201cIUU fishing undermines a nation\u2019s sovereignty, threatens its economic security, and weakens global rules-based order,\u201d said Vice Admiral Scott Buschman, U.S. Coast Guard Deputy Commandant for Operations. \u201cThe United States can provide global leadership to combat IUU fishing by creating partnerships and collaborative, durable networks. Our collective efforts start with maritime domain awareness; we must cooperate and communicate among enforcement authorities and share pertinent information, including with non-government entities that share our objectives and can help disseminate information quickly to our network of partners.\u201d\n\nThe xView3 Challenge is organized and run by DIU in collaboration with GFW, an international nonprofit organization advancing ocean governance through increased transparency of human activity at sea, with additional support from the U.S. Coast Guard, the National Oceanic and Atmospheric Agency (NOAA), and the National Maritime Intelligence-Integration Office (NMIO).\n\nTracking data from the publicly available automatic identification system (AIS)\u2014a system similar to how air traffic control tracks planes\u2014is an increasingly valuable tool to monitor fishing activity, but vessels are known to switch off their AIS devices or tamper with them to hide illicit activities. Large-scale analysis of space-based SAR has the potential to fill in these important gaps in the AIS data. \n\nThe Challenge will provide competitors with one of the largest open access maritime datasets ever derived from SAR satellites. The scalability of automated vessel detection and characterization derived from such SAR imagery could dramatically enhance situational awareness of IUU fishing activity and, as a result, lead to more focused patrols and interception of suspected offenders.\n\n\"We are excited to partner with the Defense Innovation Unit, Global Fishing Watch, and the U.S. Coast Guard on the xView3 competition to be on the cutting edge of the fight against IUU fishing,\" said James Landon, Director of NOAA's Office of Law Enforcement. \"Together we can leverage technology to tackle this pressing issue and develop solutions that strengthen our dark vessel detection capabilities\u2014allowing us to improve the interdiction and prosecution of IUU fishing.\"\n\nDIU and GFW will share the open source algorithms, reflecting the organizations\u2019 vision to harness open source, low-cost data to advance maritime domain awareness and collaboration among governments, nonprofits and academia to combat IUU fishing.\n\n\u201cThis contest is a great opportunity for us to advance state-of-the-art technology for detecting dark vessels,\" said Paul Woods, chief innovation officer at Global Fishing Watch. \"We can help eliminate illegal fishing by making the best solutions open and applying them to a global, free data set that enables other actors to also benefit from these highly scalable tools. The more people that can use these tools, the faster we will reduce the scope for bad actors to operate in our ocean.\u201d\n\nxView3 builds on the foundation laid by DIU\u2019s two prior xView Challenges, which resulted in algorithms that provide automated post-disaster assessments of roads, bridges and buildings by leveraging winning machine learning algorithms. The xView2 Challenge attracted more than 2,000 submissions; the top three algorithms had an 80 percent damage assessment success rate\u2014four times better than the government baseline algorithm. The winning solution was deployed to assist with the 2019-2020 Australian Bushfires, 2020 California fire season, and the 2020 U.S. hurricane season. The xView2 datasets are currently used by more than 30 government agencies, academic institutions, and industry partners for research and operational use. \n\n\u201cOur goal with this series of challenges is to attract new ideas to some of the most important global safety and security problems as well as to put new operational solutions in place at the speed of relevance,\u201d said Michael Brown, director of the Defense Innovation Unit.  \n\nxView3 will go live in August and the challenge itself will run for three months. Those interested in participating and learning more can register now at [https://www.diu.mil/ai-xview-challenge](https://www.diu.mil/ai-xview-challenge) to receive updates. Further details about the challenge, rules and prize breakdown will be available on the competition website.\n\n\\###\n\n**Media Contacts:**\n\nDefense Innovation Unit Contact: media@diu.mil\n\nGlobal Fishing Watch Contact: media@globalfishingwatch.org\n\n[xView Challenge Series](https://www.diu.mil/ai-xview-challenge): xView is a series of international computer vision competitions run by the Department of Defense\u2019s Defense Innovation Unit (DIU) that advance, benchmark, and procure state of the art machine learning algorithms in domains relevant to national security. \n\n[Defense Innovation Unit](https://www.diu.mil/) (DIU): A U.S. Department of Defense organization accelerating the adoption of commercial technology to advance national security.\n\n[Global Fishing Watch](https://globalfishingwatch.org/) (GFW): an international nonprofit organization dedicated to advancing the sustainability of our ocean through increased transparency of human activity at sea. \n\n[US Coast Guard](https://www.uscg.mil/iuufishing/) (USCG): The Coast Guard is the U.S. agency with the authority, resources, and capability to enforce maritime natural resource laws, and is responsible for performing at-sea enforcement operations.\n\n[National Oceanic and Atmospheric Administration](https://www.noaa.gov/) (NOAA): A scientific agency in the U.S. Department of Commerce that focuses on the conditions of the oceans, major waterways and the atmosphere.", "link": "https://www.reddit.com/r/MachineLearning/comments/otl35f/n_announcing_the_xview_3_prize_challenge/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[n] announcing the xview 3 prize challenge: darkvessels - computer vision for illegal, unreported, and unregulated fishing /!/  \n\nhttps://preview.redd.it/hsjqhjl791e71.png?width=3252&amp;format=png&amp;auto=webp&amp;s=edc08ab4e596b1773982ccaffd8ca6c2f7b7190b\n\nthe [defense innovation unit](https://www.diu.mil/) (diu) and [global fishing watch](https://globalfishingwatch.org/) (gfw) recently (on 7/22/21) announced the [xview3 challenge](https://iuu.xview.us/) prize competition to identify the best computer vision algorithms to advance the fight against illegal, unreported and unregulated (iuu) fishing. \n\nthe xview3 challenge is a global call for skilled developers to harness satellite-based synthetic aperture radar (sar) data to detect dark vessels. the $150,000 prize purse for this challenge is provided by diu and gfw, and will be awarded to u.s. and international participants (individuals and companies) with the best performing algorithms. winning machine learning algorithms must be able to automatically detect and characterize dark vessels\u2014vessels that do not publicly broadcast their location or appear in public monitoring systems\u2014in these sar images. sar technology can penetrate clouds and darkness to reveal these dark vessels\u2019 location and activity.\n\niuu fishing is a threat to food security, marine ecosystem health, and geopolitical stability. according to the food and agriculture organization of the united nations, iuu fishing accounts for 20 percent of the global seafood catch and up to 50 percent in some areas. the department of defense (dod), partners and allies, and major nonprofits are interested in rapid detection of iuu fishing activity to mitigate these damages. \n\n\u201ciuu fishing undermines a nation\u2019s sovereignty, threatens its economic security, and weakens global rules-based order,\u201d said vice admiral scott buschman, u.s. coast guard deputy commandant for operations. \u201cthe united states can provide global leadership to combat iuu fishing by creating partnerships and collaborative, durable networks. our collective efforts start with maritime domain awareness; we must cooperate and communicate among enforcement authorities and share pertinent information, including with non-government entities that share our objectives and can help disseminate information quickly to our network of partners.\u201d\n\nthe xview3 challenge is organized and run by diu in collaboration with gfw, an international nonprofit organization advancing ocean governance through increased transparency of human activity at sea, with additional support from the u.s. coast guard, the national oceanic and atmospheric agency (noaa), and the national maritime intelligence-integration office (nmio).\n\ntracking data from the publicly available automatic identification system (ais)\u2014a system similar to how air traffic control tracks planes\u2014is an increasingly valuable -----> tool !!!  to monitor fishing activity, but vessels are known to switch off their ais devices or tamper with them to hide illicit activities. large-scale analysis of space-based sar has the potential to fill in these important gaps in the ais data. \n\nthe challenge will provide competitors with one of the largest open access maritime datasets ever derived from sar satellites. the scalability of automated vessel detection and characterization derived from such sar imagery could dramatically enhance situational awareness of iuu fishing activity and, as a result, lead to more focused patrols and interception of suspected offenders.\n\n\"we are excited to partner with the defense innovation unit, global fishing watch, and the u.s. coast guard on the xview3 competition to be on the cutting edge of the fight against iuu fishing,\" said james landon, director of noaa's office of law enforcement. \"together we can leverage technology to tackle this pressing issue and develop solutions that strengthen our dark vessel detection capabilities\u2014allowing us to improve the interdiction and prosecution of iuu fishing.\"\n\ndiu and gfw will share the open source algorithms, reflecting the organizations\u2019 vision to harness open source, low-cost data to advance maritime domain awareness and collaboration among governments, nonprofits and academia to combat iuu fishing.\n\n\u201cthis contest is a great opportunity for us to advance state-of-the-art technology for detecting dark vessels,\" said paul woods, chief innovation officer at global fishing watch. \"we can help eliminate illegal fishing by making the best solutions open and applying them to a global, free data set that enables other actors to also benefit from these highly scalable tools. the more people that can use these tools, the faster we will reduce the scope for bad actors to operate in our ocean.\u201d\n\nxview3 builds on the foundation laid by diu\u2019s two prior xview challenges, which resulted in algorithms that provide automated post-disaster assessments of roads, bridges and buildings by leveraging winning machine learning algorithms. the xview2 challenge attracted more than 2,000 submissions; the top three algorithms had an 80 percent damage assessment success rate\u2014four times better than the government baseline algorithm. the winning solution was deployed to assist with the 2019-2020 australian bushfires, 2020 california fire season, and the 2020 u.s. hurricane season. the xview2 datasets are currently used by more than 30 government agencies, academic institutions, and industry partners for research and operational use. \n\n\u201cour goal with this series of challenges is to attract new ideas to some of the most important global safety and security problems as well as to put new operational solutions in place at the speed of relevance,\u201d said michael brown, director of the defense innovation unit.  \n\nxview3 will go live in august and the challenge itself will run for three months. those interested in participating and learning more can register now at [https://www.diu.mil/ai-xview-challenge](https://www.diu.mil/ai-xview-challenge) to receive updates. further details about the challenge, rules and prize breakdown will be available on the competition website.\n\n\\###\n\n**media contacts:**\n\ndefense innovation unit contact: media@diu.mil\n\nglobal fishing watch contact: media@globalfishingwatch.org\n\n[xview challenge series](https://www.diu.mil/ai-xview-challenge): xview is a series of international computer vision competitions run by the department of defense\u2019s defense innovation unit (diu) that advance, benchmark, and procure state of the art machine learning algorithms in domains relevant to national security. \n\n[defense innovation unit](https://www.diu.mil/) (diu): a u.s. department of defense organization accelerating the adoption of commercial technology to advance national security.\n\n[global fishing watch](https://globalfishingwatch.org/) (gfw): an international nonprofit organization dedicated to advancing the sustainability of our ocean through increased transparency of human activity at sea. \n\n[us coast guard](https://www.uscg.mil/iuufishing/) (uscg): the coast guard is the u.s. agency with the authority, resources, and capability to enforce maritime natural resource laws, and is responsible for performing at-sea enforcement operations.\n\n[national oceanic and atmospheric administration](https://www.noaa.gov/) (noaa): a scientific agency in the u.s. department of commerce that focuses on the conditions of the oceans, major waterways and the atmosphere.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/otl35f/n_announcing_the_xview_3_prize_challenge/',)", "identifyer": 5723393, "year": "2021"}, {"autor": "Bing_HE", "date": 1630296068000, "content": "Neural Search with Jina AI | Open-source ML Tool Explained", "link": "https://www.reddit.com/r/MachineLearning/comments/peajha/neural_search_with_jina_ai_opensource_ml_tool/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "neural search with jina ai | open-source ml -----> tool !!!  explained", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('rich:video',)", "medialink": "('https://youtube.com/watch?v=zQqbXFY0Nco&amp;feature=share',)", "identifyer": 5723471, "year": "2021"}, {"autor": "jubili-", "date": 1626418984000, "content": "[P] Search for a DS to scale up an ML algorithm /!/ I am the CIO of Lincoln (a recruitment consultancy for more than 20 years) and I participated in the co-construction of a Matching tool, allowing us to create a selection of relevant candidates for a given recruitment mission.  \n\nWe rely on the pre-qualifications, skills and history we have with these same candidates because we want to create a strong relationship with them and cultivate our own network by working with them before working with other candidats. We have the data in our database.  \n\nCurrently, the algorithm is functional, the interface is developed, and we are in the process of launching the MVP within the structure, so the model is already learning, but we would like to quickly make it evolve while structuring our data.  \n\nWe are therefore looking for a partner for a long-term mission, who can challenge our project, be a force of proposal in the development of new functionalities of the model according to the business needs, and correct the possible dysfunctions. \n\n&amp;#x200B;\n\nHere are some required skills : Python, Microsoft Azure, Spark, Databricks, NLP, Machine Learning\n\nOur priority is on Technical skills (development) :)\n\n&amp;#x200B;\n\nLet me know if you are interested!", "link": "https://www.reddit.com/r/MachineLearning/comments/olbmiv/p_search_for_a_ds_to_scale_up_an_ml_algorithm/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] search for a ds to scale up an ml algorithm /!/ i am the cio of lincoln (a recruitment consultancy for more than 20 years) and i participated in the co-construction of a matching -----> tool !!! , allowing us to create a selection of relevant candidates for a given recruitment mission.  \n\nwe rely on the pre-qualifications, skills and history we have with these same candidates because we want to create a strong relationship with them and cultivate our own network by working with them before working with other candidats. we have the data in our database.  \n\ncurrently, the algorithm is functional, the interface is developed, and we are in the process of launching the mvp within the structure, so the model is already learning, but we would like to quickly make it evolve while structuring our data.  \n\nwe are therefore looking for a partner for a long-term mission, who can challenge our project, be a force of proposal in the development of new functionalities of the model according to the business needs, and correct the possible dysfunctions. \n\n&amp;#x200b;\n\nhere are some required skills : python, microsoft azure, spark, databricks, nlp, machine learning\n\nour priority is on technical skills (development) :)\n\n&amp;#x200b;\n\nlet me know if you are interested!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/olbmiv/p_search_for_a_ds_to_scale_up_an_ml_algorithm/',)", "identifyer": 5723531, "year": "2021"}, {"autor": "ZFCD", "date": 1623788909000, "content": "Image enhancement with reference input? [D] /!/  Hello, I'm not sure if this is the best place to ask this question, so apologies if not.\n\nIs there a freely available tool that can upscale/enhance an image if I provide it with a higher resolution original? I'm animating a still image with Siarohin's first order motion model, but I'd like to restore some of the resolution that's been lost from downscaling\n\nAny help would be appreciated\n\nThanks", "link": "https://www.reddit.com/r/MachineLearning/comments/o0nt3l/image_enhancement_with_reference_input_d/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "image enhancement with reference input? [d] /!/  hello, i'm not sure if this is the best place to ask this question, so apologies if not.\n\nis there a freely available -----> tool !!!  that can upscale/enhance an image if i provide it with a higher resolution original? i'm animating a still image with siarohin's first order motion model, but i'd like to restore some of the resolution that's been lost from downscaling\n\nany help would be appreciated\n\nthanks", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 9, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/o0nt3l/image_enhancement_with_reference_input_d/',)", "identifyer": 5723664, "year": "2021"}, {"autor": "farebrosa", "date": 1631637341000, "content": "[News] The Arcade Learning Environment: Version 0.7 /!/ (disclosure, I am the current maintainer of the project)\n\nHi, r/MachineLearning,\n\nToday we're releasing version 0.7 of the Arcade Learning Environment (ALE). The goal for this release was to consolidate the benchmark into a cohesive package to reduce fragmentation across the community.\n\n**Python Package**\n\nWe now publish Python wheels under the package `ale-py` for all major platforms and architectures, this includes arm64 on macOS for those who have M1 Macs. This also allowed us to distribute SDL so users can now visualize their agents with sound support, resolution scaling, and HiDPI support all without managing any external dependencies.\n\nAs a part of this release, we're also including tools for users to manage their ROMs. This includes the command-line tool `ale-import-roms` as well as utilities for Python packages to expose ROMs to the ALE. The latter can be especially valuable for organizations to distribute a Python package containing all the required ROMs.\n\n**OpenAI Gym**\n\nAs of Gym version 0.2 all the Atari environments will now be provided by the ALE. This allows us to remain in control over the benchmark. To this end, we're introducing v5 environments in the ALE namespace which follow the best practices outlined in \"Revisiting the Arcade Learning Environment\" by Machado et al.\n\n**Additional Features**\n\nWe added support for an additional 30 games bringing the total number of games we support to just over 100. Furthermore, we added more game modes/difficulties across the board. Special thanks to DeepMind for upstreaming these contributions.\n\nFinally, we addressed an issue with determinism in the ALE. We can say with confidence the emulator is now 100% deterministic with the only source of stochasticity coming from sticky actions. Note: the last sources of emulator stochasticity were very subtle and impacted few games.\n\n\nI glanced over everything in this post, for a more detailed explainer check out the following blog post: [https://brosa.ca/blog/ale-release-v0.7](https://brosa.ca/blog/ale-release-v0.7) and the release notes at [https://github.com/mgbellemare/Arcade-Learning-Environment/releases/tag/v0.7.0](https://github.com/mgbellemare/Arcade-Learning-Environment/releases/tag/v0.7.0).", "link": "https://www.reddit.com/r/MachineLearning/comments/po6cyi/news_the_arcade_learning_environment_version_07/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[news] the arcade learning environment: version 0.7 /!/ (disclosure, i am the current maintainer of the project)\n\nhi, r/machinelearning,\n\ntoday we're releasing version 0.7 of the arcade learning environment (ale). the goal for this release was to consolidate the benchmark into a cohesive package to reduce fragmentation across the community.\n\n**python package**\n\nwe now publish python wheels under the package `ale-py` for all major platforms and architectures, this includes arm64 on macos for those who have m1 macs. this also allowed us to distribute sdl so users can now visualize their agents with sound support, resolution scaling, and hidpi support all without managing any external dependencies.\n\nas a part of this release, we're also including tools for users to manage their roms. this includes the command-line -----> tool !!!  `ale-import-roms` as well as utilities for python packages to expose roms to the ale. the latter can be especially valuable for organizations to distribute a python package containing all the required roms.\n\n**openai gym**\n\nas of gym version 0.2 all the atari environments will now be provided by the ale. this allows us to remain in control over the benchmark. to this end, we're introducing v5 environments in the ale namespace which follow the best practices outlined in \"revisiting the arcade learning environment\" by machado et al.\n\n**additional features**\n\nwe added support for an additional 30 games bringing the total number of games we support to just over 100. furthermore, we added more game modes/difficulties across the board. special thanks to deepmind for upstreaming these contributions.\n\nfinally, we addressed an issue with determinism in the ale. we can say with confidence the emulator is now 100% deterministic with the only source of stochasticity coming from sticky actions. note: the last sources of emulator stochasticity were very subtle and impacted few games.\n\n\ni glanced over everything in this post, for a more detailed explainer check out the following blog post: [https://brosa.ca/blog/ale-release-v0.7](https://brosa.ca/blog/ale-release-v0.7) and the release notes at [https://github.com/mgbellemare/arcade-learning-environment/releases/tag/v0.7.0](https://github.com/mgbellemare/arcade-learning-environment/releases/tag/v0.7.0).", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/po6cyi/news_the_arcade_learning_environment_version_07/',)", "identifyer": 5723772, "year": "2021"}, {"autor": "toby__bryant", "date": 1625056548000, "content": "[P] Atom\u2014free one-click segmentation tool /!/ Hi everyone, \n\nwe built a free one-click segmentation tool for image annotation: [https://medium.com/hasty-ai/the-first-completely-free-one-click-segmentation-tool-hasty-ai-a3c98f28603a](https://medium.com/hasty-ai/the-first-completely-free-one-click-segmentation-tool-hasty-ai-a3c98f28603a) \n\nThere are already many approaches to using clicks to annotate images, however, most methods integrate the clicks early in additional input channels (see the widely used [DEXTR](https://arxiv.org/pdf/1711.09081.pdf) for example). Often the user has to add several clicks before getting a prediction. Even with interactive segmentation techniques, additional clicks tend to be treated the same as earlier clicks. In order to improve upon initial predictions, our positive or negative clicks are taken together with the previously predicted mask. In addition to the clicks + image features, we also generate clicks + prior-prediction features independently. These separate sets of features prevent a bad initial prediction from harming the features generated from the image. By combining both sets of features, our approach allows the user to continue adding positive and negative clicks in order to reach a high IoU. \n\nYou can use it for free without any limits by creating an account here: [https://hasty.ai/annotation/](https://hasty.ai/annotation/)", "link": "https://www.reddit.com/r/MachineLearning/comments/oawexp/p_atomfree_oneclick_segmentation_tool/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] atom\u2014free one-click segmentation -----> tool !!!  /!/ hi everyone, \n\nwe built a free one-click segmentation tool for image annotation: [https://medium.com/hasty-ai/the-first-completely-free-one-click-segmentation-tool-hasty-ai-a3c98f28603a](https://medium.com/hasty-ai/the-first-completely-free-one-click-segmentation-tool-hasty-ai-a3c98f28603a) \n\nthere are already many approaches to using clicks to annotate images, however, most methods integrate the clicks early in additional input channels (see the widely used [dextr](https://arxiv.org/pdf/1711.09081.pdf) for example). often the user has to add several clicks before getting a prediction. even with interactive segmentation techniques, additional clicks tend to be treated the same as earlier clicks. in order to improve upon initial predictions, our positive or negative clicks are taken together with the previously predicted mask. in addition to the clicks + image features, we also generate clicks + prior-prediction features independently. these separate sets of features prevent a bad initial prediction from harming the features generated from the image. by combining both sets of features, our approach allows the user to continue adding positive and negative clicks in order to reach a high iou. \n\nyou can use it for free without any limits by creating an account here: [https://hasty.ai/annotation/](https://hasty.ai/annotation/)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 19, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/oawexp/p_atomfree_oneclick_segmentation_tool/',)", "identifyer": 5723870, "year": "2021"}, {"autor": "assassinshadow11", "date": 1624977082000, "content": "[P] Made a reddit Comment Ripper a while ago, decided to share it with everyone. /!/ For a while I was looking for a tool that could extract comments from relevant reddit posts for ML purposes, could not find one that would suit my needs hence, built this little tool that does the job. It's pretty easy to add tags we are interested in and tags we deem irrelevant along with some more minor controls. \n\nGithub Link: [https://github.com/assassinshadow0/RedditCommentRipper](https://github.com/assassinshadow0/RedditCommentRipper) (C#, unity. I'm sorry about the crappy UI)\n\nThe data set is used in a project that generates compliments based on selfies. It is not hyper accurate but I did get some chuckles here and there.\n\nSelfie Compliment: [https://selfieai.live/](https://selfieai.live/)\n\nHope someone finds the tool useful.", "link": "https://www.reddit.com/r/MachineLearning/comments/oa9g2r/p_made_a_reddit_comment_ripper_a_while_ago/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] made a reddit comment ripper a while ago, decided to share it with everyone. /!/ for a while i was looking for a -----> tool !!!  that could extract comments from relevant reddit posts for ml purposes, could not find one that would suit my needs hence, built this little -----> tool !!!  that does the job. it's pretty easy to add tags we are interested in and tags we deem irrelevant along with some more minor controls. \n\ngithub link: [https://github.com/assassinshadow0/redditcommentripper](https://github.com/assassinshadow0/redditcommentripper) (c#, unity. i'm sorry about the crappy ui)\n\nthe data set is used in a project that generates compliments based on selfies. it is not hyper accurate but i did get some chuckles here and there.\n\nselfie compliment: [https://selfieai.live/](https://selfieai.live/)\n\nhope someone finds the tool useful.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/oa9g2r/p_made_a_reddit_comment_ripper_a_while_ago/',)", "identifyer": 5723920, "year": "2021"}, {"autor": "Discordy", "date": 1612455401000, "content": "[P] Connected Papers partners with arXiv and Papers with Code /!/ Hi everyone!  \nWe launched [Connected Papers](https://www.connectedpapers.com/) 7 months ago in this very subreddit, with the goal to help researchers visually find and explore academic papers.\n\n**Input:** a paper of your liking.  \n**Output:** a full interactive graph of similar papers to explore.\n\nFor example, here is the [graph for EfficientNet](https://www.connectedpapers.com/main/4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9/EfficientNet-Rethinking-Model-Scaling-for-Convolutional-Neural-Networks/graph):\n\nhttps://preview.redd.it/bb1dafwlihf61.png?width=1920&amp;format=png&amp;auto=webp&amp;s=e38203ef3a517029e814548e890ebb4a96e189ba\n\nSince launch we've been positively overwhelmed with feedback from the scientific community and half a million researchers using the tool.\n\n**Today is a big milestone for us - we have partnered with** [**arXiv.org**](https://arXiv.org) **and from now on every paper page in arXiv will link to a corresponding Connected Papers graph.**\n\nIt looks like this:\n\nhttps://preview.redd.it/8eps82s7ihf61.png?width=1250&amp;format=png&amp;auto=webp&amp;s=bff988753a09eea42b340504d77a8560a84f6772\n\nAnd works like this:\n\nhttps://i.redd.it/uu5qxcl8ihf61.gif\n\nIn addition, we've lately integrated with the awesome team at [Papers with Code](https://paperswithcode.com/) and whenever a paper has a code implementation, it will be presented straight in the paper links in the graph:\n\n[The icon links to the corresponding Papers with Code page](https://preview.redd.it/6kk892mljhf61.png?width=401&amp;format=png&amp;auto=webp&amp;s=1ba0276a2076e08894f06225019c2469e678975e)\n\nWith the new additions, we hope Connected Papers will be even more helpful and accessible to Machine Learning researchers and we invite you to [try us out](https://www.connectedpapers.com/)!", "link": "https://www.reddit.com/r/MachineLearning/comments/lcj6rg/p_connected_papers_partners_with_arxiv_and_papers/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] connected papers partners with arxiv and papers with code /!/ hi everyone!  \nwe launched [connected papers](https://www.connectedpapers.com/) 7 months ago in this very subreddit, with the goal to help researchers visually find and explore academic papers.\n\n**input:** a paper of your liking.  \n**output:** a full interactive graph of similar papers to explore.\n\nfor example, here is the [graph for efficientnet](https://www.connectedpapers.com/main/4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9/efficientnet-rethinking-model-scaling-for-convolutional-neural-networks/graph):\n\nhttps://preview.redd.it/bb1dafwlihf61.png?width=1920&amp;format=png&amp;auto=webp&amp;s=e38203ef3a517029e814548e890ebb4a96e189ba\n\nsince launch we've been positively overwhelmed with feedback from the scientific community and half a million researchers using the -----> tool !!! .\n\n**today is a big milestone for us - we have partnered with** [**arxiv.org**](https://arxiv.org) **and from now on every paper page in arxiv will link to a corresponding connected papers graph.**\n\nit looks like this:\n\nhttps://preview.redd.it/8eps82s7ihf61.png?width=1250&amp;format=png&amp;auto=webp&amp;s=bff988753a09eea42b340504d77a8560a84f6772\n\nand works like this:\n\nhttps://i.redd.it/uu5qxcl8ihf61.gif\n\nin addition, we've lately integrated with the awesome team at [papers with code](https://paperswithcode.com/) and whenever a paper has a code implementation, it will be presented straight in the paper links in the graph:\n\n[the icon links to the corresponding papers with code page](https://preview.redd.it/6kk892mljhf61.png?width=401&amp;format=png&amp;auto=webp&amp;s=1ba0276a2076e08894f06225019c2469e678975e)\n\nwith the new additions, we hope connected papers will be even more helpful and accessible to machine learning researchers and we invite you to [try us out](https://www.connectedpapers.com/)!", "sortedWord": "None", "removed": "('nan',)", "score": 277, "comments": 32, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/lcj6rg/p_connected_papers_partners_with_arxiv_and_papers/',)", "identifyer": 5723947, "year": "2021"}, {"autor": "edbeeching", "date": 1634317967000, "content": "[P] Introducing Godot RL Agents /!/ We are proud to announce the release v0.1 of the [Godot RL Agents](https://github.com/edbeeching/godot_rl_agents) framework, a Deep Reinforcement Learning interface for the Godot Game Engine.\n\n[Overview trailer](https://youtu.be/g1MlZSFqIj4)\n\nThe objectives of the framework are to:\n\n*  Provide a free and open source tool for Deep RL research and game development.\n*  Enable game creators to imbue their non-player characters with unique behaviors.\n*  Allow for automated gameplay testing through interaction with an RL agent.\n\nThe library has a standard gym wrapper. Supports training of RL agents with [Ray rllib](https://docs.ray.io/en/latest/rllib.html) and [StableBaselines3](https://github.com/DLR-RM/stable-baselines3).\n\nYou will find out more on the GitHub repo [here](https://github.com/edbeeching/godot_rl_agents):\n\nWe look forward to community feedback as we continue to support this project. \nDisclaimer, this is not an official Godot project. Also the work is in beta, so please report any bugs you encounter.", "link": "https://www.reddit.com/r/MachineLearning/comments/q8sx0g/p_introducing_godot_rl_agents/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] introducing godot rl agents /!/ we are proud to announce the release v0.1 of the [godot rl agents](https://github.com/edbeeching/godot_rl_agents) framework, a deep reinforcement learning interface for the godot game engine.\n\n[overview trailer](https://youtu.be/g1mlzsfqij4)\n\nthe objectives of the framework are to:\n\n*  provide a free and open source -----> tool !!!  for deep rl research and game development.\n*  enable game creators to imbue their non-player characters with unique behaviors.\n*  allow for automated gameplay testing through interaction with an rl agent.\n\nthe library has a standard gym wrapper. supports training of rl agents with [ray rllib](https://docs.ray.io/en/latest/rllib.html) and [stablebaselines3](https://github.com/dlr-rm/stable-baselines3).\n\nyou will find out more on the github repo [here](https://github.com/edbeeching/godot_rl_agents):\n\nwe look forward to community feedback as we continue to support this project. \ndisclaimer, this is not an official godot project. also the work is in beta, so please report any bugs you encounter.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 13, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/q8sx0g/p_introducing_godot_rl_agents/',)", "identifyer": 5724133, "year": "2021"}, {"autor": "FlatPlate", "date": 1634316062000, "content": "[P] Nywspaper: comparing news using transformers /!/ Hello everyone,\n\nI have built [nywspaper](https://nywspaper.com), a news aggregator / reader / comparison tool for my bachelor's thesis, and I am very excited to share it with you here.\n\nThe goal of this tool is to make it easier for the readers to understand media bias in the news, by allowing paragraph by paragraph comparison between news articles covering the same story. When you're reading an article on nywspaper, if you click on a paragraph, you get paragraphs similar to the one you clicked from other publishers. This way you can see how a right wing news publisher delivers the same information differently than a left wing publisher.\n\nIn the main page you can see articles grouped by events, and you can just navigate to the article and begin comparing. There is also a feedback button in the similar paragraph boxes, if you particularly like or dislike a paragraph that was suggested.\n\nI am really looking forward to hearing your thoughts on this tool, and if it could be used to fight media bias. I would also hugely appreciate it if you could have a chance to fill out [this survey](https://www.questionpro.com/t/AT7qPZpHzt) after you use the tool (this would help for the thesis).\n\nThanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/q8sa64/p_nywspaper_comparing_news_using_transformers/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] nywspaper: comparing news using transformers /!/ hello everyone,\n\ni have built [nywspaper](https://nywspaper.com), a news aggregator / reader / comparison -----> tool !!!  for my bachelor's thesis, and i am very excited to share it with you here.\n\nthe goal of this tool is to make it easier for the readers to understand media bias in the news, by allowing paragraph by paragraph comparison between news articles covering the same story. when you're reading an article on nywspaper, if you click on a paragraph, you get paragraphs similar to the one you clicked from other publishers. this way you can see how a right wing news publisher delivers the same information differently than a left wing publisher.\n\nin the main page you can see articles grouped by events, and you can just navigate to the article and begin comparing. there is also a feedback button in the similar paragraph boxes, if you particularly like or dislike a paragraph that was suggested.\n\ni am really looking forward to hearing your thoughts on this tool, and if it could be used to fight media bias. i would also hugely appreciate it if you could have a chance to fill out [this survey](https://www.questionpro.com/t/at7qpzphzt) after you use the tool (this would help for the thesis).\n\nthanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 22, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/q8sa64/p_nywspaper_comparing_news_using_transformers/',)", "identifyer": 5724137, "year": "2021"}, {"autor": "the_growth_guy", "date": 1632901394000, "content": "[D] How do you choose a tool in building ML? /!/ Hey folks, since some days I have been overwhelmed by all the ML tools available out there.\n\nI understand that every tool has it's unique offering. But I want to understand what parameters do you'll evaluate a tool on. What information you'll consume to make the decision of choosing one tool over another.", "link": "https://www.reddit.com/r/MachineLearning/comments/pxqi23/d_how_do_you_choose_a_tool_in_building_ml/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] how do you choose a -----> tool !!!  in building ml? /!/ hey folks, since some days i have been overwhelmed by all the ml tools available out there.\n\ni understand that every tool has it's unique offering. but i want to understand what parameters do you'll evaluate a tool on. what information you'll consume to make the decision of choosing one tool over another.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/pxqi23/d_how_do_you_choose_a_tool_in_building_ml/',)", "identifyer": 5724258, "year": "2021"}, {"autor": "the_growth_guy", "date": 1632901199000, "content": "How do you choose a tool to build ML? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/pxqgop/how_do_you_choose_a_tool_to_build_ml/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "how do you choose a -----> tool !!!  to build ml? /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/pxqgop/how_do_you_choose_a_tool_to_build_ml/',)", "identifyer": 5724259, "year": "2021"}, {"autor": "stanmilc", "date": 1632875821000, "content": "DigiMax Global (OTC:$DBKSF, CNSX:$DIGI) to Launch CommodityHawk - AI Driven Commodity Price Trend Prediction Tool for Institutional Investors", "link": "https://www.reddit.com/r/MachineLearning/comments/pxjwho/digimax_global_otcdbksf_cnsxdigi_to_launch/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "digimax global (otc:$dbksf, cnsx:$digi) to launch commodityhawk - ai driven commodity price trend prediction -----> tool !!!  for institutional investors", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('link',)", "medialink": "('https://www.wsj.com/articles/digimax-to-launch-commodityhawk-ai-driven-commodity-price-trend-prediction-tool-for-institutional-investors-01632833437?tesla=y',)", "identifyer": 5724275, "year": "2021"}, {"autor": "freddygabbay", "date": 1621151787000, "content": "[R] MDPI mathematics - Special Issue on Computational Optimizations for Machine Learning /!/ Dear Colleagues,\n\nIn the recent decade, machine learning has emerged as an indispensable tool for incredible number of applications such as computer vision, medicine, fintech, autonomous systems, speech recognition, traffic management, social media, and many others. Machine learning models provide state-of-the-art and robust accuracy in various applications. The increasing deployment of machine learning algorithms introduces major computational challenges due to the explosive growth in model size and complexity. These challenges have been further emphasized due to the diverse hosting platforms from edge devices and cloud systems to high-performance computing. Given that each platform introduces different computational and cost constraints, the need for computational optimizations that are fine-tuned to the application and platform is crucial. This Special Issue looks for novel developments of computational optimizations for algorithms in the domain of machine learning algorithms such as:\n\nSupervised, unsupervised, reinforcement, and hybrid machine learning classes.  \nVarious types of machine learning algorithms: deep neural networks, convolutional neural networks, GANs, decision trees, linear regression, SVM, K-means clustering, Q-learning, temporal difference, deep adversarial networks, etc.  \nApplication-specific machine learning models.  \nMachine learning optimization methods such as pruning, deep compression, and others.\n\nLink to special issue submission: [https://www.mdpi.com/journal/mathematics/special\\_issues/Comput\\_Optim\\_Mach\\_Learn](https://www.mdpi.com/journal/mathematics/special_issues/Comput_Optim_Mach_Learn)\n\nProf. Dr. Freddy Gabbay  \nGuest Editor", "link": "https://www.reddit.com/r/MachineLearning/comments/ndjcyq/r_mdpi_mathematics_special_issue_on_computational/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[r] mdpi mathematics - special issue on computational optimizations for machine learning /!/ dear colleagues,\n\nin the recent decade, machine learning has emerged as an indispensable -----> tool !!!  for incredible number of applications such as computer vision, medicine, fintech, autonomous systems, speech recognition, traffic management, social media, and many others. machine learning models provide state-of-the-art and robust accuracy in various applications. the increasing deployment of machine learning algorithms introduces major computational challenges due to the explosive growth in model size and complexity. these challenges have been further emphasized due to the diverse hosting platforms from edge devices and cloud systems to high-performance computing. given that each platform introduces different computational and cost constraints, the need for computational optimizations that are fine-tuned to the application and platform is crucial. this special issue looks for novel developments of computational optimizations for algorithms in the domain of machine learning algorithms such as:\n\nsupervised, unsupervised, reinforcement, and hybrid machine learning classes.  \nvarious types of machine learning algorithms: deep neural networks, convolutional neural networks, gans, decision trees, linear regression, svm, k-means clustering, q-learning, temporal difference, deep adversarial networks, etc.  \napplication-specific machine learning models.  \nmachine learning optimization methods such as pruning, deep compression, and others.\n\nlink to special issue submission: [https://www.mdpi.com/journal/mathematics/special\\_issues/comput\\_optim\\_mach\\_learn](https://www.mdpi.com/journal/mathematics/special_issues/comput_optim_mach_learn)\n\nprof. dr. freddy gabbay  \nguest editor", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/ndjcyq/r_mdpi_mathematics_special_issue_on_computational/',)", "identifyer": 5724393, "year": "2021"}, {"autor": "manceraio", "date": 1629050523000, "content": "I made a tool for training a small NN that generates color palettes using Tensorflow.js", "link": "https://www.reddit.com/r/MachineLearning/comments/p4yems/i_made_a_tool_for_training_a_small_nn_that/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "i made a -----> tool !!!  for training a small nn that generates color palettes using tensorflow.js", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('link',)", "medialink": "('https://tailwind.ink/training',)", "identifyer": 5724461, "year": "2021"}, {"autor": "jack_oak_ai", "date": 1613075561000, "content": "[R] Cleora - the fastest graph &amp; hypergraph node embedding tool", "link": "https://www.reddit.com/r/MachineLearning/comments/lhtzth/r_cleora_the_fastest_graph_hypergraph_node/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[r] cleora - the fastest graph &amp; hypergraph node embedding -----> tool !!! ", "sortedWord": "None", "removed": "('reddit',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://arxiv.org/abs/2102.02302',)", "identifyer": 5724555, "year": "2021"}, {"autor": "apockill", "date": 1613072717000, "content": "[P] Introducing the BrainFrame Client: Now open source, a tool for visualizing real-time video-AI streams /!/ Hi all, \n\n[Announcement Blog](https://aotu.ai/en/blog/2021/02/10/open-sourcing-the-brainframe-client/)\n\nI\u2019m a developer at Aotu.ai, where we work on BrainFrame, a platform designed to simplify the deployment of AI models in real-time video streaming applications. BrainFrame allows machine learning experts to get their state-of-the-art technology deployed in real-world industry verticals without having to worry about scaling or how the end-user will interact with the product.\n\nToday we\u2019re open sourcing the BrainFrame client, which will enable people to build their own visual clients for different industry verticals. This, combined with the [OpenVisionCapsules](https://github.com/opencv/open_vision_capsules) model encapsulation format we contributed to OpenCV, will hopefully enable faster sharing of models in the real-time video analytics space. \n\nThanks for reading!\n\nSome more open source from our company can be found on [the Github](https://github.com/aotuai).", "link": "https://www.reddit.com/r/MachineLearning/comments/lhsulo/p_introducing_the_brainframe_client_now_open/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] introducing the brainframe client: now open source, a -----> tool !!!  for visualizing real-time video-ai streams /!/ hi all, \n\n[announcement blog](https://aotu.ai/en/blog/2021/02/10/open-sourcing-the-brainframe-client/)\n\ni\u2019m a developer at aotu.ai, where we work on brainframe, a platform designed to simplify the deployment of ai models in real-time video streaming applications. brainframe allows machine learning experts to get their state-of-the-art technology deployed in real-world industry verticals without having to worry about scaling or how the end-user will interact with the product.\n\ntoday we\u2019re open sourcing the brainframe client, which will enable people to build their own visual clients for different industry verticals. this, combined with the [openvisioncapsules](https://github.com/opencv/open_vision_capsules) model encapsulation format we contributed to opencv, will hopefully enable faster sharing of models in the real-time video analytics space. \n\nthanks for reading!\n\nsome more open source from our company can be found on [the github](https://github.com/aotuai).", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/lhsulo/p_introducing_the_brainframe_client_now_open/',)", "identifyer": 5724561, "year": "2021"}, {"autor": "yourboyrabbit", "date": 1613720304000, "content": "[P] Suppressing false alarms with capturing information from unstructured corpus. /!/  In our team, we had previously deployed a machine learning anomaly detection tool at a chemical plant. It has been observed in certain cases that ongoing manual operations/interventions at plant can result in process changes that might not be correctly captured by the currently deployed anomaly detection tool based on sensor data alone. In some of these cases, the tool ends up raising a false alarm. Our intention is to capture information related to these manual interventions from unstructured or semi-structured plant activity textual data records e.g. shift summary data, maintenance record data etc. and develop a plug-in tool which as a final goal helps in suppressing these false alarms to a certain extent. To this end, we plan to leverage natural language processing algorithms for text processing and retrieving critical information.\n\nThe first simplest thing that I can think of is to build a classifier model i.e. label the data that is dated from 0.5 \u2013 1.0 day before the occurrence of actual event as \u2018yes\u2019 and the rest as \u2018no\u2019 and train a simple binary classifier. It could be an idea here to try different deep learning architectures and compare them against baseline models. There could be challenges here though e.g. highly disbalanced classes, poor accuracy due to lack of semantically meaningful information in data etc.\n\nI am relatively new to Machine Learning and NLP and I would like to know what would you be thinking of if you were in my place. What would you like to do with this problem? Through this post, I am keen to get some valuable inputs or directions that I can take.", "link": "https://www.reddit.com/r/MachineLearning/comments/lna3pd/p_suppressing_false_alarms_with_capturing/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] suppressing false alarms with capturing information from unstructured corpus. /!/  in our team, we had previously deployed a machine learning anomaly detection -----> tool !!!  at a chemical plant. it has been observed in certain cases that ongoing manual operations/interventions at plant can result in process changes that might not be correctly captured by the currently deployed anomaly detection tool based on sensor data alone. in some of these cases, the tool ends up raising a false alarm. our intention is to capture information related to these manual interventions from unstructured or semi-structured plant activity textual data records e.g. shift summary data, maintenance record data etc. and develop a plug-in tool which as a final goal helps in suppressing these false alarms to a certain extent. to this end, we plan to leverage natural language processing algorithms for text processing and retrieving critical information.\n\nthe first simplest thing that i can think of is to build a classifier model i.e. label the data that is dated from 0.5 \u2013 1.0 day before the occurrence of actual event as \u2018yes\u2019 and the rest as \u2018no\u2019 and train a simple binary classifier. it could be an idea here to try different deep learning architectures and compare them against baseline models. there could be challenges here though e.g. highly disbalanced classes, poor accuracy due to lack of semantically meaningful information in data etc.\n\ni am relatively new to machine learning and nlp and i would like to know what would you be thinking of if you were in my place. what would you like to do with this problem? through this post, i am keen to get some valuable inputs or directions that i can take.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/lna3pd/p_suppressing_false_alarms_with_capturing/',)", "identifyer": 5724641, "year": "2021"}, {"autor": "surgeai", "date": 1614207388000, "content": "[P] Feedback for our data labeling tool /!/ Hey r/MachineLearning readers!\n\nWe\u2019re a group of ML engineers from Google and YC building a new data labeling platform, [Surge](https://www.surgehq.ai). It\u2019s a lot like Mechanical Turk -- except we help you with label quality, provide better tools, and allow you to label data yourself or with your team.\n\nWe\u2019ve historically operated as an enterprise managed service and have some great customers, but we're hoping to spread to more developers and expand our self-serve product. So we\u2019d love to learn about your data labeling needs to drive our roadmap! If you work on ML and are interested in a better labeling tool, please fill out this survey: [https://forms.gle/vDoL9hpUBDMndfm57](https://forms.gle/vDoL9hpUBDMndfm57)\n\nWe\u2019re also giving away $50 Amazon gift cards to anyone interested in jumping on a 30m phone call with us.\n\nIf you\u2019ve made it this far, thanks a lot for reading. We\u2019re looking forward to hearing your thoughts!", "link": "https://www.reddit.com/r/MachineLearning/comments/lrpjdv/p_feedback_for_our_data_labeling_tool/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] feedback for our data labeling -----> tool !!!  /!/ hey r/machinelearning readers!\n\nwe\u2019re a group of ml engineers from google and yc building a new data labeling platform, [surge](https://www.surgehq.ai). it\u2019s a lot like mechanical turk -- except we help you with label quality, provide better tools, and allow you to label data yourself or with your team.\n\nwe\u2019ve historically operated as an enterprise managed service and have some great customers, but we're hoping to spread to more developers and expand our self-serve product. so we\u2019d love to learn about your data labeling needs to drive our roadmap! if you work on ml and are interested in a better labeling tool, please fill out this survey: [https://forms.gle/vdol9hpubdmndfm57](https://forms.gle/vdol9hpubdmndfm57)\n\nwe\u2019re also giving away $50 amazon gift cards to anyone interested in jumping on a 30m phone call with us.\n\nif you\u2019ve made it this far, thanks a lot for reading. we\u2019re looking forward to hearing your thoughts!", "sortedWord": "None", "removed": "('nan',)", "score": 8, "comments": 6, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/lrpjdv/p_feedback_for_our_data_labeling_tool/',)", "identifyer": 5724770, "year": "2021"}, {"autor": "davidbun", "date": 1615218011000, "content": "[P] #2 in daily trends on GitHub - Version, collaborate and stream your data /!/ Hi r/MachineLearning,  \n[github.com/activeloopai/Hub](https://github.com/activeloopai/Hub) (trended #2 on the entire GitHub and #1 in Python last month)!   \nMy team and I at Activeloop (activeloop.ai) are working on unifying storage for datasets. We make unstructured dataset of any size accessible from any machine at any scale, and seamlessly stream data to machine learning frameworks like PyTorch and TF, as if it were local.\n\nIn our latest release, we\u2019ve added ability to create different versions of datasets in a manner similar to git versioning. These versions are not full copies but rather they keep track of differences between versions and are thus stored very efficiently. Unlike git, this isn\u2019t a CLI tool, but rather a python API.\n\nYou can get the our latest stable version by:-\n\npip3 install hub ([Documentation](https://docs.activeloop.ai/en/latest/))\n\n**How it works**\n\nHub datasets are always stored in a chunk wise manner. This allows us to store and load the data optimally. While versioning, Hub only creates copies of those chunks that are modified after the previous commit, the rest of the chunks are fetched from previous commits.\n\n**What can I do with Hub versioning currently?**\n\n* Modify dataset elements across different versions\n* Seamlessly switch between versions\n\nFeatures coming in the future\n\n* Modify schema across versions (add or remove Tensors)\n* Track versions across transforms\n* Delete branches\n* Your suggestions!\n\n**Benefits of Hub:**\n\n1. Create large datasets with huge (105 x 105) size arrays and store locally, on hub storage, or any cloud.\n2. Easily access and visualize any slice of the dataset without downloading the entire dataset.\n3. **(new) Collaborate with your team on the same dataset.**\n4. **(new) Version control the dataset from the API itself.**\n5. **(new) Filter datasets to only get the samples you need.**\n6. Create data pipelines and transform the data.\n7. Directly plug Hub datasets into tensorflow and pytorch and start training\n8. **(new) Transfer datasets across different locations easily**\n\n**A note regarding other git-like tools out there:** we deeply respect other projects that try to make data scientists\u2019 lives easier and strive to create git-like versioning for datasets. It is very important for reproducibility of experiments - and it is great to see other projects working to make that happen. In our opinion, file system-based diffs are difficult to manage. Unlike in git, where each line change by a developer entails meaning, modifying a line in blob doesn't contain the abstraction data scientist might need to analyze data changes. Our new method provides tensor-delta operation to help you seamlessly keep track of dataset modifications.", "link": "https://www.reddit.com/r/MachineLearning/comments/m0ibzf/p_2_in_daily_trends_on_github_version_collaborate/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] #2 in daily trends on github - version, collaborate and stream your data /!/ hi r/machinelearning,  \n[github.com/activeloopai/hub](https://github.com/activeloopai/hub) (trended #2 on the entire github and #1 in python last month)!   \nmy team and i at activeloop (activeloop.ai) are working on unifying storage for datasets. we make unstructured dataset of any size accessible from any machine at any scale, and seamlessly stream data to machine learning frameworks like pytorch and tf, as if it were local.\n\nin our latest release, we\u2019ve added ability to create different versions of datasets in a manner similar to git versioning. these versions are not full copies but rather they keep track of differences between versions and are thus stored very efficiently. unlike git, this isn\u2019t a cli -----> tool !!! , but rather a python api.\n\nyou can get the our latest stable version by:-\n\npip3 install hub ([documentation](https://docs.activeloop.ai/en/latest/))\n\n**how it works**\n\nhub datasets are always stored in a chunk wise manner. this allows us to store and load the data optimally. while versioning, hub only creates copies of those chunks that are modified after the previous commit, the rest of the chunks are fetched from previous commits.\n\n**what can i do with hub versioning currently?**\n\n* modify dataset elements across different versions\n* seamlessly switch between versions\n\nfeatures coming in the future\n\n* modify schema across versions (add or remove tensors)\n* track versions across transforms\n* delete branches\n* your suggestions!\n\n**benefits of hub:**\n\n1. create large datasets with huge (105 x 105) size arrays and store locally, on hub storage, or any cloud.\n2. easily access and visualize any slice of the dataset without downloading the entire dataset.\n3. **(new) collaborate with your team on the same dataset.**\n4. **(new) version control the dataset from the api itself.**\n5. **(new) filter datasets to only get the samples you need.**\n6. create data pipelines and transform the data.\n7. directly plug hub datasets into tensorflow and pytorch and start training\n8. **(new) transfer datasets across different locations easily**\n\n**a note regarding other git-like tools out there:** we deeply respect other projects that try to make data scientists\u2019 lives easier and strive to create git-like versioning for datasets. it is very important for reproducibility of experiments - and it is great to see other projects working to make that happen. in our opinion, file system-based diffs are difficult to manage. unlike in git, where each line change by a developer entails meaning, modifying a line in blob doesn't contain the abstraction data scientist might need to analyze data changes. our new method provides tensor-delta operation to help you seamlessly keep track of dataset modifications.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 28, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/m0ibzf/p_2_in_daily_trends_on_github_version_collaborate/',)", "identifyer": 5724900, "year": "2021"}, {"autor": "momorpheus", "date": 1615217704000, "content": "[P] #2 in daily trends on GitHub - Version, collaborate and stream your data /!/  Hi r/MachineLearning,  \n[github.com/activeloopai/Hub](https://github.com/activeloopai/Hub) (trended #2 on the entire GitHub last month, and #1 in Python)!   \nMy team and I at Activeloop (activeloop.ai) are working on unifying storage for datasets. We make unstructured dataset of any size accessible from any machine at any scale, and seamlessly stream data to machine learning frameworks like PyTorch and TF, as if it were local.\n\nIn our latest release, we\u2019ve added ability to create different versions of datasets in a manner similar to git versioning. These versions are not full copies but rather they keep track of differences between versions and are thus stored very efficiently. Unlike git, this isn\u2019t a CLI tool, but rather a python API.\n\nYou can get the our latest stable version by:-\n\npip3 install hub ([Documentation](https://docs.activeloop.ai/en/latest/))\n\n**How it works**\n\nHub datasets are always stored in a chunk wise manner. This allows us to store and load the data optimally. While versioning, Hub only creates copies of those chunks that are modified after the previous commit, the rest of the chunks are fetched from previous commits.\n\n**What can I do with Hub versioning currently?**\n\n* Modify dataset elements across different versions\n* Seamlessly switch between versions\n\nFeatures coming in the future\n\n* Modify schema across versions (add or remove Tensors)\n* Track versions across transforms\n* Delete branches\n* Your suggestions!\n\n**Benefits of Hub:**\n\n1. Create large datasets with huge (105 x 105) size arrays and store locally, on hub storage, or any cloud.\n2. Easily access and visualize any slice of the dataset without downloading the entire dataset.\n3. **(new)** Collaborate with your team on the same dataset.\n4. **(new)** Version control the dataset from the API itself.\n5. **(new)** Filter datasets to only get the samples you need.\n6. Create data pipelines and transform the data.\n7. Directly plug Hub datasets into tensorflow and pytorch and start training\n8. **(new)** Transfer datasets across different locations easily\n\n**A note regarding other git-like tools out there:** we deeply respect other projects that try to make data scientists\u2019 lives easier and strive to create git-like versioning for datasets. It is very important for reproducibility of experiments - and it is great to see other projects working to make that happen. In our opinion, file system-based diffs are difficult to manage. Unlike in git, where each line change by a developer entails meaning, modifying a line in blob doesn't contain the abstraction data scientist might need to analyze data changes. Our new method provides tensor-delta operation to help you seamlessly keep track of dataset modifications.", "link": "https://www.reddit.com/r/MachineLearning/comments/m0i7z5/p_2_in_daily_trends_on_github_version_collaborate/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] #2 in daily trends on github - version, collaborate and stream your data /!/  hi r/machinelearning,  \n[github.com/activeloopai/hub](https://github.com/activeloopai/hub) (trended #2 on the entire github last month, and #1 in python)!   \nmy team and i at activeloop (activeloop.ai) are working on unifying storage for datasets. we make unstructured dataset of any size accessible from any machine at any scale, and seamlessly stream data to machine learning frameworks like pytorch and tf, as if it were local.\n\nin our latest release, we\u2019ve added ability to create different versions of datasets in a manner similar to git versioning. these versions are not full copies but rather they keep track of differences between versions and are thus stored very efficiently. unlike git, this isn\u2019t a cli -----> tool !!! , but rather a python api.\n\nyou can get the our latest stable version by:-\n\npip3 install hub ([documentation](https://docs.activeloop.ai/en/latest/))\n\n**how it works**\n\nhub datasets are always stored in a chunk wise manner. this allows us to store and load the data optimally. while versioning, hub only creates copies of those chunks that are modified after the previous commit, the rest of the chunks are fetched from previous commits.\n\n**what can i do with hub versioning currently?**\n\n* modify dataset elements across different versions\n* seamlessly switch between versions\n\nfeatures coming in the future\n\n* modify schema across versions (add or remove tensors)\n* track versions across transforms\n* delete branches\n* your suggestions!\n\n**benefits of hub:**\n\n1. create large datasets with huge (105 x 105) size arrays and store locally, on hub storage, or any cloud.\n2. easily access and visualize any slice of the dataset without downloading the entire dataset.\n3. **(new)** collaborate with your team on the same dataset.\n4. **(new)** version control the dataset from the api itself.\n5. **(new)** filter datasets to only get the samples you need.\n6. create data pipelines and transform the data.\n7. directly plug hub datasets into tensorflow and pytorch and start training\n8. **(new)** transfer datasets across different locations easily\n\n**a note regarding other git-like tools out there:** we deeply respect other projects that try to make data scientists\u2019 lives easier and strive to create git-like versioning for datasets. it is very important for reproducibility of experiments - and it is great to see other projects working to make that happen. in our opinion, file system-based diffs are difficult to manage. unlike in git, where each line change by a developer entails meaning, modifying a line in blob doesn't contain the abstraction data scientist might need to analyze data changes. our new method provides tensor-delta operation to help you seamlessly keep track of dataset modifications.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/m0i7z5/p_2_in_daily_trends_on_github_version_collaborate/',)", "identifyer": 5724901, "year": "2021"}, {"autor": "raven3813", "date": 1615827352000, "content": "[Project] Neural Image Editor: GAN-based image editing tool /!/ &amp;#x200B;\n\n*Processing video marun5ze28n61...*\n\nWe release the neural image editor [demo](https://colab.research.google.com/github/raven38/image_edit/blob/master/image_edit_demo.ipynb). It combines [Image2StyleGAN](https://arxiv.org/abs/1904.03189) and [SeFa](https://genforce.github.io/sefa/#demo). You upload your own image and you can edit attributes of the image. It only supports face images now.  \n\nWelcome any suggestion and comment including the idea that improves the software and similar project or research.\n\nGithub: [https://github.com/raven38/image\\_edit](https://github.com/raven38/image_edit)", "link": "https://www.reddit.com/r/MachineLearning/comments/m5p6sg/project_neural_image_editor_ganbased_image/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[project] neural image editor: gan-based image editing -----> tool !!!  /!/ &amp;#x200b;\n\n*processing video marun5ze28n61...*\n\nwe release the neural image editor [demo](https://colab.research.google.com/github/raven38/image_edit/blob/master/image_edit_demo.ipynb). it combines [image2stylegan](https://arxiv.org/abs/1904.03189) and [sefa](https://genforce.github.io/sefa/#demo). you upload your own image and you can edit attributes of the image. it only supports face images now.  \n\nwelcome any suggestion and comment including the idea that improves the software and similar project or research.\n\ngithub: [https://github.com/raven38/image\\_edit](https://github.com/raven38/image_edit)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/m5p6sg/project_neural_image_editor_ganbased_image/',)", "identifyer": 5724954, "year": "2021"}, {"autor": "Notonlycs", "date": 1615766640000, "content": "Debugging Machine Learning \"[N]\" /!/  [http://erroranalysis.ai/](http://erroranalysis.ai/) is a new open-source tool for in-depth understanding and diagnosis of Machine Learning Errors. The tool is available as a highly interactive jupyter widget and brings in several visualization primitives all centered around debugging activities in ML (main repo: [https://github.com/microsoft/responsible-ai-widgets](https://github.com/microsoft/responsible-ai-widgets)).\n\nLonger blog available at: [Responsible Machine Learning with Error Analysis | by Besmira Nushi | Mar, 2021 | Towards Data Science](https://towardsdatascience.com/responsible-machine-learning-with-error-analysis-a7553f649915)", "link": "https://www.reddit.com/r/MachineLearning/comments/m57xh3/debugging_machine_learning_n/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "debugging machine learning \"[n]\" /!/  [http://erroranalysis.ai/](http://erroranalysis.ai/) is a new open-source -----> tool !!!  for in-depth understanding and diagnosis of machine learning errors. the tool is available as a highly interactive jupyter widget and brings in several visualization primitives all centered around debugging activities in ml (main repo: [https://github.com/microsoft/responsible-ai-widgets](https://github.com/microsoft/responsible-ai-widgets)).\n\nlonger blog available at: [responsible machine learning with error analysis | by besmira nushi | mar, 2021 | towards data science](https://towardsdatascience.com/responsible-machine-learning-with-error-analysis-a7553f649915)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/m57xh3/debugging_machine_learning_n/',)", "identifyer": 5725005, "year": "2021"}, {"autor": "calebkaiser", "date": 1609954102000, "content": "[P] Cortex 0.26: Beta support for GCP, micro batching, and more /!/ Repo Link: [https://github.com/cortexlabs/cortex](https://github.com/cortexlabs/cortex)\n\nWe've just released version 0.26 of Cortex, our open source deployment platform. A good deal of the work in this version was inspired not just by Cortex users, but also suggestions from r/MachineLearning, so let me quickly say \"Thank you\" here before I get into the release notes.\n\nWith 0.26, we're excited to announce that Cortex has:\n\n* Continued beta support for GCP deployments (more on that below)\n* Support for server-side micro batching\n* Improved logging, particularly on the API level\n\nYou can see the full release notes [here](https://github.com/cortexlabs/cortex/releases/tag/v0.26.0).\n\nIf you're unfamiliar with Cortex, it is an MLOps platform built to make it easy to run inference at scale, providing abstraction and automation for everything from model serving to cloud infrastructure. \n\nCortex supports realtime and batch inference for models from all Pythonic frameworks, can perform inference on CPUs/GPUs/ASICs, and provides a number of ML-relevant features out of the box including request-based autoscaling, multi-model caching, spot instance support, built-in clients for TF Serving/ONNX Runtime, deployment versioning + logging, and more. \n\nImportantly, though Cortex provides opinionated default behavior for these features (in the interest of being easy to setup), it also provides simple interfaces for defining custom behaviors, and is designed to integrate with any other tool/platform by default.\n\nIf you're deploying models to production, I'd love to hear any feedback you have about Cortex. As I said at the top, many of our features (including our prioritization of GCP support) are a direct result of feedback from people in r/MachineLearning, and we really appreciate all of you taking the time to share your thoughts as we've developed the platform.\n\n**NOTE ON GCP SUPPORT:**\n\nWe're calling this initial GCP support a beta. By that, we mean that currently, GCP deployments do not have full feature parity with AWS deployments on Cortex. As of now, GCP deployments support:\n\n* All three predictor types (Python, TensorFlow, ONNX)\n* CPU and GPU inference\n* Live reloading\n* Multi-model caching\n* Cluster autoscaling\n\nAnd in the immediate future (as in the next couple weeks), GCP deployments will also support replica-level autoscaling, just as AWS deployments currently do.\n\nAs we bring GCP deployments to feature parity with AWS deployments, we want to do it in a way that prioritizes the most important features, and implements them with respect to the specifics of developing on GCP. If you are interested in trying out GCP deployments, we would really appreciate your feedback here\u2014What features would you\u2019d like us to prioritize? What are the GCP quirks you hate dealing with the most?", "link": "https://www.reddit.com/r/MachineLearning/comments/krsjof/p_cortex_026_beta_support_for_gcp_micro_batching/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] cortex 0.26: beta support for gcp, micro batching, and more /!/ repo link: [https://github.com/cortexlabs/cortex](https://github.com/cortexlabs/cortex)\n\nwe've just released version 0.26 of cortex, our open source deployment platform. a good deal of the work in this version was inspired not just by cortex users, but also suggestions from r/machinelearning, so let me quickly say \"thank you\" here before i get into the release notes.\n\nwith 0.26, we're excited to announce that cortex has:\n\n* continued beta support for gcp deployments (more on that below)\n* support for server-side micro batching\n* improved logging, particularly on the api level\n\nyou can see the full release notes [here](https://github.com/cortexlabs/cortex/releases/tag/v0.26.0).\n\nif you're unfamiliar with cortex, it is an mlops platform built to make it easy to run inference at scale, providing abstraction and automation for everything from model serving to cloud infrastructure. \n\ncortex supports realtime and batch inference for models from all pythonic frameworks, can perform inference on cpus/gpus/asics, and provides a number of ml-relevant features out of the box including request-based autoscaling, multi-model caching, spot instance support, built-in clients for tf serving/onnx runtime, deployment versioning + logging, and more. \n\nimportantly, though cortex provides opinionated default behavior for these features (in the interest of being easy to setup), it also provides simple interfaces for defining custom behaviors, and is designed to integrate with any other -----> tool !!! /platform by default.\n\nif you're deploying models to production, i'd love to hear any feedback you have about cortex. as i said at the top, many of our features (including our prioritization of gcp support) are a direct result of feedback from people in r/machinelearning, and we really appreciate all of you taking the time to share your thoughts as we've developed the platform.\n\n**note on gcp support:**\n\nwe're calling this initial gcp support a beta. by that, we mean that currently, gcp deployments do not have full feature parity with aws deployments on cortex. as of now, gcp deployments support:\n\n* all three predictor types (python, tensorflow, onnx)\n* cpu and gpu inference\n* live reloading\n* multi-model caching\n* cluster autoscaling\n\nand in the immediate future (as in the next couple weeks), gcp deployments will also support replica-level autoscaling, just as aws deployments currently do.\n\nas we bring gcp deployments to feature parity with aws deployments, we want to do it in a way that prioritizes the most important features, and implements them with respect to the specifics of developing on gcp. if you are interested in trying out gcp deployments, we would really appreciate your feedback here\u2014what features would you\u2019d like us to prioritize? what are the gcp quirks you hate dealing with the most?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/krsjof/p_cortex_026_beta_support_for_gcp_micro_batching/',)", "identifyer": 5725196, "year": "2021"}, {"autor": "Family-456", "date": 1617848886000, "content": "[Newbie] tool/approach to describe the findings based on data in a spreadsheet /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/mmhzl3/newbie_toolapproach_to_describe_the_findings/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[newbie] -----> tool !!! /approach to describe the findings based on data in a spreadsheet /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/mmhzl3/newbie_toolapproach_to_describe_the_findings/',)", "identifyer": 5725282, "year": "2021"}, {"autor": "jordanearth", "date": 1621778603000, "content": "[P] Can I hire someone to build a tool that would auto crop and straighten thousands of images in one click? No software has been able to do this with the accuracy I'm looking for.", "link": "https://www.reddit.com/r/MachineLearning/comments/nj8dvj/p_can_i_hire_someone_to_build_a_tool_that_would/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] can i hire someone to build a -----> tool !!!  that would auto crop and straighten thousands of images in one click? no software has been able to do this with the accuracy i'm looking for.", "sortedWord": "None", "removed": "('reddit',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/gallery/nj8dvj',)", "identifyer": 5725457, "year": "2021"}, {"autor": "jordanearth", "date": 1621778461000, "content": "Is there a tool out there that can auto crop and straighten thousands of images in one click? No software has been able to do this with the accuracy I'm looking for.", "link": "https://www.reddit.com/r/MachineLearning/comments/nj8c6c/is_there_a_tool_out_there_that_can_auto_crop_and/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "is there a -----> tool !!!  out there that can auto crop and straighten thousands of images in one click? no software has been able to do this with the accuracy i'm looking for.", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/gallery/nj8c6c',)", "identifyer": 5725458, "year": "2021"}, {"autor": "jordanearth", "date": 1621778365000, "content": "Can I hire someone to build a tool that would auto crop and straighten thousands of images in one click? No software has been able to do this with the accuracy I'm looking for.", "link": "https://www.reddit.com/r/MachineLearning/comments/nj8awr/can_i_hire_someone_to_build_a_tool_that_would/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "can i hire someone to build a -----> tool !!!  that would auto crop and straighten thousands of images in one click? no software has been able to do this with the accuracy i'm looking for.", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/gallery/nj8awr',)", "identifyer": 5725459, "year": "2021"}, {"autor": "AdelSexy", "date": 1621755174000, "content": "[D] ML/DL job hunting. Points if attention. /!/ Hey guys! This is my post about job hunting in ML/DL area. Original is [here](https://irregularadel.substack.com/p/mldl-job-hunting-points-if-attention).   \nML/DL positions are growing in numbers very fast, and most of the time it is hard to understand if the position you are applying to worth it and if you and the company are a good match. Just look at this search of machine learning or data scientist in Linkein - over 300k positions. \n\n[ AI jobs everywhere. I know some part of this 300k+ jobs are PMs and stuff, but it's still a lot. ](https://preview.redd.it/7j07bmhdnt071.png?width=1090&amp;format=png&amp;auto=webp&amp;s=87a4c47cf2ec74f5ad5f62ebb40300307cd94ba5)\n\nOf course, there are tasty positions in google, Deep mind, Amazon, Apple, Tesla, etc. But it is hard to get there, and, probably, even harder to work. Meanwhile, there a lot of other companies and positions, that can be quite a good option. The problem is - the field is so hyped, that some positions are crap and some are surprisingly good while it is hard to find out which is what.\n\nTo help in that complex choice, I designed these 6 points of attention while job hunting in the ML area. Using them, you can ease this hard and exhausting process and find a good matching position.\n\n# Step 0. Define what you truly want.\n\nAsk yourself where your passion lies. Is it fundamental research? Applied research? Engineering? What field is it: computer vision, natural language processing, time series analyses? Or maybe recommendation systems? From answers to that questions, you can define directions. You will get an idea, where you want to work: academia, corporation, start-up? Do you want to develop satelite image analysis neural networks for a small growing start-up with a production-focused team? Or maybe you want to develop text/speech recognition models for internal use by the support department in a big retail corporation? What about in-depth research of generative models foundations?\n\nYou name it.\n\nThis step is essential - it will help you to get in peace with yourself, narrow down the search area, define criteria for a future position. To be honest, I think this is already 80% of success.\n\nAlso, make sure what companies understand under the name of the position you are interested in. You might be surprised how many different definitions are there for ML engineers, or, even worse, Data scientists. Try to get on the same page with the future employer. You don't want to program web applications, while you thought you will train neural networks. (I wish I were exaggerating, but this is a real example).\n\nNow, suppose you have all the answers. You applied to 20+ positions and started interviewing process on some of them.\n\n# Step 1. Ask about AI strategy.\n\nDoes the company has an AI roadmap? If yes, ask to elaborate on it. What are the goals for 5 years? How they are planning to get benefit from ML/DL. How essential is it for a company? What are the main development directions?\n\n[ Let me rephrase the classic ](https://preview.redd.it/lrx28vkmnt071.png?width=858&amp;format=png&amp;auto=webp&amp;s=d223895c84bf9f589bfda3bc45978e2f3a761a8c)\n\n If there is no AI strategy in the compay, it is not necessarily a bad thing. It might be, that company thinks about it and ready to work on a long-term vision. And maybe you will be one of the pioneers. Do you want it? Or you want to work mostly on content and not be involved in high-level roadmaps development? The choice is yours.\n\nBut in any way, please. make sure that there is no \"ML for ML\". There should be the business value that will bring need in ML.   \n\n# Step 2. Ask about data governance\n\nThat's super important since you can't work without good data governance. So it would be wise to know from the beginning what databases are there, where the data is coming from, how big is the data? Is there any annotation tool the company uses? If not, are they willing to pay attention to that (and that's the bridge to AI strategy)? How many training-ready datasets do they have? Do they have version control of the data?\n\n&amp;#x200B;\n\n[ Love this slide from one of lectures of Andrey Karpathy - Director of AI at Tesla ](https://preview.redd.it/pzvml5utnt071.png?width=1988&amp;format=png&amp;auto=webp&amp;s=b3d7bd1aaf4793bcf996e8e9acf32c2d6c07f717)\n\n Always remember garbage in -&gt; garbage out. Your comfort and effectiveness depend on that. \n\n# Step 3. Ask about the team: roles, plans for positions, working style.\n\nYou do want to know who your potential colleagues are. Are there any ML/DL experts? Newbies? Does the team have interns from time to time? Any data engineers? Full-stack developers? Is the team centralized or there are islands of expertise across the company? How many meetings do they have? What meeting? Growth plans? Do they visit conferences? Attend in them? What they wait for from you?\n\nYou have the right to understand your place in the team and the company before you go there to work. You should see your growth potential.\n\nFrom the answers, you can get an idea of how connected the team is as well as if you will fit in there.\n\n# Step 4. Any success stories?\n\nWhat has already been done in the company in the ML/DL field? Are there any working solutions? Use cases? Proves of concepts? That can give you an understanding of what to expect. You can easily get in the working rhythm if it already beats. Or you can try to create your own if there are conditions for it.\n\nSuccess stories can also indicate that the company knows the main customers/stakeholders, which is already a great achievement.\n\n# Step 5. Stack the company uses\n\nCheck the technology stack the team is using. Try to understand if they know, what they are doing. And if they don\u2019t, check if they realize that.\n\nMake sure the team has structured methods of research/production or at least is willing to have one.\n\n[ Just look at this giant landscape - credits goes to Full-Stack Deep learning course. ](https://preview.redd.it/662i8nkynt071.png?width=1952&amp;format=png&amp;auto=webp&amp;s=db9088dc32b227cfed688bd2ed2c0bf045a8a38f)\n\n How do they track their experiments? What frameworks/packages do they use? Is their code is just a pile of Jupiter notebooks? Do they work in their MLops? How they store and deploy the models? Who owns the models?  \nThis is the base for the successfull work, don\u2019t underestimate it\u2019s influence.\n\nAs a comclusion, I believe that there are ton\u2019s of beautiful machine learning positions out there. I know by my own skin, that job hunting can be exhasting and can suck all fun of your life. An I hope these few points of attention can help you ease the pain.\n\nGood luck!\n\nP.S. have a look at my blog [here.](https://irregularadel.substack.com/about)", "link": "https://www.reddit.com/r/MachineLearning/comments/nj2n0q/d_mldl_job_hunting_points_if_attention/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] ml/dl job hunting. points if attention. /!/ hey guys! this is my post about job hunting in ml/dl area. original is [here](https://irregularadel.substack.com/p/mldl-job-hunting-points-if-attention).   \nml/dl positions are growing in numbers very fast, and most of the time it is hard to understand if the position you are applying to worth it and if you and the company are a good match. just look at this search of machine learning or data scientist in linkein - over 300k positions. \n\n[ ai jobs everywhere. i know some part of this 300k+ jobs are pms and stuff, but it's still a lot. ](https://preview.redd.it/7j07bmhdnt071.png?width=1090&amp;format=png&amp;auto=webp&amp;s=87a4c47cf2ec74f5ad5f62ebb40300307cd94ba5)\n\nof course, there are tasty positions in google, deep mind, amazon, apple, tesla, etc. but it is hard to get there, and, probably, even harder to work. meanwhile, there a lot of other companies and positions, that can be quite a good option. the problem is - the field is so hyped, that some positions are crap and some are surprisingly good while it is hard to find out which is what.\n\nto help in that complex choice, i designed these 6 points of attention while job hunting in the ml area. using them, you can ease this hard and exhausting process and find a good matching position.\n\n# step 0. define what you truly want.\n\nask yourself where your passion lies. is it fundamental research? applied research? engineering? what field is it: computer vision, natural language processing, time series analyses? or maybe recommendation systems? from answers to that questions, you can define directions. you will get an idea, where you want to work: academia, corporation, start-up? do you want to develop satelite image analysis neural networks for a small growing start-up with a production-focused team? or maybe you want to develop text/speech recognition models for internal use by the support department in a big retail corporation? what about in-depth research of generative models foundations?\n\nyou name it.\n\nthis step is essential - it will help you to get in peace with yourself, narrow down the search area, define criteria for a future position. to be honest, i think this is already 80% of success.\n\nalso, make sure what companies understand under the name of the position you are interested in. you might be surprised how many different definitions are there for ml engineers, or, even worse, data scientists. try to get on the same page with the future employer. you don't want to program web applications, while you thought you will train neural networks. (i wish i were exaggerating, but this is a real example).\n\nnow, suppose you have all the answers. you applied to 20+ positions and started interviewing process on some of them.\n\n# step 1. ask about ai strategy.\n\ndoes the company has an ai roadmap? if yes, ask to elaborate on it. what are the goals for 5 years? how they are planning to get benefit from ml/dl. how essential is it for a company? what are the main development directions?\n\n[ let me rephrase the classic ](https://preview.redd.it/lrx28vkmnt071.png?width=858&amp;format=png&amp;auto=webp&amp;s=d223895c84bf9f589bfda3bc45978e2f3a761a8c)\n\n if there is no ai strategy in the compay, it is not necessarily a bad thing. it might be, that company thinks about it and ready to work on a long-term vision. and maybe you will be one of the pioneers. do you want it? or you want to work mostly on content and not be involved in high-level roadmaps development? the choice is yours.\n\nbut in any way, please. make sure that there is no \"ml for ml\". there should be the business value that will bring need in ml.   \n\n# step 2. ask about data governance\n\nthat's super important since you can't work without good data governance. so it would be wise to know from the beginning what databases are there, where the data is coming from, how big is the data? is there any annotation -----> tool !!!  the company uses? if not, are they willing to pay attention to that (and that's the bridge to ai strategy)? how many training-ready datasets do they have? do they have version control of the data?\n\n&amp;#x200b;\n\n[ love this slide from one of lectures of andrey karpathy - director of ai at tesla ](https://preview.redd.it/pzvml5utnt071.png?width=1988&amp;format=png&amp;auto=webp&amp;s=b3d7bd1aaf4793bcf996e8e9acf32c2d6c07f717)\n\n always remember garbage in -&gt; garbage out. your comfort and effectiveness depend on that. \n\n# step 3. ask about the team: roles, plans for positions, working style.\n\nyou do want to know who your potential colleagues are. are there any ml/dl experts? newbies? does the team have interns from time to time? any data engineers? full-stack developers? is the team centralized or there are islands of expertise across the company? how many meetings do they have? what meeting? growth plans? do they visit conferences? attend in them? what they wait for from you?\n\nyou have the right to understand your place in the team and the company before you go there to work. you should see your growth potential.\n\nfrom the answers, you can get an idea of how connected the team is as well as if you will fit in there.\n\n# step 4. any success stories?\n\nwhat has already been done in the company in the ml/dl field? are there any working solutions? use cases? proves of concepts? that can give you an understanding of what to expect. you can easily get in the working rhythm if it already beats. or you can try to create your own if there are conditions for it.\n\nsuccess stories can also indicate that the company knows the main customers/stakeholders, which is already a great achievement.\n\n# step 5. stack the company uses\n\ncheck the technology stack the team is using. try to understand if they know, what they are doing. and if they don\u2019t, check if they realize that.\n\nmake sure the team has structured methods of research/production or at least is willing to have one.\n\n[ just look at this giant landscape - credits goes to full-stack deep learning course. ](https://preview.redd.it/662i8nkynt071.png?width=1952&amp;format=png&amp;auto=webp&amp;s=db9088dc32b227cfed688bd2ed2c0bf045a8a38f)\n\n how do they track their experiments? what frameworks/packages do they use? is their code is just a pile of jupiter notebooks? do they work in their mlops? how they store and deploy the models? who owns the models?  \nthis is the base for the successfull work, don\u2019t underestimate it\u2019s influence.\n\nas a comclusion, i believe that there are ton\u2019s of beautiful machine learning positions out there. i know by my own skin, that job hunting can be exhasting and can suck all fun of your life. an i hope these few points of attention can help you ease the pain.\n\ngood luck!\n\np.s. have a look at my blog [here.](https://irregularadel.substack.com/about)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/nj2n0q/d_mldl_job_hunting_points_if_attention/',)", "identifyer": 5725496, "year": "2021"}, {"autor": "mltooling", "date": 1619102272000, "content": "[P] Opyrator - Turn python functions into microservices with auto-generated HTTP API, interactive UI, and more. /!/ https://preview.redd.it/j8l0enfviqu61.png?width=1564&amp;format=png&amp;auto=webp&amp;s=ee4503c29d7ec66f03d7810c22dfceffb2734bf7\n\nWe build a tool to **turn python functions into microservices**. Every service comes with an HTTP API and interactive UI automatically generated based on Python type hints. This is powered by FastAPI, Streamlit, and Pydantic.\n\nOpyrator can be used for many different machine learning tasks - from data collection, to model training and inference - as showcased in our [examples](https://github.com/ml-tooling/opyrator#examples) collection.\n\n\ud83d\udcab Live Demo: [http://opyrator-playground.mltooling.org](http://opyrator-playground.mltooling.org/)\n\n\ud83d\udd17 GitHub (happy about a \u2b50):\u00a0[https://github.com/ml-tooling/opyrator](https://github.com/ml-tooling/opyrator)\n\nThis is a very early and experimental version. We are happy about any feedback, ideas, and suggestions :)", "link": "https://www.reddit.com/r/MachineLearning/comments/mw66rl/p_opyrator_turn_python_functions_into/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] opyrator - turn python functions into microservices with auto-generated http api, interactive ui, and more. /!/ https://preview.redd.it/j8l0enfviqu61.png?width=1564&amp;format=png&amp;auto=webp&amp;s=ee4503c29d7ec66f03d7810c22dfceffb2734bf7\n\nwe build a -----> tool !!!  to **turn python functions into microservices**. every service comes with an http api and interactive ui automatically generated based on python type hints. this is powered by fastapi, streamlit, and pydantic.\n\nopyrator can be used for many different machine learning tasks - from data collection, to model training and inference - as showcased in our [examples](https://github.com/ml-tooling/opyrator#examples) collection.\n\n\ud83d\udcab live demo: [http://opyrator-playground.mltooling.org](http://opyrator-playground.mltooling.org/)\n\n\ud83d\udd17 github (happy about a \u2b50):\u00a0[https://github.com/ml-tooling/opyrator](https://github.com/ml-tooling/opyrator)\n\nthis is a very early and experimental version. we are happy about any feedback, ideas, and suggestions :)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/mw66rl/p_opyrator_turn_python_functions_into/',)", "identifyer": 5725535, "year": "2021"}, {"autor": "Sensitive-Loss1522", "date": 1619079727000, "content": "From where do i download prodigy??? [D], [P] /!/ Hey guys am trying to download [prodigy](https://prodi.gy/) tool so that i can try some of its functionality but nowhere in this link --&gt; [Installation &amp; Setup \u00b7 Prodigy \u00b7 An annotation tool for AI, Machine Learning &amp; NLP](https://prodi.gy/docs/install) am i able to find the download link to get the wheel file, install it and try it out. Was this available before they switched to a paid model.", "link": "https://www.reddit.com/r/MachineLearning/comments/mw07dg/from_where_do_i_download_prodigy_d_p/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "from where do i download prodigy??? [d], [p] /!/ hey guys am trying to download [prodigy](https://prodi.gy/) -----> tool !!!  so that i can try some of its functionality but nowhere in this link --&gt; [installation &amp; setup \u00b7 prodigy \u00b7 an annotation -----> tool !!!  for ai, machine learning &amp; nlp](https://prodi.gy/docs/install) am i able to find the download link to get the wheel file, install it and try it out. was this available before they switched to a paid model.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/mw07dg/from_where_do_i_download_prodigy_d_p/',)", "identifyer": 5725567, "year": "2021"}, {"autor": "Purple-Ad-3492", "date": 1619042230000, "content": "[D] Thoughts on using VADER for sentiment analysis on texts other than social media? /!/ Given that the package is designed for sentiment analysis tuned to social media (Twitter, NYTo, Amazon Movie Reviews). I like this tool for its sentiment rating on a -4 to 4 scale, but it\u2019s limitations seem constrained as far as its classification system, i.e. words and phrases are pre-set and unlike other SA tools, words can\u2019t be added to lists that would simply classify positive/neutral/negative on a -1 to 1 scale. \n\n(VADER tool was constructed and scores were given in these ranges based on an evaluation of 9,000 token features by 10 independent humans. )\n\nI\u2019ve seen the Harry Potter Book project subsected by chapter using VADER. Therefore my first question pertains to thoughts on reliability of these results? \n\nhttps://towardsdatascience.com/basic-nlp-on-the-texts-of-harry-potter-sentiment-analysis-1b474b13651d\n\n\nI\u2019m also looking into non-English text translations. i.e. texts that are originally written in another language and using the VADER sentiment analysis on them. I\u2019d rather find some sort of work-around other than translating the texts themselves, perhaps translating the lexicons in the .txt file and in the code example negate, booster and special_case words lists.\n\n...Although, I couldn\u2019t say the word translation would have the same effective score as its English version. So maybe the preventive hiccup method would be to translate first .... \nor use another SA tool like Dostoyevsky (for Russian).", "link": "https://www.reddit.com/r/MachineLearning/comments/mvqi0t/d_thoughts_on_using_vader_for_sentiment_analysis/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] thoughts on using vader for sentiment analysis on texts other than social media? /!/ given that the package is designed for sentiment analysis tuned to social media (twitter, nyto, amazon movie reviews). i like this -----> tool !!!  for its sentiment rating on a -4 to 4 scale, but it\u2019s limitations seem constrained as far as its classification system, i.e. words and phrases are pre-set and unlike other sa tools, words can\u2019t be added to lists that would simply classify positive/neutral/negative on a -1 to 1 scale. \n\n(vader tool was constructed and scores were given in these ranges based on an evaluation of 9,000 token features by 10 independent humans. )\n\ni\u2019ve seen the harry potter book project subsected by chapter using vader. therefore my first question pertains to thoughts on reliability of these results? \n\nhttps://towardsdatascience.com/basic-nlp-on-the-texts-of-harry-potter-sentiment-analysis-1b474b13651d\n\n\ni\u2019m also looking into non-english text translations. i.e. texts that are originally written in another language and using the vader sentiment analysis on them. i\u2019d rather find some sort of work-around other than translating the texts themselves, perhaps translating the lexicons in the .txt file and in the code example negate, booster and special_case words lists.\n\n...although, i couldn\u2019t say the word translation would have the same effective score as its english version. so maybe the preventive hiccup method would be to translate first .... \nor use another sa tool like dostoyevsky (for russian).", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 12, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/mvqi0t/d_thoughts_on_using_vader_for_sentiment_analysis/',)", "identifyer": 5725590, "year": "2021"}, {"autor": "HouseLTN", "date": 1630143095000, "content": "[D] Need guidance for setting up a work machine for multiple users /!/ We are a group of 4 students who\u2019s interested in doing ML/DL works and we have purchased a PC with 2 RTX 2060 on it. \n\nThe machine is at my home and others will need to connect remotely via AnyDesk. Sometimes AnyDesk is really buggy and others are furious \n\nMy question is, is there a tool that can satisfy our needs as below, considering the host OS is Ubuntu 20\n\n- each of us gets to connect it remotely (no UI needed, we are good with terminal) \n- each of us gets our own Jupiter instance \n- admin tool to monitor machine usage \n\nThanks in advanced!", "link": "https://www.reddit.com/r/MachineLearning/comments/pd6s7n/d_need_guidance_for_setting_up_a_work_machine_for/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] need guidance for setting up a work machine for multiple users /!/ we are a group of 4 students who\u2019s interested in doing ml/dl works and we have purchased a pc with 2 rtx 2060 on it. \n\nthe machine is at my home and others will need to connect remotely via anydesk. sometimes anydesk is really buggy and others are furious \n\nmy question is, is there a -----> tool !!!  that can satisfy our needs as below, considering the host os is ubuntu 20\n\n- each of us gets to connect it remotely (no ui needed, we are good with terminal) \n- each of us gets our own jupiter instance \n- admin -----> tool !!!  to monitor machine usage \n\nthanks in advanced!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/pd6s7n/d_need_guidance_for_setting_up_a_work_machine_for/',)", "identifyer": 5725671, "year": "2021"}, {"autor": "MikeDeltaOscar", "date": 1630135205000, "content": "[D] Kalman Filters (&amp; classical models) vs. ML, DNN /!/ Hello! I am a data engineer currently studying systems engineering part-time. I am really fascinated both by the recent advances in ML technology, but also by the impressive mathematical models used up until now.  \n\n\nLast semester was an intro to the Kalman Filter (system identification methods). What I still haven't understood though, is when to use which tool and for which scenarios. Going through this paper:  \n\n\n[https://medium.com/analytics-vidhya/kalman-filter-vs-deep-learning-for-position-estimation-27658e0cf568](https://medium.com/analytics-vidhya/kalman-filter-vs-deep-learning-for-position-estimation-27658e0cf568)  \n\n\nIt almost seems as if DNNs would be the natural modern choice for time-series predictions. A part of me though is so fascinated by the Kalman filters, that I want to believe there are scenarios where a Kalman filter or similar model would be a better choice.  \n\n\nWhat would these scenarios be if so?", "link": "https://www.reddit.com/r/MachineLearning/comments/pd5c65/d_kalman_filters_classical_models_vs_ml_dnn/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] kalman filters (&amp; classical models) vs. ml, dnn /!/ hello! i am a data engineer currently studying systems engineering part-time. i am really fascinated both by the recent advances in ml technology, but also by the impressive mathematical models used up until now.  \n\n\nlast semester was an intro to the kalman filter (system identification methods). what i still haven't understood though, is when to use which -----> tool !!!  and for which scenarios. going through this paper:  \n\n\n[https://medium.com/analytics-vidhya/kalman-filter-vs-deep-learning-for-position-estimation-27658e0cf568](https://medium.com/analytics-vidhya/kalman-filter-vs-deep-learning-for-position-estimation-27658e0cf568)  \n\n\nit almost seems as if dnns would be the natural modern choice for time-series predictions. a part of me though is so fascinated by the kalman filters, that i want to believe there are scenarios where a kalman filter or similar model would be a better choice.  \n\n\nwhat would these scenarios be if so?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 5, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/pd5c65/d_kalman_filters_classical_models_vs_ml_dnn/',)", "identifyer": 5725677, "year": "2021"}, {"autor": "patzscheck", "date": 1630068384000, "content": "[R] labelCloud for 3D Object Detection and 6D Pose Estimation /!/ Hi everyone! :)\n\nOne of my former students developed a very cool point cloud labeling tool, called labelCloud! The tool provides smooth labeling interaction with 3D point clouds to support users in generating training data for #3DObjectDetection and #6DPoseEstimation.\n\nlabelCloud is publicly available, easy to install, and simple to use. Check out our github: [https://github.com/ch-sa/labelCloud](https://github.com/ch-sa/labelCloud)\n\nIf you give it a go, we would be more than happy to receive your feedback on it. So, as we are currently evaluating it, we invite you to fill this short questionaire [https://forms.gle/moEyjGSa1Eiiq7VT8](https://forms.gle/moEyjGSa1Eiiq7VT8)  (\\~5 min)!  Thanks in advance! :)\n\nFurther information can be found in our paper from CAD'21 conference: [http://dx.doi.org/10.14733/cadconfP.2021.319-323](http://dx.doi.org/10.14733/cadconfP.2021.319-323)", "link": "https://www.reddit.com/r/MachineLearning/comments/pcmmot/r_labelcloud_for_3d_object_detection_and_6d_pose/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[r] labelcloud for 3d object detection and 6d pose estimation /!/ hi everyone! :)\n\none of my former students developed a very cool point cloud labeling -----> tool !!! , called labelcloud! the tool provides smooth labeling interaction with 3d point clouds to support users in generating training data for #3dobjectdetection and #6dposeestimation.\n\nlabelcloud is publicly available, easy to install, and simple to use. check out our github: [https://github.com/ch-sa/labelcloud](https://github.com/ch-sa/labelcloud)\n\nif you give it a go, we would be more than happy to receive your feedback on it. so, as we are currently evaluating it, we invite you to fill this short questionaire [https://forms.gle/moeyjgsa1eiiq7vt8](https://forms.gle/moeyjgsa1eiiq7vt8)  (\\~5 min)!  thanks in advance! :)\n\nfurther information can be found in our paper from cad'21 conference: [http://dx.doi.org/10.14733/cadconfp.2021.319-323](http://dx.doi.org/10.14733/cadconfp.2021.319-323)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/pcmmot/r_labelcloud_for_3d_object_detection_and_6d_pose/',)", "identifyer": 5725719, "year": "2021"}, {"autor": "ajinkyajawale", "date": 1625638316000, "content": "[D] what tool you use for data dictionary /!/ We are heavy data engineering and data science team who regularly interacts with product and Marketing teams\n\nWe don't want to Product team and marketing team to get involved into dwh or schema or any data level asking hell what's this what's that... Rather we providing the proper data dictionary like what's the table what's it's meaning fields at High level view..\n\nIt would be highly appreciated if you have used any similar relevant tools..\n\nPs.. we are not looking for data governance tools more as a data dictionary documention tool", "link": "https://www.reddit.com/r/MachineLearning/comments/ofd505/d_what_tool_you_use_for_data_dictionary/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] what -----> tool !!!  you use for data dictionary /!/ we are heavy data engineering and data science team who regularly interacts with product and marketing teams\n\nwe don't want to product team and marketing team to get involved into dwh or schema or any data level asking hell what's this what's that... rather we providing the proper data dictionary like what's the table what's it's meaning fields at high level view..\n\nit would be highly appreciated if you have used any similar relevant tools..\n\nps.. we are not looking for data governance tools more as a data dictionary documention tool", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/ofd505/d_what_tool_you_use_for_data_dictionary/',)", "identifyer": 5725877, "year": "2021"}, {"autor": "ajinkyajawale", "date": 1625638267000, "content": "What tool you use for data dictionary /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/ofd4mn/what_tool_you_use_for_data_dictionary/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what -----> tool !!!  you use for data dictionary /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/ofd4mn/what_tool_you_use_for_data_dictionary/',)", "identifyer": 5725878, "year": "2021"}, {"autor": "AlGalv", "date": 1625590319000, "content": "New cool tool for interactive segmentation /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/oeyxvc/new_cool_tool_for_interactive_segmentation/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "new cool -----> tool !!!  for interactive segmentation /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/oeyxvc/new_cool_tool_for_interactive_segmentation/',)", "identifyer": 5725907, "year": "2021"}, {"autor": "AlGalv", "date": 1625590258000, "content": "New cool tool for interactive segmentation /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/oeyx6f/new_cool_tool_for_interactive_segmentation/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "new cool -----> tool !!!  for interactive segmentation /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/oeyx6f/new_cool_tool_for_interactive_segmentation/',)", "identifyer": 5725908, "year": "2021"}, {"autor": "positiveCAPTCHAtest", "date": 1629501365000, "content": "[P] A week ago, I came across this super cool project to build Cross Modal Search. I will now share more details about the project /!/ Check the project out for yourself on GitHub here: https://github.com/jina-ai/examples/tree/master/cross-modal-search.\n\nI was looking for some projects based on search engines, and building a tool which could search across various types of data, and that's when I came across this GitHub project: https://github.com/jina-ai/jina/blob/master/.github/pages/hello-world.md#-multimodal-document-search. Encouraged by  thorough, step by step instructions on how to build a search service that can use diverse modal features to provide accurate results; I ventured through the documents till I came to the latest updated version, here: https://github.com/jina-ai/examples/tree/master/cross-modal-search.\n\nOverall, the framework offered by Jina seems to be a very interesting, search-as-a-service application. I'm going to be scouting around a lot more to see what I can build with it next.", "link": "https://www.reddit.com/r/MachineLearning/comments/p8ga09/p_a_week_ago_i_came_across_this_super_cool/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] a week ago, i came across this super cool project to build cross modal search. i will now share more details about the project /!/ check the project out for yourself on github here: https://github.com/jina-ai/examples/tree/master/cross-modal-search.\n\ni was looking for some projects based on search engines, and building a -----> tool !!!  which could search across various types of data, and that's when i came across this github project: https://github.com/jina-ai/jina/blob/master/.github/pages/hello-world.md#-multimodal-document-search. encouraged by  thorough, step by step instructions on how to build a search service that can use diverse modal features to provide accurate results; i ventured through the documents till i came to the latest updated version, here: https://github.com/jina-ai/examples/tree/master/cross-modal-search.\n\noverall, the framework offered by jina seems to be a very interesting, search-as-a-service application. i'm going to be scouting around a lot more to see what i can build with it next.", "sortedWord": "None", "removed": "('nan',)", "score": 2, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/p8ga09/p_a_week_ago_i_came_across_this_super_cool/',)", "identifyer": 5726121, "year": "2021"}, {"autor": "SubstantialRange", "date": 1627417049000, "content": "[R] DeepMind: Open-Ended Learning Leads to Generally Capable Agents /!/ [https://deepmind.com/research/publications/open-ended-learning-leads-to-generally-capable-agents](https://deepmind.com/research/publications/open-ended-learning-leads-to-generally-capable-agents)\n\n&amp;#x200B;\n\n&gt;Artificial  agents have achieved great success in individual challenging simulated  environments, mastering the particular tasks they were trained for, with  their behaviour even generalising to maps and opponents that were never  encountered in training.   \n&gt;  \n&gt;In this work we create agents that can perform  well beyond a single, individual task, that exhibit much wider  generalisation of behaviour to a massive, rich space of challenges. We  define a universe of tasks within an environment domain and demonstrate  the ability to train agents that are generally capable across this vast  space and beyond.   \n&gt;  \n&gt;The environment is natively multi-agent, spanning the  continuum of competitive, cooperative, and independent games, which are  situated within procedurally generated physical 3D worlds. The resulting  space is exceptionally diverse in terms of the challenges posed to  agents, and as such, even measuring the learning progress of an agent is  an open research problem.  \n&gt;  \n&gt;We propose an iterative notion of improvement between successive  generations of agents, rather than seeking to maximise a singular  objective, allowing us to quantify progress despite tasks being  incomparable in terms of achievable rewards. Training an agent that is  performant across such a vast space of tasks is a central challenge, one  we find that pure reinforcement learning on a fixed distribution of  training tasks does not succeed in.   \n&gt;  \n&gt;We show that through constructing an  open-ended learning process, which dynamically changes the training  task distributions and training objectives such that the agent never  stops learning, we achieve consistent learning of new behaviours. The  resulting agent is able to score reward in every one of our humanly  solvable evaluation levels, with behaviour generalising to many held-out  points in the universe of tasks. Examples of this zero-shot  generalisation include good performance on Hide and Seek, Capture the  Flag, and Tag.   \n&gt;  \n&gt;Through analysis and hand-authored probe tasks we  characterise the behaviour of our agent, and find interesting emergent  heuristic behaviours such as trial-and-error experimentation, simple  tool use, option switching, and co-operation. Finally, we demonstrate  that the general capabilities of this agent could unlock larger scale  transfer of behaviour through cheap finetuning.", "link": "https://www.reddit.com/r/MachineLearning/comments/osucfj/r_deepmind_openended_learning_leads_to_generally/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[r] deepmind: open-ended learning leads to generally capable agents /!/ [https://deepmind.com/research/publications/open-ended-learning-leads-to-generally-capable-agents](https://deepmind.com/research/publications/open-ended-learning-leads-to-generally-capable-agents)\n\n&amp;#x200b;\n\n&gt;artificial  agents have achieved great success in individual challenging simulated  environments, mastering the particular tasks they were trained for, with  their behaviour even generalising to maps and opponents that were never  encountered in training.   \n&gt;  \n&gt;in this work we create agents that can perform  well beyond a single, individual task, that exhibit much wider  generalisation of behaviour to a massive, rich space of challenges. we  define a universe of tasks within an environment domain and demonstrate  the ability to train agents that are generally capable across this vast  space and beyond.   \n&gt;  \n&gt;the environment is natively multi-agent, spanning the  continuum of competitive, cooperative, and independent games, which are  situated within procedurally generated physical 3d worlds. the resulting  space is exceptionally diverse in terms of the challenges posed to  agents, and as such, even measuring the learning progress of an agent is  an open research problem.  \n&gt;  \n&gt;we propose an iterative notion of improvement between successive  generations of agents, rather than seeking to maximise a singular  objective, allowing us to quantify progress despite tasks being  incomparable in terms of achievable rewards. training an agent that is  performant across such a vast space of tasks is a central challenge, one  we find that pure reinforcement learning on a fixed distribution of  training tasks does not succeed in.   \n&gt;  \n&gt;we show that through constructing an  open-ended learning process, which dynamically changes the training  task distributions and training objectives such that the agent never  stops learning, we achieve consistent learning of new behaviours. the  resulting agent is able to score reward in every one of our humanly  solvable evaluation levels, with behaviour generalising to many held-out  points in the universe of tasks. examples of this zero-shot  generalisation include good performance on hide and seek, capture the  flag, and tag.   \n&gt;  \n&gt;through analysis and hand-authored probe tasks we  characterise the behaviour of our agent, and find interesting emergent  heuristic behaviours such as trial-and-error experimentation, simple  -----> tool !!!  use, option switching, and co-operation. finally, we demonstrate  that the general capabilities of this agent could unlock larger scale  transfer of behaviour through cheap finetuning.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 9, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/osucfj/r_deepmind_openended_learning_leads_to_generally/',)", "identifyer": 5726177, "year": "2021"}, {"autor": "NotARedditAdmin1", "date": 1611255215000, "content": "[P] Really Cool GPT3 tool - writes emails for you! /!/ This is a really cool tool. It basically writes emails for you. Here is the link - https://waitlist.othersideai.com?kid=1F0MRZ", "link": "https://www.reddit.com/r/MachineLearning/comments/l24wzz/p_really_cool_gpt3_tool_writes_emails_for_you/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] really cool gpt3 -----> tool !!!  - writes emails for you! /!/ this is a really cool tool. it basically writes emails for you. here is the link - https://waitlist.othersideai.com?kid=1f0mrz", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/l24wzz/p_really_cool_gpt3_tool_writes_emails_for_you/',)", "identifyer": 5726354, "year": "2021"}, {"autor": "goatman12341", "date": 1611254748000, "content": "Exploring the Latent Space of Cats (Link to Tool In Comments)", "link": "https://www.reddit.com/r/MachineLearning/comments/l24qrl/exploring_the_latent_space_of_cats_link_to_tool/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "exploring the latent space of cats (link to -----> tool !!!  in comments)", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 4, "media": "('hosted:video',)", "medialink": "('https://v.redd.it/yssh2x71eqc61',)", "identifyer": 5726355, "year": "2021"}, {"autor": "NotARedditAdmin1", "date": 1611252424000, "content": "Really cool GPT3 tool that you can get early access! /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/l23z0q/really_cool_gpt3_tool_that_you_can_get_early/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "really cool gpt3 -----> tool !!!  that you can get early access! /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/l23z0q/really_cool_gpt3_tool_that_you_can_get_early/',)", "identifyer": 5726362, "year": "2021"}, {"autor": "henning_ml", "date": 1611248610000, "content": "[N] Zico Kolter: Seminar Series on Data-Driven Methods for Science and Engineering Seminar /!/ Hello [r/machinelearning](https://www.reddit.com/r/machinelearning/),\n\nWe would like to invite everyone to a seminar series organized by four post docs at the University of Washington.\n\nThe next seminar will be held tomorrow, Friday January 21st at 9.00 PST / 12.00 EST. This weeks speaker will be **Zico Kolter**. The talk will be streamed live on YouTube: [https://youtu.be/x2bfe2wYG2w](https://youtu.be/x2bfe2wYG2w)\n\nYou can signup to receive reminder emails for upcoming talks here: [https://forms.gle/qbU5jS6LMLv3fF6j7](https://forms.gle/qbU5jS6LMLv3fF6j7)\n\n&amp;#x200B;\n\n**Title:** Incorporating physics and decision making into deep learning via implicit layers\n\n**Abstract:** Despite their wide applicability, deep networks often fail to exactly capture simple \"known\" features of real-world data sets, such as those governed by physical laws. \u00a0In this talk, I'll present methods for integrating hard constraints, such as those associated with decision making, optimization problems, or physical simulation, directly into the predictions of a deep network. \u00a0Our tool for achieving this will be the use of so-called implicit layers in deep models: layers that are defined implicitly in terms of conditions we would like them to satisfy, rather than via an explicit computation graph. \u00a0I'll discuss how we can use these layers to embed (exact) physical constraints, robust control criteria, and task-based objectives, all within the framework of traditional deep learning. \u00a0I will highlight several applications of this work in reinforcement learning, control, energy systems, and other settings, and discuss generalizations and directions for future work in the area.\n\nBio: Zico Kolter is an Associate Professor in the Computer Science Department at Carnegie Mellon University, and also serves as chief scientist of AI research for the Bosch Center for Artificial Intelligence. His work spans the intersection of machine learning and optimization, with a large focus on developing more robust and rigorous methods in deep learning. In addition, he has worked in a number of application areas, highlighted by work on sustainability and smart energy systems. He is a recipient of the DARPA Young Faculty Award, a Sloan Fellowship, and best paper awards at NeurIPS, ICML (honorable mention), IJCAI, KDD, and PESGM.\n\nLooking forward to seeing you tomorrow!\n\nJoseph Bakarji, Jason Bramburger, Henning Lange, Jordan Snyder", "link": "https://www.reddit.com/r/MachineLearning/comments/l22kfd/n_zico_kolter_seminar_series_on_datadriven/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[n] zico kolter: seminar series on data-driven methods for science and engineering seminar /!/ hello [r/machinelearning](https://www.reddit.com/r/machinelearning/),\n\nwe would like to invite everyone to a seminar series organized by four post docs at the university of washington.\n\nthe next seminar will be held tomorrow, friday january 21st at 9.00 pst / 12.00 est. this weeks speaker will be **zico kolter**. the talk will be streamed live on youtube: [https://youtu.be/x2bfe2wyg2w](https://youtu.be/x2bfe2wyg2w)\n\nyou can signup to receive reminder emails for upcoming talks here: [https://forms.gle/qbu5js6lmlv3ff6j7](https://forms.gle/qbu5js6lmlv3ff6j7)\n\n&amp;#x200b;\n\n**title:** incorporating physics and decision making into deep learning via implicit layers\n\n**abstract:** despite their wide applicability, deep networks often fail to exactly capture simple \"known\" features of real-world data sets, such as those governed by physical laws. \u00a0in this talk, i'll present methods for integrating hard constraints, such as those associated with decision making, optimization problems, or physical simulation, directly into the predictions of a deep network. \u00a0our -----> tool !!!  for achieving this will be the use of so-called implicit layers in deep models: layers that are defined implicitly in terms of conditions we would like them to satisfy, rather than via an explicit computation graph. \u00a0i'll discuss how we can use these layers to embed (exact) physical constraints, robust control criteria, and task-based objectives, all within the framework of traditional deep learning. \u00a0i will highlight several applications of this work in reinforcement learning, control, energy systems, and other settings, and discuss generalizations and directions for future work in the area.\n\nbio: zico kolter is an associate professor in the computer science department at carnegie mellon university, and also serves as chief scientist of ai research for the bosch center for artificial intelligence. his work spans the intersection of machine learning and optimization, with a large focus on developing more robust and rigorous methods in deep learning. in addition, he has worked in a number of application areas, highlighted by work on sustainability and smart energy systems. he is a recipient of the darpa young faculty award, a sloan fellowship, and best paper awards at neurips, icml (honorable mention), ijcai, kdd, and pesgm.\n\nlooking forward to seeing you tomorrow!\n\njoseph bakarji, jason bramburger, henning lange, jordan snyder", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/l22kfd/n_zico_kolter_seminar_series_on_datadriven/',)", "identifyer": 5726367, "year": "2021"}, {"autor": "laminarflow027", "date": 1611244799000, "content": "[P] Scalable topic modelling for news articles /!/ Just wanted to post this out here for anybody that's interested in performing large-scale topic modelling on news articles (this was [earlier brought up in this sub](https://www.reddit.com/r/MachineLearning/comments/eyid0l/p_topic_extraction_from_news_articles/), so I thought it might be relevant here). This work was done by my research group as part of a larger project, \"The Gender Gap Tracker\", which aims to examine gender-based disparities in mainstream Canadian news media coverage, and the topic modelling work is a natural extension of that, where we try to go deeper into which topics (across several news outlets) exhibit more stark differences in language based on the gender of those quoted.\n\nThe results were scalable to very large datasets, and produced some really interesting insights! This live dashboard hosts some visualizations and observations on the results:[https://gendergaptracker.research.sfu.ca/apps/topicmodel](https://gendergaptracker.research.sfu.ca/apps/topicmodel)\n\nBecause news article datasets can very quickly become very large, we opted to extract topics using [Spark's LDA](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/3741049972324885/3783546674231782/4413065072037724/latest.html) implementation. Rather than modelling topics over a long time span containing hundreds of thousands of articles (which is definitely possible in Spark, thanks to distributed computing), we opted to generate topic keywords on a **monthly** basis. This allowed us to not only capture significant world events (such as the New Zealand Mosque shootings, or the start of the COVID-19 pandemic, and many more), but also produce more fine-grained analyses of news coverage by outlet across these periods.\n\nThe topic model routines are still faithfully running once a month (as of January 2021), and to this date, we've processed \\~700,000 news articles dating back to October 2018, with more being added every day. At the start of each month, the topic model script generates 15 sets of topic keywords, and a human goes in and manually labels the 'topics' with interpretable labels based on knowledge of the past month's events.\n\nIf anybody wants more technical details on the methodology, including how we defined stopwords, or any other preprocessing steps and hyperparameter tuning experiments we performed to get reliable, interpretable labels each month, there's a link to a PDF report at the bottom of the dashboard page. [It's also linked here](https://gendergaptracker.research.sfu.ca/static/GGT_topic_model_technical_report.pdf) for convenience. Hope this is useful to anyone that wants to use topic modelling as a tool to study news articles at scale!", "link": "https://www.reddit.com/r/MachineLearning/comments/l217et/p_scalable_topic_modelling_for_news_articles/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] scalable topic modelling for news articles /!/ just wanted to post this out here for anybody that's interested in performing large-scale topic modelling on news articles (this was [earlier brought up in this sub](https://www.reddit.com/r/machinelearning/comments/eyid0l/p_topic_extraction_from_news_articles/), so i thought it might be relevant here). this work was done by my research group as part of a larger project, \"the gender gap tracker\", which aims to examine gender-based disparities in mainstream canadian news media coverage, and the topic modelling work is a natural extension of that, where we try to go deeper into which topics (across several news outlets) exhibit more stark differences in language based on the gender of those quoted.\n\nthe results were scalable to very large datasets, and produced some really interesting insights! this live dashboard hosts some visualizations and observations on the results:[https://gendergaptracker.research.sfu.ca/apps/topicmodel](https://gendergaptracker.research.sfu.ca/apps/topicmodel)\n\nbecause news article datasets can very quickly become very large, we opted to extract topics using [spark's lda](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/3741049972324885/3783546674231782/4413065072037724/latest.html) implementation. rather than modelling topics over a long time span containing hundreds of thousands of articles (which is definitely possible in spark, thanks to distributed computing), we opted to generate topic keywords on a **monthly** basis. this allowed us to not only capture significant world events (such as the new zealand mosque shootings, or the start of the covid-19 pandemic, and many more), but also produce more fine-grained analyses of news coverage by outlet across these periods.\n\nthe topic model routines are still faithfully running once a month (as of january 2021), and to this date, we've processed \\~700,000 news articles dating back to october 2018, with more being added every day. at the start of each month, the topic model script generates 15 sets of topic keywords, and a human goes in and manually labels the 'topics' with interpretable labels based on knowledge of the past month's events.\n\nif anybody wants more technical details on the methodology, including how we defined stopwords, or any other preprocessing steps and hyperparameter tuning experiments we performed to get reliable, interpretable labels each month, there's a link to a pdf report at the bottom of the dashboard page. [it's also linked here](https://gendergaptracker.research.sfu.ca/static/ggt_topic_model_technical_report.pdf) for convenience. hope this is useful to anyone that wants to use topic modelling as a -----> tool !!!  to study news articles at scale!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/l217et/p_scalable_topic_modelling_for_news_articles/',)", "identifyer": 5726369, "year": "2021"}, {"autor": "senos64", "date": 1611238092000, "content": "The AI text summarization API/tool? [D] /!/ Does anybody have experience with different tools in this? And what would you recommend?", "link": "https://www.reddit.com/r/MachineLearning/comments/l1z4ag/the_ai_text_summarization_apitool_d/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "the ai text summarization api/-----> tool !!! ? [d] /!/ does anybody have experience with different tools in this? and what would you recommend?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/l1z4ag/the_ai_text_summarization_apitool_d/',)", "identifyer": 5726387, "year": "2021"}, {"autor": "senos64", "date": 1611237987000, "content": "The AI text summarization API/tool? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/l1z391/the_ai_text_summarization_apitool/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "the ai text summarization api/-----> tool !!! ? /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/l1z391/the_ai_text_summarization_apitool/',)", "identifyer": 5726388, "year": "2021"}, {"autor": "crubier", "date": 1619634130000, "content": "[P] Labelflow, the open source image labeling and dataset cleaning platform. /!/ Hi, all! Announcing Labelflow ( [https://www.labelflow.net/](https://www.labelflow.net/) ), the open source image labeling and dataset cleaning platform.\n\nWe are a team of 9 people with experience in image labeling and dataset curating to build quality datasets for deep learning.  We were frustrated by the amount of scripts required to move the data back and forth between tools, and by the lack of control over our data, especially when we have data that can\u2019t easily be shared. \n\nSo we started building Labelflow, an image labeling tool with all the bells and whistles, but with an open source backend that you can connect to your own data stack easily. Let me know what you think, and feel free to request early access to get up to 50% off when we release it in a few months!", "link": "https://www.reddit.com/r/MachineLearning/comments/n0l98m/p_labelflow_the_open_source_image_labeling_and/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] labelflow, the open source image labeling and dataset cleaning platform. /!/ hi, all! announcing labelflow ( [https://www.labelflow.net/](https://www.labelflow.net/) ), the open source image labeling and dataset cleaning platform.\n\nwe are a team of 9 people with experience in image labeling and dataset curating to build quality datasets for deep learning.  we were frustrated by the amount of scripts required to move the data back and forth between tools, and by the lack of control over our data, especially when we have data that can\u2019t easily be shared. \n\nso we started building labelflow, an image labeling -----> tool !!!  with all the bells and whistles, but with an open source backend that you can connect to your own data stack easily. let me know what you think, and feel free to request early access to get up to 50% off when we release it in a few months!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 23, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/n0l98m/p_labelflow_the_open_source_image_labeling_and/',)", "identifyer": 5726449, "year": "2021"}, {"autor": "wdanilo", "date": 1619583347000, "content": "[News] Enso 2.0 Syntax Reference is out! Enso is a visual / textual programming language with a seamless polyglot interoperability with Python, Java, R, and JavaScript. Written in Rust, Java, and GraalVM. /!/ &amp;#x200B;\n\nhttps://preview.redd.it/87iy7ycabuv61.png?width=2220&amp;format=png&amp;auto=webp&amp;s=1dd024ca16b1f0658e9bed1d8d4283fc688d42f5\n\nHi, I'm Wojciech, one of the founders of Enso ([https://enso.org](https://enso.org/)) :)\n\n&amp;#x200B;\n\n**What is Enso?**\n\nEnso is an award-winning interactive programming language with dual visual and textual representations. It is a tool that spans the entire stack, going from high-level visualization and communication to the nitty-gritty of backend services, all in a single language.\n\nEnso is also a polyglot language - it lets you import any library from Enso, Java, JavaScript, R, or Python, and use functions, callbacks, and data types without any wrappers. The Enso compiler and the underlying GraalVM JIT compiler, compile them to the same instruction set with a unified memory model.\n\n[See the DEMO here!](https://www.youtube.com/watch?v=fQvWMoOjmQk&amp;ab_channel=Enso)\n\n&amp;#x200B;\n\n**What is this post about?**\n\nEnso has visual and textual syntax. We have released it to the public as a free, open-source project over a week ago and we got a lot of questions regarding what is the best way to learn its textual syntax. To address these questions, we have created a dedicated page for it!\n\n**Check it out here:** [https://enso.org/docs/syntax](https://enso.org/docs/syntax)\n\n&amp;#x200B;\n\n**If you want to learn more about Enso, check the following links!**\n\n* our website: [https://enso.org](https://enso.org/)\n* our GitHub (Enso is Open Source and free to use): [https://github.com/enso-org/enso](https://github.com/enso-org/enso)", "link": "https://www.reddit.com/r/MachineLearning/comments/n06ocg/news_enso_20_syntax_reference_is_out_enso_is_a/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[news] enso 2.0 syntax reference is out! enso is a visual / textual programming language with a seamless polyglot interoperability with python, java, r, and javascript. written in rust, java, and graalvm. /!/ &amp;#x200b;\n\nhttps://preview.redd.it/87iy7ycabuv61.png?width=2220&amp;format=png&amp;auto=webp&amp;s=1dd024ca16b1f0658e9bed1d8d4283fc688d42f5\n\nhi, i'm wojciech, one of the founders of enso ([https://enso.org](https://enso.org/)) :)\n\n&amp;#x200b;\n\n**what is enso?**\n\nenso is an award-winning interactive programming language with dual visual and textual representations. it is a -----> tool !!!  that spans the entire stack, going from high-level visualization and communication to the nitty-gritty of backend services, all in a single language.\n\nenso is also a polyglot language - it lets you import any library from enso, java, javascript, r, or python, and use functions, callbacks, and data types without any wrappers. the enso compiler and the underlying graalvm jit compiler, compile them to the same instruction set with a unified memory model.\n\n[see the demo here!](https://www.youtube.com/watch?v=fqvwmoojmqk&amp;ab_channel=enso)\n\n&amp;#x200b;\n\n**what is this post about?**\n\nenso has visual and textual syntax. we have released it to the public as a free, open-source project over a week ago and we got a lot of questions regarding what is the best way to learn its textual syntax. to address these questions, we have created a dedicated page for it!\n\n**check it out here:** [https://enso.org/docs/syntax](https://enso.org/docs/syntax)\n\n&amp;#x200b;\n\n**if you want to learn more about enso, check the following links!**\n\n* our website: [https://enso.org](https://enso.org/)\n* our github (enso is open source and free to use): [https://github.com/enso-org/enso](https://github.com/enso-org/enso)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/n06ocg/news_enso_20_syntax_reference_is_out_enso_is_a/',)", "identifyer": 5726510, "year": "2021"}, {"autor": "kuszi", "date": 1610538526000, "content": "Tools for labeling texts to create clusters [D] /!/ I am trying to find clusters in the set of text documents.  \nCould you suggest any tool to support manual annotation process?", "link": "https://www.reddit.com/r/MachineLearning/comments/kwevov/tools_for_labeling_texts_to_create_clusters_d/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "tools for labeling texts to create clusters [d] /!/ i am trying to find clusters in the set of text documents.  \ncould you suggest any -----> tool !!!  to support manual annotation process?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 5, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/kwevov/tools_for_labeling_texts_to_create_clusters_d/',)", "identifyer": 5726613, "year": "2021"}, {"autor": "surgeai", "date": 1615998616000, "content": "[P] Supercharge your NLP project with a free, self-serve NER tool /!/ Hey [r/MachineLearning](https://www.reddit.com/r/MachineLearning/) readers!\n\nAfter talking to a bunch of you [recently](https://www.reddit.com/r/MachineLearning/comments/lrpjdv/p_feedback_for_our_data_labeling_tool/), we learned that a lot of people want a free, self-serve NLP labeling tool. We heard your feedback - and we\u2019re excited to share with you our new-and-improved data labeling platform! It comes with a free suite of tools for customizing your labeling projects, as well as a wide variety of labeling questions: multiple choice, free response, named entity recognition (NER), etc.\n\nSo how\u2019s our platform different from others? With Surge, you have a quick way to kick off labeling projects, access to free team management capabilities, and fully-customizable templates to support a wide range of labeling needs. You can read more about the details [here](https://www.surgehq.ai/blog/named-entity-recognition).\n\nIf you\u2019d like to check it out, go to [surgehq.ai](https://www.surgehq.ai/) and create an account. You and your team can start labeling right away for free. Or, if you don\u2019t want to do the labeling yourself, you can use our high-quality, paid workforce.\n\nOnce again, it\u2019d be great if you could send us your feedback by filling out this form: [https://forms.gle/HkdniQkaKiMqQ1JS8](https://forms.gle/HkdniQkaKiMqQ1JS8). We\u2019re also giving away $20 Amazon gift cards to anyone interested in jumping on a 30m phone call with us (if you haven't done so already).\n\nYour suggestions would really help us shape our product roadmap. We hope that you enjoy using our tool, and we\u2019re looking forward to hearing your thoughts!\n\n&amp;#x200B;\n\n*Processing video yhcx9hvv7mn61...*", "link": "https://www.reddit.com/r/MachineLearning/comments/m74c0l/p_supercharge_your_nlp_project_with_a_free/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] supercharge your nlp project with a free, self-serve ner -----> tool !!!  /!/ hey [r/machinelearning](https://www.reddit.com/r/machinelearning/) readers!\n\nafter talking to a bunch of you [recently](https://www.reddit.com/r/machinelearning/comments/lrpjdv/p_feedback_for_our_data_labeling_tool/), we learned that a lot of people want a free, self-serve nlp labeling tool. we heard your feedback - and we\u2019re excited to share with you our new-and-improved data labeling platform! it comes with a free suite of tools for customizing your labeling projects, as well as a wide variety of labeling questions: multiple choice, free response, named entity recognition (ner), etc.\n\nso how\u2019s our platform different from others? with surge, you have a quick way to kick off labeling projects, access to free team management capabilities, and fully-customizable templates to support a wide range of labeling needs. you can read more about the details [here](https://www.surgehq.ai/blog/named-entity-recognition).\n\nif you\u2019d like to check it out, go to [surgehq.ai](https://www.surgehq.ai/) and create an account. you and your team can start labeling right away for free. or, if you don\u2019t want to do the labeling yourself, you can use our high-quality, paid workforce.\n\nonce again, it\u2019d be great if you could send us your feedback by filling out this form: [https://forms.gle/hkdniqkakimqq1js8](https://forms.gle/hkdniqkakimqq1js8). we\u2019re also giving away $20 amazon gift cards to anyone interested in jumping on a 30m phone call with us (if you haven't done so already).\n\nyour suggestions would really help us shape our product roadmap. we hope that you enjoy using our tool, and we\u2019re looking forward to hearing your thoughts!\n\n&amp;#x200b;\n\n*processing video yhcx9hvv7mn61...*", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/m74c0l/p_supercharge_your_nlp_project_with_a_free/',)", "identifyer": 5726628, "year": "2021"}, {"autor": "surgeai", "date": 1615998483000, "content": "Supercharge your NLP project with a free, self-serve NER tool /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/m74a8t/supercharge_your_nlp_project_with_a_free/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "supercharge your nlp project with a free, self-serve ner -----> tool !!!  /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/m74a8t/supercharge_your_nlp_project_with_a_free/',)", "identifyer": 5726629, "year": "2021"}, {"autor": "GabeLoupe", "date": 1622298251000, "content": "[D] - Dataset management - What is the best tool for a team to collaborate over an Image/Audio dataset? /!/ Our team is doing several audio and image detection projects and we have not found a great tool for the whole team \\[of varying levels of technical expertiese\\] to collaborate in. So far, we have ended up going with Dropbox, because it's easy enough for our collection team to view and review images &amp; audio, but also has a robust API to plug into our pipeline. Seems like some options would be Scale / Roboflow / V7 Labs...? Thanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/nnoij2/d_dataset_management_what_is_the_best_tool_for_a/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] - dataset management - what is the best -----> tool !!!  for a team to collaborate over an image/audio dataset? /!/ our team is doing several audio and image detection projects and we have not found a great tool for the whole team \\[of varying levels of technical expertiese\\] to collaborate in. so far, we have ended up going with dropbox, because it's easy enough for our collection team to view and review images &amp; audio, but also has a robust api to plug into our pipeline. seems like some options would be scale / roboflow / v7 labs...? thanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/nnoij2/d_dataset_management_what_is_the_best_tool_for_a/',)", "identifyer": 5726741, "year": "2021"}, {"autor": "beanstalkim", "date": 1623103344000, "content": "[D] Recommendations on your object detection labelling tool of choice for drawing bounding boxes? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/nuowjs/d_recommendations_on_your_object_detection/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] recommendations on your object detection labelling -----> tool !!!  of choice for drawing bounding boxes? /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/nuowjs/d_recommendations_on_your_object_detection/',)", "identifyer": 5726798, "year": "2021"}, {"autor": "zedhehou", "date": 1623069894000, "content": "[R] Silas: A High-Performance Machine Learning Foundation for Logical Reasoning and Verification /!/ Silas is a generic data mining and predictive analytics software toolkit built upon advanced machine learning, automated reasoning and artificial intelligence techniques. It deals with any type of structured data and performs tasks such as classification, regression, segmentation, anomaly detection, prediction, and more.\n\nThe below paper introduces Silas as a high-performance machine learning tool, which is built to provide a more transparent, dependable and efficient data analytics service. We discuss the machine learning aspects of Silas and demonstrate the advantage of Silas in its predictive and computational performance. We show that several customised algorithms in Silas yield better predictions in a significantly shorter time compared to the state-of-the-art. Another focus of Silas is on providing a formal foundation of decision trees to support logical analysis and verification of learned prediction models. We illustrate the potential capabilities of the fusion of machine learning and logical reasoning by showcasing applications in three directions: formal verification of the prediction model against user specifications, training correct-by-construction models, and explaining the decision-making of predictions.\n\n[Journal Paper](https://www.sciencedirect.com/science/article/pii/S0957417421002475)\n\n[Preprint PDF](https://zhehou.github.io/papers/Silas_and_comparison_paper_Expert_Systems_with_Applications.pdf)\n\n[Educational Version Download](https://www.depintel.com/silas_download.html)\n\n[Related Work](https://zhehou.github.io/papers/by-topic.html)", "link": "https://www.reddit.com/r/MachineLearning/comments/nubmwp/r_silas_a_highperformance_machine_learning/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[r] silas: a high-performance machine learning foundation for logical reasoning and verification /!/ silas is a generic data mining and predictive analytics software toolkit built upon advanced machine learning, automated reasoning and artificial intelligence techniques. it deals with any type of structured data and performs tasks such as classification, regression, segmentation, anomaly detection, prediction, and more.\n\nthe below paper introduces silas as a high-performance machine learning -----> tool !!! , which is built to provide a more transparent, dependable and efficient data analytics service. we discuss the machine learning aspects of silas and demonstrate the advantage of silas in its predictive and computational performance. we show that several customised algorithms in silas yield better predictions in a significantly shorter time compared to the state-of-the-art. another focus of silas is on providing a formal foundation of decision trees to support logical analysis and verification of learned prediction models. we illustrate the potential capabilities of the fusion of machine learning and logical reasoning by showcasing applications in three directions: formal verification of the prediction model against user specifications, training correct-by-construction models, and explaining the decision-making of predictions.\n\n[journal paper](https://www.sciencedirect.com/science/article/pii/s0957417421002475)\n\n[preprint pdf](https://zhehou.github.io/papers/silas_and_comparison_paper_expert_systems_with_applications.pdf)\n\n[educational version download](https://www.depintel.com/silas_download.html)\n\n[related work](https://zhehou.github.io/papers/by-topic.html)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/nubmwp/r_silas_a_highperformance_machine_learning/',)", "identifyer": 5726829, "year": "2021"}, {"autor": "DrTurb0", "date": 1623683421000, "content": "[R] Project for my Masterthesis /!/ First I\u2019ll explain what my thesis is about:\nIt\u2019s about quality control in car manufacturing. \nCurrently a person with a tool needs to go around a finished car and needs to measure all the gaps between the metal sheets of the bodywork of the car. Now I want to write in theory, If this can be automated with AI, cameras are on the assembly line and take photos of the car while it passes by, the photos are analyzed and checked if all the gaps are perfectly wide and parallel. \nI\u2019m aware that AI is not strictly necessary, Laser scanning (3D-scanning) of the car could do the same job. I\u2019m writing if the AI is helpful and can do it better or more precise. \n\nMy masterthesis is not coding this, it\u2019s just writing about it. I want to research all the needed components though, so someone could easily Code it after reading my thesis, I do all the research. \n\nSo ideally I need a dataset of car bodies or other objects like furniture, where gaps are visible. But I\u2019m doing just prototyping, so an AI that can just recognize lines and tell if they are parallel is enough. \n\nThanks a lot, if you have any questions, feel free to ask, if you have help, comments, criticism, please tell me.", "link": "https://www.reddit.com/r/MachineLearning/comments/nzoyr6/r_project_for_my_masterthesis/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[r] project for my masterthesis /!/ first i\u2019ll explain what my thesis is about:\nit\u2019s about quality control in car manufacturing. \ncurrently a person with a -----> tool !!!  needs to go around a finished car and needs to measure all the gaps between the metal sheets of the bodywork of the car. now i want to write in theory, if this can be automated with ai, cameras are on the assembly line and take photos of the car while it passes by, the photos are analyzed and checked if all the gaps are perfectly wide and parallel. \ni\u2019m aware that ai is not strictly necessary, laser scanning (3d-scanning) of the car could do the same job. i\u2019m writing if the ai is helpful and can do it better or more precise. \n\nmy masterthesis is not coding this, it\u2019s just writing about it. i want to research all the needed components though, so someone could easily code it after reading my thesis, i do all the research. \n\nso ideally i need a dataset of car bodies or other objects like furniture, where gaps are visible. but i\u2019m doing just prototyping, so an ai that can just recognize lines and tell if they are parallel is enough. \n\nthanks a lot, if you have any questions, feel free to ask, if you have help, comments, criticism, please tell me.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 15, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/nzoyr6/r_project_for_my_masterthesis/',)", "identifyer": 5726922, "year": "2021"}, {"autor": "MarurSri", "date": 1630949824000, "content": "\u201c[Project]\u201d Manual Annotation Tool for Relation Extraction /!/ Over the past week I\u2019ve been looking for a manual annotation tool for relation extraction like this (e1,relation,e2). I have found UBIAI and Prodigy but they offer paid services.\n\nI\u2019m looking for open source tools for my project. BRAT and Inception are used for RE but I\u2019m not considering them because of their limitations.\n\nLooking for open source tools for manual annotation\n\nThanks", "link": "https://www.reddit.com/r/MachineLearning/comments/pj48ka/project_manual_annotation_tool_for_relation/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "\u201c[project]\u201d manual annotation -----> tool !!!  for relation extraction /!/ over the past week i\u2019ve been looking for a manual annotation -----> tool !!!  for relation extraction like this (e1,relation,e2). i have found ubiai and prodigy but they offer paid services.\n\ni\u2019m looking for open source tools for my project. brat and inception are used for re but i\u2019m not considering them because of their limitations.\n\nlooking for open source tools for manual annotation\n\nthanks", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 5, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/pj48ka/project_manual_annotation_tool_for_relation/',)", "identifyer": 5727013, "year": "2021"}, {"autor": "MarurSri", "date": 1630948901000, "content": "Data Annotation Tool for Relation Extraction. /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/pj3wuo/data_annotation_tool_for_relation_extraction/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "data annotation -----> tool !!!  for relation extraction. /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/pj3wuo/data_annotation_tool_for_relation_extraction/',)", "identifyer": 5727015, "year": "2021"}, {"autor": "user182740", "date": 1630934418000, "content": "[P] Cockpit: A Practical Debugging Tool for Training Deep Neural Networks /!/ After reading [this comment](https://www.reddit.com/r/MachineLearning/comments/ouiegi/d_sudden_drop_in_loss_after_hours_of_no/h72p3ly?utm_source=share&amp;utm_medium=web2x&amp;context=3), I tried to look for a tool that monitors these values, and I found Cockpit. Relevant links:\n\nPaper: [https://arxiv.org/abs/2102.06604](https://arxiv.org/abs/2102.06604)\n\nCode: [https://github.com/f-dangel/cockpit](https://github.com/f-dangel/cockpit)\n\nFrom table 1 in the paper, here is what you can monitor:\n\nhttps://preview.redd.it/0y7g5j0gvvl71.png?width=1407&amp;format=png&amp;auto=webp&amp;s=d9f746edb1acbc335a5cd20e2d20bf5568fb8044\n\nI have not tried it yet, but interested to know if anyone else has.", "link": "https://www.reddit.com/r/MachineLearning/comments/pizaps/p_cockpit_a_practical_debugging_tool_for_training/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] cockpit: a practical debugging -----> tool !!!  for training deep neural networks /!/ after reading [this comment](https://www.reddit.com/r/machinelearning/comments/ouiegi/d_sudden_drop_in_loss_after_hours_of_no/h72p3ly?utm_source=share&amp;utm_medium=web2x&amp;context=3), i tried to look for a tool that monitors these values, and i found cockpit. relevant links:\n\npaper: [https://arxiv.org/abs/2102.06604](https://arxiv.org/abs/2102.06604)\n\ncode: [https://github.com/f-dangel/cockpit](https://github.com/f-dangel/cockpit)\n\nfrom table 1 in the paper, here is what you can monitor:\n\nhttps://preview.redd.it/0y7g5j0gvvl71.png?width=1407&amp;format=png&amp;auto=webp&amp;s=d9f746edb1acbc335a5cd20e2d20bf5568fb8044\n\ni have not tried it yet, but interested to know if anyone else has.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/pizaps/p_cockpit_a_practical_debugging_tool_for_training/',)", "identifyer": 5727030, "year": "2021"}, {"autor": "CaptainI9C3G6", "date": 1630927488000, "content": "[D] Can anyone recommend a tool for sorting images into folders for image classification? /!/ Hi, so ideally I'd like a tool with which I can open a folder and show me each image in turn. ON each image I would then like to press a number key to move that image to one of the subfolders in that folder. Each image is about 3-4 MB so ideally the moving would be in the background.\n\nDoes anything like this exist?", "link": "https://www.reddit.com/r/MachineLearning/comments/pixkhd/d_can_anyone_recommend_a_tool_for_sorting_images/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] can anyone recommend a -----> tool !!!  for sorting images into folders for image classification? /!/ hi, so ideally i'd like a tool with which i can open a folder and show me each image in turn. on each image i would then like to press a number key to move that image to one of the subfolders in that folder. each image is about 3-4 mb so ideally the moving would be in the background.\n\ndoes anything like this exist?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 12, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/pixkhd/d_can_anyone_recommend_a_tool_for_sorting_images/',)", "identifyer": 5727036, "year": "2021"}, {"autor": "catthugfrank", "date": 1631540180000, "content": "[D] How to Read Data Faster? /!/ Hello all, I was wondering what techniques are there to read and label data faster? I'm new at my job and to the ML world and haven't found many techniques online. I currently run through ~500-1000 paragraphs a week for labeling and think there must be a way to do this faster.\n\nI have made one tool so far that extracts the sentence and highlights keywords so I only have to read half or a quarter of the paragraphs but I'm thinking there must be a more sophisticated method.", "link": "https://www.reddit.com/r/MachineLearning/comments/pnfl47/d_how_to_read_data_faster/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] how to read data faster? /!/ hello all, i was wondering what techniques are there to read and label data faster? i'm new at my job and to the ml world and haven't found many techniques online. i currently run through ~500-1000 paragraphs a week for labeling and think there must be a way to do this faster.\n\ni have made one -----> tool !!!  so far that extracts the sentence and highlights keywords so i only have to read half or a quarter of the paragraphs but i'm thinking there must be a more sophisticated method.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/pnfl47/d_how_to_read_data_faster/',)", "identifyer": 5727087, "year": "2021"}, {"autor": "matt_paraphrasetool", "date": 1631503174000, "content": "[P] Very Powerful Paraphrase Tool, 14 Free Modes, Elaborates Sentences with Details, Can Even Make Them Sound More Diplomatic", "link": "https://www.reddit.com/r/MachineLearning/comments/pn7ca7/p_very_powerful_paraphrase_tool_14_free_modes/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] very powerful paraphrase -----> tool !!! , 14 free modes, elaborates sentences with details, can even make them sound more diplomatic", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('link',)", "medialink": "('https://paraphrasetool.com',)", "identifyer": 5727121, "year": "2021"}, {"autor": "tiglet2", "date": 1624308849000, "content": "Implementation of AI into healthcare systems - pros and cons [D] /!/ I am working on a school project centred around the effect of AI on the healthcare system (specifically the NHS but can be viewed in a more general context). I wanted to explore different opinions on this - do you think AI could potentially lead to a poorer quality of healthcare? There is no doubt that the use of AI in these systems will only increase but are we ready for this? I have been researching a bit about issues with AI (eg. in the context of a diagnostics tool) and have found a bit about biased algorithms which arise from a lack of input data (please correct me if this is wrong). What sorts of repercussions would an issue like this have on peoples' health?\n\nWhat do you think the greatest danger that AI poses on patients and clinicians is? And are there certain groups of people who are more at risk to these problems?\n\nAny opinions and discussions would be greatly appreciated to help me gain more insight into this topic! Thank you.", "link": "https://www.reddit.com/r/MachineLearning/comments/o559zx/implementation_of_ai_into_healthcare_systems_pros/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "implementation of ai into healthcare systems - pros and cons [d] /!/ i am working on a school project centred around the effect of ai on the healthcare system (specifically the nhs but can be viewed in a more general context). i wanted to explore different opinions on this - do you think ai could potentially lead to a poorer quality of healthcare? there is no doubt that the use of ai in these systems will only increase but are we ready for this? i have been researching a bit about issues with ai (eg. in the context of a diagnostics -----> tool !!! ) and have found a bit about biased algorithms which arise from a lack of input data (please correct me if this is wrong). what sorts of repercussions would an issue like this have on peoples' health?\n\nwhat do you think the greatest danger that ai poses on patients and clinicians is? and are there certain groups of people who are more at risk to these problems?\n\nany opinions and discussions would be greatly appreciated to help me gain more insight into this topic! thank you.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/o559zx/implementation_of_ai_into_healthcare_systems_pros/',)", "identifyer": 5727251, "year": "2021"}, {"autor": "HXSC", "date": 1624887173000, "content": "[D] How to convince management of machine learning? /!/ This is a question/discussion not so much of machine learning itself, but how to convince others of the merits of ML models.\n\nFor a lot of people, it is often difficult to understand the results of machine learning, or why it makes decisions the way it does. Of course there are explainability/interpretability methods, but I often find management objecting to ML because \"it makes no sense\" and because \"I don't understand it\", especially if they have been making heuristics-based decisions for decades in the field.\n\nML can be a sledgehammer that's not always fit for all occasions, but sometimes it is. But it sometimes seems difficult to convince people that it is sometimes the right tool.", "link": "https://www.reddit.com/r/MachineLearning/comments/o9kcj2/d_how_to_convince_management_of_machine_learning/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] how to convince management of machine learning? /!/ this is a question/discussion not so much of machine learning itself, but how to convince others of the merits of ml models.\n\nfor a lot of people, it is often difficult to understand the results of machine learning, or why it makes decisions the way it does. of course there are explainability/interpretability methods, but i often find management objecting to ml because \"it makes no sense\" and because \"i don't understand it\", especially if they have been making heuristics-based decisions for decades in the field.\n\nml can be a sledgehammer that's not always fit for all occasions, but sometimes it is. but it sometimes seems difficult to convince people that it is sometimes the right -----> tool !!! .", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 7, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/o9kcj2/d_how_to_convince_management_of_machine_learning/',)", "identifyer": 5727343, "year": "2021"}, {"autor": "ProfJasonCorso", "date": 1612373619000, "content": "[D] FiftyOne update: open-source visual dataset and model performance analysis---Setting a new bar for devtools?! /!/ I wrote a new [blog](https://medium.com/voxel51/fiftyone-six-months-post-launch-7d117b975128) post this morning that updates key features that our open-source dataset and model performance analysis tool [FiftyOne](https://fiftyone.ai) has seen in the six months since launch.  After feedback from hundreds of users, the tool has seen significant growth in usability, notebook support, programmable queries.  Would love your feedback and discussion.", "link": "https://www.reddit.com/r/MachineLearning/comments/lbt06z/d_fiftyone_update_opensource_visual_dataset_and/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] fiftyone update: open-source visual dataset and model performance analysis---setting a new bar for devtools?! /!/ i wrote a new [blog](https://medium.com/voxel51/fiftyone-six-months-post-launch-7d117b975128) post this morning that updates key features that our open-source dataset and model performance analysis -----> tool !!!  [fiftyone](https://fiftyone.ai) has seen in the six months since launch.  after feedback from hundreds of users, the tool has seen significant growth in usability, notebook support, programmable queries.  would love your feedback and discussion.", "sortedWord": "None", "removed": "('nan',)", "score": 2, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/lbt06z/d_fiftyone_update_opensource_visual_dataset_and/',)", "identifyer": 5727383, "year": "2021"}, {"autor": "sgevorg", "date": 1612363882000, "content": "[N] Aim 1.3.5 is out! Take a look at the new features... /!/ Hey r/MachineLearning community!\n\n[Aim](https://github.com/aimhubio/aim) v1.3.5 is out!\n\nWe are building an open-source self-hosted tool for AI training run comparison. It can now handle 1000s of experiments at once - super easy to get started with. \ud83d\udd27\n\nThanks for the incredible support - helping us build awesome open source ML dev tools for the community. This is a result of the feedback the community have been sharing with us since Aim launched here.\n\nCheck out the new features at [play.aimstack.io](http://play.aimstack.io:43900/explore?search=eyJjaGFydCI6eyJzZXR0aW5ncyI6eyJwZXJzaXN0ZW50Ijp7InlTY2FsZSI6MCwiem9vbU1vZGUiOmZhbHNlLCJ6b29tSGlzdG9yeSI6W10sInBlcnNpc3RlbnQiOnsieVNjYWxlIjowLCJ6b29tTW9kZSI6ZmFsc2UsInpvb21IaXN0b3J5IjpbXSwicGVyc2lzdGVudCI6eyJ5U2NhbGUiOjAsInpvb21Nb2RlIjpmYWxzZSwiem9vbUhpc3RvcnkiOltdLCJwZXJzaXN0ZW50Ijp7ImRpc3BsYXlPdXRsaWVycyI6ZmFsc2UsInpvb20iOm51bGwsImludGVycG9sYXRlIjpmYWxzZX0sImRpc3BsYXlPdXRsaWVycyI6ZmFsc2UsImludGVycG9sYXRlIjp0cnVlLCJ6b29tIjp7IjAiOnsieCI6WzIuNjg1MTczMTQ4MTA1MzM4NSwzOF0sInkiOlszLjQ1NDEwMDA0ODU0MjAyMywzLjY4MjMzOTM5NzAxNzAyOF19LCIxIjp7IngiOlsyLjUzNzIwNjUxMzA3NzU1NywzOF0sInkiOlsyNy42NzE5Nzg3NjM2MTk2ODQsMzIuNTgxOTk5Nzc4NzQ3NTZdfSwiMiI6eyJ4IjpbNi4xMzAwNzcxODU4MDYzMzIsMjddLCJ5IjpbMy40MzIxNTAwNTM5Nzc5NjYsMy42NDc3MDkzNTA2NzM4NjZdfSwiMyI6eyJ4IjpbMy40OTA5MDg3NzA3OTQyNDE0LDI3XSwieSI6WzI5LjE0MjQ0OTMzOTY5ODA4MiwzMy43NzI5OTk5NTQyMjM2MzZdfSwiNCI6eyJ4IjpbNC42OTMxNjM4MzkzNzM3MDgsMjhdLCJ5IjpbMjkuNDA5MjQ3MjgyOTAwMjY0LDMyLjEwMzk5OTcxMDA4MzAxXX0sIjUiOnsieCI6WzYuOTQxMTk3MDgwOTUyMjg1LDI3LjA0MTk0NTU0NjY2ODI2XSwieSI6WzMuNDgxNjAwMDgxOTIwNjI0LDMuNTgyNzgzODQ4MDA3ODUzOF19fX0sInpvb20iOm51bGx9LCJ6b29tIjpudWxsLCJkaXNwbGF5T3V0bGllcnMiOmZhbHNlLCJpbnRlcnBvbGF0ZSI6dHJ1ZX19LCJmb2N1c2VkIjp7ImNpcmNsZSI6eyJhY3RpdmUiOmZhbHNlLCJydW5IYXNoIjpudWxsLCJtZXRyaWNOYW1lIjpudWxsLCJ0cmFjZUNvbnRleHQiOm51bGwsInN0ZXAiOm51bGx9fX0sInNlYXJjaCI6eyJxdWVyeSI6ImJsZXUgaWYgY29udGV4dC5zdWJzZXQgPT0gdGVzdCBhbmQgaHBhcmFtcy5sZWFybmluZ19yYXRlID4gMC4wMDAwMSIsInYiOjF9LCJjb250ZXh0RmlsdGVyIjp7Imdyb3VwQnlDb2xvciI6WyJwYXJhbXMuaHBhcmFtcy5hbGlnbiIsInBhcmFtcy5ocGFyYW1zLm1heF9rIiwicGFyYW1zLmRhdGFzZXQucHJlcHJvYyJdLCJncm91cEJ5Q2hhcnQiOlsicGFyYW1zLmhwYXJhbXMuYWxpZ24iXSwiZ3JvdXBCeVN0eWxlIjpbXSwiYWdncmVnYXRlZCI6dHJ1ZX19)\n\nBelow are a few gifs for main features and their descriptions:\n\n**1. Training run and experiments activities view on Dashboard**\n\n[Easily access the training runs and experiments by date\\/activity](https://i.redd.it/h20hgquhy9f61.gif)\n\n**2. X-axis alignment by epoch**\n\n[Align metrics on x-axis by epoch with a button click](https://i.redd.it/f7xv7vtly9f61.gif)\n\n**3. Ordering runs both on Explore and on Dashboard**\n\n[Sort the tables by any metric](https://i.redd.it/rx4v1n2uy9f61.gif)\n\nIf you haven't yet, drop us a [star](https://github.com/aimhubio/aim) for support! \ud83d\ude0a\n\nCome say hi at the [Aim Slack community](https://slack.aimstack.io/).\n\nCheck out this version and let us know how we can improve it further \ud83d\ude0a", "link": "https://www.reddit.com/r/MachineLearning/comments/lbp5st/n_aim_135_is_out_take_a_look_at_the_new_features/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[n] aim 1.3.5 is out! take a look at the new features... /!/ hey r/machinelearning community!\n\n[aim](https://github.com/aimhubio/aim) v1.3.5 is out!\n\nwe are building an open-source self-hosted -----> tool !!!  for ai training run comparison. it can now handle 1000s of experiments at once - super easy to get started with. \ud83d\udd27\n\nthanks for the incredible support - helping us build awesome open source ml dev tools for the community. this is a result of the feedback the community have been sharing with us since aim launched here.\n\ncheck out the new features at [play.aimstack.io](http://play.aimstack.io:43900/explore?search=eyjjagfydci6eyjzzxr0aw5ncyi6eyjwzxjzaxn0zw50ijp7inlty2fszsi6mcwiem9vbu1vzguiomzhbhnllcj6b29tsglzdg9yesi6w10sinblcnnpc3rlbnqionsievnjywxlijowlcj6b29ttw9kzsi6zmfsc2usinpvb21iaxn0b3j5ijpbxswicgvyc2lzdgvudci6eyj5u2nhbguiojasinpvb21nb2rlijpmywxzzswiem9vbuhpc3rvcnkioltdlcjwzxjzaxn0zw50ijp7imrpc3bsyxlpdxrsawvycyi6zmfsc2usinpvb20iom51bgwsimludgvycg9syxrlijpmywxzzx0simrpc3bsyxlpdxrsawvycyi6zmfsc2usimludgvycg9syxrlijp0cnvllcj6b29tijp7ijaionsieci6wziunjg1mtczmtq4mta1mzm4nswzof0sinkiolszljq1ndewmda0odu0mjaymywzljy4mjmzotm5nzaxnzayof19lcixijp7ingiolsyljuznziwnjuxmza3nzu1nywzof0sinkiolsyny42nze5nzg3njm2mtk2odqsmziuntgxotk5nzc4nzq3ntzdfswimii6eyj4ijpbni4xmzawnzcxodu4mdyzmzismjddlcj5ijpbmy40mzixntawntm5nzc5njysmy42ndc3mdkznta2nzm4njzdfswimyi6eyj4ijpbmy40ota5mdg3nza3otqynde0ldi3xswiesi6wzi5lje0mjq0otmzoty5oda4miwzmy43nzi5otk5ntqymjm2mzzdfswinci6eyj4ijpbnc42otmxnjm4mzkznzm3mdgsmjhdlcj5ijpbmjkunda5mjq3mjgyotawmjy0ldmyljewmzk5otcxmda4mzaxxx0sijuionsieci6wzyuotqxmtk3mdgwotuymjg1ldi3lja0mtk0ntu0njy2odi2xswiesi6wzmundgxnjawmdgxotiwnji0ldmuntgynzgzodq4mda3oduzof19fx0sinpvb20iom51bgx9lcj6b29tijpudwxslcjkaxnwbgf5t3v0bgllcnmiomzhbhnllcjpbnrlcnbvbgf0zsi6dhj1zx19lcjmb2n1c2vkijp7imnpcmnszsi6eyjhy3rpdmuiomzhbhnllcjydw5iyxnoijpudwxslcjtzxryawnoyw1lijpudwxslcj0cmfjzunvbnrlehqiom51bgwsinn0zxaiom51bgx9fx0sinnlyxjjaci6eyjxdwvyesi6imjszxugawygy29udgv4dc5zdwjzzxqgpt0gdgvzdcbhbmqgahbhcmftcy5szwfybmluz19yyxrlid4gmc4wmdawmsisinyiojf9lcjjb250zxh0rmlsdgvyijp7imdyb3vwqnldb2xvcii6wyjwyxjhbxmuahbhcmftcy5hbglnbiisinbhcmftcy5ocgfyyw1zlm1hef9riiwicgfyyw1zlmrhdgfzzxquchjlchjvyyjdlcjncm91cej5q2hhcnqiolsicgfyyw1zlmhwyxjhbxmuywxpz24ixswiz3jvdxbcevn0ewxlijpbxswiywdncmvnyxrlzci6dhj1zx19)\n\nbelow are a few gifs for main features and their descriptions:\n\n**1. training run and experiments activities view on dashboard**\n\n[easily access the training runs and experiments by date\\/activity](https://i.redd.it/h20hgquhy9f61.gif)\n\n**2. x-axis alignment by epoch**\n\n[align metrics on x-axis by epoch with a button click](https://i.redd.it/f7xv7vtly9f61.gif)\n\n**3. ordering runs both on explore and on dashboard**\n\n[sort the tables by any metric](https://i.redd.it/rx4v1n2uy9f61.gif)\n\nif you haven't yet, drop us a [star](https://github.com/aimhubio/aim) for support! \ud83d\ude0a\n\ncome say hi at the [aim slack community](https://slack.aimstack.io/).\n\ncheck out this version and let us know how we can improve it further \ud83d\ude0a", "sortedWord": "None", "removed": "('nan',)", "score": 37, "comments": 17, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/lbp5st/n_aim_135_is_out_take_a_look_at_the_new_features/',)", "identifyer": 5727401, "year": "2021"}, {"autor": "SentiF2078", "date": 1612353853000, "content": "[N] Full stack No-code ML: Lobe + SentiFactory + DigitalOcean /!/ Hey!\n\nWe would like to share the tool we build to complement no-code flow with end to end solution by combining Lobe + SentiFactory + DigitalOcean.\n\nLobe is used as a no-code ML tool that allows you to build CV models quickly, then SentiFactory can take those models and deploy them as RESTful APIs to DigitalOcean by creating a droplet with the container and model wrapper.\n\nIs it of any interest to the group? Are there any people who do full no-code ML? Not only AutoML but like real no-code ML?\n\nThe full scenario is here - [https://sentifactory.com/blog/how-to-build-a-machine-learning-no-code-product-for-10-using-lobe-sentifactory-and-digitalocean](https://sentifactory.com/blog/how-to-build-a-machine-learning-no-code-product-for-10-using-lobe-sentifactory-and-digitalocean)", "link": "https://www.reddit.com/r/MachineLearning/comments/lbm41m/n_full_stack_nocode_ml_lobe_sentifactory/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[n] full stack no-code ml: lobe + sentifactory + digitalocean /!/ hey!\n\nwe would like to share the -----> tool !!!  we build to complement no-code flow with end to end solution by combining lobe + sentifactory + digitalocean.\n\nlobe is used as a no-code ml tool that allows you to build cv models quickly, then sentifactory can take those models and deploy them as restful apis to digitalocean by creating a droplet with the container and model wrapper.\n\nis it of any interest to the group? are there any people who do full no-code ml? not only automl but like real no-code ml?\n\nthe full scenario is here - [https://sentifactory.com/blog/how-to-build-a-machine-learning-no-code-product-for-10-using-lobe-sentifactory-and-digitalocean](https://sentifactory.com/blog/how-to-build-a-machine-learning-no-code-product-for-10-using-lobe-sentifactory-and-digitalocean)", "sortedWord": "None", "removed": "('nan',)", "score": 0, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/lbm41m/n_full_stack_nocode_ml_lobe_sentifactory/',)", "identifyer": 5727416, "year": "2021"}, {"autor": "JohnClayborn", "date": 1633561692000, "content": "Archive Text Reader [Project] /!/ Hi all, \n\nI'm new to Machine Learning and I'm wondering if someone may have already made a tool for this, or if they might be interested in helping to develop one. I run an online military museum. We have partnered with 2 other online museums and are poring over tens of thousands of pages of data.   \n\n\nI thought about how AI might be able to serve two purposes - to be able to read the page and create a transcription of the image, and to examine scanned pages where the text is blurry or faded and provide a translation. \n\nDoes anyone know if either of these might exist on GitHub or something already? Or would anyone be willing to help create such a tool? \n\nThanks.", "link": "https://www.reddit.com/r/MachineLearning/comments/q2vtte/archive_text_reader_project/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "archive text reader [project] /!/ hi all, \n\ni'm new to machine learning and i'm wondering if someone may have already made a -----> tool !!!  for this, or if they might be interested in helping to develop one. i run an online military museum. we have partnered with 2 other online museums and are poring over tens of thousands of pages of data.   \n\n\ni thought about how ai might be able to serve two purposes - to be able to read the page and create a transcription of the image, and to examine scanned pages where the text is blurry or faded and provide a translation. \n\ndoes anyone know if either of these might exist on github or something already? or would anyone be willing to help create such a tool? \n\nthanks.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/q2vtte/archive_text_reader_project/',)", "identifyer": 5727747, "year": "2021"}, {"autor": "socratesandstark", "date": 1634070498000, "content": "[D] Text summarization for learnable content /!/ Hypothetically, if a framework of general human knowledge was made with all of the major domains, subdomains, and individual concepts mapped out and linked together with directed pathways resembling a Google Maps route, could a text summary tool be used to feasibly populate that framework with learnable content?\n\nFor accuracy, let's assume the generated content could be annotated by experts.", "link": "https://www.reddit.com/r/MachineLearning/comments/q6uxzp/d_text_summarization_for_learnable_content/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] text summarization for learnable content /!/ hypothetically, if a framework of general human knowledge was made with all of the major domains, subdomains, and individual concepts mapped out and linked together with directed pathways resembling a google maps route, could a text summary -----> tool !!!  be used to feasibly populate that framework with learnable content?\n\nfor accuracy, let's assume the generated content could be annotated by experts.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/q6uxzp/d_text_summarization_for_learnable_content/',)", "identifyer": 5727865, "year": "2021"}, {"autor": "pramook", "date": 1612218048000, "content": "[R][P] Talking Head Anime from a Single Image 2: More Expressive /!/ In 2019, I created a [network](https://www.reddit.com/r/MachineLearning/comments/e1k092/rp_talking_head_anime_from_a_single_image/) that can anime faces of anime characters. It takes a frontalized face image and a 6-dimensional vector that specify the pose (i.e. the *pose vector*), and it outputs another image of the character with the specified pose.\n\nOne of the main drawbacks of the network is its lack of expressiveness: not counting its ability to rotate faces, all it can do is closing eyes and mouths.\n\nFor the last one year or so, I have been working on a project to make the network more capable. The new version can manipulate 39 types of movements of facial features. (Most of these are equivalent to \"[blend shapes](https://en.wikipedia.org/wiki/Morph_target_animation)\" in computer graphics lingo.) It can move the eyebrows and the irises and make several different eye and mouth shapes. Characters can now make many different types of facial expressions.\n\n[Expressions generated by the network.](https://preview.redd.it/hgwelw9zsxe61.jpg?width=1055&amp;format=pjpg&amp;auto=webp&amp;s=9c6c2ca290e61c9c332c978aa18f53add9ffd7fa)\n\nWith the new network, I created three content creation tools:\n\n1. [A new GUI for detailed manipulation of anime facial expressions.](https://www.youtube.com/watch?v=535mjOjpy38)\n2. [A new system for real-time performance as anime characters.](https://www.youtube.com/watch?v=m13MLXNwdfY)\n3. A program that converts motions authored for 3D models so I can use it to drive characters images.\n\nWith the last tool, I created some music videos using downloadable 3D motions. Here's one:\n\nhttps://reddit.com/link/lafy8i/video/oaky6o6ivxe61/player\n\nPerhaps the more fun part of this project is to lip sync songs and then have some of my favorite characters sing them. Here's my rendition of the Internet famous \"Dame Da Ne\" meme, created with the help of the second tool.\n\nhttps://reddit.com/link/lafy8i/video/wtq834bbwxe61/player\n\nThis type of meme videos are typically created with the First Order Motion Model of Siarohin et al., but the model does not work well on anime characters and has a tendency to distort faces (to comedic effects). On the other hand, my network is specialized to anime characters, and you can see from the video that it preserves their beauty well.\n\nYou can read more about the project and see more contents at http://pkhungurn.github.io/talking-head-anime-2/", "link": "https://www.reddit.com/r/MachineLearning/comments/lafy8i/rp_talking_head_anime_from_a_single_image_2_more/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[r][p] talking head anime from a single image 2: more expressive /!/ in 2019, i created a [network](https://www.reddit.com/r/machinelearning/comments/e1k092/rp_talking_head_anime_from_a_single_image/) that can anime faces of anime characters. it takes a frontalized face image and a 6-dimensional vector that specify the pose (i.e. the *pose vector*), and it outputs another image of the character with the specified pose.\n\none of the main drawbacks of the network is its lack of expressiveness: not counting its ability to rotate faces, all it can do is closing eyes and mouths.\n\nfor the last one year or so, i have been working on a project to make the network more capable. the new version can manipulate 39 types of movements of facial features. (most of these are equivalent to \"[blend shapes](https://en.wikipedia.org/wiki/morph_target_animation)\" in computer graphics lingo.) it can move the eyebrows and the irises and make several different eye and mouth shapes. characters can now make many different types of facial expressions.\n\n[expressions generated by the network.](https://preview.redd.it/hgwelw9zsxe61.jpg?width=1055&amp;format=pjpg&amp;auto=webp&amp;s=9c6c2ca290e61c9c332c978aa18f53add9ffd7fa)\n\nwith the new network, i created three content creation tools:\n\n1. [a new gui for detailed manipulation of anime facial expressions.](https://www.youtube.com/watch?v=535mjojpy38)\n2. [a new system for real-time performance as anime characters.](https://www.youtube.com/watch?v=m13mlxnwdfy)\n3. a program that converts motions authored for 3d models so i can use it to drive characters images.\n\nwith the last -----> tool !!! , i created some music videos using downloadable 3d motions. here's one:\n\nhttps://reddit.com/link/lafy8i/video/oaky6o6ivxe61/player\n\nperhaps the more fun part of this project is to lip sync songs and then have some of my favorite characters sing them. here's my rendition of the internet famous \"dame da ne\" meme, created with the help of the second tool.\n\nhttps://reddit.com/link/lafy8i/video/wtq834bbwxe61/player\n\nthis type of meme videos are typically created with the first order motion model of siarohin et al., but the model does not work well on anime characters and has a tendency to distort faces (to comedic effects). on the other hand, my network is specialized to anime characters, and you can see from the video that it preserves their beauty well.\n\nyou can read more about the project and see more contents at http://pkhungurn.github.io/talking-head-anime-2/", "sortedWord": "None", "removed": "('nan',)", "score": 218, "comments": 41, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/lafy8i/rp_talking_head_anime_from_a_single_image_2_more/',)", "identifyer": 5727884, "year": "2021"}, {"autor": "benjaminpkane", "date": 1612211238000, "content": "[D] CV/ML needs better tooling /!/ I have been working on various stages of the machine learning lifecycle, specifically the CV/ML lifecycle, for over three years now. The one overarching observation I have had is the field lacks productivity, specifically with datasets. The science is great, but iterating and chipping away at problems to improve is almost impossible. Perhaps the FAANGs of the world have great tooling, but I cannot speak to that.\n\nShamelessly, I have written a [blog post](https://medium.com/voxel51/on-notebooks-and-the-future-of-computer-vision-9908da4f9c04) somewhat related to this, talking about Jupyter Notebooks and [FiftyOne](http://fiftyone.ai/), the tool I am now working on, and how I hope it moves the needle forward on CV/ML productivity.\n\nI'm wondering if anyone agrees with this issue. More open standards to improve on and experiment with datasets is critical in my opinion. I could go on about how the annotation industry is cruel and inefficient, but I think my main point is I don't think a CV/ML can justify itself in the market/industry in a democratized sense without more open tools/standards/processes...", "link": "https://www.reddit.com/r/MachineLearning/comments/lad5re/d_cvml_needs_better_tooling/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] cv/ml needs better tooling /!/ i have been working on various stages of the machine learning lifecycle, specifically the cv/ml lifecycle, for over three years now. the one overarching observation i have had is the field lacks productivity, specifically with datasets. the science is great, but iterating and chipping away at problems to improve is almost impossible. perhaps the faangs of the world have great tooling, but i cannot speak to that.\n\nshamelessly, i have written a [blog post](https://medium.com/voxel51/on-notebooks-and-the-future-of-computer-vision-9908da4f9c04) somewhat related to this, talking about jupyter notebooks and [fiftyone](http://fiftyone.ai/), the -----> tool !!!  i am now working on, and how i hope it moves the needle forward on cv/ml productivity.\n\ni'm wondering if anyone agrees with this issue. more open standards to improve on and experiment with datasets is critical in my opinion. i could go on about how the annotation industry is cruel and inefficient, but i think my main point is i don't think a cv/ml can justify itself in the market/industry in a democratized sense without more open tools/standards/processes...", "sortedWord": "None", "removed": "('nan',)", "score": 11, "comments": 1, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/lad5re/d_cvml_needs_better_tooling/',)", "identifyer": 5727891, "year": "2021"}, {"autor": "htahir1", "date": 1612198117000, "content": "[P] Deploying ML models on a budget /!/ Over the years, I've done a lot of ML side-projects, all involving deploying ML models quickly and in a somewhat robust fashion. The goal is to get the model deployed **as cheaply as possible**, with **as low downtime** as possible. However, when trying various tech, I encountered various problems:\n\n* **Using Cloud Functions (GCP Functions, AWS Lambda):** Low memory (max 2-4GB), quick timeouts, costs scale with time.\n* **Kubernetes Cluster:** Managed cluster costs &gt;$100, and add on top the cost for the resources used. Also, my Ops chops aren't good enough to manage a cluster on my own.\n* **Deploying bare-metal:** Just using a bare-metal instance is probably the most straightforward, but even there the costs were hurting me. For example, for a simple 16 GB VM on Google Cloud Platform costs $65/month. That might not be too much for many people, but for someone who does many side projects, it accumulates!\n\nSo I sat down and tried to create a solution that satisfies the low-cost requirement, and is easy and repeatable across projects. And lo and behold, I came up with my own approach: [BudgetML](https://github.com/ebhy/budgetml).\n\nBudgetML lets you deploy your model on a [Google Cloud Platform preemptible instance](https://cloud.google.com/compute/docs/instances/preemptible) (which is **\\~80% cheaper** than a regular instance) with a **secured HTTPS API** endpoint. The tool sets it up in a way that the instance **autostarts** when it shuts down (at least once every 24 hours) with **only a few minutes of downtime**. Therefore, it ensures the cheapest possible API endpoint with the lowest possible downtime.\n\nThis solved my problem: Sure, I get a few minutes of downtime every day, but that is nothing compared to the cost savings I'm getting (I can use the same 16GB VM on GCP that cost $65/month for only $20/month).\n\nCheck it out on GitHub: [https://github.com/ebhy/budgetml](https://github.com/ebhy/budgetml) . It's open-source (so its free) and (hopefully) developer-friendly. It is by no means meant to be used in a full-fledged production-ready setup. It is simply a means to get a server up and running **as fast as possible** with the **lowest costs possible**. What do you think - is it useful? I can't be the only one to have this particular intersection of requirements for my ML projects.", "link": "https://www.reddit.com/r/MachineLearning/comments/la7o0q/p_deploying_ml_models_on_a_budget/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] deploying ml models on a budget /!/ over the years, i've done a lot of ml side-projects, all involving deploying ml models quickly and in a somewhat robust fashion. the goal is to get the model deployed **as cheaply as possible**, with **as low downtime** as possible. however, when trying various tech, i encountered various problems:\n\n* **using cloud functions (gcp functions, aws lambda):** low memory (max 2-4gb), quick timeouts, costs scale with time.\n* **kubernetes cluster:** managed cluster costs &gt;$100, and add on top the cost for the resources used. also, my ops chops aren't good enough to manage a cluster on my own.\n* **deploying bare-metal:** just using a bare-metal instance is probably the most straightforward, but even there the costs were hurting me. for example, for a simple 16 gb vm on google cloud platform costs $65/month. that might not be too much for many people, but for someone who does many side projects, it accumulates!\n\nso i sat down and tried to create a solution that satisfies the low-cost requirement, and is easy and repeatable across projects. and lo and behold, i came up with my own approach: [budgetml](https://github.com/ebhy/budgetml).\n\nbudgetml lets you deploy your model on a [google cloud platform preemptible instance](https://cloud.google.com/compute/docs/instances/preemptible) (which is **\\~80% cheaper** than a regular instance) with a **secured https api** endpoint. the -----> tool !!!  sets it up in a way that the instance **autostarts** when it shuts down (at least once every 24 hours) with **only a few minutes of downtime**. therefore, it ensures the cheapest possible api endpoint with the lowest possible downtime.\n\nthis solved my problem: sure, i get a few minutes of downtime every day, but that is nothing compared to the cost savings i'm getting (i can use the same 16gb vm on gcp that cost $65/month for only $20/month).\n\ncheck it out on github: [https://github.com/ebhy/budgetml](https://github.com/ebhy/budgetml) . it's open-source (so its free) and (hopefully) developer-friendly. it is by no means meant to be used in a full-fledged production-ready setup. it is simply a means to get a server up and running **as fast as possible** with the **lowest costs possible**. what do you think - is it useful? i can't be the only one to have this particular intersection of requirements for my ml projects.", "sortedWord": "None", "removed": "('nan',)", "score": 320, "comments": 72, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/la7o0q/p_deploying_ml_models_on_a_budget/',)", "identifyer": 5727914, "year": "2021"}, {"autor": "Science_Squid", "date": 1628244206000, "content": "[D] AutoML Fall School /!/ With the interest of this subreddit on our AutoML MOOC we are happy to announce the first AutoML Fall School which will be held virtually November 8th to 12th.\n\nAutoML can be a vital tool for many machine learning practitioners and researchers. While students and professionals are eager to learn more about AutoML, it is rarely taught and addressed in courses in today\u2019s academic landscape. \n\nWith the introduction of the AutoML Fall School we aim to close this glaring gap by providing a platform for graduate students and researchers to learn about core aspects of AutoML. The event will feature lectures and invited talks by renowned experts about topics from fundamental theory to advanced state-of-the-art methods and current challenges. Additionally, you will be able to try your hands at implementing leading AutoML solutions in our hands-on sessions while being mentored by AutoML experts as well as network and exchange ideas in our social events and much more. \n\nRegistrations are now open! Find a preliminary schedule, additional information, and the registration details on our [official website](https://sites.google.com/view/automlschool21/home). We are looking forward to seeing you in November.", "link": "https://www.reddit.com/r/MachineLearning/comments/oz3ocw/d_automl_fall_school/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] automl fall school /!/ with the interest of this subreddit on our automl mooc we are happy to announce the first automl fall school which will be held virtually november 8th to 12th.\n\nautoml can be a vital -----> tool !!!  for many machine learning practitioners and researchers. while students and professionals are eager to learn more about automl, it is rarely taught and addressed in courses in today\u2019s academic landscape. \n\nwith the introduction of the automl fall school we aim to close this glaring gap by providing a platform for graduate students and researchers to learn about core aspects of automl. the event will feature lectures and invited talks by renowned experts about topics from fundamental theory to advanced state-of-the-art methods and current challenges. additionally, you will be able to try your hands at implementing leading automl solutions in our hands-on sessions while being mentored by automl experts as well as network and exchange ideas in our social events and much more. \n\nregistrations are now open! find a preliminary schedule, additional information, and the registration details on our [official website](https://sites.google.com/view/automlschool21/home). we are looking forward to seeing you in november.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 7, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/oz3ocw/d_automl_fall_school/',)", "identifyer": 5728071, "year": "2021"}, {"autor": "Arek993", "date": 1613418197000, "content": "Project management tool to manage a data science project /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/lklc9z/project_management_tool_to_manage_a_data_science/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "project management -----> tool !!!  to manage a data science project /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/lklc9z/project_management_tool_to_manage_a_data_science/',)", "identifyer": 5728091, "year": "2021"}, {"autor": "radaobc", "date": 1628931531000, "content": "[D] Anyone know if the Know Your Data tool from Google will be open sourced? /!/ [Know Your Data](https://knowyourdata-tfds.withgoogle.com/) is a relatively new tool from Google to explore your vision datasets. It\u2019s very powerful but currently limited to exploring the \\~70 datasets Google uploaded so I'm wondering if anyone has any information regarding if it will be open sourced. An open source tool of this type would be greatly beneficial to the computer vision community with all the recent research into dataset bias and mitigation.\n\n[https://knowyourdata-tfds.withgoogle.com/](https://knowyourdata-tfds.withgoogle.com/)", "link": "https://www.reddit.com/r/MachineLearning/comments/p44m4x/d_anyone_know_if_the_know_your_data_tool_from/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] anyone know if the know your data -----> tool !!!  from google will be open sourced? /!/ [know your data](https://knowyourdata-tfds.withgoogle.com/) is a relatively new tool from google to explore your vision datasets. it\u2019s very powerful but currently limited to exploring the \\~70 datasets google uploaded so i'm wondering if anyone has any information regarding if it will be open sourced. an open source tool of this type would be greatly beneficial to the computer vision community with all the recent research into dataset bias and mitigation.\n\n[https://knowyourdata-tfds.withgoogle.com/](https://knowyourdata-tfds.withgoogle.com/)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/p44m4x/d_anyone_know_if_the_know_your_data_tool_from/',)", "identifyer": 5728290, "year": "2021"}, {"autor": "radaobc", "date": 1628931317000, "content": "[Q] Anyone know if the Know Your Data tool from Google will be open sourced? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/p44kqg/q_anyone_know_if_the_know_your_data_tool_from/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[q] anyone know if the know your data -----> tool !!!  from google will be open sourced? /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/p44kqg/q_anyone_know_if_the_know_your_data_tool_from/',)", "identifyer": 5728291, "year": "2021"}, {"autor": "bvobart", "date": 1628841910000, "content": "[P][R] Announcing `mllint` \u2014 a linter for ML project software quality. /!/ Hi there, data scientists, ML engineers and Redditors! I'm doing my MSc thesis on the software quality of ML applications. I've been developing `mllint`, an open-source tool to help assess the software quality of ML projects, help productionise ML applications and bring more software engineering (SE) knowledge to the field of ML.\n\nThis tool, `mllint`, statically analyses your project for adherence to common SE practices and creates a Markdown-formatted report with recommendations on how your project can be improved. It can be run locally on your own device, but can also be integrated in CI pipelines. There is even support for defining custom rules, so you can write your own checks to verify internal company / team practices!\n\nSounds interesting? Give it a try! Check out one of these links for more information:\n\n* Website: [https://bvobart.github.io/mllint/](https://bvobart.github.io/mllint/)\n* Source: [https://github.com/bvobart/mllint](https://github.com/bvobart/mllint)\n* Installation: `pip install -U mllint`\n\nIt would mean a lot to me, `mllint` and the ICSE-SEIP paper I'm writing for my MSc thesis to hear your feedback on `mllint` and its concepts! If you can spare 15 minutes of your time to fill in this survey after playing with `mllint`, then that would be a massive help! :blush:\n\n* Survey: [https://forms.office.com/r/pXtfUKWUDA](https://forms.office.com/r/pXtfUKWUDA)\n\nFeel free to contact me here or on GitHub if you have any questions / issues! Thanks!\n\nDemo:\n\n*Processing video i7r89nx213h71...*", "link": "https://www.reddit.com/r/MachineLearning/comments/p3j2xh/pr_announcing_mllint_a_linter_for_ml_project/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p][r] announcing `mllint` \u2014 a linter for ml project software quality. /!/ hi there, data scientists, ml engineers and redditors! i'm doing my msc thesis on the software quality of ml applications. i've been developing `mllint`, an open-source -----> tool !!!  to help assess the software quality of ml projects, help productionise ml applications and bring more software engineering (se) knowledge to the field of ml.\n\nthis tool, `mllint`, statically analyses your project for adherence to common se practices and creates a markdown-formatted report with recommendations on how your project can be improved. it can be run locally on your own device, but can also be integrated in ci pipelines. there is even support for defining custom rules, so you can write your own checks to verify internal company / team practices!\n\nsounds interesting? give it a try! check out one of these links for more information:\n\n* website: [https://bvobart.github.io/mllint/](https://bvobart.github.io/mllint/)\n* source: [https://github.com/bvobart/mllint](https://github.com/bvobart/mllint)\n* installation: `pip install -u mllint`\n\nit would mean a lot to me, `mllint` and the icse-seip paper i'm writing for my msc thesis to hear your feedback on `mllint` and its concepts! if you can spare 15 minutes of your time to fill in this survey after playing with `mllint`, then that would be a massive help! :blush:\n\n* survey: [https://forms.office.com/r/pxtfukwuda](https://forms.office.com/r/pxtfukwuda)\n\nfeel free to contact me here or on github if you have any questions / issues! thanks!\n\ndemo:\n\n*processing video i7r89nx213h71...*", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 31, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/p3j2xh/pr_announcing_mllint_a_linter_for_ml_project/',)", "identifyer": 5728338, "year": "2021"}, {"autor": "Humes-Bread", "date": 1621017035000, "content": "[P] Machine learning applied to game theory resource? /!/  Hey Friends- you probably get posts from people not in the field who are curious all the time, so apologies.\n\nI'm  looking for any resource I can on tools, applications, companies, or  resources that apply an AI/machine learning approach to game theory  problems. For example, in a COVID like situation where there is a  production limited good (i.e. the vaccine), and there are various groups  of people with different beliefs (1- the vaccine will be lifesaving, 2-  It may be lifesaving, but was developed too quickly, etc. etc. etc)-  I'm interested in modeling different outcomes given different starting  conditions. I won't continue to use the vaccine analogy, as it starts to  break down here- it's just for illustrative purposes.\n\nSo  with that being said- I'd love even simply being pointed in the right  direction for where to look for a tool, application, company, or other  resource.", "link": "https://www.reddit.com/r/MachineLearning/comments/ncfieu/p_machine_learning_applied_to_game_theory_resource/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] machine learning applied to game theory resource? /!/  hey friends- you probably get posts from people not in the field who are curious all the time, so apologies.\n\ni'm  looking for any resource i can on tools, applications, companies, or  resources that apply an ai/machine learning approach to game theory  problems. for example, in a covid like situation where there is a  production limited good (i.e. the vaccine), and there are various groups  of people with different beliefs (1- the vaccine will be lifesaving, 2-  it may be lifesaving, but was developed too quickly, etc. etc. etc)-  i'm interested in modeling different outcomes given different starting  conditions. i won't continue to use the vaccine analogy, as it starts to  break down here- it's just for illustrative purposes.\n\nso  with that being said- i'd love even simply being pointed in the right  direction for where to look for a -----> tool !!! , application, company, or other  resource.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/ncfieu/p_machine_learning_applied_to_game_theory_resource/',)", "identifyer": 5728466, "year": "2021"}, {"autor": "hungry4hungary", "date": 1620504369000, "content": "[R] Looking for valued responses for my research paper /!/ Hi everybody,\n\nPlease please please fill out my survey so I can graduate!\n\nLink to the [SURVEY](https://erasmusuniversity.eu.qualtrics.com/jfe/form/SV_a5x9eEFWUtcBS98) &lt;--\n\nI\u2019m doing my master's at Erasmus University Rotterdam. \nTo be able to graduate I\u2019m currently trying to collect data from people who work with or study predictive analytics, machine learning, or something connected to the topic.\nMy research is about underlying biases in algorithms that discriminate against minority groups. \n\nThere are many articles concerning the issue I will leave them here if you\u2019d like to read them:\n\n- [Reuters - Amazon scraps secret AI recruiting tool that showed bias against women](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G)\n- [Healthaffairs - Discrimination By Artificial Intelligence In A Commercial Electronic Health Record\u2014A Case Study](https://www.healthaffairs.org/do/10.1377/hblog20200128.626576/full/)\n- [Reuters - U.S. government study finds racial bias in facial recognition tools](https://www.reuters.com/article/us-usa-crime-face-idUSKBN1YN2V1)\n\nIt would mean the world to me if I could gather around 100 responses (have 50 so far!) so each response is greatly appreciated! :)", "link": "https://www.reddit.com/r/MachineLearning/comments/n7xbcg/r_looking_for_valued_responses_for_my_research/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[r] looking for valued responses for my research paper /!/ hi everybody,\n\nplease please please fill out my survey so i can graduate!\n\nlink to the [survey](https://erasmusuniversity.eu.qualtrics.com/jfe/form/sv_a5x9eefwutcbs98) &lt;--\n\ni\u2019m doing my master's at erasmus university rotterdam. \nto be able to graduate i\u2019m currently trying to collect data from people who work with or study predictive analytics, machine learning, or something connected to the topic.\nmy research is about underlying biases in algorithms that discriminate against minority groups. \n\nthere are many articles concerning the issue i will leave them here if you\u2019d like to read them:\n\n- [reuters - amazon scraps secret ai recruiting -----> tool !!!  that showed bias against women](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-iduskcn1mk08g)\n- [healthaffairs - discrimination by artificial intelligence in a commercial electronic health record\u2014a case study](https://www.healthaffairs.org/do/10.1377/hblog20200128.626576/full/)\n- [reuters - u.s. government study finds racial bias in facial recognition tools](https://www.reuters.com/article/us-usa-crime-face-iduskbn1yn2v1)\n\nit would mean the world to me if i could gather around 100 responses (have 50 so far!) so each response is greatly appreciated! :)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/n7xbcg/r_looking_for_valued_responses_for_my_research/',)", "identifyer": 5728509, "year": "2021"}, {"autor": "ZeroHour999", "date": 1620499138000, "content": "[R] Emotion Recognition of the Singing Voice: Toward a Real-Time Analysis Tool for Singers /!/ [https://arxiv.org/abs/2105.00173](https://arxiv.org/abs/2105.00173)", "link": "https://www.reddit.com/r/MachineLearning/comments/n7vjyl/r_emotion_recognition_of_the_singing_voice_toward/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[r] emotion recognition of the singing voice: toward a real-time analysis -----> tool !!!  for singers /!/ [https://arxiv.org/abs/2105.00173](https://arxiv.org/abs/2105.00173)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/n7vjyl/r_emotion_recognition_of_the_singing_voice_toward/',)", "identifyer": 5728514, "year": "2021"}, {"autor": "ZeroHour999", "date": 1620499021000, "content": "Emotion Recognition of the Singing Voice: Toward a Real-Time Analysis Tool for Singers /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/n7viis/emotion_recognition_of_the_singing_voice_toward_a/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "emotion recognition of the singing voice: toward a real-time analysis -----> tool !!!  for singers /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/n7viis/emotion_recognition_of_the_singing_voice_toward_a/',)", "identifyer": 5728515, "year": "2021"}, {"autor": "memengenieur", "date": 1620466988000, "content": "[D] The future of open-source AI /!/ I did [this](https://github.com/priyavrat-misra/xrays-and-gradcam) project a while back, which follows an alternate and quicker approach for COVID-19 diagnosis than RT-PCR. Recently, DRDO's Centre for Artificial Intelligence and Robotics (CAIR) developed a tool following the same approach [(source)](https://in.news.yahoo.com/drdo-develops-ai-tool-covid-113220658.html).\n\nThe only few differences are that mine got lost in a sea of million projects, and theirs will get heard by millions. And mine is open-source, and theirs isn't. For all we know, there must be enough such open-source AI projects, falling victim to obscurity everyday.\n\nI can't help but wonder, \"Is this the future of AI?\" \"Will AI be driven only by organizations with influence?\", if so, \"where do open-source in AI fit in?\"", "link": "https://www.reddit.com/r/MachineLearning/comments/n7lqde/d_the_future_of_opensource_ai/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] the future of open-source ai /!/ i did [this](https://github.com/priyavrat-misra/xrays-and-gradcam) project a while back, which follows an alternate and quicker approach for covid-19 diagnosis than rt-pcr. recently, drdo's centre for artificial intelligence and robotics (cair) developed a -----> tool !!!  following the same approach [(source)](https://in.news.yahoo.com/drdo-develops-ai------> tool !!! -covid-113220658.html).\n\nthe only few differences are that mine got lost in a sea of million projects, and theirs will get heard by millions. and mine is open-source, and theirs isn't. for all we know, there must be enough such open-source ai projects, falling victim to obscurity everyday.\n\ni can't help but wonder, \"is this the future of ai?\" \"will ai be driven only by organizations with influence?\", if so, \"where do open-source in ai fit in?\"", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/n7lqde/d_the_future_of_opensource_ai/',)", "identifyer": 5728546, "year": "2021"}, {"autor": "Bundar_20_90_70", "date": 1632785543000, "content": "\"[Project]\", Virtual Try On Model /!/ Hi all,\n\nHoping someone can please recommend someone to build a tool similar to that of [zeekit.me](https://zeekit.me) for an app?\n\nThank you in advance.", "link": "https://www.reddit.com/r/MachineLearning/comments/pwtftw/project_virtual_try_on_model/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "\"[project]\", virtual try on model /!/ hi all,\n\nhoping someone can please recommend someone to build a -----> tool !!!  similar to that of [zeekit.me](https://zeekit.me) for an app?\n\nthank you in advance.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/pwtftw/project_virtual_try_on_model/',)", "identifyer": 5728602, "year": "2021"}, {"autor": "sarmientoj24", "date": 1632727087000, "content": "[P] Looking for an Online Bounding Box Annotation Tool with Good Collaboration Tools /!/ We have a project for creating an object detection database where we have a supervisor/domain expert and number of annotators. What's the best annotation tool for annotating bounding boxes in an image with the following features:\n\n* can be paid or not\n* online tool\n* has \"supervisory\" capabilities where the owner can supervise annotations of people\n* annotations from other annotators can be seen by others\n* easy-to-use\n* about 5-7 annotators\n* images ranging from 10K-50K", "link": "https://www.reddit.com/r/MachineLearning/comments/pwc26w/p_looking_for_an_online_bounding_box_annotation/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] looking for an online bounding box annotation -----> tool !!!  with good collaboration tools /!/ we have a project for creating an object detection database where we have a supervisor/domain expert and number of annotators. what's the best annotation tool for annotating bounding boxes in an image with the following features:\n\n* can be paid or not\n* online tool\n* has \"supervisory\" capabilities where the owner can supervise annotations of people\n* annotations from other annotators can be seen by others\n* easy-to-use\n* about 5-7 annotators\n* images ranging from 10k-50k", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/pwc26w/p_looking_for_an_online_bounding_box_annotation/',)", "identifyer": 5728649, "year": "2021"}, {"autor": "_Arsenie_Boca_", "date": 1632244229000, "content": "[D] List of Literature Review Tools /!/ Searching and categorizing references is a tedious task that requires a lot of manual work. In recent years, many tools/services have been proposed, that aim to make it more efficient. I have collected a few of those tools below. \n\nSearch\n\n* [https://scholar.google.de](https://scholar.google.de)\n* [https://www.semanticscholar.org/](https://www.semanticscholar.org/)\n* [https://arxiv.org/](https://arxiv.org/)\n* [http://www.arxiv-sanity.com/](http://www.arxiv-sanity.com/)\n* [https://sci-genie.com/](https://sci-genie.com/)\n\nFancy visualizations\n\n* [https://citationgecko.azurewebsites.net/](https://citationgecko.azurewebsites.net/)\n* [https://www.connectedpapers.com/](https://www.connectedpapers.com/)\n\nOrganization\n\n* [https://www.zotero.org/](https://www.zotero.org/)\n* [https://www.mendeley.com/](https://www.mendeley.com/)\n* [https://www.citavi.com/](https://www.citavi.com/de)\n* [https://www.bibcitation.com/](https://www.bibcitation.com/)\n\nWhat does your tool stack look like in literature reviews? Are there others than the ones mentioned above? Do you particularly like or dislike any of them?", "link": "https://www.reddit.com/r/MachineLearning/comments/psn6vk/d_list_of_literature_review_tools/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] list of literature review tools /!/ searching and categorizing references is a tedious task that requires a lot of manual work. in recent years, many tools/services have been proposed, that aim to make it more efficient. i have collected a few of those tools below. \n\nsearch\n\n* [https://scholar.google.de](https://scholar.google.de)\n* [https://www.semanticscholar.org/](https://www.semanticscholar.org/)\n* [https://arxiv.org/](https://arxiv.org/)\n* [http://www.arxiv-sanity.com/](http://www.arxiv-sanity.com/)\n* [https://sci-genie.com/](https://sci-genie.com/)\n\nfancy visualizations\n\n* [https://citationgecko.azurewebsites.net/](https://citationgecko.azurewebsites.net/)\n* [https://www.connectedpapers.com/](https://www.connectedpapers.com/)\n\norganization\n\n* [https://www.zotero.org/](https://www.zotero.org/)\n* [https://www.mendeley.com/](https://www.mendeley.com/)\n* [https://www.citavi.com/](https://www.citavi.com/de)\n* [https://www.bibcitation.com/](https://www.bibcitation.com/)\n\nwhat does your -----> tool !!!  stack look like in literature reviews? are there others than the ones mentioned above? do you particularly like or dislike any of them?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 16, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/psn6vk/d_list_of_literature_review_tools/',)", "identifyer": 5728703, "year": "2021"}, {"autor": "mrtac96", "date": 1632164312000, "content": "[D] How to track progress in deep learning model building procedure. /!/ I am working on a problem, where i will try different deep learning models, augmentations, losses. At the end, there are alot of combinations and messy things. Is there a tool that can keep progress of what ever we try?", "link": "https://www.reddit.com/r/MachineLearning/comments/ps1akv/d_how_to_track_progress_in_deep_learning_model/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] how to track progress in deep learning model building procedure. /!/ i am working on a problem, where i will try different deep learning models, augmentations, losses. at the end, there are alot of combinations and messy things. is there a -----> tool !!!  that can keep progress of what ever we try?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/ps1akv/d_how_to_track_progress_in_deep_learning_model/',)", "identifyer": 5728762, "year": "2021"}, {"autor": "yourboyrabbit", "date": 1613626134000, "content": "[P] Information Retrieval and Event Prediction from Unstructured Document Corpus /!/  My question is quite open ended. In some chemical plant, by using the sensor data available we first deployed a machine learning tool that can predict the onset on anomalous behavior with some decent accuracy that can be improved further. We now have been provided huge corpuses of unstructured document eg. daily plant shift summaries, maintenance log etc. and the client wants us to retrieve important information, do sequence mining, pattern analysis etc. such that it helps in support systems. In the best case, the information extracted from these documents should also point to the anomalous behavior that occurs. My idea to approach this problem would have been to do some *sentence classification* where we try to infer the label of a sentence say: symptom, event, normal etc. However, the problem here is that its impossible to get the sentences labeled with an SME. In that case, another thing that I could think of was to simply cluster these documents into two types: normal and abnormal plant behavior.\n\nI am relatively new to Machine Learning and NLP and I would like to know what would you be thinking of if you were in my place. What would you like to do with this problem? Keen to get the inputs.", "link": "https://www.reddit.com/r/MachineLearning/comments/lmer7c/p_information_retrieval_and_event_prediction_from/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] information retrieval and event prediction from unstructured document corpus /!/  my question is quite open ended. in some chemical plant, by using the sensor data available we first deployed a machine learning -----> tool !!!  that can predict the onset on anomalous behavior with some decent accuracy that can be improved further. we now have been provided huge corpuses of unstructured document eg. daily plant shift summaries, maintenance log etc. and the client wants us to retrieve important information, do sequence mining, pattern analysis etc. such that it helps in support systems. in the best case, the information extracted from these documents should also point to the anomalous behavior that occurs. my idea to approach this problem would have been to do some *sentence classification* where we try to infer the label of a sentence say: symptom, event, normal etc. however, the problem here is that its impossible to get the sentences labeled with an sme. in that case, another thing that i could think of was to simply cluster these documents into two types: normal and abnormal plant behavior.\n\ni am relatively new to machine learning and nlp and i would like to know what would you be thinking of if you were in my place. what would you like to do with this problem? keen to get the inputs.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/lmer7c/p_information_retrieval_and_event_prediction_from/',)", "identifyer": 5728810, "year": "2021"}, {"autor": "brahshta4", "date": 1613458710000, "content": "A paid tool for feature extraction of Twitter data /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/lky9hv/a_paid_tool_for_feature_extraction_of_twitter_data/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "a paid -----> tool !!!  for feature extraction of twitter data /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/lky9hv/a_paid_tool_for_feature_extraction_of_twitter_data/',)", "identifyer": 5728867, "year": "2021"}, {"autor": "Razcle", "date": 1614084092000, "content": "[D] Is deep learning really Software 2.0? /!/ Three years on since Kapathy's blog on DL as software 2.0  Im curious what people's thinking is now. [https://medium.com/@karpathy/software-2-0-a64152b37c35](https://medium.com/@karpathy/software-2-0-a64152b37c35)\n\nHe argues that DL is more than a tool in the arsenal but a better way to write software for many applications. He cites fixed runtime and specialised hardware as well as more uniform code as advantages to DL.  \n\n\nHe also argues that viewing DL this way suggests that we need new tooling for DL the way we needed git and IDEs for software 1.0.  \n\n\nHow well do you think this ideas aged? does it resonate today?", "link": "https://www.reddit.com/r/MachineLearning/comments/lqhjjg/d_is_deep_learning_really_software_20/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] is deep learning really software 2.0? /!/ three years on since kapathy's blog on dl as software 2.0  im curious what people's thinking is now. [https://medium.com/@karpathy/software-2-0-a64152b37c35](https://medium.com/@karpathy/software-2-0-a64152b37c35)\n\nhe argues that dl is more than a -----> tool !!!  in the arsenal but a better way to write software for many applications. he cites fixed runtime and specialised hardware as well as more uniform code as advantages to dl.  \n\n\nhe also argues that viewing dl this way suggests that we need new tooling for dl the way we needed git and ides for software 1.0.  \n\n\nhow well do you think this ideas aged? does it resonate today?", "sortedWord": "None", "removed": "('nan',)", "score": 5, "comments": 27, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/lqhjjg/d_is_deep_learning_really_software_20/',)", "identifyer": 5729084, "year": "2021"}, {"autor": "paswut", "date": 1614892286000, "content": "[D] Web-Dev for ML Engineers - any advice on deploying models in websites for clients with backend support /!/ Have any ML engineers here (with a Python background) learned web-dev. Did you manage with a Flask approach or did you opt to learn javascript for this purpose?\n\nI see Streamlit is an interesting tool, but maybe not for finished products which have user authentication, and requires access to databases to save data from user sessions. I think I would like to make that happen, but I am not sure what steps I should take in order to do so. I think I would choose Flask over learning Javascript since I don't think I would need the model to run on the client-side.\n\nAre there any great examples of ML websites like artbreeder that are motivating examples for a python ML dev?", "link": "https://www.reddit.com/r/MachineLearning/comments/lxvjkh/d_webdev_for_ml_engineers_any_advice_on_deploying/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] web-dev for ml engineers - any advice on deploying models in websites for clients with backend support /!/ have any ml engineers here (with a python background) learned web-dev. did you manage with a flask approach or did you opt to learn javascript for this purpose?\n\ni see streamlit is an interesting -----> tool !!! , but maybe not for finished products which have user authentication, and requires access to databases to save data from user sessions. i think i would like to make that happen, but i am not sure what steps i should take in order to do so. i think i would choose flask over learning javascript since i don't think i would need the model to run on the client-side.\n\nare there any great examples of ml websites like artbreeder that are motivating examples for a python ml dev?", "sortedWord": "None", "removed": "('nan',)", "score": 11, "comments": 9, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/lxvjkh/d_webdev_for_ml_engineers_any_advice_on_deploying/',)", "identifyer": 5729125, "year": "2021"}, {"autor": "Jnd-02", "date": 1614887092000, "content": "[D] Does anyone know a tool that can help me with this specific task ? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/lxtcma/d_does_anyone_know_a_tool_that_can_help_me_with/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] does anyone know a -----> tool !!!  that can help me with this specific task ? /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 0, "comments": 10, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/lxtcma/d_does_anyone_know_a_tool_that_can_help_me_with/',)", "identifyer": 5729132, "year": "2021"}, {"autor": "Jnd-02", "date": 1614886977000, "content": "Does anyone know a tool that can help me with this specific task ? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/lxtaxu/does_anyone_know_a_tool_that_can_help_me_with/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "does anyone know a -----> tool !!!  that can help me with this specific task ? /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/lxtaxu/does_anyone_know_a_tool_that_can_help_me_with/',)", "identifyer": 5729133, "year": "2021"}, {"autor": "spiffiestwig", "date": 1615470994000, "content": "Machine Learning Tool to Select Still from Videos /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/m2qkwl/machine_learning_tool_to_select_still_from_videos/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "machine learning -----> tool !!!  to select still from videos /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 1, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/m2qkwl/machine_learning_tool_to_select_still_from_videos/',)", "identifyer": 5729226, "year": "2021"}, {"autor": "donshell", "date": 1615433118000, "content": "[P] NumPy-style histograms in PyTorch with torchist /!/ I've been working with distributions and histograms in Python for a while now, but it always annoyed me that PyTorch did not provide any tool for computing histograms in several dimensions, like NumPy's `histogramdd`.\n\nBut recently, I figured out: why not implementing it myself ? And so I did.\n\n`torchist` is a small Python package to compute and interact with histograms in PyTorch, like you would in NumPy. Especially, the package implements [`histogram`](https://numpy.org/doc/stable/reference/generated/numpy.histogram.html) and [`histogramdd`](https://numpy.org/doc/stable/reference/generated/numpy.histogramdd.html) with support for non-uniform binning.\n\nThere are also \"bonus\" features like the support of `sparse` histograms, to handle large dimensionalities, or the functions `ravel_multi_index` and `unravel_index` which are not provided in `torch`.\n\nHope you like it ;)\n\nRepository: [https://github.com/francois-rozet/torchist](https://github.com/francois-rozet/torchist)\n\nTL;DR. I made `torchist`, a small Python package to compute and interact with histograms in PyTorch.", "link": "https://www.reddit.com/r/MachineLearning/comments/m2gqtm/p_numpystyle_histograms_in_pytorch_with_torchist/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] numpy-style histograms in pytorch with torchist /!/ i've been working with distributions and histograms in python for a while now, but it always annoyed me that pytorch did not provide any -----> tool !!!  for computing histograms in several dimensions, like numpy's `histogramdd`.\n\nbut recently, i figured out: why not implementing it myself ? and so i did.\n\n`torchist` is a small python package to compute and interact with histograms in pytorch, like you would in numpy. especially, the package implements [`histogram`](https://numpy.org/doc/stable/reference/generated/numpy.histogram.html) and [`histogramdd`](https://numpy.org/doc/stable/reference/generated/numpy.histogramdd.html) with support for non-uniform binning.\n\nthere are also \"bonus\" features like the support of `sparse` histograms, to handle large dimensionalities, or the functions `ravel_multi_index` and `unravel_index` which are not provided in `torch`.\n\nhope you like it ;)\n\nrepository: [https://github.com/francois-rozet/torchist](https://github.com/francois-rozet/torchist)\n\ntl;dr. i made `torchist`, a small python package to compute and interact with histograms in pytorch.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 16, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/m2gqtm/p_numpystyle_histograms_in_pytorch_with_torchist/',)", "identifyer": 5729260, "year": "2021"}, {"autor": "chaachans", "date": 1615726178000, "content": "[N] New AI tool detects Deepfakes by analyzing light reflections in the eyes", "link": "https://www.reddit.com/r/MachineLearning/comments/m4urk6/n_new_ai_tool_detects_deepfakes_by_analyzing/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[n] new ai -----> tool !!!  detects deepfakes by analyzing light reflections in the eyes", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 43, "media": "('link',)", "medialink": "('https://www.futurspot.com/how-spot-deepfakes-look-light-reflection-eyes/',)", "identifyer": 5729311, "year": "2021"}, {"autor": "Techllamaandcat", "date": 1615712219000, "content": "[Discussion] Researchers from NVIDIA and Harvard Develop Deep Learning tool for Epigenomics", "link": "https://www.reddit.com/r/MachineLearning/comments/m4roj5/discussion_researchers_from_nvidia_and_harvard/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[discussion] researchers from nvidia and harvard develop deep learning -----> tool !!!  for epigenomics", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('link',)", "medialink": "('https://www.globaltechoutlook.com/researchers-from-nvidia-and-hardvard-develop-deep-learning-tool-for-epigenomics/',)", "identifyer": 5729324, "year": "2021"}, {"autor": "Xxyjoel", "date": 1617007872000, "content": "ML + Infrastructure [P] /!/ Hey All,\n\nI   have been in the data science and machine learning space for the   majority of my career, but have more recently spent time in meddling around with infrastructure.  Could be the naivety, though the complexity (and inefficiency) of bundled cloud tools bothered me, so I built a tool to help manage costs.\n\nIt still requires some policy finagling, so it's not self service yet, however, I'd love y'alls candid feedback on the tool - [BlueArch.io](https://bluearch.io/)\n\nApologies  if this is against the sub's rules... sharing your work can be scary but I'm pretty excited about project.", "link": "https://www.reddit.com/r/MachineLearning/comments/mfl637/ml_infrastructure_p/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "ml + infrastructure [p] /!/ hey all,\n\ni   have been in the data science and machine learning space for the   majority of my career, but have more recently spent time in meddling around with infrastructure.  could be the naivety, though the complexity (and inefficiency) of bundled cloud tools bothered me, so i built a -----> tool !!!  to help manage costs.\n\nit still requires some policy finagling, so it's not self service yet, however, i'd love y'alls candid feedback on the tool - [bluearch.io](https://bluearch.io/)\n\napologies  if this is against the sub's rules... sharing your work can be scary but i'm pretty excited about project.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/mfl637/ml_infrastructure_p/',)", "identifyer": 5729395, "year": "2021"}, {"autor": "fripperML", "date": 1616972625000, "content": "[D] What\u2019s the simplest, most lightweight but complete and 100% open source MLOps toolkit? /!/ I know this has been asked many times and in many different ways. And there are tons of blog posts, articles, videos and courses addressing this and comparing hundreds of tools, libraries, frameworks\u2026 And that\u2019s part of my problem: I am facing so many options that I feel like Buridan\u2019s ass, dying of starvation for not knowing what to do.\n\nAlthough I don\u2019t want to write too much, I need to speak a little about our situation, in order to put the question in our context.\n\n**Our Team**\n\nOur Team is small. We have only four people, which could be qualified as beginner data scientists. One of us has a profile that is a little bit more \u201cengineer\u201d, so data engineer could be more suitable for him. Anyway, we don\u2019t have much experience, neither in Python Projects nor in Machine Learning. What we have is passion and love for ML!\n\nFor a couple of years, we have been functioning with SAS, but now we plan to change to the Python landscape, as it is much more vivid and exciting. In the last year, we have made two projects in Python, but without using any good practices at all. Every step was made by hand and prone to error, models were neither monitored nor even deployed (they only were used for making some batch predictions), projects were not properly structured, documentation was painful\u2026\n\nSo we know that we need to change it before it becomes unmanageable.\n\nWe don\u2019t expect the size of the team to grow fast. Let\u2019s say in a couple of years we can expect 10-12 people working with us (the organization knows the importance of Machine Learning, but economic issues can be an obstacle).\n\n**Our Projects**\n\nFor the moment, we have only made \u201cclassical\u201d Machine Learning. I mean: no Deep Learning. We have used Pandas and Scikit-Learn, XGBoost, etc. And only in Batch mode. But we expect it to change in less than a year, because we will need to train an image classifier to detect anomalies in customs packages, so it will need to be:\n\nTrained using a deep learning convolutional network.\n\nIntegrated with other applications (that are coded in Java) and fast (real-time).\n\nOther change we expect is to need more distributed computing, as we will need to manage some huge databases that simply do not fit in a pandas dataframe. This are the most important challenges we face.\n\n**Our Company**\n\nWe work for a big company, which also imposes some restrictions to us. Mainly:\n\nWe do not have budget to spend in MLOps solutions, so everything has to be open and free.\n\nWe won\u2019t hire data scientist / data engineers for the moment.\n\nThere are some tools, uses by other teams, that we should use as part of the MLOps stack, although they are not the best in the class.\n\nRegarding the last item, a short list of this set of restrictions is the following:\n\n* We have a Cloudera Express installation. It\u2019s the most basic and cheaper Cloudera option, so it does not come with any tool for Machine Learning management. It only gives to us HDFS, Impala, Spark and a set of nodes to run Python scripts on them.\n* We have Control-M as the orchestrator and workflow manager tool.\n* We have DataStage as the ETL tool.\n* We use SVN as the code version system (yes, no git).\n* We deploy our projects using a very simplified and self-made version of Docker. It\u2019s a little bit awkward and I think that, if we push a little bit, we could convince the organization to let us use Docker. But if Docker is reachable, Kubernetes is out of our capabilities.\n* We have Jenkins for CI.\n* We have Visual Studio Code professional licenses.\n\n**Toolset**\n\nWith this premises, I have two different and opposed concerns or even fears.\n\n* Fear of not using enough tools and good practices and arriving in a couple of years to a state where we cannot manage our own code, project and models.\n* Fear of using so many tools that they impose a burden our small team cannot bear.\n\nIt\u2019s clear that we need some MLOps, but how much, I don\u2019t know. I will review some things I have been reading, and I hope you can help me choosing the right tools.\n\n**Python Programming**\n\nIt looks like we will program using Visual Studio. We will use a remote interpreter, because we will run things on the Cloudera Nodes, although we will program locally and integrate the code with a SVN repository.\n\nDo we need tools for standardizing our code, like PyLint, Flake8, MyPy or Black? Would you recommend any of those?\n\n**CI and Deployment**\n\nWe will use Jenkins. For deployment of our code, is Docker a no brainer, a minimum standard? I tend to think so from what I read, but I\u2019d like to be sure and to have good arguments.\n\nDo we need more tools?\n\n**Project Scaffolding**\n\nI have been reading about PyScaffold, CookieCutter and, best of all (from my point of view), Kedro. I think we will stick to Kedro template, because it offers much more functionality, and I like to think of each project as a set of pipelines to be run. What do you think of Kedro?\n\n**Documentation**\n\nWould you recommend having separate documents, or generating the documentation from the projects, using Sphinx or another similar tool? I tend to prefer the second option, because the first one very likely tend to generate obsolete docs. But I don\u2019t know if the \u201cburden\u201d of the second is too big, and if the generated docs can suffice for a typical ML project.\n\n**Project registry**\n\nIs there any tool that could be used as a \u201cproject registry\u201d, like a simple web app where we could navigate through our projects, read the docs and thinks like that? I don\u2019t know. If not, the registry will be the SVN repo with all our projects as folders, and that\u2019s all.\n\n**Data Exploration and Preparation**\n\nI think that Matplotlib, Seaborn and Pandas should suffice, and when things go big, we should use PySpark, Scala or even plain SQL in Impala. However, I know Dask exists, and newer tools like Koalas or Vaex. What do you think?\n\nFor creating data transformation pipelines, we will use Kedro, although there are lots of tools that look interesting, like Dagster.\n\nWhen we enter the \u201cdeep learning\u201d realm, can we keep using the same tools? Should we use another framework like TFX? I\u2019d prefer not, cause learning one framework is hard, and two is worse. If a solution is valid for all our projects it\u2019s better. Or TFX is valid for \u201cclassical\u201d ML and Deep Learning?\n\n**Tests**\n\nI think unit testing can be too much burden for us. But I have come to Great Expectations library and think it\u2019s well suited for ML projects. Would you recommend it as an important part of our MLOps stack?\n\nBy the way, there is a Kedro-Great Expectations plugin, so we could benefit from that.\n\n**Feature Store**\n\nIs it really needed, especially considering our team size and experience? If so, I have read about Feast and Snorkel.\n\n**Data Versioning**\n\nIs it really needed, especially considering our team size and experience? If so, I have read about DVC.\n\n**Experimenting**\n\nI think it\u2019s an important piece, although I wonder if we really need a tool or we could use our own standard of reports and artifacts to follow what we have tried. But the risk that it goes unmanageable is high.\n\nKedro has a journal, I don\u2019t know if it can suffice. Also it has a Kedro-MLFlow plugin, so that we could benefit from using MLFlow as the experiment tool.\n\nI have also read about Guild, that seems really lightweigh and easy. I don\u2019t know much more.\n\n**Training**\n\nI developed my own library for doing nested cross validation and, with the same function:\n\nOptimizing hyperparameters (of model and pipeline).\n\nGenerating a report of the training to assess the quality of the model.\n\nIt\u2019s build on top of Skopt. I did it pip installable, it\u2019s here:\n\n[https://github.com/JaimeArboleda/nestedcvtraining](https://github.com/JaimeArboleda/nestedcvtraining)\n\nSo we plan to use it with the typical models like XGBoost, LightGBM and all Scikit-Learn. And when we need other frameworks like TensorFlow or Keras, we will see.\n\n**Model Registry**\n\nI think it\u2019s an important piece, although I don\u2019t know if we even could build our own with an standard database. If not, MLFlow seems a mature option.\n\n**Model serving**\n\nI am not sure if it\u2019s included in the previous point or not. Anyway, I have read about Streamlint and FastAPI. Would you recommend any of those?\n\nIs Apache Kafka needed for real time predictions?\n\n**Visualization**\n\nWith this I mean sharing with the organization basic web apps with customizable plots, explainable predictions and things like that. I have read about panel, which has the ability of transform a Jupyter Notebook into a simple web app. It might be interesting.\n\n**Model monitoring**\n\nIs there a good free tool for monitoring the models and detecting loss of accuracy, data drift and things like that? Or we should better generate our own script of monitoring to be run periodically?\n\n**BigData**\n\nAs I said before, we plan to use mainly Spark when need.\n\nI know it\u2019s a lot of info. Maybe I have overcomplicated myself and I should use only 20% of what I think I should. Or maybe not. I have no idea. Any help will be GREATLY appreciated. Thanks in advance.", "link": "https://www.reddit.com/r/MachineLearning/comments/mfca0p/d_whats_the_simplest_most_lightweight_but/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] what\u2019s the simplest, most lightweight but complete and 100% open source mlops toolkit? /!/ i know this has been asked many times and in many different ways. and there are tons of blog posts, articles, videos and courses addressing this and comparing hundreds of tools, libraries, frameworks\u2026 and that\u2019s part of my problem: i am facing so many options that i feel like buridan\u2019s ass, dying of starvation for not knowing what to do.\n\nalthough i don\u2019t want to write too much, i need to speak a little about our situation, in order to put the question in our context.\n\n**our team**\n\nour team is small. we have only four people, which could be qualified as beginner data scientists. one of us has a profile that is a little bit more \u201cengineer\u201d, so data engineer could be more suitable for him. anyway, we don\u2019t have much experience, neither in python projects nor in machine learning. what we have is passion and love for ml!\n\nfor a couple of years, we have been functioning with sas, but now we plan to change to the python landscape, as it is much more vivid and exciting. in the last year, we have made two projects in python, but without using any good practices at all. every step was made by hand and prone to error, models were neither monitored nor even deployed (they only were used for making some batch predictions), projects were not properly structured, documentation was painful\u2026\n\nso we know that we need to change it before it becomes unmanageable.\n\nwe don\u2019t expect the size of the team to grow fast. let\u2019s say in a couple of years we can expect 10-12 people working with us (the organization knows the importance of machine learning, but economic issues can be an obstacle).\n\n**our projects**\n\nfor the moment, we have only made \u201cclassical\u201d machine learning. i mean: no deep learning. we have used pandas and scikit-learn, xgboost, etc. and only in batch mode. but we expect it to change in less than a year, because we will need to train an image classifier to detect anomalies in customs packages, so it will need to be:\n\ntrained using a deep learning convolutional network.\n\nintegrated with other applications (that are coded in java) and fast (real-time).\n\nother change we expect is to need more distributed computing, as we will need to manage some huge databases that simply do not fit in a pandas dataframe. this are the most important challenges we face.\n\n**our company**\n\nwe work for a big company, which also imposes some restrictions to us. mainly:\n\nwe do not have budget to spend in mlops solutions, so everything has to be open and free.\n\nwe won\u2019t hire data scientist / data engineers for the moment.\n\nthere are some tools, uses by other teams, that we should use as part of the mlops stack, although they are not the best in the class.\n\nregarding the last item, a short list of this set of restrictions is the following:\n\n* we have a cloudera express installation. it\u2019s the most basic and cheaper cloudera option, so it does not come with any -----> tool !!!  for machine learning management. it only gives to us hdfs, impala, spark and a set of nodes to run python scripts on them.\n* we have control-m as the orchestrator and workflow manager tool.\n* we have datastage as the etl tool.\n* we use svn as the code version system (yes, no git).\n* we deploy our projects using a very simplified and self-made version of docker. it\u2019s a little bit awkward and i think that, if we push a little bit, we could convince the organization to let us use docker. but if docker is reachable, kubernetes is out of our capabilities.\n* we have jenkins for ci.\n* we have visual studio code professional licenses.\n\n**toolset**\n\nwith this premises, i have two different and opposed concerns or even fears.\n\n* fear of not using enough tools and good practices and arriving in a couple of years to a state where we cannot manage our own code, project and models.\n* fear of using so many tools that they impose a burden our small team cannot bear.\n\nit\u2019s clear that we need some mlops, but how much, i don\u2019t know. i will review some things i have been reading, and i hope you can help me choosing the right tools.\n\n**python programming**\n\nit looks like we will program using visual studio. we will use a remote interpreter, because we will run things on the cloudera nodes, although we will program locally and integrate the code with a svn repository.\n\ndo we need tools for standardizing our code, like pylint, flake8, mypy or black? would you recommend any of those?\n\n**ci and deployment**\n\nwe will use jenkins. for deployment of our code, is docker a no brainer, a minimum standard? i tend to think so from what i read, but i\u2019d like to be sure and to have good arguments.\n\ndo we need more tools?\n\n**project scaffolding**\n\ni have been reading about pyscaffold, cookiecutter and, best of all (from my point of view), kedro. i think we will stick to kedro template, because it offers much more functionality, and i like to think of each project as a set of pipelines to be run. what do you think of kedro?\n\n**documentation**\n\nwould you recommend having separate documents, or generating the documentation from the projects, using sphinx or another similar tool? i tend to prefer the second option, because the first one very likely tend to generate obsolete docs. but i don\u2019t know if the \u201cburden\u201d of the second is too big, and if the generated docs can suffice for a typical ml project.\n\n**project registry**\n\nis there any tool that could be used as a \u201cproject registry\u201d, like a simple web app where we could navigate through our projects, read the docs and thinks like that? i don\u2019t know. if not, the registry will be the svn repo with all our projects as folders, and that\u2019s all.\n\n**data exploration and preparation**\n\ni think that matplotlib, seaborn and pandas should suffice, and when things go big, we should use pyspark, scala or even plain sql in impala. however, i know dask exists, and newer tools like koalas or vaex. what do you think?\n\nfor creating data transformation pipelines, we will use kedro, although there are lots of tools that look interesting, like dagster.\n\nwhen we enter the \u201cdeep learning\u201d realm, can we keep using the same tools? should we use another framework like tfx? i\u2019d prefer not, cause learning one framework is hard, and two is worse. if a solution is valid for all our projects it\u2019s better. or tfx is valid for \u201cclassical\u201d ml and deep learning?\n\n**tests**\n\ni think unit testing can be too much burden for us. but i have come to great expectations library and think it\u2019s well suited for ml projects. would you recommend it as an important part of our mlops stack?\n\nby the way, there is a kedro-great expectations plugin, so we could benefit from that.\n\n**feature store**\n\nis it really needed, especially considering our team size and experience? if so, i have read about feast and snorkel.\n\n**data versioning**\n\nis it really needed, especially considering our team size and experience? if so, i have read about dvc.\n\n**experimenting**\n\ni think it\u2019s an important piece, although i wonder if we really need a tool or we could use our own standard of reports and artifacts to follow what we have tried. but the risk that it goes unmanageable is high.\n\nkedro has a journal, i don\u2019t know if it can suffice. also it has a kedro-mlflow plugin, so that we could benefit from using mlflow as the experiment tool.\n\ni have also read about guild, that seems really lightweigh and easy. i don\u2019t know much more.\n\n**training**\n\ni developed my own library for doing nested cross validation and, with the same function:\n\noptimizing hyperparameters (of model and pipeline).\n\ngenerating a report of the training to assess the quality of the model.\n\nit\u2019s build on top of skopt. i did it pip installable, it\u2019s here:\n\n[https://github.com/jaimearboleda/nestedcvtraining](https://github.com/jaimearboleda/nestedcvtraining)\n\nso we plan to use it with the typical models like xgboost, lightgbm and all scikit-learn. and when we need other frameworks like tensorflow or keras, we will see.\n\n**model registry**\n\ni think it\u2019s an important piece, although i don\u2019t know if we even could build our own with an standard database. if not, mlflow seems a mature option.\n\n**model serving**\n\ni am not sure if it\u2019s included in the previous point or not. anyway, i have read about streamlint and fastapi. would you recommend any of those?\n\nis apache kafka needed for real time predictions?\n\n**visualization**\n\nwith this i mean sharing with the organization basic web apps with customizable plots, explainable predictions and things like that. i have read about panel, which has the ability of transform a jupyter notebook into a simple web app. it might be interesting.\n\n**model monitoring**\n\nis there a good free tool for monitoring the models and detecting loss of accuracy, data drift and things like that? or we should better generate our own script of monitoring to be run periodically?\n\n**bigdata**\n\nas i said before, we plan to use mainly spark when need.\n\ni know it\u2019s a lot of info. maybe i have overcomplicated myself and i should use only 20% of what i think i should. or maybe not. i have no idea. any help will be greatly appreciated. thanks in advance.", "sortedWord": "None", "removed": "('nan',)", "score": 3, "comments": 90, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/mfca0p/d_whats_the_simplest_most_lightweight_but/',)", "identifyer": 5729417, "year": "2021"}, {"autor": "techsucker", "date": 1616957686000, "content": "[R] Researchers at Lawrence Livermore National Laboratory (LLNL) Developed a Novel Deep Learning Framework for Symbolic Regression /!/ At the Lawrence Livermore National Laboratory (LLNL), scientists have developed a novel framework and an accompanying visualization tool that utilizes deep reinforcement learning for symbolic regression problems, outperforming baseline methods on benchmark problems.\n\nTheir paper was recently accepted as an oral presentation at the International Conference on Learning Representations (ICLR 2021). In their paper, the researchers describe applying deep reinforcement learning to discrete optimization. Discrete optimization focuses on problems that deal with discrete \u201cbuilding blocks\u201d that must be combined in a particular order or configuration to optimize the desired property. They focused on a type of discrete optimization called symbolic regression. Symbolic regression finds short mathematical expressions that fit data gathered from an experiment. It aims to discover the underlying equations or dynamics of a physical process.\n\nSummary: [https://www.marktechpost.com/2021/03/28/researchers-at-lawrence-livermore-national-laboratory-llnl-developed-a-novel-deep-learning-framework-for-symbolic-regression/](https://www.marktechpost.com/2021/03/28/researchers-at-lawrence-livermore-national-laboratory-llnl-developed-a-novel-deep-learning-framework-for-symbolic-regression/) \n\nPaper: [https://openreview.net/forum?id=m5Qsh0kBQG](https://openreview.net/forum?id=m5Qsh0kBQG)", "link": "https://www.reddit.com/r/MachineLearning/comments/mf79br/r_researchers_at_lawrence_livermore_national/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[r] researchers at lawrence livermore national laboratory (llnl) developed a novel deep learning framework for symbolic regression /!/ at the lawrence livermore national laboratory (llnl), scientists have developed a novel framework and an accompanying visualization -----> tool !!!  that utilizes deep reinforcement learning for symbolic regression problems, outperforming baseline methods on benchmark problems.\n\ntheir paper was recently accepted as an oral presentation at the international conference on learning representations (iclr 2021). in their paper, the researchers describe applying deep reinforcement learning to discrete optimization. discrete optimization focuses on problems that deal with discrete \u201cbuilding blocks\u201d that must be combined in a particular order or configuration to optimize the desired property. they focused on a type of discrete optimization called symbolic regression. symbolic regression finds short mathematical expressions that fit data gathered from an experiment. it aims to discover the underlying equations or dynamics of a physical process.\n\nsummary: [https://www.marktechpost.com/2021/03/28/researchers-at-lawrence-livermore-national-laboratory-llnl-developed-a-novel-deep-learning-framework-for-symbolic-regression/](https://www.marktechpost.com/2021/03/28/researchers-at-lawrence-livermore-national-laboratory-llnl-developed-a-novel-deep-learning-framework-for-symbolic-regression/) \n\npaper: [https://openreview.net/forum?id=m5qsh0kbqg](https://openreview.net/forum?id=m5qsh0kbqg)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 5, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/mf79br/r_researchers_at_lawrence_livermore_national/',)", "identifyer": 5729439, "year": "2021"}, {"autor": "Lairv", "date": 1609537874000, "content": "[D] Audio denoising with machine learning /!/ I'm trying to find work done about denoising audio sample : for example remove the small interference during a phone call to make the voice crystal clear, or removing the crowd sounds from an audio recording etc..\n\nI guess there are ways to do this kind of things without deep learning, with some fourier transform and then filtering some frequencies, but deep learning also seems to be an appropriate tool to tackle this kind of problems.\n\nI haven't found anything by i'm probably not searching the right keywords. If you have any resources that may help me i would be happy to hear about it!", "link": "https://www.reddit.com/r/MachineLearning/comments/kojxsk/d_audio_denoising_with_machine_learning/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] audio denoising with machine learning /!/ i'm trying to find work done about denoising audio sample : for example remove the small interference during a phone call to make the voice crystal clear, or removing the crowd sounds from an audio recording etc..\n\ni guess there are ways to do this kind of things without deep learning, with some fourier transform and then filtering some frequencies, but deep learning also seems to be an appropriate -----> tool !!!  to tackle this kind of problems.\n\ni haven't found anything by i'm probably not searching the right keywords. if you have any resources that may help me i would be happy to hear about it!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/kojxsk/d_audio_denoising_with_machine_learning/',)", "identifyer": 5729576, "year": "2021"}, {"autor": "emergencycomms", "date": 1609813391000, "content": "TF-IDF Tool? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/kqo9p9/tfidf_tool/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "tf-idf -----> tool !!! ? /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/kqo9p9/tfidf_tool/',)", "identifyer": 5729683, "year": "2021"}, {"autor": "doublevr", "date": 1617475404000, "content": "[D][P] Looking for ML/CNN tools for interactive VR sex toys scripts creation based on the action in a video /!/ Looking for ML/CNN tools to automate [interactive sex toys scripts](https://www.sexlikereal.com/tags/interactive-sex-toys-vr) creation. Scripts synchronize connected sex toys with the videos to recreate every action in the videos. \n\nThings might work quite well with collecting, annotating and augmenting data and putting these into Keras/Tensorflow or any other tool\n\nEmail philip \u00c4 sexlikereal.com with \"CNN engineer\" in the subject", "link": "https://www.reddit.com/r/MachineLearning/comments/mjej11/dp_looking_for_mlcnn_tools_for_interactive_vr_sex/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d][p] looking for ml/cnn tools for interactive vr sex toys scripts creation based on the action in a video /!/ looking for ml/cnn tools to automate [interactive sex toys scripts](https://www.sexlikereal.com/tags/interactive-sex-toys-vr) creation. scripts synchronize connected sex toys with the videos to recreate every action in the videos. \n\nthings might work quite well with collecting, annotating and augmenting data and putting these into keras/tensorflow or any other -----> tool !!! \n\nemail philip \u00e4 sexlikereal.com with \"cnn engineer\" in the subject", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/mjej11/dp_looking_for_mlcnn_tools_for_interactive_vr_sex/',)", "identifyer": 5729759, "year": "2021"}, {"autor": "binaryfor", "date": 1617468530000, "content": "[P] Deep Daze - A simple command line tool for text to image generation using OpenAI's CLIP and Siren", "link": "https://www.reddit.com/r/MachineLearning/comments/mjc8q4/p_deep_daze_a_simple_command_line_tool_for_text/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] deep daze - a simple command line -----> tool !!!  for text to image generation using openai's clip and siren", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('link',)", "medialink": "('https://github.com/lucidrains/deep-daze',)", "identifyer": 5729771, "year": "2021"}, {"autor": "binaryfor", "date": 1617467973000, "content": "Deep Daze - A simple command line tool for text to image generation using OpenAI's CLIP and Siren", "link": "https://www.reddit.com/r/MachineLearning/comments/mjc2d4/deep_daze_a_simple_command_line_tool_for_text_to/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "deep daze - a simple command line -----> tool !!!  for text to image generation using openai's clip and siren", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('link',)", "medialink": "('https://github.com/lucidrains/deep-daze',)", "identifyer": 5729772, "year": "2021"}, {"autor": "habibTheCoel", "date": 1617727441000, "content": "[P] AI based search engine { Startup idea } /!/  Obviously AI based search engine is the next step . Because of too much data available today on internet . We need this tool . I think it will be more interactive version of google . Where you can search based on size or similarity to other website , article or post . Or specify some part of text and find similar parts from other data sources . To rate the quality of the search after search query . And there could be a lot more features .\n\nIdea is quite simple , it's all about execution . That's why I am searching for cofounders . Feel free to DM me . Potential investors are welcome too . Thank you for reading this ! Stay hungry , stay foolish )", "link": "https://www.reddit.com/r/MachineLearning/comments/mlfyrf/p_ai_based_search_engine_startup_idea/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] ai based search engine { startup idea } /!/  obviously ai based search engine is the next step . because of too much data available today on internet . we need this -----> tool !!!  . i think it will be more interactive version of google . where you can search based on size or similarity to other website , article or post . or specify some part of text and find similar parts from other data sources . to rate the quality of the search after search query . and there could be a lot more features .\n\nidea is quite simple , it's all about execution . that's why i am searching for cofounders . feel free to dm me . potential investors are welcome too . thank you for reading this ! stay hungry , stay foolish )", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/mlfyrf/p_ai_based_search_engine_startup_idea/',)", "identifyer": 5729877, "year": "2021"}, {"autor": "rexlow0823", "date": 1625383866000, "content": "[D] Looking for a data/model management tool/solution /!/ Hey devs!\n\nI\u2019ve got a MLOps question here. How do you guys manage data or perhaps what tools/stacks do you use to version control your data and models? \n\nI recently discovered Kubeflow with add-ons like Rok but it\u2019s so painful to deploy in an on-premise cluster (mainly some networking bugs in the library)\n\nWould love to explore some other options. I\u2019m looking for a framework-agnostic solution here. Thanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/odfkmv/d_looking_for_a_datamodel_management_toolsolution/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] looking for a data/model management -----> tool !!! /solution /!/ hey devs!\n\ni\u2019ve got a mlops question here. how do you guys manage data or perhaps what tools/stacks do you use to version control your data and models? \n\ni recently discovered kubeflow with add-ons like rok but it\u2019s so painful to deploy in an on-premise cluster (mainly some networking bugs in the library)\n\nwould love to explore some other options. i\u2019m looking for a framework-agnostic solution here. thanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/odfkmv/d_looking_for_a_datamodel_management_toolsolution/',)", "identifyer": 5729965, "year": "2021"}, {"autor": "rexlow0823", "date": 1625383809000, "content": "Looking for a data and model management tool/solution /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/odfk7v/looking_for_a_data_and_model_management/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "looking for a data and model management -----> tool !!! /solution /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/odfk7v/looking_for_a_data_and_model_management/',)", "identifyer": 5729966, "year": "2021"}, {"autor": "Umarghouri", "date": 1625258384000, "content": "Machine tool works", "link": "https://www.reddit.com/r/MachineLearning/comments/ocixaq/machine_tool_works/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "machine -----> tool !!!  works", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('rich:video',)", "medialink": "('https://youtu.be/QZg31XJOEA4',)", "identifyer": 5730054, "year": "2021"}, {"autor": "UBIAI", "date": 1611596727000, "content": "[N] Announcing UBIAI: Easy-to-Use Text Annotation Tool for NLP Applications /!/ [UBIAI](https://ubiai.tools) was born out of frustration with existing solutions, which either have a low quality/price ratio or are expensive and geared towards large companies. We know that data labeling is the bottleneck for creating custom NLP models (NER, entity relations, classification, etc) and is here to stay. We wanted to create the most accessible, easy-to-use and automated solution at an affordable price.\n\nIf you are launching a new NLP project, please explore our tool [https://ubiai.tools](https://ubiai.tools) (we offer free 14 day trial) and give us your feedback.\n\nWe are open for collaboration and partnership, if you're interested just send us an email at [admin@ubiai.tools](mailto:admin@ubiai.tools)", "link": "https://www.reddit.com/r/MachineLearning/comments/l4t1vx/n_announcing_ubiai_easytouse_text_annotation_tool/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[n] announcing ubiai: easy-to-use text annotation -----> tool !!!  for nlp applications /!/ [ubiai](https://ubiai.tools) was born out of frustration with existing solutions, which either have a low quality/price ratio or are expensive and geared towards large companies. we know that data labeling is the bottleneck for creating custom nlp models (ner, entity relations, classification, etc) and is here to stay. we wanted to create the most accessible, easy-to-use and automated solution at an affordable price.\n\nif you are launching a new nlp project, please explore our tool [https://ubiai.tools](https://ubiai.tools) (we offer free 14 day trial) and give us your feedback.\n\nwe are open for collaboration and partnership, if you're interested just send us an email at [admin@ubiai.tools](mailto:admin@ubiai.tools)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/l4t1vx/n_announcing_ubiai_easytouse_text_annotation_tool/',)", "identifyer": 5730124, "year": "2021"}, {"autor": "UBIAI", "date": 1611596495000, "content": "Announcing UBIAI: Easy-to-Use Text Annotation Tool for NLP Applications /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/l4syp0/announcing_ubiai_easytouse_text_annotation_tool/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "announcing ubiai: easy-to-use text annotation -----> tool !!!  for nlp applications /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/l4syp0/announcing_ubiai_easytouse_text_annotation_tool/',)", "identifyer": 5730127, "year": "2021"}, {"autor": "menkeybhai", "date": 1611549141000, "content": "[R] 3D Point-cloud annotation tool with automated annotation utilities. /!/ Github Link: [https://github.com/hasanari/sane/](https://github.com/hasanari/sane/) \n\nPaper Link: [https://ieeexplore.ieee.org/document/9143095](https://ieeexplore.ieee.org/document/9143095)\n\nAddressing the need for high-quality, time-efficient, and easy-to-use annotation tools, we propose SAnE, a semi-automatic annotation tool for labeling point cloud data. While most current methods rely on multi-sensor approaches to provide bounding box annotations, we focus on the potential of the point cloud data itself for providing high-quality labeling in an efficient way. The contributions of this paper are threefold: (1) we propose a denoising pointwise segmentation strategy enabling a fast implementation of one-click annotation, (2) we expand the motion model technique with our guided-tracking algorithm, easing the frame-to-frame annotation processes, and (3) we provide an interactive yet robust open-source point cloud annotation tool, targeting both skilled and crowdsourcing annotators to create high-quality bounding box annotations. Using the KITTI dataset, we show that the SAnE speeds up the annotation process by a factor of 4.44 while achieving Intersection over Union (IoU) agreements of 84.27%. Furthermore, in experiments using crowdsourcing services, the full-featured SAnE achieves an accuracy of 79.36% while reducing the annotation time by a factor of 3, a significant improvement compared to the baseline accuracy of 62.02%. This result shows the potential of AI-assisted annotation tools, such as SAnE, for providing fast and accurate annotation labels for large-scale datasets with a significantly reduced price. \n\n&amp;#x200B;\n\nThis is to provide a quick 3D point cloud annotation tool to the open-source community; with all the latest models for annotation pre-labeling in \\*segmentation, bounding box detection &amp; shape completion\\*.\n\nCheck out the cool related repositories!", "link": "https://www.reddit.com/r/MachineLearning/comments/l4fzmd/r_3d_pointcloud_annotation_tool_with_automated/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[r] 3d point-cloud annotation -----> tool !!!  with automated annotation utilities. /!/ github link: [https://github.com/hasanari/sane/](https://github.com/hasanari/sane/) \n\npaper link: [https://ieeexplore.ieee.org/document/9143095](https://ieeexplore.ieee.org/document/9143095)\n\naddressing the need for high-quality, time-efficient, and easy-to-use annotation tools, we propose sane, a semi-automatic annotation tool for labeling point cloud data. while most current methods rely on multi-sensor approaches to provide bounding box annotations, we focus on the potential of the point cloud data itself for providing high-quality labeling in an efficient way. the contributions of this paper are threefold: (1) we propose a denoising pointwise segmentation strategy enabling a fast implementation of one-click annotation, (2) we expand the motion model technique with our guided-tracking algorithm, easing the frame-to-frame annotation processes, and (3) we provide an interactive yet robust open-source point cloud annotation tool, targeting both skilled and crowdsourcing annotators to create high-quality bounding box annotations. using the kitti dataset, we show that the sane speeds up the annotation process by a factor of 4.44 while achieving intersection over union (iou) agreements of 84.27%. furthermore, in experiments using crowdsourcing services, the full-featured sane achieves an accuracy of 79.36% while reducing the annotation time by a factor of 3, a significant improvement compared to the baseline accuracy of 62.02%. this result shows the potential of ai-assisted annotation tools, such as sane, for providing fast and accurate annotation labels for large-scale datasets with a significantly reduced price. \n\n&amp;#x200b;\n\nthis is to provide a quick 3d point cloud annotation tool to the open-source community; with all the latest models for annotation pre-labeling in \\*segmentation, bounding box detection &amp; shape completion\\*.\n\ncheck out the cool related repositories!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/l4fzmd/r_3d_pointcloud_annotation_tool_with_automated/',)", "identifyer": 5730152, "year": "2021"}, {"autor": "Goodd0ctor", "date": 1611429666000, "content": "[RESEARCH] My Experience as an Undergraduate AI Researcher /!/ If there is one thing that I have taken away from my first 3 years studying at university, it\u2019s that time is the most important resource that we all have. For some of us time goes by way too fast while for others it seems as if the hour just won\u2019t pass by. While I fall into the first category, I\u2019d like to believe that at one point in time everyone starts to wish that there were more hours in a day to do the things that matter to them. I also like to believe that a lot of people feel the pain of doing things in a day that just don\u2019t really matter in the big picture, but still have to get done. Although it\u2019s not possible to create more hours, there are ways to cut down the time that certain things take in a day in order to pursue the more important things in life. I\u2019d like to share my experience in doing research in AI that led me to learning the importance of time.\n\nIn my 2nd year of university, I took on research for both the computational brain lab and machine learning (ML) lab at my school. I loved doing research for both labs, and devoted countless hours a week to it even while in school. During university breaks I would often devote full time hours to research. A common mistake that lots of people make is overcommitting to things and feeling like they can get a lot more things done than they actually can. That was me. I thought that at the same time of excelling in my research that I could also spend time learning the skills needed to build a software business one day. Although I loved research, and still do, there came a point where I started to wish the hours spent on it could be spent on learning and honing the different skills that mattered to me in the bigger picture of things. This was my first lesson learned in my time doing research; excellent research takes a lot of time and is not something that can be done on the side.\n\nSince I was younger, I dreamed of creating my own business one day and nurturing it like a baby, then seeing it mature in its later phases. My friend shared the same dream as I, but on a more practical level. While I was doing research and *dreaming* of starting a business one day, my friend was *learning* *how* to start a business one day. I couldn\u2019t take it anymore, and realized that although I loved research, to me it ought to just be a tool that could be used to pursue my bigger dream. So I quit research and buried any plans on pursuing more schooling after my undergraduate studies end.\n\nWith the addition of so many hours to my day after ceasing my research, I started to finally pick up the skills that mattered for my dream of starting a business. I read countless books, finally learned how to build a website and scale it, and I was happy. At the same time, I realized that I should probably get some experience in the industry and see how other great businesses thrive before starting my own one day, so I started to prepare for internships. I applied to hundreds of them and realized that even now I was losing a lot of time from both applying to and worrying about getting an internship for the summer. We all know what this stress is like, seeing so many other people get the internships that you wish you had and seeing so many posts on LinkedIn of other peoples\u2019 success, then filling in the same old information on seemingly worthless applications that would most likely get ghosted.\n\nTo alleviate this pain point, my friend suggested that we work on a service that could make the job application process easier and faster. Remember, if there is one thing that matters it is time. My friend and I are pursuing this idea of a service that will be able to save hours a week for people looking for jobs and internships as well as bring them peace of mind in the entire stressful process. This way, people applying to jobs will be able to spend more of their time doing the things they love or learning the skills that actually matter to them, rather than filling in the same old information on applications and worrying about the places that have ghosted or rejected them or not.\n\nWe're planning to create the app along side our beta users, so if you are interested in a tool like this and would like to be a part of this process, sign up for the beta app and email list at [http://beehired.io/](http://beehired.io/)! Stay safe and love yourself. You're exactly where you need to be.", "link": "https://www.reddit.com/r/MachineLearning/comments/l3if7e/research_my_experience_as_an_undergraduate_ai/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[research] my experience as an undergraduate ai researcher /!/ if there is one thing that i have taken away from my first 3 years studying at university, it\u2019s that time is the most important resource that we all have. for some of us time goes by way too fast while for others it seems as if the hour just won\u2019t pass by. while i fall into the first category, i\u2019d like to believe that at one point in time everyone starts to wish that there were more hours in a day to do the things that matter to them. i also like to believe that a lot of people feel the pain of doing things in a day that just don\u2019t really matter in the big picture, but still have to get done. although it\u2019s not possible to create more hours, there are ways to cut down the time that certain things take in a day in order to pursue the more important things in life. i\u2019d like to share my experience in doing research in ai that led me to learning the importance of time.\n\nin my 2nd year of university, i took on research for both the computational brain lab and machine learning (ml) lab at my school. i loved doing research for both labs, and devoted countless hours a week to it even while in school. during university breaks i would often devote full time hours to research. a common mistake that lots of people make is overcommitting to things and feeling like they can get a lot more things done than they actually can. that was me. i thought that at the same time of excelling in my research that i could also spend time learning the skills needed to build a software business one day. although i loved research, and still do, there came a point where i started to wish the hours spent on it could be spent on learning and honing the different skills that mattered to me in the bigger picture of things. this was my first lesson learned in my time doing research; excellent research takes a lot of time and is not something that can be done on the side.\n\nsince i was younger, i dreamed of creating my own business one day and nurturing it like a baby, then seeing it mature in its later phases. my friend shared the same dream as i, but on a more practical level. while i was doing research and *dreaming* of starting a business one day, my friend was *learning* *how* to start a business one day. i couldn\u2019t take it anymore, and realized that although i loved research, to me it ought to just be a -----> tool !!!  that could be used to pursue my bigger dream. so i quit research and buried any plans on pursuing more schooling after my undergraduate studies end.\n\nwith the addition of so many hours to my day after ceasing my research, i started to finally pick up the skills that mattered for my dream of starting a business. i read countless books, finally learned how to build a website and scale it, and i was happy. at the same time, i realized that i should probably get some experience in the industry and see how other great businesses thrive before starting my own one day, so i started to prepare for internships. i applied to hundreds of them and realized that even now i was losing a lot of time from both applying to and worrying about getting an internship for the summer. we all know what this stress is like, seeing so many other people get the internships that you wish you had and seeing so many posts on linkedin of other peoples\u2019 success, then filling in the same old information on seemingly worthless applications that would most likely get ghosted.\n\nto alleviate this pain point, my friend suggested that we work on a service that could make the job application process easier and faster. remember, if there is one thing that matters it is time. my friend and i are pursuing this idea of a service that will be able to save hours a week for people looking for jobs and internships as well as bring them peace of mind in the entire stressful process. this way, people applying to jobs will be able to spend more of their time doing the things they love or learning the skills that actually matter to them, rather than filling in the same old information on applications and worrying about the places that have ghosted or rejected them or not.\n\nwe're planning to create the app along side our beta users, so if you are interested in a tool like this and would like to be a part of this process, sign up for the beta app and email list at [http://beehired.io/](http://beehired.io/)! stay safe and love yourself. you're exactly where you need to be.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/l3if7e/research_my_experience_as_an_undergraduate_ai/',)", "identifyer": 5730205, "year": "2021"}, {"autor": "Somespecialcharacter", "date": 1630017637000, "content": "[D] Repeated Measure /!/ Hello folks,\n\nI am a newbie with predictive modeling and honestly I have a minor in statistics and I am moving more towards machine learning concepts. \nNow, this may be completely absurd but I just had a thought and needed an opinion.\nI worked on a research project where we had a binary outcome Yes/No looking at weather they visited a store for a set of 1699 participants. Total number of stores in our study were 49. We created a tool that gives us the quality of the stores on a percentage scale.\n\nOut hypotheses was, the better the store score the more the users.\nThe way we linked 1699 participants to 49 stores was by the zipcode the lived in and the zipcode the store was in. And to make sure of the sampling we ascertain the percentages of participants from each zipcode and store. \n\nThe dataset looked something like 1699 rows with either yes or no on the visit to store and the score for the store repeated for participants in each zip code.\n\nFor eg\n\n1 yes 87.55 zipcode x\n2 no  87.55 zipcode x \n3 yes 91.77 zipcode y\n4 yes 91.77 zipcode y\n5 yes 87.55 zipcode x\n6 no  73.01  zipcode z\n\u2026\u2026.\n1699 \n\n\nI hope the data makes sense now. We used a GEE model to adjust for repeated measures. But our model was not successful. (Any suggestions on this would also be helpful)\n\nNow, while learning ML concepts I was wondering if there could potentially be any algorithm that kind of fits our data and study design to build a predictive model that predicts the usage from the score that we build for the store.\nSorry for the long post and thank you for making till the end and showing interest. \nYour help is highly appreciated!", "link": "https://www.reddit.com/r/MachineLearning/comments/pcadn1/d_repeated_measure/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] repeated measure /!/ hello folks,\n\ni am a newbie with predictive modeling and honestly i have a minor in statistics and i am moving more towards machine learning concepts. \nnow, this may be completely absurd but i just had a thought and needed an opinion.\ni worked on a research project where we had a binary outcome yes/no looking at weather they visited a store for a set of 1699 participants. total number of stores in our study were 49. we created a -----> tool !!!  that gives us the quality of the stores on a percentage scale.\n\nout hypotheses was, the better the store score the more the users.\nthe way we linked 1699 participants to 49 stores was by the zipcode the lived in and the zipcode the store was in. and to make sure of the sampling we ascertain the percentages of participants from each zipcode and store. \n\nthe dataset looked something like 1699 rows with either yes or no on the visit to store and the score for the store repeated for participants in each zip code.\n\nfor eg\n\n1 yes 87.55 zipcode x\n2 no  87.55 zipcode x \n3 yes 91.77 zipcode y\n4 yes 91.77 zipcode y\n5 yes 87.55 zipcode x\n6 no  73.01  zipcode z\n\u2026\u2026.\n1699 \n\n\ni hope the data makes sense now. we used a gee model to adjust for repeated measures. but our model was not successful. (any suggestions on this would also be helpful)\n\nnow, while learning ml concepts i was wondering if there could potentially be any algorithm that kind of fits our data and study design to build a predictive model that predicts the usage from the score that we build for the store.\nsorry for the long post and thank you for making till the end and showing interest. \nyour help is highly appreciated!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/pcadn1/d_repeated_measure/',)", "identifyer": 5730282, "year": "2021"}, {"autor": "ai-lover", "date": 1621651510000, "content": "[N] Facebook\u2019s \u2018Expire-Span\u2019 Tool Enables Machine Learning Models to Forget Irrelevant Data (Paper and Code included) /!/ Facebook has recently developed an AI tool that allows machine learning models to preserve certain information while forgetting the rest. It claims that the tool,\u00a0[**Expire-Span**](https://arxiv.org/pdf/2105.06548.pdf), can predict information most relevant to a task at hand thereby, allowing AI systems to process data at larger scales.\u00a0\n\nConventionally, AI models memorize information without distinction, unlike humans. Therefore, creating the ability to decide whether to forget the information or not at the software level is challenging. Usually, state-of-the-art models struggle with large quantities of information like books or videos and incurring high computing costs. This can lead to many other problems such as catastrophic learning or catastrophic interference, a situation where AI systems fail to recall what they\u2019ve learned from a training dataset.\u00a0\n\nSource: [https://www.marktechpost.com/2021/05/21/facebooks-expire-span-tool-enables-machine-learning-models-to-forget-irrelevant-data/](https://www.marktechpost.com/2021/05/21/facebooks-expire-span-tool-enables-machine-learning-models-to-forget-irrelevant-data/?_ga=2.232088284.2144888320.1621650511-488125022.1618729090)\n\nCodes: [https://github.com/facebookresearch/transformer-sequential](https://github.com/facebookresearch/transformer-sequential) \n\nPaper: [https://arxiv.org/pdf/2105.06548.pdf](https://arxiv.org/pdf/2105.06548.pdf)", "link": "https://www.reddit.com/r/MachineLearning/comments/ni90r9/n_facebooks_expirespan_tool_enables_machine/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[n] facebook\u2019s \u2018expire-span\u2019 -----> tool !!!  enables machine learning models to forget irrelevant data (paper and code included) /!/ facebook has recently developed an ai tool that allows machine learning models to preserve certain information while forgetting the rest. it claims that the tool,\u00a0[**expire-span**](https://arxiv.org/pdf/2105.06548.pdf), can predict information most relevant to a task at hand thereby, allowing ai systems to process data at larger scales.\u00a0\n\nconventionally, ai models memorize information without distinction, unlike humans. therefore, creating the ability to decide whether to forget the information or not at the software level is challenging. usually, state-of-the-art models struggle with large quantities of information like books or videos and incurring high computing costs. this can lead to many other problems such as catastrophic learning or catastrophic interference, a situation where ai systems fail to recall what they\u2019ve learned from a training dataset.\u00a0\n\nsource: [https://www.marktechpost.com/2021/05/21/facebooks-expire-span-tool-enables-machine-learning-models-to-forget-irrelevant-data/](https://www.marktechpost.com/2021/05/21/facebooks-expire-span-tool-enables-machine-learning-models-to-forget-irrelevant-data/?_ga=2.232088284.2144888320.1621650511-488125022.1618729090)\n\ncodes: [https://github.com/facebookresearch/transformer-sequential](https://github.com/facebookresearch/transformer-sequential) \n\npaper: [https://arxiv.org/pdf/2105.06548.pdf](https://arxiv.org/pdf/2105.06548.pdf)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/ni90r9/n_facebooks_expirespan_tool_enables_machine/',)", "identifyer": 5730611, "year": "2021"}, {"autor": "stanmilc", "date": 1618774072000, "content": "To be ready for Crypto corrections when \"Bitcoin plunges to levels not seen since last month\" start using the right tool predictive AI trend indicator CryptoDivine.ai by DigiMax Global (OTC: #DBKSF, CSE: #DIGI ),deep-learning AI that predicts price movements in #BTC and #ETH", "link": "https://www.reddit.com/r/MachineLearning/comments/mtjkz2/to_be_ready_for_crypto_corrections_when_bitcoin/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "to be ready for crypto corrections when \"bitcoin plunges to levels not seen since last month\" start using the right -----> tool !!!  predictive ai trend indicator cryptodivine.ai by digimax global (otc: #dbksf, cse: #digi ),deep-learning ai that predicts price movements in #btc and #eth", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('image',)", "medialink": "('https://i.redd.it/c4cigysjkyt61.jpg',)", "identifyer": 5730673, "year": "2021"}, {"autor": "crazyfrogspb", "date": 1625475286000, "content": "[D] Best practices of storing annotations for image data /!/ What are the best practices of storing annotations (bounding boxes, masks) and other kind of meta data for image datasets?\n\nThere are some options that we tried out or considering right now:\n\n1) JSON/CSV + storing train/val/test splits in the repo\n\n2) JSON/CSV + DVC\n\n3) Relational database\n\nAll of them seem to have some pros and cons. Here are the factors that we consider:\n\n1) Simplicity. Simple JSONs win here, there is no need to maintain any DB or use an external tool.\n\n2) Reproducibility of experiments. DVC leads here, whereas it might be difficult to version control a DB.\n\n3) Clear structure + Ability to easily add new annotations and information - databases seem to win here. We mainly work with medical images, and there is a large nested structure of the data (e.g., hospital - patient - study - left/right breast - CC/MLO view - annotation). In addition, there are usually multiple annotators per image. Storing and linking all this information in CSV/JSONs slowly becomes a nightmare.\n\n&amp;#x200B;\n\nOverall, it seems like there is no clear winner. Are we missing any other options?", "link": "https://www.reddit.com/r/MachineLearning/comments/oe374e/d_best_practices_of_storing_annotations_for_image/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] best practices of storing annotations for image data /!/ what are the best practices of storing annotations (bounding boxes, masks) and other kind of meta data for image datasets?\n\nthere are some options that we tried out or considering right now:\n\n1) json/csv + storing train/val/test splits in the repo\n\n2) json/csv + dvc\n\n3) relational database\n\nall of them seem to have some pros and cons. here are the factors that we consider:\n\n1) simplicity. simple jsons win here, there is no need to maintain any db or use an external -----> tool !!! .\n\n2) reproducibility of experiments. dvc leads here, whereas it might be difficult to version control a db.\n\n3) clear structure + ability to easily add new annotations and information - databases seem to win here. we mainly work with medical images, and there is a large nested structure of the data (e.g., hospital - patient - study - left/right breast - cc/mlo view - annotation). in addition, there are usually multiple annotators per image. storing and linking all this information in csv/jsons slowly becomes a nightmare.\n\n&amp;#x200b;\n\noverall, it seems like there is no clear winner. are we missing any other options?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 11, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/oe374e/d_best_practices_of_storing_annotations_for_image/',)", "identifyer": 5730819, "year": "2021"}, {"autor": "IborkedyourGPU", "date": 1625911691000, "content": "[D] Gradio vs Streamlit /!/ Gradio is getting quite marketed quite a bit on social media, but I see much less attention for Streamlit, which seems to me to be quite similar (and possibly more flexible). Do you have any experience using them? What are the pros and cons of each, in your opinion?\n\nPS I heard good things about [opyrator](https://github.com/ml-tooling/opyrator) too, but it's obviously a less mature tool.", "link": "https://www.reddit.com/r/MachineLearning/comments/ohg1dw/d_gradio_vs_streamlit/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] gradio vs streamlit /!/ gradio is getting quite marketed quite a bit on social media, but i see much less attention for streamlit, which seems to me to be quite similar (and possibly more flexible). do you have any experience using them? what are the pros and cons of each, in your opinion?\n\nps i heard good things about [opyrator](https://github.com/ml-tooling/opyrator) too, but it's obviously a less mature -----> tool !!! .", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/ohg1dw/d_gradio_vs_streamlit/',)", "identifyer": 5730907, "year": "2021"}, {"autor": "frank_sobotka_", "date": 1625843151000, "content": "[D] Feedback on modeling approach, building an auto-design tool using multi-task learning methods /!/ Greetings friends!\n\nI am currently working as a MLE for a startup in the e-commerce space and have recently been tasked with developing a ML model to personalize landing/web pages based on user account data as well as text data that the user provides beforehand. Without getting too deep into the company jargon and weeds my inputs are as follows:\n\n* BERT embeddings generated from the long form text provided by user\n* Account info and standard headers from http requests\n\nMy target output is generating a series of parameters/categories that go into a config file (json) that set the formatting of the page, there are roughly \\~30 of these, each having roughly \\~10 options within, ex: \"border color: #fffffff\" or \" positonalElement4: #000000\" and so on... I have existing configs that have been previously generated (randomly) but selected by a user.\n\n**My question for the the sub is as follows:** How would you recommend architecting a system that can learn these parameters? My current approach is to utilize multi-task learning to build a unified trunk (backbone) and have a head for each param. This will result in roughly 30 heads.  Does this seem doable / reasonable? \n\nIf anyone knows of any other projects that have been done that are similar to this use case please let me know, any research papers or github links would be appreciated.", "link": "https://www.reddit.com/r/MachineLearning/comments/ogxh4o/d_feedback_on_modeling_approach_building_an/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] feedback on modeling approach, building an auto-design -----> tool !!!  using multi-task learning methods /!/ greetings friends!\n\ni am currently working as a mle for a startup in the e-commerce space and have recently been tasked with developing a ml model to personalize landing/web pages based on user account data as well as text data that the user provides beforehand. without getting too deep into the company jargon and weeds my inputs are as follows:\n\n* bert embeddings generated from the long form text provided by user\n* account info and standard headers from http requests\n\nmy target output is generating a series of parameters/categories that go into a config file (json) that set the formatting of the page, there are roughly \\~30 of these, each having roughly \\~10 options within, ex: \"border color: #fffffff\" or \" positonalelement4: #000000\" and so on... i have existing configs that have been previously generated (randomly) but selected by a user.\n\n**my question for the the sub is as follows:** how would you recommend architecting a system that can learn these parameters? my current approach is to utilize multi-task learning to build a unified trunk (backbone) and have a head for each param. this will result in roughly 30 heads.  does this seem doable / reasonable? \n\nif anyone knows of any other projects that have been done that are similar to this use case please let me know, any research papers or github links would be appreciated.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/ogxh4o/d_feedback_on_modeling_approach_building_an/',)", "identifyer": 5730949, "year": "2021"}, {"autor": "Yolmack", "date": 1625819423000, "content": "[P] Prediction with machine learning /!/ Hello,\n\nI am gathering a set of data to do data prediction.\n\nLet say my set of data have the following variables\n\n\\- var1: string\n\n\\- var2: int\n\n\\- var3: bool\n\nI would like depending on var1 and var2 predict var3\n\nHow many data will I need to get a good quality of prediction ?  \nvar2 is an int can I take +- 5% for example ?  \nAt the moment I do not only have two variables to predict the third one, a have 10, did I need more ?  \nDo you recommend any online tool to upload the set of data, so it can make the prediction, or do it by myself ?\n\nThanks !", "link": "https://www.reddit.com/r/MachineLearning/comments/ogrbd7/p_prediction_with_machine_learning/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] prediction with machine learning /!/ hello,\n\ni am gathering a set of data to do data prediction.\n\nlet say my set of data have the following variables\n\n\\- var1: string\n\n\\- var2: int\n\n\\- var3: bool\n\ni would like depending on var1 and var2 predict var3\n\nhow many data will i need to get a good quality of prediction ?  \nvar2 is an int can i take +- 5% for example ?  \nat the moment i do not only have two variables to predict the third one, a have 10, did i need more ?  \ndo you recommend any online -----> tool !!!  to upload the set of data, so it can make the prediction, or do it by myself ?\n\nthanks !", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/ogrbd7/p_prediction_with_machine_learning/',)", "identifyer": 5730963, "year": "2021"}, {"autor": "nadermx", "date": 1627055436000, "content": "[P] Remove background from video and image using machine learning, open source /!/ Hi r/MachineLearning,\n\nI was playing around with some things and after a while ended up making [BackgroundRemover.app](https://BackgroundRemover.app), which is a tool to remove background from images and videos.  If you want to see the source code, or play with it, I put it [here](https://github.com/nadermx/backgroundremover).", "link": "https://www.reddit.com/r/MachineLearning/comments/oq57ac/p_remove_background_from_video_and_image_using/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] remove background from video and image using machine learning, open source /!/ hi r/machinelearning,\n\ni was playing around with some things and after a while ended up making [backgroundremover.app](https://backgroundremover.app), which is a -----> tool !!!  to remove background from images and videos.  if you want to see the source code, or play with it, i put it [here](https://github.com/nadermx/backgroundremover).", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/oq57ac/p_remove_background_from_video_and_image_using/',)", "identifyer": 5731141, "year": "2021"}, {"autor": "monfre_97", "date": 1626735338000, "content": "[D] Voice Style Transfer: State of the Art /!/ I am looking around for the state of the art on voice style transfer models. This is widely recognized as a domain with many industrial application, but (of course) I haven't found anything fully satisfactory around, meaning a good quality tool ready for use. Too much optimistic and lazy, but this is what ideally I was shooting for. \n\nSo, plan B is to dig deeper for myself, looking for cool models to implement or similar. Personally I am curious to experiment style transfer from a source to a target speaker in music (singing). Do you have any model, repository, paper to advice to frame what's the state of the art as of today? Any help is really appreciated", "link": "https://www.reddit.com/r/MachineLearning/comments/onpdlq/d_voice_style_transfer_state_of_the_art/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] voice style transfer: state of the art /!/ i am looking around for the state of the art on voice style transfer models. this is widely recognized as a domain with many industrial application, but (of course) i haven't found anything fully satisfactory around, meaning a good quality -----> tool !!!  ready for use. too much optimistic and lazy, but this is what ideally i was shooting for. \n\nso, plan b is to dig deeper for myself, looking for cool models to implement or similar. personally i am curious to experiment style transfer from a source to a target speaker in music (singing). do you have any model, repository, paper to advice to frame what's the state of the art as of today? any help is really appreciated", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/onpdlq/d_voice_style_transfer_state_of_the_art/',)", "identifyer": 5731228, "year": "2021"}, {"autor": "rg089", "date": 1626602365000, "content": "[Project] Newsemble: An API to fetch current news data /!/ &amp;#x200B;\n\nhttps://preview.redd.it/kzsxxkld2yb71.png?width=480&amp;format=png&amp;auto=webp&amp;s=015490a1ebd09d11f0a22c979f592375e7242436\n\nNote: *This is not exactly a Machine Learning project, but a tool that can be useful in many Machine Learning (NLP)  projects.*\n\nHey everyone,\n\nI (along with 2 other people) made a project called **Newsemble**.      It is an API that allows for fast retrieval of current news (at the     moment, only Indian websites are supported, but we can add others if     anyone wants that). It's a REST API built using *Flask, MongoDB and BeautifulSoup*.      Due to some of the drawbacks of current news APIs (full content not     available, character limit, limited requests), we wanted to build  our    own as were looking to do news analysis.\n\nWe have made all the code open source. Please refer to the [medium blog](https://medium.com/@rg089/newsemble-3311d2dc9817) for further details and implementation of this API.\n\n*This will be useful for news analysis, trend detection, keyword detection amongst other NLP tasks.*\n\n*We are planning to release some NLP projects using this API very soon!*\n\n***Most  importantly, if there are any additional features or extra news  sites    you want, or any other improvements in general, please do let  us  know. Thanks!\ud83e\udd1d\ud83c\udffb***\n\nIf you found the project useful, please \ud83d\udc4f the article or \ud83c\udf1f the repo. It really motivates us going forward!\n\nBlog link :\n\n[https://medium.com/@rg089/newsemble-3311d2dc9817](https://medium.com/@rg089/newsemble-3311d2dc9817)\n\nSource code :\n\n[https://github.com/rg089/newsemble](https://github.com/rg089/newsemble)\n\nAPI link:\n\n[http://www.newsemble.ml/news/](http://www.newsemble.ml/news/)", "link": "https://www.reddit.com/r/MachineLearning/comments/omnow9/project_newsemble_an_api_to_fetch_current_news/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[project] newsemble: an api to fetch current news data /!/ &amp;#x200b;\n\nhttps://preview.redd.it/kzsxxkld2yb71.png?width=480&amp;format=png&amp;auto=webp&amp;s=015490a1ebd09d11f0a22c979f592375e7242436\n\nnote: *this is not exactly a machine learning project, but a -----> tool !!!  that can be useful in many machine learning (nlp)  projects.*\n\nhey everyone,\n\ni (along with 2 other people) made a project called **newsemble**.      it is an api that allows for fast retrieval of current news (at the     moment, only indian websites are supported, but we can add others if     anyone wants that). it's a rest api built using *flask, mongodb and beautifulsoup*.      due to some of the drawbacks of current news apis (full content not     available, character limit, limited requests), we wanted to build  our    own as were looking to do news analysis.\n\nwe have made all the code open source. please refer to the [medium blog](https://medium.com/@rg089/newsemble-3311d2dc9817) for further details and implementation of this api.\n\n*this will be useful for news analysis, trend detection, keyword detection amongst other nlp tasks.*\n\n*we are planning to release some nlp projects using this api very soon!*\n\n***most  importantly, if there are any additional features or extra news  sites    you want, or any other improvements in general, please do let  us  know. thanks!\ud83e\udd1d\ud83c\udffb***\n\nif you found the project useful, please \ud83d\udc4f the article or \ud83c\udf1f the repo. it really motivates us going forward!\n\nblog link :\n\n[https://medium.com/@rg089/newsemble-3311d2dc9817](https://medium.com/@rg089/newsemble-3311d2dc9817)\n\nsource code :\n\n[https://github.com/rg089/newsemble](https://github.com/rg089/newsemble)\n\napi link:\n\n[http://www.newsemble.ml/news/](http://www.newsemble.ml/news/)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/omnow9/project_newsemble_an_api_to_fetch_current_news/',)", "identifyer": 5731286, "year": "2021"}, {"autor": "leadgenerationtool", "date": 1627321086000, "content": "What Tool Can I Use To Extract Emails Of Registered Users From Websites?", "link": "https://www.reddit.com/r/MachineLearning/comments/os3sve/what_tool_can_i_use_to_extract_emails_of/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what -----> tool !!!  can i use to extract emails of registered users from websites?", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('link',)", "medialink": "('https://ahmadsoftware784584843.wordpress.com/2021/07/26/what-tool-can-i-use-to-extract-emails-of-registered-users-from-websites/',)", "identifyer": 5731386, "year": "2021"}, {"autor": "t3chflicks", "date": 1626174977000, "content": "I made an emoji search tool using embeddings and wrote a blog on it", "link": "https://www.reddit.com/r/MachineLearning/comments/ojdge8/i_made_an_emoji_search_tool_using_embeddings_and/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "i made an emoji search -----> tool !!!  using embeddings and wrote a blog on it", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('link',)", "medialink": "('https://github.com/sk-t3ch/emoji-search',)", "identifyer": 5731684, "year": "2021"}, {"autor": "rirhun", "date": 1629184390000, "content": "[D] Kubeflow vs. Argo for ML Pipeline Tool /!/ Hello all,\n\nI'm wondering if anybody has used either Kubeflow or Argo and has any recommendations as to which one would be better to adopt. At our organization, we are currently using Airflow to orchestrate ML Pipeline tasks. I would rather we move away from using Airflow since it's more of a general purpose workflow orchestration tool that wasn't really meant for resource intensive ML pipeline tasks. A lot of our ML training would involve NLP on billions of documents, so we need a something that is stable, scalable, highly available but also developer-friendly. \n\nI've heard some bad things about Kubeflow and that it's not quite developer friendly, but that was a year ago. A lot of the out of the box features like model management and experiment tracking look promising though and I have plans to do model serving on Seldon Core/KfServing which pairs well with Kubeflow. On the other hand, I've heard nothing but great things about Argo from those who've used it. It seems a bit more developer friendly than Kubeflow and it would seem Kubeflow is based on Argo. So not sure which one I should go with. Any advice would be greatly appreciated!", "link": "https://www.reddit.com/r/MachineLearning/comments/p5ytxk/d_kubeflow_vs_argo_for_ml_pipeline_tool/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] kubeflow vs. argo for ml pipeline -----> tool !!!  /!/ hello all,\n\ni'm wondering if anybody has used either kubeflow or argo and has any recommendations as to which one would be better to adopt. at our organization, we are currently using airflow to orchestrate ml pipeline tasks. i would rather we move away from using airflow since it's more of a general purpose workflow orchestration tool that wasn't really meant for resource intensive ml pipeline tasks. a lot of our ml training would involve nlp on billions of documents, so we need a something that is stable, scalable, highly available but also developer-friendly. \n\ni've heard some bad things about kubeflow and that it's not quite developer friendly, but that was a year ago. a lot of the out of the box features like model management and experiment tracking look promising though and i have plans to do model serving on seldon core/kfserving which pairs well with kubeflow. on the other hand, i've heard nothing but great things about argo from those who've used it. it seems a bit more developer friendly than kubeflow and it would seem kubeflow is based on argo. so not sure which one i should go with. any advice would be greatly appreciated!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 30, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/p5ytxk/d_kubeflow_vs_argo_for_ml_pipeline_tool/',)", "identifyer": 5731847, "year": "2021"}, {"autor": "shelper9527", "date": 1629158210000, "content": "[Discussion] Open source scheduler and queuing system for model training/inferencing tasks? /!/  I work in a team that shares a high end workstation for Machine Learning model training. since it is often difficult to share the GPUs, so I would like to setup a task scheduler and queuing system so that anyone who wants to run a task, can wrap up the task in docker and submit it. when his/her task is done, he/she will get notification, and then the system will run the next task if any. we can also set a time out for each task, say: 10 hours, 2 days, etc. so it is not preempted for ever by single task.\n\nis there such an open source tool for this purpose?\n\nI also wonder if such tool is available for google cloud. it would be nice if we can use GCP for training rather than maintaining our own workstation. but my boss is worried about huge bills from Google, which did happen in other team. so I wonder if there is some way we can setup a similar scheduler and queuing system. e.g., we can have a cheap low end VM running 24x7 on GCP that takes in tasks and queues them, the VM will then submit tasks to expensive GCP TPU nodes for training, the low end VM will monitor the status of each training task and reports back if anything wrong, or cap the training time to avoid expensive bills.\n\nI did some googling and could not find an existing mature solution...", "link": "https://www.reddit.com/r/MachineLearning/comments/p5sbxu/discussion_open_source_scheduler_and_queuing/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[discussion] open source scheduler and queuing system for model training/inferencing tasks? /!/  i work in a team that shares a high end workstation for machine learning model training. since it is often difficult to share the gpus, so i would like to setup a task scheduler and queuing system so that anyone who wants to run a task, can wrap up the task in docker and submit it. when his/her task is done, he/she will get notification, and then the system will run the next task if any. we can also set a time out for each task, say: 10 hours, 2 days, etc. so it is not preempted for ever by single task.\n\nis there such an open source -----> tool !!!  for this purpose?\n\ni also wonder if such tool is available for google cloud. it would be nice if we can use gcp for training rather than maintaining our own workstation. but my boss is worried about huge bills from google, which did happen in other team. so i wonder if there is some way we can setup a similar scheduler and queuing system. e.g., we can have a cheap low end vm running 24x7 on gcp that takes in tasks and queues them, the vm will then submit tasks to expensive gcp tpu nodes for training, the low end vm will monitor the status of each training task and reports back if anything wrong, or cap the training time to avoid expensive bills.\n\ni did some googling and could not find an existing mature solution...", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/p5sbxu/discussion_open_source_scheduler_and_queuing/',)", "identifyer": 5731858, "year": "2021"}, {"autor": "EricW_CS", "date": 1611081853000, "content": "[D] Is there a tool to format your data nicely? (Ex. convert ROS bags to nuScenes or KITTI)? /!/ I am working with a bunch of ROS bags and I was hoping there would be an easy way to convert the data into a format that is accepted by machine learning frameworks without having to write a bunch of custom data loaders. I was considering modifying nuScenes to support my custom data, but it means I have to change all the labels and modify a bunch of stuff because my data is for autonomous boats (not autonomous vehicles).\n\nBefore I go down this rabbit hole, I was wondering if anyone knew of any existing tools that could easily convert data (PCD, images, transformations, imu, sonar, etc), to different formats.\n\nI'd also appreciate if anyone knew of any universal data formats. If there isn't one, it seems like a good project.", "link": "https://www.reddit.com/r/MachineLearning/comments/l0q7tk/d_is_there_a_tool_to_format_your_data_nicely_ex/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] is there a -----> tool !!!  to format your data nicely? (ex. convert ros bags to nuscenes or kitti)? /!/ i am working with a bunch of ros bags and i was hoping there would be an easy way to convert the data into a format that is accepted by machine learning frameworks without having to write a bunch of custom data loaders. i was considering modifying nuscenes to support my custom data, but it means i have to change all the labels and modify a bunch of stuff because my data is for autonomous boats (not autonomous vehicles).\n\nbefore i go down this rabbit hole, i was wondering if anyone knew of any existing tools that could easily convert data (pcd, images, transformations, imu, sonar, etc), to different formats.\n\ni'd also appreciate if anyone knew of any universal data formats. if there isn't one, it seems like a good project.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/l0q7tk/d_is_there_a_tool_to_format_your_data_nicely_ex/',)", "identifyer": 5731959, "year": "2021"}, {"autor": "EricW_CS", "date": 1611081741000, "content": "Is there a tool to format your data nicely? (Ex. convert ROS bags to nuScenes or KITTI)? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/l0q68d/is_there_a_tool_to_format_your_data_nicely_ex/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "is there a -----> tool !!!  to format your data nicely? (ex. convert ros bags to nuscenes or kitti)? /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/l0q68d/is_there_a_tool_to_format_your_data_nicely_ex/',)", "identifyer": 5731961, "year": "2021"}, {"autor": "stanmilc", "date": 1619538867000, "content": "Anyone Want a free $1200 trading tool For digital currency? DigiCrypts blockchain is promoting its new app!! $DBKSF $DiGi.C", "link": "https://www.reddit.com/r/MachineLearning/comments/mzrkw8/anyone_want_a_free_1200_trading_tool_for_digital/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "anyone want a free $1200 trading -----> tool !!!  for digital currency? digicrypts blockchain is promoting its new app!! $dbksf $digi.c", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('link',)", "medialink": "('/r/Wallstreetbetsnew/comments/mzr0el/anyone_want_a_free_1200_trading_tool_for_digital/',)", "identifyer": 5732118, "year": "2021"}, {"autor": "the21st", "date": 1619516433000, "content": "[P] I missed writing native SQL queries inside jupyter so I built a Python+SQL interop feature in our notebook tool /!/ Hi there! I'm Simon, an engineer at Deepnote, previously I worked as an ML team lead in an e-commerce company. I recently built a new feature in Deepnote and I just wanted to brag a little bit because I'm proud of it \u263a\ufe0f\n\nIt's native SQL cells with Python interop, check out an [example project](https://deepnote.com/project/RNA-Exploration-Duplicate-xlaRWiCeRNqQ0Id0v2RY5A/%2Fnotebook.ipynb). Full docs for this feature are [here](https://docs.deepnote.com/features/sql-cells).\n\nThis was possible thanks to standing on the shoulders of open-source giants such as pandas and jinjasql, which I hugely appreciate and it makes me truly happy.\n\nI'd appreciate any feedback and your thoughts on what innovation you'd love to see around notebooks in the future \ud83d\ude42", "link": "https://www.reddit.com/r/MachineLearning/comments/mzkqam/p_i_missed_writing_native_sql_queries_inside/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] i missed writing native sql queries inside jupyter so i built a python+sql interop feature in our notebook -----> tool !!!  /!/ hi there! i'm simon, an engineer at deepnote, previously i worked as an ml team lead in an e-commerce company. i recently built a new feature in deepnote and i just wanted to brag a little bit because i'm proud of it \u263a\ufe0f\n\nit's native sql cells with python interop, check out an [example project](https://deepnote.com/project/rna-exploration-duplicate-xlarwicernqq0id0v2ry5a/%2fnotebook.ipynb). full docs for this feature are [here](https://docs.deepnote.com/features/sql-cells).\n\nthis was possible thanks to standing on the shoulders of open-source giants such as pandas and jinjasql, which i hugely appreciate and it makes me truly happy.\n\ni'd appreciate any feedback and your thoughts on what innovation you'd love to see around notebooks in the future \ud83d\ude42", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/mzkqam/p_i_missed_writing_native_sql_queries_inside/',)", "identifyer": 5732144, "year": "2021"}, {"autor": "the21st", "date": 1619516297000, "content": "I missed writing native SQL queries inside jupyter so I built a Python+SQL interop feature in our notebook tool /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/mzkp7q/i_missed_writing_native_sql_queries_inside/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "i missed writing native sql queries inside jupyter so i built a python+sql interop feature in our notebook -----> tool !!!  /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/mzkp7q/i_missed_writing_native_sql_queries_inside/',)", "identifyer": 5732145, "year": "2021"}, {"autor": "the21st", "date": 1619516067000, "content": "I missed writing native SQL queries inside jupyter so I built a Python+SQL interop feature in our notebook tool", "link": "https://www.reddit.com/r/MachineLearning/comments/mzknf6/i_missed_writing_native_sql_queries_inside/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "i missed writing native sql queries inside jupyter so i built a python+sql interop feature in our notebook -----> tool !!! ", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://deepnote.com/project/RNA-Exploration-Duplicate-xlaRWiCeRNqQ0Id0v2RY5A/%2Fnotebook.ipynb',)", "identifyer": 5732146, "year": "2021"}, {"autor": "samajavaragamana", "date": 1610476470000, "content": "[D] Evaluating Online Experimentation Platforms, A/B Testing Analytics teams /!/ Hi Everyone, \n\nI understand the AB Testing/ Online Experimentation team is structured in a couple of ways in an organization: \n\n1.If  there is an in-house exp platform, a core team maintains the platform, while product managers, email marketers and so on run the experiments. \n\n2. Or, the testing is done by Analytics teams. This may usually be the case when an off-the-shelf tool such as Optimizely/SiteSpect are used. \n\nMy questions are: \n\na. In the decentralized set-up option, what is the level of supervision of the experiments being conducted by product managers/others who may not have the expertise to design or interpret tests. \n\nb. How do you measure the impact of an AB Testing org/platform? What would you say are the KPI';s for such a team. To justify their existence, it is possible the folks running the tests are inflating test wins, and impact of the wins? How can this be averted/avoided/handled? \n\nc. Do you think overall A/B tests in the online space are being conducted with rigor? \n\nThank you!", "link": "https://www.reddit.com/r/MachineLearning/comments/kvxk85/d_evaluating_online_experimentation_platforms_ab/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] evaluating online experimentation platforms, a/b testing analytics teams /!/ hi everyone, \n\ni understand the ab testing/ online experimentation team is structured in a couple of ways in an organization: \n\n1.if  there is an in-house exp platform, a core team maintains the platform, while product managers, email marketers and so on run the experiments. \n\n2. or, the testing is done by analytics teams. this may usually be the case when an off-the-shelf -----> tool !!!  such as optimizely/sitespect are used. \n\nmy questions are: \n\na. in the decentralized set-up option, what is the level of supervision of the experiments being conducted by product managers/others who may not have the expertise to design or interpret tests. \n\nb. how do you measure the impact of an ab testing org/platform? what would you say are the kpi';s for such a team. to justify their existence, it is possible the folks running the tests are inflating test wins, and impact of the wins? how can this be averted/avoided/handled? \n\nc. do you think overall a/b tests in the online space are being conducted with rigor? \n\nthank you!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/kvxk85/d_evaluating_online_experimentation_platforms_ab/',)", "identifyer": 5732222, "year": "2021"}, {"autor": "fredfredbur", "date": 1610466501000, "content": "[D] How many of you use Python scripts versus notebooks? /!/ I'm curious how many people use Python notebooks like Jupyter or Colab versus just writing Python scripts when working on an ML project. I personally just use scripts at the moment but I'm interested in hearing some reasons why you prefer notebooks instead.\n\n&amp;#x200B;\n\nAdditionally, I'm hoping to get some feedback on the notebook support recently added to the [open-source ML tool, FiftyOne](https://voxel51.com/docs/fiftyone/), that I've been working on. FiftyOne is a [Python API](https://github.com/voxel51/fiftyone) \\+ App that lets you load and explore your image and video datasets and model predictions to debug your datasets and models. \n\nYou can now load the App in the output cell of a notebook to explore your dataset in the notebook itself, previously you had to launch it in a separate window. \n\nWhile other tools like Tensorboard and Matplotlib have notebook support, their output plots generally don't get updated by code further down in the notebook, like what might happen with FiftyOne.\n\nSince there wasn't really a precedent to follow and I don't have much experience with notebook workflows, I was hoping to get some feedback here about how it could be improved. \n\nYou can try it out in Colab here: [https://colab.research.google.com/github/voxel51/fiftyone/blob/v0.7.1.2/docs/source/tutorials/evaluate\\_detections.ipynb](https://colab.research.google.com/github/voxel51/fiftyone/blob/v0.7.1.2/docs/source/tutorials/evaluate_detections.ipynb)\n\n[View Poll](https://www.reddit.com/poll/kvu4ly)", "link": "https://www.reddit.com/r/MachineLearning/comments/kvu4ly/d_how_many_of_you_use_python_scripts_versus/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] how many of you use python scripts versus notebooks? /!/ i'm curious how many people use python notebooks like jupyter or colab versus just writing python scripts when working on an ml project. i personally just use scripts at the moment but i'm interested in hearing some reasons why you prefer notebooks instead.\n\n&amp;#x200b;\n\nadditionally, i'm hoping to get some feedback on the notebook support recently added to the [open-source ml -----> tool !!! , fiftyone](https://voxel51.com/docs/fiftyone/), that i've been working on. fiftyone is a [python api](https://github.com/voxel51/fiftyone) \\+ app that lets you load and explore your image and video datasets and model predictions to debug your datasets and models. \n\nyou can now load the app in the output cell of a notebook to explore your dataset in the notebook itself, previously you had to launch it in a separate window. \n\nwhile other tools like tensorboard and matplotlib have notebook support, their output plots generally don't get updated by code further down in the notebook, like what might happen with fiftyone.\n\nsince there wasn't really a precedent to follow and i don't have much experience with notebook workflows, i was hoping to get some feedback here about how it could be improved. \n\nyou can try it out in colab here: [https://colab.research.google.com/github/voxel51/fiftyone/blob/v0.7.1.2/docs/source/tutorials/evaluate\\_detections.ipynb](https://colab.research.google.com/github/voxel51/fiftyone/blob/v0.7.1.2/docs/source/tutorials/evaluate_detections.ipynb)\n\n[view poll](https://www.reddit.com/poll/kvu4ly)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 18, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/kvu4ly/d_how_many_of_you_use_python_scripts_versus/',)", "identifyer": 5732234, "year": "2021"}, {"autor": "SirCortGodfrey", "date": 1622776735000, "content": "[D] PIFuHD for video instead of still image /!/  I've been really curious about using this tool for a dance-film project I'm working on, and have had success with it using single images. I have tried exporting a png sequence from my video file, but don't know if you can batch upload/export because I'm not very coding/tech savvy. So I was curious if anyone has used this successfully for video/obj sequences?", "link": "https://www.reddit.com/r/MachineLearning/comments/nrvcg9/d_pifuhd_for_video_instead_of_still_image/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] pifuhd for video instead of still image /!/  i've been really curious about using this -----> tool !!!  for a dance-film project i'm working on, and have had success with it using single images. i have tried exporting a png sequence from my video file, but don't know if you can batch upload/export because i'm not very coding/tech savvy. so i was curious if anyone has used this successfully for video/obj sequences?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/nrvcg9/d_pifuhd_for_video_instead_of_still_image/',)", "identifyer": 5732381, "year": "2021"}, {"autor": "NotGoodSocially", "date": 1622739137000, "content": "[D] Does anyone use Orange in their ML workflow? /!/ Orange seems to be default included in Anaconda and I've always wondered how useful it it. I've gone through a couple tutorials on how to use it, but i cant really find a use for it in my own work.\n\n&amp;#x200B;\n\nhas anyone here actually used it before, and if so do you find it useful and do you use it regularly?\n\nwhat tasks does it work well to streamline?\n\nand finally, once you've completed your analysis/testing on a ML design, what do you typically do after that for full implementation? (whats your process?)\n\n&amp;#x200B;\n\ni feel like it may actually be a really cool tool if i can just pend enough time to actually learn it fully, but am not sure if its really worth committing too as i haven't really heard anyone use it, and the online resources are very limited.", "link": "https://www.reddit.com/r/MachineLearning/comments/nri3p5/d_does_anyone_use_orange_in_their_ml_workflow/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] does anyone use orange in their ml workflow? /!/ orange seems to be default included in anaconda and i've always wondered how useful it it. i've gone through a couple tutorials on how to use it, but i cant really find a use for it in my own work.\n\n&amp;#x200b;\n\nhas anyone here actually used it before, and if so do you find it useful and do you use it regularly?\n\nwhat tasks does it work well to streamline?\n\nand finally, once you've completed your analysis/testing on a ml design, what do you typically do after that for full implementation? (whats your process?)\n\n&amp;#x200b;\n\ni feel like it may actually be a really cool -----> tool !!!  if i can just pend enough time to actually learn it fully, but am not sure if its really worth committing too as i haven't really heard anyone use it, and the online resources are very limited.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/nri3p5/d_does_anyone_use_orange_in_their_ml_workflow/',)", "identifyer": 5732416, "year": "2021"}, {"autor": "sdpmas", "date": 1630742274000, "content": "Open AI Codex/GitHub Copilot Python alternative for VS Code (a code generation + code search tool) [P]", "link": "https://www.reddit.com/r/MachineLearning/comments/phnetx/open_ai_codexgithub_copilot_python_alternative/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "open ai codex/github copilot python alternative for vs code (a code generation + code search -----> tool !!! ) [p]", "sortedWord": "None", "removed": "('reddit',)", "score": 1, "comments": 0, "media": "('link',)", "medialink": "('/r/Python/comments/phh7hx/open_ai_codexgithub_copilot_alternative_for_vs/',)", "identifyer": 5732517, "year": "2021"}, {"autor": "sdpmas", "date": 1630741987000, "content": "Open AI Codex/GitHub Copilot Python alternative for VS Code [a code generation + code search tool]", "link": "https://www.reddit.com/r/MachineLearning/comments/phncxl/open_ai_codexgithub_copilot_python_alternative/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "open ai codex/github copilot python alternative for vs code [a code generation + code search -----> tool !!! ]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('link',)", "medialink": "('/r/Python/comments/phh7hx/open_ai_codexgithub_copilot_alternative_for_vs/',)", "identifyer": 5732518, "year": "2021"}, {"autor": "Bzdeco", "date": 1630590955000, "content": "[D] Tool for visualizing and comparing custom ML/CV benchmarks /!/ Does anyone know a tool/platform/framework (could be opensource, self-hosted or not) that can be used for visualizing and comparing custom benchmarks of an ML/CV pipeline?\n\nSomething we're looking for is like a combination of a CI tool (such as Jenkins) and visualization tool (like WandB). Ideally one could construct \"test suits\" comprised of a set of benchmarks. Benchmarks would be implemented through custom code, e.g. implementing some model performance metrics. When creating the \"test suits\" one should be able to select for it the benchmarks fromm among the implemented ones. In the simplest scenario a single test suite is a single benchmark performed on some data, but combining several benchmarks together into one test suite would be valuable as well. \nSuch configured test suits should be possible to be triggered manually for a specific commit in a repository or triggered via some configurable git hooks (e.g. after pushing, submitting PR etc.). \n\nThe results produced by the given test suit should be possible to be compared with the other results, i.e. previous runs of the same test suit. The comparison could be simply some joint plot or other suitable visualization. Ideally, one could also drill down to single benchmarks and compare them -- even if the same benchmark was used in two different test suits, one should be able to compare between the results of these benchmarks.\n\nThank you in advance for your help or pinpointing us to the tools/combinations of them!", "link": "https://www.reddit.com/r/MachineLearning/comments/pgidnn/d_tool_for_visualizing_and_comparing_custom_mlcv/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] -----> tool !!!  for visualizing and comparing custom ml/cv benchmarks /!/ does anyone know a tool/platform/framework (could be opensource, self-hosted or not) that can be used for visualizing and comparing custom benchmarks of an ml/cv pipeline?\n\nsomething we're looking for is like a combination of a ci tool (such as jenkins) and visualization tool (like wandb). ideally one could construct \"test suits\" comprised of a set of benchmarks. benchmarks would be implemented through custom code, e.g. implementing some model performance metrics. when creating the \"test suits\" one should be able to select for it the benchmarks fromm among the implemented ones. in the simplest scenario a single test suite is a single benchmark performed on some data, but combining several benchmarks together into one test suite would be valuable as well. \nsuch configured test suits should be possible to be triggered manually for a specific commit in a repository or triggered via some configurable git hooks (e.g. after pushing, submitting pr etc.). \n\nthe results produced by the given test suit should be possible to be compared with the other results, i.e. previous runs of the same test suit. the comparison could be simply some joint plot or other suitable visualization. ideally, one could also drill down to single benchmarks and compare them -- even if the same benchmark was used in two different test suits, one should be able to compare between the results of these benchmarks.\n\nthank you in advance for your help or pinpointing us to the tools/combinations of them!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/pgidnn/d_tool_for_visualizing_and_comparing_custom_mlcv/',)", "identifyer": 5732600, "year": "2021"}, {"autor": "speedy0wl", "date": 1630506574000, "content": "[P] LabelFlow is live! The open image annotation and dataset cleaning platform /!/ Hi all! 4 months ago we announced Labelflow ([https://www.labelflow.ai/](https://www.labelflow.ai/)), the open image annotation and dataset cleaning platform.\n\n**What was then just a landing page is now a product** that you can try for free with no login required, the code is also publicly available on GitHub. ([https://github.com/Labelflow/labelflow/](https://github.com/Labelflow/labelflow/)).\n\nIn this first version, we are releasing your most wanted features: a straightforward online image annotation tool. For privacy concerns, your images are never uploaded to our server! You can create bounding boxes, polygons, export labels to COCO format and we added plenty of keyboard shortcuts for productivity!\n\nWe\u2019re excited to hear your feedback, tell us what features would make your life easier ([https://labelflow.canny.io/feature-requests](https://labelflow.canny.io/feature-requests))  and upvote what you would like us to build. Stay tuned, It\u2019s just the beginning of a long story.", "link": "https://www.reddit.com/r/MachineLearning/comments/pfv252/p_labelflow_is_live_the_open_image_annotation_and/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] labelflow is live! the open image annotation and dataset cleaning platform /!/ hi all! 4 months ago we announced labelflow ([https://www.labelflow.ai/](https://www.labelflow.ai/)), the open image annotation and dataset cleaning platform.\n\n**what was then just a landing page is now a product** that you can try for free with no login required, the code is also publicly available on github. ([https://github.com/labelflow/labelflow/](https://github.com/labelflow/labelflow/)).\n\nin this first version, we are releasing your most wanted features: a straightforward online image annotation -----> tool !!! . for privacy concerns, your images are never uploaded to our server! you can create bounding boxes, polygons, export labels to coco format and we added plenty of keyboard shortcuts for productivity!\n\nwe\u2019re excited to hear your feedback, tell us what features would make your life easier ([https://labelflow.canny.io/feature-requests](https://labelflow.canny.io/feature-requests))  and upvote what you would like us to build. stay tuned, it\u2019s just the beginning of a long story.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 32, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/pfv252/p_labelflow_is_live_the_open_image_annotation_and/',)", "identifyer": 5732638, "year": "2021"}, {"autor": "SeucheAchat9115", "date": 1623581637000, "content": "[D] Organizing the Work /!/ Maybe not specific to Machine Learning, but how do you organize your open packages/ to-dos? Which tool do you use for you personal and which for your team.", "link": "https://www.reddit.com/r/MachineLearning/comments/nytu3h/d_organizing_the_work/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] organizing the work /!/ maybe not specific to machine learning, but how do you organize your open packages/ to-dos? which -----> tool !!!  do you use for you personal and which for your team.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/nytu3h/d_organizing_the_work/',)", "identifyer": 5732699, "year": "2021"}, {"autor": "histoire_guy", "date": 1623366616000, "content": "[P] PixLab Annotate - Online Image Annotation, Labeling and Segmentation Tool /!/ Annotate is  A web based image annotation, labeling &amp; segmentation tool for Machine Learning model training tasks and beyond.\n\nhttps://annotate.pixlab.io/\n\n###Features Set:\n\n* Rectangle, Polygon, Zoom &amp; Drag labeling tool.\n* Consistent JSON output accepted by most Machine Learning frameworks.\n* Optimized for instance segmentation (Mask R-CNN, etc).\n* Client-side persistent storage - No data transfer involved.\n* Persistent &amp; easy label management (Create, Modify &amp; Delete).\n* Full screen display &amp; Snapshot capture.", "link": "https://www.reddit.com/r/MachineLearning/comments/nx1asa/p_pixlab_annotate_online_image_annotation/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] pixlab annotate - online image annotation, labeling and segmentation -----> tool !!!  /!/ annotate is  a web based image annotation, labeling &amp; segmentation -----> tool !!!  for machine learning model training tasks and beyond.\n\nhttps://annotate.pixlab.io/\n\n###features set:\n\n* rectangle, polygon, zoom &amp; drag labeling tool.\n* consistent json output accepted by most machine learning frameworks.\n* optimized for instance segmentation (mask r-cnn, etc).\n* client-side persistent storage - no data transfer involved.\n* persistent &amp; easy label management (create, modify &amp; delete).\n* full screen display &amp; snapshot capture.", "sortedWord": "None", "removed": "('nan',)", "score": 2, "comments": 2, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/nx1asa/p_pixlab_annotate_online_image_annotation/',)", "identifyer": 5732769, "year": "2021"}, {"autor": "[deleted]", "date": 1623365930000, "content": "[P] Annotate - Online Image Annotation, Labeling and Segmentation Tool for ML training tasks /!/ [deleted]", "link": "https://www.reddit.com/r/MachineLearning/comments/nx120h/p_annotate_online_image_annotation_labeling_and/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] annotate - online image annotation, labeling and segmentation -----> tool !!!  for ml training tasks /!/ [deleted]", "sortedWord": "None", "removed": "('deleted',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://annotate.pixlab.io/',)", "identifyer": 5732770, "year": "2021"}, {"autor": "redna11", "date": 1623333494000, "content": "[D] Who said AI Art has no soul? A summed up history of progress in the AI art world. /!/ When i started experimenting with A.I. art back in 2018, the compute power was barely enough to generate blobs. It wasn't too different for the art stars of those days who produced fairly appalling aesthetics ([Edmond de Belamy](https://en.wikipedia.org/wiki/Edmond_de_Belamy) anyone?). The issue with this is that it stuck in the art commentators mind as gimmicky and far from the real thing, the art created by trained artists. But we had to start somewhere.\n\nA lot of water has passed under the bridge since then and today it is difficult to tell the origin of an AI work of art. Was it created by a human, was it created by a machine?\n\nNext step in this movement is the merging of A.I. | Art | Blockchain and the delivery of A.I. art as NFTs.\n\nOne day my real hope is that (a few) human artists will draw what the A.I. has helped them imagine, it is an amazing tool to enhance imagination.  \n\nThe medium article illustrates those aspirations.", "link": "https://www.reddit.com/r/MachineLearning/comments/nwob3j/d_who_said_ai_art_has_no_soul_a_summed_up_history/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] who said ai art has no soul? a summed up history of progress in the ai art world. /!/ when i started experimenting with a.i. art back in 2018, the compute power was barely enough to generate blobs. it wasn't too different for the art stars of those days who produced fairly appalling aesthetics ([edmond de belamy](https://en.wikipedia.org/wiki/edmond_de_belamy) anyone?). the issue with this is that it stuck in the art commentators mind as gimmicky and far from the real thing, the art created by trained artists. but we had to start somewhere.\n\na lot of water has passed under the bridge since then and today it is difficult to tell the origin of an ai work of art. was it created by a human, was it created by a machine?\n\nnext step in this movement is the merging of a.i. | art | blockchain and the delivery of a.i. art as nfts.\n\none day my real hope is that (a few) human artists will draw what the a.i. has helped them imagine, it is an amazing -----> tool !!!  to enhance imagination.  \n\nthe medium article illustrates those aspirations.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 11, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/nwob3j/d_who_said_ai_art_has_no_soul_a_summed_up_history/',)", "identifyer": 5732819, "year": "2021"}, {"autor": "mlvpj", "date": 1610276994000, "content": "[P] LabML: Update - Monitor standard output from mobile phone /!/ Github: [https://github.com/lab-ml/labml](https://github.com/lab-ml/labml)\n\nMe and my friend added terminal output monitoring to LabML App. We are happy to share it here, and hope it helps. You can integrate LabML to your model training program with a couple of lines of code or you can run it through labml to monitor outputs from your phone. For example, `labml capture python train.py`.\n\nHere's a link to a sample training run: [https://web.lab-ml.com/stdout?uuid=39b03a1e454011ebbaff2b26e3148b3d](https://web.lab-ml.com/stdout?uuid=39b03a1e454011ebbaff2b26e3148b3d)\n\nIf you are not familiar with LabML, it's an open source tool to monitor model training progress (charts of loss and other metrics, gradients, etc). It works with PyTorch, TensorFlow and PyTorch lightening.", "link": "https://www.reddit.com/r/MachineLearning/comments/kucntv/p_labml_update_monitor_standard_output_from/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] labml: update - monitor standard output from mobile phone /!/ github: [https://github.com/lab-ml/labml](https://github.com/lab-ml/labml)\n\nme and my friend added terminal output monitoring to labml app. we are happy to share it here, and hope it helps. you can integrate labml to your model training program with a couple of lines of code or you can run it through labml to monitor outputs from your phone. for example, `labml capture python train.py`.\n\nhere's a link to a sample training run: [https://web.lab-ml.com/stdout?uuid=39b03a1e454011ebbaff2b26e3148b3d](https://web.lab-ml.com/stdout?uuid=39b03a1e454011ebbaff2b26e3148b3d)\n\nif you are not familiar with labml, it's an open source -----> tool !!!  to monitor model training progress (charts of loss and other metrics, gradients, etc). it works with pytorch, tensorflow and pytorch lightening.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/kucntv/p_labml_update_monitor_standard_output_from/',)", "identifyer": 5732971, "year": "2021"}, {"autor": "Islander_robotics", "date": 1624197479000, "content": "Easiest Way To Read A CSV In Python | Stock Analysis Tool", "link": "https://www.reddit.com/r/MachineLearning/comments/o4689t/easiest_way_to_read_a_csv_in_python_stock/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "easiest way to read a csv in python | stock analysis -----> tool !!! ", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('rich:video',)", "medialink": "('https://youtu.be/vJ46v1E8h9o',)", "identifyer": 5733344, "year": "2021"}, {"autor": "KeineAhnungDavonViel", "date": 1624179856000, "content": "[R] Looking for Data Labelling Tool for Volumetric Multifeature Datasets /!/  \n\nDear all,\n\nwe are currently working on 3D medical imagery with 6 layers of different information, a sample has a size of around 120 Gb.\n\nWe are looking for a way to label these conveniently according to the following criteria:\n\n\\- extraction of individual z-sections of the XYZ volume (we have this covered)\n\n\\- labelling of one of these z-sections with the ability to turn on- and off- the individual 6 layers while there is always a merged picture of all the activated layers visible\n\n\\- adjusting gamme &amp; histogram of these layers individually\n\nLater on we would like to do this with volumetric data the same way, labelling structures with a volume based on several z-sections of the volume.\n\nWe couldn't find a standard solution and are evaluating we should adjust one of the open source solutions out there.\n\nDoes anyone know if any other data labelling tool has some of the above mentioned features?", "link": "https://www.reddit.com/r/MachineLearning/comments/o41in6/r_looking_for_data_labelling_tool_for_volumetric/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[r] looking for data labelling -----> tool !!!  for volumetric multifeature datasets /!/  \n\ndear all,\n\nwe are currently working on 3d medical imagery with 6 layers of different information, a sample has a size of around 120 gb.\n\nwe are looking for a way to label these conveniently according to the following criteria:\n\n\\- extraction of individual z-sections of the xyz volume (we have this covered)\n\n\\- labelling of one of these z-sections with the ability to turn on- and off- the individual 6 layers while there is always a merged picture of all the activated layers visible\n\n\\- adjusting gamme &amp; histogram of these layers individually\n\nlater on we would like to do this with volumetric data the same way, labelling structures with a volume based on several z-sections of the volume.\n\nwe couldn't find a standard solution and are evaluating we should adjust one of the open source solutions out there.\n\ndoes anyone know if any other data labelling tool has some of the above mentioned features?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/o41in6/r_looking_for_data_labelling_tool_for_volumetric/',)", "identifyer": 5733362, "year": "2021"}, {"autor": "Professional_Slip599", "date": 1634945119000, "content": "[Discussion] Framerate interpolation with other, lower quality video, which has a higher framerate /!/ There are two same content videos. The catch is that one is low quality 60Fps, but the other one is high quality 30Fps. Is there a tool to interpolate the high quality to 60 fps that can be trained with the low quality video to reduce interpolation artifacts?", "link": "https://www.reddit.com/r/MachineLearning/comments/qdsycu/discussion_framerate_interpolation_with_other/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[discussion] framerate interpolation with other, lower quality video, which has a higher framerate /!/ there are two same content videos. the catch is that one is low quality 60fps, but the other one is high quality 30fps. is there a -----> tool !!!  to interpolate the high quality to 60 fps that can be trained with the low quality video to reduce interpolation artifacts?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/qdsycu/discussion_framerate_interpolation_with_other/',)", "identifyer": 5733440, "year": "2021"}, {"autor": "grib0ed0v", "date": 1634660035000, "content": "[R][P] OpenFL: An open-source framework for Federated Learning /!/ Federated learning (FL) is a computational paradigm that enables organisations to collaborate on machine learning (ML) projects without sharing sensitive data, such as, patient records, financial data, or classified secrets. Open Federated Learning (OpenFL) is an open-source framework for training ML algorithms using the data-private collaborative learning paradigm of FL. OpenFL works with training pipelines built with both TensorFlow and PyTorch, and can be easily extended to other ML and deep learning frameworks. Here, we summarise the motivation and development characteristics of OpenFL, with the intention of facilitating its application to existing ML model training in a production environment. Finally, we describe the first use of the OpenFL framework to train consensus ML models in a consortium of international healthcare organisations, as well as how it facilitates the first computational competition on FL.\n\nThe main goal of OpenFL framework is provide an easy-enough tool to deal with FL training jobs. To start working with OpenFL, you could use *pip*, *Docker*, or *source code* installation:\n\nPyPI:\n\n    pip install openfl\n\nDocker:\n\n    docker pull intel/openfl\n\nFrom source:\n\n    git clone https://github.com/intel/openfl.git\n    cd openfl\n    pip install -e .\n\nSome useful links can be found below:\n\nPaper: [https://arxiv.org/abs/2105.06413](https://arxiv.org/abs/2104.04767)\n\nCode: [https://github.com/intel/openfl](https://github.com/intel/openfl)\n\nDocs: [https://openfl.readthedocs.io/en/latest/](https://openfl.readthedocs.io/en/latest/)\n\nSlack: [https://join.slack.com/t/openfl/shared\\_invite/zt-ovzbohvn-T5fApk05\\~YS\\_iZhjJ5yaTw](https://join.slack.com/t/openfl/shared_invite/zt-ovzbohvn-T5fApk05~YS_iZhjJ5yaTw)", "link": "https://www.reddit.com/r/MachineLearning/comments/qbetp0/rp_openfl_an_opensource_framework_for_federated/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[r][p] openfl: an open-source framework for federated learning /!/ federated learning (fl) is a computational paradigm that enables organisations to collaborate on machine learning (ml) projects without sharing sensitive data, such as, patient records, financial data, or classified secrets. open federated learning (openfl) is an open-source framework for training ml algorithms using the data-private collaborative learning paradigm of fl. openfl works with training pipelines built with both tensorflow and pytorch, and can be easily extended to other ml and deep learning frameworks. here, we summarise the motivation and development characteristics of openfl, with the intention of facilitating its application to existing ml model training in a production environment. finally, we describe the first use of the openfl framework to train consensus ml models in a consortium of international healthcare organisations, as well as how it facilitates the first computational competition on fl.\n\nthe main goal of openfl framework is provide an easy-enough -----> tool !!!  to deal with fl training jobs. to start working with openfl, you could use *pip*, *docker*, or *source code* installation:\n\npypi:\n\n    pip install openfl\n\ndocker:\n\n    docker pull intel/openfl\n\nfrom source:\n\n    git clone https://github.com/intel/openfl.git\n    cd openfl\n    pip install -e .\n\nsome useful links can be found below:\n\npaper: [https://arxiv.org/abs/2105.06413](https://arxiv.org/abs/2104.04767)\n\ncode: [https://github.com/intel/openfl](https://github.com/intel/openfl)\n\ndocs: [https://openfl.readthedocs.io/en/latest/](https://openfl.readthedocs.io/en/latest/)\n\nslack: [https://join.slack.com/t/openfl/shared\\_invite/zt-ovzbohvn-t5fapk05\\~ys\\_izhjj5yatw](https://join.slack.com/t/openfl/shared_invite/zt-ovzbohvn-t5fapk05~ys_izhjj5yatw)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/qbetp0/rp_openfl_an_opensource_framework_for_federated/',)", "identifyer": 5733519, "year": "2021"}, {"autor": "RekhaKG", "date": 1634648056000, "content": "Open Interest Analysis Excel Tool", "link": "https://www.reddit.com/r/MachineLearning/comments/qbawlc/open_interest_analysis_excel_tool/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "open interest analysis excel -----> tool !!! ", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('rich:video',)", "medialink": "('https://www.youtube.com/watch?v=icx7hhvKD_Y',)", "identifyer": 5733532, "year": "2021"}, {"autor": "sonalg", "date": 1634409680000, "content": "[P] Demo of open Source Entity Resolution Tool using Active Learning", "link": "https://www.reddit.com/r/MachineLearning/comments/q9hwmy/p_demo_of_open_source_entity_resolution_tool/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] demo of open source entity resolution -----> tool !!!  using active learning", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('rich:video',)", "medialink": "('https://www.youtube.com/watch?v=zOabyZxN9b0&amp;t=0',)", "identifyer": 5733633, "year": "2021"}, {"autor": "sonalg", "date": 1634409606000, "content": "[P] Demo of Open Source Entity Resolution Tool Zingg Using Active Learning", "link": "https://www.reddit.com/r/MachineLearning/comments/q9hvr7/p_demo_of_open_source_entity_resolution_tool/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] demo of open source entity resolution -----> tool !!!  zingg using active learning", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('rich:video',)", "medialink": "('https://www.youtube.com/watch?v=zOabyZxN9b0',)", "identifyer": 5733634, "year": "2021"}, {"autor": "sonalg", "date": 1634408951000, "content": "Training an entity resolution tool using active learning", "link": "https://www.reddit.com/r/MachineLearning/comments/q9ho27/training_an_entity_resolution_tool_using_active/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "training an entity resolution -----> tool !!!  using active learning", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('rich:video',)", "medialink": "('https://www.youtube.com/watch?v=zOabyZxN9b0',)", "identifyer": 5733636, "year": "2021"}, {"autor": "dontiettt", "date": 1631119891000, "content": "[D] Why Hasn't FOSS Drag-and-Drop ML tools taken off yet? /!/ Currently, I am looking around for modules for [Knime](https://www.knime.com/) and [Orange](https://orangedatamining.com/) and looked at some of the modules, and realized that it does not have enough tools within their tool kit (e.g. text data analysis, network analysis, image classification).\n\nWhy hasn't someone bolt Keras on top of their system to supercharge it into being more powerful?", "link": "https://www.reddit.com/r/MachineLearning/comments/pkecte/d_why_hasnt_foss_draganddrop_ml_tools_taken_off/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] why hasn't foss drag-and-drop ml tools taken off yet? /!/ currently, i am looking around for modules for [knime](https://www.knime.com/) and [orange](https://orangedatamining.com/) and looked at some of the modules, and realized that it does not have enough tools within their -----> tool !!!  kit (e.g. text data analysis, network analysis, image classification).\n\nwhy hasn't someone bolt keras on top of their system to supercharge it into being more powerful?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 13, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/pkecte/d_why_hasnt_foss_draganddrop_ml_tools_taken_off/',)", "identifyer": 5734187, "year": "2021"}, {"autor": "ai-lover", "date": 1623960682000, "content": "[N] IBM Releases UQ360 AI tool, An Open Source Tool To Measure Model Uncertainty /!/ Deep learning-based Artificial Intelligence (AI) systems have a history of generating overconfident predictions, even when they are inaccurate, which can have significant repercussions. If a self-driving car firmly misidentifies the side of a tractor as a brightly illuminated sky and refuses to brake or alert the human driver, would you prefer to travel in that? I doubt it. Self-driving cars aren\u2019t the only issue. There is a slew of additional applications where AI\u2019s ability to convey doubt is essential. For example, if a chatbot is uncertain when a pharmacy shuts and gives a false answer, a patient may not receive the medicines they require.\n\nHere\u2019s where IBM\u2019s Uncertainty Quantification 360 (UQ360) comes in to rescue the day. UQ360 allows the AI to communicate its uncertainty, making it more intellectually humble and increasing the safety of its deployment. Its goal is to provide data scientists and developers with cutting-edge algorithms for quantifying, analyzing, enhancing, and exposing the uncertainty of machine learning models.\n\nArticle: [https://www.marktechpost.com/2021/06/17/ibm-releases-uq360-ai-tool-an-open-source-tool-to-measure-model-uncertainty/?\\_ga=2.217770833.636390090.1623335762-488125022.1618729090](https://www.marktechpost.com/2021/06/17/ibm-releases-uq360-ai-tool-an-open-source-tool-to-measure-model-uncertainty/?_ga=2.217770833.636390090.1623335762-488125022.1618729090) \n\nIBM Blog: [https://www.research.ibm.com/blog/uncertainty-quantification-360](https://www.research.ibm.com/blog/uncertainty-quantification-360)", "link": "https://www.reddit.com/r/MachineLearning/comments/o26ks9/n_ibm_releases_uq360_ai_tool_an_open_source_tool/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[n] ibm releases uq360 ai -----> tool !!! , an open source -----> tool !!!  to measure model uncertainty /!/ deep learning-based artificial intelligence (ai) systems have a history of generating overconfident predictions, even when they are inaccurate, which can have significant repercussions. if a self-driving car firmly misidentifies the side of a tractor as a brightly illuminated sky and refuses to brake or alert the human driver, would you prefer to travel in that? i doubt it. self-driving cars aren\u2019t the only issue. there is a slew of additional applications where ai\u2019s ability to convey doubt is essential. for example, if a chatbot is uncertain when a pharmacy shuts and gives a false answer, a patient may not receive the medicines they require.\n\nhere\u2019s where ibm\u2019s uncertainty quantification 360 (uq360) comes in to rescue the day. uq360 allows the ai to communicate its uncertainty, making it more intellectually humble and increasing the safety of its deployment. its goal is to provide data scientists and developers with cutting-edge algorithms for quantifying, analyzing, enhancing, and exposing the uncertainty of machine learning models.\n\narticle: [https://www.marktechpost.com/2021/06/17/ibm-releases-uq360-ai-tool-an-open-source-tool-to-measure-model-uncertainty/?\\_ga=2.217770833.636390090.1623335762-488125022.1618729090](https://www.marktechpost.com/2021/06/17/ibm-releases-uq360-ai-tool-an-open-source-tool-to-measure-model-uncertainty/?_ga=2.217770833.636390090.1623335762-488125022.1618729090) \n\nibm blog: [https://www.research.ibm.com/blog/uncertainty-quantification-360](https://www.research.ibm.com/blog/uncertainty-quantification-360)", "sortedWord": "None", "removed": "('nan',)", "score": 2, "comments": 1, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/o26ks9/n_ibm_releases_uq360_ai_tool_an_open_source_tool/',)", "identifyer": 5734302, "year": "2021"}, {"autor": "iketaco", "date": 1624580602000, "content": "[P] Content Generation /!/ I want to use artificial intelligence tools such as GPT3 or GPT2 and somehow use them to create articles based off of other articles I give it. For example, let\u2019s say I want to write an article about why trees are green. I would give it 5 to 10 articles about why trees are green and I would like the AI tool to create its own unique article about why trees are green based off of the given 5 to 10 articles.\n\nThe tools that are currently out there using GPT3 do not have this feature. Most of them have you just enter a keyword and limit the amount of information you can give the AI to start creating it\u2019s own article.  But most of the time these articles produce filler content and say inaccurate things. Is it even possible to do what I said above? If so, how would I approach this? Would I use open source AI tools like GPT2 or should I take a different approach to achieve this type of content that I\u2019m looking for?\n\nI tried searching up resources and tutorials on how to do this, but I did not find anything that would fit my needs. I think it\u2019s because I don\u2019t know the correct terminology or name of the training process that I\u2019m looking for. Any help or point in the right direction would be greatly appreciated! Thanks", "link": "https://www.reddit.com/r/MachineLearning/comments/o7cjzf/p_content_generation/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] content generation /!/ i want to use artificial intelligence tools such as gpt3 or gpt2 and somehow use them to create articles based off of other articles i give it. for example, let\u2019s say i want to write an article about why trees are green. i would give it 5 to 10 articles about why trees are green and i would like the ai -----> tool !!!  to create its own unique article about why trees are green based off of the given 5 to 10 articles.\n\nthe tools that are currently out there using gpt3 do not have this feature. most of them have you just enter a keyword and limit the amount of information you can give the ai to start creating it\u2019s own article.  but most of the time these articles produce filler content and say inaccurate things. is it even possible to do what i said above? if so, how would i approach this? would i use open source ai tools like gpt2 or should i take a different approach to achieve this type of content that i\u2019m looking for?\n\ni tried searching up resources and tutorials on how to do this, but i did not find anything that would fit my needs. i think it\u2019s because i don\u2019t know the correct terminology or name of the training process that i\u2019m looking for. any help or point in the right direction would be greatly appreciated! thanks", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/o7cjzf/p_content_generation/',)", "identifyer": 5734372, "year": "2021"}, {"autor": "enthusiast_bob", "date": 1633277766000, "content": "[P] SpotML - Managed ML Training on cheap AWS/GCP Spot Instances /!/ Hi there, \n\nWe built a tool called spotML to make training on AWS/GCP cheaper.\n\n[https://spotml.io/](https://spotml.io/)\n\nSpot Instances are 70% cheaper than On-Demand instances but are prone to interruptions. We mitigate the downside of these interruptions through the use of persistence features, including optional fallback to On-Demand instances. So you can optimize workflows according to your budget and time constraints.\n\nHistory: We were working on a neural rendering startup that needed a lot of GAN training which was getting very expensive. We were blowing roughly $1000, to train a single category class. Training on Spot instances was cheaper, but still a mess. It needed lot of hand holding/devops stuff to make it usable. So we built SpotML to automate a lot of things.\n\nPS: While the above startup failed, posting this here to see if the community finds this helpful :)", "link": "https://www.reddit.com/r/MachineLearning/comments/q0kypk/p_spotml_managed_ml_training_on_cheap_awsgcp_spot/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] spotml - managed ml training on cheap aws/gcp spot instances /!/ hi there, \n\nwe built a -----> tool !!!  called spotml to make training on aws/gcp cheaper.\n\n[https://spotml.io/](https://spotml.io/)\n\nspot instances are 70% cheaper than on-demand instances but are prone to interruptions. we mitigate the downside of these interruptions through the use of persistence features, including optional fallback to on-demand instances. so you can optimize workflows according to your budget and time constraints.\n\nhistory: we were working on a neural rendering startup that needed a lot of gan training which was getting very expensive. we were blowing roughly $1000, to train a single category class. training on spot instances was cheaper, but still a mess. it needed lot of hand holding/devops stuff to make it usable. so we built spotml to automate a lot of things.\n\nps: while the above startup failed, posting this here to see if the community finds this helpful :)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 60, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/q0kypk/p_spotml_managed_ml_training_on_cheap_awsgcp_spot/',)", "identifyer": 5734466, "year": "2021"}, {"autor": "kkwestside", "date": 1628741411000, "content": "What algorithm or ML tool should I use for monthly revenue production? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/p2sadw/what_algorithm_or_ml_tool_should_i_use_for/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what algorithm or ml -----> tool !!!  should i use for monthly revenue production? /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/p2sadw/what_algorithm_or_ml_tool_should_i_use_for/',)", "identifyer": 5734657, "year": "2021"}, {"autor": "techsucker", "date": 1628727044000, "content": "[N] TruEra Open Sources \u2018TruLens\u2019, A Cross-Framework Python Library For Deep Learning Explainability /!/ [TruEra](https://truera.com/), the provider of a suite of AI quality solutions, release \u2018[TruLens](https://www.trulens.org/)\u2018 \u2013 an open-source explainability tool for machine learning (ML) models based on neural networks.\n\nTruLens offers the ability to have constant explanations and insights into how deep neural network systems work by providing a uniform API for explaining TensorFlow, Pytorch or Keras-based model versions without having to switch between different libraries.\n\nQuick Read: [https://www.marktechpost.com/2021/08/11/truera-open-sources-trulens-a-cross-framework-python-library-for-deep-learning-explainability/](https://www.marktechpost.com/2021/08/11/truera-open-sources-trulens-a-cross-framework-python-library-for-deep-learning-explainability/) \n\nGithub: [https://github.com/truera/trulens](https://github.com/truera/trulens) \n\nInstallation: [https://pypi.org/project/trulens/](https://pypi.org/project/trulens/) \n\nPaper: [https://arxiv.org/abs/1802.03788](https://arxiv.org/abs/1802.03788)", "link": "https://www.reddit.com/r/MachineLearning/comments/p2pm4t/n_truera_open_sources_trulens_a_crossframework/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[n] truera open sources \u2018trulens\u2019, a cross-framework python library for deep learning explainability /!/ [truera](https://truera.com/), the provider of a suite of ai quality solutions, release \u2018[trulens](https://www.trulens.org/)\u2018 \u2013 an open-source explainability -----> tool !!!  for machine learning (ml) models based on neural networks.\n\ntrulens offers the ability to have constant explanations and insights into how deep neural network systems work by providing a uniform api for explaining tensorflow, pytorch or keras-based model versions without having to switch between different libraries.\n\nquick read: [https://www.marktechpost.com/2021/08/11/truera-open-sources-trulens-a-cross-framework-python-library-for-deep-learning-explainability/](https://www.marktechpost.com/2021/08/11/truera-open-sources-trulens-a-cross-framework-python-library-for-deep-learning-explainability/) \n\ngithub: [https://github.com/truera/trulens](https://github.com/truera/trulens) \n\ninstallation: [https://pypi.org/project/trulens/](https://pypi.org/project/trulens/) \n\npaper: [https://arxiv.org/abs/1802.03788](https://arxiv.org/abs/1802.03788)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/p2pm4t/n_truera_open_sources_trulens_a_crossframework/',)", "identifyer": 5734662, "year": "2021"}, {"autor": "northwolf56", "date": 1628692892000, "content": "[N] Visual Computation/Data Flow Tool for Python (and other languages) /!/ Hi,  \nI'm building an advanced graph-based computational tool and platform for doing, basically anything, but with emphasis on high-volume/throughput dataflow/transfer and AI model building.\n\nHave a look if interested! Early stages, but design is in a functional state right now.\n\n[https://github.com/radiantone/pyfi](https://github.com/radiantone/pyfi)", "link": "https://www.reddit.com/r/MachineLearning/comments/p2eezu/n_visual_computationdata_flow_tool_for_python_and/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[n] visual computation/data flow -----> tool !!!  for python (and other languages) /!/ hi,  \ni'm building an advanced graph-based computational tool and platform for doing, basically anything, but with emphasis on high-volume/throughput dataflow/transfer and ai model building.\n\nhave a look if interested! early stages, but design is in a functional state right now.\n\n[https://github.com/radiantone/pyfi](https://github.com/radiantone/pyfi)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/p2eezu/n_visual_computationdata_flow_tool_for_python_and/',)", "identifyer": 5734703, "year": "2021"}, {"autor": "northwolf56", "date": 1628692736000, "content": "Visual Computation/Data Flow Tool for Python (and other languages) /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/p2ed7c/visual_computationdata_flow_tool_for_python_and/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "visual computation/data flow -----> tool !!!  for python (and other languages) /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/p2ed7c/visual_computationdata_flow_tool_for_python_and/',)", "identifyer": 5734704, "year": "2021"}, {"autor": "vaaal88", "date": 1620755933000, "content": "[P] Unity Tool for generating a 2D images of 3D objects from several viewpoints /!/ Hi guys, for my research project I have created a Unity tool that allows the creation of image datasets by taking snapshots of 3D objects from a variety of viewpoints. You can change the latitude and logitude area, the distance from the object, the lightning, and some more things.\n\nSo today I have decided to clean it up, get rid of some unnecessary features, and write some documentation. Check it out and let me know what you think :)\n\n[ValerioB88/UnityML\\_DatasetGenerator (github.com)](https://github.com/ValerioB88/UnityML_DatasetGenerator)", "link": "https://www.reddit.com/r/MachineLearning/comments/na36i1/p_unity_tool_for_generating_a_2d_images_of_3d/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] unity -----> tool !!!  for generating a 2d images of 3d objects from several viewpoints /!/ hi guys, for my research project i have created a unity tool that allows the creation of image datasets by taking snapshots of 3d objects from a variety of viewpoints. you can change the latitude and logitude area, the distance from the object, the lightning, and some more things.\n\nso today i have decided to clean it up, get rid of some unnecessary features, and write some documentation. check it out and let me know what you think :)\n\n[valeriob88/unityml\\_datasetgenerator (github.com)](https://github.com/valeriob88/unityml_datasetgenerator)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/na36i1/p_unity_tool_for_generating_a_2d_images_of_3d/',)", "identifyer": 5734728, "year": "2021"}, {"autor": "vaaal88", "date": 1620755418000, "content": "Unity Tool to generate image Dataset from 3D objects from a variety of viewpoints /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/na2zd0/unity_tool_to_generate_image_dataset_from_3d/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "unity -----> tool !!!  to generate image dataset from 3d objects from a variety of viewpoints /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/na2zd0/unity_tool_to_generate_image_dataset_from_3d/',)", "identifyer": 5734729, "year": "2021"}, {"autor": "vaaal88", "date": 1620755083000, "content": "[P] Unity Tool to create 2D datasets from 3D objects", "link": "https://www.reddit.com/r/MachineLearning/comments/na2uj5/p_unity_tool_to_create_2d_datasets_from_3d_objects/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] unity -----> tool !!!  to create 2d datasets from 3d objects", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('link',)", "medialink": "('https://github.com/ValerioB88/UnityML_DatasetGenerator',)", "identifyer": 5734731, "year": "2021"}, {"autor": "sbb_ml", "date": 1613310555000, "content": "[R] Towards Algorithm Auditing: A Survey on Managing Legal, Ethical and Technological Risks of AI, ML and Associated Algorithms /!/ If any user is interested in our recent paper, please do not hesitate to send me a PM.\n\n[link to paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3778998)\n\nAbstract:\n\n&gt;Business reliance on algorithms are  becoming ubiquitous, and companies are increasingly concerned about  their algorithms causing major financial or reputational damage.  High-profile cases include VW\u2019s Dieselgate scandal with fines worth of  $34.69B, Knight Capital\u2019s bankruptcy (\\~$450M) by a glitch in its  algorithmic trading system, and Amazon\u2019s AI recruiting tool being  scrapped after showing bias against women. In response, governments are  legislating and imposing bans, regulators fining companies, and the  Judiciary discussing potentially making algorithms artificial \u201cpersons\u201d  in Law.   \n  \n&gt;  \n&gt;Soon there will be \u2018billions\u2019 of algorithms making  decisions with minimal human intervention; from autonomous vehicles and  finance, to medical treatment, employment, and legal decisions. Indeed,  scaling to problems beyond the human is a major point of using such  algorithms in the first place. As with Financial Audit, governments,  business and society will require Algorithm Audit; formal assurance that algorithms are legal, ethical and safe. A new industry is envisaged: Auditing  and Assurance of Algorithms (cf. Data privacy), with the remit to  professionalize and industrialize AI, ML and associated algorithms.   \n  \n&gt;  \n&gt;The  stakeholders range from those working on policy and regulation, to  industry practitioners and developers. We also anticipate the nature and  scope of the auditing  levels and framework presented will inform those interested in systems  of governance and compliance to regulation/standards. Our goal in this  paper is to survey the key areas necessary to perform auditing and assurance, and instigate the debate in this novel area of research and practice.  \n   \n&gt;  \n&gt;**Keywords:**  Robustness, Explainability, Privacy, Bias, Fairness, Transparency, Accountability, Governance, Compliance", "link": "https://www.reddit.com/r/MachineLearning/comments/ljop7y/r_towards_algorithm_auditing_a_survey_on_managing/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[r] towards algorithm auditing: a survey on managing legal, ethical and technological risks of ai, ml and associated algorithms /!/ if any user is interested in our recent paper, please do not hesitate to send me a pm.\n\n[link to paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3778998)\n\nabstract:\n\n&gt;business reliance on algorithms are  becoming ubiquitous, and companies are increasingly concerned about  their algorithms causing major financial or reputational damage.  high-profile cases include vw\u2019s dieselgate scandal with fines worth of  $34.69b, knight capital\u2019s bankruptcy (\\~$450m) by a glitch in its  algorithmic trading system, and amazon\u2019s ai recruiting -----> tool !!!  being  scrapped after showing bias against women. in response, governments are  legislating and imposing bans, regulators fining companies, and the  judiciary discussing potentially making algorithms artificial \u201cpersons\u201d  in law.   \n  \n&gt;  \n&gt;soon there will be \u2018billions\u2019 of algorithms making  decisions with minimal human intervention; from autonomous vehicles and  finance, to medical treatment, employment, and legal decisions. indeed,  scaling to problems beyond the human is a major point of using such  algorithms in the first place. as with financial audit, governments,  business and society will require algorithm audit; formal assurance that algorithms are legal, ethical and safe. a new industry is envisaged: auditing  and assurance of algorithms (cf. data privacy), with the remit to  professionalize and industrialize ai, ml and associated algorithms.   \n  \n&gt;  \n&gt;the  stakeholders range from those working on policy and regulation, to  industry practitioners and developers. we also anticipate the nature and  scope of the auditing  levels and framework presented will inform those interested in systems  of governance and compliance to regulation/standards. our goal in this  paper is to survey the key areas necessary to perform auditing and assurance, and instigate the debate in this novel area of research and practice.  \n   \n&gt;  \n&gt;**keywords:**  robustness, explainability, privacy, bias, fairness, transparency, accountability, governance, compliance", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/ljop7y/r_towards_algorithm_auditing_a_survey_on_managing/',)", "identifyer": 5734852, "year": "2021"}, {"autor": "Hot-Painter4924", "date": 1633438249000, "content": "[D] Precisely count the number of operations in Keras model /!/ I'm working in a tool to predict the inference speed of a model, based on input shape and model definition, i want to give the inference speed in different platforms. To this, i already measured the inference speed of a wide range of models in different platforms.\n\nI believe that this speed is a direct product of number of operations inside model (not only FLOPS), and i'm skeptical that even tf.profiler gives a very precise count of these operations.\n\n&amp;#x200B;\n\nEg: sigmoid is defined by:\n\n1/(1+exp(-x))\n\n&amp;#x200B;\n\nI dive deep into tensorflow to find this implementation and found out that actually it uses a wrapper of Eigen. At the end of the day, as there is no built in implementation of exponential function in c++, it is done by a Taylor series approximation, defined as:\n\n&amp;#x200B;\n\ne(x) = 1 + x + (x\\*x)/2 + (x\\*x\\*x)/6 + (x\\*x\\*x\\*x)/24 ...\n\n&amp;#x200B;\n\nThere is a number of other multiplications and sums that are not even implemented in tensorflow, so how could it count them?\n\nSo, how could i count all operations that a keras model do? can i rely on tf.profiler.ProfileOptionBuilder.float\\_operation() ?", "link": "https://www.reddit.com/r/MachineLearning/comments/q1vpud/d_precisely_count_the_number_of_operations_in/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] precisely count the number of operations in keras model /!/ i'm working in a -----> tool !!!  to predict the inference speed of a model, based on input shape and model definition, i want to give the inference speed in different platforms. to this, i already measured the inference speed of a wide range of models in different platforms.\n\ni believe that this speed is a direct product of number of operations inside model (not only flops), and i'm skeptical that even tf.profiler gives a very precise count of these operations.\n\n&amp;#x200b;\n\neg: sigmoid is defined by:\n\n1/(1+exp(-x))\n\n&amp;#x200b;\n\ni dive deep into tensorflow to find this implementation and found out that actually it uses a wrapper of eigen. at the end of the day, as there is no built in implementation of exponential function in c++, it is done by a taylor series approximation, defined as:\n\n&amp;#x200b;\n\ne(x) = 1 + x + (x\\*x)/2 + (x\\*x\\*x)/6 + (x\\*x\\*x\\*x)/24 ...\n\n&amp;#x200b;\n\nthere is a number of other multiplications and sums that are not even implemented in tensorflow, so how could it count them?\n\nso, how could i count all operations that a keras model do? can i rely on tf.profiler.profileoptionbuilder.float\\_operation() ?", "sortedWord": "None", "removed": "('nan',)", "score": 0, "comments": 4, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/q1vpud/d_precisely_count_the_number_of_operations_in/',)", "identifyer": 5735115, "year": "2021"}, {"autor": "onyx-zero-software", "date": 1633699363000, "content": "[D] Your Go-To Image Dataset Analysis Tools? /!/ Hey all, I'm just curious what tools you have come across for image data exploration? \n\nI have a fairly large dataset and had a (surprisingly) hard time finding a generic tool I could use to explore the data without rolling my own in a collection of notebooks. I found a few duplicate-image detectors, but for the most part none of them did what I needed. My gut feeling is that there is a very obvious tool and I just haven't heard of it before, so I wanted to ask the community. \n\nThings I would like to have from such a tool:\n\n- Duplicate image detection\n- PCA/LDA (or other methods of unsupervised feature analysis)\n- Anomaly detection\n- Clustering by image content and/or Metadata (k means or mean-shift)\n\nNone of these (other than maybe anomaly detection) seem like they'd be particularly novel and have been around for a long time, which is why I figured there must be something out there.\n\nThanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/q3xo2a/d_your_goto_image_dataset_analysis_tools/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] your go-to image dataset analysis tools? /!/ hey all, i'm just curious what tools you have come across for image data exploration? \n\ni have a fairly large dataset and had a (surprisingly) hard time finding a generic -----> tool !!!  i could use to explore the data without rolling my own in a collection of notebooks. i found a few duplicate-image detectors, but for the most part none of them did what i needed. my gut feeling is that there is a very obvious tool and i just haven't heard of it before, so i wanted to ask the community. \n\nthings i would like to have from such a tool:\n\n- duplicate image detection\n- pca/lda (or other methods of unsupervised feature analysis)\n- anomaly detection\n- clustering by image content and/or metadata (k means or mean-shift)\n\nnone of these (other than maybe anomaly detection) seem like they'd be particularly novel and have been around for a long time, which is why i figured there must be something out there.\n\nthanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 11, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/q3xo2a/d_your_goto_image_dataset_analysis_tools/',)", "identifyer": 5735220, "year": "2021"}, {"autor": "JKHighLight", "date": 1612100475000, "content": "Looking for simple machine learning tool /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/l9chps/looking_for_simple_machine_learning_tool/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "looking for simple machine learning -----> tool !!!  /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/l9chps/looking_for_simple_machine_learning_tool/',)", "identifyer": 5735270, "year": "2021"}, {"autor": "Bulky_Preparation_37", "date": 1627889975000, "content": "[P] Helix - a tool for digital immortality /!/ This past weekend, I built a tool that allows you to create a personal AI chatbot of yourself: [https://twitter.com/daraladje/status/1422079654370836482](https://twitter.com/daraladje/status/1422079654370836482)\n\nYou add content by answering questions or linking to writings across the internet. Anyone who has access to your chatbot can ask you questions about your life, experiences, knowledge, and values.\n\nHelix uses semantic search to return the most relevant document and then GPT3 to provide a conversational response.\n\nQuite powerful if you think about the long term implications of creating a digital twin of yourself that could last hundreds of years beyond your death", "link": "https://www.reddit.com/r/MachineLearning/comments/ow9tvl/p_helix_a_tool_for_digital_immortality/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] helix - a -----> tool !!!  for digital immortality /!/ this past weekend, i built a tool that allows you to create a personal ai chatbot of yourself: [https://twitter.com/daraladje/status/1422079654370836482](https://twitter.com/daraladje/status/1422079654370836482)\n\nyou add content by answering questions or linking to writings across the internet. anyone who has access to your chatbot can ask you questions about your life, experiences, knowledge, and values.\n\nhelix uses semantic search to return the most relevant document and then gpt3 to provide a conversational response.\n\nquite powerful if you think about the long term implications of creating a digital twin of yourself that could last hundreds of years beyond your death", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/ow9tvl/p_helix_a_tool_for_digital_immortality/',)", "identifyer": 5735375, "year": "2021"}, {"autor": "Bulky_Preparation_37", "date": 1627889840000, "content": "Helix - a tool for digital immortality /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/ow9t0f/helix_a_tool_for_digital_immortality/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "helix - a -----> tool !!!  for digital immortality /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/ow9t0f/helix_a_tool_for_digital_immortality/',)", "identifyer": 5735377, "year": "2021"}, {"autor": "Wendy-Machining", "date": 1620963457000, "content": "What a fast tool!", "link": "https://www.reddit.com/r/MachineLearning/comments/nbzfke/what_a_fast_tool/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "what a fast -----> tool !!! !", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('image',)", "medialink": "('https://i.redd.it/w6xaj7jya0z61.gif',)", "identifyer": 5735404, "year": "2021"}, {"autor": "keremidk0", "date": 1620916119000, "content": "Can you recommend a data annotation tool for ML ? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/nbhx2d/can_you_recommend_a_data_annotation_tool_for_ml/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "can you recommend a data annotation -----> tool !!!  for ml ? /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/nbhx2d/can_you_recommend_a_data_annotation_tool_for_ml/',)", "identifyer": 5735451, "year": "2021"}, {"autor": "Ordinary-Somewhere-2", "date": 1620893179000, "content": "[P] Can someone help me with this business project please?? It's really important and I would be really helpful to hear your opinion about it! /!/ Recommendation engines are present in all the spheres of our digital life. Predictive Analysis is used to make predictions about unknown future events. Ignoring the enormous data sets concerning customers preferences, transactions and actions would result in a significant loss for the company.\n\nThe Pricing recommendation engine is a set of smart algorithms depicting customers buying behavior and making a prediction about the future sales of a set of products. A new Pricing recommendation engine is to be integrated in China next year.\n\nPresent your approach/plan on how you would build &amp; integrate an impact analysis tool/process for the China roll out. Detail the approach, data science methodologies and steps needed to take to create a model which can analyze the business/financial impact that the price recommendation tool is expected to make in its new market.\n\n* Please prepare your own source data, company/product financials and reporting\n* What Methodologies/Metrics/KPI\u2019s would you use\n* Create a Proof of Concept (POC) on how you would assess the financial impact of the  \nnew Pricing recommendation tool", "link": "https://www.reddit.com/r/MachineLearning/comments/nbbi8p/p_can_someone_help_me_with_this_business_project/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] can someone help me with this business project please?? it's really important and i would be really helpful to hear your opinion about it! /!/ recommendation engines are present in all the spheres of our digital life. predictive analysis is used to make predictions about unknown future events. ignoring the enormous data sets concerning customers preferences, transactions and actions would result in a significant loss for the company.\n\nthe pricing recommendation engine is a set of smart algorithms depicting customers buying behavior and making a prediction about the future sales of a set of products. a new pricing recommendation engine is to be integrated in china next year.\n\npresent your approach/plan on how you would build &amp; integrate an impact analysis -----> tool !!! /process for the china roll out. detail the approach, data science methodologies and steps needed to take to create a model which can analyze the business/financial impact that the price recommendation tool is expected to make in its new market.\n\n* please prepare your own source data, company/product financials and reporting\n* what methodologies/metrics/kpi\u2019s would you use\n* create a proof of concept (poc) on how you would assess the financial impact of the  \nnew pricing recommendation tool", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/nbbi8p/p_can_someone_help_me_with_this_business_project/',)", "identifyer": 5735473, "year": "2021"}, {"autor": "Separate_Run2806", "date": 1620376282000, "content": "[D] Tool to help ML Engineering follow code best practices. /!/ Hello, my data science team was involved in an increasing amount of ML engineering tasks but had small knowledge on code quality best practices so we made this VS Code Extension to help python developers build unit tests quickly and efficiently. Our AI makes suggestions of input and then generate the test file for you. Would love to hear your feedback if you are willing to give a try to it. It's available here [www.ponicode.com](https://www.ponicode.com)\n\nAre you doing unit tests at all when implementing your models? Are you ever testing your code at the pre processing or evaluation stage?", "link": "https://www.reddit.com/r/MachineLearning/comments/n6trji/d_tool_to_help_ml_engineering_follow_code_best/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] -----> tool !!!  to help ml engineering follow code best practices. /!/ hello, my data science team was involved in an increasing amount of ml engineering tasks but had small knowledge on code quality best practices so we made this vs code extension to help python developers build unit tests quickly and efficiently. our ai makes suggestions of input and then generate the test file for you. would love to hear your feedback if you are willing to give a try to it. it's available here [www.ponicode.com](https://www.ponicode.com)\n\nare you doing unit tests at all when implementing your models? are you ever testing your code at the pre processing or evaluation stage?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 26, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/n6trji/d_tool_to_help_ml_engineering_follow_code_best/',)", "identifyer": 5735522, "year": "2021"}, {"autor": "Separate_Run2806", "date": 1620376029000, "content": "ML Engineering tool /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/n6tpjz/ml_engineering_tool/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "ml engineering -----> tool !!!  /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/n6tpjz/ml_engineering_tool/',)", "identifyer": 5735523, "year": "2021"}, {"autor": "sharvil", "date": 1620146526000, "content": "[P] ArxivDiff: view diffs of arXiv paper revisions /!/ I built a tool to show diffs between any two revisions of a paper on arXiv. Just take any arXiv URL and replace arxiv.org with arxivdiff.org, e.g. https://arxiv.org/abs/2009.09761 becomes https://arxivdiff.org/abs/2009.09761", "link": "https://www.reddit.com/r/MachineLearning/comments/n4sy6c/p_arxivdiff_view_diffs_of_arxiv_paper_revisions/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] arxivdiff: view diffs of arxiv paper revisions /!/ i built a -----> tool !!!  to show diffs between any two revisions of a paper on arxiv. just take any arxiv url and replace arxiv.org with arxivdiff.org, e.g. https://arxiv.org/abs/2009.09761 becomes https://arxivdiff.org/abs/2009.09761", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 9, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/n4sy6c/p_arxivdiff_view_diffs_of_arxiv_paper_revisions/',)", "identifyer": 5735612, "year": "2021"}, {"autor": "davidbun", "date": 1620139610000, "content": "[N] Access Google Objectron (~1.92 TBs) in less than 5 seconds with Activeloop Hub /!/ &amp;#x200B;\n\nhttps://i.redd.it/y52s5u8594x61.gif\n\nHi r/machinelearning,\n\nMy team at Activeloop partnered with Google to make Google Objectron available in under \u00b15 seconds (per dataset category). Google Objectron is one of Google\u2019s most popular datasets, containing short object centric video clips with pose annotations (15000 annotated videos and 4M annotated images). \n\nAll you need to do to get started is:\n\nInstall [Hub, the open-source package](https://github.com/activeloopai/Hub) that converts computer vision datasets into cloud-native NumPy-like arrays and enables a few nifty features like streaming to PyTorch and TensorFlow, dataset version-control, collaboration, etc.  \n\n`pip install hub`\n\nAnd then Load the data for the bike category.\n\n`import hub`\n\n`bikes = hub.Dataset(\"gcs://objectron_hub/bike\")`\n\nIn \u00b15 seconds, the dataset will be available to work on (e.g. filter, apply transformations, etc.). The whole dataset ( \\~1.92 TBs +metadata) would take about 33 seconds to access.\n\nThanks to Hub, you can visualize Google Objectron or any other computer vision dataset through our web app ([app.activeloop.ai](https://app.activeloop.ai/datasets/popular?tag=google%2Fshoe&amp;utm_source=social&amp;utm_medium=reddit&amp;utm_campaign=objectron)). \n\nMore details on using [Objectron with Hub are available in the release blogpost](https://www.activeloop.ai/blog/2021-05-03-accessing-google-objectron-data-in-less-than-5-seconds-?utm_source=social&amp;utm_medium=reddit&amp;utm_campaign=objectron). \n\nhttps://i.redd.it/e1179xa394x61.gif\n\nWe\u2019re working to get more datasets on the platform and improve [github.com/activeloopai/Hub](https://github.com/activeloopai/Hub) as a tool. Let us know if you have any feedback - we\u2019d like to deliver maximum value to the community.   \nThanks,  \nDavitBun", "link": "https://www.reddit.com/r/MachineLearning/comments/n4q4bp/n_access_google_objectron_192_tbs_in_less_than_5/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[n] access google objectron (~1.92 tbs) in less than 5 seconds with activeloop hub /!/ &amp;#x200b;\n\nhttps://i.redd.it/y52s5u8594x61.gif\n\nhi r/machinelearning,\n\nmy team at activeloop partnered with google to make google objectron available in under \u00b15 seconds (per dataset category). google objectron is one of google\u2019s most popular datasets, containing short object centric video clips with pose annotations (15000 annotated videos and 4m annotated images). \n\nall you need to do to get started is:\n\ninstall [hub, the open-source package](https://github.com/activeloopai/hub) that converts computer vision datasets into cloud-native numpy-like arrays and enables a few nifty features like streaming to pytorch and tensorflow, dataset version-control, collaboration, etc.  \n\n`pip install hub`\n\nand then load the data for the bike category.\n\n`import hub`\n\n`bikes = hub.dataset(\"gcs://objectron_hub/bike\")`\n\nin \u00b15 seconds, the dataset will be available to work on (e.g. filter, apply transformations, etc.). the whole dataset ( \\~1.92 tbs +metadata) would take about 33 seconds to access.\n\nthanks to hub, you can visualize google objectron or any other computer vision dataset through our web app ([app.activeloop.ai](https://app.activeloop.ai/datasets/popular?tag=google%2fshoe&amp;utm_source=social&amp;utm_medium=reddit&amp;utm_campaign=objectron)). \n\nmore details on using [objectron with hub are available in the release blogpost](https://www.activeloop.ai/blog/2021-05-03-accessing-google-objectron-data-in-less-than-5-seconds-?utm_source=social&amp;utm_medium=reddit&amp;utm_campaign=objectron). \n\nhttps://i.redd.it/e1179xa394x61.gif\n\nwe\u2019re working to get more datasets on the platform and improve [github.com/activeloopai/hub](https://github.com/activeloopai/hub) as a -----> tool !!! . let us know if you have any feedback - we\u2019d like to deliver maximum value to the community.   \nthanks,  \ndavitbun", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 37, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/n4q4bp/n_access_google_objectron_192_tbs_in_less_than_5/',)", "identifyer": 5735620, "year": "2021"}, {"autor": "davidbun", "date": 1620139472000, "content": "[N] Access Google Objectron (~1.92 TBs) in less than 5 seconds with Activeloop Hub /!/ Hi r/machinelearning,\n\nMy team at Activeloop partnered with Google to make Google Objectron available in under \u00b15 seconds (per dataset category). Google Objectron is one of Google\u2019s most popular datasets, containing short object centric video clips with pose annotations (15000 annotated videos and 4M annotated images). \n\nAll you need to do to get started is:\n\nInstall [Hub, the open-source package](https://github.com/activeloopai/Hub) that converts computer vision datasets into cloud-native NumPy-like arrays and enables a few nifty features like streaming to PyTorch and TensorFlow, dataset version-control, collaboration, etc.  \n\n`pip install hub`\n\nAnd then Load the data for the bike category.\n\n`import hub`\n\n`bikes = hub.Dataset(\"gcs://objectron_hub/bike\")`\n\nIn \u00b15 seconds, the dataset will be available to work on (e.g. filter, apply transformations, etc.). The whole dataset ( \\~1.92 TBs +metadata) would take about 33 seconds to access.\n\nThanks to Hub, you can visualize Google Objectron or any other computer vision dataset through our web app ([app.activeloop.ai](https://app.activeloop.ai/datasets/popular?tag=google%2Fshoe&amp;utm_source=social&amp;utm_medium=reddit&amp;utm_campaign=objectron)). \n\nMore details on using [Objectron with Hub are available in the release blogpost](https://www.activeloop.ai/blog/2021-05-03-accessing-google-objectron-data-in-less-than-5-seconds-?utm_source=social&amp;utm_medium=reddit&amp;utm_campaign=objectron). \n\n[Google Objectron within app.activeloop.ai](https://i.redd.it/5jmfdpwn84x61.gif)\n\nWe\u2019re working to get more datasets on the platform and improve [github.com/activeloopai/Hub](https://github.com/activeloopai/Hub) as a tool. Let us know if you have any feedback - we\u2019d like to deliver maximum value to the community.   \nThanks,  \nDavitBun", "link": "https://www.reddit.com/r/MachineLearning/comments/n4q2by/n_access_google_objectron_192_tbs_in_less_than_5/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[n] access google objectron (~1.92 tbs) in less than 5 seconds with activeloop hub /!/ hi r/machinelearning,\n\nmy team at activeloop partnered with google to make google objectron available in under \u00b15 seconds (per dataset category). google objectron is one of google\u2019s most popular datasets, containing short object centric video clips with pose annotations (15000 annotated videos and 4m annotated images). \n\nall you need to do to get started is:\n\ninstall [hub, the open-source package](https://github.com/activeloopai/hub) that converts computer vision datasets into cloud-native numpy-like arrays and enables a few nifty features like streaming to pytorch and tensorflow, dataset version-control, collaboration, etc.  \n\n`pip install hub`\n\nand then load the data for the bike category.\n\n`import hub`\n\n`bikes = hub.dataset(\"gcs://objectron_hub/bike\")`\n\nin \u00b15 seconds, the dataset will be available to work on (e.g. filter, apply transformations, etc.). the whole dataset ( \\~1.92 tbs +metadata) would take about 33 seconds to access.\n\nthanks to hub, you can visualize google objectron or any other computer vision dataset through our web app ([app.activeloop.ai](https://app.activeloop.ai/datasets/popular?tag=google%2fshoe&amp;utm_source=social&amp;utm_medium=reddit&amp;utm_campaign=objectron)). \n\nmore details on using [objectron with hub are available in the release blogpost](https://www.activeloop.ai/blog/2021-05-03-accessing-google-objectron-data-in-less-than-5-seconds-?utm_source=social&amp;utm_medium=reddit&amp;utm_campaign=objectron). \n\n[google objectron within app.activeloop.ai](https://i.redd.it/5jmfdpwn84x61.gif)\n\nwe\u2019re working to get more datasets on the platform and improve [github.com/activeloopai/hub](https://github.com/activeloopai/hub) as a -----> tool !!! . let us know if you have any feedback - we\u2019d like to deliver maximum value to the community.   \nthanks,  \ndavitbun", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/n4q2by/n_access_google_objectron_192_tbs_in_less_than_5/',)", "identifyer": 5735622, "year": "2021"}, {"autor": "_conquistador", "date": 1620076306000, "content": "[P] Hi r/machinelearning! We created an AI-assisted video annotation tool that speeds up labelling time by 17x - looking for BETA testers to help us refine web application /!/ As part of our ML postgrad, we developed a tool for our lab that speeds up video labelling times by 15-20x using few-shot classification / human-in-the-loop input (e.g., a person labels a few frames and the algorithm handles the rest).\n\nWe\u2019re looking for beta testers to help refine the platform to additional real-world use-cases. Besides early access to the platform, we\u2019ll give any early testers free lifetime access once we launch.\n\nPlease fill out this form to get early access: [https://forms.gle/CGWd29xNv24Kwm1Y6](https://forms.gle/CGWd29xNv24Kwm1Y6)", "link": "https://www.reddit.com/r/MachineLearning/comments/n47gyy/p_hi_rmachinelearning_we_created_an_aiassisted/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] hi r/machinelearning! we created an ai-assisted video annotation -----> tool !!!  that speeds up labelling time by 17x - looking for beta testers to help us refine web application /!/ as part of our ml postgrad, we developed a tool for our lab that speeds up video labelling times by 15-20x using few-shot classification / human-in-the-loop input (e.g., a person labels a few frames and the algorithm handles the rest).\n\nwe\u2019re looking for beta testers to help refine the platform to additional real-world use-cases. besides early access to the platform, we\u2019ll give any early testers free lifetime access once we launch.\n\nplease fill out this form to get early access: [https://forms.gle/cgwd29xnv24kwm1y6](https://forms.gle/cgwd29xnv24kwm1y6)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/n47gyy/p_hi_rmachinelearning_we_created_an_aiassisted/',)", "identifyer": 5735666, "year": "2021"}, {"autor": "_conquistador", "date": 1620074643000, "content": "Hi r/machinelearning! We created an AI-assisted video annotation tool that speeds up labelling time by 17x - looking for BETA testers to help us refine web application /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/n46t33/hi_rmachinelearning_we_created_an_aiassisted/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "hi r/machinelearning! we created an ai-assisted video annotation -----> tool !!!  that speeds up labelling time by 17x - looking for beta testers to help us refine web application /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/n46t33/hi_rmachinelearning_we_created_an_aiassisted/',)", "identifyer": 5735667, "year": "2021"}, {"autor": "matt_paraphrasetool", "date": 1632448391000, "content": "I made this paraphrase tool with 15 free modes using Google's T5 and Distilbert classifiers to separate the modes. It can elaborate, simplify, summarize, and do much more. Link: paraphrasetool.com Would love feedback :)", "link": "https://www.reddit.com/r/MachineLearning/comments/pu9tvf/i_made_this_paraphrase_tool_with_15_free_modes/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "i made this paraphrase -----> tool !!!  with 15 free modes using google's t5 and distilbert classifiers to separate the modes. it can elaborate, simplify, summarize, and do much more. link: paraphrasetool.com would love feedback :)", "sortedWord": "None", "removed": "('reddit',)", "score": 1, "comments": 2, "media": "('image',)", "medialink": "('https://i.redd.it/wwrx5n5jxcp71.png',)", "identifyer": 5735790, "year": "2021"}, {"autor": "carlml", "date": 1632443397000, "content": "[D] Tool for annotating and seeing other annotations of papers in the cloud /!/ I remember someone posted a website to access papers where you could annotate and read other people's annotations. I couldn't find it, unfortunately. Does anyone have that link? Thanks", "link": "https://www.reddit.com/r/MachineLearning/comments/pu8gz0/d_tool_for_annotating_and_seeing_other/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] -----> tool !!!  for annotating and seeing other annotations of papers in the cloud /!/ i remember someone posted a website to access papers where you could annotate and read other people's annotations. i couldn't find it, unfortunately. does anyone have that link? thanks", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/pu8gz0/d_tool_for_annotating_and_seeing_other/',)", "identifyer": 5735796, "year": "2021"}, {"autor": "Throwaway00000000028", "date": 1632440937000, "content": "[D] How do you ensure reproducibility? /!/  So my main issue here is that I'll train a model but then later add some additional feature along the way. So when I go back to evaluate that model the source code is now different and I have to revert my changes or implement a workaround.\n\nWhat I've done before is automatically copy the source code into the run folder so I can always reproduce it and see what changes I made. But obviously this is not ideal.\n\nI guess one of the ML tool providers (wandb, etc) probably have a solution for this. Does any one have personal experience? Thanks in advance", "link": "https://www.reddit.com/r/MachineLearning/comments/pu7rxp/d_how_do_you_ensure_reproducibility/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] how do you ensure reproducibility? /!/  so my main issue here is that i'll train a model but then later add some additional feature along the way. so when i go back to evaluate that model the source code is now different and i have to revert my changes or implement a workaround.\n\nwhat i've done before is automatically copy the source code into the run folder so i can always reproduce it and see what changes i made. but obviously this is not ideal.\n\ni guess one of the ml -----> tool !!!  providers (wandb, etc) probably have a solution for this. does any one have personal experience? thanks in advance", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 30, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/pu7rxp/d_how_do_you_ensure_reproducibility/',)", "identifyer": 5735799, "year": "2021"}, {"autor": "techsucker", "date": 1631849580000, "content": "[R] Israeli Researchers Unveil DeepSIM, a Neural Generative Model for Conditional Image Manipulation Based on a Single Image /!/ In recent years, deep neural networks have been proven effective at performing image manipulation tasks for which large training datasets are available such as, mapping facial landmarks to facial images. When dealing with a unique image, finding suitable training data that includes many samples of the same input-output pairing is often difficult. In some cases, when you use a large dataset to create your model, it may lead to unwanted outputs that do not preserve the specific characteristics of what was desired.\u00a0\n\nGenerative models like the ones used in neural networks can be trained to generate new images based on just one input. This exciting research direction holds the potential for these techniques to extend beyond basic image manipulation methods and create more unique art styles or designs with endless possibilities. Researchers at [The Hebrew University of Jerusalem have developed a new method, called \u2018DeepSIM,\u2019](https://arxiv.org/pdf/2109.06151.pdf) for training deep conditional generative models from just one image pair. The DeepSIM method is an incredibly powerful tool that can solve various image manipulation tasks, including shape warping, object rearrangement, and removal of objects; addition or creation of new ones. It also allows for painting/photorealistic animated clips to be created quickly.\n\n# [5 Min Read](https://www.marktechpost.com/2021/09/16/israeli-researchers-unveil-deepsim-a-neural-generative-model-for-conditional-image-manipulation-based-on-a-single-image/) | [Paper](https://arxiv.org/pdf/2109.06151.pdf) | [Project](http://www.vision.huji.ac.il/deepsim/) | [Code](https://github.com/eliahuhorwitz/DeepSIM)\n\n&amp;#x200B;\n\n![video](ntztn2i3hzn71)", "link": "https://www.reddit.com/r/MachineLearning/comments/ppsh12/r_israeli_researchers_unveil_deepsim_a_neural/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[r] israeli researchers unveil deepsim, a neural generative model for conditional image manipulation based on a single image /!/ in recent years, deep neural networks have been proven effective at performing image manipulation tasks for which large training datasets are available such as, mapping facial landmarks to facial images. when dealing with a unique image, finding suitable training data that includes many samples of the same input-output pairing is often difficult. in some cases, when you use a large dataset to create your model, it may lead to unwanted outputs that do not preserve the specific characteristics of what was desired.\u00a0\n\ngenerative models like the ones used in neural networks can be trained to generate new images based on just one input. this exciting research direction holds the potential for these techniques to extend beyond basic image manipulation methods and create more unique art styles or designs with endless possibilities. researchers at [the hebrew university of jerusalem have developed a new method, called \u2018deepsim,\u2019](https://arxiv.org/pdf/2109.06151.pdf) for training deep conditional generative models from just one image pair. the deepsim method is an incredibly powerful -----> tool !!!  that can solve various image manipulation tasks, including shape warping, object rearrangement, and removal of objects; addition or creation of new ones. it also allows for painting/photorealistic animated clips to be created quickly.\n\n# [5 min read](https://www.marktechpost.com/2021/09/16/israeli-researchers-unveil-deepsim-a-neural-generative-model-for-conditional-image-manipulation-based-on-a-single-image/) | [paper](https://arxiv.org/pdf/2109.06151.pdf) | [project](http://www.vision.huji.ac.il/deepsim/) | [code](https://github.com/eliahuhorwitz/deepsim)\n\n&amp;#x200b;\n\n![video](ntztn2i3hzn71)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 11, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/ppsh12/r_israeli_researchers_unveil_deepsim_a_neural/',)", "identifyer": 5736019, "year": "2021"}, {"autor": "mKaloer", "date": 1631828008000, "content": "[P] TFServingCache: A distributed LRU-cache for serving TensorFlow models in Kubernetes /!/ The tools for deployment of machine learning models keep getting better. However, I\u2019ve found that most tools are lacking when it comes to serving a large number of models, e.g. a model per user. So I\u2019ve built my own tool, called TFServingCache, which automatically loads and unloads models in a distributed cluster of TensorFlow Serving instances.\n\nI\u2019ve written a post about the project on medium: [TFServingCache: A distributed LRU-cache for serving TensorFlow models in Kubernetes](https://link.medium.com/lEeZEBkMBjb).\n\nThe project is written in Go, and open source on GitHub: [TFServingCache](https://github.com/mKaloer/TFServingCache). Give it a try!", "link": "https://www.reddit.com/r/MachineLearning/comments/ppmn86/p_tfservingcache_a_distributed_lrucache_for/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] tfservingcache: a distributed lru-cache for serving tensorflow models in kubernetes /!/ the tools for deployment of machine learning models keep getting better. however, i\u2019ve found that most tools are lacking when it comes to serving a large number of models, e.g. a model per user. so i\u2019ve built my own -----> tool !!! , called tfservingcache, which automatically loads and unloads models in a distributed cluster of tensorflow serving instances.\n\ni\u2019ve written a post about the project on medium: [tfservingcache: a distributed lru-cache for serving tensorflow models in kubernetes](https://link.medium.com/leezebkmbjb).\n\nthe project is written in go, and open source on github: [tfservingcache](https://github.com/mkaloer/tfservingcache). give it a try!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/ppmn86/p_tfservingcache_a_distributed_lrucache_for/',)", "identifyer": 5736027, "year": "2021"}, {"autor": "nyquist_karma", "date": 1631791023000, "content": "Background removal too strong when using U2Net [Project] /!/ Hello, I am successfully using [U2Net](https://github.com/xuebinqin/U-2-Net) to **remove** the **background** of images using the terminal and I am also using the nice interface of [this](https://github.com/OPHoperHPO/image-background-remove-tool) repo to do the same thing just in an easier way and **validate** the similarity of the results. However, my issue is that the background removal is too strong for images like this:\n\n[Original Image](https://i.stack.imgur.com/heoNw.png)\n\nWhere I get the following result (i.e. ***packaging*** ***is*** ***also*** ***removed***): [After U2Net Image](https://i.stack.imgur.com/cPxyQ.png)\n\nIf I upload the image in [Foco clipping website](https://www.fococlipping.com/) and I select **Type=='Graphic,** I get **exactly the same results**. That means that the website is using the **same or similar** algorithm to remove the background for Graphic-type images. Nevertheless, if I select **Type=='Product**, then the result is the following and **is exactly what I want**:\n\n[Desired result which is obtained from Foco website](https://i.stack.imgur.com/PM8ji.png)", "link": "https://www.reddit.com/r/MachineLearning/comments/ppbaxg/background_removal_too_strong_when_using_u2net/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "background removal too strong when using u2net [project] /!/ hello, i am successfully using [u2net](https://github.com/xuebinqin/u-2-net) to **remove** the **background** of images using the terminal and i am also using the nice interface of [this](https://github.com/ophoperhpo/image-background-remove------> tool !!! ) repo to do the same thing just in an easier way and **validate** the similarity of the results. however, my issue is that the background removal is too strong for images like this:\n\n[original image](https://i.stack.imgur.com/heonw.png)\n\nwhere i get the following result (i.e. ***packaging*** ***is*** ***also*** ***removed***): [after u2net image](https://i.stack.imgur.com/cpxyq.png)\n\nif i upload the image in [foco clipping website](https://www.fococlipping.com/) and i select **type=='graphic,** i get **exactly the same results**. that means that the website is using the **same or similar** algorithm to remove the background for graphic-type images. nevertheless, if i select **type=='product**, then the result is the following and **is exactly what i want**:\n\n[desired result which is obtained from foco website](https://i.stack.imgur.com/pm8ji.png)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/ppbaxg/background_removal_too_strong_when_using_u2net/',)", "identifyer": 5736061, "year": "2021"}, {"autor": "RileyStang", "date": 1632099154000, "content": "[Project] Looking for help with TensorflowJS based midi pattern generation project /!/  \n\nFirstly, here's a demo video of the original software generating midi drum patterns based on input from another midi or audio file: [https://youtu.be/eYUaYzfZUCo](https://youtu.be/eYUaYzfZUCo)\n\nTo further explain what is happening: A Google research team had several professional drummers come in to play on electronic drum kits that turn their performances into midi files. They then trained a neural network with Tensorflow on over 14 hours of drum midis played by professional drummers. This is what creates such profound results as seen in the above video. And I have used this myself and found that if you repeatedly give it the same input, it will give nearly the same output, only with very slight variations, as I would expect from a real drummer that is improvising.\n\nFor further details on this project from the Google research team that started it, go here: [https://magenta.tensorflow.org/studio-announce](https://magenta.tensorflow.org/studio-announce)\n\nMy overall goal is to create a fork of this software that has Drumify with a model that is trained on my data-set of midi drum files. So far the furthest I've gotten is a MatMul error, after following one of the repo contributor's instructions that he gave me. It was not without many other issues, like being unable to rebuild natively on Windows, and having to do it through the Windows Subsystem for Linux to get further. I will link here to the issue that has more detailed information. This seems like an old, seemingly abandoned project, and the people who worked on it are probably busy trying to attend to their job responsibilities so they can stay financially afloat. I just figured I would try this avenue for more assistance since I see a lot of potential in this project when artists can control the output of the plugins through training their models with the data of their choice.\n\nI'm here to have a tool that is useful to other musicians to streamline their creative workflow, without stifling their style of expression. So, anyone that helps, their contribution will be forever made known to the world.\n\nHere is the link to the issue I've created on the repo for more detailed information: [https://github.com/magenta/magenta-studio/issues/54](https://github.com/magenta/magenta-studio/issues/54)\n\nThank you for taking the time to read!", "link": "https://www.reddit.com/r/MachineLearning/comments/prkbfq/project_looking_for_help_with_tensorflowjs_based/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[project] looking for help with tensorflowjs based midi pattern generation project /!/  \n\nfirstly, here's a demo video of the original software generating midi drum patterns based on input from another midi or audio file: [https://youtu.be/eyuayzfzuco](https://youtu.be/eyuayzfzuco)\n\nto further explain what is happening: a google research team had several professional drummers come in to play on electronic drum kits that turn their performances into midi files. they then trained a neural network with tensorflow on over 14 hours of drum midis played by professional drummers. this is what creates such profound results as seen in the above video. and i have used this myself and found that if you repeatedly give it the same input, it will give nearly the same output, only with very slight variations, as i would expect from a real drummer that is improvising.\n\nfor further details on this project from the google research team that started it, go here: [https://magenta.tensorflow.org/studio-announce](https://magenta.tensorflow.org/studio-announce)\n\nmy overall goal is to create a fork of this software that has drumify with a model that is trained on my data-set of midi drum files. so far the furthest i've gotten is a matmul error, after following one of the repo contributor's instructions that he gave me. it was not without many other issues, like being unable to rebuild natively on windows, and having to do it through the windows subsystem for linux to get further. i will link here to the issue that has more detailed information. this seems like an old, seemingly abandoned project, and the people who worked on it are probably busy trying to attend to their job responsibilities so they can stay financially afloat. i just figured i would try this avenue for more assistance since i see a lot of potential in this project when artists can control the output of the plugins through training their models with the data of their choice.\n\ni'm here to have a -----> tool !!!  that is useful to other musicians to streamline their creative workflow, without stifling their style of expression. so, anyone that helps, their contribution will be forever made known to the world.\n\nhere is the link to the issue i've created on the repo for more detailed information: [https://github.com/magenta/magenta-studio/issues/54](https://github.com/magenta/magenta-studio/issues/54)\n\nthank you for taking the time to read!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/prkbfq/project_looking_for_help_with_tensorflowjs_based/',)", "identifyer": 5736096, "year": "2021"}, {"autor": "Ruurd2020", "date": 1631978935000, "content": "Why isn't there a tool for my groseries /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/pqoht7/why_isnt_there_a_tool_for_my_groseries/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "why isn't there a -----> tool !!!  for my groseries /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/pqoht7/why_isnt_there_a_tool_for_my_groseries/',)", "identifyer": 5736161, "year": "2021"}, {"autor": "alvisanovari", "date": 1609756878000, "content": "We Built A Tool To Get The TLDR Of Any YouTube Video In Seconds /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/kq6are/we_built_a_tool_to_get_the_tldr_of_any_youtube/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "we built a -----> tool !!!  to get the tldr of any youtube video in seconds /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/kq6are/we_built_a_tool_to_get_the_tldr_of_any_youtube/',)", "identifyer": 5736220, "year": "2021"}, {"autor": "chaachans", "date": 1615615191000, "content": "[N]New AI tool detects Deepfakes by analyzing light reflections in the eyes /!/ **University at Buffalo deepfake spotting tool proves 94% effective with portrait-like photos, according to study.**\n\n[https://www.futurspot.com/how-spot-deepfakes-look-light-reflection-eyes/](https://www.futurspot.com/how-spot-deepfakes-look-light-reflection-eyes/)", "link": "https://www.reddit.com/r/MachineLearning/comments/m40dpm/nnew_ai_tool_detects_deepfakes_by_analyzing_light/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[n]new ai -----> tool !!!  detects deepfakes by analyzing light reflections in the eyes /!/ **university at buffalo deepfake spotting tool proves 94% effective with portrait-like photos, according to study.**\n\n[https://www.futurspot.com/how-spot-deepfakes-look-light-reflection-eyes/](https://www.futurspot.com/how-spot-deepfakes-look-light-reflection-eyes/)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/m40dpm/nnew_ai_tool_detects_deepfakes_by_analyzing_light/',)", "identifyer": 5736413, "year": "2021"}, {"autor": "lkhphuc", "date": 1615567407000, "content": "[P] Pytorch Lightning + Hydra + Tensorboard project template with best/good software practices. /!/ Link: [https://github.com/lkhphuc/lightning-hydra-template](https://github.com/lkhphuc/lightning-hydra-template) \n\nHi all,\n\nI recently created a Deep Learning template on Github for project that use Pytorch Lightning, Hydra and Tensorboard. It includes a brief section describe how all these tools are combined together, with some tips and tricks for best practices in running, debugging and organizing experiments. It mainly for my own PhD use, thus aim toward small-scale, research project. \n\nCheck it out here and let me know what you think? What did I miss and what could be improved?   \n\nI prefer this combination over other huge ML-pipeline frameworks (MLFlow, DeterminedAI, ...) for my research project since they are relative simple and powerful, with clear and well-defined scope.\n\n* Lightning provides training functionalities.\n* Hydra manages configuration and command line launcher\n* Tensorboard is simple, open-source yet powerful visualization tool.\n\nThough each of them is very simple and easy to use, to get them run together the way I want still need some boilerplate. I create this template because I'm tired of searching and go back to copy from old projects (and I'm procrastinating too, somebody helps me :(). \n\nI hope it will be useful for some of you.", "link": "https://www.reddit.com/r/MachineLearning/comments/m3li7t/p_pytorch_lightning_hydra_tensorboard_project/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] pytorch lightning + hydra + tensorboard project template with best/good software practices. /!/ link: [https://github.com/lkhphuc/lightning-hydra-template](https://github.com/lkhphuc/lightning-hydra-template) \n\nhi all,\n\ni recently created a deep learning template on github for project that use pytorch lightning, hydra and tensorboard. it includes a brief section describe how all these tools are combined together, with some tips and tricks for best practices in running, debugging and organizing experiments. it mainly for my own phd use, thus aim toward small-scale, research project. \n\ncheck it out here and let me know what you think? what did i miss and what could be improved?   \n\ni prefer this combination over other huge ml-pipeline frameworks (mlflow, determinedai, ...) for my research project since they are relative simple and powerful, with clear and well-defined scope.\n\n* lightning provides training functionalities.\n* hydra manages configuration and command line launcher\n* tensorboard is simple, open-source yet powerful visualization -----> tool !!! .\n\nthough each of them is very simple and easy to use, to get them run together the way i want still need some boilerplate. i create this template because i'm tired of searching and go back to copy from old projects (and i'm procrastinating too, somebody helps me :(). \n\ni hope it will be useful for some of you.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/m3li7t/p_pytorch_lightning_hydra_tensorboard_project/',)", "identifyer": 5736456, "year": "2021"}, {"autor": "tuliosarmento", "date": 1615566674000, "content": "[D] Interpreting log loss in a specific scenario /!/ Hi! I'm studying data science and got a project here as a homework. I made a logistic regression model to predict some rare events (3% or less). This event is labeled as my \"1\". Then, instead of getting an ideal model with perfect balance sensitivity and specificity, I did what every shithead would do and focused on creating a model who would predict the \"0\" or non-event with an \"marvellous\" precision, ignoring all the false positives. (To do that, I filtered all the scenarios where the probability of 0 would be grater than a high value).\nResult: I got a specificity of 99.95% (yaaay!) and a sensitivity of 2% or something like that. \nBut now, I'm looking for the probabilities for each scenario being 0 or 1 (I'm using python and the predict_proba() method, if it's a useful info).\nI would like to assess these probabilities and I was told the log loss would be the ideal tool. I did it for the model and got a really shitty value (0.98 or something like that). This got me thinking: Is this value specific for the probabilities of \"1\" and, therefore, I could use the (1 - 0.98) to assess the probabilities of non-event (\"0\")? I guess probably not but that's my doubt.\n\nSo in cases like this, where just specificity matters (and it shows a semi-perfect value), how should I assess the probabilities (outputs of the model) individually?\n\nThanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/m3l8p4/d_interpreting_log_loss_in_a_specific_scenario/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] interpreting log loss in a specific scenario /!/ hi! i'm studying data science and got a project here as a homework. i made a logistic regression model to predict some rare events (3% or less). this event is labeled as my \"1\". then, instead of getting an ideal model with perfect balance sensitivity and specificity, i did what every shithead would do and focused on creating a model who would predict the \"0\" or non-event with an \"marvellous\" precision, ignoring all the false positives. (to do that, i filtered all the scenarios where the probability of 0 would be grater than a high value).\nresult: i got a specificity of 99.95% (yaaay!) and a sensitivity of 2% or something like that. \nbut now, i'm looking for the probabilities for each scenario being 0 or 1 (i'm using python and the predict_proba() method, if it's a useful info).\ni would like to assess these probabilities and i was told the log loss would be the ideal -----> tool !!! . i did it for the model and got a really shitty value (0.98 or something like that). this got me thinking: is this value specific for the probabilities of \"1\" and, therefore, i could use the (1 - 0.98) to assess the probabilities of non-event (\"0\")? i guess probably not but that's my doubt.\n\nso in cases like this, where just specificity matters (and it shows a semi-perfect value), how should i assess the probabilities (outputs of the model) individually?\n\nthanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/m3l8p4/d_interpreting_log_loss_in_a_specific_scenario/',)", "identifyer": 5736459, "year": "2021"}, {"autor": "Price-Down", "date": 1617389879000, "content": "[D] Looking for baseline functional LSTM structure /!/ More  specifically though, I just want something that can predict the next  element in a sequence of those elements. Not stock prediction or  something more specialized. just a tool to identify and exemplify  ineffective \"randomness\" within a broad sequence of simple digits.\n\nDoes anyone have something to offer?", "link": "https://www.reddit.com/r/MachineLearning/comments/miqzcp/d_looking_for_baseline_functional_lstm_structure/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] looking for baseline functional lstm structure /!/ more  specifically though, i just want something that can predict the next  element in a sequence of those elements. not stock prediction or  something more specialized. just a -----> tool !!!  to identify and exemplify  ineffective \"randomness\" within a broad sequence of simple digits.\n\ndoes anyone have something to offer?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/miqzcp/d_looking_for_baseline_functional_lstm_structure/',)", "identifyer": 5736596, "year": "2021"}, {"autor": "elbogotazo", "date": 1617384423000, "content": "Discovering column mappings [R] /!/ I have a challenge to work on at work and am trying to figure out the approach. We have an internal system that stores transactional data in a tabular form.\n\nWe receive daily files with data from the same domain (transactions + metadata) but the column names are not standardised, and the data fields are not always the exact same (e.g. The amount field may have 3 digits behind the comma, where our system expects 1 digit or what our system calls \"amount\" might be called \"quantity1\" in the incoming files etc.. ). There will be multiple amount, date, free text and categorical fields and there will be relationships between those fields. Note that the source data comes from external parties and I have no control over the format of the incoming files. \n\nWe have a manual mapping and transformation defined for each incoming file, but the volume of different formats and sources is ever increasing. Im looking for a way to take any input file and to train a model that predicts for each column what the most likely corresponding column in the target file is.\n\nI've been looking into a few things : using NLP\\spacy to train a model that recognises patterns in the column data. E.g. Numeric + period + comma is likely to correspond to amount. I've also looked at modeling the data and extracting an RDF representation using a open source tool called Karma to see if I can train a model on a network graph. But really struggling to see how to implement this. Regex would only get us part of the way there and I'm really trying to see if there is a scalable way to implement this. \n\nIs anyone aware of the formal name of this type of problem and if there are tried and tested approaches\\implementations out there that I could build upon?", "link": "https://www.reddit.com/r/MachineLearning/comments/mip21w/discovering_column_mappings_r/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "discovering column mappings [r] /!/ i have a challenge to work on at work and am trying to figure out the approach. we have an internal system that stores transactional data in a tabular form.\n\nwe receive daily files with data from the same domain (transactions + metadata) but the column names are not standardised, and the data fields are not always the exact same (e.g. the amount field may have 3 digits behind the comma, where our system expects 1 digit or what our system calls \"amount\" might be called \"quantity1\" in the incoming files etc.. ). there will be multiple amount, date, free text and categorical fields and there will be relationships between those fields. note that the source data comes from external parties and i have no control over the format of the incoming files. \n\nwe have a manual mapping and transformation defined for each incoming file, but the volume of different formats and sources is ever increasing. im looking for a way to take any input file and to train a model that predicts for each column what the most likely corresponding column in the target file is.\n\ni've been looking into a few things : using nlp\\spacy to train a model that recognises patterns in the column data. e.g. numeric + period + comma is likely to correspond to amount. i've also looked at modeling the data and extracting an rdf representation using a open source -----> tool !!!  called karma to see if i can train a model on a network graph. but really struggling to see how to implement this. regex would only get us part of the way there and i'm really trying to see if there is a scalable way to implement this. \n\nis anyone aware of the formal name of this type of problem and if there are tried and tested approaches\\implementations out there that i could build upon?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/mip21w/discovering_column_mappings_r/',)", "identifyer": 5736603, "year": "2021"}, {"autor": "techsucker", "date": 1617380494000, "content": "[N] Introducing PyTorch Profiler \u2013 The New And Improved Performance Debugging Profiler For PyTorch /!/ The analysis and refinement of the large-scale deep learning model\u2019s performance is a constant challenge that increases in importance with the model\u2019s size. Owing to a lack of available resources, PyTorch users had a hard time overcoming this problem. There were common GPU hardware-level debugging tools, but PyTorch-specific background of operations was not available. Users had to merge multi-tools or apply minimal correlation information manually to make sense of the data to retrieve the missing information.\n\nThe PyTorch Profiler came to the rescue, an open-source tool for precise, efficient, and troubleshooting performance investigations of large-scale deep learning models.\u00a0\n\nSummary: [https://www.marktechpost.com/2021/04/02/introducing-pytorch-profiler-the-new-and-improved-performance-debugging-profiler-for-pytorch/](https://www.marktechpost.com/2021/04/02/introducing-pytorch-profiler-the-new-and-improved-performance-debugging-profiler-for-pytorch/)\n\nSource: https://pytorch.org/blog/introducing-pytorch-profiler-the-new-and-improved-performance-tool/", "link": "https://www.reddit.com/r/MachineLearning/comments/minp9l/n_introducing_pytorch_profiler_the_new_and/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[n] introducing pytorch profiler \u2013 the new and improved performance debugging profiler for pytorch /!/ the analysis and refinement of the large-scale deep learning model\u2019s performance is a constant challenge that increases in importance with the model\u2019s size. owing to a lack of available resources, pytorch users had a hard time overcoming this problem. there were common gpu hardware-level debugging tools, but pytorch-specific background of operations was not available. users had to merge multi-tools or apply minimal correlation information manually to make sense of the data to retrieve the missing information.\n\nthe pytorch profiler came to the rescue, an open-source -----> tool !!!  for precise, efficient, and troubleshooting performance investigations of large-scale deep learning models.\u00a0\n\nsummary: [https://www.marktechpost.com/2021/04/02/introducing-pytorch-profiler-the-new-and-improved-performance-debugging-profiler-for-pytorch/](https://www.marktechpost.com/2021/04/02/introducing-pytorch-profiler-the-new-and-improved-performance-debugging-profiler-for-pytorch/)\n\nsource: https://pytorch.org/blog/introducing-pytorch-profiler-the-new-and-improved-performance-tool/", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 19, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/minp9l/n_introducing_pytorch_profiler_the_new_and/',)", "identifyer": 5736608, "year": "2021"}, {"autor": "legoonest", "date": 1617351719000, "content": "[P] VinDr Lab - an open-source annotation platform for Medical AI /!/ Department of Medical Imaging, VinBigdata has decided to release our DICOM annotation tool into the open-source. It's called **VinDr Lab**. It is a web-based tool that allows multiple annotators to work at the same time and remotely. This is the software that we've used to build the dataset for our [Kaggle competition](https://www.kaggle.com/c/vinbigdata-chest-xray-abnormalities-detection/). Following the releasing of the large-scale dataset VinDr-CXR, this is our next contribution in data sharing as well as tools for AI development. We encourage the community to promote the data sharing and tools to drive AI research and development.  \nHope that somebody finds it useful. Enjoy and send your feedback! The tool is publicly available at  \n[https://github.com/vinbigdata-medical/vindr-lab](https://github.com/vinbigdata-medical/vindr-lab)\n\n[VinDr Lab DICOM Viewer](https://preview.redd.it/gye835b7vpq61.png?width=3836&amp;format=png&amp;auto=webp&amp;s=2e1e425f996fc8fd747f09b9a3adba8a1948c6ef)", "link": "https://www.reddit.com/r/MachineLearning/comments/mifyne/p_vindr_lab_an_opensource_annotation_platform_for/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] vindr lab - an open-source annotation platform for medical ai /!/ department of medical imaging, vinbigdata has decided to release our dicom annotation -----> tool !!!  into the open-source. it's called **vindr lab**. it is a web-based tool that allows multiple annotators to work at the same time and remotely. this is the software that we've used to build the dataset for our [kaggle competition](https://www.kaggle.com/c/vinbigdata-chest-xray-abnormalities-detection/). following the releasing of the large-scale dataset vindr-cxr, this is our next contribution in data sharing as well as tools for ai development. we encourage the community to promote the data sharing and tools to drive ai research and development.  \nhope that somebody finds it useful. enjoy and send your feedback! the tool is publicly available at  \n[https://github.com/vinbigdata-medical/vindr-lab](https://github.com/vinbigdata-medical/vindr-lab)\n\n[vindr lab dicom viewer](https://preview.redd.it/gye835b7vpq61.png?width=3836&amp;format=png&amp;auto=webp&amp;s=2e1e425f996fc8fd747f09b9a3adba8a1948c6ef)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 16, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/mifyne/p_vindr_lab_an_opensource_annotation_platform_for/',)", "identifyer": 5736631, "year": "2021"}, {"autor": "1000_island_stare", "date": 1613856168000, "content": "[D] Converting StyleGAN2-ADA weights to StyleGAN2 format? /!/ Maybe too specific of an inquiry, but does anyone know of a tool that will convert weights created from NVIDIA's Pytorch implementation of StyleGAN2-ADA ([https://github.com/NVlabs/stylegan2-ada-pytorch](https://github.com/NVlabs/stylegan2-ada-pytorch)) to StyleGAN2 (without ADA)  weight architecture  ([https://github.com/NVlabs/stylegan2](https://github.com/NVlabs/stylegan2) or similar). Is anyone familiar with such a tool?", "link": "https://www.reddit.com/r/MachineLearning/comments/loh2fd/d_converting_stylegan2ada_weights_to_stylegan2/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] converting stylegan2-ada weights to stylegan2 format? /!/ maybe too specific of an inquiry, but does anyone know of a -----> tool !!!  that will convert weights created from nvidia's pytorch implementation of stylegan2-ada ([https://github.com/nvlabs/stylegan2-ada-pytorch](https://github.com/nvlabs/stylegan2-ada-pytorch)) to stylegan2 (without ada)  weight architecture  ([https://github.com/nvlabs/stylegan2](https://github.com/nvlabs/stylegan2) or similar). is anyone familiar with such a tool?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/loh2fd/d_converting_stylegan2ada_weights_to_stylegan2/',)", "identifyer": 5736664, "year": "2021"}, {"autor": "techsucker", "date": 1613843923000, "content": "[R] Georgia Tech and Facebook AI Researchers Devise a New Tensor Train Approach to Reduce the Size of Deep Learning Recommendation Models up to 112x /!/ A recent study conducted jointly by the Georgia Institute and Facebook AI researchers has opened the door to a new method called TT-Rec (Tensor-Train for DLRM). If employed successfully, this method would be a leap forward in the arena of deep learning as it will significantly reduce the size of the Deep Learning Recommendation Models (DLRM) and make the process of their deployment uncomplicated. The driving force behind the reduction of the model\u2019s size will be the replacement of the large embedding tables in DLRM with a sequence of matrix products that would be developed by making use of the tensor train decomposition. It is a tool structured to efficiently work with tensors using a generalization of the low-rank decomposition.\u00a0  \nFull Paper Summary: [https://www.marktechpost.com/2021/02/20/georgia-tech-and-facebook-ai-researchers-devise-a-new-tensor-train-approach-to-reduce-the-size-of-deep-learning-recommendation-models-up-to-112x/](https://www.marktechpost.com/2021/02/20/georgia-tech-and-facebook-ai-researchers-devise-a-new-tensor-train-approach-to-reduce-the-size-of-deep-learning-recommendation-models-up-to-112x/)   \nPaper: [https://arxiv.org/pdf/2101.11714.pdf](https://arxiv.org/pdf/2101.11714.pdf)   \nGithub: [https://github.com/facebookresearch/FBTT-Embedding](https://github.com/facebookresearch/FBTT-Embedding)", "link": "https://www.reddit.com/r/MachineLearning/comments/locmen/r_georgia_tech_and_facebook_ai_researchers_devise/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[r] georgia tech and facebook ai researchers devise a new tensor train approach to reduce the size of deep learning recommendation models up to 112x /!/ a recent study conducted jointly by the georgia institute and facebook ai researchers has opened the door to a new method called tt-rec (tensor-train for dlrm). if employed successfully, this method would be a leap forward in the arena of deep learning as it will significantly reduce the size of the deep learning recommendation models (dlrm) and make the process of their deployment uncomplicated. the driving force behind the reduction of the model\u2019s size will be the replacement of the large embedding tables in dlrm with a sequence of matrix products that would be developed by making use of the tensor train decomposition. it is a -----> tool !!!  structured to efficiently work with tensors using a generalization of the low-rank decomposition.\u00a0  \nfull paper summary: [https://www.marktechpost.com/2021/02/20/georgia-tech-and-facebook-ai-researchers-devise-a-new-tensor-train-approach-to-reduce-the-size-of-deep-learning-recommendation-models-up-to-112x/](https://www.marktechpost.com/2021/02/20/georgia-tech-and-facebook-ai-researchers-devise-a-new-tensor-train-approach-to-reduce-the-size-of-deep-learning-recommendation-models-up-to-112x/)   \npaper: [https://arxiv.org/pdf/2101.11714.pdf](https://arxiv.org/pdf/2101.11714.pdf)   \ngithub: [https://github.com/facebookresearch/fbtt-embedding](https://github.com/facebookresearch/fbtt-embedding)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/locmen/r_georgia_tech_and_facebook_ai_researchers_devise/',)", "identifyer": 5736685, "year": "2021"}, {"autor": "Valuable-Offer-2243", "date": 1613829876000, "content": "18.ai - A deep learning text-to-speech tool for generating natural high-quality voices of characters with minimal data (MIT) /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/lo7rqb/18ai_a_deep_learning_texttospeech_tool_for/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "18.ai - a deep learning text-to-speech -----> tool !!!  for generating natural high-quality voices of characters with minimal data (mit) /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/lo7rqb/18ai_a_deep_learning_texttospeech_tool_for/',)", "identifyer": 5736703, "year": "2021"}, {"autor": "lamaai_io", "date": 1614042034000, "content": "[News] AI 360: 22/02/21. This week in AI: Superhuman performance on ATARI; and the world's largest Turing test /!/ Read the full article and watch the video on our website: [https://www.lamaai.io/posts/ai-360-22-02-2021-superhuman-performance-on-atari-the-worlds-largest-turing-test](https://www.lamaai.io/posts/ai-360-22-02-2021-superhuman-performance-on-atari-the-worlds-largest-turing-test)\n\nKey points from this week in AI.\n\n* [GoogleAI](https://research.google/) announce [DreamerV2](https://ai.googleblog.com/2021/02/mastering-atari-with-discrete-world.html), a reinforcement learning algorithm which has achieved superhuman performance\n* [HuggingFace](https://huggingface.co/) announce a zero-shot classification tool, a new translation model and an update to [Datasets](https://huggingface.co/datasets)\n* [PapersWithCode](https://paperswithcode.com/) achieve 1000 ML methods, release PapersWithLibraries and announce a research challenge\n* Google announce [BIG-Bench](http://github.com/google/BIG-Bench) \\- a collaborative benchmark for large language models\n* [AiCore](https://theaicore.com/) are running the worlds largest Turing test (sign-ups still open!)", "link": "https://www.reddit.com/r/MachineLearning/comments/lq547b/news_ai_360_220221_this_week_in_ai_superhuman/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[news] ai 360: 22/02/21. this week in ai: superhuman performance on atari; and the world's largest turing test /!/ read the full article and watch the video on our website: [https://www.lamaai.io/posts/ai-360-22-02-2021-superhuman-performance-on-atari-the-worlds-largest-turing-test](https://www.lamaai.io/posts/ai-360-22-02-2021-superhuman-performance-on-atari-the-worlds-largest-turing-test)\n\nkey points from this week in ai.\n\n* [googleai](https://research.google/) announce [dreamerv2](https://ai.googleblog.com/2021/02/mastering-atari-with-discrete-world.html), a reinforcement learning algorithm which has achieved superhuman performance\n* [huggingface](https://huggingface.co/) announce a zero-shot classification -----> tool !!! , a new translation model and an update to [datasets](https://huggingface.co/datasets)\n* [paperswithcode](https://paperswithcode.com/) achieve 1000 ml methods, release paperswithlibraries and announce a research challenge\n* google announce [big-bench](http://github.com/google/big-bench) \\- a collaborative benchmark for large language models\n* [aicore](https://theaicore.com/) are running the worlds largest turing test (sign-ups still open!)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/lq547b/news_ai_360_220221_this_week_in_ai_superhuman/',)", "identifyer": 5736797, "year": "2021"}, {"autor": "Flewizzle", "date": 1618845326000, "content": "[P] Suggestions on projects I could do to bring value to a digital marketing agency to show my unique worth as the only data analyst/ software developer in their company /!/ I would be coming in as a data analyst / software developer with an interest in marketing through running an ecommerce business in my spare time, I will be the first DA/SD in the agency (about 100 people there, big clients, access to lots of data). My contact at the company who I\u2019ve worked with for 9 months has asked me what I envision myself working on if I was to join, ready for an interview later this week.\n\nits an open ended question and to have the ability to decide is great, I\u2019m aware of the trap of technology first approaches instead of problem first approaches so don\u2019t want to fall into that, but I want to be able to justify my DA/SD skills by bringing something to the table that can\u2019t be done in google analytics,\n\nSo far I have created a causal inference modelling tool and a data driven attribution modelling tool for them(done for the company as a university project). I\u2019m currently looking into cohort analysis but can see that that is possible in google analytics. I\u2019m wondering if any DAs / SDs can offer some inspiration on what you are currently doing in work that I could also suggest.\n\nDon\u2019t worry I\u2019m also doing a ton of research myself, just putting as many rods in the water as possible to yield the best results ready for the interview \ud83d\ude0a Any help would be super appreciated!", "link": "https://www.reddit.com/r/MachineLearning/comments/mu2t3d/p_suggestions_on_projects_i_could_do_to_bring/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] suggestions on projects i could do to bring value to a digital marketing agency to show my unique worth as the only data analyst/ software developer in their company /!/ i would be coming in as a data analyst / software developer with an interest in marketing through running an ecommerce business in my spare time, i will be the first da/sd in the agency (about 100 people there, big clients, access to lots of data). my contact at the company who i\u2019ve worked with for 9 months has asked me what i envision myself working on if i was to join, ready for an interview later this week.\n\nits an open ended question and to have the ability to decide is great, i\u2019m aware of the trap of technology first approaches instead of problem first approaches so don\u2019t want to fall into that, but i want to be able to justify my da/sd skills by bringing something to the table that can\u2019t be done in google analytics,\n\nso far i have created a causal inference modelling -----> tool !!!  and a data driven attribution modelling -----> tool !!!  for them(done for the company as a university project). i\u2019m currently looking into cohort analysis but can see that that is possible in google analytics. i\u2019m wondering if any das / sds can offer some inspiration on what you are currently doing in work that i could also suggest.\n\ndon\u2019t worry i\u2019m also doing a ton of research myself, just putting as many rods in the water as possible to yield the best results ready for the interview \ud83d\ude0a any help would be super appreciated!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/mu2t3d/p_suggestions_on_projects_i_could_do_to_bring/',)", "identifyer": 5736910, "year": "2021"}, {"autor": "Philipp", "date": 1618835247000, "content": "[P] [D] Using GPT-3 to write short stories /!/ Hopfully this is of interest to some, I created an ongoing [series](https://aiwrotethis.substack.com/) of short stories on Substack which were co-written wih the GPT-3 AI. Here's [a video](https://www.youtube.com/watch?v=8fWK0k7aSRs) showing some approaches on how to move a story in a certain direction.\n\nWould also love to discuss this topic of AI fiction writing. I'm utterly fascinated by the use of GTP-3 as a creative tool.", "link": "https://www.reddit.com/r/MachineLearning/comments/mtzhxp/p_d_using_gpt3_to_write_short_stories/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] [d] using gpt-3 to write short stories /!/ hopfully this is of interest to some, i created an ongoing [series](https://aiwrotethis.substack.com/) of short stories on substack which were co-written wih the gpt-3 ai. here's [a video](https://www.youtube.com/watch?v=8fwk0k7asrs) showing some approaches on how to move a story in a certain direction.\n\nwould also love to discuss this topic of ai fiction writing. i'm utterly fascinated by the use of gtp-3 as a creative -----> tool !!! .", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 16, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/mtzhxp/p_d_using_gpt3_to_write_short_stories/',)", "identifyer": 5736918, "year": "2021"}, {"autor": "xela-sedinnaoi", "date": 1618815073000, "content": "[P] [D] The benefits of training the simplest model you can think of and deploying it to production, as soon as you can. /!/ I\u2019ve had many successes with this approach. With this in mind, I\u2019ve put together an [example](https://www.bodyworkml.com/posts/scikit-learn-meet-production) of how to make this Agile approach to developing machine learning systems a reality, by demonstrating that it takes under 15 minutes to deploy a Scikit-Learn model, using FastAPI with [Bodywork](https://bodywork.readthedocs.io/en/latest/) (an open-source MLOps tool that I have built).\n\nHow does this compare to your experiences? I\u2019d be interested to get people\u2019s thoughts, as my background is largely with structured data.", "link": "https://www.reddit.com/r/MachineLearning/comments/mtuyuv/p_d_the_benefits_of_training_the_simplest_model/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] [d] the benefits of training the simplest model you can think of and deploying it to production, as soon as you can. /!/ i\u2019ve had many successes with this approach. with this in mind, i\u2019ve put together an [example](https://www.bodyworkml.com/posts/scikit-learn-meet-production) of how to make this agile approach to developing machine learning systems a reality, by demonstrating that it takes under 15 minutes to deploy a scikit-learn model, using fastapi with [bodywork](https://bodywork.readthedocs.io/en/latest/) (an open-source mlops -----> tool !!!  that i have built).\n\nhow does this compare to your experiences? i\u2019d be interested to get people\u2019s thoughts, as my background is largely with structured data.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 33, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/mtuyuv/p_d_the_benefits_of_training_the_simplest_model/',)", "identifyer": 5736942, "year": "2021"}, {"autor": "regalalgorithm", "date": 1629860879000, "content": "[D] AI as a Hype Tool /!/ Hi there, just sharing this new essay [AI as a Hype Tool](https://www.skynettoday.com/editorials/ai-hype). It doesn't present anything too novel, but is hopefully a good summary of the subject. And IMO it's less negative on AI than similar such takes, acknowledging that it has made great strides while also still being plagued by tons of silly hype.", "link": "https://www.reddit.com/r/MachineLearning/comments/pb2p5v/d_ai_as_a_hype_tool/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] ai as a hype -----> tool !!!  /!/ hi there, just sharing this new essay [ai as a hype tool](https://www.skynettoday.com/editorials/ai-hype). it doesn't present anything too novel, but is hopefully a good summary of the subject. and imo it's less negative on ai than similar such takes, acknowledging that it has made great strides while also still being plagued by tons of silly hype.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 5, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/pb2p5v/d_ai_as_a_hype_tool/',)", "identifyer": 5737018, "year": "2021"}, {"autor": "designer1one", "date": 1618667070000, "content": "[P] *Semantic* Video Search with OpenAI\u2019s CLIP Neural Network /!/ I made a simple tool that lets you search a video \\*semantically\\* with AI. \ud83c\udf9e\ufe0f\ud83d\udd0d\n\n\u2728 Live web app: [http://whichframe.com](http://whichframe.com/) \u2728\n\nExample: Which video frame has a person with sunglasses and earphones?\n\nThe querying is powered by OpenAI\u2019s CLIP neural network for performing \"zero-shot\" image classification and the interface was built with Streamlit.\n\nTry searching with text, image, or text + image and please share your discoveries!\n\n\ud83d\udc47 More examples  \n[https://twitter.com/chuanenlin/status/1383411082853683208](https://twitter.com/chuanenlin/status/1383411082853683208)", "link": "https://www.reddit.com/r/MachineLearning/comments/msr0cm/p_semantic_video_search_with_openais_clip_neural/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] *semantic* video search with openai\u2019s clip neural network /!/ i made a simple -----> tool !!!  that lets you search a video \\*semantically\\* with ai. \ud83c\udf9e\ufe0f\ud83d\udd0d\n\n\u2728 live web app: [http://whichframe.com](http://whichframe.com/) \u2728\n\nexample: which video frame has a person with sunglasses and earphones?\n\nthe querying is powered by openai\u2019s clip neural network for performing \"zero-shot\" image classification and the interface was built with streamlit.\n\ntry searching with text, image, or text + image and please share your discoveries!\n\n\ud83d\udc47 more examples  \n[https://twitter.com/chuanenlin/status/1383411082853683208](https://twitter.com/chuanenlin/status/1383411082853683208)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 16, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/msr0cm/p_semantic_video_search_with_openais_clip_neural/',)", "identifyer": 5737245, "year": "2021"}, {"autor": "ritan0n", "date": 1626036601000, "content": "i want to play with an AI tool but i don't know how to run it /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/oicd7p/i_want_to_play_with_an_ai_tool_but_i_dont_know/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "i want to play with an ai -----> tool !!!  but i don't know how to run it /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/oicd7p/i_want_to_play_with_an_ai_tool_but_i_dont_know/',)", "identifyer": 5737529, "year": "2021"}, {"autor": "UTSAVBUSA", "date": 1625974244000, "content": "Tool Wear, Tool Life &amp; Machinability", "link": "https://www.reddit.com/r/MachineLearning/comments/ohwzp4/tool_wear_tool_life_machinability/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "-----> tool !!!  wear, -----> tool !!!  life &amp; machinability", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('link',)", "medialink": "('https://www.mechstech.com/2021/07/tool-wear-tool-life-machinability.html',)", "identifyer": 5737567, "year": "2021"}, {"autor": "techsucker", "date": 1621344029000, "content": "[R] Ways To Stop AI From Recognizing Your Face In Selfies /!/ We upload so many personal photos on the internet, so we might have questions like who else would have access to them, what would they do with them\u2014and which\u00a0[machine-learning](https://www.technologyreview.com/topic/artificial-intelligence/)\u00a0algorithms would be trained with this data?\n\n[Clearview AI](https://en.wikipedia.org/wiki/Clearview_AI), an American facial recognition company, has already provided a facial recognition tool trained on millions of such photos scraped from the public web to US law enforcement agencies. But that was likely just the start. It\u2019s easy for anyone with basic coding skills to develop\u00a0[facial recognition software](https://www.technologyreview.com/topic/artificial-intelligence/face-recognition/). Thus, it\u2019s easier to abuse tech in everything, from sexual harassment and racial discrimination to political oppression and religious persecution.\n\nTo address this issue, there\u2019s a requirement to develop ways to make sure AIs can\u2019t learn from the personal data people upload.\u00a0**Emily Wenger**\u00a0at\u00a0**the University of Chicago**\u00a0and her colleagues developed[\u00a0one of the first tools to do this](https://arxiv.org/pdf/2002.08327.pdf), called\u00a0**Fawkes**.\n\nSummary: https://www.marktechpost.com/2021/05/18/ways-to-stop-ai-from-recognizing-your-face-in-selfies/\n\nPaper: http://people.cs.uchicago.edu/\\~ravenben/publications/pdf/fawkes-usenix20.pdf\n\nProject: https://sandlab.cs.uchicago.edu/fawkes/\n\nCodes: https://sandlab.cs.uchicago.edu/fawkes/#code", "link": "https://www.reddit.com/r/MachineLearning/comments/nfak4u/r_ways_to_stop_ai_from_recognizing_your_face_in/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[r] ways to stop ai from recognizing your face in selfies /!/ we upload so many personal photos on the internet, so we might have questions like who else would have access to them, what would they do with them\u2014and which\u00a0[machine-learning](https://www.technologyreview.com/topic/artificial-intelligence/)\u00a0algorithms would be trained with this data?\n\n[clearview ai](https://en.wikipedia.org/wiki/clearview_ai), an american facial recognition company, has already provided a facial recognition -----> tool !!!  trained on millions of such photos scraped from the public web to us law enforcement agencies. but that was likely just the start. it\u2019s easy for anyone with basic coding skills to develop\u00a0[facial recognition software](https://www.technologyreview.com/topic/artificial-intelligence/face-recognition/). thus, it\u2019s easier to abuse tech in everything, from sexual harassment and racial discrimination to political oppression and religious persecution.\n\nto address this issue, there\u2019s a requirement to develop ways to make sure ais can\u2019t learn from the personal data people upload.\u00a0**emily wenger**\u00a0at\u00a0**the university of chicago**\u00a0and her colleagues developed[\u00a0one of the first tools to do this](https://arxiv.org/pdf/2002.08327.pdf), called\u00a0**fawkes**.\n\nsummary: https://www.marktechpost.com/2021/05/18/ways-to-stop-ai-from-recognizing-your-face-in-selfies/\n\npaper: http://people.cs.uchicago.edu/\\~ravenben/publications/pdf/fawkes-usenix20.pdf\n\nproject: https://sandlab.cs.uchicago.edu/fawkes/\n\ncodes: https://sandlab.cs.uchicago.edu/fawkes/#code", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 50, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/nfak4u/r_ways_to_stop_ai_from_recognizing_your_face_in/',)", "identifyer": 5737636, "year": "2021"}, {"autor": "nielsrolf", "date": 1626470404000, "content": "[P] I made a tool for tracking hyperparameters, metrics and artifacts, and I like it better than tensorboard or mlflow /!/ Hi r/MachineLearning\n\nI made \\[this\\]([https://github.com/nielsrolf/pandas\\_db/](https://github.com/nielsrolf/pandas_db/)) - a tool that can be used to save any tabular data and files with associated metadata. There is also a small UI, mainly for browsing through images and audio files.\n\n**Why not use tensorboard?**\n\nTensorboard provides a bunch of graphs, but if you want to plot anything that isn't in there, you're going to have a hard time since it is not easily possible to export all tensorboard data into one dataframe. Another issue is that if you create many artifacts, it is hard to find the image or audio file that you are looking for.\n\n**Why not use MLFlow?**\n\nMLFlow is nice, but again it is hard to find and compare media files associated with certain metadata and if you create a lot of data, it gets unusably slow. I also don't like that it comes with the concept of experiments and runs - if my eval script is a separate script from the training script, I don't want the created data to be associated with a new run.\n\n&amp;#x200B;\n\nIf anyone would like to try it and/or give feedback, I would be happy. Just don't use it from two processes in parallel - I can add that feature if anyone is interested, but since I didn't need it yet and I don't know if anyone else wants to use it, I didn't invest the time yet. Same for documentation - I am happy to add documentation if something is unclear and there are users, but I didn't do much in that regard yet.", "link": "https://www.reddit.com/r/MachineLearning/comments/olqb64/p_i_made_a_tool_for_tracking_hyperparameters/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] i made a -----> tool !!!  for tracking hyperparameters, metrics and artifacts, and i like it better than tensorboard or mlflow /!/ hi r/machinelearning\n\ni made \\[this\\]([https://github.com/nielsrolf/pandas\\_db/](https://github.com/nielsrolf/pandas_db/)) - a tool that can be used to save any tabular data and files with associated metadata. there is also a small ui, mainly for browsing through images and audio files.\n\n**why not use tensorboard?**\n\ntensorboard provides a bunch of graphs, but if you want to plot anything that isn't in there, you're going to have a hard time since it is not easily possible to export all tensorboard data into one dataframe. another issue is that if you create many artifacts, it is hard to find the image or audio file that you are looking for.\n\n**why not use mlflow?**\n\nmlflow is nice, but again it is hard to find and compare media files associated with certain metadata and if you create a lot of data, it gets unusably slow. i also don't like that it comes with the concept of experiments and runs - if my eval script is a separate script from the training script, i don't want the created data to be associated with a new run.\n\n&amp;#x200b;\n\nif anyone would like to try it and/or give feedback, i would be happy. just don't use it from two processes in parallel - i can add that feature if anyone is interested, but since i didn't need it yet and i don't know if anyone else wants to use it, i didn't invest the time yet. same for documentation - i am happy to add documentation if something is unclear and there are users, but i didn't do much in that regard yet.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/olqb64/p_i_made_a_tool_for_tracking_hyperparameters/',)", "identifyer": 5737792, "year": "2021"}, {"autor": "FarFlounder5209", "date": 1626455779000, "content": "[P] A tool to save your experiment result as structured data /!/ Hey guys, I just made a small tool to manage experiment data: [autodump](https://github.com/noilreed/autodump)\n\nThe reason is I feel my previous experimental data is very messy, espicially when using different models and other people's code, they have different log styles and different ways to store data. it's even easier to get messy when I'm running experiments remotely on different platforms. Also it is very easy to make mistake and lose historical data.  As far as know, there is no good workflow specifically for manage experiment data.\n\nWith this thing, you can easily persist your data in a preconfigured datasource, by only adding a little bit code. Therefore the saved data becomes structured, clean and very easy to track and use through SQL, you'll never lose previous data as well.\n\nCurrently it only supports saving data in MySQL, so you better have a running MySQL server locally or remotely. I will consider implement more data storage type  (other databases, local YAML/JSON, etc.) in future version. I'm also conceiving a browser-based data visualization module, so besides using SQL, you can also directly visualize saved data. At least I feel it's extremely helpful in my experiment. So if anyone is interested, please try to use it!\n\nPlease contact me if you have any concerns, questions. (will be nice if could star the repo if you are interested :p)", "link": "https://www.reddit.com/r/MachineLearning/comments/ollde3/p_a_tool_to_save_your_experiment_result_as/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] a -----> tool !!!  to save your experiment result as structured data /!/ hey guys, i just made a small tool to manage experiment data: [autodump](https://github.com/noilreed/autodump)\n\nthe reason is i feel my previous experimental data is very messy, espicially when using different models and other people's code, they have different log styles and different ways to store data. it's even easier to get messy when i'm running experiments remotely on different platforms. also it is very easy to make mistake and lose historical data.  as far as know, there is no good workflow specifically for manage experiment data.\n\nwith this thing, you can easily persist your data in a preconfigured datasource, by only adding a little bit code. therefore the saved data becomes structured, clean and very easy to track and use through sql, you'll never lose previous data as well.\n\ncurrently it only supports saving data in mysql, so you better have a running mysql server locally or remotely. i will consider implement more data storage type  (other databases, local yaml/json, etc.) in future version. i'm also conceiving a browser-based data visualization module, so besides using sql, you can also directly visualize saved data. at least i feel it's extremely helpful in my experiment. so if anyone is interested, please try to use it!\n\nplease contact me if you have any concerns, questions. (will be nice if could star the repo if you are interested :p)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 12, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/ollde3/p_a_tool_to_save_your_experiment_result_as/',)", "identifyer": 5737802, "year": "2021"}, {"autor": "oFlamingo", "date": 1627151310000, "content": "[P] A Standalone Tool For Tesseract OCR (C#) /!/  Github: [https://github.com/angry-coder-room/TextReader-Tesseract-OCR](https://github.com/angry-coder-room/TextReader-Tesseract-OCR)\n\nApplication: The intention was to read code from a playing YouTube video, so I don't have to copy &amp; type code line by line.\n\nFeature: An built-in snipping tool to capture images on the screen, and apply the Tesseract model just in a single click.\n\nCons\n\n1. Used a pre-trained model, so I have no control over the model.\n2. Most of the time predictions are terrible, sometimes really good.", "link": "https://www.reddit.com/r/MachineLearning/comments/oqvg2d/p_a_standalone_tool_for_tesseract_ocr_c/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] a standalone -----> tool !!!  for tesseract ocr (c#) /!/  github: [https://github.com/angry-coder-room/textreader-tesseract-ocr](https://github.com/angry-coder-room/textreader-tesseract-ocr)\n\napplication: the intention was to read code from a playing youtube video, so i don't have to copy &amp; type code line by line.\n\nfeature: an built-in snipping tool to capture images on the screen, and apply the tesseract model just in a single click.\n\ncons\n\n1. used a pre-trained model, so i have no control over the model.\n2. most of the time predictions are terrible, sometimes really good.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/oqvg2d/p_a_standalone_tool_for_tesseract_ocr_c/',)", "identifyer": 5737859, "year": "2021"}, {"autor": "oFlamingo", "date": 1627151093000, "content": "A Standalone Tool For Tesseract OCR (C#) /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/oqvdlv/a_standalone_tool_for_tesseract_ocr_c/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "a standalone -----> tool !!!  for tesseract ocr (c#) /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/oqvdlv/a_standalone_tool_for_tesseract_ocr_c/',)", "identifyer": 5737860, "year": "2021"}, {"autor": "calebkaiser", "date": 1610382964000, "content": "[P] Cortex 0.26: Beta support for GCP, micro-batching, and more /!/ Repo link: [https://github.com/cortexlabs/cortex](https://github.com/cortexlabs/cortex)\n\nWe've just released Cortex 0.26, and I wanted to share it here with the community.\n\nAs of this release, Cortex supports several new features (some of which were direct suggestions from people within r/MachineLearning, so thank you for that), including:\n\n* **Beta support for GCP.** Thanks to feedback from quite a few members of r/MachineLearning and Cortex users, we've prioritized GCP support and rolled out an initial beta. More on that at the bottom of this post.\n* **Server-side batching**. Prediction serving APIs can now run inference in micro batches, allowing you to maximize throughput at the expense of latency without giving up on-demand inference. [More details here](https://docs.cortex.dev/workloads/realtime/server-side-batching).\n* **Improved batch inference.** Including the implementation of a dead-letter queue, configurable retry behavior, and an overall better system for handling inference errors. [More details here.](https://docs.cortex.dev/workloads/batch/example)\n\nIn addition to many other more incremental changes (improvements to logging, networking, UX, etc.)\n\nIf you're completely unfamiliar, Cortex is a platform that makes it easy to build, deploy, and manage machine learning APIs at scale on AWS (and now, GCP). At a high level, Cortex's core features include:\n\n* **Infrastructure automation.** Cortex automates all the underlying infrastructure, including autoscaling, cluster management, cloud services, server configuration, monitoring, and more, providing opinionated defaults out of the box, while remaining fully customizable.\n* **Reproducible deployment pipelines.** On deploy, Cortex automatically packages models, deploys them to the Cortex cluster as autoscaling APIs, and allows you to version them. An API's configuration can be specified in YAML or via Cortex's Python client, depending on your preference.\n* **Advanced inference features.** Beyond supporting basic batch and realtime inference, Cortex supports GPU/ASIC inference, multi-model caching (1,000s of models on one instance), server-side micro batching, and more. Additionally, Cortex also supports deployments to Spot instances, which can reduce cloud costs by \\~40%.\n* **Open platform.** Serve models from any framework, integrate with any tool, implement whatever inference logic you'd like\u2014Cortex is designed to be flexible and modular to fit your stack.\n\nIf you have any questions or feedback, I'd love to hear it. If you want to dig deeper, Cortex has [documentation here](https://docs.cortex.dev/).\n\nNOTE ON GCP:\n\nWe're calling this initial GCP support a beta. By that, we mean that currently, GCP deployments do not have full feature parity with AWS deployments on Cortex. As of now, GCP deployments support:\n\n* All three predictor types (Python, TensorFlow, ONNX)\n* CPU and GPU inference\n* Live reloading\n* Multi-model caching\n* Cluster autoscaling\n\nAnd in the immediate future (as in the next couple weeks), GCP deployments will also support replica-level autoscaling, just as AWS deployments currently do.\n\nAs we bring GCP deployments to feature parity with AWS deployments, we want to do it in a way that prioritizes the most important features, and implements them with respect to the specifics of developing on GCP. If you are interested in trying out GCP deployments, we would really appreciate your feedback here\u2014What features would you\u2019d like us to prioritize? What are the GCP quirks you hate dealing with the most?", "link": "https://www.reddit.com/r/MachineLearning/comments/kv6897/p_cortex_026_beta_support_for_gcp_microbatching/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] cortex 0.26: beta support for gcp, micro-batching, and more /!/ repo link: [https://github.com/cortexlabs/cortex](https://github.com/cortexlabs/cortex)\n\nwe've just released cortex 0.26, and i wanted to share it here with the community.\n\nas of this release, cortex supports several new features (some of which were direct suggestions from people within r/machinelearning, so thank you for that), including:\n\n* **beta support for gcp.** thanks to feedback from quite a few members of r/machinelearning and cortex users, we've prioritized gcp support and rolled out an initial beta. more on that at the bottom of this post.\n* **server-side batching**. prediction serving apis can now run inference in micro batches, allowing you to maximize throughput at the expense of latency without giving up on-demand inference. [more details here](https://docs.cortex.dev/workloads/realtime/server-side-batching).\n* **improved batch inference.** including the implementation of a dead-letter queue, configurable retry behavior, and an overall better system for handling inference errors. [more details here.](https://docs.cortex.dev/workloads/batch/example)\n\nin addition to many other more incremental changes (improvements to logging, networking, ux, etc.)\n\nif you're completely unfamiliar, cortex is a platform that makes it easy to build, deploy, and manage machine learning apis at scale on aws (and now, gcp). at a high level, cortex's core features include:\n\n* **infrastructure automation.** cortex automates all the underlying infrastructure, including autoscaling, cluster management, cloud services, server configuration, monitoring, and more, providing opinionated defaults out of the box, while remaining fully customizable.\n* **reproducible deployment pipelines.** on deploy, cortex automatically packages models, deploys them to the cortex cluster as autoscaling apis, and allows you to version them. an api's configuration can be specified in yaml or via cortex's python client, depending on your preference.\n* **advanced inference features.** beyond supporting basic batch and realtime inference, cortex supports gpu/asic inference, multi-model caching (1,000s of models on one instance), server-side micro batching, and more. additionally, cortex also supports deployments to spot instances, which can reduce cloud costs by \\~40%.\n* **open platform.** serve models from any framework, integrate with any -----> tool !!! , implement whatever inference logic you'd like\u2014cortex is designed to be flexible and modular to fit your stack.\n\nif you have any questions or feedback, i'd love to hear it. if you want to dig deeper, cortex has [documentation here](https://docs.cortex.dev/).\n\nnote on gcp:\n\nwe're calling this initial gcp support a beta. by that, we mean that currently, gcp deployments do not have full feature parity with aws deployments on cortex. as of now, gcp deployments support:\n\n* all three predictor types (python, tensorflow, onnx)\n* cpu and gpu inference\n* live reloading\n* multi-model caching\n* cluster autoscaling\n\nand in the immediate future (as in the next couple weeks), gcp deployments will also support replica-level autoscaling, just as aws deployments currently do.\n\nas we bring gcp deployments to feature parity with aws deployments, we want to do it in a way that prioritizes the most important features, and implements them with respect to the specifics of developing on gcp. if you are interested in trying out gcp deployments, we would really appreciate your feedback here\u2014what features would you\u2019d like us to prioritize? what are the gcp quirks you hate dealing with the most?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 8, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/kv6897/p_cortex_026_beta_support_for_gcp_microbatching/',)", "identifyer": 5737993, "year": "2021"}, {"autor": "CRACKTHERE", "date": 1610338709000, "content": "Miracle Box 2021 Crack Setup Tool Free Version Full Download", "link": "https://www.reddit.com/r/MachineLearning/comments/kuv0ga/miracle_box_2021_crack_setup_tool_free_version/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "miracle box 2021 crack setup -----> tool !!!  free version full download", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('link',)", "medialink": "('https://vsthd.com/miracle-box-crack/',)", "identifyer": 5738051, "year": "2021"}, {"autor": "machinemask", "date": 1617699100000, "content": "[D] Best system load monitoring tool? /!/ Hello, I'm looking for a tool that can monitor and display information such as cpu/gpu usage over time. I'm envisioning something like htop together with an interactive dashboard to filter the data on e.g. time or user. I'd like to install it on a company gpu server to get a better understanding of how it's used. We mostly use the servers for training computer vision models so that's what I'm most interested in.\n\nHere are a few features that I think would be interesting to have.\n\n* CPU usage. Blocking vs non-blocking\n* GPU utilization and memory usage\n* RAM usage\n* Disk read and write\n* Support for multiple machines\n* Display saved data in a dashboard\n* Filter displayed data on e.g. user, process, time\n\n&amp;#x200B;\n\nI found some projects that do similar things. [Permon](https://github.com/bminixhofer/permon), [GPU Monitor](https://github.com/msalvaris/gpu_monitor), [NVTOP](https://github.com/Syllo/nvtop), [Weights &amp; Biases](https://wandb.ai/site/articles/monitor-improve-gpu-usage-for-model-training)\n\nWhat do you think? Is it a good idea to set up such a system? Is there software out there that I've missed?\n\nAny experiences and suggestions are welcome!", "link": "https://www.reddit.com/r/MachineLearning/comments/ml79cr/d_best_system_load_monitoring_tool/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] best system load monitoring -----> tool !!! ? /!/ hello, i'm looking for a tool that can monitor and display information such as cpu/gpu usage over time. i'm envisioning something like htop together with an interactive dashboard to filter the data on e.g. time or user. i'd like to install it on a company gpu server to get a better understanding of how it's used. we mostly use the servers for training computer vision models so that's what i'm most interested in.\n\nhere are a few features that i think would be interesting to have.\n\n* cpu usage. blocking vs non-blocking\n* gpu utilization and memory usage\n* ram usage\n* disk read and write\n* support for multiple machines\n* display saved data in a dashboard\n* filter displayed data on e.g. user, process, time\n\n&amp;#x200b;\n\ni found some projects that do similar things. [permon](https://github.com/bminixhofer/permon), [gpu monitor](https://github.com/msalvaris/gpu_monitor), [nvtop](https://github.com/syllo/nvtop), [weights &amp; biases](https://wandb.ai/site/articles/monitor-improve-gpu-usage-for-model-training)\n\nwhat do you think? is it a good idea to set up such a system? is there software out there that i've missed?\n\nany experiences and suggestions are welcome!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 5, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/ml79cr/d_best_system_load_monitoring_tool/',)", "identifyer": 5738078, "year": "2021"}, {"autor": "machinemask", "date": 1617698879000, "content": "Best system load monitoring tool? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/ml77pt/best_system_load_monitoring_tool/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "best system load monitoring -----> tool !!! ? /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/ml77pt/best_system_load_monitoring_tool/',)", "identifyer": 5738079, "year": "2021"}, {"autor": "deshara128", "date": 1617657970000, "content": "is there a free/simple to use tool for AI up-rendering an image? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/mkvckf/is_there_a_freesimple_to_use_tool_for_ai/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "is there a free/simple to use -----> tool !!!  for ai up-rendering an image? /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/mkvckf/is_there_a_freesimple_to_use_tool_for_ai/',)", "identifyer": 5738104, "year": "2021"}, {"autor": "TheCollaboratory", "date": 1619194117000, "content": "[P] Introducing The Collaboratory: A place to explore and discover research based on your interests /!/ Keeping track of new research is an increasingly time-consuming task in many fields. Fast-paced and interdisciplinary domains like ML are even harder to stay on top of, and require researchers to self-curate from a variety of sources (bookmarks, newsletters, arXiv reviews, etc.) on the topics relevant to them. The Collaboratory is a webservice that aims to ease the pain of exploring and keeping up with the research that\u2019s most relevant to you.\n\n\nOur platform automatically sources and recommends the latest research relevant to your interests, based on papers you seed into \u201creading lists\u201d. These recommendations are powered by large language models that a) transform research descriptions into semantic embeddings, and b) compare those with pre-computed embeddings from a growing index of over 100M+ papers and datasets. Our models ensure that your recommendations are highly relevant, and our indexing efforts ensure that they are timely.\n\n\nWe were inspired to solve this problem after encountering it in our own daily lives, checking dozens of sources a week to stay on top of the literature in a few different fields. We think this tool will be really useful for people spinning up on new topics, people whose interests lie in-between fields, and for people who need to keep up with a stream of research that moves quickly.\n\n\nPlease take a look at the site and let us know what you think: [thecollaboratory.ai](https://thecollaboratory.ai). \n\n\nThe platform remains a work in progress, but we want to keep making it better - please let us know how we can make it better for you!", "link": "https://www.reddit.com/r/MachineLearning/comments/mwyp49/p_introducing_the_collaboratory_a_place_to/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] introducing the collaboratory: a place to explore and discover research based on your interests /!/ keeping track of new research is an increasingly time-consuming task in many fields. fast-paced and interdisciplinary domains like ml are even harder to stay on top of, and require researchers to self-curate from a variety of sources (bookmarks, newsletters, arxiv reviews, etc.) on the topics relevant to them. the collaboratory is a webservice that aims to ease the pain of exploring and keeping up with the research that\u2019s most relevant to you.\n\n\nour platform automatically sources and recommends the latest research relevant to your interests, based on papers you seed into \u201creading lists\u201d. these recommendations are powered by large language models that a) transform research descriptions into semantic embeddings, and b) compare those with pre-computed embeddings from a growing index of over 100m+ papers and datasets. our models ensure that your recommendations are highly relevant, and our indexing efforts ensure that they are timely.\n\n\nwe were inspired to solve this problem after encountering it in our own daily lives, checking dozens of sources a week to stay on top of the literature in a few different fields. we think this -----> tool !!!  will be really useful for people spinning up on new topics, people whose interests lie in-between fields, and for people who need to keep up with a stream of research that moves quickly.\n\n\nplease take a look at the site and let us know what you think: [thecollaboratory.ai](https://thecollaboratory.ai). \n\n\nthe platform remains a work in progress, but we want to keep making it better - please let us know how we can make it better for you!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 12, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/mwyp49/p_introducing_the_collaboratory_a_place_to/',)", "identifyer": 5738231, "year": "2021"}, {"autor": "sarmientoj24", "date": 1619179833000, "content": "[P] Is it possible to create a benchmark OSes performance in terms of ML training, prediction? /!/ I am thinking of a possible topic for Advanced OS (for a small research paper):  \n\n\nMy goal is to try and check the performance evaluation of different Operating Systems such as Ubuntu, Debian, Fedora, Mint, Windows, CentOS, etc. in terms of training and prediction from simple ML models to CNN and RNNs.\n\nIs it possible to do some benchmarks **focusing on Operating Systems** for this? If so, what kind of tools can I use to do some benchmarks? For example, check CPU usage, RAM usage, etc. \n\nI am planning to do the following:  \n\\- Create a Docker container of the specified OS  \n\\- Start tool benchmark/measurement   \n\\- Start Training #1  \n\\- Start Prediction #1  \n\\- Start Training #2  \n...  \nand so on...  \n\n\nMy questions would be:  \n\\- is it possible? I haven't seen papers tackling OS effect on Machine Learning training  \n\\- Is there a measurement tool?  \n\\- Can I do these given the outline of my preferred methodology", "link": "https://www.reddit.com/r/MachineLearning/comments/mwttnx/p_is_it_possible_to_create_a_benchmark_oses/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] is it possible to create a benchmark oses performance in terms of ml training, prediction? /!/ i am thinking of a possible topic for advanced os (for a small research paper):  \n\n\nmy goal is to try and check the performance evaluation of different operating systems such as ubuntu, debian, fedora, mint, windows, centos, etc. in terms of training and prediction from simple ml models to cnn and rnns.\n\nis it possible to do some benchmarks **focusing on operating systems** for this? if so, what kind of tools can i use to do some benchmarks? for example, check cpu usage, ram usage, etc. \n\ni am planning to do the following:  \n\\- create a docker container of the specified os  \n\\- start -----> tool !!!  benchmark/measurement   \n\\- start training #1  \n\\- start prediction #1  \n\\- start training #2  \n...  \nand so on...  \n\n\nmy questions would be:  \n\\- is it possible? i haven't seen papers tackling os effect on machine learning training  \n\\- is there a measurement tool?  \n\\- can i do these given the outline of my preferred methodology", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/mwttnx/p_is_it_possible_to_create_a_benchmark_oses/',)", "identifyer": 5738243, "year": "2021"}, {"autor": "Shoulder_Feeling", "date": 1619417114000, "content": "[Project] DataTap provides droplets ( containers for datasets) to make working on popular deep learning datasets easy. /!/ Excited to share [DataTap](https://www.datatap.dev), An open-source dataset management tool that makes it easy to \"containerize\" datasets to let you  focus on machine learning not data ops.  DataTap lets you build data set droplets ( think of a droplet as a docker container for data ).  A droplet encapsulates a dataset that can then easily be used , imported, shared across different teams and projects.\n\nEach Data Droplet consists for 2 items\n\n* **Droplet Template**, similar to a docker file this specifies the dataset schema\n* **Dataset annotations, metadata and media** (this is typically images / videos / rich media )\n\nLearn more about how you can start using this here [https://github.com/zensors/datatap-python](https://github.com/zensors/datatap-python)\n\nMany machine learning projects use proprietary data formats that require tools and utilities to be re-written from scratch to accommodate them. Not only does this slow down development substantially, but it also increases the probability that developers introduce bugs in the very code that validates models\u2019 performance.\n\nAs part of dataTap\u2019s efforts to allow machine learning engineers to focus only on the machine learning itself, we introduced an open-source data interchange format called Droplet. The data container format, called \u201cannotation,\u201d provides a standardized way to describe what is in an image. DataTap is designed to be the data platform for Software 2.0. Machine learning on reach media like images , audio or video needs a special data pipeline to version and manage data much like there are MLOps tools to version and manage models\n\nCurrently the project has common datasets available that you can download or stream with 3 lines of code.\n\n* coco\n* Open-Imagees\n* AI food Dataset\n* Large Person Dataset\n* Combined Vehicles Dataset\n\nSee the full list \n\n[https://app.datatap.dev/databases/1b81ec5a-b880-4cf4-ba67-86e301cada9e](https://app.datatap.dev/databases/1b81ec5a-b880-4cf4-ba67-86e301cada9e)\n\nRequest Your own to be added in or use the open source tools to import data into the droplet format using this example\n\n[https://zensors.typeform.com/to/WXo3ZlSN](https://zensors.typeform.com/to/WXo3ZlSN)", "link": "https://www.reddit.com/r/MachineLearning/comments/myqyls/project_datatap_provides_droplets_containers_for/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[project] datatap provides droplets ( containers for datasets) to make working on popular deep learning datasets easy. /!/ excited to share [datatap](https://www.datatap.dev), an open-source dataset management -----> tool !!!  that makes it easy to \"containerize\" datasets to let you  focus on machine learning not data ops.  datatap lets you build data set droplets ( think of a droplet as a docker container for data ).  a droplet encapsulates a dataset that can then easily be used , imported, shared across different teams and projects.\n\neach data droplet consists for 2 items\n\n* **droplet template**, similar to a docker file this specifies the dataset schema\n* **dataset annotations, metadata and media** (this is typically images / videos / rich media )\n\nlearn more about how you can start using this here [https://github.com/zensors/datatap-python](https://github.com/zensors/datatap-python)\n\nmany machine learning projects use proprietary data formats that require tools and utilities to be re-written from scratch to accommodate them. not only does this slow down development substantially, but it also increases the probability that developers introduce bugs in the very code that validates models\u2019 performance.\n\nas part of datatap\u2019s efforts to allow machine learning engineers to focus only on the machine learning itself, we introduced an open-source data interchange format called droplet. the data container format, called \u201cannotation,\u201d provides a standardized way to describe what is in an image. datatap is designed to be the data platform for software 2.0. machine learning on reach media like images , audio or video needs a special data pipeline to version and manage data much like there are mlops tools to version and manage models\n\ncurrently the project has common datasets available that you can download or stream with 3 lines of code.\n\n* coco\n* open-imagees\n* ai food dataset\n* large person dataset\n* combined vehicles dataset\n\nsee the full list \n\n[https://app.datatap.dev/databases/1b81ec5a-b880-4cf4-ba67-86e301cada9e](https://app.datatap.dev/databases/1b81ec5a-b880-4cf4-ba67-86e301cada9e)\n\nrequest your own to be added in or use the open source tools to import data into the droplet format using this example\n\n[https://zensors.typeform.com/to/wxo3zlsn](https://zensors.typeform.com/to/wxo3zlsn)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/myqyls/project_datatap_provides_droplets_containers_for/',)", "identifyer": 5738483, "year": "2021"}, {"autor": "carlml", "date": 1626656654000, "content": "[D] what tools/practices do you use to document your code/functions? /!/ I noticed that I am writing certain functions over and over again because I didn't document very well, so finding them becomes hard, and I usually just write them again. Is there a tool that allows one to document functions? I like the way numpy or pytorch's functions are documented.\n\n&amp;#x200B;\n\nI know this is not directly to machine learning, and if my post definitely does not belong here, I'll delete it.", "link": "https://www.reddit.com/r/MachineLearning/comments/on3lth/d_what_toolspractices_do_you_use_to_document_your/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] what tools/practices do you use to document your code/functions? /!/ i noticed that i am writing certain functions over and over again because i didn't document very well, so finding them becomes hard, and i usually just write them again. is there a -----> tool !!!  that allows one to document functions? i like the way numpy or pytorch's functions are documented.\n\n&amp;#x200b;\n\ni know this is not directly to machine learning, and if my post definitely does not belong here, i'll delete it.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 14, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/on3lth/d_what_toolspractices_do_you_use_to_document_your/',)", "identifyer": 5738516, "year": "2021"}, {"autor": "jayalammar", "date": 1611061790000, "content": "[R] Finding The Words To Say: Hidden State Visualizations For Language Models /!/  \n\nHi [r/MachineLearning](https://www.reddit.com/r/MachineLearning/),\n\nThe hidden states of Transformers are an interesting place to seek clues for how the various layers process an input token and collectively select an output token. In this post, I demonstrate three ways of visualizing the hidden state. I build on awesome previous work in the space.\n\nWhat I find most exciting is that this post is only a demo for the open-source package creating these visualizations ([Ecco](https://www.eccox.io/)). I'm sure other people will find more interesting ways to probe these models using this tool. I hope you find it useful!\n\nTL;DR: The output tokens tend to be selected by the latter half of model layers. But there are cases where even layer 0 is certain of the output token (look for solid dark pink columns in the visuals).\n\n[https://jalammar.github.io/hidden-states/](https://jalammar.github.io/hidden-states/)", "link": "https://www.reddit.com/r/MachineLearning/comments/l0jiy0/r_finding_the_words_to_say_hidden_state/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[r] finding the words to say: hidden state visualizations for language models /!/  \n\nhi [r/machinelearning](https://www.reddit.com/r/machinelearning/),\n\nthe hidden states of transformers are an interesting place to seek clues for how the various layers process an input token and collectively select an output token. in this post, i demonstrate three ways of visualizing the hidden state. i build on awesome previous work in the space.\n\nwhat i find most exciting is that this post is only a demo for the open-source package creating these visualizations ([ecco](https://www.eccox.io/)). i'm sure other people will find more interesting ways to probe these models using this -----> tool !!! . i hope you find it useful!\n\ntl;dr: the output tokens tend to be selected by the latter half of model layers. but there are cases where even layer 0 is certain of the output token (look for solid dark pink columns in the visuals).\n\n[https://jalammar.github.io/hidden-states/](https://jalammar.github.io/hidden-states/)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/l0jiy0/r_finding_the_words_to_say_hidden_state/',)", "identifyer": 5738608, "year": "2021"}, {"autor": "OleguerCanal", "date": 1611059308000, "content": "[D] Tool to track and relate key ideas of the papers readen /!/ I'm wondering what tool do researchers use to log key ideas of the papers they read.\n\nSorry for the vague question, I guess I am thinking of some kind of digital whiteboard with boxes representing papers and bullet points with main contributions and then some kind of personal comments and relations among papers.\n\nIs there anything with a similar purpose you would recommend?", "link": "https://www.reddit.com/r/MachineLearning/comments/l0iw9t/d_tool_to_track_and_relate_key_ideas_of_the/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] -----> tool !!!  to track and relate key ideas of the papers readen /!/ i'm wondering what tool do researchers use to log key ideas of the papers they read.\n\nsorry for the vague question, i guess i am thinking of some kind of digital whiteboard with boxes representing papers and bullet points with main contributions and then some kind of personal comments and relations among papers.\n\nis there anything with a similar purpose you would recommend?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 16, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/l0iw9t/d_tool_to_track_and_relate_key_ideas_of_the/',)", "identifyer": 5738611, "year": "2021"}, {"autor": "OleguerCanal", "date": 1611059063000, "content": "Tool to track and relate key paper ideas /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/l0iu6n/tool_to_track_and_relate_key_paper_ideas/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "-----> tool !!!  to track and relate key paper ideas /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/l0iu6n/tool_to_track_and_relate_key_paper_ideas/',)", "identifyer": 5738612, "year": "2021"}, {"autor": "Streletzky", "date": 1623433936000, "content": "[D] Best package in Python to manually set up a neural network without layers purely for evaluation (without back-propagation training) /!/ I have some decent experience using neural networks, and for everything else before this I have gotten away with just coding up the neural network directly or a script that creates a NN based on some inputs. I'm about to start working on something that steps up the complexity, so I feel like I should definitely leverage one of the big ML tools in Python to do the heavy lifting.\n\n&amp;#x200B;\n\nEverything I see online about Keras, Tensorflow, and Pytorch all involve creating layers of a NN and using those tools do to the training, which is not very helpful to me. I'm looking to get use out of a tool where I can specify which nodes are connected, the exact weight values, and just tell it to evaluate the network based on my inputs to get the output node values. I can't use any standard layer creation methods since there is no simple geometry for the connections between the nodes in the networks.\n\n&amp;#x200B;\n\nSo I guess my question is: what Python ML tool makes doing this the easiest? \n\n&amp;#x200B;\n\nBonus if the tool can handle time recurrent neural network evaluation in addition to just forward propagation.", "link": "https://www.reddit.com/r/MachineLearning/comments/nxlzja/d_best_package_in_python_to_manually_set_up_a/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] best package in python to manually set up a neural network without layers purely for evaluation (without back-propagation training) /!/ i have some decent experience using neural networks, and for everything else before this i have gotten away with just coding up the neural network directly or a script that creates a nn based on some inputs. i'm about to start working on something that steps up the complexity, so i feel like i should definitely leverage one of the big ml tools in python to do the heavy lifting.\n\n&amp;#x200b;\n\neverything i see online about keras, tensorflow, and pytorch all involve creating layers of a nn and using those tools do to the training, which is not very helpful to me. i'm looking to get use out of a -----> tool !!!  where i can specify which nodes are connected, the exact weight values, and just tell it to evaluate the network based on my inputs to get the output node values. i can't use any standard layer creation methods since there is no simple geometry for the connections between the nodes in the networks.\n\n&amp;#x200b;\n\nso i guess my question is: what python ml tool makes doing this the easiest? \n\n&amp;#x200b;\n\nbonus if the tool can handle time recurrent neural network evaluation in addition to just forward propagation.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 14, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/nxlzja/d_best_package_in_python_to_manually_set_up_a/',)", "identifyer": 5738710, "year": "2021"}, {"autor": "danFromTelAviv", "date": 1623156831000, "content": "[D] Best Cloud Dev Tool /!/ Hi all, \n\nwhat is the best cloud development setup you've come across for industry dl research? (python,gcp,pytorch)\n\nI'm talking about using a cloud sever as a work station for both development + training + eval. \n\nJupyter is great because connection problems don't stop the run. But it doesn't have modern tools like debugging, auto import ...etc \n\npycharm/vscode in remote dev - require connection to sustain through out training / dev session. But they have amazing tools that make jupyter look like childplay. \n\ncode-server - ultra buggy\n\nanything else? \npreferably paid off the shelf solution.", "link": "https://www.reddit.com/r/MachineLearning/comments/nv3d0v/d_best_cloud_dev_tool/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] best cloud dev -----> tool !!!  /!/ hi all, \n\nwhat is the best cloud development setup you've come across for industry dl research? (python,gcp,pytorch)\n\ni'm talking about using a cloud sever as a work station for both development + training + eval. \n\njupyter is great because connection problems don't stop the run. but it doesn't have modern tools like debugging, auto import ...etc \n\npycharm/vscode in remote dev - require connection to sustain through out training / dev session. but they have amazing tools that make jupyter look like childplay. \n\ncode-server - ultra buggy\n\nanything else? \npreferably paid off the shelf solution.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 12, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/nv3d0v/d_best_cloud_dev_tool/',)", "identifyer": 5738856, "year": "2021"}, {"autor": "profclems", "date": 1610793347000, "content": "glab: an open source GitLab CLI tool bringing GitLab to your terminal next to where you are already working with git and your code without switching between windows and browser tabs", "link": "https://www.reddit.com/r/MachineLearning/comments/kygpwc/glab_an_open_source_gitlab_cli_tool_bringing/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "glab: an open source gitlab cli -----> tool !!!  bringing gitlab to your terminal next to where you are already working with git and your code without switching between windows and browser tabs", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('link',)", "medialink": "('https://github.com/profclems/glab',)", "identifyer": 5738915, "year": "2021"}, {"autor": "dicroce", "date": 1610769483000, "content": "A tool for Pascal VOC dataset creation... /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/kybftd/a_tool_for_pascal_voc_dataset_creation/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "a -----> tool !!!  for pascal voc dataset creation... /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/kybftd/a_tool_for_pascal_voc_dataset_creation/',)", "identifyer": 5738934, "year": "2021"}, {"autor": "da_chosen1", "date": 1621836395000, "content": "[R] Machine Learning Baseline Model /!/ Assume you work for an apparel retailer as a data scientist and your task is to send out 100,000 advertising mail pieces to past customers about a new line of fall apparel. You have access to a database of information about past customers, including what they have purchased and their demographic information (Ramakrishnan 2018). What model should you build first?\n\nA baseline!\n\nIn this article, we will discuss what a baseline is and where it fits in our data analysis projects. We will see that there are two different types of baselines, one which refers to a simple model, and another which refers to the best model from previous works. A baseline guides our selection of more complex models and provides insights into the task at hand. Nonetheless, such a useful tool is not easy to handle. A literature review (Lin 2019; Mignan 2019; Rendle et al. 2019; Yang et al. 2019) shows that many researchers tend to compare their novel models against weak baselines which poses a problem in the current research sphere as it leads to optimistic, but false results. A discussion on such contemporary issues and open-ended questions is also provided at the end, followed by suggestions aimed at the community.\n\nhttps://blog.ml.cmu.edu/2020/08/31/3-baselines/", "link": "https://www.reddit.com/r/MachineLearning/comments/njr0vr/r_machine_learning_baseline_model/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[r] machine learning baseline model /!/ assume you work for an apparel retailer as a data scientist and your task is to send out 100,000 advertising mail pieces to past customers about a new line of fall apparel. you have access to a database of information about past customers, including what they have purchased and their demographic information (ramakrishnan 2018). what model should you build first?\n\na baseline!\n\nin this article, we will discuss what a baseline is and where it fits in our data analysis projects. we will see that there are two different types of baselines, one which refers to a simple model, and another which refers to the best model from previous works. a baseline guides our selection of more complex models and provides insights into the task at hand. nonetheless, such a useful -----> tool !!!  is not easy to handle. a literature review (lin 2019; mignan 2019; rendle et al. 2019; yang et al. 2019) shows that many researchers tend to compare their novel models against weak baselines which poses a problem in the current research sphere as it leads to optimistic, but false results. a discussion on such contemporary issues and open-ended questions is also provided at the end, followed by suggestions aimed at the community.\n\nhttps://blog.ml.cmu.edu/2020/08/31/3-baselines/", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/njr0vr/r_machine_learning_baseline_model/',)", "identifyer": 5739098, "year": "2021"}, {"autor": "bigbeachenergy", "date": 1622048377000, "content": "[P] AutoML for unsupervised learning /!/ Hi r/machinelearning! We are All Vision Technologies, an early-stage startup in NYC and long-time readers of this subreddit. We wanted to invite you to try out our new tool, called the Model Zoo. It's an AutoML platform where you can simultaneously run and compare 14 clustering and anomaly detection algorithms, optimize hyperparameters with no code and see model performance. It's available both as a platform with a UI and an API. \ud83d\udc05\ud83d\udc18\ud83e\udd95\n\nWe've reimplemented many of these algorithms from scratch, achieving up to 10x speedups in comparison with open-source implementations like scikit-learn.\n\nThis release is our first milestone in a larger journey of building powerful and easy to use unsupervised learning tools, aimed at both individual contributors and enterprise customers. We saw a need for this type of product and tooling while working on fraud prevention at Capital One and then realized that a ton of data science teams struggle with finding production-ready unsupervised learning tools.\n\nWe have a 14-day free trial for anyone who wants to try it out. To get started, make an account at [https://all.vision/](https://all.vision/) and upload any unlabeled dataset!", "link": "https://www.reddit.com/r/MachineLearning/comments/nlluep/p_automl_for_unsupervised_learning/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] automl for unsupervised learning /!/ hi r/machinelearning! we are all vision technologies, an early-stage startup in nyc and long-time readers of this subreddit. we wanted to invite you to try out our new -----> tool !!! , called the model zoo. it's an automl platform where you can simultaneously run and compare 14 clustering and anomaly detection algorithms, optimize hyperparameters with no code and see model performance. it's available both as a platform with a ui and an api. \ud83d\udc05\ud83d\udc18\ud83e\udd95\n\nwe've reimplemented many of these algorithms from scratch, achieving up to 10x speedups in comparison with open-source implementations like scikit-learn.\n\nthis release is our first milestone in a larger journey of building powerful and easy to use unsupervised learning tools, aimed at both individual contributors and enterprise customers. we saw a need for this type of product and tooling while working on fraud prevention at capital one and then realized that a ton of data science teams struggle with finding production-ready unsupervised learning tools.\n\nwe have a 14-day free trial for anyone who wants to try it out. to get started, make an account at [https://all.vision/](https://all.vision/) and upload any unlabeled dataset!", "sortedWord": "None", "removed": "('nan',)", "score": 2, "comments": 9, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/nlluep/p_automl_for_unsupervised_learning/',)", "identifyer": 5739216, "year": "2021"}, {"autor": "alongub", "date": 1624095079000, "content": "[P] KubeSurvival - Easy K8s Cost Optimization - Useful for clusters with a lot of ML training jobs &amp; model servers /!/ Just wanted to share a cool new open-source tool I built to\u00a0**significantly reduce Kubernetes compute costs**, by finding the cheapest machines that successfully run your workloads.\n\nIt's designed for clusters with a lot of ML training jobs &amp; model servers - these can get really expensive (especially if you use GPUs).\n\nCheck it out: [https://github.com/aporia-ai/kubesurvival](https://github.com/aporia-ai/kubesurvival)", "link": "https://www.reddit.com/r/MachineLearning/comments/o3c7p0/p_kubesurvival_easy_k8s_cost_optimization_useful/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] kubesurvival - easy k8s cost optimization - useful for clusters with a lot of ml training jobs &amp; model servers /!/ just wanted to share a cool new open-source -----> tool !!!  i built to\u00a0**significantly reduce kubernetes compute costs**, by finding the cheapest machines that successfully run your workloads.\n\nit's designed for clusters with a lot of ml training jobs &amp; model servers - these can get really expensive (especially if you use gpus).\n\ncheck it out: [https://github.com/aporia-ai/kubesurvival](https://github.com/aporia-ai/kubesurvival)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/o3c7p0/p_kubesurvival_easy_k8s_cost_optimization_useful/',)", "identifyer": 5739273, "year": "2021"}, {"autor": "cluhedos", "date": 1624659134000, "content": "[D] Anime face recognotion 'beauty' rating tool? /!/ Does anyone know what tool might have been used for the youtube channel (see below) to rate the anime characters? I have tried to look for the mentioned 'Hotiis' and 'Beautiis' but couldn't find anything. Thanks for every reply.\n\n[https://www.youtube.com/user/ShinSaikan/videos](https://www.youtube.com/user/ShinSaikan/videos) ?", "link": "https://www.reddit.com/r/MachineLearning/comments/o7xtrp/d_anime_face_recognotion_beauty_rating_tool/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] anime face recognotion 'beauty' rating -----> tool !!! ? /!/ does anyone know what tool might have been used for the youtube channel (see below) to rate the anime characters? i have tried to look for the mentioned 'hotiis' and 'beautiis' but couldn't find anything. thanks for every reply.\n\n[https://www.youtube.com/user/shinsaikan/videos](https://www.youtube.com/user/shinsaikan/videos) ?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/o7xtrp/d_anime_face_recognotion_beauty_rating_tool/',)", "identifyer": 5739379, "year": "2021"}, {"autor": "toby__bryant", "date": 1628083385000, "content": "[P] Clean your image data 35x cheaper using Confident Learning /!/ Hey everyone, Tobias from [hasty.ai](http://hasty.ai/) here.\n\nWe just launched our [Error Finder](https://hasty.ai/annotation/quality-assurance/), a tool automating the data QC process using Confident Learning \u2014 We'd love to hear your thoughts and feedback! \n\n**Aim**\n\nAt [hasty.ai](http://hasty.ai/), we're working on agile ML tooling for vision AI to help our users get to production more reliably. A huge part of this is to automate and speed the data preparation process. \n\nWe were already able to automate 85-95% of the initial labeling process. Now, it's time to tackle the next big issue: quality control for your data. Up to 70% of all work to create a data asset is spent on it, and so far, it has been a manual, highly redundant process.\n\n**Problem**\n\nThe golden standard in this field is [consensus scoring](https://medium.com/hackernoon/how-to-measure-quality-when-training-machine-learning-models-cc9196dd377a). The basic idea is to have multiple annotators labeling the same image. If the annotators did similar annotations, you can be secure in being aligned and providing quality data.\n\nWhereas this gives robust results, it's a lot of redundant manual work blowing up overall project time and costs. \n\n**Solution**\n\nWe automated the process by implementing and further developing ideas on Confident Learning developed by [Northcutt et al. (2019)](https://arxiv.org/abs/1911.00068#:~:text=Confident%20learning%20(CL)%20is%20an,examples%20to%20train%20with%20confidence)\u2014special shoutouts here! \n\nThe result is a cost reduction by 35x compared to doing consensus scoring with an established tool like SageMaker. *Wait what? 35X?* Check out [this blog post](https://medium.com/hasty-ai/the-first-end-to-end-vision-ai-platform-with-truly-automated-data-cleaning-capabilities-ac647be07d13) to see that this is not the typical marketing BS you see but that we can back the number up.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/198u0cl4ecf71.png?width=1950&amp;format=png&amp;auto=webp&amp;s=0bfb6795fa409eac4749278bd8762bcf9388d5d7\n\nIn [the blog post](https://medium.com/hasty-ai/the-first-end-to-end-vision-ai-platform-with-truly-automated-data-cleaning-capabilities-ac647be07d13), we also explain a bit about how the tool works. \n\n**Try it** \n\nThe tool is now live for every Hasty user. You can create an account and try it today\u2014no strings attached. Every new user in Hasty gets $70 worth of credits. It's also possible to import data that you labeled somewhere else. If you have any questions, feedback, or get stuck, feel free to email me at tobias@hasty.ai.", "link": "https://www.reddit.com/r/MachineLearning/comments/oxsl9u/p_clean_your_image_data_35x_cheaper_using/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] clean your image data 35x cheaper using confident learning /!/ hey everyone, tobias from [hasty.ai](http://hasty.ai/) here.\n\nwe just launched our [error finder](https://hasty.ai/annotation/quality-assurance/), a -----> tool !!!  automating the data qc process using confident learning \u2014 we'd love to hear your thoughts and feedback! \n\n**aim**\n\nat [hasty.ai](http://hasty.ai/), we're working on agile ml tooling for vision ai to help our users get to production more reliably. a huge part of this is to automate and speed the data preparation process. \n\nwe were already able to automate 85-95% of the initial labeling process. now, it's time to tackle the next big issue: quality control for your data. up to 70% of all work to create a data asset is spent on it, and so far, it has been a manual, highly redundant process.\n\n**problem**\n\nthe golden standard in this field is [consensus scoring](https://medium.com/hackernoon/how-to-measure-quality-when-training-machine-learning-models-cc9196dd377a). the basic idea is to have multiple annotators labeling the same image. if the annotators did similar annotations, you can be secure in being aligned and providing quality data.\n\nwhereas this gives robust results, it's a lot of redundant manual work blowing up overall project time and costs. \n\n**solution**\n\nwe automated the process by implementing and further developing ideas on confident learning developed by [northcutt et al. (2019)](https://arxiv.org/abs/1911.00068#:~:text=confident%20learning%20(cl)%20is%20an,examples%20to%20train%20with%20confidence)\u2014special shoutouts here! \n\nthe result is a cost reduction by 35x compared to doing consensus scoring with an established tool like sagemaker. *wait what? 35x?* check out [this blog post](https://medium.com/hasty-ai/the-first-end-to-end-vision-ai-platform-with-truly-automated-data-cleaning-capabilities-ac647be07d13) to see that this is not the typical marketing bs you see but that we can back the number up.\n\n&amp;#x200b;\n\nhttps://preview.redd.it/198u0cl4ecf71.png?width=1950&amp;format=png&amp;auto=webp&amp;s=0bfb6795fa409eac4749278bd8762bcf9388d5d7\n\nin [the blog post](https://medium.com/hasty-ai/the-first-end-to-end-vision-ai-platform-with-truly-automated-data-cleaning-capabilities-ac647be07d13), we also explain a bit about how the tool works. \n\n**try it** \n\nthe tool is now live for every hasty user. you can create an account and try it today\u2014no strings attached. every new user in hasty gets $70 worth of credits. it's also possible to import data that you labeled somewhere else. if you have any questions, feedback, or get stuck, feel free to email me at tobias@hasty.ai.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/oxsl9u/p_clean_your_image_data_35x_cheaper_using/',)", "identifyer": 5739934, "year": "2021"}, {"autor": "treebeard_hasty_ai", "date": 1623847208000, "content": "[P] Model Playground: machine vision ML Ops tool /!/ Hey peeps, Tristan here, [hasty.ai](http://hasty.ai/) cofounder.\n\nCheck out our ModelPlayground at: [https://hasty.ai/model-playground/](https://hasty.ai/model-playground/) \u2013 We'd love to hear your thoughts and feedback!\n\n**Aim**\n\nWe are building a smooth transition from our annotation tool to MLOps for data-centric ML, as you cannot know *a priori* where improvements in performance will come from.\n\nWe believe in building a good process for ML dev ([https://www.youtube.com/watch?v=06-AZXmwHjo](https://www.youtube.com/watch?v=06-AZXmwHjo)), rather than chasing SOTA. And we hate the ML Frankensuite.\n\n**Problem**\n\nWhen starting with Hasty no one talked about data-centric ML, we had to learn our lessons the hard way. Kostya, Alex\u2014my co-founders\u2014, and I worked on several projects in the German industrial space and we felt the burn.\n\nGiven our domain, standard approaches to jump-start learning with unsupervised or semi-supervised learning, synthetic data, GANs, pre-trained models \\[inset random technique here\\], etc. weren't cutting it. So, we ended up spending evenings labeling data.\n\n*There had to be a better way!*\n\n**Solution**\n\nFirst, we build an annotation tool that scales up custom-trained models for annotation automation and QA \u2013 that is working well.\n\nNOW, we are launching MP for you to be able to experiment, prep, and deploy models. Bringing the worlds of ML Ops and Annotation together.", "link": "https://www.reddit.com/r/MachineLearning/comments/o13x1k/p_model_playground_machine_vision_ml_ops_tool/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] model playground: machine vision ml ops -----> tool !!!  /!/ hey peeps, tristan here, [hasty.ai](http://hasty.ai/) cofounder.\n\ncheck out our modelplayground at: [https://hasty.ai/model-playground/](https://hasty.ai/model-playground/) \u2013 we'd love to hear your thoughts and feedback!\n\n**aim**\n\nwe are building a smooth transition from our annotation tool to mlops for data-centric ml, as you cannot know *a priori* where improvements in performance will come from.\n\nwe believe in building a good process for ml dev ([https://www.youtube.com/watch?v=06-azxmwhjo](https://www.youtube.com/watch?v=06-azxmwhjo)), rather than chasing sota. and we hate the ml frankensuite.\n\n**problem**\n\nwhen starting with hasty no one talked about data-centric ml, we had to learn our lessons the hard way. kostya, alex\u2014my co-founders\u2014, and i worked on several projects in the german industrial space and we felt the burn.\n\ngiven our domain, standard approaches to jump-start learning with unsupervised or semi-supervised learning, synthetic data, gans, pre-trained models \\[inset random technique here\\], etc. weren't cutting it. so, we ended up spending evenings labeling data.\n\n*there had to be a better way!*\n\n**solution**\n\nfirst, we build an annotation tool that scales up custom-trained models for annotation automation and qa \u2013 that is working well.\n\nnow, we are launching mp for you to be able to experiment, prep, and deploy models. bringing the worlds of ml ops and annotation together.", "sortedWord": "None", "removed": "('nan',)", "score": 74, "comments": 31, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/o13x1k/p_model_playground_machine_vision_ml_ops_tool/',)", "identifyer": 5740277, "year": "2021"}, {"autor": "matt_paraphrasetool", "date": 1631729243000, "content": "[P] Made a paraphrase tool using Google's T5, Quora Question Pairs, and DistilBert Classifiers to separate outputs into 15 modes. What other approaches have you seen taken? /!/ [link](https://paraphrasetool.com)", "link": "https://www.reddit.com/r/MachineLearning/comments/povj1l/p_made_a_paraphrase_tool_using_googles_t5_quora/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] made a paraphrase -----> tool !!!  using google's t5, quora question pairs, and distilbert classifiers to separate outputs into 15 modes. what other approaches have you seen taken? /!/ [link](https://paraphrasetool.com)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/povj1l/p_made_a_paraphrase_tool_using_googles_t5_quora/',)", "identifyer": 5740373, "year": "2021"}, {"autor": "matt_paraphrasetool", "date": 1631718267000, "content": "Made a paraphrase tool using Google's T5, Quora Question Pairs, and DistilBert Classifiers to separate the 15 modes. What other approaches have you seen taken? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/porwsc/made_a_paraphrase_tool_using_googles_t5_quora/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "made a paraphrase -----> tool !!!  using google's t5, quora question pairs, and distilbert classifiers to separate the 15 modes. what other approaches have you seen taken? /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/porwsc/made_a_paraphrase_tool_using_googles_t5_quora/',)", "identifyer": 5740391, "year": "2021"}, {"autor": "LoudCup7182", "date": 1632475092000, "content": "[D] Best web framework in 2021 for Data Science Apps /!/ Hello guys!\n\nAfter several years leading the AI initiatives in a big corporate, I am thinking about founding a consulting company on AI &amp; Data Science.\n\nI am working on finding the best Tech Stack to deploy my models in simple but beautiful web applications for my future clients (hopefully!). In my experience, most of Data Science projects for corporate are basically dashboards that incorporate ML, and I am seriously considering Plotly Dash for Enterprise.\n\nI am much more a Data Scientist/Mathematician than a Software Engineer, and Dash seems a serious tool to allow me to focus on the modeling part, and deploy web apps easily.\n\nIn your experience, is it the case? Or do you think it's better to have teams of Full stack Software Engineers to manage all the Software part?\n\nThanks a lot for your help guys!", "link": "https://www.reddit.com/r/MachineLearning/comments/pug2tt/d_best_web_framework_in_2021_for_data_science_apps/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] best web framework in 2021 for data science apps /!/ hello guys!\n\nafter several years leading the ai initiatives in a big corporate, i am thinking about founding a consulting company on ai &amp; data science.\n\ni am working on finding the best tech stack to deploy my models in simple but beautiful web applications for my future clients (hopefully!). in my experience, most of data science projects for corporate are basically dashboards that incorporate ml, and i am seriously considering plotly dash for enterprise.\n\ni am much more a data scientist/mathematician than a software engineer, and dash seems a serious -----> tool !!!  to allow me to focus on the modeling part, and deploy web apps easily.\n\nin your experience, is it the case? or do you think it's better to have teams of full stack software engineers to manage all the software part?\n\nthanks a lot for your help guys!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/pug2tt/d_best_web_framework_in_2021_for_data_science_apps/',)", "identifyer": 5740447, "year": "2021"}, {"autor": "andrewthehockeyguy9", "date": 1615300733000, "content": "[Research] Photoshop\u2019s AI neural filters can tweak age and expression with a few clicks /!/ Artificial intelligence is changing the world of image editing and manipulation, and Adobe doesn\u2019t want to be left behind. Today, the company is releasing an update to Photoshop version 22.0 that comes with a host of AI-powered features, some new, some already shared with the public. These include a sky replacement tool, improved AI edge selection, and \u2014 the star of the show \u2014 a suite of image-editing tools that Adobe calls \u201cneural filters.\u201d\n\nThese filters include a number of simple overlays and effects but also tools that allow for deeper edits, particularly to portraits. With neural filters, Photoshop can adjust a subject\u2019s age and facial expression, amplifying or reducing feelings like \u201cjoy,\u201d \u201csurprise,\u201d or \u201canger\u201d with simple sliders. You can remove someone\u2019s glasses or smooth out their spots. One of the weirder filters even lets you transfer makeup from one person to another. And it\u2019s all done in just a few clicks, with the output easily tweaked or reversed entirely.\n\n\u201cThis is where I feel we can now say that Photoshop is the world\u2019s most advanced AI application,\u201d Maria Yap, Adobe\u2019s vice president of digital imaging told *The Verge*. \u201cWe\u2019re creating things in images that weren\u2019t there before.\u201d\n\nTo achieve these effects, Adobe is harnessing the power of generative adversarial networks \u2014 or GANs \u2014 a type of machine learning technique that\u2019s proved particularly adept at generating visual imagery. Some of the processing is done locally and some in the cloud, depending on the computational demands of each individual tool, but each filter takes just seconds to apply. (The demo we saw was done on an old Mac Book Pro and was perfectly fast enough.)\n\nMany of these filters are familiar to those who follow AI image editing. They\u2019re the sort of tools that have been turning up in papers and demos for years. But it\u2019s always significant when techniques like these go from bleeding-edge experiments, shared on Twitter among those in the know, to headline features in consumer juggernauts like Photoshop.\n\nAs always with these sorts of features, the proof will be in the editing, and the actual utility of neural filters will depend on how Photoshop\u2019s many users react to them. But in a virtual demo *The Verge* saw, the new tools delivered fast and good quality results (though we didn\u2019t see the facial expression adjustment tool). These AI-powered edits weren\u2019t flawless, and most professional retouchers would want to step in and make some adjustments of their own afterwards, but they seemed like they would speed up many editing tasks.", "link": "https://www.reddit.com/r/MachineLearning/comments/m17pkb/research_photoshops_ai_neural_filters_can_tweak/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[research] photoshop\u2019s ai neural filters can tweak age and expression with a few clicks /!/ artificial intelligence is changing the world of image editing and manipulation, and adobe doesn\u2019t want to be left behind. today, the company is releasing an update to photoshop version 22.0 that comes with a host of ai-powered features, some new, some already shared with the public. these include a sky replacement -----> tool !!! , improved ai edge selection, and \u2014 the star of the show \u2014 a suite of image-editing tools that adobe calls \u201cneural filters.\u201d\n\nthese filters include a number of simple overlays and effects but also tools that allow for deeper edits, particularly to portraits. with neural filters, photoshop can adjust a subject\u2019s age and facial expression, amplifying or reducing feelings like \u201cjoy,\u201d \u201csurprise,\u201d or \u201canger\u201d with simple sliders. you can remove someone\u2019s glasses or smooth out their spots. one of the weirder filters even lets you transfer makeup from one person to another. and it\u2019s all done in just a few clicks, with the output easily tweaked or reversed entirely.\n\n\u201cthis is where i feel we can now say that photoshop is the world\u2019s most advanced ai application,\u201d maria yap, adobe\u2019s vice president of digital imaging told *the verge*. \u201cwe\u2019re creating things in images that weren\u2019t there before.\u201d\n\nto achieve these effects, adobe is harnessing the power of generative adversarial networks \u2014 or gans \u2014 a type of machine learning technique that\u2019s proved particularly adept at generating visual imagery. some of the processing is done locally and some in the cloud, depending on the computational demands of each individual tool, but each filter takes just seconds to apply. (the demo we saw was done on an old mac book pro and was perfectly fast enough.)\n\nmany of these filters are familiar to those who follow ai image editing. they\u2019re the sort of tools that have been turning up in papers and demos for years. but it\u2019s always significant when techniques like these go from bleeding-edge experiments, shared on twitter among those in the know, to headline features in consumer juggernauts like photoshop.\n\nas always with these sorts of features, the proof will be in the editing, and the actual utility of neural filters will depend on how photoshop\u2019s many users react to them. but in a virtual demo *the verge* saw, the new tools delivered fast and good quality results (though we didn\u2019t see the facial expression adjustment tool). these ai-powered edits weren\u2019t flawless, and most professional retouchers would want to step in and make some adjustments of their own afterwards, but they seemed like they would speed up many editing tasks.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/m17pkb/research_photoshops_ai_neural_filters_can_tweak/',)", "identifyer": 5740580, "year": "2021"}, {"autor": "natavk", "date": 1617342371000, "content": "How to increase image resolution? [P] /!/ I've been trying an image enhancement tool to increase image resolution. But instead of making the image crispy, it just increased the number of pixels, while keeping the image blurry. Is there any chance to really enhance an image quality?", "link": "https://www.reddit.com/r/MachineLearning/comments/midzgt/how_to_increase_image_resolution_p/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "how to increase image resolution? [p] /!/ i've been trying an image enhancement -----> tool !!!  to increase image resolution. but instead of making the image crispy, it just increased the number of pixels, while keeping the image blurry. is there any chance to really enhance an image quality?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/midzgt/how_to_increase_image_resolution_p/',)", "identifyer": 5740602, "year": "2021"}, {"autor": "zr0_day", "date": 1619392479000, "content": "[R] NLP for privacy policy /!/ Hi all, do you know any open source NLP tool or model to analyse privacy policy texts?\nI haven't found any, nor some policies datasets who might have been useful as well.\n\nMy aim is to extract the main entities from a privacy policy and highlight the sentences which are containing such entities. \n\nI'm not an NLP expert, so any help would be much appreciated.", "link": "https://www.reddit.com/r/MachineLearning/comments/myk66l/r_nlp_for_privacy_policy/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[r] nlp for privacy policy /!/ hi all, do you know any open source nlp -----> tool !!!  or model to analyse privacy policy texts?\ni haven't found any, nor some policies datasets who might have been useful as well.\n\nmy aim is to extract the main entities from a privacy policy and highlight the sentences which are containing such entities. \n\ni'm not an nlp expert, so any help would be much appreciated.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/myk66l/r_nlp_for_privacy_policy/',)", "identifyer": 5740669, "year": "2021"}, {"autor": "techsucker", "date": 1619392197000, "content": "[R] Scientists From The Max Planck Florida Institute For Neuroscience (MPFI) Have Developed \u2018Gold Digger\u2019, A Software Tool That Uses A Modified PIX2PIX Deep Learning Network /!/ Electron microscopy (EM) is a method used for high-resolution images of biological and non-biological samples. It requires precise and time-consuming steps from sample preparation to image acquisition to produce the clarity and details required to visualize small cell structures with high resolution. Additionally, extracting the biological information out of EM-created images is a very laborious and time-intensive task. This is because the current EM analysis software usually requires the skilled eye to examine hundreds of pictures manually.\n\nA team of scientists from the Max Planck Florida Institute for Neuroscience (MPFI) has applied neural networks to create a novel analysis software \u2018Gold Digger,\u2019 aimed at streamlining part of the lengthy process. [Diego Jerez and Eleanor Stuart, two high school data science students](https://techxplore.com/news/2021-04-gold-digger-neural-networks-nexus.html), started working on this project out of curiosity. But later, it turned into a more complex and interdisciplinary project.\n\nSummary: [https://www.marktechpost.com/2021/04/25/scientists-from-the-max-planck-florida-institute-for-neuroscience-mpfi-have-developed-gold-digger-a-software-tool-that-uses-a-modified-pix2pix-deep-learning-network/](https://www.marktechpost.com/2021/04/25/scientists-from-the-max-planck-florida-institute-for-neuroscience-mpfi-have-developed-gold-digger-a-software-tool-that-uses-a-modified-pix2pix-deep-learning-network/) \n\nPaper: [https://www.nature.com/articles/s41598-021-87015-2](https://www.nature.com/articles/s41598-021-87015-2)", "link": "https://www.reddit.com/r/MachineLearning/comments/myk387/r_scientists_from_the_max_planck_florida/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[r] scientists from the max planck florida institute for neuroscience (mpfi) have developed \u2018gold digger\u2019, a software -----> tool !!!  that uses a modified pix2pix deep learning network /!/ electron microscopy (em) is a method used for high-resolution images of biological and non-biological samples. it requires precise and time-consuming steps from sample preparation to image acquisition to produce the clarity and details required to visualize small cell structures with high resolution. additionally, extracting the biological information out of em-created images is a very laborious and time-intensive task. this is because the current em analysis software usually requires the skilled eye to examine hundreds of pictures manually.\n\na team of scientists from the max planck florida institute for neuroscience (mpfi) has applied neural networks to create a novel analysis software \u2018gold digger,\u2019 aimed at streamlining part of the lengthy process. [diego jerez and eleanor stuart, two high school data science students](https://techxplore.com/news/2021-04-gold-digger-neural-networks-nexus.html), started working on this project out of curiosity. but later, it turned into a more complex and interdisciplinary project.\n\nsummary: [https://www.marktechpost.com/2021/04/25/scientists-from-the-max-planck-florida-institute-for-neuroscience-mpfi-have-developed-gold-digger-a-software-tool-that-uses-a-modified-pix2pix-deep-learning-network/](https://www.marktechpost.com/2021/04/25/scientists-from-the-max-planck-florida-institute-for-neuroscience-mpfi-have-developed-gold-digger-a-software-tool-that-uses-a-modified-pix2pix-deep-learning-network/) \n\npaper: [https://www.nature.com/articles/s41598-021-87015-2](https://www.nature.com/articles/s41598-021-87015-2)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/myk387/r_scientists_from_the_max_planck_florida/',)", "identifyer": 5740670, "year": "2021"}, {"autor": "Vexac6", "date": 1621251055000, "content": "[P] Magic Cards classifier in streaming /!/ Hi guys, first post here and also first serious project with ML :)\n\nMy master degree's thesis consists in a tool that detects and classifies *Magic: the Gathering* cards during a tabletop streaming (no online games). MtG official tournaments have some streaming procedures to follow, like a top-view camera, card positioning on the table, a certain lighting etc that can help the machine a lot by avoiding harsh conditions.\n\n**Main Issue:** There are around **20.000** different cards, and some of them have more than 1 image representing em (alternative artworks or frames). Is classification with that number of classes even doable in a reasonable training time?\n\n**How to react:** I could accept training on a smaller subset (like maybe the newest cards), but I would like to create something useful, and not just a demonstrative project. So for now I'm adopting a predictive pipeline to reduce the problem complexity and dispatch the classification to one of many networks trained on a specific different subsets of cards. This should be the standard for face recognition, so maybe it's the right answer. With that, every network should work \"only\" on \\~1000 classes (with **very few datapoints per class**, sadly), and I don't know if it's the right path to follow.\n\nDo you think there's a better approach to this kind of problem?\n\nP.S. Macro-features are recognized with a \\~99% accuracy, so they're not a problem at all.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/3dzrzfup0oz61.png?width=1862&amp;format=png&amp;auto=webp&amp;s=89b3adf4539baca8fbfa54af62233b78a572c6ab", "link": "https://www.reddit.com/r/MachineLearning/comments/nedrar/p_magic_cards_classifier_in_streaming/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] magic cards classifier in streaming /!/ hi guys, first post here and also first serious project with ml :)\n\nmy master degree's thesis consists in a -----> tool !!!  that detects and classifies *magic: the gathering* cards during a tabletop streaming (no online games). mtg official tournaments have some streaming procedures to follow, like a top-view camera, card positioning on the table, a certain lighting etc that can help the machine a lot by avoiding harsh conditions.\n\n**main issue:** there are around **20.000** different cards, and some of them have more than 1 image representing em (alternative artworks or frames). is classification with that number of classes even doable in a reasonable training time?\n\n**how to react:** i could accept training on a smaller subset (like maybe the newest cards), but i would like to create something useful, and not just a demonstrative project. so for now i'm adopting a predictive pipeline to reduce the problem complexity and dispatch the classification to one of many networks trained on a specific different subsets of cards. this should be the standard for face recognition, so maybe it's the right answer. with that, every network should work \"only\" on \\~1000 classes (with **very few datapoints per class**, sadly), and i don't know if it's the right path to follow.\n\ndo you think there's a better approach to this kind of problem?\n\np.s. macro-features are recognized with a \\~99% accuracy, so they're not a problem at all.\n\n&amp;#x200b;\n\nhttps://preview.redd.it/3dzrzfup0oz61.png?width=1862&amp;format=png&amp;auto=webp&amp;s=89b3adf4539baca8fbfa54af62233b78a572c6ab", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 15, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/nedrar/p_magic_cards_classifier_in_streaming/',)", "identifyer": 5740845, "year": "2021"}, {"autor": "xela-sedinnaoi", "date": 1621248138000, "content": "[P] [D] How are you approaching prediction uncertainty in ML systems? /!/ Most models return a point estimate of some sort, regardless of the task. In some situations (e.g. finance and risk management), the uncertainty in the prediction is just as important as the prediction itself. How are people dealing with these scenarios?\n\nI usually turn to generative models - e.g. probabilistic programs and Bayesian inference. I\u2019ve written-up [my thoughts](https://www.bodyworkml.com/posts/serving-uncertainty) on how to engineer these into a \u2018production system\u2019 deployed to Kubernetes, using PyMC and [Bodywork](https://github.com/bodywork-ml/bodywork-core) (an open-source ML deployment tool that I contribute to).\n\nGiven the simulation-based nature of generative models, perhaps it\u2019s unsurprising that the resulting system is a little slow. I\u2019d be really interested to get some feedback on the approach or hear about alternatives!", "link": "https://www.reddit.com/r/MachineLearning/comments/necuzi/p_d_how_are_you_approaching_prediction/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] [d] how are you approaching prediction uncertainty in ml systems? /!/ most models return a point estimate of some sort, regardless of the task. in some situations (e.g. finance and risk management), the uncertainty in the prediction is just as important as the prediction itself. how are people dealing with these scenarios?\n\ni usually turn to generative models - e.g. probabilistic programs and bayesian inference. i\u2019ve written-up [my thoughts](https://www.bodyworkml.com/posts/serving-uncertainty) on how to engineer these into a \u2018production system\u2019 deployed to kubernetes, using pymc and [bodywork](https://github.com/bodywork-ml/bodywork-core) (an open-source ml deployment -----> tool !!!  that i contribute to).\n\ngiven the simulation-based nature of generative models, perhaps it\u2019s unsurprising that the resulting system is a little slow. i\u2019d be really interested to get some feedback on the approach or hear about alternatives!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 42, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/necuzi/p_d_how_are_you_approaching_prediction/',)", "identifyer": 5740846, "year": "2021"}, {"autor": "KarlaNour96", "date": 1618579734000, "content": "[R] Ubiai Survey /!/ Hi everyone,\n\nWe are looking on a new text annotation tool for Natural Language Processing ( NLP) application and we are conducting a survey to get feedback from NLP practitioners regarding  the text annotation process.\nIf you have a moment, can you please help us answering few questions ( survey time 1 min ) \n\n https://docs.google.com/forms/d/e/1FAIpQLSd61v-ohbQn8QB_cAFQ0tEFFFwircLbwHps6TZnQuWUJQgZSA/viewform?usp=sf_link\n\nYour help will be greatly appreciated ! \n\nUBIAI team\n\nhttps://ubiai.tools", "link": "https://www.reddit.com/r/MachineLearning/comments/ms35nv/r_ubiai_survey/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[r] ubiai survey /!/ hi everyone,\n\nwe are looking on a new text annotation -----> tool !!!  for natural language processing ( nlp) application and we are conducting a survey to get feedback from nlp practitioners regarding  the text annotation process.\nif you have a moment, can you please help us answering few questions ( survey time 1 min ) \n\n https://docs.google.com/forms/d/e/1faipqlsd61v-ohbqn8qb_cafq0tefffwirclbwhps6tznquwujqgzsa/viewform?usp=sf_link\n\nyour help will be greatly appreciated ! \n\nubiai team\n\nhttps://ubiai.tools", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/ms35nv/r_ubiai_survey/',)", "identifyer": 5740896, "year": "2021"}, {"autor": "gabegabe6", "date": 1618579597000, "content": "[D] What tools can you recommend for GPU resource alocation? /!/ The problem is given: There is a team, and there are GPU servers with &gt;1 GPUs. What is the best way, to signal who uses which GPUs for how long? E.g. I am thinking about a simple tool with which you can say, that from 2 days later, you'll need 5 GPUs, and you \"allocate it\" for that time. Of course, everyone in the team could see this.\n\nI was planning to create such a simple CLI tool, but first wanted to get some feedback, on who uses what for this purpose.\n\n(I really don't want to use excel sheets for this.)", "link": "https://www.reddit.com/r/MachineLearning/comments/ms345v/d_what_tools_can_you_recommend_for_gpu_resource/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] what tools can you recommend for gpu resource alocation? /!/ the problem is given: there is a team, and there are gpu servers with &gt;1 gpus. what is the best way, to signal who uses which gpus for how long? e.g. i am thinking about a simple -----> tool !!!  with which you can say, that from 2 days later, you'll need 5 gpus, and you \"allocate it\" for that time. of course, everyone in the team could see this.\n\ni was planning to create such a simple cli tool, but first wanted to get some feedback, on who uses what for this purpose.\n\n(i really don't want to use excel sheets for this.)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/ms345v/d_what_tools_can_you_recommend_for_gpu_resource/',)", "identifyer": 5740898, "year": "2021"}, {"autor": "frecklebars", "date": 1610954716000, "content": "[P] Machine Learning in a flow-based visual coding environment using Ryven /!/ Hello everyone,\n\nI recently worked on creating a fork of a visual coding tool for python that implements machine learning algorithms with scikit-learn. Its called [Ryven](https://github.com/frecklebars/Ryven), and I managed to use it to create a solution to the Titanic problem Kaggle. I wrote a [fairly detailed notebook about the working process here](https://www.kaggle.com/frecklebars/ml-flow-based-visual-coding-using-ryven). The goal is to make it easier to use data science and machine learning for people with limited coding experience.\n\nDo you think machine learning could be simplified and made more accessible in this way? We would really appreciate feedback on both the tool and the concept (especially in this early stage of the project).\n\nIf you would like to test it, Ryven is available here: [https://github.com/frecklebars/Ryven](https://github.com/frecklebars/Ryven)\n\nAnd the Kaggle notebook solving the Titanic problem here: [https://www.kaggle.com/frecklebars/ml-flow-based-visual-coding-using-ryven](https://www.kaggle.com/frecklebars/ml-flow-based-visual-coding-using-ryven)\n\nIf you have any questions, comments or suggestions about this project, please reach out either on this discussion thread, or directly message them to me [on my Tweeter](https://twitter.com/frecklebars) , @/frecklebars\n\nThank you", "link": "https://www.reddit.com/r/MachineLearning/comments/kzptl7/p_machine_learning_in_a_flowbased_visual_coding/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[p] machine learning in a flow-based visual coding environment using ryven /!/ hello everyone,\n\ni recently worked on creating a fork of a visual coding -----> tool !!!  for python that implements machine learning algorithms with scikit-learn. its called [ryven](https://github.com/frecklebars/ryven), and i managed to use it to create a solution to the titanic problem kaggle. i wrote a [fairly detailed notebook about the working process here](https://www.kaggle.com/frecklebars/ml-flow-based-visual-coding-using-ryven). the goal is to make it easier to use data science and machine learning for people with limited coding experience.\n\ndo you think machine learning could be simplified and made more accessible in this way? we would really appreciate feedback on both the tool and the concept (especially in this early stage of the project).\n\nif you would like to test it, ryven is available here: [https://github.com/frecklebars/ryven](https://github.com/frecklebars/ryven)\n\nand the kaggle notebook solving the titanic problem here: [https://www.kaggle.com/frecklebars/ml-flow-based-visual-coding-using-ryven](https://www.kaggle.com/frecklebars/ml-flow-based-visual-coding-using-ryven)\n\nif you have any questions, comments or suggestions about this project, please reach out either on this discussion thread, or directly message them to me [on my tweeter](https://twitter.com/frecklebars) , @/frecklebars\n\nthank you", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 16, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/kzptl7/p_machine_learning_in_a_flowbased_visual_coding/',)", "identifyer": 5740930, "year": "2021"}, {"autor": "NotVector", "date": 1610951150000, "content": "[D] How can I get introduced to machine learning as a high school student? /!/ I like coding and computer science and I think AI and machine learning is an interesting field and I would like to get immersed in it a bit to see what it's like.\n\nIs TensorFlow a good tool for ML? And are there like courses or tutorials that are good (maybe edX)?", "link": "https://www.reddit.com/r/MachineLearning/comments/kzoyhu/d_how_can_i_get_introduced_to_machine_learning_as/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] how can i get introduced to machine learning as a high school student? /!/ i like coding and computer science and i think ai and machine learning is an interesting field and i would like to get immersed in it a bit to see what it's like.\n\nis tensorflow a good -----> tool !!!  for ml? and are there like courses or tutorials that are good (maybe edx)?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 7, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/kzoyhu/d_how_can_i_get_introduced_to_machine_learning_as/',)", "identifyer": 5740932, "year": "2021"}, {"autor": "TheGopherBro", "date": 1622042033000, "content": "[Project] I made AI based CV/Resume Summary Generator for my girlfriend /!/ Hey, y'all I hope you are doing well and staying safe.\n\nMy girlfriend was really struggled to create a good Resume Summary. I thought it would be really cool to make an AI tool that can help her to write a compelling summary using few keywords. So, I built the tool. She loves it! and I figured you might like it too.\n\nCheck it out here [To write your Resume summary by AI](https://copywriterpro.ai/)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/orpoa9b5eh171.png?width=934&amp;format=png&amp;auto=webp&amp;s=78e2ccbbab65a27017e4a89442b9c29ef9eadb05\n\nhttps://preview.redd.it/ddgn5ab5eh171.png?width=934&amp;format=png&amp;auto=webp&amp;s=5504ebb4637d1160aac74fa695081f711747b02d", "link": "https://www.reddit.com/r/MachineLearning/comments/nljego/project_i_made_ai_based_cvresume_summary/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[project] i made ai based cv/resume summary generator for my girlfriend /!/ hey, y'all i hope you are doing well and staying safe.\n\nmy girlfriend was really struggled to create a good resume summary. i thought it would be really cool to make an ai -----> tool !!!  that can help her to write a compelling summary using few keywords. so, i built the tool. she loves it! and i figured you might like it too.\n\ncheck it out here [to write your resume summary by ai](https://copywriterpro.ai/)\n\n&amp;#x200b;\n\nhttps://preview.redd.it/orpoa9b5eh171.png?width=934&amp;format=png&amp;auto=webp&amp;s=78e2ccbbab65a27017e4a89442b9c29ef9eadb05\n\nhttps://preview.redd.it/ddgn5ab5eh171.png?width=934&amp;format=png&amp;auto=webp&amp;s=5504ebb4637d1160aac74fa695081f711747b02d", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/nljego/project_i_made_ai_based_cvresume_summary/',)", "identifyer": 5741045, "year": "2021"}, {"autor": "RepresentativeCod613", "date": 1622032741000, "content": "[D] Version Control with DVC - High-Level Usage /!/ Hey All!\n\nI'm a data scientist at DAGsHub (one of the reasons I'm wearing a T-shirt with our HUGE logo \ud83d\ude06), and I recently started working with DVC. It's a really cool open-source tool for versioning large files.\n\nGetting to know the workflow of DVC was a bit complicated for me. I felt I needed a high-level explanation of how to use it with Git, what are the '.dvc' pointer files and how I can retrieve old versions of the files tracked by DVC.\n\nAfter gaining hands-on experience working with DVC, it became an integral part of my projects. It helps me reproduce experiments easily, create a clear structure to my project and run the entire project pipeline in one command.\n\nTo help others where I struggled, I decided to [review the high-level usage of Git &amp; DVC - with NO CODE](https://www.youtube.com/watch?v=QYNgWWearZ4). In the video, I explain what DVC is, how it can help us as data scientists, and the high-level usage of Git &amp; DVC.\n\nI would love to hear your thoughts!", "link": "https://www.reddit.com/r/MachineLearning/comments/nlg070/d_version_control_with_dvc_highlevel_usage/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "[d] version control with dvc - high-level usage /!/ hey all!\n\ni'm a data scientist at dagshub (one of the reasons i'm wearing a t-shirt with our huge logo \ud83d\ude06), and i recently started working with dvc. it's a really cool open-source -----> tool !!!  for versioning large files.\n\ngetting to know the workflow of dvc was a bit complicated for me. i felt i needed a high-level explanation of how to use it with git, what are the '.dvc' pointer files and how i can retrieve old versions of the files tracked by dvc.\n\nafter gaining hands-on experience working with dvc, it became an integral part of my projects. it helps me reproduce experiments easily, create a clear structure to my project and run the entire project pipeline in one command.\n\nto help others where i struggled, i decided to [review the high-level usage of git &amp; dvc - with no code](https://www.youtube.com/watch?v=qyngwwearz4). in the video, i explain what dvc is, how it can help us as data scientists, and the high-level usage of git &amp; dvc.\n\ni would love to hear your thoughts!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/nlg070/d_version_control_with_dvc_highlevel_usage/',)", "identifyer": 5741058, "year": "2021"}], "name": "toolMachineLearning2021"}
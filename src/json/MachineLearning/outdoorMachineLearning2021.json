{"interestingcomments": [{"autor": "alexk_wong", "date": 1634147184000, "content": "[R] ICCV2021 oral paper -- Unsupervised Depth Completion with Calibrated Backprojection Layers improves generalization across sensor platforms /!/ Our work \"Unsupervised Depth Completion with Calibrated Backprojection Layers\" has been accepted as an oral paper at ICCV 2021! We will be giving our talk during Session 10 (10/13 2-3 pm PST / 5-6 pm EST and 10/15 7-8 am PST / 10-11 am EST, https://www.eventscribe.net/2021/ICCV/fsPopup.asp?efp=WlJFS0tHTEMxNTgzMA%20&amp;PosterID=428697%20&amp;rnd=0.4100732&amp;mode=posterinfo). This is joint work with Stefano Soatto at the UCLA Vision Lab.\n\nIn a nutshell: we propose a method for point cloud densification (from camera, IMU, range sensor) that can generalize well across different sensor platforms. The figure in this link  illustrates our improvement over existing works: https://github.com/alexklwong/calibrated-backprojection-network/blob/master/figures/overview_teaser.gif\n\nThe slightly longer version: previous methods, when trained on one sensor platform, have problem generalizing to different ones when deployed to the wild. This is because they are overfitted to the sensors used to collect the training set. Our method takes image, sparse point cloud and camera calibration as input, which allows us to use a different calibration at test time. This significantly improves generalization to novel scenes captured by sensors different than those used during training. Amongst our innovations is a \"calibrated backprojection layer\" that imposes strong inductive bias on the network (as opposed trying to learn everything from the data). This design allows our method to achieve the state of the art on both indoor and outdoor scenarios while using a smaller model size and boasting a faster inference time.\n\nFor those interested, here are the links to\npaper: https://arxiv.org/pdf/2108.10531.pdf\ncode (pytorch): https://github.com/alexklwong/calibrated-backprojection-network", "link": "https://www.reddit.com/r/MachineLearning/comments/q7gisc/r_iccv2021_oral_paper_unsupervised_depth/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "outdoor", "selectorShort": "outdoor", "MarkedSent": "[r] iccv2021 oral paper -- unsupervised depth completion with calibrated backprojection layers improves generalization across sensor platforms /!/ our work \"unsupervised depth completion with calibrated backprojection layers\" has been accepted as an oral paper at iccv 2021! we will be giving our talk during session 10 (10/13 2-3 pm pst / 5-6 pm est and 10/15 7-8 am pst / 10-11 am est, https://www.eventscribe.net/2021/iccv/fspopup.asp?efp=wljfs0thtemxntgzma%20&amp;posterid=428697%20&amp;rnd=0.4100732&amp;mode=posterinfo). this is joint work with stefano soatto at the ucla vision lab.\n\nin a nutshell: we propose a method for point cloud densification (from camera, imu, range sensor) that can generalize well across different sensor platforms. the figure in this link  illustrates our improvement over existing works: https://github.com/alexklwong/calibrated-backprojection-network/blob/master/figures/overview_teaser.gif\n\nthe slightly longer version: previous methods, when trained on one sensor platform, have problem generalizing to different ones when deployed to the wild. this is because they are overfitted to the sensors used to collect the training set. our method takes image, sparse point cloud and camera calibration as input, which allows us to use a different calibration at test time. this significantly improves generalization to novel scenes captured by sensors different than those used during training. amongst our innovations is a \"calibrated backprojection layer\" that imposes strong inductive bias on the network (as opposed trying to learn everything from the data). this design allows our method to achieve the state of the art on both indoor and -----> outdoor !!!  scenarios while using a smaller model size and boasting a faster inference time.\n\nfor those interested, here are the links to\npaper: https://arxiv.org/pdf/2108.10531.pdf\ncode (pytorch): https://github.com/alexklwong/calibrated-backprojection-network", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/q7gisc/r_iccv2021_oral_paper_unsupervised_depth/',)", "identifyer": 5727806, "year": "2021"}, {"autor": "papersCruncher3528", "date": 1614758310000, "content": "[R] Generative Adversarial Transformers (2103.01209) /!/ **Link:** [https://arxiv.org/abs/2103.01209](https://arxiv.org/abs/2103.01209)  \n**PDF:** [https://arxiv.org/pdf/2103.01209.pdf](https://arxiv.org/pdf/2103.01209.pdf)  \n**Github:** [https://github.com/dorarad/gansformer](https://github.com/dorarad/gansformer)\n\n**Abstract:** We introduce the GANsformer, a novel and efficient type of transformer, and explore it for the task of visual generative modeling. The network employs a bipartite structure that enables long-range interactions across the image, while maintaining computation of linearly efficiency, that can readily scale to high-resolution synthesis. It iteratively propagates information from a set of latent variables to the evolving visual features and vice versa, to support the refinement of each in light of the other and encourage the emergence of compositional representations of objects and scenes. In contrast to the classic transformer architecture, it utilizes multiplicative integration that allows flexible region-based modulation, and can thus be seen as a generalization of the successful StyleGAN network. We demonstrate the model's strength and robustness through a careful evaluation over a range of datasets, from simulated multi-object environments to rich real-world indoor and outdoor scenes, showing it achieves state-of-the-art results in terms of image quality and diversity, while enjoying fast learning and better data-efficiency. Further qualitative and quantitative experiments offer us an insight into the model's inner workings, revealing improved interpretability and stronger disentanglement, and illustrating the benefits and efficacy of our approach. An implementation of the model is available at [https://github.com/dorarad/gansformer](https://github.com/dorarad/gansformer).  \n\n\nhttps://preview.redd.it/a25l6rs9prk61.png?width=945&amp;format=png&amp;auto=webp&amp;s=927b31fdd81c459e3bfb51d4429c7409307ea721", "link": "https://www.reddit.com/r/MachineLearning/comments/lwoj5i/r_generative_adversarial_transformers_210301209/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "outdoor", "selectorShort": "outdoor", "MarkedSent": "[r] generative adversarial transformers (2103.01209) /!/ **link:** [https://arxiv.org/abs/2103.01209](https://arxiv.org/abs/2103.01209)  \n**pdf:** [https://arxiv.org/pdf/2103.01209.pdf](https://arxiv.org/pdf/2103.01209.pdf)  \n**github:** [https://github.com/dorarad/gansformer](https://github.com/dorarad/gansformer)\n\n**abstract:** we introduce the gansformer, a novel and efficient type of transformer, and explore it for the task of visual generative modeling. the network employs a bipartite structure that enables long-range interactions across the image, while maintaining computation of linearly efficiency, that can readily scale to high-resolution synthesis. it iteratively propagates information from a set of latent variables to the evolving visual features and vice versa, to support the refinement of each in light of the other and encourage the emergence of compositional representations of objects and scenes. in contrast to the classic transformer architecture, it utilizes multiplicative integration that allows flexible region-based modulation, and can thus be seen as a generalization of the successful stylegan network. we demonstrate the model's strength and robustness through a careful evaluation over a range of datasets, from simulated multi-object environments to rich real-world indoor and -----> outdoor !!!  scenes, showing it achieves state-of-the-art results in terms of image quality and diversity, while enjoying fast learning and better data-efficiency. further qualitative and quantitative experiments offer us an insight into the model's inner workings, revealing improved interpretability and stronger disentanglement, and illustrating the benefits and efficacy of our approach. an implementation of the model is available at [https://github.com/dorarad/gansformer](https://github.com/dorarad/gansformer).  \n\n\nhttps://preview.redd.it/a25l6rs9prk61.png?width=945&amp;format=png&amp;auto=webp&amp;s=927b31fdd81c459e3bfb51d4429c7409307ea721", "sortedWord": "None", "removed": "('nan',)", "score": 28, "comments": 56, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/lwoj5i/r_generative_adversarial_transformers_210301209/',)", "identifyer": 5731538, "year": "2021"}, {"autor": "gerry_mandering_50", "date": 1623437592000, "content": "[D] Wyze cam and Ring cam campfire discussion for computer vision purposes /!/ Basically, I'm doing a CV system with deep neural nets that is monitoring specific outdoor images of natural environment.  \n\nIt would be nice to use the consumer-grade cams from Ring or Wyze.  These have cloud subscriptions and some under-100 dollar cams.  I have already acquired and paid for cloud and cam for both systems hoping one would work well for CV data capture.\n\nI expect that an RDTP server on cam might help since I expect I then can write code for my own custom client and save any video stills that I find interesting.\n\nIn the past (2008) I made pre-machine-learning vision app with cheap PC webcams to automatically measure the speed of car traffic and it was semi-successful.  As you can guess, special cases began to overwhelm my rule-driven analysis code, and machine learning will work much much better, I expect. The current apps I am working on are not vehicle speed measurers.\n\nMy findings so far, for Wyze cams \"Outdoor\" and \"V3\" as of early 2021:\n\n- Must click in the UI to manually download pre-motion-detected segments one at a time, even for paid cloud subscribers. This is a problem.\n\n- No programmatic API, even for paid cloud subscribers. THis is a problem.\n\n- For some not all cams they sell, there is an RDTP server firmware you can put on the cam, to replace the original firmware.  They call it \"beta\" release level not production level, with all that implies.\n\n- RDTP would enable my code to read all video all the time, so it will not miss anything.\n\n- The provided motion-detection too often misses my subjects that move, in my manual tests. I guess it's PIR driven, not AI driven in the provided motion-detection, and my subjects are not different enough in temperature than the background.  This means that their provided motion detection probably is more trouble than benefit.  I will have to do my own motion detection, which is fine really as I am confident I can do this myself.\n\n- Wyze has a mode provided in the standard cam firmware, to do so-called time-lapse, which is basically taking a still photo at a chosen fixed frequency, like once per minute, during a defined time range, like 6 hours.  This might help me reduce the file sizes that need to be saved to the local SD memory card.  It would effectively be sampling instead of continuous-time recording.  Thus I would miss some interesting events, but on the other hand I will still get some interesting events.\n\n- The wifi that is built-in makes them very easy to install in good locations for my projects. \n\nAs for the Ring cams:\n\n- Slightly more expensive, slightly less useful and flexible.  Basically all of the same problems as Wyze cams.  \n\n- No RDTP that I could find (let me know if I overlooked)\n\n\nTO summarize:\n\nI might have to go back to PC web cams. At least I can do everything with the images.  It's not a closed proprietary cloud holding my data.\n\nIt would be a shame to go back to PC web cams, because the wifi in Ring and Wyze cams make them very easy to install in good locations for my projects.  Also these new cams have greatly improved resolution and color versus my old web cams from 10+ years ago.\n\n- I feel like approaching these companies, to ask for API to be installed and published so scientists can use their cams.\n\n- I feel like anyone who pays a subscription to their cloud, should ethically and morally be allowed by these companies to use the API without restriction to read (download) their own image data from the cloud.\n\n\nIn your own CV projects, what have you found that works for a cam system?\n\nI expect a phone cams is another possibility, esp older phones that have no other uses and just sit around in a drawer.  \n\nIs there an app for RDTP serving on android phones?\n\n\nThanks for reading and sharing.", "link": "https://www.reddit.com/r/MachineLearning/comments/nxnci9/d_wyze_cam_and_ring_cam_campfire_discussion_for/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "outdoor", "selectorShort": "outdoor", "MarkedSent": "[d] wyze cam and ring cam campfire discussion for computer vision purposes /!/ basically, i'm doing a cv system with deep neural nets that is monitoring specific -----> outdoor !!!  images of natural environment.  \n\nit would be nice to use the consumer-grade cams from ring or wyze.  these have cloud subscriptions and some under-100 dollar cams.  i have already acquired and paid for cloud and cam for both systems hoping one would work well for cv data capture.\n\ni expect that an rdtp server on cam might help since i expect i then can write code for my own custom client and save any video stills that i find interesting.\n\nin the past (2008) i made pre-machine-learning vision app with cheap pc webcams to automatically measure the speed of car traffic and it was semi-successful.  as you can guess, special cases began to overwhelm my rule-driven analysis code, and machine learning will work much much better, i expect. the current apps i am working on are not vehicle speed measurers.\n\nmy findings so far, for wyze cams \"outdoor\" and \"v3\" as of early 2021:\n\n- must click in the ui to manually download pre-motion-detected segments one at a time, even for paid cloud subscribers. this is a problem.\n\n- no programmatic api, even for paid cloud subscribers. this is a problem.\n\n- for some not all cams they sell, there is an rdtp server firmware you can put on the cam, to replace the original firmware.  they call it \"beta\" release level not production level, with all that implies.\n\n- rdtp would enable my code to read all video all the time, so it will not miss anything.\n\n- the provided motion-detection too often misses my subjects that move, in my manual tests. i guess it's pir driven, not ai driven in the provided motion-detection, and my subjects are not different enough in temperature than the background.  this means that their provided motion detection probably is more trouble than benefit.  i will have to do my own motion detection, which is fine really as i am confident i can do this myself.\n\n- wyze has a mode provided in the standard cam firmware, to do so-called time-lapse, which is basically taking a still photo at a chosen fixed frequency, like once per minute, during a defined time range, like 6 hours.  this might help me reduce the file sizes that need to be saved to the local sd memory card.  it would effectively be sampling instead of continuous-time recording.  thus i would miss some interesting events, but on the other hand i will still get some interesting events.\n\n- the wifi that is built-in makes them very easy to install in good locations for my projects. \n\nas for the ring cams:\n\n- slightly more expensive, slightly less useful and flexible.  basically all of the same problems as wyze cams.  \n\n- no rdtp that i could find (let me know if i overlooked)\n\n\nto summarize:\n\ni might have to go back to pc web cams. at least i can do everything with the images.  it's not a closed proprietary cloud holding my data.\n\nit would be a shame to go back to pc web cams, because the wifi in ring and wyze cams make them very easy to install in good locations for my projects.  also these new cams have greatly improved resolution and color versus my old web cams from 10+ years ago.\n\n- i feel like approaching these companies, to ask for api to be installed and published so scientists can use their cams.\n\n- i feel like anyone who pays a subscription to their cloud, should ethically and morally be allowed by these companies to use the api without restriction to read (download) their own image data from the cloud.\n\n\nin your own cv projects, what have you found that works for a cam system?\n\ni expect a phone cams is another possibility, esp older phones that have no other uses and just sit around in a drawer.  \n\nis there an app for rdtp serving on android phones?\n\n\nthanks for reading and sharing.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/nxnci9/d_wyze_cam_and_ring_cam_campfire_discussion_for/',)", "identifyer": 5738703, "year": "2021"}, {"autor": "gopal_chitalia", "date": 1628595623000, "content": "[P] LSTM predictions are one-time step lagging /!/ My problem involves electricity prediction (time-series problem) for 1-hour ahead. I am using LSTM to forecast.\n\n* **Length of Dataset:** 1 year at one-hour interval\n* **Input:** Outdoor Temperature (`O_t`), Relative Humidity (`RH_t`), historical electricity (`E_t`)\n* **Output:** Electricity (`E_t+1`)\n\n**Note:** `t` denotes timestamp\n\nThe `pearson correlation` between (`E_t`) and (`E_t+1`) is \\~95%.  However, my forecast is lagging by 1-hour (one timestep) i.e. it is predicting `E_t` at time (t+1).\n\n**Network Architecture:**\n\n    epoch = 400\n    batch_size = 24\n    lr = 0.001\n    \n    model = Sequential()\n    \n    model.add((LSTM(units=20, return_sequences=True, input_shape=(look_back, X_train.shape[2]), activation='relu')))\n    model.add((LSTM(units=10, return_sequences=True, activation='relu')))\n    # model.add(Dropout(0.2))\n    \n    model.add(Flatten())\n    model.add(Dense(units = 1,activation='sigmoid'))\n    \n    adam = optimizers.Adam(lr=lr)\n    model.compile(loss = 'mean_squared_error', optimizer = adam)\n    \n    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epoch, batch_size=batch_size,verbose=1, shuffle = False)\n    model.summary()\n\n**Model loss graph**\n\n&amp;#x200B;\n\nhttps://preview.redd.it/6bmakbw7pig71.png?width=425&amp;format=png&amp;auto=webp&amp;s=c53027a5508906ae7465ef6fe8dbf83c29b8b45b\n\n**Predictions Graph**\n\n&amp;#x200B;\n\nhttps://preview.redd.it/2l0qc1i8pig71.jpg?width=1182&amp;format=pjpg&amp;auto=webp&amp;s=cc244373826768abd072651f16f2b41652d05229\n\nThe left-sided figure is the actual prediction whereas the right-sided figure is one-hour shifted. As could be seen the actual predictions are one-hour lagging.\n\nI have done hyper-parameter tuning as well using grid search.\n\nCan anyone help me out on how to improve the performance? Is it because of the very low loss values ( in range of `1e-3`) after 10-20 epochs and the incentive to learn is quite low?\n\nAny thoughts?", "link": "https://www.reddit.com/r/MachineLearning/comments/p1ngjc/p_lstm_predictions_are_onetime_step_lagging/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "outdoor", "selectorShort": "outdoor", "MarkedSent": "[p] lstm predictions are one-time step lagging /!/ my problem involves electricity prediction (time-series problem) for 1-hour ahead. i am using lstm to forecast.\n\n* **length of dataset:** 1 year at one-hour interval\n* **input:** -----> outdoor !!!  temperature (`o_t`), relative humidity (`rh_t`), historical electricity (`e_t`)\n* **output:** electricity (`e_t+1`)\n\n**note:** `t` denotes timestamp\n\nthe `pearson correlation` between (`e_t`) and (`e_t+1`) is \\~95%.  however, my forecast is lagging by 1-hour (one timestep) i.e. it is predicting `e_t` at time (t+1).\n\n**network architecture:**\n\n    epoch = 400\n    batch_size = 24\n    lr = 0.001\n    \n    model = sequential()\n    \n    model.add((lstm(units=20, return_sequences=true, input_shape=(look_back, x_train.shape[2]), activation='relu')))\n    model.add((lstm(units=10, return_sequences=true, activation='relu')))\n    # model.add(dropout(0.2))\n    \n    model.add(flatten())\n    model.add(dense(units = 1,activation='sigmoid'))\n    \n    adam = optimizers.adam(lr=lr)\n    model.compile(loss = 'mean_squared_error', optimizer = adam)\n    \n    history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=epoch, batch_size=batch_size,verbose=1, shuffle = false)\n    model.summary()\n\n**model loss graph**\n\n&amp;#x200b;\n\nhttps://preview.redd.it/6bmakbw7pig71.png?width=425&amp;format=png&amp;auto=webp&amp;s=c53027a5508906ae7465ef6fe8dbf83c29b8b45b\n\n**predictions graph**\n\n&amp;#x200b;\n\nhttps://preview.redd.it/2l0qc1i8pig71.jpg?width=1182&amp;format=pjpg&amp;auto=webp&amp;s=cc244373826768abd072651f16f2b41652d05229\n\nthe left-sided figure is the actual prediction whereas the right-sided figure is one-hour shifted. as could be seen the actual predictions are one-hour lagging.\n\ni have done hyper-parameter tuning as well using grid search.\n\ncan anyone help me out on how to improve the performance? is it because of the very low loss values ( in range of `1e-3`) after 10-20 epochs and the incentive to learn is quite low?\n\nany thoughts?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 16, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/p1ngjc/p_lstm_predictions_are_onetime_step_lagging/',)", "identifyer": 5739634, "year": "2021"}], "name": "outdoorMachineLearning2021"}
{"interestingcomments": [{"autor": "tars9999", "date": 1617288820000, "content": "Mixture of experts - tradeoffs vs traditional single nets [D] /!/ I was personally surprised that \"branched NN's\" (seemingly called Mixture-Of-Experts) was not a more popular (or sucessful) technique, (eg. for imagenet entries) but it's appeared in some recent stories.\n\nIs there any research comparing training times, model size, evaluation time between 1 dense net and mixture-of-experts ?\n\ne.g. how would it pan out if you tried to do imagenet with it? (eg 33 branches of 33 categories each, or 10 vs 100 ..)\n\nalso is there any research on the tradeoffs of more branches ('heirachical MoE') (like 1000 caterogies = 10x10x10) \n\nPerhaps for a particular problem there is an optimum branching depth, and it just happens below a certain size the optimum number of branches is just '1'.", "link": "https://www.reddit.com/r/MachineLearning/comments/mhx0ih/mixture_of_experts_tradeoffs_vs_traditional/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "branches", "selectorShort": "branch", "MarkedSent": "mixture of experts - tradeoffs vs traditional single nets [d] /!/ i was personally surprised that \"branched nn's\" (seemingly called mixture-of-experts) was not a more popular (or sucessful) technique, (eg. for imagenet entries) but it's appeared in some recent stories.\n\nis there any research comparing training times, model size, evaluation time between 1 dense net and mixture-of-experts ?\n\ne.g. how would it pan out if you tried to do imagenet with it? (eg 33 -----> branches !!!  of 33 categories each, or 10 vs 100 ..)\n\nalso is there any research on the tradeoffs of more -----> branches !!!  ('heirachical moe') (like 1000 caterogies = 10x10x10) \n\nperhaps for a particular problem there is an optimum branching depth, and it just happens below a certain size the optimum number of branches is just '1'.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/mhx0ih/mixture_of_experts_tradeoffs_vs_traditional/',)", "identifyer": 5721574, "year": "2021"}, {"autor": "kk7nc", "date": 1625079867000, "content": "[D] Toward Knowledge Discovery Framework for Data Science Job Market in the United States /!/ &amp;#x200B;\n\n*Processing gif rippdc16bg871...*\n\n## Toward Knowledge Discovery Framework for Data Science Job Market in the United States\n\nThe growth of the data science field requires better tools to understand such a fast-paced growing domain. Moreover, individuals from different backgrounds became interested in following a career as data scientists. Therefore, providing a quantitative guide for individuals and organizations to understand the skills required in the job market would be crucial. This paper introduces a framework to analyze the job market for data science-related jobs within the US while providing an interface to access insights in this market. The proposed framework includes three sub-modules allowing continuous data collection, information extraction, and a web-based dashboard visualization to investigate the spatial and temporal distribution of data science-related jobs and skills. The result of this work shows important skills for the main branches of data science jobs and attempts to provide a skill-based definition of these data science branches. The current version of this application is deployed on the web and allows individuals and institutes to investigate skills required for data science positions through the industry lens.\n\n## Method\n\n## Overview of data science skill tracking system\n\nThe figure shows an overview of the proposed system. Each of these modules will be described in more detail in the following. As it can be seen in this Figure, this system heavily relies on cloud computing structures that provide stable facilities for continuous tasks with scheduling to collect, analyze, and present its findings dynamically.\n\nhttps://preview.redd.it/bseupc47bg871.jpg?width=1073&amp;format=pjpg&amp;auto=webp&amp;s=3111a41af3eacb10021c9d9961c2937f10305dfe\n\n&amp;#x200B;\n\n## Data Collection Sample\n\nThis shows samples of the returned fields for queries on data analysts. For a framework like the one presented, it is not feasible to use a local machine for data collection as it requires to work continuously for a long period of time with no interruptions (such as internet disconnection or power outage, etc.). Such a machine and maintaining its connectivity would cause a lot of hassle and scrapy cloud services solve this problem by understanding the data collection framework. It allows to create periodic jobs to continuously collect the data using this platform. This module runs its data collection spiders every week and samples job postings on the web periodically. The results of these queries are then delivered into a mysql database on Amazon web services (AWS) for further processing that will be explained in the next section.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/hdjm9ik7bg871.jpg?width=1177&amp;format=pjpg&amp;auto=webp&amp;s=e4d1b2351b81d95a88224dd1dc3c55ca1ce682b1\n\n## Interaction graph of visualization components\n\nThe effect and interactions between these components are complex. Figure shows these interactions in our dash model. As it can be seen the spatial and temporal aspect of skills could be investigated by using these components. Other components would be added as the work continues.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/2exurp78bg871.jpg?width=891&amp;format=pjpg&amp;auto=webp&amp;s=1118ca9b1eb8c70423e7cc9782404513b5209c5a\n\n## Top 20 results based on no. job by states and company\n\nThis Table shows the top 20 companies during this period. Despite the expectation that tech sector dominate the field, it appears that other sectors contribute to this market significantly. Namely, consultant and advisory section (e.g. Booz Allen, Deloitte)and financial institutes(e.g JPMorgan, Wells Fargo) and government organization contribute significantly to this market along with the tech sector.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/saieccq8bg871.jpg?width=787&amp;format=pjpg&amp;auto=webp&amp;s=5c9ba2c501f2255d0fe5031344289ce67e76d842\n\n## Top skills for the three main tracks\n\nThis Table shows the top 20 skills in each job posting category. The result in this table clearly shows the difference between these main tracks of data science. While machine learning engineer job postings mostly shows a domination of skills such as programming, machine learning, cloud, and big data technologies, a data analyst mostly needs skills for data retrieving (e.g. SQL, Excel,database), and Visualization (e.g Tableau, Power BI).\n\nhttps://preview.redd.it/t22ztw79bg871.jpg?width=971&amp;format=pjpg&amp;auto=webp&amp;s=2273264a8c3074fba8ef3e607f218615566fa9ec\n\n&amp;#x200B;\n\n## Temporal insights of data science skills\n\nLeft Figure shows this comparison between top programming languages mentioned during this period.As it can be seen, the top language is python following by R and Java. This indicated not only python is the most dominant language but the trend and the gap shows it appears to be in demand in future in comparison to others. Similarly, a comparison between deep learning framework is presented in right Figure. The result shows that tensorflow followed by pytorch are the most mentioned deep learning frameworks. Also H2O\\~(an R based package) is mentioned much less which indicates the overwhelming attention to python for deep learning. As it can be seen, the framework allows temporal insights to this job market and based on their general importance, such insights can be added to the interface designed for users.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/je1ah7k9bg871.jpg?width=1211&amp;format=pjpg&amp;auto=webp&amp;s=f307e15bf06da7d112a0a69c42efdf684d5f4ab2\n\n## Conclusion\n\nThis paper introduces a framework capable of collecting and analysing of spatial and temporal aspects of data science's job market using available tools within data science toolbox. The framework provides a free front-end interface developed using plotly dash for this market. The result provides a quantitative guide for individuals and organizations to recognize the most important skills and concepts in this domain based on industry perception. Possible extension to the work here includes sector analysis of job posting and improving on skill extraction technique. Other temporal insights will be added to the interface based on the importance of the information they can add for individuals using this interface.\n\n## Error and Comments:\n\nSend an email to [mh4pk@virginia.edu](mailto:mh4pk@virginia.edu)\n\n&amp;#x200B;\n\n**GitHub:** [**https://github.com/mojtaba-Hsafa/data-science-jobs-app**](https://github.com/mojtaba-Hsafa/data-science-jobs-app)", "link": "https://www.reddit.com/r/MachineLearning/comments/ob43wl/d_toward_knowledge_discovery_framework_for_data/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "branches", "selectorShort": "branch", "MarkedSent": "[d] toward knowledge discovery framework for data science job market in the united states /!/ &amp;#x200b;\n\n*processing gif rippdc16bg871...*\n\n## toward knowledge discovery framework for data science job market in the united states\n\nthe growth of the data science field requires better tools to understand such a fast-paced growing domain. moreover, individuals from different backgrounds became interested in following a career as data scientists. therefore, providing a quantitative guide for individuals and organizations to understand the skills required in the job market would be crucial. this paper introduces a framework to analyze the job market for data science-related jobs within the us while providing an interface to access insights in this market. the proposed framework includes three sub-modules allowing continuous data collection, information extraction, and a web-based dashboard visualization to investigate the spatial and temporal distribution of data science-related jobs and skills. the result of this work shows important skills for the main -----> branches !!!  of data science jobs and attempts to provide a skill-based definition of these data science -----> branches !!! . the current version of this application is deployed on the web and allows individuals and institutes to investigate skills required for data science positions through the industry lens.\n\n## method\n\n## overview of data science skill tracking system\n\nthe figure shows an overview of the proposed system. each of these modules will be described in more detail in the following. as it can be seen in this figure, this system heavily relies on cloud computing structures that provide stable facilities for continuous tasks with scheduling to collect, analyze, and present its findings dynamically.\n\nhttps://preview.redd.it/bseupc47bg871.jpg?width=1073&amp;format=pjpg&amp;auto=webp&amp;s=3111a41af3eacb10021c9d9961c2937f10305dfe\n\n&amp;#x200b;\n\n## data collection sample\n\nthis shows samples of the returned fields for queries on data analysts. for a framework like the one presented, it is not feasible to use a local machine for data collection as it requires to work continuously for a long period of time with no interruptions (such as internet disconnection or power outage, etc.). such a machine and maintaining its connectivity would cause a lot of hassle and scrapy cloud services solve this problem by understanding the data collection framework. it allows to create periodic jobs to continuously collect the data using this platform. this module runs its data collection spiders every week and samples job postings on the web periodically. the results of these queries are then delivered into a mysql database on amazon web services (aws) for further processing that will be explained in the next section.\n\n&amp;#x200b;\n\nhttps://preview.redd.it/hdjm9ik7bg871.jpg?width=1177&amp;format=pjpg&amp;auto=webp&amp;s=e4d1b2351b81d95a88224dd1dc3c55ca1ce682b1\n\n## interaction graph of visualization components\n\nthe effect and interactions between these components are complex. figure shows these interactions in our dash model. as it can be seen the spatial and temporal aspect of skills could be investigated by using these components. other components would be added as the work continues.\n\n&amp;#x200b;\n\nhttps://preview.redd.it/2exurp78bg871.jpg?width=891&amp;format=pjpg&amp;auto=webp&amp;s=1118ca9b1eb8c70423e7cc9782404513b5209c5a\n\n## top 20 results based on no. job by states and company\n\nthis table shows the top 20 companies during this period. despite the expectation that tech sector dominate the field, it appears that other sectors contribute to this market significantly. namely, consultant and advisory section (e.g. booz allen, deloitte)and financial institutes(e.g jpmorgan, wells fargo) and government organization contribute significantly to this market along with the tech sector.\n\n&amp;#x200b;\n\nhttps://preview.redd.it/saieccq8bg871.jpg?width=787&amp;format=pjpg&amp;auto=webp&amp;s=5c9ba2c501f2255d0fe5031344289ce67e76d842\n\n## top skills for the three main tracks\n\nthis table shows the top 20 skills in each job posting category. the result in this table clearly shows the difference between these main tracks of data science. while machine learning engineer job postings mostly shows a domination of skills such as programming, machine learning, cloud, and big data technologies, a data analyst mostly needs skills for data retrieving (e.g. sql, excel,database), and visualization (e.g tableau, power bi).\n\nhttps://preview.redd.it/t22ztw79bg871.jpg?width=971&amp;format=pjpg&amp;auto=webp&amp;s=2273264a8c3074fba8ef3e607f218615566fa9ec\n\n&amp;#x200b;\n\n## temporal insights of data science skills\n\nleft figure shows this comparison between top programming languages mentioned during this period.as it can be seen, the top language is python following by r and java. this indicated not only python is the most dominant language but the trend and the gap shows it appears to be in demand in future in comparison to others. similarly, a comparison between deep learning framework is presented in right figure. the result shows that tensorflow followed by pytorch are the most mentioned deep learning frameworks. also h2o\\~(an r based package) is mentioned much less which indicates the overwhelming attention to python for deep learning. as it can be seen, the framework allows temporal insights to this job market and based on their general importance, such insights can be added to the interface designed for users.\n\n&amp;#x200b;\n\nhttps://preview.redd.it/je1ah7k9bg871.jpg?width=1211&amp;format=pjpg&amp;auto=webp&amp;s=f307e15bf06da7d112a0a69c42efdf684d5f4ab2\n\n## conclusion\n\nthis paper introduces a framework capable of collecting and analysing of spatial and temporal aspects of data science's job market using available tools within data science toolbox. the framework provides a free front-end interface developed using plotly dash for this market. the result provides a quantitative guide for individuals and organizations to recognize the most important skills and concepts in this domain based on industry perception. possible extension to the work here includes sector analysis of job posting and improving on skill extraction technique. other temporal insights will be added to the interface based on the importance of the information they can add for individuals using this interface.\n\n## error and comments:\n\nsend an email to [mh4pk@virginia.edu](mailto:mh4pk@virginia.edu)\n\n&amp;#x200b;\n\n**github:** [**https://github.com/mojtaba-hsafa/data-science-jobs-app**](https://github.com/mojtaba-hsafa/data-science-jobs-app)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/ob43wl/d_toward_knowledge_discovery_framework_for_data/',)", "identifyer": 5723840, "year": "2021"}, {"autor": "WeekendClassic", "date": 1634215826000, "content": "[R] Supervised learning and other branches of machine learning /!/ I was searching for how machine learning models operate and came across this piece a friend of mine at work wrote.\n\nI'm sharing [this guide on supervised learning](https://blog.superannotate.com/supervised-learning-and-other-machine-learning-tasks/) and other branches of machine learning got you covered.", "link": "https://www.reddit.com/r/MachineLearning/comments/q7z1ir/r_supervised_learning_and_other_branches_of/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "branches", "selectorShort": "branch", "MarkedSent": "[r] supervised learning and other -----> branches !!!  of machine learning /!/ i was searching for how machine learning models operate and came across this piece a friend of mine at work wrote.\n\ni'm sharing [this guide on supervised learning](https://blog.superannotate.com/supervised-learning-and-other-machine-learning-tasks/) and other branches of machine learning got you covered.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/q7z1ir/r_supervised_learning_and_other_branches_of/',)", "identifyer": 5724216, "year": "2021"}, {"autor": "WeekendClassic", "date": 1634215706000, "content": "Supervised learning and other branches of machine learning /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/q7z09u/supervised_learning_and_other_branches_of_machine/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "branches", "selectorShort": "branch", "MarkedSent": "supervised learning and other -----> branches !!!  of machine learning /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/q7z09u/supervised_learning_and_other_branches_of_machine/',)", "identifyer": 5724217, "year": "2021"}, {"autor": "regalalgorithm", "date": 1621123405000, "content": "[D] Feedback for the first edition of The Gradient's newsletter /!/ Hi there. I am one of the editors from [The Gradient](https://thegradient.pub/), which I think (or hope) many on here have seen and like. Having been around for more than 3 years now, we've decided to branch out beyond what we've been doing and just released [the first edition](https://thegradientpub.substack.com/p/update-1-fbi-usage-of-facial-recognition) of our newsletter, The Update. \n\nThough admittedly I am making this post partially to promote it, since it's the very first one I also really want to ask for your opinions on the format/length/content of it and whether you find it compelling. We discussed how to create something we thought was different from other newsletters out there and actually interesting/worth reading, but it's hard to tell if what we came up with is actually good.\n\nSo would appreciate your feedback!", "link": "https://www.reddit.com/r/MachineLearning/comments/ndbzgm/d_feedback_for_the_first_edition_of_the_gradients/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "branch", "selectorShort": "branch", "MarkedSent": "[d] feedback for the first edition of the gradient's newsletter /!/ hi there. i am one of the editors from [the gradient](https://thegradient.pub/), which i think (or hope) many on here have seen and like. having been around for more than 3 years now, we've decided to -----> branch !!!  out beyond what we've been doing and just released [the first edition](https://thegradientpub.substack.com/p/update-1-fbi-usage-of-facial-recognition) of our newsletter, the update. \n\nthough admittedly i am making this post partially to promote it, since it's the very first one i also really want to ask for your opinions on the format/length/content of it and whether you find it compelling. we discussed how to create something we thought was different from other newsletters out there and actually interesting/worth reading, but it's hard to tell if what we came up with is actually good.\n\nso would appreciate your feedback!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/ndbzgm/d_feedback_for_the_first_edition_of_the_gradients/',)", "identifyer": 5724410, "year": "2021"}, {"autor": "DingXiaoHan", "date": 1623136265000, "content": "[R] RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition /!/ This paper is about MLP but does not intend to build pure-MLP models like MLP-Mixer, gMLP and ResMLP. The motivation is that FC has global capacity but lacks a local prior so we use convolutions to incorporate locality into FC. \n\nIt proposes **RepMLP**, an MLP-style building block, and uses convolutions to incorporate locality into FC layers to make them effective in extracting features in images. It significantly improves the performance of CNN like ResNet-50 by up to 2% **(78.03% top-1 acc ---&gt; 80.07%)**. Like [RepVGG](https://arxiv.org/pdf/2101.03697.pdf)(CVPR 2021), [ACNet](https://openaccess.thecvf.com/content_ICCV_2019/papers/Ding_ACNet_Strengthening_the_Kernel_Skeletons_for_Powerful_CNN_via_Asymmetric_ICCV_2019_paper.pdf) (ICCV 2019) and [Diverse Branch Block](https://arxiv.org/pdf/2103.13425.pdf) (CVPR 2021), it is also a member of the Structural Re-parameterization Universe as **the convolutions can be equivalently merged into the FC** via an elegant and platform-agnostic algorithm.\n\nIt turns out that FC is significantly faster than conv with a comparable number of parameters and has a stronger representational capacity. \n\nIt also suggests that **viewing convolution as a degraded FC** may bring more exciting discoveries.\n\npaper: [https://arxiv.org/pdf/2105.01883.pdf](https://arxiv.org/pdf/2105.01883.pdf)\n\ncode: [https://github.com/DingXiaoH/RepMLP](https://github.com/DingXiaoH/RepMLP)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/9i3fz5jvrz371.png?width=840&amp;format=png&amp;auto=webp&amp;s=07b445cc260ccbc7aa6e8fbe87028d961909fabe", "link": "https://www.reddit.com/r/MachineLearning/comments/nuysrd/r_repmlp_reparameterizing_convolutions_into/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "branch", "selectorShort": "branch", "MarkedSent": "[r] repmlp: re-parameterizing convolutions into fully-connected layers for image recognition /!/ this paper is about mlp but does not intend to build pure-mlp models like mlp-mixer, gmlp and resmlp. the motivation is that fc has global capacity but lacks a local prior so we use convolutions to incorporate locality into fc. \n\nit proposes **repmlp**, an mlp-style building block, and uses convolutions to incorporate locality into fc layers to make them effective in extracting features in images. it significantly improves the performance of cnn like resnet-50 by up to 2% **(78.03% top-1 acc ---&gt; 80.07%)**. like [repvgg](https://arxiv.org/pdf/2101.03697.pdf)(cvpr 2021), [acnet](https://openaccess.thecvf.com/content_iccv_2019/papers/ding_acnet_strengthening_the_kernel_skeletons_for_powerful_cnn_via_asymmetric_iccv_2019_paper.pdf) (iccv 2019) and [diverse -----> branch !!!  block](https://arxiv.org/pdf/2103.13425.pdf) (cvpr 2021), it is also a member of the structural re-parameterization universe as **the convolutions can be equivalently merged into the fc** via an elegant and platform-agnostic algorithm.\n\nit turns out that fc is significantly faster than conv with a comparable number of parameters and has a stronger representational capacity. \n\nit also suggests that **viewing convolution as a degraded fc** may bring more exciting discoveries.\n\npaper: [https://arxiv.org/pdf/2105.01883.pdf](https://arxiv.org/pdf/2105.01883.pdf)\n\ncode: [https://github.com/dingxiaoh/repmlp](https://github.com/dingxiaoh/repmlp)\n\n&amp;#x200b;\n\nhttps://preview.redd.it/9i3fz5jvrz371.png?width=840&amp;format=png&amp;auto=webp&amp;s=07b445cc260ccbc7aa6e8fbe87028d961909fabe", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 22, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/nuysrd/r_repmlp_reparameterizing_convolutions_into/',)", "identifyer": 5726786, "year": "2021"}, {"autor": "Fancy-Stress-806", "date": 1620494296000, "content": "[D] Need help checking if my code is correct /!/  \n\nI was wondering if I could get a check on my code to see if it is correct (or at least close to). Basically, I am trying to replicate the Neural Network from Figure 1 from this paper: [http://cs231n.stanford.edu/reports/2017/pdfs/602.pdf](http://cs231n.stanford.edu/reports/2017/pdfs/602.pdf). However, instead of past video game frames, I am using rainfall radar images and instead of actions of the player, I am using climate data (e.g. temperature, humidity, o3 concentration, etc.). These inputs combined will generate a rainfall output image at a future timestep. I just don't need the reward branches after the element wise multiplication in the middle. Therefore, I am attempting to replicate and adapt the entire network except the reward branches.\n\nFor additional context, my rainfall radar inputs are (2258, 110, 110, 12) where the 1st, 2nd, 3rd and 4th dimensions correspond to the number of samples, height of the image, width of the image, depth dimension (number of timesteps used for inputs) respectively. The climate data is a (2258, 6) table/array where the 6 columns correspond to the following features: Elevation above mean sea level, o3 concentration, temperature, dewpoint, humidity, wind speeds. I've coded out the network in a way I think is right (hopefully) and it runs but I'm not sure if it is correct so was wondering if I could get a check here.\n\n    # RAINFALL BRANCH\n    def rainCNN(height, width, depth):\n        \n        rain_inputs = Input(shape = (height, width, depth))\n    \n        # LEFT HAND SIDE\n        conv1 = Conv2D(64, kernel_size = 8, strides = 2, padding = \"valid\")(rain_inputs)\n        conv1 = Activation(\"relu\")(conv1)\n        \n        conv2 = Conv2D(128, kernel_size = 6, strides = 2, padding = \"valid\")(conv1)\n        conv2 = Activation(\"relu\")(conv2)\n    \n        conv3 = Conv2D(128, kernel_size = 6, strides = 2, padding = \"valid\")(conv2)\n        conv3 = Activation(\"relu\")(conv3)\n    \n        conv4 = Conv2D(128, kernel_size = 6, strides = 2, padding = \"valid\")(conv3)\n        conv4 = Activation(\"relu\")(conv4)\n    \n        fc1 = Flatten()(conv4)\n        fc1 = Dense(1024, activation = \"relu\")(fc1)\n    \n        fc2 = Dense(2048, activation = \"linear\")(fc1)\n        \n        model = Model(inputs = rain_inputs, outputs = fc2)\n        \n        return model\n    \n    # CLIMATE BRANCH\n    def climateCNN(n_columns):\n        \n        # CLIMATE DATA (ACTION \"ONE HOT\" BRANCH) / MODEL 2\n        model = Sequential()\n        model.add(Dense(2048, input_dim = n_columns, activation = \"linear\"))\n        \n        return model\n    \n    # CREATE THE BRANCHES\n    climate = climateCNN(climate_data.shape[1])\n    rain = rainCNN(110, 110, 12)\n    \n    # ELEMENT WISE MULTIPLICATION TO COMBINE\n    multiplied = Multiply()([rain.output, climate.output])\n    \n    # RIGHT SIDE\n    fc4 = Dense(1024, activation = \"linear\")(multiplied)\n    fc5 = Dense(1152, activation = \"relu\")(fc4)\n    deconv0 = Reshape((3,3,128))(fc5)\n    \n    deconv1 = Conv2DTranspose(128, kernel_size = 6, strides = 2, padding = \"valid\")(deconv0)\n    deconv2 = Conv2DTranspose(128, kernel_size = 6, strides = 2, padding = \"valid\")(deconv1)\n    deconv3 = Conv2DTranspose(64, kernel_size = 6, strides = 2, padding = \"valid\")(deconv2)\n    \n    # OUTPUT IMAGE\n    outputs = Conv2DTranspose(1, kernel_size = 8, strides = 2, padding = \"valid\")(deconv3)\n    \n    model1 = Model(inputs = [rain.input, climate.input], outputs = outputs)\n    \n    # CALL OUT MODEL FUNCTION\n    model1.compile(optimizer = 'adam', loss = 'logcosh', metrics = ['accuracy'])\n    \n    history = model1.fit([X_train, climate_train], Y_train, epochs = 100, validation_data = ([X_val, climate_val], Y_val), verbose = 2)\n\nI basically assumed this Network to take the form of a Multiple input/multi data type neural network so I followed the guide from this website: [https://heartbeat.fritz.ai/building-a-mixed-data-neural-network-in-keras-to-predict-accident-locations-d51a63b738cf](https://heartbeat.fritz.ai/building-a-mixed-data-neural-network-in-keras-to-predict-accident-locations-d51a63b738cf). Obviously, I am quite new to Machine Learning so I'm worried if my code is not correct (i.e., doesn't replicate the video game network by Stanford correctly).\n\nI have also tried emailing the original authors for their code but received no reply so here I am. Thanks in advance guys!", "link": "https://www.reddit.com/r/MachineLearning/comments/n7tvp1/d_need_help_checking_if_my_code_is_correct/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "branches", "selectorShort": "branch", "MarkedSent": "[d] need help checking if my code is correct /!/  \n\ni was wondering if i could get a check on my code to see if it is correct (or at least close to). basically, i am trying to replicate the neural network from figure 1 from this paper: [http://cs231n.stanford.edu/reports/2017/pdfs/602.pdf](http://cs231n.stanford.edu/reports/2017/pdfs/602.pdf). however, instead of past video game frames, i am using rainfall radar images and instead of actions of the player, i am using climate data (e.g. temperature, humidity, o3 concentration, etc.). these inputs combined will generate a rainfall output image at a future timestep. i just don't need the reward -----> branches !!!  after the element wise multiplication in the middle. therefore, i am attempting to replicate and adapt the entire network except the reward branches.\n\nfor additional context, my rainfall radar inputs are (2258, 110, 110, 12) where the 1st, 2nd, 3rd and 4th dimensions correspond to the number of samples, height of the image, width of the image, depth dimension (number of timesteps used for inputs) respectively. the climate data is a (2258, 6) table/array where the 6 columns correspond to the following features: elevation above mean sea level, o3 concentration, temperature, dewpoint, humidity, wind speeds. i've coded out the network in a way i think is right (hopefully) and it runs but i'm not sure if it is correct so was wondering if i could get a check here.\n\n    # rainfall branch\n    def raincnn(height, width, depth):\n        \n        rain_inputs = input(shape = (height, width, depth))\n    \n        # left hand side\n        conv1 = conv2d(64, kernel_size = 8, strides = 2, padding = \"valid\")(rain_inputs)\n        conv1 = activation(\"relu\")(conv1)\n        \n        conv2 = conv2d(128, kernel_size = 6, strides = 2, padding = \"valid\")(conv1)\n        conv2 = activation(\"relu\")(conv2)\n    \n        conv3 = conv2d(128, kernel_size = 6, strides = 2, padding = \"valid\")(conv2)\n        conv3 = activation(\"relu\")(conv3)\n    \n        conv4 = conv2d(128, kernel_size = 6, strides = 2, padding = \"valid\")(conv3)\n        conv4 = activation(\"relu\")(conv4)\n    \n        fc1 = flatten()(conv4)\n        fc1 = dense(1024, activation = \"relu\")(fc1)\n    \n        fc2 = dense(2048, activation = \"linear\")(fc1)\n        \n        model = model(inputs = rain_inputs, outputs = fc2)\n        \n        return model\n    \n    # climate branch\n    def climatecnn(n_columns):\n        \n        # climate data (action \"one hot\" branch) / model 2\n        model = sequential()\n        model.add(dense(2048, input_dim = n_columns, activation = \"linear\"))\n        \n        return model\n    \n    # create the branches\n    climate = climatecnn(climate_data.shape[1])\n    rain = raincnn(110, 110, 12)\n    \n    # element wise multiplication to combine\n    multiplied = multiply()([rain.output, climate.output])\n    \n    # right side\n    fc4 = dense(1024, activation = \"linear\")(multiplied)\n    fc5 = dense(1152, activation = \"relu\")(fc4)\n    deconv0 = reshape((3,3,128))(fc5)\n    \n    deconv1 = conv2dtranspose(128, kernel_size = 6, strides = 2, padding = \"valid\")(deconv0)\n    deconv2 = conv2dtranspose(128, kernel_size = 6, strides = 2, padding = \"valid\")(deconv1)\n    deconv3 = conv2dtranspose(64, kernel_size = 6, strides = 2, padding = \"valid\")(deconv2)\n    \n    # output image\n    outputs = conv2dtranspose(1, kernel_size = 8, strides = 2, padding = \"valid\")(deconv3)\n    \n    model1 = model(inputs = [rain.input, climate.input], outputs = outputs)\n    \n    # call out model function\n    model1.compile(optimizer = 'adam', loss = 'logcosh', metrics = ['accuracy'])\n    \n    history = model1.fit([x_train, climate_train], y_train, epochs = 100, validation_data = ([x_val, climate_val], y_val), verbose = 2)\n\ni basically assumed this network to take the form of a multiple input/multi data type neural network so i followed the guide from this website: [https://heartbeat.fritz.ai/building-a-mixed-data-neural-network-in-keras-to-predict-accident-locations-d51a63b738cf](https://heartbeat.fritz.ai/building-a-mixed-data-neural-network-in-keras-to-predict-accident-locations-d51a63b738cf). obviously, i am quite new to machine learning so i'm worried if my code is not correct (i.e., doesn't replicate the video game network by stanford correctly).\n\ni have also tried emailing the original authors for their code but received no reply so here i am. thanks in advance guys!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/n7tvp1/d_need_help_checking_if_my_code_is_correct/',)", "identifyer": 5728518, "year": "2021"}, {"autor": "michelin_chalupa", "date": 1632203093000, "content": "[D] best practices for organizing experiments with DVC /!/ What are some of the best practices here? It seems that the most predictable way of organizing experiments, is to have one branch per experiment.\n\nIn order to reduce the number of branches, I've tried sequencing a number of experiments as commits along a single branch, but I've found it difficult to view all the results at once, since most viewing options just show those from the most recent experiment.\n\nFor those of you using DVC, how are you organizing experiments? \n\nHow do you keep new results organized, when rerunning experiments with updated data?\n\nHow do you build off experiments, along different axes? For example: if you run several experiments evaluating different architectures, and land on the best one; how do you go about organizing additional experiments to tune hyper parameters and such for the selected architecture?", "link": "https://www.reddit.com/r/MachineLearning/comments/pscjen/d_best_practices_for_organizing_experiments_with/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "branch", "selectorShort": "branch", "MarkedSent": "[d] best practices for organizing experiments with dvc /!/ what are some of the best practices here? it seems that the most predictable way of organizing experiments, is to have one -----> branch !!!  per experiment.\n\nin order to reduce the number of branches, i've tried sequencing a number of experiments as commits along a single branch, but i've found it difficult to view all the results at once, since most viewing options just show those from the most recent experiment.\n\nfor those of you using dvc, how are you organizing experiments? \n\nhow do you keep new results organized, when rerunning experiments with updated data?\n\nhow do you build off experiments, along different axes? for example: if you run several experiments evaluating different architectures, and land on the best one; how do you go about organizing additional experiments to tune hyper parameters and such for the selected architecture?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/pscjen/d_best_practices_for_organizing_experiments_with/',)", "identifyer": 5728736, "year": "2021"}, {"autor": "zjost85", "date": 1611672394000, "content": "[P] Guitar + ML /!/ I've recently been diving into the world of guitar circuit/amp modeling and where ML is starting to have an impact.  I am still working on the video, in which I interview a researcher from Neural DSP (Lauri Juvela), but have just published the sister blog post.  Would love to get any community feedback or questions, which we can hopefully get answered by Lauri.\n\n*For proper formatting, the article can be viewed on* [*blog.zakjost.com*](https://blog.zakjost.com/post/guitarml/)*. If you have questions, you can come ask them to me or Neural DSP researcher, Lauri Juvela, in this thread or the official discussion thread* [*here*](https://www.welcomeaioverlords.com/t/guitarml-w-lauri-juvela-from-neural-dsp/41)*.*\n\n## Introduction\n\nIf you play electric guitar, you probably know that \u201ctube amps\u201d have been the gold standard since the beginning. Originally, amplifiers were just about making the guitar louder, but it's become more than that\u2013it's about manipulating the *character* of the audio to make it sound more pleasing or achieve some artistic effect. What makes vacuum tubes so attractive is the peculiar non-linearities they introduce to the incoming signal as a byproduct of their interactions with the rest of the circuit when they're pushed to the limits of their operating ranges. Or \u201cdistortion\u201d for short. If you push a sine wave of 1 kHz through a tube amp at low levels, it will more or less faithfully reproduce the sine wave but at a higher amplitude. But if you increase the input level of that sine wave such that the amp doesn't have enough power to increase its amplitude, it will asymmetrically round off the peaks, which will in turn add *new frequency components* to the output at the harmonics (e.g. 2 kHz, 3 kHz, 4 kHz\u2026etc), and its this \u201cfingerprint\u201d of new harmonic content that people like, as some harmonics sound more pleasing than others.\n\nBut tube amps have drawbacks in almost all other facets: They're often expensive, big and heavy, rely on tech from the WWII era, need ongoing maintenance, operate at lethal voltage/current if you ever need to open one up\u2026etc. Additionally, they're not very versatile in the types of sounds you can get, as the sonic character of an amp is mostly determined by its circuit design. For these reasons, other technologies have been used to try to mimic the tube sound, but without all the hassles.\n\nThere is a lot of history here, but this post is going to focus on the recent work of leveraging Machine Learning to directly learn the audio processing characteristics of circuits. This stands in a modern context where digital modeling amplifiers like the Kemper, which rely on traditional Digital Signal Processing (DSP) techniques rather than Machine Learning, have seen mass adoption in the last several years. Here we will discuss what it means to digitally model an amplifier and how Machine Learning is beginning to make an impact. We'll start by discussing the basics of the problem from a Control Theory perspective and how DSP has approached the solution. We'll then present some work on how people are using Machine Learning to solve the problem as well as point to some open source projects so you can build your own ML-powered guitar circuit models.\n\n&amp;#x200B;\n\n*Processing img 4raaw2lnvod61...*\n\n## Traditional Methods\n\n## Control Theory basics\n\nControl Theory is about understanding how a system responds to input to generate output and e.g., how to use feedback to better control the system. This is a nice formalism for studying electronic circuits. The \u201ctransfer function\u201d is the function that specifies how inputs are transformed to outputs. If we can learn the transfer function of an amplifier, then that function is effectively a substitute for the amp. The process of using real-world data from a system to fit the parameters of a mathematical model of that system is called \u201csystem identification\u201d in this literature.\n\nThere are different branches of Control Theory to handle different types of systems. [Linear Time-Invariant (LTI) systems](https://en.wikipedia.org/wiki/Linear_time-invariant_system) are ones where the transfer function is linear and does not depend on time, and these are particularly simple to handle. For example, LTI systems have the property that the output is simply the result of a convolution operation on the input with the \u201cimpulse response\u201d function, which is relatively easy to obtain from data.\n\nWhile many audio-related circuits like an EQ section of an amplifier can be appropriately modeled as an LTI system, tube distortion is inherently non-linear *and* time-varying. It's non-linear because you get more than just the sum of its parts (harmonics in the output that didn't exist in the input) and it's time-varying because the output for a given input will depend on the *state of the system*, which varies with time. For example, capacitors charge and discharge at rates that depend on their component properties, and the amp will behave differently depending on how much charge this capacitor currently has stored, which depends on *previous inputs*. The result is that impulse responses are no longer sufficient for characterizing the system. This class of problems is much more difficult to mathematically model and there are a number of specialized techniques that have been developed that make various assumptions that are appropriate for a narrow set of problems.\n\n## Traditional Solutions\n\nOne of the approaches to solving this \u201cnon-linear time-varying system identification\u201d problem is to model the amplifier in blocks [1](https://blog.zakjost.com/post/guitarml/#fn:1). For example, the \u201cWiener-Hammerstein model\u201d has three blocks connected in series:\n\n1. A time-varying linear block\n2. A time-invariant non-linear block\n3. Another time-varying linear block\n\n[A Wiener-Hammerstein model.](https://preview.redd.it/r8ksrmwsvod61.jpg?width=576&amp;format=pjpg&amp;auto=webp&amp;s=ab9225779144bf1a38348cac180eb83f8730d532)\n\nOverall, this system is non-linear and time-varying as we need. By constraining its structure to these serially connected blocks we can separately solve for the parameters of each component at the cost of limiting the types of models we can obtain. For example, it's not clear how well this structure can model the time-varying non-linearities of tubes since all of the time-varying part is captured within the linear blocks and the non-linearity part is merely a static function, like an activation function in a neural network.\n\nThere is a spectrum of these types of solutions. A \u201cblackbox\u201d model would treat the real amp design as an unknown and merely attempt to map inputs to outputs. A \u201cwhitebox\u201d model would first do some circuit analysis of the amp and try to intelligently segment the circuit so that different functional blocks, like a gain stage, would have dedicated modeling. Once the model structure is set, the process of obtaining a model like this consists of capturing both input and output data of a real amplifier and estimating the parameters of these blocks.\n\nThere are other limitations of these methods in addition to the potential performance impact of the constrained solution space of the block models. Notably, the system identification process of fitting parameters based on real amplifier data will do so at a *single setting on the amp*, but the real amp has multiple knobs you can twist to change the circuit parameters and alter the sound. For example, the gain knob will control the amount of non-linear tube distortion. A different knob setting essentially requires a separate model, and the number of possible amp settings scales exponentially with the number of knobs, which is often more than 5.\n\nI'm not aware of all the ways that real-world systems like the Kemper solve for this, but it's clear that at least some of these problems are avoided by generically modeling things like EQ settings and copy/pasting that to all different amp models, rather than actually modeling how the knob of an amplifier interacts with the rest of the amp. In other words, they capture a single amp setting and apply standard DSP pre- and post-processing to approximate what the knobs would do.\n\n## ML for blackbox modeling\n\nUsing end-to-end Machine Learning in a blackbox setting affords new possibilities. First, there's no need to think about it from a Control Theory perspective and worry about what is time-varying or non-linear. It's just learning a function that directly maps inputs to outputs. Second, the values of the knobs can be just another input to the model and it's conceivable that a single model could be learned that meaningfully captured the interaction between the knob values and the sound of the real amp.\n\nLet's pause to think about audio data in general. Humans can typically hear frequencies between about 20 Hz and 20 kHz, which spans *3 orders of magnitude*. While you might need 48k points per second to accurately describe the highest frequencies, this is clearly way too much to describe the lower frequencies. Conversely, while you might need 10 ms of audio to capture a complete cycle of the lowest frequencies, this is clearly way too much than required to describe higher frequencies. But the very thing that defines \u201cthe tone\u201d of an amplifier is in how it responds to different frequencies and amplitudes, so we need to be able to represent and model this full spectrum. The first challenge is in figuring out how we can model relationships that operate at vastly different time-scales. (Translating this challenge to the visual domain, that would be like needing to model object details from meters all the way down to millimeters.)\n\nAnother challenge is around model inference speed. For a guitar player to be able to use the model in real-time, it needs to process the audio with around single-digit millisecond latency. This becomes a significant challenge when we have a throughput requirement of 48k samples per second and also need to somehow represent a sliding window of historical data since the amp is a time-varying system.\n\n## Model Architectures\n\nOne of the model designs that's popular in the literature for solving these problems is WaveNet [2](https://blog.zakjost.com/post/guitarml/#fn:2), which was originally developed at DeepMind to generate high quality audio of speech, like for the voices of Alexa or Siri. The key innovation in the WaveNet architecture is that the input audio is represented *hierarchically*, so that each layer uses a higher level summary of the audio. This allows deeper layers to see increasingly further back in time and ignore fine structure. These are called \u201cdilated convolutions\u201d and are implemented by having each layer skip 2 times as many inputs as the previous layer, resulting in a receptive field that increases exponentially with the number of layers.\n\n&amp;#x200B;\n\n[Diagram of Dilated Convolutions. From DeepMind's WaveNet paper \\[2\\]](https://preview.redd.it/nhqw0eqvvod61.jpg?width=1818&amp;format=pjpg&amp;auto=webp&amp;s=0a28c895cf09949c86ff3a42521034ed0a7af1d1)\n\nHowever, the processing required for these models is expensive and real-time performance is difficult to achieve. In reality, this constrains the size of models that can be used both in the depth (which controls receptive field) and number of convolutional channels [3](https://blog.zakjost.com/post/guitarml/#fn:3) [4](https://blog.zakjost.com/post/guitarml/#fn:4). These parameters are strongly correlated with final quality and so there exists a natural trade-off between quality and speed.\n\nOther papers in this space [4](https://blog.zakjost.com/post/guitarml/#fn:4) use a Recurrent Neural Net architecture like an LSTM to help combat some of these engineering challenges. This has the obvious advantage that there is a memory state that represents the past time steps so that a smaller chunk of audio can be used during inference, which eliminates the need to add more layers to increase the receptive field. These models did not perform quite as well in high gain settings when comparing loss values, but human listening tests scored them comparably.\n\nOverall it seems that there are multiple architectures that can solve this problem and it really comes down to finding those that can model it *efficiently* so that high quality and real-time performance can be achieved simultaneously with the given compute budget. This will perhaps become less of an issue as compute continues to scale.\n\n## Results\n\nIf you'd like to hear some systematic results that compare these approaches, there's [a demo page](http://research.spa.aalto.fi/publications/papers/applsci-deep/) for the LSTM paper [4](https://blog.zakjost.com/post/guitarml/#fn:4). Other than that, NeuralDSP is the leader in this space and have a number of incredible sounding demos. Here is their release video of the new plugin that was developed in partnership with Joe Duplantier of Gojira. If you want to test the state of the art, NeuralDSP offers limited free trials of their products.\n\nIf you'd like to test some open source pre-trained models or train your own models for use in a real-time plugin, the [GuitarML](https://github.com/GuitarML) project from Keith Bloemer brings together the efforts of many into a single place. The [SmartGuitarPedal](https://github.com/GuitarML/SmartGuitarPedal) repo has pre-trained overdrive pedals and [SmartGuitarAmp](https://github.com/GuitarML/SmartGuitarAmp) has multi-channel tube amp clones. All of these models are WaveNet based and integrate with any Digital Audio Workstation in the form of a standard VST plugin.\n\n## Conclusion\n\nBefore wrapping up, let me first give thanks to Lauri Juvela, who is a researcher at Neural DSP and an author on foundational papers in this field \\[3\\]\\[4\\], for taking the time to talk with me and share his expertise. I recorded my interview with him and will link to that soon. He is also gracious enough to continue the conversation with the broader community in [this thread](https://www.welcomeaioverlords.com/t/guitarml-w-lauri-juvela-from-neural-dsp/41) if you'd like to ask questions.\n\nLooking forward, I think we can safely say that Machine Learning will become a standard tool in the toolbox of digital amp modeling. Deep Learning in particular is almost perfectly suited for the challenge of jointly optimizing an end-to-end non-linear system on unstructured data. The traditional approaches achieve effective results, but are labor intensive and ripe for disruption. They also have limitations that ML approaches do not, like the inability to elegantly model the interactions of knobs on the amp.\n\nThere are still challenges that remain for ML-based solutions in this space. The first is just the engineering to get these models to run in real-time on the hardware that's available. This hurdle should only get smaller with time as software tools are built for making it easier to translate models to embedded systems and hardware innovations continue.\n\nThe second type of challenge is in learning \u201cthe art of ML\u201d in this domain. Problem domains like computer vision and NLP evolved a \u201cbag of tricks\u201d that, taken together, make big quality differences. In reading these papers, I see some of these nuances like: pre-emphasis filtering that makes the cost function more sensitive to higher frequencies, which humans perceive as louder; or having the LSTM layer predict the *residual* of output and input by summing a skip connection rather than directly mapping input to output. Some of these will be more important than others and it will take time and information sharing to get broader understanding.\n\nPerhaps most importantly, there's a data availability challenge, as the models can only be as good as the data. The cost of creating a high quality, systematic dataset of an expensive piece of gear is substantial. Entire businesses like [Top Jimi Profiles](https://topjimi.com/) exist to do that, where the hard part is going through the laborious process of setting up excellent sounding guitar signal chains and the easy part is running a tool suite to capture it (Kemper profiling in Top Jimi's case). Once this data is created, there's little incentive to share it. We'll need projects like GuitarML for progress to be made in the larger community rather than being confined within the walls of private institutions.\n\nThe bad news is also the great news: for us to make progress in this domain, we just need to fiddle with guitar gear more.\n\n## References\n\n1. Eichas, Felix, Stephan M\u00f6ller, and Udo Z\u00f6lzer. \u201c*Block-oriented gray box modeling of guitar amplifiers.*\u201d Proceedings of the International Conference on Digital Audio Effects (DAFx), Edinburgh, UK. 2017.[\u21a9](https://blog.zakjost.com/post/guitarml/#fnref:1)\n2. Oord, Aaron van den, et al. \u201c*Wavenet: A generative model for raw audio.*\u201d arXiv preprint arXiv:1609.03499 (2016).[\u21a9](https://blog.zakjost.com/post/guitarml/#fnref:2)\n3. E. Damsk\u00e4gg, L. Juvela, E. Thuillier and V. V\u00e4lim\u00e4ki, \u201c*Deep Learning for Tube Amplifier Emulation*,\u201d ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Brighton, United Kingdom, 2019, pp. 471-475, doi: 10.1109/ICASSP.2019.8682805..[\u21a9](https://blog.zakjost.com/post/guitarml/#fnref:3)\n4. Wright A, Damsk\u00e4gg E-P, Juvela L, V\u00e4lim\u00e4ki V. *Real-Time Guitar Amplifier Emulation with Deep Learning. Applied Sciences*. 2020; 10(3):766. [https://doi.org/10.3390/app10030766](https://doi.org/10.3390/app10030766)[\u21a9](https://blog.zakjost.com/post/guitarml/#fnref:4)", "link": "https://www.reddit.com/r/MachineLearning/comments/l5feqt/p_guitar_ml/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "branches", "selectorShort": "branch", "MarkedSent": "[p] guitar + ml /!/ i've recently been diving into the world of guitar circuit/amp modeling and where ml is starting to have an impact.  i am still working on the video, in which i interview a researcher from neural dsp (lauri juvela), but have just published the sister blog post.  would love to get any community feedback or questions, which we can hopefully get answered by lauri.\n\n*for proper formatting, the article can be viewed on* [*blog.zakjost.com*](https://blog.zakjost.com/post/guitarml/)*. if you have questions, you can come ask them to me or neural dsp researcher, lauri juvela, in this thread or the official discussion thread* [*here*](https://www.welcomeaioverlords.com/t/guitarml-w-lauri-juvela-from-neural-dsp/41)*.*\n\n## introduction\n\nif you play electric guitar, you probably know that \u201ctube amps\u201d have been the gold standard since the beginning. originally, amplifiers were just about making the guitar louder, but it's become more than that\u2013it's about manipulating the *character* of the audio to make it sound more pleasing or achieve some artistic effect. what makes vacuum tubes so attractive is the peculiar non-linearities they introduce to the incoming signal as a byproduct of their interactions with the rest of the circuit when they're pushed to the limits of their operating ranges. or \u201cdistortion\u201d for short. if you push a sine wave of 1 khz through a tube amp at low levels, it will more or less faithfully reproduce the sine wave but at a higher amplitude. but if you increase the input level of that sine wave such that the amp doesn't have enough power to increase its amplitude, it will asymmetrically round off the peaks, which will in turn add *new frequency components* to the output at the harmonics (e.g. 2 khz, 3 khz, 4 khz\u2026etc), and its this \u201cfingerprint\u201d of new harmonic content that people like, as some harmonics sound more pleasing than others.\n\nbut tube amps have drawbacks in almost all other facets: they're often expensive, big and heavy, rely on tech from the wwii era, need ongoing maintenance, operate at lethal voltage/current if you ever need to open one up\u2026etc. additionally, they're not very versatile in the types of sounds you can get, as the sonic character of an amp is mostly determined by its circuit design. for these reasons, other technologies have been used to try to mimic the tube sound, but without all the hassles.\n\nthere is a lot of history here, but this post is going to focus on the recent work of leveraging machine learning to directly learn the audio processing characteristics of circuits. this stands in a modern context where digital modeling amplifiers like the kemper, which rely on traditional digital signal processing (dsp) techniques rather than machine learning, have seen mass adoption in the last several years. here we will discuss what it means to digitally model an amplifier and how machine learning is beginning to make an impact. we'll start by discussing the basics of the problem from a control theory perspective and how dsp has approached the solution. we'll then present some work on how people are using machine learning to solve the problem as well as point to some open source projects so you can build your own ml-powered guitar circuit models.\n\n&amp;#x200b;\n\n*processing img 4raaw2lnvod61...*\n\n## traditional methods\n\n## control theory basics\n\ncontrol theory is about understanding how a system responds to input to generate output and e.g., how to use feedback to better control the system. this is a nice formalism for studying electronic circuits. the \u201ctransfer function\u201d is the function that specifies how inputs are transformed to outputs. if we can learn the transfer function of an amplifier, then that function is effectively a substitute for the amp. the process of using real-world data from a system to fit the parameters of a mathematical model of that system is called \u201csystem identification\u201d in this literature.\n\nthere are different -----> branches !!!  of control theory to handle different types of systems. [linear time-invariant (lti) systems](https://en.wikipedia.org/wiki/linear_time-invariant_system) are ones where the transfer function is linear and does not depend on time, and these are particularly simple to handle. for example, lti systems have the property that the output is simply the result of a convolution operation on the input with the \u201cimpulse response\u201d function, which is relatively easy to obtain from data.\n\nwhile many audio-related circuits like an eq section of an amplifier can be appropriately modeled as an lti system, tube distortion is inherently non-linear *and* time-varying. it's non-linear because you get more than just the sum of its parts (harmonics in the output that didn't exist in the input) and it's time-varying because the output for a given input will depend on the *state of the system*, which varies with time. for example, capacitors charge and discharge at rates that depend on their component properties, and the amp will behave differently depending on how much charge this capacitor currently has stored, which depends on *previous inputs*. the result is that impulse responses are no longer sufficient for characterizing the system. this class of problems is much more difficult to mathematically model and there are a number of specialized techniques that have been developed that make various assumptions that are appropriate for a narrow set of problems.\n\n## traditional solutions\n\none of the approaches to solving this \u201cnon-linear time-varying system identification\u201d problem is to model the amplifier in blocks [1](https://blog.zakjost.com/post/guitarml/#fn:1). for example, the \u201cwiener-hammerstein model\u201d has three blocks connected in series:\n\n1. a time-varying linear block\n2. a time-invariant non-linear block\n3. another time-varying linear block\n\n[a wiener-hammerstein model.](https://preview.redd.it/r8ksrmwsvod61.jpg?width=576&amp;format=pjpg&amp;auto=webp&amp;s=ab9225779144bf1a38348cac180eb83f8730d532)\n\noverall, this system is non-linear and time-varying as we need. by constraining its structure to these serially connected blocks we can separately solve for the parameters of each component at the cost of limiting the types of models we can obtain. for example, it's not clear how well this structure can model the time-varying non-linearities of tubes since all of the time-varying part is captured within the linear blocks and the non-linearity part is merely a static function, like an activation function in a neural network.\n\nthere is a spectrum of these types of solutions. a \u201cblackbox\u201d model would treat the real amp design as an unknown and merely attempt to map inputs to outputs. a \u201cwhitebox\u201d model would first do some circuit analysis of the amp and try to intelligently segment the circuit so that different functional blocks, like a gain stage, would have dedicated modeling. once the model structure is set, the process of obtaining a model like this consists of capturing both input and output data of a real amplifier and estimating the parameters of these blocks.\n\nthere are other limitations of these methods in addition to the potential performance impact of the constrained solution space of the block models. notably, the system identification process of fitting parameters based on real amplifier data will do so at a *single setting on the amp*, but the real amp has multiple knobs you can twist to change the circuit parameters and alter the sound. for example, the gain knob will control the amount of non-linear tube distortion. a different knob setting essentially requires a separate model, and the number of possible amp settings scales exponentially with the number of knobs, which is often more than 5.\n\ni'm not aware of all the ways that real-world systems like the kemper solve for this, but it's clear that at least some of these problems are avoided by generically modeling things like eq settings and copy/pasting that to all different amp models, rather than actually modeling how the knob of an amplifier interacts with the rest of the amp. in other words, they capture a single amp setting and apply standard dsp pre- and post-processing to approximate what the knobs would do.\n\n## ml for blackbox modeling\n\nusing end-to-end machine learning in a blackbox setting affords new possibilities. first, there's no need to think about it from a control theory perspective and worry about what is time-varying or non-linear. it's just learning a function that directly maps inputs to outputs. second, the values of the knobs can be just another input to the model and it's conceivable that a single model could be learned that meaningfully captured the interaction between the knob values and the sound of the real amp.\n\nlet's pause to think about audio data in general. humans can typically hear frequencies between about 20 hz and 20 khz, which spans *3 orders of magnitude*. while you might need 48k points per second to accurately describe the highest frequencies, this is clearly way too much to describe the lower frequencies. conversely, while you might need 10 ms of audio to capture a complete cycle of the lowest frequencies, this is clearly way too much than required to describe higher frequencies. but the very thing that defines \u201cthe tone\u201d of an amplifier is in how it responds to different frequencies and amplitudes, so we need to be able to represent and model this full spectrum. the first challenge is in figuring out how we can model relationships that operate at vastly different time-scales. (translating this challenge to the visual domain, that would be like needing to model object details from meters all the way down to millimeters.)\n\nanother challenge is around model inference speed. for a guitar player to be able to use the model in real-time, it needs to process the audio with around single-digit millisecond latency. this becomes a significant challenge when we have a throughput requirement of 48k samples per second and also need to somehow represent a sliding window of historical data since the amp is a time-varying system.\n\n## model architectures\n\none of the model designs that's popular in the literature for solving these problems is wavenet [2](https://blog.zakjost.com/post/guitarml/#fn:2), which was originally developed at deepmind to generate high quality audio of speech, like for the voices of alexa or siri. the key innovation in the wavenet architecture is that the input audio is represented *hierarchically*, so that each layer uses a higher level summary of the audio. this allows deeper layers to see increasingly further back in time and ignore fine structure. these are called \u201cdilated convolutions\u201d and are implemented by having each layer skip 2 times as many inputs as the previous layer, resulting in a receptive field that increases exponentially with the number of layers.\n\n&amp;#x200b;\n\n[diagram of dilated convolutions. from deepmind's wavenet paper \\[2\\]](https://preview.redd.it/nhqw0eqvvod61.jpg?width=1818&amp;format=pjpg&amp;auto=webp&amp;s=0a28c895cf09949c86ff3a42521034ed0a7af1d1)\n\nhowever, the processing required for these models is expensive and real-time performance is difficult to achieve. in reality, this constrains the size of models that can be used both in the depth (which controls receptive field) and number of convolutional channels [3](https://blog.zakjost.com/post/guitarml/#fn:3) [4](https://blog.zakjost.com/post/guitarml/#fn:4). these parameters are strongly correlated with final quality and so there exists a natural trade-off between quality and speed.\n\nother papers in this space [4](https://blog.zakjost.com/post/guitarml/#fn:4) use a recurrent neural net architecture like an lstm to help combat some of these engineering challenges. this has the obvious advantage that there is a memory state that represents the past time steps so that a smaller chunk of audio can be used during inference, which eliminates the need to add more layers to increase the receptive field. these models did not perform quite as well in high gain settings when comparing loss values, but human listening tests scored them comparably.\n\noverall it seems that there are multiple architectures that can solve this problem and it really comes down to finding those that can model it *efficiently* so that high quality and real-time performance can be achieved simultaneously with the given compute budget. this will perhaps become less of an issue as compute continues to scale.\n\n## results\n\nif you'd like to hear some systematic results that compare these approaches, there's [a demo page](http://research.spa.aalto.fi/publications/papers/applsci-deep/) for the lstm paper [4](https://blog.zakjost.com/post/guitarml/#fn:4). other than that, neuraldsp is the leader in this space and have a number of incredible sounding demos. here is their release video of the new plugin that was developed in partnership with joe duplantier of gojira. if you want to test the state of the art, neuraldsp offers limited free trials of their products.\n\nif you'd like to test some open source pre-trained models or train your own models for use in a real-time plugin, the [guitarml](https://github.com/guitarml) project from keith bloemer brings together the efforts of many into a single place. the [smartguitarpedal](https://github.com/guitarml/smartguitarpedal) repo has pre-trained overdrive pedals and [smartguitaramp](https://github.com/guitarml/smartguitaramp) has multi-channel tube amp clones. all of these models are wavenet based and integrate with any digital audio workstation in the form of a standard vst plugin.\n\n## conclusion\n\nbefore wrapping up, let me first give thanks to lauri juvela, who is a researcher at neural dsp and an author on foundational papers in this field \\[3\\]\\[4\\], for taking the time to talk with me and share his expertise. i recorded my interview with him and will link to that soon. he is also gracious enough to continue the conversation with the broader community in [this thread](https://www.welcomeaioverlords.com/t/guitarml-w-lauri-juvela-from-neural-dsp/41) if you'd like to ask questions.\n\nlooking forward, i think we can safely say that machine learning will become a standard tool in the toolbox of digital amp modeling. deep learning in particular is almost perfectly suited for the challenge of jointly optimizing an end-to-end non-linear system on unstructured data. the traditional approaches achieve effective results, but are labor intensive and ripe for disruption. they also have limitations that ml approaches do not, like the inability to elegantly model the interactions of knobs on the amp.\n\nthere are still challenges that remain for ml-based solutions in this space. the first is just the engineering to get these models to run in real-time on the hardware that's available. this hurdle should only get smaller with time as software tools are built for making it easier to translate models to embedded systems and hardware innovations continue.\n\nthe second type of challenge is in learning \u201cthe art of ml\u201d in this domain. problem domains like computer vision and nlp evolved a \u201cbag of tricks\u201d that, taken together, make big quality differences. in reading these papers, i see some of these nuances like: pre-emphasis filtering that makes the cost function more sensitive to higher frequencies, which humans perceive as louder; or having the lstm layer predict the *residual* of output and input by summing a skip connection rather than directly mapping input to output. some of these will be more important than others and it will take time and information sharing to get broader understanding.\n\nperhaps most importantly, there's a data availability challenge, as the models can only be as good as the data. the cost of creating a high quality, systematic dataset of an expensive piece of gear is substantial. entire businesses like [top jimi profiles](https://topjimi.com/) exist to do that, where the hard part is going through the laborious process of setting up excellent sounding guitar signal chains and the easy part is running a tool suite to capture it (kemper profiling in top jimi's case). once this data is created, there's little incentive to share it. we'll need projects like guitarml for progress to be made in the larger community rather than being confined within the walls of private institutions.\n\nthe bad news is also the great news: for us to make progress in this domain, we just need to fiddle with guitar gear more.\n\n## references\n\n1. eichas, felix, stephan m\u00f6ller, and udo z\u00f6lzer. \u201c*block-oriented gray box modeling of guitar amplifiers.*\u201d proceedings of the international conference on digital audio effects (dafx), edinburgh, uk. 2017.[\u21a9](https://blog.zakjost.com/post/guitarml/#fnref:1)\n2. oord, aaron van den, et al. \u201c*wavenet: a generative model for raw audio.*\u201d arxiv preprint arxiv:1609.03499 (2016).[\u21a9](https://blog.zakjost.com/post/guitarml/#fnref:2)\n3. e. damsk\u00e4gg, l. juvela, e. thuillier and v. v\u00e4lim\u00e4ki, \u201c*deep learning for tube amplifier emulation*,\u201d icassp 2019 - 2019 ieee international conference on acoustics, speech and signal processing (icassp), brighton, united kingdom, 2019, pp. 471-475, doi: 10.1109/icassp.2019.8682805..[\u21a9](https://blog.zakjost.com/post/guitarml/#fnref:3)\n4. wright a, damsk\u00e4gg e-p, juvela l, v\u00e4lim\u00e4ki v. *real-time guitar amplifier emulation with deep learning. applied sciences*. 2020; 10(3):766. [https://doi.org/10.3390/app10030766](https://doi.org/10.3390/app10030766)[\u21a9](https://blog.zakjost.com/post/guitarml/#fnref:4)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/l5feqt/p_guitar_ml/',)", "identifyer": 5730071, "year": "2021"}, {"autor": "Bubribri", "date": 1619542715000, "content": "[R] Ideas for a Master Thesis (Law) in AI/ML. What do you think is a good topic that requires attention? Specific troubles companies face in their research/development tasks; uncertain aspects that bring insecurity to their processes; ethical regulatory discussions. Please, share your thoughts. /!/ I\u2019m currently pursuing a Master in Legal Practice in order to be able to access the bar exam in my country (Western Europe). For some years I have felt strongly attracted to tech, to the point of considering making a drastic career change and directing myself to STEM (one of my options when I had to choose what studies to go for years ago, as I went through the technical/scientific branch before University). In any case, at this point, I want to finish this Master\u2019s and have the possibility to become a lawyer. \n\nIf I continue in the Law path I have it clear that I want to be as close to tech as possible. Ideally, I\u2019d like to collaborate with companies/projects that develop AI or work with ML. I\u2019m specially interested in health tech, although that is only a sectorial ideal preference, and one can\u2019t always choose. What I can choose is the topic for my final thesis. I\u2019d like it to be in something useful to start my academic approach to these fields, and that hopefully will be useful to continue my education/career (whether I decide to enroll in a specialization Master in those issues; to get a specialized internship; to have more knowledge I can present to future employers; whatever). \n\nWhat would you say is an interesting, needed, in development issue regarding regulation/law and AI/ML? I can also consider ethical, more abstract discussions. Do you know of some specific trouble companies are currently having in their developer/research tasks? What do you think needs more urgent intervention, or what do you think will be the main source of litigation/conflict in the near future? \n\nI appreciate all insights and ideas! Thanks a lot.", "link": "https://www.reddit.com/r/MachineLearning/comments/mzt18l/r_ideas_for_a_master_thesis_law_in_aiml_what_do/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "branch", "selectorShort": "branch", "MarkedSent": "[r] ideas for a master thesis (law) in ai/ml. what do you think is a good topic that requires attention? specific troubles companies face in their research/development tasks; uncertain aspects that bring insecurity to their processes; ethical regulatory discussions. please, share your thoughts. /!/ i\u2019m currently pursuing a master in legal practice in order to be able to access the bar exam in my country (western europe). for some years i have felt strongly attracted to tech, to the point of considering making a drastic career change and directing myself to stem (one of my options when i had to choose what studies to go for years ago, as i went through the technical/scientific -----> branch !!!  before university). in any case, at this point, i want to finish this master\u2019s and have the possibility to become a lawyer. \n\nif i continue in the law path i have it clear that i want to be as close to tech as possible. ideally, i\u2019d like to collaborate with companies/projects that develop ai or work with ml. i\u2019m specially interested in health tech, although that is only a sectorial ideal preference, and one can\u2019t always choose. what i can choose is the topic for my final thesis. i\u2019d like it to be in something useful to start my academic approach to these fields, and that hopefully will be useful to continue my education/career (whether i decide to enroll in a specialization master in those issues; to get a specialized internship; to have more knowledge i can present to future employers; whatever). \n\nwhat would you say is an interesting, needed, in development issue regarding regulation/law and ai/ml? i can also consider ethical, more abstract discussions. do you know of some specific trouble companies are currently having in their developer/research tasks? what do you think needs more urgent intervention, or what do you think will be the main source of litigation/conflict in the near future? \n\ni appreciate all insights and ideas! thanks a lot.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/mzt18l/r_ideas_for_a_master_thesis_law_in_aiml_what_do/',)", "identifyer": 5732101, "year": "2021"}, {"autor": "A_Time_Space_Person", "date": 1635080172000, "content": "[Discussion] Do you need to know math as good as a physicist in order to be good in machine learning? /!/ I am a computer scientist. That is, I have a bachelor's in Information Systems and I have a master's in Computer Science.\n\nHere's my experience with various branches of mathematics related to machine learning:  \n\n\n* **Calculus**: I understand deritvatives and integrals. I understand partial derivatives. I fumble a bit when I have double or triple integrals, but I understand what they mean. I don't have any experience in differential equations.\n* **Linear algebra**: I understand vector spaces, vectors and various matrix decompositions. I'd say I'm solid here.\n* **Probability**: I will most likely mess something up when calculating how many full house combinations there are in a set of poker cards. However, I do understand Bayes' rule. I understand the concept of probability distributions. I am aware of different probability distributions and what they are used for. I have a grasp of maximum likelihood estimation and maximum aposteriori. I do sometimes get perplexed when reading formulas for autoencoder, but I manage to get the point.\n* **Statistics**: I understand p-value tests, various types of averages, various types of samplings etc.\n\n**My ultimate goal is to be a freelancer and work on websites such as Toptal.** Or start my own startup, but that's a different thing.\n\n**My question:** Should I learn math in my free time (such as from [this book](https://www.amazon.com/Mathematical-Methods-Physics-Engineering-Comprehensive/dp/0521679710/ref=pd_vtp_2/135-6904752-3002439?pd_rd_w=geVXI&amp;pf_rd_p=96226b5f-2d9a-439b-be45-97603787c682&amp;pf_rd_r=91K7WFXNRD5YDDZMQEAC&amp;pd_rd_r=0c1e4405-f909-42b0-932d-aca0fc742558&amp;pd_rd_wg=OC9xp&amp;pd_rd_i=0521679710&amp;psc=1), which is for physicists)? I personally think I know enough for what I want to do (freelancing). I think learning more math would be required if I wanted to do a PhD or something, but since I don't want to do it, I think reading the linked book may be an overkill.\n\n**What's your 2 cents?**", "link": "https://www.reddit.com/r/MachineLearning/comments/qermmy/discussion_do_you_need_to_know_math_as_good_as_a/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "branches", "selectorShort": "branch", "MarkedSent": "[discussion] do you need to know math as good as a physicist in order to be good in machine learning? /!/ i am a computer scientist. that is, i have a bachelor's in information systems and i have a master's in computer science.\n\nhere's my experience with various -----> branches !!!  of mathematics related to machine learning:  \n\n\n* **calculus**: i understand deritvatives and integrals. i understand partial derivatives. i fumble a bit when i have double or triple integrals, but i understand what they mean. i don't have any experience in differential equations.\n* **linear algebra**: i understand vector spaces, vectors and various matrix decompositions. i'd say i'm solid here.\n* **probability**: i will most likely mess something up when calculating how many full house combinations there are in a set of poker cards. however, i do understand bayes' rule. i understand the concept of probability distributions. i am aware of different probability distributions and what they are used for. i have a grasp of maximum likelihood estimation and maximum aposteriori. i do sometimes get perplexed when reading formulas for autoencoder, but i manage to get the point.\n* **statistics**: i understand p-value tests, various types of averages, various types of samplings etc.\n\n**my ultimate goal is to be a freelancer and work on websites such as toptal.** or start my own startup, but that's a different thing.\n\n**my question:** should i learn math in my free time (such as from [this book](https://www.amazon.com/mathematical-methods-physics-engineering-comprehensive/dp/0521679710/ref=pd_vtp_2/135-6904752-3002439?pd_rd_w=gevxi&amp;pf_rd_p=96226b5f-2d9a-439b-be45-97603787c682&amp;pf_rd_r=91k7wfxnrd5yddzmqeac&amp;pd_rd_r=0c1e4405-f909-42b0-932d-aca0fc742558&amp;pd_rd_wg=oc9xp&amp;pd_rd_i=0521679710&amp;psc=1), which is for physicists)? i personally think i know enough for what i want to do (freelancing). i think learning more math would be required if i wanted to do a phd or something, but since i don't want to do it, i think reading the linked book may be an overkill.\n\n**what's your 2 cents?**", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/qermmy/discussion_do_you_need_to_know_math_as_good_as_a/',)", "identifyer": 5733376, "year": "2021"}, {"autor": "PsycAndrew", "date": 1631110438000, "content": "[D] Where to Start in Neural Networks? /!/  I have a strong background in statistics, but I've mostly focused on forms of regression in my work. I would like to branch out into Neural Networks; Any books? websites? Articles you could recommend to get started? Thanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/pkbca8/d_where_to_start_in_neural_networks/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "branch", "selectorShort": "branch", "MarkedSent": "[d] where to start in neural networks? /!/  i have a strong background in statistics, but i've mostly focused on forms of regression in my work. i would like to -----> branch !!!  out into neural networks; any books? websites? articles you could recommend to get started? thanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 5, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/pkbca8/d_where_to_start_in_neural_networks/',)", "identifyer": 5734200, "year": "2021"}, {"autor": "Bibbidi_Babbidi_Boo", "date": 1628750706000, "content": "[Help][D] Fresh PhD student looking for current research topics to start off with /!/ Hey guys, \n\nI'm an incoming PhD student in AI and robotics. Previously my background was in robotics with classical stuff like path planning, control, etc. Now I am looking to shift towards the AI field.\n\nInitially I was planning on working in an intersection between AI and robotics, but the more I think about it the less likely it seems possible. Most robotics researchers feel Deep Learning models cannot outperform classical systems, and dont prefer learning in their work. On the other hand, robotics as a huge research area isnt really wide spread in the industry so Im not really sure what'd be my direction after PhD.\n\nSo now, I'm trying to find new topics in AI that might interest me, and also is a part of the current research. I understand this is a very broad question, and there are several branches in this field, which is my very reason for asking. I would like to know current SOTA and research in each subfield  like NLP, CV, etc. Also if anybody feels Im wrong about the above (AI and robotics not mixing well, and robotics not being a big industry yet), do comment as well, and let me know of any work that involves the two of these. \n\nThanks for your time", "link": "https://www.reddit.com/r/MachineLearning/comments/p2ufz8/helpd_fresh_phd_student_looking_for_current/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "branches", "selectorShort": "branch", "MarkedSent": "[help][d] fresh phd student looking for current research topics to start off with /!/ hey guys, \n\ni'm an incoming phd student in ai and robotics. previously my background was in robotics with classical stuff like path planning, control, etc. now i am looking to shift towards the ai field.\n\ninitially i was planning on working in an intersection between ai and robotics, but the more i think about it the less likely it seems possible. most robotics researchers feel deep learning models cannot outperform classical systems, and dont prefer learning in their work. on the other hand, robotics as a huge research area isnt really wide spread in the industry so im not really sure what'd be my direction after phd.\n\nso now, i'm trying to find new topics in ai that might interest me, and also is a part of the current research. i understand this is a very broad question, and there are several -----> branches !!!  in this field, which is my very reason for asking. i would like to know current sota and research in each subfield  like nlp, cv, etc. also if anybody feels im wrong about the above (ai and robotics not mixing well, and robotics not being a big industry yet), do comment as well, and let me know of any work that involves the two of these. \n\nthanks for your time", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 34, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/p2ufz8/helpd_fresh_phd_student_looking_for_current/',)", "identifyer": 5734646, "year": "2021"}, {"autor": "hardmaru", "date": 1628750531000, "content": "[R] Single cortical neurons as deep artificial neural networks /!/ New Neuron [paper](https://www.sciencedirect.com/science/article/pii/S0896627321005018?dgcid=coauthor) analyzes the computational complexity of single biological cortical neurons as a deep neural network.\n\n**Brief Summary**\n\nUsing a modern machine learning approach, we show that the I/O characteristics of cortical pyramidal neurons can be approximated, at the millisecond resolution (single spike precision), by a temporally convolutional neural network with five to eight layers.This computational complexity stems mainly from the interplay between NMDA receptors and dendritic morphology.\n\n**Highlights**\n\n- Cortical neurons are well approximated by a deep neural network (DNN) with 5\u20138 layers\n\n- DNN\u2019s depth arises from the interaction between NMDA receptors and dendritic morphology\n\n- Dendritic branches can be conceptualized as a set of spatiotemporal pattern detectors\n\n- We provide a unified method to assess the computational complexity of any neuron type\n\nLink to paper (web): https://www.sciencedirect.com/science/article/pii/S0896627321005018?dgcid=coauthor\n\nLink to pdf: https://www.sciencedirect.com/science/article/pii/S0896627321005018/pdfft?md5=2623a408d97c4bcefed0d93345efeeb6&amp;pid=1-s2.0-S0896627321005018-main.pdf\n\nSome discussion here: https://twitter.com/KordingLab/status/1425511953439485953", "link": "https://www.reddit.com/r/MachineLearning/comments/p2ueou/r_single_cortical_neurons_as_deep_artificial/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "branches", "selectorShort": "branch", "MarkedSent": "[r] single cortical neurons as deep artificial neural networks /!/ new neuron [paper](https://www.sciencedirect.com/science/article/pii/s0896627321005018?dgcid=coauthor) analyzes the computational complexity of single biological cortical neurons as a deep neural network.\n\n**brief summary**\n\nusing a modern machine learning approach, we show that the i/o characteristics of cortical pyramidal neurons can be approximated, at the millisecond resolution (single spike precision), by a temporally convolutional neural network with five to eight layers.this computational complexity stems mainly from the interplay between nmda receptors and dendritic morphology.\n\n**highlights**\n\n- cortical neurons are well approximated by a deep neural network (dnn) with 5\u20138 layers\n\n- dnn\u2019s depth arises from the interaction between nmda receptors and dendritic morphology\n\n- dendritic -----> branches !!!  can be conceptualized as a set of spatiotemporal pattern detectors\n\n- we provide a unified method to assess the computational complexity of any neuron type\n\nlink to paper (web): https://www.sciencedirect.com/science/article/pii/s0896627321005018?dgcid=coauthor\n\nlink to pdf: https://www.sciencedirect.com/science/article/pii/s0896627321005018/pdfft?md5=2623a408d97c4bcefed0d93345efeeb6&amp;pid=1-s2.0-s0896627321005018-main.pdf\n\nsome discussion here: https://twitter.com/kordinglab/status/1425511953439485953", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 40, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/p2ueou/r_single_cortical_neurons_as_deep_artificial/',)", "identifyer": 5734647, "year": "2021"}, {"autor": "RawCS", "date": 1628114379000, "content": "[D] What masters level statistics courses are crucial for building a strong foundation for machine learning? /!/ Right now I am finishing the foundational courses in my masters program, Texas A&amp;M, and am getting to branch in to the electives courses. I am a software engineer who works with big data, but haven\u2019t done a lot of ML. I am super comfortable with Python though, as that\u2019s the main language I work with. So, I am curious what classes I should take to have a good baseline for machine learning/AI work in the future.", "link": "https://www.reddit.com/r/MachineLearning/comments/oy3fqn/d_what_masters_level_statistics_courses_are/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "branch", "selectorShort": "branch", "MarkedSent": "[d] what masters level statistics courses are crucial for building a strong foundation for machine learning? /!/ right now i am finishing the foundational courses in my masters program, texas a&amp;m, and am getting to -----> branch !!!  in to the electives courses. i am a software engineer who works with big data, but haven\u2019t done a lot of ml. i am super comfortable with python though, as that\u2019s the main language i work with. so, i am curious what classes i should take to have a good baseline for machine learning/ai work in the future.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/oy3fqn/d_what_masters_level_statistics_courses_are/',)", "identifyer": 5735017, "year": "2021"}, {"autor": "KirillTheMunchKing", "date": 1632098140000, "content": "[D] Object-NeRF Paper Explained - Learning Object-Compositional Neural Radiance Field for Editable Scene Rendering (5-minute summary) /!/ [Object-NeRF](https://preview.redd.it/aycfasmvzjo71.png?width=600&amp;format=png&amp;auto=webp&amp;s=20d14fecfca1cb6c18244b88f31b9e8d56ed1d2d)\n\nNeRF models have come a long way since the initial \u201cexplosion\u201d last year. Yet one of the things they still can\u2019t quite handle is scene compositionality, meaning that the model is not aware of the distinct objects that make up the scene. Object NeRF aims to tackle this issue using a dual-branch model that separately encodes the global context of the scene and each object in it. This approach not only reaches competitive levels of quality with current SOTA methods on static scenes but also enables object-level editing. For example, adding or moving furniture in a real-world scene.\n\nCheck out the [full paper summary](https://www.casualganpapers.com/multi-object-nerf-3d-scene-editing/Object-NeRF-explained.html) on Casual GAN Papers (Reading time \\~5 minutes).\n\nSubscribe to [my channel](https://t.me/casual_gan) and follow me on [Twitter](https://twitter.com/KirillDemochkin) for weekly AI paper summaries!", "link": "https://www.reddit.com/r/MachineLearning/comments/prk1xx/d_objectnerf_paper_explained_learning/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "branch", "selectorShort": "branch", "MarkedSent": "[d] object-nerf paper explained - learning object-compositional neural radiance field for editable scene rendering (5-minute summary) /!/ [object-nerf](https://preview.redd.it/aycfasmvzjo71.png?width=600&amp;format=png&amp;auto=webp&amp;s=20d14fecfca1cb6c18244b88f31b9e8d56ed1d2d)\n\nnerf models have come a long way since the initial \u201cexplosion\u201d last year. yet one of the things they still can\u2019t quite handle is scene compositionality, meaning that the model is not aware of the distinct objects that make up the scene. object nerf aims to tackle this issue using a dual------> branch !!!  model that separately encodes the global context of the scene and each object in it. this approach not only reaches competitive levels of quality with current sota methods on static scenes but also enables object-level editing. for example, adding or moving furniture in a real-world scene.\n\ncheck out the [full paper summary](https://www.casualganpapers.com/multi-object-nerf-3d-scene-editing/object-nerf-explained.html) on casual gan papers (reading time \\~5 minutes).\n\nsubscribe to [my channel](https://t.me/casual_gan) and follow me on [twitter](https://twitter.com/kirilldemochkin) for weekly ai paper summaries!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/prk1xx/d_objectnerf_paper_explained_learning/',)", "identifyer": 5736099, "year": "2021"}, {"autor": "Vivoka_", "date": 1615394298000, "content": "[R] How to design a good user Voice User Experience (VUX) /!/ &amp;#x200B;\n\nhttps://preview.redd.it/zeu7tih2b8m61.jpg?width=1366&amp;format=pjpg&amp;auto=webp&amp;s=8f935154bffc92214374cfcecac9064a90e43aaf\n\nWith the advent of [speech recognition](https://vivoka.com/automatic-speech-recognition/)  and its democratization with voice assistants, new uses have emerged.  These new habits, driven by the advantages of this new human-machine  interface, have profoundly rethought the way people design products and  services. For example, search engine results are changing rapidly to  incorporate voice requests. With this rapid evolution of consumers, a  delay has naturally set in in the practices of companies. This imbalance  has given rise to new branches of marketing, namely Voice Marketing and  Voice UX (VUX).\n\nHow to think about your [vocal experience](https://vivoka.com/voice-development-kit/) (VUX)? How to understand these new practices related to voice assistants?\n\n# What is Voice Marketing and Voice UX (VUX)?\n\nVoice  Marketing covers all brand experiences and contact points enabled by  voice technologies, in the same way that digital marketing has emerged  via the Internet and smartphones. This new form of marketing improves  the customer experience by being more engaging and fun. In addition, it  makes it possible to deliver the brand message to the consumer\u2019s home  through uses that constitute real added value for the consumer.\n\nThis  brings us to the VUX which refers to the art and way of designing vocal  experiences designed for (and often by) users. Also called \u201cvoice  design\u201d, it refers to the development of the user experience as well as  the sound design of the experience but also to other considerations such  as potential gamification.\n\nIt is a set of principles to provide a friction-free and truly engaging experience for the user.\n\n# How do we think about VUX strategies ?\n\nThe  Voice Marketing aspect is the result of consultation with the brand and  must respect both the brand discourse and the tone used in its other  communication channels. This aspect varies according to the brand\u2019s  expectations, which may be attached to a more experiential or ROI  dimension, although the two are not incompatible, it is even the  opposite.\n\nAs  far as VUX is concerned, it is more the result of an internal  reflection that adapts to the brand\u2019s objectives. Some principles are  unavoidable, whatever the experience and some are based on common sense.  For example, it is counterproductive to offer more than three options  to the user within a single interaction in the sense that the user will  have difficulty retaining more than three.\n\n# What does it consist of ?\n\nThe  research part is a major step in the creation of an application, it is  even the most important part. It is included in the design design  process, it must be methodical with a focus on the end user. Over  several stages, many hypotheses will be tested. This makes it possible  to determine the nature of the application: from an emotional point of  view, from a functional point of view, the tone and nature of the voice\u2026\n\nIt  also covers many considerations and prerequisites, namely: the choice  of voice (we obviously recommend the use of a human actor\u2019s voice) but  also the sound universe or sound branding. Sound branding is expressed  through music, sound effects, earcons (the audio equivalent of icons),  ASMR, etc.\n\nAs  mentioned above, it is also important to respect conversational design  rules in order to make the user experience as pleasant as possible.\n\n# What should we pay particular attention to ?\n\nAs  voice technology is still a relatively new field that remains to be  conquered for most brands, every aspect of the experience is important.  The most important axis is probably the design because nothing is more  frustrating than an experience with which it is difficult or even  impossible to interact.\n\nBeyond  that, the choice of use case is also of crucial importance. Without  careful prior reflection, the experience created may not meet the  brand\u2019s expectations while creating no added value for the end user.\n\n# What are the best practices today, the case studies in this area ?\n\nBe  aware of the limits of technology even if they are likely to evolve, it  is useless to think of an experiment that is too complex. This is why  the best examples of voice-commerce (Starbucks to name only one) very  quickly realized that voice was ideal to repeat an order already placed  previously.\n\n# No VUX vs. Good VUX: the results ?\n\nIt\u2019s  simple: not providing your brand with a VUX is missing an opportunity  to reach your consumers in a different way. By doing so, the brand is  able to offer them an improved and innovative customer experience.\n\nIt  also means depriving oneself of a privileged conversational universe  that allows each consumer to enjoy a space dedicated to him.\n\nWith  the growing and emerging enthusiasm for voice and the opportunities for  customer experience, brands can be tempted to build a voice experience  at any cost. However, a number of prerequisites must be studied to  really offer an engaging experience. As we have seen previously, it is a  question of identifying the right use case(s) and at the same time  defining the design associated with the experiment.", "link": "https://www.reddit.com/r/MachineLearning/comments/m21orj/r_how_to_design_a_good_user_voice_user_experience/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "branches", "selectorShort": "branch", "MarkedSent": "[r] how to design a good user voice user experience (vux) /!/ &amp;#x200b;\n\nhttps://preview.redd.it/zeu7tih2b8m61.jpg?width=1366&amp;format=pjpg&amp;auto=webp&amp;s=8f935154bffc92214374cfcecac9064a90e43aaf\n\nwith the advent of [speech recognition](https://vivoka.com/automatic-speech-recognition/)  and its democratization with voice assistants, new uses have emerged.  these new habits, driven by the advantages of this new human-machine  interface, have profoundly rethought the way people design products and  services. for example, search engine results are changing rapidly to  incorporate voice requests. with this rapid evolution of consumers, a  delay has naturally set in in the practices of companies. this imbalance  has given rise to new -----> branches !!!  of marketing, namely voice marketing and  voice ux (vux).\n\nhow to think about your [vocal experience](https://vivoka.com/voice-development-kit/) (vux)? how to understand these new practices related to voice assistants?\n\n# what is voice marketing and voice ux (vux)?\n\nvoice  marketing covers all brand experiences and contact points enabled by  voice technologies, in the same way that digital marketing has emerged  via the internet and smartphones. this new form of marketing improves  the customer experience by being more engaging and fun. in addition, it  makes it possible to deliver the brand message to the consumer\u2019s home  through uses that constitute real added value for the consumer.\n\nthis  brings us to the vux which refers to the art and way of designing vocal  experiences designed for (and often by) users. also called \u201cvoice  design\u201d, it refers to the development of the user experience as well as  the sound design of the experience but also to other considerations such  as potential gamification.\n\nit is a set of principles to provide a friction-free and truly engaging experience for the user.\n\n# how do we think about vux strategies ?\n\nthe  voice marketing aspect is the result of consultation with the brand and  must respect both the brand discourse and the tone used in its other  communication channels. this aspect varies according to the brand\u2019s  expectations, which may be attached to a more experiential or roi  dimension, although the two are not incompatible, it is even the  opposite.\n\nas  far as vux is concerned, it is more the result of an internal  reflection that adapts to the brand\u2019s objectives. some principles are  unavoidable, whatever the experience and some are based on common sense.  for example, it is counterproductive to offer more than three options  to the user within a single interaction in the sense that the user will  have difficulty retaining more than three.\n\n# what does it consist of ?\n\nthe  research part is a major step in the creation of an application, it is  even the most important part. it is included in the design design  process, it must be methodical with a focus on the end user. over  several stages, many hypotheses will be tested. this makes it possible  to determine the nature of the application: from an emotional point of  view, from a functional point of view, the tone and nature of the voice\u2026\n\nit  also covers many considerations and prerequisites, namely: the choice  of voice (we obviously recommend the use of a human actor\u2019s voice) but  also the sound universe or sound branding. sound branding is expressed  through music, sound effects, earcons (the audio equivalent of icons),  asmr, etc.\n\nas  mentioned above, it is also important to respect conversational design  rules in order to make the user experience as pleasant as possible.\n\n# what should we pay particular attention to ?\n\nas  voice technology is still a relatively new field that remains to be  conquered for most brands, every aspect of the experience is important.  the most important axis is probably the design because nothing is more  frustrating than an experience with which it is difficult or even  impossible to interact.\n\nbeyond  that, the choice of use case is also of crucial importance. without  careful prior reflection, the experience created may not meet the  brand\u2019s expectations while creating no added value for the end user.\n\n# what are the best practices today, the case studies in this area ?\n\nbe  aware of the limits of technology even if they are likely to evolve, it  is useless to think of an experiment that is too complex. this is why  the best examples of voice-commerce (starbucks to name only one) very  quickly realized that voice was ideal to repeat an order already placed  previously.\n\n# no vux vs. good vux: the results ?\n\nit\u2019s  simple: not providing your brand with a vux is missing an opportunity  to reach your consumers in a different way. by doing so, the brand is  able to offer them an improved and innovative customer experience.\n\nit  also means depriving oneself of a privileged conversational universe  that allows each consumer to enjoy a space dedicated to him.\n\nwith  the growing and emerging enthusiasm for voice and the opportunities for  customer experience, brands can be tempted to build a voice experience  at any cost. however, a number of prerequisites must be studied to  really offer an engaging experience. as we have seen previously, it is a  question of identifying the right use case(s) and at the same time  defining the design associated with the experiment.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/m21orj/r_how_to_design_a_good_user_voice_user_experience/',)", "identifyer": 5736313, "year": "2021"}, {"autor": "NarimanMammadli", "date": 1618671322000, "content": "Hidden implications of frequentist and Bayesian biases to almost every branch of science. /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/mss774/hidden_implications_of_frequentist_and_bayesian/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "branch", "selectorShort": "branch", "MarkedSent": "hidden implications of frequentist and bayesian biases to almost every -----> branch !!!  of science. /!/ [removed]", "sortedWord": "None", "removed": "('moderator',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/mss774/hidden_implications_of_frequentist_and_bayesian/',)", "identifyer": 5737241, "year": "2021"}, {"autor": "Fancy-Stress-806", "date": 1618589647000, "content": "[D] Need some help coding out this CNN architecture in Python /!/ Hi guys. Just wanted to ask for some help on coding out this CNN architecture: [http://cs231n.stanford.edu/reports/2017/pdfs/602.pdf](http://cs231n.stanford.edu/reports/2017/pdfs/602.pdf) (Figure 1 is the network I am looking at). I am using the same network architecture but without the predicted end of episode and predicted reward branch. I just need to predict the next frame. Would anyone be so kind as to show me their code for just the architecture (i.e., just the layers using Keras in Python, no need for data preparation and stuff, I'll handle that myself). Just having trouble on figuring out how to code this architecture out in Python Keras because I am fairly new to CNNs.\n\nThanks in advance guys!", "link": "https://www.reddit.com/r/MachineLearning/comments/ms6jik/d_need_some_help_coding_out_this_cnn_architecture/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "branch", "selectorShort": "branch", "MarkedSent": "[d] need some help coding out this cnn architecture in python /!/ hi guys. just wanted to ask for some help on coding out this cnn architecture: [http://cs231n.stanford.edu/reports/2017/pdfs/602.pdf](http://cs231n.stanford.edu/reports/2017/pdfs/602.pdf) (figure 1 is the network i am looking at). i am using the same network architecture but without the predicted end of episode and predicted reward -----> branch !!! . i just need to predict the next frame. would anyone be so kind as to show me their code for just the architecture (i.e., just the layers using keras in python, no need for data preparation and stuff, i'll handle that myself). just having trouble on figuring out how to code this architecture out in python keras because i am fairly new to cnns.\n\nthanks in advance guys!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 7, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/ms6jik/d_need_some_help_coding_out_this_cnn_architecture/',)", "identifyer": 5737315, "year": "2021"}, {"autor": "CKL-IT", "date": 1611536687000, "content": "[N] 720+ new NLP models, 300+ supported languages, translation, summarization, question answering and more with T5 and Marian models! - John Snow Labs NLU 1.1.0 /!/ &amp;#x200B;\n\n\\# 720+ new  NLP models, 300+ supported languages, translation, summarization, question answering and more with T5 and Marian models!  - John Snow Labs NLU 1.1.0\n\n&amp;#x200B;\n\n\\##  NLU 1.1.0 Release Notes\n\n&amp;#x200B;\n\nWe are incredibly excited to release NLU 1.1.0!\n\nThis release integrates the 720+ new models from the latest \\[Spark-NLP 2.7.0 + releases\\]([https://github.com/JohnSnowLabs/spark-nlp/releases](https://github.com/JohnSnowLabs/spark-nlp/releases))\n\nYou can now achieve state-of-the-art results with Sequence2Sequence transformers on problems like text summarization, question answering, translation between  192+ languages, and extract Named Entity in various Right to Left written languages like  Arabic, Persian, Urdu, and languages that require segmentation like Koreas, Japanese, Chinese, and many more in 1 line of code!     \n\nThese new features are possible because of the integration of the \\[Google's T5 models\\]([https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)) and \\[Microsoft's Marian models\\]([https://marian-nmt.github.io/publications/](https://marian-nmt.github.io/publications/))  transformers.\n\n&amp;#x200B;\n\nNLU 1.1.0 has over 720+ new pretrained models and pipelines while extending the support of multi-lingual models to 192+ languages such as Chinese, Japanese, Korean, Arabic, Persian, Urdu, and Hebrew.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nIn addition to this, NLU 1.1.0 comes with 9 new notebooks showcasing training classifiers for various review and sentiment datasets and 7 notebooks for the new features and models.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\### NLU 1.1.0  New Features\n\n\\* \\*\\*720+\\*\\* new models you can find an overview of all NLU models \\[here\\]([https://nlu.johnsnowlabs.com/docs/en/namespace](https://nlu.johnsnowlabs.com/docs/en/namespace)) and further documentation in the \\[models hub\\]([https://nlp.johnsnowlabs.com/models](https://nlp.johnsnowlabs.com/models))\n\n\\* \\*\\*NEW:\\*\\* Introducing MarianTransformer annotator for machine translation based on MarianNMT models. Marian is an efficient, free Neural Machine Translation framework mainly being developed by the Microsoft Translator team (646+ pretrained models &amp; pipelines in 192+ languages)\n\n\\* \\*\\*NEW:\\*\\* Introducing T5Transformer annotator for Text-To-Text Transfer Transformer (Google T5) models to achieve state-of-the-art results on multiple NLP tasks such as Translation, Summarization, Question Answering, Sentence Similarity, and so on\n\n\\* \\*\\*NEW:\\*\\* Introducing brand new and refactored language detection and identification models. The new LanguageDetectorDL is faster, more accurate, and supports up to 375 languages\n\n\\* \\*\\*NEW:\\*\\* Introducing WordSegmenter model for word segmentation of languages without any rule-based tokenization such as Chinese, Japanese, or Korean\n\n\\* \\*\\*NEW:\\*\\* Introducing DocumentNormalizer component for cleaning content from HTML or XML documents, applying either data cleansing using an arbitrary number of custom regular expressions either data extraction following the different parameters\n\n&amp;#x200B;\n\n\\## Translation\n\n\\[Translation example\\]([https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component\\_examples/sequence2sequence/translation\\_demo.ipynb](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/translation_demo.ipynb))       \n\nYou can translate between more than 192 Languages pairs with the \\[Marian Models\\]([https://marian-nmt.github.io/publications/](https://marian-nmt.github.io/publications/))\n\nYou need to specify the language your data is in as \\`start\\_language\\` and the language you want to translate to as \\`target\\_language\\`.    \n\nThe language references must be \\[ISO language codes\\]([https://en.wikipedia.org/wiki/List\\_of\\_ISO\\_639-1\\_codes](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes))\n\n&amp;#x200B;\n\n\\`nlu.load('&lt;start\\_language&gt;.translate.&lt;target\\_language&gt;')\\`\n\n&amp;#x200B;\n\n\\*\\*Translate English to French :\\*\\*     \n\n\\`\\`\\`\n\nnlu.load('en.translate\\_to.fr').predict(\"Hello from John Snow Labs\")\n\n\\&gt;&gt;&gt; Output: Bonjour des laboratoires de neige de John!\t \n\n&amp;#x200B;\n\n\\`\\`\\`\n\n\\*\\*Translate English to Inukitut :\\*\\*     \n\n\\`\\`\\`\n\nnlu.load('en.translate\\_to.lu').predict(\"Hello from John Snow Labs\")\n\n\\&gt;&gt;&gt; Output: kalunganyembo ka mashika makamankate \n\n\\`\\`\\`\n\n\\*\\*Translate English to Hungarian :\\*\\*\n\n\\`\\`\\`\n\nnlu.load('en.translate\\_to.hu').predict(\"Hello from John Snow Labs\")\n\n\\&gt;&gt;&gt; Output: Hell\u00f3 John h\u00f3 laborj\u00e1b\u00f3l.\n\n\\`\\`\\`\n\n\\*\\*Translate English to German :\\*\\*\n\n\\`\\`\\`\n\nnlu.load('en.translate\\_to.de').predict(\"Hello from John Snow Labs!\")\n\n\\&gt;&gt;&gt; Output: Hallo aus John Schnee Labors \n\n\\`\\`\\`\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\`\\`\\`python\n\ntranslate\\_pipe = nlu.load('en.translate\\_to.de')\n\ndf = translate\\_pipe.predict('Billy likes to go to the mall every sunday')\n\ndf\n\n\\`\\`\\`\n\n&amp;#x200B;\n\n|\tsentence|\ttranslation|\n\n|-----------|--------------|\n\n|Billy likes to go to the mall every sunday\t| Billy geht gerne jeden Sonntag ins Einkaufszentrum|\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\## T5\n\n\\[Example of every T5 task\\]([https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component\\_examples/sequence2sequence/T5\\_tasks\\_summarize\\_question\\_answering\\_and\\_more](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/T5_tasks_summarize_question_answering_and_more))\n\n\\### Overview of every task available with T5\n\n\\[The T5 model\\]([https://arxiv.org/pdf/1910.10683.pdf](https://arxiv.org/pdf/1910.10683.pdf)) is trained on various datasets for 17 different tasks which fall into 8 categories.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n1. Text summarization\n\n2. Question answering\n\n3. Translation\n\n4. Sentiment analysis\n\n5. Natural Language inference\n\n6. Coreference resolution\n\n7. Sentence Completion\n\n8. Word sense disambiguation\n\n&amp;#x200B;\n\n\\### Every T5 Task with explanation:\n\n&amp;#x200B;\n\n|Task Name | Explanation | \n\n|----------|--------------|\n\n|\\[1.CoLA\\]([https://nyu-mll.github.io/CoLA/](https://nyu-mll.github.io/CoLA/))                   | Classify if a sentence is gramaticaly correct|\n\n|\\[2.RTE\\]([https://dl.acm.org/doi/10.1007/11736790\\_9](https://dl.acm.org/doi/10.1007/11736790_9))                    | Classify whether if a statement can be deducted from a sentence|\n\n|\\[3.MNLI\\]([https://arxiv.org/abs/1704.05426](https://arxiv.org/abs/1704.05426))                   | Classify for a hypothesis and premise whether they contradict or contradict each other or neither of both (3 class).|\n\n|\\[4.MRPC\\]([https://www.aclweb.org/anthology/I05-5002.pdf](https://www.aclweb.org/anthology/I05-5002.pdf))                   | Classify whether a pair of sentences is a re-phrasing of each other (semantically equivalent)|\n\n|\\[5.QNLI\\]([https://arxiv.org/pdf/1804.07461.pdf](https://arxiv.org/pdf/1804.07461.pdf))                   | Classify whether the answer to a question can be deducted from an answer candidate.|\n\n|\\[6.QQP\\]([https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs](https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs))                    | Classify whether a pair of questions is a re-phrasing of each other (semantically equivalent)|\n\n|\\[7.SST2\\]([https://www.aclweb.org/anthology/D13-1170.pdf](https://www.aclweb.org/anthology/D13-1170.pdf))                   | Classify the sentiment of a sentence as positive or negative|\n\n|\\[8.STSB\\]([https://www.aclweb.org/anthology/S17-2001/](https://www.aclweb.org/anthology/S17-2001/))                   | Classify the sentiment of a sentence on a scale from 1 to 5 (21 Sentiment classes)|\n\n|\\[9.CB\\]([https://ojs.ub.uni-konstanz.de/sub/index.php/sub/article/view/601](https://ojs.ub.uni-konstanz.de/sub/index.php/sub/article/view/601))                     | Classify for a premise and a hypothesis whether they contradict each other or not (binary).|\n\n|\\[10.COPA\\]([https://www.aaai.org/ocs/index.php/SSS/SSS11/paper/view/2418/0](https://www.aaai.org/ocs/index.php/SSS/SSS11/paper/view/2418/0))                   | Classify for a question, premise, and 2 choices which choice the correct choice is (binary).|\n\n|\\[11.MultiRc\\]([https://www.aclweb.org/anthology/N18-1023.pdf](https://www.aclweb.org/anthology/N18-1023.pdf))                | Classify for a question, a paragraph of text, and an answer candidate, if the answer is correct (binary),|\n\n|\\[12.WiC\\]([https://arxiv.org/abs/1808.09121](https://arxiv.org/abs/1808.09121))                    | Classify for a pair of sentences and a disambigous word if the word has the same meaning in both sentences.|\n\n|\\[13.WSC/DPR\\]([https://www.aaai.org/ocs/index.php/KR/KR12/paper/view/4492/0](https://www.aaai.org/ocs/index.php/KR/KR12/paper/view/4492/0))       | Predict for an ambiguous pronoun in a sentence what it is referring to.  |\n\n|\\[14.Summarization\\]([https://arxiv.org/abs/1506.03340](https://arxiv.org/abs/1506.03340))          | Summarize text into a shorter representation.|\n\n|\\[15.SQuAD\\]([https://arxiv.org/abs/1606.05250](https://arxiv.org/abs/1606.05250))                  | Answer a question for a given context.|\n\n|\\[16.WMT1.\\]([https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762))                  | Translate English to German|\n\n|\\[17.WMT2.\\]([https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762))                   | Translate English to French|\n\n|\\[18.WMT3.\\]([https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762))                   | Translate English to Romanian|\n\n&amp;#x200B;\n\n\\- \\[Every T5 Task example notebook\\]([https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component\\_examples/sequence2sequence/T5\\_tasks\\_summarize\\_question\\_answering\\_and\\_more](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/T5_tasks_summarize_question_answering_and_more)) to see how to use every T5 Task.\n\n\\- \\[T5 Open and Closed Book question answering  notebook\\]([https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component\\_examples/sequence2sequence/T5\\_question\\_answering.ipynb](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/T5_question_answering.ipynb))\n\n&amp;#x200B;\n\n\\# \\`Open book\\` and \\`Closed book\\` question answering with Google's T5\n\n\\[T5 Open and Closed Book question answering tutorial\\]([https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component\\_examples/sequence2sequence/T5\\_question\\_answering.ipynb](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/T5_question_answering.ipynb))\n\n&amp;#x200B;\n\nWith the latest NLU release and Google's T5 you can answer \\*\\*general knowledge based questions given no context\\*\\* and in addition answer \\*\\*questions on text databases\\*\\*.      \n\nThese questions can be asked in natural human language and answerd in just 1 line with NLU!.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\## What is a \\`open book question\\`?\n\nYou can imagine an \\`open book\\` question similar to an examen where you are allowed to bring in text documents or cheat sheets that help you answer questions in an examen. Kinda like bringing a history book to an history examen.\n\n&amp;#x200B;\n\nIn \\`T5's\\` terms, this means the model is given a \\`question\\` and an \\*\\*additional piece of textual information\\*\\* or so called \\`context\\`.\n\n&amp;#x200B;\n\nThis enables the \\`T5\\` model to answer questions on textual datasets like \\`medical records\\`,\\`newsarticles\\` , \\`wiki-databases\\` , \\`stories\\` and \\`movie scripts\\` , \\`product descriptions\\`, 'legal documents' and many more.\n\n&amp;#x200B;\n\nYou can answer \\`open book question\\` in 1 line of code, leveraging the latest NLU release and Google's T5.     \n\nAll it takes is :\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\`\\`\\`python\n\nnlu.load('answer\\_question').predict(\"\"\"\n\nWhere did Jebe die?\n\ncontext: Ghenkis Khan recalled Subtai back to Mongolia soon afterwards,\n\n and Jebe died on the road back to Samarkand\"\"\")\n\n\\&gt;&gt;&gt; Output: Samarkand\n\n\\`\\`\\`\n\n&amp;#x200B;\n\nExample for answering medical questions based on medical context\n\n\\`\\`\\` python\n\nquestion ='''\n\nWhat does increased oxygen concentrations in the patient\u2019s lungs displace? \n\ncontext: Hyperbaric (high-pressure) medicine uses special oxygen chambers to increase the partial pressure of O 2 around the patient and, when needed, the medical staff. \n\nCarbon monoxide poisoning, gas gangrene, and decompression sickness (the \u2019bends\u2019) are sometimes treated using these devices. Increased O 2 concentration in the lungs helps to displace carbon monoxide from the heme group of hemoglobin.\n\n Oxygen gas is poisonous to the anaerobic bacteria that cause gas gangrene, so increasing its partial pressure helps kill them. Decompression sickness occurs in divers who decompress too quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming in their blood. Increasing the pressure of O 2 as soon as possible is part of the treatment.\n\n'''\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\#Predict on text data with T5\n\nnlu.load('answer\\_question').predict(question)\n\n\\&gt;&gt;&gt; Output: carbon monoxide\t\n\n\\`\\`\\`\n\n&amp;#x200B;\n\nTake a look at this example on a recent news article snippet :\n\n\\`\\`\\`python\n\nquestion1 = 'Who is Jack ma?'\n\nquestion2 = 'Who is founder of Alibaba Group?'\n\nquestion3 = 'When did Jack Ma re-appear?'\n\nquestion4 = 'How did Alibaba stocks react?'\n\nquestion5 = 'Whom did Jack Ma meet?'\n\nquestion6 = 'Who did Jack Ma hide from?'\n\n&amp;#x200B;\n\n\\# from [https://www.bbc.com/news/business-55728338](https://www.bbc.com/news/business-55728338) \n\nnews\\_article\\_snippet = \"\"\" context:\n\nAlibaba Group founder Jack Ma has made his first appearance since Chinese regulators cracked down on his business empire.\n\nHis absence had fuelled speculation over his whereabouts amid increasing official scrutiny of his businesses.\n\nThe billionaire met 100 rural teachers in China via a video meeting on Wednesday, according to local government media.\n\nAlibaba shares surged 5% on Hong Kong's stock exchange on the news.\n\n\"\"\"\n\n\\# join question with context, works with Pandas DF aswell!\n\nquestions = \\[\n\nquestion1+ news\\_article\\_snippet,\n\nquestion2+ news\\_article\\_snippet,\n\nquestion3+ news\\_article\\_snippet,\n\nquestion4+ news\\_article\\_snippet,\n\nquestion5+ news\\_article\\_snippet,\n\nquestion6+ news\\_article\\_snippet,\\]\n\nnlu.load('answer\\_question').predict(questions)\n\n\\`\\`\\`\n\nThis will output a Pandas Dataframe similar to this :\n\n&amp;#x200B;\n\n|Answer|Question|\n\n|-----|---------|\n\nAlibaba Group founder| \tWho is Jack ma? |        \n\n|Jack Ma\t|Who is founder of Alibaba Group? |  \n\nWednesday\t| When did Jack Ma re-appear? | \n\nsurged 5%\t| How did Alibaba stocks react? | \n\n100 rural teachers\t| Whom did Jack Ma meet? | \n\nChinese regulators\t|Who did Jack Ma hide from?|\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\## What is a \\`closed book question\\`?\n\nA \\`closed book question\\` is the exact opposite of a \\`open book question\\`. In an examen scenario, you are only allowed to use what you have memorized in your brain and nothing else.      \n\nIn \\`T5's\\` terms this means that T5 can only use it's stored weights to answer a \\`question\\` and is given \\*\\*no aditional context\\*\\*.        \n\n\\`T5\\` was pre-trained on the \\[C4 dataset\\]([https://commoncrawl.org/](https://commoncrawl.org/)) which contains \\*\\*petabytes  of web crawling data\\*\\*  collected over the last 8 years, including Wikipedia in every language.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nThis gives \\`T5\\` the broad knowledge of the internet stored in it's weights to answer various \\`closed book questions\\`\n\n&amp;#x200B;\n\nYou can answer \\`closed book question\\` in 1 line of code, leveraging the latest NLU release and Google's T5.     \n\nYou need to pass one string to NLU, which starts which a \\`question\\` and is followed by  a \\`context:\\` tag and then the actual context contents.\n\nAll it takes is :\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\`\\`\\`python\n\nnlu.load('en.t5').predict('Who is president of Nigeria?')\n\n\\&gt;&gt;&gt; Muhammadu Buhari \n\n\\`\\`\\`\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\`\\`\\`python\n\nnlu.load('en.t5').predict('What is the most spoken language in India?')\n\n\\&gt;&gt;&gt; Hindi\n\n\\`\\`\\`\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\`\\`\\`python\n\nnlu.load('en.t5').predict('What is the capital of Germany?')\n\n\\&gt;&gt;&gt; Berlin\n\n\\`\\`\\`\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\## Text Summarization with T5\n\n\\[Summarization example\\]([https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component\\_examples/sequence2sequence/T5\\_tasks\\_summarize\\_question\\_answering\\_and\\_more](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/T5_tasks_summarize_question_answering_and_more))\n\n&amp;#x200B;\n\n\\`Summarizes\\` a paragraph into a shorter version with the same semantic meaning, based on \\[this paper\\]([https://arxiv.org/abs/1506.03340](https://arxiv.org/abs/1506.03340))\n\n&amp;#x200B;\n\n\\`\\`\\`python\n\n\\# Set the task on T5\n\npipe = nlu.load('summarize')\n\n&amp;#x200B;\n\n\\# define Data, add additional tags between sentences\n\ndata = \\[\n\n'''\n\nThe belgian duo took to the dance floor on monday night with some friends . manchester united face newcastle in the premier league on wednesday . red devils will be looking for just their second league away win in seven . louis van gaal\u2019s side currently sit two points clear of liverpool in fourth .\n\n''',\n\n'''  Calculus, originally called infinitesimal calculus or \"the calculus of infinitesimals\", is the mathematical study of continuous change, in the same way that geometry is the study of shape and algebra is the study of generalizations of arithmetic operations. It has two major branches, differential calculus and integral calculus; the former concerns instantaneous rates of change, and the slopes of curves, while integral calculus concerns accumulation of quantities, and areas under or between curves. These two branches are related to each other by the fundamental theorem of calculus, and they make use of the fundamental notions of convergence of infinite sequences and infinite series to a well-defined limit.\\[1\\] Infinitesimal calculus was developed independently in the late 17th century by Isaac Newton and Gottfried Wilhelm Leibniz.\\[2\\]\\[3\\] Today, calculus has widespread uses in science, engineering, and economics.\\[4\\] In mathematics education, calculus denotes courses of elementary mathematical analysis, which are mainly devoted to the study of functions and limits. The word calculus (plural calculi) is a Latin word, meaning originally \"small pebble\" (this meaning is kept in medicine \u2013 see Calculus (medicine)). Because such pebbles were used for calculation, the meaning of the word has evolved and today usually means a method of computation. It is therefore used for naming specific methods of calculation and related theories, such as propositional calculus, Ricci calculus, calculus of variations, lambda calculus, and process calculus.'''\n\n\\]\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\#Predict on text data with T5\n\npipe.predict(data)\n\n\\`\\`\\`\n\n&amp;#x200B;\n\n| Predicted summary| Text | \n\n|------------------|-------|\n\n| manchester united face newcastle in the premier league on wednesday . louis van gaal's side currently sit two points clear of liverpool in fourth . the belgian duo took to the dance floor on monday night with some friends .            | the belgian duo took to the dance floor on monday night with some friends . manchester united face newcastle in the premier league on wednesday . red devils will be looking for just their second league away win in seven . louis van gaal\u2019s side currently sit two points clear of liverpool in fourth . | \n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\## Binary Sentence similarity/ Paraphrasing\n\n\\[Binary sentence similarity example\\]([https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component\\_examples/sequence2sequence/T5\\_tasks\\_summarize\\_question\\_answering\\_and\\_more](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/T5_tasks_summarize_question_answering_and_more))\n\nClassify whether one sentence is a re-phrasing or similar to another sentence      \n\nThis is a sub-task of \\[GLUE\\]([https://arxiv.org/pdf/1804.07461.pdf](https://arxiv.org/pdf/1804.07461.pdf)) and based on \\[MRPC - Binary Paraphrasing/ sentence similarity classification \\]([https://www.aclweb.org/anthology/I05-5002.pdf](https://www.aclweb.org/anthology/I05-5002.pdf))\n\n&amp;#x200B;\n\n\\`\\`\\`\n\nt5 = nlu.load('en.t5.base')\n\n\\# Set the task on T5\n\nt5\\['t5'\\].setTask('mrpc ')\n\n&amp;#x200B;\n\n\\# define Data, add additional tags between sentences\n\ndata = \\[\n\n''' sentence1: We acted because we saw the existing evidence in a new light , through the prism of our experience on 11 September , \" Rumsfeld said .\n\nsentence2: Rather , the US acted because the administration saw \" existing evidence in a new light , through the prism of our experience on September 11 \"\n\n'''\n\n,\n\n'''  \n\nsentence1: I like to eat peanutbutter for breakfast\n\nsentence2: \tI like to play football.\n\n'''\n\n\\]\n\n&amp;#x200B;\n\n\\#Predict on text data with T5\n\nt5.predict(data)\n\n\\`\\`\\`\n\n| Sentence1 | Sentence2 | prediction|\n\n|------------|------------|----------|\n\n|We acted because we saw the existing evidence in a new light , through the prism of our experience on 11 September , \" Rumsfeld said .| Rather , the US acted because the administration saw \" existing evidence in a new light , through the prism of our experience on September 11 \" . | equivalent | \n\n| I like to eat peanutbutter for breakfast| I like to play football | not\\_equivalent | \n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\### How to configure T5 task for MRPC and pre-process text\n\n\\`.setTask('mrpc sentence1:)\\` and prefix second sentence with \\`sentence2:\\`\n\n&amp;#x200B;\n\n\\### Example pre-processed input for T5 MRPC - Binary Paraphrasing/ sentence similarity\n\n&amp;#x200B;\n\n\\`\\`\\`\n\nmrpc \n\nsentence1: We acted because we saw the existing evidence in a new light , through the prism of our experience on 11 September , \" Rumsfeld said . \n\nsentence2: Rather , the US acted because the administration saw \" existing evidence in a new light , through the prism of our experience on September 11\",\n\n\\`\\`\\`\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\## Regressive Sentence similarity/ Paraphrasing\n\n&amp;#x200B;\n\nMeasures how similar two sentences are on a scale from 0 to 5 with 21 classes representing a regressive label.     \n\nThis is a sub-task of \\[GLUE\\]([https://arxiv.org/pdf/1804.07461.pdf](https://arxiv.org/pdf/1804.07461.pdf)) and based on\\[STSB - Regressive semantic sentence similarity\\]([https://www.aclweb.org/anthology/S17-2001/](https://www.aclweb.org/anthology/S17-2001/)) .\n\n&amp;#x200B;\n\n\\`\\`\\`python\n\nt5 = nlu.load('en.t5.base')\n\n\\# Set the task on T5\n\nt5\\['t5'\\].setTask('stsb ') \n\n&amp;#x200B;\n\n\\# define Data, add additional tags between sentences\n\ndata = \\[\n\n\n\n''' sentence1:  What attributes would have made you highly desirable in ancient Rome?  \n\nsentence2:  How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?'\n\n'''\n\n,\n\n'''  \n\nsentence1: What was it like in Ancient rome?\n\nsentence2: \tWhat was Ancient rome like?\n\n''',\n\n'''  \n\nsentence1: What was live like as a King in Ancient Rome??\n\nsentence2: \tWhat was Ancient rome like?\n\n'''\n\n&amp;#x200B;\n\n\\]\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\#Predict on text data with T5\n\nt5.predict(data)\n\n&amp;#x200B;\n\n\\`\\`\\`\n\n&amp;#x200B;\n\n| sentence1 | sentence2 | prediction|\n\n|------------|------------|----------|\n\n|What attributes would have made you highly desirable in ancient Rome?        | How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER? | 0 | \n\n|What was it like in Ancient rome?  | What was Ancient rome like?| 5.0 | \n\n|What was live like as a King in Ancient Rome??       | What is it like to live in Rome? | 3.2 | \n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\### How to configure T5 task for stsb and pre-process text\n\n\\`.setTask('stsb sentence1:)\\` and prefix second sentence with \\`sentence2:\\`\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\### Example pre-processed input for T5 STSB - Regressive semantic sentence similarity\n\n&amp;#x200B;\n\n\\`\\`\\`\n\nstsb\n\nsentence1: What attributes would have made you highly desirable in ancient Rome?        \n\nsentence2: How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?',\n\n\\`\\`\\`\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\## Grammar Checking\n\n\\[Grammar checking with T5 example\\]([https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component\\_examples/sequence2sequence/T5\\_tasks\\_summarize\\_question\\_answering\\_and\\_more](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/T5_tasks_summarize_question_answering_and_more))\n\nJudges if a sentence is grammatically acceptable.    \n\nBased on \\[CoLA - Binary Grammatical Sentence acceptability classification\\]([https://nyu-mll.github.io/CoLA/](https://nyu-mll.github.io/CoLA/))\n\n&amp;#x200B;\n\n\\`\\`\\`python\n\npipe = nlu.load('grammar\\_correctness')\n\n\\# Set the task on T5\n\npipe\\['t5'\\].setTask('cola sentence: ')\n\n\\# define Data\n\ndata = \\['Anna and Mike is going skiing and they is liked is','Anna and Mike like to dance'\\]\n\n\\#Predict on text data with T5\n\npipe.predict(data)\n\n\\`\\`\\`\n\n|sentence  | prediction|\n\n|------------|------------|\n\n| Anna and Mike is going skiing and they is liked is | unacceptable |      \n\n| Anna and Mike like to dance | acceptable | \n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\## Document Normalization\n\n\\[Document Normalizer example\\]([https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component\\_examples/text\\_pre\\_processing\\_and\\_cleaning/document\\_normalizer\\_demo.ipynb](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/text_pre_processing_and_cleaning/document_normalizer_demo.ipynb))     \n\nThe DocumentNormalizer extracts content from HTML or XML documents, applying either data cleansing using an arbitrary number of custom regular expressions either data extraction following the different parameters\n\n&amp;#x200B;\n\n\\`\\`\\`python\n\npipe = nlu.load('norm\\_document')\n\ndata = '&lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Example&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p&gt;This is an example of a simple HTML page with one paragraph.&lt;/p&gt; &lt;/body&gt; &lt;/html&gt;'\n\ndf = pipe.predict(data,output\\_level='document')\n\ndf\n\n\\`\\`\\`\n\n|text|normalized\\_text|\n\n|------|-------------|\n\n| \\`&lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Example&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p&gt;This is an example of a simple HTML page with one paragraph.&lt;/p&gt; &lt;/body&gt; &lt;/html&gt;\\`       |Example This is an example of a simple HTML page with one paragraph.|\n\n&amp;#x200B;\n\n\\## Word Segmenter\n\n\\[Word Segmenter Example\\]([https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component\\_examples/multilingual/japanese\\_ner\\_pos\\_and\\_tokenization.ipynb](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/multilingual/japanese_ner_pos_and_tokenization.ipynb))     \n\nThe WordSegmenter segments languages without any rule-based tokenization such as Chinese, Japanese, or Korean\n\n\\`\\`\\`python\n\npipe = nlu.load('ja.segment\\_words')\n\n\\# japanese for 'Donald Trump and Angela Merkel dont share many opinions'\n\nja\\_data = \\['\u30c9\u30ca\u30eb\u30c9\u30fb\u30c8\u30e9\u30f3\u30d7\u3068\u30a2\u30f3\u30b2\u30e9\u30fb\u30e1\u30eb\u30b1\u30eb\u306f\u591a\u304f\u306e\u610f\u898b\u3092\u5171\u6709\u3057\u3066\u3044\u307e\u305b\u3093'\\]\n\ndf = pipe.predict(ja\\_data, output\\_level='token')\n\ndf\n\n&amp;#x200B;\n\n\\`\\`\\`\n\n&amp;#x200B;\n\n|\ttoken|\n\n|--------|\n\n|\t\u30c9\u30ca\u30eb\u30c9|\n\n|\t\u30fb|\n\n|\t\u30c8\u30e9\u30f3\u30d7|\n\n|\t\u3068|\n\n|\t\u30a2\u30f3\u30b2\u30e9|\n\n|\t\u30fb|\n\n|\t\u30e1\u30eb\u30b1\u30eb|\n\n|\t\u306f|\n\n|\t\u591a\u304f|\n\n|\t\u306e|\n\n|\t\u610f\u898b|\n\n|\t\u3092|\n\n|\t\u5171\u6709|\n\n|\t\u3057|\n\n|\t\u3066|\n\n|\t\u3044|\n\n|\t\u307e\u305b|\n\n|\t\u3093|\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\# Named Entity Extraction (NER) in Various Languages \n\nNLU now support NER for over 60 languages, including Korean, Japanese, Chinese and many more!   \n\n\\`\\`\\`python\n\n&amp;#x200B;\n\n\\# Extract named chinese entities\n\npipe = nlu.load('zh.ner')\n\n\\# Chinese for 'Donald Trump and Angela Merkel dont share many opinions'\n\nzh\\_data = \\['\u5510\u7eb3\u5fb7\u7279\u6717\u666e\u548c\u5b89\u5409\u62c9\u00b7\u9ed8\u514b\u5c14\u6ca1\u6709\u592a\u591a\u610f\u89c1'\\]\n\ndf = pipe.predict(zh\\_data, output\\_level='document')\n\ndf\n\n\\&gt;&gt;&gt; Output : \\[\u5510\u7eb3\u5fb7, \u5b89\u5409\u62c9\\]\n\n&amp;#x200B;\n\n\\# Now translate \\[\u5510\u7eb3\u5fb7, \u5b89\u5409\u62c9\\] back to english with NLU!\n\ntranslate\\_pipe = nlu.load('zh.translate\\_to.en')\n\nen\\_entities = translate\\_pipe.predict(\\['\u5510\u7eb3\u5fb7', '\u5b89\u5409\u62c9'\\])\n\n\\&gt;&gt;&gt; Output :\n\n\\`\\`\\`\n\n|Translation|\tChinese| \n\n|------|------|\n\n|Donald | \u5510\u7eb3\u5fb7 |\n\n|Angela | \t\u5b89\u5409\u62c9|\n\n&amp;#x200B;\n\n\\# New NLU Notebooks\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\### NLU 1.1.0  New Notebooks for new features\n\n\\- \\[Translate between 192+ languages with marian\\]([https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component\\_examples/sequence2sequence/translation\\_demo.ipynb](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/translation_demo.ipynb))\n\n\\- \\[Try out the 18 Tasks like Summarization Question Answering and more on T5\\]([https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component\\_examples/sequence2sequence/T5\\_tasks\\_summarize\\_question\\_answering\\_and\\_more](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/T5_tasks_summarize_question_answering_and_more))\n\n\\- \\[T5 Open and Closed Book question answering tutorial\\]([https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component\\_examples/sequence2sequence/T5\\_question\\_answering.ipynb](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/T5_question_answering.ipynb))\n\n\\- \\[Tokenize, extract POS and NER in Chinese\\]([https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component\\_examples/multilingual/chinese\\_ner\\_pos\\_and\\_tokenization.ipynb](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/multilingual/chinese_ner_pos_and_tokenization.ipynb))\n\n\\- \\[Tokenize, extract POS and NER in Korean\\]([https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component\\_examples/multilingual/korean\\_ner\\_pos\\_and\\_tokenization.ipynb](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/multilingual/korean_ner_pos_and_tokenization.ipynb))\n\n\\- \\[Tokenize, extract POS and NER in Japanese\\]([https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component\\_examples/multilingual/japanese\\_ner\\_pos\\_and\\_tokenization.ipynb](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/multilingual/japanese_ner_pos_and_tokenization.ipynb))\n\n\\- \\[Normalize documents\\]([https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component\\_examples/text\\_pre\\_processing\\_and\\_cleaning/document\\_normalizer\\_demo.ipynb](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/text_pre_processing_and_cleaning/document_normalizer_demo.ipynb))\n\n\\- \\[Aspect based sentiment NER sentiment for restaurants\\]([https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component\\_examples/named\\_entity\\_recognition\\_(NER)/aspect\\_based\\_ner\\_sentiment\\_restaurants.ipynb](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/named_entity_recognition_(NER)/aspect_based_ner_sentiment_restaurants.ipynb))\n\n&amp;#x200B;\n\n\\### NLU 1.1.0 New Classifier Training Tutorials\n\n\\#### Binary Classifier training Jupyter tutorials\n\n\\- \\[2 class Finance News sentiment classifier training\\]([https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/Training/binary\\_text\\_classification/NLU\\_training\\_sentiment\\_classifier\\_demo\\_apple\\_twitter.ipynb](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/Training/binary_text_classification/NLU_training_sentiment_classifier_demo_apple_twitter.ipynb))\n\n\\- \\[2 class Reddit comment sentiment classifier training\\]([https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/Training/binary\\_text\\_classification/NLU\\_training\\_sentiment\\_classifier\\_demo\\_reddit.ipynb](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/Training/binary_text_classification/NLU_training_sentiment_classifier_demo_reddit.ipynb))\n\n\\- \\[2 class Apple Tweets sentiment classifier training\\]([https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/Training/binary\\_text\\_classification/NLU\\_training\\_sentiment\\_classifier\\_demo\\_IMDB.ipynb](https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/Training/binary_text_classification/NLU_training_s", "link": "https://www.reddit.com/r/MachineLearning/comments/l4cb40/n_720_new_nlp_models_300_supported_languages/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "branches", "selectorShort": "branch", "MarkedSent": "[n] 720+ new nlp models, 300+ supported languages, translation, summarization, question answering and more with t5 and marian models! - john snow labs nlu 1.1.0 /!/ &amp;#x200b;\n\n\\# 720+ new  nlp models, 300+ supported languages, translation, summarization, question answering and more with t5 and marian models!  - john snow labs nlu 1.1.0\n\n&amp;#x200b;\n\n\\##  nlu 1.1.0 release notes\n\n&amp;#x200b;\n\nwe are incredibly excited to release nlu 1.1.0!\n\nthis release integrates the 720+ new models from the latest \\[spark-nlp 2.7.0 + releases\\]([https://github.com/johnsnowlabs/spark-nlp/releases](https://github.com/johnsnowlabs/spark-nlp/releases))\n\nyou can now achieve state-of-the-art results with sequence2sequence transformers on problems like text summarization, question answering, translation between  192+ languages, and extract named entity in various right to left written languages like  arabic, persian, urdu, and languages that require segmentation like koreas, japanese, chinese, and many more in 1 line of code!     \n\nthese new features are possible because of the integration of the \\[google's t5 models\\]([https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)) and \\[microsoft's marian models\\]([https://marian-nmt.github.io/publications/](https://marian-nmt.github.io/publications/))  transformers.\n\n&amp;#x200b;\n\nnlu 1.1.0 has over 720+ new pretrained models and pipelines while extending the support of multi-lingual models to 192+ languages such as chinese, japanese, korean, arabic, persian, urdu, and hebrew.\n\n&amp;#x200b;\n\n&amp;#x200b;\n\nin addition to this, nlu 1.1.0 comes with 9 new notebooks showcasing training classifiers for various review and sentiment datasets and 7 notebooks for the new features and models.\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n\\### nlu 1.1.0  new features\n\n\\* \\*\\*720+\\*\\* new models you can find an overview of all nlu models \\[here\\]([https://nlu.johnsnowlabs.com/docs/en/namespace](https://nlu.johnsnowlabs.com/docs/en/namespace)) and further documentation in the \\[models hub\\]([https://nlp.johnsnowlabs.com/models](https://nlp.johnsnowlabs.com/models))\n\n\\* \\*\\*new:\\*\\* introducing mariantransformer annotator for machine translation based on mariannmt models. marian is an efficient, free neural machine translation framework mainly being developed by the microsoft translator team (646+ pretrained models &amp; pipelines in 192+ languages)\n\n\\* \\*\\*new:\\*\\* introducing t5transformer annotator for text-to-text transfer transformer (google t5) models to achieve state-of-the-art results on multiple nlp tasks such as translation, summarization, question answering, sentence similarity, and so on\n\n\\* \\*\\*new:\\*\\* introducing brand new and refactored language detection and identification models. the new languagedetectordl is faster, more accurate, and supports up to 375 languages\n\n\\* \\*\\*new:\\*\\* introducing wordsegmenter model for word segmentation of languages without any rule-based tokenization such as chinese, japanese, or korean\n\n\\* \\*\\*new:\\*\\* introducing documentnormalizer component for cleaning content from html or xml documents, applying either data cleansing using an arbitrary number of custom regular expressions either data extraction following the different parameters\n\n&amp;#x200b;\n\n\\## translation\n\n\\[translation example\\]([https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/component\\_examples/sequence2sequence/translation\\_demo.ipynb](https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/translation_demo.ipynb))       \n\nyou can translate between more than 192 languages pairs with the \\[marian models\\]([https://marian-nmt.github.io/publications/](https://marian-nmt.github.io/publications/))\n\nyou need to specify the language your data is in as \\`start\\_language\\` and the language you want to translate to as \\`target\\_language\\`.    \n\nthe language references must be \\[iso language codes\\]([https://en.wikipedia.org/wiki/list\\_of\\_iso\\_639-1\\_codes](https://en.wikipedia.org/wiki/list_of_iso_639-1_codes))\n\n&amp;#x200b;\n\n\\`nlu.load('&lt;start\\_language&gt;.translate.&lt;target\\_language&gt;')\\`\n\n&amp;#x200b;\n\n\\*\\*translate english to french :\\*\\*     \n\n\\`\\`\\`\n\nnlu.load('en.translate\\_to.fr').predict(\"hello from john snow labs\")\n\n\\&gt;&gt;&gt; output: bonjour des laboratoires de neige de john!\t \n\n&amp;#x200b;\n\n\\`\\`\\`\n\n\\*\\*translate english to inukitut :\\*\\*     \n\n\\`\\`\\`\n\nnlu.load('en.translate\\_to.lu').predict(\"hello from john snow labs\")\n\n\\&gt;&gt;&gt; output: kalunganyembo ka mashika makamankate \n\n\\`\\`\\`\n\n\\*\\*translate english to hungarian :\\*\\*\n\n\\`\\`\\`\n\nnlu.load('en.translate\\_to.hu').predict(\"hello from john snow labs\")\n\n\\&gt;&gt;&gt; output: hell\u00f3 john h\u00f3 laborj\u00e1b\u00f3l.\n\n\\`\\`\\`\n\n\\*\\*translate english to german :\\*\\*\n\n\\`\\`\\`\n\nnlu.load('en.translate\\_to.de').predict(\"hello from john snow labs!\")\n\n\\&gt;&gt;&gt; output: hallo aus john schnee labors \n\n\\`\\`\\`\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n\\`\\`\\`python\n\ntranslate\\_pipe = nlu.load('en.translate\\_to.de')\n\ndf = translate\\_pipe.predict('billy likes to go to the mall every sunday')\n\ndf\n\n\\`\\`\\`\n\n&amp;#x200b;\n\n|\tsentence|\ttranslation|\n\n|-----------|--------------|\n\n|billy likes to go to the mall every sunday\t| billy geht gerne jeden sonntag ins einkaufszentrum|\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n\\## t5\n\n\\[example of every t5 task\\]([https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/component\\_examples/sequence2sequence/t5\\_tasks\\_summarize\\_question\\_answering\\_and\\_more](https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/t5_tasks_summarize_question_answering_and_more))\n\n\\### overview of every task available with t5\n\n\\[the t5 model\\]([https://arxiv.org/pdf/1910.10683.pdf](https://arxiv.org/pdf/1910.10683.pdf)) is trained on various datasets for 17 different tasks which fall into 8 categories.\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n1. text summarization\n\n2. question answering\n\n3. translation\n\n4. sentiment analysis\n\n5. natural language inference\n\n6. coreference resolution\n\n7. sentence completion\n\n8. word sense disambiguation\n\n&amp;#x200b;\n\n\\### every t5 task with explanation:\n\n&amp;#x200b;\n\n|task name | explanation | \n\n|----------|--------------|\n\n|\\[1.cola\\]([https://nyu-mll.github.io/cola/](https://nyu-mll.github.io/cola/))                   | classify if a sentence is gramaticaly correct|\n\n|\\[2.rte\\]([https://dl.acm.org/doi/10.1007/11736790\\_9](https://dl.acm.org/doi/10.1007/11736790_9))                    | classify whether if a statement can be deducted from a sentence|\n\n|\\[3.mnli\\]([https://arxiv.org/abs/1704.05426](https://arxiv.org/abs/1704.05426))                   | classify for a hypothesis and premise whether they contradict or contradict each other or neither of both (3 class).|\n\n|\\[4.mrpc\\]([https://www.aclweb.org/anthology/i05-5002.pdf](https://www.aclweb.org/anthology/i05-5002.pdf))                   | classify whether a pair of sentences is a re-phrasing of each other (semantically equivalent)|\n\n|\\[5.qnli\\]([https://arxiv.org/pdf/1804.07461.pdf](https://arxiv.org/pdf/1804.07461.pdf))                   | classify whether the answer to a question can be deducted from an answer candidate.|\n\n|\\[6.qqp\\]([https://www.quora.com/q/quoradata/first-quora-dataset-release-question-pairs](https://www.quora.com/q/quoradata/first-quora-dataset-release-question-pairs))                    | classify whether a pair of questions is a re-phrasing of each other (semantically equivalent)|\n\n|\\[7.sst2\\]([https://www.aclweb.org/anthology/d13-1170.pdf](https://www.aclweb.org/anthology/d13-1170.pdf))                   | classify the sentiment of a sentence as positive or negative|\n\n|\\[8.stsb\\]([https://www.aclweb.org/anthology/s17-2001/](https://www.aclweb.org/anthology/s17-2001/))                   | classify the sentiment of a sentence on a scale from 1 to 5 (21 sentiment classes)|\n\n|\\[9.cb\\]([https://ojs.ub.uni-konstanz.de/sub/index.php/sub/article/view/601](https://ojs.ub.uni-konstanz.de/sub/index.php/sub/article/view/601))                     | classify for a premise and a hypothesis whether they contradict each other or not (binary).|\n\n|\\[10.copa\\]([https://www.aaai.org/ocs/index.php/sss/sss11/paper/view/2418/0](https://www.aaai.org/ocs/index.php/sss/sss11/paper/view/2418/0))                   | classify for a question, premise, and 2 choices which choice the correct choice is (binary).|\n\n|\\[11.multirc\\]([https://www.aclweb.org/anthology/n18-1023.pdf](https://www.aclweb.org/anthology/n18-1023.pdf))                | classify for a question, a paragraph of text, and an answer candidate, if the answer is correct (binary),|\n\n|\\[12.wic\\]([https://arxiv.org/abs/1808.09121](https://arxiv.org/abs/1808.09121))                    | classify for a pair of sentences and a disambigous word if the word has the same meaning in both sentences.|\n\n|\\[13.wsc/dpr\\]([https://www.aaai.org/ocs/index.php/kr/kr12/paper/view/4492/0](https://www.aaai.org/ocs/index.php/kr/kr12/paper/view/4492/0))       | predict for an ambiguous pronoun in a sentence what it is referring to.  |\n\n|\\[14.summarization\\]([https://arxiv.org/abs/1506.03340](https://arxiv.org/abs/1506.03340))          | summarize text into a shorter representation.|\n\n|\\[15.squad\\]([https://arxiv.org/abs/1606.05250](https://arxiv.org/abs/1606.05250))                  | answer a question for a given context.|\n\n|\\[16.wmt1.\\]([https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762))                  | translate english to german|\n\n|\\[17.wmt2.\\]([https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762))                   | translate english to french|\n\n|\\[18.wmt3.\\]([https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762))                   | translate english to romanian|\n\n&amp;#x200b;\n\n\\- \\[every t5 task example notebook\\]([https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/component\\_examples/sequence2sequence/t5\\_tasks\\_summarize\\_question\\_answering\\_and\\_more](https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/t5_tasks_summarize_question_answering_and_more)) to see how to use every t5 task.\n\n\\- \\[t5 open and closed book question answering  notebook\\]([https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/component\\_examples/sequence2sequence/t5\\_question\\_answering.ipynb](https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/t5_question_answering.ipynb))\n\n&amp;#x200b;\n\n\\# \\`open book\\` and \\`closed book\\` question answering with google's t5\n\n\\[t5 open and closed book question answering tutorial\\]([https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/component\\_examples/sequence2sequence/t5\\_question\\_answering.ipynb](https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/t5_question_answering.ipynb))\n\n&amp;#x200b;\n\nwith the latest nlu release and google's t5 you can answer \\*\\*general knowledge based questions given no context\\*\\* and in addition answer \\*\\*questions on text databases\\*\\*.      \n\nthese questions can be asked in natural human language and answerd in just 1 line with nlu!.\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n\\## what is a \\`open book question\\`?\n\nyou can imagine an \\`open book\\` question similar to an examen where you are allowed to bring in text documents or cheat sheets that help you answer questions in an examen. kinda like bringing a history book to an history examen.\n\n&amp;#x200b;\n\nin \\`t5's\\` terms, this means the model is given a \\`question\\` and an \\*\\*additional piece of textual information\\*\\* or so called \\`context\\`.\n\n&amp;#x200b;\n\nthis enables the \\`t5\\` model to answer questions on textual datasets like \\`medical records\\`,\\`newsarticles\\` , \\`wiki-databases\\` , \\`stories\\` and \\`movie scripts\\` , \\`product descriptions\\`, 'legal documents' and many more.\n\n&amp;#x200b;\n\nyou can answer \\`open book question\\` in 1 line of code, leveraging the latest nlu release and google's t5.     \n\nall it takes is :\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n\\`\\`\\`python\n\nnlu.load('answer\\_question').predict(\"\"\"\n\nwhere did jebe die?\n\ncontext: ghenkis khan recalled subtai back to mongolia soon afterwards,\n\n and jebe died on the road back to samarkand\"\"\")\n\n\\&gt;&gt;&gt; output: samarkand\n\n\\`\\`\\`\n\n&amp;#x200b;\n\nexample for answering medical questions based on medical context\n\n\\`\\`\\` python\n\nquestion ='''\n\nwhat does increased oxygen concentrations in the patient\u2019s lungs displace? \n\ncontext: hyperbaric (high-pressure) medicine uses special oxygen chambers to increase the partial pressure of o 2 around the patient and, when needed, the medical staff. \n\ncarbon monoxide poisoning, gas gangrene, and decompression sickness (the \u2019bends\u2019) are sometimes treated using these devices. increased o 2 concentration in the lungs helps to displace carbon monoxide from the heme group of hemoglobin.\n\n oxygen gas is poisonous to the anaerobic bacteria that cause gas gangrene, so increasing its partial pressure helps kill them. decompression sickness occurs in divers who decompress too quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming in their blood. increasing the pressure of o 2 as soon as possible is part of the treatment.\n\n'''\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n\\#predict on text data with t5\n\nnlu.load('answer\\_question').predict(question)\n\n\\&gt;&gt;&gt; output: carbon monoxide\t\n\n\\`\\`\\`\n\n&amp;#x200b;\n\ntake a look at this example on a recent news article snippet :\n\n\\`\\`\\`python\n\nquestion1 = 'who is jack ma?'\n\nquestion2 = 'who is founder of alibaba group?'\n\nquestion3 = 'when did jack ma re-appear?'\n\nquestion4 = 'how did alibaba stocks react?'\n\nquestion5 = 'whom did jack ma meet?'\n\nquestion6 = 'who did jack ma hide from?'\n\n&amp;#x200b;\n\n\\# from [https://www.bbc.com/news/business-55728338](https://www.bbc.com/news/business-55728338) \n\nnews\\_article\\_snippet = \"\"\" context:\n\nalibaba group founder jack ma has made his first appearance since chinese regulators cracked down on his business empire.\n\nhis absence had fuelled speculation over his whereabouts amid increasing official scrutiny of his businesses.\n\nthe billionaire met 100 rural teachers in china via a video meeting on wednesday, according to local government media.\n\nalibaba shares surged 5% on hong kong's stock exchange on the news.\n\n\"\"\"\n\n\\# join question with context, works with pandas df aswell!\n\nquestions = \\[\n\nquestion1+ news\\_article\\_snippet,\n\nquestion2+ news\\_article\\_snippet,\n\nquestion3+ news\\_article\\_snippet,\n\nquestion4+ news\\_article\\_snippet,\n\nquestion5+ news\\_article\\_snippet,\n\nquestion6+ news\\_article\\_snippet,\\]\n\nnlu.load('answer\\_question').predict(questions)\n\n\\`\\`\\`\n\nthis will output a pandas dataframe similar to this :\n\n&amp;#x200b;\n\n|answer|question|\n\n|-----|---------|\n\nalibaba group founder| \twho is jack ma? |        \n\n|jack ma\t|who is founder of alibaba group? |  \n\nwednesday\t| when did jack ma re-appear? | \n\nsurged 5%\t| how did alibaba stocks react? | \n\n100 rural teachers\t| whom did jack ma meet? | \n\nchinese regulators\t|who did jack ma hide from?|\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n\\## what is a \\`closed book question\\`?\n\na \\`closed book question\\` is the exact opposite of a \\`open book question\\`. in an examen scenario, you are only allowed to use what you have memorized in your brain and nothing else.      \n\nin \\`t5's\\` terms this means that t5 can only use it's stored weights to answer a \\`question\\` and is given \\*\\*no aditional context\\*\\*.        \n\n\\`t5\\` was pre-trained on the \\[c4 dataset\\]([https://commoncrawl.org/](https://commoncrawl.org/)) which contains \\*\\*petabytes  of web crawling data\\*\\*  collected over the last 8 years, including wikipedia in every language.\n\n&amp;#x200b;\n\n&amp;#x200b;\n\nthis gives \\`t5\\` the broad knowledge of the internet stored in it's weights to answer various \\`closed book questions\\`\n\n&amp;#x200b;\n\nyou can answer \\`closed book question\\` in 1 line of code, leveraging the latest nlu release and google's t5.     \n\nyou need to pass one string to nlu, which starts which a \\`question\\` and is followed by  a \\`context:\\` tag and then the actual context contents.\n\nall it takes is :\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n\\`\\`\\`python\n\nnlu.load('en.t5').predict('who is president of nigeria?')\n\n\\&gt;&gt;&gt; muhammadu buhari \n\n\\`\\`\\`\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n\\`\\`\\`python\n\nnlu.load('en.t5').predict('what is the most spoken language in india?')\n\n\\&gt;&gt;&gt; hindi\n\n\\`\\`\\`\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n\\`\\`\\`python\n\nnlu.load('en.t5').predict('what is the capital of germany?')\n\n\\&gt;&gt;&gt; berlin\n\n\\`\\`\\`\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n\\## text summarization with t5\n\n\\[summarization example\\]([https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/component\\_examples/sequence2sequence/t5\\_tasks\\_summarize\\_question\\_answering\\_and\\_more](https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/t5_tasks_summarize_question_answering_and_more))\n\n&amp;#x200b;\n\n\\`summarizes\\` a paragraph into a shorter version with the same semantic meaning, based on \\[this paper\\]([https://arxiv.org/abs/1506.03340](https://arxiv.org/abs/1506.03340))\n\n&amp;#x200b;\n\n\\`\\`\\`python\n\n\\# set the task on t5\n\npipe = nlu.load('summarize')\n\n&amp;#x200b;\n\n\\# define data, add additional tags between sentences\n\ndata = \\[\n\n'''\n\nthe belgian duo took to the dance floor on monday night with some friends . manchester united face newcastle in the premier league on wednesday . red devils will be looking for just their second league away win in seven . louis van gaal\u2019s side currently sit two points clear of liverpool in fourth .\n\n''',\n\n'''  calculus, originally called infinitesimal calculus or \"the calculus of infinitesimals\", is the mathematical study of continuous change, in the same way that geometry is the study of shape and algebra is the study of generalizations of arithmetic operations. it has two major -----> branches !!! , differential calculus and integral calculus; the former concerns instantaneous rates of change, and the slopes of curves, while integral calculus concerns accumulation of quantities, and areas under or between curves. these two branches are related to each other by the fundamental theorem of calculus, and they make use of the fundamental notions of convergence of infinite sequences and infinite series to a well-defined limit.\\[1\\] infinitesimal calculus was developed independently in the late 17th century by isaac newton and gottfried wilhelm leibniz.\\[2\\]\\[3\\] today, calculus has widespread uses in science, engineering, and economics.\\[4\\] in mathematics education, calculus denotes courses of elementary mathematical analysis, which are mainly devoted to the study of functions and limits. the word calculus (plural calculi) is a latin word, meaning originally \"small pebble\" (this meaning is kept in medicine \u2013 see calculus (medicine)). because such pebbles were used for calculation, the meaning of the word has evolved and today usually means a method of computation. it is therefore used for naming specific methods of calculation and related theories, such as propositional calculus, ricci calculus, calculus of variations, lambda calculus, and process calculus.'''\n\n\\]\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n\\#predict on text data with t5\n\npipe.predict(data)\n\n\\`\\`\\`\n\n&amp;#x200b;\n\n| predicted summary| text | \n\n|------------------|-------|\n\n| manchester united face newcastle in the premier league on wednesday . louis van gaal's side currently sit two points clear of liverpool in fourth . the belgian duo took to the dance floor on monday night with some friends .            | the belgian duo took to the dance floor on monday night with some friends . manchester united face newcastle in the premier league on wednesday . red devils will be looking for just their second league away win in seven . louis van gaal\u2019s side currently sit two points clear of liverpool in fourth . | \n\n&amp;#x200b;\n\n&amp;#x200b;\n\n\\## binary sentence similarity/ paraphrasing\n\n\\[binary sentence similarity example\\]([https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/component\\_examples/sequence2sequence/t5\\_tasks\\_summarize\\_question\\_answering\\_and\\_more](https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/t5_tasks_summarize_question_answering_and_more))\n\nclassify whether one sentence is a re-phrasing or similar to another sentence      \n\nthis is a sub-task of \\[glue\\]([https://arxiv.org/pdf/1804.07461.pdf](https://arxiv.org/pdf/1804.07461.pdf)) and based on \\[mrpc - binary paraphrasing/ sentence similarity classification \\]([https://www.aclweb.org/anthology/i05-5002.pdf](https://www.aclweb.org/anthology/i05-5002.pdf))\n\n&amp;#x200b;\n\n\\`\\`\\`\n\nt5 = nlu.load('en.t5.base')\n\n\\# set the task on t5\n\nt5\\['t5'\\].settask('mrpc ')\n\n&amp;#x200b;\n\n\\# define data, add additional tags between sentences\n\ndata = \\[\n\n''' sentence1: we acted because we saw the existing evidence in a new light , through the prism of our experience on 11 september , \" rumsfeld said .\n\nsentence2: rather , the us acted because the administration saw \" existing evidence in a new light , through the prism of our experience on september 11 \"\n\n'''\n\n,\n\n'''  \n\nsentence1: i like to eat peanutbutter for breakfast\n\nsentence2: \ti like to play football.\n\n'''\n\n\\]\n\n&amp;#x200b;\n\n\\#predict on text data with t5\n\nt5.predict(data)\n\n\\`\\`\\`\n\n| sentence1 | sentence2 | prediction|\n\n|------------|------------|----------|\n\n|we acted because we saw the existing evidence in a new light , through the prism of our experience on 11 september , \" rumsfeld said .| rather , the us acted because the administration saw \" existing evidence in a new light , through the prism of our experience on september 11 \" . | equivalent | \n\n| i like to eat peanutbutter for breakfast| i like to play football | not\\_equivalent | \n\n&amp;#x200b;\n\n&amp;#x200b;\n\n\\### how to configure t5 task for mrpc and pre-process text\n\n\\`.settask('mrpc sentence1:)\\` and prefix second sentence with \\`sentence2:\\`\n\n&amp;#x200b;\n\n\\### example pre-processed input for t5 mrpc - binary paraphrasing/ sentence similarity\n\n&amp;#x200b;\n\n\\`\\`\\`\n\nmrpc \n\nsentence1: we acted because we saw the existing evidence in a new light , through the prism of our experience on 11 september , \" rumsfeld said . \n\nsentence2: rather , the us acted because the administration saw \" existing evidence in a new light , through the prism of our experience on september 11\",\n\n\\`\\`\\`\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n\\## regressive sentence similarity/ paraphrasing\n\n&amp;#x200b;\n\nmeasures how similar two sentences are on a scale from 0 to 5 with 21 classes representing a regressive label.     \n\nthis is a sub-task of \\[glue\\]([https://arxiv.org/pdf/1804.07461.pdf](https://arxiv.org/pdf/1804.07461.pdf)) and based on\\[stsb - regressive semantic sentence similarity\\]([https://www.aclweb.org/anthology/s17-2001/](https://www.aclweb.org/anthology/s17-2001/)) .\n\n&amp;#x200b;\n\n\\`\\`\\`python\n\nt5 = nlu.load('en.t5.base')\n\n\\# set the task on t5\n\nt5\\['t5'\\].settask('stsb ') \n\n&amp;#x200b;\n\n\\# define data, add additional tags between sentences\n\ndata = \\[\n\n\n\n''' sentence1:  what attributes would have made you highly desirable in ancient rome?  \n\nsentence2:  how i get oppertinuty to join it company as a fresher?'\n\n'''\n\n,\n\n'''  \n\nsentence1: what was it like in ancient rome?\n\nsentence2: \twhat was ancient rome like?\n\n''',\n\n'''  \n\nsentence1: what was live like as a king in ancient rome??\n\nsentence2: \twhat was ancient rome like?\n\n'''\n\n&amp;#x200b;\n\n\\]\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n\\#predict on text data with t5\n\nt5.predict(data)\n\n&amp;#x200b;\n\n\\`\\`\\`\n\n&amp;#x200b;\n\n| sentence1 | sentence2 | prediction|\n\n|------------|------------|----------|\n\n|what attributes would have made you highly desirable in ancient rome?        | how i get oppertinuty to join it company as a fresher? | 0 | \n\n|what was it like in ancient rome?  | what was ancient rome like?| 5.0 | \n\n|what was live like as a king in ancient rome??       | what is it like to live in rome? | 3.2 | \n\n&amp;#x200b;\n\n&amp;#x200b;\n\n\\### how to configure t5 task for stsb and pre-process text\n\n\\`.settask('stsb sentence1:)\\` and prefix second sentence with \\`sentence2:\\`\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n\\### example pre-processed input for t5 stsb - regressive semantic sentence similarity\n\n&amp;#x200b;\n\n\\`\\`\\`\n\nstsb\n\nsentence1: what attributes would have made you highly desirable in ancient rome?        \n\nsentence2: how i get oppertinuty to join it company as a fresher?',\n\n\\`\\`\\`\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n\\## grammar checking\n\n\\[grammar checking with t5 example\\]([https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/component\\_examples/sequence2sequence/t5\\_tasks\\_summarize\\_question\\_answering\\_and\\_more](https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/t5_tasks_summarize_question_answering_and_more))\n\njudges if a sentence is grammatically acceptable.    \n\nbased on \\[cola - binary grammatical sentence acceptability classification\\]([https://nyu-mll.github.io/cola/](https://nyu-mll.github.io/cola/))\n\n&amp;#x200b;\n\n\\`\\`\\`python\n\npipe = nlu.load('grammar\\_correctness')\n\n\\# set the task on t5\n\npipe\\['t5'\\].settask('cola sentence: ')\n\n\\# define data\n\ndata = \\['anna and mike is going skiing and they is liked is','anna and mike like to dance'\\]\n\n\\#predict on text data with t5\n\npipe.predict(data)\n\n\\`\\`\\`\n\n|sentence  | prediction|\n\n|------------|------------|\n\n| anna and mike is going skiing and they is liked is | unacceptable |      \n\n| anna and mike like to dance | acceptable | \n\n&amp;#x200b;\n\n&amp;#x200b;\n\n\\## document normalization\n\n\\[document normalizer example\\]([https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/component\\_examples/text\\_pre\\_processing\\_and\\_cleaning/document\\_normalizer\\_demo.ipynb](https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/component_examples/text_pre_processing_and_cleaning/document_normalizer_demo.ipynb))     \n\nthe documentnormalizer extracts content from html or xml documents, applying either data cleansing using an arbitrary number of custom regular expressions either data extraction following the different parameters\n\n&amp;#x200b;\n\n\\`\\`\\`python\n\npipe = nlu.load('norm\\_document')\n\ndata = '&lt;!doctype html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;example&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p&gt;this is an example of a simple html page with one paragraph.&lt;/p&gt; &lt;/body&gt; &lt;/html&gt;'\n\ndf = pipe.predict(data,output\\_level='document')\n\ndf\n\n\\`\\`\\`\n\n|text|normalized\\_text|\n\n|------|-------------|\n\n| \\`&lt;!doctype html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;example&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p&gt;this is an example of a simple html page with one paragraph.&lt;/p&gt; &lt;/body&gt; &lt;/html&gt;\\`       |example this is an example of a simple html page with one paragraph.|\n\n&amp;#x200b;\n\n\\## word segmenter\n\n\\[word segmenter example\\]([https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/component\\_examples/multilingual/japanese\\_ner\\_pos\\_and\\_tokenization.ipynb](https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/component_examples/multilingual/japanese_ner_pos_and_tokenization.ipynb))     \n\nthe wordsegmenter segments languages without any rule-based tokenization such as chinese, japanese, or korean\n\n\\`\\`\\`python\n\npipe = nlu.load('ja.segment\\_words')\n\n\\# japanese for 'donald trump and angela merkel dont share many opinions'\n\nja\\_data = \\['\u30c9\u30ca\u30eb\u30c9\u30fb\u30c8\u30e9\u30f3\u30d7\u3068\u30a2\u30f3\u30b2\u30e9\u30fb\u30e1\u30eb\u30b1\u30eb\u306f\u591a\u304f\u306e\u610f\u898b\u3092\u5171\u6709\u3057\u3066\u3044\u307e\u305b\u3093'\\]\n\ndf = pipe.predict(ja\\_data, output\\_level='token')\n\ndf\n\n&amp;#x200b;\n\n\\`\\`\\`\n\n&amp;#x200b;\n\n|\ttoken|\n\n|--------|\n\n|\t\u30c9\u30ca\u30eb\u30c9|\n\n|\t\u30fb|\n\n|\t\u30c8\u30e9\u30f3\u30d7|\n\n|\t\u3068|\n\n|\t\u30a2\u30f3\u30b2\u30e9|\n\n|\t\u30fb|\n\n|\t\u30e1\u30eb\u30b1\u30eb|\n\n|\t\u306f|\n\n|\t\u591a\u304f|\n\n|\t\u306e|\n\n|\t\u610f\u898b|\n\n|\t\u3092|\n\n|\t\u5171\u6709|\n\n|\t\u3057|\n\n|\t\u3066|\n\n|\t\u3044|\n\n|\t\u307e\u305b|\n\n|\t\u3093|\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n\\# named entity extraction (ner) in various languages \n\nnlu now support ner for over 60 languages, including korean, japanese, chinese and many more!   \n\n\\`\\`\\`python\n\n&amp;#x200b;\n\n\\# extract named chinese entities\n\npipe = nlu.load('zh.ner')\n\n\\# chinese for 'donald trump and angela merkel dont share many opinions'\n\nzh\\_data = \\['\u5510\u7eb3\u5fb7\u7279\u6717\u666e\u548c\u5b89\u5409\u62c9\u00b7\u9ed8\u514b\u5c14\u6ca1\u6709\u592a\u591a\u610f\u89c1'\\]\n\ndf = pipe.predict(zh\\_data, output\\_level='document')\n\ndf\n\n\\&gt;&gt;&gt; output : \\[\u5510\u7eb3\u5fb7, \u5b89\u5409\u62c9\\]\n\n&amp;#x200b;\n\n\\# now translate \\[\u5510\u7eb3\u5fb7, \u5b89\u5409\u62c9\\] back to english with nlu!\n\ntranslate\\_pipe = nlu.load('zh.translate\\_to.en')\n\nen\\_entities = translate\\_pipe.predict(\\['\u5510\u7eb3\u5fb7', '\u5b89\u5409\u62c9'\\])\n\n\\&gt;&gt;&gt; output :\n\n\\`\\`\\`\n\n|translation|\tchinese| \n\n|------|------|\n\n|donald | \u5510\u7eb3\u5fb7 |\n\n|angela | \t\u5b89\u5409\u62c9|\n\n&amp;#x200b;\n\n\\# new nlu notebooks\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n\\### nlu 1.1.0  new notebooks for new features\n\n\\- \\[translate between 192+ languages with marian\\]([https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/component\\_examples/sequence2sequence/translation\\_demo.ipynb](https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/translation_demo.ipynb))\n\n\\- \\[try out the 18 tasks like summarization question answering and more on t5\\]([https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/component\\_examples/sequence2sequence/t5\\_tasks\\_summarize\\_question\\_answering\\_and\\_more](https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/t5_tasks_summarize_question_answering_and_more))\n\n\\- \\[t5 open and closed book question answering tutorial\\]([https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/component\\_examples/sequence2sequence/t5\\_question\\_answering.ipynb](https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/component_examples/sequence2sequence/t5_question_answering.ipynb))\n\n\\- \\[tokenize, extract pos and ner in chinese\\]([https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/component\\_examples/multilingual/chinese\\_ner\\_pos\\_and\\_tokenization.ipynb](https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/component_examples/multilingual/chinese_ner_pos_and_tokenization.ipynb))\n\n\\- \\[tokenize, extract pos and ner in korean\\]([https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/component\\_examples/multilingual/korean\\_ner\\_pos\\_and\\_tokenization.ipynb](https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/component_examples/multilingual/korean_ner_pos_and_tokenization.ipynb))\n\n\\- \\[tokenize, extract pos and ner in japanese\\]([https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/component\\_examples/multilingual/japanese\\_ner\\_pos\\_and\\_tokenization.ipynb](https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/component_examples/multilingual/japanese_ner_pos_and_tokenization.ipynb))\n\n\\- \\[normalize documents\\]([https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/component\\_examples/text\\_pre\\_processing\\_and\\_cleaning/document\\_normalizer\\_demo.ipynb](https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/component_examples/text_pre_processing_and_cleaning/document_normalizer_demo.ipynb))\n\n\\- \\[aspect based sentiment ner sentiment for restaurants\\]([https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/component\\_examples/named\\_entity\\_recognition\\_(ner)/aspect\\_based\\_ner\\_sentiment\\_restaurants.ipynb](https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/component_examples/named_entity_recognition_(ner)/aspect_based_ner_sentiment_restaurants.ipynb))\n\n&amp;#x200b;\n\n\\### nlu 1.1.0 new classifier training tutorials\n\n\\#### binary classifier training jupyter tutorials\n\n\\- \\[2 class finance news sentiment classifier training\\]([https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/training/binary\\_text\\_classification/nlu\\_training\\_sentiment\\_classifier\\_demo\\_apple\\_twitter.ipynb](https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/training/binary_text_classification/nlu_training_sentiment_classifier_demo_apple_twitter.ipynb))\n\n\\- \\[2 class reddit comment sentiment classifier training\\]([https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/training/binary\\_text\\_classification/nlu\\_training\\_sentiment\\_classifier\\_demo\\_reddit.ipynb](https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/training/binary_text_classification/nlu_training_sentiment_classifier_demo_reddit.ipynb))\n\n\\- \\[2 class apple tweets sentiment classifier training\\]([https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/training/binary\\_text\\_classification/nlu\\_training\\_sentiment\\_classifier\\_demo\\_imdb.ipynb](https://github.com/johnsnowlabs/nlu/blob/master/examples/colab/training/binary_text_classification/nlu_training_s", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/l4cb40/n_720_new_nlp_models_300_supported_languages/',)", "identifyer": 5737338, "year": "2021"}, {"autor": "DingXiaoHan", "date": 1622616857000, "content": "[R]RepVGG: Making VGG-style ConvNets Great Again /!/ Do you still remember what happiness ConvNets (convolutional neural networks) brought to you seven years ago, when you could improve the performance by simply stacking several more conv layers?\n\n&amp;#x200B;\n\nOur recent work RepVGG is a super simple VGG-like architecture. The body has nothing but a stack of 3x3 conv and ReLU. It has a favorable speed-accuracy trade-off compared to other state-of-the-art models. On ImageNet, it achieves over 80% top-1 accuracy! Such good performance is realized by a structural RE-Parameterization so that it is named RepVGG.\n\n&amp;#x200B;\n\nRepVGG uses no NAS, no attention, no novel activation functions, and even no branches! How could a model with nothing but a stack of 3x3 conv and ReLU achieve SOTA performance?\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nhttps://preview.redd.it/jhwha161vs271.png?width=567&amp;format=png&amp;auto=webp&amp;s=d13db178e19dc71d19c6bdfd86df081feb76170f\n\nhttps://preview.redd.it/9smbikw1vs271.png?width=558&amp;format=png&amp;auto=webp&amp;s=7b033d870e76cdc45bb43f9c009a17205d84a722\n\n  \n\nPaper: [https://arxiv.org/abs/2101.03697](https://arxiv.org/abs/2101.03697)\n\nPretrained models and code\uff08PyTorch\uff09\uff1a[https://github.com/DingXiaoH/RepVGG](https://github.com/DingXiaoH/RepVGG). Got 1.7K stars and pretty much positive feedback! \n\n  \n\n## How simple can it be?\n\nAfter reading the paper, you may finish writing the code and start training in one hour. You will see the results the next day if you use eight 1080Ti GPUs. If you don\u2019t have time for reading the paper (or even this blog), just read the first 100 lines of the following code, and everything will be crystal clear. [https://github.com/DingXiaoH/RepVGG/blob/main/repvgg.py](https://github.com/DingXiaoH/RepVGG/blob/main/repvgg.py) \n\n  \n\n## What is VGG-like?\n\nWhen we are talking about VGG-like, we mean\n\n1. The model shall have no branches. We usually use \u201cplain\u201d or \u201cfeed-forward\u201d to describe such a topology.\n\n2. The model shall use only 3x3 conv. \n\n3. The model shall use only ReLU as the activation function.\n\nThe basic architecture is simple: over 20 3x3 conv layers are stacked up and split into five stages, and the first conv of every stage down-samples with stride=2.\n\nThe specifications (depth and width) are simple: an instance RepVGG-A has \\[1, 2, 4, 14, 1\\] layers for its five stages; RepVGG-B has \\[1, 4, 6, 16, 1\\]; the widths are \\[64, 128, 256, 512\\] scaled by multipliers like 1.5, 2, 2.5. The depth and width are casually set without careful tuning. \n\nThe training settings are simple: we trained for 120 epochs on ImageNet without tricks. You can even train it with a PyTorch-official-example-style script ([https://github.com/DingXiaoH/RepVGG/blob/main/train.py](https://github.com/DingXiaoH/RepVGG/blob/main/train.py)).\n\nSo why do we want such a super simple model, and how can it achieve SOTA performance? \n\n  \n\n## Why do we want VGG-like model?\n\nExcept for our pursuit for simplicity, a VGG-like super simple model has at least five advantages in practice (the paper has more details).\n\n1. 3x3 conv is very efficient. On GPU, the **computational density** (theoretical FLOPs/time usage) may achieve **four times** as that of 1x1 or 5x5 conv.\n\n2. Single-path architecture is very efficient because it has a **high degree of parallelism**. With the same FLOPs, a few big operators are much faster than many small operators.\n\n3. Single-path architecture is **memory-economical**. For example, the shortcut of ResNet increases 1X memory footprint.\n\n4. Single-path architecture is flexible because we can easily change the width of every layer (e.g., via channel pruning).\n\n5. The body of RepVGG has only one type of operator: 3x3conv-ReLU. When designing a specialized inference chip, given the chip size or power consumption, the fewer types of operator we require, the more computing units we can integrate onto the chip. So that we can integrate an enormous number of 3x3conv-ReLU units to make the inference extremely efficient. Don\u2019t forget that single-path architecture also allows us to use fewer memory units.\n\n  \n\n## Structural Re-parameterization makes VGG great again\n\nThe primary shortcoming of VGG is, of course, the poor performance. These years, a lot of research interests have been shifted from VGG to the numerous multi-branch architectures (ResNet, Inception, DenseNet, NAS-generated models, etc.), and it has been recognized that multi-branch models are usually more powerful than VGG-like ones. For example, a prior work stated that an explanation to the good performance of ResNet is that its shortcuts produce an implicit ensemble of numerous sub-models (because the total number of paths doubles at each branch). Obviously, a VGG-like model has no such advantage.\n\nA multi-branch architecture is beneficial to training, but we want the deployed model to be single-path. So we propose to **decouple the training-time multi-branch and inference-time single-path architecture**.\n\nWe are used to using ConvNets like this:\n\n1. Train a model\n\n2. Deploy that model\n\nBut here we propose a new methodology:\n\n1. Train a multi-branch model\n\n2. **Equivalently transform** the multi-branch model into a single-path model\n\n3. Deploy the single-path model\n\nIn this way, we can take advantage of the multi-branch training (high performance) and single-path inference (fast and memory-economical).\n\nApparently, the key is how to construct such a multi-branch model and the corresponding transformation.\n\nOur implementation adds a parallel 1x1 conv and an identity branch (if the input and output dimensions match) for each 3x3 conv to form a RepVGG block. This design borrows the idea of ResNet, but the difference is that ResNet adds a branch every two or three layers, but we add two branches for every 3x3 layer. \n\n&amp;#x200B;\n\nhttps://preview.redd.it/45fritq7vs271.png?width=259&amp;format=png&amp;auto=webp&amp;s=4db6e4316a34340ca285c392954335276d1253de\n\n  \n\nAfter training, we do the equivalent transformation to get the model for deployment. This transformation is quite simple because a 1x1 conv is a special (with many zero values) 3x3 conv, and an identity mapping is a special (the kernel is an identity matrix) 1x1 conv! By the linearity (more precisely, additivity) of convolution, we can merge the three branches of a RepVGG block into a single 3x3 conv.\n\nThe following figure describes the transformation. In this example, we have 2 input channels and output channels, so that the parameters of the 3x3 conv are four 3x3 matrices, the parameters of the 1x1 conv form a 2x2 matrix. Note that the three branches all have BN (batch normalization), and the parameters include the accumulated mean, standard deviation, and the learned scaling factor and bias. BN does not hinder our transformation because a conv and its following inference-time BN can be equivalently converted into a conv with bias (we usually refer to this as \u201cBN fusion\u201d). The paper and code contains some details. Just a few lines of code!\n\n&amp;#x200B;\n\n*Processing img 3cwurmb9vs271...*\n\n After \u201cBN fusion\u201d of the three branches (note that identity can be viewed as a \u201cconv\u201d and the parameters form a 2x2 identity matrix), we use 0 to pad the 1x1 kernel into 3x3. At last, we simply add up the three kernels and three biases. In this way, every transformed RepVGG Block has the same outputs as before, so that the trained model can be equivalently transformed into a single-path model with only 3x3 conv. \n\n&amp;#x200B;\n\n*Processing img sbhuuy5avs271...*\n\n Here we can see what \u201cstructural re-parameterization\u201d means. The training-time structure is coupled with a set of parameters, and the inference-time structure is coupled with another set. By equivalently transforming the parameters of the former into the latter, we can equivalently transform the structure of the former into the latter. \n\n  \n\n## Experimental results\n\nOn 1080Ti, RepVGG models have a favorable speed-accuracy trade-off. With the same training settings. The speed (examples/second) of RepVGG models are 183% of ResNet-50, 201% of ResNet-101, 259% of EfficientNet and 131% of RegNet. Note that compared to EfficientNet and RegNet, RepVGG used no NAS nor heavy iterative manual design.\n\n\n\nhttps://preview.redd.it/crtj04icvs271.png?width=226&amp;format=png&amp;auto=webp&amp;s=0ff836e309f9d907556d14d0a941127432655ea3\n\nIt is also shown that it may be inappropriate to measure the speed of different architectures with the theoretical FLOPs. For example, RepVGG-B2 has 10X FLOPs as EfficientNet-B3 but runs 2X as fast on 1080Ti, so that the former has 20X computational density as the latter.\n\nSemantic segmentation experiments on Cityscapes show that RepVGG models deliver 1% \\~ 1.7% higher mIoU than ResNets with higher speed or run 62% faster with 0.37% higher mIoU.\n\n \n\nhttps://preview.redd.it/zrdh2n3cvs271.png?width=239&amp;format=png&amp;auto=webp&amp;s=725c94f66def83d1dbff81509ad6824bc56d25db\n\nA set of ablation studies and comparisons have shown that structural re-parameterization is the key to the good performance of RepVGG. The paper has more details. \n\n  \n\n## FAQs\n\nPlease refer to the GitHub repo for the details and explanations.\n\n1. Is the inference-time model\u2019s output the same as the training-time model? Yes.\n\n2. How to quantize a RepVGG model? Post-training quantization or quantization-aware-training are both okay.\n\nHow to finetune a pretrained RepVGG model on other tasks? Finetune the training-time model and do the transformation at the end.   \n\n## Reference\n\n\\[1\\] Andreas Veit, Michael J Wilber, and Serge Belongie. Residual networks behave like ensembles of relatively shallow networks. In Advances in neural information processing systems, pages 550\u2013558, 2016. 2, 4, 8", "link": "https://www.reddit.com/r/MachineLearning/comments/nqflsp/rrepvgg_making_vggstyle_convnets_great_again/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "branches", "selectorShort": "branch", "MarkedSent": "[r]repvgg: making vgg-style convnets great again /!/ do you still remember what happiness convnets (convolutional neural networks) brought to you seven years ago, when you could improve the performance by simply stacking several more conv layers?\n\n&amp;#x200b;\n\nour recent work repvgg is a super simple vgg-like architecture. the body has nothing but a stack of 3x3 conv and relu. it has a favorable speed-accuracy trade-off compared to other state-of-the-art models. on imagenet, it achieves over 80% top-1 accuracy! such good performance is realized by a structural re-parameterization so that it is named repvgg.\n\n&amp;#x200b;\n\nrepvgg uses no nas, no attention, no novel activation functions, and even no -----> branches !!! ! how could a model with nothing but a stack of 3x3 conv and relu achieve sota performance?\n\n&amp;#x200b;\n\n&amp;#x200b;\n\nhttps://preview.redd.it/jhwha161vs271.png?width=567&amp;format=png&amp;auto=webp&amp;s=d13db178e19dc71d19c6bdfd86df081feb76170f\n\nhttps://preview.redd.it/9smbikw1vs271.png?width=558&amp;format=png&amp;auto=webp&amp;s=7b033d870e76cdc45bb43f9c009a17205d84a722\n\n  \n\npaper: [https://arxiv.org/abs/2101.03697](https://arxiv.org/abs/2101.03697)\n\npretrained models and code\uff08pytorch\uff09\uff1a[https://github.com/dingxiaoh/repvgg](https://github.com/dingxiaoh/repvgg). got 1.7k stars and pretty much positive feedback! \n\n  \n\n## how simple can it be?\n\nafter reading the paper, you may finish writing the code and start training in one hour. you will see the results the next day if you use eight 1080ti gpus. if you don\u2019t have time for reading the paper (or even this blog), just read the first 100 lines of the following code, and everything will be crystal clear. [https://github.com/dingxiaoh/repvgg/blob/main/repvgg.py](https://github.com/dingxiaoh/repvgg/blob/main/repvgg.py) \n\n  \n\n## what is vgg-like?\n\nwhen we are talking about vgg-like, we mean\n\n1. the model shall have no branches. we usually use \u201cplain\u201d or \u201cfeed-forward\u201d to describe such a topology.\n\n2. the model shall use only 3x3 conv. \n\n3. the model shall use only relu as the activation function.\n\nthe basic architecture is simple: over 20 3x3 conv layers are stacked up and split into five stages, and the first conv of every stage down-samples with stride=2.\n\nthe specifications (depth and width) are simple: an instance repvgg-a has \\[1, 2, 4, 14, 1\\] layers for its five stages; repvgg-b has \\[1, 4, 6, 16, 1\\]; the widths are \\[64, 128, 256, 512\\] scaled by multipliers like 1.5, 2, 2.5. the depth and width are casually set without careful tuning. \n\nthe training settings are simple: we trained for 120 epochs on imagenet without tricks. you can even train it with a pytorch-official-example-style script ([https://github.com/dingxiaoh/repvgg/blob/main/train.py](https://github.com/dingxiaoh/repvgg/blob/main/train.py)).\n\nso why do we want such a super simple model, and how can it achieve sota performance? \n\n  \n\n## why do we want vgg-like model?\n\nexcept for our pursuit for simplicity, a vgg-like super simple model has at least five advantages in practice (the paper has more details).\n\n1. 3x3 conv is very efficient. on gpu, the **computational density** (theoretical flops/time usage) may achieve **four times** as that of 1x1 or 5x5 conv.\n\n2. single-path architecture is very efficient because it has a **high degree of parallelism**. with the same flops, a few big operators are much faster than many small operators.\n\n3. single-path architecture is **memory-economical**. for example, the shortcut of resnet increases 1x memory footprint.\n\n4. single-path architecture is flexible because we can easily change the width of every layer (e.g., via channel pruning).\n\n5. the body of repvgg has only one type of operator: 3x3conv-relu. when designing a specialized inference chip, given the chip size or power consumption, the fewer types of operator we require, the more computing units we can integrate onto the chip. so that we can integrate an enormous number of 3x3conv-relu units to make the inference extremely efficient. don\u2019t forget that single-path architecture also allows us to use fewer memory units.\n\n  \n\n## structural re-parameterization makes vgg great again\n\nthe primary shortcoming of vgg is, of course, the poor performance. these years, a lot of research interests have been shifted from vgg to the numerous multi-branch architectures (resnet, inception, densenet, nas-generated models, etc.), and it has been recognized that multi-branch models are usually more powerful than vgg-like ones. for example, a prior work stated that an explanation to the good performance of resnet is that its shortcuts produce an implicit ensemble of numerous sub-models (because the total number of paths doubles at each branch). obviously, a vgg-like model has no such advantage.\n\na multi-branch architecture is beneficial to training, but we want the deployed model to be single-path. so we propose to **decouple the training-time multi-branch and inference-time single-path architecture**.\n\nwe are used to using convnets like this:\n\n1. train a model\n\n2. deploy that model\n\nbut here we propose a new methodology:\n\n1. train a multi-branch model\n\n2. **equivalently transform** the multi-branch model into a single-path model\n\n3. deploy the single-path model\n\nin this way, we can take advantage of the multi-branch training (high performance) and single-path inference (fast and memory-economical).\n\napparently, the key is how to construct such a multi-branch model and the corresponding transformation.\n\nour implementation adds a parallel 1x1 conv and an identity branch (if the input and output dimensions match) for each 3x3 conv to form a repvgg block. this design borrows the idea of resnet, but the difference is that resnet adds a branch every two or three layers, but we add two branches for every 3x3 layer. \n\n&amp;#x200b;\n\nhttps://preview.redd.it/45fritq7vs271.png?width=259&amp;format=png&amp;auto=webp&amp;s=4db6e4316a34340ca285c392954335276d1253de\n\n  \n\nafter training, we do the equivalent transformation to get the model for deployment. this transformation is quite simple because a 1x1 conv is a special (with many zero values) 3x3 conv, and an identity mapping is a special (the kernel is an identity matrix) 1x1 conv! by the linearity (more precisely, additivity) of convolution, we can merge the three branches of a repvgg block into a single 3x3 conv.\n\nthe following figure describes the transformation. in this example, we have 2 input channels and output channels, so that the parameters of the 3x3 conv are four 3x3 matrices, the parameters of the 1x1 conv form a 2x2 matrix. note that the three branches all have bn (batch normalization), and the parameters include the accumulated mean, standard deviation, and the learned scaling factor and bias. bn does not hinder our transformation because a conv and its following inference-time bn can be equivalently converted into a conv with bias (we usually refer to this as \u201cbn fusion\u201d). the paper and code contains some details. just a few lines of code!\n\n&amp;#x200b;\n\n*processing img 3cwurmb9vs271...*\n\n after \u201cbn fusion\u201d of the three branches (note that identity can be viewed as a \u201cconv\u201d and the parameters form a 2x2 identity matrix), we use 0 to pad the 1x1 kernel into 3x3. at last, we simply add up the three kernels and three biases. in this way, every transformed repvgg block has the same outputs as before, so that the trained model can be equivalently transformed into a single-path model with only 3x3 conv. \n\n&amp;#x200b;\n\n*processing img sbhuuy5avs271...*\n\n here we can see what \u201cstructural re-parameterization\u201d means. the training-time structure is coupled with a set of parameters, and the inference-time structure is coupled with another set. by equivalently transforming the parameters of the former into the latter, we can equivalently transform the structure of the former into the latter. \n\n  \n\n## experimental results\n\non 1080ti, repvgg models have a favorable speed-accuracy trade-off. with the same training settings. the speed (examples/second) of repvgg models are 183% of resnet-50, 201% of resnet-101, 259% of efficientnet and 131% of regnet. note that compared to efficientnet and regnet, repvgg used no nas nor heavy iterative manual design.\n\n\n\nhttps://preview.redd.it/crtj04icvs271.png?width=226&amp;format=png&amp;auto=webp&amp;s=0ff836e309f9d907556d14d0a941127432655ea3\n\nit is also shown that it may be inappropriate to measure the speed of different architectures with the theoretical flops. for example, repvgg-b2 has 10x flops as efficientnet-b3 but runs 2x as fast on 1080ti, so that the former has 20x computational density as the latter.\n\nsemantic segmentation experiments on cityscapes show that repvgg models deliver 1% \\~ 1.7% higher miou than resnets with higher speed or run 62% faster with 0.37% higher miou.\n\n \n\nhttps://preview.redd.it/zrdh2n3cvs271.png?width=239&amp;format=png&amp;auto=webp&amp;s=725c94f66def83d1dbff81509ad6824bc56d25db\n\na set of ablation studies and comparisons have shown that structural re-parameterization is the key to the good performance of repvgg. the paper has more details. \n\n  \n\n## faqs\n\nplease refer to the github repo for the details and explanations.\n\n1. is the inference-time model\u2019s output the same as the training-time model? yes.\n\n2. how to quantize a repvgg model? post-training quantization or quantization-aware-training are both okay.\n\nhow to finetune a pretrained repvgg model on other tasks? finetune the training-time model and do the transformation at the end.   \n\n## reference\n\n\\[1\\] andreas veit, michael j wilber, and serge belongie. residual networks behave like ensembles of relatively shallow networks. in advances in neural information processing systems, pages 550\u2013558, 2016. 2, 4, 8", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 65, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/nqflsp/rrepvgg_making_vggstyle_convnets_great_again/',)", "identifyer": 5737927, "year": "2021"}, {"autor": "skeering", "date": 1619970192000, "content": "[R][D] Starting a Post-Doc and Looking for Advice on Research Area /!/ So I'll be starting a post-doc within the next 6-12 months, and I'm looking around for opportunities. My supervisor advised me not to do a post-doc in the same area as my PhD and instead to \"branch out\" into two main fields, to diversify.\n\nMy PhD was/is in XAI, and I'm very happy with how the last 3 years went. Going forward, I was looking for \"hot\" research areas aside from this, and what I personally just like/find interesting. Correct me if I'm wrong, but it seems the big questions in AI going forward are.\n\n1. How to generalise better (e.g., take what you learn from MNIST and apply it to FashionMNIST).\n2. How to make systems immune to adversarial attacks.\n3. How to get explanations from opaque models (e.g., in medical radiology).\n4. How to learn from fewer examples (one shot learning and semi-supervised learning). The ultimate goal being unsupervised learning that works well in the \"real world\".\n\nI'm not sure if I missed anything? Probably what I'm most interested in (aside from XAI) is number 4 above. Would I be correct in assuming any of these four areas are important in the next generation of AI technology? Is there any area I missed? Lastly, would you agree that number 4. is a good area to \"get into\" as a second research interest the next 5-10 years?\n\nThanks and have a great day.", "link": "https://www.reddit.com/r/MachineLearning/comments/n3919q/rd_starting_a_postdoc_and_looking_for_advice_on/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "branch", "selectorShort": "branch", "MarkedSent": "[r][d] starting a post-doc and looking for advice on research area /!/ so i'll be starting a post-doc within the next 6-12 months, and i'm looking around for opportunities. my supervisor advised me not to do a post-doc in the same area as my phd and instead to \"-----> branch !!!  out\" into two main fields, to diversify.\n\nmy phd was/is in xai, and i'm very happy with how the last 3 years went. going forward, i was looking for \"hot\" research areas aside from this, and what i personally just like/find interesting. correct me if i'm wrong, but it seems the big questions in ai going forward are.\n\n1. how to generalise better (e.g., take what you learn from mnist and apply it to fashionmnist).\n2. how to make systems immune to adversarial attacks.\n3. how to get explanations from opaque models (e.g., in medical radiology).\n4. how to learn from fewer examples (one shot learning and semi-supervised learning). the ultimate goal being unsupervised learning that works well in the \"real world\".\n\ni'm not sure if i missed anything? probably what i'm most interested in (aside from xai) is number 4 above. would i be correct in assuming any of these four areas are important in the next generation of ai technology? is there any area i missed? lastly, would you agree that number 4. is a good area to \"get into\" as a second research interest the next 5-10 years?\n\nthanks and have a great day.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 5, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/n3919q/rd_starting_a_postdoc_and_looking_for_advice_on/',)", "identifyer": 5740356, "year": "2021"}], "name": "branchMachineLearning2021"}
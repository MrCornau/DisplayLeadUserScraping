{"interestingcomments": [{"autor": "grid_world", "date": 1621773162000, "content": "[R] Over-fitting in Iterative Pruning /!/ In global, unstructured and iterative pruning algorithms such as:\n\n1. \"Learning both Weights and Connections for Efficient Neural Networks\" by Han et al.\n2. \"Deep Compression\" by Han et al.\n3. \"The Lottery Ticket Hypothesis\" by Frankle et al.\n\nexcept \"The Lottery Ticket Hypothesis\" where the weights are rewind-ed to their original values and resulting sub-network is trained from scratch thereby needed more time/epoch.\n\nSince the usual algorithm is:\n\nTake a trained neural network and repeat steps 1 and 2:\n\n1. prune globally smallest magnitude p% of weights\n2. re-train/fine-tune pruned neural network to recover from pruning\n\nUsually, the number of pruning rounds needed to go from original and unpruned network (sparsity = 0%) to 99% sparsity requires 25-34 rounds depending on the exact architecture and number of trainable parameters.\n\nIn my experiments I have observed that during this repeated *prune and repeat* algorithm, the resulting pruned neural networks start to overfit to the training dataset, which is to be expected. Apart from using techniques such as regularization, dropout, data augmentation, learning rate scheduler, etc. are there any other techniques to prevent this overfit?\n\nI assume that such a resulting pruned sub-network when used for real world tasks might not perform as expected due to the overfitting induced due to the *iterative* process. Correct me if I am wrong.\n\nThanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/nj6ui3/r_overfitting_in_iterative_pruning/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "prune", "selectorShort": "prune", "MarkedSent": "[r] over-fitting in iterative pruning /!/ in global, unstructured and iterative pruning algorithms such as:\n\n1. \"learning both weights and connections for efficient neural networks\" by han et al.\n2. \"deep compression\" by han et al.\n3. \"the lottery ticket hypothesis\" by frankle et al.\n\nexcept \"the lottery ticket hypothesis\" where the weights are rewind-ed to their original values and resulting sub-network is trained from scratch thereby needed more time/epoch.\n\nsince the usual algorithm is:\n\ntake a trained neural network and repeat steps 1 and 2:\n\n1. -----> prune !!!  globally smallest magnitude p% of weights\n2. re-train/fine-tune pruned neural network to recover from pruning\n\nusually, the number of pruning rounds needed to go from original and unpruned network (sparsity = 0%) to 99% sparsity requires 25-34 rounds depending on the exact architecture and number of trainable parameters.\n\nin my experiments i have observed that during this repeated *prune and repeat* algorithm, the resulting pruned neural networks start to overfit to the training dataset, which is to be expected. apart from using techniques such as regularization, dropout, data augmentation, learning rate scheduler, etc. are there any other techniques to prevent this overfit?\n\ni assume that such a resulting pruned sub-network when used for real world tasks might not perform as expected due to the overfitting induced due to the *iterative* process. correct me if i am wrong.\n\nthanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/nj6ui3/r_overfitting_in_iterative_pruning/',)", "identifyer": 5725467, "year": "2021"}, {"autor": "markurtz", "date": 1629552061000, "content": "[P] Tutorial: Prune and quantize YOLOv5 for 12x smaller size and 10x better performance on CPUs", "link": "https://www.reddit.com/r/MachineLearning/comments/p8rcm3/p_tutorial_prune_and_quantize_yolov5_for_12x/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "prune", "selectorShort": "prune", "MarkedSent": "[p] tutorial: -----> prune !!!  and quantize yolov5 for 12x smaller size and 10x better performance on cpus", "sortedWord": "None", "removed": "('nan',)", "score": 107, "comments": 22, "media": "('hosted:video',)", "medialink": "('https://v.redd.it/aqe51fiwopi71',)", "identifyer": 5726088, "year": "2021"}, {"autor": "grid_world", "date": 1626850673000, "content": "[D] Prune Neural Networks layers for f% sparsity - TensorFlow2 /!/ I am using TensorFlow 2.5 and Python3.8 where I have a simple TF2 CNN having one conv layer and an output layer for binary classification as follows:\n\n        num_filters = 32    \n        def cnn_model():\n                model = Sequential()\n                \n                model.add(\n                    InputLayer(input_shape = (32, 32, 3))\n                )\n                \n                model.add(\n                    Conv2D(\n                        filters = num_filters, kernel_size = (3, 3),\n                        activation = 'relu', kernel_initializer = tf.initializers.he_normal(),\n                        strides = (1, 1), padding = 'same',\n                        use_bias = True, \n                        bias_initializer = RandomNormal(mean = 0.0, stddev = 0.05)\n                        # kernel_regularizer = regularizers.l2(weight_decay)\n                    )\n                )\n                \n                model.add(Flatten())\n                \n                model.add(\n                    Dense(\n                        units = 1, activation = 'sigmoid'\n                    )\n                )\n                \n                return model\n    \n        \n        # I then instantiate two instances of it:\n    \n        model = cnn_model()\n        model2 = cnn_model()\n    \n        model.summary()\n        '''\n        Model: \"sequential_2\"\n        _________________________________________________________________\n        Layer (type)                 Output Shape              Param #   \n        =================================================================\n        conv2d_5 (Conv2D)            (None, 32, 32, 32)        896       \n        _________________________________________________________________\n        flatten_2 (Flatten)          (None, 32768)             0         \n        _________________________________________________________________\n        dense_2 (Dense)              (None, 1)                 32769     \n        =================================================================\n        Total params: 33,665\n        Trainable params: 33,665\n        Non-trainable params: 0\n        '''\n    \n        def count_nonzero_params(model):\n            # Count number of non-zero parameters in each layer and in total-\n            model_sum_params = 0\n            \n            for layer in model.trainable_weights:\n                loc_param = tf.math.count_nonzero(layer, axis = None).numpy()\n                model_sum_params += loc_param\n            \n            # print(\"Total number of trainable parameters = {0}\\n\".format(model_sum_params))\n            \n            return model_sum_params\n    \n        # Sanity check-\n        count_nonzero_params(model)\n        # 33664\n\nA random input is used to make predictions using the two models-\n\n        x = tf.random.normal(shape = (5, 32, 32, 3))\n        pred = model(x)\n        pred2 = model2(x)\n        pred.shape, pred.shape\n        # (TensorShape([5, 1]), TensorShape([5, 1]))\n\nA pruning function has been defined to prune f% of smallest magnitude weights **for model1** *for each layer* such that:\n\n&gt;for connections in model, **only those connections are pruned** (per layer) **which are f% of smallest magnitude weights in both the models** viz., model and model2\n\n        def custom_pruning(model1, model2, p):\n            \"\"\"\n            Function to prune p% of smallest magnitude weights of \n            a given CNN model globally.\n            \n            Input:\n            model1            TF2 Convolutional Neural Network model\n            model2            TF2 Convolutional Neural Network model\n            \n                              \n            p                 Prune p% of smallest magnitude weights globally\n            \n            Output:\n            Returns a Python3 list containing layer-wise pruned weights.    \n            \"\"\"\n            \n            # Python3 list to hold weights of model1-\n            model1_np_wts = []\n            \n            for layer in model1.weights:\n                model1_np_wts.append(layer.numpy())\n            \n            # Python3 list to hold flattened weights-\n            flattened_wts = []\n        \n            for layer in model1_np_wts:\n                flattened_wts.append(np.abs(layer.flatten()))\n        \n            # Compute pth percentile threshold using all weights from model1-\n            threshold_weights1 = np.percentile(np.concatenate(flattened_wts), p)\n            \n            del flattened_wts\n            \n            \n            # Python3 list to hold weights of model2-\n            model2_np_wts = []\n        \n            for layer in model2.weights:\n                model2_np_wts.append(layer.numpy())\n        \n            # Python3 list to hold flattened weights for model2-\n            flattened_wts2 = []\n        \n            for layer in model2_np_wts:\n                flattened_wts2.append(np.abs(layer.flatten()))\n        \n            # Compute pth percentile threshold using all weights from model2-\n            threshold_weights2 = np.percentile(np.concatenate(flattened_wts2), p)\n            \n            del flattened_wts2\n            \n            \n            # Python3 list to contain pruned weights-\n            pruned_wts = []\n            \n            for layer_model1, layer_model2 in zip(model1_np_wts, model2_np_wts):\n                if len(layer_model1.shape) == 4:\n                    layer_wts_abs = np.abs(layer_model1)\n                    layer_wts2_abs = np.abs(layer_model2)\n                    layer_wts_abs[(layer_wts_abs &lt; threshold_weights1) &amp; (layer_wts2_abs &lt; threshold_weights2)] = 0\n                    layer_mod = np.where(layer_wts_abs == 0, 0, layer_model1)\n                    pruned_wts.append(layer_mod)\n                elif len(layer_model1.shape) == 2:\n                    layer_wts_abs = np.abs(layer_model1)\n                    layer_wts2_abs = np.abs(layer_model2)\n                    layer_wts_abs[(layer_wts_abs &lt; threshold_weights1) &amp; (layer_wts2_abs &lt; threshold_weights2)] = 0\n                    layer_mod = np.where(layer_wts_abs == 0, 0, layer_model1)\n                    pruned_wts.append(layer_mod)\n                else:\n                    pruned_wts.append(layer_model1)\n                \n                \n            return pruned_wts\n            \n    \n        # Prune 15% of smallest magnitude weights-\n        pruned_wts = custom_pruning(model1 = model, model2 = model2, p = 15)\n    \n        # Initialize and load weights for pruned model-\n        new_model = cnn_model()\n        new_model.set_weights(pruned_wts)\n        \n        # Count original and unpruned parameters-\n        orig_params = count_nonzero_params(model)\n        \n        # Count pruned parameters-\n        pruned_params = count_nonzero_params(new_model)\n        \n        # Compute actual sparsity-\n        sparsity = ((orig_params - pruned_params) / orig_params) * 100\n        \n        print(f\"actual sparsity = {sparsity:.2f}% for a given sparsity = 15%\")\n        # actual sparsity = 2.22% for a given sparsity = 15%\n\nThe problem is that for a given sparsity of 15%, only 2.22% connections are pruned. To achieve the desired 15% sparsity, a hit and trial method to find 'p' parameter's value-\n\n        # Prune 15% of smallest magnitude weights-\n        pruned_wts = custom_pruning(model1 = model, model2 = model2, p = 38)\n    \n        # Initialize and load weights for pruned model-\n        new_model = cnn_model()\n        new_model.set_weights(pruned_wts)\n        \n        # Count pruned parameters-\n        pruned_params = count_nonzero_params(new_model)\n        \n        # Compute actual sparsity-\n        sparsity = ((orig_params - pruned_params) / orig_params) * 100\n        \n        print(f\"actual sparsity = {sparsity:.2f}% for a given sparsity = 15%\")\n        # actual sparsity = 14.40% for a given sparsity = 15%\n\nDue to two conditions while filtering in 'custom\\_pruning()', this difference between desired and actual sparsity levels are occurring.\n\n&amp;#x200B;\n\nIs there some other better way to achieve this that I am missing out?\n\n&amp;#x200B;\n\nThanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/ookrnu/d_prune_neural_networks_layers_for_f_sparsity/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "prune", "selectorShort": "prune", "MarkedSent": "[d] -----> prune !!!  neural networks layers for f% sparsity - tensorflow2 /!/ i am using tensorflow 2.5 and python3.8 where i have a simple tf2 cnn having one conv layer and an output layer for binary classification as follows:\n\n        num_filters = 32    \n        def cnn_model():\n                model = sequential()\n                \n                model.add(\n                    inputlayer(input_shape = (32, 32, 3))\n                )\n                \n                model.add(\n                    conv2d(\n                        filters = num_filters, kernel_size = (3, 3),\n                        activation = 'relu', kernel_initializer = tf.initializers.he_normal(),\n                        strides = (1, 1), padding = 'same',\n                        use_bias = true, \n                        bias_initializer = randomnormal(mean = 0.0, stddev = 0.05)\n                        # kernel_regularizer = regularizers.l2(weight_decay)\n                    )\n                )\n                \n                model.add(flatten())\n                \n                model.add(\n                    dense(\n                        units = 1, activation = 'sigmoid'\n                    )\n                )\n                \n                return model\n    \n        \n        # i then instantiate two instances of it:\n    \n        model = cnn_model()\n        model2 = cnn_model()\n    \n        model.summary()\n        '''\n        model: \"sequential_2\"\n        _________________________________________________________________\n        layer (type)                 output shape              param #   \n        =================================================================\n        conv2d_5 (conv2d)            (none, 32, 32, 32)        896       \n        _________________________________________________________________\n        flatten_2 (flatten)          (none, 32768)             0         \n        _________________________________________________________________\n        dense_2 (dense)              (none, 1)                 32769     \n        =================================================================\n        total params: 33,665\n        trainable params: 33,665\n        non-trainable params: 0\n        '''\n    \n        def count_nonzero_params(model):\n            # count number of non-zero parameters in each layer and in total-\n            model_sum_params = 0\n            \n            for layer in model.trainable_weights:\n                loc_param = tf.math.count_nonzero(layer, axis = none).numpy()\n                model_sum_params += loc_param\n            \n            # print(\"total number of trainable parameters = {0}\\n\".format(model_sum_params))\n            \n            return model_sum_params\n    \n        # sanity check-\n        count_nonzero_params(model)\n        # 33664\n\na random input is used to make predictions using the two models-\n\n        x = tf.random.normal(shape = (5, 32, 32, 3))\n        pred = model(x)\n        pred2 = model2(x)\n        pred.shape, pred.shape\n        # (tensorshape([5, 1]), tensorshape([5, 1]))\n\na pruning function has been defined to prune f% of smallest magnitude weights **for model1** *for each layer* such that:\n\n&gt;for connections in model, **only those connections are pruned** (per layer) **which are f% of smallest magnitude weights in both the models** viz., model and model2\n\n        def custom_pruning(model1, model2, p):\n            \"\"\"\n            function to prune p% of smallest magnitude weights of \n            a given cnn model globally.\n            \n            input:\n            model1            tf2 convolutional neural network model\n            model2            tf2 convolutional neural network model\n            \n                              \n            p                 prune p% of smallest magnitude weights globally\n            \n            output:\n            returns a python3 list containing layer-wise pruned weights.    \n            \"\"\"\n            \n            # python3 list to hold weights of model1-\n            model1_np_wts = []\n            \n            for layer in model1.weights:\n                model1_np_wts.append(layer.numpy())\n            \n            # python3 list to hold flattened weights-\n            flattened_wts = []\n        \n            for layer in model1_np_wts:\n                flattened_wts.append(np.abs(layer.flatten()))\n        \n            # compute pth percentile threshold using all weights from model1-\n            threshold_weights1 = np.percentile(np.concatenate(flattened_wts), p)\n            \n            del flattened_wts\n            \n            \n            # python3 list to hold weights of model2-\n            model2_np_wts = []\n        \n            for layer in model2.weights:\n                model2_np_wts.append(layer.numpy())\n        \n            # python3 list to hold flattened weights for model2-\n            flattened_wts2 = []\n        \n            for layer in model2_np_wts:\n                flattened_wts2.append(np.abs(layer.flatten()))\n        \n            # compute pth percentile threshold using all weights from model2-\n            threshold_weights2 = np.percentile(np.concatenate(flattened_wts2), p)\n            \n            del flattened_wts2\n            \n            \n            # python3 list to contain pruned weights-\n            pruned_wts = []\n            \n            for layer_model1, layer_model2 in zip(model1_np_wts, model2_np_wts):\n                if len(layer_model1.shape) == 4:\n                    layer_wts_abs = np.abs(layer_model1)\n                    layer_wts2_abs = np.abs(layer_model2)\n                    layer_wts_abs[(layer_wts_abs &lt; threshold_weights1) &amp; (layer_wts2_abs &lt; threshold_weights2)] = 0\n                    layer_mod = np.where(layer_wts_abs == 0, 0, layer_model1)\n                    pruned_wts.append(layer_mod)\n                elif len(layer_model1.shape) == 2:\n                    layer_wts_abs = np.abs(layer_model1)\n                    layer_wts2_abs = np.abs(layer_model2)\n                    layer_wts_abs[(layer_wts_abs &lt; threshold_weights1) &amp; (layer_wts2_abs &lt; threshold_weights2)] = 0\n                    layer_mod = np.where(layer_wts_abs == 0, 0, layer_model1)\n                    pruned_wts.append(layer_mod)\n                else:\n                    pruned_wts.append(layer_model1)\n                \n                \n            return pruned_wts\n            \n    \n        # prune 15% of smallest magnitude weights-\n        pruned_wts = custom_pruning(model1 = model, model2 = model2, p = 15)\n    \n        # initialize and load weights for pruned model-\n        new_model = cnn_model()\n        new_model.set_weights(pruned_wts)\n        \n        # count original and unpruned parameters-\n        orig_params = count_nonzero_params(model)\n        \n        # count pruned parameters-\n        pruned_params = count_nonzero_params(new_model)\n        \n        # compute actual sparsity-\n        sparsity = ((orig_params - pruned_params) / orig_params) * 100\n        \n        print(f\"actual sparsity = {sparsity:.2f}% for a given sparsity = 15%\")\n        # actual sparsity = 2.22% for a given sparsity = 15%\n\nthe problem is that for a given sparsity of 15%, only 2.22% connections are pruned. to achieve the desired 15% sparsity, a hit and trial method to find 'p' parameter's value-\n\n        # prune 15% of smallest magnitude weights-\n        pruned_wts = custom_pruning(model1 = model, model2 = model2, p = 38)\n    \n        # initialize and load weights for pruned model-\n        new_model = cnn_model()\n        new_model.set_weights(pruned_wts)\n        \n        # count pruned parameters-\n        pruned_params = count_nonzero_params(new_model)\n        \n        # compute actual sparsity-\n        sparsity = ((orig_params - pruned_params) / orig_params) * 100\n        \n        print(f\"actual sparsity = {sparsity:.2f}% for a given sparsity = 15%\")\n        # actual sparsity = 14.40% for a given sparsity = 15%\n\ndue to two conditions while filtering in 'custom\\_pruning()', this difference between desired and actual sparsity levels are occurring.\n\n&amp;#x200b;\n\nis there some other better way to achieve this that i am missing out?\n\n&amp;#x200b;\n\nthanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/ookrnu/d_prune_neural_networks_layers_for_f_sparsity/',)", "identifyer": 5726307, "year": "2021"}, {"autor": "gauravc2796", "date": 1632245127000, "content": "[D] New Research Paper video Explainer: Block Pruning For Faster Transformers by Hugging Face /!/  Hi,\n\nA new research paper explainer has been released:\n\nBlock Pruning For Faster Transformers by Hugging Face [https://youtu.be/CyJdzkcdGl0](https://youtu.be/CyJdzkcdGl0)\n\nVideo explains the basics of pruning, distillation, paper overview, and also codes.\n\nTLDR:\n\n1. Large pre-trained NN to have billions of parameters to train. It is computationally expensive to load or download these models.\n2. Here comes pruning and distillation. Both of these techniques try to eliminate parameters (weights) with very little drop in accuracy.\n3. If we want to prune weights when finetuning our model, then movement pruning works best for that. However, the time and space requirements to train these models again to drop unnecessary could be expensive.\n4. Here comes block pruning where it creates square blocks in the weight matrix and then tries to perform movement pruning on the blocks rather than weights.\n\nFor an in-depth overview refer to the video explainer.\n\nIf you like this content, do share it with friends and support the channel. If any opinions, clarifications, or anything you can mention in the comments.", "link": "https://www.reddit.com/r/MachineLearning/comments/psni6i/d_new_research_paper_video_explainer_block/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "prune", "selectorShort": "prune", "MarkedSent": "[d] new research paper video explainer: block pruning for faster transformers by hugging face /!/  hi,\n\na new research paper explainer has been released:\n\nblock pruning for faster transformers by hugging face [https://youtu.be/cyjdzkcdgl0](https://youtu.be/cyjdzkcdgl0)\n\nvideo explains the basics of pruning, distillation, paper overview, and also codes.\n\ntldr:\n\n1. large pre-trained nn to have billions of parameters to train. it is computationally expensive to load or download these models.\n2. here comes pruning and distillation. both of these techniques try to eliminate parameters (weights) with very little drop in accuracy.\n3. if we want to -----> prune !!!  weights when finetuning our model, then movement pruning works best for that. however, the time and space requirements to train these models again to drop unnecessary could be expensive.\n4. here comes block pruning where it creates square blocks in the weight matrix and then tries to perform movement pruning on the blocks rather than weights.\n\nfor an in-depth overview refer to the video explainer.\n\nif you like this content, do share it with friends and support the channel. if any opinions, clarifications, or anything you can mention in the comments.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/psni6i/d_new_research_paper_video_explainer_block/',)", "identifyer": 5728701, "year": "2021"}, {"autor": "grid_world", "date": 1617415793000, "content": "[R] Iterative Pruning: LeNet-300-100 - PyTorch /!/ I am trying to implement iterative pruning  algorithm (as described in the research papers in[\\[1\\]](https://arxiv.org/abs/1506.02626), [\\[2\\]](https://arxiv.org/abs/1803.03635))  which is: train a model, prune p% of smallest weights per layer,  re-train the pruned model and repeat. For experiment purposes, I  am  using LeNet-300-100 neural network on MNIST.\n\nThe code can be accessed [here](https://github.com/arjun-majumdar/Lottery_Ticket_Hypothesis-TensorFlow_2/blob/master/LeNet_300_100-Iterative_Pruning.ipynb)\n\nWithin  the function \u201ctrain\\_with\\_grad\\_freezing(model, epoch)\u201d, I am  using the  following lines of code for freezing the pruned weights by  making their  computed gradients equal to 0:\n\n    for layer_name, param in model.named_parameters():\n        if 'weight' in layer_name:\n            tensor = param.data.cpu().numpy()\n            grad_tensor = param.grad.data.cpu().numpy()\n            grad_tensor = np.where(tensor == 0, 0, grad_tensor)\n            param.grad.data = torch.from_numpy(grad_tensor).to(device)  \n\nThe first time I train the model, the code works fine after which I prune the layers by using the code:\n\n    # Prune 15% of smallest magnitude weights in FC layers and 10% in output layer- pruned_d = prune_lenet(model = best_model, pruning_params_fc = 15, pruning_params_op = 10) \n    # Initialize and load pruned Python3 dict into a new model- pruned_model = LeNet300() pruned_model.load_state_dict(pruned_d)  \n\nHowever, on re-training this pruned model, the training metric is stuck for these values:\n\n&gt;training loss = 0.0285, training accuracy = 99.04%, val\\_loss = 0.0910 &amp; val\\_accuracy = 97.68%\n\nWhat\u2019s going wrong?", "link": "https://www.reddit.com/r/MachineLearning/comments/miz601/r_iterative_pruning_lenet300100_pytorch/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "prune", "selectorShort": "prune", "MarkedSent": "[r] iterative pruning: lenet-300-100 - pytorch /!/ i am trying to implement iterative pruning  algorithm (as described in the research papers in[\\[1\\]](https://arxiv.org/abs/1506.02626), [\\[2\\]](https://arxiv.org/abs/1803.03635))  which is: train a model, -----> prune !!!  p% of smallest weights per layer,  re-train the pruned model and repeat. for experiment purposes, i  am  using lenet-300-100 neural network on mnist.\n\nthe code can be accessed [here](https://github.com/arjun-majumdar/lottery_ticket_hypothesis-tensorflow_2/blob/master/lenet_300_100-iterative_pruning.ipynb)\n\nwithin  the function \u201ctrain\\_with\\_grad\\_freezing(model, epoch)\u201d, i am  using the  following lines of code for freezing the pruned weights by  making their  computed gradients equal to 0:\n\n    for layer_name, param in model.named_parameters():\n        if 'weight' in layer_name:\n            tensor = param.data.cpu().numpy()\n            grad_tensor = param.grad.data.cpu().numpy()\n            grad_tensor = np.where(tensor == 0, 0, grad_tensor)\n            param.grad.data = torch.from_numpy(grad_tensor).to(device)  \n\nthe first time i train the model, the code works fine after which i prune the layers by using the code:\n\n    # prune 15% of smallest magnitude weights in fc layers and 10% in output layer- pruned_d = prune_lenet(model = best_model, pruning_params_fc = 15, pruning_params_op = 10) \n    # initialize and load pruned python3 dict into a new model- pruned_model = lenet300() pruned_model.load_state_dict(pruned_d)  \n\nhowever, on re-training this pruned model, the training metric is stuck for these values:\n\n&gt;training loss = 0.0285, training accuracy = 99.04%, val\\_loss = 0.0910 &amp; val\\_accuracy = 97.68%\n\nwhat\u2019s going wrong?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/miz601/r_iterative_pruning_lenet300100_pytorch/',)", "identifyer": 5736574, "year": "2021"}, {"autor": "arcful1", "date": 1626635531000, "content": "[D] My take on conscious neural networks and how we get past backpropagation and gradient descent /!/ First, thanks for the feedback. That's as short as i can make it. A valence system and better model of neurons is how we get past backpropagation and gradient descent (that's about as much as i know about current ANNs)\n\n\nLet positive valence mean beneficial to the organism's genes odds of survival. Let negative valence be the opposite.\n\n\nStart with 2 network regions\n\nMake one 100x bigger and call it Cortex. This size difference is crucial.\n\nCall the small region the Valence Core (VC)\n\nLet one end of a VC be positive and the other negative (in a semantic way, not electrical)\n\nLet there be a \"Sparse Sampling Network\" between the cortex and valence core.\n\nLet there be a \"Sparse Sampling Network\" between the cortex and valence core.\n- Imagine if we broke up all of the cortex and VC into an equal number of tiny cubes. Now let each tiny cube in the VC have a set of connections to a fairly random and uniform set of cortex tiny cubes.\n- Now imagine the connections actual endpoint in the tiny cubes to be at any random point in that tiny cube.\n- Now imagine that these connection have a (synaptic) strength greater than that of the average connection\n- Now imagine that these connections are in essence bi-directional (perhaps 2 mirrored sampling networks)\n\nLet Valence Ambiguity be how much you know if any individual sensory signal/impulse is beneficial or detrimental to the organism. (sweet taste = not ambiguous. Vision and audition = very ambiguous)\n\nNext you might ask, where do the senses come in? Their location in the network is a function of two things:\n- The less Valence Ambiguous any individual sensory signal/impulse is, the closer you plug it to the VC.\n- The more useful (to gene survival) any individual sensory signal is, the closer you plug it to the positive end of the valence core (sweet berries)\n- The more harmful (to gene survival) any individual sensory signal is, the closer you plug it to the negative end of the valence core (poison berries)\n\nIn essence, the purpose of the sampling network is to allow the VC to capture a summary of the cortex's current state. And vice-versa. Two parallel computation networks sampling each other.\n\nThe VC learns, via association, the sampled cortex state to the current state of the valence core, which is primarily driven by sensory information from the \"Known Valence Senses\". The Cortex learns, via association, the sampled valence core state to the current state of the cortex, which is primarily driven by sensory information from valence-ambiguous sensory modalities.\n\n\nNow imagine that the cortex is composed of smaller regions (cortical columns-ish)\n- with an inter-column sampling network\n- with either the columns being the thing being sampled by the VC, or a layer interface sampling the columns is the thing being sampled by the VC\n\nLet the neurons and neuron connections have certain properties of growth that are in function of activation.\n\nLet the neurons and neuron connections have certain properties of prune/decay that are in function of time (weak, all over effect)\n\nLet the prune/decay be inversely proportional to their strength (for connections), and recent activation history (for neurons)\n\nLet Activation Circuits (AC) by what cognitive psychology calls Neural Circuits\n\nLet Activation Pattern (AP) be the number of neuron activations over time in an AC.\n\n\nLet there be 2 types of degradation that ACs can suffer from:\n- Degradation By Pruning (DBP)\n- Degradation by Growth Interference (DBGI) (of other neural circuit growth)\n\nLet Vastness by a characteristic of a circuits driven by the number of its inputs\nLet Frailness be a characteristic of a circuits by a function of its growth (activation usage)\n\nLets infer that Vastness is affected strongly by DBP\nLets infer that Frailness is affected strongly by DBGI\n\nLet ACs have a set of inputs (other ACs), a set of outputs (other ACs, motor neurons, etc.) and an arbitrary function\n\nLet ACs be created by an unprecedented set of inputs. At the time of inception, the scope of the AC is exactly that of the AP (maximum frailness).\n\nLet learning be the optimization of ACs by Growth and Pruning. This translates roughly to learning = Growth - DBP - DBGI\n\nTo be precise it is the conjunction of a set of ACs optimizing internal, and optimizing along their input/output interface\n\n\n\n\nOk here is where it gets more abstract/esoteric. Its how i chose to view it, but its probably not for everyone.\n-----------------\n\nLet Activation Density (AD) be a measure activation density of an AP in its AC. (AD = AP/AC) (aka consciousness)\n\nLet Sufficient Activation Density (SAD) be the range of AD values that an organism is capable of generating via its neural structure (neurons, receptors, etc.) in its likely environment. This is a range, not a value. This is why the consciousness level of a thought or a person is an off/off single vale threshold.\n\nLet consciousness be a thought (AC) be measured like AD = SAD - AP\n\nLet organisms/species have a certain SAD. Now try to not speculate about the perception of smaller and smaller bodily functions as we look at organisms smaller and smaller (dogs? mice? insects?)\n\n\nLet sleep/unconsciousness be the period of time where, by virtue of the absence of much of the sensory stimuli, the network is running a slightly different set of ACs. By virtue of sleep being a much less coherent environment to observe and learn from (than the real physical world), the ACs would be smaller and frailer. Coherence leads to stability, stability leads to repetition, and repetition is growth. I\n\n\nLet dreams be near SAD ACs created during sleep. By virtue of some (most?) wakefulness ACs being shared with sleep ACs, dreams can reach near SAD.\n\n\nSome examples of where to plug in the human senses in my theory in relation to the valence value spectrum:\n-----------------\n```\n- Located more towards the positive:\n  - sensory inputs from the smell/taste \"endpoints\" that respond to things that are \"good\" for us to eat. (yummy)\n  - sensory inputs from the sexual organs\n  - sensory inputs from hardwired cortex pathways of positive valence cue (smiles, some speech rhythms, sound amplitude, and so on)\n  - perhaps some sensory inputs from \"having a full belly\"\n- Located more towards the negative:\n  - sensory inputs from \"no food inside my digestive system\" category\n  - sensory inputs from the smell/taste \"endpoints\" that respond to things that are \"bad\" for us to eat. (gross)\n  - sensory inputs from pain receptors\n  - sensory inputs from hardwired cortex pathways of negative valence cue (frowns, shouts, and so on)\n  - sensory inputs from the hardwired motor functions associated muscle cost/effort \n```\n\nAnd of course the body would have a ton of systems to regulate i know nothing about, but i REALLY don't see how it cant just happen on this valence cores, OR on other, nearby, similar but maybe smaller valence cores.\n\n\nAbout intelligent robots.\n-----------------\n\nLet us build an artificial network, not with multiple Valence Cores like in the brain, not with thousands of sensory and motor systems. But with simple vision, simple audition, and various synthetic senses that give the limb movement and speech synthesis some cost or effort.\n\nLet us build a robot that has senses that can observe its outputs (motor or else)\n\nLet us build an intelligent artificial network such that it does not live in an environment that can end its life over the decision it makes or the decisions of its competitors. (or the weather)\n\nLet us make our own meaning of Valence for the intelligent networks. One that is defined by a progression of meaning over the development age of the AI\n- Start with giving valence to proper body regulation (learning the basics about our bodies)\n- Then give valence to understand the basics of the environment (the concept of objects, object permanence, textures, sounds, heat/cold, etc.)\n- Then give valence to looking at human faces and to a lesser degree their environment\n- Then give valence to body movement (smooth, efficient, safe, etc.)\n- Then give valence to exploring the environment by moving through it\n- Then give valence to exploring abstract concepts/environment \n\nLet the valence be given to the robot via artificial stimulation of a region of the valence core who's location is based on our meaning of valence, and at a stimulation level/area proportional to the importance of the lesson perhaps.\n\nLet us reproduce the period of helplessness in human, as smaller robotic bodies that are much more robust to na\u00efve experimentation.  Let the human trainers enforce movement on the young AI so that its network does not settle on an unproductive equilibriums.\n\n\nVery much how we would teach a child.\n\nLet us then think about the morality of it all. It is not clear how this network would think about its own mortality, without an environment that can kill it. Except with a precedent of humans de-activating AGIs. Perhaps our own survival will one day depend on already having robot rights laws in place when AGI happens.\n\nThank you.", "link": "https://www.reddit.com/r/MachineLearning/comments/omx5k3/d_my_take_on_conscious_neural_networks_and_how_we/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "prune", "selectorShort": "prune", "MarkedSent": "[d] my take on conscious neural networks and how we get past backpropagation and gradient descent /!/ first, thanks for the feedback. that's as short as i can make it. a valence system and better model of neurons is how we get past backpropagation and gradient descent (that's about as much as i know about current anns)\n\n\nlet positive valence mean beneficial to the organism's genes odds of survival. let negative valence be the opposite.\n\n\nstart with 2 network regions\n\nmake one 100x bigger and call it cortex. this size difference is crucial.\n\ncall the small region the valence core (vc)\n\nlet one end of a vc be positive and the other negative (in a semantic way, not electrical)\n\nlet there be a \"sparse sampling network\" between the cortex and valence core.\n\nlet there be a \"sparse sampling network\" between the cortex and valence core.\n- imagine if we broke up all of the cortex and vc into an equal number of tiny cubes. now let each tiny cube in the vc have a set of connections to a fairly random and uniform set of cortex tiny cubes.\n- now imagine the connections actual endpoint in the tiny cubes to be at any random point in that tiny cube.\n- now imagine that these connection have a (synaptic) strength greater than that of the average connection\n- now imagine that these connections are in essence bi-directional (perhaps 2 mirrored sampling networks)\n\nlet valence ambiguity be how much you know if any individual sensory signal/impulse is beneficial or detrimental to the organism. (sweet taste = not ambiguous. vision and audition = very ambiguous)\n\nnext you might ask, where do the senses come in? their location in the network is a function of two things:\n- the less valence ambiguous any individual sensory signal/impulse is, the closer you plug it to the vc.\n- the more useful (to gene survival) any individual sensory signal is, the closer you plug it to the positive end of the valence core (sweet berries)\n- the more harmful (to gene survival) any individual sensory signal is, the closer you plug it to the negative end of the valence core (poison berries)\n\nin essence, the purpose of the sampling network is to allow the vc to capture a summary of the cortex's current state. and vice-versa. two parallel computation networks sampling each other.\n\nthe vc learns, via association, the sampled cortex state to the current state of the valence core, which is primarily driven by sensory information from the \"known valence senses\". the cortex learns, via association, the sampled valence core state to the current state of the cortex, which is primarily driven by sensory information from valence-ambiguous sensory modalities.\n\n\nnow imagine that the cortex is composed of smaller regions (cortical columns-ish)\n- with an inter-column sampling network\n- with either the columns being the thing being sampled by the vc, or a layer interface sampling the columns is the thing being sampled by the vc\n\nlet the neurons and neuron connections have certain properties of growth that are in function of activation.\n\nlet the neurons and neuron connections have certain properties of -----> prune !!! /decay that are in function of time (weak, all over effect)\n\nlet the -----> prune !!! /decay be inversely proportional to their strength (for connections), and recent activation history (for neurons)\n\nlet activation circuits (ac) by what cognitive psychology calls neural circuits\n\nlet activation pattern (ap) be the number of neuron activations over time in an ac.\n\n\nlet there be 2 types of degradation that acs can suffer from:\n- degradation by pruning (dbp)\n- degradation by growth interference (dbgi) (of other neural circuit growth)\n\nlet vastness by a characteristic of a circuits driven by the number of its inputs\nlet frailness be a characteristic of a circuits by a function of its growth (activation usage)\n\nlets infer that vastness is affected strongly by dbp\nlets infer that frailness is affected strongly by dbgi\n\nlet acs have a set of inputs (other acs), a set of outputs (other acs, motor neurons, etc.) and an arbitrary function\n\nlet acs be created by an unprecedented set of inputs. at the time of inception, the scope of the ac is exactly that of the ap (maximum frailness).\n\nlet learning be the optimization of acs by growth and pruning. this translates roughly to learning = growth - dbp - dbgi\n\nto be precise it is the conjunction of a set of acs optimizing internal, and optimizing along their input/output interface\n\n\n\n\nok here is where it gets more abstract/esoteric. its how i chose to view it, but its probably not for everyone.\n-----------------\n\nlet activation density (ad) be a measure activation density of an ap in its ac. (ad = ap/ac) (aka consciousness)\n\nlet sufficient activation density (sad) be the range of ad values that an organism is capable of generating via its neural structure (neurons, receptors, etc.) in its likely environment. this is a range, not a value. this is why the consciousness level of a thought or a person is an off/off single vale threshold.\n\nlet consciousness be a thought (ac) be measured like ad = sad - ap\n\nlet organisms/species have a certain sad. now try to not speculate about the perception of smaller and smaller bodily functions as we look at organisms smaller and smaller (dogs? mice? insects?)\n\n\nlet sleep/unconsciousness be the period of time where, by virtue of the absence of much of the sensory stimuli, the network is running a slightly different set of acs. by virtue of sleep being a much less coherent environment to observe and learn from (than the real physical world), the acs would be smaller and frailer. coherence leads to stability, stability leads to repetition, and repetition is growth. i\n\n\nlet dreams be near sad acs created during sleep. by virtue of some (most?) wakefulness acs being shared with sleep acs, dreams can reach near sad.\n\n\nsome examples of where to plug in the human senses in my theory in relation to the valence value spectrum:\n-----------------\n```\n- located more towards the positive:\n  - sensory inputs from the smell/taste \"endpoints\" that respond to things that are \"good\" for us to eat. (yummy)\n  - sensory inputs from the sexual organs\n  - sensory inputs from hardwired cortex pathways of positive valence cue (smiles, some speech rhythms, sound amplitude, and so on)\n  - perhaps some sensory inputs from \"having a full belly\"\n- located more towards the negative:\n  - sensory inputs from \"no food inside my digestive system\" category\n  - sensory inputs from the smell/taste \"endpoints\" that respond to things that are \"bad\" for us to eat. (gross)\n  - sensory inputs from pain receptors\n  - sensory inputs from hardwired cortex pathways of negative valence cue (frowns, shouts, and so on)\n  - sensory inputs from the hardwired motor functions associated muscle cost/effort \n```\n\nand of course the body would have a ton of systems to regulate i know nothing about, but i really don't see how it cant just happen on this valence cores, or on other, nearby, similar but maybe smaller valence cores.\n\n\nabout intelligent robots.\n-----------------\n\nlet us build an artificial network, not with multiple valence cores like in the brain, not with thousands of sensory and motor systems. but with simple vision, simple audition, and various synthetic senses that give the limb movement and speech synthesis some cost or effort.\n\nlet us build a robot that has senses that can observe its outputs (motor or else)\n\nlet us build an intelligent artificial network such that it does not live in an environment that can end its life over the decision it makes or the decisions of its competitors. (or the weather)\n\nlet us make our own meaning of valence for the intelligent networks. one that is defined by a progression of meaning over the development age of the ai\n- start with giving valence to proper body regulation (learning the basics about our bodies)\n- then give valence to understand the basics of the environment (the concept of objects, object permanence, textures, sounds, heat/cold, etc.)\n- then give valence to looking at human faces and to a lesser degree their environment\n- then give valence to body movement (smooth, efficient, safe, etc.)\n- then give valence to exploring the environment by moving through it\n- then give valence to exploring abstract concepts/environment \n\nlet the valence be given to the robot via artificial stimulation of a region of the valence core who's location is based on our meaning of valence, and at a stimulation level/area proportional to the importance of the lesson perhaps.\n\nlet us reproduce the period of helplessness in human, as smaller robotic bodies that are much more robust to na\u00efve experimentation.  let the human trainers enforce movement on the young ai so that its network does not settle on an unproductive equilibriums.\n\n\nvery much how we would teach a child.\n\nlet us then think about the morality of it all. it is not clear how this network would think about its own mortality, without an environment that can kill it. except with a precedent of humans de-activating agis. perhaps our own survival will one day depend on already having robot rights laws in place when agi happens.\n\nthank you.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/omx5k3/d_my_take_on_conscious_neural_networks_and_how_we/',)", "identifyer": 5738522, "year": "2021"}, {"autor": "latticeprep", "date": 1617339115000, "content": "[D] How does stripe use GBT's to find edge similarity? /!/ I'm reading Stripe's [blog post on how it finds similar accounts to flag fraudulent activity](https://stripe.com/blog/similarity-clustering) and it just doesn't make any sense to me. I'm wondering if anyone has any idea on how GBT's can be used on adjacency lists like this.\n\nFrom the post:\n\n&gt;Over the years, our risk underwriting teams have manually compiled many examples of existing clusters of fraudulent accounts through our investigations of fraud rings, and we can use these reference clusters as training data to learn our similarity function. By sampling edges from these groups, we obtain a dataset consisting of pairs of accounts along with a label for each pair indicating whether or not the two accounts belong to the same cluster. We use intra-cluster edges as positive training examples and inter-cluster edges as negative training examples, where an edge denotes a pair of accounts.  \n&gt;  \n&gt;Because of the wide variety of features we can construct from given pairs of accounts, we decided to use\u00a0[gradient-boosted decision trees](https://en.wikipedia.org/wiki/Gradient_boosting)\u00a0(GBDTs) to represent our similarity model. In practice, we\u2019ve found GBDTs strike the right balance between being easy to train, having strong predictive power and being robust despite variations in the data. When we started this project we wanted to get something out the door quickly that was effective, had well-understood properties, and was straightforward to fine-tune. The variant that we use,\u00a0[XGBoost](https://xgboost.readthedocs.io/en/latest/), is one of the best performing off-the-shelf models for cases with structured (also known as tabular) data, and we have well-developed infrastructure to train and serve them. You can read more about the infrastructure we use to\u00a0[train machine learning models](https://stripe.com/en-ca/blog/railyard-training-models)\u00a0at Stripe in a previous post.  \n&gt;  \n&gt;Now that we have a trained model, we can use it to predict fraudulent activity. Since this model operates on pairs of Stripe accounts, it\u2019s not feasible to feed it all possible pairs of accounts and compute scores across all pairs. Instead, we first generate a candidate set of edges to be scored. We do this by taking recently created Stripe accounts and creating edges between accounts that share certain attributes. Although this isn\u2019t an exhaustive approach, this heuristic works well in practice to prune the set of candidate edges to a reasonable number.  \n&gt;  \n&gt;Once the candidate edges are scored, we then filter edges by selecting those with a similarity score above some threshold. We then compute the connected components on the resulting graph. The final output is a set of high-fidelity account clusters which we can analyze, process, or manually inspect together as a unit. In particular, a fraud analyst may want to examine clusters which contain known fraudulent accounts and investigate the remaining accounts in that cluster.  \n&gt;  \n&gt;This is an iterative process; as each individual cluster grows, we can quickly identify increasing similarity as fake accounts in a fraudster\u2019s operation are created. And the more fraud rings we detect and shutdown at Stripe, the more accurate our clustering model becomes at identifying new clusters in the future.\n\nI found this baffling. I didn't realize GBTs could be used to effectively find cliques in a graph. Has anyone tried this or understand how it works better? I don't really understand how these edge candidates are scored against past known-fraudulent edges. I just don't have a sense of the feature space for this at all.", "link": "https://www.reddit.com/r/MachineLearning/comments/mid7m6/d_how_does_stripe_use_gbts_to_find_edge_similarity/", "origin": "Reddit", "suborigin": "MachineLearning", "result": true, "Selector": "prune", "selectorShort": "prune", "MarkedSent": "[d] how does stripe use gbt's to find edge similarity? /!/ i'm reading stripe's [blog post on how it finds similar accounts to flag fraudulent activity](https://stripe.com/blog/similarity-clustering) and it just doesn't make any sense to me. i'm wondering if anyone has any idea on how gbt's can be used on adjacency lists like this.\n\nfrom the post:\n\n&gt;over the years, our risk underwriting teams have manually compiled many examples of existing clusters of fraudulent accounts through our investigations of fraud rings, and we can use these reference clusters as training data to learn our similarity function. by sampling edges from these groups, we obtain a dataset consisting of pairs of accounts along with a label for each pair indicating whether or not the two accounts belong to the same cluster. we use intra-cluster edges as positive training examples and inter-cluster edges as negative training examples, where an edge denotes a pair of accounts.  \n&gt;  \n&gt;because of the wide variety of features we can construct from given pairs of accounts, we decided to use\u00a0[gradient-boosted decision trees](https://en.wikipedia.org/wiki/gradient_boosting)\u00a0(gbdts) to represent our similarity model. in practice, we\u2019ve found gbdts strike the right balance between being easy to train, having strong predictive power and being robust despite variations in the data. when we started this project we wanted to get something out the door quickly that was effective, had well-understood properties, and was straightforward to fine-tune. the variant that we use,\u00a0[xgboost](https://xgboost.readthedocs.io/en/latest/), is one of the best performing off-the-shelf models for cases with structured (also known as tabular) data, and we have well-developed infrastructure to train and serve them. you can read more about the infrastructure we use to\u00a0[train machine learning models](https://stripe.com/en-ca/blog/railyard-training-models)\u00a0at stripe in a previous post.  \n&gt;  \n&gt;now that we have a trained model, we can use it to predict fraudulent activity. since this model operates on pairs of stripe accounts, it\u2019s not feasible to feed it all possible pairs of accounts and compute scores across all pairs. instead, we first generate a candidate set of edges to be scored. we do this by taking recently created stripe accounts and creating edges between accounts that share certain attributes. although this isn\u2019t an exhaustive approach, this heuristic works well in practice to -----> prune !!!  the set of candidate edges to a reasonable number.  \n&gt;  \n&gt;once the candidate edges are scored, we then filter edges by selecting those with a similarity score above some threshold. we then compute the connected components on the resulting graph. the final output is a set of high-fidelity account clusters which we can analyze, process, or manually inspect together as a unit. in particular, a fraud analyst may want to examine clusters which contain known fraudulent accounts and investigate the remaining accounts in that cluster.  \n&gt;  \n&gt;this is an iterative process; as each individual cluster grows, we can quickly identify increasing similarity as fake accounts in a fraudster\u2019s operation are created. and the more fraud rings we detect and shutdown at stripe, the more accurate our clustering model becomes at identifying new clusters in the future.\n\ni found this baffling. i didn't realize gbts could be used to effectively find cliques in a graph. has anyone tried this or understand how it works better? i don't really understand how these edge candidates are scored against past known-fraudulent edges. i just don't have a sense of the feature space for this at all.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 11, "media": "('self',)", "medialink": "('https://www.reddit.com/r/MachineLearning/comments/mid7m6/d_how_does_stripe_use_gbts_to_find_edge_similarity/',)", "identifyer": 5740605, "year": "2021"}], "name": "pruneMachineLearning2021"}
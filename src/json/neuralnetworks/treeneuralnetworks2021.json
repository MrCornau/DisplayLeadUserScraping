{"interestingcomments": [{"autor": "wild-eagle", "date": 1611531871000, "content": "Tree \u2013 a lib for working with nested data structures, open-sourced by deepmind", "link": "https://www.reddit.com/r/neuralnetworks/comments/l4at3p/tree_a_lib_for_working_with_nested_data/", "origin": "Reddit", "suborigin": "neuralnetworks", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "-----> tree !!!  \u2013 a lib for working with nested data structures, open-sourced by deepmind", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('link',)", "medialink": "('https://github.com/deepmind/tree',)", "identifyer": 5754593, "year": "2021"}, {"autor": "grid_world", "date": 1619545297000, "content": "Tree bark cross-section - CNN /!/ I have a use case where I am supposed to use different tree bark cross section images like this:\n\n&amp;#x200B;\n\n[example image](https://preview.redd.it/3u8jg4z36rv61.jpg?width=487&amp;format=pjpg&amp;auto=webp&amp;s=0a503e70e943f88d2e31e7e14a4ad0121b22f8c9)\n\nThe goal is to have a CNN model to recognize the tree bark and then predict the tree from to which it belongs to? Do you have any such dataset/similar problem into which I can look into?\n\n&amp;#x200B;\n\nThanks!", "link": "https://www.reddit.com/r/neuralnetworks/comments/mzu246/tree_bark_crosssection_cnn/", "origin": "Reddit", "suborigin": "neuralnetworks", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "-----> tree !!!  bark cross-section - cnn /!/ i have a use case where i am supposed to use different tree bark cross section images like this:\n\n&amp;#x200b;\n\n[example image](https://preview.redd.it/3u8jg4z36rv61.jpg?width=487&amp;format=pjpg&amp;auto=webp&amp;s=0a503e70e943f88d2e31e7e14a4ad0121b22f8c9)\n\nthe goal is to have a cnn model to recognize the tree bark and then predict the tree from to which it belongs to? do you have any such dataset/similar problem into which i can look into?\n\n&amp;#x200b;\n\nthanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/neuralnetworks/comments/mzu246/tree_bark_crosssection_cnn/',)", "identifyer": 5754766, "year": "2021"}, {"autor": "fishdontwearhats", "date": 1619180353000, "content": "I think it's broken... Can anyone help me? /!/ Dear reddit... I've coded very strange neural network in raw python for a final year project. Here's the puzzle: When I train it on a simple data set I made using cross entropy as my cost function the cross entropy goes down, the root mean squared error goes up and the accuracy stays the same.\n\nIf that's not confusing enough, I can also do the same thing but using the RMSE as my cost function and now both RMSE and cross entropy go UP and accuracy STILL stays the same.\n\nAnyone have any ideas about how to interpret this or what to try? If you'd like to see any of my code drop a comment and I can screen shot...\n\nTLDR: my creation appears to be learning but is somehow getting dumber. The apple doesn't fall far from the tree...", "link": "https://www.reddit.com/r/neuralnetworks/comments/mwtyz2/i_think_its_broken_can_anyone_help_me/", "origin": "Reddit", "suborigin": "neuralnetworks", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "i think it's broken... can anyone help me? /!/ dear reddit... i've coded very strange neural network in raw python for a final year project. here's the puzzle: when i train it on a simple data set i made using cross entropy as my cost function the cross entropy goes down, the root mean squared error goes up and the accuracy stays the same.\n\nif that's not confusing enough, i can also do the same thing but using the rmse as my cost function and now both rmse and cross entropy go up and accuracy still stays the same.\n\nanyone have any ideas about how to interpret this or what to try? if you'd like to see any of my code drop a comment and i can screen shot...\n\ntldr: my creation appears to be learning but is somehow getting dumber. the apple doesn't fall far from the -----> tree !!! ...", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/neuralnetworks/comments/mwtyz2/i_think_its_broken_can_anyone_help_me/',)", "identifyer": 5754776, "year": "2021"}, {"autor": "techsucker", "date": 1631948902000, "content": "Google AI Introduces Two New Families of Neural Networks Called \u2018EfficientNetV2\u2019 and \u2018CoAtNet\u2019 For Image Recognition /!/ Training efficiency has become a significant factor for deep learning as the neural network models, and training data size grows. [GPT-3](https://arxiv.org/abs/2005.14165) is an excellent example to show how critical training efficiency factor could be as it takes weeks of training with thousands of GPUs to demonstrate remarkable capabilities in few-shot learning.\n\nTo address this problem, the Google AI team introduce two families of neural networks for image recognition. First is\u00a0[EfficientNetV2](https://arxiv.org/abs/2104.00298), consisting of CNN (Convolutional neural networks) with a small-scale dataset for faster training efficiency such as\u00a0[ImageNet1k](https://www.image-net.org/)\u00a0(with 1.28 million images). Second is a hybrid model called\u00a0[CoAtNet](https://arxiv.org/abs/2106.04803), which combines\u00a0[convolution](https://en.wikipedia.org/wiki/Convolution)\u00a0and\u00a0[self-attention](https://en.wikipedia.org/wiki/Self-attention)\u00a0to achieve higher accuracy on large-scale datasets such as\u00a0[ImageNet21](https://www.image-net.org/)\u00a0(with 13 million images) and\u00a0[JFT](https://ai.googleblog.com/2017/07/revisiting-unreasonable-effectiveness.html)\u00a0(with billions of images). As per the research report by Google,\u00a0[EfficientNetV2](https://arxiv.org/abs/2104.00298)\u00a0and\u00a0[CoAtNet](https://arxiv.org/abs/2106.04803)\u00a0both are 4 to 10 times faster while achieving state-of-the-art and 90.88% top-1 accuracy on the well-established\u00a0[ImageNet](https://www.image-net.org/)\u00a0dataset.\n\n# [7 Min Read](https://www.marktechpost.com/2021/09/17/google-ai-introduces-two-new-families-of-neural-networks-called-efficientnetv2-and-coatnet-for-image-recognition/) | [Paper (CoAtNet)](https://arxiv.org/abs/2106.04803) | [Paper (EfficientNetV2)](https://arxiv.org/abs/2104.00298) | [Google blog](https://ai.googleblog.com/2021/09/toward-fast-and-accurate-neural.html) | [Code](https://github.com/google/automl/tree/master/efficientnetv2)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/7j7257jao7o71.png?width=1392&amp;format=png&amp;auto=webp&amp;s=010fac3098f222e24ed7893e3e1a905b745020c4", "link": "https://www.reddit.com/r/neuralnetworks/comments/pqhqc9/google_ai_introduces_two_new_families_of_neural/", "origin": "Reddit", "suborigin": "neuralnetworks", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "google ai introduces two new families of neural networks called \u2018efficientnetv2\u2019 and \u2018coatnet\u2019 for image recognition /!/ training efficiency has become a significant factor for deep learning as the neural network models, and training data size grows. [gpt-3](https://arxiv.org/abs/2005.14165) is an excellent example to show how critical training efficiency factor could be as it takes weeks of training with thousands of gpus to demonstrate remarkable capabilities in few-shot learning.\n\nto address this problem, the google ai team introduce two families of neural networks for image recognition. first is\u00a0[efficientnetv2](https://arxiv.org/abs/2104.00298), consisting of cnn (convolutional neural networks) with a small-scale dataset for faster training efficiency such as\u00a0[imagenet1k](https://www.image-net.org/)\u00a0(with 1.28 million images). second is a hybrid model called\u00a0[coatnet](https://arxiv.org/abs/2106.04803), which combines\u00a0[convolution](https://en.wikipedia.org/wiki/convolution)\u00a0and\u00a0[self-attention](https://en.wikipedia.org/wiki/self-attention)\u00a0to achieve higher accuracy on large-scale datasets such as\u00a0[imagenet21](https://www.image-net.org/)\u00a0(with 13 million images) and\u00a0[jft](https://ai.googleblog.com/2017/07/revisiting-unreasonable-effectiveness.html)\u00a0(with billions of images). as per the research report by google,\u00a0[efficientnetv2](https://arxiv.org/abs/2104.00298)\u00a0and\u00a0[coatnet](https://arxiv.org/abs/2106.04803)\u00a0both are 4 to 10 times faster while achieving state-of-the-art and 90.88% top-1 accuracy on the well-established\u00a0[imagenet](https://www.image-net.org/)\u00a0dataset.\n\n# [7 min read](https://www.marktechpost.com/2021/09/17/google-ai-introduces-two-new-families-of-neural-networks-called-efficientnetv2-and-coatnet-for-image-recognition/) | [paper (coatnet)](https://arxiv.org/abs/2106.04803) | [paper (efficientnetv2)](https://arxiv.org/abs/2104.00298) | [google blog](https://ai.googleblog.com/2021/09/toward-fast-and-accurate-neural.html) | [code](https://github.com/google/automl/-----> tree !!! /master/efficientnetv2)\n\n&amp;#x200b;\n\nhttps://preview.redd.it/7j7257jao7o71.png?width=1392&amp;format=png&amp;auto=webp&amp;s=010fac3098f222e24ed7893e3e1a905b745020c4", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/neuralnetworks/comments/pqhqc9/google_ai_introduces_two_new_families_of_neural/',)", "identifyer": 5755218, "year": "2021"}], "name": "treeneuralnetworks2021"}
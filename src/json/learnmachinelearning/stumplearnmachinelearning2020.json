{"interestingcomments": [{"autor": "ALior1", "date": 1597516617000, "content": "AdaBoost Algorithm using Logistic Regression as Classifier feasibility /!/ Hi,\n\nI implemented AdaBoost with a simple stump from sklearn.tree.DecisionTreeClassifier .\n\n&amp;#x200B;\n\nCan I replace DecisionTreeClassifier  with Logistic Regression ?\n\n&amp;#x200B;\n\nI'm trying to achieve it for a few days now, and would appreciate any help.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/iacs06/adaboost_algorithm_using_logistic_regression_as/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "stump", "selectorShort": "stump", "MarkedSent": "adaboost algorithm using logistic regression as classifier feasibility /!/ hi,\n\ni implemented adaboost with a simple -----> stump !!!  from sklearn.tree.decisiontreeclassifier .\n\n&amp;#x200b;\n\ncan i replace decisiontreeclassifier  with logistic regression ?\n\n&amp;#x200b;\n\ni'm trying to achieve it for a few days now, and would appreciate any help.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/iacs06/adaboost_algorithm_using_logistic_regression_as/',)", "identifyer": 5749842, "year": "2020"}, {"autor": "iamrealadvait", "date": 1589083921000, "content": "What is 'Ensemble Learning Methods' and How to do it? /!/ &amp;#x200B;\n\n**MACHINE LEARNING : Ensemble Learning Methods**\n\nCombine all \u201cweak\u201d learner to form ensemble.\n\nAveraging : Equal weights are assigned to different model.\n\n&amp;#x200B;\n\nBagging (reduces variance) :\n\nBagging or bootstrap aggregation \u2018reduces variance\u2019 of an estimate by taking mean of multiple estimates.\n\nSteps :\n\n1. Create randomly sampled datasets of the original training data.\n2. 2. Build and fit several classifiers to each of these diverse copies.\n3. 3. Take the average of all the predictions to make final overall prediction.\n\nBoosting (reduces bias) :\n\nBoosting \u2018reduces bias\u2019 by training weak learner sequentially,each trying to correct its predecessor.\n\nSteps:\n\n1. Train a Classfier H1 that best classifies the data with respect to accuracy.\n2. 2. Identify the region where H1 produces errors,add weights to it and produce a H2 classifier.\n3. 3. Exaggerate those samples for which H1 gives a different result from H2 and produces H3 classifier. Repeat step 02 for a new classifier.\n\nAdaboost : Consider a scenario, where there are \u2018+\u2019 and \u2018-\u2018\n\nAdaboost Working : Step 1\n\n1. Assign equal weights to each data point.\n2. 2. Apply a decision stump to classify them as +(plus) and -(minus)\n3. 3. Decision stump (D1) has generated vertical plane at left side to classify.\n4. 4. Apply higher weights to incorrectly predicted three +(plus) and add another decision stump.\n\nAdaboost Working : Step 2\n\n1. Size of incorrectly predicted +(plus) is made bigger as compared to rest of the data points.\n2. 2. The second decision stump (D2) will try to predict them correctly.\n3. 3. Now, vertical plane(D2) has classified three mis-classified +(plus) correctly.\n4. 4. D2 has also caused mis-classification errors to three -(minus)\n\nAdaboost Working : Step 3\n\n1. D3 adds higher weights to three \u2013 (minus)\n2. 2. Horizontal line is generated to classify + (plus) and \u2013 (minus) based on higher weight of mis-classified observation.\n\nAdaboost Working : Step 4\n\n1. D1, D2 and D3 are combined to form a strong prediction having complex rule as compared to individual weak learner.\n\nAdaboost Algorithm :\n\nStep 1 : Initially each data point is weighted equally with weight .\n\n. Wi = 1/n\n\n. Where n is the number of samples.\n\nStep 2 : A classifier \u2018H1\u2019 is picked up the best classifies the data with minimal error rate.\n\nStep 3 : The weighing factor \u2018@ (alpha)\u2019 is dependant on errors (e) caused by the H1 classifier.\n\n. @\\^t =1/2 ln 1-e/ e\n\n. @ \u2013 alpha , ln \u2013 log ,\n\n. e \u2013 error , t \u2013 time ,\n\n. \\^ \u2013 to power\n\nStep 4 : Weight after time \u2018t\u2019 is given as :\n\n. Wi\\^t+1/z e\\^-at.h1(x).y(x)\n\n.\n\n. Z \u2013 Normalizing factor\n\n. h1(x).y(x) \u2013 sign of the current output\n\nGradient Boosting (GBM) : Gradient boosting involves three elements.\n\n1. A loss function to be optimised.\n2. 2. A weak learner to make predictions.\n3. 3. An additive model to add weak learners to minimize the loss function.\n\nGBM Mechanism :\n\n1. GBM predicts the residuals or error of prior models and then sums them to make the final prediction.\n2. 2. One weak learner is added at a time and existing weak learners in the model are left unchanged.\n3. 3. GBM repetitively leverages the patterns in residuals and strengthens. a model with weak predictions.\n4. 4. Modeling is stopped when residuals do not have any pattern that can be modeled.\n\nAdaboost Working : Step 1\n\n1. Assign equal weights to each data point.\n2. 2. Apply a decision stump to classify them as +(plus) and -(minus)\n3. 3. Decision stump (D1) has generated vertical plane at left side to classify.\n4. 4. Apply higher weights to incorrectly predicted three +(plus) and add another decision stump.\n\nAdaboost Working : Step 2\n\n1. Size of incorrectly predicted +(plus) is made bigger as compared to rest of the data points.\n2. 2. The second decision stump (D2) will try to predict them correctly.\n3. 3. Now, vertical plane(D2) has classified three mis-classified +(plus) correctly.\n4. 4. D2 has also caused mis-classification errors to three -(minus)\n\nAdaboost Working : Step 3\n\n1. D3 adds higher weights to three \u2013 (minus)\n2. 2. Horizontal line is generated to classify + (plus) and \u2013 (minus) based on higher weight of mis-classified observation.\n\nAdaboost Working : Step 4\n\n1. D1, D2 and D3 are combined to form a strong prediction having complex rule as compared to individual weak learner.\n\nAdaboost Algorithm :\n\nStep 1 : Initially each data point is weighted equally with weight .\n\n. Wi = 1/n\n\n. Where n is the number of samples.\n\nStep 2 : A classifier \u2018H1\u2019 is picked up the best classifies the data with minimal error rate.\n\nStep 3 : The weighing factor \u2018@ (alpha)\u2019 is dependant on errors (e) caused by the H1 classifier.\n\n. @\\^t =1/2 ln 1-e/ e\n\n. @ \u2013 alpha , ln \u2013 log ,\n\n. e \u2013 error , t \u2013 time ,\n\n. \\^ \u2013 to power\n\nStep 4 : Weight after time \u2018t\u2019 is given as :\n\n. Wi\\^t+1/z e\\^-at.h1(x).y(x)\n\n.\n\n. Z \u2013 Normalizing factor\n\n. h1(x).y(x) \u2013 sign of the current output\n\nGradient Boosting (GBM) : Gradient boosting involves three elements.\n\n1. A loss function to be optimised.\n2. 2. A weak learner to make predictions.\n3. 3. An additive model to add weak learners to minimize the loss function.\n\nGBM Mechanism :\n\n1. GBM predicts the residuals or error of prior models and then sums them to make the final prediction.\n2. 2. One weak learner is added at a time and existing weak learners in the model are left unchanged.\n3. 3. GBM repetitively leverages the patterns in residuals and strengthens. a model with weak predictions.\n4. 4. Modelling is stopped when residuals do not have any pattern that can be modelled.\n\n&amp;#x200B;\n\nGBM Algorithm Steps :\n\n1. Fit a simple regression or classification model.\n2. 2. Calculate error residuals (actual value \u2013 predicted value)\n3. 3. Fit a new model on error residuals as targets variable with same input variables.\n4. 4. Add the predicted residuals to the previous predictions.\n5. 5. Fit another model on residuals that are remaining and repeat steps 2 and 5 until the model is overfitting or the sum of residuals becomes constant.\n\nXGBoost: eXtreme Gradient Boosting is a library for developing fast and high -performance gradient boosting tree models.XGBoost is extensively used in ML competitions as it is almost 10 times faster than other gradient boosting techniques.\n\nXGBoost Parameters :\n\n1. General Parameters: Number of threads.\n2. 2. Booster Parameters: a. Step size. b. Regularisation\n3. 3. Task Parameters: a. Objective. b. Evaluation metric.\n\nGeneral Parameters :\n\nnthread :\n\n1. Number of parallel threads.\n2. 2. If no value is entered,algorithm automatically detects the number of cores and runs on all the cores.\n\nbooster :\n\n1. gbtree : tree-based model\n2.  gblinear : linear function\n\nSilent \\[default =0\\] :\n\n1. If set to 1, no running messages will be printed.Hence,keep it \u20180\u2019 as the messages might help in understanding the model.\n\nBooster Parameters : Booster parameters guide individual booster (Tree/Regression) at each.\n\nParameters for tree booster :\n\neta : Step size shrinkage is used in update to prevent overfitting. Range in \\[0,1\\], default 0.3\n\ngamma : Minimum loss reduction required to make a split. Range \\[0,infinite\\],default 0\n\nmax\\_depth : Maximum depth of the tree. Range \\[1,infinite\\],default 6\n\nmin\\_child\\_weight : minimum sum of instance weight needed in a child. Range \\[0,Range\\] , default 1\n\n**For Videos and More:** [www.facebook.com/seevecoding](https://www.facebook.com/seevecoding)\n\n **Blog Source:** \n\n[https://medium.com/@seeve/machine-learning-ensemble-learning-methods-4aab47158b41](https://medium.com/@seeve/machine-learning-ensemble-learning-methods-4aab47158b41)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ggum9d/what_is_ensemble_learning_methods_and_how_to_do_it/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "stump", "selectorShort": "stump", "MarkedSent": "what is 'ensemble learning methods' and how to do it? /!/ &amp;#x200b;\n\n**machine learning : ensemble learning methods**\n\ncombine all \u201cweak\u201d learner to form ensemble.\n\naveraging : equal weights are assigned to different model.\n\n&amp;#x200b;\n\nbagging (reduces variance) :\n\nbagging or bootstrap aggregation \u2018reduces variance\u2019 of an estimate by taking mean of multiple estimates.\n\nsteps :\n\n1. create randomly sampled datasets of the original training data.\n2. 2. build and fit several classifiers to each of these diverse copies.\n3. 3. take the average of all the predictions to make final overall prediction.\n\nboosting (reduces bias) :\n\nboosting \u2018reduces bias\u2019 by training weak learner sequentially,each trying to correct its predecessor.\n\nsteps:\n\n1. train a classfier h1 that best classifies the data with respect to accuracy.\n2. 2. identify the region where h1 produces errors,add weights to it and produce a h2 classifier.\n3. 3. exaggerate those samples for which h1 gives a different result from h2 and produces h3 classifier. repeat step 02 for a new classifier.\n\nadaboost : consider a scenario, where there are \u2018+\u2019 and \u2018-\u2018\n\nadaboost working : step 1\n\n1. assign equal weights to each data point.\n2. 2. apply a decision -----> stump !!!  to classify them as +(plus) and -(minus)\n3. 3. decision stump (d1) has generated vertical plane at left side to classify.\n4. 4. apply higher weights to incorrectly predicted three +(plus) and add another decision stump.\n\nadaboost working : step 2\n\n1. size of incorrectly predicted +(plus) is made bigger as compared to rest of the data points.\n2. 2. the second decision stump (d2) will try to predict them correctly.\n3. 3. now, vertical plane(d2) has classified three mis-classified +(plus) correctly.\n4. 4. d2 has also caused mis-classification errors to three -(minus)\n\nadaboost working : step 3\n\n1. d3 adds higher weights to three \u2013 (minus)\n2. 2. horizontal line is generated to classify + (plus) and \u2013 (minus) based on higher weight of mis-classified observation.\n\nadaboost working : step 4\n\n1. d1, d2 and d3 are combined to form a strong prediction having complex rule as compared to individual weak learner.\n\nadaboost algorithm :\n\nstep 1 : initially each data point is weighted equally with weight .\n\n. wi = 1/n\n\n. where n is the number of samples.\n\nstep 2 : a classifier \u2018h1\u2019 is picked up the best classifies the data with minimal error rate.\n\nstep 3 : the weighing factor \u2018@ (alpha)\u2019 is dependant on errors (e) caused by the h1 classifier.\n\n. @\\^t =1/2 ln 1-e/ e\n\n. @ \u2013 alpha , ln \u2013 log ,\n\n. e \u2013 error , t \u2013 time ,\n\n. \\^ \u2013 to power\n\nstep 4 : weight after time \u2018t\u2019 is given as :\n\n. wi\\^t+1/z e\\^-at.h1(x).y(x)\n\n.\n\n. z \u2013 normalizing factor\n\n. h1(x).y(x) \u2013 sign of the current output\n\ngradient boosting (gbm) : gradient boosting involves three elements.\n\n1. a loss function to be optimised.\n2. 2. a weak learner to make predictions.\n3. 3. an additive model to add weak learners to minimize the loss function.\n\ngbm mechanism :\n\n1. gbm predicts the residuals or error of prior models and then sums them to make the final prediction.\n2. 2. one weak learner is added at a time and existing weak learners in the model are left unchanged.\n3. 3. gbm repetitively leverages the patterns in residuals and strengthens. a model with weak predictions.\n4. 4. modeling is stopped when residuals do not have any pattern that can be modeled.\n\nadaboost working : step 1\n\n1. assign equal weights to each data point.\n2. 2. apply a decision -----> stump !!!  to classify them as +(plus) and -(minus)\n3. 3. decision stump (d1) has generated vertical plane at left side to classify.\n4. 4. apply higher weights to incorrectly predicted three +(plus) and add another decision stump.\n\nadaboost working : step 2\n\n1. size of incorrectly predicted +(plus) is made bigger as compared to rest of the data points.\n2. 2. the second decision stump (d2) will try to predict them correctly.\n3. 3. now, vertical plane(d2) has classified three mis-classified +(plus) correctly.\n4. 4. d2 has also caused mis-classification errors to three -(minus)\n\nadaboost working : step 3\n\n1. d3 adds higher weights to three \u2013 (minus)\n2. 2. horizontal line is generated to classify + (plus) and \u2013 (minus) based on higher weight of mis-classified observation.\n\nadaboost working : step 4\n\n1. d1, d2 and d3 are combined to form a strong prediction having complex rule as compared to individual weak learner.\n\nadaboost algorithm :\n\nstep 1 : initially each data point is weighted equally with weight .\n\n. wi = 1/n\n\n. where n is the number of samples.\n\nstep 2 : a classifier \u2018h1\u2019 is picked up the best classifies the data with minimal error rate.\n\nstep 3 : the weighing factor \u2018@ (alpha)\u2019 is dependant on errors (e) caused by the h1 classifier.\n\n. @\\^t =1/2 ln 1-e/ e\n\n. @ \u2013 alpha , ln \u2013 log ,\n\n. e \u2013 error , t \u2013 time ,\n\n. \\^ \u2013 to power\n\nstep 4 : weight after time \u2018t\u2019 is given as :\n\n. wi\\^t+1/z e\\^-at.h1(x).y(x)\n\n.\n\n. z \u2013 normalizing factor\n\n. h1(x).y(x) \u2013 sign of the current output\n\ngradient boosting (gbm) : gradient boosting involves three elements.\n\n1. a loss function to be optimised.\n2. 2. a weak learner to make predictions.\n3. 3. an additive model to add weak learners to minimize the loss function.\n\ngbm mechanism :\n\n1. gbm predicts the residuals or error of prior models and then sums them to make the final prediction.\n2. 2. one weak learner is added at a time and existing weak learners in the model are left unchanged.\n3. 3. gbm repetitively leverages the patterns in residuals and strengthens. a model with weak predictions.\n4. 4. modelling is stopped when residuals do not have any pattern that can be modelled.\n\n&amp;#x200b;\n\ngbm algorithm steps :\n\n1. fit a simple regression or classification model.\n2. 2. calculate error residuals (actual value \u2013 predicted value)\n3. 3. fit a new model on error residuals as targets variable with same input variables.\n4. 4. add the predicted residuals to the previous predictions.\n5. 5. fit another model on residuals that are remaining and repeat steps 2 and 5 until the model is overfitting or the sum of residuals becomes constant.\n\nxgboost: extreme gradient boosting is a library for developing fast and high -performance gradient boosting tree models.xgboost is extensively used in ml competitions as it is almost 10 times faster than other gradient boosting techniques.\n\nxgboost parameters :\n\n1. general parameters: number of threads.\n2. 2. booster parameters: a. step size. b. regularisation\n3. 3. task parameters: a. objective. b. evaluation metric.\n\ngeneral parameters :\n\nnthread :\n\n1. number of parallel threads.\n2. 2. if no value is entered,algorithm automatically detects the number of cores and runs on all the cores.\n\nbooster :\n\n1. gbtree : tree-based model\n2.  gblinear : linear function\n\nsilent \\[default =0\\] :\n\n1. if set to 1, no running messages will be printed.hence,keep it \u20180\u2019 as the messages might help in understanding the model.\n\nbooster parameters : booster parameters guide individual booster (tree/regression) at each.\n\nparameters for tree booster :\n\neta : step size shrinkage is used in update to prevent overfitting. range in \\[0,1\\], default 0.3\n\ngamma : minimum loss reduction required to make a split. range \\[0,infinite\\],default 0\n\nmax\\_depth : maximum depth of the tree. range \\[1,infinite\\],default 6\n\nmin\\_child\\_weight : minimum sum of instance weight needed in a child. range \\[0,range\\] , default 1\n\n**for videos and more:** [www.facebook.com/seevecoding](https://www.facebook.com/seevecoding)\n\n **blog source:** \n\n[https://medium.com/@seeve/machine-learning-ensemble-learning-methods-4aab47158b41](https://medium.com/@seeve/machine-learning-ensemble-learning-methods-4aab47158b41)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/ggum9d/what_is_ensemble_learning_methods_and_how_to_do_it/',)", "identifyer": 5750421, "year": "2020"}], "name": "stumplearnmachinelearning2020"}
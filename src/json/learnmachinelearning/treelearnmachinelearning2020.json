{"interestingcomments": [{"autor": "boys_quick_come_see", "date": 1583934834000, "content": "Issue with pruning using rpart /!/ I was pruning a classification tree and using cross validation to identify the cp with lowest error. The #splits at the cp I chose was 0 but when I plot the model using rpart.plot I end up with 9 terminal nodes instead of 1, can anyone explain this?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fgxi3z/issue_with_pruning_using_rpart/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "issue with pruning using rpart /!/ i was pruning a classification -----> tree !!!  and using cross validation to identify the cp with lowest error. the #splits at the cp i chose was 0 but when i plot the model using rpart.plot i end up with 9 terminal nodes instead of 1, can anyone explain this?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/fgxi3z/issue_with_pruning_using_rpart/',)", "identifyer": 5741170, "year": "2020"}, {"autor": "ezeeetm", "date": 1587065774000, "content": "How to use Cross Validation on a single model? /!/ I've been studying Cross Validation this week, and the concept is clear to me as it relates to using CV to compare between different algorithms.\n\nWhat's not clear to me is, how to use CV when you've decided on a single algorithm to train.  For example, say I've decided to use CV to train a simple decision tree.  I use *k* folds CV to get 10 chunks of training/validation.  I have two questions at this point:  \n\n\n1. shouldn't I also set aside a test set that CV doesn't account for, as a final metric to test models against?\n2. what do I do with 10 chunks of training/validation data?  Like:\n   1. Train 10 trees and average the score?  which tree would I keep then for inference in production?\n   2. train 10 trees, then train each of those on each of the other folds, and pick the winner?\n   3. train 10 trees, then score them against the held out test data and pick the winner?\n   4. some combination of 2 and 3 that's not obvious to me?  \n\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g2lx24/how_to_use_cross_validation_on_a_single_model/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "how to use cross validation on a single model? /!/ i've been studying cross validation this week, and the concept is clear to me as it relates to using cv to compare between different algorithms.\n\nwhat's not clear to me is, how to use cv when you've decided on a single algorithm to train.  for example, say i've decided to use cv to train a simple decision -----> tree !!! .  i use *k* folds cv to get 10 chunks of training/validation.  i have two questions at this point:  \n\n\n1. shouldn't i also set aside a test set that cv doesn't account for, as a final metric to test models against?\n2. what do i do with 10 chunks of training/validation data?  like:\n   1. train 10 trees and average the score?  which tree would i keep then for inference in production?\n   2. train 10 trees, then train each of those on each of the other folds, and pick the winner?\n   3. train 10 trees, then score them against the held out test data and pick the winner?\n   4. some combination of 2 and 3 that's not obvious to me?  \n\n\nthanks!", "sortedWord": "None", "removed": "('nan',)", "score": 3, "comments": 5, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/g2lx24/how_to_use_cross_validation_on_a_single_model/',)", "identifyer": 5741283, "year": "2020"}, {"autor": "sourabhch45", "date": 1580918890000, "content": "How do I build a multidimensional decision tree regression? /!/ google couldn't help", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ezbroc/how_do_i_build_a_multidimensional_decision_tree/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "how do i build a multidimensional decision -----> tree !!!  regression? /!/ google couldn't help", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/ezbroc/how_do_i_build_a_multidimensional_decision_tree/',)", "identifyer": 5741300, "year": "2020"}, {"autor": "sourabhch45", "date": 1580831780000, "content": "Need help with decision tree regression. (building a multi-dimensional model) /!/ I am working on a very basic project for predicting the marks of a student. I am gonna use the previous marks of a bunch of students in every single subject they learnt, and the marks of the seniors who faced the *test* subjects, to predict the future marks for the *test* subject of the student.\n\n* I am implementing Decision tree regression for this project. Is this the appropriate model to use?\n* I need to train the model on the various subjects and therefore have a multidimensional decision tree regression. The regressor.fit(X, y) doesn't suit this condition since I would need something like: regressor.fit(subject1, subject2, subject3)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/eys2xr/need_help_with_decision_tree_regression_building/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "need help with decision -----> tree !!!  regression. (building a multi-dimensional model) /!/ i am working on a very basic project for predicting the marks of a student. i am gonna use the previous marks of a bunch of students in every single subject they learnt, and the marks of the seniors who faced the *test* subjects, to predict the future marks for the *test* subject of the student.\n\n* i am implementing decision tree regression for this project. is this the appropriate model to use?\n* i need to train the model on the various subjects and therefore have a multidimensional decision tree regression. the regressor.fit(x, y) doesn't suit this condition since i would need something like: regressor.fit(subject1, subject2, subject3)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/eys2xr/need_help_with_decision_tree_regression_building/',)", "identifyer": 5741336, "year": "2020"}, {"autor": "KrishnaAgarwalFeb1", "date": 1590261606000, "content": "Decision Trees from scratch in python /!/ Hi everyone, suggestions needed kindly drop your choice below on tutorial for decision trees , how it works under the hood, what all parameters to consider while applying your own decision tree classifier(Language: English) :\n\nMy Youtube channel: [https://www.youtube.com/channel/UCbfNArcXwjp9kPWpw-5x-3g?view\\_as=subscriber](https://www.youtube.com/channel/UCbfNArcXwjp9kPWpw-5x-3g?view_as=subscriber)\n\nSubscribe for regular updates :)\n\n[View Poll](https://www.reddit.com/poll/gpatn4)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gpatn4/decision_trees_from_scratch_in_python/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "decision trees from scratch in python /!/ hi everyone, suggestions needed kindly drop your choice below on tutorial for decision trees , how it works under the hood, what all parameters to consider while applying your own decision -----> tree !!!  classifier(language: english) :\n\nmy youtube channel: [https://www.youtube.com/channel/ucbfnarcxwjp9kpwpw-5x-3g?view\\_as=subscriber](https://www.youtube.com/channel/ucbfnarcxwjp9kpwpw-5x-3g?view_as=subscriber)\n\nsubscribe for regular updates :)\n\n[view poll](https://www.reddit.com/poll/gpatn4)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/gpatn4/decision_trees_from_scratch_in_python/',)", "identifyer": 5741453, "year": "2020"}, {"autor": "samirasam12", "date": 1590221891000, "content": "Random Forest: how are predictions related to node values? /!/  Hello everyone,\n\nI am working on a project to export a sklearn's RF classifier to Verilog.\n\nI came across a useful package ( [https://github.com/thesps/conifer](https://github.com/thesps/conifer)), but it does not support multiclass classification for RF.\n\nI am hence trying to understand how to modify the source code to allow for multiclass classification too.\n\nThis is the function that does the trick:\n\n    def convert_random_forest(bdt):   ensembleDict = {'max_depth' : bdt.max_depth, 'n_trees' : bdt.n_estimators,                   'n_features' : bdt.n_features_,                   'n_classes' : bdt.n_classes_, 'trees' : [],                   'init_predict' : [0] * bdt.n_classes_,                   'norm' : 1}   for tree in bdt.estimators_:     treesl = []     tree = treeToDict(bdt, tree.tree_)     tree = padTree(ensembleDict, tree)     # Random forest takes the mean prediction, do that here     # Also need to scale the values by their sum     v = np.array(tree['value'])     tree['value'] = (v / v.sum(axis=2)[:, np.newaxis] / bdt.n_estimators)[:,0,0].tolist()     treesl.append(tree)     ensembleDict['trees'].append(treesl) \n\nwhere v is an array of (n\\_nodes, 1, n\\_classes) shape and each of the n\\_classes-element arrays corresponds to the amount of training samples that end up in the respective node for each class (ax explained here: [https://stackoverflow.com/questions/47719001/what-does-scikit-learn-decisiontreeclassifier-tree-value-do](https://stackoverflow.com/questions/47719001/what-does-scikit-learn-decisiontreeclassifier-tree-value-do)).\n\nHowever, I truly do not understand:\n\n1. Why only the first element of each (1, 2) (for binary classification) array is considered (in the code represented by -&gt; \\[:, 0, 0\\]) ?\n2. What the creators mean by \"random forest takes the mean prediction, do that here\"? How is the amount of training samples that end up in the respective node for each class related to predictions?\n\nI would greatly appreciate any help!\n\nSam", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gp1com/random_forest_how_are_predictions_related_to_node/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "random forest: how are predictions related to node values? /!/  hello everyone,\n\ni am working on a project to export a sklearn's rf classifier to verilog.\n\ni came across a useful package ( [https://github.com/thesps/conifer](https://github.com/thesps/conifer)), but it does not support multiclass classification for rf.\n\ni am hence trying to understand how to modify the source code to allow for multiclass classification too.\n\nthis is the function that does the trick:\n\n    def convert_random_forest(bdt):   ensembledict = {'max_depth' : bdt.max_depth, 'n_trees' : bdt.n_estimators,                   'n_features' : bdt.n_features_,                   'n_classes' : bdt.n_classes_, 'trees' : [],                   'init_predict' : [0] * bdt.n_classes_,                   'norm' : 1}   for -----> tree !!!  in bdt.estimators_:     treesl = []     -----> tree !!!  = treetodict(bdt, tree.tree_)     -----> tree !!!  = padtree(ensembledict, -----> tree !!! )     # random forest takes the mean prediction, do that here     # also need to scale the values by their sum     v = np.array(tree['value'])     tree['value'] = (v / v.sum(axis=2)[:, np.newaxis] / bdt.n_estimators)[:,0,0].tolist()     treesl.append(tree)     ensembledict['trees'].append(treesl) \n\nwhere v is an array of (n\\_nodes, 1, n\\_classes) shape and each of the n\\_classes-element arrays corresponds to the amount of training samples that end up in the respective node for each class (ax explained here: [https://stackoverflow.com/questions/47719001/what-does-scikit-learn-decisiontreeclassifier-tree-value-do](https://stackoverflow.com/questions/47719001/what-does-scikit-learn-decisiontreeclassifier-tree-value-do)).\n\nhowever, i truly do not understand:\n\n1. why only the first element of each (1, 2) (for binary classification) array is considered (in the code represented by -&gt; \\[:, 0, 0\\]) ?\n2. what the creators mean by \"random forest takes the mean prediction, do that here\"? how is the amount of training samples that end up in the respective node for each class related to predictions?\n\ni would greatly appreciate any help!\n\nsam", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/gp1com/random_forest_how_are_predictions_related_to_node/',)", "identifyer": 5741479, "year": "2020"}, {"autor": "Potato_Knishes", "date": 1596560570000, "content": "Decision Tree Classifiers and Epochs /!/ Hi everyone, for this small project I am doing, I implemented a decision tree classifier to predict either a 0 or a 1 on my data.  This works well with good percentages given, but my mentor wants me to plot Training set Accuracy vs Test set Accuracy per every epoch and plot them.  \n\nThe problem here is that my decision tree only runs once(?)(so 1 only 1 epoch?).  How would weights be updated through every epoch if it just re-runs itself?  My plot as it stands would just be a sort-of straight line bouncing around 89-91% and wouldn't start low and actually \"train\" and fit the data by each epoch. \n\nI feel like I am just confused on terminology or how epochs actually work, and before you tell me to just ask my mentor I have and the language and understanding barrier is very hard to get this problem across between us.  Thank you to any and all help:)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i3n4ud/decision_tree_classifiers_and_epochs/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "decision -----> tree !!!  classifiers and epochs /!/ hi everyone, for this small project i am doing, i implemented a decision tree classifier to predict either a 0 or a 1 on my data.  this works well with good percentages given, but my mentor wants me to plot training set accuracy vs test set accuracy per every epoch and plot them.  \n\nthe problem here is that my decision tree only runs once(?)(so 1 only 1 epoch?).  how would weights be updated through every epoch if it just re-runs itself?  my plot as it stands would just be a sort-of straight line bouncing around 89-91% and wouldn't start low and actually \"train\" and fit the data by each epoch. \n\ni feel like i am just confused on terminology or how epochs actually work, and before you tell me to just ask my mentor i have and the language and understanding barrier is very hard to get this problem across between us.  thank you to any and all help:)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/i3n4ud/decision_tree_classifiers_and_epochs/',)", "identifyer": 5741651, "year": "2020"}, {"autor": "azteker", "date": 1599685671000, "content": "which lib would you recommend for decision tree solution /!/ sklearn has a good wrap-up of a decision tree. TensorFlow also implements its decision tree methods, but to me it is more complicated to use. I know there are other libs for the decision tree. Which would you recommend?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ipp7n4/which_lib_would_you_recommend_for_decision_tree/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "which lib would you recommend for decision -----> tree !!!  solution /!/ sklearn has a good wrap-up of a decision tree. tensorflow also implements its decision tree methods, but to me it is more complicated to use. i know there are other libs for the decision tree. which would you recommend?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/ipp7n4/which_lib_would_you_recommend_for_decision_tree/',)", "identifyer": 5741754, "year": "2020"}, {"autor": "bazarover", "date": 1602987993000, "content": "Tree based models and non ordinal numeric categorical variables /!/ Hello everybody,\n\nI am using random forest and gradient boost classifiers and one of the input features is a cluster group number from a Kmeans model (0-6). Is it okay to leave as is or should the feature be labeled \u201cgroup1,\u201d \u201cgroup2\u201d etc? Will the model process it and possibly make a split like \u201cif cluster_group &gt; 4...\u201d? And if so is that a bad thing or not? \n\nThanks for helping!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jd887h/tree_based_models_and_non_ordinal_numeric/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "-----> tree !!!  based models and non ordinal numeric categorical variables /!/ hello everybody,\n\ni am using random forest and gradient boost classifiers and one of the input features is a cluster group number from a kmeans model (0-6). is it okay to leave as is or should the feature be labeled \u201cgroup1,\u201d \u201cgroup2\u201d etc? will the model process it and possibly make a split like \u201cif cluster_group &gt; 4...\u201d? and if so is that a bad thing or not? \n\nthanks for helping!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/jd887h/tree_based_models_and_non_ordinal_numeric/',)", "identifyer": 5741792, "year": "2020"}, {"autor": "dhiraj8899", "date": 1602770767000, "content": "Learn Machine Learning Algorithms from Scratch like Neural Network, Decision Tree, Logistic Regression, Linear Regression and Gradient Descent /!/ [https://www.youtube.com/channel/UCuOT2b1Umrr0MittMzuxNcA?sub\\_confirmation=1](https://www.youtube.com/channel/UCuOT2b1Umrr0MittMzuxNcA?sub_confirmation=1)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jbo6po/learn_machine_learning_algorithms_from_scratch/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "learn machine learning algorithms from scratch like neural network, decision -----> tree !!! , logistic regression, linear regression and gradient descent /!/ [https://www.youtube.com/channel/ucuot2b1umrr0mittmzuxnca?sub\\_confirmation=1](https://www.youtube.com/channel/ucuot2b1umrr0mittmzuxnca?sub_confirmation=1)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/jbo6po/learn_machine_learning_algorithms_from_scratch/',)", "identifyer": 5741868, "year": "2020"}, {"autor": "lucascrafter", "date": 1606114976000, "content": "Machine Learning For Beginners - Python Decision Tree", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jzcw58/machine_learning_for_beginners_python_decision/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "machine learning for beginners - python decision -----> tree !!! ", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('rich:video',)", "medialink": "('https://youtu.be/aVEfKRfWjHc',)", "identifyer": 5742400, "year": "2020"}, {"autor": "pvm_64", "date": 1609175539000, "content": "NLP Text Classification Help /!/ Hi everyone. I have recently taken on an automated database development project for a professor. This primarily imvolves web-scraping news/journal articles, classifying the topics of the articles, and extracting the articles relevant to the database's theme of environmental technology. As a geography student my background in machine learning is limited, though I have sufficient Python abilities. \n\nSo far I have web scrapped about 50k news articles from various website. I am currently working on the NLP text classification component. Using Scikit.Learn I got a Naive Bayes Classification program working using a TF-IDF Vector approach, however the accuracy is essentially zero. I suspect this is because the volume of training data is incredibly low for the number of classes. Currently I only have a couple hundred manually classified articles, with between 3-20 articles per class.\n\nQuestions: \n\n1) How many classes is too much? Is 100 classes a bad idea?\n\n2) Approximately how many training data values should you have for each class? \n\nI expect to be processing 50k + articles. Ideally I suspect a couple thousand would be required per class, however this would require manually classifying thousands of articles. Any advice?\n\n3) How do you set hyperparameters? \nWhich NLP algorithms allow for hyperparameters? \n\nI have created a long list of keywords that help identify each of the classes. From my reading it seems that hyperparameters can greatly improve the abilities of the machine learning algorithms. However, I haven't seen any clear information/guides online for how to set your own hyperparameters in the algorithm. \n\nAdditionally, I suspect that only some of the NLP algorithms allow for hyperparameters. Is that correct? If so, which algorithms? (Naive Bayes, Logistic Regression, Support Vector Machine, K-Nearest Neighbours, Random Forest, Stochastic Gradient Descent, Decision Tree)\n\n4) Which of the classification algorithms mentioned above allow for multi-classification?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kluf0a/nlp_text_classification_help/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "nlp text classification help /!/ hi everyone. i have recently taken on an automated database development project for a professor. this primarily imvolves web-scraping news/journal articles, classifying the topics of the articles, and extracting the articles relevant to the database's theme of environmental technology. as a geography student my background in machine learning is limited, though i have sufficient python abilities. \n\nso far i have web scrapped about 50k news articles from various website. i am currently working on the nlp text classification component. using scikit.learn i got a naive bayes classification program working using a tf-idf vector approach, however the accuracy is essentially zero. i suspect this is because the volume of training data is incredibly low for the number of classes. currently i only have a couple hundred manually classified articles, with between 3-20 articles per class.\n\nquestions: \n\n1) how many classes is too much? is 100 classes a bad idea?\n\n2) approximately how many training data values should you have for each class? \n\ni expect to be processing 50k + articles. ideally i suspect a couple thousand would be required per class, however this would require manually classifying thousands of articles. any advice?\n\n3) how do you set hyperparameters? \nwhich nlp algorithms allow for hyperparameters? \n\ni have created a long list of keywords that help identify each of the classes. from my reading it seems that hyperparameters can greatly improve the abilities of the machine learning algorithms. however, i haven't seen any clear information/guides online for how to set your own hyperparameters in the algorithm. \n\nadditionally, i suspect that only some of the nlp algorithms allow for hyperparameters. is that correct? if so, which algorithms? (naive bayes, logistic regression, support vector machine, k-nearest neighbours, random forest, stochastic gradient descent, decision -----> tree !!! )\n\n4) which of the classification algorithms mentioned above allow for multi-classification?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/kluf0a/nlp_text_classification_help/',)", "identifyer": 5742544, "year": "2020"}, {"autor": "pspatel602", "date": 1585584497000, "content": "Could anyone can give suggestion on building 5 ML Project for Each ML algorithm to become advanced pro player in the field? /!/ I mean that anyone could please suggest a 5 project (Basic to **advanced pro player** type and it helps if it we use them in **real life projects** )  for each following category so it helps to develop pro skills .\n\n1. Linear Regression\n2. Logistic Regression\n3. Decision Tree\n4. SVM\n5. Naive Bayes\n6. kNN\n7. K-Means\n8. Random Forest\n9. Dimensionality Reduction Algorithms\n10. Gradient Boosting algorithms\n11. GBM\n12. XGBoost\n13. LightGBM\n14. CatBoost\n\n**\"I will be grateful for any help you can provide.\"**", "link": "https://www.reddit.com/r/learnmachinelearning/comments/frtky5/could_anyone_can_give_suggestion_on_building_5_ml/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "could anyone can give suggestion on building 5 ml project for each ml algorithm to become advanced pro player in the field? /!/ i mean that anyone could please suggest a 5 project (basic to **advanced pro player** type and it helps if it we use them in **real life projects** )  for each following category so it helps to develop pro skills .\n\n1. linear regression\n2. logistic regression\n3. decision -----> tree !!! \n4. svm\n5. naive bayes\n6. knn\n7. k-means\n8. random forest\n9. dimensionality reduction algorithms\n10. gradient boosting algorithms\n11. gbm\n12. xgboost\n13. lightgbm\n14. catboost\n\n**\"i will be grateful for any help you can provide.\"**", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/frtky5/could_anyone_can_give_suggestion_on_building_5_ml/',)", "identifyer": 5742592, "year": "2020"}, {"autor": "trekhleb", "date": 1588692664000, "content": "\ud83e\udd16 Interactive Machine Learning Experiments /!/ ## TL;DR\n\nHey readers!\n\nI've open-sourced new [**\ud83e\udd16 Interactive Machine Learning Experiments**](https://github.com/trekhleb/machine-learning-experiments) project on GitHub. Each experiment consists of \ud83c\udfcb\ufe0f *Jupyter/Colab notebook* (to see how a model was trained) and \ud83c\udfa8 *demo page* (to see a model in action right in your browser).\n\nAlthough the models may be a little dumb (remember, these are just experiments, not a production ready code), they will try to do their best to:\n\n* \ud83d\udd8c Recognize digits or sketches you draw in your browser\n* \ud83d\udcf8 Detect and recognize the objects you'll show to your camera\n* \ud83c\udf05 Classify your uploaded image\n* \ud83d\udcdd Write a Shakespeare poem with you\n* \u270a\ud83d\udd90\u270c\ufe0f Play with you in Rock-Paper-Scissors game\n* etc.\n\nI've trained the models on *Python* using *TensorFlow 2* with *Keras* support  and then consumed them for a demo in a browser using *React* and *JavaScript* version of *Tensorflow*.\n\nhttps://preview.redd.it/3u5jdch1uyw41.png?width=1800&amp;format=png&amp;auto=webp&amp;s=946b73c9dc85f432abb025e285286f9209fb62ae\n\n## Models performance\n\n\u26a0\ufe0f First, let's set our expectations.\ufe0f The repository contains machine learning **experiments** and **not** a production ready, reusable, optimised and fine-tuned code and models. This is rather a sandbox or a playground for learning and trying different machine learning approaches, algorithms and data-sets. Models might not perform well and there is a place for overfitting/underfitting.\n\nTherefore, sometimes you might see things like this:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/j2q8whk2uyw41.png?width=1198&amp;format=png&amp;auto=webp&amp;s=a70ae224254ea2f19886c952cfb294d2b0d42317\n\nBut be patient, sometimes the model might get smarter \ud83e\udd13 and give you this:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/eyvhdtp3uyw41.png?width=1208&amp;format=png&amp;auto=webp&amp;s=60ade63facde47c0ea2c6d1737679ee35b463d6a\n\n## Background\n\nI'm a [software engineer](https://www.linkedin.com/in/trekhleb/) and for the last several years now I've been doing mostly frontend and backend programming. In my spare time, as a hobby, I decided to dig into machine learning topics to make it less *like magic* and *more like math* to myself.\n\n1. \ud83d\uddd3 Since **Python** might be a good choice to start experimenting with Machine Learning I decided to learn its basic syntax first. As a result a [\ud83d\udc0d Playground and Cheatsheet for Learning Python](https://github.com/trekhleb/learn-python) project came out. This was just to practice Python and at the same time to have a cheatsheet of basic syntax once I need it (for things like `dict_via_comprehension = {x: x**2 for x in (2, 4, 6)}` etc.).\n2. \ud83d\uddd3 After learning a bit of Python I wanted to dig into the basic **math** behind Machine Learning. So after passing an awesome [Machine Learning course by Andrew Ng](https://www.coursera.org/learn/machine-learning) on Coursera the [\ud83e\udd16 Homemade Machine Learning](https://github.com/trekhleb/homemade-machine-learning) project came out. This time it was about creating a cheatsheet for basic machine learning math algorithms like linear regression, logistic regression, k-means, multilayer perceptron etc.\n3. \ud83d\uddd3 The next attempt to play around with basic Machine Learning math was [\ud83e\udd16 NanoNeuron](https://github.com/trekhleb/nano-neuron). It was about 7 simple JavaScript functions that supposed to give you a feeling of how machines can actually \"learn\".\n4. \ud83d\uddd3 After finishing yet another awesome [Deep Learning Specialization by Andrew Ng](https://www.coursera.org/specializations/deep-learning) on Coursera I decided to practice a bit more with **multilayer perceptrons**, **convolutional** and **recurrent neural networks** (CNNs and RNNs). This time instead of implementing everything from scratch I decided to start using some machine learning framework. I ended up using [TensorFlow 2](https://www.tensorflow.org/) with [Keras](https://www.tensorflow.org/guide/keras/overview). I also didn't want to focus too much on math (letting the framework do it for me) and instead I wanted to come up with something more practical, applicable and something I could try to play with right in my browser. As a result new [\ud83e\udd16 Interactive Machine Learning Experiments](https://github.com/trekhleb/machine-learning-experiments) came out that I want to describe a bit more here.\n\n## Tech-stack\n\n## Models training\n\n* \ud83c\udfcb\ud83c\udffb\u200d I used [Keras](https://www.tensorflow.org/guide/keras/overview) inside [TensorFlow 2](https://www.tensorflow.org/) for modelling and training. Since I had zero experience with machine learning frameworks, I needed to start with something. One of the selling points in favor of TensorFlow was that it has both Python and [JavaScript flavor](https://www.tensorflow.org/js) of the library with similar API. So eventually I used Python version for training and JavaScript version for demos.\n* \ud83c\udfcb\ud83c\udffb\u200d I trained TensorFlow models on Python inside [Jupyter](https://jupyter.org/) notebooks locally and sometimes used [Colab](https://colab.research.google.com/) to make the training faster on GPU.\n* \ud83d\udcbb Most of the models were trained on good old MacBook's Pro CPU (2,9 GHz Dual-Core Intel Core i5).\n* \ud83d\udd22 Of course there is no way you could run away from [NumPy](https://numpy.org/) for matrix/tensors operations.\n\n## Models demo\n\n* \ud83c\udfcb\ud83c\udffb\u200d I used [TensorFlow.js](https://www.tensorflow.org/js) to do predictions with previously trained models.\n* \u267b\ufe0f To convert *Keras HDF5* models to *TensorFlow.js Layers* format I used [TensorFlow.js converter](https://github.com/tensorflow/tfjs/tree/master/tfjs-converter). This might be inefficient to transfer the whole model (megabytes of data) to the browser instead of making predictions through HTTP requests, but again, remember that these are just experiments and not production-ready code and architecture. I wanted to avoid having a dedicated back-end service to make architecture simpler.\n* \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfa8 The [Demo application](http://trekhleb.github.io/machine-learning-experiments) was created on [React](https://reactjs.org/) using [create-react-app](https://github.com/facebook/create-react-app) starter with a default [Flow](https://flow.org/en/) flavour for type checking.\n* \ud83d\udc85\ud83c\udffb For styling, I used [Material UI](https://material-ui.com/). It was, as they say, \"to kill two birds\" at once and try out a new styling framework (sorry, [Bootstrap](https://getbootstrap.com/) \ud83e\udd37\ud83c\udffb\u200d).\n\n## Experiments\n\nSo, in short, you may access Demo page and Jupyter notebooks by these links:\n\n* \ud83c\udfa8 [**Launch ML experiments demo**](http://trekhleb.github.io/machine-learning-experiments)\n* \ud83c\udfcb\ufe0f [**Check ML experiments Jupyter notebooks**](https://github.com/trekhleb/machine-learning-experiments)\n\n## Experiments with Multilayer Perceptron (MLP)\n\n&gt;A [multilayer perceptron (MLP)](https://en.wikipedia.org/wiki/Multilayer_perceptron) is a class of feedforward artificial neural network (ANN). Multilayer perceptrons are sometimes referred to as \"vanilla\" neural networks (composed of multiple layers of perceptrons), especially when they have a single hidden layer.\n\n## Handwritten Digits Recognition\n\nYou draw a digit, and the model tries to recognize it.\n\n* \ud83c\udfa8 [Demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/DigitsRecognitionMLP)\n* \ud83c\udfcb\ufe0f [Training in Jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/digits_recognition_mlp/digits_recognition_mlp.ipynb)\n* \ufe0f\ud83c\udfcb\ufe0f  [Training in Colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/digits_recognition_mlp/digits_recognition_mlp.ipynb)\n\n&amp;#x200B;\n\nhttps://i.redd.it/8rc55ws7uyw41.gif\n\n## Handwritten Sketch Recognition\n\nYou draw a sketch, and the model tries to recognize it.\n\n* \ud83c\udfa8 [Demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/SketchRecognitionMLP)\n* \ud83c\udfcb\ufe0f [Training in Jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/sketch_recognition_mlp/sketch_recognition_mlp.ipynb)\n* \ufe0f\ud83c\udfcb\ufe0f  [Training in Colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/sketch_recognition_mlp/sketch_recognition_mlp.ipynb)\n\n&amp;#x200B;\n\nhttps://i.redd.it/uhavc6d9uyw41.gif\n\n## Experiments with Convolutional Neural Networks (CNN)\n\n&gt;A [convolutional neural network (CNN, or ConvNet)](https://en.wikipedia.org/wiki/Convolutional_neural_network) is a class of deep neural networks, most commonly applied to analyzing visual imagery (photos, videos). They are used for detecting and classifying objects on photos and videos, style transfer, face recognition, pose estimation etc.\n\n## Handwritten Digits Recognition (CNN)\n\nYou draw a digit, and the model tries to recognize it. This experiment is similar to the one from MLP section, but it uses CNN under the hood.\n\n* \ud83c\udfa8 [Demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/DigitsRecognitionCNN)\n* \ud83c\udfcb\ufe0f [Training in Jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/digits_recognition_cnn/digits_recognition_cnn.ipynb)\n* \ufe0f\ud83c\udfcb\ufe0f  [Training in Colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/digits_recognition_cnn/digits_recognition_cnn.ipynb)\n\n&amp;#x200B;\n\nhttps://i.redd.it/jakuk4lauyw41.gif\n\n## Handwritten Sketch Recognition (CNN)\n\nYou draw a sketch, and the model tries to recognize it. This experiment is similar to the one from MLP section, but it uses CNN under the hood.\n\n* \ud83c\udfa8 [Demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/SketchRecognitionCNN)\n* \ud83c\udfcb\ufe0f [Training in Jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/sketch_recognition_cnn/sketch_recognition_cnn.ipynb)\n* \ufe0f\ud83c\udfcb\ufe0f  [Training in Colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/sketch_recognition_cnn/sketch_recognition_cnn.ipynb)\n\n&amp;#x200B;\n\nhttps://i.redd.it/dzc4aghbuyw41.gif\n\n## Rock Paper Scissors (CNN)\n\nYou play a Rock-Paper-Scissors game with the model. This experiment uses CNN that is trained from scratch.\n\n* \ud83c\udfa8 [Demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/RockPaperScissorsCNN)\n* \ud83c\udfcb\ufe0f [Training in Jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/rock_paper_scissors_cnn/rock_paper_scissors_cnn.ipynb)\n* \ufe0f\ud83c\udfcb\ufe0f  [Training in Colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/rock_paper_scissors_cnn/rock_paper_scissors_cnn.ipynb)\n\n&amp;#x200B;\n\nhttps://i.redd.it/ewnsaudcuyw41.gif\n\n## Rock Paper Scissors (MobilenetV2)\n\nYou play a Rock-Paper-Scissors game with the model. This model uses transfer learning and is based on [MobilenetV2](https://www.tensorflow.org/api_docs/python/tf/keras/applications/MobileNetV2).\n\n* \ud83c\udfa8 [Demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/RockPaperScissorsMobilenetV2)\n* \ud83c\udfcb\ufe0f [Training in Jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/rock_paper_scissors_mobilenet_v2/rock_paper_scissors_mobilenet_v2.ipynb)\n* \ufe0f\ud83c\udfcb\ufe0f  [Training in Colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/rock_paper_scissors_mobilenet_v2/rock_paper_scissors_mobilenet_v2.ipynb)\n\n&amp;#x200B;\n\nhttps://i.redd.it/70ruj86euyw41.gif\n\n## Objects Detection (MobileNetV2)\n\nYou show to the model your environment through your camera, and it will try to detect and recognize the objects. This model uses transfer learning and is based on [MobilenetV2](https://www.tensorflow.org/api_docs/python/tf/keras/applications/MobileNetV2).\n\n* \ud83c\udfa8 [Demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/ObjectsDetectionSSDLiteMobilenetV2)\n* \ud83c\udfcb\ufe0f [Training in Jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/objects_detection_ssdlite_mobilenet_v2/objects_detection_ssdlite_mobilenet_v2.ipynb)\n* \ufe0f\ud83c\udfcb\ufe0f  [Training in Colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/objects_detection_ssdlite_mobilenet_v2/objects_detection_ssdlite_mobilenet_v2.ipynb)\n\n&amp;#x200B;\n\nhttps://i.redd.it/16xi3nthuyw41.gif\n\n## Image Classification (MobileNetV2)\n\nYou upload a picture, and the model tries to classify it depending on what it \"sees\" on the picture. This model uses transfer learning and is based on [MobilenetV2](https://www.tensorflow.org/api_docs/python/tf/keras/applications/MobileNetV2).\n\n* \ud83c\udfa8 [Demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/ImageClassificationMobilenetV2)\n* \ud83c\udfcb\ufe0f [Training in Jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/image_classification_mobilenet_v2/image_classification_mobilenet_v2.ipynb)\n* \ufe0f\ud83c\udfcb\ufe0f  [Training in Colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/image_classification_mobilenet_v2/image_classification_mobilenet_v2.ipynb)\n\n&amp;#x200B;\n\nhttps://i.redd.it/pv8zkkuiuyw41.gif\n\n## Experiments with Recurrent Neural Networks (RNN)\n\n&gt;A [recurrent neural network (RNN)](https://en.wikipedia.org/wiki/Recurrent_neural_network) is a class of deep neural networks, most commonly applied to sequence-based data like speech, voice, text or music. They are used for machine translation, speech recognition, voice synthesis etc.\n\n## Numbers Summation\n\nYou type a summation expression (i.e. `17+38`), and the model predicts the result (i.e. `55`). The interesting part here is that the model treats the input as a *sequence*, meaning it learned that when you type a sequence `1` \u2192 `17` \u2192 `17+` \u2192 `17+3` \u2192 `17+38` it \"translates\" it to another sequence `55`. You may think about it as translating a Spanish `Hola` sequence to English `Hello`.\n\n* \ud83c\udfa8 [Demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/NumbersSummationRNN)\n* \ud83c\udfcb\ufe0f [Training in Jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/numbers_summation_rnn/numbers_summation_rnn.ipynb)\n* \ufe0f\ud83c\udfcb\ufe0f  [Training in Colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/numbers_summation_rnn/numbers_summation_rnn.ipynb)\n\n&amp;#x200B;\n\nhttps://i.redd.it/rix288wjuyw41.gif\n\n## Shakespeare Text Generation\n\nYou start typing a poem like Shakespeare, and the model will continue it like Shakespeare. At least it will try to do so \ud83d\ude00.\n\n* \ud83c\udfa8 [Demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/TextGenerationShakespeareRNN)\n* \ud83c\udfcb\ufe0f [Training in Jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/text_generation_shakespeare_rnn/text_generation_shakespeare_rnn.ipynb)\n* \ufe0f\ud83c\udfcb\ufe0f  [Training in Colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/text_generation_shakespeare_rnn/text_generation_shakespeare_rnn.ipynb)\n\n&amp;#x200B;\n\nhttps://i.redd.it/nbkm3askuyw41.gif\n\n## Wikipedia Text Generation\n\nYou start typing a Wiki article, and the model tries to continue it.\n\n* \ud83c\udfa8 [Demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/TextGenerationWikipediaRNN)\n* \ud83c\udfcb\ufe0f [Training in Jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/text_generation_wikipedia_rnn/text_generation_wikipedia_rnn.ipynb)\n* \ufe0f\ud83c\udfcb\ufe0f  [Training in Colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/text_generation_wikipedia_rnn/text_generation_wikipedia_rnn.ipynb)\n\n&amp;#x200B;\n\nhttps://i.redd.it/a0l2g1pluyw41.gif\n\n## Future plans\n\nAs I've mentioned above the main purpose of [the repository](https://github.com/trekhleb/machine-learning-experiments) is to be more like a playground for learning rather than for production-ready models. Therefore, the main plan is to **continue learning and experimenting** with deep-learning challenges and approaches. The next interesting challenges to play with might be:\n\n* Emotions detection\n* Style transfer\n* Language translation\n* Generating images (i.e. handwritten numbers)\n* etc.\n\nAnother interesting opportunity would be to **tune existing models to make them more performant**. I believe it might give a better understanding of how to overcome overfitting and underfitting and what to do with the model if it just stuck on `60%` accuracy level for both training and validation sets and doesn't want to improve anymore \ud83e\udd14.\n\nAnyways, I hope you might find some useful insights for models training from [the repository](https://github.com/trekhleb/machine-learning-experiments) or at least to have some fun playing around with the demos!\n\nHappy learning! \ud83e\udd16", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gdzv8h/interactive_machine_learning_experiments/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "\ud83e\udd16 interactive machine learning experiments /!/ ## tl;dr\n\nhey readers!\n\ni've open-sourced new [**\ud83e\udd16 interactive machine learning experiments**](https://github.com/trekhleb/machine-learning-experiments) project on github. each experiment consists of \ud83c\udfcb\ufe0f *jupyter/colab notebook* (to see how a model was trained) and \ud83c\udfa8 *demo page* (to see a model in action right in your browser).\n\nalthough the models may be a little dumb (remember, these are just experiments, not a production ready code), they will try to do their best to:\n\n* \ud83d\udd8c recognize digits or sketches you draw in your browser\n* \ud83d\udcf8 detect and recognize the objects you'll show to your camera\n* \ud83c\udf05 classify your uploaded image\n* \ud83d\udcdd write a shakespeare poem with you\n* \u270a\ud83d\udd90\u270c\ufe0f play with you in rock-paper-scissors game\n* etc.\n\ni've trained the models on *python* using *tensorflow 2* with *keras* support  and then consumed them for a demo in a browser using *react* and *javascript* version of *tensorflow*.\n\nhttps://preview.redd.it/3u5jdch1uyw41.png?width=1800&amp;format=png&amp;auto=webp&amp;s=946b73c9dc85f432abb025e285286f9209fb62ae\n\n## models performance\n\n\u26a0\ufe0f first, let's set our expectations.\ufe0f the repository contains machine learning **experiments** and **not** a production ready, reusable, optimised and fine-tuned code and models. this is rather a sandbox or a playground for learning and trying different machine learning approaches, algorithms and data-sets. models might not perform well and there is a place for overfitting/underfitting.\n\ntherefore, sometimes you might see things like this:\n\n&amp;#x200b;\n\nhttps://preview.redd.it/j2q8whk2uyw41.png?width=1198&amp;format=png&amp;auto=webp&amp;s=a70ae224254ea2f19886c952cfb294d2b0d42317\n\nbut be patient, sometimes the model might get smarter \ud83e\udd13 and give you this:\n\n&amp;#x200b;\n\nhttps://preview.redd.it/eyvhdtp3uyw41.png?width=1208&amp;format=png&amp;auto=webp&amp;s=60ade63facde47c0ea2c6d1737679ee35b463d6a\n\n## background\n\ni'm a [software engineer](https://www.linkedin.com/in/trekhleb/) and for the last several years now i've been doing mostly frontend and backend programming. in my spare time, as a hobby, i decided to dig into machine learning topics to make it less *like magic* and *more like math* to myself.\n\n1. \ud83d\uddd3 since **python** might be a good choice to start experimenting with machine learning i decided to learn its basic syntax first. as a result a [\ud83d\udc0d playground and cheatsheet for learning python](https://github.com/trekhleb/learn-python) project came out. this was just to practice python and at the same time to have a cheatsheet of basic syntax once i need it (for things like `dict_via_comprehension = {x: x**2 for x in (2, 4, 6)}` etc.).\n2. \ud83d\uddd3 after learning a bit of python i wanted to dig into the basic **math** behind machine learning. so after passing an awesome [machine learning course by andrew ng](https://www.coursera.org/learn/machine-learning) on coursera the [\ud83e\udd16 homemade machine learning](https://github.com/trekhleb/homemade-machine-learning) project came out. this time it was about creating a cheatsheet for basic machine learning math algorithms like linear regression, logistic regression, k-means, multilayer perceptron etc.\n3. \ud83d\uddd3 the next attempt to play around with basic machine learning math was [\ud83e\udd16 nanoneuron](https://github.com/trekhleb/nano-neuron). it was about 7 simple javascript functions that supposed to give you a feeling of how machines can actually \"learn\".\n4. \ud83d\uddd3 after finishing yet another awesome [deep learning specialization by andrew ng](https://www.coursera.org/specializations/deep-learning) on coursera i decided to practice a bit more with **multilayer perceptrons**, **convolutional** and **recurrent neural networks** (cnns and rnns). this time instead of implementing everything from scratch i decided to start using some machine learning framework. i ended up using [tensorflow 2](https://www.tensorflow.org/) with [keras](https://www.tensorflow.org/guide/keras/overview). i also didn't want to focus too much on math (letting the framework do it for me) and instead i wanted to come up with something more practical, applicable and something i could try to play with right in my browser. as a result new [\ud83e\udd16 interactive machine learning experiments](https://github.com/trekhleb/machine-learning-experiments) came out that i want to describe a bit more here.\n\n## tech-stack\n\n## models training\n\n* \ud83c\udfcb\ud83c\udffb\u200d i used [keras](https://www.tensorflow.org/guide/keras/overview) inside [tensorflow 2](https://www.tensorflow.org/) for modelling and training. since i had zero experience with machine learning frameworks, i needed to start with something. one of the selling points in favor of tensorflow was that it has both python and [javascript flavor](https://www.tensorflow.org/js) of the library with similar api. so eventually i used python version for training and javascript version for demos.\n* \ud83c\udfcb\ud83c\udffb\u200d i trained tensorflow models on python inside [jupyter](https://jupyter.org/) notebooks locally and sometimes used [colab](https://colab.research.google.com/) to make the training faster on gpu.\n* \ud83d\udcbb most of the models were trained on good old macbook's pro cpu (2,9 ghz dual-core intel core i5).\n* \ud83d\udd22 of course there is no way you could run away from [numpy](https://numpy.org/) for matrix/tensors operations.\n\n## models demo\n\n* \ud83c\udfcb\ud83c\udffb\u200d i used [tensorflow.js](https://www.tensorflow.org/js) to do predictions with previously trained models.\n* \u267b\ufe0f to convert *keras hdf5* models to *tensorflow.js layers* format i used [tensorflow.js converter](https://github.com/tensorflow/tfjs/-----> tree !!! /master/tfjs-converter). this might be inefficient to transfer the whole model (megabytes of data) to the browser instead of making predictions through http requests, but again, remember that these are just experiments and not production-ready code and architecture. i wanted to avoid having a dedicated back-end service to make architecture simpler.\n* \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfa8 the [demo application](http://trekhleb.github.io/machine-learning-experiments) was created on [react](https://reactjs.org/) using [create-react-app](https://github.com/facebook/create-react-app) starter with a default [flow](https://flow.org/en/) flavour for type checking.\n* \ud83d\udc85\ud83c\udffb for styling, i used [material ui](https://material-ui.com/). it was, as they say, \"to kill two birds\" at once and try out a new styling framework (sorry, [bootstrap](https://getbootstrap.com/) \ud83e\udd37\ud83c\udffb\u200d).\n\n## experiments\n\nso, in short, you may access demo page and jupyter notebooks by these links:\n\n* \ud83c\udfa8 [**launch ml experiments demo**](http://trekhleb.github.io/machine-learning-experiments)\n* \ud83c\udfcb\ufe0f [**check ml experiments jupyter notebooks**](https://github.com/trekhleb/machine-learning-experiments)\n\n## experiments with multilayer perceptron (mlp)\n\n&gt;a [multilayer perceptron (mlp)](https://en.wikipedia.org/wiki/multilayer_perceptron) is a class of feedforward artificial neural network (ann). multilayer perceptrons are sometimes referred to as \"vanilla\" neural networks (composed of multiple layers of perceptrons), especially when they have a single hidden layer.\n\n## handwritten digits recognition\n\nyou draw a digit, and the model tries to recognize it.\n\n* \ud83c\udfa8 [demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/digitsrecognitionmlp)\n* \ud83c\udfcb\ufe0f [training in jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/digits_recognition_mlp/digits_recognition_mlp.ipynb)\n* \ufe0f\ud83c\udfcb\ufe0f  [training in colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/digits_recognition_mlp/digits_recognition_mlp.ipynb)\n\n&amp;#x200b;\n\nhttps://i.redd.it/8rc55ws7uyw41.gif\n\n## handwritten sketch recognition\n\nyou draw a sketch, and the model tries to recognize it.\n\n* \ud83c\udfa8 [demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/sketchrecognitionmlp)\n* \ud83c\udfcb\ufe0f [training in jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/sketch_recognition_mlp/sketch_recognition_mlp.ipynb)\n* \ufe0f\ud83c\udfcb\ufe0f  [training in colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/sketch_recognition_mlp/sketch_recognition_mlp.ipynb)\n\n&amp;#x200b;\n\nhttps://i.redd.it/uhavc6d9uyw41.gif\n\n## experiments with convolutional neural networks (cnn)\n\n&gt;a [convolutional neural network (cnn, or convnet)](https://en.wikipedia.org/wiki/convolutional_neural_network) is a class of deep neural networks, most commonly applied to analyzing visual imagery (photos, videos). they are used for detecting and classifying objects on photos and videos, style transfer, face recognition, pose estimation etc.\n\n## handwritten digits recognition (cnn)\n\nyou draw a digit, and the model tries to recognize it. this experiment is similar to the one from mlp section, but it uses cnn under the hood.\n\n* \ud83c\udfa8 [demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/digitsrecognitioncnn)\n* \ud83c\udfcb\ufe0f [training in jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/digits_recognition_cnn/digits_recognition_cnn.ipynb)\n* \ufe0f\ud83c\udfcb\ufe0f  [training in colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/digits_recognition_cnn/digits_recognition_cnn.ipynb)\n\n&amp;#x200b;\n\nhttps://i.redd.it/jakuk4lauyw41.gif\n\n## handwritten sketch recognition (cnn)\n\nyou draw a sketch, and the model tries to recognize it. this experiment is similar to the one from mlp section, but it uses cnn under the hood.\n\n* \ud83c\udfa8 [demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/sketchrecognitioncnn)\n* \ud83c\udfcb\ufe0f [training in jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/sketch_recognition_cnn/sketch_recognition_cnn.ipynb)\n* \ufe0f\ud83c\udfcb\ufe0f  [training in colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/sketch_recognition_cnn/sketch_recognition_cnn.ipynb)\n\n&amp;#x200b;\n\nhttps://i.redd.it/dzc4aghbuyw41.gif\n\n## rock paper scissors (cnn)\n\nyou play a rock-paper-scissors game with the model. this experiment uses cnn that is trained from scratch.\n\n* \ud83c\udfa8 [demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/rockpaperscissorscnn)\n* \ud83c\udfcb\ufe0f [training in jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/rock_paper_scissors_cnn/rock_paper_scissors_cnn.ipynb)\n* \ufe0f\ud83c\udfcb\ufe0f  [training in colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/rock_paper_scissors_cnn/rock_paper_scissors_cnn.ipynb)\n\n&amp;#x200b;\n\nhttps://i.redd.it/ewnsaudcuyw41.gif\n\n## rock paper scissors (mobilenetv2)\n\nyou play a rock-paper-scissors game with the model. this model uses transfer learning and is based on [mobilenetv2](https://www.tensorflow.org/api_docs/python/tf/keras/applications/mobilenetv2).\n\n* \ud83c\udfa8 [demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/rockpaperscissorsmobilenetv2)\n* \ud83c\udfcb\ufe0f [training in jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/rock_paper_scissors_mobilenet_v2/rock_paper_scissors_mobilenet_v2.ipynb)\n* \ufe0f\ud83c\udfcb\ufe0f  [training in colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/rock_paper_scissors_mobilenet_v2/rock_paper_scissors_mobilenet_v2.ipynb)\n\n&amp;#x200b;\n\nhttps://i.redd.it/70ruj86euyw41.gif\n\n## objects detection (mobilenetv2)\n\nyou show to the model your environment through your camera, and it will try to detect and recognize the objects. this model uses transfer learning and is based on [mobilenetv2](https://www.tensorflow.org/api_docs/python/tf/keras/applications/mobilenetv2).\n\n* \ud83c\udfa8 [demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/objectsdetectionssdlitemobilenetv2)\n* \ud83c\udfcb\ufe0f [training in jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/objects_detection_ssdlite_mobilenet_v2/objects_detection_ssdlite_mobilenet_v2.ipynb)\n* \ufe0f\ud83c\udfcb\ufe0f  [training in colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/objects_detection_ssdlite_mobilenet_v2/objects_detection_ssdlite_mobilenet_v2.ipynb)\n\n&amp;#x200b;\n\nhttps://i.redd.it/16xi3nthuyw41.gif\n\n## image classification (mobilenetv2)\n\nyou upload a picture, and the model tries to classify it depending on what it \"sees\" on the picture. this model uses transfer learning and is based on [mobilenetv2](https://www.tensorflow.org/api_docs/python/tf/keras/applications/mobilenetv2).\n\n* \ud83c\udfa8 [demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/imageclassificationmobilenetv2)\n* \ud83c\udfcb\ufe0f [training in jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/image_classification_mobilenet_v2/image_classification_mobilenet_v2.ipynb)\n* \ufe0f\ud83c\udfcb\ufe0f  [training in colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/image_classification_mobilenet_v2/image_classification_mobilenet_v2.ipynb)\n\n&amp;#x200b;\n\nhttps://i.redd.it/pv8zkkuiuyw41.gif\n\n## experiments with recurrent neural networks (rnn)\n\n&gt;a [recurrent neural network (rnn)](https://en.wikipedia.org/wiki/recurrent_neural_network) is a class of deep neural networks, most commonly applied to sequence-based data like speech, voice, text or music. they are used for machine translation, speech recognition, voice synthesis etc.\n\n## numbers summation\n\nyou type a summation expression (i.e. `17+38`), and the model predicts the result (i.e. `55`). the interesting part here is that the model treats the input as a *sequence*, meaning it learned that when you type a sequence `1` \u2192 `17` \u2192 `17+` \u2192 `17+3` \u2192 `17+38` it \"translates\" it to another sequence `55`. you may think about it as translating a spanish `hola` sequence to english `hello`.\n\n* \ud83c\udfa8 [demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/numberssummationrnn)\n* \ud83c\udfcb\ufe0f [training in jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/numbers_summation_rnn/numbers_summation_rnn.ipynb)\n* \ufe0f\ud83c\udfcb\ufe0f  [training in colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/numbers_summation_rnn/numbers_summation_rnn.ipynb)\n\n&amp;#x200b;\n\nhttps://i.redd.it/rix288wjuyw41.gif\n\n## shakespeare text generation\n\nyou start typing a poem like shakespeare, and the model will continue it like shakespeare. at least it will try to do so \ud83d\ude00.\n\n* \ud83c\udfa8 [demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/textgenerationshakespearernn)\n* \ud83c\udfcb\ufe0f [training in jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/text_generation_shakespeare_rnn/text_generation_shakespeare_rnn.ipynb)\n* \ufe0f\ud83c\udfcb\ufe0f  [training in colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/text_generation_shakespeare_rnn/text_generation_shakespeare_rnn.ipynb)\n\n&amp;#x200b;\n\nhttps://i.redd.it/nbkm3askuyw41.gif\n\n## wikipedia text generation\n\nyou start typing a wiki article, and the model tries to continue it.\n\n* \ud83c\udfa8 [demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/textgenerationwikipediarnn)\n* \ud83c\udfcb\ufe0f [training in jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/text_generation_wikipedia_rnn/text_generation_wikipedia_rnn.ipynb)\n* \ufe0f\ud83c\udfcb\ufe0f  [training in colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/text_generation_wikipedia_rnn/text_generation_wikipedia_rnn.ipynb)\n\n&amp;#x200b;\n\nhttps://i.redd.it/a0l2g1pluyw41.gif\n\n## future plans\n\nas i've mentioned above the main purpose of [the repository](https://github.com/trekhleb/machine-learning-experiments) is to be more like a playground for learning rather than for production-ready models. therefore, the main plan is to **continue learning and experimenting** with deep-learning challenges and approaches. the next interesting challenges to play with might be:\n\n* emotions detection\n* style transfer\n* language translation\n* generating images (i.e. handwritten numbers)\n* etc.\n\nanother interesting opportunity would be to **tune existing models to make them more performant**. i believe it might give a better understanding of how to overcome overfitting and underfitting and what to do with the model if it just stuck on `60%` accuracy level for both training and validation sets and doesn't want to improve anymore \ud83e\udd14.\n\nanyways, i hope you might find some useful insights for models training from [the repository](https://github.com/trekhleb/machine-learning-experiments) or at least to have some fun playing around with the demos!\n\nhappy learning! \ud83e\udd16", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/gdzv8h/interactive_machine_learning_experiments/',)", "identifyer": 5742708, "year": "2020"}, {"autor": "ZER_0_NE", "date": 1588657321000, "content": "Start implementing reasearch papers with maths /!/ I've been lately reading the mathematics behind ML algorithms and also learning the statistics and maths that goes with it and coding them along the way in my [github](https://github.com/ZER-0-NE/ML_problems/tree/master/ML-practice) repo.  \n\n\nI'm doing my best to understand these things but I want to get started on the side implementing some research papers. This can really help me understand (or at least get a start) on how to approach them.  \nHow did you get started with implementing research papers? What papers did you start with? Do you have some papers I could get started with as a beginner?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gds0tx/start_implementing_reasearch_papers_with_maths/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "start implementing reasearch papers with maths /!/ i've been lately reading the mathematics behind ml algorithms and also learning the statistics and maths that goes with it and coding them along the way in my [github](https://github.com/zer-0-ne/ml_problems/-----> tree !!! /master/ml-practice) repo.  \n\n\ni'm doing my best to understand these things but i want to get started on the side implementing some research papers. this can really help me understand (or at least get a start) on how to approach them.  \nhow did you get started with implementing research papers? what papers did you start with? do you have some papers i could get started with as a beginner?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 7, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/gds0tx/start_implementing_reasearch_papers_with_maths/',)", "identifyer": 5742732, "year": "2020"}, {"autor": "Medo3337", "date": 1588589275000, "content": "Machine Learning Automated CSV Preprocesing and Training /!/  Hello,\n\nI'm using sklearn and trying to automate training and data preprocessing for any CSV file (handling missing values and making the data ready for the model)\n\nI'm using panda and thinking about this approach:\n\n1. Load the data using pandas\n\n2. Use pandas fillna() to handle missing values:\n\ncsv\\_data.fillna(csv\\_data.mean(), inplace=True)\n\n3. Go through each column, if the column is not numeric value, apply sklearn LabelEncoder to that column, if it's numeric, leave it as it's\n\n&amp;#x200B;\n\n1. Create 7 instances of the same dataset and apply the following on each dataset:\n\nDataset Instance 1 use MinMaxScaler\n\nDataset Instance 2 use MaxAbsScaler \n\nDataset Instance 3 use StandardScaler\n\nDataset Instance 4 use RobustScaler\n\nDataset Instance 5 use Normalizer\n\nDataset Instance 6 use QuantileTransformer\n\nDataset Instance 7 use PowerTransformer\n\n&amp;#x200B;\n\n1. Finally, create 7 instances of the model, train the models based on the dataset instances, and evaluate against test data to determine which one is scoring the highest success.\n\nThe question is: Will this work any data loaded from CSV? Or I'm missing something? Do I need to tweak this for every model, assuming I'm training the following models: Logistic Regression, SVM, Decision Tree, Random Forest, KNearest, KMean Clustering, etc..", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gd9dfk/machine_learning_automated_csv_preprocesing_and/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "machine learning automated csv preprocesing and training /!/  hello,\n\ni'm using sklearn and trying to automate training and data preprocessing for any csv file (handling missing values and making the data ready for the model)\n\ni'm using panda and thinking about this approach:\n\n1. load the data using pandas\n\n2. use pandas fillna() to handle missing values:\n\ncsv\\_data.fillna(csv\\_data.mean(), inplace=true)\n\n3. go through each column, if the column is not numeric value, apply sklearn labelencoder to that column, if it's numeric, leave it as it's\n\n&amp;#x200b;\n\n1. create 7 instances of the same dataset and apply the following on each dataset:\n\ndataset instance 1 use minmaxscaler\n\ndataset instance 2 use maxabsscaler \n\ndataset instance 3 use standardscaler\n\ndataset instance 4 use robustscaler\n\ndataset instance 5 use normalizer\n\ndataset instance 6 use quantiletransformer\n\ndataset instance 7 use powertransformer\n\n&amp;#x200b;\n\n1. finally, create 7 instances of the model, train the models based on the dataset instances, and evaluate against test data to determine which one is scoring the highest success.\n\nthe question is: will this work any data loaded from csv? or i'm missing something? do i need to tweak this for every model, assuming i'm training the following models: logistic regression, svm, decision -----> tree !!! , random forest, knearest, kmean clustering, etc..", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/gd9dfk/machine_learning_automated_csv_preprocesing_and/',)", "identifyer": 5742775, "year": "2020"}, {"autor": "djleviathan", "date": 1588564645000, "content": "Creating dendrogram in python from directory tree data /!/ Hello,\n\n&amp;#x200B;\n\nI've got a directory containing several subdirectories that I'm trying to visualize as a dendrogram in python. I would like to also perform some hierarchical clustering to it; however, I'm having issues because some of the my data is text (i.e. the directory or subdirectory name), and this is my first foray into such clustering algorithms. Does anyone have any suggestions for how I can visualize my data? I've tried looking online but haven't been able to find anything that really helps.\n\n&amp;#x200B;\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gd4h63/creating_dendrogram_in_python_from_directory_tree/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "creating dendrogram in python from directory -----> tree !!!  data /!/ hello,\n\n&amp;#x200b;\n\ni've got a directory containing several subdirectories that i'm trying to visualize as a dendrogram in python. i would like to also perform some hierarchical clustering to it; however, i'm having issues because some of the my data is text (i.e. the directory or subdirectory name), and this is my first foray into such clustering algorithms. does anyone have any suggestions for how i can visualize my data? i've tried looking online but haven't been able to find anything that really helps.\n\n&amp;#x200b;\n\nthanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/gd4h63/creating_dendrogram_in_python_from_directory_tree/',)", "identifyer": 5742788, "year": "2020"}, {"autor": "InfinityCodeX", "date": 1590211221000, "content": "14 MOST ESSENTIAL Concepts About Decision Tree You Need To Know Right Now [BE A PRO] /!/ 14 MOST ESSENTIAL Concepts About Decision Tree You Need To Know Right Now \\[BE A PRO\\] ...Must check out this article \ud83e\udd2f\n\n[https://www.infinitycodex.in/14-most-essential-concepts-about](https://www.infinitycodex.in/14-most-essential-concepts-about)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/goze1w/14_most_essential_concepts_about_decision_tree/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "14 most essential concepts about decision -----> tree !!!  you need to know right now [be a pro] /!/ 14 most essential concepts about decision -----> tree !!!  you need to know right now \\[be a pro\\] ...must check out this article \ud83e\udd2f\n\n[https://www.infinitycodex.in/14-most-essential-concepts-about](https://www.infinitycodex.in/14-most-essential-concepts-about)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/goze1w/14_most_essential_concepts_about_decision_tree/',)", "identifyer": 5742789, "year": "2020"}, {"autor": "kuldeeprp", "date": 1590160905000, "content": "Model Evaluation /!/ Albert Einstein once said, \"if you judge a fish on its ability to climb a tree, it will live its whole life believing that it is stupid.\" This quote really highlights the importance of choosing the right evaluation metric.\n\nLets look out a some ways of Model Evaluation in this blog. \n\nhttps://highontechs.com/machine-learning/model-evaluation/\n\nDo subscribe to our blogs!\nAnd Join our telegram channel for daily such cool content\nhttp://t.me/highontechs", "link": "https://www.reddit.com/r/learnmachinelearning/comments/golalk/model_evaluation/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "model evaluation /!/ albert einstein once said, \"if you judge a fish on its ability to climb a -----> tree !!! , it will live its whole life believing that it is stupid.\" this quote really highlights the importance of choosing the right evaluation metric.\n\nlets look out a some ways of model evaluation in this blog. \n\nhttps://highontechs.com/machine-learning/model-evaluation/\n\ndo subscribe to our blogs!\nand join our telegram channel for daily such cool content\nhttp://t.me/highontechs", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/golalk/model_evaluation/',)", "identifyer": 5742809, "year": "2020"}, {"autor": "leooister", "date": 1590131624000, "content": "Learn ML using Python (Starcraft 2 &amp; Supervised learning) /!/ 1 months ago, I created and posted  videos from my machine  learning  course on the LearnPython  subreddit. So far they have  been  received very well! As the  resources is based on Machine learning using Python  coding I wanted to share it here as well  for those interrested.  Feedback is appreciated.\n\n**Supervised machine learning algorithm**\n\n* **Machine Learning Tutorial Part 1 | Machine Learning For Beginners**  \n\n   * [https://youtu.be/E3l\\_aeGjkeI](https://youtu.be/E3l_aeGjkeI)\n* **Machine Learning Tutorial Part 2 | Machine Learning For Beginners - MI environment**  \n\n   * [https://youtu.be/HqyrqxyDwPU](https://youtu.be/HqyrqxyDwPU)\n* **Machine Learning Tutorial Part 3 | Machine Learning For Beginners - Python Decision Tree**  \n\n   * [https://youtu.be/8isUCINSmys](https://youtu.be/8isUCINSmys)\n* **Machine Learning Tutorial Part 4 | Machine Learning For Beginners - Python Decision Tree**  \n\n   * [https://youtu.be/24mxQzd3EsU](https://youtu.be/24mxQzd3EsU)\n* **Machine Learning Tutorial Part 5 | Machine Learning For Beginners - Python Decision Tree**  \n\n   * [https://youtu.be/aVEfKRfWjHc](https://youtu.be/aVEfKRfWjHc)\n* **Machine Learning Tutorial Part 6 | Knn(Friend Recommender) For Beginners**  \n\n   * [https://youtu.be/LK0zgA6Mr6k](https://youtu.be/LK0zgA6Mr6k)\n* **Machine Learning Tutorial Part 7 | Machine Learning For Beginners - 5-Fold Cross Validation**  \n\n   * [https://youtu.be/Zx5cz8pXnOM](https://youtu.be/Zx5cz8pXnOM)\n* **Machine Learning Tutorial Part 8 | Accuracy(MAE/RMSE) - Python Machine Learning For Beginners**  \n\n   * [https://youtu.be/lHAEPyWNgyY](https://youtu.be/lHAEPyWNgyY)\n\n**SC2 - Deepmind**\n\n* **Machine Learning - StarCraft 2 Python AI part 1**  \n\n   * [**https://youtu.be/WFugpcvIil4**](https://youtu.be/WFugpcvIil4)\n* **Machine Learning - StarCraft 2 Python AI part 2 - Barracks**  \n\n   * [**https://youtu.be/O1fwJgjfh1w**](https://youtu.be/O1fwJgjfh1w)\n* **Machine Learning - StarCraft 2 Python AI part 3 - Build Marines**  \n\n   * [https://youtu.be/XzjLwMwhd\\_Y](https://youtu.be/XzjLwMwhd_Y)\n* **Machine Learning - StarCraft 2 Python AI part 4 - Attack**  \n\n   * [https://youtu.be/90m-itTP\\_Zo](https://youtu.be/90m-itTP_Zo)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/goells/learn_ml_using_python_starcraft_2_supervised/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "learn ml using python (starcraft 2 &amp; supervised learning) /!/ 1 months ago, i created and posted  videos from my machine  learning  course on the learnpython  subreddit. so far they have  been  received very well! as the  resources is based on machine learning using python  coding i wanted to share it here as well  for those interrested.  feedback is appreciated.\n\n**supervised machine learning algorithm**\n\n* **machine learning tutorial part 1 | machine learning for beginners**  \n\n   * [https://youtu.be/e3l\\_aegjkei](https://youtu.be/e3l_aegjkei)\n* **machine learning tutorial part 2 | machine learning for beginners - mi environment**  \n\n   * [https://youtu.be/hqyrqxydwpu](https://youtu.be/hqyrqxydwpu)\n* **machine learning tutorial part 3 | machine learning for beginners - python decision -----> tree !!! **  \n\n   * [https://youtu.be/8isucinsmys](https://youtu.be/8isucinsmys)\n* **machine learning tutorial part 4 | machine learning for beginners - python decision tree**  \n\n   * [https://youtu.be/24mxqzd3esu](https://youtu.be/24mxqzd3esu)\n* **machine learning tutorial part 5 | machine learning for beginners - python decision tree**  \n\n   * [https://youtu.be/avefkrfwjhc](https://youtu.be/avefkrfwjhc)\n* **machine learning tutorial part 6 | knn(friend recommender) for beginners**  \n\n   * [https://youtu.be/lk0zga6mr6k](https://youtu.be/lk0zga6mr6k)\n* **machine learning tutorial part 7 | machine learning for beginners - 5-fold cross validation**  \n\n   * [https://youtu.be/zx5cz8pxnom](https://youtu.be/zx5cz8pxnom)\n* **machine learning tutorial part 8 | accuracy(mae/rmse) - python machine learning for beginners**  \n\n   * [https://youtu.be/lhaepywngyy](https://youtu.be/lhaepywngyy)\n\n**sc2 - deepmind**\n\n* **machine learning - starcraft 2 python ai part 1**  \n\n   * [**https://youtu.be/wfugpcviil4**](https://youtu.be/wfugpcviil4)\n* **machine learning - starcraft 2 python ai part 2 - barracks**  \n\n   * [**https://youtu.be/o1fwjgjfh1w**](https://youtu.be/o1fwjgjfh1w)\n* **machine learning - starcraft 2 python ai part 3 - build marines**  \n\n   * [https://youtu.be/xzjlwmwhd\\_y](https://youtu.be/xzjlwmwhd_y)\n* **machine learning - starcraft 2 python ai part 4 - attack**  \n\n   * [https://youtu.be/90m-ittp\\_zo](https://youtu.be/90m-ittp_zo)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/goells/learn_ml_using_python_starcraft_2_supervised/',)", "identifyer": 5742826, "year": "2020"}, {"autor": "puracalidad13", "date": 1591817407000, "content": "Worsened classifier performance with increased amount of data /!/ Hey All,\n\nI have 10 datasets for a multi class classification task (same input features and targets). When I train and test on individual datasets with Random Forest and KNN, I get good results, classification accuracy over 90%. However, I tried to create a single dataset by merging the 10 datasets and then use the same classification algorithms, but their classification accuracy decreased a lot. I then tried increasing the number of estimators and maximum depth of each tree in random forest classifier and the accuracies increased, but it still lagged behind the accuracies in individual datasets. \n\nI am a bit confused. I thought the classifiers perform better with more data and I am not sure how to interpret the situation.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/h0i7a2/worsened_classifier_performance_with_increased/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "worsened classifier performance with increased amount of data /!/ hey all,\n\ni have 10 datasets for a multi class classification task (same input features and targets). when i train and test on individual datasets with random forest and knn, i get good results, classification accuracy over 90%. however, i tried to create a single dataset by merging the 10 datasets and then use the same classification algorithms, but their classification accuracy decreased a lot. i then tried increasing the number of estimators and maximum depth of each -----> tree !!!  in random forest classifier and the accuracies increased, but it still lagged behind the accuracies in individual datasets. \n\ni am a bit confused. i thought the classifiers perform better with more data and i am not sure how to interpret the situation.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/h0i7a2/worsened_classifier_performance_with_increased/',)", "identifyer": 5742919, "year": "2020"}, {"autor": "RLnobish", "date": 1594949252000, "content": "Can I apply AdaBoost on Random forest? /!/  I know the Random forest is a bagging technique. But what if my random forest overfits on a dataset, so I reduce the depth of the decision tree and now it underfitting. In this scenario, can I take the underfitted random forest with little depth and try to boost it?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hsmorq/can_i_apply_adaboost_on_random_forest/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "can i apply adaboost on random forest? /!/  i know the random forest is a bagging technique. but what if my random forest overfits on a dataset, so i reduce the depth of the decision -----> tree !!!  and now it underfitting. in this scenario, can i take the underfitted random forest with little depth and try to boost it?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/hsmorq/can_i_apply_adaboost_on_random_forest/',)", "identifyer": 5743020, "year": "2020"}, {"autor": "shreex7522", "date": 1598039377000, "content": "Steps used for making Decision Tree", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ie3vph/steps_used_for_making_decision_tree/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "steps used for making decision -----> tree !!! ", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('link',)", "medialink": "('https://www.asquero.com/article/steps-used-for-making-decision-tree/',)", "identifyer": 5743130, "year": "2020"}, {"autor": "Vaishali_Advani", "date": 1597996235000, "content": "Almost all Machine learning algorithms(except tree-based algos) work better with numerical data as compared to categorical Data. Here are some methods that can be used to convert your categorical values into numeric values.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/idsjzk/almost_all_machine_learning_algorithmsexcept/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "almost all machine learning algorithms(except -----> tree !!! -based algos) work better with numerical data as compared to categorical data. here are some methods that can be used to convert your categorical values into numeric values.", "sortedWord": "None", "removed": "('reddit',)", "score": 1, "comments": 0, "media": "('link',)", "medialink": "('https://www.mygreatlearning.com/blog/label-encoding-in-python/?utm_source=lencoding',)", "identifyer": 5743152, "year": "2020"}, {"autor": "Coprosmo", "date": 1597968271000, "content": "PyTorch Geometric: Replicating GNN model from paper, with complex MPNN setup /!/ Hi there,\n\nI'm relatively new to PyTorch Geometric (I've coded up one GNN so far, though have some experience working with PyTorch), and for some research I'm doing, I want to implement the message-passing scheme described on page 4 of this paper. It appears to be more complex than the examples I've seen.\n\nThe data is in a tree format, and for each neighbour of a node, the message-passing algorithm operates differently (see step 2 in the referenced paper) depending on whether the neighbour is the child or parent of the node (it passes the neighbour's embedding through one of two MLPs). I'm not sure how I could code my MPNN to recognize when a neighbor is a parent or a child - perhaps it could be encoded as a feature in the edge between them?\n\nThe second issue I had was that the parent and child embeddings of a node are aggregated separately - the mean of each is taken, and concatenated with the original node's embedding (step 3 of the paper). As far as I can see, PyG only allows a simpler aggregation method where all of the neighbors are considered at once.\n\nIf anyone has any thoughts on how these might be approached, I'd be very interested to hear them :)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/idm1mh/pytorch_geometric_replicating_gnn_model_from/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "pytorch geometric: replicating gnn model from paper, with complex mpnn setup /!/ hi there,\n\ni'm relatively new to pytorch geometric (i've coded up one gnn so far, though have some experience working with pytorch), and for some research i'm doing, i want to implement the message-passing scheme described on page 4 of this paper. it appears to be more complex than the examples i've seen.\n\nthe data is in a -----> tree !!!  format, and for each neighbour of a node, the message-passing algorithm operates differently (see step 2 in the referenced paper) depending on whether the neighbour is the child or parent of the node (it passes the neighbour's embedding through one of two mlps). i'm not sure how i could code my mpnn to recognize when a neighbor is a parent or a child - perhaps it could be encoded as a feature in the edge between them?\n\nthe second issue i had was that the parent and child embeddings of a node are aggregated separately - the mean of each is taken, and concatenated with the original node's embedding (step 3 of the paper). as far as i can see, pyg only allows a simpler aggregation method where all of the neighbors are considered at once.\n\nif anyone has any thoughts on how these might be approached, i'd be very interested to hear them :)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/idm1mh/pytorch_geometric_replicating_gnn_model_from/',)", "identifyer": 5743166, "year": "2020"}, {"autor": "Competitive_Mongoose", "date": 1597945470000, "content": "What kind of network will generate an image from just a single number? /!/ I want to start a project where the end result will be a network that you can put a number into and it will give an output image (2D matrix). I am trying to do this in Tensorflow/Python. I am planning on training this by giving the network the number and then an image in a 2D numpy array (eg 1 -&gt; Mona Lisa, 2 -&gt; Starry Night, 3 -&gt; a world map ...). I want to do this and see what the network generates when I put something like 1.5 or 2.31.\n\nI was wondering if anyone wiser than me could recommend a way forward to me.\n\nI have tried three ways so far. The first one is having a single number as an input and somehow outputting a 2D array (just black and white). I am a beginner and got completely stuck with the shapes of everything.\n\nThe second way is creating an array filled with the single number that is the same shape as the output array. Again here I am also getting a bit stuck with Tensorflow's dimension errors.\n\nThe third is to flatten the image and have an array of the number that is the same length as the image.  From what I have read online, this one is my best bet.\n\nI'm a beginner and trying to do my first independent project and I am not sure of things like what layers to use, what kind of network is best and how I can fix errors about shapes that I'm getting (I'm pretty sure I'm getting them because I'm barking up the wrong tree).\n\nCould anyone advise?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/idf0df/what_kind_of_network_will_generate_an_image_from/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "what kind of network will generate an image from just a single number? /!/ i want to start a project where the end result will be a network that you can put a number into and it will give an output image (2d matrix). i am trying to do this in tensorflow/python. i am planning on training this by giving the network the number and then an image in a 2d numpy array (eg 1 -&gt; mona lisa, 2 -&gt; starry night, 3 -&gt; a world map ...). i want to do this and see what the network generates when i put something like 1.5 or 2.31.\n\ni was wondering if anyone wiser than me could recommend a way forward to me.\n\ni have tried three ways so far. the first one is having a single number as an input and somehow outputting a 2d array (just black and white). i am a beginner and got completely stuck with the shapes of everything.\n\nthe second way is creating an array filled with the single number that is the same shape as the output array. again here i am also getting a bit stuck with tensorflow's dimension errors.\n\nthe third is to flatten the image and have an array of the number that is the same length as the image.  from what i have read online, this one is my best bet.\n\ni'm a beginner and trying to do my first independent project and i am not sure of things like what layers to use, what kind of network is best and how i can fix errors about shapes that i'm getting (i'm pretty sure i'm getting them because i'm barking up the wrong -----> tree !!! ).\n\ncould anyone advise?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/idf0df/what_kind_of_network_will_generate_an_image_from/',)", "identifyer": 5743177, "year": "2020"}, {"autor": "bioinfo_ml", "date": 1599571193000, "content": "Is it possible to normalise features by one specific feature? /!/  I have a dataset of genes with features that describe the genes at different scales (epigenetic, protein, cell, drug data etc. all numeric data). I use this dataset in supervised ML with a xgboost regression model scoring the genes between 0 to 1 (with 1 being most likely to cause a disease and 0 being least likely).\n\nHowever, genes of longer lengths are more likely to have more data only due to their gene size and not due to them truly being more causal to the disease.\n\nI am looking to regulate all my features by gene length, however I am not sure how to do this. By using xgboost I cannot normalize my features as tree-based models are invariant to monotone transformations.\n\nAre there any methods of regulating/normalizing features by another specific feature? I have also tried just including gene length as a feature for the ML model to use itself but this is a difficult blackbox to interpret if the model is truly considering gene length correlations with other features as it should.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/iotewm/is_it_possible_to_normalise_features_by_one/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "is it possible to normalise features by one specific feature? /!/  i have a dataset of genes with features that describe the genes at different scales (epigenetic, protein, cell, drug data etc. all numeric data). i use this dataset in supervised ml with a xgboost regression model scoring the genes between 0 to 1 (with 1 being most likely to cause a disease and 0 being least likely).\n\nhowever, genes of longer lengths are more likely to have more data only due to their gene size and not due to them truly being more causal to the disease.\n\ni am looking to regulate all my features by gene length, however i am not sure how to do this. by using xgboost i cannot normalize my features as -----> tree !!! -based models are invariant to monotone transformations.\n\nare there any methods of regulating/normalizing features by another specific feature? i have also tried just including gene length as a feature for the ml model to use itself but this is a difficult blackbox to interpret if the model is truly considering gene length correlations with other features as it should.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/iotewm/is_it_possible_to_normalise_features_by_one/',)", "identifyer": 5743218, "year": "2020"}, {"autor": "Janji98", "date": 1599479519000, "content": "Decision tree building algorithm (ID3): Problem printing desired results /!/ [removed]", "link": "https://www.reddit.com/r/learnmachinelearning/comments/io62rw/decision_tree_building_algorithm_id3_problem/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "decision -----> tree !!!  building algorithm (id3): problem printing desired results /!/ [removed]", "sortedWord": "None", "removed": "('reddit',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/io62rw/decision_tree_building_algorithm_id3_problem/',)", "identifyer": 5743251, "year": "2020"}, {"autor": "Janji98", "date": 1599400917000, "content": "Decision tree learning ID3 /!/ [removed]", "link": "https://www.reddit.com/r/learnmachinelearning/comments/inmhqs/decision_tree_learning_id3/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "decision -----> tree !!!  learning id3 /!/ [removed]", "sortedWord": "None", "removed": "('reddit',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/inmhqs/decision_tree_learning_id3/',)", "identifyer": 5743265, "year": "2020"}, {"autor": "jaienabzanx", "date": 1599398257000, "content": "Decision Tree Error /!/ I gave it a one-line function still it says expecting an indented block.\n\nAlso, I am trying to implement ML algorithms with raw code and as well as scikit learn. But it takes a lot of time when I try to do it in raw code because I run into errors all the time. Should I continue doing that or just do it only using scikit learn.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/inlub3/decision_tree_error/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "decision -----> tree !!!  error /!/ i gave it a one-line function still it says expecting an indented block.\n\nalso, i am trying to implement ml algorithms with raw code and as well as scikit learn. but it takes a lot of time when i try to do it in raw code because i run into errors all the time. should i continue doing that or just do it only using scikit learn.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/inlub3/decision_tree_error/',)", "identifyer": 5743266, "year": "2020"}, {"autor": "RubiksCodeNMZ", "date": 1601277696000, "content": "Back to Machine Learning Basics - Decision Tree &amp; Random Forest", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j17kwq/back_to_machine_learning_basics_decision_tree/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "back to machine learning basics - decision -----> tree !!!  &amp; random forest", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('link',)", "medialink": "('https://rubikscode.net/2020/09/28/back-to-machine-learning-basics-decision-tree-random-forest/',)", "identifyer": 5743295, "year": "2020"}, {"autor": "madhuwhatever", "date": 1601241167000, "content": "Implementing Decision Tree learning algorithm from Scratch in Python", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j0yzro/implementing_decision_tree_learning_algorithm/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "implementing decision -----> tree !!!  learning algorithm from scratch in python", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('link',)", "medialink": "('http://madhugnadig.com/articles/machine-learning/2017/05/01/implementing-decision-trees-from-scratch-in-python.html',)", "identifyer": 5743305, "year": "2020"}, {"autor": "madhuwhatever", "date": 1601241058000, "content": "Implementing Decision Tree algorithm from scratch in Python", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j0yyn9/implementing_decision_tree_algorithm_from_scratch/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "implementing decision -----> tree !!!  algorithm from scratch in python", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('http://madhugnadig.com/articles/2017/05/01/implementing-decision-trees-from-scratch-in-python.html',)", "identifyer": 5743306, "year": "2020"}, {"autor": "Muted_Dot", "date": 1601151747000, "content": "One hot encoding (simple, possibly stupid question) /!/ Hey guys, I'm going through a coursera course on machine learning and for one assignment I have to build a classifier. So far I've only learned technical knowledge tbh and my practical skills are somewhat nonexistent. Anyway here goes my question: I have categorical data and in order to use that data in a decision tree first I have to represent those strings numerically. So far this makes sense to me. However what doesn't make sense is that when I use pandas.get\\_dummies method for creating one hot encoding of strings, I get this\n\n|index|x|y|\n|:-|:-|:-|\n|1.|0|1|\n|2.|1|0|\n\nbut I thought I should've gotten something like this\n\n|index|encoding|\n|:-|:-|\n|1.|01|\n|2.|10|\n\nSurely it's not impossible to transform the one I get to this, however I wonder if I misunderstood some part of this. As far as I understood it, the encoding representation will be given to the classifier as a whole. Am I wrong about this? If someone could clarify it, it would really be of great help.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j0d3xn/one_hot_encoding_simple_possibly_stupid_question/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "one hot encoding (simple, possibly stupid question) /!/ hey guys, i'm going through a coursera course on machine learning and for one assignment i have to build a classifier. so far i've only learned technical knowledge tbh and my practical skills are somewhat nonexistent. anyway here goes my question: i have categorical data and in order to use that data in a decision -----> tree !!!  first i have to represent those strings numerically. so far this makes sense to me. however what doesn't make sense is that when i use pandas.get\\_dummies method for creating one hot encoding of strings, i get this\n\n|index|x|y|\n|:-|:-|:-|\n|1.|0|1|\n|2.|1|0|\n\nbut i thought i should've gotten something like this\n\n|index|encoding|\n|:-|:-|\n|1.|01|\n|2.|10|\n\nsurely it's not impossible to transform the one i get to this, however i wonder if i misunderstood some part of this. as far as i understood it, the encoding representation will be given to the classifier as a whole. am i wrong about this? if someone could clarify it, it would really be of great help.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/j0d3xn/one_hot_encoding_simple_possibly_stupid_question/',)", "identifyer": 5743355, "year": "2020"}, {"autor": "laksansan", "date": 1601068887000, "content": "Here's an overview of Weka's decision tree algorithms and their differences. This might be a good introduction to start creating strong baselines for machine learning projects.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/izsbtg/heres_an_overview_of_wekas_decision_tree/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "here's an overview of weka's decision -----> tree !!!  algorithms and their differences. this might be a good introduction to start creating strong baselines for machine learning projects.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('link',)", "medialink": "('https://biasedml.com/weka-trees/',)", "identifyer": 5743380, "year": "2020"}, {"autor": "dhiraj8899", "date": 1602689114000, "content": "Learn Machine Learning Algorithms from Scratch in Python like Neural Network, Decision Tree, Logistic Regression,", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jb2r7h/learn_machine_learning_algorithms_from_scratch_in/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "learn machine learning algorithms from scratch in python like neural network, decision -----> tree !!! , logistic regression,", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('link',)", "medialink": "('https://www.youtube.com/channel/UCuOT2b1Umrr0MittMzuxNcA?sub_confirmation=1',)", "identifyer": 5743505, "year": "2020"}, {"autor": "thatascience", "date": 1602603110000, "content": "Understanding the Gini Index and Information Gain in Decision Trees | Explained with code examples /!/ # [https://thatascience.com/learn-machine-learning/gini-entropy/](https://thatascience.com/learn-machine-learning/gini-entropy/)\n\n# Gini index and entropy are the criteria for calculating information gain. Decision tree algorithms use information gain to split a node.\n\nBoth gini and entropy are measures of impurity of a node. A node having multiple classes is impure whereas a node having only one class is pure.\u00a0 Entropy in statistics is analogous to entropy in thermodynamics where it signifies disorder. If there are multiple classes in a node, there is disorder in that node.\u00a0 This tutorial explains in detail about the two mathematical entities. Also compares the effect of choosing them as spliting criterion in decision trees.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jafrbe/understanding_the_gini_index_and_information_gain/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "understanding the gini index and information gain in decision trees | explained with code examples /!/ # [https://thatascience.com/learn-machine-learning/gini-entropy/](https://thatascience.com/learn-machine-learning/gini-entropy/)\n\n# gini index and entropy are the criteria for calculating information gain. decision -----> tree !!!  algorithms use information gain to split a node.\n\nboth gini and entropy are measures of impurity of a node. a node having multiple classes is impure whereas a node having only one class is pure.\u00a0 entropy in statistics is analogous to entropy in thermodynamics where it signifies disorder. if there are multiple classes in a node, there is disorder in that node.\u00a0 this tutorial explains in detail about the two mathematical entities. also compares the effect of choosing them as spliting criterion in decision trees.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/jafrbe/understanding_the_gini_index_and_information_gain/',)", "identifyer": 5743537, "year": "2020"}, {"autor": "lukescriptwalker", "date": 1579676943000, "content": "Moving Into ML - 2020 reference list /!/ If some should be in need of ressourcers to get started within ML this year, I have listed some of my resources with focuses on Game AI's and Supervised Algorithms below:\n\n**Supervised machine learning algorithm**\n\n* **Machine Learning Tutorial Part 1 | Machine Learning For Beginners**  \n\n   * [https://youtu.be/E3l\\_aeGjkeI](https://youtu.be/E3l_aeGjkeI)\n* **Machine Learning Tutorial Part 2 | Machine Learning For Beginners - MI environment**  \n\n   * [https://youtu.be/HqyrqxyDwPU](https://youtu.be/HqyrqxyDwPU)\n* **Machine Learning Tutorial Part 3 | Machine Learning For Beginners - Python Decision Tree**  \n\n   * [https://youtu.be/8isUCINSmys](https://youtu.be/8isUCINSmys)\n* **Machine Learning Tutorial Part 4 | Machine Learning For Beginners - Python Decision Tree**  \n\n   * [https://youtu.be/24mxQzd3EsU](https://youtu.be/24mxQzd3EsU)\n* **Machine Learning Tutorial Part 5 | Machine Learning For Beginners - Python Decision Tree**  \n\n   * [https://youtu.be/aVEfKRfWjHc](https://youtu.be/aVEfKRfWjHc)\n* **Machine Learning Tutorial Part 6 | Knn(Friend Recommender) For Beginners**  \n\n   * [https://youtu.be/LK0zgA6Mr6k](https://youtu.be/LK0zgA6Mr6k)\n* **Machine Learning Tutorial Part 7 | Machine Learning For Beginners - 5-Fold Cross Validation**  \n\n   * [https://youtu.be/Zx5cz8pXnOM](https://youtu.be/Zx5cz8pXnOM)\n* **Machine Learning Tutorial Part 8 | Accuracy(MAE/RMSE) - Python Machine Learning For Beginners**  \n\n   * [https://youtu.be/lHAEPyWNgyY](https://youtu.be/lHAEPyWNgyY)\n\n&amp;#x200B;\n\n**SC2 - Deepmind**\n\n* **Machine Learning - StarCraft 2 Python AI part 1**  \n\n   * [**https://youtu.be/WFugpcvIil4**](https://youtu.be/WFugpcvIil4)\n* **Machine Learning - StarCraft 2 Python AI part 2 - Barracks**  \n\n   * [**https://youtu.be/O1fwJgjfh1w**](https://youtu.be/O1fwJgjfh1w)\n* **Machine Learning - StarCraft 2 Python AI part 3 - Build Marines**  \n\n   * [https://youtu.be/XzjLwMwhd\\_Y](https://youtu.be/XzjLwMwhd_Y)\n* **Machine Learning - StarCraft 2 Python AI part 4 - Attack**  \n\n   * [https://youtu.be/90m-itTP\\_Zo](https://youtu.be/90m-itTP_Zo)\n\n&amp;#x200B;\n\n**Intro to NN**\n\n* **Neural Networks (Deep Learning) - Part 1 Introduction**  \n\n   * [**https://youtu.be/SG9FYMKIu68**](https://youtu.be/SG9FYMKIu68)\n* **Neural Networks (Deep Learning) - Part 2 Types of NN**  \n\n   * [https://youtu.be/hp-wGZ4-CpU](https://youtu.be/hp-wGZ4-CpU)\n* **Neural Networks (Deep Learning) - Part 3 - Neurons &amp; Logistic Regression**  \n\n   * [https://youtu.be/mb-a9m5Yrzo](https://youtu.be/mb-a9m5Yrzo)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/es8gu5/moving_into_ml_2020_reference_list/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "moving into ml - 2020 reference list /!/ if some should be in need of ressourcers to get started within ml this year, i have listed some of my resources with focuses on game ai's and supervised algorithms below:\n\n**supervised machine learning algorithm**\n\n* **machine learning tutorial part 1 | machine learning for beginners**  \n\n   * [https://youtu.be/e3l\\_aegjkei](https://youtu.be/e3l_aegjkei)\n* **machine learning tutorial part 2 | machine learning for beginners - mi environment**  \n\n   * [https://youtu.be/hqyrqxydwpu](https://youtu.be/hqyrqxydwpu)\n* **machine learning tutorial part 3 | machine learning for beginners - python decision -----> tree !!! **  \n\n   * [https://youtu.be/8isucinsmys](https://youtu.be/8isucinsmys)\n* **machine learning tutorial part 4 | machine learning for beginners - python decision tree**  \n\n   * [https://youtu.be/24mxqzd3esu](https://youtu.be/24mxqzd3esu)\n* **machine learning tutorial part 5 | machine learning for beginners - python decision tree**  \n\n   * [https://youtu.be/avefkrfwjhc](https://youtu.be/avefkrfwjhc)\n* **machine learning tutorial part 6 | knn(friend recommender) for beginners**  \n\n   * [https://youtu.be/lk0zga6mr6k](https://youtu.be/lk0zga6mr6k)\n* **machine learning tutorial part 7 | machine learning for beginners - 5-fold cross validation**  \n\n   * [https://youtu.be/zx5cz8pxnom](https://youtu.be/zx5cz8pxnom)\n* **machine learning tutorial part 8 | accuracy(mae/rmse) - python machine learning for beginners**  \n\n   * [https://youtu.be/lhaepywngyy](https://youtu.be/lhaepywngyy)\n\n&amp;#x200b;\n\n**sc2 - deepmind**\n\n* **machine learning - starcraft 2 python ai part 1**  \n\n   * [**https://youtu.be/wfugpcviil4**](https://youtu.be/wfugpcviil4)\n* **machine learning - starcraft 2 python ai part 2 - barracks**  \n\n   * [**https://youtu.be/o1fwjgjfh1w**](https://youtu.be/o1fwjgjfh1w)\n* **machine learning - starcraft 2 python ai part 3 - build marines**  \n\n   * [https://youtu.be/xzjlwmwhd\\_y](https://youtu.be/xzjlwmwhd_y)\n* **machine learning - starcraft 2 python ai part 4 - attack**  \n\n   * [https://youtu.be/90m-ittp\\_zo](https://youtu.be/90m-ittp_zo)\n\n&amp;#x200b;\n\n**intro to nn**\n\n* **neural networks (deep learning) - part 1 introduction**  \n\n   * [**https://youtu.be/sg9fymkiu68**](https://youtu.be/sg9fymkiu68)\n* **neural networks (deep learning) - part 2 types of nn**  \n\n   * [https://youtu.be/hp-wgz4-cpu](https://youtu.be/hp-wgz4-cpu)\n* **neural networks (deep learning) - part 3 - neurons &amp; logistic regression**  \n\n   * [https://youtu.be/mb-a9m5yrzo](https://youtu.be/mb-a9m5yrzo)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/es8gu5/moving_into_ml_2020_reference_list/',)", "identifyer": 5743635, "year": "2020"}, {"autor": "wstcpyt1988", "date": 1579664880000, "content": "Intuitive Machine Learning (4) - Decision Tree Classifiers, original post: https://soo.nr/Fc4y", "link": "https://www.reddit.com/r/learnmachinelearning/comments/es6d9z/intuitive_machine_learning_4_decision_tree/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "intuitive machine learning (4) - decision -----> tree !!!  classifiers, original post: https://soo.nr/fc4y", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('hosted:video',)", "medialink": "('https://v.redd.it/rtam5x0259c41',)", "identifyer": 5743639, "year": "2020"}, {"autor": "wstcpyt1988", "date": 1579664740000, "content": "Intuitive Machine Learning - Decision Tree Classifiers /!/ [Original Slide Post](https://www.instagram.com/p/B7lisOCJ_S7/?utm_source=ig_web_copy_link)\n\n![video](aw59dqtk49c41)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/es6cc1/intuitive_machine_learning_decision_tree/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "intuitive machine learning - decision -----> tree !!!  classifiers /!/ [original slide post](https://www.instagram.com/p/b7lisocj_s7/?utm_source=ig_web_copy_link)\n\n![video](aw59dqtk49c41)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/es6cc1/intuitive_machine_learning_decision_tree/',)", "identifyer": 5743640, "year": "2020"}, {"autor": "wstcpyt1988", "date": 1579664246000, "content": "Intuitive Machine Learning - Decision Tree Classifiers https://soo.nr/Fc4y", "link": "https://www.reddit.com/r/learnmachinelearning/comments/es68tk/intuitive_machine_learning_decision_tree/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "intuitive machine learning - decision -----> tree !!!  classifiers https://soo.nr/fc4y", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('hosted:video',)", "medialink": "('https://v.redd.it/2qr9rjec29c41',)", "identifyer": 5743641, "year": "2020"}, {"autor": "m1thr", "date": 1580294263000, "content": "How to chose proper algorithm for event correlation /!/ Hello, beginner here. I need second thoughts on problem I have. \nI have a set of network traffic in form of a log in particular format. Using binary classificator (svn) I was able to create anomaly detection (I had training set as well). \n\nI would like to use similar dataset to create user profile model but I can\u2019t decide which algorithm should I use.\nMy first thoughts was multi class clasificator (k-nn or decision tree) where each label would be particular user but more as I think of that usage of agglomerative clustering should fit here as well where each cluster would represent particular user. \n\nCould anyone give me a hint which approach would be better in this scenario?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/evl8uk/how_to_chose_proper_algorithm_for_event/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "how to chose proper algorithm for event correlation /!/ hello, beginner here. i need second thoughts on problem i have. \ni have a set of network traffic in form of a log in particular format. using binary classificator (svn) i was able to create anomaly detection (i had training set as well). \n\ni would like to use similar dataset to create user profile model but i can\u2019t decide which algorithm should i use.\nmy first thoughts was multi class clasificator (k-nn or decision -----> tree !!! ) where each label would be particular user but more as i think of that usage of agglomerative clustering should fit here as well where each cluster would represent particular user. \n\ncould anyone give me a hint which approach would be better in this scenario?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/evl8uk/how_to_chose_proper_algorithm_for_event/',)", "identifyer": 5743720, "year": "2020"}, {"autor": "eva10898", "date": 1580191965000, "content": "Is it possible to use a custom-defined decision tree classifier in Scikit-learn? /!/ I have a predefined decision tree, which I built from knowledge-based splits, that I want to use to make predictions. I could try to implement a decision tree classifier from scratch, but then I would not be able to use build in Scikit functions like predict. Is there a way to convert my tree in pmml and import this pmml to make my prediction with scikit-learn? Or do I need to do something completely different? My first attempt was to use \u201cfake training data\u201d to force the algorithm to build the tree the way I like it, this would end up in a lot of work because I need to create different trees depending on the user input.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ev1n3r/is_it_possible_to_use_a_customdefined_decision/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "is it possible to use a custom-defined decision -----> tree !!!  classifier in scikit-learn? /!/ i have a predefined decision tree, which i built from knowledge-based splits, that i want to use to make predictions. i could try to implement a decision tree classifier from scratch, but then i would not be able to use build in scikit functions like predict. is there a way to convert my tree in pmml and import this pmml to make my prediction with scikit-learn? or do i need to do something completely different? my first attempt was to use \u201cfake training data\u201d to force the algorithm to build the tree the way i like it, this would end up in a lot of work because i need to create different trees depending on the user input.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/ev1n3r/is_it_possible_to_use_a_customdefined_decision/',)", "identifyer": 5743769, "year": "2020"}, {"autor": "AggravatingOrder", "date": 1578246130000, "content": "ML / DS skill tree? /!/ Hola, recently started learning ML (Ng, Coursera). Reading over recent posts, wonder if a skill tree like Skyrim or such would help newbies. Likely would be more than one tree, because some skills are common, but the way they're combined differs for the end goal.\n\nFor example, data science and ML both use some common concepts, but combining these would result in an illegible mess. Even within ML, there's a difference in skills for an engineer vs model builder vs product manager.\n\nHas anyone attempted this? Would it make sense / be useful?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ekg6yo/ml_ds_skill_tree/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "ml / ds skill -----> tree !!! ? /!/ hola, recently started learning ml (ng, coursera). reading over recent posts, wonder if a skill tree like skyrim or such would help newbies. likely would be more than one tree, because some skills are common, but the way they're combined differs for the end goal.\n\nfor example, data science and ml both use some common concepts, but combining these would result in an illegible mess. even within ml, there's a difference in skills for an engineer vs model builder vs product manager.\n\nhas anyone attempted this? would it make sense / be useful?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/ekg6yo/ml_ds_skill_tree/',)", "identifyer": 5743828, "year": "2020"}, {"autor": "Stack3", "date": 1578116180000, "content": "How do you define the space of all possible models that describe the data rather than producing one model that describes the data? (Details inside) /!/ Sorry for the noob question, I'm just trying to understand this conceptually.\n\nSo, I can use a neutral net or decision tree or something and train a model on some data, say mnist, or whatever. By training a model with some algorithm I basically produce a function of inputs to desired output (the classification model).\n\nBut are there any algorithms that don't produce one model, but instead produce a description of the space of all possible models that describe the data?\n\nDoes that make sense? In some way, as near as I can tell, a model represents a set of priors (biases).\n\nTake the following riddle as a conceptual example, '''A man and his son are in a terrible accident and are rushed to the hospital in critical care. The doctor looks at the boy and exclaims \"I can't operate on this boy, he's my son!\" How could this be?'''\n\nYou might have a prior that says, \"most surgeons are men\" and another prior that says, \"most parents are heterosexual.\" So you have a space of two models that would explain the data: either the boy has two fathers, or the surgeon is his mother.\n\nThe key is, both of these models would work to explain the data and you don't know which model is true until you get more data. Also, you can map each model's relationship to your prior beliefs so when you learn which one is valid you can update you're priors accordingly.\n\nAnyway, I know this was long, but are there any algorithms or machine learning techniques that do this? And furthermore how do you even go about mathematically describing a possibility space of all possible valid models according to their likelihood, according to your priors?\n\nThanks for your help!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ejsfy4/how_do_you_define_the_space_of_all_possible/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "how do you define the space of all possible models that describe the data rather than producing one model that describes the data? (details inside) /!/ sorry for the noob question, i'm just trying to understand this conceptually.\n\nso, i can use a neutral net or decision -----> tree !!!  or something and train a model on some data, say mnist, or whatever. by training a model with some algorithm i basically produce a function of inputs to desired output (the classification model).\n\nbut are there any algorithms that don't produce one model, but instead produce a description of the space of all possible models that describe the data?\n\ndoes that make sense? in some way, as near as i can tell, a model represents a set of priors (biases).\n\ntake the following riddle as a conceptual example, '''a man and his son are in a terrible accident and are rushed to the hospital in critical care. the doctor looks at the boy and exclaims \"i can't operate on this boy, he's my son!\" how could this be?'''\n\nyou might have a prior that says, \"most surgeons are men\" and another prior that says, \"most parents are heterosexual.\" so you have a space of two models that would explain the data: either the boy has two fathers, or the surgeon is his mother.\n\nthe key is, both of these models would work to explain the data and you don't know which model is true until you get more data. also, you can map each model's relationship to your prior beliefs so when you learn which one is valid you can update you're priors accordingly.\n\nanyway, i know this was long, but are there any algorithms or machine learning techniques that do this? and furthermore how do you even go about mathematically describing a possibility space of all possible valid models according to their likelihood, according to your priors?\n\nthanks for your help!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/ejsfy4/how_do_you_define_the_space_of_all_possible/',)", "identifyer": 5743862, "year": "2020"}, {"autor": "BotBark", "date": 1578085065000, "content": "Visualize or Print Decision Tree Algorithm Model", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ejlmri/visualize_or_print_decision_tree_algorithm_model/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "visualize or print decision -----> tree !!!  algorithm model", "sortedWord": "None", "removed": "('nan',)", "score": 2, "comments": 0, "media": "('link',)", "medialink": "('https://botbark.com/2020/01/03/visualize-or-print-decision-tree-algorithm-model/',)", "identifyer": 5743867, "year": "2020"}, {"autor": "rangerranvir", "date": 1586106811000, "content": "A practical approach to Tree Pruning using sklearn", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fvhaip/a_practical_approach_to_tree_pruning_using_sklearn/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "a practical approach to -----> tree !!!  pruning using sklearn", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('link',)", "medialink": "('https://ranvir.xyz/blog/practical-approach-to-tree-pruning-using-sklearn/',)", "identifyer": 5743945, "year": "2020"}, {"autor": "PythonReddit1", "date": 1586854380000, "content": "A machine learning based DDOS detection program using a DescisionTreeClassifier. /!/ Hello,\n\nMy first machine learning program analyzes packet data from packet files (pcap,pcapng) and determines if they are from a Denial of Service attack or not. I found applying machine learning to networking a very interesting experience. I used sk-learn for my model and I visualized the tree using Graphiz.\n\nI wrote a how-to guide for the code, which explains what each function is used for as well as a video version:  [https://www.youtube.com/watch?v=6VqTpga0\\_j4&amp;t=1s](https://www.youtube.com/watch?v=6VqTpga0_j4&amp;t=1s) \n\n&amp;#x200B;\n\nHere is a GitHub link to the project: [https://github.com/bcheevers/RaspPiPacketMachineLearning](https://github.com/bcheevers/RaspPiPacketMachineLearning) \n\n&amp;#x200B;\n\n&amp;#x200B;\n\n[Decision Tree Produced](https://preview.redd.it/rtmelvfvyqs41.png?width=772&amp;format=png&amp;auto=webp&amp;s=3298b4ca1f2a98e82fdebc488cef748b8eb9e874)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g121v9/a_machine_learning_based_ddos_detection_program/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "a machine learning based ddos detection program using a descisiontreeclassifier. /!/ hello,\n\nmy first machine learning program analyzes packet data from packet files (pcap,pcapng) and determines if they are from a denial of service attack or not. i found applying machine learning to networking a very interesting experience. i used sk-learn for my model and i visualized the -----> tree !!!  using graphiz.\n\ni wrote a how-to guide for the code, which explains what each function is used for as well as a video version:  [https://www.youtube.com/watch?v=6vqtpga0\\_j4&amp;t=1s](https://www.youtube.com/watch?v=6vqtpga0_j4&amp;t=1s) \n\n&amp;#x200b;\n\nhere is a github link to the project: [https://github.com/bcheevers/rasppipacketmachinelearning](https://github.com/bcheevers/rasppipacketmachinelearning) \n\n&amp;#x200b;\n\n&amp;#x200b;\n\n[decision tree produced](https://preview.redd.it/rtmelvfvyqs41.png?width=772&amp;format=png&amp;auto=webp&amp;s=3298b4ca1f2a98e82fdebc488cef748b8eb9e874)", "sortedWord": "None", "removed": "('nan',)", "score": 2, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/g121v9/a_machine_learning_based_ddos_detection_program/',)", "identifyer": 5743992, "year": "2020"}, {"autor": "hacknomus", "date": 1605875179000, "content": "#125 Building a Regression Tree in R", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jxo4mu/125_building_a_regression_tree_in_r/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "#125 building a regression -----> tree !!!  in r", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('rich:video',)", "medialink": "('https://youtu.be/OOppSPpvyK4',)", "identifyer": 5744103, "year": "2020"}, {"autor": "PengyDesu", "date": 1605756181000, "content": "Why decision trees over other machine learning methods. /!/ As the title says, I\u2019m just curious to know the main reasons why one would use a decision tree vs use other machine learning methods.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jwv15n/why_decision_trees_over_other_machine_learning/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "why decision trees over other machine learning methods. /!/ as the title says, i\u2019m just curious to know the main reasons why one would use a decision -----> tree !!!  vs use other machine learning methods.", "sortedWord": "None", "removed": "('nan',)", "score": 4, "comments": 9, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/jwv15n/why_decision_trees_over_other_machine_learning/',)", "identifyer": 5744163, "year": "2020"}, {"autor": "Stack3", "date": 1578922248000, "content": "Decision tree model creation /!/ I have a question about how decision trees produce models.\n\nWhen you take a decision tree does it retain the structure of the model as it trains such that it never trains a model such that it would make a bad prediction on a previously seen observation?\n\nIn other words, does it naturally overfit?\n\nLet me explain, if a neural network is trained, using back prop, it may see an observation early on and make the right prediction, but as it sees more observations and trains its model it may modify the model structure such that if it sees that particular observation again it will miscategorize it. See, the back prop training process makes it better on average, that is globally, but can degrade it locally.\n\nI'm looking for a model that retains structure such that it doesn't degrade locally during training.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/eo44qf/decision_tree_model_creation/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "decision -----> tree !!!  model creation /!/ i have a question about how decision trees produce models.\n\nwhen you take a decision tree does it retain the structure of the model as it trains such that it never trains a model such that it would make a bad prediction on a previously seen observation?\n\nin other words, does it naturally overfit?\n\nlet me explain, if a neural network is trained, using back prop, it may see an observation early on and make the right prediction, but as it sees more observations and trains its model it may modify the model structure such that if it sees that particular observation again it will miscategorize it. see, the back prop training process makes it better on average, that is globally, but can degrade it locally.\n\ni'm looking for a model that retains structure such that it doesn't degrade locally during training.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/eo44qf/decision_tree_model_creation/',)", "identifyer": 5744196, "year": "2020"}, {"autor": "19Summer", "date": 1578741659000, "content": "ELI5:How does feature importance is calculated in Decision tree algorithm? /!/ Hi,everyone.\nI read an article which explained to me how decision trees work and how impurity criterions are calculated.\nHere\u2019s the link-&gt;&gt;&gt;[KDnuggets,Decision trees](https://www.kdnuggets.com/2019/08/understanding-decision-trees-classification-python.html)\nIt mentions feature importance, but does bot elaborate on how it is calculated. It only says that it is normalised to have a sum of 1.\n\nI searched the Internet but did not find anything that was comprehensible to me.\n\nSo maybe anyone has a link to a good article which explains feature importance calculation in a friendly way with examples?\n\nThanks in advance.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/en6ark/eli5how_does_feature_importance_is_calculated_in/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "eli5:how does feature importance is calculated in decision -----> tree !!!  algorithm? /!/ hi,everyone.\ni read an article which explained to me how decision trees work and how impurity criterions are calculated.\nhere\u2019s the link-&gt;&gt;&gt;[kdnuggets,decision trees](https://www.kdnuggets.com/2019/08/understanding-decision-trees-classification-python.html)\nit mentions feature importance, but does bot elaborate on how it is calculated. it only says that it is normalised to have a sum of 1.\n\ni searched the internet but did not find anything that was comprehensible to me.\n\nso maybe anyone has a link to a good article which explains feature importance calculation in a friendly way with examples?\n\nthanks in advance.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/en6ark/eli5how_does_feature_importance_is_calculated_in/',)", "identifyer": 5744239, "year": "2020"}, {"autor": "lukescriptwalker", "date": 1582737129000, "content": "Machine Learning Tutorial Part 4 | Machine Learning For Beginners - Python Decision Tree", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f9whki/machine_learning_tutorial_part_4_machine_learning/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "machine learning tutorial part 4 | machine learning for beginners - python decision -----> tree !!! ", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('rich:video',)", "medialink": "('https://youtu.be/24mxQzd3EsU',)", "identifyer": 5744386, "year": "2020"}, {"autor": "ashyjoints", "date": 1593329315000, "content": "Building a tree height database: is it good practice to use the average of a max and min range? /!/ Beginner to ML and statistics in general, so please be gentle.\n\nI'm building a tree height database by scraping webpages related to 1500 trees in order to get height data, to add into an ML model as training input.\n\nThe intention of the model on the whole is for a user to input the desired characteristics of a tree, and for the model to return a tree species.\n\nThe results of scraping usually gives me ranges like Tree Species X 6-10m etc., and for it to be an input I figure it should be one number...so im taking the average.\n\nI'm aware tree species X might be most commonly 7 or 9 m, but I'm trying to work with the data I can... I don't see another option.\n\nWhat is usually done in this case?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hhac90/building_a_tree_height_database_is_it_good/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "building a -----> tree !!!  height database: is it good practice to use the average of a max and min range? /!/ beginner to ml and statistics in general, so please be gentle.\n\ni'm building a tree height database by scraping webpages related to 1500 trees in order to get height data, to add into an ml model as training input.\n\nthe intention of the model on the whole is for a user to input the desired characteristics of a tree, and for the model to return a tree species.\n\nthe results of scraping usually gives me ranges like tree species x 6-10m etc., and for it to be an input i figure it should be one number...so im taking the average.\n\ni'm aware tree species x might be most commonly 7 or 9 m, but i'm trying to work with the data i can... i don't see another option.\n\nwhat is usually done in this case?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/hhac90/building_a_tree_height_database_is_it_good/',)", "identifyer": 5744418, "year": "2020"}, {"autor": "leooister", "date": 1583477890000, "content": "Machine Learning Tutorial Part 3 | Machine Learning For Beginners - Python Decision Tree", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fe9rbm/machine_learning_tutorial_part_3_machine_learning/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "machine learning tutorial part 3 | machine learning for beginners - python decision -----> tree !!! ", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://youtu.be/8isUCINSmys',)", "identifyer": 5744517, "year": "2020"}, {"autor": "WhyTryIfYouGonnaDie", "date": 1583267143000, "content": "Could someone review my ANN implementation? /!/ Full disclosure, I am working on creating a 3-layer ANN (input, hidden, output) as an assignment for my CS class.  \n\nI am developing an ANN to solve a classification problem, however my backpropagation code seems to run rather slow. Would someone be kind enough to please review it and provide feedback?  \n\n[Source](https://github.com/Roy-05/machine-learning/tree/master/ANN)  \n\n```\n# Activation function is a sigmoid function\n# sigmoid = 1/(1 + e^-y)\ndef activate(y):\n    return 1.0/(1.0 + exp(-y))\n\n\ndef feedForward(weights, inputs):\n    # Initialize with bias\n    sigma = weights[-1]\n    for i in range(len(weights)-1):\n        sigma += weights[i]*inputs[i]\n    \n    return sigma\n\n\n#Forward propagate input to the network output\ndef forward_propagation(row):\n    inputs = row\n    for layer in neural_net:\n        new_inputs = []\n        for neuron in layer:\n            y = feedForward(neuron['w'], inputs)\n            neuron['output'] = activate(y)\n            new_inputs.append(neuron['output'])\n        inputs = new_inputs\n    return inputs\n\n\n# Calculate the derivative\ndef sigmoid_prime(y):\n    return y * (1.0 - y)\n\n\n# Back propagate errors\ndef back_propagation(row, expected):\n    for i in reversed(range(len(neural_net))):\n        layer = neural_net[i]\n        inputs = row[:-1] if i==0 else [neuron['output'] for neuron in neural_net[i-1]]\n\n        for neuron,j in zip(layer,range(len(layer))):  \n            # Case: Output layer\n            if i == (len(neural_net) - 1):\n                error = expected[j] - neuron[\"output\"]\n                neuron['delta'] = error * sigmoid_prime(neuron['output'])\n            else:\n                error = 0.0\n                for next_layer_neuron in neural_net[i+1]:\n                    error += next_layer_neuron['w'][j] * next_layer_neuron['delta']\n\n                neuron['delta'] = error * sigmoid_prime(neuron['output'])\n                \n            # Update weights\n            for k in range(len(inputs)):\n                neuron['w'][k] += learning_rate * neuron['delta'] * inputs[k]\n            neuron['w'][-1] += learning_rate * neuron['delta'] # Update Bias\n        \n```", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fd0vh3/could_someone_review_my_ann_implementation/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "could someone review my ann implementation? /!/ full disclosure, i am working on creating a 3-layer ann (input, hidden, output) as an assignment for my cs class.  \n\ni am developing an ann to solve a classification problem, however my backpropagation code seems to run rather slow. would someone be kind enough to please review it and provide feedback?  \n\n[source](https://github.com/roy-05/machine-learning/-----> tree !!! /master/ann)  \n\n```\n# activation function is a sigmoid function\n# sigmoid = 1/(1 + e^-y)\ndef activate(y):\n    return 1.0/(1.0 + exp(-y))\n\n\ndef feedforward(weights, inputs):\n    # initialize with bias\n    sigma = weights[-1]\n    for i in range(len(weights)-1):\n        sigma += weights[i]*inputs[i]\n    \n    return sigma\n\n\n#forward propagate input to the network output\ndef forward_propagation(row):\n    inputs = row\n    for layer in neural_net:\n        new_inputs = []\n        for neuron in layer:\n            y = feedforward(neuron['w'], inputs)\n            neuron['output'] = activate(y)\n            new_inputs.append(neuron['output'])\n        inputs = new_inputs\n    return inputs\n\n\n# calculate the derivative\ndef sigmoid_prime(y):\n    return y * (1.0 - y)\n\n\n# back propagate errors\ndef back_propagation(row, expected):\n    for i in reversed(range(len(neural_net))):\n        layer = neural_net[i]\n        inputs = row[:-1] if i==0 else [neuron['output'] for neuron in neural_net[i-1]]\n\n        for neuron,j in zip(layer,range(len(layer))):  \n            # case: output layer\n            if i == (len(neural_net) - 1):\n                error = expected[j] - neuron[\"output\"]\n                neuron['delta'] = error * sigmoid_prime(neuron['output'])\n            else:\n                error = 0.0\n                for next_layer_neuron in neural_net[i+1]:\n                    error += next_layer_neuron['w'][j] * next_layer_neuron['delta']\n\n                neuron['delta'] = error * sigmoid_prime(neuron['output'])\n                \n            # update weights\n            for k in range(len(inputs)):\n                neuron['w'][k] += learning_rate * neuron['delta'] * inputs[k]\n            neuron['w'][-1] += learning_rate * neuron['delta'] # update bias\n        \n```", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/fd0vh3/could_someone_review_my_ann_implementation/',)", "identifyer": 5744585, "year": "2020"}, {"autor": "plasalm", "date": 1581550412000, "content": "Rule-based learning - is it just CART? /!/ I\u2019ve been reading up on rule-based learning as a potential first-step after data processing in my firm. I grabbed a book on the topic and while it\u2019s mostly focused on learning classification rules, it seems like the main algorithms are equivalent to tree-based methods. It also kind of seems like a dead (or at least very small) subfield.\n\nAm I right in thinking that rules of rule-based learning are typically just the branches of a classification or regression tree? Is this an avenue worth exploring? For additional context, the goal is to assign a predicted value to a database record that lives in a high-dimensional (sparse) feature space. The predicted values are continuous, and would be used in conjunction with a traditional expert system as inputs to an anomaly detection algorithm that flags records for additional review. The records where the rule-based learner yields outcomes substantially different from the expert system would be of particular interest in cases where the anomaly detector fails.... if the rule-based learner is capable of producing results.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f30dp0/rulebased_learning_is_it_just_cart/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "rule-based learning - is it just cart? /!/ i\u2019ve been reading up on rule-based learning as a potential first-step after data processing in my firm. i grabbed a book on the topic and while it\u2019s mostly focused on learning classification rules, it seems like the main algorithms are equivalent to -----> tree !!! -based methods. it also kind of seems like a dead (or at least very small) subfield.\n\nam i right in thinking that rules of rule-based learning are typically just the branches of a classification or regression tree? is this an avenue worth exploring? for additional context, the goal is to assign a predicted value to a database record that lives in a high-dimensional (sparse) feature space. the predicted values are continuous, and would be used in conjunction with a traditional expert system as inputs to an anomaly detection algorithm that flags records for additional review. the records where the rule-based learner yields outcomes substantially different from the expert system would be of particular interest in cases where the anomaly detector fails.... if the rule-based learner is capable of producing results.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/f30dp0/rulebased_learning_is_it_just_cart/',)", "identifyer": 5744592, "year": "2020"}, {"autor": "WaveyJP", "date": 1581456541000, "content": "How do i learn to write my own programs once i understand concepts? /!/ I'm currently trying to learn how to write a decision tree classifier.\n\nI understand the concepts and could probably do this by hand ( which isnt impressive considering i am a human lol), I have a coded example that im looking at and adding my own comments to understand the code behind it. But how do i bridge the gap between understanding the code and actually writing my own decision tree? \n\n&amp;#x200B;\n\nI know this is a simplistic topic but im new to this. Would doing this by hand actually be useful? Should i memorise the methods used in examples? Any help will be appreciated and please feel free to give more general advice since i hope to move on to other concepts like clustering, regression etc", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f2fejx/how_do_i_learn_to_write_my_own_programs_once_i/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "how do i learn to write my own programs once i understand concepts? /!/ i'm currently trying to learn how to write a decision -----> tree !!!  classifier.\n\ni understand the concepts and could probably do this by hand ( which isnt impressive considering i am a human lol), i have a coded example that im looking at and adding my own comments to understand the code behind it. but how do i bridge the gap between understanding the code and actually writing my own decision tree? \n\n&amp;#x200b;\n\ni know this is a simplistic topic but im new to this. would doing this by hand actually be useful? should i memorise the methods used in examples? any help will be appreciated and please feel free to give more general advice since i hope to move on to other concepts like clustering, regression etc", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/f2fejx/how_do_i_learn_to_write_my_own_programs_once_i/',)", "identifyer": 5744633, "year": "2020"}, {"autor": "SQL_beginner", "date": 1582056139000, "content": "R/python : displaying full model for random forest and neural network /!/ Hello! I am new here.\n\nUsing python/R - is there a function to determine the rule file for a random forest/decision tree and the full matrix equations of a neutral network?\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f5xr5m/rpython_displaying_full_model_for_random_forest/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "r/python : displaying full model for random forest and neural network /!/ hello! i am new here.\n\nusing python/r - is there a function to determine the rule file for a random forest/decision -----> tree !!!  and the full matrix equations of a neutral network?\n\nthanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/f5xr5m/rpython_displaying_full_model_for_random_forest/',)", "identifyer": 5744734, "year": "2020"}, {"autor": "dhiraj8899", "date": 1581958757000, "content": "Implementing Regression Using a Decision Tree and Scikit-Learn /!/ [removed]", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f5bpck/implementing_regression_using_a_decision_tree_and/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "implementing regression using a decision -----> tree !!!  and scikit-learn /!/ [removed]", "sortedWord": "None", "removed": "('reddit',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/f5bpck/implementing_regression_using_a_decision_tree_and/',)", "identifyer": 5744777, "year": "2020"}, {"autor": "MrJamerJamer", "date": 1604353503000, "content": "Can someone explain very basically artificial neural networks /!/ I'm trying to wrap me head around the concepts of machine learning and more specifically neural networks. For example I know a fair bit about seq2seq models but am unsure where they fit in under the \"machine learning tree\".\n\nI'd mainly like to know what are some general examples of neural networks, some examples of seq2seq models (if they have names) and what's the difference between them all.\n\nAll the online resources I have found seem to discuss one part in detail but never link them together or show how one is a model of another.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jmwz7y/can_someone_explain_very_basically_artificial/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "can someone explain very basically artificial neural networks /!/ i'm trying to wrap me head around the concepts of machine learning and more specifically neural networks. for example i know a fair bit about seq2seq models but am unsure where they fit in under the \"machine learning -----> tree !!! \".\n\ni'd mainly like to know what are some general examples of neural networks, some examples of seq2seq models (if they have names) and what's the difference between them all.\n\nall the online resources i have found seem to discuss one part in detail but never link them together or show how one is a model of another.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/jmwz7y/can_someone_explain_very_basically_artificial/',)", "identifyer": 5744826, "year": "2020"}, {"autor": "Ridwan102", "date": 1607464574000, "content": "Android App crashes with TensorFlow Lite Model converted from YOLOv4 Custom Model /!/ I am trying to run my TensorFlow Lite model (converted from YOLOv4) into an Android App but it crashes every time I run the app. However, it runs properly with the TensorFlow Lite model that's converted from the pre-trained YOLOv4 model.\n\nHere's a snippet of the code:\n\n`private static final String TF_OD_API_MODEL_FILE = \"custom-416-rev3.tflite\";`  \n`private static final String TF_OD_API_LABELS_FILE = \"file:///android_asset/obj.txt\";`\n\nAnd the open-source repo I'm using: [Github Repo](https://github.com/theAIGuysCode/tensorflow-yolov4-tflite/tree/master/android)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k9e0au/android_app_crashes_with_tensorflow_lite_model/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "android app crashes with tensorflow lite model converted from yolov4 custom model /!/ i am trying to run my tensorflow lite model (converted from yolov4) into an android app but it crashes every time i run the app. however, it runs properly with the tensorflow lite model that's converted from the pre-trained yolov4 model.\n\nhere's a snippet of the code:\n\n`private static final string tf_od_api_model_file = \"custom-416-rev3.tflite\";`  \n`private static final string tf_od_api_labels_file = \"file:///android_asset/obj.txt\";`\n\nand the open-source repo i'm using: [github repo](https://github.com/theaiguyscode/tensorflow-yolov4-tflite/-----> tree !!! /master/android)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/k9e0au/android_app_crashes_with_tensorflow_lite_model/',)", "identifyer": 5744955, "year": "2020"}, {"autor": "lefnire", "date": 1609023793000, "content": "ML learning-resources list, in tree structure, with filters. Goal is to make learning machine learning a clean step-by-step, but with options to dive deeper /!/ https://ocdevel.com/mlg/resources\n\nIt's part of a \"syllabus\" I keep up with for my listeners on the [Machine Learning Guide](https://ocdevel.com/mlg) podcast, and I want it to be a one-stop for each step of the journey. I've seen various awesome-ml-\\* repos syndicating resources; as well as blog posts doing the same. The problem with both is they often get outdated before long, and it's difficult to drill into your specific need (hence my filters). Work in progress, please do recommend anything I might be missing / contend anything there you find less valuable.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kkrz8m/ml_learningresources_list_in_tree_structure_with/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "ml learning-resources list, in -----> tree !!!  structure, with filters. goal is to make learning machine learning a clean step-by-step, but with options to dive deeper /!/ https://ocdevel.com/mlg/resources\n\nit's part of a \"syllabus\" i keep up with for my listeners on the [machine learning guide](https://ocdevel.com/mlg) podcast, and i want it to be a one-stop for each step of the journey. i've seen various awesome-ml-\\* repos syndicating resources; as well as blog posts doing the same. the problem with both is they often get outdated before long, and it's difficult to drill into your specific need (hence my filters). work in progress, please do recommend anything i might be missing / contend anything there you find less valuable.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/kkrz8m/ml_learningresources_list_in_tree_structure_with/',)", "identifyer": 5744999, "year": "2020"}, {"autor": "doraekuma", "date": 1608977032000, "content": "Learn AI Game Playing Algorithm Part II \u2014 Monte Carlo Tree Search", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kkg9m2/learn_ai_game_playing_algorithm_part_ii_monte/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "learn ai game playing algorithm part ii \u2014 monte carlo -----> tree !!!  search", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('link',)", "medialink": "('https://xyzml.medium.com/learn-ai-game-playing-algorithm-part-ii-monte-carlo-tree-search-2113896d6072',)", "identifyer": 5745030, "year": "2020"}, {"autor": "ottawalanguages", "date": 1608928730000, "content": "Gradient Boosting and Time Series /!/ is this the right idea of gradient boosting (e.g. xgboost) with time series? Suppose you have weekly earnings for a company, e.g. 2010- Jan-1st week- $100,000.00\n\nyou turn the above data into 4 numeric variables: year, month, week of the month, earnings.\n\nthen, you make a tree based regression model :\n\nearnings ~ f(month, year, week of the month)\n\nmathematically, does this approach make sense?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kk5lrd/gradient_boosting_and_time_series/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "gradient boosting and time series /!/ is this the right idea of gradient boosting (e.g. xgboost) with time series? suppose you have weekly earnings for a company, e.g. 2010- jan-1st week- $100,000.00\n\nyou turn the above data into 4 numeric variables: year, month, week of the month, earnings.\n\nthen, you make a -----> tree !!!  based regression model :\n\nearnings ~ f(month, year, week of the month)\n\nmathematically, does this approach make sense?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/kk5lrd/gradient_boosting_and_time_series/',)", "identifyer": 5745048, "year": "2020"}, {"autor": "rat22s", "date": 1597223156000, "content": "Decision tree: A short hypothesis that fits the data is unlikely to be a coincidence /!/  What does it really mean to have 'coincidence' in a **decision tree** hypothesis. Does it mean that If the tree is long than it may have the same subtree at different parts of the tree? Can't really get the picture how it works.\n\nAlso if my assumption is right (same subtree), that means I can use the same attribute more than once in the same tree, like same attribute for different nodes? I thought once an attribute is used it cannot be tested for later nodes/descendants.\n\nTIA", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i8aftk/decision_tree_a_short_hypothesis_that_fits_the/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "decision -----> tree !!! : a short hypothesis that fits the data is unlikely to be a coincidence /!/  what does it really mean to have 'coincidence' in a **decision tree** hypothesis. does it mean that if the tree is long than it may have the same subtree at different parts of the tree? can't really get the picture how it works.\n\nalso if my assumption is right (same subtree), that means i can use the same attribute more than once in the same tree, like same attribute for different nodes? i thought once an attribute is used it cannot be tested for later nodes/descendants.\n\ntia", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/i8aftk/decision_tree_a_short_hypothesis_that_fits_the/',)", "identifyer": 5745331, "year": "2020"}, {"autor": "portirfer", "date": 1597870312000, "content": "About the structure of filters in a CNN /!/ From my understanding roughly speaking there are different filters that detect different patterns of the pixels in a picture and then there could be a multiple of higher order filters that detects patterns of the activation of one filter and only one filter lower down. I imagine as an analogy a tree where the higher the order the more branches/filters. \n\nBut wouldn\u2019t it make sense if one higher order filter would detect the activation of combined and different filters lower down like a network instead of a branching tree? Like a filter that recognizes (in a picture) eyes and then another filter recognizing ears and then there is a filter higher up that detects the combination of eyes and ears. And as far I understand that\u2019s not how it works...?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/icw5mt/about_the_structure_of_filters_in_a_cnn/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "about the structure of filters in a cnn /!/ from my understanding roughly speaking there are different filters that detect different patterns of the pixels in a picture and then there could be a multiple of higher order filters that detects patterns of the activation of one filter and only one filter lower down. i imagine as an analogy a -----> tree !!!  where the higher the order the more branches/filters. \n\nbut wouldn\u2019t it make sense if one higher order filter would detect the activation of combined and different filters lower down like a network instead of a branching tree? like a filter that recognizes (in a picture) eyes and then another filter recognizing ears and then there is a filter higher up that detects the combination of eyes and ears. and as far i understand that\u2019s not how it works...?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/icw5mt/about_the_structure_of_filters_in_a_cnn/',)", "identifyer": 5745408, "year": "2020"}, {"autor": "Vishesh1597", "date": 1597812906000, "content": "Bagging with linear models /!/ I have always heard people talking about bagged trees, tree models in general are high variance models(they overfit the data) and therefore it makes sense to bag them. \n\nBut I was thinking, If I create an ensemble of overfitting linear models, say a bunch of overfitting logistic regression models. What can possibly go wrong with this approach?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ichbxq/bagging_with_linear_models/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "bagging with linear models /!/ i have always heard people talking about bagged trees, -----> tree !!!  models in general are high variance models(they overfit the data) and therefore it makes sense to bag them. \n\nbut i was thinking, if i create an ensemble of overfitting linear models, say a bunch of overfitting logistic regression models. what can possibly go wrong with this approach?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/ichbxq/bagging_with_linear_models/',)", "identifyer": 5745438, "year": "2020"}, {"autor": "CkmCpvis", "date": 1594752689000, "content": "Does scaling negatively effect GBM models /!/ I have read that tree models scaling data is not required but does it negatively effect the model if the data is scaled?\n\nI\u2019m comparing the results to other models such as SVM and ideally I\u2019d like to be consistent with just one dataset If it does not hurt the results of the gradient boosted decision tree.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hr7emg/does_scaling_negatively_effect_gbm_models/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "does scaling negatively effect gbm models /!/ i have read that -----> tree !!!  models scaling data is not required but does it negatively effect the model if the data is scaled?\n\ni\u2019m comparing the results to other models such as svm and ideally i\u2019d like to be consistent with just one dataset if it does not hurt the results of the gradient boosted decision tree.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/hr7emg/does_scaling_negatively_effect_gbm_models/',)", "identifyer": 5745528, "year": "2020"}, {"autor": "wstcpyt1988", "date": 1594183983000, "content": "Decision Tree: Important things to know", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hnatns/decision_tree_important_things_to_know/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "decision -----> tree !!! : important things to know", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 14, "media": "('nan',)", "medialink": "('https://www.youtube.com/watch?v=JcI5E2Ng6r4',)", "identifyer": 5745594, "year": "2020"}, {"autor": "SquareTechAcademy", "date": 1594101495000, "content": "Moving Into AI - 2020 /!/ I have seen several questions here on this subreddit asking for MI resources and basic questions about how to get started with a MI career. So I wanted to share some of my resources if these can help some of you to get started either with Game AI's or Supervised Algorithms.\n\n**Supervised machine learning algorithm**\n\n* **Machine Learning Tutorial Part 1 | Machine Learning For Beginners**\n   * [https://youtu.be/E3l\\_aeGjkeI](https://youtu.be/E3l_aeGjkeI)\n* **Machine Learning Tutorial Part 2 | Machine Learning For Beginners - MI environment**\n   * [https://youtu.be/HqyrqxyDwPU](https://youtu.be/HqyrqxyDwPU)\n* **Machine Learning Tutorial Part 3 | Machine Learning For Beginners - Python Decision Tree**\n   * [https://youtu.be/8isUCINSmys](https://youtu.be/8isUCINSmys)\n* **Machine Learning Tutorial Part 4 | Machine Learning For Beginners - Python Decision Tree**\n   * [https://youtu.be/24mxQzd3EsU](https://youtu.be/24mxQzd3EsU)\n* **Machine Learning Tutorial Part 5 | Machine Learning For Beginners - Python Decision Tree**\n   * [https://youtu.be/aVEfKRfWjHc](https://youtu.be/aVEfKRfWjHc)\n* **Machine Learning Tutorial Part 6 | Knn(Friend Recommender) For Beginners**\n   * [https://youtu.be/LK0zgA6Mr6k](https://youtu.be/LK0zgA6Mr6k)\n* **Machine Learning Tutorial Part 7 | Machine Learning For Beginners - 5-Fold Cross Validation**\n   * [https://youtu.be/Zx5cz8pXnOM](https://youtu.be/Zx5cz8pXnOM)\n* **Machine Learning Tutorial Part 8 | Accuracy(MAE/RMSE) - Python Machine Learning For Beginners**\n   * [https://youtu.be/lHAEPyWNgyY](https://youtu.be/lHAEPyWNgyY)\n\n&amp;#x200B;\n\n**SC2 - Deepmind**\n\n* **Machine Learning - StarCraft 2 Python AI part 1**\n   * [**https://youtu.be/WFugpcvIil4**](https://youtu.be/WFugpcvIil4)\n* **Machine Learning - StarCraft 2 Python AI part 2 - Barracks**\n   * [**https://youtu.be/O1fwJgjfh1w**](https://youtu.be/O1fwJgjfh1w)\n* **Machine Learning - StarCraft 2 Python AI part 3 - Build Marines**\n   * [https://youtu.be/XzjLwMwhd\\_Y](https://youtu.be/XzjLwMwhd_Y)\n* **Machine Learning - StarCraft 2 Python AI part 4 - Attack**\n   * [https://youtu.be/90m-itTP\\_Zo](https://youtu.be/90m-itTP_Zo)\n\n&amp;#x200B;\n\n**Intro to NN**\n\n* **Neural Networks (Deep Learning) - Part 1 Introduction**\n   * [**https://youtu.be/SG9FYMKIu68**](https://youtu.be/SG9FYMKIu68)\n* **Neural Networks (Deep Learning) - Part 2 Types of NN**\n   * [https://youtu.be/hp-wGZ4-CpU](https://youtu.be/hp-wGZ4-CpU)\n* **Neural Networks (Deep Learning) - Part 3 - Neurons &amp; Logistic Regression**\n   * [https://youtu.be/mb-a9m5Yrzo](https://youtu.be/mb-a9m5Yrzo)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hmowki/moving_into_ai_2020/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "moving into ai - 2020 /!/ i have seen several questions here on this subreddit asking for mi resources and basic questions about how to get started with a mi career. so i wanted to share some of my resources if these can help some of you to get started either with game ai's or supervised algorithms.\n\n**supervised machine learning algorithm**\n\n* **machine learning tutorial part 1 | machine learning for beginners**\n   * [https://youtu.be/e3l\\_aegjkei](https://youtu.be/e3l_aegjkei)\n* **machine learning tutorial part 2 | machine learning for beginners - mi environment**\n   * [https://youtu.be/hqyrqxydwpu](https://youtu.be/hqyrqxydwpu)\n* **machine learning tutorial part 3 | machine learning for beginners - python decision -----> tree !!! **\n   * [https://youtu.be/8isucinsmys](https://youtu.be/8isucinsmys)\n* **machine learning tutorial part 4 | machine learning for beginners - python decision tree**\n   * [https://youtu.be/24mxqzd3esu](https://youtu.be/24mxqzd3esu)\n* **machine learning tutorial part 5 | machine learning for beginners - python decision tree**\n   * [https://youtu.be/avefkrfwjhc](https://youtu.be/avefkrfwjhc)\n* **machine learning tutorial part 6 | knn(friend recommender) for beginners**\n   * [https://youtu.be/lk0zga6mr6k](https://youtu.be/lk0zga6mr6k)\n* **machine learning tutorial part 7 | machine learning for beginners - 5-fold cross validation**\n   * [https://youtu.be/zx5cz8pxnom](https://youtu.be/zx5cz8pxnom)\n* **machine learning tutorial part 8 | accuracy(mae/rmse) - python machine learning for beginners**\n   * [https://youtu.be/lhaepywngyy](https://youtu.be/lhaepywngyy)\n\n&amp;#x200b;\n\n**sc2 - deepmind**\n\n* **machine learning - starcraft 2 python ai part 1**\n   * [**https://youtu.be/wfugpcviil4**](https://youtu.be/wfugpcviil4)\n* **machine learning - starcraft 2 python ai part 2 - barracks**\n   * [**https://youtu.be/o1fwjgjfh1w**](https://youtu.be/o1fwjgjfh1w)\n* **machine learning - starcraft 2 python ai part 3 - build marines**\n   * [https://youtu.be/xzjlwmwhd\\_y](https://youtu.be/xzjlwmwhd_y)\n* **machine learning - starcraft 2 python ai part 4 - attack**\n   * [https://youtu.be/90m-ittp\\_zo](https://youtu.be/90m-ittp_zo)\n\n&amp;#x200b;\n\n**intro to nn**\n\n* **neural networks (deep learning) - part 1 introduction**\n   * [**https://youtu.be/sg9fymkiu68**](https://youtu.be/sg9fymkiu68)\n* **neural networks (deep learning) - part 2 types of nn**\n   * [https://youtu.be/hp-wgz4-cpu](https://youtu.be/hp-wgz4-cpu)\n* **neural networks (deep learning) - part 3 - neurons &amp; logistic regression**\n   * [https://youtu.be/mb-a9m5yrzo](https://youtu.be/mb-a9m5yrzo)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 5, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/hmowki/moving_into_ai_2020/',)", "identifyer": 5745632, "year": "2020"}, {"autor": "benitorosenberg", "date": 1594037375000, "content": "Awesome Classification and Regression Tree Papers", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hm6gwx/awesome_classification_and_regression_tree_papers/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "awesome classification and regression -----> tree !!!  papers", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('link',)", "medialink": "('https://github.com/benedekrozemberczki/awesome-decision-tree-papers',)", "identifyer": 5745671, "year": "2020"}, {"autor": "benitorosenberg", "date": 1594037178000, "content": "Awesome Classification and Regression Tree Papers", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hm6f8x/awesome_classification_and_regression_tree_papers/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "awesome classification and regression -----> tree !!!  papers", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://github.com/benedekrozemberczki/awesome-decision-tree-papers',)", "identifyer": 5745672, "year": "2020"}, {"autor": "superare09", "date": 1591566083000, "content": "CNN with image as a label /!/ I want to make a CNN where the input is an image and the label is also an image. For example, the input image can be a tree and the label is an outline of that tree. Can this be done in Pytorch? Are there any examples of pytorch code where this has been done? Thanks in advance", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gyl8og/cnn_with_image_as_a_label/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "cnn with image as a label /!/ i want to make a cnn where the input is an image and the label is also an image. for example, the input image can be a -----> tree !!!  and the label is an outline of that -----> tree !!! . can this be done in pytorch? are there any examples of pytorch code where this has been done? thanks in advance", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/gyl8og/cnn_with_image_as_a_label/',)", "identifyer": 5745761, "year": "2020"}, {"autor": "caroleber", "date": 1589358121000, "content": "A complete guide to Decision Tree in Sklearn", "link": "https://www.reddit.com/r/learnmachinelearning/comments/giux2d/a_complete_guide_to_decision_tree_in_sklearn/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "a complete guide to decision -----> tree !!!  in sklearn", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://kanoki.org/2020/05/13/decision-tree-in-sklearn/',)", "identifyer": 5745996, "year": "2020"}, {"autor": "rijal_suprabhat", "date": 1588428439000, "content": "Machine Learning by the Decision Tree Algorithm", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gc717r/machine_learning_by_the_decision_tree_algorithm/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "machine learning by the decision -----> tree !!!  algorithm", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('link',)", "medialink": "('http://www.suppe.me/post/machine-learning-by-the-decision-tree-algorithm/',)", "identifyer": 5746137, "year": "2020"}, {"autor": "dhiraj8899", "date": 1601624523000, "content": "https://www.youtube.com/channel/UCuOT2b1Umrr0MittMzuxNcA?sub_confirmation=1 /!/ Learn Machine Learning Algorithms from Scratch-like Neural Network, Decision Tree, Logistic Regression, Linear Regression, and Gradient Descent", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j3qdr1/httpswwwyoutubecomchannelucuot2b1umrr0mittmzuxncas/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "https://www.youtube.com/channel/ucuot2b1umrr0mittmzuxnca?sub_confirmation=1 /!/ learn machine learning algorithms from scratch-like neural network, decision -----> tree !!! , logistic regression, linear regression, and gradient descent", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/j3qdr1/httpswwwyoutubecomchannelucuot2b1umrr0mittmzuxncas/',)", "identifyer": 5746251, "year": "2020"}, {"autor": "Gamer2477DAW", "date": 1601541762000, "content": "Can logic tree based classification and regression be useful in any scenario or does it have its limits? It's my personal favorite to get results quickly. Are there any better or faster methods though?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j34trq/can_logic_tree_based_classification_and/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "can logic -----> tree !!!  based classification and regression be useful in any scenario or does it have its limits? it's my personal favorite to get results quickly. are there any better or faster methods though?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/j34trq/can_logic_tree_based_classification_and/',)", "identifyer": 5746274, "year": "2020"}, {"autor": "Nt12345678909876", "date": 1602267127000, "content": "Understanding Gini index and information gain in decision tree", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j84575/understanding_gini_index_and_information_gain_in/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "understanding gini index and information gain in decision -----> tree !!! ", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('link',)", "medialink": "('https://medium.com/analytics-steps/understanding-the-gini-index-and-information-gain-in-decision-trees-ab4720518ba8',)", "identifyer": 5746454, "year": "2020"}, {"autor": "MatthewNagy", "date": 1579961178000, "content": "What is tree depth? /!/ Stupid question but surprisingly I can't easily find the answer for this on Google.  What is tree depth?  Let's say you have a max tree depth of 4, what does that mean?\n\n&amp;#x200B;\n\nThanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/etr5om/what_is_tree_depth/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "what is -----> tree !!!  depth? /!/ stupid question but surprisingly i can't easily find the answer for this on google.  what is -----> tree !!!  depth?  let's say you have a max tree depth of 4, what does that mean?\n\n&amp;#x200b;\n\nthanks", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 5, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/etr5om/what_is_tree_depth/',)", "identifyer": 5746523, "year": "2020"}, {"autor": "big_hand_larry", "date": 1579923460000, "content": "Help with first machine learning project /!/ Hey guys, I just started a machine learning class and I'm not really sure what I'm doing.  The problem is as follows: 13 training set examples in a csv file of CD's where the columns are Example, Type, Price, and Category.  Example is just a numbered list, Type can be Hip-Hop, Jazz, or Rock, Price can be Cheap or Expensive, and Category is Yes or No and represents if the CD is bought.  There is another csv test file formatted the same way with 5 test cases.  I was told I would need scikit-learn for training and testing the data, and numpy.  I have found so far that panda worked great for retrieving the file and fitting it to a dummy.  Here is my code:\n\n    import pandas as pd\n    import numpy as np\n    from sklearn import tree\n    from sklearn.preprocessing import OneHotEncoder\n    #import for accuracy tests\n    from sklearn.metrics import accuracy_score\n    \n    \n    data_dir = 'C:/Users/parke/PycharmProjects/simpletest.py/'\n    train_file = data_dir + 'train.csv'\n    test_file = data_dir + 'test.csv'\n    x_train = pd.read_csv(train_file)\n    x_test = pd.read_csv(test_file)\n    \n    y_train = x_train[x_train.columns[-1]]\n    y_test = x_test[x_test.columns[-1]]\n    \n    x_train.drop('Category', axis=1, inplace=True)\n    x_train.drop('Example', axis=1, inplace=True)\n    \n    x_test.drop('Category', axis=1, inplace=True)\n    x_test.drop('Example', axis=1, inplace=True)\n    \n    x_temp1 = pd.get_dummies(x_train.Type)\n    x_temp2 = pd.get_dummies(x_train.Price)\n    x_temp3 = pd.get_dummies(x_test.Type)\n    x_temp4 = pd.get_dummies(x_test.Price)\n    \n    x_train = pd.concat([x_temp1, x_temp2], axis=1)\n    x_test = pd.concat([x_temp3, x_temp4], axis=1)\n    \n    y_train = pd.get_dummies(y_train)\n    y_test = pd.get_dummies(y_test)\n    \n    clf = tree.DecisionTreeClassifier()\n    clf.fit(x_train, y_train)\n    print(clf.predict(x_test, y_test))\n    \n\nHere is the error I get: Traceback (most recent call last):\n\n  File \"C:/Users/parke/PycharmProjects/simpletest.py/test.py\", line 33, in &lt;module&gt;\n\nprint(clf.predict(x\\_test, y\\_test))\n\n  File \"C:\\\\Users\\\\parke\\\\Anaconda3\\\\libs\\\\lib\\\\site-packages\\\\sklearn\\\\tree\\\\\\_classes.py\", line 419, in predict\n\nX = self.\\_validate\\_X\\_predict(X, check\\_input)\n\n  File \"C:\\\\Users\\\\parke\\\\Anaconda3\\\\libs\\\\lib\\\\site-packages\\\\sklearn\\\\tree\\\\\\_classes.py\", line 379, in \\_validate\\_X\\_predict\n\nif check\\_input:\n\n  File \"C:\\\\Users\\\\parke\\\\Anaconda3\\\\libs\\\\lib\\\\site-packages\\\\pandas\\\\core\\\\[generic.py](https://generic.py)\", line 1552, in \\_\\_nonzero\\_\\_\n\nraise ValueError(\n\nValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n\nIf anyone can explain what I'm doing wrong and how I could go about fixing it that would help me so much.  I think I'm close but I'm out of ideas.  Thanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/etlkp9/help_with_first_machine_learning_project/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "help with first machine learning project /!/ hey guys, i just started a machine learning class and i'm not really sure what i'm doing.  the problem is as follows: 13 training set examples in a csv file of cd's where the columns are example, type, price, and category.  example is just a numbered list, type can be hip-hop, jazz, or rock, price can be cheap or expensive, and category is yes or no and represents if the cd is bought.  there is another csv test file formatted the same way with 5 test cases.  i was told i would need scikit-learn for training and testing the data, and numpy.  i have found so far that panda worked great for retrieving the file and fitting it to a dummy.  here is my code:\n\n    import pandas as pd\n    import numpy as np\n    from sklearn import -----> tree !!! \n    from sklearn.preprocessing import onehotencoder\n    #import for accuracy tests\n    from sklearn.metrics import accuracy_score\n    \n    \n    data_dir = 'c:/users/parke/pycharmprojects/simpletest.py/'\n    train_file = data_dir + 'train.csv'\n    test_file = data_dir + 'test.csv'\n    x_train = pd.read_csv(train_file)\n    x_test = pd.read_csv(test_file)\n    \n    y_train = x_train[x_train.columns[-1]]\n    y_test = x_test[x_test.columns[-1]]\n    \n    x_train.drop('category', axis=1, inplace=true)\n    x_train.drop('example', axis=1, inplace=true)\n    \n    x_test.drop('category', axis=1, inplace=true)\n    x_test.drop('example', axis=1, inplace=true)\n    \n    x_temp1 = pd.get_dummies(x_train.type)\n    x_temp2 = pd.get_dummies(x_train.price)\n    x_temp3 = pd.get_dummies(x_test.type)\n    x_temp4 = pd.get_dummies(x_test.price)\n    \n    x_train = pd.concat([x_temp1, x_temp2], axis=1)\n    x_test = pd.concat([x_temp3, x_temp4], axis=1)\n    \n    y_train = pd.get_dummies(y_train)\n    y_test = pd.get_dummies(y_test)\n    \n    clf = tree.decisiontreeclassifier()\n    clf.fit(x_train, y_train)\n    print(clf.predict(x_test, y_test))\n    \n\nhere is the error i get: traceback (most recent call last):\n\n  file \"c:/users/parke/pycharmprojects/simpletest.py/test.py\", line 33, in &lt;module&gt;\n\nprint(clf.predict(x\\_test, y\\_test))\n\n  file \"c:\\\\users\\\\parke\\\\anaconda3\\\\libs\\\\lib\\\\site-packages\\\\sklearn\\\\tree\\\\\\_classes.py\", line 419, in predict\n\nx = self.\\_validate\\_x\\_predict(x, check\\_input)\n\n  file \"c:\\\\users\\\\parke\\\\anaconda3\\\\libs\\\\lib\\\\site-packages\\\\sklearn\\\\tree\\\\\\_classes.py\", line 379, in \\_validate\\_x\\_predict\n\nif check\\_input:\n\n  file \"c:\\\\users\\\\parke\\\\anaconda3\\\\libs\\\\lib\\\\site-packages\\\\pandas\\\\core\\\\[generic.py](https://generic.py)\", line 1552, in \\_\\_nonzero\\_\\_\n\nraise valueerror(\n\nvalueerror: the truth value of a dataframe is ambiguous. use a.empty, a.bool(), a.item(), a.any() or a.all().\n\nif anyone can explain what i'm doing wrong and how i could go about fixing it that would help me so much.  i think i'm close but i'm out of ideas.  thanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/etlkp9/help_with_first_machine_learning_project/',)", "identifyer": 5746533, "year": "2020"}, {"autor": "pp314159", "date": 1600959197000, "content": "Apply AutoML on Titanic - get most of the analysis done with 2 lines of python code! /!/ I'm working on open-source AutoML [mljar-supervised](https://github.com/mljar/mljar-supervised) which I used to analyze the Titanic dataset. You should check out how many of the analysis can be done automatically with 2 lines of code:\n\n```python\nautoml = AutoML()\nautoml.fit(X, y)\n```\n\nI've created [documentation](https://supervised.mljar.com/tutorials/titanic/) with a step-by-step explanations of what results are produced from AutoML:\n\n- there will be trained following algorithms: Baseline, Decision Tree, Linear, Random Forest, Xgboost, Neural Network, and Ensemble,\n- the Automated Exploratory Data Analysis will be done,\n- the full explanations will be created.\n\nLink to documentation: https://supervised.mljar.com/tutorials/titanic/\n\nLink to GitHub with all results: https://github.com/mljar/mljar-examples/tree/master/Titanic_Classification\n\n\nI hope you will find it valuable not only for Titanic but also for other competitions and Machine Learning challenges.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/iyypde/apply_automl_on_titanic_get_most_of_the_analysis/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "apply automl on titanic - get most of the analysis done with 2 lines of python code! /!/ i'm working on open-source automl [mljar-supervised](https://github.com/mljar/mljar-supervised) which i used to analyze the titanic dataset. you should check out how many of the analysis can be done automatically with 2 lines of code:\n\n```python\nautoml = automl()\nautoml.fit(x, y)\n```\n\ni've created [documentation](https://supervised.mljar.com/tutorials/titanic/) with a step-by-step explanations of what results are produced from automl:\n\n- there will be trained following algorithms: baseline, decision -----> tree !!! , linear, random forest, xgboost, neural network, and ensemble,\n- the automated exploratory data analysis will be done,\n- the full explanations will be created.\n\nlink to documentation: https://supervised.mljar.com/tutorials/titanic/\n\nlink to github with all results: https://github.com/mljar/mljar-examples/tree/master/titanic_classification\n\n\ni hope you will find it valuable not only for titanic but also for other competitions and machine learning challenges.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/iyypde/apply_automl_on_titanic_get_most_of_the_analysis/',)", "identifyer": 5746678, "year": "2020"}, {"autor": "MachineLearningGuy07", "date": 1598726195000, "content": "My Random Forest never predicts anything. Not even once? /!/ Hello!\n\nI have a question about Random Forest where I seem to have a problem.\n\nMy Random Forest is never predicting anything. The Forest is never predicting anything. I wonder why this is happening and if I could have any guess where the problem could be?\n\n**I will explain the scenario I have. I have 928 samples of data:**\n\n1. I have 1 Million Features with GINI impurity: 0 - 0.39. Where 700,000 features has GINI impurity 0. (Is this a clue to the problem? )\n2. I have trained/created 1000 trees in the forest. Each tree uses on Random 65% of the 928 samples and on random 1000 features each of the 1 Million Features.\n3. When all trees are created, I go through all 928 samples and find the trees that never used that particular sample in the training. This means that do a **Out of Bag (OOB)** evaluation here. Approx 520 trees are found for each sample.\n\nNow comes the problem. None of those 928 samples that I run through its trees(approx. 520 trees) gives a prediction of TRUE or FALSE.\n\n1. Scenario 1: Hypothes/predicting variable says TRUE, the Random Forest says FALSE\n2. Scenario 2: Hypothes/predicting variable says FALSE, the Random Forest says TRUE\n\nNot once, I get both saying TRUE or both saying FALSE. Why is this happening? I have heard about unbalanced data but don't know much about it and if that even is the problem. Does the 700,000 features with GINI impurity of 0 gives any clue?\n\n&amp;#x200B;\n\nI would be so greatful for any id\u00e9as what the problem could be.\n\nThank you!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/iixnbz/my_random_forest_never_predicts_anything_not_even/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "my random forest never predicts anything. not even once? /!/ hello!\n\ni have a question about random forest where i seem to have a problem.\n\nmy random forest is never predicting anything. the forest is never predicting anything. i wonder why this is happening and if i could have any guess where the problem could be?\n\n**i will explain the scenario i have. i have 928 samples of data:**\n\n1. i have 1 million features with gini impurity: 0 - 0.39. where 700,000 features has gini impurity 0. (is this a clue to the problem? )\n2. i have trained/created 1000 trees in the forest. each -----> tree !!!  uses on random 65% of the 928 samples and on random 1000 features each of the 1 million features.\n3. when all trees are created, i go through all 928 samples and find the trees that never used that particular sample in the training. this means that do a **out of bag (oob)** evaluation here. approx 520 trees are found for each sample.\n\nnow comes the problem. none of those 928 samples that i run through its trees(approx. 520 trees) gives a prediction of true or false.\n\n1. scenario 1: hypothes/predicting variable says true, the random forest says false\n2. scenario 2: hypothes/predicting variable says false, the random forest says true\n\nnot once, i get both saying true or both saying false. why is this happening? i have heard about unbalanced data but don't know much about it and if that even is the problem. does the 700,000 features with gini impurity of 0 gives any clue?\n\n&amp;#x200b;\n\ni would be so greatful for any id\u00e9as what the problem could be.\n\nthank you!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 17, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/iixnbz/my_random_forest_never_predicts_anything_not_even/',)", "identifyer": 5746759, "year": "2020"}, {"autor": "springuni", "date": 1598608100000, "content": "Algorithm selection problem (Interview Question) /!/ I've been asked this question on an interview and I'll also share what I answered.\n\nI would like to know if there's anything I haven't thought of?\n\n**Question:** Given the dataset (feature1, feature2, label) (0, 0, 0), (0, 1, 1), (1, 0, 1), (1, 1, 0), would you prefer to train a Logistic Regression or a Decision Tree Classifier for label prediction? Why?\n\n**Answer:**  If the dataset is just that, I wouldn't bother with training a classifier and introduce unnecessary complexity. It can be seen that \\`label = feature1 XOR feature2\\` is a correct and simple model.\n\nGenerally speaking, Logistic Regression is a great fit when the decision boundary is linear, ie. when data points can be separated easily by a hyper-plane. this is not the case here. From plotting the data points, it can be seen that is isn't the case here.\n\nThanks in advance!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ii46jf/algorithm_selection_problem_interview_question/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "algorithm selection problem (interview question) /!/ i've been asked this question on an interview and i'll also share what i answered.\n\ni would like to know if there's anything i haven't thought of?\n\n**question:** given the dataset (feature1, feature2, label) (0, 0, 0), (0, 1, 1), (1, 0, 1), (1, 1, 0), would you prefer to train a logistic regression or a decision -----> tree !!!  classifier for label prediction? why?\n\n**answer:**  if the dataset is just that, i wouldn't bother with training a classifier and introduce unnecessary complexity. it can be seen that \\`label = feature1 xor feature2\\` is a correct and simple model.\n\ngenerally speaking, logistic regression is a great fit when the decision boundary is linear, ie. when data points can be separated easily by a hyper-plane. this is not the case here. from plotting the data points, it can be seen that is isn't the case here.\n\nthanks in advance!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/ii46jf/algorithm_selection_problem_interview_question/',)", "identifyer": 5746812, "year": "2020"}, {"autor": "informacja", "date": 1587772883000, "content": "Machine Learning BA thesis. /!/ Hello all\n\n&amp;#x200B;\n\nI am writing my BA thesis on machine learning. Initially, the idea was to conduct an analysis of failed companies based on financial indicators.\n\nAs you know, you need to do some research in your BA thesis. Analysis of this data would guarantee just such an analysis.\n\n&amp;#x200B;\n\nUnfortunately, I cannot use the same data that has already been used in another study.\n\n&amp;#x200B;\n\nAs I'm a beginner in the subject, I wanted to find some research that I can do using simple, ready-made algorithms using python 3 and the scikit-learn library. I am still working on a chapter on theory, although I have a month to go and I need to find an idea where I could apply these algorithms to pass my research in my BA thesis.\n\n&amp;#x200B;\n\nI know that databases are available on pages like kaggle. If you have any idea where I could use simple classifiers in the form of an examination certain event, I would be very grateful.\n\n&amp;#x200B;\n\nI am talking about classifiers such as: Logistic Regression, Support Vector Machine, Naive Bayes classifier, Decision Tree classifier, Random Forest Classification.\n\n&amp;#x200B;\n\nFor all your help THANK YOU!.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g7jlzf/machine_learning_ba_thesis/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "machine learning ba thesis. /!/ hello all\n\n&amp;#x200b;\n\ni am writing my ba thesis on machine learning. initially, the idea was to conduct an analysis of failed companies based on financial indicators.\n\nas you know, you need to do some research in your ba thesis. analysis of this data would guarantee just such an analysis.\n\n&amp;#x200b;\n\nunfortunately, i cannot use the same data that has already been used in another study.\n\n&amp;#x200b;\n\nas i'm a beginner in the subject, i wanted to find some research that i can do using simple, ready-made algorithms using python 3 and the scikit-learn library. i am still working on a chapter on theory, although i have a month to go and i need to find an idea where i could apply these algorithms to pass my research in my ba thesis.\n\n&amp;#x200b;\n\ni know that databases are available on pages like kaggle. if you have any idea where i could use simple classifiers in the form of an examination certain event, i would be very grateful.\n\n&amp;#x200b;\n\ni am talking about classifiers such as: logistic regression, support vector machine, naive bayes classifier, decision -----> tree !!!  classifier, random forest classification.\n\n&amp;#x200b;\n\nfor all your help thank you!.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/g7jlzf/machine_learning_ba_thesis/',)", "identifyer": 5746920, "year": "2020"}, {"autor": "hernancrespo89", "date": 1587727832000, "content": "I can't fix a false positive in vehicle detection? /!/ Hi,\n\nI've been trying to develop a vehicle detection project using [EfficientDet.](https://github.com/google/automl/tree/master/efficientdet) It works well on my test videos except one thing. There is a false positive here.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/y6bgr0h34ru41.png?width=1202&amp;format=png&amp;auto=webp&amp;s=c5db9953b3e3a994b173030c14e597652fb01b08\n\nI've tried all EfficientDet versions of d0...to d7. I've tried to re-train model with images which I've labeled. But This false positive always occurs. I am a newbie, and I have little knowledge. How can I fix it?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g76rvs/i_cant_fix_a_false_positive_in_vehicle_detection/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "i can't fix a false positive in vehicle detection? /!/ hi,\n\ni've been trying to develop a vehicle detection project using [efficientdet.](https://github.com/google/automl/-----> tree !!! /master/efficientdet) it works well on my test videos except one thing. there is a false positive here.\n\n&amp;#x200b;\n\nhttps://preview.redd.it/y6bgr0h34ru41.png?width=1202&amp;format=png&amp;auto=webp&amp;s=c5db9953b3e3a994b173030c14e597652fb01b08\n\ni've tried all efficientdet versions of d0...to d7. i've tried to re-train model with images which i've labeled. but this false positive always occurs. i am a newbie, and i have little knowledge. how can i fix it?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/g76rvs/i_cant_fix_a_false_positive_in_vehicle_detection/',)", "identifyer": 5746940, "year": "2020"}, {"autor": "dhiraj8899", "date": 1605688222000, "content": "Top 5 Decision Tree Algorithm Advantages and Disadvantages", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jwc63z/top_5_decision_tree_algorithm_advantages_and/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "top 5 decision -----> tree !!!  algorithm advantages and disadvantages", "sortedWord": "None", "removed": "('nan',)", "score": 4, "comments": 2, "media": "('rich:video',)", "medialink": "('https://youtu.be/kV__PHlVLxI',)", "identifyer": 5746958, "year": "2020"}, {"autor": "jinnyjuice", "date": 1605599527000, "content": "Tree of machine learning algorithms in four different languages", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jvp3fk/tree_of_machine_learning_algorithms_in_four/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "-----> tree !!!  of machine learning algorithms in four different languages", "sortedWord": "None", "removed": "('nan',)", "score": 3, "comments": 0, "media": "('link',)", "medialink": "('https://kumu.io/jads/tree-of-machine-learning-algorithms#tree',)", "identifyer": 5747012, "year": "2020"}, {"autor": "osm3000", "date": 1582615033000, "content": "Evolutionary optimization for MNIST neural network classifier /!/ I am experimenting with evolutionary algorithms (evolutionary strategies and simple genetic algorithms) in order to train a MLP to classify MNIST. I tested this MLP before with SGD, it gives a good performance. However, with ES and SGA, I am having a REALLY difficult time getting over 20% accuracy.\n\nI am using [PyGMO](https://esa.github.io/pagmo2/) as the framework for the algorithms, and [PyTorch](https://pytorch.org/) for the network implementation. For each individual, I sample a random batch from the training data and use it in order to measure the fitness of that individual (similar in the spirit to averaging over multiple rollouts mentioned [here](http://blog.otoro.net/2017/11/12/evolving-stable-strategies/)). The batch size is 64 - it was a random choice -.\n\nI checked this [blog post](http://blog.otoro.net/2017/10/29/visual-evolution-strategies/) and the [accompanying code](https://github.com/hardmaru/pytorch_notebooks/blob/master/mnist_es/pytorch_mnist_mini_es_ga.ipynb) about classification of MNIST, but by far I am not able to replicate those numbers, not even close!\n\n[Here is my code](https://github.com/osm3000/code_issues/tree/master/evolve_mnist_classifier) for reference. I would appreciate any insight", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f96j1d/evolutionary_optimization_for_mnist_neural/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "evolutionary optimization for mnist neural network classifier /!/ i am experimenting with evolutionary algorithms (evolutionary strategies and simple genetic algorithms) in order to train a mlp to classify mnist. i tested this mlp before with sgd, it gives a good performance. however, with es and sga, i am having a really difficult time getting over 20% accuracy.\n\ni am using [pygmo](https://esa.github.io/pagmo2/) as the framework for the algorithms, and [pytorch](https://pytorch.org/) for the network implementation. for each individual, i sample a random batch from the training data and use it in order to measure the fitness of that individual (similar in the spirit to averaging over multiple rollouts mentioned [here](http://blog.otoro.net/2017/11/12/evolving-stable-strategies/)). the batch size is 64 - it was a random choice -.\n\ni checked this [blog post](http://blog.otoro.net/2017/10/29/visual-evolution-strategies/) and the [accompanying code](https://github.com/hardmaru/pytorch_notebooks/blob/master/mnist_es/pytorch_mnist_mini_es_ga.ipynb) about classification of mnist, but by far i am not able to replicate those numbers, not even close!\n\n[here is my code](https://github.com/osm3000/code_issues/-----> tree !!! /master/evolve_mnist_classifier) for reference. i would appreciate any insight", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/f96j1d/evolutionary_optimization_for_mnist_neural/',)", "identifyer": 5747108, "year": "2020"}, {"autor": "KnurpsBram", "date": 1582544053000, "content": "Monte Carlo Tree Search for games with Imperfect Information and Chance (MOISMCTS) /!/ Kariba is a card game with imperfect information and an element of chance. Which means Vanilla MCTS can't be applied directly to Kariba. However, MCTS can be upgraded to MOISMCTS (Multiple-Observer Information Set Monte Carlo Tree Search) which can deal with these properties.\n\nBlog:\n\n[https://medium.com/@bramkooiman94/artificial-intelligence-plays-kariba-c281052e8dce](https://medium.com/@bramkooiman94/artificial-intelligence-plays-kariba-c281052e8dce)\n\nGit repo:\n\n[https://github.com/KnurpsBram/AI\\_plays\\_kariba](https://github.com/KnurpsBram/AI_plays_kariba)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f8pygf/monte_carlo_tree_search_for_games_with_imperfect/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "monte carlo -----> tree !!!  search for games with imperfect information and chance (moismcts) /!/ kariba is a card game with imperfect information and an element of chance. which means vanilla mcts can't be applied directly to kariba. however, mcts can be upgraded to moismcts (multiple-observer information set monte carlo tree search) which can deal with these properties.\n\nblog:\n\n[https://medium.com/@bramkooiman94/artificial-intelligence-plays-kariba-c281052e8dce](https://medium.com/@bramkooiman94/artificial-intelligence-plays-kariba-c281052e8dce)\n\ngit repo:\n\n[https://github.com/knurpsbram/ai\\_plays\\_kariba](https://github.com/knurpsbram/ai_plays_kariba)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/f8pygf/monte_carlo_tree_search_for_games_with_imperfect/',)", "identifyer": 5747132, "year": "2020"}, {"autor": "BotBark", "date": 1577911995000, "content": "Decision Tree Regression in Python in 10 lines /!/ Decision tree algorithm creates a tree like conditional control statements to create its model hence it is named as decision tree.\n\nDecision tree machine learning algorithm can be used to solve both regression and classification problem.\n\nIn this post we will be implementing a simple decision tree regression model using python and sklearn.\n\n [https://botbark.com/2020/01/01/decision-tree-regression-in-python-in-10-lines/](https://botbark.com/2020/01/01/decision-tree-regression-in-python-in-10-lines/)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/einlzm/decision_tree_regression_in_python_in_10_lines/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "decision -----> tree !!!  regression in python in 10 lines /!/ decision tree algorithm creates a tree like conditional control statements to create its model hence it is named as decision tree.\n\ndecision tree machine learning algorithm can be used to solve both regression and classification problem.\n\nin this post we will be implementing a simple decision tree regression model using python and sklearn.\n\n [https://botbark.com/2020/01/01/decision-tree-regression-in-python-in-10-lines/](https://botbark.com/2020/01/01/decision-tree-regression-in-python-in-10-lines/)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/einlzm/decision_tree_regression_in_python_in_10_lines/',)", "identifyer": 5747371, "year": "2020"}, {"autor": "lukescriptwalker", "date": 1577911346000, "content": "Machine Learning 2020 - The Year of MI /!/ 3 months ago, I created and posted videos from my Python machine learning course on the learnProgramming subreddit. So far they have been received very well! I have also posted some of the videos on this Subreddit but not in a combined post. As we now write 2020 I wanted to share my resources for those who has a New Year Wish of learning MI this year. \n\n* **Part 1 - Machine Learning For Beginners - Basics**\n\n[https://youtu.be/E3l\\_aeGjkeI](https://youtu.be/E3l_aeGjkeI)\n\n* **Part 2 - MI environment**\n\n[https://youtu.be/HqyrqxyDwPU](https://youtu.be/HqyrqxyDwPU)\n\n* **Part 3 - Python Decision Tree (Theory)**\n\n[https://youtu.be/8isUCINSmys](https://youtu.be/8isUCINSmys)\n\n* **Part 4 - Python Decision Tree (Coding)**\n\n[https://youtu.be/24mxQzd3EsU](https://youtu.be/24mxQzd3EsU)\n\n* **Part 5 - Python Decision Tree (Graphiviz)**\n\n[https://youtu.be/aVEfKRfWjHc](https://youtu.be/aVEfKRfWjHc)\n\n* **Part 6 - Knn(Friend Recommender)**\n\n[https://youtu.be/LK0zgA6Mr6k](https://youtu.be/LK0zgA6Mr6k)\n\n* **Part 7- 5-Fold Cross Validation**\n\n[https://youtu.be/Zx5cz8pXnOM](https://youtu.be/Zx5cz8pXnOM)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/eingr8/machine_learning_2020_the_year_of_mi/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "machine learning 2020 - the year of mi /!/ 3 months ago, i created and posted videos from my python machine learning course on the learnprogramming subreddit. so far they have been received very well! i have also posted some of the videos on this subreddit but not in a combined post. as we now write 2020 i wanted to share my resources for those who has a new year wish of learning mi this year. \n\n* **part 1 - machine learning for beginners - basics**\n\n[https://youtu.be/e3l\\_aegjkei](https://youtu.be/e3l_aegjkei)\n\n* **part 2 - mi environment**\n\n[https://youtu.be/hqyrqxydwpu](https://youtu.be/hqyrqxydwpu)\n\n* **part 3 - python decision -----> tree !!!  (theory)**\n\n[https://youtu.be/8isucinsmys](https://youtu.be/8isucinsmys)\n\n* **part 4 - python decision tree (coding)**\n\n[https://youtu.be/24mxqzd3esu](https://youtu.be/24mxqzd3esu)\n\n* **part 5 - python decision tree (graphiviz)**\n\n[https://youtu.be/avefkrfwjhc](https://youtu.be/avefkrfwjhc)\n\n* **part 6 - knn(friend recommender)**\n\n[https://youtu.be/lk0zga6mr6k](https://youtu.be/lk0zga6mr6k)\n\n* **part 7- 5-fold cross validation**\n\n[https://youtu.be/zx5cz8pxnom](https://youtu.be/zx5cz8pxnom)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 17, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/eingr8/machine_learning_2020_the_year_of_mi/',)", "identifyer": 5747372, "year": "2020"}, {"autor": "leooister", "date": 1600238438000, "content": "Moving Into AI - 2020 (Game AI &amp; Supervised Learning) /!/ I would like to share some of my resources within the area of ML if these can help some of you to get started either with Game AI's or Supervised Algorithms.\n\n**Supervised machine learning algorithm**\n\n* **Machine Learning Tutorial Part 1 | Machine Learning For Beginners**  \n\n   * [https://youtu.be/E3l\\_aeGjkeI](https://youtu.be/E3l_aeGjkeI)\n* **Machine Learning Tutorial Part 2 | Machine Learning For Beginners - MI environment**  \n\n   * [https://youtu.be/HqyrqxyDwPU](https://youtu.be/HqyrqxyDwPU)\n* **Machine Learning Tutorial Part 3 | Machine Learning For Beginners - Python Decision Tree**  \n\n   * [https://youtu.be/8isUCINSmys](https://youtu.be/8isUCINSmys)\n* **Machine Learning Tutorial Part 4 | Machine Learning For Beginners - Python Decision Tree**  \n\n   * [https://youtu.be/24mxQzd3EsU](https://youtu.be/24mxQzd3EsU)\n* **Machine Learning Tutorial Part 5 | Machine Learning For Beginners - Python Decision Tree**  \n\n   * [https://youtu.be/aVEfKRfWjHc](https://youtu.be/aVEfKRfWjHc)\n* **Machine Learning Tutorial Part 6 | Knn(Friend Recommender) For Beginners**  \n\n   * [https://youtu.be/LK0zgA6Mr6k](https://youtu.be/LK0zgA6Mr6k)\n* **Machine Learning Tutorial Part 7 | Machine Learning For Beginners - 5-Fold Cross Validation**  \n\n   * [https://youtu.be/Zx5cz8pXnOM](https://youtu.be/Zx5cz8pXnOM)\n* **Machine Learning Tutorial Part 8 | Accuracy(MAE/RMSE) - Python Machine Learning For Beginners**  \n\n   * [https://youtu.be/lHAEPyWNgyY](https://youtu.be/lHAEPyWNgyY)\n\n**SC2 - Deepmind**\n\n* **Machine Learning - StarCraft 2 Python AI part 1**  \n\n   * [**https://youtu.be/WFugpcvIil4**](https://youtu.be/WFugpcvIil4)\n* **Machine Learning - StarCraft 2 Python AI part 2 - Barracks**  \n\n   * [**https://youtu.be/O1fwJgjfh1w**](https://youtu.be/O1fwJgjfh1w)\n* **Machine Learning - StarCraft 2 Python AI part 3 - Build Marines**  \n\n   * [https://youtu.be/XzjLwMwhd\\_Y](https://youtu.be/XzjLwMwhd_Y)\n* **Machine Learning - StarCraft 2 Python AI part 4 - Attack**  \n\n   * [https://youtu.be/90m-itTP\\_Zo](https://youtu.be/90m-itTP_Zo)\n\n**Intro to NN**\n\n* **Neural Networks (Deep Learning) - Part 1 Introduction**  \n\n   * [**https://youtu.be/SG9FYMKIu68**](https://youtu.be/SG9FYMKIu68)\n* **Neural Networks (Deep Learning) - Part 2 Types of NN**  \n\n   * [https://youtu.be/hp-wGZ4-CpU](https://youtu.be/hp-wGZ4-CpU)\n* **Neural Networks (Deep Learning) - Part 3 - Neurons &amp; Logistic Regression**  \n\n   * [https://youtu.be/mb-a9m5Yrzo](https://youtu.be/mb-a9m5Yrzo)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/itqgwm/moving_into_ai_2020_game_ai_supervised_learning/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "moving into ai - 2020 (game ai &amp; supervised learning) /!/ i would like to share some of my resources within the area of ml if these can help some of you to get started either with game ai's or supervised algorithms.\n\n**supervised machine learning algorithm**\n\n* **machine learning tutorial part 1 | machine learning for beginners**  \n\n   * [https://youtu.be/e3l\\_aegjkei](https://youtu.be/e3l_aegjkei)\n* **machine learning tutorial part 2 | machine learning for beginners - mi environment**  \n\n   * [https://youtu.be/hqyrqxydwpu](https://youtu.be/hqyrqxydwpu)\n* **machine learning tutorial part 3 | machine learning for beginners - python decision -----> tree !!! **  \n\n   * [https://youtu.be/8isucinsmys](https://youtu.be/8isucinsmys)\n* **machine learning tutorial part 4 | machine learning for beginners - python decision tree**  \n\n   * [https://youtu.be/24mxqzd3esu](https://youtu.be/24mxqzd3esu)\n* **machine learning tutorial part 5 | machine learning for beginners - python decision tree**  \n\n   * [https://youtu.be/avefkrfwjhc](https://youtu.be/avefkrfwjhc)\n* **machine learning tutorial part 6 | knn(friend recommender) for beginners**  \n\n   * [https://youtu.be/lk0zga6mr6k](https://youtu.be/lk0zga6mr6k)\n* **machine learning tutorial part 7 | machine learning for beginners - 5-fold cross validation**  \n\n   * [https://youtu.be/zx5cz8pxnom](https://youtu.be/zx5cz8pxnom)\n* **machine learning tutorial part 8 | accuracy(mae/rmse) - python machine learning for beginners**  \n\n   * [https://youtu.be/lhaepywngyy](https://youtu.be/lhaepywngyy)\n\n**sc2 - deepmind**\n\n* **machine learning - starcraft 2 python ai part 1**  \n\n   * [**https://youtu.be/wfugpcviil4**](https://youtu.be/wfugpcviil4)\n* **machine learning - starcraft 2 python ai part 2 - barracks**  \n\n   * [**https://youtu.be/o1fwjgjfh1w**](https://youtu.be/o1fwjgjfh1w)\n* **machine learning - starcraft 2 python ai part 3 - build marines**  \n\n   * [https://youtu.be/xzjlwmwhd\\_y](https://youtu.be/xzjlwmwhd_y)\n* **machine learning - starcraft 2 python ai part 4 - attack**  \n\n   * [https://youtu.be/90m-ittp\\_zo](https://youtu.be/90m-ittp_zo)\n\n**intro to nn**\n\n* **neural networks (deep learning) - part 1 introduction**  \n\n   * [**https://youtu.be/sg9fymkiu68**](https://youtu.be/sg9fymkiu68)\n* **neural networks (deep learning) - part 2 types of nn**  \n\n   * [https://youtu.be/hp-wgz4-cpu](https://youtu.be/hp-wgz4-cpu)\n* **neural networks (deep learning) - part 3 - neurons &amp; logistic regression**  \n\n   * [https://youtu.be/mb-a9m5yrzo](https://youtu.be/mb-a9m5yrzo)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/itqgwm/moving_into_ai_2020_game_ai_supervised_learning/',)", "identifyer": 5747459, "year": "2020"}, {"autor": "User1377420", "date": 1595435411000, "content": "I can't figure out Monte Carlo Tree Search /!/ I have now tried to inplement it several times for Tic Tac Toe and everytime the result (the best choice from the root node) is pretty much random. The best choice for the first move are the corners I think. This is my implementation: https://paste.ofcode.org/YhKRXmth4JYDM8kt6k4xu8\nI would appreciate it if anyone could help. (Player 1 is displayed as 1 on the board, Player 2 as 2 and nothing as 0)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hvwx7x/i_cant_figure_out_monte_carlo_tree_search/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "i can't figure out monte carlo -----> tree !!!  search /!/ i have now tried to inplement it several times for tic tac toe and everytime the result (the best choice from the root node) is pretty much random. the best choice for the first move are the corners i think. this is my implementation: https://paste.ofcode.org/yhkrxmth4jydm8kt6k4xu8\ni would appreciate it if anyone could help. (player 1 is displayed as 1 on the board, player 2 as 2 and nothing as 0)", "sortedWord": "None", "removed": "('nan',)", "score": 5, "comments": 60, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/hvwx7x/i_cant_figure_out_monte_carlo_tree_search/',)", "identifyer": 5747578, "year": "2020"}, {"autor": "_quanttrader_", "date": 1586580969000, "content": "A python library for decision tree visualization and model interpretation.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fywsz3/a_python_library_for_decision_tree_visualization/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "a python library for decision -----> tree !!!  visualization and model interpretation.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://github.com/parrt/dtreeviz',)", "identifyer": 5747746, "year": "2020"}, {"autor": "SQL_beginner", "date": 1605030161000, "content": "What kind of data is this? Panel Data? Longitudinal Data? Cross Section Data? Time Series Data? /!/ I created some fake data that represents the problem I am working with :\n\nhttps://imgur.com/a/jWsmulO\n\nIn short: Patients arrive at the hospital and have their \"health measurements\" taken. It is also recorded whether they were immediately discharged from the hospital, or if they were required to stay. Given a new patient that enters the hospital and their health measurements are taken, I need to predict the response_variable for this patient - will they be immediately discharged from the hospital, or will they have to stay?\n\nI have data from the last 50 years. It is obvious, that society, people's health and medicine have changed over the last 50 years (e.g. life expectancy has increased in many parts of the world). Perhaps in 1971, an elderly man who smoked his entire life would have had to stay for a prolonged period in the hospital - but in 2019, he might have been given a new drug that was developed in the last 5 years and then immediately discharged. The big question is : do these macro and micro time dependent trends within the variables need to be taken into account when creating statistical models?\n\nSo far, I created a decision tree (C5 algorithm) which completely ignores the \"time\" variable. That is, it is assumed that an elderly man from 1971 who smokes is identical to an elderly man that smokes in 2019. By completely ignoring the \"time\" variable, I was able to create a standard decision tree model that produces good results. I tested this two different ways:\n\n1st way: I trained the decision tree using a random 70% sample of the data, and recorded its performance on the remaining 30% of the data.\n\n2nd way: I kept data from 2019 separate. I then trained a decision tree using using data from previous years. Then I recorded the performance of the decision tree on 2019's data.\n\nIn both tests, the performance of the decision tree was good enough for my requirements.\n\nBut is just makes me curious: (looking at the imgur picture) what kind of data do I have? In statistics, is this considered Panel Data? Cross Section Data? Longitudinal Data? Time Series Data?\n\nI have a feeling that is not longitudinal data - as far as I understand, longitudinal data repeatedly records measurements on the same patient (e.g. once every year). As far as I am concerned, all patients in my data are unique. Perhaps I am dealing with multivariate time series data?\n\nCan someone please help me identify what kind of data this is? And what models are usually used for classifying this kind of data ?\n\nMy model results are good enough as is, but I am still curious.\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jrpitu/what_kind_of_data_is_this_panel_data_longitudinal/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "what kind of data is this? panel data? longitudinal data? cross section data? time series data? /!/ i created some fake data that represents the problem i am working with :\n\nhttps://imgur.com/a/jwsmulo\n\nin short: patients arrive at the hospital and have their \"health measurements\" taken. it is also recorded whether they were immediately discharged from the hospital, or if they were required to stay. given a new patient that enters the hospital and their health measurements are taken, i need to predict the response_variable for this patient - will they be immediately discharged from the hospital, or will they have to stay?\n\ni have data from the last 50 years. it is obvious, that society, people's health and medicine have changed over the last 50 years (e.g. life expectancy has increased in many parts of the world). perhaps in 1971, an elderly man who smoked his entire life would have had to stay for a prolonged period in the hospital - but in 2019, he might have been given a new drug that was developed in the last 5 years and then immediately discharged. the big question is : do these macro and micro time dependent trends within the variables need to be taken into account when creating statistical models?\n\nso far, i created a decision -----> tree !!!  (c5 algorithm) which completely ignores the \"time\" variable. that is, it is assumed that an elderly man from 1971 who smokes is identical to an elderly man that smokes in 2019. by completely ignoring the \"time\" variable, i was able to create a standard decision tree model that produces good results. i tested this two different ways:\n\n1st way: i trained the decision tree using a random 70% sample of the data, and recorded its performance on the remaining 30% of the data.\n\n2nd way: i kept data from 2019 separate. i then trained a decision tree using using data from previous years. then i recorded the performance of the decision tree on 2019's data.\n\nin both tests, the performance of the decision tree was good enough for my requirements.\n\nbut is just makes me curious: (looking at the imgur picture) what kind of data do i have? in statistics, is this considered panel data? cross section data? longitudinal data? time series data?\n\ni have a feeling that is not longitudinal data - as far as i understand, longitudinal data repeatedly records measurements on the same patient (e.g. once every year). as far as i am concerned, all patients in my data are unique. perhaps i am dealing with multivariate time series data?\n\ncan someone please help me identify what kind of data this is? and what models are usually used for classifying this kind of data ?\n\nmy model results are good enough as is, but i am still curious.\n\nthanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/jrpitu/what_kind_of_data_is_this_panel_data_longitudinal/',)", "identifyer": 5747805, "year": "2020"}, {"autor": "TheAnonymous123456", "date": 1604944567000, "content": "For some reason, searching to a lesser depth is yielding better results than searching greater in the minimax tree. /!/ I created a connect four AI where searching five moves deep the minimax tree caused the algorithm to be better than searching 6-7 moves deep. What could the possible causes be and the solution to those causes? My 6-7 deep search still blocks my moves and tries to win, but the 5 move ahead search is much better at winning.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jr2ijk/for_some_reason_searching_to_a_lesser_depth_is/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "for some reason, searching to a lesser depth is yielding better results than searching greater in the minimax -----> tree !!! . /!/ i created a connect four ai where searching five moves deep the minimax tree caused the algorithm to be better than searching 6-7 moves deep. what could the possible causes be and the solution to those causes? my 6-7 deep search still blocks my moves and tries to win, but the 5 move ahead search is much better at winning.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/jr2ijk/for_some_reason_searching_to_a_lesser_depth_is/',)", "identifyer": 5747848, "year": "2020"}, {"autor": "connar_with_a_t", "date": 1578595819000, "content": "Must a node reach purity /!/ So title, every decision tree example I have seen has the final node being pure. However, it seems like in real life you would lack to data to make this conclusion every time( overfitted). \n\nSo could a tree stop at an impure node? If so what does that look like?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/eme0zm/must_a_node_reach_purity/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "must a node reach purity /!/ so title, every decision -----> tree !!!  example i have seen has the final node being pure. however, it seems like in real life you would lack to data to make this conclusion every time( overfitted). \n\nso could a tree stop at an impure node? if so what does that look like?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/eme0zm/must_a_node_reach_purity/',)", "identifyer": 5747892, "year": "2020"}, {"autor": "Terrible_Schedule320", "date": 1607209707000, "content": "Question about one-hot encoding in Random Forests or similar models /!/ Hi, I am fairly new to ML and was wondering if someone could help me understand a couple things. I have been working on the Kaggle Titanic problem so that is the context but mainly I am just generally curious.\n\nI am aware of the \"Curse of Dimensionality\" and that one-hot encoding can make tree classifiers less effective. So let's say for example in the Titanic problem, I have the gender column and I know that this contributes highly to the outcome. Every example I have seen thus far uses the pd.getdummies() function to create one-hot columns for this data. Why would I not simply convert the column to, say, 0 for male and 1 for female?\n\nSame thing for say, \"Embarked\" where there are 3 possible options, you could possibly encode them as 0, 1, 2 in the same column rather than creating separate columns.\n\nI have tried doing this in my submissions but it did not seem to help my score in any way.\n\nI am aware that binary encoding is an option as well. I have tried, for example, using the binarizer() function on the gender column but it gives an error on the string input. From a logical standpoint, I would think that 0/1 encoding in the column has the same effect. \n\nSo in general, why would I use one-hot encoding for a categorical variable, particularly a \"binary\" variable like \"male/female\" and why is it not more effective to keep the data in a single column?\n\nPart of this is motivated by the fact that scikit-learn throws an error if I attempt to fit the model with string data still in the columns. I would think the backend would be \"smart\" enough to be able to handle such a task, i.e. the tree would just do something like \"If column A is string X then outcome 1, if string Y then outcome 2\". Is there any fundamental reason why this does not happen and strings need to be explicitly encoded one way or another, or is it just a limitation of sci-kit learn and there are libraries/utilities that could handle this out-of-the-box?\n\nI hope this all makes sense, like I said, I am very new to this so it's quite possible I've overlooked or misunderstood something, but it would be great if someone could assist me. Any help is appreciated, thank you.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k7hvw3/question_about_onehot_encoding_in_random_forests/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "question about one-hot encoding in random forests or similar models /!/ hi, i am fairly new to ml and was wondering if someone could help me understand a couple things. i have been working on the kaggle titanic problem so that is the context but mainly i am just generally curious.\n\ni am aware of the \"curse of dimensionality\" and that one-hot encoding can make -----> tree !!!  classifiers less effective. so let's say for example in the titanic problem, i have the gender column and i know that this contributes highly to the outcome. every example i have seen thus far uses the pd.getdummies() function to create one-hot columns for this data. why would i not simply convert the column to, say, 0 for male and 1 for female?\n\nsame thing for say, \"embarked\" where there are 3 possible options, you could possibly encode them as 0, 1, 2 in the same column rather than creating separate columns.\n\ni have tried doing this in my submissions but it did not seem to help my score in any way.\n\ni am aware that binary encoding is an option as well. i have tried, for example, using the binarizer() function on the gender column but it gives an error on the string input. from a logical standpoint, i would think that 0/1 encoding in the column has the same effect. \n\nso in general, why would i use one-hot encoding for a categorical variable, particularly a \"binary\" variable like \"male/female\" and why is it not more effective to keep the data in a single column?\n\npart of this is motivated by the fact that scikit-learn throws an error if i attempt to fit the model with string data still in the columns. i would think the backend would be \"smart\" enough to be able to handle such a task, i.e. the tree would just do something like \"if column a is string x then outcome 1, if string y then outcome 2\". is there any fundamental reason why this does not happen and strings need to be explicitly encoded one way or another, or is it just a limitation of sci-kit learn and there are libraries/utilities that could handle this out-of-the-box?\n\ni hope this all makes sense, like i said, i am very new to this so it's quite possible i've overlooked or misunderstood something, but it would be great if someone could assist me. any help is appreciated, thank you.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/k7hvw3/question_about_onehot_encoding_in_random_forests/',)", "identifyer": 5748141, "year": "2020"}, {"autor": "Reginald_Martin", "date": 1606741729000, "content": "Free Data Science Foundation Bootcamp /!/ **Free Data Science Foundation Bootcamp**\n\n* Knowledge-packed, project-based, 2 weeks intensive program\n* Hands-on practice live-online from 8 PM to 11 PM ET every day\n* Training and Mentoring from industry experts and super supportive TAs\n* A Certificate on successful completion of the program\n\n**Data Science Foundation Program**\n\nPick up Data Science by actually getting your hands dirty! Wondering whether you should go through all the trouble? Take this Bootcamp if ...\n\n* You are looking forward to start your data science career\n* You're on the fence on whether data science is the right choice\n* You want to see data science in action and talk to industry experts\n* You simply want to take up a data science project\n\n**Ready for a Data Science Career?**\n\nYear after year, the one profession that consistently ranks on top amongst emerging jobs is Data Science. Growing exponentially at more than 25% every year, Data Science finds increasing use in more and more industries by the day, with exciting applications such as self-driving cars, intelligent automation, and dynamic business decision support systems to show for. This is a growing profession whose demand in every industry and sector is increasing by the day.\n\nData Science is without a shadow of doubt, a bright and promising career choice.\n\n**Upcoming Dates:**\n\nThis program is scheduled to run from 8 pm to 11 pm ET every day for two weeks. Our instructors, mentors, and support team will be there to help keep it interactive, so feel to ask doubts, and have fun following along!\n\n**Start Date:** December 03, 2020\n\n**Duration:** 2 weeks\n\n**Time:** Mon - Fri ( 8:00 PM - 11:00 PM ET)\n\n**Program Curriculum**\n\nDesigned by our trainers who come with years of industry experience, this bootcamp is a blend of both up-to-date theory and relevant practical application. Broadly, the topics will cover in this program are -\n\n* Introduction to Linear Regression\n* Decision Tree Regression\n* Random Forest Regression\n* Decision Tree vs. Random Forest\n* Real-world Project\n\n**Project Overview: Predict the Success of a Startup**\n\nApply your newly gained knowledge on Data Science to predict a startup's success. This project would require everything you have learned throughout the program to effectively build a Decision tree and a Random Forest to predict a startup's success. We will look at the top 50 startup trends and features to predict the success of a startup. This portfolio-worthy project demonstrates your proficiency with the most sought after skills of machine learning in the market.\n\n[**Click here to Register**](https://www.eventbrite.com/e/free-data-science-foundation-bootcamp-tickets-130755331717)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k3vfxw/free_data_science_foundation_bootcamp/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "free data science foundation bootcamp /!/ **free data science foundation bootcamp**\n\n* knowledge-packed, project-based, 2 weeks intensive program\n* hands-on practice live-online from 8 pm to 11 pm et every day\n* training and mentoring from industry experts and super supportive tas\n* a certificate on successful completion of the program\n\n**data science foundation program**\n\npick up data science by actually getting your hands dirty! wondering whether you should go through all the trouble? take this bootcamp if ...\n\n* you are looking forward to start your data science career\n* you're on the fence on whether data science is the right choice\n* you want to see data science in action and talk to industry experts\n* you simply want to take up a data science project\n\n**ready for a data science career?**\n\nyear after year, the one profession that consistently ranks on top amongst emerging jobs is data science. growing exponentially at more than 25% every year, data science finds increasing use in more and more industries by the day, with exciting applications such as self-driving cars, intelligent automation, and dynamic business decision support systems to show for. this is a growing profession whose demand in every industry and sector is increasing by the day.\n\ndata science is without a shadow of doubt, a bright and promising career choice.\n\n**upcoming dates:**\n\nthis program is scheduled to run from 8 pm to 11 pm et every day for two weeks. our instructors, mentors, and support team will be there to help keep it interactive, so feel to ask doubts, and have fun following along!\n\n**start date:** december 03, 2020\n\n**duration:** 2 weeks\n\n**time:** mon - fri ( 8:00 pm - 11:00 pm et)\n\n**program curriculum**\n\ndesigned by our trainers who come with years of industry experience, this bootcamp is a blend of both up-to-date theory and relevant practical application. broadly, the topics will cover in this program are -\n\n* introduction to linear regression\n* decision -----> tree !!!  regression\n* random forest regression\n* decision -----> tree !!!  vs. random forest\n* real-world project\n\n**project overview: predict the success of a startup**\n\napply your newly gained knowledge on data science to predict a startup's success. this project would require everything you have learned throughout the program to effectively build a decision tree and a random forest to predict a startup's success. we will look at the top 50 startup trends and features to predict the success of a startup. this portfolio-worthy project demonstrates your proficiency with the most sought after skills of machine learning in the market.\n\n[**click here to register**](https://www.eventbrite.com/e/free-data-science-foundation-bootcamp-tickets-130755331717)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/k3vfxw/free_data_science_foundation_bootcamp/',)", "identifyer": 5748181, "year": "2020"}, {"autor": "Obvious_Ad4468", "date": 1606721735000, "content": "A suggestion of appropriate graph clustering methods for my use-case /!/ I am currently looking for graph clustering algorithms that can cluster a single graph into clusters, the graph in question is a weighted directed graph, and the clusters need to be based on its connections and high weight affinity (meaning nodes connected to other nodes with high weight shall be clustered). The end result shall be displayed like [this image](https://www.researchgate.net/profile/Thanh_Hai_Nguyen4/publication/268655306/figure/fig4/AS:671516505145353@1537113419521/An-example-for-the-graph-partitioning-problem-with-knapsack-constraints-K-3-on-Pcut-and.png).   \nI previously asked a question pertaining to spectral clustering but I do not think that the answer wrt PCA will work in my case as my dataset (adjacency matrix) is a 1-D array.   \nI have created the graph using networkx library using Digraph.\n\nMy assumptions:\n\n1- K-means cannot be used on weighted adjacency matrix\n\n2- Minimum Spanning Tree looks promising but I don't know whether to normalize weights\n\n3- Spectral clusters are giving results but when it comes to visualizing like the example given above, I do not know whether it is the right tool  \n4- Girvan Newman clustering algorithm also looks promising but I am finding conflicting views regarding its usage on weighted directed graphs\n\n5- Agglomerative clustering maybe?  \n\n\n6- Infomap is another good option but I intend to use its Python API, as using the main site's functionality produces only a single node as a cluster  \n\n\nCan anyone guide me to use an appropriate clustering algorithm for my use-case? and also provide Python libraries for obtaining the desired result as I am not proficient in Data Science, and writing algorithm in Python will be impossible for me", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k3raqs/a_suggestion_of_appropriate_graph_clustering/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "a suggestion of appropriate graph clustering methods for my use-case /!/ i am currently looking for graph clustering algorithms that can cluster a single graph into clusters, the graph in question is a weighted directed graph, and the clusters need to be based on its connections and high weight affinity (meaning nodes connected to other nodes with high weight shall be clustered). the end result shall be displayed like [this image](https://www.researchgate.net/profile/thanh_hai_nguyen4/publication/268655306/figure/fig4/as:671516505145353@1537113419521/an-example-for-the-graph-partitioning-problem-with-knapsack-constraints-k-3-on-pcut-and.png).   \ni previously asked a question pertaining to spectral clustering but i do not think that the answer wrt pca will work in my case as my dataset (adjacency matrix) is a 1-d array.   \ni have created the graph using networkx library using digraph.\n\nmy assumptions:\n\n1- k-means cannot be used on weighted adjacency matrix\n\n2- minimum spanning -----> tree !!!  looks promising but i don't know whether to normalize weights\n\n3- spectral clusters are giving results but when it comes to visualizing like the example given above, i do not know whether it is the right tool  \n4- girvan newman clustering algorithm also looks promising but i am finding conflicting views regarding its usage on weighted directed graphs\n\n5- agglomerative clustering maybe?  \n\n\n6- infomap is another good option but i intend to use its python api, as using the main site's functionality produces only a single node as a cluster  \n\n\ncan anyone guide me to use an appropriate clustering algorithm for my use-case? and also provide python libraries for obtaining the desired result as i am not proficient in data science, and writing algorithm in python will be impossible for me", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/k3raqs/a_suggestion_of_appropriate_graph_clustering/',)", "identifyer": 5748190, "year": "2020"}, {"autor": "tomk23_reddit", "date": 1603973839000, "content": "Decision Tree Leaf Nodes? /!/ So I just discovered that we can put as many leaf nodes as we want in decision tree and it turns out the accuracy from the infinity leaf nodes is of course, 100% accuracy.\n\nSo the question is, if every model that put unlimited leaf nodes to decision tree model come out as 100%, then how can decision tree can be reliable model?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jk8xql/decision_tree_leaf_nodes/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "decision -----> tree !!!  leaf nodes? /!/ so i just discovered that we can put as many leaf nodes as we want in decision tree and it turns out the accuracy from the infinity leaf nodes is of course, 100% accuracy.\n\nso the question is, if every model that put unlimited leaf nodes to decision tree model come out as 100%, then how can decision tree can be reliable model?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 17, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/jk8xql/decision_tree_leaf_nodes/',)", "identifyer": 5748338, "year": "2020"}, {"autor": "Maryellefoa", "date": 1603382253000, "content": "How do i choose the best model? /!/ So I have that well-known problem to resolve  with \"employee attrition/churn\". The database for it is all over the internet even on yt. People solved it with random forest and decision tree. But how did they choose this models? I am a begginner and i want to find out HOW can i determine the best model. Like, THE BEST. My teacher said something to me like \"you have to be better than the -0 information precision-\" and i can't find anything about this on the internet. So what is the best scientific approach to find the answer to \"what ml algorithm will best fit my employee attrition prediction dataset\" and how do I prove my option?? I have to write 10 pages about that so yeah I need some ideas on how to prove that a model is good, because the test on the test data and that coeficient of success in prediction seems not to be enough for my teacher.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jg2b9s/how_do_i_choose_the_best_model/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "how do i choose the best model? /!/ so i have that well-known problem to resolve  with \"employee attrition/churn\". the database for it is all over the internet even on yt. people solved it with random forest and decision -----> tree !!! . but how did they choose this models? i am a begginner and i want to find out how can i determine the best model. like, the best. my teacher said something to me like \"you have to be better than the -0 information precision-\" and i can't find anything about this on the internet. so what is the best scientific approach to find the answer to \"what ml algorithm will best fit my employee attrition prediction dataset\" and how do i prove my option?? i have to write 10 pages about that so yeah i need some ideas on how to prove that a model is good, because the test on the test data and that coeficient of success in prediction seems not to be enough for my teacher.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/jg2b9s/how_do_i_choose_the_best_model/',)", "identifyer": 5748422, "year": "2020"}, {"autor": "burdin271", "date": 1581888057000, "content": "Machine Learning Tutorial Part 3 | Machine Learning For Beginners - Python Decision Tree", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f4xh5p/machine_learning_tutorial_part_3_machine_learning/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "machine learning tutorial part 3 | machine learning for beginners - python decision -----> tree !!! ", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://youtu.be/8isUCINSmys',)", "identifyer": 5748475, "year": "2020"}, {"autor": "bb_boogie", "date": 1592377824000, "content": "Best algorithms to extract relevant features of.a variable? /!/ So far I've used a decision tree classifier and a support vector classifier to analyze the features most important for my target variable (a binary label ) I like that both models when trained on the dataset agree on 3 out of the top 5 features based on their importance (dt) and coefficient (svc). Which other estimators could I use to see similar things?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hambz4/best_algorithms_to_extract_relevant_features_ofa/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "best algorithms to extract relevant features of.a variable? /!/ so far i've used a decision -----> tree !!!  classifier and a support vector classifier to analyze the features most important for my target variable (a binary label ) i like that both models when trained on the dataset agree on 3 out of the top 5 features based on their importance (dt) and coefficient (svc). which other estimators could i use to see similar things?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 9, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/hambz4/best_algorithms_to_extract_relevant_features_ofa/',)", "identifyer": 5748951, "year": "2020"}, {"autor": "bioinfo_ml", "date": 1584974149000, "content": "How best to understand reasoning for model performances? /!/ I've got a regression classification problem where I compare a few scikit-learn models (random forest, gradient boosting, extreme gradient boosting, decision tree, k-nearest neighbours, and support vector  machine for regression). Almost all models perform with an r2 between 0.5-0.8 on nested cross validation except the support vector machine which is at -3.2 r2 - what ways are there for me to investigate why this is? I've looked at permutating feature importance, it finds no features important, but not sure whatever steps I can take to figure out why. I do gridsearch parameter tuning so also trying a range of parameters - is it likely I am not tuning these well enough or something else I should look into?  \n\n\nSVM parameter tuning I used is based on:   \n\n\n`svr_params = {`\n\n`'kernel':('linear', 'rbf'),` \n\n`'C':(0.1, 0.25, 0.5, 0.75, 1.0)}`  \n\n\nI'm new to machine learning so any help in the right direcion would be appreciated", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fnkymb/how_best_to_understand_reasoning_for_model/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "how best to understand reasoning for model performances? /!/ i've got a regression classification problem where i compare a few scikit-learn models (random forest, gradient boosting, extreme gradient boosting, decision -----> tree !!! , k-nearest neighbours, and support vector  machine for regression). almost all models perform with an r2 between 0.5-0.8 on nested cross validation except the support vector machine which is at -3.2 r2 - what ways are there for me to investigate why this is? i've looked at permutating feature importance, it finds no features important, but not sure whatever steps i can take to figure out why. i do gridsearch parameter tuning so also trying a range of parameters - is it likely i am not tuning these well enough or something else i should look into?  \n\n\nsvm parameter tuning i used is based on:   \n\n\n`svr_params = {`\n\n`'kernel':('linear', 'rbf'),` \n\n`'c':(0.1, 0.25, 0.5, 0.75, 1.0)}`  \n\n\ni'm new to machine learning so any help in the right direcion would be appreciated", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/fnkymb/how_best_to_understand_reasoning_for_model/',)", "identifyer": 5749017, "year": "2020"}, {"autor": "promach", "date": 1584868415000, "content": "Empirical example of MCTS calculation PUCT formula /!/ Would anyone remind me how to [calculate](https://slides.com/crem/lc0#/9) the [values inside each of the MCTS nodes before AND after a child node is created](https://medium.com/@quasimik/monte-carlo-tree-search-applied-to-letterpress-34f41c86e238) ?\n\n&amp;#x200B;\n\nhttps://preview.redd.it/gsjgpxgox6o41.png?width=2040&amp;format=png&amp;auto=webp&amp;s=328adfb8d10fa796eb85081bf13a69f11c12db1b\n\n&amp;#x200B;\n\nhttps://preview.redd.it/by5b1kxyx6o41.png?width=1145&amp;format=png&amp;auto=webp&amp;s=aeed42eb0defa05224156e12a32c44310d2e7397", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fmx3kv/empirical_example_of_mcts_calculation_puct_formula/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "empirical example of mcts calculation puct formula /!/ would anyone remind me how to [calculate](https://slides.com/crem/lc0#/9) the [values inside each of the mcts nodes before and after a child node is created](https://medium.com/@quasimik/monte-carlo------> tree !!! -search-applied-to-letterpress-34f41c86e238) ?\n\n&amp;#x200b;\n\nhttps://preview.redd.it/gsjgpxgox6o41.png?width=2040&amp;format=png&amp;auto=webp&amp;s=328adfb8d10fa796eb85081bf13a69f11c12db1b\n\n&amp;#x200b;\n\nhttps://preview.redd.it/by5b1kxyx6o41.png?width=1145&amp;format=png&amp;auto=webp&amp;s=aeed42eb0defa05224156e12a32c44310d2e7397", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 5, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/fmx3kv/empirical_example_of_mcts_calculation_puct_formula/',)", "identifyer": 5749058, "year": "2020"}, {"autor": "lucascrafter", "date": 1596887511000, "content": "Machine Learning For Beginners - Python Decision Tree", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i5xuzs/machine_learning_for_beginners_python_decision/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "machine learning for beginners - python decision -----> tree !!! ", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://youtu.be/aVEfKRfWjHc',)", "identifyer": 5749134, "year": "2020"}, {"autor": "StrasJam", "date": 1590565045000, "content": "Is it possible to accurately classify these two small and overlapping classes? /!/ Disclaimer: I am a bit limited in my machine learning/stats knowledge. I come from more of a coding background. \n\nI have a small dataset that I am trying to classify into two groups. Each member of each group represents a tree branch that has a certain percentage of damage from pests (this is the Y variable). Overall, the two groups come from a larger dataset where the mean value of damage does show a greater difference between the groups, but we only have a few images of the branches so we are stuck with this small subset.\n\n  \nGroup 1: Y variable: \\[ 26.7,  33.3,  40.0,  32.0,  42.9,  16.0,  6.3,  37.8,  28.6,  29.4,  59.1\\]\n\nGroup 2: Y variable: \\[ 43.8,  15.4,  72.2,  16.0,  85.7,  0.0\\]\n\nI am trying to use a RF to classify them using a number of spectral features from the image (about 13 features). Since the dataset is so small I just use a leave-on-out cross validation at the moment to test the performance.\n\nTo me, I feel that given the two groups don't really have much seperation in terms of their percentage of damage, that no classification method I try is going to achieve a very high accuracy (so far I achieve 65% accuracy). \n\nDoes this seem reasonable? Or do you think that I could manage to get better performance on this subset, though a different classifier perhaps (I have read that SVM can work better with low amounts of data)?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/grf6qi/is_it_possible_to_accurately_classify_these_two/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "is it possible to accurately classify these two small and overlapping classes? /!/ disclaimer: i am a bit limited in my machine learning/stats knowledge. i come from more of a coding background. \n\ni have a small dataset that i am trying to classify into two groups. each member of each group represents a -----> tree !!!  branch that has a certain percentage of damage from pests (this is the y variable). overall, the two groups come from a larger dataset where the mean value of damage does show a greater difference between the groups, but we only have a few images of the branches so we are stuck with this small subset.\n\n  \ngroup 1: y variable: \\[ 26.7,  33.3,  40.0,  32.0,  42.9,  16.0,  6.3,  37.8,  28.6,  29.4,  59.1\\]\n\ngroup 2: y variable: \\[ 43.8,  15.4,  72.2,  16.0,  85.7,  0.0\\]\n\ni am trying to use a rf to classify them using a number of spectral features from the image (about 13 features). since the dataset is so small i just use a leave-on-out cross validation at the moment to test the performance.\n\nto me, i feel that given the two groups don't really have much seperation in terms of their percentage of damage, that no classification method i try is going to achieve a very high accuracy (so far i achieve 65% accuracy). \n\ndoes this seem reasonable? or do you think that i could manage to get better performance on this subset, though a different classifier perhaps (i have read that svm can work better with low amounts of data)?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/grf6qi/is_it_possible_to_accurately_classify_these_two/',)", "identifyer": 5749195, "year": "2020"}, {"autor": "sonicking12", "date": 1589643311000, "content": "[Question]Boolean predictor in Machine Learning and Decision Tree /!/  Hi,\n\nThis is a question for R. If we have Boolean predictors (value is either 1 or 0), do I still need to explicitly make it as.factor before they go into Random Forest or Decision Tree?\n\nBut I also want to know in general. Do those algorithms treat a numeric Boolean predictor differently from a two-level categorical variable (say, with value being either Male or Female)? Thanks.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gkwl9d/questionboolean_predictor_in_machine_learning_and/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "[question]boolean predictor in machine learning and decision -----> tree !!!  /!/  hi,\n\nthis is a question for r. if we have boolean predictors (value is either 1 or 0), do i still need to explicitly make it as.factor before they go into random forest or decision tree?\n\nbut i also want to know in general. do those algorithms treat a numeric boolean predictor differently from a two-level categorical variable (say, with value being either male or female)? thanks.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/gkwl9d/questionboolean_predictor_in_machine_learning_and/',)", "identifyer": 5749351, "year": "2020"}, {"autor": "[deleted]", "date": 1589564644000, "content": "Rattle decision tree question /!/ [deleted]", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gkdrt6/rattle_decision_tree_question/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "rattle decision -----> tree !!!  question /!/ [deleted]", "sortedWord": "None", "removed": "('deleted',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/gkdrt6/rattle_decision_tree_question/',)", "identifyer": 5749374, "year": "2020"}, {"autor": "NightKing_GOT", "date": 1590689750000, "content": "Time Series methods vs Regression/Tree Models /!/ I am working on a problem where I need to forecast sales. The model should be fairly accurate as well as tell us the key drivers for the sales and their impact. This makes you choose regression or tree based models. We can forecast sales based on the values of its predictors at the desired forecast time. The issue with these models is there are some predictors whose value we don't know for next week or next month to make a weekly or monthly sales forecast. Now if I choose Times series based models then I don't know the key drivers for sales. Has anyone faced this situation before? It would be great if you can share your experience on to handle this", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gsbhni/time_series_methods_vs_regressiontree_models/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "time series methods vs regression/-----> tree !!!  models /!/ i am working on a problem where i need to forecast sales. the model should be fairly accurate as well as tell us the key drivers for the sales and their impact. this makes you choose regression or tree based models. we can forecast sales based on the values of its predictors at the desired forecast time. the issue with these models is there are some predictors whose value we don't know for next week or next month to make a weekly or monthly sales forecast. now if i choose times series based models then i don't know the key drivers for sales. has anyone faced this situation before? it would be great if you can share your experience on to handle this", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/gsbhni/time_series_methods_vs_regressiontree_models/',)", "identifyer": 5749552, "year": "2020"}, {"autor": "goddySHO", "date": 1608703747000, "content": "Multiclass classification XGBoost output /!/ Hi all,\n\nI have been working on a project using multiclass classification with mostly tree based models. The project was first implemented in Python and then implemented in R. I believe we ended up using the SkLearn wrapper for XGBoost in Python and when we switched to R, re-training on the same train/test split such as Python with native XGBoost gave us better results overall, (higher AUC and slightly higher accuracy) as well. \n\nNow the problem that I am facing is, when I tried to import the same .model file into Python and run inference on the same test set again, just to check if the native implementation outperforms in both languages, I cannot seem to get the probabilities for all the classes. Can someone please help with this? \n\nModel creation in R -\n\n    # Create XGBoost Dmatrix\n    X_train = subset(training, select = -c(DIAS_PAGO_FLAG))\n    y_train = subset(training, select = c(DIAS_PAGO_FLAG))\n    X_val = subset(validation, select = -c(DIAS_PAGO_FLAG))\n    y_val = subset(validation, select = c(DIAS_PAGO_FLAG))\n    \n    dtrain = xgb.DMatrix(data = as.matrix(X_train), label = as.matrix(y_train))\n    dtest = xgb.DMatrix(data = as.matrix(X_val), label = as.matrix(y_val))\n    \n    set.seed(0)\n    \n    # XGBoost model training\n    \n    num_class = 5\n    params = list(\n      booster=\"gbtree\",\n      # n_estimators = 250,\n      max_depth=15,\n      gamma=0,\n      subsample= 1,\n      colsample_bytree=0.3,\n      objective=\"multi:softmax\",\n      eval_metric=\"mlogloss\",\n      num_class=num_class,\n      nthread = -1\n    )\n    \n    model = xgboost(\n      params = params,\n      data = dtrain,\n      nrounds = 100\n    )\n    \n    y_pred = predict(model, dtest)\n    \n    test_prediction = matrix(y_pred, nrow = num_class,\n                             ncol=length(y_pred)/num_class) %&gt;%\n      t() %&gt;%\n      data.frame() %&gt;%\n      mutate(label = y_val + 1,\n             max_prob = max.col(., \"last\"))\n    \n    confusionMatrix(factor(test_prediction$max_prob),\n                    factor(test_prediction$label$DIAS_PAGO_FLAG))\n    \n    roc_multi = multiclass.roc(test_prediction$max_prob, test_prediction$label$DIAS_PAGO_FLAG, direction = \"auto\")\n    print(roc_multi)\n    \n    Multi-class area under the curve: 0.8931\n    \n    # Saved model file to be reused in Python\n    xgb.save(model, \"XGB_Model_R_v2.model\")\n\nSo I got the AUC value of around 0.89, running a similar implementation in Python throws an error -\n\n    # I already got the same train &amp; test files loaded separately\n    X_train = train.drop(\"DIAS_PAGO_FLAG\", axis = 1)\n    y_train = train[\"DIAS_PAGO_FLAG\"]\n    X_test = test.drop(\"DIAS_PAGO_FLAG\", axis = 1)\n    y_test = test[\"DIAS_PAGO_FLAG\"]\n    \n    boost = xgb.Booster({'nthread': -1})\n    boost.load_model(\"XGB_Model_R_v2.model\")\n    \n    dtest = xgb.DMatrix(X_test)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kinm4l/multiclass_classification_xgboost_output/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "multiclass classification xgboost output /!/ hi all,\n\ni have been working on a project using multiclass classification with mostly -----> tree !!!  based models. the project was first implemented in python and then implemented in r. i believe we ended up using the sklearn wrapper for xgboost in python and when we switched to r, re-training on the same train/test split such as python with native xgboost gave us better results overall, (higher auc and slightly higher accuracy) as well. \n\nnow the problem that i am facing is, when i tried to import the same .model file into python and run inference on the same test set again, just to check if the native implementation outperforms in both languages, i cannot seem to get the probabilities for all the classes. can someone please help with this? \n\nmodel creation in r -\n\n    # create xgboost dmatrix\n    x_train = subset(training, select = -c(dias_pago_flag))\n    y_train = subset(training, select = c(dias_pago_flag))\n    x_val = subset(validation, select = -c(dias_pago_flag))\n    y_val = subset(validation, select = c(dias_pago_flag))\n    \n    dtrain = xgb.dmatrix(data = as.matrix(x_train), label = as.matrix(y_train))\n    dtest = xgb.dmatrix(data = as.matrix(x_val), label = as.matrix(y_val))\n    \n    set.seed(0)\n    \n    # xgboost model training\n    \n    num_class = 5\n    params = list(\n      booster=\"gbtree\",\n      # n_estimators = 250,\n      max_depth=15,\n      gamma=0,\n      subsample= 1,\n      colsample_bytree=0.3,\n      objective=\"multi:softmax\",\n      eval_metric=\"mlogloss\",\n      num_class=num_class,\n      nthread = -1\n    )\n    \n    model = xgboost(\n      params = params,\n      data = dtrain,\n      nrounds = 100\n    )\n    \n    y_pred = predict(model, dtest)\n    \n    test_prediction = matrix(y_pred, nrow = num_class,\n                             ncol=length(y_pred)/num_class) %&gt;%\n      t() %&gt;%\n      data.frame() %&gt;%\n      mutate(label = y_val + 1,\n             max_prob = max.col(., \"last\"))\n    \n    confusionmatrix(factor(test_prediction$max_prob),\n                    factor(test_prediction$label$dias_pago_flag))\n    \n    roc_multi = multiclass.roc(test_prediction$max_prob, test_prediction$label$dias_pago_flag, direction = \"auto\")\n    print(roc_multi)\n    \n    multi-class area under the curve: 0.8931\n    \n    # saved model file to be reused in python\n    xgb.save(model, \"xgb_model_r_v2.model\")\n\nso i got the auc value of around 0.89, running a similar implementation in python throws an error -\n\n    # i already got the same train &amp; test files loaded separately\n    x_train = train.drop(\"dias_pago_flag\", axis = 1)\n    y_train = train[\"dias_pago_flag\"]\n    x_test = test.drop(\"dias_pago_flag\", axis = 1)\n    y_test = test[\"dias_pago_flag\"]\n    \n    boost = xgb.booster({'nthread': -1})\n    boost.load_model(\"xgb_model_r_v2.model\")\n    \n    dtest = xgb.dmatrix(x_test)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/kinm4l/multiclass_classification_xgboost_output/',)", "identifyer": 5749627, "year": "2020"}, {"autor": "dhiraj8899", "date": 1608668385000, "content": "Why Decision Tree Machine Learning algorithm is NOT affected by feature magnitude /!/ [https://youtu.be/ML5wOKCr5f4](https://youtu.be/ML5wOKCr5f4)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kid6n5/why_decision_tree_machine_learning_algorithm_is/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "why decision -----> tree !!!  machine learning algorithm is not affected by feature magnitude /!/ [https://youtu.be/ml5wokcr5f4](https://youtu.be/ml5wokcr5f4)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/kid6n5/why_decision_tree_machine_learning_algorithm_is/',)", "identifyer": 5749639, "year": "2020"}, {"autor": "deep-data-diver", "date": 1584251291000, "content": "First ML Submission on Kaggle ML Titanic Challenge \u2014 77% accuracy! (: Any ideas how I could improve to get better? /!/ I\u2019m at a break point between considering my self a complete newbie and having a general idea of what I\u2019m doing when it comes to ML with python. \n\nThis last week I finished up my ML Titanic challenge on kaggle with a decision tree training model and a logistic regression model. I\u2019ve been hovering around 77% percent accuracy and have tried extracting as many features as possible from the data set; unfortunately it seems that the label most heavily favorited is the gender and none of the other features do much to increase accuracy (when i run the model with just gender it does not differ much when I run it with the rest of the labels).\n\nI\u2019m pretty happy with the results so far cause my knowledge of python and ML has improved a lot but want to get better. \n\n\nI\u2019m curious does anyone who is familiar with this challenge have any tips on what I could do to get my accuracy higher that would help me learn new ML methods? \n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fiwqrb/first_ml_submission_on_kaggle_ml_titanic/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "first ml submission on kaggle ml titanic challenge \u2014 77% accuracy! (: any ideas how i could improve to get better? /!/ i\u2019m at a break point between considering my self a complete newbie and having a general idea of what i\u2019m doing when it comes to ml with python. \n\nthis last week i finished up my ml titanic challenge on kaggle with a decision -----> tree !!!  training model and a logistic regression model. i\u2019ve been hovering around 77% percent accuracy and have tried extracting as many features as possible from the data set; unfortunately it seems that the label most heavily favorited is the gender and none of the other features do much to increase accuracy (when i run the model with just gender it does not differ much when i run it with the rest of the labels).\n\ni\u2019m pretty happy with the results so far cause my knowledge of python and ml has improved a lot but want to get better. \n\n\ni\u2019m curious does anyone who is familiar with this challenge have any tips on what i could do to get my accuracy higher that would help me learn new ml methods? \n\nthanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/fiwqrb/first_ml_submission_on_kaggle_ml_titanic/',)", "identifyer": 5749737, "year": "2020"}, {"autor": "lukescriptwalker", "date": 1588315724000, "content": "Machine Learning Weekend - Sharing is Caring (Post your resources) /!/ 3 months ago, I created and posted videos from my Python machine learning course on the learnProgramming subreddit. So far they have been received very well! I have also posted some of the videos on this Subreddit but would like to do a combined post as weekend is approaching. So if someone is going to dig into Machine Learning here are some initial resources. For people who are already well digged into Machine Learning please share your ressources in the comment section below, for me and others to benefit from.\n\n* **Part 1 - Machine Learning For Beginners - Basics**\n\n[https://youtu.be/E3l\\_aeGjkeI](https://youtu.be/E3l_aeGjkeI)\n\n* **Part 2 - MI environment**\n\n[https://youtu.be/HqyrqxyDwPU](https://youtu.be/HqyrqxyDwPU)\n\n* **Part 3 - Python Decision Tree (Theory)**\n\n[https://youtu.be/8isUCINSmys](https://youtu.be/8isUCINSmys)\n\n* **Part 4 - Python Decision Tree (Coding)**\n\n[https://youtu.be/24mxQzd3EsU](https://youtu.be/24mxQzd3EsU)\n\n* **Part 5 - Python Decision Tree (Graphiviz)**\n\n[https://youtu.be/aVEfKRfWjHc](https://youtu.be/aVEfKRfWjHc)\n\n* **Part 6 - Knn(Friend Recommender)**\n\n[https://youtu.be/LK0zgA6Mr6k](https://youtu.be/LK0zgA6Mr6k)\n\n* **Part 7- 5-Fold Cross Validation**\n\n[https://youtu.be/Zx5cz8pXnOM](https://youtu.be/Zx5cz8pXnOM)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gbd9ne/machine_learning_weekend_sharing_is_caring_post/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "machine learning weekend - sharing is caring (post your resources) /!/ 3 months ago, i created and posted videos from my python machine learning course on the learnprogramming subreddit. so far they have been received very well! i have also posted some of the videos on this subreddit but would like to do a combined post as weekend is approaching. so if someone is going to dig into machine learning here are some initial resources. for people who are already well digged into machine learning please share your ressources in the comment section below, for me and others to benefit from.\n\n* **part 1 - machine learning for beginners - basics**\n\n[https://youtu.be/e3l\\_aegjkei](https://youtu.be/e3l_aegjkei)\n\n* **part 2 - mi environment**\n\n[https://youtu.be/hqyrqxydwpu](https://youtu.be/hqyrqxydwpu)\n\n* **part 3 - python decision -----> tree !!!  (theory)**\n\n[https://youtu.be/8isucinsmys](https://youtu.be/8isucinsmys)\n\n* **part 4 - python decision tree (coding)**\n\n[https://youtu.be/24mxqzd3esu](https://youtu.be/24mxqzd3esu)\n\n* **part 5 - python decision tree (graphiviz)**\n\n[https://youtu.be/avefkrfwjhc](https://youtu.be/avefkrfwjhc)\n\n* **part 6 - knn(friend recommender)**\n\n[https://youtu.be/lk0zga6mr6k](https://youtu.be/lk0zga6mr6k)\n\n* **part 7- 5-fold cross validation**\n\n[https://youtu.be/zx5cz8pxnom](https://youtu.be/zx5cz8pxnom)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/gbd9ne/machine_learning_weekend_sharing_is_caring_post/',)", "identifyer": 5750568, "year": "2020"}, {"autor": "Sturm_Liouville2", "date": 1588294558000, "content": "C-tree Relative Importance Interpretation? /!/ Hi everyone, sorry if this has already been asked, I can\u2019t find an answer to this anywhere: \n\nwhen relative importance is derived using C-tree  conditional permutation, how do you interpret the differences in relative importance numbers between different variables? \n\nFor example, if one variable has an importance twice that of another, is it twice as important, 4x?... etc.\n\nAny help is appreciated", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gb8ewv/ctree_relative_importance_interpretation/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "c------> tree !!!  relative importance interpretation? /!/ hi everyone, sorry if this has already been asked, i can\u2019t find an answer to this anywhere: \n\nwhen relative importance is derived using c-tree  conditional permutation, how do you interpret the differences in relative importance numbers between different variables? \n\nfor example, if one variable has an importance twice that of another, is it twice as important, 4x?... etc.\n\nany help is appreciated", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/gb8ewv/ctree_relative_importance_interpretation/',)", "identifyer": 5750578, "year": "2020"}, {"autor": "pranayprasad3", "date": 1588234666000, "content": "What should i do next ? /!/ Hello guys , I am quite new to Machine Learning. I recently completed a 40hr Udemy course on it. The course covered topics:\n\nRegression :\n1. Simple Linear Regression\n2. Muliple Linear Regression\n3. Polynomial Regression\n4. Support Vector Machine (SVR)\n5. Decision Tree Regression\n6. Random Forest Regression\n\nClassification :\n1. Logistic Regression\n2. K- Nearest Neighbours\n3. Support Vector Machines\n4. Kernel Support Vector Machines\n5. Naive Based Classification\n6. Decision Tree Classification\n7. Random Forest Classification\n\nEvaluating Classification model's performance:\n1. FALSE positives and False Negatives on Confusion Matrix\n2. Accuracy Investment\n3. CAP curve\n\nClustering:\n1. K-Means Clustering\n2. Hierarchical Clustering\n\nAssociation :\n1. Apriori\n2. Eclat\n\nReinforcement Learning:\n1. Upper Confidence Bound\n2. Thompson's Sampling\n\nNatural Language Processing (Basic)\n\nDeep Learning:\n1. Artificial Neural Networks\n2. Convolutional Neural Network\n\nDimensionality Reduction :\n1. Principal Component Analysis (PCA)\n2. Linear Discriminant Analysis\n3. Kernel PCA\n\nModel Selection and Boosting :\n1. K - Fold Cross Validation\n2. Grid Search\n3. XGBoost\n\nCould you please advise me what do do next ? I guess i should start working on some projects on Kaggle. It looks very daunting to me seeing the big codes in that website. I guess i will start small. Could you please advise what can i do more (Like topics to cover , tools to learn like Power Bi ) to improve myself. I really want to get a job in this. Thank You.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/garphv/what_should_i_do_next/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "what should i do next ? /!/ hello guys , i am quite new to machine learning. i recently completed a 40hr udemy course on it. the course covered topics:\n\nregression :\n1. simple linear regression\n2. muliple linear regression\n3. polynomial regression\n4. support vector machine (svr)\n5. decision -----> tree !!!  regression\n6. random forest regression\n\nclassification :\n1. logistic regression\n2. k- nearest neighbours\n3. support vector machines\n4. kernel support vector machines\n5. naive based classification\n6. decision tree classification\n7. random forest classification\n\nevaluating classification model's performance:\n1. false positives and false negatives on confusion matrix\n2. accuracy investment\n3. cap curve\n\nclustering:\n1. k-means clustering\n2. hierarchical clustering\n\nassociation :\n1. apriori\n2. eclat\n\nreinforcement learning:\n1. upper confidence bound\n2. thompson's sampling\n\nnatural language processing (basic)\n\ndeep learning:\n1. artificial neural networks\n2. convolutional neural network\n\ndimensionality reduction :\n1. principal component analysis (pca)\n2. linear discriminant analysis\n3. kernel pca\n\nmodel selection and boosting :\n1. k - fold cross validation\n2. grid search\n3. xgboost\n\ncould you please advise me what do do next ? i guess i should start working on some projects on kaggle. it looks very daunting to me seeing the big codes in that website. i guess i will start small. could you please advise what can i do more (like topics to cover , tools to learn like power bi ) to improve myself. i really want to get a job in this. thank you.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 7, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/garphv/what_should_i_do_next/',)", "identifyer": 5750613, "year": "2020"}, {"autor": "user19911506", "date": 1588176363000, "content": "Impact of large number of continuous features on decision tree /!/ So I was building a diagnostic classification model to understand behavioural pattern of shoppers and have about 500 odd features, I have around 500k users to go along with them.\n\nThe decision to build a decision tree was because I wanted to visualize how each feature split impact the class distribution. This information is useful in another process to make decisions.\n\nNow obviously with 500 features it will be hard to do this, so will use hyper parameterd like minimum number of samples in a class and max tree depth.\n\nStill I feel the huge number of features can cause issues, the alternative could be to use important variables from a rf output and feed it to the decision tree.\n\nSo any thoughts?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gac7uf/impact_of_large_number_of_continuous_features_on/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "impact of large number of continuous features on decision -----> tree !!!  /!/ so i was building a diagnostic classification model to understand behavioural pattern of shoppers and have about 500 odd features, i have around 500k users to go along with them.\n\nthe decision to build a decision tree was because i wanted to visualize how each feature split impact the class distribution. this information is useful in another process to make decisions.\n\nnow obviously with 500 features it will be hard to do this, so will use hyper parameterd like minimum number of samples in a class and max tree depth.\n\nstill i feel the huge number of features can cause issues, the alternative could be to use important variables from a rf output and feed it to the decision tree.\n\nso any thoughts?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/gac7uf/impact_of_large_number_of_continuous_features_on/',)", "identifyer": 5750638, "year": "2020"}, {"autor": "dhiraj8899", "date": 1601407533000, "content": "Learn Machine Learning Algorithms from Scratch, like Neural Network, Decision Tree, Logistic Regression, Linear Regression, and Gradient Descent", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j26ac7/learn_machine_learning_algorithms_from_scratch/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "learn machine learning algorithms from scratch, like neural network, decision -----> tree !!! , logistic regression, linear regression, and gradient descent", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('link',)", "medialink": "('https://www.youtube.com/channel/UCuOT2b1Umrr0MittMzuxNcA?sub_confirmation=1',)", "identifyer": 5750687, "year": "2020"}, {"autor": "CkmCpvis", "date": 1598899275000, "content": "Rain Detection /!/ I am trying to make program that can determine if it has been raining from images alone. I have played with image pre-processing like the photo below where the left side includes all moving objects (rain/cars) and the right side is the same image with the large clusters (cars/other) removed:\n\nhttps://preview.redd.it/lgb39pgvsdk51.png?width=3782&amp;format=png&amp;auto=webp&amp;s=4fa817e9c187e1530250f32cb8b7494e94134d58\n\nThis worked well, unfortunately it turns out the cameras I will be using are potatoes (See below):\n\nhttps://preview.redd.it/7k1i6vicqdk51.png?width=1374&amp;format=png&amp;auto=webp&amp;s=94ada1a28227a45c2808e2dd8efa70656f676bf9\n\nhttps://preview.redd.it/02efod2eqdk51.png?width=1425&amp;format=png&amp;auto=webp&amp;s=d3245197bb26c7f758f9a950152c36cc98f7410c\n\nhttps://preview.redd.it/56dkfj8gqdk51.png?width=1433&amp;format=png&amp;auto=webp&amp;s=0b18d9ff91d6be78cb8ddf45ca7bedee0c8f39ad\n\nhow should I generalize?\n\nlabel the sky types (95% show the sky)?\n\nlabel head lights and determine if they are on or off (most headlights are automatic and turn on even it over cast but we could do like percent of headlights on, this could only be used as supporting evidence)?\n\nroad reflectance (because the road gets shinny lol)\n\ntime of day vs brightness?\n\n&amp;#x200B;\n\nor maybe something to do with noticing how the image gets hazy when it rains?\n\n&amp;#x200B;\n\na decision tree that combines any of the above?\n\n&amp;#x200B;\n\nwhat do you think?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ik3ttf/rain_detection/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "rain detection /!/ i am trying to make program that can determine if it has been raining from images alone. i have played with image pre-processing like the photo below where the left side includes all moving objects (rain/cars) and the right side is the same image with the large clusters (cars/other) removed:\n\nhttps://preview.redd.it/lgb39pgvsdk51.png?width=3782&amp;format=png&amp;auto=webp&amp;s=4fa817e9c187e1530250f32cb8b7494e94134d58\n\nthis worked well, unfortunately it turns out the cameras i will be using are potatoes (see below):\n\nhttps://preview.redd.it/7k1i6vicqdk51.png?width=1374&amp;format=png&amp;auto=webp&amp;s=94ada1a28227a45c2808e2dd8efa70656f676bf9\n\nhttps://preview.redd.it/02efod2eqdk51.png?width=1425&amp;format=png&amp;auto=webp&amp;s=d3245197bb26c7f758f9a950152c36cc98f7410c\n\nhttps://preview.redd.it/56dkfj8gqdk51.png?width=1433&amp;format=png&amp;auto=webp&amp;s=0b18d9ff91d6be78cb8ddf45ca7bedee0c8f39ad\n\nhow should i generalize?\n\nlabel the sky types (95% show the sky)?\n\nlabel head lights and determine if they are on or off (most headlights are automatic and turn on even it over cast but we could do like percent of headlights on, this could only be used as supporting evidence)?\n\nroad reflectance (because the road gets shinny lol)\n\ntime of day vs brightness?\n\n&amp;#x200b;\n\nor maybe something to do with noticing how the image gets hazy when it rains?\n\n&amp;#x200b;\n\na decision -----> tree !!!  that combines any of the above?\n\n&amp;#x200b;\n\nwhat do you think?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/ik3ttf/rain_detection/',)", "identifyer": 5750829, "year": "2020"}, {"autor": "learning_with_data", "date": 1595874994000, "content": "NLP classification/clustering of work orders in Python /!/ I\u2019m trying to learn some NLP / ML tasks, but I\u2019m having trouble wrapping my head around some parts. Any guidance is greatly appreciated.\n\nEnd goal: classify work orders (500+ classes)/perform clustering\n\nDataset description:\n\nWork order documents (\\~200k rows) with:\n\n* customer complaint\n* work order number\n* applicable service bulletin\n* model number\n* product family\n* manufacture date\n* manufacture plant id\n* manufacture batch id\n* assembly batch id\n* assembly part number\n* component number\n* technician(s)\n\nA single work order may have multiple row entries if more than one service bulletin applies to the work order. \n\nCustomer complaint is not null, but complaint may be as simple as \u201cBees\u201d up to very technical descriptions. Some are long-winded stories with examples, others are boilerplate comments added by a CSR. There are often references to specific components.\n\nThe manufacture date is a timestamp in mm/dd/YYYY format. The remaining fields are all alphanumeric strings.\n\nAdditionally, when each work order is resolved, the technician applies one of more than 500 classification labels. When the fault couldn\u2019t be identified the technicians applied an \u201cUnknown\u201d label. Part of the end goal is to try and label these.\n\nMy first concern is the data preprocessing. I\u2019ve browsed through the literature; looked at the NLTK, spaCy, and Gensim packages; and many blog posts even tangentially related. I am probably overthinking things, but after I lemmatize each customer complaint I\u2019m confused and don\u2019t know where to go. Should component references be added to the vocabulary, or perhaps included with the stop words? I\u2019m leaning towards the former. Should I feed this into Doc2Vec or perhaps some other algorithm? How can I include the additional data fields for analysis? Would some sort of neural network be well suited to this?\n\nThe existing code, which I do not trust at all (e.g. there are function definitions within nested for loops) is like this:\n\n    import pandas as pd \n    from sklearn.feature_extraction.text import TfidfVectorizer #\n    # Models \n    from sklearn.dummy import DummyClassifier\n    from sklearn.linear_model import SGDClassifier\n    from sklearn.naive_bayes import GaussianNB\n    from sklearn.tree import DecisionTreeClassifier\n    from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.naive_bayes import MultinomialNB\n    from sklearn.svm import SVC\n    # Read in data file\n    df = pd.read_csv(\"work_orders.csv\")\n    # Create Features and Targets\n    column_names = list(df.columns)\n    # Grab all dependent variables - Targets\n    target_columns = column_names[-7:]\n    target_columns.append(\"Classification Labels\")\n    # Remove dependent variables from the column list - Features\n    feature_columns = [x for x in column_names if x not in target_columns]\n    # Perform One-Hot Encoding on the features\n    # Create Seperate dfs \n    features_df = df[feature_columns]\n    target_df = df[target_columns]\n    # Create concatenated column - Build the features\n    features = features_df[features_df.columns[-7:]].apply(lambda x: \" \".join(x.dropna().astype(str)), axis = 1)\n    # Vectorize the features\n    tfidf_vect = TfidfVectorizer()\n    x = tfidf_vect.fit_transform(features)\n    x = x.toarray()\n    # Create targets\n    target_list = list(target_df.columns)\n\nThis is then looped through the following:\n\n    model_dict = {'Dummy' : DummyClassifier(strategy = \"stratified\", random_state = 42),\n    'Stochastic Gradient Descent' : SGDClassifier(random_state = 42, loss='log'),\n    'Random Forest': RandomForestClassifier(random_state = 42),\n    'Decision Tree': DecisionTreeClassifier(random_state = 42),\n    'AdaBoost': AdaBoostClassifier(random_state = 42),\n    'Gaussian Naive Bayes': GaussianNB(),\n    'Multinomial Naive Bayes': MultinomialNB(),\n    'K Nearest Neighbor': KNeighborsClassifier(),\n    'SVM': SVC(kernel = 'rbf', random_state = 42)\n    }\n\nThis takes approximately 6 hours to execute.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hyy0nw/nlp_classificationclustering_of_work_orders_in/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "nlp classification/clustering of work orders in python /!/ i\u2019m trying to learn some nlp / ml tasks, but i\u2019m having trouble wrapping my head around some parts. any guidance is greatly appreciated.\n\nend goal: classify work orders (500+ classes)/perform clustering\n\ndataset description:\n\nwork order documents (\\~200k rows) with:\n\n* customer complaint\n* work order number\n* applicable service bulletin\n* model number\n* product family\n* manufacture date\n* manufacture plant id\n* manufacture batch id\n* assembly batch id\n* assembly part number\n* component number\n* technician(s)\n\na single work order may have multiple row entries if more than one service bulletin applies to the work order. \n\ncustomer complaint is not null, but complaint may be as simple as \u201cbees\u201d up to very technical descriptions. some are long-winded stories with examples, others are boilerplate comments added by a csr. there are often references to specific components.\n\nthe manufacture date is a timestamp in mm/dd/yyyy format. the remaining fields are all alphanumeric strings.\n\nadditionally, when each work order is resolved, the technician applies one of more than 500 classification labels. when the fault couldn\u2019t be identified the technicians applied an \u201cunknown\u201d label. part of the end goal is to try and label these.\n\nmy first concern is the data preprocessing. i\u2019ve browsed through the literature; looked at the nltk, spacy, and gensim packages; and many blog posts even tangentially related. i am probably overthinking things, but after i lemmatize each customer complaint i\u2019m confused and don\u2019t know where to go. should component references be added to the vocabulary, or perhaps included with the stop words? i\u2019m leaning towards the former. should i feed this into doc2vec or perhaps some other algorithm? how can i include the additional data fields for analysis? would some sort of neural network be well suited to this?\n\nthe existing code, which i do not trust at all (e.g. there are function definitions within nested for loops) is like this:\n\n    import pandas as pd \n    from sklearn.feature_extraction.text import tfidfvectorizer #\n    # models \n    from sklearn.dummy import dummyclassifier\n    from sklearn.linear_model import sgdclassifier\n    from sklearn.naive_bayes import gaussiannb\n    from sklearn.tree import decisiontreeclassifier\n    from sklearn.ensemble import randomforestclassifier, adaboostclassifier\n    from sklearn.neighbors import kneighborsclassifier\n    from sklearn.naive_bayes import multinomialnb\n    from sklearn.svm import svc\n    # read in data file\n    df = pd.read_csv(\"work_orders.csv\")\n    # create features and targets\n    column_names = list(df.columns)\n    # grab all dependent variables - targets\n    target_columns = column_names[-7:]\n    target_columns.append(\"classification labels\")\n    # remove dependent variables from the column list - features\n    feature_columns = [x for x in column_names if x not in target_columns]\n    # perform one-hot encoding on the features\n    # create seperate dfs \n    features_df = df[feature_columns]\n    target_df = df[target_columns]\n    # create concatenated column - build the features\n    features = features_df[features_df.columns[-7:]].apply(lambda x: \" \".join(x.dropna().astype(str)), axis = 1)\n    # vectorize the features\n    tfidf_vect = tfidfvectorizer()\n    x = tfidf_vect.fit_transform(features)\n    x = x.toarray()\n    # create targets\n    target_list = list(target_df.columns)\n\nthis is then looped through the following:\n\n    model_dict = {'dummy' : dummyclassifier(strategy = \"stratified\", random_state = 42),\n    'stochastic gradient descent' : sgdclassifier(random_state = 42, loss='log'),\n    'random forest': randomforestclassifier(random_state = 42),\n    'decision -----> tree !!! ': decisiontreeclassifier(random_state = 42),\n    'adaboost': adaboostclassifier(random_state = 42),\n    'gaussian naive bayes': gaussiannb(),\n    'multinomial naive bayes': multinomialnb(),\n    'k nearest neighbor': kneighborsclassifier(),\n    'svm': svc(kernel = 'rbf', random_state = 42)\n    }\n\nthis takes approximately 6 hours to execute.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/hyy0nw/nlp_classificationclustering_of_work_orders_in/',)", "identifyer": 5751014, "year": "2020"}, {"autor": "lwilson747", "date": 1600722688000, "content": "Natural Language Processing Engineer with Python Certification /!/ Software colleagues, this Natural Language Processing Engineer with Python program will take you through the essentials of text processing all the way up to classifying texts using Machine Learning algorithms. You will learn various concepts such as Tokenization, Stemming, Lemmatization, POS tagging, Named Entity Recognition, Syntax Tree Parsing and so on using Python\u2019s most famous NLTK package. Once you delve into NLP, you will learn to build your own text classifier using the Na\u00efve Bayes algorithm. This course is for anyone who works with data and text\u2013 with good analytical background and little exposure to Python Programming Language. It is designed to help you understand the important concepts and techniques used in Natural Language Processing using Python Programming Language.  You will be able to build your own machine learning model for text classification. Towards the end of the course, we will be discussing various practical use cases of NLP in python programming language to enhance your learning experience. You will gain high demand, marketable skills in: 1) Introduction to Text Mining and NLP, 2) Extracting, Cleaning and Pre-processing and Machine Learning, 3) Analyzing Sentence Structure, 3) Text Classification \u2013 I &amp; II, and 4) In Class Project on Sentiment Classification.\n\nRegister today at: https://fxo.co/8m2j \n\nCareer success awaits you, Lawrence Wilson \u2013 Artificial Intelligence Academy (AIA)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ix9h22/natural_language_processing_engineer_with_python/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "natural language processing engineer with python certification /!/ software colleagues, this natural language processing engineer with python program will take you through the essentials of text processing all the way up to classifying texts using machine learning algorithms. you will learn various concepts such as tokenization, stemming, lemmatization, pos tagging, named entity recognition, syntax -----> tree !!!  parsing and so on using python\u2019s most famous nltk package. once you delve into nlp, you will learn to build your own text classifier using the na\u00efve bayes algorithm. this course is for anyone who works with data and text\u2013 with good analytical background and little exposure to python programming language. it is designed to help you understand the important concepts and techniques used in natural language processing using python programming language.  you will be able to build your own machine learning model for text classification. towards the end of the course, we will be discussing various practical use cases of nlp in python programming language to enhance your learning experience. you will gain high demand, marketable skills in: 1) introduction to text mining and nlp, 2) extracting, cleaning and pre-processing and machine learning, 3) analyzing sentence structure, 3) text classification \u2013 i &amp; ii, and 4) in class project on sentiment classification.\n\nregister today at: https://fxo.co/8m2j \n\ncareer success awaits you, lawrence wilson \u2013 artificial intelligence academy (aia)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/ix9h22/natural_language_processing_engineer_with_python/',)", "identifyer": 5751049, "year": "2020"}, {"autor": "dhiraj8899", "date": 1600719483000, "content": "Decision Tree from Scratch in Python", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ix8eij/decision_tree_from_scratch_in_python/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "decision -----> tree !!!  from scratch in python", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('link',)", "medialink": "('https://medium.com/@dhiraj8899/decision-tree-from-scratch-in-python-629631ec3e3a',)", "identifyer": 5751052, "year": "2020"}, {"autor": "burdin271", "date": 1600596021000, "content": "Machine Learning For Beginners - Python Decision Tree", "link": "https://www.reddit.com/r/learnmachinelearning/comments/iwb6so/machine_learning_for_beginners_python_decision/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "machine learning for beginners - python decision -----> tree !!! ", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('rich:video',)", "medialink": "('https://youtu.be/24mxQzd3EsU',)", "identifyer": 5751103, "year": "2020"}, {"autor": "leockl", "date": 1598513087000, "content": "Does the issue with imbalanced datasets occur with ANNs /!/ I know the issue with imbalanced datasets can occur with more traditional ML models like logistic regression, naive bayes classifier etc.\n\nIn general, does this issue also occur with Artificial Neural Nets (ANNs)? Ie. in general, are ANNs \u201cpowerful\u201d enough to resolve this issue if the dataset is imbalanced?\n\nAlso, and extra question, in general, are decision tree based models also \u201cpowerful\u201d enough to resolve this issue if the dataset is imbalanced?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ihgn6r/does_the_issue_with_imbalanced_datasets_occur/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "does the issue with imbalanced datasets occur with anns /!/ i know the issue with imbalanced datasets can occur with more traditional ml models like logistic regression, naive bayes classifier etc.\n\nin general, does this issue also occur with artificial neural nets (anns)? ie. in general, are anns \u201cpowerful\u201d enough to resolve this issue if the dataset is imbalanced?\n\nalso, and extra question, in general, are decision -----> tree !!!  based models also \u201cpowerful\u201d enough to resolve this issue if the dataset is imbalanced?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/ihgn6r/does_the_issue_with_imbalanced_datasets_occur/',)", "identifyer": 5751144, "year": "2020"}, {"autor": "Trungks_Ousi", "date": 1598478728000, "content": "Should i complete the theory first or start directly with coding? /!/ I have started learning machine learning by watching Andrew NG Stanford videos. I already know 3 ML algorithms(Decision Tree,Regression,K-Means). But i thought of learning more about ML algorithms as i find it interesting. After watching about 3 of the videos,i am finding it boring as most of them are just sums and derivations. There is nothing about how to write those algorithms in codes. Should i keep watching those videos and complete the theoretical part of ML? Or should i just move to some other tutorials where there is morr focus on the coding part?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ih8bvq/should_i_complete_the_theory_first_or_start/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "should i complete the theory first or start directly with coding? /!/ i have started learning machine learning by watching andrew ng stanford videos. i already know 3 ml algorithms(decision -----> tree !!! ,regression,k-means). but i thought of learning more about ml algorithms as i find it interesting. after watching about 3 of the videos,i am finding it boring as most of them are just sums and derivations. there is nothing about how to write those algorithms in codes. should i keep watching those videos and complete the theoretical part of ml? or should i just move to some other tutorials where there is morr focus on the coding part?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/ih8bvq/should_i_complete_the_theory_first_or_start/',)", "identifyer": 5751157, "year": "2020"}, {"autor": "thewristenthusiast", "date": 1587382806000, "content": "Very basic bayesian machine learning, is my pseudocode correct? /!/ Hey all,\n\nI'm trying to use a bayesian approach in a phylogeny reconstruction project.\n\nBasically, there are 3 different algorithms to rearrange a tree (NNI, SPR, TBR), and they each perform better at different stages of the search (better defined as generating a shorter tree). I'm trying to implement a penalty based system that uses a probability array, such that when an algorithm is picked, and if it is successful, a number will be added to the probability array, increasing its probability to be picked next round.\n\nEssentially, a machine learning approach that has a prior/posterior based on succes/failure. What kind of approach is this? Am i missing anything? Any help will be greatly appreciated. Here's an image of my [pseudocode](https://i.imgur.com/r2jZc5s.png)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g4r8qs/very_basic_bayesian_machine_learning_is_my/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "very basic bayesian machine learning, is my pseudocode correct? /!/ hey all,\n\ni'm trying to use a bayesian approach in a phylogeny reconstruction project.\n\nbasically, there are 3 different algorithms to rearrange a -----> tree !!!  (nni, spr, tbr), and they each perform better at different stages of the search (better defined as generating a shorter -----> tree !!! ). i'm trying to implement a penalty based system that uses a probability array, such that when an algorithm is picked, and if it is successful, a number will be added to the probability array, increasing its probability to be picked next round.\n\nessentially, a machine learning approach that has a prior/posterior based on succes/failure. what kind of approach is this? am i missing anything? any help will be greatly appreciated. here's an image of my [pseudocode](https://i.imgur.com/r2jzc5s.png)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/g4r8qs/very_basic_bayesian_machine_learning_is_my/',)", "identifyer": 5751294, "year": "2020"}, {"autor": "BongSauce11", "date": 1587347344000, "content": "Multiclass Adaboost/binary decision tree depth /!/ Background: I am working on an assignment which is asking me to write my own implementation of the Adaboost algorithm in Python and test it using the MNIST handwritten digits database. Thus, there are 10 output classes (labels). My algorithm is working, and I am using sklearn's DecisionTreeClassifier as my base estimator. I am attempting to tune the parameters to improve my model, but am not entirely sure if my understanding of the problem is accurate. \n\nQ1) The default base learner for Adaboost is a decision tree of depth 1. For a binary output problem, this makes sense. Each leaf corresponds to one of the classes. But for a multiclass problem, a tree of depth 1 doesn't have enough leaves for all of the classes. This predictably results in poor accuracy, as it's only ever predicting one of 2 classes (I think?). Thus, according to my assumptions, the max depth D of the base tree should meet the requirements 2^D &gt;= (number_of_classes), so in my case D should be at least 4. Is this correct? I know the point of using shorter trees is to reduce overfitting, so I wanted to double check my logic.\n\nQ2) Although my implementation of Adaboost largely follows from the sklearn source code and seems to work well, I noticed something that seemed fishy to me: looking at the training error of the base estimators, which (from my understanding) should start relatively high (slightly better than a random guess) and improve with each iteration of boosting, the first estimator had an error of about 0.05 and by about the 15th estimator the error reaches 0. This seems too fast to me, and I wonder if this is due to overfitting from the base estimators or if somewhere in my code I somehow miscalculated the training error. With these results, I am getting a test accuracy of about 86% so maybe I am overthinking this. Am I doing something wrong or just misunderstanding the concepts?\n\nMy apologies if my questions are slightly vague, I'd just like to make sure I understand my results before turning in the assignment. Any feedback would be much appreciated! Thanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g4jzf5/multiclass_adaboostbinary_decision_tree_depth/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "multiclass adaboost/binary decision -----> tree !!!  depth /!/ background: i am working on an assignment which is asking me to write my own implementation of the adaboost algorithm in python and test it using the mnist handwritten digits database. thus, there are 10 output classes (labels). my algorithm is working, and i am using sklearn's decisiontreeclassifier as my base estimator. i am attempting to tune the parameters to improve my model, but am not entirely sure if my understanding of the problem is accurate. \n\nq1) the default base learner for adaboost is a decision tree of depth 1. for a binary output problem, this makes sense. each leaf corresponds to one of the classes. but for a multiclass problem, a tree of depth 1 doesn't have enough leaves for all of the classes. this predictably results in poor accuracy, as it's only ever predicting one of 2 classes (i think?). thus, according to my assumptions, the max depth d of the base tree should meet the requirements 2^d &gt;= (number_of_classes), so in my case d should be at least 4. is this correct? i know the point of using shorter trees is to reduce overfitting, so i wanted to double check my logic.\n\nq2) although my implementation of adaboost largely follows from the sklearn source code and seems to work well, i noticed something that seemed fishy to me: looking at the training error of the base estimators, which (from my understanding) should start relatively high (slightly better than a random guess) and improve with each iteration of boosting, the first estimator had an error of about 0.05 and by about the 15th estimator the error reaches 0. this seems too fast to me, and i wonder if this is due to overfitting from the base estimators or if somewhere in my code i somehow miscalculated the training error. with these results, i am getting a test accuracy of about 86% so maybe i am overthinking this. am i doing something wrong or just misunderstanding the concepts?\n\nmy apologies if my questions are slightly vague, i'd just like to make sure i understand my results before turning in the assignment. any feedback would be much appreciated! thanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/g4jzf5/multiclass_adaboostbinary_decision_tree_depth/',)", "identifyer": 5751310, "year": "2020"}, {"autor": "hacknomus", "date": 1605116083000, "content": "Building a classification Tree in R", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jscjwz/building_a_classification_tree_in_r/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "building a classification -----> tree !!!  in r", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 5, "media": "('rich:video',)", "medialink": "('https://youtu.be/LBBExUBVZkw',)", "identifyer": 5751536, "year": "2020"}, {"autor": "shyamcody", "date": 1605435293000, "content": "spacy learning curve shared /!/ Last 2 months, I have been learning about spaCy and have written about the learning thoroughly in my blog. I am sharing these here so that anyone interested in spaCy can go through these and try using it as a resource.   \n(1) [spacy introduction](https://shyambhu20.blogspot.com/2020/09/introduction-to-spacy-basic-nlp-usage.html)  \n(2) [dependency tree creation using spacy](https://shyambhu20.blogspot.com/2020/09/dependency-parsing-using-spacy-spacy.html)   \n(3) [word similarity using spacy](https://shyambhu20.blogspot.com/2020/10/calculate-word-similarity-spacy-nlp.html)  \n(4) [updating or creating a ](https://shyambhu20.blogspot.com/2020/10/spacy-nlp-neural-net-model-training.html)[neural network model using spacy](https://shyambhu20.blogspot.com/2020/10/spacy-nlp-neural-net-model-training.html)  \n(5) [how to download and use spacy models](https://shyambhu20.blogspot.com/2020/11/spacy-model-pipelines-english-usage-download.html)  \n(6) [Understanding of pytextrank: a spacy based 3rd party module for summarization](https://shyambhu20.blogspot.com/2020/11/pytextrank-spacy-phrase-extraction-summarization.html)\n\nI ought to mention that I show ads on the above posts and stand to get some monetary help on viewing. Also, I have not mentioned it as a tutorial as I am still an amateur in spacy and therefore will not call it a tutorial.\n\nExpectation is that people don't have to spend the 100 around hours behind spacy as I did to get a full picture of the framework. If you get helped please let me know. If you think some major concept is left/not discussed in details/ wrongly discussed; please let me know so that I can improve this list.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jujs9y/spacy_learning_curve_shared/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "spacy learning curve shared /!/ last 2 months, i have been learning about spacy and have written about the learning thoroughly in my blog. i am sharing these here so that anyone interested in spacy can go through these and try using it as a resource.   \n(1) [spacy introduction](https://shyambhu20.blogspot.com/2020/09/introduction-to-spacy-basic-nlp-usage.html)  \n(2) [dependency -----> tree !!!  creation using spacy](https://shyambhu20.blogspot.com/2020/09/dependency-parsing-using-spacy-spacy.html)   \n(3) [word similarity using spacy](https://shyambhu20.blogspot.com/2020/10/calculate-word-similarity-spacy-nlp.html)  \n(4) [updating or creating a ](https://shyambhu20.blogspot.com/2020/10/spacy-nlp-neural-net-model-training.html)[neural network model using spacy](https://shyambhu20.blogspot.com/2020/10/spacy-nlp-neural-net-model-training.html)  \n(5) [how to download and use spacy models](https://shyambhu20.blogspot.com/2020/11/spacy-model-pipelines-english-usage-download.html)  \n(6) [understanding of pytextrank: a spacy based 3rd party module for summarization](https://shyambhu20.blogspot.com/2020/11/pytextrank-spacy-phrase-extraction-summarization.html)\n\ni ought to mention that i show ads on the above posts and stand to get some monetary help on viewing. also, i have not mentioned it as a tutorial as i am still an amateur in spacy and therefore will not call it a tutorial.\n\nexpectation is that people don't have to spend the 100 around hours behind spacy as i did to get a full picture of the framework. if you get helped please let me know. if you think some major concept is left/not discussed in details/ wrongly discussed; please let me know so that i can improve this list.", "sortedWord": "None", "removed": "('nan',)", "score": 14, "comments": 14, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/jujs9y/spacy_learning_curve_shared/',)", "identifyer": 5751572, "year": "2020"}, {"autor": "dhiraj8899", "date": 1605425924000, "content": "Decision Tree Classification Machine Learning in Python and Sklearn", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jui8sq/decision_tree_classification_machine_learning_in/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "decision -----> tree !!!  classification machine learning in python and sklearn", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('rich:video',)", "medialink": "('https://youtu.be/XWloBKAHP_Q',)", "identifyer": 5751576, "year": "2020"}, {"autor": "dhiraj8899", "date": 1605339651000, "content": "Decision Tree Regression Machine Learning in Python and Sklearn", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jty9pi/decision_tree_regression_machine_learning_in/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "decision -----> tree !!!  regression machine learning in python and sklearn", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('rich:video',)", "medialink": "('https://youtu.be/3THvMLacVb0',)", "identifyer": 5751607, "year": "2020"}, {"autor": "thebigpleb952", "date": 1595395440000, "content": "Any course on artificial intelligence? /!/ I am enrolled in a subject called \"Introduction to Artificial Intelligence\". As per the subject outline, they teach concepts like AI techniques for searching, acquring knowledge, learning, reasoning and more. \n\nAlgorithms include hill climbing, breadth first tree search, depth first tree search, etc.\n\nAre there any online courses that I can do so that I can get a headstart with this university subject?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hvnvn5/any_course_on_artificial_intelligence/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "any course on artificial intelligence? /!/ i am enrolled in a subject called \"introduction to artificial intelligence\". as per the subject outline, they teach concepts like ai techniques for searching, acquring knowledge, learning, reasoning and more. \n\nalgorithms include hill climbing, breadth first -----> tree !!!  search, depth first -----> tree !!!  search, etc.\n\nare there any online courses that i can do so that i can get a headstart with this university subject?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/hvnvn5/any_course_on_artificial_intelligence/',)", "identifyer": 5751732, "year": "2020"}, {"autor": "CareforData", "date": 1595345601000, "content": "Decision tree algorithm in machine learning python | machine learning Le...", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hv9fqw/decision_tree_algorithm_in_machine_learning/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "decision -----> tree !!!  algorithm in machine learning python | machine learning le...", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('rich:video',)", "medialink": "('https://www.youtube.com/watch?v=_Ls3lxd87p4&amp;feature=share',)", "identifyer": 5751757, "year": "2020"}, {"autor": "Laurence-Lin", "date": 1581580475000, "content": "How to select models for ensemble learning? /!/ As title, I've used GBDT, lightgbm, Xgboost, RandomForest on problems, but these ensembles simply applies decision tree as base models. \n\n&amp;#x200B;\n\nNow I'm trying to implement stacking ensemble model, which I would like to introduce diverse models together(ANN, Decision Tree, GBDT, linear model...), but I'm stucked at which model to select for my ensemble model.\n\n&amp;#x200B;\n\nI'd looked up to \"Ensemble: Select From Library of Models\" that introduced forward selection method-- start with empty ensemble, then add model one by one to improve ensemble performance. But it seems time consuming, and I wander how does others select model for stacking.   \n\n\nAny advice is appreciated, thanks a lot!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f36n39/how_to_select_models_for_ensemble_learning/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "how to select models for ensemble learning? /!/ as title, i've used gbdt, lightgbm, xgboost, randomforest on problems, but these ensembles simply applies decision -----> tree !!!  as base models. \n\n&amp;#x200b;\n\nnow i'm trying to implement stacking ensemble model, which i would like to introduce diverse models together(ann, decision tree, gbdt, linear model...), but i'm stucked at which model to select for my ensemble model.\n\n&amp;#x200b;\n\ni'd looked up to \"ensemble: select from library of models\" that introduced forward selection method-- start with empty ensemble, then add model one by one to improve ensemble performance. but it seems time consuming, and i wander how does others select model for stacking.   \n\n\nany advice is appreciated, thanks a lot!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/f36n39/how_to_select_models_for_ensemble_learning/',)", "identifyer": 5751854, "year": "2020"}, {"autor": "throwawayAcc16777216", "date": 1603055582000, "content": "What neural network would be best for classifying tree nodes based on their children? /!/ I have a project where I need to train a neural network based on tree-data. In this tree, each node has a value and a class. The class of each node is dependent upon its value or the values of its children (it has either children or a value; not both). I would like to provide a series of labeled trees, with values and classes, as training data. I would then like the neural network to classify a tree without any classes available, classifying each node based on either its value or its children.\n\nWhat would be the best neural network to accomplish this?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jdog47/what_neural_network_would_be_best_for_classifying/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "what neural network would be best for classifying -----> tree !!!  nodes based on their children? /!/ i have a project where i need to train a neural network based on tree-data. in this tree, each node has a value and a class. the class of each node is dependent upon its value or the values of its children (it has either children or a value; not both). i would like to provide a series of labeled trees, with values and classes, as training data. i would then like the neural network to classify a tree without any classes available, classifying each node based on either its value or its children.\n\nwhat would be the best neural network to accomplish this?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/jdog47/what_neural_network_would_be_best_for_classifying/',)", "identifyer": 5751913, "year": "2020"}, {"autor": "freegems1", "date": 1606487034000, "content": "Begginer at ML and Decision trees /!/ Hello everyone,\n\nI have a little problem understanding decision trees and im hoping you can help me out.\n\nFor my project, im creating decision tree for backjack to predict if player count cards or no. I have a large dataset of 3.2mil hands, half card counter, half normal player.\n\nSo the play is 16 v 10:\n\nCard counter Stand at True count 0 or higher and Hit at -1 or lower\nNormal player Hit at every True count\n\nThe problem im facing is, at True count -1, both players Hit, so Decision tree puts both Card counter and Normal player samples in same box(for example [5000, 4950]. And if this event occurs im not sure what Decision Tree return. My guess is it returns the value with higher sample, in this case DT will say player count cards, even tho the play is same for both.\n\nwhen runing test results are pretty close even tho card counter plays 100% perfectly(decision tree gives him no counter mark for Hit at True count -1 and lower(where both players play same).\n\nHow can i solve this? should i change dataset to hands with only True count 0 or higher and ignore the others(which then creates another problem, if normal player decides to stand on every hand 16v10, DT will mark him as 100% card counter)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k21z4u/begginer_at_ml_and_decision_trees/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "begginer at ml and decision trees /!/ hello everyone,\n\ni have a little problem understanding decision trees and im hoping you can help me out.\n\nfor my project, im creating decision -----> tree !!!  for backjack to predict if player count cards or no. i have a large dataset of 3.2mil hands, half card counter, half normal player.\n\nso the play is 16 v 10:\n\ncard counter stand at true count 0 or higher and hit at -1 or lower\nnormal player hit at every true count\n\nthe problem im facing is, at true count -1, both players hit, so decision tree puts both card counter and normal player samples in same box(for example [5000, 4950]. and if this event occurs im not sure what decision tree return. my guess is it returns the value with higher sample, in this case dt will say player count cards, even tho the play is same for both.\n\nwhen runing test results are pretty close even tho card counter plays 100% perfectly(decision tree gives him no counter mark for hit at true count -1 and lower(where both players play same).\n\nhow can i solve this? should i change dataset to hands with only true count 0 or higher and ignore the others(which then creates another problem, if normal player decides to stand on every hand 16v10, dt will mark him as 100% card counter)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/k21z4u/begginer_at_ml_and_decision_trees/',)", "identifyer": 5752061, "year": "2020"}, {"autor": "khannom", "date": 1606453413000, "content": "Text vectorizing technique with low number of dimensions /!/ Hello everyone, I'm working in a text classifier using k-nearest neighbors(KNN) technique for a homework, and it's required to use kd-tree to query the knn. Since it's a kd-tree, the number of dimensions can't be very high(I have a database of 45k rows), so I need a text vectorization technique that doesn't have many dimensions.\n\nI'm looking for suggestions, any help will be really appreciated.\n\nMy database is the one from this link: [https://www.kaggle.com/imoore/60k-stack-overflow-questions-with-quality-rate](https://www.kaggle.com/imoore/60k-stack-overflow-questions-with-quality-rate)\n\nGoal: Predict tags for a stack-overflow-question", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k1ushe/text_vectorizing_technique_with_low_number_of/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "text vectorizing technique with low number of dimensions /!/ hello everyone, i'm working in a text classifier using k-nearest neighbors(knn) technique for a homework, and it's required to use kd------> tree !!!  to query the knn. since it's a kd-tree, the number of dimensions can't be very high(i have a database of 45k rows), so i need a text vectorization technique that doesn't have many dimensions.\n\ni'm looking for suggestions, any help will be really appreciated.\n\nmy database is the one from this link: [https://www.kaggle.com/imoore/60k-stack-overflow-questions-with-quality-rate](https://www.kaggle.com/imoore/60k-stack-overflow-questions-with-quality-rate)\n\ngoal: predict tags for a stack-overflow-question", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/k1ushe/text_vectorizing_technique_with_low_number_of/',)", "identifyer": 5752075, "year": "2020"}, {"autor": "blueest", "date": 1606450455000, "content": "Really Stumped With This Question - Feeling Helpless, Could Someone Please Try to Help? (about network graphs) /!/  \n\nI posted a question on stackoverflow a few weeks and still have not gotten an answer:\n\n[https://stackoverflow.com/questions/64849921/r-k-means-clustering-vs-community-detection-algorithms-weighted-correlation-ne](https://stackoverflow.com/questions/64849921/r-k-means-clustering-vs-community-detection-algorithms-weighted-correlation-ne)\n\nI am confused with this basic concept relating to network graphs and clustering (also called \"community detection\") : if you have nodes and edges, how do you incorporate node attributes into the clustering? I have often seen \"edge weights\" being used in network graph clustering (e.g. see here: [igraph.org/r/doc/cluster\\_louvain.html](https://igraph.org/r/doc/cluster_louvain.html)) ... but I have never seen anyone being able to make use of \"node attributes\" (covariates associated with the nodes) within the clustering.\n\nIt seems to me, if you really want to make a network graph in which \"node attributes\" are used for the clustering - you can either make something called a \"weighted correlation network\" ([cran.r-project.org/web/packages/WGCNA/index.html](https://cran.r-project.org/web/packages/WGCNA/index.html)) and perform clustering algorithms on this weighted correlation network (made using pairwise correlations between points) ..... or you can make a KNN (K Nearest Neighbor) graph (using Euclidean Distances between points) and run MST (Minimum Spanning Tree) clustering algorithms on the graph.\n\nBUT, when you use the Weighted Correlation Network or the KNN graph - you will inevitably end up losing (valuable) information about the natural relationships between nodes within the network. Example : suppose you have relationships between a group of friends (edges: John knows Jack, Jack knows Jim, Jim knows John) and you have Node Attributes for each member in the group (Age, Salary, Debt, etc.). A standard binary graph would just be made using the relationships between these friends ...but would not be able to take advantage of the age, salary and debt of these friends for clustering purposes. On the other hand: making a KNN graph or Weighted Correlation Network would be able to make use of the age, salary and debt of these friends .... but would not be able to make use of the natural relationships between these friends. I guess you could infer (via the principle of \"homophily\") that people are more likely to make friends with people who are more similar (in terms of node attributes) to them ... but it still seems that \"natural edges\" (friendships) will be lost.\n\nMy question: is there a way to do graph network clustering using node attributes AND the edges?\n\nCould someone please try and help me out?\n\nThanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k1u45e/really_stumped_with_this_question_feeling/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "really stumped with this question - feeling helpless, could someone please try to help? (about network graphs) /!/  \n\ni posted a question on stackoverflow a few weeks and still have not gotten an answer:\n\n[https://stackoverflow.com/questions/64849921/r-k-means-clustering-vs-community-detection-algorithms-weighted-correlation-ne](https://stackoverflow.com/questions/64849921/r-k-means-clustering-vs-community-detection-algorithms-weighted-correlation-ne)\n\ni am confused with this basic concept relating to network graphs and clustering (also called \"community detection\") : if you have nodes and edges, how do you incorporate node attributes into the clustering? i have often seen \"edge weights\" being used in network graph clustering (e.g. see here: [igraph.org/r/doc/cluster\\_louvain.html](https://igraph.org/r/doc/cluster_louvain.html)) ... but i have never seen anyone being able to make use of \"node attributes\" (covariates associated with the nodes) within the clustering.\n\nit seems to me, if you really want to make a network graph in which \"node attributes\" are used for the clustering - you can either make something called a \"weighted correlation network\" ([cran.r-project.org/web/packages/wgcna/index.html](https://cran.r-project.org/web/packages/wgcna/index.html)) and perform clustering algorithms on this weighted correlation network (made using pairwise correlations between points) ..... or you can make a knn (k nearest neighbor) graph (using euclidean distances between points) and run mst (minimum spanning -----> tree !!! ) clustering algorithms on the graph.\n\nbut, when you use the weighted correlation network or the knn graph - you will inevitably end up losing (valuable) information about the natural relationships between nodes within the network. example : suppose you have relationships between a group of friends (edges: john knows jack, jack knows jim, jim knows john) and you have node attributes for each member in the group (age, salary, debt, etc.). a standard binary graph would just be made using the relationships between these friends ...but would not be able to take advantage of the age, salary and debt of these friends for clustering purposes. on the other hand: making a knn graph or weighted correlation network would be able to make use of the age, salary and debt of these friends .... but would not be able to make use of the natural relationships between these friends. i guess you could infer (via the principle of \"homophily\") that people are more likely to make friends with people who are more similar (in terms of node attributes) to them ... but it still seems that \"natural edges\" (friendships) will be lost.\n\nmy question: is there a way to do graph network clustering using node attributes and the edges?\n\ncould someone please try and help me out?\n\nthanks", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/k1u45e/really_stumped_with_this_question_feeling/',)", "identifyer": 5752076, "year": "2020"}, {"autor": "jadegerlitz", "date": 1607012840000, "content": "Random forest - Python vs R final classification issues /!/ Hi, I'm a first year Master's student currently working on wetland mapping using random forest. Short overview of work: I use lidar and spatial data from ArcGIS to create different parameters, which is exported to a .csv. The .csv is read into Python using sklearn randomforestclassifier. I export 30 different trees to .txt files and use those to create trees in another Python file to classify the remaining/new data in 8 different classes (0-7). \n\nWorks great in Python, but my major professor wants me to use R since it's more statistically robust (her words not mine, I don't know as much about R). I've tried running the data through R without pruning or truncating the tree in anyway. I do prune in Python, but I didn't want to right off the bat in R. After creating my tree I read it as a .txt file since the visualization doesn't work for how large the tree is, and I noticed that it wasn't using whole numbers for it's final classification. Rather than 5 it would sometimes say 5.23938 or something of the sorts. The problem is, it has to be on of the whole numbers since each number corresponds with a different land type. \n\nWhy would it do this in R and not Python? Is there a way to fix it? Please let me know if this isn't the right place to post this, and thank you in advanced.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k60fhy/random_forest_python_vs_r_final_classification/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "random forest - python vs r final classification issues /!/ hi, i'm a first year master's student currently working on wetland mapping using random forest. short overview of work: i use lidar and spatial data from arcgis to create different parameters, which is exported to a .csv. the .csv is read into python using sklearn randomforestclassifier. i export 30 different trees to .txt files and use those to create trees in another python file to classify the remaining/new data in 8 different classes (0-7). \n\nworks great in python, but my major professor wants me to use r since it's more statistically robust (her words not mine, i don't know as much about r). i've tried running the data through r without pruning or truncating the -----> tree !!!  in anyway. i do prune in python, but i didn't want to right off the bat in r. after creating my tree i read it as a .txt file since the visualization doesn't work for how large the tree is, and i noticed that it wasn't using whole numbers for it's final classification. rather than 5 it would sometimes say 5.23938 or something of the sorts. the problem is, it has to be on of the whole numbers since each number corresponds with a different land type. \n\nwhy would it do this in r and not python? is there a way to fix it? please let me know if this isn't the right place to post this, and thank you in advanced.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/k60fhy/random_forest_python_vs_r_final_classification/',)", "identifyer": 5752198, "year": "2020"}, {"autor": "seraschka", "date": 1607906728000, "content": "All video recordings for STAT 451: Intro to Machine Learning /!/ Hi all,\n\nI just made a page collecting all (78!) video recordings of the STAT 451: Introduction to Machine Learning class that I was teaching at UW-Madison this Fall. Thought making those available might be useful to someone.\n\n&amp;#x200B;\n\nLink: [https://sebastianraschka.com/resources/ml-lectures-1.html](https://sebastianraschka.com/resources/ml-lectures-1.html)\n\n&amp;#x200B;\n\nThe topics are as follows:\n\n* [Part I: Introduction](#part-i-introduction)\n   * [L01: What is Machine Learning](#l01-what-is-machine-learning)\n   * [L02: Nearest Neighbor Methods](#l02-nearest-neighbor-methods)\n* [Part II: Computational Foundations](#part-ii-computational-foundations)\n   * [L03: (Optional) Python Programming](#l03-optional-python-programming)\n   * [L04: Scientific Computing in Python](#l04-scientific-computing-in-python)\n   * [L05: Machine Learning with Scikit-Learn](#l05-machine-learning-with-scikit-learn)\n* [Part III: Tree-Based Methods](#part-iii-tree-based-methods)\n   * [L06: Decision Trees](#l06-decision-trees)\n   * [L07: Ensemble Methods](#l07-ensemble-methods)\n* [Part IV: Evaluation](#part-iv-evaluation)\n   * [L08: Model Evaluation Part 1 -- Basics: Underfitting &amp; Overfitting](#l08-model-evaluation-part-1----basics-underfitting--overfitting)\n   * [L09: Model Evaluation Part 2 -- Resampling Methods](#l09-model-evaluation-part-2----resampling-methods)\n   * [L10: Model Evaluation Part 3 -- Cross Valdiation](#l10-model-evaluation-part-3----cross-valdiation)\n   * [L11: Model Evaluation Part 4 -- Statistical Tests and Algorithm Selection](#l11-model-evaluation-part-4----statistical-tests-and-algorithm-selection)\n   * [L12: Model Evaluation Part 5 -- Evaluation Metrics](#l12-model-evaluation-part-5----evaluation-metrics)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kcmu3v/all_video_recordings_for_stat_451_intro_to/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "all video recordings for stat 451: intro to machine learning /!/ hi all,\n\ni just made a page collecting all (78!) video recordings of the stat 451: introduction to machine learning class that i was teaching at uw-madison this fall. thought making those available might be useful to someone.\n\n&amp;#x200b;\n\nlink: [https://sebastianraschka.com/resources/ml-lectures-1.html](https://sebastianraschka.com/resources/ml-lectures-1.html)\n\n&amp;#x200b;\n\nthe topics are as follows:\n\n* [part i: introduction](#part-i-introduction)\n   * [l01: what is machine learning](#l01-what-is-machine-learning)\n   * [l02: nearest neighbor methods](#l02-nearest-neighbor-methods)\n* [part ii: computational foundations](#part-ii-computational-foundations)\n   * [l03: (optional) python programming](#l03-optional-python-programming)\n   * [l04: scientific computing in python](#l04-scientific-computing-in-python)\n   * [l05: machine learning with scikit-learn](#l05-machine-learning-with-scikit-learn)\n* [part iii: -----> tree !!! -based methods](#part-iii------> tree !!! -based-methods)\n   * [l06: decision trees](#l06-decision-trees)\n   * [l07: ensemble methods](#l07-ensemble-methods)\n* [part iv: evaluation](#part-iv-evaluation)\n   * [l08: model evaluation part 1 -- basics: underfitting &amp; overfitting](#l08-model-evaluation-part-1----basics-underfitting--overfitting)\n   * [l09: model evaluation part 2 -- resampling methods](#l09-model-evaluation-part-2----resampling-methods)\n   * [l10: model evaluation part 3 -- cross valdiation](#l10-model-evaluation-part-3----cross-valdiation)\n   * [l11: model evaluation part 4 -- statistical tests and algorithm selection](#l11-model-evaluation-part-4----statistical-tests-and-algorithm-selection)\n   * [l12: model evaluation part 5 -- evaluation metrics](#l12-model-evaluation-part-5----evaluation-metrics)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 29, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/kcmu3v/all_video_recordings_for_stat_451_intro_to/',)", "identifyer": 5752306, "year": "2020"}, {"autor": "tomk23_reddit", "date": 1604834127000, "content": "Any other regression models? /!/ Hi guys, just looking for other regression models available in machine learning other than linear regression, random forest, decision tree, SVM, or LSTM?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jqa3zy/any_other_regression_models/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "any other regression models? /!/ hi guys, just looking for other regression models available in machine learning other than linear regression, random forest, decision -----> tree !!! , svm, or lstm?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 5, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/jqa3zy/any_other_regression_models/',)", "identifyer": 5752363, "year": "2020"}, {"autor": "dhiraj8899", "date": 1604819237000, "content": "Top 5 Decision Tree Algorithm Advantages and Disadvantages", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jq7hfm/top_5_decision_tree_algorithm_advantages_and/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "top 5 decision -----> tree !!!  algorithm advantages and disadvantages", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('rich:video',)", "medialink": "('https://youtu.be/kV__PHlVLxI',)", "identifyer": 5752370, "year": "2020"}, {"autor": "Ok_Slice4231", "date": 1604519473000, "content": "Please help! Really bad results with sklearn decision tree ensemble and bayesian optimization! /!/ Without bayesian optimization:\n\n    model = BaggingClassifier(base_estimator=DecisionTreeClassifier(min_samples_split=15), n_estimators=100, random_state=7)\n\nResults:\n\n    Training set - Matthews correlation coefficient:0.93\n    Test set - Matthews correlation coefficient: 0.45584530253849204\n\nModel Params:\n\n    model.get_params()\n    \n    &gt;&gt; {\n        'base_estimator__ccp_alpha': 0.0,\n        'base_estimator__class_weight': None,\n        'base_estimator__criterion': 'gini',\n        'base_estimator__max_depth': None,\n        'base_estimator__max_features': None,\n        'base_estimator__max_leaf_nodes': None,\n        'base_estimator__min_impurity_decrease': 0.0,\n        'base_estimator__min_impurity_split': None,\n        'base_estimator__min_samples_leaf': 1,\n        'base_estimator__min_samples_split': 15,\n        'base_estimator__min_weight_fraction_leaf': 0.0,\n        'base_estimator__presort': 'deprecated',\n        'base_estimator__random_state': None,\n        'base_estimator__splitter': 'best',\n        'base_estimator': DecisionTreeClassifier(min_samples_split=15),\n        'bootstrap': True,\n        'bootstrap_features': False,\n        'max_features': 1.0,\n        'max_samples': 1.0,\n        'n_estimators': 100,\n        'n_jobs': None,\n        'oob_score': False,\n        'random_state': 7,\n        'verbose': 0,\n        'warm_start': False\n    }\n\nI decided to do bayesian optimization to reduce the overfitting:\n\n    param_hyperopt = {\n        'ccp_alpha': hp.uniform('ccp_alpha', 0, 1),\n        'max_depth': scope.int(hp.quniform('max_depth', 5, 20, 1)),\n        'n_estimators': scope.int(hp.quniform('n_estimators', 20, 200, 1)),\n        'max_features': scope.int(hp.quniform('max_features', 2, 10, 1)),\n        'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 40, 1)),\n        'splitter': hp.choice('splitter', ['best', 'random']),\n        'criterion': hp.choice('criterion', ['gini', 'entropy']),\n        'max_leaf_nodes': scope.int(hp.quniform('max_leaf_nodes', 2, 20, 1)),\n        'min_impurity_decrease': hp.uniform('min_impurity_decrease', 0, 1),\n        'min_samples_split': scope.int(hp.quniform('min_samples_split', 3, 40, 1)),\n        'min_weight_fraction_leaf': hp.uniform('min_weight_fraction_leaf', 0, 0.5),\n        \"max_samples\" : scope.int(hp.quniform('max_samples', 1, 10, 1)),\n    }\n    \n    def objective_function(params):\n        n_estimators = params[\"n_estimators\"]\n        max_samples = params[\"max_samples\"]\n        del params[\"n_estimators\"]\n        del params[\"max_samples\"]\n        clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(**params), n_estimators=n_estimators, max_samples=max_samples, random_state=7)\n        score = cross_val_score(clf, X_train, np.ravel(y_train), cv=5).mean()\n        return {'loss': -score, 'status': STATUS_OK}\n    \n    trials = Trials()\n    best_param = fmin(objective_function, \n                      param_hyperopt, \n                      algo=tpe.suggest, \n                      max_evals=200, \n                      trials=trials,\n                      rstate= np.random.RandomState(1))\n    loss = [x['result']['loss'] for x in trials.trials]\n    \n    best_param_values = [x for x in best_param.values()]\n\nI got these results:\n\n    {'ccp_alpha': 0.5554600863908586,\n     'criterion': 1,\n     'max_depth': 15.0,\n     'max_features': 9,\n     'max_leaf_nodes': 3,\n     'min_impurity_decrease': 0.6896630931867213,\n     'min_samples_leaf': 38,\n     'min_samples_split': 4,\n     'min_weight_fraction_leaf': 0.48094992349222787,\n     'splitter': 1}\n\nModel with tuned parameters:\n\n    clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(\n        ccp_alpha =best_param[\"ccp_alpha\"],\n        criterion=\"entropy\",\n        max_depth=best_param[\"max_depth\"],\n        max_features=best_param[\"max_features\"],\n        max_leaf_nodes=best_param[\"max_leaf_nodes\"],\n        min_impurity_decrease=best_param[\"min_impurity_decrease\"],\n        min_samples_leaf=best_param[\"min_samples_leaf\"],\n        min_samples_split=best_param[\"min_samples_split\"],\n        min_weight_fraction_leaf=best_param[\"min_weight_fraction_leaf\"],\n        splitter=\"random\"\n        \n    ), n_estimators=int(n_estimators), max_samples=int(max_samples), random_state=702120)\n    \n    clf.fit(X_train, np.ravel(y_train))\n\nThis is the result I got! - Confusion Matrix:\n\n    array([[   0, 5897],\n           [   0, 5974]])\n\nIt's putting everything in the same class! Why is this happening?\n\nPlease help! Thank you!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jo3l9h/please_help_really_bad_results_with_sklearn/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "please help! really bad results with sklearn decision -----> tree !!!  ensemble and bayesian optimization! /!/ without bayesian optimization:\n\n    model = baggingclassifier(base_estimator=decisiontreeclassifier(min_samples_split=15), n_estimators=100, random_state=7)\n\nresults:\n\n    training set - matthews correlation coefficient:0.93\n    test set - matthews correlation coefficient: 0.45584530253849204\n\nmodel params:\n\n    model.get_params()\n    \n    &gt;&gt; {\n        'base_estimator__ccp_alpha': 0.0,\n        'base_estimator__class_weight': none,\n        'base_estimator__criterion': 'gini',\n        'base_estimator__max_depth': none,\n        'base_estimator__max_features': none,\n        'base_estimator__max_leaf_nodes': none,\n        'base_estimator__min_impurity_decrease': 0.0,\n        'base_estimator__min_impurity_split': none,\n        'base_estimator__min_samples_leaf': 1,\n        'base_estimator__min_samples_split': 15,\n        'base_estimator__min_weight_fraction_leaf': 0.0,\n        'base_estimator__presort': 'deprecated',\n        'base_estimator__random_state': none,\n        'base_estimator__splitter': 'best',\n        'base_estimator': decisiontreeclassifier(min_samples_split=15),\n        'bootstrap': true,\n        'bootstrap_features': false,\n        'max_features': 1.0,\n        'max_samples': 1.0,\n        'n_estimators': 100,\n        'n_jobs': none,\n        'oob_score': false,\n        'random_state': 7,\n        'verbose': 0,\n        'warm_start': false\n    }\n\ni decided to do bayesian optimization to reduce the overfitting:\n\n    param_hyperopt = {\n        'ccp_alpha': hp.uniform('ccp_alpha', 0, 1),\n        'max_depth': scope.int(hp.quniform('max_depth', 5, 20, 1)),\n        'n_estimators': scope.int(hp.quniform('n_estimators', 20, 200, 1)),\n        'max_features': scope.int(hp.quniform('max_features', 2, 10, 1)),\n        'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 1, 40, 1)),\n        'splitter': hp.choice('splitter', ['best', 'random']),\n        'criterion': hp.choice('criterion', ['gini', 'entropy']),\n        'max_leaf_nodes': scope.int(hp.quniform('max_leaf_nodes', 2, 20, 1)),\n        'min_impurity_decrease': hp.uniform('min_impurity_decrease', 0, 1),\n        'min_samples_split': scope.int(hp.quniform('min_samples_split', 3, 40, 1)),\n        'min_weight_fraction_leaf': hp.uniform('min_weight_fraction_leaf', 0, 0.5),\n        \"max_samples\" : scope.int(hp.quniform('max_samples', 1, 10, 1)),\n    }\n    \n    def objective_function(params):\n        n_estimators = params[\"n_estimators\"]\n        max_samples = params[\"max_samples\"]\n        del params[\"n_estimators\"]\n        del params[\"max_samples\"]\n        clf = baggingclassifier(base_estimator=decisiontreeclassifier(**params), n_estimators=n_estimators, max_samples=max_samples, random_state=7)\n        score = cross_val_score(clf, x_train, np.ravel(y_train), cv=5).mean()\n        return {'loss': -score, 'status': status_ok}\n    \n    trials = trials()\n    best_param = fmin(objective_function, \n                      param_hyperopt, \n                      algo=tpe.suggest, \n                      max_evals=200, \n                      trials=trials,\n                      rstate= np.random.randomstate(1))\n    loss = [x['result']['loss'] for x in trials.trials]\n    \n    best_param_values = [x for x in best_param.values()]\n\ni got these results:\n\n    {'ccp_alpha': 0.5554600863908586,\n     'criterion': 1,\n     'max_depth': 15.0,\n     'max_features': 9,\n     'max_leaf_nodes': 3,\n     'min_impurity_decrease': 0.6896630931867213,\n     'min_samples_leaf': 38,\n     'min_samples_split': 4,\n     'min_weight_fraction_leaf': 0.48094992349222787,\n     'splitter': 1}\n\nmodel with tuned parameters:\n\n    clf = baggingclassifier(base_estimator=decisiontreeclassifier(\n        ccp_alpha =best_param[\"ccp_alpha\"],\n        criterion=\"entropy\",\n        max_depth=best_param[\"max_depth\"],\n        max_features=best_param[\"max_features\"],\n        max_leaf_nodes=best_param[\"max_leaf_nodes\"],\n        min_impurity_decrease=best_param[\"min_impurity_decrease\"],\n        min_samples_leaf=best_param[\"min_samples_leaf\"],\n        min_samples_split=best_param[\"min_samples_split\"],\n        min_weight_fraction_leaf=best_param[\"min_weight_fraction_leaf\"],\n        splitter=\"random\"\n        \n    ), n_estimators=int(n_estimators), max_samples=int(max_samples), random_state=702120)\n    \n    clf.fit(x_train, np.ravel(y_train))\n\nthis is the result i got! - confusion matrix:\n\n    array([[   0, 5897],\n           [   0, 5974]])\n\nit's putting everything in the same class! why is this happening?\n\nplease help! thank you!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/jo3l9h/please_help_really_bad_results_with_sklearn/',)", "identifyer": 5752409, "year": "2020"}, {"autor": "RubiksCodeNMZ", "date": 1604478683000, "content": "Back to Machine Learning Basics &amp;#x2d; Decision Tree &amp; Random Forest", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jnt64j/back_to_machine_learning_basics_x2d_decision_tree/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "back to machine learning basics &amp;#x2d; decision -----> tree !!!  &amp; random forest", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('link',)", "medialink": "('https://rubikscode.net/2020/09/28/back-to-machine-learning-basics-decision-tree-random-forest/',)", "identifyer": 5752421, "year": "2020"}, {"autor": "dhiraj8899", "date": 1607680036000, "content": "Why the Decision Tree algorithm is NOT affected by feature magnitude?\ufeff", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kazuir/why_the_decision_tree_algorithm_is_not_affected/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "why the decision -----> tree !!!  algorithm is not affected by feature magnitude?\ufeff", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('link',)", "medialink": "('https://www.youtube.com/post/UgyrQu76ISssO3-g7xx4AaABCQ',)", "identifyer": 5752603, "year": "2020"}, {"autor": "pp314159", "date": 1592830104000, "content": "The 4 Ways to Visualize Decision Tree from Sklearn in Python", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hdrqo7/the_4_ways_to_visualize_decision_tree_from/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "the 4 ways to visualize decision -----> tree !!!  from sklearn in python", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://mljar.com/blog/visualize-decision-tree/',)", "identifyer": 5752824, "year": "2020"}, {"autor": "ElectricalFilm2", "date": 1592076851000, "content": "Resources/tutorials for learning basic concepts at a high level /!/ My day job is that of a data engineer, but I want a refresher on machine learning concepts, and different techniques. I want to be able to understand (1) what a technique is and how it works and (2) what kinds of problems can be solved using a particular technique. Understanding the math in-depth would be nice, but it is not high priority; and neither is being able to know how to execute a certain technique in Python (I can learn that later if needed). I would like to cover specific topics like Naive Bayes, Decision Tree, SVMs, Neural Networks and Clustering, among others.\n\nI was looking for video based tutorials (or online classes) I could make use of, that would take 30 hours or less to run through. Any suggestions?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/h8e2vj/resourcestutorials_for_learning_basic_concepts_at/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "resources/tutorials for learning basic concepts at a high level /!/ my day job is that of a data engineer, but i want a refresher on machine learning concepts, and different techniques. i want to be able to understand (1) what a technique is and how it works and (2) what kinds of problems can be solved using a particular technique. understanding the math in-depth would be nice, but it is not high priority; and neither is being able to know how to execute a certain technique in python (i can learn that later if needed). i would like to cover specific topics like naive bayes, decision -----> tree !!! , svms, neural networks and clustering, among others.\n\ni was looking for video based tutorials (or online classes) i could make use of, that would take 30 hours or less to run through. any suggestions?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/h8e2vj/resourcestutorials_for_learning_basic_concepts_at/',)", "identifyer": 5752853, "year": "2020"}, {"autor": "19Summer", "date": 1591955572000, "content": "Do I need to standardize/normalize target data when applying Regression Tree algorithm? /!/ Greetings.  \nI have just finished watching Josh Stamer's video about regression trees. \n\nDecision trees in usual implementation do not need feature scaling, however for regression trees we use metric such as MSE, which calculates distance between target and predicted values.\n\nCould anyone please give me an advice whether I should to normalize/standardize data and can I do it only for target values and leave features  unscaled?\n\nThanks in advance.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/h7hsu0/do_i_need_to_standardizenormalize_target_data/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "do i need to standardize/normalize target data when applying regression -----> tree !!!  algorithm? /!/ greetings.  \ni have just finished watching josh stamer's video about regression trees. \n\ndecision trees in usual implementation do not need feature scaling, however for regression trees we use metric such as mse, which calculates distance between target and predicted values.\n\ncould anyone please give me an advice whether i should to normalize/standardize data and can i do it only for target values and leave features  unscaled?\n\nthanks in advance.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/h7hsu0/do_i_need_to_standardizenormalize_target_data/',)", "identifyer": 5752909, "year": "2020"}, {"autor": "mwitiderrick", "date": 1592317946000, "content": "[D] Fast Gradient Boosting with CatBoost /!/ In gradient boosting, predictions are made from an ensemble of weak learners. Unlike a random forest that creates a decision tree for each sample, in gradient boosting, trees are created one after the other. Previous trees in the model are not altered. Results from the previous tree are used to improve the next one. In this piece, we\u2019ll take a closer look at a gradient boosting library called CatBoost.\n\n[https://heartbeat.fritz.ai/fast-gradient-boosting-with-catboost-38779b0d5d9a](https://heartbeat.fritz.ai/fast-gradient-boosting-with-catboost-38779b0d5d9a)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ha54fe/d_fast_gradient_boosting_with_catboost/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "[d] fast gradient boosting with catboost /!/ in gradient boosting, predictions are made from an ensemble of weak learners. unlike a random forest that creates a decision -----> tree !!!  for each sample, in gradient boosting, trees are created one after the other. previous trees in the model are not altered. results from the previous tree are used to improve the next one. in this piece, we\u2019ll take a closer look at a gradient boosting library called catboost.\n\n[https://heartbeat.fritz.ai/fast-gradient-boosting-with-catboost-38779b0d5d9a](https://heartbeat.fritz.ai/fast-gradient-boosting-with-catboost-38779b0d5d9a)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/ha54fe/d_fast_gradient_boosting_with_catboost/',)", "identifyer": 5752954, "year": "2020"}, {"autor": "Fredbull", "date": 1592259409000, "content": "How the hell doesn't boosting lead to overfitting? /!/ Hi all,\n\nso lately I've been brushing up on my decision tree, random forests and whatnot, and now I've come to boosting. Now, I understand how boosting works and I can't deny the effectiveness of these techniques. However, I simply can't understand how boosting doesn't immediately lead to overfitting, since fitting a model to the residual error of the previous model seems to me like the exact definition of overfitting.\n\nI am certain I am missing some part of the puzzle. I've considered that a model which fits the previous model's residual to perfection would be generating huge residuals of its own on all other training samples, and would therefore be discarded... however, each model in the sequence gives more weight to the previous model's misclassifications, so this to me looks like it would only make the situation worse by incentivizing a perfect training fit.\n\nI have also considered that maybe, if only very weak learners are used at each step, then maybe the residuals aren't only noise, but actual variance of the training data that the previous model couldn't explain. This way, I can see how we can avoid overfit for a couple of models, but after N models?? How aren't we fast approaching a situation where each sequential model is just fitting noise?\n\nI would really appreciate if anyone could share their insights on this topic, or share some references that specifically address it. I've found a couple of articles online, but they all just gloss over the topic and pretty much just say that \"boosting is very robust against overfitting\".\n\n&amp;#x200B;\n\nThanks a lot!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/h9q6iv/how_the_hell_doesnt_boosting_lead_to_overfitting/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "how the hell doesn't boosting lead to overfitting? /!/ hi all,\n\nso lately i've been brushing up on my decision -----> tree !!! , random forests and whatnot, and now i've come to boosting. now, i understand how boosting works and i can't deny the effectiveness of these techniques. however, i simply can't understand how boosting doesn't immediately lead to overfitting, since fitting a model to the residual error of the previous model seems to me like the exact definition of overfitting.\n\ni am certain i am missing some part of the puzzle. i've considered that a model which fits the previous model's residual to perfection would be generating huge residuals of its own on all other training samples, and would therefore be discarded... however, each model in the sequence gives more weight to the previous model's misclassifications, so this to me looks like it would only make the situation worse by incentivizing a perfect training fit.\n\ni have also considered that maybe, if only very weak learners are used at each step, then maybe the residuals aren't only noise, but actual variance of the training data that the previous model couldn't explain. this way, i can see how we can avoid overfit for a couple of models, but after n models?? how aren't we fast approaching a situation where each sequential model is just fitting noise?\n\ni would really appreciate if anyone could share their insights on this topic, or share some references that specifically address it. i've found a couple of articles online, but they all just gloss over the topic and pretty much just say that \"boosting is very robust against overfitting\".\n\n&amp;#x200b;\n\nthanks a lot!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/h9q6iv/how_the_hell_doesnt_boosting_lead_to_overfitting/',)", "identifyer": 5752983, "year": "2020"}, {"autor": "Fredbull", "date": 1592159715000, "content": "Decision trees - Information Gain is a misleading name? /!/ Hi everyone,\n\nso I'm brushing up on tree-building algorithms and I've read that for example in the ID3 algorithm, we select the feature with the highest information gain at each split. \n\nLet's consider the problem of binary classification, for simplicity, and imagine we are still in the root node and p=0.5 (so the entropy is maximum in that noe). The goal is to find the test that generates two partitions with the lowest average entropy; ideally, the full dataset is immediately separated by a single test, and we end up with two partitions with 0 entropy.\n\nNow, I've always heard entropy and information as being interchangeable, in the sense that in a situation with low entropy, where an outcome is certain, we get no information from observing the variable.\n\nTherefore, my question is: if we are going from a state where entropy = 1, into states where entropy = 0, why the hell do we call it information gain? Shouldn't we call it information or entropy loss instead? I am pretty confused about this, can anyone shed some light on the issue?\n\n&amp;#x200B;\n\nThanks a lot!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/h8z2ft/decision_trees_information_gain_is_a_misleading/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "decision trees - information gain is a misleading name? /!/ hi everyone,\n\nso i'm brushing up on -----> tree !!! -building algorithms and i've read that for example in the id3 algorithm, we select the feature with the highest information gain at each split. \n\nlet's consider the problem of binary classification, for simplicity, and imagine we are still in the root node and p=0.5 (so the entropy is maximum in that noe). the goal is to find the test that generates two partitions with the lowest average entropy; ideally, the full dataset is immediately separated by a single test, and we end up with two partitions with 0 entropy.\n\nnow, i've always heard entropy and information as being interchangeable, in the sense that in a situation with low entropy, where an outcome is certain, we get no information from observing the variable.\n\ntherefore, my question is: if we are going from a state where entropy = 1, into states where entropy = 0, why the hell do we call it information gain? shouldn't we call it information or entropy loss instead? i am pretty confused about this, can anyone shed some light on the issue?\n\n&amp;#x200b;\n\nthanks a lot!", "sortedWord": "None", "removed": "('nan',)", "score": 2, "comments": 8, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/h8z2ft/decision_trees_information_gain_is_a_misleading/',)", "identifyer": 5753035, "year": "2020"}, {"autor": "redd-griffin-it", "date": 1593721051000, "content": "MAchine Learning and Decision Trees !! /!/ [https://www.analyticssteps.com/blogs/decision-tree-machine-learning. ](https://www.analyticssteps.com/blogs/decision-tree-machine-learning)\n\nWhat to do ? It is the most basic question but yet very difficult to answer . A whole decision tree is required to get the most optimal answers .  know How , Where and Why decision trees are used in machine learning in just one click :  \n\n&amp;#x200B;\n\n&amp;#x200B;\n\n  .", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hk4jas/machine_learning_and_decision_trees/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "machine learning and decision trees !! /!/ [https://www.analyticssteps.com/blogs/decision-tree-machine-learning. ](https://www.analyticssteps.com/blogs/decision-tree-machine-learning)\n\nwhat to do ? it is the most basic question but yet very difficult to answer . a whole decision -----> tree !!!  is required to get the most optimal answers .  know how , where and why decision trees are used in machine learning in just one click :  \n\n&amp;#x200b;\n\n&amp;#x200b;\n\n  .", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/hk4jas/machine_learning_and_decision_trees/',)", "identifyer": 5753194, "year": "2020"}, {"autor": "DEAD-HUNTER", "date": 1594294100000, "content": "ML algorithm for detecting previously unknown patterns for time series forecasting /!/ Hello,  \nI was wondering if there is a python/tensorflow supported ML library/model that allows for detecting patterns and then making time series forecasts based do it? Let me elaborate with a time series of weather data.  \n\n\nImagine a time series dataset that contains, date, month, day, humidity, air pressure, wind and temperature. The responsevariable is a simple binary classification problem, will it rain or not rain? What I am looking for is something that trains on this dataset and creates a decision tree but also incorporates an LSTM model.  \nThe end result could be a set of rules along the lines of:  \nIf humidity&lt;60 and airpressure&gt;10 and January&lt;Month of year&lt;may and wind\\*0,001278+0,000978 then its a 73,46% chance of raining the next day.  \n\n\nOn top of this above example ruleset, the algorithm would be capable of producing several different rulesets for each binary classification. That way you might end up with a very huge but diverse set of rules for predicting wether it will rain or not.  \n\n\nIm fairly new to ML so I'm certain this already exists in python, however I have yet to stumble across something similar.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ho1ge4/ml_algorithm_for_detecting_previously_unknown/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "ml algorithm for detecting previously unknown patterns for time series forecasting /!/ hello,  \ni was wondering if there is a python/tensorflow supported ml library/model that allows for detecting patterns and then making time series forecasts based do it? let me elaborate with a time series of weather data.  \n\n\nimagine a time series dataset that contains, date, month, day, humidity, air pressure, wind and temperature. the responsevariable is a simple binary classification problem, will it rain or not rain? what i am looking for is something that trains on this dataset and creates a decision -----> tree !!!  but also incorporates an lstm model.  \nthe end result could be a set of rules along the lines of:  \nif humidity&lt;60 and airpressure&gt;10 and january&lt;month of year&lt;may and wind\\*0,001278+0,000978 then its a 73,46% chance of raining the next day.  \n\n\non top of this above example ruleset, the algorithm would be capable of producing several different rulesets for each binary classification. that way you might end up with a very huge but diverse set of rules for predicting wether it will rain or not.  \n\n\nim fairly new to ml so i'm certain this already exists in python, however i have yet to stumble across something similar.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/ho1ge4/ml_algorithm_for_detecting_previously_unknown/',)", "identifyer": 5753313, "year": "2020"}, {"autor": "Terrock56", "date": 1590650450000, "content": "How to derive a parse tree from a CFG? /!/ How can I construct a parse tree from context-free grammar?\n\nFor a parse tree such as \n\n&amp;#x200B;\n\nhttps://preview.redd.it/hfpbbgr0ig151.png?width=230&amp;format=png&amp;auto=webp&amp;s=b8e38d0ad0fa494332105bd3ca8848fa62370c12\n\nThe left most derivation is explicit such as \n\n&amp;#x200B;\n\nhttps://preview.redd.it/nwvr5626ig151.png?width=199&amp;format=png&amp;auto=webp&amp;s=2e98030ff10bf5e02d00a06a0170aea736e32161\n\nThe right subtree on the other hand is not as feasible and generates a lot of ambiguity in extracting a CFG from the derivation.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/0pkzn42fig151.png?width=752&amp;format=png&amp;auto=webp&amp;s=fdc0bed7b3f3299cc82fc67e81a142b4a9dfe9bc\n\nI was wondering if anyone here can give me some tips on how I can construct a parse tree from context free grammar. The other way around is not as easy, only for a height of 1-2 the derivation is possible.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gs1joo/how_to_derive_a_parse_tree_from_a_cfg/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "how to derive a parse -----> tree !!!  from a cfg? /!/ how can i construct a parse tree from context-free grammar?\n\nfor a parse tree such as \n\n&amp;#x200b;\n\nhttps://preview.redd.it/hfpbbgr0ig151.png?width=230&amp;format=png&amp;auto=webp&amp;s=b8e38d0ad0fa494332105bd3ca8848fa62370c12\n\nthe left most derivation is explicit such as \n\n&amp;#x200b;\n\nhttps://preview.redd.it/nwvr5626ig151.png?width=199&amp;format=png&amp;auto=webp&amp;s=2e98030ff10bf5e02d00a06a0170aea736e32161\n\nthe right subtree on the other hand is not as feasible and generates a lot of ambiguity in extracting a cfg from the derivation.\n\n&amp;#x200b;\n\nhttps://preview.redd.it/0pkzn42fig151.png?width=752&amp;format=png&amp;auto=webp&amp;s=fdc0bed7b3f3299cc82fc67e81a142b4a9dfe9bc\n\ni was wondering if anyone here can give me some tips on how i can construct a parse tree from context free grammar. the other way around is not as easy, only for a height of 1-2 the derivation is possible.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/gs1joo/how_to_derive_a_parse_tree_from_a_cfg/',)", "identifyer": 5753420, "year": "2020"}, {"autor": "CULTimate", "date": 1608516173000, "content": "Interviewing for my first Data Science Internship /!/ This week I am interviewing for a Data Science internship. My data science knowledge is limited, however this is the career path I'm planning on going down. \n\nThe first half of the interview will be the standard fare, the second half will be me going through one of my jupyter notebook homework assignments or personal projects. \n\nI have spent a few hours this week creating a notebook that rips NFL season statistics from the internet and then uses multiple classifiers to evaluate data and the choose winners for games. \n\nI am currently only familiar with the decision tree and naive bayes classifiers, and I was wondering if there were many more that are a part of sk learn and that I can learn at a level to explain during my interview.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kh7mh5/interviewing_for_my_first_data_science_internship/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "interviewing for my first data science internship /!/ this week i am interviewing for a data science internship. my data science knowledge is limited, however this is the career path i'm planning on going down. \n\nthe first half of the interview will be the standard fare, the second half will be me going through one of my jupyter notebook homework assignments or personal projects. \n\ni have spent a few hours this week creating a notebook that rips nfl season statistics from the internet and then uses multiple classifiers to evaluate data and the choose winners for games. \n\ni am currently only familiar with the decision -----> tree !!!  and naive bayes classifiers, and i was wondering if there were many more that are a part of sk learn and that i can learn at a level to explain during my interview.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/kh7mh5/interviewing_for_my_first_data_science_internship/',)", "identifyer": 5753515, "year": "2020"}, {"autor": "jms4607", "date": 1597443315000, "content": "Is there any work in object detection on having class sub-types/a class type tree? /!/ Lets say you wanted to train a model to predict the type of vehicle and the exact make of the vehicle on the road. When detecting a ford f-150, I imagine it would be beneficial to penalize the model more for predicting a vw-bug compared to detecting a ram truck. Has there been any work where an ml model detects an object type(truck, car, etc.) and then goes furth to detect a sub type(f-150, avalance, ram,...)?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i9vkkr/is_there_any_work_in_object_detection_on_having/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "is there any work in object detection on having class sub-types/a class type -----> tree !!! ? /!/ lets say you wanted to train a model to predict the type of vehicle and the exact make of the vehicle on the road. when detecting a ford f-150, i imagine it would be beneficial to penalize the model more for predicting a vw-bug compared to detecting a ram truck. has there been any work where an ml model detects an object type(truck, car, etc.) and then goes furth to detect a sub type(f-150, avalance, ram,...)?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/i9vkkr/is_there_any_work_in_object_detection_on_having/',)", "identifyer": 5753573, "year": "2020"}, {"autor": "Alternative_Craft_35", "date": 1597374046000, "content": "How can I load the decision tree regressor?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i9e4v5/how_can_i_load_the_decision_tree_regressor/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "how can i load the decision -----> tree !!!  regressor?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('image',)", "medialink": "('https://i.imgur.com/ZkDpsa6.jpg',)", "identifyer": 5753607, "year": "2020"}, {"autor": "Illustrious-Command7", "date": 1591308509000, "content": "Pong Reinforcement Learning Agent isn't converging /!/ I'm trying to implement a Pong playing Deep Q Network with OpenAI gym based on the [Deep Reinforcement Learning Hands On code I've been reading](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/tree/master/Chapter06). I'm using Tensorflow/Keras instead of the book's choice of Pytorch to force myself to better understand the concepts (I can't just copy and paste.) I feel like I understand the concepts but my model won't converge. It just hovers around -20.5 forever. (I've tested up to around 800000 frames.)\n\nI'm doing my work in a Jupyter Notebook so some of the imports are in odd locations and I haven't included it but I'm also using [the wrappers.py](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/lib/wrappers.py) code for pre-processing.\n\nI would really appreciate some guidance on this. I've been trying to figure it out all week and I'm stumped.\n\n    \n    import gym\n    from gym import envs\n    from keras import Sequential, layers, models\n    from keras.optimizers import Adam\n    import wrappers\n    import collections\n    import random\n    \n    envString = \"PongNoFrameskip-v4\"\n    env = wrappers.make_env(envString)\n    \n    observationInput = env.observation_space.shape\n    actionOutput = env.action_space.n\n    \n    LEARNING_RATE=1e-4 \n    \n    SYNC_TARGET_FRAMES = 1000\n    \n    REPLAY_START_SIZE = 20000 \n    \n    GAMMA=0.99\n    \n    EPSILON_START=1.0\n    EPSILON_FINAL=0.02\n    EPSILON_DECAY=(10**5)*2\n    \n    MEAN_REWARD_BOUND = 17\n    \n    BATCH_SIZE = 32\n    \n    \n    #randomly pull past experiences from to ensure SGD works correctly\n    #it needs to store (s, a, r, s')\n    Experience = collections.namedtuple('Experience', \n                                        field_names=['state', 'action',\n                                                    'reward', 'done',\n                                                    'new_state'])\n    \n    class ExperienceBuffer:\n        def __init__(self, capacity):\n            self.buffer = collections.deque(maxlen=capacity)\n            \n        def __len__(self):\n            return len(self.buffer)\n        \n        def append(self, experience):\n            self.buffer.append(experience)\n            \n        def sample(self, batch_size):\n            return random.sample(self.buffer, batch_size)\n              \n    \n    #We need an agent to act upon the environment. It only needs to \n    #set itself up, reset itself, and play a step\n    import numpy as np\n    import tensorflow as tf\n    \n    class Agent:\n        def __init__(self, env, exp_buffer):\n            self.env = env\n            self.exp_buffer = exp_buffer\n            self._reset() \n            \n            self.model = self.buildModel()\n            self.targetModel = self.buildModel()\n            \n            self.inputBatch = np.zeros([BATCH_SIZE,] +  list(self.model.input_shape[-3:]), dtype='float')\n            self.newStateBatch = np.zeros([BATCH_SIZE,] +  list(self.model.input_shape[-3:]), dtype='float')\n            \n            \n        def _reset(self):\n            self.state = env.reset()\n            self.total_reward = 0.0\n            \n        def buildModel(self):\n            model = Sequential()\n            model.add(layers.Conv2D(32,\n                            (8,8),\n                            strides=4,\n                            activation='relu',\n                            data_format='channels_first',\n                            input_shape=observationInput))\n            model.add(layers.Conv2D(64,\n                            (4,4),\n                            strides=2,\n                            activation='relu',\n                            data_format='channels_first'))\n            model.add(layers.Conv2D(64,\n                            (3,3),\n                            strides=1,\n                            activation='relu',\n                            data_format='channels_first'))\n            model.add(layers.Flatten())\n            model.add(layers.Dense(512, activation='relu'))\n            model.add(layers.Dense(actionOutput, activation=None))\n            \n            model.compile(loss=\"mean_squared_error\",\n                              optimizer=Adam(lr=LEARNING_RATE))\n            return model\n            \n        def play_step(self, epsilon=0.0):\n            done_reward = None        \n            \n            if(np.random.random() &lt; epsilon): #pick action randomly\n                action = env.action_space.sample()        \n            else: #pick action based on current best Q val\n                state_a = np.array([self.state], copy=False)\n                qVals = self.model.predict(state_a)\n                action = np.argmax(qVals[0])\n    \n                 \n            new_state, reward, is_done, _ = self.env.step(action)           \n            self.total_reward += reward\n            \n            exp = Experience(self.state, action, reward, is_done, new_state)\n            self.exp_buffer.append(exp)\n            self.state = new_state\n            if is_done:\n                done_reward = self.total_reward\n                self._reset()\n            return done_reward\n        \n        def replay(self):\n            assert(len(self.exp_buffer) &gt; BATCH_SIZE)\n            experiences = self.exp_buffer.sample(BATCH_SIZE)         \n    \n            for i in range(len(experiences)):\n                sample = experiences[i]\n                state = sample.state\n                reward = sample.reward\n                action = sample.action\n                newState  = sample.new_state\n                done = sample.done\n                \n                self.inputBatch [i, ...] = state\n                self.newStateBatch [i, ...] = newState\n                \n            targetBatch = self.model.predict(self.inputBatch) \n            nextStateQValues = self.targetModel.predict(self.newStateBatch)      \n    \n            for i in range(len(experiences)):\n                if experiences[i].done:\n                    targetBatch[i][action] = reward\n                else:                \n                    qFuture = max(nextStateQValues[i])\n                    targetBatch[i][action] = reward + (qFuture*GAMMA)              \n    \n            self.model.train_on_batch(self.inputBatch, targetBatch)\n     \n    import sys\n    import time\n    \n    buffer = ExperienceBuffer(200000)\n    agent = Agent(env, buffer)\n    epsilon = EPSILON_START\n    \n    agent.model.summary()\n    \n    total_rewards=[]\n    frame_idx = frameTime = 0\n    \n    mean_reward = -1\n    bestReward = None\n    \n    print(\"beginning...\")\n    startTime = ts = time.time()\n    while True:    \n        frame_idx += 1\n        \n        epsilon = max(EPSILON_FINAL, EPSILON_START-frame_idx/EPSILON_DECAY)\n        reward = agent.play_step(epsilon)\n        \n        minutes = (time.time() - startTime)  / 60.0  \n    \n        if(reward is not None):        \n            total_rewards.append(reward)\n            mean_reward = np.mean(total_rewards[-100:])\n            \n            string = \"Frame {0}, best reward {1}, reward avg {2}, total games {3}, epsilon {4}, minutes {5}\"\n            \n            if(bestReward is None or bestReward &lt; reward):\n                bestReward = reward\n                print(\"new best reward!\")\n                print(string.format(frame_idx, bestReward, mean_reward, len(total_rewards), epsilon, minutes))\n                \n            \n            if(mean_reward &gt; MEAN_REWARD_BOUND):\n                print(\"Solved in %d frames!\" % frame_idx)\n                break\n                \n                \n        if(len(buffer) &lt; REPLAY_START_SIZE):\n            continue\n            \n        \n        if(frame_idx % 5000 == 0):\n            print(\"\\n\")\n            print(string.format(frame_idx, bestReward, mean_reward, len(total_rewards), epsilon, minutes))\n       \n        sys.stdout.flush()\n            \n        if(frame_idx % SYNC_TARGET_FRAMES == 0):\n            agent.targetModel.set_weights(agent.model.get_weights())\n       \n        agent.replay()", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gwrfvh/pong_reinforcement_learning_agent_isnt_converging/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "pong reinforcement learning agent isn't converging /!/ i'm trying to implement a pong playing deep q network with openai gym based on the [deep reinforcement learning hands on code i've been reading](https://github.com/packtpublishing/deep-reinforcement-learning-hands-on/-----> tree !!! /master/chapter06). i'm using tensorflow/keras instead of the book's choice of pytorch to force myself to better understand the concepts (i can't just copy and paste.) i feel like i understand the concepts but my model won't converge. it just hovers around -20.5 forever. (i've tested up to around 800000 frames.)\n\ni'm doing my work in a jupyter notebook so some of the imports are in odd locations and i haven't included it but i'm also using [the wrappers.py](https://github.com/packtpublishing/deep-reinforcement-learning-hands-on/blob/master/chapter06/lib/wrappers.py) code for pre-processing.\n\ni would really appreciate some guidance on this. i've been trying to figure it out all week and i'm stumped.\n\n    \n    import gym\n    from gym import envs\n    from keras import sequential, layers, models\n    from keras.optimizers import adam\n    import wrappers\n    import collections\n    import random\n    \n    envstring = \"pongnoframeskip-v4\"\n    env = wrappers.make_env(envstring)\n    \n    observationinput = env.observation_space.shape\n    actionoutput = env.action_space.n\n    \n    learning_rate=1e-4 \n    \n    sync_target_frames = 1000\n    \n    replay_start_size = 20000 \n    \n    gamma=0.99\n    \n    epsilon_start=1.0\n    epsilon_final=0.02\n    epsilon_decay=(10**5)*2\n    \n    mean_reward_bound = 17\n    \n    batch_size = 32\n    \n    \n    #randomly pull past experiences from to ensure sgd works correctly\n    #it needs to store (s, a, r, s')\n    experience = collections.namedtuple('experience', \n                                        field_names=['state', 'action',\n                                                    'reward', 'done',\n                                                    'new_state'])\n    \n    class experiencebuffer:\n        def __init__(self, capacity):\n            self.buffer = collections.deque(maxlen=capacity)\n            \n        def __len__(self):\n            return len(self.buffer)\n        \n        def append(self, experience):\n            self.buffer.append(experience)\n            \n        def sample(self, batch_size):\n            return random.sample(self.buffer, batch_size)\n              \n    \n    #we need an agent to act upon the environment. it only needs to \n    #set itself up, reset itself, and play a step\n    import numpy as np\n    import tensorflow as tf\n    \n    class agent:\n        def __init__(self, env, exp_buffer):\n            self.env = env\n            self.exp_buffer = exp_buffer\n            self._reset() \n            \n            self.model = self.buildmodel()\n            self.targetmodel = self.buildmodel()\n            \n            self.inputbatch = np.zeros([batch_size,] +  list(self.model.input_shape[-3:]), dtype='float')\n            self.newstatebatch = np.zeros([batch_size,] +  list(self.model.input_shape[-3:]), dtype='float')\n            \n            \n        def _reset(self):\n            self.state = env.reset()\n            self.total_reward = 0.0\n            \n        def buildmodel(self):\n            model = sequential()\n            model.add(layers.conv2d(32,\n                            (8,8),\n                            strides=4,\n                            activation='relu',\n                            data_format='channels_first',\n                            input_shape=observationinput))\n            model.add(layers.conv2d(64,\n                            (4,4),\n                            strides=2,\n                            activation='relu',\n                            data_format='channels_first'))\n            model.add(layers.conv2d(64,\n                            (3,3),\n                            strides=1,\n                            activation='relu',\n                            data_format='channels_first'))\n            model.add(layers.flatten())\n            model.add(layers.dense(512, activation='relu'))\n            model.add(layers.dense(actionoutput, activation=none))\n            \n            model.compile(loss=\"mean_squared_error\",\n                              optimizer=adam(lr=learning_rate))\n            return model\n            \n        def play_step(self, epsilon=0.0):\n            done_reward = none        \n            \n            if(np.random.random() &lt; epsilon): #pick action randomly\n                action = env.action_space.sample()        \n            else: #pick action based on current best q val\n                state_a = np.array([self.state], copy=false)\n                qvals = self.model.predict(state_a)\n                action = np.argmax(qvals[0])\n    \n                 \n            new_state, reward, is_done, _ = self.env.step(action)           \n            self.total_reward += reward\n            \n            exp = experience(self.state, action, reward, is_done, new_state)\n            self.exp_buffer.append(exp)\n            self.state = new_state\n            if is_done:\n                done_reward = self.total_reward\n                self._reset()\n            return done_reward\n        \n        def replay(self):\n            assert(len(self.exp_buffer) &gt; batch_size)\n            experiences = self.exp_buffer.sample(batch_size)         \n    \n            for i in range(len(experiences)):\n                sample = experiences[i]\n                state = sample.state\n                reward = sample.reward\n                action = sample.action\n                newstate  = sample.new_state\n                done = sample.done\n                \n                self.inputbatch [i, ...] = state\n                self.newstatebatch [i, ...] = newstate\n                \n            targetbatch = self.model.predict(self.inputbatch) \n            nextstateqvalues = self.targetmodel.predict(self.newstatebatch)      \n    \n            for i in range(len(experiences)):\n                if experiences[i].done:\n                    targetbatch[i][action] = reward\n                else:                \n                    qfuture = max(nextstateqvalues[i])\n                    targetbatch[i][action] = reward + (qfuture*gamma)              \n    \n            self.model.train_on_batch(self.inputbatch, targetbatch)\n     \n    import sys\n    import time\n    \n    buffer = experiencebuffer(200000)\n    agent = agent(env, buffer)\n    epsilon = epsilon_start\n    \n    agent.model.summary()\n    \n    total_rewards=[]\n    frame_idx = frametime = 0\n    \n    mean_reward = -1\n    bestreward = none\n    \n    print(\"beginning...\")\n    starttime = ts = time.time()\n    while true:    \n        frame_idx += 1\n        \n        epsilon = max(epsilon_final, epsilon_start-frame_idx/epsilon_decay)\n        reward = agent.play_step(epsilon)\n        \n        minutes = (time.time() - starttime)  / 60.0  \n    \n        if(reward is not none):        \n            total_rewards.append(reward)\n            mean_reward = np.mean(total_rewards[-100:])\n            \n            string = \"frame {0}, best reward {1}, reward avg {2}, total games {3}, epsilon {4}, minutes {5}\"\n            \n            if(bestreward is none or bestreward &lt; reward):\n                bestreward = reward\n                print(\"new best reward!\")\n                print(string.format(frame_idx, bestreward, mean_reward, len(total_rewards), epsilon, minutes))\n                \n            \n            if(mean_reward &gt; mean_reward_bound):\n                print(\"solved in %d frames!\" % frame_idx)\n                break\n                \n                \n        if(len(buffer) &lt; replay_start_size):\n            continue\n            \n        \n        if(frame_idx % 5000 == 0):\n            print(\"\\n\")\n            print(string.format(frame_idx, bestreward, mean_reward, len(total_rewards), epsilon, minutes))\n       \n        sys.stdout.flush()\n            \n        if(frame_idx % sync_target_frames == 0):\n            agent.targetmodel.set_weights(agent.model.get_weights())\n       \n        agent.replay()", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/gwrfvh/pong_reinforcement_learning_agent_isnt_converging/',)", "identifyer": 5753649, "year": "2020"}, {"autor": "wkns", "date": 1588758944000, "content": "Similarity metric for trees /!/ I have a classification problem where I use a GridSearch strategy to find a classifier to classify cells (the hard part is to get the signals from a new optical system I designed, the ML part I'm working on is for sure trivial for ML people). The GridSearch converges towards the same classifier each time (GradientBoostingCLassifier) with very good performances.\n\nI then create overlay images (volumes to be more specific) with synthetic colors for each class of cells with predicted and true classes on the validation set. The problem is only a few cells are colored because most of the cells are in the train and test sets. So my idea is to do a sort of cross training/testing/validating where the validation set is switched so that each cell appear once in the validation set so I can create an overlay result with all the cells.\n\nBefore attempting such monstrosity I would like to control that the Decision Trees are quite similar. Although the performances are stable when shuffling the train, test and validation datasets I don't know if the classifier converges towards the **same** tree. The problem for me is to define the similarity between GradientBoostingClassifier, I tried to look at the literature but it's more about theoretical work rather than actual implementations.\n\nThank you for your help!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/geh2k2/similarity_metric_for_trees/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "similarity metric for trees /!/ i have a classification problem where i use a gridsearch strategy to find a classifier to classify cells (the hard part is to get the signals from a new optical system i designed, the ml part i'm working on is for sure trivial for ml people). the gridsearch converges towards the same classifier each time (gradientboostingclassifier) with very good performances.\n\ni then create overlay images (volumes to be more specific) with synthetic colors for each class of cells with predicted and true classes on the validation set. the problem is only a few cells are colored because most of the cells are in the train and test sets. so my idea is to do a sort of cross training/testing/validating where the validation set is switched so that each cell appear once in the validation set so i can create an overlay result with all the cells.\n\nbefore attempting such monstrosity i would like to control that the decision trees are quite similar. although the performances are stable when shuffling the train, test and validation datasets i don't know if the classifier converges towards the **same** -----> tree !!! . the problem for me is to define the similarity between gradientboostingclassifier, i tried to look at the literature but it's more about theoretical work rather than actual implementations.\n\nthank you for your help!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/geh2k2/similarity_metric_for_trees/',)", "identifyer": 5753682, "year": "2020"}, {"autor": "martin1285", "date": 1598184447000, "content": "Unsupervised Learning Model Interpretability? /!/ Hello,\n\nI am working on a small unsupervised learning project in Python where I am utilizing methods in Scikit-learn to create clusters. I am wondering what sort of global explainability methods are available? Are there any existing methods to find what features are driving cluster assignments?\n\nGiven that this is unsupervised, I am a bit unsure how methods like SHAP and ELI5 would work. Are there any other methods that would work? I was considering labeling the data (what cluster it belongs to) and using this as a surrogate, and training another tree based model on the new labeled data set for the explainability. \n\nAny help would be great!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/if21zr/unsupervised_learning_model_interpretability/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "unsupervised learning model interpretability? /!/ hello,\n\ni am working on a small unsupervised learning project in python where i am utilizing methods in scikit-learn to create clusters. i am wondering what sort of global explainability methods are available? are there any existing methods to find what features are driving cluster assignments?\n\ngiven that this is unsupervised, i am a bit unsure how methods like shap and eli5 would work. are there any other methods that would work? i was considering labeling the data (what cluster it belongs to) and using this as a surrogate, and training another -----> tree !!!  based model on the new labeled data set for the explainability. \n\nany help would be great!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/if21zr/unsupervised_learning_model_interpretability/',)", "identifyer": 5753813, "year": "2020"}, {"autor": "BepisMann3000", "date": 1590423907000, "content": "Suggestions for a beginner /!/ Hello there!\n\nI'm a high school student and I am looking to study Computer Science at University. I prefer learning by doing and as such want to do a couple small projects on machine learning. \n\nI am a bit idea starved though and I don't really know what to apply the concepts to... I was thinking of doing some sort of Minimax Game Tree for something simple but not completely basic and some multiple linear regression. \n\nCould you please help me out with some ideas and reference me to some open source database which I could use to that end?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gqe0ac/suggestions_for_a_beginner/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "suggestions for a beginner /!/ hello there!\n\ni'm a high school student and i am looking to study computer science at university. i prefer learning by doing and as such want to do a couple small projects on machine learning. \n\ni am a bit idea starved though and i don't really know what to apply the concepts to... i was thinking of doing some sort of minimax game -----> tree !!!  for something simple but not completely basic and some multiple linear regression. \n\ncould you please help me out with some ideas and reference me to some open source database which i could use to that end?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/gqe0ac/suggestions_for_a_beginner/',)", "identifyer": 5754004, "year": "2020"}, {"autor": "hacknomus", "date": 1606157367000, "content": "#134 Classification tree in Python Training", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jznqf2/134_classification_tree_in_python_training/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "#134 classification -----> tree !!!  in python training", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('rich:video',)", "medialink": "('https://www.youtube.com/watch?v=Y9apg5h71tU&amp;feature=share',)", "identifyer": 5754111, "year": "2020"}, {"autor": "Skaman2050", "date": 1592750141000, "content": "Decision tree classifier in machine learning using scikit learn. | explanation with code. /!/ Decision tree classifier in machine learning using Scikit-learn. Explanation of every term like Gini index, information gain etc. Explanation in easy language. Link is given below click and add some more information about decision tree to your brain.\n\n[**click and boost your knowledge**](https://www.machinexp.com/post/machine-learning-decision-tree-classifier-algorithm-python)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hd7xcz/decision_tree_classifier_in_machine_learning/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "decision -----> tree !!!  classifier in machine learning using scikit learn. | explanation with code. /!/ decision tree classifier in machine learning using scikit-learn. explanation of every term like gini index, information gain etc. explanation in easy language. link is given below click and add some more information about decision tree to your brain.\n\n[**click and boost your knowledge**](https://www.machinexp.com/post/machine-learning-decision-tree-classifier-algorithm-python)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/hd7xcz/decision_tree_classifier_in_machine_learning/',)", "identifyer": 5754262, "year": "2020"}, {"autor": "promach", "date": 1608294358000, "content": "Selection phase of Monte-Carlo Tree Search", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kfk6m8/selection_phase_of_montecarlo_tree_search/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "selection phase of monte-carlo -----> tree !!!  search", "sortedWord": "None", "removed": "('nan',)", "score": 0, "comments": 0, "media": "('link',)", "medialink": "('/r/reinforcementlearning/comments/kfg6qo/selection_phase_of_montecarlo_tree_search/',)", "identifyer": 5754446, "year": "2020"}, {"autor": "statmandu", "date": 1592155436000, "content": "Training set design for rare disease classification? (i.e. ratio of healthy/diseased in the training set?) /!/ Been looking at old kaggle datasets to learn on (specifically the small ones since lack of GPU), and found this one -\u00a0https://www.kaggle.com/c/predicting-a-rare-condition/data?select=health-diagnostics-train.csv\n\n\nI was trying to design a training dataset but realized I wasn't sure what kind of ratio of healthy/diseased to use in the training set. I was going to look at the performance of neural nets and different tree based methods, but I kinda arbitrarily chose a 1:3 ratio of diseased/healthy in the training set.\n\n\nSay I have 5000 \"diseased\" samples, but I know the true prevalence in the population is 1%, should I then build the training set with 5000 diseased/500,000 healthy?\n\n\nCouldn't find anything definitive on google but just wondering how people usually deal with this problem?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/h8xt6j/training_set_design_for_rare_disease/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "training set design for rare disease classification? (i.e. ratio of healthy/diseased in the training set?) /!/ been looking at old kaggle datasets to learn on (specifically the small ones since lack of gpu), and found this one -\u00a0https://www.kaggle.com/c/predicting-a-rare-condition/data?select=health-diagnostics-train.csv\n\n\ni was trying to design a training dataset but realized i wasn't sure what kind of ratio of healthy/diseased to use in the training set. i was going to look at the performance of neural nets and different -----> tree !!!  based methods, but i kinda arbitrarily chose a 1:3 ratio of diseased/healthy in the training set.\n\n\nsay i have 5000 \"diseased\" samples, but i know the true prevalence in the population is 1%, should i then build the training set with 5000 diseased/500,000 healthy?\n\n\ncouldn't find anything definitive on google but just wondering how people usually deal with this problem?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/h8xt6j/training_set_design_for_rare_disease/',)", "identifyer": 5754488, "year": "2020"}], "name": "treelearnmachinelearning2020"}
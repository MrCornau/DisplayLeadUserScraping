{"interestingcomments": [{"autor": "grid_world", "date": 1592375174000, "content": "Deep Compression: Fine-Tuning /!/ Hey guys, I was reading the paper [Deep Compression](https://arxiv.org/abs/1510.00149) and in \"Trained Quantisation and Weight Sharing\" it was mentioned that:\n\n1. Weights are clustered using K-Means algorithm for each layer   \n\n2. Generate code book (clustered centroid/effective weights)\n\n3. Quantize the weights with code book\n\n4. Retrain code book\n\n\nMy questions are:\n\n1.) what is meant by \"retraining\" in step 4? Does it mean that the clustered resulting network is trained until convergence or only fine-tuned, say for 2-3 epochs?\n\n2 ) what if you skip Quantisation to reduce number of bits per floating point number and only focus on retraining code book (effective weights)?\n\n\nMy understanding so far is that in \"Pruning\" step:\n\nYou first train the network, prune lowest p% of lowest magnitude weights in each layer and retrain the resulting network.\n\nHere, \"retraining\" means that you retrain the resulting network until convergence (say using early stopping). I am assuming this since it's not mentioned in the paper.\n\nCorrect me if I am wrong.\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/halqs4/deep_compression_finetuning/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "prune", "selectorShort": "prune", "MarkedSent": "deep compression: fine-tuning /!/ hey guys, i was reading the paper [deep compression](https://arxiv.org/abs/1510.00149) and in \"trained quantisation and weight sharing\" it was mentioned that:\n\n1. weights are clustered using k-means algorithm for each layer   \n\n2. generate code book (clustered centroid/effective weights)\n\n3. quantize the weights with code book\n\n4. retrain code book\n\n\nmy questions are:\n\n1.) what is meant by \"retraining\" in step 4? does it mean that the clustered resulting network is trained until convergence or only fine-tuned, say for 2-3 epochs?\n\n2 ) what if you skip quantisation to reduce number of bits per floating point number and only focus on retraining code book (effective weights)?\n\n\nmy understanding so far is that in \"pruning\" step:\n\nyou first train the network, -----> prune !!!  lowest p% of lowest magnitude weights in each layer and retrain the resulting network.\n\nhere, \"retraining\" means that you retrain the resulting network until convergence (say using early stopping). i am assuming this since it's not mentioned in the paper.\n\ncorrect me if i am wrong.\n\nthanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/halqs4/deep_compression_finetuning/',)", "identifyer": 5748953, "year": "2020"}, {"autor": "zacheism", "date": 1582472411000, "content": "How can I use a Hessian matrix to prune a neural network? /!/ I was asked this question in an interview and even though I know what a Hessian matrix is and how to do NN pruning using other techniques, I have no idea how I might use a Hessian to prune a NN.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f8b0ef/how_can_i_use_a_hessian_matrix_to_prune_a_neural/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "prune", "selectorShort": "prune", "MarkedSent": "how can i use a hessian matrix to -----> prune !!!  a neural network? /!/ i was asked this question in an interview and even though i know what a hessian matrix is and how to do nn pruning using other techniques, i have no idea how i might use a hessian to prune a nn.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 7, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/f8b0ef/how_can_i_use_a_hessian_matrix_to_prune_a_neural/',)", "identifyer": 5751624, "year": "2020"}], "name": "prunelearnmachinelearning2020"}
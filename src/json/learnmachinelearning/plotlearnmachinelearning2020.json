{"interestingcomments": [{"autor": "MattR0se", "date": 1583934847000, "content": "Getting all zeros for permutation feature importance with breast cancer data /!/ I wanted to compare Mean Decrease in Impurity vs permutation feature importance and started with some toy data (breast cancer dataset), and used the method described here:\n\n[https://scikit-learn.org/stable/auto\\_examples/inspection/plot\\_permutation\\_importance.html](https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html)\n\n&amp;#x200B;\n\nYou can find my whole code here, but it works only with scikit-learnversion 0.22.1 or higher:  [https://pastebin.com/WdM2ua04](https://pastebin.com/WdM2ua04)\n\n&amp;#x200B;\n\nI fit the estimator to the training data which consists of 80% of the total data. First I extract the feature\\_importances\\_ property from the random forest, sort and plot them. This works fine.\n\nAfter that I use the [permulation\\_importance](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html#sklearn.inspection.permutation_importance) function with the training data, **but for some reason I only get zeros for every feature**. Did I miss something obvious?\n\n    # Calculate feature importances\n    result = permutation_importance(clf, X_train, y_train,\n                                    random_state=42)\n    importances = result.importances_mean\n    # Sort feature importances in descending order\n    indices = np.argsort(importances)[::-1]\n    # Rearrange feature names so they match the sorted feature importances\n    names = [features[i] for i in indices]", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fgxi7f/getting_all_zeros_for_permutation_feature/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "getting all zeros for permutation feature importance with breast cancer data /!/ i wanted to compare mean decrease in impurity vs permutation feature importance and started with some toy data (breast cancer dataset), and used the method described here:\n\n[https://scikit-learn.org/stable/auto\\_examples/inspection/plot\\_permutation\\_importance.html](https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html)\n\n&amp;#x200b;\n\nyou can find my whole code here, but it works only with scikit-learnversion 0.22.1 or higher:  [https://pastebin.com/wdm2ua04](https://pastebin.com/wdm2ua04)\n\n&amp;#x200b;\n\ni fit the estimator to the training data which consists of 80% of the total data. first i extract the feature\\_importances\\_ property from the random forest, sort and -----> plot !!!  them. this works fine.\n\nafter that i use the [permulation\\_importance](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html#sklearn.inspection.permutation_importance) function with the training data, **but for some reason i only get zeros for every feature**. did i miss something obvious?\n\n    # calculate feature importances\n    result = permutation_importance(clf, x_train, y_train,\n                                    random_state=42)\n    importances = result.importances_mean\n    # sort feature importances in descending order\n    indices = np.argsort(importances)[::-1]\n    # rearrange feature names so they match the sorted feature importances\n    names = [features[i] for i in indices]", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/fgxi7f/getting_all_zeros_for_permutation_feature/',)", "identifyer": 5741169, "year": "2020"}, {"autor": "SrChang-ComCollege", "date": 1587156117000, "content": "plotting AUC from AUC values /!/ Hey Hey!\n\nI have AUC values (and only that) I want to use these values and plot them in one AUC graph.\n\nwould appreciate your suggestions!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g3a2jt/plotting_auc_from_auc_values/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "plotting auc from auc values /!/ hey hey!\n\ni have auc values (and only that) i want to use these values and -----> plot !!!  them in one auc graph.\n\nwould appreciate your suggestions!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/g3a2jt/plotting_auc_from_auc_values/',)", "identifyer": 5741235, "year": "2020"}, {"autor": "GoBacksIn", "date": 1580754112000, "content": "How can I predict time series data using Tensorflow? /!/ here is my code\n\n&amp;#x200B;\n\n    def create_dataset(signal_data, look_back=1):\n        dataX, dataY = [], []\n        for i in range(len(signal_data) - look_back):\n            dataX.append(signal_data[i:(i + look_back), :])\n            dataY.append(signal_data[i + look_back, -1])\n        return np.array(dataX), np.array(dataY)\n    \n    look_back = 20\n    \n    stock = 'kospi'\n    \n    df = pd.read_csv(stock+'.csv')\n    \n    signal_data = df[[\"close\"]].values.astype('float32')\n    \n    \n    train_size = int(len(signal_data) * 0.80)\n    test_size = len(signal_data) - train_size - int(len(signal_data) * 0.05)\n    val_size = len(signal_data) - train_size - test_size\n    train = signal_data[0:train_size]\n    val = signal_data[train_size:train_size + val_size]\n    test = signal_data[train_size + val_size:len(signal_data)]\n    \n    \n    x_train, y_train = create_dataset(train, look_back)\n    x_test, y_test = create_dataset(test, look_back)\n    x_val, y_val = create_dataset(val, look_back)\n    \n    \n    model = Sequential([\n        layers.LSTM(20, input_shape=(None, 1)),\n        layers.Dense(1)\n    ])\n    model.compile(optimizer='adam', loss='mae')\n    \n    history = model.fit(x_train, y_train, epochs=50, batch_size=64, verbose=0, validation_data=(x_val, y_val))\n    \n\n&amp;#x200B;\n\nI want to predict the future, so I tried\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n    a = y_val[-20:] # == signal_data[-20:]\n    for i in range(20):\n        tmp = model.predict(a.reshape(-1, look_back, 1)) #predicted value\n        a = a[1:] #remove first\n        a = np.append(a, tmp) #insert predicted value\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nBut the results are terrible like below link. (Plot like a linear function)\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nhttps://preview.redd.it/jvnwx3r83re41.png?width=388&amp;format=png&amp;auto=webp&amp;s=dd8a30ecaa93ae63f46d39dcffaf9cf8cd704100\n\n&amp;#x200B;\n\nI wonder if this is the right way to do this.\n\nif I wrong, how can I predict future data? \n\nPython : 3.6\n\n&amp;#x200B;\n\nTensorflow : 2.1.0", "link": "https://www.reddit.com/r/learnmachinelearning/comments/eybazv/how_can_i_predict_time_series_data_using/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "how can i predict time series data using tensorflow? /!/ here is my code\n\n&amp;#x200b;\n\n    def create_dataset(signal_data, look_back=1):\n        datax, datay = [], []\n        for i in range(len(signal_data) - look_back):\n            datax.append(signal_data[i:(i + look_back), :])\n            datay.append(signal_data[i + look_back, -1])\n        return np.array(datax), np.array(datay)\n    \n    look_back = 20\n    \n    stock = 'kospi'\n    \n    df = pd.read_csv(stock+'.csv')\n    \n    signal_data = df[[\"close\"]].values.astype('float32')\n    \n    \n    train_size = int(len(signal_data) * 0.80)\n    test_size = len(signal_data) - train_size - int(len(signal_data) * 0.05)\n    val_size = len(signal_data) - train_size - test_size\n    train = signal_data[0:train_size]\n    val = signal_data[train_size:train_size + val_size]\n    test = signal_data[train_size + val_size:len(signal_data)]\n    \n    \n    x_train, y_train = create_dataset(train, look_back)\n    x_test, y_test = create_dataset(test, look_back)\n    x_val, y_val = create_dataset(val, look_back)\n    \n    \n    model = sequential([\n        layers.lstm(20, input_shape=(none, 1)),\n        layers.dense(1)\n    ])\n    model.compile(optimizer='adam', loss='mae')\n    \n    history = model.fit(x_train, y_train, epochs=50, batch_size=64, verbose=0, validation_data=(x_val, y_val))\n    \n\n&amp;#x200b;\n\ni want to predict the future, so i tried\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n    a = y_val[-20:] # == signal_data[-20:]\n    for i in range(20):\n        tmp = model.predict(a.reshape(-1, look_back, 1)) #predicted value\n        a = a[1:] #remove first\n        a = np.append(a, tmp) #insert predicted value\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n&amp;#x200b;\n\nbut the results are terrible like below link. (-----> plot !!!  like a linear function)\n\n&amp;#x200b;\n\n&amp;#x200b;\n\nhttps://preview.redd.it/jvnwx3r83re41.png?width=388&amp;format=png&amp;auto=webp&amp;s=dd8a30ecaa93ae63f46d39dcffaf9cf8cd704100\n\n&amp;#x200b;\n\ni wonder if this is the right way to do this.\n\nif i wrong, how can i predict future data? \n\npython : 3.6\n\n&amp;#x200b;\n\ntensorflow : 2.1.0", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 5, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/eybazv/how_can_i_predict_time_series_data_using/',)", "identifyer": 5741366, "year": "2020"}, {"autor": "bobthemagiccan", "date": 1590221232000, "content": "Which clustering technique should I use? /!/ Hi all,\n\nI have a list of patients, their geography (i.e. location), lots of characteristics and the treatment they are on. I want to cluster the patients to see if there are common characteristics among those that are on a certain treatment. However, I also want to know if they cluster by geography. \n\nNow I know I could use K-means and that would help me group them into clusters. And then I can simply plot them and examine visually if there are clusters. \n\nHowever, Is there a way that I hold a variable constant (i.e geography) and see if there's clusters around that? Or am I thinking of this problem wrong and what I really want is to do a hot spot analysis (however hot spot analysis would tell me if there are clusters by geography, but not if these also cluster by characteristics)\n\nIs there a way to do both? Find clusters of patients that are on a certain treatment and see if they cluster by geography in a systematic way? \n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gp18li/which_clustering_technique_should_i_use/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "which clustering technique should i use? /!/ hi all,\n\ni have a list of patients, their geography (i.e. location), lots of characteristics and the treatment they are on. i want to cluster the patients to see if there are common characteristics among those that are on a certain treatment. however, i also want to know if they cluster by geography. \n\nnow i know i could use k-means and that would help me group them into clusters. and then i can simply -----> plot !!!  them and examine visually if there are clusters. \n\nhowever, is there a way that i hold a variable constant (i.e geography) and see if there's clusters around that? or am i thinking of this problem wrong and what i really want is to do a hot spot analysis (however hot spot analysis would tell me if there are clusters by geography, but not if these also cluster by characteristics)\n\nis there a way to do both? find clusters of patients that are on a certain treatment and see if they cluster by geography in a systematic way? \n\nthanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/gp18li/which_clustering_technique_should_i_use/',)", "identifyer": 5741480, "year": "2020"}, {"autor": "fffrost", "date": 1593544953000, "content": "k-means with different scaling /!/ I have a large dataset with many features and I attempted to use k-means as a first go at clustering, and I saw drastically different results dependent on how I scaled the data. \n\nI first used standardization to scale the data (centred at zero), mostly without much thought. Then ran k-means 25 times to check how it performs at different numbers for k. The resulting within-cluster SSE dropped from \\~50000 to 20000 when I plotted it and looked approximately linear, so it did not seem great. I thought I'd test out how it differed with min-max scaling (between 0 and 1), expecting it to be roughly the same outcome. Instead the performance was quite different with the resulting elbow plot ranging from \\~1050 to just over 400 WCSSE, and there were a couple of clear inflection points. My question is how is it possible that changing the scaling from standardization to min-max could have such a large effect?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hiu7fp/kmeans_with_different_scaling/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "k-means with different scaling /!/ i have a large dataset with many features and i attempted to use k-means as a first go at clustering, and i saw drastically different results dependent on how i scaled the data. \n\ni first used standardization to scale the data (centred at zero), mostly without much thought. then ran k-means 25 times to check how it performs at different numbers for k. the resulting within-cluster sse dropped from \\~50000 to 20000 when i plotted it and looked approximately linear, so it did not seem great. i thought i'd test out how it differed with min-max scaling (between 0 and 1), expecting it to be roughly the same outcome. instead the performance was quite different with the resulting elbow -----> plot !!!  ranging from \\~1050 to just over 400 wcsse, and there were a couple of clear inflection points. my question is how is it possible that changing the scaling from standardization to min-max could have such a large effect?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/hiu7fp/kmeans_with_different_scaling/',)", "identifyer": 5741495, "year": "2020"}, {"autor": "BestTacticsGame2040", "date": 1593391852000, "content": "Can you believe there's only 6 basic plot in this entire world out of all stories that has ever existed? Thank God for perfect answers", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hhq5hx/can_you_believe_theres_only_6_basic_plot_in_this/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "can you believe there's only 6 basic -----> plot !!!  in this entire world out of all stories that has ever existed? thank god for perfect answers", "sortedWord": "None", "removed": "('nan',)", "score": 0, "comments": 2, "media": "('link',)", "medialink": "('https://www.bbc.com/culture/article/20180525-every-story-in-the-world-has-one-of-these-six-basic-plots',)", "identifyer": 5741579, "year": "2020"}, {"autor": "kbobstone", "date": 1596550907000, "content": "What ML algorithm should I use that suits this data? /!/ What if I have some data, let's say I'm trying to answer if education level and IQ affect earnings, and I want to analyze this data and put in a regression model to predict earnings based on the IQ and education level. My confusion is, what if the data is not linear or polynomial? What if it's a mess but there are still patterns that the linear plane algorithm can't capture? How do I figure out if plotting all of the independent variables will form a line or a polynomial curve like here? \n\nhttps://i.stack.imgur.com/LZ5Bd.png\n\nI mean, with one dependent and one independent variable it's easy because you can plot it and see, but in a situation with multiple independent variables... how do I figure out if the relationship is linear or something like this? How do I figure out if I should use a regression model?\n\nLet's say I want to predict a store's daily revenue based on the day of the week, weather and the number of people arrived in the city. My data would look something like this:\n\n    +-----------+---------+----------------+---------+\n    | DAY       | WEATHER | PEOPLE ARRIVED | REVENUE |\n    +-----------+---------+----------------+---------+\n    | Monday    | Sunny   | 1115           | $500    |\n    +-----------+---------+----------------+---------+\n    | Tuesday   | Cloudy  | 808            | $250    |\n    +-----------+---------+----------------+---------+\n    | Wednesday | Sunny   | 450            | $300    |\n    +-----------+---------+----------------+---------+\n\nI'm a bit confused about what ML algorithm I should use in such a scenario. I can represent the days of the week as (Monday - 1, Tuesday - 2, Wednesday - 3, etc.) and the weather as (Sunny - 1, Cloudy - 2, Normal - 3, etc.) but would a regression model work? I'm skeptical because I'm not sure if there's a linear relationship between the variables and I'm not sure if a hyperplane can create accurate representation of what's going on.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i3k6dy/what_ml_algorithm_should_i_use_that_suits_this/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "what ml algorithm should i use that suits this data? /!/ what if i have some data, let's say i'm trying to answer if education level and iq affect earnings, and i want to analyze this data and put in a regression model to predict earnings based on the iq and education level. my confusion is, what if the data is not linear or polynomial? what if it's a mess but there are still patterns that the linear plane algorithm can't capture? how do i figure out if plotting all of the independent variables will form a line or a polynomial curve like here? \n\nhttps://i.stack.imgur.com/lz5bd.png\n\ni mean, with one dependent and one independent variable it's easy because you can -----> plot !!!  it and see, but in a situation with multiple independent variables... how do i figure out if the relationship is linear or something like this? how do i figure out if i should use a regression model?\n\nlet's say i want to predict a store's daily revenue based on the day of the week, weather and the number of people arrived in the city. my data would look something like this:\n\n    +-----------+---------+----------------+---------+\n    | day       | weather | people arrived | revenue |\n    +-----------+---------+----------------+---------+\n    | monday    | sunny   | 1115           | $500    |\n    +-----------+---------+----------------+---------+\n    | tuesday   | cloudy  | 808            | $250    |\n    +-----------+---------+----------------+---------+\n    | wednesday | sunny   | 450            | $300    |\n    +-----------+---------+----------------+---------+\n\ni'm a bit confused about what ml algorithm i should use in such a scenario. i can represent the days of the week as (monday - 1, tuesday - 2, wednesday - 3, etc.) and the weather as (sunny - 1, cloudy - 2, normal - 3, etc.) but would a regression model work? i'm skeptical because i'm not sure if there's a linear relationship between the variables and i'm not sure if a hyperplane can create accurate representation of what's going on.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/i3k6dy/what_ml_algorithm_should_i_use_that_suits_this/',)", "identifyer": 5741662, "year": "2020"}, {"autor": "Papadude13", "date": 1599755196000, "content": "Just have a question of on a part that I'm stuck. /!/ I understand the code I'm just confuse in #plotting the decisions surface. Thank you guys\n\n&amp;#x200B;\n\nfrom matplotlib.colors import ListedColormap\n\nimport matplotlib.pyplot as plt\n\ndef plot\\_decision\\_regions(X, y, classifier, test\\_idx=None,\n\nresolution=0.02):\n\n\\# setup marker generator and color map\n\nmarkers = ('s', 'x', 'o', '\\^', 'v')\n\ncolors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n\ncmap = ListedColormap(colors\\[:len(np.unique(y))\\])\n\n\n\n\\# plot the decision surface\n\nx1\\_min, x1\\_max = X\\[:, 0\\].min() - 1, X\\[:, 0\\].max() + 1\n\nx2\\_min, x2\\_max = X\\[:, 1\\].min() - 1, X\\[:, 1\\].max() + 1\n\nxx1, xx2 = np.meshgrid(np.arange(x1\\_min, x1\\_max, resolution),\n\nnp.arange(x2\\_min, x2\\_max, resolution))\n\nZ = classifier.predict(np.array(\\[xx1.ravel(), xx2.ravel()\\]).T)\n\nZ = Z.reshape(xx1.shape)\n\nplt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n\nplt.xlim(xx1.min(), xx1.max())\n\nplt.ylim(xx2.min(), xx2.max())\n\n\n\nfor idx, cl in enumerate(np.unique(y)):\n\nplt.scatter(x=X\\[y == cl, 0\\], y=X\\[y == cl, 1\\],\n\nalpha=0.8, c=colors\\[idx\\],\n\nmarker=markers\\[idx\\], label=cl,\n\nedgecolor='black')\n\n\n\n\\# highlight test examples\n\nif test\\_idx:\n\n\\# plot all examples\n\nX\\_test, y\\_test = X\\[test\\_idx, :\\], y\\[test\\_idx\\]\n\n\n\nplt.scatter(X\\_test\\[:, 0\\], X\\_test\\[:, 1\\],\n\nc='', edgecolor='black', alpha=1.0,\n\nlinewidth=1, marker='o',\n\ns=100, label='test set')", "link": "https://www.reddit.com/r/learnmachinelearning/comments/iq6o4u/just_have_a_question_of_on_a_part_that_im_stuck/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "just have a question of on a part that i'm stuck. /!/ i understand the code i'm just confuse in #plotting the decisions surface. thank you guys\n\n&amp;#x200b;\n\nfrom matplotlib.colors import listedcolormap\n\nimport matplotlib.pyplot as plt\n\ndef plot\\_decision\\_regions(x, y, classifier, test\\_idx=none,\n\nresolution=0.02):\n\n\\# setup marker generator and color map\n\nmarkers = ('s', 'x', 'o', '\\^', 'v')\n\ncolors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n\ncmap = listedcolormap(colors\\[:len(np.unique(y))\\])\n\n\n\n\\# -----> plot !!!  the decision surface\n\nx1\\_min, x1\\_max = x\\[:, 0\\].min() - 1, x\\[:, 0\\].max() + 1\n\nx2\\_min, x2\\_max = x\\[:, 1\\].min() - 1, x\\[:, 1\\].max() + 1\n\nxx1, xx2 = np.meshgrid(np.arange(x1\\_min, x1\\_max, resolution),\n\nnp.arange(x2\\_min, x2\\_max, resolution))\n\nz = classifier.predict(np.array(\\[xx1.ravel(), xx2.ravel()\\]).t)\n\nz = z.reshape(xx1.shape)\n\nplt.contourf(xx1, xx2, z, alpha=0.3, cmap=cmap)\n\nplt.xlim(xx1.min(), xx1.max())\n\nplt.ylim(xx2.min(), xx2.max())\n\n\n\nfor idx, cl in enumerate(np.unique(y)):\n\nplt.scatter(x=x\\[y == cl, 0\\], y=x\\[y == cl, 1\\],\n\nalpha=0.8, c=colors\\[idx\\],\n\nmarker=markers\\[idx\\], label=cl,\n\nedgecolor='black')\n\n\n\n\\# highlight test examples\n\nif test\\_idx:\n\n\\# plot all examples\n\nx\\_test, y\\_test = x\\[test\\_idx, :\\], y\\[test\\_idx\\]\n\n\n\nplt.scatter(x\\_test\\[:, 0\\], x\\_test\\[:, 1\\],\n\nc='', edgecolor='black', alpha=1.0,\n\nlinewidth=1, marker='o',\n\ns=100, label='test set')", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/iq6o4u/just_have_a_question_of_on_a_part_that_im_stuck/',)", "identifyer": 5741728, "year": "2020"}, {"autor": "JesW87", "date": 1602954856000, "content": "How to know which regression hypothesis to use? /!/ If I'm given a set of data, and I need to use regression to predict some values, how can I decide which regression model to use (out of like, linear, cubic, quadratic, square-root function, etc...).  If there is a y and only one x, then I can plot the points and kind of eyeball it.  But how do I go about doing this when there is more than one variable?  We went over [this](https://i.imgur.com/2l8dIJC.png) example in class, and the professor jumped to the hypothesis he used without any explanation for how he chose it.  How do I get from *that* set of data to *that* hypothesis?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jcz1ut/how_to_know_which_regression_hypothesis_to_use/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "how to know which regression hypothesis to use? /!/ if i'm given a set of data, and i need to use regression to predict some values, how can i decide which regression model to use (out of like, linear, cubic, quadratic, square-root function, etc...).  if there is a y and only one x, then i can -----> plot !!!  the points and kind of eyeball it.  but how do i go about doing this when there is more than one variable?  we went over [this](https://i.imgur.com/2l8dijc.png) example in class, and the professor jumped to the hypothesis he used without any explanation for how he chose it.  how do i get from *that* set of data to *that* hypothesis?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/jcz1ut/how_to_know_which_regression_hypothesis_to_use/',)", "identifyer": 5741804, "year": "2020"}, {"autor": "MattR0se", "date": 1580389995000, "content": "Getting nonsense result with sklearn permutation importance /!/ I want to compare Mean Decrease in Impurity vs permutation feature importance and started with some toy data (breast cancer dataset). I oriented myself after this article: \n\n[https://scikit-learn.org/stable/auto\\_examples/inspection/plot\\_permutation\\_importance.html](https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html)\n\nYou can find the whole code here, but it works only with the latest version of scikit-learn (0.22.1): [https://pastebin.com/WdM2ua04](https://pastebin.com/WdM2ua04)\n\nI fit the estimator to the training data which consists of 80% of the total data. First I extract the feature\\_importances\\_ property from the random forest, sort and plot them. This works fine.\n\nAfter that I use the [permulation\\_importance](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html#sklearn.inspection.permutation_importance) function with the training data, but for some reason I only get zeros for every feature. Did I miss something obvious?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ew58vk/getting_nonsense_result_with_sklearn_permutation/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "getting nonsense result with sklearn permutation importance /!/ i want to compare mean decrease in impurity vs permutation feature importance and started with some toy data (breast cancer dataset). i oriented myself after this article: \n\n[https://scikit-learn.org/stable/auto\\_examples/inspection/plot\\_permutation\\_importance.html](https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html)\n\nyou can find the whole code here, but it works only with the latest version of scikit-learn (0.22.1): [https://pastebin.com/wdm2ua04](https://pastebin.com/wdm2ua04)\n\ni fit the estimator to the training data which consists of 80% of the total data. first i extract the feature\\_importances\\_ property from the random forest, sort and -----> plot !!!  them. this works fine.\n\nafter that i use the [permulation\\_importance](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html#sklearn.inspection.permutation_importance) function with the training data, but for some reason i only get zeros for every feature. did i miss something obvious?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/ew58vk/getting_nonsense_result_with_sklearn_permutation/',)", "identifyer": 5741986, "year": "2020"}, {"autor": "insatiableone2", "date": 1587020688000, "content": "What is this regression analysis chart called? /!/ Hi, \n\nI am working on a machine learning project that takes several parameters to predict one parameter numerical value (a regression chart). I was following along this tutorial by tensorflow: [https://www.tensorflow.org/tutorials/keras/regression](https://www.tensorflow.org/tutorials/keras/regression) \n\n&amp;#x200B;\n\nAt the end, after training is done, a chart is coded that looks like this: \n\n&amp;#x200B;\n\n&amp;#x200B;\n\nhttps://preview.redd.it/pd62rjdsp4t41.png?width=1390&amp;format=png&amp;auto=webp&amp;s=13db02bafbd3a3bb39a564d2f75c86dff1b2d95d\n\n&amp;#x200B;\n\nIt shows a plot of all predicted numbers with the actual value on the x-axis and the predicted value on the y-axis. A line of slope 1 is drawn which represents a perfect model if all the predictions were on that line. \n\n&amp;#x200B;\n\nMy question is... what exactly is this graph called? regression scatterplot? linear regression chart? \n\n&amp;#x200B;\n\nThanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g2a1s7/what_is_this_regression_analysis_chart_called/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "what is this regression analysis chart called? /!/ hi, \n\ni am working on a machine learning project that takes several parameters to predict one parameter numerical value (a regression chart). i was following along this tutorial by tensorflow: [https://www.tensorflow.org/tutorials/keras/regression](https://www.tensorflow.org/tutorials/keras/regression) \n\n&amp;#x200b;\n\nat the end, after training is done, a chart is coded that looks like this: \n\n&amp;#x200b;\n\n&amp;#x200b;\n\nhttps://preview.redd.it/pd62rjdsp4t41.png?width=1390&amp;format=png&amp;auto=webp&amp;s=13db02bafbd3a3bb39a564d2f75c86dff1b2d95d\n\n&amp;#x200b;\n\nit shows a -----> plot !!!  of all predicted numbers with the actual value on the x-axis and the predicted value on the y-axis. a line of slope 1 is drawn which represents a perfect model if all the predictions were on that line. \n\n&amp;#x200b;\n\nmy question is... what exactly is this graph called? regression scatterplot? linear regression chart? \n\n&amp;#x200b;\n\nthanks", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/g2a1s7/what_is_this_regression_analysis_chart_called/',)", "identifyer": 5742109, "year": "2020"}, {"autor": "rahull33t", "date": 1583769476000, "content": "How to derive statistical significance from an XG Boost model? /!/ I am working on a dataset with around 16k rows and 80 features.  The target variable has 4 classes.  Around 30-40 variables have missing values and there is high class imbalance, so I decided to use XGB model for prediction.  Although prediction is the main goal, by using the feature importance plot I got decent insights after digging into the important features.  When I presented them to the business user, they were \u201cintrigued\u201d by the patterns.  But they asked me how can you say that these variables are statistically significant among other variables?  I did not know what to say.\n\nAlso, more generally, can we prove statistical significance in boosted (or bagged for that matter) trees?  Does it make sense to think in this way?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ffwti4/how_to_derive_statistical_significance_from_an_xg/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "how to derive statistical significance from an xg boost model? /!/ i am working on a dataset with around 16k rows and 80 features.  the target variable has 4 classes.  around 30-40 variables have missing values and there is high class imbalance, so i decided to use xgb model for prediction.  although prediction is the main goal, by using the feature importance -----> plot !!!  i got decent insights after digging into the important features.  when i presented them to the business user, they were \u201cintrigued\u201d by the patterns.  but they asked me how can you say that these variables are statistically significant among other variables?  i did not know what to say.\n\nalso, more generally, can we prove statistical significance in boosted (or bagged for that matter) trees?  does it make sense to think in this way?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/ffwti4/how_to_derive_statistical_significance_from_an_xg/',)", "identifyer": 5742232, "year": "2020"}, {"autor": "__in_control", "date": 1582317226000, "content": "How to visualize/plot data with large number of features ( &gt;10K ) /!/ What is your go-to method to plot/visualize data with a huge number of feature variables ?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f7h777/how_to_visualizeplot_data_with_large_number_of/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "how to visualize/-----> plot !!!  data with large number of features ( &gt;10k ) /!/ what is your go-to method to plot/visualize data with a huge number of feature variables ?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/f7h777/how_to_visualizeplot_data_with_large_number_of/',)", "identifyer": 5742323, "year": "2020"}, {"autor": "AbroadPotential", "date": 1606045469000, "content": "Why would you plot the test loss/test accuracy? /!/ Hi all!\n\nI am relatively new in Machine Learning. In a course, we have to classify images as benign or malignant breast cancer. We have written a CNN. My group members who I am not reaching right now asked me to visualize the training accuracy, training loss, testing accuracy and testing loss as a function of epochs. I am confused right now why you would visualize the testing loss and the testing accuracy. It makes complete sense to me that you visualize the training accuracy and the training loss cause you would expect your network to learn. But why then does it make sense to visualize the testing accuracy and testing loss as a function of epochs if your network already learned and there should be no adapting?\n\nBest wishes!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jyudzf/why_would_you_plot_the_test_losstest_accuracy/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "why would you -----> plot !!!  the test loss/test accuracy? /!/ hi all!\n\ni am relatively new in machine learning. in a course, we have to classify images as benign or malignant breast cancer. we have written a cnn. my group members who i am not reaching right now asked me to visualize the training accuracy, training loss, testing accuracy and testing loss as a function of epochs. i am confused right now why you would visualize the testing loss and the testing accuracy. it makes complete sense to me that you visualize the training accuracy and the training loss cause you would expect your network to learn. but why then does it make sense to visualize the testing accuracy and testing loss as a function of epochs if your network already learned and there should be no adapting?\n\nbest wishes!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/jyudzf/why_would_you_plot_the_test_losstest_accuracy/',)", "identifyer": 5742440, "year": "2020"}, {"autor": "taknev83", "date": 1606041341000, "content": "How To Build Interactive EDA In 2 Lines Of Code Using Pywedge /!/  I am happy to share an addition to my Python package - Pywedge-Make\\_Charts.  \n\n\nPywedge-Make\\_Charts can plot 29 interactive plots with interactive axis selection widgets for any given dataset in 2 lines of code, spend your quality time to uncover the pattern in Charts! (pip install pywedge)  \n\n\nKindly have a glimpse over the article on Pywedge-Make\\_Charts published in [Analytics India Magazine](https://analyticsindiamag.com/how-to-build-interactive-eda-in-2-lines-of-code-using-pywedge/)  \n[GitHub](https://github.com/taknev83/pywedge) &amp; [PyPi](https://pypi.org/project/pywedge/) links  \n \n\nPywedge is a[ pip installable](https://pypi.org/project/pywedge/) python package that intends to,\n\n1. Make multiple charts in a single line of code, to enable the user to quickly read through the charts and can make informed choices in further pre-processing steps\n2. Quickly preprocess the data by taking the user\u2019s preferred choice of pre-processing techniques\n3. Make a baseline model summary, which can return ten various baseline models, which can point the user to explore the best performing baseline model.\n\nPlease feel free to pip install pywedge, use &amp; share your valuable feedback, it will motivate me to fine-tune the pywedge. Thanks :)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jytmwz/how_to_build_interactive_eda_in_2_lines_of_code/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "how to build interactive eda in 2 lines of code using pywedge /!/  i am happy to share an addition to my python package - pywedge-make\\_charts.  \n\n\npywedge-make\\_charts can -----> plot !!!  29 interactive plots with interactive axis selection widgets for any given dataset in 2 lines of code, spend your quality time to uncover the pattern in charts! (pip install pywedge)  \n\n\nkindly have a glimpse over the article on pywedge-make\\_charts published in [analytics india magazine](https://analyticsindiamag.com/how-to-build-interactive-eda-in-2-lines-of-code-using-pywedge/)  \n[github](https://github.com/taknev83/pywedge) &amp; [pypi](https://pypi.org/project/pywedge/) links  \n \n\npywedge is a[ pip installable](https://pypi.org/project/pywedge/) python package that intends to,\n\n1. make multiple charts in a single line of code, to enable the user to quickly read through the charts and can make informed choices in further pre-processing steps\n2. quickly preprocess the data by taking the user\u2019s preferred choice of pre-processing techniques\n3. make a baseline model summary, which can return ten various baseline models, which can point the user to explore the best performing baseline model.\n\nplease feel free to pip install pywedge, use &amp; share your valuable feedback, it will motivate me to fine-tune the pywedge. thanks :)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/jytmwz/how_to_build_interactive_eda_in_2_lines_of_code/',)", "identifyer": 5742441, "year": "2020"}, {"autor": "hiphop1987", "date": 1605970197000, "content": "Pandas Web API /!/ I've made an [open-source project](https://github.com/romanorac/pandas-analytics-server) that simplifies connecting pandas to a real-time data feed, testing hypotheses and visualizing results in a web browser.\n\nThe server is also live and you can try it [here](http://ec2-54-154-144-55.eu-west-1.compute.amazonaws.com:5000/plot) (hit refresh couple of times).\n\nIt responds to 2 endpoints:\n\n* [a simple status update](http://ec2-54-154-144-55.eu-west-1.compute.amazonaws.com:5000/) so that we know the server is running,\n* [a page with plots to visualize the data](http://ec2-54-154-144-55.eu-west-1.compute.amazonaws.com:5000/plot). It accepts the *n\\_entries* query parameter to limit the number of data points to visualize (e.g. *?n\\_entries=10)*.\n\nhttps://preview.redd.it/dabc2me9wl061.png?width=1200&amp;format=png&amp;auto=webp&amp;s=c73d45aaa68ed7059ac22ec6b5cd10c028805b4b\n\n# Use case\n\nYou have found an interesting pattern in a certain dataset (eg. stocks, cryptocurrencies, weather data). You have written a quick-and-dirty code in a Jupyter Notebook to analyze and plot the data. The results of the analysis seem promising and you would like to test your hypothesis on live data.\n\nTo test the hypothesis on live data, you need to port the Jupyter Notebook code to the server-side. This is a cumbersome process if you are not well versed in backend development. So much time went into tweaking matplotlib parameters, now you need to reimplement them in JavaScript.\n\nIt enables us to copy-paste pandas code from Jupyter Notebook, start the server and observe how our strategy performs in the real world. It enables us to reuse matplotlib code as pandas-analytics-server renders it on a web page.\n\n# Want to learn more?\n\nSee [Pandas Web\u00a0API](https://towardsdatascience.com/pandas-analytics-server-d64d20ef01be?sk=0292c30f7a54f42c037b0da6af9782e4) to learn more.\n\nThanks for reading. Feedback is welcome.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jybxqj/pandas_web_api/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "pandas web api /!/ i've made an [open-source project](https://github.com/romanorac/pandas-analytics-server) that simplifies connecting pandas to a real-time data feed, testing hypotheses and visualizing results in a web browser.\n\nthe server is also live and you can try it [here](http://ec2-54-154-144-55.eu-west-1.compute.amazonaws.com:5000/-----> plot !!! ) (hit refresh couple of times).\n\nit responds to 2 endpoints:\n\n* [a simple status update](http://ec2-54-154-144-55.eu-west-1.compute.amazonaws.com:5000/) so that we know the server is running,\n* [a page with plots to visualize the data](http://ec2-54-154-144-55.eu-west-1.compute.amazonaws.com:5000/plot). it accepts the *n\\_entries* query parameter to limit the number of data points to visualize (e.g. *?n\\_entries=10)*.\n\nhttps://preview.redd.it/dabc2me9wl061.png?width=1200&amp;format=png&amp;auto=webp&amp;s=c73d45aaa68ed7059ac22ec6b5cd10c028805b4b\n\n# use case\n\nyou have found an interesting pattern in a certain dataset (eg. stocks, cryptocurrencies, weather data). you have written a quick-and-dirty code in a jupyter notebook to analyze and plot the data. the results of the analysis seem promising and you would like to test your hypothesis on live data.\n\nto test the hypothesis on live data, you need to port the jupyter notebook code to the server-side. this is a cumbersome process if you are not well versed in backend development. so much time went into tweaking matplotlib parameters, now you need to reimplement them in javascript.\n\nit enables us to copy-paste pandas code from jupyter notebook, start the server and observe how our strategy performs in the real world. it enables us to reuse matplotlib code as pandas-analytics-server renders it on a web page.\n\n# want to learn more?\n\nsee [pandas web\u00a0api](https://towardsdatascience.com/pandas-analytics-server-d64d20ef01be?sk=0292c30f7a54f42c037b0da6af9782e4) to learn more.\n\nthanks for reading. feedback is welcome.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/jybxqj/pandas_web_api/',)", "identifyer": 5742471, "year": "2020"}, {"autor": "hiphop1987", "date": 1609253688000, "content": "Don\u2019t Make These Pandas Mistakes /!/ Have you ever planned you\u2019d need an hour to finish a short task, but then you spend a whole day working on it? If yes, welcome to my world!\n\nI originally published this in \"[Don\u2019t Make These 3 Pandas Mistakes](https://towardsdatascience.com/dont-make-these-3-pandas-mistakes-f22337a47e31?sk=f04765d3180176c4eb39ebe647c74b74)\", so take a look in case you'll like to avoid more pitfalls with pandas.\n\nOnly a fool learns from his own mistakes. The wise man learns from the mistakes of others.\n\n# How NOT to visualize a Weighted Average\n\nThe weighted average is similar to an ordinary average, except that some data points contribute more than others.\n\n[Weighted arithmetic mean from Wikipedia](https://preview.redd.it/s5ulxhqo25861.png?width=724&amp;format=png&amp;auto=webp&amp;s=442b477d4c30dc6899bd94e1d924bc479e9a55db)\n\nThis mistake with pandas occurred to me when I was working on an Exploratory Data Analysis \u2014 a typical day for a Data Scientist.\n\nI extracted the data from the database, loaded it with pandas and followed a piece of advice from an ancient philosopher:\n\n&gt;A picture is worth a thousand words.\n\nIn other words, I\u2019ve visualized the data.\n\nSomething caught my eye on the plot and I wasn\u2019t sure what at first.\n\n**Mistake**\n\nLet\u2019s repeat the steps and see if you spot the mistake. First, let\u2019s define a sample DataFrame:\n\n    df = pd.DataFrame({\"bin\": [0, 1, 2, 4, 5], \"value\": [1, 2, 3, 2, 5]})\n\nhttps://preview.redd.it/rqs76vts25861.png?width=1394&amp;format=png&amp;auto=webp&amp;s=2e5d9ec01c4cd79d32af0162ef4714489b8ab07c\n\nCalculate the weighted average, which is 3.15.\n\n    weighted_average = (df.bin * df.value).sum() / df.value.sum()\n\n\u2026 and visualize the plot:\n\n    # visualize the plot\nax = df.plot.bar(x=\"bin\", y=\"value\", figsize=(14, 7))\n    \n    # plot the weighted average\nax.axvline(weighted_average, color=\"black\", linestyle=\"--\", label=\"Weighted average\")\n\nhttps://preview.redd.it/td8cfyfb35861.png?width=1400&amp;format=png&amp;auto=webp&amp;s=9c8f72c8eb0be16f0c88ee91b66a9eb0234618fb\n\nIt took me a while to figure out what was the mistake.\n\nThe weighted average is 3.15, but on the plot, it\u2019s shown at \\~4.20. What\u2019s happening here? A bug in matplotlib?\n\n**Solution**\n\nNot really! The problem is that bin 3 is not present in the DataFrame. Let\u2019s add it and set the value to 0 as it\u2019s not present.\n\n    df = df.append({\"bin\": 3, \"value\": 0}, ignore_index=True)\ndf = df.sort_values([\"bin\"]).reset_index(drop=True)\n\nhttps://preview.redd.it/tgu5p0mf35861.png?width=1376&amp;format=png&amp;auto=webp&amp;s=871e2443e2398482fee98b6d05a8cece8ddd165a\n\nWe don\u2019t need to recalculate the weighted average as it stays the same.\n\nLet\u2019s look at the plot now. Better, right?\n\n    # visualize the plot\nax = df.plot.bar(x=\"bin\", y=\"value\", figsize=(14, 7))\n    \n    # plot the weighted average\nax.axvline(weighted_average, color=\"black\", linestyle=\"--\", label=\"Weighted average\")\n\nhttps://preview.redd.it/g7a8zoji35861.png?width=1400&amp;format=png&amp;auto=webp&amp;s=020cba69e614af675d3d6759852f6e822cfcb1e7", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kmfvcb/dont_make_these_pandas_mistakes/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "don\u2019t make these pandas mistakes /!/ have you ever planned you\u2019d need an hour to finish a short task, but then you spend a whole day working on it? if yes, welcome to my world!\n\ni originally published this in \"[don\u2019t make these 3 pandas mistakes](https://towardsdatascience.com/dont-make-these-3-pandas-mistakes-f22337a47e31?sk=f04765d3180176c4eb39ebe647c74b74)\", so take a look in case you'll like to avoid more pitfalls with pandas.\n\nonly a fool learns from his own mistakes. the wise man learns from the mistakes of others.\n\n# how not to visualize a weighted average\n\nthe weighted average is similar to an ordinary average, except that some data points contribute more than others.\n\n[weighted arithmetic mean from wikipedia](https://preview.redd.it/s5ulxhqo25861.png?width=724&amp;format=png&amp;auto=webp&amp;s=442b477d4c30dc6899bd94e1d924bc479e9a55db)\n\nthis mistake with pandas occurred to me when i was working on an exploratory data analysis \u2014 a typical day for a data scientist.\n\ni extracted the data from the database, loaded it with pandas and followed a piece of advice from an ancient philosopher:\n\n&gt;a picture is worth a thousand words.\n\nin other words, i\u2019ve visualized the data.\n\nsomething caught my eye on the -----> plot !!!  and i wasn\u2019t sure what at first.\n\n**mistake**\n\nlet\u2019s repeat the steps and see if you spot the mistake. first, let\u2019s define a sample dataframe:\n\n    df = pd.dataframe({\"bin\": [0, 1, 2, 4, 5], \"value\": [1, 2, 3, 2, 5]})\n\nhttps://preview.redd.it/rqs76vts25861.png?width=1394&amp;format=png&amp;auto=webp&amp;s=2e5d9ec01c4cd79d32af0162ef4714489b8ab07c\n\ncalculate the weighted average, which is 3.15.\n\n    weighted_average = (df.bin * df.value).sum() / df.value.sum()\n\n\u2026 and visualize the plot:\n\n    # visualize the plot\nax = df.plot.bar(x=\"bin\", y=\"value\", figsize=(14, 7))\n    \n    # plot the weighted average\nax.axvline(weighted_average, color=\"black\", linestyle=\"--\", label=\"weighted average\")\n\nhttps://preview.redd.it/td8cfyfb35861.png?width=1400&amp;format=png&amp;auto=webp&amp;s=9c8f72c8eb0be16f0c88ee91b66a9eb0234618fb\n\nit took me a while to figure out what was the mistake.\n\nthe weighted average is 3.15, but on the plot, it\u2019s shown at \\~4.20. what\u2019s happening here? a bug in matplotlib?\n\n**solution**\n\nnot really! the problem is that bin 3 is not present in the dataframe. let\u2019s add it and set the value to 0 as it\u2019s not present.\n\n    df = df.append({\"bin\": 3, \"value\": 0}, ignore_index=true)\ndf = df.sort_values([\"bin\"]).reset_index(drop=true)\n\nhttps://preview.redd.it/tgu5p0mf35861.png?width=1376&amp;format=png&amp;auto=webp&amp;s=871e2443e2398482fee98b6d05a8cece8ddd165a\n\nwe don\u2019t need to recalculate the weighted average as it stays the same.\n\nlet\u2019s look at the plot now. better, right?\n\n    # visualize the plot\nax = df.plot.bar(x=\"bin\", y=\"value\", figsize=(14, 7))\n    \n    # plot the weighted average\nax.axvline(weighted_average, color=\"black\", linestyle=\"--\", label=\"weighted average\")\n\nhttps://preview.redd.it/g7a8zoji35861.png?width=1400&amp;format=png&amp;auto=webp&amp;s=020cba69e614af675d3d6759852f6e822cfcb1e7", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/kmfvcb/dont_make_these_pandas_mistakes/',)", "identifyer": 5742499, "year": "2020"}, {"autor": "steph_back41", "date": 1609101633000, "content": "Feature Engineering for Classification Problem /!/ I am currently doing EDA for a classification problem of predicting soccer players positions based on their statistics. There are certain statistics that I am sure do not offer any help in classifying the positions. Is it enough to plot a histogram of each of the features, with the different positions as different colours/patterns, that shows clearly that in the histogram there is no clear distinction between the different positions? Is this enough to eliminate the feature from the training set, or is there a metric based method that I should take?\n\nWhat are usually the methods for feature engineering in classification problems to eliminate unnecessary features?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/klb125/feature_engineering_for_classification_problem/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "feature engineering for classification problem /!/ i am currently doing eda for a classification problem of predicting soccer players positions based on their statistics. there are certain statistics that i am sure do not offer any help in classifying the positions. is it enough to -----> plot !!!  a histogram of each of the features, with the different positions as different colours/patterns, that shows clearly that in the histogram there is no clear distinction between the different positions? is this enough to eliminate the feature from the training set, or is there a metric based method that i should take?\n\nwhat are usually the methods for feature engineering in classification problems to eliminate unnecessary features?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/klb125/feature_engineering_for_classification_problem/',)", "identifyer": 5742578, "year": "2020"}, {"autor": "iamrealadvait", "date": 1590130010000, "content": "Machine Learning with Python : Data Visualisation : How to plot pie-chart and use matplotlib and seaborn library. more on : www.facebook.com/seevecoding", "link": "https://www.reddit.com/r/learnmachinelearning/comments/goeais/machine_learning_with_python_data_visualisation/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "machine learning with python : data visualisation : how to -----> plot !!!  pie-chart and use matplotlib and seaborn library. more on : www.facebook.com/seevecoding", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('hosted:video',)", "medialink": "('https://v.redd.it/8aque9xcj9051',)", "identifyer": 5742828, "year": "2020"}, {"autor": "alex_karavaev", "date": 1590062885000, "content": "Way to plot good-looking rewards plots?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gnvo5m/way_to_plot_goodlooking_rewards_plots/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "way to -----> plot !!!  good-looking rewards plots?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('/r/reinforcementlearning/comments/gnvlcp/way_to_plot_goodlooking_rewards_plots/',)", "identifyer": 5742866, "year": "2020"}, {"autor": "Vishesh1597", "date": 1594980428000, "content": "How to tune for precision &amp; recall? /!/ Choosing a threshold from precision recall curve\n\nI\u2019m working on a problem that predicts customer churns.\nIt is an imbalanced dataset.\n{+ve class: 85%, -ve class: 15%}\nI used gridsearchCV and scoring : f1 score\nAnd got a result with very high precision but a relatively low recall. \nFor this problem, I was looking more for a good balance between precision and recall.\nHow can I achieve that?\nDo I plot a precision recall curve for the validation set after training and select a proper threshold that balanced between the two?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hst3mt/how_to_tune_for_precision_recall/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "how to tune for precision &amp; recall? /!/ choosing a threshold from precision recall curve\n\ni\u2019m working on a problem that predicts customer churns.\nit is an imbalanced dataset.\n{+ve class: 85%, -ve class: 15%}\ni used gridsearchcv and scoring : f1 score\nand got a result with very high precision but a relatively low recall. \nfor this problem, i was looking more for a good balance between precision and recall.\nhow can i achieve that?\ndo i -----> plot !!!  a precision recall curve for the validation set after training and select a proper threshold that balanced between the two?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 5, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/hst3mt/how_to_tune_for_precision_recall/',)", "identifyer": 5743007, "year": "2020"}, {"autor": "Gymandpuppies", "date": 1594942023000, "content": "Are medoids actual points on the dataset or do we plot them randomly? (PAM) /!/ Trying to understand PAM (Partitioning around medoids) Algorithm.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hsku4i/are_medoids_actual_points_on_the_dataset_or_do_we/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "are medoids actual points on the dataset or do we -----> plot !!!  them randomly? (pam) /!/ trying to understand pam (partitioning around medoids) algorithm.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/hsku4i/are_medoids_actual_points_on_the_dataset_or_do_we/',)", "identifyer": 5743022, "year": "2020"}, {"autor": "Python_noob11", "date": 1599354040000, "content": "Help understanding Python linear regression model code issue /!/ Hello,\n\nI'm working on my first Linear Regression code using a Tech with Tim video ([https://www.youtube.com/watch?v=45ryDIPHdGg](https://www.youtube.com/watch?v=45ryDIPHdGg)) and have run into a snag.  I'm using the UCI student data from here:  [https://archive.ics.uci.edu/ml/datasets/Student+Performance](https://archive.ics.uci.edu/ml/datasets/Student+Performance)\n\nMy initial model code ran fine.  Then I iterated to find an optimal accuracy model, which was fine.  Where it starts to go off the rails is that I tried to then inject those optimal coefficients into a new model, then run two predictions: \n\n1) The very first model (pre-optimization loop)\n\n2) The optimized model\n\nAgainst the same x\\_test1 data set.  To compare the two, I simply summed the squared difference between predicted and actual y values.  Then I also recorded the final accuracy of both models.\n\nI've done something wrong because the accuracy of my new \"optimized\" model is the same or lower as the very first model, and the difference values is very similar as well.  I expected the optimized model to have much less error and a higher accuracy.\n\nCan someone help me to see the error?   I suspect the error lies after the plot section of code.  Thanks in advance, code below.\n\n    # Import libraries\n    import pandas as pd\n    import numpy as np\n    import sklearn\n    import pickle\n    import matplotlib.pyplot as plt\n    from sklearn import linear_model\n    from math import sqrt\n    from sklearn.linear_model import LinearRegression\n    from matplotlib import style\n    \n    # from sklearn.utils import shuffle\n    \n    # Read in Data\n    data = pd.read_csv(\"student-mat.csv\", sep=\";\")\n    \n    # Slice data to include only desired headings\n    data = data[[\"G1\", \"G2\", \"G3\", \"studytime\", \"failures\", \"absences\"]]\n    \n    # Define the attribute we are trying to predict; called \"label\".\n    # Others are \"features\" and used to predict label\n    predict = \"G3\"\n    \n    # Create array of features and label\n    X = np.array(data.drop([predict], 1))\n    y = np.array(data[predict])\n    \n    # Split data into training and testing data.  90% used for training, 10% testing\n    # Test size 0.1 = 10% of array size\n    x_train1, x_test1, y_train1, y_test1 = sklearn.model_selection.train_test_split(X, y, test_size=0.1)\n    \n    # Create 1st linear model and fit\n    linear = linear_model.LinearRegression()\n    linear.fit(x_train1, y_train1)\n    \n    # Compute accuracy of model\n    acc = linear.score(x_test1, y_test1)\n    \n    # Iterate for a given number of times (max_iter) to find an optimal accuracy value and record best coefficients\n    loop_num = 1\n    max_iter = 1000\n    best_acc = acc\n    best_coef = linear.coef_\n    best_int = linear.intercept_\n    acc_counter = [acc]\n    \n    print(\"\\nInitial Accuracy: %4.3f\" % acc)\n    \n    while loop_num &lt; max_iter + 1:\n        x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.1)\n        linear2 = linear_model.LinearRegression()\n        linear2.fit(x_train, y_train)\n        acc = linear2.score(x_test, y_test)\n        acc_counter.append(acc)\n        print(\"\\nAccuracy of run \" + str(loop_num) + \" is: %4.3f\" % acc)\n        if acc &gt; best_acc:\n            print(\"\\n\\tBetter accuracy found.\")\n            best_acc = acc\n            best_coef = linear2.coef_\n            best_int = linear2.intercept_\n            print(\"Co: \\n\", linear2.coef_)\n            print(\"Intercept: \\n\", linear2.intercept_)\n        else:\n            print(\"\\n\\tFit Discarded.\")\n        loop_num += 1\n    \n    print(\"\\nBest Acccuracy: \\n%4.3f\" % best_acc)\n    print(\"\\nBest Coefficients: \\n\", best_coef)\n    print(\"\\nBest Intercept: \\n\", best_int)\n    \n    # Plot Accuracy over time\n    x_scale = []\n    for x in range(max_iter + 1):\n        x_scale.append(x)\n    \n    plt.plot(x_scale, acc_counter, color='green', linestyle='dashed', linewidth=3, marker='o',\n             markerfacecolor='blue', markersize=5)\n    \n    ymax = max(acc_counter)\n    ymin = min(acc_counter)\n    xpos = acc_counter.index(ymax)\n    xmax = x_scale[xpos]\n    annot_max_acc = str(ymax)\n    plt.annotate('Max Accuracy = ' + annot_max_acc[0:4], xy=(xmax, ymax), xycoords='data', xytext=(.8, .95),\n                 textcoords='axes fraction',\n                 arrowprops=dict(facecolor='black', shrink=0.05), horizontalalignment='right', verticalalignment='top')\n    plt.ylim(ymin, 1.0)\n    plt.xlabel('Run Number')\n    plt.ylabel('Accuracy')\n    plt.title('Prediction Accuracy over Time')\n    plt.show()\n    \n    # Create model with best coefficients from above\n    new_model = linear_model.LinearRegression()\n    new_model.intercept_ = best_int\n    new_model.coef_ = best_coef\n    \n    # Predict y values for 1st model (not best) then compute difference between predictions and actual values\n    print(\"\\n\\n\\nBREAK\")\n    comp = []\n    predictions = linear.predict(x_test1)\n    for x in range(len(predictions)):\n        print(predictions[x], x_test1[x], y_test1[x])\n        diff = sqrt((predictions[x] - y_test1[x])**2)\n        print(\"\\tDifference is \", diff)\n        comp.append(diff)\n    print(\"\\n\\n\\nBREAK\")\n    print(comp)\n    print(\"\\nSum of errors is \", sum(comp))\n    \n    # Predict y values of best model (with optimal coefficients from above) using same x_test1 values as 1st model\n    # then compute difference between predictions and actual values\n    print(\"\\n\\n\\nBREAK\")\n    comp2 = []\n    predictions_new_model = new_model.predict(x_test1)\n    for x in range(len(predictions_new_model)):\n        print(predictions_new_model[x], x_test1[x], y_test1[x])\n        diff2 = sqrt((predictions_new_model[x] - y_test1[x])**2)\n        print(\"\\tDifference is \", diff2)\n        comp2.append(diff2)\n    \n    print(\"\\n\\n\\nBREAK\")\n    print(comp2)\n    print(\"\\nSum of errors is \", sum(comp2))\n    \n    print(\"\\n\\n\\nFirst model fit difference: \", sum(comp))\n    print(\"\\nSecond model fit difference \", sum(comp2))\n    \n    print('\\n1st model score: ',linear.score(x_train1, y_train1))\n    \n    print('\\nBest model score: ',new_model.score(x_train1, y_train1))", "link": "https://www.reddit.com/r/learnmachinelearning/comments/incvbo/help_understanding_python_linear_regression_model/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "help understanding python linear regression model code issue /!/ hello,\n\ni'm working on my first linear regression code using a tech with tim video ([https://www.youtube.com/watch?v=45rydiphdgg](https://www.youtube.com/watch?v=45rydiphdgg)) and have run into a snag.  i'm using the uci student data from here:  [https://archive.ics.uci.edu/ml/datasets/student+performance](https://archive.ics.uci.edu/ml/datasets/student+performance)\n\nmy initial model code ran fine.  then i iterated to find an optimal accuracy model, which was fine.  where it starts to go off the rails is that i tried to then inject those optimal coefficients into a new model, then run two predictions: \n\n1) the very first model (pre-optimization loop)\n\n2) the optimized model\n\nagainst the same x\\_test1 data set.  to compare the two, i simply summed the squared difference between predicted and actual y values.  then i also recorded the final accuracy of both models.\n\ni've done something wrong because the accuracy of my new \"optimized\" model is the same or lower as the very first model, and the difference values is very similar as well.  i expected the optimized model to have much less error and a higher accuracy.\n\ncan someone help me to see the error?   i suspect the error lies after the -----> plot !!!  section of code.  thanks in advance, code below.\n\n    # import libraries\n    import pandas as pd\n    import numpy as np\n    import sklearn\n    import pickle\n    import matplotlib.pyplot as plt\n    from sklearn import linear_model\n    from math import sqrt\n    from sklearn.linear_model import linearregression\n    from matplotlib import style\n    \n    # from sklearn.utils import shuffle\n    \n    # read in data\n    data = pd.read_csv(\"student-mat.csv\", sep=\";\")\n    \n    # slice data to include only desired headings\n    data = data[[\"g1\", \"g2\", \"g3\", \"studytime\", \"failures\", \"absences\"]]\n    \n    # define the attribute we are trying to predict; called \"label\".\n    # others are \"features\" and used to predict label\n    predict = \"g3\"\n    \n    # create array of features and label\n    x = np.array(data.drop([predict], 1))\n    y = np.array(data[predict])\n    \n    # split data into training and testing data.  90% used for training, 10% testing\n    # test size 0.1 = 10% of array size\n    x_train1, x_test1, y_train1, y_test1 = sklearn.model_selection.train_test_split(x, y, test_size=0.1)\n    \n    # create 1st linear model and fit\n    linear = linear_model.linearregression()\n    linear.fit(x_train1, y_train1)\n    \n    # compute accuracy of model\n    acc = linear.score(x_test1, y_test1)\n    \n    # iterate for a given number of times (max_iter) to find an optimal accuracy value and record best coefficients\n    loop_num = 1\n    max_iter = 1000\n    best_acc = acc\n    best_coef = linear.coef_\n    best_int = linear.intercept_\n    acc_counter = [acc]\n    \n    print(\"\\ninitial accuracy: %4.3f\" % acc)\n    \n    while loop_num &lt; max_iter + 1:\n        x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x, y, test_size=0.1)\n        linear2 = linear_model.linearregression()\n        linear2.fit(x_train, y_train)\n        acc = linear2.score(x_test, y_test)\n        acc_counter.append(acc)\n        print(\"\\naccuracy of run \" + str(loop_num) + \" is: %4.3f\" % acc)\n        if acc &gt; best_acc:\n            print(\"\\n\\tbetter accuracy found.\")\n            best_acc = acc\n            best_coef = linear2.coef_\n            best_int = linear2.intercept_\n            print(\"co: \\n\", linear2.coef_)\n            print(\"intercept: \\n\", linear2.intercept_)\n        else:\n            print(\"\\n\\tfit discarded.\")\n        loop_num += 1\n    \n    print(\"\\nbest acccuracy: \\n%4.3f\" % best_acc)\n    print(\"\\nbest coefficients: \\n\", best_coef)\n    print(\"\\nbest intercept: \\n\", best_int)\n    \n    # plot accuracy over time\n    x_scale = []\n    for x in range(max_iter + 1):\n        x_scale.append(x)\n    \n    plt.plot(x_scale, acc_counter, color='green', linestyle='dashed', linewidth=3, marker='o',\n             markerfacecolor='blue', markersize=5)\n    \n    ymax = max(acc_counter)\n    ymin = min(acc_counter)\n    xpos = acc_counter.index(ymax)\n    xmax = x_scale[xpos]\n    annot_max_acc = str(ymax)\n    plt.annotate('max accuracy = ' + annot_max_acc[0:4], xy=(xmax, ymax), xycoords='data', xytext=(.8, .95),\n                 textcoords='axes fraction',\n                 arrowprops=dict(facecolor='black', shrink=0.05), horizontalalignment='right', verticalalignment='top')\n    plt.ylim(ymin, 1.0)\n    plt.xlabel('run number')\n    plt.ylabel('accuracy')\n    plt.title('prediction accuracy over time')\n    plt.show()\n    \n    # create model with best coefficients from above\n    new_model = linear_model.linearregression()\n    new_model.intercept_ = best_int\n    new_model.coef_ = best_coef\n    \n    # predict y values for 1st model (not best) then compute difference between predictions and actual values\n    print(\"\\n\\n\\nbreak\")\n    comp = []\n    predictions = linear.predict(x_test1)\n    for x in range(len(predictions)):\n        print(predictions[x], x_test1[x], y_test1[x])\n        diff = sqrt((predictions[x] - y_test1[x])**2)\n        print(\"\\tdifference is \", diff)\n        comp.append(diff)\n    print(\"\\n\\n\\nbreak\")\n    print(comp)\n    print(\"\\nsum of errors is \", sum(comp))\n    \n    # predict y values of best model (with optimal coefficients from above) using same x_test1 values as 1st model\n    # then compute difference between predictions and actual values\n    print(\"\\n\\n\\nbreak\")\n    comp2 = []\n    predictions_new_model = new_model.predict(x_test1)\n    for x in range(len(predictions_new_model)):\n        print(predictions_new_model[x], x_test1[x], y_test1[x])\n        diff2 = sqrt((predictions_new_model[x] - y_test1[x])**2)\n        print(\"\\tdifference is \", diff2)\n        comp2.append(diff2)\n    \n    print(\"\\n\\n\\nbreak\")\n    print(comp2)\n    print(\"\\nsum of errors is \", sum(comp2))\n    \n    print(\"\\n\\n\\nfirst model fit difference: \", sum(comp))\n    print(\"\\nsecond model fit difference \", sum(comp2))\n    \n    print('\\n1st model score: ',linear.score(x_train1, y_train1))\n    \n    print('\\nbest model score: ',new_model.score(x_train1, y_train1))", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/incvbo/help_understanding_python_linear_regression_model/',)", "identifyer": 5743281, "year": "2020"}, {"autor": "Hollowax", "date": 1601291517000, "content": "Discovered(?) weird way to create visual clusters from a trained classification model /!/ After I trained an LSTM for a classification task, I wanted to create clusters to create a nice visualization. So I fed all train samples agaiin through the already trained model and saved all the outputs of the layer before the last softmax layer. Then I used (supervised) contrastive learning and created a simple fc model that outputs 3 coordinates so I could plot the names in 3d space. And it seemed to work.\n\nThen accidentally, I fed the outputs-embeddings into a not trained (randomly initialized) model but it created nice clusters anyways. Then I tried to just create clusters from a single, untrained linear layer and also nice clusters appeared.\n\nMy conclusion: I can take the layer (shape; N, 1) before the last (and even that before), multiply it with a random matrix (3, N) so that a 3 component vector comes out and plot it for every element in my train and test set and I get nice looking clusters. Is this trivial? A known method?\n\nTL;DR\n\nWhen I take, for every element of my dataset, the layer before the last one of my classification model and multiply it with a random matrix so that it produces a 3 component vector and plot it in 3d space, color it (for", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j1a28m/discovered_weird_way_to_create_visual_clusters/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "discovered(?) weird way to create visual clusters from a trained classification model /!/ after i trained an lstm for a classification task, i wanted to create clusters to create a nice visualization. so i fed all train samples agaiin through the already trained model and saved all the outputs of the layer before the last softmax layer. then i used (supervised) contrastive learning and created a simple fc model that outputs 3 coordinates so i could -----> plot !!!  the names in 3d space. and it seemed to work.\n\nthen accidentally, i fed the outputs-embeddings into a not trained (randomly initialized) model but it created nice clusters anyways. then i tried to just create clusters from a single, untrained linear layer and also nice clusters appeared.\n\nmy conclusion: i can take the layer (shape; n, 1) before the last (and even that before), multiply it with a random matrix (3, n) so that a 3 component vector comes out and plot it for every element in my train and test set and i get nice looking clusters. is this trivial? a known method?\n\ntl;dr\n\nwhen i take, for every element of my dataset, the layer before the last one of my classification model and multiply it with a random matrix so that it produces a 3 component vector and plot it in 3d space, color it (for", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/j1a28m/discovered_weird_way_to_create_visual_clusters/',)", "identifyer": 5743289, "year": "2020"}, {"autor": "adityaa260", "date": 1596396053000, "content": "Matplotlib tutorial: matplotlib line plot part-1| How to create a line c...", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i2hwrj/matplotlib_tutorial_matplotlib_line_plot_part1/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "matplotlib tutorial: matplotlib line -----> plot !!!  part-1| how to create a line c...", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('rich:video',)", "medialink": "('https://www.youtube.com/watch?v=0howBF8eic8&amp;feature=share',)", "identifyer": 5743423, "year": "2020"}, {"autor": "DejavuJS", "date": 1596380105000, "content": "High school student looking for ways to get better time series forecast result /!/ Hello everyone, I am a high school student interested in both investing and machine learning. Here are my two questions.\n\n1. After following this great Sir's blog post, I did not know how to come to the last plot. [http://rwanjohi.rbind.io/2018/04/05/time-series-forecasting-using-lstm-in-r/](http://rwanjohi.rbind.io/2018/04/05/time-series-forecasting-using-lstm-in-r/)\n\n2. The data I am using is S&amp;P500 daily close prices from 2005 to 2015. I previously applied auto ARIMA and ETS to it and got the result that ETS has a better fit, predicting the S&amp;P will experience an uptick in the first half-year of 2015\u2014a correct trend. However, I am intrigued by the big confidence interval. This is contrary to what I expected, for this is a considerable interval with a linear line attached to the input data, as opposed to the more accurate result I previously had in mind. Are there better models that have detailed tutorials? I know that it is possible to use Independent Component Analysis and Relative Hamming Distance to determine noise components and then apply models, but I know nothing about the necessary steps to achieve this end. Please share your ideas. Thank you!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i2dc3j/high_school_student_looking_for_ways_to_get/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "high school student looking for ways to get better time series forecast result /!/ hello everyone, i am a high school student interested in both investing and machine learning. here are my two questions.\n\n1. after following this great sir's blog post, i did not know how to come to the last -----> plot !!! . [http://rwanjohi.rbind.io/2018/04/05/time-series-forecasting-using-lstm-in-r/](http://rwanjohi.rbind.io/2018/04/05/time-series-forecasting-using-lstm-in-r/)\n\n2. the data i am using is s&amp;p500 daily close prices from 2005 to 2015. i previously applied auto arima and ets to it and got the result that ets has a better fit, predicting the s&amp;p will experience an uptick in the first half-year of 2015\u2014a correct trend. however, i am intrigued by the big confidence interval. this is contrary to what i expected, for this is a considerable interval with a linear line attached to the input data, as opposed to the more accurate result i previously had in mind. are there better models that have detailed tutorials? i know that it is possible to use independent component analysis and relative hamming distance to determine noise components and then apply models, but i know nothing about the necessary steps to achieve this end. please share your ideas. thank you!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/i2dc3j/high_school_student_looking_for_ways_to_get/',)", "identifyer": 5743429, "year": "2020"}, {"autor": "HorseJungler", "date": 1580172886000, "content": "Building a linear regression model in PyTorch /!/ I have a machine learning class that uses Python and I have almost no Python experience, so yes I probably shouldn't be in this class but it was an elective for my Math degree and I desperately wanted some useful classes other than math theory. So I am reading Python Crash course trying to catch up, but I need help with this problem if anyone can assist.\n\nI need to build a linear regression model with these details:\n\n1. training data has 20 pairs of x,y values, representing noisy measurements on a SINE function. Create a plot of the data \n2. model equations used will be yi = a0 + (a1)(xi) + (a2)(xi\\^2) + (a3)(xi\\^3)\n3. The model constructed by gradient search along the lines with numpy\n4. Train the model and plot predicted y values against x values\n5. Play with choice of learning rate and change degree of polynomial to 2 and 4 (it is currently degree 3)\n\nSo I managed to get code to graph the sine function with noise here:\n\n    import numpy as np\n    import torch\n    import matplotlib.pyplot as plt\n    n_points = 20\n    theta = np.linspace(0,1,n_points) # Generates equally spaced points in specified interval\n    x = 1.41*np.sin(theta*6.28)+0.1*np.random.randn(n_points) # 20 noisy valuues from sin function\n    plt.plot(theta, x)\n    plt.ylim(-2, 2)\n    plt.show()\n\nBut now idk where to go from here. We are using PyTorch. Can anyone push me in the right direction?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/euxu7l/building_a_linear_regression_model_in_pytorch/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "building a linear regression model in pytorch /!/ i have a machine learning class that uses python and i have almost no python experience, so yes i probably shouldn't be in this class but it was an elective for my math degree and i desperately wanted some useful classes other than math theory. so i am reading python crash course trying to catch up, but i need help with this problem if anyone can assist.\n\ni need to build a linear regression model with these details:\n\n1. training data has 20 pairs of x,y values, representing noisy measurements on a sine function. create a -----> plot !!!  of the data \n2. model equations used will be yi = a0 + (a1)(xi) + (a2)(xi\\^2) + (a3)(xi\\^3)\n3. the model constructed by gradient search along the lines with numpy\n4. train the model and plot predicted y values against x values\n5. play with choice of learning rate and change degree of polynomial to 2 and 4 (it is currently degree 3)\n\nso i managed to get code to graph the sine function with noise here:\n\n    import numpy as np\n    import torch\n    import matplotlib.pyplot as plt\n    n_points = 20\n    theta = np.linspace(0,1,n_points) # generates equally spaced points in specified interval\n    x = 1.41*np.sin(theta*6.28)+0.1*np.random.randn(n_points) # 20 noisy valuues from sin function\n    plt.plot(theta, x)\n    plt.ylim(-2, 2)\n    plt.show()\n\nbut now idk where to go from here. we are using pytorch. can anyone push me in the right direction?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 8, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/euxu7l/building_a_linear_regression_model_in_pytorch/',)", "identifyer": 5743773, "year": "2020"}, {"autor": "scrippington", "date": 1586808928000, "content": "Roadmap for an ML project? Predictive video. /!/ Hi all, I'm pretty new to ML and I'm planning out my first major project--a sort of predictive video synthesizer. I've got some coding experience, mostly for pushing points around in 3D applications and simulation stuff,  but this will be my first real foray into ML aside from your run-of-the-mill MNIST-style exercises. I'm trying to put together a rough list of concepts I'll need to understand to make the algorithm train.\n\nIn the simplest terms possible, I'm trying to create a model that will, given a set number of frames from a video, predict the next n frames. While I wouldn't expect it to preserve any real notion of plot, I'd like to try and at least present the viewer with a sequence of recognizable forms that evolve at approximately movie-pace. \n\nAny suggestions for concepts to look into for this? So far I'm looking at CNN + LSTM, but that's really all I have to go off of at the moment.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g0qpgi/roadmap_for_an_ml_project_predictive_video/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "roadmap for an ml project? predictive video. /!/ hi all, i'm pretty new to ml and i'm planning out my first major project--a sort of predictive video synthesizer. i've got some coding experience, mostly for pushing points around in 3d applications and simulation stuff,  but this will be my first real foray into ml aside from your run-of-the-mill mnist-style exercises. i'm trying to put together a rough list of concepts i'll need to understand to make the algorithm train.\n\nin the simplest terms possible, i'm trying to create a model that will, given a set number of frames from a video, predict the next n frames. while i wouldn't expect it to preserve any real notion of -----> plot !!! , i'd like to try and at least present the viewer with a sequence of recognizable forms that evolve at approximately movie-pace. \n\nany suggestions for concepts to look into for this? so far i'm looking at cnn + lstm, but that's really all i have to go off of at the moment.", "sortedWord": "None", "removed": "('nan',)", "score": 2, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/g0qpgi/roadmap_for_an_ml_project_predictive_video/',)", "identifyer": 5744010, "year": "2020"}, {"autor": "martian_rover", "date": 1605902414000, "content": "[Question] cross_val_score is returning nan list of scores in scikit learn /!/ I am trying to handle imbalanced `multi label dataset` using `cross validation` but scikit learn `cross_val_score` is returning `nan list of values` on running classifier.\n\nHere is the code:\n\n    import pandas as pd\n    import numpy as np\n    data = pd.DataFrame.from_dict(dict, orient = 'index') # save the given data below in dict variable to run this line\n    \n    from sklearn.model_selection import train_test_split\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    from sklearn.preprocessing import MultiLabelBinarizer\n    from sklearn.multiclass import OneVsRestClassifier\n    \n    multilabel = MultiLabelBinarizer()\n    y = multilabel.fit_transform(data['Tags']) \n    from nltk.corpus import stopwords \n    stop_words = set(stopwords.words('english')) \n    tfidf = TfidfVectorizer(stop_words = stop_words,max_features= 40000, ngram_range = (1,3))\n    X = tfidf.fit_transform(data['cleaned_title'])\n    \n    from skmultilearn.model_selection import IterativeStratification\n    k_fold = IterativeStratification(n_splits=10, order=1)\n    \n    \n    from sklearn.linear_model import LogisticRegression\n    from sklearn.metrics import jaccard_score\n    class_weight = {0:1,1:10}\n    lr = LogisticRegression(class_weight = class_weight, n_jobs = -1)\n    scores = cross_val_score(lr, X, y, cv=k_fold, scoring = 'f1_micro')\n    scores\n\nHere is the data (first 10 rows) using `data.head(10).to_dict()`\n\n    {0: {'Tags': ['python', 'list', 'loops', 'for-loop', 'indexing'],\n      'cleaned_title': 'for loop we use any local variable   what if we use any number present in a list  ',\n      'cleaned_text_of_ques': 'in the for   loop we use any local variable   what if we use any number in a list   what will be the output   a   [ 1 2 3 4 5 6 ] b   [ ] for a[ 1 ] in a   b append a[ 1 ]   print b  '},\n     1: {'Tags': ['python', 'loops', 'tkinter', 'algorithm-animation'],\n      'cleaned_title': 'contain a mainloop [ duplicate ]',\n      'cleaned_text_of_ques': 'my code be a bubble sort that i be try to visualise   but i be struggle to find a way to make a block of code only be use once   i also think that if i could only mainloop a section that would'},\n     2: {'Tags': ['android',\n       'android-lifecycle',\n       'activity-lifecycle',\n       'onsaveinstancestate'],\n      'cleaned_title': 'when onrestoreinstancestate be not call  ',\n      'cleaned_text_of_ques': 'docs describe when onrestoreinstancestate be call   this method be call after onstart     when the activity be be re   initialize from a previously save state   give here in savedinstancestate  '},\n     3: {'Tags': ['python', 'r', 'bash', 'conda', 'spyder'],\n      'cleaned_title': 'point conda r to already   instal version of r',\n      'cleaned_text_of_ques': 'my problem have to do with the fact that rstudio and conda be point to different version of r  my r and rstudio be instal independent of anaconda   and everything be work great   in my    '},\n     4: {'Tags': ['android',\n       'firebase',\n       'firebase-realtime-database',\n       'android-recyclerview'],\n      'cleaned_title': 'how to use a recycleview with several different layout   accord to the datum collect in firebase   [ close ]',\n      'cleaned_text_of_ques': 'i have a problem   there be day that i do research and test code   but nothing work   my application will have a window where i will post datum take in firebase   use a recycleview   with the'},\n     5: {'Tags': ['html', 'css', 'layout'],\n      'cleaned_title': 'how to create side by side layout of an image and label  ',\n      'cleaned_text_of_ques': 'i have be try for a while now and can not seem to achive the bellow design    exploreitem   background   color     353258       rgba 31   31   31   1       border   1px solid   4152f1   color  '},\n     6: {'Tags': ['php', 'jquery', 'file'],\n      'cleaned_title': 'php jquery ajax   _ files[ file   ] undefined index error',\n      'cleaned_text_of_ques': 'i have a form that upload image file and it be not work   i have try submit and click event   the error appear when i have remove the if statement   thank in advance for your help  '},\n     7: {'Tags': ['python', 'pandas', 'dataframe'],\n      'cleaned_title': 'how to update value in pandas dataframe in a for loop  ',\n      'cleaned_text_of_ques': 'i be try to make a data frame that can store variable coeff value after each iteration   i be able to plot the graph after each iteration   but when i try to insert the value in the data frame'},\n     8: {'Tags': ['xpath', 'web-scraping', 'scrapy'],\n      'cleaned_title': 'scrapy   how can i handle a random number of element  ',\n      'cleaned_text_of_ques': 'i have a scrapy crawler that i can comfortably acquire the first desire paragraph   but sometimes there be a second or third paragraph   response xpath f string   h2[contains text           card   ] '},\n     9: {'Tags': ['bootstrap-4', 'tabs', 'collapse'],\n      'cleaned_title': 'collapse three column with bootstrap',\n      'cleaned_text_of_ques': 'i be try to make three tab with cross   reference with one tab visible at the time   i be use the bootstrap v4 collapse scheme with functionality support by jquery   here be the example   https  '}}\n\nThis is how I get the `cross_val_score` in `scores` variable \n\n`array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])`.\n\nIt should have each value in range `0-1`. However, this is happening with `every algorithm model`.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jxw9y1/question_cross_val_score_is_returning_nan_list_of/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "[question] cross_val_score is returning nan list of scores in scikit learn /!/ i am trying to handle imbalanced `multi label dataset` using `cross validation` but scikit learn `cross_val_score` is returning `nan list of values` on running classifier.\n\nhere is the code:\n\n    import pandas as pd\n    import numpy as np\n    data = pd.dataframe.from_dict(dict, orient = 'index') # save the given data below in dict variable to run this line\n    \n    from sklearn.model_selection import train_test_split\n    from sklearn.feature_extraction.text import tfidfvectorizer\n    from sklearn.preprocessing import multilabelbinarizer\n    from sklearn.multiclass import onevsrestclassifier\n    \n    multilabel = multilabelbinarizer()\n    y = multilabel.fit_transform(data['tags']) \n    from nltk.corpus import stopwords \n    stop_words = set(stopwords.words('english')) \n    tfidf = tfidfvectorizer(stop_words = stop_words,max_features= 40000, ngram_range = (1,3))\n    x = tfidf.fit_transform(data['cleaned_title'])\n    \n    from skmultilearn.model_selection import iterativestratification\n    k_fold = iterativestratification(n_splits=10, order=1)\n    \n    \n    from sklearn.linear_model import logisticregression\n    from sklearn.metrics import jaccard_score\n    class_weight = {0:1,1:10}\n    lr = logisticregression(class_weight = class_weight, n_jobs = -1)\n    scores = cross_val_score(lr, x, y, cv=k_fold, scoring = 'f1_micro')\n    scores\n\nhere is the data (first 10 rows) using `data.head(10).to_dict()`\n\n    {0: {'tags': ['python', 'list', 'loops', 'for-loop', 'indexing'],\n      'cleaned_title': 'for loop we use any local variable   what if we use any number present in a list  ',\n      'cleaned_text_of_ques': 'in the for   loop we use any local variable   what if we use any number in a list   what will be the output   a   [ 1 2 3 4 5 6 ] b   [ ] for a[ 1 ] in a   b append a[ 1 ]   print b  '},\n     1: {'tags': ['python', 'loops', 'tkinter', 'algorithm-animation'],\n      'cleaned_title': 'contain a mainloop [ duplicate ]',\n      'cleaned_text_of_ques': 'my code be a bubble sort that i be try to visualise   but i be struggle to find a way to make a block of code only be use once   i also think that if i could only mainloop a section that would'},\n     2: {'tags': ['android',\n       'android-lifecycle',\n       'activity-lifecycle',\n       'onsaveinstancestate'],\n      'cleaned_title': 'when onrestoreinstancestate be not call  ',\n      'cleaned_text_of_ques': 'docs describe when onrestoreinstancestate be call   this method be call after onstart     when the activity be be re   initialize from a previously save state   give here in savedinstancestate  '},\n     3: {'tags': ['python', 'r', 'bash', 'conda', 'spyder'],\n      'cleaned_title': 'point conda r to already   instal version of r',\n      'cleaned_text_of_ques': 'my problem have to do with the fact that rstudio and conda be point to different version of r  my r and rstudio be instal independent of anaconda   and everything be work great   in my    '},\n     4: {'tags': ['android',\n       'firebase',\n       'firebase-realtime-database',\n       'android-recyclerview'],\n      'cleaned_title': 'how to use a recycleview with several different layout   accord to the datum collect in firebase   [ close ]',\n      'cleaned_text_of_ques': 'i have a problem   there be day that i do research and test code   but nothing work   my application will have a window where i will post datum take in firebase   use a recycleview   with the'},\n     5: {'tags': ['html', 'css', 'layout'],\n      'cleaned_title': 'how to create side by side layout of an image and label  ',\n      'cleaned_text_of_ques': 'i have be try for a while now and can not seem to achive the bellow design    exploreitem   background   color     353258       rgba 31   31   31   1       border   1px solid   4152f1   color  '},\n     6: {'tags': ['php', 'jquery', 'file'],\n      'cleaned_title': 'php jquery ajax   _ files[ file   ] undefined index error',\n      'cleaned_text_of_ques': 'i have a form that upload image file and it be not work   i have try submit and click event   the error appear when i have remove the if statement   thank in advance for your help  '},\n     7: {'tags': ['python', 'pandas', 'dataframe'],\n      'cleaned_title': 'how to update value in pandas dataframe in a for loop  ',\n      'cleaned_text_of_ques': 'i be try to make a data frame that can store variable coeff value after each iteration   i be able to -----> plot !!!  the graph after each iteration   but when i try to insert the value in the data frame'},\n     8: {'tags': ['xpath', 'web-scraping', 'scrapy'],\n      'cleaned_title': 'scrapy   how can i handle a random number of element  ',\n      'cleaned_text_of_ques': 'i have a scrapy crawler that i can comfortably acquire the first desire paragraph   but sometimes there be a second or third paragraph   response xpath f string   h2[contains text           card   ] '},\n     9: {'tags': ['bootstrap-4', 'tabs', 'collapse'],\n      'cleaned_title': 'collapse three column with bootstrap',\n      'cleaned_text_of_ques': 'i be try to make three tab with cross   reference with one tab visible at the time   i be use the bootstrap v4 collapse scheme with functionality support by jquery   here be the example   https  '}}\n\nthis is how i get the `cross_val_score` in `scores` variable \n\n`array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])`.\n\nit should have each value in range `0-1`. however, this is happening with `every algorithm model`.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/jxw9y1/question_cross_val_score_is_returning_nan_list_of/',)", "identifyer": 5744091, "year": "2020"}, {"autor": "hiphop1987", "date": 1605879983000, "content": "Pandas Web API /!/ I've made an [open-source project](https://github.com/romanorac/pandas-analytics-server) that simplifies connecting pandas to a real-time data feed, testing hypotheses and visualizing results in a web browser.\n\nThe server is also live and you can try it [here](http://ec2-54-154-144-55.eu-west-1.compute.amazonaws.com:5000/plot).\n\nIt responds to 2 endpoints:\n\n* [a simple status update](http://ec2-54-154-144-55.eu-west-1.compute.amazonaws.com:5000/) so that we know the server is running,\n* [a page with plots to visualize the data](http://ec2-54-154-144-55.eu-west-1.compute.amazonaws.com:5000/plot). It accepts the *n\\_entries* query parameter to limit the number of data points to visualize (e.g. *?n\\_entries=10)*.\n\nhttps://preview.redd.it/rso1jfscfe061.png?width=1200&amp;format=png&amp;auto=webp&amp;s=8a5f8d3a3de61105163a559a89144d5c3ced1887\n\n# Use case\n\nYou have found an interesting pattern in a certain dataset (eg. stocks, cryptocurrencies, weather data). You have written a quick-and-dirty code in a Jupyter Notebook to analyze and plot the data. The results of the analysis seem promising and you would like to test your hypothesis on live data.\n\nTo test the hypothesis on live data, you need to port the Jupyter Notebook code to the server-side. This is a cumbersome process if you are not well versed in backend development. So much time went into tweaking matplotlib parameters, now you need to reimplement them in JavaScript. If you are like me and don\u2019t know JavaScript, learning a new plotting framework will take some time.\n\n# Want to learn more?\n\nSee [Pandas Web\u00a0API](https://towardsdatascience.com/pandas-analytics-server-d64d20ef01be?sk=0292c30f7a54f42c037b0da6af9782e4) to learn more.\n\nThanks for reading. Feedback is welcome.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jxp9ug/pandas_web_api/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "pandas web api /!/ i've made an [open-source project](https://github.com/romanorac/pandas-analytics-server) that simplifies connecting pandas to a real-time data feed, testing hypotheses and visualizing results in a web browser.\n\nthe server is also live and you can try it [here](http://ec2-54-154-144-55.eu-west-1.compute.amazonaws.com:5000/-----> plot !!! ).\n\nit responds to 2 endpoints:\n\n* [a simple status update](http://ec2-54-154-144-55.eu-west-1.compute.amazonaws.com:5000/) so that we know the server is running,\n* [a page with plots to visualize the data](http://ec2-54-154-144-55.eu-west-1.compute.amazonaws.com:5000/plot). it accepts the *n\\_entries* query parameter to limit the number of data points to visualize (e.g. *?n\\_entries=10)*.\n\nhttps://preview.redd.it/rso1jfscfe061.png?width=1200&amp;format=png&amp;auto=webp&amp;s=8a5f8d3a3de61105163a559a89144d5c3ced1887\n\n# use case\n\nyou have found an interesting pattern in a certain dataset (eg. stocks, cryptocurrencies, weather data). you have written a quick-and-dirty code in a jupyter notebook to analyze and plot the data. the results of the analysis seem promising and you would like to test your hypothesis on live data.\n\nto test the hypothesis on live data, you need to port the jupyter notebook code to the server-side. this is a cumbersome process if you are not well versed in backend development. so much time went into tweaking matplotlib parameters, now you need to reimplement them in javascript. if you are like me and don\u2019t know javascript, learning a new plotting framework will take some time.\n\n# want to learn more?\n\nsee [pandas web\u00a0api](https://towardsdatascience.com/pandas-analytics-server-d64d20ef01be?sk=0292c30f7a54f42c037b0da6af9782e4) to learn more.\n\nthanks for reading. feedback is welcome.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/jxp9ug/pandas_web_api/',)", "identifyer": 5744101, "year": "2020"}, {"autor": "Stack3", "date": 1578717955000, "content": "Where to get simple ML datasets for learning and experimentation? /!/ I want a dataset where each observation is made up of 1 or 2 numbers and is labeled as 1 of 2 possible labels.\n\nThis will let me plot the observations on 2D so I can get a feel for how all the ml algorithms work to define boundaries.\n\nPerhaps there's a place that has lots of datasets, I'm sure I can find what I'm looking for if you can point me in the right direction.\n\nThanks for your help!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/en2ygx/where_to_get_simple_ml_datasets_for_learning_and/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "where to get simple ml datasets for learning and experimentation? /!/ i want a dataset where each observation is made up of 1 or 2 numbers and is labeled as 1 of 2 possible labels.\n\nthis will let me -----> plot !!!  the observations on 2d so i can get a feel for how all the ml algorithms work to define boundaries.\n\nperhaps there's a place that has lots of datasets, i'm sure i can find what i'm looking for if you can point me in the right direction.\n\nthanks for your help!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/en2ygx/where_to_get_simple_ml_datasets_for_learning_and/',)", "identifyer": 5744241, "year": "2020"}, {"autor": "gonzaloGarde", "date": 1578650878000, "content": "[Help] - Regression to output coordinates /!/ Good moorning,\n\nI am working in a project where I am building a regressor that, given an image and two additional numerical inputs, tries to infer a coordinate so the output of the regressor consist on two values, X and Y. The X and Y values are in the range \\[-1,1\\]. However, when I plot the regressor predicted values vs the ground truth, I obtain the following results:\n\n![img](crldqxvc8x941 \"Fig.1 Output of the regressor. The ground truth is marked as an \\\"x\\\". The infered values as a circle.\")\n\nFrom what I see, it seems like the regressor understands the distribution (the color order in colums and rows is correct and matches the order of the ground truth) but the range of the output is between \\[-0.4, 0.3\\] for both axis. I dont scale the predicted values neither apply any other kind of postprocessing to the predicted values, and I plot them directly against the ground truth value. The model that I am using for the regressor is this:\n\n*Resnet18,\u00a0\\_\u00a0=\u00a0Classifiers.get('resnet18')*  \n*input\\_image\u00a0=\u00a0keras.layers.Input(shape=(None,None,3))*\n\n*base\\_model\u00a0=\u00a0Resnet18(input\\_tensor=input\\_image,\u00a0weights='imagenet',\u00a0include\\_top=False)*  \n*x\u00a0=\u00a0base\\_model.output*  \n*x\u00a0=\u00a0keras.layers.GlobalAveragePooling2D()(x)*  \n*size =\u00a0keras.layers.Input(shape=(1))*  \n*distance =\u00a0keras.layers.Input(shape=(1))*  \n*concat\u00a0=\u00a0keras.layers.Concatenate(name='Concatenate\\_size\\_distance\\_and\\_img')(\\[x,\u00a0size,\u00a0distance\\])*  \n*x\u00a0=\u00a0keras.layers.Dense(64,\u00a0activation='relu',\u00a0name='Middle\\_Dense\\_1')(concat)*  \n*x\u00a0=\u00a0keras.layers.Dense(64,\u00a0activation='relu',\u00a0name='Middle\\_Dense\\_2')(x)*  \n*x\u00a0=\u00a0keras.layers.Dense(32,\u00a0activation='relu',\u00a0name='Middle\\_Dense\\_3')(x)*  \n*x\\_val\u00a0=\u00a0keras.layers.Dense(32,\u00a0activation='relu',\u00a0name='Dense\\_x\\_1')(x)*  \n*x\\_val\u00a0=\u00a0keras.layers.Dense(16,\u00a0activation='relu',\u00a0name='Dense\\_x\\_2')(x\\_val)*  \n*x\\_val\u00a0=\u00a0keras.layers.Dense(16,\u00a0activation='relu',\u00a0name='Dense\\_x\\_3')(x\\_val)*  \n*regression\\_x\u00a0=\u00a0keras.layers.Dense(1,\u00a0activation='linear',\u00a0name='regression\\_layer\\_x')(x\\_val)*  \n   \n*y\\_val\u00a0=\u00a0keras.layers.Dense(32,\u00a0activation='relu',\u00a0name='Dense\\_y\\_1')(x)*  \n*y\\_val\u00a0=\u00a0keras.layers.Dense(16,\u00a0activation='relu',\u00a0name='Dense\\_y\\_2')(y\\_val)*  \n*y\\_val\u00a0=\u00a0keras.layers.Dense(16,\u00a0activation='relu',\u00a0name='Dense\\_y\\_3')(y\\_val)*  \n*regression\\_y\u00a0=\u00a0keras.layers.Dense(1,\u00a0activation='linear',\u00a0name='regression\\_layer\\_y')(y\\_val)*  \n*regression\u00a0=\u00a0keras.layers.Concatenate(name='Concatenate\\_Regression')(\\[regression\\_x,\u00a0regression\\_y\\])*  \n*model\u00a0=\u00a0keras.models.Model(inputs=\\[base\\_model.input,\u00a0size,\u00a0distance\\],\u00a0outputs=\\[regression\\_x,\u00a0regression\\_y,\u00a0regression\\])*\n\nThe idea of the model is that I have a box that is in at an specific distance from camera (known and used as numerical input) and with some size (known and also used as input) and I want to infer the coordinates of a pattern that occurs in the box relative to the borders of the object. I can not add a real image as I am not propietary of the dataset, but it would be similar to the following sketch:\n\n&amp;#x200B;\n\n![img](k2o13asucx941 \"Fig 2. Example of the scenario. Three boxes, 2 of them with the same size but at different distances. One of the with twice the size but at half of the distance. The regressor outcome for all of them should be equal to (0,0) as the cross is in the middle (origin of coordinates) of the box.\")\n\nI have a hard time trying to understand what it is limiting the output range of the model to (-0.4, 0.3) while the distribution of the points seems correct and the final activation is linear.\n\nThank you for your time, have a good day!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/emozd1/help_regression_to_output_coordinates/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "[help] - regression to output coordinates /!/ good moorning,\n\ni am working in a project where i am building a regressor that, given an image and two additional numerical inputs, tries to infer a coordinate so the output of the regressor consist on two values, x and y. the x and y values are in the range \\[-1,1\\]. however, when i -----> plot !!!  the regressor predicted values vs the ground truth, i obtain the following results:\n\n![img](crldqxvc8x941 \"fig.1 output of the regressor. the ground truth is marked as an \\\"x\\\". the infered values as a circle.\")\n\nfrom what i see, it seems like the regressor understands the distribution (the color order in colums and rows is correct and matches the order of the ground truth) but the range of the output is between \\[-0.4, 0.3\\] for both axis. i dont scale the predicted values neither apply any other kind of postprocessing to the predicted values, and i plot them directly against the ground truth value. the model that i am using for the regressor is this:\n\n*resnet18,\u00a0\\_\u00a0=\u00a0classifiers.get('resnet18')*  \n*input\\_image\u00a0=\u00a0keras.layers.input(shape=(none,none,3))*\n\n*base\\_model\u00a0=\u00a0resnet18(input\\_tensor=input\\_image,\u00a0weights='imagenet',\u00a0include\\_top=false)*  \n*x\u00a0=\u00a0base\\_model.output*  \n*x\u00a0=\u00a0keras.layers.globalaveragepooling2d()(x)*  \n*size =\u00a0keras.layers.input(shape=(1))*  \n*distance =\u00a0keras.layers.input(shape=(1))*  \n*concat\u00a0=\u00a0keras.layers.concatenate(name='concatenate\\_size\\_distance\\_and\\_img')(\\[x,\u00a0size,\u00a0distance\\])*  \n*x\u00a0=\u00a0keras.layers.dense(64,\u00a0activation='relu',\u00a0name='middle\\_dense\\_1')(concat)*  \n*x\u00a0=\u00a0keras.layers.dense(64,\u00a0activation='relu',\u00a0name='middle\\_dense\\_2')(x)*  \n*x\u00a0=\u00a0keras.layers.dense(32,\u00a0activation='relu',\u00a0name='middle\\_dense\\_3')(x)*  \n*x\\_val\u00a0=\u00a0keras.layers.dense(32,\u00a0activation='relu',\u00a0name='dense\\_x\\_1')(x)*  \n*x\\_val\u00a0=\u00a0keras.layers.dense(16,\u00a0activation='relu',\u00a0name='dense\\_x\\_2')(x\\_val)*  \n*x\\_val\u00a0=\u00a0keras.layers.dense(16,\u00a0activation='relu',\u00a0name='dense\\_x\\_3')(x\\_val)*  \n*regression\\_x\u00a0=\u00a0keras.layers.dense(1,\u00a0activation='linear',\u00a0name='regression\\_layer\\_x')(x\\_val)*  \n   \n*y\\_val\u00a0=\u00a0keras.layers.dense(32,\u00a0activation='relu',\u00a0name='dense\\_y\\_1')(x)*  \n*y\\_val\u00a0=\u00a0keras.layers.dense(16,\u00a0activation='relu',\u00a0name='dense\\_y\\_2')(y\\_val)*  \n*y\\_val\u00a0=\u00a0keras.layers.dense(16,\u00a0activation='relu',\u00a0name='dense\\_y\\_3')(y\\_val)*  \n*regression\\_y\u00a0=\u00a0keras.layers.dense(1,\u00a0activation='linear',\u00a0name='regression\\_layer\\_y')(y\\_val)*  \n*regression\u00a0=\u00a0keras.layers.concatenate(name='concatenate\\_regression')(\\[regression\\_x,\u00a0regression\\_y\\])*  \n*model\u00a0=\u00a0keras.models.model(inputs=\\[base\\_model.input,\u00a0size,\u00a0distance\\],\u00a0outputs=\\[regression\\_x,\u00a0regression\\_y,\u00a0regression\\])*\n\nthe idea of the model is that i have a box that is in at an specific distance from camera (known and used as numerical input) and with some size (known and also used as input) and i want to infer the coordinates of a pattern that occurs in the box relative to the borders of the object. i can not add a real image as i am not propietary of the dataset, but it would be similar to the following sketch:\n\n&amp;#x200b;\n\n![img](k2o13asucx941 \"fig 2. example of the scenario. three boxes, 2 of them with the same size but at different distances. one of the with twice the size but at half of the distance. the regressor outcome for all of them should be equal to (0,0) as the cross is in the middle (origin of coordinates) of the box.\")\n\ni have a hard time trying to understand what it is limiting the output range of the model to (-0.4, 0.3) while the distribution of the points seems correct and the final activation is linear.\n\nthank you for your time, have a good day!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/emozd1/help_regression_to_output_coordinates/',)", "identifyer": 5744269, "year": "2020"}, {"autor": "pythonistaaaaaaa", "date": 1582915624000, "content": "Beginner PyTorch - trying to plot a confusion matrix /!/ I'm trying to plot a confusion matrix and it doesn't work. I'm getting a weird result and I'm not sure how to interpret it (see below). I think my problem comes from just having the last confusion matrix and plotting it, but I'm not even sure because it should still plot something that looks like the 2nd picture, I think?\n\nIf someone can take a look at this and help that'd be amazing. \n\n[my current confusion matrix][1]\n\n\n[what I would like to have][2]\n\n\nHere's my code generating this:\n\n    model = torch.load('model-5-layers.pt')\n    \n    correct = 0\n    total = 0\n    \n    # Why don't we need gradients? What happens if we do include gradients?\n    with torch.no_grad():\n        \n        # Iterate over the test set\n        for data in test_loader:\n            images, labels = data\n    \n            images = images.to(device)\n            labels = labels.to(device)\n            \n            outputs = model(images)\n            \n            # torch.max is an argmax operation\n            _, predicted = torch.max(outputs.data, 1)\n            \n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n\n    print('Accuracy of the network on the test images: %d %%' % (100 * correct / total))\n\n**which prints an accuracy of 48%.**\n\nand my plotting function:\n\n\n    from sklearn.metrics import confusion_matrix\n    import matplotlib.pyplot as plt\n    \n    \n    cm = confusion_matrix(labels, predicted)\n    \n    import itertools\n    \n    \n    def plot_confusion_matrix(cm,\n                              classes,\n                              normalize=False,\n                              title='Confusion matrix',\n                              cmap=plt.cm.Blues):\n        \"\"\"\n        This function prints and plots the confusion matrix very prettily.\n        Normalization can be applied by setting `normalize=True`.\n        \"\"\"\n        if normalize:\n            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n            print(\"Normalized confusion matrix\")\n        else:\n            print('Confusion matrix, without normalization')\n    \n        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n        plt.title(title)\n    \n        # Specify the tick marks and axis text\n        tick_marks = np.arange(len(classes))\n        plt.xticks(tick_marks, classes, rotation=90)\n        plt.yticks(tick_marks, classes)\n    \n        # The data formatting\n        fmt = '.2f' if normalize else 'd'\n        thresh = cm.max() / 2.\n    \n        # Print the text of the matrix, adjusting text colour for display\n        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n            plt.text(j, i, format(cm[i, j], fmt),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] &gt; thresh else \"black\")\n    \n        plt.ylabel('True label')\n        plt.xlabel('Predicted label')\n        plt.tight_layout()\n        plt.show()\n    \n    plot_confusion_matrix(cm, classes)\n\n  [1]: https://i.stack.imgur.com/B4Ez3.png\n  [2]: https://i.stack.imgur.com/DVVSn.png", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fazhk6/beginner_pytorch_trying_to_plot_a_confusion_matrix/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "beginner pytorch - trying to -----> plot !!!  a confusion matrix /!/ i'm trying to plot a confusion matrix and it doesn't work. i'm getting a weird result and i'm not sure how to interpret it (see below). i think my problem comes from just having the last confusion matrix and plotting it, but i'm not even sure because it should still plot something that looks like the 2nd picture, i think?\n\nif someone can take a look at this and help that'd be amazing. \n\n[my current confusion matrix][1]\n\n\n[what i would like to have][2]\n\n\nhere's my code generating this:\n\n    model = torch.load('model-5-layers.pt')\n    \n    correct = 0\n    total = 0\n    \n    # why don't we need gradients? what happens if we do include gradients?\n    with torch.no_grad():\n        \n        # iterate over the test set\n        for data in test_loader:\n            images, labels = data\n    \n            images = images.to(device)\n            labels = labels.to(device)\n            \n            outputs = model(images)\n            \n            # torch.max is an argmax operation\n            _, predicted = torch.max(outputs.data, 1)\n            \n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n\n    print('accuracy of the network on the test images: %d %%' % (100 * correct / total))\n\n**which prints an accuracy of 48%.**\n\nand my plotting function:\n\n\n    from sklearn.metrics import confusion_matrix\n    import matplotlib.pyplot as plt\n    \n    \n    cm = confusion_matrix(labels, predicted)\n    \n    import itertools\n    \n    \n    def plot_confusion_matrix(cm,\n                              classes,\n                              normalize=false,\n                              title='confusion matrix',\n                              cmap=plt.cm.blues):\n        \"\"\"\n        this function prints and plots the confusion matrix very prettily.\n        normalization can be applied by setting `normalize=true`.\n        \"\"\"\n        if normalize:\n            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n            print(\"normalized confusion matrix\")\n        else:\n            print('confusion matrix, without normalization')\n    \n        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n        plt.title(title)\n    \n        # specify the tick marks and axis text\n        tick_marks = np.arange(len(classes))\n        plt.xticks(tick_marks, classes, rotation=90)\n        plt.yticks(tick_marks, classes)\n    \n        # the data formatting\n        fmt = '.2f' if normalize else 'd'\n        thresh = cm.max() / 2.\n    \n        # print the text of the matrix, adjusting text colour for display\n        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n            plt.text(j, i, format(cm[i, j], fmt),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] &gt; thresh else \"black\")\n    \n        plt.ylabel('true label')\n        plt.xlabel('predicted label')\n        plt.tight_layout()\n        plt.show()\n    \n    plot_confusion_matrix(cm, classes)\n\n  [1]: https://i.stack.imgur.com/b4ez3.png\n  [2]: https://i.stack.imgur.com/dvvsn.png", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/fazhk6/beginner_pytorch_trying_to_plot_a_confusion_matrix/',)", "identifyer": 5744311, "year": "2020"}, {"autor": "aerobic_respiration", "date": 1593347566000, "content": "How do you plot the decision boundary of an RBF kernel? /!/ Since you don't know what the feature vector \u03d5(**x**) is, you can't use **w** = \u03a3 a_j * \u03d5(**x_j**). How do you plot the decision boundary?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hhdm49/how_do_you_plot_the_decision_boundary_of_an_rbf/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "how do you -----> plot !!!  the decision boundary of an rbf kernel? /!/ since you don't know what the feature vector \u03d5(**x**) is, you can't use **w** = \u03c3 a_j * \u03d5(**x_j**). how do you plot the decision boundary?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/hhdm49/how_do_you_plot_the_decision_boundary_of_an_rbf/',)", "identifyer": 5744413, "year": "2020"}, {"autor": "matchedpotential", "date": 1593202753000, "content": "Is there a way to generate a table from tSNE data in Python? /!/  I've created a tSNE plot, and I'm looking to find out what points cluster together. Is there a way that I can call on the tSNE data to do this for me?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hgf6b1/is_there_a_way_to_generate_a_table_from_tsne_data/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "is there a way to generate a table from tsne data in python? /!/  i've created a tsne -----> plot !!! , and i'm looking to find out what points cluster together. is there a way that i can call on the tsne data to do this for me?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/hgf6b1/is_there_a_way_to_generate_a_table_from_tsne_data/',)", "identifyer": 5744459, "year": "2020"}, {"autor": "dmarketing339", "date": 1583305494000, "content": "machine learning course hyderabad /!/ 360digiTMG is best training institute in machine learning course in hyderabad. Offers hands-on practical experience on live Machine Learning based projects and in depth-understanding of Machine Learning along with 100% assistance.\n\nThe machine learning course hyderabad you will be trained under the best industry experts. With a record of over 10, 000 students trained from the 360digiTMG machine learning malasia. Popular machine learning modules that you need to learn for batter job opportunities.\n\nThis course equips the student with a strong foundation in Python, R, and R Studio. Specifically, the use of R studio to develop statistical software is highlighted. The student then develops algorithms for skewness and kurtosis, box plot, hypothesis testing (parametric and non-parametric test), correlation analysis, linear regression, multiple linear regression, logistic regression, multiple logistic regression, supervised machine learning, KNN, Naive Bayes, Decision Tree, Random Forest, ANN, and SVM. Enabling Unsupervised learning and Reinforcement Learning with Python and R is also dealt with. Students are trained to develop compelling data visualizations using Python and R. This is the most comprehensive course on Machine Learning with Python and R.\n\n[https://360digitmg.com/machine-learning-course-training-in-hyderabad](https://360digitmg.com/machine-learning-course-training-in-hyderabad)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fd9b8a/machine_learning_course_hyderabad/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "machine learning course hyderabad /!/ 360digitmg is best training institute in machine learning course in hyderabad. offers hands-on practical experience on live machine learning based projects and in depth-understanding of machine learning along with 100% assistance.\n\nthe machine learning course hyderabad you will be trained under the best industry experts. with a record of over 10, 000 students trained from the 360digitmg machine learning malasia. popular machine learning modules that you need to learn for batter job opportunities.\n\nthis course equips the student with a strong foundation in python, r, and r studio. specifically, the use of r studio to develop statistical software is highlighted. the student then develops algorithms for skewness and kurtosis, box -----> plot !!! , hypothesis testing (parametric and non-parametric test), correlation analysis, linear regression, multiple linear regression, logistic regression, multiple logistic regression, supervised machine learning, knn, naive bayes, decision tree, random forest, ann, and svm. enabling unsupervised learning and reinforcement learning with python and r is also dealt with. students are trained to develop compelling data visualizations using python and r. this is the most comprehensive course on machine learning with python and r.\n\n[https://360digitmg.com/machine-learning-course-training-in-hyderabad](https://360digitmg.com/machine-learning-course-training-in-hyderabad)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/fd9b8a/machine_learning_course_hyderabad/',)", "identifyer": 5744576, "year": "2020"}, {"autor": "altdataguy", "date": 1604362831000, "content": "Is my model useless? - prediction discrepancy /!/ I built a random forest model to predict customer churn and trained it on historical customer data from October 2019 - September 2020. The outcome was promising with high scores (precision, recall, etc.) across the board. \n\nI ran current client data (open/undecided customers) through the model, which predicted that 55% of the current \"open\" clients would churn and 45% would stay.\n\nAll good so far - I then decided to train the model on historical data from January 2019 to December 2019 instead. Again, the outcome looked promising with strong model scores.\n\nHowever, when I ran our current client data through this new model, which was now trained on 2019 data, the predictions changed drastically. It predicted - instead of 55% of churn - over 80% churn! \n\nSee attached scatter plot, where I compare the binary probabilities of the 2019 model and the one that I first built. \n\nhttps://preview.redd.it/8eqodwtj4xw51.png?width=373&amp;format=png&amp;auto=webp&amp;s=4dd20f3adcce2ae46465c951f5280ab1609c543d\n\nAre my models completely useless? What would be a good next step in order to verify whether my model could be successful in predicting future client behavior?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jmzuzi/is_my_model_useless_prediction_discrepancy/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "is my model useless? - prediction discrepancy /!/ i built a random forest model to predict customer churn and trained it on historical customer data from october 2019 - september 2020. the outcome was promising with high scores (precision, recall, etc.) across the board. \n\ni ran current client data (open/undecided customers) through the model, which predicted that 55% of the current \"open\" clients would churn and 45% would stay.\n\nall good so far - i then decided to train the model on historical data from january 2019 to december 2019 instead. again, the outcome looked promising with strong model scores.\n\nhowever, when i ran our current client data through this new model, which was now trained on 2019 data, the predictions changed drastically. it predicted - instead of 55% of churn - over 80% churn! \n\nsee attached scatter -----> plot !!! , where i compare the binary probabilities of the 2019 model and the one that i first built. \n\nhttps://preview.redd.it/8eqodwtj4xw51.png?width=373&amp;format=png&amp;auto=webp&amp;s=4dd20f3adcce2ae46465c951f5280ab1609c543d\n\nare my models completely useless? what would be a good next step in order to verify whether my model could be successful in predicting future client behavior?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/jmzuzi/is_my_model_useless_prediction_discrepancy/',)", "identifyer": 5744824, "year": "2020"}, {"autor": "Maro0101", "date": 1604348334000, "content": "I have proplem with this code /!/  [https://ideone.com/YbyHIH](https://ideone.com/YbyHIH)  \nfirst : the don't plot the graph\n\nsecond : i cant understand anything from this code starting from// def randomize(X,Y)\n\nso if someone please help me understand it i will be so thankful", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jmv8xj/i_have_proplem_with_this_code/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "i have proplem with this code /!/  [https://ideone.com/ybyhih](https://ideone.com/ybyhih)  \nfirst : the don't -----> plot !!!  the graph\n\nsecond : i cant understand anything from this code starting from// def randomize(x,y)\n\nso if someone please help me understand it i will be so thankful", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 5, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/jmv8xj/i_have_proplem_with_this_code/',)", "identifyer": 5744828, "year": "2020"}, {"autor": "tomk23_reddit", "date": 1604265619000, "content": "Which is the best residual plot? which is worst? Why? and how do you know?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jmab7a/which_is_the_best_residual_plot_which_is_worst/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "which is the best residual -----> plot !!! ? which is worst? why? and how do you know?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('image',)", "medialink": "('https://i.redd.it/qssulazs3pw51.png',)", "identifyer": 5744857, "year": "2020"}, {"autor": "jj4646", "date": 1608973858000, "content": "Clustering: Next Steps /!/ I have a large supervised dataset. Suppose I run k means clustering on this data. For a given value of k, suppose i get a good \"sum squared error\", good rands/dunn index, and the silhouette plot shows me all observations cluster homogeneously within their cluster.\n\nWhat are the next steps? How would the results of clustering help in a binary classification problem on the same dataset? \n\nSuppose the data was unsupervised. How does clustering help me? How do I proceed?\n\nE.g. Suppose its the iris dataset, i could find out the average petal width, petal length, sepal length, sepal width of all observations within a given cluster. Now what?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kkfqho/clustering_next_steps/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "clustering: next steps /!/ i have a large supervised dataset. suppose i run k means clustering on this data. for a given value of k, suppose i get a good \"sum squared error\", good rands/dunn index, and the silhouette -----> plot !!!  shows me all observations cluster homogeneously within their cluster.\n\nwhat are the next steps? how would the results of clustering help in a binary classification problem on the same dataset? \n\nsuppose the data was unsupervised. how does clustering help me? how do i proceed?\n\ne.g. suppose its the iris dataset, i could find out the average petal width, petal length, sepal length, sepal width of all observations within a given cluster. now what?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/kkfqho/clustering_next_steps/',)", "identifyer": 5745031, "year": "2020"}, {"autor": "thraway15", "date": 1608862908000, "content": "Expected to explain in interview why Lasso does variable selection but not Ridge? /!/ Lasso uses L1 penalty while Ridge uses L2. Lasso can force some coefficients to be 0, while Ridge can't\n\nI've seen the plot of the ellipses with the x,y coefficients and Lasso represented with a square and Ridge with the circle, but I don't understand why Lasso can force some coefficients to be 0 and Ridge can't\n\nIs it not uncommon for interviewers to ask MLE/DS candidates as to why Lasso can force some coefficients to be 0 and Ridge can't? If so, what is a good explanation for this?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kjrex1/expected_to_explain_in_interview_why_lasso_does/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "expected to explain in interview why lasso does variable selection but not ridge? /!/ lasso uses l1 penalty while ridge uses l2. lasso can force some coefficients to be 0, while ridge can't\n\ni've seen the -----> plot !!!  of the ellipses with the x,y coefficients and lasso represented with a square and ridge with the circle, but i don't understand why lasso can force some coefficients to be 0 and ridge can't\n\nis it not uncommon for interviewers to ask mle/ds candidates as to why lasso can force some coefficients to be 0 and ridge can't? if so, what is a good explanation for this?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 17, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/kjrex1/expected_to_explain_in_interview_why_lasso_does/',)", "identifyer": 5745077, "year": "2020"}, {"autor": "DLTSMan", "date": 1585142063000, "content": "clustering on images for quantitative analysis /!/ Hello,\n\nI have some black and grey images and I would like to cluster them like highlighted by the red circles here:\n\nhttps://imgur.com/a/vRyq6Am\n\nThe goal is later to have extract quantitative information from it (cluster sizes and clusters distances mostly).\n\nI am really a beginner at this. What I have looked for now some tutorials on kmeans clustering on RGB images for colors reductions but I don't think it applies here. I do not want to cluster them by intensity but more by distances (like kmeans on a cloud plot).\n\nCan anyone could suggest me some hints or some resource to look at for this specific problem?\n\nMany thanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/foq2hu/clustering_on_images_for_quantitative_analysis/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "clustering on images for quantitative analysis /!/ hello,\n\ni have some black and grey images and i would like to cluster them like highlighted by the red circles here:\n\nhttps://imgur.com/a/vryq6am\n\nthe goal is later to have extract quantitative information from it (cluster sizes and clusters distances mostly).\n\ni am really a beginner at this. what i have looked for now some tutorials on kmeans clustering on rgb images for colors reductions but i don't think it applies here. i do not want to cluster them by intensity but more by distances (like kmeans on a cloud -----> plot !!! ).\n\ncan anyone could suggest me some hints or some resource to look at for this specific problem?\n\nmany thanks", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/foq2hu/clustering_on_images_for_quantitative_analysis/',)", "identifyer": 5745284, "year": "2020"}, {"autor": "drainbamagex", "date": 1594769769000, "content": "How to plot SVR tuning? /!/ Hi folks, \n\nI've a regression task and I'm building a SVR model using a library by [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html). Well, here I've a option to gamma value scaled by features and it's over. So, I'm tuning only \"C\" and \"epsilon\" hyperparameters using a RandomizedSearch strategy w/ cross-validation.\n\nI did make a \"history\" to each error metric according of each pair of values tested, but maybe I got a [2D/3D chart uninformative](https://i.imgur.com/tli83kK.png).\n\nMy question is:\n\n**Is there a cool way to plot these tuning?** \n\n&amp;#x200B;\n\nMany tks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hrcnx5/how_to_plot_svr_tuning/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "how to -----> plot !!!  svr tuning? /!/ hi folks, \n\ni've a regression task and i'm building a svr model using a library by [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.svm.svr.html). well, here i've a option to gamma value scaled by features and it's over. so, i'm tuning only \"c\" and \"epsilon\" hyperparameters using a randomizedsearch strategy w/ cross-validation.\n\ni did make a \"history\" to each error metric according of each pair of values tested, but maybe i got a [2d/3d chart uninformative](https://i.imgur.com/tli83kk.png).\n\nmy question is:\n\n**is there a cool way to plot these tuning?** \n\n&amp;#x200b;\n\nmany tks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/hrcnx5/how_to_plot_svr_tuning/',)", "identifyer": 5745521, "year": "2020"}, {"autor": "Not_a_krusty_krab_36", "date": 1593999938000, "content": "Stock price prediction AI (Looking for advice) /!/ I followed some youtube tutorials and wrote a script for predicting stock prices. I'm very new to AI/deep learning. I understand my code may have issues with leakage. Any advice or tips would be very much appreciated :)\n\nScript is posted below:\n\nimport math  \nimport pandas as pd  \nimport tensorflow  \nimport pandas\\_datareader as web  \nimport numpy as np  \nfrom sklearn.preprocessing import MinMaxScaler  \nfrom keras.models import Sequential  \nfrom keras.layers import Dense, LSTM  \nimport matplotlib.pyplot as plt  \n\n\nplt.style.use('fivethirtyeight')  \n\n\n\\# get stock quote  \ndf = web.DataReader('AAPL', data\\_source='yahoo', start='2012-01-01', end='2019-12-17')  \n\n\n\\# Show stonk data  \nprint(df)  \n\\# get number of rows and columns  \nprint(df.shape)  \n\\# visualize the closing price history  \nplt.figure(figsize=(16, 8))  \nplt.title('Close Price History')  \nplt.plot(df\\['Close'\\])  \nplt.xlabel('Date', Fontsize=18)  \nplt.ylabel('Close Price USD', Fontsize=18)  \nplt.show()  \n\\# Create new dataframe with Close column  \ndata = df.filter(\\['Close'\\])  \n\\# Convert the data frame to numpy array  \ndataset = data.values  \n\\# get the number of rows to train the program with  \ntraining\\_data\\_len = math.ceil(len(dataset) \\* .8)  \nprint(training\\_data\\_len)  \n\\# Scale the data  \nscaler = MinMaxScaler(feature\\_range=(0, 1))  \nscaler\\_data = scaler.fit\\_transform(dataset)  \n\n\nprint(scaler\\_data)  \n\\# create the training data set  \ntrain\\_data = scaler\\_data\\[0:training\\_data\\_len\\]  \n\\# Split data into x and y train  \nx\\_train = \\[\\]  \ny\\_train = \\[\\]  \n\n\nfor i in range(60, len(train\\_data)):  \nx\\_train.append(train\\_data\\[i - 60:i, 0\\])  \ny\\_train.append(train\\_data\\[i, 0\\])  \n if i &lt;= 60:  \n print(x\\_train)  \n print(y\\_train)  \n print()  \n\\# convert the x\\_train and y\\_train to numpy arrays  \nx\\_train, y\\_train = np.array(x\\_train), np.array(y\\_train)  \n\n\n\\# Reshape data  \nx\\_train = np.reshape(x\\_train, (x\\_train.shape\\[0\\], x\\_train.shape\\[1\\], 1))  \nprint(x\\_train.shape)  \n\n\n\\# Build the LSTM model  \nmodel = Sequential()  \nmodel.add(LSTM(50, return\\_sequences=True, input\\_shape=(x\\_train.shape\\[1\\], 1)))  \nmodel.add(LSTM(50, return\\_sequences=False))  \nmodel.add(Dense(25))  \nmodel.add(Dense(1))  \n\n\n\\# compile the model  \nmodel.complie(optimizer='adam', loss='mean\\_aquared\\_error')  \n\n\n\\# train the model  \nmodel.fit(x\\_train, y\\_train, batch\\_size=1, epochs=1)  \n\n\n\\# create the testing data set  \n\\# create ne array with scaled values  \ntest\\_data = scaler\\_data\\[training\\_data\\_len - 60:, :\\]  \n\\# Create the data sets x\\_test and y\\_test  \nx\\_test = \\[\\]  \ny\\_test = dataset\\[training\\_data\\_len, :\\]  \nfor i in range(60, len(test\\_data)):  \nx\\_test.append(test\\_data\\[i - 60:i, 0\\])  \n\\# Convert the data to a numpy array  \nx\\_test = np.array(x\\_test)  \nx\\_test = np.reshape(x\\_test, (x\\_test.shape\\[0\\], x\\_test.shape\\[1\\], 1))  \n\\# Get models predicted price values  \npredictions = model.predict(x\\_test)  \npredictions = scaler.inverse\\_transform(predictions)  \n\\# Get the root mean squared error (RMSE)  \nrmse = np.sqrt(np.mean(predictions - y\\_test) \\*\\* 2)  \nprint(rmse)  \n\\# plot the data  \ntrain = data\\[:training\\_data\\_len\\]  \nvalid = data\\[training\\_data\\_len\\]  \nvalid\\['Predictions'\\] = predictions  \n\\# Visulaize the data  \nplt.figure(figsize=(16, 8))  \nplt.xlabel('Date', fontsize=18)  \nplt.plot(train\\['Close'\\])  \nplt.plot(valid\\[\\['Close', 'Predictions'\\]\\])  \nplt.legend(\\['Train', 'Val', 'Predictions'\\], loc='lower right')  \nplt.show()", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hlyoc5/stock_price_prediction_ai_looking_for_advice/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "stock price prediction ai (looking for advice) /!/ i followed some youtube tutorials and wrote a script for predicting stock prices. i'm very new to ai/deep learning. i understand my code may have issues with leakage. any advice or tips would be very much appreciated :)\n\nscript is posted below:\n\nimport math  \nimport pandas as pd  \nimport tensorflow  \nimport pandas\\_datareader as web  \nimport numpy as np  \nfrom sklearn.preprocessing import minmaxscaler  \nfrom keras.models import sequential  \nfrom keras.layers import dense, lstm  \nimport matplotlib.pyplot as plt  \n\n\nplt.style.use('fivethirtyeight')  \n\n\n\\# get stock quote  \ndf = web.datareader('aapl', data\\_source='yahoo', start='2012-01-01', end='2019-12-17')  \n\n\n\\# show stonk data  \nprint(df)  \n\\# get number of rows and columns  \nprint(df.shape)  \n\\# visualize the closing price history  \nplt.figure(figsize=(16, 8))  \nplt.title('close price history')  \nplt.plot(df\\['close'\\])  \nplt.xlabel('date', fontsize=18)  \nplt.ylabel('close price usd', fontsize=18)  \nplt.show()  \n\\# create new dataframe with close column  \ndata = df.filter(\\['close'\\])  \n\\# convert the data frame to numpy array  \ndataset = data.values  \n\\# get the number of rows to train the program with  \ntraining\\_data\\_len = math.ceil(len(dataset) \\* .8)  \nprint(training\\_data\\_len)  \n\\# scale the data  \nscaler = minmaxscaler(feature\\_range=(0, 1))  \nscaler\\_data = scaler.fit\\_transform(dataset)  \n\n\nprint(scaler\\_data)  \n\\# create the training data set  \ntrain\\_data = scaler\\_data\\[0:training\\_data\\_len\\]  \n\\# split data into x and y train  \nx\\_train = \\[\\]  \ny\\_train = \\[\\]  \n\n\nfor i in range(60, len(train\\_data)):  \nx\\_train.append(train\\_data\\[i - 60:i, 0\\])  \ny\\_train.append(train\\_data\\[i, 0\\])  \n if i &lt;= 60:  \n print(x\\_train)  \n print(y\\_train)  \n print()  \n\\# convert the x\\_train and y\\_train to numpy arrays  \nx\\_train, y\\_train = np.array(x\\_train), np.array(y\\_train)  \n\n\n\\# reshape data  \nx\\_train = np.reshape(x\\_train, (x\\_train.shape\\[0\\], x\\_train.shape\\[1\\], 1))  \nprint(x\\_train.shape)  \n\n\n\\# build the lstm model  \nmodel = sequential()  \nmodel.add(lstm(50, return\\_sequences=true, input\\_shape=(x\\_train.shape\\[1\\], 1)))  \nmodel.add(lstm(50, return\\_sequences=false))  \nmodel.add(dense(25))  \nmodel.add(dense(1))  \n\n\n\\# compile the model  \nmodel.complie(optimizer='adam', loss='mean\\_aquared\\_error')  \n\n\n\\# train the model  \nmodel.fit(x\\_train, y\\_train, batch\\_size=1, epochs=1)  \n\n\n\\# create the testing data set  \n\\# create ne array with scaled values  \ntest\\_data = scaler\\_data\\[training\\_data\\_len - 60:, :\\]  \n\\# create the data sets x\\_test and y\\_test  \nx\\_test = \\[\\]  \ny\\_test = dataset\\[training\\_data\\_len, :\\]  \nfor i in range(60, len(test\\_data)):  \nx\\_test.append(test\\_data\\[i - 60:i, 0\\])  \n\\# convert the data to a numpy array  \nx\\_test = np.array(x\\_test)  \nx\\_test = np.reshape(x\\_test, (x\\_test.shape\\[0\\], x\\_test.shape\\[1\\], 1))  \n\\# get models predicted price values  \npredictions = model.predict(x\\_test)  \npredictions = scaler.inverse\\_transform(predictions)  \n\\# get the root mean squared error (rmse)  \nrmse = np.sqrt(np.mean(predictions - y\\_test) \\*\\* 2)  \nprint(rmse)  \n\\# -----> plot !!!  the data  \ntrain = data\\[:training\\_data\\_len\\]  \nvalid = data\\[training\\_data\\_len\\]  \nvalid\\['predictions'\\] = predictions  \n\\# visulaize the data  \nplt.figure(figsize=(16, 8))  \nplt.xlabel('date', fontsize=18)  \nplt.plot(train\\['close'\\])  \nplt.plot(valid\\[\\['close', 'predictions'\\]\\])  \nplt.legend(\\['train', 'val', 'predictions'\\], loc='lower right')  \nplt.show()", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/hlyoc5/stock_price_prediction_ai_looking_for_advice/',)", "identifyer": 5745686, "year": "2020"}, {"autor": "AJ20190", "date": 1591575094000, "content": "I need to set a reminder to always plot out my loss /!/ I let a model run for 5 days but forgot to store my loss over time in a CSV file. I hate my life right now \ud83d\ude2d", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gyo2x6/i_need_to_set_a_reminder_to_always_plot_out_my/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "i need to set a reminder to always -----> plot !!!  out my loss /!/ i let a model run for 5 days but forgot to store my loss over time in a csv file. i hate my life right now \ud83d\ude2d", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/gyo2x6/i_need_to_set_a_reminder_to_always_plot_out_my/',)", "identifyer": 5745758, "year": "2020"}, {"autor": "GetStuffTogether", "date": 1591549401000, "content": "PCA vs TruncatedSVD for visualizing vectorized text data? Advantages vs disadvantages? /!/ PCA can\u2019t take in sparse matrix, therefore I am getting an error. I came across SVD, which can take in sparse matrix and produce a scatter plot of text data.\n\nBut comparing graphs on small datasets, there are minor differences between PCA and Truncated SVD. What are they? Which one is better?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gyg15y/pca_vs_truncatedsvd_for_visualizing_vectorized/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "pca vs truncatedsvd for visualizing vectorized text data? advantages vs disadvantages? /!/ pca can\u2019t take in sparse matrix, therefore i am getting an error. i came across svd, which can take in sparse matrix and produce a scatter -----> plot !!!  of text data.\n\nbut comparing graphs on small datasets, there are minor differences between pca and truncated svd. what are they? which one is better?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/gyg15y/pca_vs_truncatedsvd_for_visualizing_vectorized/',)", "identifyer": 5745775, "year": "2020"}, {"autor": "phi_beta_kappa", "date": 1589987790000, "content": "How would you analyze the effect of temperature in this scenario? /!/ Let's say that each record represents an event that occurred on any given day. One of the variables of that record is the temperature of that day. Some days could have only one event, and other days could have many occurrences.\n\nNow suppose you want to analyze whether the temperature has any effect on the number of occurrences. My question is,\n\n1. Would you aggregate the number of occurrences by each degree in Fahrenheit and then plot the temperature against the aggregated values.\n2. Would you aggregate the number of occurrences by the date, i.e. 1/1/2020, and then plot the temperature against the aggregated values.\n\nI guess the question is simply whether you would group by temperature or group by date. Or is there a different approach you would take in this scenario?\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gnd6q6/how_would_you_analyze_the_effect_of_temperature/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "how would you analyze the effect of temperature in this scenario? /!/ let's say that each record represents an event that occurred on any given day. one of the variables of that record is the temperature of that day. some days could have only one event, and other days could have many occurrences.\n\nnow suppose you want to analyze whether the temperature has any effect on the number of occurrences. my question is,\n\n1. would you aggregate the number of occurrences by each degree in fahrenheit and then -----> plot !!!  the temperature against the aggregated values.\n2. would you aggregate the number of occurrences by the date, i.e. 1/1/2020, and then plot the temperature against the aggregated values.\n\ni guess the question is simply whether you would group by temperature or group by date. or is there a different approach you would take in this scenario?\n\nthanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/gnd6q6/how_would_you_analyze_the_effect_of_temperature/',)", "identifyer": 5745901, "year": "2020"}, {"autor": "muscleache", "date": 1589918090000, "content": "Introductory Machines Learning Course Assignment /!/ Hello Everyone,\n\nIf this is not the right place for this post, can you please point me in the right direction ?\n\nI am taking an introductory machine learning course and just got my first assignment withPython and I am a bit lost.\n\nFirst Issue,\n\nI was asked to calculate the first 2  principal components for 6  columns of data  in the CSV using numpy only  and I was able to do so with help from [machinelearningmastery.com](https://machinelearningmastery.com/calculate-principal-component-analysis-scratch-python/). But then I'm asked to plot the first two principal components in a scatter plot and use a different color based on a value in a binary field for which the principal component was not calculated.  My problem is that I don't  understand what need to be done or how to do it. I  have 12 PC calculated, now how does that relate tot he binary field and what am I supposed to plot exactly ?\n\nSecond Issue,\n\nI am asked to write a functions function that takes some data x and binary target y and fits a logistic regression on the data. The function should return the coefficients (betas) of every feature. That should be easy, but what is meant by every feature ? Any ideas ?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gmvxar/introductory_machines_learning_course_assignment/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "introductory machines learning course assignment /!/ hello everyone,\n\nif this is not the right place for this post, can you please point me in the right direction ?\n\ni am taking an introductory machine learning course and just got my first assignment withpython and i am a bit lost.\n\nfirst issue,\n\ni was asked to calculate the first 2  principal components for 6  columns of data  in the csv using numpy only  and i was able to do so with help from [machinelearningmastery.com](https://machinelearningmastery.com/calculate-principal-component-analysis-scratch-python/). but then i'm asked to -----> plot !!!  the first two principal components in a scatter -----> plot !!!  and use a different color based on a value in a binary field for which the principal component was not calculated.  my problem is that i don't  understand what need to be done or how to do it. i  have 12 pc calculated, now how does that relate tot he binary field and what am i supposed to plot exactly ?\n\nsecond issue,\n\ni am asked to write a functions function that takes some data x and binary target y and fits a logistic regression on the data. the function should return the coefficients (betas) of every feature. that should be easy, but what is meant by every feature ? any ideas ?", "sortedWord": "None", "removed": "('nan',)", "score": 2, "comments": 1, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/gmvxar/introductory_machines_learning_course_assignment/',)", "identifyer": 5745947, "year": "2020"}, {"autor": "iamrealadvait", "date": 1589901292000, "content": "Machine Learning with Python : HeatMap: How to plot Heatmap and use Seaborn library. more on : www.facebook.com/seevecoding", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gmqgn6/machine_learning_with_python_heatmap_how_to_plot/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "machine learning with python : heatmap: how to -----> plot !!!  heatmap and use seaborn library. more on : www.facebook.com/seevecoding", "sortedWord": "None", "removed": "('nan',)", "score": 0, "comments": 0, "media": "('hosted:video',)", "medialink": "('https://v.redd.it/4jbwb4o9nqz41',)", "identifyer": 5745960, "year": "2020"}, {"autor": "codefreak-123", "date": 1589316372000, "content": "Machine learning curve? /!/ &amp;#x200B;\n\nHey fellow ML engineers!\n\nI have been doing an introductory ML course on edX for a while. Even though I love to code and build things, I get frustrated when I get stuck with complex code. For example, consider this :\n\nbeta\\_1 = 0.10\n\nbeta\\_2 = 1990.0\n\n#logistic function\n\nY\\_pred = sigmoid(x\\_data, beta\\_1 , beta\\_2\n\n#plot initial prediction against datapoints\n\nplt.plot(x\\_data, Y\\_pred\\*15000000000000.)\n\nplt.plot(x\\_data, y\\_data, 'ro')\n\nFor me, it was really difficult to get pass through it. I kept researching for like an hour until I realized I  wasted like an hour on this one problem. So, my question is how do you get past a complex code? Also, when I get stuck, I really question myself if I really want to be a ML engineer.\n\nI am sorry if I am all over the place, but I am still questioning my career towards ML. I have tried web development but that didn't click me. Maybe it's because of these hard problems that's stopping me from being a ML engineer.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gik0px/machine_learning_curve/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "machine learning curve? /!/ &amp;#x200b;\n\nhey fellow ml engineers!\n\ni have been doing an introductory ml course on edx for a while. even though i love to code and build things, i get frustrated when i get stuck with complex code. for example, consider this :\n\nbeta\\_1 = 0.10\n\nbeta\\_2 = 1990.0\n\n#logistic function\n\ny\\_pred = sigmoid(x\\_data, beta\\_1 , beta\\_2\n\n#-----> plot !!!  initial prediction against datapoints\n\nplt.plot(x\\_data, y\\_pred\\*15000000000000.)\n\nplt.plot(x\\_data, y\\_data, 'ro')\n\nfor me, it was really difficult to get pass through it. i kept researching for like an hour until i realized i  wasted like an hour on this one problem. so, my question is how do you get past a complex code? also, when i get stuck, i really question myself if i really want to be a ml engineer.\n\ni am sorry if i am all over the place, but i am still questioning my career towards ml. i have tried web development but that didn't click me. maybe it's because of these hard problems that's stopping me from being a ml engineer.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/gik0px/machine_learning_curve/',)", "identifyer": 5746014, "year": "2020"}, {"autor": "phoenix__191", "date": 1588559719000, "content": "Require help to interpret GAN generator output /!/ I'm trying to train a DCGAN on the CIFAR10 dataset (in PyTorch). The last layer in my generator model is a convolutional layer with a tanh activation, thus outputting values in the range (-1,1). To then plot the images that have been generated I did `x_creation = (x_creation + 1)/2` where `x_creation` was the output tensor of the generator model to rescale the tensor back to the range (0,1).\n\n  \nHowever, on plotting the [images](https://imgur.com/a/eXLZKXr), I get a whitish layer on top of them and I can't understand where exactly I am going wrong. Should I try using a sigmoid activation and plotting the values as is instead? Or is my scaling the inherent problem? The same model works correctly when implemented in Keras. I used the `permute()` function of PyTorch to manipulate the tensor channels to match the default requirements of the `pyplot.imshow()` function.\n\n  \n Any help regarding the same would be appreciated.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gd3bzz/require_help_to_interpret_gan_generator_output/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "require help to interpret gan generator output /!/ i'm trying to train a dcgan on the cifar10 dataset (in pytorch). the last layer in my generator model is a convolutional layer with a tanh activation, thus outputting values in the range (-1,1). to then -----> plot !!!  the images that have been generated i did `x_creation = (x_creation + 1)/2` where `x_creation` was the output tensor of the generator model to rescale the tensor back to the range (0,1).\n\n  \nhowever, on plotting the [images](https://imgur.com/a/exlzkxr), i get a whitish layer on top of them and i can't understand where exactly i am going wrong. should i try using a sigmoid activation and plotting the values as is instead? or is my scaling the inherent problem? the same model works correctly when implemented in keras. i used the `permute()` function of pytorch to manipulate the tensor channels to match the default requirements of the `pyplot.imshow()` function.\n\n  \n any help regarding the same would be appreciated.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/gd3bzz/require_help_to_interpret_gan_generator_output/',)", "identifyer": 5746088, "year": "2020"}, {"autor": "longdong93", "date": 1599155428000, "content": "Is it possible to plot a multidimensional partial dependence plot with more than 3 features in it? /!/ I have built a RF model which has around 20 features in it. It is accurate enough to be deployed but I want to make a prescriptive model rather than a predictive model. Regression models are easier to be implemented but they don't give me good accuracy to use it. So I want to proceed with a better model, that is RF but it is very hard to interpret when compared to regression model, where you can obtain a simple equation out. So I came across partial dependence plots and I was wondering if I can plot a multidimensional partial dependence plot to interpret the random forest and get an equation for the curve, but Google doesn't provide me much material about multidimensional PDP. So is it possible for me to build a multidimensional PDP which will include all the features? And is it possible to derive an equation from the above said plot? Thanks in advance!\n\nTL;dr Want to interpret my RF model with 20 features, is it possible to plot a multidimensional PDP with all the features in it in a single plot.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ilyehl/is_it_possible_to_plot_a_multidimensional_partial/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "is it possible to -----> plot !!!  a multidimensional partial dependence -----> plot !!!  with more than 3 features in it? /!/ i have built a rf model which has around 20 features in it. it is accurate enough to be deployed but i want to make a prescriptive model rather than a predictive model. regression models are easier to be implemented but they don't give me good accuracy to use it. so i want to proceed with a better model, that is rf but it is very hard to interpret when compared to regression model, where you can obtain a simple equation out. so i came across partial dependence plots and i was wondering if i can plot a multidimensional partial dependence plot to interpret the random forest and get an equation for the curve, but google doesn't provide me much material about multidimensional pdp. so is it possible for me to build a multidimensional pdp which will include all the features? and is it possible to derive an equation from the above said plot? thanks in advance!\n\ntl;dr want to interpret my rf model with 20 features, is it possible to plot a multidimensional pdp with all the features in it in a single plot.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/ilyehl/is_it_possible_to_plot_a_multidimensional_partial/',)", "identifyer": 5746363, "year": "2020"}, {"autor": "shreex7522", "date": 1596191337000, "content": "Creating a Scatter Plot in Matplotlib", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i1588t/creating_a_scatter_plot_in_matplotlib/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "creating a scatter -----> plot !!!  in matplotlib", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('link',)", "medialink": "('https://www.asquero.com/article/creating-a-scatter-plot-in-matplotlib/',)", "identifyer": 5746578, "year": "2020"}, {"autor": "canarysplit", "date": 1596024265000, "content": "If the intercept is 0, and slope of the curve is -5.7, why does the equation looks like this when plotted? /!/  \n\nI'm following the tutorial that is explaining Gradient Descent and now I came to the part where the derivation of the sum of the squared residuals is being explained. I've plotted the quadratic function which represents all the calculations of the sum of the squared residuals for different intercepts.\n\nHowever, when I'm trying to plot the same line as them that is a linear function I'm not able to do it?\n\nI'm plotting the function f(x) = -5.7\\*x + 0 as these values are implied from the image. When I plot this function I get something different. How can I get this line plot that is this image?\n\nhttps://preview.redd.it/c72cmv80esd51.png?width=1604&amp;format=png&amp;auto=webp&amp;s=03239ab0c08bbbd27778a95df0a19a3e127514eb\n\nSource (9:50 - [https://www.youtube.com/watch?v=sDv4f4s2SB8](https://www.youtube.com/watch?v=sDv4f4s2SB8))", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hzziow/if_the_intercept_is_0_and_slope_of_the_curve_is/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "if the intercept is 0, and slope of the curve is -5.7, why does the equation looks like this when plotted? /!/  \n\ni'm following the tutorial that is explaining gradient descent and now i came to the part where the derivation of the sum of the squared residuals is being explained. i've plotted the quadratic function which represents all the calculations of the sum of the squared residuals for different intercepts.\n\nhowever, when i'm trying to -----> plot !!!  the same line as them that is a linear function i'm not able to do it?\n\ni'm plotting the function f(x) = -5.7\\*x + 0 as these values are implied from the image. when i plot this function i get something different. how can i get this line plot that is this image?\n\nhttps://preview.redd.it/c72cmv80esd51.png?width=1604&amp;format=png&amp;auto=webp&amp;s=03239ab0c08bbbd27778a95df0a19a3e127514eb\n\nsource (9:50 - [https://www.youtube.com/watch?v=sdv4f4s2sb8](https://www.youtube.com/watch?v=sdv4f4s2sb8))", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 9, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/hzziow/if_the_intercept_is_0_and_slope_of_the_curve_is/',)", "identifyer": 5746650, "year": "2020"}, {"autor": "Klein225", "date": 1596020583000, "content": "Computing the loss (MSE) for every iteration and time Tensorflow /!/ I'm fairly new at neural networks but I am writing about it as part of my thesis.\n\nI want to use Tensorboard to plot the mean squared error (y-axis) for every iteration over a given time frame (x-axis), say 5 minutes.\n\nI only know how to plot the MSE given every epoch and set a callback at 5 minutes. This does not however solve my problem.\n\nI have tried looking at the internet for some solutions to how you can maybe set a maximum number of iterations rather than epochs when doing model.fit, but without luck. I know iterations is the number of batches needed to complete one epoch, but as I want to tune the batch\\_size, I prefer to use the iterations.\n\nMy code currently looks like the following:\n\n    input_size = len(train_dataset.keys())\n    output_size = 10\n    hidden_layer_size = 100\n    n_epochs = 3\n    \n    weights_initializer = keras.initializers.GlorotUniform()\n    \n    #A function that trains and validates the model and returns the MSE\n    def train_val_model(run_dir, hparams):\n        model = keras.models.Sequential([\n                #Layer to be used as an entry point into a Network\n                keras.layers.InputLayer(input_shape=[len(train_dataset.keys())]),\n                #Dense layer 1\n                keras.layers.Dense(hidden_layer_size, activation='sigmoid', \n                                   kernel_initializer = weights_initializer,\n                                   name='Layer_1'),\n                #Dense layer 2\n                keras.layers.Dense(hidden_layer_size, activation='sigmoid', \n                                   kernel_initializer = weights_initializer,\n                                   name='Layer_2'),\n                #activation function is linear since we are doing regression\n                keras.layers.Dense(output_size, activation='linear', name='Output_layer')\n                                    ])\n        \n        #Use the stochastic gradient descent optimizer but change batch_size to get BSG, SGD or MiniSGD\n        optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.0,\n                                            nesterov=False)\n        \n        #Compiling the model\n        model.compile(optimizer=optimizer, \n                      loss='mean_squared_error', #Computes the mean of squares of errors between labels and predictions\n                      metrics=['mean_squared_error']) #Computes the mean squared error between y_true and y_pred\n        \n        # initialize TimeStopping callback \n        time_stopping_callback = tfa.callbacks.TimeStopping(seconds=5*60, verbose=1)\n        \n        #Training the network\n        history = model.fit(normed_train_data, train_labels, \n             epochs=n_epochs,\n             batch_size=hparams['batch_size'], \n             verbose=1,\n             #validation_split=0.2,\n             callbacks=[tf.keras.callbacks.TensorBoard(run_dir + \"/Keras\"), time_stopping_callback])\n        \n        return history\n    \n    train_val_model(\"logs/sample\", {'batch_size': len(normed_train_data)})\n    train_val_model(\"logs/sample1\", {'batch_size': 1})\n\nI use tensorboard to plot the graphs:\n\n    %tensorboard --logdir_spec=BSG:logs/sample,SGD:logs/sample1\n\nThis results on the following output:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/373ihe5t2sd51.png?width=539&amp;format=png&amp;auto=webp&amp;s=a2f63d2d10b64f245f7d4de53093e058928f6121\n\nThe desired output however, should look something like this:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/d5l9pg1u2sd51.png?width=1492&amp;format=png&amp;auto=webp&amp;s=54d0f93277ad6deb1ad87223190c99d37a96e8cd\n\nand this:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/x7cb3zf23sd51.png?width=1510&amp;format=png&amp;auto=webp&amp;s=193a5e339d8ead70ffd537e2a37fd904a3155787\n\nThe problem is that i want to visually compare the two types of gradient descent (Batch gradient descent and stochastic gradient descent). Doing stochastic gradient descent takes way too long per one epoch, so if there is a way I could show the number of steps or iterations instead of epochs, as well as the time it takes to calculate, that would make the different types of gradient descent more comparable", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hzyqyn/computing_the_loss_mse_for_every_iteration_and/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "computing the loss (mse) for every iteration and time tensorflow /!/ i'm fairly new at neural networks but i am writing about it as part of my thesis.\n\ni want to use tensorboard to -----> plot !!!  the mean squared error (y-axis) for every iteration over a given time frame (x-axis), say 5 minutes.\n\ni only know how to plot the mse given every epoch and set a callback at 5 minutes. this does not however solve my problem.\n\ni have tried looking at the internet for some solutions to how you can maybe set a maximum number of iterations rather than epochs when doing model.fit, but without luck. i know iterations is the number of batches needed to complete one epoch, but as i want to tune the batch\\_size, i prefer to use the iterations.\n\nmy code currently looks like the following:\n\n    input_size = len(train_dataset.keys())\n    output_size = 10\n    hidden_layer_size = 100\n    n_epochs = 3\n    \n    weights_initializer = keras.initializers.glorotuniform()\n    \n    #a function that trains and validates the model and returns the mse\n    def train_val_model(run_dir, hparams):\n        model = keras.models.sequential([\n                #layer to be used as an entry point into a network\n                keras.layers.inputlayer(input_shape=[len(train_dataset.keys())]),\n                #dense layer 1\n                keras.layers.dense(hidden_layer_size, activation='sigmoid', \n                                   kernel_initializer = weights_initializer,\n                                   name='layer_1'),\n                #dense layer 2\n                keras.layers.dense(hidden_layer_size, activation='sigmoid', \n                                   kernel_initializer = weights_initializer,\n                                   name='layer_2'),\n                #activation function is linear since we are doing regression\n                keras.layers.dense(output_size, activation='linear', name='output_layer')\n                                    ])\n        \n        #use the stochastic gradient descent optimizer but change batch_size to get bsg, sgd or minisgd\n        optimizer = tf.keras.optimizers.sgd(learning_rate=0.001, momentum=0.0,\n                                            nesterov=false)\n        \n        #compiling the model\n        model.compile(optimizer=optimizer, \n                      loss='mean_squared_error', #computes the mean of squares of errors between labels and predictions\n                      metrics=['mean_squared_error']) #computes the mean squared error between y_true and y_pred\n        \n        # initialize timestopping callback \n        time_stopping_callback = tfa.callbacks.timestopping(seconds=5*60, verbose=1)\n        \n        #training the network\n        history = model.fit(normed_train_data, train_labels, \n             epochs=n_epochs,\n             batch_size=hparams['batch_size'], \n             verbose=1,\n             #validation_split=0.2,\n             callbacks=[tf.keras.callbacks.tensorboard(run_dir + \"/keras\"), time_stopping_callback])\n        \n        return history\n    \n    train_val_model(\"logs/sample\", {'batch_size': len(normed_train_data)})\n    train_val_model(\"logs/sample1\", {'batch_size': 1})\n\ni use tensorboard to plot the graphs:\n\n    %tensorboard --logdir_spec=bsg:logs/sample,sgd:logs/sample1\n\nthis results on the following output:\n\n&amp;#x200b;\n\nhttps://preview.redd.it/373ihe5t2sd51.png?width=539&amp;format=png&amp;auto=webp&amp;s=a2f63d2d10b64f245f7d4de53093e058928f6121\n\nthe desired output however, should look something like this:\n\n&amp;#x200b;\n\nhttps://preview.redd.it/d5l9pg1u2sd51.png?width=1492&amp;format=png&amp;auto=webp&amp;s=54d0f93277ad6deb1ad87223190c99d37a96e8cd\n\nand this:\n\n&amp;#x200b;\n\nhttps://preview.redd.it/x7cb3zf23sd51.png?width=1510&amp;format=png&amp;auto=webp&amp;s=193a5e339d8ead70ffd537e2a37fd904a3155787\n\nthe problem is that i want to visually compare the two types of gradient descent (batch gradient descent and stochastic gradient descent). doing stochastic gradient descent takes way too long per one epoch, so if there is a way i could show the number of steps or iterations instead of epochs, as well as the time it takes to calculate, that would make the different types of gradient descent more comparable", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/hzyqyn/computing_the_loss_mse_for_every_iteration_and/',)", "identifyer": 5746654, "year": "2020"}, {"autor": "ShahabBngsh", "date": 1598701626000, "content": "Digit Recognition from scratch in python using MNIST /!/ Yay! Finally finished my first project . By first project I mean , first project worth showing off.  I used **MNIST** dataset from ***kaggle***, which consists of 28\\*28 pixel images. I built everything from scratch including activation function. However ***numpy*** is used to speed up the computation.\n\nI followed **PEP8** standard and commented the code where code wasn't self explanatory.\n\n# What I learned from this project?\n\nUnlike other fields in CS, ML can be daunting sometime. You don't get segfault that often, your code will run fine but you'll not get the desired result/accuracy. These bugs are notoriously difficult to chase and hunt.\n\nThese simple tools really help me speed up my debugging process.\n\n* Always visualize the dataset, it'll give some visual insight into the dataset.\n* Keep track of matrix dimensions on paper for correctness of your code.\n* After implementing Forward Propagation you can check its correctness on AND/OR logic gate.\n* Plot *Model Accuracy and Loss* graph to visualize your model performance.\n* Confusion matrix will show overall performance on each output node.\n\n[GitHub link](https://github.com/ShahabBngsh/Digit_Recogniser)\n\nPS Any feedback, Dos and Don'ts would be appreciated.\n\nI hope you like it. As an introvert person, I thought a tons of time before posting it here ;)\n\nTHANKS!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/iir2qm/digit_recognition_from_scratch_in_python_using/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "digit recognition from scratch in python using mnist /!/ yay! finally finished my first project . by first project i mean , first project worth showing off.  i used **mnist** dataset from ***kaggle***, which consists of 28\\*28 pixel images. i built everything from scratch including activation function. however ***numpy*** is used to speed up the computation.\n\ni followed **pep8** standard and commented the code where code wasn't self explanatory.\n\n# what i learned from this project?\n\nunlike other fields in cs, ml can be daunting sometime. you don't get segfault that often, your code will run fine but you'll not get the desired result/accuracy. these bugs are notoriously difficult to chase and hunt.\n\nthese simple tools really help me speed up my debugging process.\n\n* always visualize the dataset, it'll give some visual insight into the dataset.\n* keep track of matrix dimensions on paper for correctness of your code.\n* after implementing forward propagation you can check its correctness on and/or logic gate.\n* -----> plot !!!  *model accuracy and loss* graph to visualize your model performance.\n* confusion matrix will show overall performance on each output node.\n\n[github link](https://github.com/shahabbngsh/digit_recogniser)\n\nps any feedback, dos and don'ts would be appreciated.\n\ni hope you like it. as an introvert person, i thought a tons of time before posting it here ;)\n\nthanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/iir2qm/digit_recognition_from_scratch_in_python_using/',)", "identifyer": 5746777, "year": "2020"}, {"autor": "SQL_beginner", "date": 1598678844000, "content": "Criticism of the tsne algorithm /!/ https://distill.pub/2016/misread-tsne/\n\nI read this article over here in which the perceived advantages of the tsne algorithm are heavily scrutinized - one of the main one being: on a tsne plot, distances between points may not mean anything.\n\nThis is pretty concerning. If this is true, what are the advantages of using tsne? The article seems to criticize the tsne algorithm pretty badly.\n\nIs it still worth using the tsne algorithm?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/iin1dm/criticism_of_the_tsne_algorithm/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "criticism of the tsne algorithm /!/ https://distill.pub/2016/misread-tsne/\n\ni read this article over here in which the perceived advantages of the tsne algorithm are heavily scrutinized - one of the main one being: on a tsne -----> plot !!! , distances between points may not mean anything.\n\nthis is pretty concerning. if this is true, what are the advantages of using tsne? the article seems to criticize the tsne algorithm pretty badly.\n\nis it still worth using the tsne algorithm?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/iin1dm/criticism_of_the_tsne_algorithm/',)", "identifyer": 5746791, "year": "2020"}, {"autor": "tiltsk8t", "date": 1605543886000, "content": "Moving backwards from keyword extraction? /!/ Hey all still new to machine learning here, \n\nI'm working on a simple keyword extraction for comments pertaining to a sample product. The goal is to use keyword extraction to summarize the issues with said product. \n\nNow, while I'm able to get some useful data, I'm stuck on where to go from here. \n\nMy current goal is to use a a standard plot to show where the keywords originated. However, the thing is. I want to be able to click a keyword on the graph and produce a list of original comments in which the keyword is found. \n\nWhat would be the best approach to this? I can't seem to find good tools or examples for this scenerio.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jv9o69/moving_backwards_from_keyword_extraction/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "moving backwards from keyword extraction? /!/ hey all still new to machine learning here, \n\ni'm working on a simple keyword extraction for comments pertaining to a sample product. the goal is to use keyword extraction to summarize the issues with said product. \n\nnow, while i'm able to get some useful data, i'm stuck on where to go from here. \n\nmy current goal is to use a a standard -----> plot !!!  to show where the keywords originated. however, the thing is. i want to be able to click a keyword on the graph and produce a list of original comments in which the keyword is found. \n\nwhat would be the best approach to this? i can't seem to find good tools or examples for this scenerio.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/jv9o69/moving_backwards_from_keyword_extraction/',)", "identifyer": 5747035, "year": "2020"}, {"autor": "hashtag_kehl", "date": 1582681658000, "content": "Creating a random dataset /!/ I am trying to create a dataset that fits the problem. I really don't know how to generate and plot it in Matplotlib.\n\nOnce I get the data, I think i can do the rest\n\n1. Program your own MLP in Python for a basic neural network with one or to hidden layers and binary output (select the proper activation function)\n\nGenerate a random data set for binary classification where each class correspond to the 2D region depicted in the figure. The random data should have normal distribution with variance \ud835\udf0e2 = 0.08. Use 200 points (100 in each region) to train your neural network and report the results in terms of the loss function and the training epochs.\n\n&amp;#x200B;\n\n[1st linear data](https://preview.redd.it/25zyiur5b6j41.png?width=646&amp;format=png&amp;auto=webp&amp;s=a66a4d303601091e3b9ddfea8653290c4e96912d)\n\n&amp;#x200B;\n\n[second non-linear problem](https://preview.redd.it/jfkeht78b6j41.png?width=424&amp;format=png&amp;auto=webp&amp;s=e705c0fc75c2300781ff0c1f8a781a3f2cc8e18b)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f9lfv9/creating_a_random_dataset/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "creating a random dataset /!/ i am trying to create a dataset that fits the problem. i really don't know how to generate and -----> plot !!!  it in matplotlib.\n\nonce i get the data, i think i can do the rest\n\n1. program your own mlp in python for a basic neural network with one or to hidden layers and binary output (select the proper activation function)\n\ngenerate a random data set for binary classification where each class correspond to the 2d region depicted in the figure. the random data should have normal distribution with variance \ud835\udf0e2 = 0.08. use 200 points (100 in each region) to train your neural network and report the results in terms of the loss function and the training epochs.\n\n&amp;#x200b;\n\n[1st linear data](https://preview.redd.it/25zyiur5b6j41.png?width=646&amp;format=png&amp;auto=webp&amp;s=a66a4d303601091e3b9ddfea8653290c4e96912d)\n\n&amp;#x200b;\n\n[second non-linear problem](https://preview.redd.it/jfkeht78b6j41.png?width=424&amp;format=png&amp;auto=webp&amp;s=e705c0fc75c2300781ff0c1f8a781a3f2cc8e18b)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/f9lfv9/creating_a_random_dataset/',)", "identifyer": 5747078, "year": "2020"}, {"autor": "the_sammich_man", "date": 1582594679000, "content": "LinAlgWarning Fix /!/ Hi Peeps,\n\nI'm currently working learning linear regression and came across a problem when implementing the code below. There is a warning \"LinAlgWarning: Ill-conditioned matrix (rcond=1.31404e-21): result may not be accurate.\" How would I go upon fixing that?\n\n    # shape the data for ridge regression.\n    mon_x1 = np.array(mon_group_1.hour).reshape(-1,1) #shape (79,1)\n    mon_y1 = np.array(mon_group_1.monday) #shape(79,)\n    print(mon_x1.shape, mon_y1.shape)\n    \n    # create ridge object.\n    ridge = linear_model.Ridge(alpha = .5)\n    poly = PolynomialFeatures(degree = 11)\n    poly_x = poly.fit_transform(mon_x1)\n    ridge.fit(poly_x, mon_y1)\n    \n    # plot the results.\n    plt.scatter(mon_x1, mon_y1)\n    plt.plot(mon_x1, np.dot(poly_x,ridge.coef_) + ridge.intercept_, c = 'r')", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f92a2a/linalgwarning_fix/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "linalgwarning fix /!/ hi peeps,\n\ni'm currently working learning linear regression and came across a problem when implementing the code below. there is a warning \"linalgwarning: ill-conditioned matrix (rcond=1.31404e-21): result may not be accurate.\" how would i go upon fixing that?\n\n    # shape the data for ridge regression.\n    mon_x1 = np.array(mon_group_1.hour).reshape(-1,1) #shape (79,1)\n    mon_y1 = np.array(mon_group_1.monday) #shape(79,)\n    print(mon_x1.shape, mon_y1.shape)\n    \n    # create ridge object.\n    ridge = linear_model.ridge(alpha = .5)\n    poly = polynomialfeatures(degree = 11)\n    poly_x = poly.fit_transform(mon_x1)\n    ridge.fit(poly_x, mon_y1)\n    \n    # -----> plot !!!  the results.\n    plt.scatter(mon_x1, mon_y1)\n    plt.plot(mon_x1, np.dot(poly_x,ridge.coef_) + ridge.intercept_, c = 'r')", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/f92a2a/linalgwarning_fix/',)", "identifyer": 5747115, "year": "2020"}, {"autor": "raidicy", "date": 1586443402000, "content": "Help plotting weight vs error in \"Grokking Deep Learning\" /!/ Hello. \n\nI am trying to figure out how to plot these [graphs](https://i.imgur.com/O1hbkDi.png) according to this code(below) from the book \"Grokking Deep Learning\". I'm not really well acquainted with `matplotlib`. (Note: I realize the code below can be written with better libraries. It is from the book on page 86)\n\n```Python\ndef neural_network(input, weights):\n out = 0\n for i in range(len(input)):\n  out += (input[i] * weights[i]) # dot product\n return out\n\ndef ele_mul(scalar, vector):\n out = [0,0,0]\n for i in range(len(out)):\n  out[i] = vector[i] * scalar\n return out\n\ntoes = [8.5, 9.5, 9.9, 9.0]\nwlrec = [0.65, 0.8, 0.8, 0.9]\nnfans = [1.2, 1.3, 0.5, 1.0]\nwin_or_lose_binary = [1, 1, 0, 1]\ntrue = win_or_lose_binary[0]\nalpha = 0.01\nweights = [0.1, 0.2, -.1]\ninput = [toes[0],wlrec[0],nfans[0]]\n\nfor iter in range(3):\n pred = neural_network(input,weights)\n error = (pred - true) ** 2\n delta = pred - true\n weight_deltas=ele_mul(delta,input)\n\n for i in range(len(weights)):\n  weights[i]-=alpha*weight_deltas[i]\n```", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fxtml2/help_plotting_weight_vs_error_in_grokking_deep/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "help plotting weight vs error in \"grokking deep learning\" /!/ hello. \n\ni am trying to figure out how to -----> plot !!!  these [graphs](https://i.imgur.com/o1hbkdi.png) according to this code(below) from the book \"grokking deep learning\". i'm not really well acquainted with `matplotlib`. (note: i realize the code below can be written with better libraries. it is from the book on page 86)\n\n```python\ndef neural_network(input, weights):\n out = 0\n for i in range(len(input)):\n  out += (input[i] * weights[i]) # dot product\n return out\n\ndef ele_mul(scalar, vector):\n out = [0,0,0]\n for i in range(len(out)):\n  out[i] = vector[i] * scalar\n return out\n\ntoes = [8.5, 9.5, 9.9, 9.0]\nwlrec = [0.65, 0.8, 0.8, 0.9]\nnfans = [1.2, 1.3, 0.5, 1.0]\nwin_or_lose_binary = [1, 1, 0, 1]\ntrue = win_or_lose_binary[0]\nalpha = 0.01\nweights = [0.1, 0.2, -.1]\ninput = [toes[0],wlrec[0],nfans[0]]\n\nfor iter in range(3):\n pred = neural_network(input,weights)\n error = (pred - true) ** 2\n delta = pred - true\n weight_deltas=ele_mul(delta,input)\n\n for i in range(len(weights)):\n  weights[i]-=alpha*weight_deltas[i]\n```", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/fxtml2/help_plotting_weight_vs_error_in_grokking_deep/',)", "identifyer": 5747156, "year": "2020"}, {"autor": "WisePolicy7", "date": 1586394352000, "content": "Ever wondered what content Netflix picked for us over the years? /!/ Ever wonder what content [**Netflix**](https://www.linkedin.com/company/netflix/) picked for us over the years?  Applying text mining and NLP techniques on multiple datasets, I tried to triangulate the choices [**Netflix**](https://www.linkedin.com/company/netflix/) made in the recent years to curate the content for you. Found a few interesting focus areas in terms of certain genres and plot-themes. Hope you like it! Please let me know if you have any thoughts/comments.\n\n[https://medium.com/@srinivasvadrevu7/deciphering-netflixs-content-strategy-through-nlp-b59cd2e84873](https://medium.com/@srinivasvadrevu7/deciphering-netflixs-content-strategy-through-nlp-b59cd2e84873)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fxivzj/ever_wondered_what_content_netflix_picked_for_us/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "ever wondered what content netflix picked for us over the years? /!/ ever wonder what content [**netflix**](https://www.linkedin.com/company/netflix/) picked for us over the years?  applying text mining and nlp techniques on multiple datasets, i tried to triangulate the choices [**netflix**](https://www.linkedin.com/company/netflix/) made in the recent years to curate the content for you. found a few interesting focus areas in terms of certain genres and -----> plot !!! -themes. hope you like it! please let me know if you have any thoughts/comments.\n\n[https://medium.com/@srinivasvadrevu7/deciphering-netflixs-content-strategy-through-nlp-b59cd2e84873](https://medium.com/@srinivasvadrevu7/deciphering-netflixs-content-strategy-through-nlp-b59cd2e84873)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/fxivzj/ever_wondered_what_content_netflix_picked_for_us/',)", "identifyer": 5747183, "year": "2020"}, {"autor": "blickbeared", "date": 1585975275000, "content": "I'm wanting to get into machine learning, and I have idea but I'm not sure if it's even possible. /!/ Would it be possible to make an AI that writes short novels that keeps a consistent plot and has an understandable plot? If so, what would be the best way to go about this?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/funi8c/im_wanting_to_get_into_machine_learning_and_i/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "i'm wanting to get into machine learning, and i have idea but i'm not sure if it's even possible. /!/ would it be possible to make an ai that writes short novels that keeps a consistent -----> plot !!!  and has an understandable -----> plot !!! ? if so, what would be the best way to go about this?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/funi8c/im_wanting_to_get_into_machine_learning_and_i/',)", "identifyer": 5747257, "year": "2020"}, {"autor": "starzmustdie", "date": 1585917718000, "content": "Should I impute the missing values of timeseries data? /!/ I have the following task - predicting the next 12 hours of PM10 particles based on historical data of previous 24 hours of PM10, O3 (ozone) and CO (carbon monoxide). \n\nWhen I plot the present vs missing values in the dataset, this is what I get (blue = present, white = missing):\n\n[dataset](https://preview.redd.it/8924slhcklq41.jpg?width=1089&amp;format=pjpg&amp;auto=webp&amp;s=86418a1254c3e51785eb405b618dc181cf466492)\n\nMy question is, should I impute the missing values in the dataset?\n\nIf I do end up imputing the values, won't that have an effect on the performance of the model, since PM10 in my case is both input and target variable. \n\nOn the other hand in case I drop the values, then I will have a lot of work in constructing the dataset so that the previous 24 hours and the following 12 hours for each timestamp are consecutive timesteps (there should be no overlap because of truncation).", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fu7qbp/should_i_impute_the_missing_values_of_timeseries/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "should i impute the missing values of timeseries data? /!/ i have the following task - predicting the next 12 hours of pm10 particles based on historical data of previous 24 hours of pm10, o3 (ozone) and co (carbon monoxide). \n\nwhen i -----> plot !!!  the present vs missing values in the dataset, this is what i get (blue = present, white = missing):\n\n[dataset](https://preview.redd.it/8924slhcklq41.jpg?width=1089&amp;format=pjpg&amp;auto=webp&amp;s=86418a1254c3e51785eb405b618dc181cf466492)\n\nmy question is, should i impute the missing values in the dataset?\n\nif i do end up imputing the values, won't that have an effect on the performance of the model, since pm10 in my case is both input and target variable. \n\non the other hand in case i drop the values, then i will have a lot of work in constructing the dataset so that the previous 24 hours and the following 12 hours for each timestamp are consecutive timesteps (there should be no overlap because of truncation).", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/fu7qbp/should_i_impute_the_missing_values_of_timeseries/',)", "identifyer": 5747289, "year": "2020"}, {"autor": "ionezation", "date": 1577911316000, "content": "Scatter Plot /!/ I am trying to generate a Scatter Plot with this but its giving error that \n\n&amp;#x200B;\n\n ValueError: x and y must be the same size \n\n&amp;#x200B;\n\n`plt.scatter(X, y);`\n\n`plt.plot(X_test, y_pred, color='red')`\n\n`plt.title('IRIS DATASET')`\n\n`plt.xlabel('X Values')`\n\n`plt.ylabel('Y Values')`\n\n[`plt.show`](https://plt.show)`()`\n\n&amp;#x200B;\n\nEven I tried to change the X with iloc functions :/ \n\nI tried these\n\nX.iloc\\[:,0:\\].values\n\nX = ds.iloc\\[:,0:0\\].values\n\n&amp;#x200B;\n\nPlease, guide how I can make a scatter plot with a  DataFrame ?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/eingi7/scatter_plot/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "scatter -----> plot !!!  /!/ i am trying to generate a scatter plot with this but its giving error that \n\n&amp;#x200b;\n\n valueerror: x and y must be the same size \n\n&amp;#x200b;\n\n`plt.scatter(x, y);`\n\n`plt.plot(x_test, y_pred, color='red')`\n\n`plt.title('iris dataset')`\n\n`plt.xlabel('x values')`\n\n`plt.ylabel('y values')`\n\n[`plt.show`](https://plt.show)`()`\n\n&amp;#x200b;\n\neven i tried to change the x with iloc functions :/ \n\ni tried these\n\nx.iloc\\[:,0:\\].values\n\nx = ds.iloc\\[:,0:0\\].values\n\n&amp;#x200b;\n\nplease, guide how i can make a scatter plot with a  dataframe ?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/eingi7/scatter_plot/',)", "identifyer": 5747373, "year": "2020"}, {"autor": "TauntArrow", "date": 1595410853000, "content": "How to plot a graph for a linear regression using normal equation, using Octave? /!/ Is it possible to plot a graph for a normal equation variables?\nIf so how?\nI am unable to build the intuition or the syntaxes required for making so..\n\nThis is no homework,I just wanted to learn the extras,pls help me on this,code is based on Andrew Ng's Coursera week 2 optional assignment.\nCodes are available on here:\n [GitHub](https://github.com/hangim/machine-learning-ex1/tree/master/ex1)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hvquu1/how_to_plot_a_graph_for_a_linear_regression_using/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "how to -----> plot !!!  a graph for a linear regression using normal equation, using octave? /!/ is it possible to plot a graph for a normal equation variables?\nif so how?\ni am unable to build the intuition or the syntaxes required for making so..\n\nthis is no homework,i just wanted to learn the extras,pls help me on this,code is based on andrew ng's coursera week 2 optional assignment.\ncodes are available on here:\n [github](https://github.com/hangim/machine-learning-ex1/tree/master/ex1)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/hvquu1/how_to_plot_a_graph_for_a_linear_regression_using/',)", "identifyer": 5747588, "year": "2020"}, {"autor": "pythonistaaaaaaa", "date": 1579366317000, "content": "[Beginner] How to find the weight vector of a perceptron? /!/ Hi,\n\nI'm preparing for an exam and I have some problems with this question:   \n\n&gt; Construct a perceptron able to separate the points: &lt;1,1,0&gt;, &lt;2,3,1&gt; where the last element\n&gt; is the class. \n\nI first plot those elements, and I can then decide that, for this example, the dataset is linearly separable with boundary y=2.\n\nBut then, how do I calculate the weight vector? (The solution is &lt;\u22122,0,1&gt;)\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/eqirxf/beginner_how_to_find_the_weight_vector_of_a/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "[beginner] how to find the weight vector of a perceptron? /!/ hi,\n\ni'm preparing for an exam and i have some problems with this question:   \n\n&gt; construct a perceptron able to separate the points: &lt;1,1,0&gt;, &lt;2,3,1&gt; where the last element\n&gt; is the class. \n\ni first -----> plot !!!  those elements, and i can then decide that, for this example, the dataset is linearly separable with boundary y=2.\n\nbut then, how do i calculate the weight vector? (the solution is &lt;\u22122,0,1&gt;)\n\nthanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/eqirxf/beginner_how_to_find_the_weight_vector_of_a/',)", "identifyer": 5747654, "year": "2020"}, {"autor": "mutt1771", "date": 1605042817000, "content": "How to create a model that takes FFT data from a speech sound and identifies the vowel sound /!/ I would like to create a web app that takes microphone input and then tells the user the vowel sound they are making. For example, the user says \"Ah\" and the app sends back the International Phonetic Alphabet symbol for that sound. Or even better, it shows where the users vowel might be on a vowel chart (you can google vowel diagram) and moves as they change the sound. \n\nMy idea is to use FFTs of the input sound and use ML to identify the vowel sound. Then the ML model could classify the sound as a certain vowel based on the FFT.\n\n\\- What would be the best architecture to do this?\n\n\\- Would it be feasible to plot the relative closeness of a vowel sound on a vowel chart? So if the sound is in between two different sounds, it might show in between on a chart, and the more similar it is to a sound, the closer it will get to it. Would the output probabilities be a reliable way to do this?\n\n\\- Since this will be a web app, is there any particular library that would be best to train my model so that I can then use it with Tensorflow js?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jrtr79/how_to_create_a_model_that_takes_fft_data_from_a/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "how to create a model that takes fft data from a speech sound and identifies the vowel sound /!/ i would like to create a web app that takes microphone input and then tells the user the vowel sound they are making. for example, the user says \"ah\" and the app sends back the international phonetic alphabet symbol for that sound. or even better, it shows where the users vowel might be on a vowel chart (you can google vowel diagram) and moves as they change the sound. \n\nmy idea is to use ffts of the input sound and use ml to identify the vowel sound. then the ml model could classify the sound as a certain vowel based on the fft.\n\n\\- what would be the best architecture to do this?\n\n\\- would it be feasible to -----> plot !!!  the relative closeness of a vowel sound on a vowel chart? so if the sound is in between two different sounds, it might show in between on a chart, and the more similar it is to a sound, the closer it will get to it. would the output probabilities be a reliable way to do this?\n\n\\- since this will be a web app, is there any particular library that would be best to train my model so that i can then use it with tensorflow js?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/jrtr79/how_to_create_a_model_that_takes_fft_data_from_a/',)", "identifyer": 5747798, "year": "2020"}, {"autor": "dogkarl", "date": 1578430581000, "content": "Advanced python data visualisation course /!/ Looking for something that teaches you how to effectively convey insights without over-complicating your plots and distracting the viewer; rather than, for example, a course that just shows you the code to make a violin plot. Have experience with matplotlib and seaborn (as well as gg plot in R). Thanks in advance :)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/elhlsf/advanced_python_data_visualisation_course/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "advanced python data visualisation course /!/ looking for something that teaches you how to effectively convey insights without over-complicating your plots and distracting the viewer; rather than, for example, a course that just shows you the code to make a violin -----> plot !!! . have experience with matplotlib and seaborn (as well as gg plot in r). thanks in advance :)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/elhlsf/advanced_python_data_visualisation_course/',)", "identifyer": 5747944, "year": "2020"}, {"autor": "tomk23_reddit", "date": 1604089367000, "content": "Residual plot? /!/ It seems after we fit the data, it is important to do residual plot. So what is exactly does residual plot do?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jl4gf8/residual_plot/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "residual -----> plot !!! ? /!/ it seems after we fit the data, it is important to do residual plot. so what is exactly does residual plot do?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/jl4gf8/residual_plot/',)", "identifyer": 5748278, "year": "2020"}, {"autor": "BolshevikPower", "date": 1603402666000, "content": "Improving model fitting based on multiple data sets /!/ Hi all - \n\nHave a project I'm working on and am running into an issue. Essentially I these points scattered across an x / y plot. I have one test point, where I get the target data (y) for the classification (number from 1 - 6). I have lots points where I have depth indexed data, with some features. The issue with these points is that I don't get a lot of data per point (maybe 100 points).\n\nI'm using the point closest to the test point to fit the model, then trying to generalize that to the other points that are farther apart. It's not giving me great results.\n\nI understand there's not a lot of data to fit to so I'm trying to improve the model by adding a set of 'k' points close to the test point.\n\nThese points all share the same columns, so I've tried to add vertically, but then my indexes don't match with the predictor variable y.\n\nI've tried to concat them at the end using a suffix denoting the specific point id, but then I get an error about the amount of input features (for one point) when I try predicting again with the model using combined features.\n\nIs there another model or technique I can use for this?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jg991a/improving_model_fitting_based_on_multiple_data/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "improving model fitting based on multiple data sets /!/ hi all - \n\nhave a project i'm working on and am running into an issue. essentially i these points scattered across an x / y -----> plot !!! . i have one test point, where i get the target data (y) for the classification (number from 1 - 6). i have lots points where i have depth indexed data, with some features. the issue with these points is that i don't get a lot of data per point (maybe 100 points).\n\ni'm using the point closest to the test point to fit the model, then trying to generalize that to the other points that are farther apart. it's not giving me great results.\n\ni understand there's not a lot of data to fit to so i'm trying to improve the model by adding a set of 'k' points close to the test point.\n\nthese points all share the same columns, so i've tried to add vertically, but then my indexes don't match with the predictor variable y.\n\ni've tried to concat them at the end using a suffix denoting the specific point id, but then i get an error about the amount of input features (for one point) when i try predicting again with the model using combined features.\n\nis there another model or technique i can use for this?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/jg991a/improving_model_fitting_based_on_multiple_data/',)", "identifyer": 5748409, "year": "2020"}, {"autor": "GoBacksIn", "date": 1581797171000, "content": "How can I predicting future values with Tensorflow LSTM? /!/ Thank you for reading Sir. I'm not good at English. The introduction will be short.\n\n&amp;#x200B;\n\nafter training, I want to forecast the future for time series.\n\nI am wondering how to predict and get future time series data after model training\n\nSo, I tried\n\n    a = y_val[-look_back:] \n    for i in range(N-step prediction): #predict a new value n times.\n        tmp = model.predict(a.reshape(-1, look_back, num_feature)) #predicted value     \n        a = a[1:] #remove first     \n        a = np.append(a, tmp) #insert predicted value\n\nBut the results is...\n\nEach time run the code, the result plot to shape of y = ax or y = -ax.\n\nlike below\n\n&amp;#x200B;\n\n[result 1](https://preview.redd.it/9555eqmh95h41.png?width=405&amp;format=png&amp;auto=webp&amp;s=07f3e3130f05aad127a482462969400e301e2821)\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n[result 2](https://preview.redd.it/xvwazsug95h41.png?width=409&amp;format=png&amp;auto=webp&amp;s=c453a1d3da850b5786bb568341845c09afaa9171)\n\n&amp;#x200B;\n\nI wonder if this is the right way to do this.\n\n&amp;#x200B;\n\nPython 3.6\n\nTensorflow 2.1.0\n\n&amp;#x200B;\n\nfull source is\n\n    def create_dataset(signal_data, look_back=1):\n        dataX, dataY = [], []\n        for i in range(len(signal_data) - look_back):\n            dataX.append(signal_data[i:(i + look_back), :])\n            dataY.append(signal_data[i + look_back, -1])\n        return np.array(dataX), np.array(dataY)\n    \n    look_back = 20\n    \n    df = pd.read_csv('kospi.csv')\n    \n    signal_data = df[[\"close\"]].values.astype('float32')\n    \n    \n    train_size = int(len(signal_data) * 0.80)\n    test_size = len(signal_data) - train_size - int(len(signal_data) * 0.05)\n    val_size = len(signal_data) - train_size - test_size\n    train = signal_data[0:train_size]\n    val = signal_data[train_size:train_size + val_size]\n    test = signal_data[train_size + val_size:len(signal_data)]\n    \n    \n    x_train, y_train = create_dataset(train, look_back)\n    x_test, y_test = create_dataset(test, look_back)\n    x_val, y_val = create_dataset(val, look_back)\n    \n    \n    model = Sequential([\n        layers.LSTM(20, input_shape=(None, 1)),\n        layers.Dense(1)\n    ])\n    model.compile(optimizer='adam', loss='mae')\n    \n    history = model.fit(x_train, y_train, epochs=50, batch_size=64, verbose=0, validation_data=(x_val, y_val))\n    \n    a = y_val[-look_back:] \n    for i in range(N):\n        tmp = model.predict(a.reshape(-1, look_back, num_feature)) #predicted value\n        a = a[1:] #remove first\n        a = np.append(a, tmp) #insert predicted value", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f4euhh/how_can_i_predicting_future_values_with/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "how can i predicting future values with tensorflow lstm? /!/ thank you for reading sir. i'm not good at english. the introduction will be short.\n\n&amp;#x200b;\n\nafter training, i want to forecast the future for time series.\n\ni am wondering how to predict and get future time series data after model training\n\nso, i tried\n\n    a = y_val[-look_back:] \n    for i in range(n-step prediction): #predict a new value n times.\n        tmp = model.predict(a.reshape(-1, look_back, num_feature)) #predicted value     \n        a = a[1:] #remove first     \n        a = np.append(a, tmp) #insert predicted value\n\nbut the results is...\n\neach time run the code, the result -----> plot !!!  to shape of y = ax or y = -ax.\n\nlike below\n\n&amp;#x200b;\n\n[result 1](https://preview.redd.it/9555eqmh95h41.png?width=405&amp;format=png&amp;auto=webp&amp;s=07f3e3130f05aad127a482462969400e301e2821)\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n[result 2](https://preview.redd.it/xvwazsug95h41.png?width=409&amp;format=png&amp;auto=webp&amp;s=c453a1d3da850b5786bb568341845c09afaa9171)\n\n&amp;#x200b;\n\ni wonder if this is the right way to do this.\n\n&amp;#x200b;\n\npython 3.6\n\ntensorflow 2.1.0\n\n&amp;#x200b;\n\nfull source is\n\n    def create_dataset(signal_data, look_back=1):\n        datax, datay = [], []\n        for i in range(len(signal_data) - look_back):\n            datax.append(signal_data[i:(i + look_back), :])\n            datay.append(signal_data[i + look_back, -1])\n        return np.array(datax), np.array(datay)\n    \n    look_back = 20\n    \n    df = pd.read_csv('kospi.csv')\n    \n    signal_data = df[[\"close\"]].values.astype('float32')\n    \n    \n    train_size = int(len(signal_data) * 0.80)\n    test_size = len(signal_data) - train_size - int(len(signal_data) * 0.05)\n    val_size = len(signal_data) - train_size - test_size\n    train = signal_data[0:train_size]\n    val = signal_data[train_size:train_size + val_size]\n    test = signal_data[train_size + val_size:len(signal_data)]\n    \n    \n    x_train, y_train = create_dataset(train, look_back)\n    x_test, y_test = create_dataset(test, look_back)\n    x_val, y_val = create_dataset(val, look_back)\n    \n    \n    model = sequential([\n        layers.lstm(20, input_shape=(none, 1)),\n        layers.dense(1)\n    ])\n    model.compile(optimizer='adam', loss='mae')\n    \n    history = model.fit(x_train, y_train, epochs=50, batch_size=64, verbose=0, validation_data=(x_val, y_val))\n    \n    a = y_val[-look_back:] \n    for i in range(n):\n        tmp = model.predict(a.reshape(-1, look_back, num_feature)) #predicted value\n        a = a[1:] #remove first\n        a = np.append(a, tmp) #insert predicted value", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/f4euhh/how_can_i_predicting_future_values_with/',)", "identifyer": 5748493, "year": "2020"}, {"autor": "sdbhavsar3", "date": 1581772972000, "content": "From the lag plot, is the AutoCorrelation model good for prediction of next numberseries. /!/ I have read autocorrelation  gives good prediction result if the points in lag plot are aligned towards diagonal. This doesn't seem to be the case here. So, Is autocorrelation not good model for this case? If not, what are good models that can be used for short series.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f49c5w/from_the_lag_plot_is_the_autocorrelation_model/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "from the lag -----> plot !!! , is the autocorrelation model good for prediction of next numberseries. /!/ i have read autocorrelation  gives good prediction result if the points in lag plot are aligned towards diagonal. this doesn't seem to be the case here. so, is autocorrelation not good model for this case? if not, what are good models that can be used for short series.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/f49c5w/from_the_lag_plot_is_the_autocorrelation_model/',)", "identifyer": 5748502, "year": "2020"}, {"autor": "GoBacksIn", "date": 1581009846000, "content": "Is the right way to predict time series data? /!/  Thank you for reading Sir. I'm not good at English. The introduction will be short.\n\n&amp;#x200B;\n\nafter training, I want to forecast the future for time series.\n\nI am wondering how to predict and get future time series data after model training\n\nSo, I tried\n\n \n\n    a = y_val[-look_back:] \n    for i in range(N-step prediction): #predict a new value n times.\n        tmp = model.predict(a.reshape(-1, look_back, num_feature)) #predicted value     \n        a = a[1:] #remove first     \n        a = np.append(a, tmp) #insert predicted value\n    \n\nBut the results is...\n\nEach time run the code, the result plot to shape of y = ax or y = -ax.\n\nlike below\n\n&amp;#x200B;\n\n[result 1](https://preview.redd.it/cmf5qegn7cf41.png?width=405&amp;format=png&amp;auto=webp&amp;s=dd586daede4506a3ed52ad4744b3cc1f0b06e858)\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n[result 2](https://preview.redd.it/wvtsxyoq7cf41.png?width=409&amp;format=png&amp;auto=webp&amp;s=2b1259e01ff93e444fa6141c34268275ce7732ec)\n\nI wonder if this is the right way to do this.\n\n&amp;#x200B;\n\nPython 3.6\n\nTensorflow 2.1.0", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ezvl1x/is_the_right_way_to_predict_time_series_data/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "is the right way to predict time series data? /!/  thank you for reading sir. i'm not good at english. the introduction will be short.\n\n&amp;#x200b;\n\nafter training, i want to forecast the future for time series.\n\ni am wondering how to predict and get future time series data after model training\n\nso, i tried\n\n \n\n    a = y_val[-look_back:] \n    for i in range(n-step prediction): #predict a new value n times.\n        tmp = model.predict(a.reshape(-1, look_back, num_feature)) #predicted value     \n        a = a[1:] #remove first     \n        a = np.append(a, tmp) #insert predicted value\n    \n\nbut the results is...\n\neach time run the code, the result -----> plot !!!  to shape of y = ax or y = -ax.\n\nlike below\n\n&amp;#x200b;\n\n[result 1](https://preview.redd.it/cmf5qegn7cf41.png?width=405&amp;format=png&amp;auto=webp&amp;s=dd586daede4506a3ed52ad4744b3cc1f0b06e858)\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n[result 2](https://preview.redd.it/wvtsxyoq7cf41.png?width=409&amp;format=png&amp;auto=webp&amp;s=2b1259e01ff93e444fa6141c34268275ce7732ec)\n\ni wonder if this is the right way to do this.\n\n&amp;#x200b;\n\npython 3.6\n\ntensorflow 2.1.0", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/ezvl1x/is_the_right_way_to_predict_time_series_data/',)", "identifyer": 5748658, "year": "2020"}, {"autor": "Dr_Boltzmann_Gauss", "date": 1597069204000, "content": "Cross Validation function in R /!/  \n\n# \n\nHi, I am trying to cross validate my model and plot the predicted against the actual, can anyone help?\n\ni keep gettign this error message, probably coming from the NA observations in the variable observation collumns\n\nedit - how could I turn this into a cross validation function, which I can apply to any model\n\nfor(i in 3){\n\ncross.val = sample(1:nrow(dat.df), 0.9\\*nrow(dat.df), replace=FALSE)\n\ntraining.set = dat.df\\[cross.val, \\]\n\ntest.set = dat.df\\[-cross.val, \\]\n\n\\# fit the model\n\ncv.Income.lm = lm(Income\\~unemployment\\*new.benefits+new.rel.stat+sex+new.finances+new.tenure+new.region+new.russell+new.ethnicity+new.children, data=training.set)\n\npred.val.set = data.frame(predicted=predict(cv.Income.lm, test.set), original = test.set$Income, error = (predict(cv.Income.lm, test.set)-test.set$Income))\n\nif(i==1){\n\np1 = ggplot(data=pred.val.set, aes(y=original, y=predicted))+geom\\_point()+theme\\_bw()+geom\\_smooth(method=\"lm\", se= FALSE)+geom\\_abline(slope=1,intercept=0, linetype=\"dashed\")\n\np2 = ggplot(data=pred.val.set, aes(y=error, x=predicted))+geom\\_point()+theme\\_bw()\n\n}else{\n\nif(i==2){\n\np1 = p1 + geom\\_point(data=pred.val.set, aes(y=original, x=predicted), color=\"red\")+geom\\_smooth(method=\"lm\", se=FALSE, color=\"darkred\")\n\np2 = p2+geom\\_point(data=pred.val.set, aes(y=error, x=predicted), color=\"red\")\n\n}else{\n\np1=p1+geom\\_point(data=pred.val.set, aes(y=original, x=predicted),color=\"green\")\n\np1=p1+geom\\_smooth(method=\"lm\", se=FALSE, color=\"darkgreen\")\n\np2=p2+geom\\_point(data=pred.val.set, aes(y=error, x=predicted),color=\"green\")\n\np2=p2+geom\\_abline(slope=0,intercept=sd(pred.val.set$error),linetype=\"dashed\")\n\np2=p2+geom\\_abline(slope=0, intercept=0)\n\np2=p2+geom\\_abline(slope=0, intercept=-sd(pred.val.set$error), linetype=\"dashed\")\n\n}}}\n\ngrid.arrange(p1,p2,nrow=1)\n\n\\&gt;Error in model.frame.default(formula = Income \\~ unemployment \\* new.benefits + : variable lengths differ (found for 'new.benefits')", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i75o6t/cross_validation_function_in_r/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "cross validation function in r /!/  \n\n# \n\nhi, i am trying to cross validate my model and -----> plot !!!  the predicted against the actual, can anyone help?\n\ni keep gettign this error message, probably coming from the na observations in the variable observation collumns\n\nedit - how could i turn this into a cross validation function, which i can apply to any model\n\nfor(i in 3){\n\ncross.val = sample(1:nrow(dat.df), 0.9\\*nrow(dat.df), replace=false)\n\ntraining.set = dat.df\\[cross.val, \\]\n\ntest.set = dat.df\\[-cross.val, \\]\n\n\\# fit the model\n\ncv.income.lm = lm(income\\~unemployment\\*new.benefits+new.rel.stat+sex+new.finances+new.tenure+new.region+new.russell+new.ethnicity+new.children, data=training.set)\n\npred.val.set = data.frame(predicted=predict(cv.income.lm, test.set), original = test.set$income, error = (predict(cv.income.lm, test.set)-test.set$income))\n\nif(i==1){\n\np1 = ggplot(data=pred.val.set, aes(y=original, y=predicted))+geom\\_point()+theme\\_bw()+geom\\_smooth(method=\"lm\", se= false)+geom\\_abline(slope=1,intercept=0, linetype=\"dashed\")\n\np2 = ggplot(data=pred.val.set, aes(y=error, x=predicted))+geom\\_point()+theme\\_bw()\n\n}else{\n\nif(i==2){\n\np1 = p1 + geom\\_point(data=pred.val.set, aes(y=original, x=predicted), color=\"red\")+geom\\_smooth(method=\"lm\", se=false, color=\"darkred\")\n\np2 = p2+geom\\_point(data=pred.val.set, aes(y=error, x=predicted), color=\"red\")\n\n}else{\n\np1=p1+geom\\_point(data=pred.val.set, aes(y=original, x=predicted),color=\"green\")\n\np1=p1+geom\\_smooth(method=\"lm\", se=false, color=\"darkgreen\")\n\np2=p2+geom\\_point(data=pred.val.set, aes(y=error, x=predicted),color=\"green\")\n\np2=p2+geom\\_abline(slope=0,intercept=sd(pred.val.set$error),linetype=\"dashed\")\n\np2=p2+geom\\_abline(slope=0, intercept=0)\n\np2=p2+geom\\_abline(slope=0, intercept=-sd(pred.val.set$error), linetype=\"dashed\")\n\n}}}\n\ngrid.arrange(p1,p2,nrow=1)\n\n\\&gt;error in model.frame.default(formula = income \\~ unemployment \\* new.benefits + : variable lengths differ (found for 'new.benefits')", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/i75o6t/cross_validation_function_in_r/',)", "identifyer": 5749077, "year": "2020"}, {"autor": "lifesaboxofchoco", "date": 1596819690000, "content": "At what gradient value would you say your model is suffering from exploding gradient problem? /!/ I am currently fine-tuning a GPT2 model and am plotting some distribution plot of the distribution. Most of the gradients are in the range of -200 to 200. The training loss is decreasing although very slowly.  \n\n\nHow wold you diagnose whether you are suffering from an exploding gradient problem?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i5hjj1/at_what_gradient_value_would_you_say_your_model/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "at what gradient value would you say your model is suffering from exploding gradient problem? /!/ i am currently fine-tuning a gpt2 model and am plotting some distribution -----> plot !!!  of the distribution. most of the gradients are in the range of -200 to 200. the training loss is decreasing although very slowly.  \n\n\nhow wold you diagnose whether you are suffering from an exploding gradient problem?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/i5hjj1/at_what_gradient_value_would_you_say_your_model/',)", "identifyer": 5749151, "year": "2020"}, {"autor": "the_parallax_II", "date": 1589842719000, "content": "How to examine if my ML model overfitted in scikit-learn? /!/ When i was training in PyTorch, at each epoch i could save the train and validation loss and thus i could plot them in order to see if there was an overfitting or underfitting.  Now that i am working in sklearn, the training process is just one line of code.\n\nGiven a trained ML model in sckikit-learn how can you examine after, if that model overfitted?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gmccxq/how_to_examine_if_my_ml_model_overfitted_in/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "how to examine if my ml model overfitted in scikit-learn? /!/ when i was training in pytorch, at each epoch i could save the train and validation loss and thus i could -----> plot !!!  them in order to see if there was an overfitting or underfitting.  now that i am working in sklearn, the training process is just one line of code.\n\ngiven a trained ml model in sckikit-learn how can you examine after, if that model overfitted?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/gmccxq/how_to_examine_if_my_ml_model_overfitted_in/',)", "identifyer": 5749264, "year": "2020"}, {"autor": "ottawalanguages", "date": 1608777733000, "content": "Time Series Forecasting - Am I Basically Scr*wed ? /!/ I have a (very chaotic, stationary confirmed with ADF Test) univariate time series (monthly totals, 300 historical points available ... interested in forecasting the next few future values). I took the natural logarithm of this time series in an effort to stabilize it.\n\nHere are some visuals of this time series:\n\nPlot and Decomposition: [https://imgur.com/a/Cj2MrFM](https://imgur.com/a/Cj2MrFM)\n\nACF and PCF: [https://imgur.com/a/318E1Tk](https://imgur.com/a/318E1Tk)\n\nSpectral Periodogram: [https://imgur.com/a/hBqNk6D](https://imgur.com/a/hBqNk6D)\n\nSince I have taken the natural logarithm of this time series, the error of a candidate forecasting model (e.g. MSE) is deceptive. That is, even if the MSE is low, the absolute difference between the forecasted value and the actual value can be quite high (e.g. predicted = 17, actual = 18 ... e17/ee18 is WAY bigger than 17/18).\n\nDue to the properties of this time series, standard time series models like arima (different orders of arima), exponential smoothing, structural time series models, basic kalman filter, etc ... all provide poor results - both because of the chaotic nature of the time series, as well as because of the log scale (some of these models wont run on the original data, so I took the log).\n\nI am interested in trying GARCH and harmonic regression (spectral decomposition, fourier), but I have a feeling that these wont be much better.\n\nI have resorted to recurrent neural networks to try and improve the forecasting quality, such as Echo State Networks and LSTM networks. But I am also skeptical about their performance.\n\nHas anyone ever dealt with this kind of problem? Or am I basically scr\\*wed?\n\nThanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kj72nd/time_series_forecasting_am_i_basically_scrwed/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "time series forecasting - am i basically scr*wed ? /!/ i have a (very chaotic, stationary confirmed with adf test) univariate time series (monthly totals, 300 historical points available ... interested in forecasting the next few future values). i took the natural logarithm of this time series in an effort to stabilize it.\n\nhere are some visuals of this time series:\n\n-----> plot !!!  and decomposition: [https://imgur.com/a/cj2mrfm](https://imgur.com/a/cj2mrfm)\n\nacf and pcf: [https://imgur.com/a/318e1tk](https://imgur.com/a/318e1tk)\n\nspectral periodogram: [https://imgur.com/a/hbqnk6d](https://imgur.com/a/hbqnk6d)\n\nsince i have taken the natural logarithm of this time series, the error of a candidate forecasting model (e.g. mse) is deceptive. that is, even if the mse is low, the absolute difference between the forecasted value and the actual value can be quite high (e.g. predicted = 17, actual = 18 ... e17/ee18 is way bigger than 17/18).\n\ndue to the properties of this time series, standard time series models like arima (different orders of arima), exponential smoothing, structural time series models, basic kalman filter, etc ... all provide poor results - both because of the chaotic nature of the time series, as well as because of the log scale (some of these models wont run on the original data, so i took the log).\n\ni am interested in trying garch and harmonic regression (spectral decomposition, fourier), but i have a feeling that these wont be much better.\n\ni have resorted to recurrent neural networks to try and improve the forecasting quality, such as echo state networks and lstm networks. but i am also skeptical about their performance.\n\nhas anyone ever dealt with this kind of problem? or am i basically scr\\*wed?\n\nthanks", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/kj72nd/time_series_forecasting_am_i_basically_scrwed/',)", "identifyer": 5749591, "year": "2020"}, {"autor": "SQL_beginner", "date": 1608761571000, "content": "understanding what exactly is a \"decision boundary\" /!/ https://michael.hahsler.net/SMU/EMIS7332/R/viz_classifier.html\n\nHere, the author shows how to plot decision boundaries. Decision boundaries seem to represent what the classifier (e.g. random forest) is doing in the background.\n\nMy question : how exactly is the decision boundary (the black lines) made?\n\nFrom the website:\n\n\"The following plot adds the decision boundary by evaluating the classifier at evenly spaced grid points\"\n\nIf you look at the first example, with the iris dataset and the knn (k nearest neighbor) classifier: i understand that if you take two variables (e.g. sepal length and sepal width), you can plot each observation from the dataset. \n\nFrom here on, i am confused. You can now take many different combinations of (sepal length, sepal wdth) and \"give \" them to the classifier. \n\nBut how are the dark lines plotted?\n\nThanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kj2jb3/understanding_what_exactly_is_a_decision_boundary/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "understanding what exactly is a \"decision boundary\" /!/ https://michael.hahsler.net/smu/emis7332/r/viz_classifier.html\n\nhere, the author shows how to -----> plot !!!  decision boundaries. decision boundaries seem to represent what the classifier (e.g. random forest) is doing in the background.\n\nmy question : how exactly is the decision boundary (the black lines) made?\n\nfrom the website:\n\n\"the following plot adds the decision boundary by evaluating the classifier at evenly spaced grid points\"\n\nif you look at the first example, with the iris dataset and the knn (k nearest neighbor) classifier: i understand that if you take two variables (e.g. sepal length and sepal width), you can plot each observation from the dataset. \n\nfrom here on, i am confused. you can now take many different combinations of (sepal length, sepal wdth) and \"give \" them to the classifier. \n\nbut how are the dark lines plotted?\n\nthanks", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/kj2jb3/understanding_what_exactly_is_a_decision_boundary/',)", "identifyer": 5749597, "year": "2020"}, {"autor": "Unknown_chap", "date": 1608659923000, "content": "Feature selection for the regression model /!/ Dear folks,\n\nI am doing a regression model for a company and to be honest, I am new in this field of study, thus I need your help. I have done a reg plot for each one of my features versus the target, some features have a linear relationship with the target, but some don't have.\n\nhttps://preview.redd.it/qu3ai3b72s661.png?width=380&amp;format=png&amp;auto=webp&amp;s=643ee2a5f722e4a84eb0c6112cc1df7011165163\n\nShould I remove this feature? what are the rules?\n\nCheers", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kiaer3/feature_selection_for_the_regression_model/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "feature selection for the regression model /!/ dear folks,\n\ni am doing a regression model for a company and to be honest, i am new in this field of study, thus i need your help. i have done a reg -----> plot !!!  for each one of my features versus the target, some features have a linear relationship with the target, but some don't have.\n\nhttps://preview.redd.it/qu3ai3b72s661.png?width=380&amp;format=png&amp;auto=webp&amp;s=643ee2a5f722e4a84eb0c6112cc1df7011165163\n\nshould i remove this feature? what are the rules?\n\ncheers", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 6, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/kiaer3/feature_selection_for_the_regression_model/',)", "identifyer": 5749646, "year": "2020"}, {"autor": "daniel-data", "date": 1608656333000, "content": "Using Predictive Power Score to Pinpoint Non-linear Correlations /!/ ## Correlations\n\nIn statistics, correlation or dependence is any statistical relationship, whether causal or not, between two random variables or bivariate data. In the broadest sense, correlation is any statistical association, although it commonly refers to the degree to which a pair of variables are related linearly.\n\nKnown examples of dependent phenomena include the correlation between the height of parents and their children and the correlation between the price of a good and the quantity that consumers are willing to buy, as represented by the so-called demand curve. Correlations are useful because they can indicate a predictive relationship that can be exploited in practice.\n\nFor example, a electric utility may produce less energy on a warm day based on the correlation between electricity demand and climate. In this example, there is a causal relationship because extreme weather causes people to use more electricity to heat or cool themselves\n\nHowever, in general, the presence of a correlation is not sufficient to infer the presence of a causal relationship (i.e., correlation does not imply causality). Formally, random variables are dependent if they do not satisfy a mathematical property of probabilistic independence. In informal language, correlation is synonymous with dependence.\n\nEssentially, correlation is the measure of how two or more variables relate to each other. There are several correlation coefficients. The most common of these is **Pearson's correlation coefficient**, which is sensitive only to a linear relationship between two variables (which may be present even when one variable is a non-linear function of the other)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/1iihbml9mr661.png?width=986&amp;format=png&amp;auto=webp&amp;s=a4c86cb7cd068e488c00f23cbcfa4218be708f52\n\n&amp;#x200B;\n\nOther correlation coefficients - such as Spearman's range correlation - have been developed to be more robust than Pearson's, i.e. more sensitive to non-linear relationships. Mutual information can also be applied to measure the dependence between two variables. Here we can see correlations with a value of 0, but that there is indeed some kind of correlation:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/vcv8rlaamr661.png?width=1234&amp;format=png&amp;auto=webp&amp;s=aba92be692afdbec906aa1e97bb58ebab4484729\n\n&amp;#x200B;\n\nCorrelations are scored from -1 to 1 and indicate whether there is a strong linear relationship - either in a positive or negative direction. However, there are many non-linear relationships that this type of score simply will not detect. In addition, the correlation is only defined for the numerical columns. So, we leave out all the categorical columns.\n\nThe same will happen if you transform the categorical columns, because they are not ordinal and if we do OneHotEncodingwe will end up with an array with many different values (with high cardinality). The symmetry in the correlations means that the correlation is the same whether we calculate the correlation of A and B or the correlation of B and A. However, relationships in the real world are rarely symmetrical. More often, relationships are asymmetrical\n\nA quick example: a column with 2 unique values (Trueor Falsefor example) will never be able to perfectly predict another column with 100unique values. But the opposite could be true. Clearly, asymmetry is important because it is very common in the real world.\n\nHave you ever asked:\n\n* Is there a score that tells us if there is any relationship between two columns - no matter if the relationship is linear, non-linear, Gaussian, or some other type of relationship?\n* Of course, the score should be asymmetrical because I want to detect all the strange relationships between two variables.\n* The score should be 0if there is no relationship and the score should be 1if there is a perfect relationship\n* And that the score helps to answer the question Are there correlations between the columns? with a correlation matrix, then you make a scatter plot over the two columns to compare them and see if there is indeed a strong correlation.\n* And like the icing on the cake, the score should be able to handle both categorical and numerical columns by default.\n\nIn short, an asymmetric and data-type agnostic score for predictive relationships between two columns ranging from 0to 1. Well, there is the Predictive Power Score library and it can be found at the following link: [Predictive Power Score](https://github.com/8080labs/ppscore)\n\nRead all here: [https://www.narrativetext.co/data-hub/using-predictive-power-score-to-pinpoint-non-linear-correlations](https://www.narrativetext.co/data-hub/using-predictive-power-score-to-pinpoint-non-linear-correlations)\n\nSo, let's work the library on this notebook!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ki96p6/using_predictive_power_score_to_pinpoint/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "using predictive power score to pinpoint non-linear correlations /!/ ## correlations\n\nin statistics, correlation or dependence is any statistical relationship, whether causal or not, between two random variables or bivariate data. in the broadest sense, correlation is any statistical association, although it commonly refers to the degree to which a pair of variables are related linearly.\n\nknown examples of dependent phenomena include the correlation between the height of parents and their children and the correlation between the price of a good and the quantity that consumers are willing to buy, as represented by the so-called demand curve. correlations are useful because they can indicate a predictive relationship that can be exploited in practice.\n\nfor example, a electric utility may produce less energy on a warm day based on the correlation between electricity demand and climate. in this example, there is a causal relationship because extreme weather causes people to use more electricity to heat or cool themselves\n\nhowever, in general, the presence of a correlation is not sufficient to infer the presence of a causal relationship (i.e., correlation does not imply causality). formally, random variables are dependent if they do not satisfy a mathematical property of probabilistic independence. in informal language, correlation is synonymous with dependence.\n\nessentially, correlation is the measure of how two or more variables relate to each other. there are several correlation coefficients. the most common of these is **pearson's correlation coefficient**, which is sensitive only to a linear relationship between two variables (which may be present even when one variable is a non-linear function of the other)\n\n&amp;#x200b;\n\nhttps://preview.redd.it/1iihbml9mr661.png?width=986&amp;format=png&amp;auto=webp&amp;s=a4c86cb7cd068e488c00f23cbcfa4218be708f52\n\n&amp;#x200b;\n\nother correlation coefficients - such as spearman's range correlation - have been developed to be more robust than pearson's, i.e. more sensitive to non-linear relationships. mutual information can also be applied to measure the dependence between two variables. here we can see correlations with a value of 0, but that there is indeed some kind of correlation:\n\n&amp;#x200b;\n\nhttps://preview.redd.it/vcv8rlaamr661.png?width=1234&amp;format=png&amp;auto=webp&amp;s=aba92be692afdbec906aa1e97bb58ebab4484729\n\n&amp;#x200b;\n\ncorrelations are scored from -1 to 1 and indicate whether there is a strong linear relationship - either in a positive or negative direction. however, there are many non-linear relationships that this type of score simply will not detect. in addition, the correlation is only defined for the numerical columns. so, we leave out all the categorical columns.\n\nthe same will happen if you transform the categorical columns, because they are not ordinal and if we do onehotencodingwe will end up with an array with many different values (with high cardinality). the symmetry in the correlations means that the correlation is the same whether we calculate the correlation of a and b or the correlation of b and a. however, relationships in the real world are rarely symmetrical. more often, relationships are asymmetrical\n\na quick example: a column with 2 unique values (trueor falsefor example) will never be able to perfectly predict another column with 100unique values. but the opposite could be true. clearly, asymmetry is important because it is very common in the real world.\n\nhave you ever asked:\n\n* is there a score that tells us if there is any relationship between two columns - no matter if the relationship is linear, non-linear, gaussian, or some other type of relationship?\n* of course, the score should be asymmetrical because i want to detect all the strange relationships between two variables.\n* the score should be 0if there is no relationship and the score should be 1if there is a perfect relationship\n* and that the score helps to answer the question are there correlations between the columns? with a correlation matrix, then you make a scatter -----> plot !!!  over the two columns to compare them and see if there is indeed a strong correlation.\n* and like the icing on the cake, the score should be able to handle both categorical and numerical columns by default.\n\nin short, an asymmetric and data-type agnostic score for predictive relationships between two columns ranging from 0to 1. well, there is the predictive power score library and it can be found at the following link: [predictive power score](https://github.com/8080labs/ppscore)\n\nread all here: [https://www.narrativetext.co/data-hub/using-predictive-power-score-to-pinpoint-non-linear-correlations](https://www.narrativetext.co/data-hub/using-predictive-power-score-to-pinpoint-non-linear-correlations)\n\nso, let's work the library on this notebook!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/ki96p6/using_predictive_power_score_to_pinpoint/',)", "identifyer": 5749651, "year": "2020"}, {"autor": "SQL_beginner", "date": 1608653225000, "content": "Visualizing decision boundaries for machine learning classifiers /!/ I am trying to learn how to plot the decision boundaries for machine learning classifiers (e.g. random forest). Using the R programming language, i figured out how to do this and visualize the results using tsne. \n\nNow, I am trying to make these results \"interactive\" (with the plotly library). \n\nI posted a lengthy question on stackoverflow: https://stackoverflow.com/questions/65404432/r-superimpose-stack-plots-on-top-of-each-other-ggplot2-plotly\n\nCould someone please take a look at it and let me know if you have any ideas? \n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ki85yf/visualizing_decision_boundaries_for_machine/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "visualizing decision boundaries for machine learning classifiers /!/ i am trying to learn how to -----> plot !!!  the decision boundaries for machine learning classifiers (e.g. random forest). using the r programming language, i figured out how to do this and visualize the results using tsne. \n\nnow, i am trying to make these results \"interactive\" (with the plotly library). \n\ni posted a lengthy question on stackoverflow: https://stackoverflow.com/questions/65404432/r-superimpose-stack-plots-on-top-of-each-other-ggplot2-plotly\n\ncould someone please take a look at it and let me know if you have any ideas? \n\nthanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/ki85yf/visualizing_decision_boundaries_for_machine/',)", "identifyer": 5749653, "year": "2020"}, {"autor": "7irz", "date": 1584336303000, "content": "Can someone explain \"train and validation accuracy plots\" to a physician? /!/ Hello everyone,\n\nDisclaimer: I am not a machine learning scientist or a programmer. I am clinician, I see patients and do clinical research. I have a basic understanding of the biostatistics I need to do my job. My understanding of machine learning comes from watching a lot of lay videos and reading manuscripts about it in medical journals. I hope you will forgive my ignorance and help me with this challenge that I am facing.\n\nI am writing a grant and will be working with some machine learning scientists on a method to automate medical ultrasound images. In the statistics section, I am required to explain the statistics we would use to test the model and determine its accuracy. We are using a convoluted neural network (a U-Net).\n\nI asked my machine learning folks and they said that a \"train and validation accuracy plot during training\" would be fairly straight forward and will answer that question.\n\nWhile that is good, what does that mean? This is what I found online:\nhttps://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\nhttps://www.pyimagesearch.com/2017/12/25/plot-accuracy-loss-mxnet/ \nhttps://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html\n\nIt leaves me with the following questions:\n- How is the data required for the plot collected? what does it mean?\n- How is the above plot calculated and evaluated (slope? cut-off? means with p-value?) ? \n- When is it \"successful\"? \n\nI will continue reading and asking people questions. Meanwhile, I would appreciate any guidance.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fjfr3y/can_someone_explain_train_and_validation_accuracy/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "can someone explain \"train and validation accuracy plots\" to a physician? /!/ hello everyone,\n\ndisclaimer: i am not a machine learning scientist or a programmer. i am clinician, i see patients and do clinical research. i have a basic understanding of the biostatistics i need to do my job. my understanding of machine learning comes from watching a lot of lay videos and reading manuscripts about it in medical journals. i hope you will forgive my ignorance and help me with this challenge that i am facing.\n\ni am writing a grant and will be working with some machine learning scientists on a method to automate medical ultrasound images. in the statistics section, i am required to explain the statistics we would use to test the model and determine its accuracy. we are using a convoluted neural network (a u-net).\n\ni asked my machine learning folks and they said that a \"train and validation accuracy -----> plot !!!  during training\" would be fairly straight forward and will answer that question.\n\nwhile that is good, what does that mean? this is what i found online:\nhttps://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\nhttps://www.pyimagesearch.com/2017/12/25/plot-accuracy-loss-mxnet/ \nhttps://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html\n\nit leaves me with the following questions:\n- how is the data required for the plot collected? what does it mean?\n- how is the above plot calculated and evaluated (slope? cut-off? means with p-value?) ? \n- when is it \"successful\"? \n\ni will continue reading and asking people questions. meanwhile, i would appreciate any guidance.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 16, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/fjfr3y/can_someone_explain_train_and_validation_accuracy/',)", "identifyer": 5749714, "year": "2020"}, {"autor": "markovianChains", "date": 1584301047000, "content": "Text vectors clustering &amp; NLP /!/ I am new to this field yet I had done so much reading. It is my first time coding a model so please point out what might the mistake be.\n\nI have a corpus of data of 50000 paragraphs which is an array with text of average length of 120 word. It is mainly medical text just for context.\n\nMy end objective is to cluster these into topics\n\nWhat I did is get the data, then tokenize with nltk and remove \u2018a\u2019 \u2018the\u2019 and other words, then feed the array to a doc2vec model.\n\nThe doc2vec model is trained with vector size 100 and iterations of 2\n\n*Clustering*\n\nI used PCA to reduce to 2 dims to plot\n\nAfter training it, I feed it to kmeans to cluster. I am using the silhouetting to find best cluster number. When I use the whole 50000 of data points, it just shows as an oval of data points spread and kmeans just cut it as straight lines from the center like a pizza of 3 slices\n\n\nI also tried Dbscan, which gives all noise when used without pca, and gives 24 clusters with it, but one cluster takes all the outer part and others are scattered inside overlayed\n\nI also am trying to use lda but I am struggling to implement the code since giving it the doc_vecs crashes the code\n\nHow should I go with this? What might be the mistake?\n\nCan you supply me any piece of code that does the same from a text array so that I can experiment and compare with this?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fj7334/text_vectors_clustering_nlp/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "text vectors clustering &amp; nlp /!/ i am new to this field yet i had done so much reading. it is my first time coding a model so please point out what might the mistake be.\n\ni have a corpus of data of 50000 paragraphs which is an array with text of average length of 120 word. it is mainly medical text just for context.\n\nmy end objective is to cluster these into topics\n\nwhat i did is get the data, then tokenize with nltk and remove \u2018a\u2019 \u2018the\u2019 and other words, then feed the array to a doc2vec model.\n\nthe doc2vec model is trained with vector size 100 and iterations of 2\n\n*clustering*\n\ni used pca to reduce to 2 dims to -----> plot !!! \n\nafter training it, i feed it to kmeans to cluster. i am using the silhouetting to find best cluster number. when i use the whole 50000 of data points, it just shows as an oval of data points spread and kmeans just cut it as straight lines from the center like a pizza of 3 slices\n\n\ni also tried dbscan, which gives all noise when used without pca, and gives 24 clusters with it, but one cluster takes all the outer part and others are scattered inside overlayed\n\ni also am trying to use lda but i am struggling to implement the code since giving it the doc_vecs crashes the code\n\nhow should i go with this? what might be the mistake?\n\ncan you supply me any piece of code that does the same from a text array so that i can experiment and compare with this?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/fj7334/text_vectors_clustering_nlp/',)", "identifyer": 5749720, "year": "2020"}, {"autor": "debaser32", "date": 1584294597000, "content": "Bus Network routing using Genetic Algorithm /!/ Hi,\n\nI am an undergrad currently developing a project to reroute an existing bus network using a genetic algorithm to plot more efficient routes to the existing bus stops. Does anyone have any advice on how to achieve this or reading material to help me with this project. \n\nAny advise or suggestion is appreciated\n\nThanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fj59j9/bus_network_routing_using_genetic_algorithm/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "bus network routing using genetic algorithm /!/ hi,\n\ni am an undergrad currently developing a project to reroute an existing bus network using a genetic algorithm to -----> plot !!!  more efficient routes to the existing bus stops. does anyone have any advice on how to achieve this or reading material to help me with this project. \n\nany advise or suggestion is appreciated\n\nthanks", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/fj59j9/bus_network_routing_using_genetic_algorithm/',)", "identifyer": 5749725, "year": "2020"}, {"autor": "kosar7", "date": 1597546431000, "content": "ValueError: could not broadcast input array from shape (256,2) into shape (256) in Python /!/ I am working on creating a plot for my ARIMA model in Python and comparing it with my train and test set but I am unable to successfully get it plot. Everytime I check out a tutorial and try what others have done I get a different plotting error. I would really appreciate if anyone can help me with this issue.\n\n**Data**\n\n    \tDate\t       Crime\n    0\t2019-10-31\t147\n    1\t2019-11-01\t162\n    2\t2019-11-02\t130\n    3\t2019-11-03\t150\n    4\t2019-11-04\t130\n    ...\t...\t...\n    251\t2020-07-08\t63\n    252\t2020-07-09\t65\n    253\t2020-07-10\t49\n    254\t2020-07-11\t45\n    255\t2020-07-12\t31\n\n**Code**\n\n    import pandas as pd\n    from statsmodels.tsa.arima_model import ARIMA\n    import seaborn as sns\n    import matplotlib.pyplot as plt\n    from matplotlib.dates import DateFormatter\n    \n    # Change Date format \n    df_train['Date'] = pd.to_datetime(df_train.Date, format ='%Y-%m-%d',errors='coerce')\n    df_train = df_train.resample('D', on = 'Date').mean() \n    \n    df_test['Date'] = pd.to_datetime(df_test.Date, format ='%Y-%m-%d', errors='coerce')\n    df_test = df_test.resample('D', on = 'Date').mean() \n    \n    # Model &amp; Forecast\n    arima_mod = ARIMA(df_train, order=(1,1,1)).fit(disp=0)\n    arima_mod.summary()\n    \n    arima_for = arima_mod.forecast(len(df_test)) \n    \n    # Plot\n    fig,plt.figure() \n    fig, ax = plt.subplots()\n    \n    plt.title('ARIMA Model', fontsize = 20, pad=70)\n    plt.xlabel(\"Date\",labelpad=30) \n    ax.xaxis.set_major_formatter(DateFormatter('%b %Y'))\n    \n    plt.plot(df_train, label = 'Train') \n    plt.plot(df_test, label = 'Test')\n    plt.plot(arima_for,label = 'ARIMA') # Where I get the errors\n    \n    plt.legend() \n    \n\n**Error Message**\n\n    \n    ---------------------------------------------------------------------------\n    AttributeError                            Traceback (most recent call last)\n    ~\\anaconda3\\lib\\site-packages\\matplotlib\\cbook\\__init__.py in index_of(y)\n       1626     try:\n    -&gt; 1627         return y.index.values, y.values\n       1628     except AttributeError:\n    \n    AttributeError: 'builtin_function_or_method' object has no attribute 'values'\n    \n    During handling of the above exception, another exception occurred:\n    \n    ValueError                                Traceback (most recent call last)\n    &lt;ipython-input-39-f85e69646e74&gt; in &lt;module&gt;\n         10 plt.plot(df_train, label = 'Train')\n         11 plt.plot(df_test, label = 'Test')\n    ---&gt; 12 plt.plot(arima_for,label = 'ARIMA') \n         13 \n         14 \n    \n    ~\\anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py in plot(scalex, scaley, data, *args, **kwargs)\n       2761     return gca().plot(\n       2762         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n    -&gt; 2763         is not None else {}), **kwargs)\n       2764 \n       2765 \n    \n    ~\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py in plot(self, scalex, scaley, data, *args, **kwargs)\n       1645         \"\"\"\n       1646         kwargs = cbook.normalize_kwargs(kwargs, mlines.Line2D)\n    -&gt; 1647         lines = [*self._get_lines(*args, data=data, **kwargs)]\n       1648         for line in lines:\n       1649             self.add_line(line)\n    \n    ~\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py in __call__(self, *args, **kwargs)\n        214                 this += args[0],\n        215                 args = args[1:]\n    --&gt; 216             yield from self._plot_args(this, kwargs)\n        217 \n        218     def get_next_color(self):\n    \n    ~\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py in _plot_args(self, tup, kwargs)\n        332             y = _check_1d(tup[-1])\n        333         else:\n    --&gt; 334             x, y = index_of(tup[-1])\n        335 \n        336         if self.axes.xaxis is not None:\n    \n    ~\\anaconda3\\lib\\site-packages\\matplotlib\\cbook\\__init__.py in index_of(y)\n       1627         return y.index.values, y.values\n       1628     except AttributeError:\n    -&gt; 1629         y = _check_1d(y)\n       1630         return np.arange(y.shape[0], dtype=float), y\n       1631 \n    \n    ~\\anaconda3\\lib\\site-packages\\matplotlib\\cbook\\__init__.py in _check_1d(x)\n       1324     '''\n       1325     if not hasattr(x, 'shape') or len(x.shape) &lt; 1:\n    -&gt; 1326         return np.atleast_1d(x)\n       1327     else:\n       1328         try:\n    \n    &lt;__array_function__ internals&gt; in atleast_1d(*args, **kwargs)\n    \n    ~\\anaconda3\\lib\\site-packages\\numpy\\core\\shape_base.py in atleast_1d(*arys)\n         65     res = []\n         66     for ary in arys:\n    ---&gt; 67         ary = asanyarray(ary)\n         68         if ary.ndim == 0:\n         69             result = ary.reshape(1)\n    \n    ~\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py in asanyarray(a, dtype, order)\n        136 \n        137     \"\"\"\n    --&gt; 138     return array(a, dtype, copy=False, order=order, subok=True)\n        139 \n        140 \n    \n    ValueError: could not broadcast input array from shape (256,2) into shape (256)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/iakymh/valueerror_could_not_broadcast_input_array_from/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "valueerror: could not broadcast input array from shape (256,2) into shape (256) in python /!/ i am working on creating a -----> plot !!!  for my arima model in python and comparing it with my train and test set but i am unable to successfully get it plot. everytime i check out a tutorial and try what others have done i get a different plotting error. i would really appreciate if anyone can help me with this issue.\n\n**data**\n\n    \tdate\t       crime\n    0\t2019-10-31\t147\n    1\t2019-11-01\t162\n    2\t2019-11-02\t130\n    3\t2019-11-03\t150\n    4\t2019-11-04\t130\n    ...\t...\t...\n    251\t2020-07-08\t63\n    252\t2020-07-09\t65\n    253\t2020-07-10\t49\n    254\t2020-07-11\t45\n    255\t2020-07-12\t31\n\n**code**\n\n    import pandas as pd\n    from statsmodels.tsa.arima_model import arima\n    import seaborn as sns\n    import matplotlib.pyplot as plt\n    from matplotlib.dates import dateformatter\n    \n    # change date format \n    df_train['date'] = pd.to_datetime(df_train.date, format ='%y-%m-%d',errors='coerce')\n    df_train = df_train.resample('d', on = 'date').mean() \n    \n    df_test['date'] = pd.to_datetime(df_test.date, format ='%y-%m-%d', errors='coerce')\n    df_test = df_test.resample('d', on = 'date').mean() \n    \n    # model &amp; forecast\n    arima_mod = arima(df_train, order=(1,1,1)).fit(disp=0)\n    arima_mod.summary()\n    \n    arima_for = arima_mod.forecast(len(df_test)) \n    \n    # plot\n    fig,plt.figure() \n    fig, ax = plt.subplots()\n    \n    plt.title('arima model', fontsize = 20, pad=70)\n    plt.xlabel(\"date\",labelpad=30) \n    ax.xaxis.set_major_formatter(dateformatter('%b %y'))\n    \n    plt.plot(df_train, label = 'train') \n    plt.plot(df_test, label = 'test')\n    plt.plot(arima_for,label = 'arima') # where i get the errors\n    \n    plt.legend() \n    \n\n**error message**\n\n    \n    ---------------------------------------------------------------------------\n    attributeerror                            traceback (most recent call last)\n    ~\\anaconda3\\lib\\site-packages\\matplotlib\\cbook\\__init__.py in index_of(y)\n       1626     try:\n    -&gt; 1627         return y.index.values, y.values\n       1628     except attributeerror:\n    \n    attributeerror: 'builtin_function_or_method' object has no attribute 'values'\n    \n    during handling of the above exception, another exception occurred:\n    \n    valueerror                                traceback (most recent call last)\n    &lt;ipython-input-39-f85e69646e74&gt; in &lt;module&gt;\n         10 plt.plot(df_train, label = 'train')\n         11 plt.plot(df_test, label = 'test')\n    ---&gt; 12 plt.plot(arima_for,label = 'arima') \n         13 \n         14 \n    \n    ~\\anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py in plot(scalex, scaley, data, *args, **kwargs)\n       2761     return gca().plot(\n       2762         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n    -&gt; 2763         is not none else {}), **kwargs)\n       2764 \n       2765 \n    \n    ~\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py in plot(self, scalex, scaley, data, *args, **kwargs)\n       1645         \"\"\"\n       1646         kwargs = cbook.normalize_kwargs(kwargs, mlines.line2d)\n    -&gt; 1647         lines = [*self._get_lines(*args, data=data, **kwargs)]\n       1648         for line in lines:\n       1649             self.add_line(line)\n    \n    ~\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py in __call__(self, *args, **kwargs)\n        214                 this += args[0],\n        215                 args = args[1:]\n    --&gt; 216             yield from self._plot_args(this, kwargs)\n        217 \n        218     def get_next_color(self):\n    \n    ~\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py in _plot_args(self, tup, kwargs)\n        332             y = _check_1d(tup[-1])\n        333         else:\n    --&gt; 334             x, y = index_of(tup[-1])\n        335 \n        336         if self.axes.xaxis is not none:\n    \n    ~\\anaconda3\\lib\\site-packages\\matplotlib\\cbook\\__init__.py in index_of(y)\n       1627         return y.index.values, y.values\n       1628     except attributeerror:\n    -&gt; 1629         y = _check_1d(y)\n       1630         return np.arange(y.shape[0], dtype=float), y\n       1631 \n    \n    ~\\anaconda3\\lib\\site-packages\\matplotlib\\cbook\\__init__.py in _check_1d(x)\n       1324     '''\n       1325     if not hasattr(x, 'shape') or len(x.shape) &lt; 1:\n    -&gt; 1326         return np.atleast_1d(x)\n       1327     else:\n       1328         try:\n    \n    &lt;__array_function__ internals&gt; in atleast_1d(*args, **kwargs)\n    \n    ~\\anaconda3\\lib\\site-packages\\numpy\\core\\shape_base.py in atleast_1d(*arys)\n         65     res = []\n         66     for ary in arys:\n    ---&gt; 67         ary = asanyarray(ary)\n         68         if ary.ndim == 0:\n         69             result = ary.reshape(1)\n    \n    ~\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py in asanyarray(a, dtype, order)\n        136 \n        137     \"\"\"\n    --&gt; 138     return array(a, dtype, copy=false, order=order, subok=true)\n        139 \n        140 \n    \n    valueerror: could not broadcast input array from shape (256,2) into shape (256)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/iakymh/valueerror_could_not_broadcast_input_array_from/',)", "identifyer": 5749831, "year": "2020"}, {"autor": "n-smj", "date": 1593798988000, "content": "How to visualize DQN results? /!/ I've been training a Flappy Bird agent using a DQN built with Keras and I want to see if its scores are converging. Currently I'm using *matplotlib.pyplot.scatter* and I get graphs like [this](https://imgur.com/ehMwxhz). Those are supposed to be dots but it almost looks like a histogram. I've later realized my code is not right and instead of 1 dot for each game it plots a dot for each score during a game.\n\nWhat I want to achieve is to calculate the average scores as well and get something like [this](https://camo.githubusercontent.com/acc74a82be4f1a06bb3ee87dc68b57459f9d3613/687474703a2f2f692e696d6775722e636f6d2f45335679304f522e706e67). I guess I'm supposed to do the calculation in the code myself and use subplot to plot them together in the same figure but I don't know which plot type should I use. Any help is appreciated.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hknp9b/how_to_visualize_dqn_results/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "how to visualize dqn results? /!/ i've been training a flappy bird agent using a dqn built with keras and i want to see if its scores are converging. currently i'm using *matplotlib.pyplot.scatter* and i get graphs like [this](https://imgur.com/ehmwxhz). those are supposed to be dots but it almost looks like a histogram. i've later realized my code is not right and instead of 1 dot for each game it plots a dot for each score during a game.\n\nwhat i want to achieve is to calculate the average scores as well and get something like [this](https://camo.githubusercontent.com/acc74a82be4f1a06bb3ee87dc68b57459f9d3613/687474703a2f2f692e696d6775722e636f6d2f45335679304f522e706e67). i guess i'm supposed to do the calculation in the code myself and use subplot to -----> plot !!!  them together in the same figure but i don't know which -----> plot !!!  type should i use. any help is appreciated.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/hknp9b/how_to_visualize_dqn_results/',)", "identifyer": 5750030, "year": "2020"}, {"autor": "westkorn", "date": 1593770799000, "content": "How to draw the decision boundary of SVM /!/ I have trained an SVM in matlab and therefore I have the values of w and b. I know the boundary satisfies the equation w\\*x+b=0 , but what do i put in x ? If i put arbitrary values i dont get the results i want. How can i use w and b to plot the boundary ? Thanks in advance", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hkggml/how_to_draw_the_decision_boundary_of_svm/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "how to draw the decision boundary of svm /!/ i have trained an svm in matlab and therefore i have the values of w and b. i know the boundary satisfies the equation w\\*x+b=0 , but what do i put in x ? if i put arbitrary values i dont get the results i want. how can i use w and b to -----> plot !!!  the boundary ? thanks in advance", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/hkggml/how_to_draw_the_decision_boundary_of_svm/',)", "identifyer": 5750043, "year": "2020"}, {"autor": "hiphop1987", "date": 1591216744000, "content": "How to explain a Regression model? /!/ **Are High R-squared Values Inherently Good?**\n\nNote, high  R-squared doesn\u2019t mean that your model is good. We need to check the residual plot when fitting a regression model. One of the assumptions of Linear Regression is Homoscedasticity, which means that the variance of residual is the same for any value of X.\n\n**What is the meaning of regression coefficients?**\n\nThe sign of a coefficient tells us whether there is a positive or negative correlation between a feature and a target variable.\n\nThe positive coefficient indicates when the feature increases, the mean of the target also increases. A negative coefficient indicates as the feature value decreases, the target tends to decrease.\n\n**And more:**\n\n[https://towardsdatascience.com/how-to-explain-a-regression-model-244882e6cc0c](https://towardsdatascience.com/how-to-explain-a-regression-model-244882e6cc0c)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gw24m0/how_to_explain_a_regression_model/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "how to explain a regression model? /!/ **are high r-squared values inherently good?**\n\nnote, high  r-squared doesn\u2019t mean that your model is good. we need to check the residual -----> plot !!!  when fitting a regression model. one of the assumptions of linear regression is homoscedasticity, which means that the variance of residual is the same for any value of x.\n\n**what is the meaning of regression coefficients?**\n\nthe sign of a coefficient tells us whether there is a positive or negative correlation between a feature and a target variable.\n\nthe positive coefficient indicates when the feature increases, the mean of the target also increases. a negative coefficient indicates as the feature value decreases, the target tends to decrease.\n\n**and more:**\n\n[https://towardsdatascience.com/how-to-explain-a-regression-model-244882e6cc0c](https://towardsdatascience.com/how-to-explain-a-regression-model-244882e6cc0c)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/gw24m0/how_to_explain_a_regression_model/',)", "identifyer": 5750112, "year": "2020"}, {"autor": "VeryCuriousLearner", "date": 1588097649000, "content": "multi layer perceptron , weights of hidden layer seem random /!/ I was trying to visualize how the hidden layer weights of a multi layer perceptron look, but it is very confusing and without much perceptible pattern.\n\nThe input layer is 784,\n\nhidden\\_1 :121 ,\n\nhidden\\_2 = 64 and\n\n[hidden layer](https://preview.redd.it/zfb4chxznlv41.png?width=292&amp;format=png&amp;auto=webp&amp;s=fe0cbb2574d01917738c9053714ca86f6c7facd4)\n\noutput = 10.\n\nwhile the weights of first to hidden\\_1 have very recognizable pattern, the hidden\\_1 to hidden\\_2 and last layer weights look very random.\n\nI used the method as in :[https://scikit-learn.org/stable/auto\\_examples/neural\\_networks/plot\\_mnist\\_filters.html](https://scikit-learn.org/stable/auto_examples/neural_networks/plot_mnist_filters.html) and my weights in first layer look as in the link.\n\nfor the second layer  with weights of shape ( 64,  121), I plot 64 small plots , each of 11x11.\n\nI understnd weights have much more meaning in conv nets and features are more perceptible as we go deeper, but this randomness in MLP after the first layer is making me doubt if I am plotting right? PLease explain.\n\n[input layer](https://preview.redd.it/wv9f31ewnlv41.png?width=292&amp;format=png&amp;auto=webp&amp;s=972f73adb7d8e24ce41efe6549a898fe31279170)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g9scsw/multi_layer_perceptron_weights_of_hidden_layer/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "multi layer perceptron , weights of hidden layer seem random /!/ i was trying to visualize how the hidden layer weights of a multi layer perceptron look, but it is very confusing and without much perceptible pattern.\n\nthe input layer is 784,\n\nhidden\\_1 :121 ,\n\nhidden\\_2 = 64 and\n\n[hidden layer](https://preview.redd.it/zfb4chxznlv41.png?width=292&amp;format=png&amp;auto=webp&amp;s=fe0cbb2574d01917738c9053714ca86f6c7facd4)\n\noutput = 10.\n\nwhile the weights of first to hidden\\_1 have very recognizable pattern, the hidden\\_1 to hidden\\_2 and last layer weights look very random.\n\ni used the method as in :[https://scikit-learn.org/stable/auto\\_examples/neural\\_networks/plot\\_mnist\\_filters.html](https://scikit-learn.org/stable/auto_examples/neural_networks/plot_mnist_filters.html) and my weights in first layer look as in the link.\n\nfor the second layer  with weights of shape ( 64,  121), i -----> plot !!!  64 small plots , each of 11x11.\n\ni understnd weights have much more meaning in conv nets and features are more perceptible as we go deeper, but this randomness in mlp after the first layer is making me doubt if i am plotting right? please explain.\n\n[input layer](https://preview.redd.it/wv9f31ewnlv41.png?width=292&amp;format=png&amp;auto=webp&amp;s=972f73adb7d8e24ce41efe6549a898fe31279170)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/g9scsw/multi_layer_perceptron_weights_of_hidden_layer/',)", "identifyer": 5750472, "year": "2020"}, {"autor": "matusba093", "date": 1588056951000, "content": "python One Class SVM outlier detection /!/ Hi,   \nI'm comparing some outlier detection methods for my project, I'm having hard time with One Class SVM, specifically with it's plot and scale invariant property.\n\nData points are from 2D normal distribution, this is my code:  \n \n\nclusteringSVM\u00a0=\u00a0OneClassSVM(nu=0.05,kernel=\"rbf\").fit(points\\_origin)  \noutlier\\_SVM\u00a0=\u00a0points\\_origin\\[clusteringSVM.predict(points\\_origin)==-1\\]  \nplt.plot(points\\_origin\\[:,0\\],\u00a0points\\_origin\\[:,1\\],\u00a0'o')  \nplt.plot(outlier\\_SVM\\[:,0\\],\u00a0outlier\\_SVM\\[:,1\\],\u00a0'ro')  \nFor some reason, plot looks like this:\n\n&amp;#x200B;\n\n[OneClassSVM, nu = 0.05, points from normal dist.](https://preview.redd.it/gdkfdkrr9iv41.png?width=495&amp;format=png&amp;auto=webp&amp;s=22a2f28aa119c57f8eb46a320ca3c83c82dc9724)\n\nI dont know, why there is that inner ellipse of red outlier points. \n\nFurthermore, when I try to show, if is this method scale invariant, I multiply y-axis 10 or 100 times. The result is even more weird:\n\n&amp;#x200B;\n\n[y = 10\\*y](https://preview.redd.it/35fsa2bkaiv41.png?width=504&amp;format=png&amp;auto=webp&amp;s=befff7a63e100e45d8dbf5948e261c8715e058b8)\n\n&amp;#x200B;\n\n[y=100\\*y](https://preview.redd.it/tr9b7tlnaiv41.png?width=512&amp;format=png&amp;auto=webp&amp;s=f8f8839e352960aac924c4f6d88bd5e4403aedb4)\n\nMethod fails to detect any outliers whatsoever.\n\n&amp;#x200B;\n\nThank you very very much for any help, it would be extremly helpful.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g9ibm9/python_one_class_svm_outlier_detection/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "python one class svm outlier detection /!/ hi,   \ni'm comparing some outlier detection methods for my project, i'm having hard time with one class svm, specifically with it's -----> plot !!!  and scale invariant property.\n\ndata points are from 2d normal distribution, this is my code:  \n \n\nclusteringsvm\u00a0=\u00a0oneclasssvm(nu=0.05,kernel=\"rbf\").fit(points\\_origin)  \noutlier\\_svm\u00a0=\u00a0points\\_origin\\[clusteringsvm.predict(points\\_origin)==-1\\]  \nplt.plot(points\\_origin\\[:,0\\],\u00a0points\\_origin\\[:,1\\],\u00a0'o')  \nplt.plot(outlier\\_svm\\[:,0\\],\u00a0outlier\\_svm\\[:,1\\],\u00a0'ro')  \nfor some reason, plot looks like this:\n\n&amp;#x200b;\n\n[oneclasssvm, nu = 0.05, points from normal dist.](https://preview.redd.it/gdkfdkrr9iv41.png?width=495&amp;format=png&amp;auto=webp&amp;s=22a2f28aa119c57f8eb46a320ca3c83c82dc9724)\n\ni dont know, why there is that inner ellipse of red outlier points. \n\nfurthermore, when i try to show, if is this method scale invariant, i multiply y-axis 10 or 100 times. the result is even more weird:\n\n&amp;#x200b;\n\n[y = 10\\*y](https://preview.redd.it/35fsa2bkaiv41.png?width=504&amp;format=png&amp;auto=webp&amp;s=befff7a63e100e45d8dbf5948e261c8715e058b8)\n\n&amp;#x200b;\n\n[y=100\\*y](https://preview.redd.it/tr9b7tlnaiv41.png?width=512&amp;format=png&amp;auto=webp&amp;s=f8f8839e352960aac924c4f6d88bd5e4403aedb4)\n\nmethod fails to detect any outliers whatsoever.\n\n&amp;#x200b;\n\nthank you very very much for any help, it would be extremly helpful.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/g9ibm9/python_one_class_svm_outlier_detection/',)", "identifyer": 5750497, "year": "2020"}, {"autor": "1099shateme", "date": 1588151458000, "content": "Understanding differences in clustering result (PCA + Kmeans and heatmap) /!/  \n\nI'm a first year PhD student with a CS background but have been on and off with data sci.\n\nI  ran a large metabolomic experiment and am trying to identify  differences in my cells. This is my second time running this and both  time I noticed that the heatmap hierarchal clustering didn't seperate my  groups as nicely as when I tried PCA + kmeans.\n\nThe  first time, it appeared that there were some differences when visually  inspecting the map, yet for some of my samples, the hierarchal  clustering mixed some of the groupings up while the rest stayed intact.  It was strange to me because visually the groups that got mixed together  appeared to be distict. I ran heatmap2 on R and just used the default  clustering algorithm.\n\nI ran PCA +  Kmeans later and got very nice seperation between the groups that made  biological sense. So I definitely have a few questions.\n\n1. What  factors should I consider when deciding between the different  hierarchal clustering methods? I'm not sure if there's a reason I need  to switch over from the default method, other than the fact that my  results weren't as expected.\n2. I  think I understand how/why we choose certain algorithms for cases where  we are building predictive models. In those cases, we can test  different models on data, look for accurary and performance, etc. But in  this case, where I am doing exploratory data analysis and not planning  on building a deployable machine learning model, how much does it  matter? The reason I chose K-means is because it's something I  understand and also since I have an idea of how many biologically  different groups there are, I also have some logic for choosing a  certain number of clusters. I don't see a need to choose another  clustering algorithm unless things just don't work out for me... which  leads me to 3\n3. In  a scientific environment where these techniques are being used to  understand our data, how do you choose between different techniques  especially when they provide conflicting results. I favor the k-means  over the hierarchal clustering, but only because I understand the  biological context. But at that point, am I introducing bias into my  interpretation as well? The only pseudo-mathematical reason I can think  of is that the PCA accounts for variation across the entire set of  variables where as the heatmap looks at things on a more individualistic  basis. So the PCA and subsequent clustering is able to account for the  summation of all the minute variability in my samples.\n4. From  what I udnerstand, PCA is great for reducing multidimensional data. The  reason I think it's important to run on my dataset is because without  dimensionality reduction, there would be no way to visually plot my data  since I have about 100 different variables in each cell. However,  technically, k-means could have been run on my samples without PCA, it  might have been more computationally intensive and impossible to plot  visually as I would then have to choose what I believed to be the most 2  or 3 most representative variables from my data.  Do I have the correct  idea?\n5. In  my first set of data, my mentor was very happy with my results and  asked me to figure out what variables were most responsible for  seperation. This seemed a little fishy to me... from what I understand  it's really hard to get something meaningful out of the components and  the weights assigned to the variables. I did notice that there were  groups of 5 or 6 variables that were in related pathways that had the  highest absolute weights. Is it therefore correct to say that those  cells are likely to different within those specific pathways (although  of course this must be experimentally verified\n\nSorry for the lengthy message, but I appreciate your input!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ga6490/understanding_differences_in_clustering_result/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "understanding differences in clustering result (pca + kmeans and heatmap) /!/  \n\ni'm a first year phd student with a cs background but have been on and off with data sci.\n\ni  ran a large metabolomic experiment and am trying to identify  differences in my cells. this is my second time running this and both  time i noticed that the heatmap hierarchal clustering didn't seperate my  groups as nicely as when i tried pca + kmeans.\n\nthe  first time, it appeared that there were some differences when visually  inspecting the map, yet for some of my samples, the hierarchal  clustering mixed some of the groupings up while the rest stayed intact.  it was strange to me because visually the groups that got mixed together  appeared to be distict. i ran heatmap2 on r and just used the default  clustering algorithm.\n\ni ran pca +  kmeans later and got very nice seperation between the groups that made  biological sense. so i definitely have a few questions.\n\n1. what  factors should i consider when deciding between the different  hierarchal clustering methods? i'm not sure if there's a reason i need  to switch over from the default method, other than the fact that my  results weren't as expected.\n2. i  think i understand how/why we choose certain algorithms for cases where  we are building predictive models. in those cases, we can test  different models on data, look for accurary and performance, etc. but in  this case, where i am doing exploratory data analysis and not planning  on building a deployable machine learning model, how much does it  matter? the reason i chose k-means is because it's something i  understand and also since i have an idea of how many biologically  different groups there are, i also have some logic for choosing a  certain number of clusters. i don't see a need to choose another  clustering algorithm unless things just don't work out for me... which  leads me to 3\n3. in  a scientific environment where these techniques are being used to  understand our data, how do you choose between different techniques  especially when they provide conflicting results. i favor the k-means  over the hierarchal clustering, but only because i understand the  biological context. but at that point, am i introducing bias into my  interpretation as well? the only pseudo-mathematical reason i can think  of is that the pca accounts for variation across the entire set of  variables where as the heatmap looks at things on a more individualistic  basis. so the pca and subsequent clustering is able to account for the  summation of all the minute variability in my samples.\n4. from  what i udnerstand, pca is great for reducing multidimensional data. the  reason i think it's important to run on my dataset is because without  dimensionality reduction, there would be no way to visually -----> plot !!!  my data  since i have about 100 different variables in each cell. however,  technically, k-means could have been run on my samples without pca, it  might have been more computationally intensive and impossible to plot  visually as i would then have to choose what i believed to be the most 2  or 3 most representative variables from my data.  do i have the correct  idea?\n5. in  my first set of data, my mentor was very happy with my results and  asked me to figure out what variables were most responsible for  seperation. this seemed a little fishy to me... from what i understand  it's really hard to get something meaningful out of the components and  the weights assigned to the variables. i did notice that there were  groups of 5 or 6 variables that were in related pathways that had the  highest absolute weights. is it therefore correct to say that those  cells are likely to different within those specific pathways (although  of course this must be experimentally verified\n\nsorry for the lengthy message, but i appreciate your input!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/ga6490/understanding_differences_in_clustering_result/',)", "identifyer": 5750654, "year": "2020"}, {"autor": "instigator-001", "date": 1599123532000, "content": "Activation functions and their purpose in neural networks|know why ReLU is used mostly as default activation functions|We shall also know how to tackle some of the limitations of ReLu with another activation function with Leaky ReLU|You ll also see how to plot these activation functions using python", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ilpwqy/activation_functions_and_their_purpose_in_neural/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "activation functions and their purpose in neural networks|know why relu is used mostly as default activation functions|we shall also know how to tackle some of the limitations of relu with another activation function with leaky relu|you ll also see how to -----> plot !!!  these activation functions using python", "sortedWord": "None", "removed": "('reddit',)", "score": 1, "comments": 0, "media": "('link',)", "medialink": "('https://www.mygreatlearning.com/blog/relu-activation-function/?utm_source=greatlearnML2',)", "identifyer": 5750747, "year": "2020"}, {"autor": "Tyron_Slothrop", "date": 1599101622000, "content": "RNN over-fitting due to Label Encoder, or just an easy dataset for the model to learn? /!/ Hey Everyone. I built a simple RNN that predicts whether a mushroom is poisonous based on 22 features. The model is badly over-fitting and the predictions on unknown data are inconsistent. I'm wondering if anyone could offer suggestions on what exactly went wrong? The initial dataset used letters to denote certain features, so I used label encoder to transform them; from my understand, this might make the model think there's an order to the data, right? Would my model avoid over-fitting if I used, say, OneHotENcoder instead of Label Encoder? \n\n    X = df.drop('class', axis=1)\n    y = df['class']\n    \n    from sklearn.preprocessing import LabelEncoder\n    \n    Encoder_X = LabelEncoder() \n    \n    for col in X.columns:\n        X[col] = Encoder_X.fit_transform(X[col])\n    Encoder_y=LabelEncoder()\n    \n    y = Encoder_y.fit_transform(y)\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n    \n    #define the Dense layers and activation functions\n    \n    nn = tf.keras.Sequential()\n    \n    nn.add(Dense(22, activation='relu'))\n    \n    nn.add(Dropout(0.2))\n    \n    nn.add(Dense(10, activation='relu'))\n    \n    nn.add(Dropout(0.2))\n    \n    nn.add(Dense(5, activation='relu'))\n    \n    nn.add(Dropout(0.2))\n    \n    nn.add(Dense(1, activation='sigmoid'))\n    \n    nn.compile(optimizer = 'adam', loss = 'binary_crossentropy', scoring=['accuracy'],metrics=['accuracy'])\n    \n    early_stop = EarlyStopping(monitor='val_loss',mode='min',verbose=1, patience=25)\n    \n    nn.fit(X_train, y_train, epochs = 1000, validation_data=(X_test, y_test),\n             callbacks=[early_stop])\n    \n    #plot the model loss\n    \n    model_loss = pd.DataFrame(nn.history.history)\n    model_loss.plot()\n    \n    pred = nn.predict_classes(X_test)\n    pred = pred.flatten()\n    \n    print(classification_report(y_test,pred))\n    \n    print(confusion_matrix(y_test, pred))\n    \n    mean_squared_error(y_test, pred)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/illdn6/rnn_overfitting_due_to_label_encoder_or_just_an/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "rnn over-fitting due to label encoder, or just an easy dataset for the model to learn? /!/ hey everyone. i built a simple rnn that predicts whether a mushroom is poisonous based on 22 features. the model is badly over-fitting and the predictions on unknown data are inconsistent. i'm wondering if anyone could offer suggestions on what exactly went wrong? the initial dataset used letters to denote certain features, so i used label encoder to transform them; from my understand, this might make the model think there's an order to the data, right? would my model avoid over-fitting if i used, say, onehotencoder instead of label encoder? \n\n    x = df.drop('class', axis=1)\n    y = df['class']\n    \n    from sklearn.preprocessing import labelencoder\n    \n    encoder_x = labelencoder() \n    \n    for col in x.columns:\n        x[col] = encoder_x.fit_transform(x[col])\n    encoder_y=labelencoder()\n    \n    y = encoder_y.fit_transform(y)\n    \n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30)\n    \n    #define the dense layers and activation functions\n    \n    nn = tf.keras.sequential()\n    \n    nn.add(dense(22, activation='relu'))\n    \n    nn.add(dropout(0.2))\n    \n    nn.add(dense(10, activation='relu'))\n    \n    nn.add(dropout(0.2))\n    \n    nn.add(dense(5, activation='relu'))\n    \n    nn.add(dropout(0.2))\n    \n    nn.add(dense(1, activation='sigmoid'))\n    \n    nn.compile(optimizer = 'adam', loss = 'binary_crossentropy', scoring=['accuracy'],metrics=['accuracy'])\n    \n    early_stop = earlystopping(monitor='val_loss',mode='min',verbose=1, patience=25)\n    \n    nn.fit(x_train, y_train, epochs = 1000, validation_data=(x_test, y_test),\n             callbacks=[early_stop])\n    \n    #-----> plot !!!  the model loss\n    \n    model_loss = pd.dataframe(nn.history.history)\n    model_loss.plot()\n    \n    pred = nn.predict_classes(x_test)\n    pred = pred.flatten()\n    \n    print(classification_report(y_test,pred))\n    \n    print(confusion_matrix(y_test, pred))\n    \n    mean_squared_error(y_test, pred)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/illdn6/rnn_overfitting_due_to_label_encoder_or_just_an/',)", "identifyer": 5750750, "year": "2020"}, {"autor": "hedgehogist", "date": 1599044386000, "content": "My first blog post \u2013 on how to plot decision boundaries /!/ Hey guys, I have written my first blog post on how to plot decision boundaries for classification models. Plotting decision boundaries can help immensely in the model selection and hyperparameter tuning process, as it can help detect overfitting or underfitting. [Here](https://vivianrjkmr.github.io/2020/09/01/decision-boundary-visualization) is the link.\n\n**The motivation for my post**\n\nI noticed that many online explanations and tutorials explaining overfitting rely on 2D datasets (i.e. datasets with only two input features). Such boundaries can be easily plotted on a 2D plane, but what if your data contains six input features? One can use dimensionality reduction to plot the actual points, but what about the decision boundaries themselves? This is what my post explores.\n\nIt would be an amazing help if you could spare some time to take a look, and maybe give me some feedback. If you liked it, feel free to \"like\" my post.\n\nHope you find it useful and/or fun to read. Have a nice day!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/il4nn4/my_first_blog_post_on_how_to_plot_decision/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "my first blog post \u2013 on how to -----> plot !!!  decision boundaries /!/ hey guys, i have written my first blog post on how to plot decision boundaries for classification models. plotting decision boundaries can help immensely in the model selection and hyperparameter tuning process, as it can help detect overfitting or underfitting. [here](https://vivianrjkmr.github.io/2020/09/01/decision-boundary-visualization) is the link.\n\n**the motivation for my post**\n\ni noticed that many online explanations and tutorials explaining overfitting rely on 2d datasets (i.e. datasets with only two input features). such boundaries can be easily plotted on a 2d plane, but what if your data contains six input features? one can use dimensionality reduction to plot the actual points, but what about the decision boundaries themselves? this is what my post explores.\n\nit would be an amazing help if you could spare some time to take a look, and maybe give me some feedback. if you liked it, feel free to \"like\" my post.\n\nhope you find it useful and/or fun to read. have a nice day!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/il4nn4/my_first_blog_post_on_how_to_plot_decision/',)", "identifyer": 5750781, "year": "2020"}, {"autor": "boriswinner", "date": 1587443531000, "content": "VAE Music Generation Network learns to generate just ONE melody /!/ Hello! \n\nI'm a 4th year AMI student. I'm currently working on study project: Variational Autoencoder for music generation. I'm inspired by this project : [https://github.com/HackerPoet/Composer](https://github.com/HackerPoet/Composer) (explanation: [https://youtu.be/UWxfnNXlVy8](https://youtu.be/UWxfnNXlVy8) ). I'm trying to write a similar network as a study project, but with the ability for multitrack music generation.\n\nI'm using Keras with TensorFlow backend. My piano rolls have 3rd dimension which represents MIDI instrument.\n\nThe layer architecture and loss function are almost the same as in your network (in VAE mode). I strip empty note heights (below the lowest note and upon the hightst notes) and MIDI instruments.\n\n&amp;#x200B;\n\nHowever, I've faced some problems. I'm asking for your help.\n\n&amp;#x200B;\n\nMy network tends to learn to generate just one melody. In the first epochs (1..20) the output is somewhat musical and different, but when I try to launch more epochs (for example, 100) it just learns to generate one song. This song is rhytmic, but has only about 4 repeating notes. After about 20 epochs loss and val\\_loss stop becoming smaller. When I try to plot the first two dimensions of the encoder prediction (latent space), with low number of epochs it looks like on picture 1, and with higher number of epochs it is simply a dot.\n\n&amp;#x200B;\n\nI tried to simplify the network architecture so it has only one hidden layer. In this case the result of first epochs is also somewhat musical, but with more epochs it learns to generate pure mess (a lot of notes at the same time).\n\n&amp;#x200B;\n\nThe question is - what can I be doing wrong?  \n\n&amp;#x200B;\n\nThe code can be found here:\n\n[https://github.com/boriswinner/BDB-VAE](https://github.com/boriswinner/BDB-VAE)\n\n&amp;#x200B;\n\nThe VAE code is in [VAE.py](https://VAE.py)\n\n[20 epochs plot of first two vatiables of latent dimension on test data](https://preview.redd.it/wqw0hr9dm3u41.png?width=600&amp;format=png&amp;auto=webp&amp;s=59ec8b3c298b9ea261245930b8820c66973290a0)\n\n&amp;#x200B;\n\n[Same, but 100 epochs](https://preview.redd.it/6odzqkjhm3u41.png?width=600&amp;format=png&amp;auto=webp&amp;s=0ca9a4c269141a4b61ec1905e9d4024d8e7cd649)\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n[Network architecture](https://preview.redd.it/2t5a2gjjm3u41.png?width=545&amp;format=png&amp;auto=webp&amp;s=3d1bad9f5957609091c011221fe3f0349ea585a6)\n\n&amp;#x200B;\n\n[Encoder](https://preview.redd.it/8oamt321n3u41.png?width=709&amp;format=png&amp;auto=webp&amp;s=ab12a78620ebfbe9687bdec56a37db80780b769d)\n\n&amp;#x200B;\n\n[Decoder](https://preview.redd.it/bue2z282n3u41.png?width=656&amp;format=png&amp;auto=webp&amp;s=52f4a879c745a398adcc9afcc683c15ce584de9d)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g58nyq/vae_music_generation_network_learns_to_generate/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "vae music generation network learns to generate just one melody /!/ hello! \n\ni'm a 4th year ami student. i'm currently working on study project: variational autoencoder for music generation. i'm inspired by this project : [https://github.com/hackerpoet/composer](https://github.com/hackerpoet/composer) (explanation: [https://youtu.be/uwxfnnxlvy8](https://youtu.be/uwxfnnxlvy8) ). i'm trying to write a similar network as a study project, but with the ability for multitrack music generation.\n\ni'm using keras with tensorflow backend. my piano rolls have 3rd dimension which represents midi instrument.\n\nthe layer architecture and loss function are almost the same as in your network (in vae mode). i strip empty note heights (below the lowest note and upon the hightst notes) and midi instruments.\n\n&amp;#x200b;\n\nhowever, i've faced some problems. i'm asking for your help.\n\n&amp;#x200b;\n\nmy network tends to learn to generate just one melody. in the first epochs (1..20) the output is somewhat musical and different, but when i try to launch more epochs (for example, 100) it just learns to generate one song. this song is rhytmic, but has only about 4 repeating notes. after about 20 epochs loss and val\\_loss stop becoming smaller. when i try to -----> plot !!!  the first two dimensions of the encoder prediction (latent space), with low number of epochs it looks like on picture 1, and with higher number of epochs it is simply a dot.\n\n&amp;#x200b;\n\ni tried to simplify the network architecture so it has only one hidden layer. in this case the result of first epochs is also somewhat musical, but with more epochs it learns to generate pure mess (a lot of notes at the same time).\n\n&amp;#x200b;\n\nthe question is - what can i be doing wrong?  \n\n&amp;#x200b;\n\nthe code can be found here:\n\n[https://github.com/boriswinner/bdb-vae](https://github.com/boriswinner/bdb-vae)\n\n&amp;#x200b;\n\nthe vae code is in [vae.py](https://vae.py)\n\n[20 epochs plot of first two vatiables of latent dimension on test data](https://preview.redd.it/wqw0hr9dm3u41.png?width=600&amp;format=png&amp;auto=webp&amp;s=59ec8b3c298b9ea261245930b8820c66973290a0)\n\n&amp;#x200b;\n\n[same, but 100 epochs](https://preview.redd.it/6odzqkjhm3u41.png?width=600&amp;format=png&amp;auto=webp&amp;s=0ca9a4c269141a4b61ec1905e9d4024d8e7cd649)\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n[network architecture](https://preview.redd.it/2t5a2gjjm3u41.png?width=545&amp;format=png&amp;auto=webp&amp;s=3d1bad9f5957609091c011221fe3f0349ea585a6)\n\n&amp;#x200b;\n\n[encoder](https://preview.redd.it/8oamt321n3u41.png?width=709&amp;format=png&amp;auto=webp&amp;s=ab12a78620ebfbe9687bdec56a37db80780b769d)\n\n&amp;#x200b;\n\n[decoder](https://preview.redd.it/bue2z282n3u41.png?width=656&amp;format=png&amp;auto=webp&amp;s=52f4a879c745a398adcc9afcc683c15ce584de9d)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/g58nyq/vae_music_generation_network_learns_to_generate/',)", "identifyer": 5751255, "year": "2020"}, {"autor": "ie11_is_my_fetish", "date": 1587430865000, "content": "Would there be some kind of pattern/structure to \"compiled\" models /!/ This is a weird \"crayons are gluons\" thought but was curious if you could \"normalize\" models and plot them somehow, the trained ones... would you see some kind of pattern", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g55evi/would_there_be_some_kind_of_patternstructure_to/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "would there be some kind of pattern/structure to \"compiled\" models /!/ this is a weird \"crayons are gluons\" thought but was curious if you could \"normalize\" models and -----> plot !!!  them somehow, the trained ones... would you see some kind of pattern", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/g55evi/would_there_be_some_kind_of_patternstructure_to/',)", "identifyer": 5751269, "year": "2020"}, {"autor": "resolver101", "date": 1587411877000, "content": "classifying a part a robot makes from its power draw /!/ Hello,\n\nI work for a company that has a robot that makes parts. I have a IOT devices connected to the robot that measures and stores how much power the robot draws. The power data gives a clear pattern of when a single part is made as shown below plot. You can see that it takes roughly 2 minutes for a part to be made and draws up to 50 units.\n\n[power draw when a part is being made](https://preview.redd.it/0lvq15bn01u41.png?width=314&amp;format=png&amp;auto=webp&amp;s=597ce71d281366e95c8f8510b6a19722bc0dec3d)\n\nBelow is a plot of multiple parts being made one after the other. Parts labeled odd numbers are on the robots left had side and evens part numbers are on the right hand side. The right hand side take slightly more power because the robot has to reach a little further.\n\n[ power draw when multiple parts are being made ](https://preview.redd.it/mp4u41lp01u41.png?width=942&amp;format=png&amp;auto=webp&amp;s=975ce6c8b74e0f8724fa1239933a2f51863fa5ad)\n\nI want to setup a machine learning algorithm to count the parts coming off the machine. What would be the a good algorithm to use in this situation?\n\nA sample of the data can be [found here.](https://drive.google.com/file/d/1XyIpDlM0F7o__evtdBmq3dYd_Ikj6ovT/view?usp=sharing)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g4zo7j/classifying_a_part_a_robot_makes_from_its_power/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "classifying a part a robot makes from its power draw /!/ hello,\n\ni work for a company that has a robot that makes parts. i have a iot devices connected to the robot that measures and stores how much power the robot draws. the power data gives a clear pattern of when a single part is made as shown below -----> plot !!! . you can see that it takes roughly 2 minutes for a part to be made and draws up to 50 units.\n\n[power draw when a part is being made](https://preview.redd.it/0lvq15bn01u41.png?width=314&amp;format=png&amp;auto=webp&amp;s=597ce71d281366e95c8f8510b6a19722bc0dec3d)\n\nbelow is a plot of multiple parts being made one after the other. parts labeled odd numbers are on the robots left had side and evens part numbers are on the right hand side. the right hand side take slightly more power because the robot has to reach a little further.\n\n[ power draw when multiple parts are being made ](https://preview.redd.it/mp4u41lp01u41.png?width=942&amp;format=png&amp;auto=webp&amp;s=975ce6c8b74e0f8724fa1239933a2f51863fa5ad)\n\ni want to setup a machine learning algorithm to count the parts coming off the machine. what would be the a good algorithm to use in this situation?\n\na sample of the data can be [found here.](https://drive.google.com/file/d/1xyipdlm0f7o__evtdbmq3dyd_ikj6ovt/view?usp=sharing)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/g4zo7j/classifying_a_part_a_robot_makes_from_its_power/',)", "identifyer": 5751281, "year": "2020"}, {"autor": "raghhuveer", "date": 1587634724000, "content": "Exploratory Data Analysis /!/ Are there any courses for conducting EDA?\n\nI understand that EDA is subjective to each problem.\n\nWhat I am asking is a course for the understanding mathematical tools like univariate analysis etc. used for EDA. \nIt is easy to plot a distplot (seaborn), but what does this plot signify?\n\nA course which answers questions like these, anyone?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g6jqja/exploratory_data_analysis/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "exploratory data analysis /!/ are there any courses for conducting eda?\n\ni understand that eda is subjective to each problem.\n\nwhat i am asking is a course for the understanding mathematical tools like univariate analysis etc. used for eda. \nit is easy to -----> plot !!!  a distplot (seaborn), but what does this -----> plot !!!  signify?\n\na course which answers questions like these, anyone?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/g6jqja/exploratory_data_analysis/',)", "identifyer": 5751385, "year": "2020"}, {"autor": "sdbhavsar3", "date": 1587549752000, "content": "Interpretation of seasonality and auto-correlation from the ACF plot of time series. /!/ I am new to time series analysis. I have a ACF plot of the time series, which is added below.\n\n&amp;#x200B;\n\n![img](7vyo1kk3ecu41)\n\nFrom what I have read, If there is a peak in correlation coefficient above confidence interval then the point will be used as a lag for auto-correlation. It seems that there is not strong auto-correlation at any point. Also , no seasonality is present in the time series as the pattern doesn't seem to repeat. Data is collected on monthly basis, but there are no peaks on 12,24.. marks. \n\nIs my interpretation correct? If not, what can we say about auto-correlation and seasonality? What more information can we get from the ACF plot ?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g5yf7d/interpretation_of_seasonality_and_autocorrelation/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "interpretation of seasonality and auto-correlation from the acf -----> plot !!!  of time series. /!/ i am new to time series analysis. i have a acf plot of the time series, which is added below.\n\n&amp;#x200b;\n\n![img](7vyo1kk3ecu41)\n\nfrom what i have read, if there is a peak in correlation coefficient above confidence interval then the point will be used as a lag for auto-correlation. it seems that there is not strong auto-correlation at any point. also , no seasonality is present in the time series as the pattern doesn't seem to repeat. data is collected on monthly basis, but there are no peaks on 12,24.. marks. \n\nis my interpretation correct? if not, what can we say about auto-correlation and seasonality? what more information can we get from the acf plot ?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/g5yf7d/interpretation_of_seasonality_and_autocorrelation/',)", "identifyer": 5751431, "year": "2020"}, {"autor": "xman236", "date": 1605455931000, "content": "How to build a regression model for the given dataset/plot /!/  \n\nHi all,\n\nMy goal is to estimate the speed of my robot axis for the given PWM value. The steps are as below:\n\n1. I have collected the joint angle values and time for each PWM values.\n2. For the given actuation range \\[-10,94\\], I calculate the velocity \\*delta\\_joint\\_angle/delta\\_time\\* over the actuation range.\n3. The resulting plot clearly shows that the speed of the axis is dependant on the PWM value (as expected ) but also on the actuation range.\n\n\\*\\*Questions:\\*\\*\n\n1. I want to create a function out of these data. For example, given a \\*new PWM value\\* 52 and its \\*actuation range\\* 60 degree, I want to have the corresponding speed value as a return.\n2. Is there any specific name for the curves as those in the plot?\n\n&amp;#x200B;\n\nhttps://preview.redd.it/qeinoos5ffz51.png?width=877&amp;format=png&amp;auto=webp&amp;s=16e0fba33655134036a783749af4af552c99e62a\n\n&amp;#x200B;\n\n \n\n`#PWM 127`\n\n`time_rec_arr_127= [-0.0001, 0.011904, 0.019957, 0.06122, 0.069186, 0.112189, 0.119187, 0.138182, 0.164646, 0.168611, 0.181123, 0.184149, 0.214126, 0.280989, 0.318277, 0.335865, 0.339869, 0.341869, 0.367903, 0.384722, 0.466847, 0.517933, 0.568947, 0.619093, 0.668763, 0.721958, 0.778761, 0.828763, 0.87868, 0.929024, 0.949024, 0.979023, 1.001027, 1.031207, 1.050207, 1.051263, 1.087883, 1.094893, 1.138643, 1.144639, 1.196708, 1.212708, 1.247311, 1.248311, 1.267334, 1.299844, 1.301225, 1.350856, 1.389739,`\n\n`1.448732, 1.651696, 1.70275, 1.749744, 1.808745, 1.829483, 1.831488, 1.854899, 1.857897, 1.865894, 1.902896, 1.905894, 1.907919, 1.987615, 2.023258, 2.025156, 2.031727, 2.04972, 2.090646, 2.147084, 2.175075, 2.177073, 2.179072, 2.180415, 2.182072, 2.207092, 2.211104, 2.264763, 2.337766, 2.339056, 2.418064, 2.423426, 2.472685, 2.47336, 2.531806, 2.574872, 2.575872, 2.595977, 2.636126, 2.674815, 2.722884, 2.724884, 2.753831, 2.756828, 2.789916, 2.791918, 2.793916, 2.848047, 2.874168, 2.881214, 2.963238, 2.98924, 2.991241, 2.993277, 2.99527, 3.022002, 3.047965, 3.054406, 3.055964, 3.057964, 3.08917, 3.09217, 3.09417, 3.138091, 3.1897, 3.194698, 3.236701, 3.239833, 3.288984, 3.29399, 3.336025, 3.339014, 3.342034, 3.344015, 3.387213, 3.389211, 3.39121, 3.39423, 3.443217, 3.461697, 3.462696, 3.501704, 3.504696, 3.506696, 3.507696, 3.528723, 3.545697, 3.561701, 3.6107, 3.649696, 3.657697, 3.700662, 3.911075, 3.928074, 3.984248, 4.019815, 4.027816, 4.060401, 4.097016, 4.102152, 4.106053, 4.10905, 4.132684, 4.13768, 4.140481, 4.142487, 4.158488, 4.194103, 4.218187, 4.256776, 4.276872, 4.334954, 4.340953, 4.359956, 4.377988, 4.518976, 4.52701, 4.593064, 4.625465, 4.641982, 4.656948, 4.659947, 4.706946, 4.724996, 4.740489, 4.79288, 4.834953, 4.837879, 4.840879, 4.84388, 4.90988, 4.942341, 4.965277, 4.968275, 4.976293, 5.050181, 5.053007, 5.098034, 5.100452, 5.148103, 5.152058, 5.155057, 5.198541, 5.200535, 5.203559, 5.246079, 5.29721, 5.300222, 5.302202, 5.30521, 5.353387, 5.356386, 5.372613, 5.421645, 5.424645, 5.47471, 5.49071, 5.524791, 5.556875, 5.572875, 5.588975, 5.606968, 5.623971, 5.640969, 5.656009, 5.67501, 5.707009, 5.733009, 5.736009, 5.771015, 5.797173, 5.854627, 5.889182, 5.907697, 5.923697, 5.940697, 5.973724, 5.989697, 6.022697, 6.040716, 6.065269, 6.069249, 6.156322, 6.173883, 6.178849, 6.18885, 6.212944, 6.278934, 6.284934, 6.305936, 6.355034]`\n\n`axis5_arr_127= [95.5263, 95.5263, 95.0, 95.3509, 95.3509, 95.3509, 95.5263, 95.3509, 95.3509, 95.3509, 95.5263, 95.3509, 95.3509, 95.3509, 95.3509, 95.1754, 95.3509, 95.1754, 95.3509, 95.1754, 95.0, 95.3509, 95.1754, 95.5263, 95.1754, 95.3509, 95.3509, 95.1754, 95.1754, 95.3509, 95.3509, 95.5263, 95.0, 95.3509, 95.3509, 95.1754, 95.1754, 93.7719, 93.7719, 91.8421, 91.6667, 90.4386, 90.2632, 89.9123, 89.0351, 89.0351, 87.4561, 85.8772, 83.7719, 77.2807, 75.5263, 73.7719, 72.0175, 71.8421, 71.4912, 71.3158, 70.4386, 69.9123, 69.7368, 69.386, 69.386, 66.5789, 65.1754, 64.8246, 64.4737, 64.2982, 63.7719, 61.1404, 60.7895, 60.614, 60.4386, 59.7368, 59.386, 59.2105, 58.8596, 56.4035, 54.6491, 54.4737, 51.3158, 51.1404, 49.386, 49.386, 47.807, 46.4035, 46.2281, 45.7018, 44.1228, 42.7193, 42.193, 42.193, 40.614, 40.614, 39.386, 39.2105, 39.0351, 37.2807, 37.2807, 35.8772, 34.1228, 33.7719, 33.5965, 32.3684, 32.0175, 31.8421, 30.7895, 30.2632, 30.2632, 29.9123, 29.0351, 28.6842, 28.5088, 27.2807, 25.3509, 25.3509, 24.8246, 24.8246, 22.3684, 21.8421, 21.4912, 21.1404, 20.2632, 19.9123, 19.9123, 19.5614, 19.0351, 18.5088, 16.9298, 16.4035, 16.2281, 16.0526, 15.3509,`\n\n`15.0, 14.6491, 14.4737, 14.4737, 13.7719, 12.193, 10.0877, 9.5614, 8.5088, 1.1404, 0.9649, -0.2632, -2.3684, -2.5439, -2.8947, -3.2456, -3.9474, -4.4737, -4.4737, -4.8246, -5.7018, -6.0526, -6.0526, -6.5789, -6.9298, -8.8596, -10.4386, -10.614, -12.7193, -13.4211, -13.5965, -14.1228, -19.5614, -19.7368, -20.9649, -21.3158, -21.3158, -22.7193, -23.0702, -24.4737, -25.0, -26.4035, -27.9825, -28.1579, -28.5088, -29.5614, -29.9123, -31.3158, -31.3158, -31.8421, -32.7193, -34.4737, -36.5789, -36.7544, -38.3333, -38.5088, -39.9123, -39.9123, -40.2632, -41.3158, -41.6667, -41.8421, -43.0702, -44.8246, -45.0, -45.1754, -45.5263, -46.5789, -46.5789, -46.7544, -47.4561, -47.807, -47.6316, -47.6316, -47.2807, -47.6316, -47.6316, -47.2807, -47.4561, -47.4561, -47.2807, -47.6316, -47.6316, -47.4561, -47.6316, -47.4561, -47.807, -47.2807, -47.6316, -47.6316, -47.807, -47.4561, -47.6316, -47.4561, -47.4561, -47.4561, -47.2807, -47.6316, -47.4561, -47.6316, -47.6316, -47.4561, -47.4561, -47.4561, -47.6316, -47.4561, -47.4561, -47.4561, -47.6316]`\n\n`# pwm 118`\n\n`time_rec_arr_118= [0.673386, 0.689433, 0.691463, 0.707491, 0.724535, 0.739536, 0.773204, 0.792561, 0.806596, 0.807557, 0.839556, 0.840929, 0.855653, 0.873374, 0.920374, 0.987874, 1.005402, 1.02343, 1.043474, 1.072891, 1.13009, 1.180739, 1.231413, 1.238413, 1.28251, 1.332511, 1.383498, 1.403514, 1.435173, 1.454417, 1.488441, 1.535685, 1.594504, 1.601503, 1.621505, 1.672655, 1.705938, 1.720528, 1.722727, 1.756519, 1.7924, 1.795398, 1.798393, 1.844395, 1.851394, 1.87218, 1.887488, 1.905487, 1.922318, 1.937606, 1.955653, 1.97169, 2.022369, 2.028395, 2.032818, 2.059598, 2.099548, 2.149547, 2.171589, 2.188946, 2.236516, 2.269515, 2.308747, 2.315743, 2.3435, 2.361135, 2.36313, 2.36413, 2.386714, 2.404426, 2.437282, 2.486609, 2.509611, 2.566089, 2.618146, 2.637604, 2.668382, 2.687628, 2.753453, 2.771455, 2.797543, 2.799543, 2.801543, 2.803543, 2.837657, 2.869658, 2.887764, 2.923341, 2.928336, 2.930336, 2.966067, 3.016571, 3.067571, 3.098288, 3.126235, 3.177256, 3.183256, 3.225409, 3.276739, 3.28566, 3.3408, 3.342718, 3.390667, 3.393587, 3.440587, 3.442587, 3.498265, 3.552715, 3.56624, 3.600799, 3.649839, 3.664798, 3.7008, 3.7008, 3.730835, 3.735835, 3.81392, 3.817884, 3.834884, 3.863884, 3.866885, 3.881885, 3.916912, 3.955885, 3.957885, 4.008224, 4.058376, 4.099038, 4.159361, 4.20501, 4.213354, 4.249498, 4.266059, 4.298184, 4.315184, 4.329184, 4.333489, 4.365479, 4.384478, 4.41548, 4.430557, 4.4795, 4.482795, 4.498499, 4.51339, 4.51739, 4.578528, 4.582505, 4.598507, 4.615507, 4.632904, 4.650506, 4.666826, 4.724505, 4.727505, 4.730506, 4.733228, 4.76346, 4.766459, 4.821788, 4.871335, 4.92997, 4.945974, 4.994735, 5.04285, 5.045848, 5.04885, 5.082072, 5.111778, 5.115778, 5.145779, 5.162344, 5.17834, 5.194341, 5.199346, 5.230612, 5.247336, 5.300343, 5.345339, 5.388338, 5.440375, 5.488283, 5.544441, 5.580831, 5.64248, 5.681494, 5.712442, 5.747258, 5.78226, 5.813777, 5.831189, 5.860851, 5.876857, 5.880589, 5.897546, 5.914608, 5.964271, 6.012271, 6.050368, 6.055353, 6.058354, 6.062353, 6.079291, 6.12729, 6.146772, 6.178266, 6.201727, 6.205691, 6.209691, 6.243405, 6.263403, 6.298684, 6.304679, 6.308679, 6.351416, 6.356394, 6.360358, 6.379397, 6.427253, 6.445548, 6.476537, 6.512973, 6.544248,`\n\n`6.576248, 6.611255, 6.643286, 6.661293, 6.679281]`\n\n`axis5_arr_118= [ 95.0, 95.0, 94.6491, 93.9474, 93.9474, 92.7193, 92.3684, 92.193, 92.0175, 91.1404, 90.9649, 90.7895, 90.0877, 88.5088, 86.7544, 86.4035, 85.8772, 85.7018, 84.1228, 82.8947, 81.3158, 79.7368, 79.5614, 78.3333, 77.4561, 75.8772, 75.8772, 74.2982, 73.9474, 72.7193, 71.6667, 69.7368, 69.5614, 69.386, 68.1579, 66.9298, 66.9298, 66.5789, 65.5263, 65.5263, 65.1754, 65.0, 62.7193, 62.5439, 62.3684, 62.0175, 61.3158, 61.1404, 60.9649, 60.7895, 59.9123, 58.3333, 57.9825, 57.807, 57.1053, 55.3509, 54.1228, 53.5965, 53.4211, 53.2456, 52.3684, 49.5614, 49.2105, 49.0351, 48.6842, 47.9825, 47.6316, 47.2807, 47.2807, 46.7544, 45.3509, 43.5965, 42.0175, 40.614, 40.2632, 40.2632, 39.9123, 39.2105, 38.8596, 38.6842, 36.0526, 35.8772, 35.7018, 35.5263, 35.0, 33.4211, 33.2456, 32.0175, 32.0175, 30.614, 29.2105, 29.0351, 27.6316, 26.0526, 24.6491, 24.6491, 23.4211, 21.8421, 21.6667, 20.2632, 19.9123, 19.0351, 18.6842, 17.807, 17.2807, 16.0526, 14.8246, 14.2982, 13.0702, 11.6667, 11.3158, 10.2632, 10.0877, 9.9123, 9.2105, 7.807, 7.2807, 7.1053, 6.2281, 5.8772, 5.7018, 4.6491, 2.8947, 3.0702, 1.6667, 0.0877, -1.1404, -2.7193, -4.8246, -4.4737, -5.7018, -6.0526, -6.2281, -6.4035, -7.2807, -7.6316, -7.9825, -7.9825, -8.8596, -10.2632, -11.4912, -11.8421, -12.0175, -12.0175, -12.7193, -14.4737, -14.8246, -15.0, -15.3509, -15.3509, -15.8772, -17.4561, -18.6842, -19.0351, -19.2105, -19.386, -19.7368, -20.2632, -21.3158, -23.2456, -24.6491, -24.8246, -26.2281, -27.807, -27.9825, -28.5088, -28.5088, -29.2105, -29.5614, -29.5614, -30.7895, -30.9649, -31.3158, -31.8421, -32.3684, -33.4211, -35.0, -36.0526, -37.6316, -38.8596, -40.7895, -42.193, -43.2456, -44.8246, -46.0526, -46.4035, -47.2807, -47.2807, -47.4561, -47.9825, -47.9825, -48.1579, -48.3333, -48.5088, -48.3333, -48.3333, -48.3333, -48.3333, -48.5088, -48.3333, -48.5088, -48.1579, -48.3333, -48.1579, -48.5088, -48.3333, -48.5088,`\n\n`-48.1579, -48.3333, -48.3333, -48.5088, -48.5088, -48.3333, -48.3333, -48.3333, -48.5088, -48.6842, -48.1579, -48.3333, -48.3333, -48.3333, -48.1579, -48.3333, -48.3333, -48.3333, -48.1579, -48.6842, -48.3333]`\n\n`axis5_arr_58= [95.1754, 95.3509, 95.3509, 95.1754, 95.3509, 95.5263, 95.3509, 95.5263, 95.0, 95.3509, 95.3509, 95.3509, 95.1754, 95.3509, 95.5263, 95.3509, 95.3509, 95.3509, 95.0, 95.1754, 95.5263, 95.3509, 95.3509, 95.1754, 95.1754, 95.3509, 95.0, 95.1754, 95.1754, 95.3509, 95.3509, 95.3509, 95.5263, 95.5263, 95.3509, 95.3509, 95.1754, 94.8246, 95.1754, 95.0, 95.0, 94.6491, 94.2982, 94.1228, 94.2982, 94.1228, 94.2982, 94.1228, 93.9474, 94.1228, 93.7719, 93.7719, 93.4211, 93.2456, 93.2456, 93.4211, 92.8947, 92.8947, 92.8947, 92.7193, 92.3684, 92.5439, 92.5439, 92.5439, 92.3684, 92.193, 92.0175, 91.8421, 91.8421, 91.6667, 92.0175, 91.8421, 91.4912, 91.3158, 91.3158, 90.9649, 90.7895, 90.9649, 90.4386, 90.614, 90.2632, 89.9123, 89.5614, 89.386, 89.386, 89.2105, 88.8596, 88.8596, 88.5088, 88.5088, 88.3333, 87.807, 87.6316, 87.2807, 86.7544, 86.5789, 86.4035, 86.2281, 85.7018, 85.8772, 85.5263, 85.3509, 85.0, 85.1754, 85.0, 84.6491, 84.4737, 84.4737, 84.2982, 83.9474, 84.1228, 84.1228, 83.7719,`\n\n`83.7719, 83.4211, 83.4211, 83.0702, 83.2456, 83.2456, 82.8947, 82.8947, 83.0702, 82.8947, 82.5439, 82.3684, 82.5439, 82.193, 82.3684, 82.3684, 82.193, 82.0175, 82.0175, 82.193, 81.8421, 81.8421, 81.6667, 81.4912, 81.4912, 81.3158, 80.9649, 80.9649, 80.9649, 80.614, 80.614, 80.4386, 80.2632, 80.2632, 80.2632, 80.4386, 80.0877, 80.0877, 80.0877, 80.2632, 79.9123, 79.5614, 79.7368, 79.7368, 79.386, 79.386, 79.2105, 79.2105, 79.0351, 79.0351, 79.0351, 79.0351, 78.6842, 78.5088, 78.1579, 78.1579, 78.1579, 77.9825, 78.1579, 77.807, 77.9825, 77.9825, 77.6316, 77.4561, 77.6316, 77.4561, 77.4561, 77.2807, 77.1053, 77.2807, 76.9298, 76.7544, 76.5789, 76.5789, 76.5789, 76.5789, 76.2281, 75.8772, 75.3509, 75.1754, 75.1754, 74.6491, 74.2982, 74.2982, 73.9474, 73.4211, 73.0702, 72.7193, 73.0702, 72.7193, 72.8947, 72.3684, 72.3684, 72.3684, 72.0175, 72.193, 72.193, 71.6667, 72.0175, 71.8421, 71.4912, 71.6667, 71.4912, 71.3158, 71.1404, 71.3158, 70.9649, 70.7895, 70.7895, 70.7895, 70.614, 70.4386, 69.9123, 70.0877, 70.0877, 70.0877, 69.7368, 69.9123, 69.7368, 69.7368, 69.5614, 69.386, 69.0351, 69.2105, 69.0351, 68.8596, 68.8596, 68.8596, 68.5088, 68.6842, 68.6842, 68.3333, 68.1579, 67.9825, 67.807, 67.807, 67.9825, 67.6316, 67.1053, 67.2807, 67.1053, 67.1053, 66.9298, 66.7544, 66.9298, 66.9298, 66.7544, 66.5789, 66.7544, 66.5789,`\n\n`66.4035, 66.4035, 66.0526, 66.4035, 66.2281, 66.0526, 66.0526, 66.0526, 66.0526, 65.7018, 65.7018, 65.7018, 65.7018, 65.5263, 65.5263, 65.5263, 65.3509, 65.0, 65.0, 65.1754, 64.8246, 64.8246, 64.6491, 64.6491, 64.4737, 64.6491, 64.4737, 64.2982, 63.9474, 64.1228, 63.9474, 63.9474, 63.9474, 63.7719, 63.7719, 63.9474, 63.5965, 63.4211, 63.0702, 63.2456, 63.2456, 63.0702, 62.8947, 63.0702, 62.8947, 62.8947, 62.7193, 62.5439, 62.5439, 62.193, 62.0175, 62.193, 61.8421, 61.8421, 61.6667, 61.4912, 61.4912, 61.6667, 60.9649, 60.9649, 60.614, 60.4386, 60.614, 60.2632, 59.5614, 59.9123, 59.5614, 59.5614, 59.386, 59.2105, 58.8596, 59.0351, 58.8596, 58.6842, 58.5088, 58.5088, 58.3333, 58.3333, 58.3333, 58.1579, 57.9825, 57.9825, 57.6316, 57.807, 57.807, 57.4561, 57.4561, 57.2807, 57.2807, 57.1053, 57.1053, 56.7544, 56.7544, 56.5789, 56.4035, 56.4035, 56.7544, 56.2281, 56.2281, 56.2281, 55.8772, 55.8772, 55.8772, 55.7018, 55.8772, 55.5263, 55.3509, 55.5263, 55.5263, 55.0, 55.1754, 55.1754, 55.0, 54.8246, 54.8246, 54.2982, 53.9474, 53.7719, 53.4211, 52.8947, 52.8947, 52.5439, 52.7193, 52.3684, 52.0175, 51.6667, 51.6667, 51.6667, 51.3158, 50.9649, 50.614, 50.614, 50.2632, 49.7368, 49.386, 49.5614, 49.5614, 49.2105, 48.8596, 48.5088, 48.5088, 48.3333, 48.5088, 48.1579, 47.807, 47.9825, 47.6316, 47.2807, 47.4561, 47.4561, 47.4561,`\n\n`46.9298, 46.7544, 46.5789, 46.5789, 46.4035, 46.2281, 46.2281, 46.0526, 46.0526, 46.0526, 46.0526, 45.8772, 45.7018, 45.8772, 45.5263, 45.3509, 45.3509, 45.5263, 45.1754, 45.0, 45.0, 45.0, 45.0, 45.0, 44.8246, 44.2982, 44.2982, 44.2982, 44.2982, 44.1228, 44.1228, 44.2982, 44.1228, 43.9474, 43.7719, 43.7719, 43.5965, 43.4211, 43.4211, 43.5965, 43.2456, 43.2456, 43.2456, 43.2456, 43.2456, 43.0702, 43.2456, 42.8947, 43.0702, 43.0702, 42.8947, 42.8947, 42.7193, 42.7193, 42.5439, 42.193, 42.3684, 42.0175, 42.3684, 42.193, 42.0175, 41.6667, 41.8421, 41.6667, 41.6667, 41.3158, 41.3158, 41.3158, 41.1404, 41.1404, 40.9649, 41.1404, 40.9649, 40.7895, 40.9649, 40.7895, 40.614, 40.614, 40.614, 40.2632, 40.0877, 40.0877, 40.0877, 40.0877, 39.7368, 39.9123, 39.7368, 39.5614, 39.7368, 39.386, 39.386, 39.2105, 39.0351, 38.8596, 38.5088, 38.3333, 37.9825, 37.6316, 37.2807, 37.1053, 36.7544, 36.7544, 36.5789, 36.2281, 36.0526, 35.8772, 35.5263, 35.7018, 35.3509, 35.1754, 35.0, 34.8246, 34.8246, 34.4737, 34.1228, 34.1228, 33.7719, 33.4211, 33.4211, 33.2456, 33.5965, 32.8947, 32.8947, 32.7193, 32.7193, 32.5439, 32.5439, 32.5439, 32.193, 32.0175, 32.0175, 31.8421, 32.0175, 31.6667, 31.4912, 31.4912, 31.3158, 31.3158, 31.3158, 30.9649, 30.7895, 30.7895, 30.614, 30.614, 30.4386, 30.4386, 30.4386, 30.4386, 30.4386, 30.0877, 30.0877, 29.9123, 29.9123, 29.7368, 29.9123, 29.5614, 29.7368, 29.5614, 29.5614, 29.5614, 29.386, 29.386, 28.8596, 28.5088, 28.8596, 28.6842, 28.5088, 28.6842, 28.5088, 28.6842, 28.3333, 28.3333, 28.1579, 27.9825, 27.9825, 27.807, 27.4561, 27.4561, 27.4561, 27.4561, 27.2807, 26.9298, 26.9298, 26.7544, 26.7544, 26.5789, 26.5789, 26.5789, 26.4035,`\n\n`26.5789, 26.7544, 26.2281, 26.2281, 25.8772, 26.0526, 25.8772, 25.7018, 25.7018, 25.5263, 25.1754, 24.8246, 24.6491, 24.8246, 24.6491, 24.4737, 24.1228, 23.9474, 23.7719, 23.4211, 22.8947, 22.8947, 22.3684, 22.193, 21.8421, 22.0175, 21.8421, 21.4912, 20.7895, 20.9649, 20.7895, 20.7895, 20.7895, 20.614, 20.614, 20.2632, 20.4386, 20.0877, 20.2632, 19.9123, 19.9123, 19.7368, 19.7368, 19.5614, 19.7368, 19.2105, 19.386, 19.0351, 19.2105, 18.8596, 19.0351, 18.6842, 18.6842, 18.5088, 18.6842, 18.3333, 18.1579, 18.1579, 17.9825, 17.807, 17.807, 17.6316, 17.6316, 17.4561, 17.6316, 17.2807, 16.9298, 16.9298, 16.9298, 16.9298, 16.5789, 16.5789, 16.5789, 16.2281, 16.0526, 15.8772, 15.8772, 15.8772, 15.5263, 15.7018, 15.3509, 15.1754, 15.1754, 15.0, 15.0, 14.6491, 14.2982, 14.2982, 14.4737, 13.9474, 13.9474, 13.9474, 13.5965, 13.9474, 13.7719, 13.9474, 13.5965, 13.4211, 13.2456, 13.4211, 13.4211, 12.8947, 12.8947, 12.8947, 12.7193, 12.8947, 12.7193, 12.7193, 12.8947, 12.3684, 12.3684, 12.193, 12.0175, 12.0175, 11.6667, 11.6667, 11.8421, 11.8421, 11.8421, 11.4912, 11.3158, 11.1404, 11.1404, 11.1404, 11.1404, 10.7895, 10.9649, 10.9649, 10.4386, 10.4386, 10.2632, 10.4386, 10.4386, 10.4386, 9.9123, 9.7368, 9.7368, 9.5614, 9.2105, 9.2105, 9.2105, 9.386, 9.0351, 8.5088, 8.5088, 8.5088, 8.3333, 7.9825, 8.1579, 7.807, 7.807, 7.6316, 7.6316, 7.807, 7.4561, 7.4561, 7.4561, 7.4561, 7.2807, 6.7544, 6.5789, 6.9298, 6.5789, 6.5789, 6.4035, 6.5789, 6.5789, 6.2281, 6.4035, 6.2281, 6.0526, 5.5263, 5.7018, 5.7018, 5.5263, 5.1754, 5.0, 4.4737, 4.4737, 4.2982, 4.2982, 3.9474, 3.5965, 3.7719, 3.4211, 3.4211, 3.5965, 3.2456, 3.2456, 3.0702, 2.8947, 2.3684, 2.3684, 2.3684]`\n\n`time_rec_arr_58= [ 0.01342, 0.06516, 0.12308, 0.177018, 0.243046, 0.248356, 0.277973, 0.294017, 0.324266, 0.344595, 0.383019, 0.389654, 0.423115, 0.450117, 0.450117, 0.483922, 0.487936, 0.532136, 0.538199, 0.573943, 0.600514, 0.602247, 0.603513, 0.635145, 0.638287, 0.668582, 0.703175, 0.703736, 0.736097, 0.744289, 0.786205, 0.78719, 0.852216, 0.887478, 0.922137, 0.923136, 0.923136, 0.952411, 0.953011, 0.983801, 0.984768, 1.008911, 1.070235, 1.072236, 1.074541, 1.106241, 1.10724, 1.10824, 1.135292, 1.136773, 1.171732, 1.173164, 1.199479, 1.205942, 1.232959, 1.235411, 1.271545, 1.304878, 1.337022, 1.339045, 1.359051, 1.368258, 1.369843, 1.371828, 1.392055, 1.418092, 1.458963, 1.465946, 1.468987, 1.497936, 1.500944, 1.524962, 1.552524, 1.587543, 1.589058, 1.620324, 1.622102, 1.649349, 1.684896, 1.717958, 1.764054, 1.813197, 1.871957, 1.930841, 1.932842, 1.934841, 1.968895, 2.032058, 2.068385, 2.071751, 2.133044, 2.171068, 2.220186, 2.283181, 2.372319, 2.419012, 2.422014, 2.481236, 2.528189, 2.533196, 2.598468, 2.600534, 2.633011, 2.683301, 2.684318, 2.745301, 2.748842, 2.782485, 2.816704, 2.851095, 2.853171, 2.881347, 2.91295, 2.918952, 2.949357, 2.950356, 3.013163, 3.015153, 3.017152, 3.04825, 3.050166, 3.053152, 3.088567, 3.192188, 3.194148, 3.200151, 3.202151, 3.205149, 3.226557, 3.22973, 3.231843, 3.23452, 3.237854, 3.262568, 3.266059, 3.268966, 3.289924, 3.292923, 3.346009, 3.402092, 3.438186, 3.470196, 3.494194, 3.497187, 3.499617, 3.501062, 3.528112, 3.530108, 3.534108,`\n\n`3.55811, 3.562207, 3.56515, 3.594001, 3.629215, 3.631215, 3.633215, 3.635233, 3.670906, 3.70334, 3.727003, 3.731003, 3.757053, 3.763052, 3.769056, 3.792146, 3.825887,`\n\n`3.830198, 3.860579, 3.868227, 3.913921, 3.937883, 3.942883, 3.945884, 3.973424, 3.976409, 4.010119, 4.016117, 4.043128, 4.045104, 4.074645, 4.076766, 4.082644, 4.101845, 4.142885, 4.146885, 4.174922, 4.180196, 4.202953, 4.214966, 4.262989, 4.317954, 4.375087, 4.427085, 4.47527, 4.525989, 4.575991, 4.626038, 4.675997, 4.727992, 4.778186, 4.828362, 4.865592, 4.888178, 4.894168, 4.927579, 4.951128, 4.956162, 5.000129, 5.01816, 5.06613, 5.100059, 5.132992, 5.149009, 5.189553, 5.193583, 5.196568, 5.220157, 5.224146, 5.228172, 5.233144, 5.255152, 5.260145, 5.263177, 5.318256, 5.323255, 5.361198, 5.38636, 5.389196, 5.394197, 5.397228, 5.422532, 5.427524, 5.431523, 5.457157, 5.499363, 5.532227, 5.565227, 5.603251, 5.607285, 5.61235, 5.635546, 5.640515, 5.644516, 5.648517, 5.673038, 5.724634, 5.753676, 5.761198, 5.802209, 5.806197,`\n\n`5.809215, 5.853949, 5.858976, 5.897998, 5.923986, 5.928989, 5.949992, 5.976001, 5.980269, 6.015273, 6.042831, 6.046795, 6.064797, 6.090257, 6.096232, 6.122416, 6.125812, 6.13081, 6.16493, 6.204058, 6.209059, 6.214253, 6.235055, 6.239081, 6.243082, 6.247083, 6.263084, 6.28101, 6.339067, 6.349053, 6.397483, 6.414482, 6.484464, 6.489473, 6.492456, 6.496456, 6.513456, 6.562517, 6.58171, 6.608944, 6.612977, 6.664965, 6.737944, 6.741944, 6.746975, 6.772227, 6.777108, 6.819741, 6.825738, 6.854917, 6.860915, 6.876915, 6.895065, 6.945878, 6.985837, 6.989835, 6.993834, 7.036106, 7.041327, 7.045835, 7.103162, 7.109164, 7.113162, 7.134163, 7.138163, 7.142163, 7.146163, 7.163164, 7.212167, 7.235164, 7.239164, 7.244163, 7.261162, 7.286136, 7.290146, 7.294137, 7.312103, 7.336236, 7.342978, 7.378204, 7.412255, 7.438221, 7.442222, 7.461959, 7.489049, 7.494004, 7.546173, 7.586061, 7.594318, 7.645982, 7.731855, 7.737841, 7.743844, 7.769845, 7.774878, 7.793055, 7.848873, 7.855847, 7.876849, 7.909915, 7.933939, 7.937938, 7.942939, 7.961958, 7.9939, 8.02708, 8.057862, 8.077902, 8.126641, 8.194637, 8.226791, 8.273764, 8.293916, 8.326022, 8.376098, 8.40888, 8.426879, 8.476216, 8.494216, 8.541929, 8.60246, 8.609018, 8.626392, 8.657334, 8.707495, 8.72501, 8.773012, 8.792011, 8.809011, 8.815014, 8.82003, 8.82503, 8.844032, 8.866127, 8.871127, 8.875127, 8.906155, 8.941151, 8.973155, 8.991175, 9.022054, 9.058176, 9.125108, 9.157896, 9.192268, 9.255997, 9.274996, 9.324997, 9.364828, 9.36986, 9.373826, 9.406145, 9.42514, 9.455105, 9.474171, 9.505183, 9.541249, 9.589983, 9.625096, 9.65711, 9.708118, 9.740053, 9.79008, 9.823066, 9.871483, 9.890143, 9.953975, 9.973477, 10.042106, 10.088645, 10.153224, 10.187264, 10.219264, 10.240247, 10.288291, 10.32218, 10.37097, 10.388967, 10.452969, 10.473022, 10.498993, 10.503993, 10.542941, 10.548966, 10.555966, 10.582474, 10.588274, 10.637902, 10.670128, 10.689128, 10.752889, 10.78589, 10.804887, 10.837257, 10.886965, 10.918969, 10.938025, 10.997076, 11.003076, 11.035099, 11.0551, 11.088103, 11.170033, 11.217879, 11.250854, 11.270879, 11.334886, 11.386264, 11.401272, 11.453354, 11.503674, 11.553716, 11.587283, 11.650908, 11.683923, 11.717924, 11.750924, 11.78498, 11.82006, 11.878624, 11.885303, 11.936857, 11.969826, 12.002927, 12.052032, 12.08505, 12.133471, 12.166662, 12.200996, 12.234125,`\n\n`12.267168, 12.301872, 12.33602, 12.378293, 12.384276, 12.41733, 12.448114, 12.468172, 12.518455, 12.549435, 12.582433, 12.601797, 12.64314, 12.649142, 12.681142, 12.717141, 12.750141, 12.784254, 12.846883, 12.875172, 12.881174, 12.900927, 12.964926, 12.997988, 13.033957, 13.070938, 13.077929, 13.08398, 13.164099, 13.199602, 13.266198, 13.316222, 13.396254, 13.43024, 13.465024, 13.515879, 13.559097, 13.566132, 13.600093, 13.650031, 13.69722, 13.761631, 13.796059, 13.829983, 13.865045, 13.913233, 13.964444, 14.031192, 14.096115, 14.162747, 14.210811, 14.245953, 14.278972, 14.31319, 14.371054, 14.377051, 14.412258, 14.439276, 14.446288, 14.480323, 14.544215, 14.576196, 14.626878, 14.6968, 14.745957, 14.811249, 14.872427, 14.893255, 14.928254, 14.993306, 15.027231, 15.06137, 15.095007, 15.160916, 15.194957, 15.242929, 15.295928, 15.344957, 15.394006, 15.442006, 15.477009, 15.527657, 15.575276, 15.61165, 15.659996, 15.694853, 15.759141, 15.791224, 15.824998, 15.859369, 15.885152, 15.892172, 15.924173, 15.95885, 15.99241, 16.026442, 16.078017, 16.131988, 16.139983, 16.174985, 16.223009, 16.259008, 16.309404, 16.356927, 16.407214, 16.44304, 16.492221, 16.540243, 16.57522, 16.608301, 16.689201, 16.726198, 16.773933, 16.806938, 16.840988, 16.862945, 16.875464, 16.938927, 16.973897, 17.008901, 17.058131, 17.108122, 17.156986, 17.207069, 17.241094, 17.289131, 17.324129, 17.407127, 17.471153, 17.507185, 17.588086, 17.638921, 17.673451, 17.722971, 17.787998, 17.839027, 17.881113, 17.888113,`\n\n`17.938115, 17.986224, 18.023232, 18.070484, 18.105426, 18.172536, 18.239465, 18.304669, 18.355735, 18.401502, 18.452121, 18.488053, 18.555025, 18.603056, 18.644143, 18.652537, 18.68915, 18.753149, 18.818375, 18.854348, 18.898943, 18.920024, 18.967945, 19.003944, 19.067947, 19.102967, 19.152996, 19.201997, 19.253219, 19.303357, 19.369405, 19.418109, 19.468083, 19.519139, 19.553164, 19.603037, 19.642833, 19.650834, 19.715833, 19.751834, 19.817834, 19.866834, 19.91786, 19.98334, 20.050848, 20.099849, 20.142097, 20.150096, 20.201271, 20.249886, 20.315205, 20.366261, 20.415998, 20.468387, 20.517025, 20.596874, 20.648995, 20.698035, 20.749014, 20.799032, 20.847086,`\n\n`20.898838, 20.933925, 20.998227, 21.063493, 21.099021, 21.141637, 21.163191, 21.213214, 21.279844, 21.315845, 21.366468, 21.432188, 21.498972, 21.548041, 21.59528, 21.663983, 21.714913, 21.763873, 21.81289, 21.862874, 21.927918, 21.964929, 22.014916, 22.064826, 22.130826, 22.180826, 22.261946, 22.312869, 22.362847, 22.426321, 22.463895, 22.51287, 22.56187, 22.611902, 22.662993, 22.726853, 22.777898, 22.842853, 22.896873, 22.976485, 23.029913, 23.079066, 23.143257, 23.193878, 23.243957, 23.291977, 23.341977, 23.393011, 23.443726, 23.491152, 23.544885, 23.609842, 23.661368, 23.711368, 23.809851, 23.86005, 23.92505, 23.975053, 24.02605, 24.093997, 24.158564, 24.209714, 24.259895, 24.326029, 24.374029, 24.442848, 24.524285, 24.574008, 24.624286, 24.707886, 24.775651, 24.838039, 24.921235, 24.990098, 25.040862, 25.091897, 25.188123, 25.240209, 25.289191, 25.355819, 25.413186, 25.422181, 25.458182, 25.518908, 25.556857, 25.606297, 25.673476, 25.738836, 25.788355, 25.838075, 25.906028, 25.973012, 26.040013, 26.102012, 26.168845, 26.236988, 26.287988, 26.369035, 26.415952, 26.453034, 26.505033, 26.55596, 26.62007, 26.685697, 26.751048, 26.801293, 26.838227, 26.903295, 26.953379, 27.00367, 27.066581, 27.118684, 27.217879, 27.279561, 27.289839, 27.300836, 27.384017, 27.53471]`\n\n`axis5_pwm118_over_actuaction_range=[]`\n\n`axis5_pwm127_over_actuaction_range=[]`\n\n`axis5_pwm58_over_actuaction_range=[]`\n\n`for i in (range(-10,94)):`\n\n`tmp=(95-i)/(np.interp(i,axis5_arr_118[::-1],time_rec_arr_118[::-1]))`\n\n`axis5_pwm118_over_actuaction_range.append(tmp)`\n\n`tmp=(95-i)/(np.interp(i,axis5_arr_127[::-1],time_rec_arr_127[::-1]))`\n\n`axis5_pwm127_over_actuaction_range.append(tmp)`\n\n`tmp=(95-i)/(np.interp(i,axis5_arr_58[::-1],time_rec_arr_58[::-1]))`\n\n`axis5_pwm58_over_actuaction_range.append(tmp)`\n\n`fig4= plt.figure(figsize=(10,6))`\n\n`ax4=fig4.add_subplot(111)`\n\n`print (axis5_pwm118_over_actuaction_range)`\n\n`ax4.plot(range(-10,94),axis5_pwm118_over_actuaction_range,label='pwm118')`\n\n`ax4.plot(range(-10,94),axis5_pwm127_over_actuaction_range,label='pwm127')`\n\n`ax4.plot(range(-10,94),axis5_pwm58_over_actuaction_range,label='pwm58')`\n\n[`plt.show`](https://plt.show/)`()`", "link": "https://www.reddit.com/r/learnmachinelearning/comments/juo21f/how_to_build_a_regression_model_for_the_given/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "how to build a regression model for the given dataset/-----> plot !!!  /!/  \n\nhi all,\n\nmy goal is to estimate the speed of my robot axis for the given pwm value. the steps are as below:\n\n1. i have collected the joint angle values and time for each pwm values.\n2. for the given actuation range \\[-10,94\\], i calculate the velocity \\*delta\\_joint\\_angle/delta\\_time\\* over the actuation range.\n3. the resulting plot clearly shows that the speed of the axis is dependant on the pwm value (as expected ) but also on the actuation range.\n\n\\*\\*questions:\\*\\*\n\n1. i want to create a function out of these data. for example, given a \\*new pwm value\\* 52 and its \\*actuation range\\* 60 degree, i want to have the corresponding speed value as a return.\n2. is there any specific name for the curves as those in the plot?\n\n&amp;#x200b;\n\nhttps://preview.redd.it/qeinoos5ffz51.png?width=877&amp;format=png&amp;auto=webp&amp;s=16e0fba33655134036a783749af4af552c99e62a\n\n&amp;#x200b;\n\n \n\n`#pwm 127`\n\n`time_rec_arr_127= [-0.0001, 0.011904, 0.019957, 0.06122, 0.069186, 0.112189, 0.119187, 0.138182, 0.164646, 0.168611, 0.181123, 0.184149, 0.214126, 0.280989, 0.318277, 0.335865, 0.339869, 0.341869, 0.367903, 0.384722, 0.466847, 0.517933, 0.568947, 0.619093, 0.668763, 0.721958, 0.778761, 0.828763, 0.87868, 0.929024, 0.949024, 0.979023, 1.001027, 1.031207, 1.050207, 1.051263, 1.087883, 1.094893, 1.138643, 1.144639, 1.196708, 1.212708, 1.247311, 1.248311, 1.267334, 1.299844, 1.301225, 1.350856, 1.389739,`\n\n`1.448732, 1.651696, 1.70275, 1.749744, 1.808745, 1.829483, 1.831488, 1.854899, 1.857897, 1.865894, 1.902896, 1.905894, 1.907919, 1.987615, 2.023258, 2.025156, 2.031727, 2.04972, 2.090646, 2.147084, 2.175075, 2.177073, 2.179072, 2.180415, 2.182072, 2.207092, 2.211104, 2.264763, 2.337766, 2.339056, 2.418064, 2.423426, 2.472685, 2.47336, 2.531806, 2.574872, 2.575872, 2.595977, 2.636126, 2.674815, 2.722884, 2.724884, 2.753831, 2.756828, 2.789916, 2.791918, 2.793916, 2.848047, 2.874168, 2.881214, 2.963238, 2.98924, 2.991241, 2.993277, 2.99527, 3.022002, 3.047965, 3.054406, 3.055964, 3.057964, 3.08917, 3.09217, 3.09417, 3.138091, 3.1897, 3.194698, 3.236701, 3.239833, 3.288984, 3.29399, 3.336025, 3.339014, 3.342034, 3.344015, 3.387213, 3.389211, 3.39121, 3.39423, 3.443217, 3.461697, 3.462696, 3.501704, 3.504696, 3.506696, 3.507696, 3.528723, 3.545697, 3.561701, 3.6107, 3.649696, 3.657697, 3.700662, 3.911075, 3.928074, 3.984248, 4.019815, 4.027816, 4.060401, 4.097016, 4.102152, 4.106053, 4.10905, 4.132684, 4.13768, 4.140481, 4.142487, 4.158488, 4.194103, 4.218187, 4.256776, 4.276872, 4.334954, 4.340953, 4.359956, 4.377988, 4.518976, 4.52701, 4.593064, 4.625465, 4.641982, 4.656948, 4.659947, 4.706946, 4.724996, 4.740489, 4.79288, 4.834953, 4.837879, 4.840879, 4.84388, 4.90988, 4.942341, 4.965277, 4.968275, 4.976293, 5.050181, 5.053007, 5.098034, 5.100452, 5.148103, 5.152058, 5.155057, 5.198541, 5.200535, 5.203559, 5.246079, 5.29721, 5.300222, 5.302202, 5.30521, 5.353387, 5.356386, 5.372613, 5.421645, 5.424645, 5.47471, 5.49071, 5.524791, 5.556875, 5.572875, 5.588975, 5.606968, 5.623971, 5.640969, 5.656009, 5.67501, 5.707009, 5.733009, 5.736009, 5.771015, 5.797173, 5.854627, 5.889182, 5.907697, 5.923697, 5.940697, 5.973724, 5.989697, 6.022697, 6.040716, 6.065269, 6.069249, 6.156322, 6.173883, 6.178849, 6.18885, 6.212944, 6.278934, 6.284934, 6.305936, 6.355034]`\n\n`axis5_arr_127= [95.5263, 95.5263, 95.0, 95.3509, 95.3509, 95.3509, 95.5263, 95.3509, 95.3509, 95.3509, 95.5263, 95.3509, 95.3509, 95.3509, 95.3509, 95.1754, 95.3509, 95.1754, 95.3509, 95.1754, 95.0, 95.3509, 95.1754, 95.5263, 95.1754, 95.3509, 95.3509, 95.1754, 95.1754, 95.3509, 95.3509, 95.5263, 95.0, 95.3509, 95.3509, 95.1754, 95.1754, 93.7719, 93.7719, 91.8421, 91.6667, 90.4386, 90.2632, 89.9123, 89.0351, 89.0351, 87.4561, 85.8772, 83.7719, 77.2807, 75.5263, 73.7719, 72.0175, 71.8421, 71.4912, 71.3158, 70.4386, 69.9123, 69.7368, 69.386, 69.386, 66.5789, 65.1754, 64.8246, 64.4737, 64.2982, 63.7719, 61.1404, 60.7895, 60.614, 60.4386, 59.7368, 59.386, 59.2105, 58.8596, 56.4035, 54.6491, 54.4737, 51.3158, 51.1404, 49.386, 49.386, 47.807, 46.4035, 46.2281, 45.7018, 44.1228, 42.7193, 42.193, 42.193, 40.614, 40.614, 39.386, 39.2105, 39.0351, 37.2807, 37.2807, 35.8772, 34.1228, 33.7719, 33.5965, 32.3684, 32.0175, 31.8421, 30.7895, 30.2632, 30.2632, 29.9123, 29.0351, 28.6842, 28.5088, 27.2807, 25.3509, 25.3509, 24.8246, 24.8246, 22.3684, 21.8421, 21.4912, 21.1404, 20.2632, 19.9123, 19.9123, 19.5614, 19.0351, 18.5088, 16.9298, 16.4035, 16.2281, 16.0526, 15.3509,`\n\n`15.0, 14.6491, 14.4737, 14.4737, 13.7719, 12.193, 10.0877, 9.5614, 8.5088, 1.1404, 0.9649, -0.2632, -2.3684, -2.5439, -2.8947, -3.2456, -3.9474, -4.4737, -4.4737, -4.8246, -5.7018, -6.0526, -6.0526, -6.5789, -6.9298, -8.8596, -10.4386, -10.614, -12.7193, -13.4211, -13.5965, -14.1228, -19.5614, -19.7368, -20.9649, -21.3158, -21.3158, -22.7193, -23.0702, -24.4737, -25.0, -26.4035, -27.9825, -28.1579, -28.5088, -29.5614, -29.9123, -31.3158, -31.3158, -31.8421, -32.7193, -34.4737, -36.5789, -36.7544, -38.3333, -38.5088, -39.9123, -39.9123, -40.2632, -41.3158, -41.6667, -41.8421, -43.0702, -44.8246, -45.0, -45.1754, -45.5263, -46.5789, -46.5789, -46.7544, -47.4561, -47.807, -47.6316, -47.6316, -47.2807, -47.6316, -47.6316, -47.2807, -47.4561, -47.4561, -47.2807, -47.6316, -47.6316, -47.4561, -47.6316, -47.4561, -47.807, -47.2807, -47.6316, -47.6316, -47.807, -47.4561, -47.6316, -47.4561, -47.4561, -47.4561, -47.2807, -47.6316, -47.4561, -47.6316, -47.6316, -47.4561, -47.4561, -47.4561, -47.6316, -47.4561, -47.4561, -47.4561, -47.6316]`\n\n`# pwm 118`\n\n`time_rec_arr_118= [0.673386, 0.689433, 0.691463, 0.707491, 0.724535, 0.739536, 0.773204, 0.792561, 0.806596, 0.807557, 0.839556, 0.840929, 0.855653, 0.873374, 0.920374, 0.987874, 1.005402, 1.02343, 1.043474, 1.072891, 1.13009, 1.180739, 1.231413, 1.238413, 1.28251, 1.332511, 1.383498, 1.403514, 1.435173, 1.454417, 1.488441, 1.535685, 1.594504, 1.601503, 1.621505, 1.672655, 1.705938, 1.720528, 1.722727, 1.756519, 1.7924, 1.795398, 1.798393, 1.844395, 1.851394, 1.87218, 1.887488, 1.905487, 1.922318, 1.937606, 1.955653, 1.97169, 2.022369, 2.028395, 2.032818, 2.059598, 2.099548, 2.149547, 2.171589, 2.188946, 2.236516, 2.269515, 2.308747, 2.315743, 2.3435, 2.361135, 2.36313, 2.36413, 2.386714, 2.404426, 2.437282, 2.486609, 2.509611, 2.566089, 2.618146, 2.637604, 2.668382, 2.687628, 2.753453, 2.771455, 2.797543, 2.799543, 2.801543, 2.803543, 2.837657, 2.869658, 2.887764, 2.923341, 2.928336, 2.930336, 2.966067, 3.016571, 3.067571, 3.098288, 3.126235, 3.177256, 3.183256, 3.225409, 3.276739, 3.28566, 3.3408, 3.342718, 3.390667, 3.393587, 3.440587, 3.442587, 3.498265, 3.552715, 3.56624, 3.600799, 3.649839, 3.664798, 3.7008, 3.7008, 3.730835, 3.735835, 3.81392, 3.817884, 3.834884, 3.863884, 3.866885, 3.881885, 3.916912, 3.955885, 3.957885, 4.008224, 4.058376, 4.099038, 4.159361, 4.20501, 4.213354, 4.249498, 4.266059, 4.298184, 4.315184, 4.329184, 4.333489, 4.365479, 4.384478, 4.41548, 4.430557, 4.4795, 4.482795, 4.498499, 4.51339, 4.51739, 4.578528, 4.582505, 4.598507, 4.615507, 4.632904, 4.650506, 4.666826, 4.724505, 4.727505, 4.730506, 4.733228, 4.76346, 4.766459, 4.821788, 4.871335, 4.92997, 4.945974, 4.994735, 5.04285, 5.045848, 5.04885, 5.082072, 5.111778, 5.115778, 5.145779, 5.162344, 5.17834, 5.194341, 5.199346, 5.230612, 5.247336, 5.300343, 5.345339, 5.388338, 5.440375, 5.488283, 5.544441, 5.580831, 5.64248, 5.681494, 5.712442, 5.747258, 5.78226, 5.813777, 5.831189, 5.860851, 5.876857, 5.880589, 5.897546, 5.914608, 5.964271, 6.012271, 6.050368, 6.055353, 6.058354, 6.062353, 6.079291, 6.12729, 6.146772, 6.178266, 6.201727, 6.205691, 6.209691, 6.243405, 6.263403, 6.298684, 6.304679, 6.308679, 6.351416, 6.356394, 6.360358, 6.379397, 6.427253, 6.445548, 6.476537, 6.512973, 6.544248,`\n\n`6.576248, 6.611255, 6.643286, 6.661293, 6.679281]`\n\n`axis5_arr_118= [ 95.0, 95.0, 94.6491, 93.9474, 93.9474, 92.7193, 92.3684, 92.193, 92.0175, 91.1404, 90.9649, 90.7895, 90.0877, 88.5088, 86.7544, 86.4035, 85.8772, 85.7018, 84.1228, 82.8947, 81.3158, 79.7368, 79.5614, 78.3333, 77.4561, 75.8772, 75.8772, 74.2982, 73.9474, 72.7193, 71.6667, 69.7368, 69.5614, 69.386, 68.1579, 66.9298, 66.9298, 66.5789, 65.5263, 65.5263, 65.1754, 65.0, 62.7193, 62.5439, 62.3684, 62.0175, 61.3158, 61.1404, 60.9649, 60.7895, 59.9123, 58.3333, 57.9825, 57.807, 57.1053, 55.3509, 54.1228, 53.5965, 53.4211, 53.2456, 52.3684, 49.5614, 49.2105, 49.0351, 48.6842, 47.9825, 47.6316, 47.2807, 47.2807, 46.7544, 45.3509, 43.5965, 42.0175, 40.614, 40.2632, 40.2632, 39.9123, 39.2105, 38.8596, 38.6842, 36.0526, 35.8772, 35.7018, 35.5263, 35.0, 33.4211, 33.2456, 32.0175, 32.0175, 30.614, 29.2105, 29.0351, 27.6316, 26.0526, 24.6491, 24.6491, 23.4211, 21.8421, 21.6667, 20.2632, 19.9123, 19.0351, 18.6842, 17.807, 17.2807, 16.0526, 14.8246, 14.2982, 13.0702, 11.6667, 11.3158, 10.2632, 10.0877, 9.9123, 9.2105, 7.807, 7.2807, 7.1053, 6.2281, 5.8772, 5.7018, 4.6491, 2.8947, 3.0702, 1.6667, 0.0877, -1.1404, -2.7193, -4.8246, -4.4737, -5.7018, -6.0526, -6.2281, -6.4035, -7.2807, -7.6316, -7.9825, -7.9825, -8.8596, -10.2632, -11.4912, -11.8421, -12.0175, -12.0175, -12.7193, -14.4737, -14.8246, -15.0, -15.3509, -15.3509, -15.8772, -17.4561, -18.6842, -19.0351, -19.2105, -19.386, -19.7368, -20.2632, -21.3158, -23.2456, -24.6491, -24.8246, -26.2281, -27.807, -27.9825, -28.5088, -28.5088, -29.2105, -29.5614, -29.5614, -30.7895, -30.9649, -31.3158, -31.8421, -32.3684, -33.4211, -35.0, -36.0526, -37.6316, -38.8596, -40.7895, -42.193, -43.2456, -44.8246, -46.0526, -46.4035, -47.2807, -47.2807, -47.4561, -47.9825, -47.9825, -48.1579, -48.3333, -48.5088, -48.3333, -48.3333, -48.3333, -48.3333, -48.5088, -48.3333, -48.5088, -48.1579, -48.3333, -48.1579, -48.5088, -48.3333, -48.5088,`\n\n`-48.1579, -48.3333, -48.3333, -48.5088, -48.5088, -48.3333, -48.3333, -48.3333, -48.5088, -48.6842, -48.1579, -48.3333, -48.3333, -48.3333, -48.1579, -48.3333, -48.3333, -48.3333, -48.1579, -48.6842, -48.3333]`\n\n`axis5_arr_58= [95.1754, 95.3509, 95.3509, 95.1754, 95.3509, 95.5263, 95.3509, 95.5263, 95.0, 95.3509, 95.3509, 95.3509, 95.1754, 95.3509, 95.5263, 95.3509, 95.3509, 95.3509, 95.0, 95.1754, 95.5263, 95.3509, 95.3509, 95.1754, 95.1754, 95.3509, 95.0, 95.1754, 95.1754, 95.3509, 95.3509, 95.3509, 95.5263, 95.5263, 95.3509, 95.3509, 95.1754, 94.8246, 95.1754, 95.0, 95.0, 94.6491, 94.2982, 94.1228, 94.2982, 94.1228, 94.2982, 94.1228, 93.9474, 94.1228, 93.7719, 93.7719, 93.4211, 93.2456, 93.2456, 93.4211, 92.8947, 92.8947, 92.8947, 92.7193, 92.3684, 92.5439, 92.5439, 92.5439, 92.3684, 92.193, 92.0175, 91.8421, 91.8421, 91.6667, 92.0175, 91.8421, 91.4912, 91.3158, 91.3158, 90.9649, 90.7895, 90.9649, 90.4386, 90.614, 90.2632, 89.9123, 89.5614, 89.386, 89.386, 89.2105, 88.8596, 88.8596, 88.5088, 88.5088, 88.3333, 87.807, 87.6316, 87.2807, 86.7544, 86.5789, 86.4035, 86.2281, 85.7018, 85.8772, 85.5263, 85.3509, 85.0, 85.1754, 85.0, 84.6491, 84.4737, 84.4737, 84.2982, 83.9474, 84.1228, 84.1228, 83.7719,`\n\n`83.7719, 83.4211, 83.4211, 83.0702, 83.2456, 83.2456, 82.8947, 82.8947, 83.0702, 82.8947, 82.5439, 82.3684, 82.5439, 82.193, 82.3684, 82.3684, 82.193, 82.0175, 82.0175, 82.193, 81.8421, 81.8421, 81.6667, 81.4912, 81.4912, 81.3158, 80.9649, 80.9649, 80.9649, 80.614, 80.614, 80.4386, 80.2632, 80.2632, 80.2632, 80.4386, 80.0877, 80.0877, 80.0877, 80.2632, 79.9123, 79.5614, 79.7368, 79.7368, 79.386, 79.386, 79.2105, 79.2105, 79.0351, 79.0351, 79.0351, 79.0351, 78.6842, 78.5088, 78.1579, 78.1579, 78.1579, 77.9825, 78.1579, 77.807, 77.9825, 77.9825, 77.6316, 77.4561, 77.6316, 77.4561, 77.4561, 77.2807, 77.1053, 77.2807, 76.9298, 76.7544, 76.5789, 76.5789, 76.5789, 76.5789, 76.2281, 75.8772, 75.3509, 75.1754, 75.1754, 74.6491, 74.2982, 74.2982, 73.9474, 73.4211, 73.0702, 72.7193, 73.0702, 72.7193, 72.8947, 72.3684, 72.3684, 72.3684, 72.0175, 72.193, 72.193, 71.6667, 72.0175, 71.8421, 71.4912, 71.6667, 71.4912, 71.3158, 71.1404, 71.3158, 70.9649, 70.7895, 70.7895, 70.7895, 70.614, 70.4386, 69.9123, 70.0877, 70.0877, 70.0877, 69.7368, 69.9123, 69.7368, 69.7368, 69.5614, 69.386, 69.0351, 69.2105, 69.0351, 68.8596, 68.8596, 68.8596, 68.5088, 68.6842, 68.6842, 68.3333, 68.1579, 67.9825, 67.807, 67.807, 67.9825, 67.6316, 67.1053, 67.2807, 67.1053, 67.1053, 66.9298, 66.7544, 66.9298, 66.9298, 66.7544, 66.5789, 66.7544, 66.5789,`\n\n`66.4035, 66.4035, 66.0526, 66.4035, 66.2281, 66.0526, 66.0526, 66.0526, 66.0526, 65.7018, 65.7018, 65.7018, 65.7018, 65.5263, 65.5263, 65.5263, 65.3509, 65.0, 65.0, 65.1754, 64.8246, 64.8246, 64.6491, 64.6491, 64.4737, 64.6491, 64.4737, 64.2982, 63.9474, 64.1228, 63.9474, 63.9474, 63.9474, 63.7719, 63.7719, 63.9474, 63.5965, 63.4211, 63.0702, 63.2456, 63.2456, 63.0702, 62.8947, 63.0702, 62.8947, 62.8947, 62.7193, 62.5439, 62.5439, 62.193, 62.0175, 62.193, 61.8421, 61.8421, 61.6667, 61.4912, 61.4912, 61.6667, 60.9649, 60.9649, 60.614, 60.4386, 60.614, 60.2632, 59.5614, 59.9123, 59.5614, 59.5614, 59.386, 59.2105, 58.8596, 59.0351, 58.8596, 58.6842, 58.5088, 58.5088, 58.3333, 58.3333, 58.3333, 58.1579, 57.9825, 57.9825, 57.6316, 57.807, 57.807, 57.4561, 57.4561, 57.2807, 57.2807, 57.1053, 57.1053, 56.7544, 56.7544, 56.5789, 56.4035, 56.4035, 56.7544, 56.2281, 56.2281, 56.2281, 55.8772, 55.8772, 55.8772, 55.7018, 55.8772, 55.5263, 55.3509, 55.5263, 55.5263, 55.0, 55.1754, 55.1754, 55.0, 54.8246, 54.8246, 54.2982, 53.9474, 53.7719, 53.4211, 52.8947, 52.8947, 52.5439, 52.7193, 52.3684, 52.0175, 51.6667, 51.6667, 51.6667, 51.3158, 50.9649, 50.614, 50.614, 50.2632, 49.7368, 49.386, 49.5614, 49.5614, 49.2105, 48.8596, 48.5088, 48.5088, 48.3333, 48.5088, 48.1579, 47.807, 47.9825, 47.6316, 47.2807, 47.4561, 47.4561, 47.4561,`\n\n`46.9298, 46.7544, 46.5789, 46.5789, 46.4035, 46.2281, 46.2281, 46.0526, 46.0526, 46.0526, 46.0526, 45.8772, 45.7018, 45.8772, 45.5263, 45.3509, 45.3509, 45.5263, 45.1754, 45.0, 45.0, 45.0, 45.0, 45.0, 44.8246, 44.2982, 44.2982, 44.2982, 44.2982, 44.1228, 44.1228, 44.2982, 44.1228, 43.9474, 43.7719, 43.7719, 43.5965, 43.4211, 43.4211, 43.5965, 43.2456, 43.2456, 43.2456, 43.2456, 43.2456, 43.0702, 43.2456, 42.8947, 43.0702, 43.0702, 42.8947, 42.8947, 42.7193, 42.7193, 42.5439, 42.193, 42.3684, 42.0175, 42.3684, 42.193, 42.0175, 41.6667, 41.8421, 41.6667, 41.6667, 41.3158, 41.3158, 41.3158, 41.1404, 41.1404, 40.9649, 41.1404, 40.9649, 40.7895, 40.9649, 40.7895, 40.614, 40.614, 40.614, 40.2632, 40.0877, 40.0877, 40.0877, 40.0877, 39.7368, 39.9123, 39.7368, 39.5614, 39.7368, 39.386, 39.386, 39.2105, 39.0351, 38.8596, 38.5088, 38.3333, 37.9825, 37.6316, 37.2807, 37.1053, 36.7544, 36.7544, 36.5789, 36.2281, 36.0526, 35.8772, 35.5263, 35.7018, 35.3509, 35.1754, 35.0, 34.8246, 34.8246, 34.4737, 34.1228, 34.1228, 33.7719, 33.4211, 33.4211, 33.2456, 33.5965, 32.8947, 32.8947, 32.7193, 32.7193, 32.5439, 32.5439, 32.5439, 32.193, 32.0175, 32.0175, 31.8421, 32.0175, 31.6667, 31.4912, 31.4912, 31.3158, 31.3158, 31.3158, 30.9649, 30.7895, 30.7895, 30.614, 30.614, 30.4386, 30.4386, 30.4386, 30.4386, 30.4386, 30.0877, 30.0877, 29.9123, 29.9123, 29.7368, 29.9123, 29.5614, 29.7368, 29.5614, 29.5614, 29.5614, 29.386, 29.386, 28.8596, 28.5088, 28.8596, 28.6842, 28.5088, 28.6842, 28.5088, 28.6842, 28.3333, 28.3333, 28.1579, 27.9825, 27.9825, 27.807, 27.4561, 27.4561, 27.4561, 27.4561, 27.2807, 26.9298, 26.9298, 26.7544, 26.7544, 26.5789, 26.5789, 26.5789, 26.4035,`\n\n`26.5789, 26.7544, 26.2281, 26.2281, 25.8772, 26.0526, 25.8772, 25.7018, 25.7018, 25.5263, 25.1754, 24.8246, 24.6491, 24.8246, 24.6491, 24.4737, 24.1228, 23.9474, 23.7719, 23.4211, 22.8947, 22.8947, 22.3684, 22.193, 21.8421, 22.0175, 21.8421, 21.4912, 20.7895, 20.9649, 20.7895, 20.7895, 20.7895, 20.614, 20.614, 20.2632, 20.4386, 20.0877, 20.2632, 19.9123, 19.9123, 19.7368, 19.7368, 19.5614, 19.7368, 19.2105, 19.386, 19.0351, 19.2105, 18.8596, 19.0351, 18.6842, 18.6842, 18.5088, 18.6842, 18.3333, 18.1579, 18.1579, 17.9825, 17.807, 17.807, 17.6316, 17.6316, 17.4561, 17.6316, 17.2807, 16.9298, 16.9298, 16.9298, 16.9298, 16.5789, 16.5789, 16.5789, 16.2281, 16.0526, 15.8772, 15.8772, 15.8772, 15.5263, 15.7018, 15.3509, 15.1754, 15.1754, 15.0, 15.0, 14.6491, 14.2982, 14.2982, 14.4737, 13.9474, 13.9474, 13.9474, 13.5965, 13.9474, 13.7719, 13.9474, 13.5965, 13.4211, 13.2456, 13.4211, 13.4211, 12.8947, 12.8947, 12.8947, 12.7193, 12.8947, 12.7193, 12.7193, 12.8947, 12.3684, 12.3684, 12.193, 12.0175, 12.0175, 11.6667, 11.6667, 11.8421, 11.8421, 11.8421, 11.4912, 11.3158, 11.1404, 11.1404, 11.1404, 11.1404, 10.7895, 10.9649, 10.9649, 10.4386, 10.4386, 10.2632, 10.4386, 10.4386, 10.4386, 9.9123, 9.7368, 9.7368, 9.5614, 9.2105, 9.2105, 9.2105, 9.386, 9.0351, 8.5088, 8.5088, 8.5088, 8.3333, 7.9825, 8.1579, 7.807, 7.807, 7.6316, 7.6316, 7.807, 7.4561, 7.4561, 7.4561, 7.4561, 7.2807, 6.7544, 6.5789, 6.9298, 6.5789, 6.5789, 6.4035, 6.5789, 6.5789, 6.2281, 6.4035, 6.2281, 6.0526, 5.5263, 5.7018, 5.7018, 5.5263, 5.1754, 5.0, 4.4737, 4.4737, 4.2982, 4.2982, 3.9474, 3.5965, 3.7719, 3.4211, 3.4211, 3.5965, 3.2456, 3.2456, 3.0702, 2.8947, 2.3684, 2.3684, 2.3684]`\n\n`time_rec_arr_58= [ 0.01342, 0.06516, 0.12308, 0.177018, 0.243046, 0.248356, 0.277973, 0.294017, 0.324266, 0.344595, 0.383019, 0.389654, 0.423115, 0.450117, 0.450117, 0.483922, 0.487936, 0.532136, 0.538199, 0.573943, 0.600514, 0.602247, 0.603513, 0.635145, 0.638287, 0.668582, 0.703175, 0.703736, 0.736097, 0.744289, 0.786205, 0.78719, 0.852216, 0.887478, 0.922137, 0.923136, 0.923136, 0.952411, 0.953011, 0.983801, 0.984768, 1.008911, 1.070235, 1.072236, 1.074541, 1.106241, 1.10724, 1.10824, 1.135292, 1.136773, 1.171732, 1.173164, 1.199479, 1.205942, 1.232959, 1.235411, 1.271545, 1.304878, 1.337022, 1.339045, 1.359051, 1.368258, 1.369843, 1.371828, 1.392055, 1.418092, 1.458963, 1.465946, 1.468987, 1.497936, 1.500944, 1.524962, 1.552524, 1.587543, 1.589058, 1.620324, 1.622102, 1.649349, 1.684896, 1.717958, 1.764054, 1.813197, 1.871957, 1.930841, 1.932842, 1.934841, 1.968895, 2.032058, 2.068385, 2.071751, 2.133044, 2.171068, 2.220186, 2.283181, 2.372319, 2.419012, 2.422014, 2.481236, 2.528189, 2.533196, 2.598468, 2.600534, 2.633011, 2.683301, 2.684318, 2.745301, 2.748842, 2.782485, 2.816704, 2.851095, 2.853171, 2.881347, 2.91295, 2.918952, 2.949357, 2.950356, 3.013163, 3.015153, 3.017152, 3.04825, 3.050166, 3.053152, 3.088567, 3.192188, 3.194148, 3.200151, 3.202151, 3.205149, 3.226557, 3.22973, 3.231843, 3.23452, 3.237854, 3.262568, 3.266059, 3.268966, 3.289924, 3.292923, 3.346009, 3.402092, 3.438186, 3.470196, 3.494194, 3.497187, 3.499617, 3.501062, 3.528112, 3.530108, 3.534108,`\n\n`3.55811, 3.562207, 3.56515, 3.594001, 3.629215, 3.631215, 3.633215, 3.635233, 3.670906, 3.70334, 3.727003, 3.731003, 3.757053, 3.763052, 3.769056, 3.792146, 3.825887,`\n\n`3.830198, 3.860579, 3.868227, 3.913921, 3.937883, 3.942883, 3.945884, 3.973424, 3.976409, 4.010119, 4.016117, 4.043128, 4.045104, 4.074645, 4.076766, 4.082644, 4.101845, 4.142885, 4.146885, 4.174922, 4.180196, 4.202953, 4.214966, 4.262989, 4.317954, 4.375087, 4.427085, 4.47527, 4.525989, 4.575991, 4.626038, 4.675997, 4.727992, 4.778186, 4.828362, 4.865592, 4.888178, 4.894168, 4.927579, 4.951128, 4.956162, 5.000129, 5.01816, 5.06613, 5.100059, 5.132992, 5.149009, 5.189553, 5.193583, 5.196568, 5.220157, 5.224146, 5.228172, 5.233144, 5.255152, 5.260145, 5.263177, 5.318256, 5.323255, 5.361198, 5.38636, 5.389196, 5.394197, 5.397228, 5.422532, 5.427524, 5.431523, 5.457157, 5.499363, 5.532227, 5.565227, 5.603251, 5.607285, 5.61235, 5.635546, 5.640515, 5.644516, 5.648517, 5.673038, 5.724634, 5.753676, 5.761198, 5.802209, 5.806197,`\n\n`5.809215, 5.853949, 5.858976, 5.897998, 5.923986, 5.928989, 5.949992, 5.976001, 5.980269, 6.015273, 6.042831, 6.046795, 6.064797, 6.090257, 6.096232, 6.122416, 6.125812, 6.13081, 6.16493, 6.204058, 6.209059, 6.214253, 6.235055, 6.239081, 6.243082, 6.247083, 6.263084, 6.28101, 6.339067, 6.349053, 6.397483, 6.414482, 6.484464, 6.489473, 6.492456, 6.496456, 6.513456, 6.562517, 6.58171, 6.608944, 6.612977, 6.664965, 6.737944, 6.741944, 6.746975, 6.772227, 6.777108, 6.819741, 6.825738, 6.854917, 6.860915, 6.876915, 6.895065, 6.945878, 6.985837, 6.989835, 6.993834, 7.036106, 7.041327, 7.045835, 7.103162, 7.109164, 7.113162, 7.134163, 7.138163, 7.142163, 7.146163, 7.163164, 7.212167, 7.235164, 7.239164, 7.244163, 7.261162, 7.286136, 7.290146, 7.294137, 7.312103, 7.336236, 7.342978, 7.378204, 7.412255, 7.438221, 7.442222, 7.461959, 7.489049, 7.494004, 7.546173, 7.586061, 7.594318, 7.645982, 7.731855, 7.737841, 7.743844, 7.769845, 7.774878, 7.793055, 7.848873, 7.855847, 7.876849, 7.909915, 7.933939, 7.937938, 7.942939, 7.961958, 7.9939, 8.02708, 8.057862, 8.077902, 8.126641, 8.194637, 8.226791, 8.273764, 8.293916, 8.326022, 8.376098, 8.40888, 8.426879, 8.476216, 8.494216, 8.541929, 8.60246, 8.609018, 8.626392, 8.657334, 8.707495, 8.72501, 8.773012, 8.792011, 8.809011, 8.815014, 8.82003, 8.82503, 8.844032, 8.866127, 8.871127, 8.875127, 8.906155, 8.941151, 8.973155, 8.991175, 9.022054, 9.058176, 9.125108, 9.157896, 9.192268, 9.255997, 9.274996, 9.324997, 9.364828, 9.36986, 9.373826, 9.406145, 9.42514, 9.455105, 9.474171, 9.505183, 9.541249, 9.589983, 9.625096, 9.65711, 9.708118, 9.740053, 9.79008, 9.823066, 9.871483, 9.890143, 9.953975, 9.973477, 10.042106, 10.088645, 10.153224, 10.187264, 10.219264, 10.240247, 10.288291, 10.32218, 10.37097, 10.388967, 10.452969, 10.473022, 10.498993, 10.503993, 10.542941, 10.548966, 10.555966, 10.582474, 10.588274, 10.637902, 10.670128, 10.689128, 10.752889, 10.78589, 10.804887, 10.837257, 10.886965, 10.918969, 10.938025, 10.997076, 11.003076, 11.035099, 11.0551, 11.088103, 11.170033, 11.217879, 11.250854, 11.270879, 11.334886, 11.386264, 11.401272, 11.453354, 11.503674, 11.553716, 11.587283, 11.650908, 11.683923, 11.717924, 11.750924, 11.78498, 11.82006, 11.878624, 11.885303, 11.936857, 11.969826, 12.002927, 12.052032, 12.08505, 12.133471, 12.166662, 12.200996, 12.234125,`\n\n`12.267168, 12.301872, 12.33602, 12.378293, 12.384276, 12.41733, 12.448114, 12.468172, 12.518455, 12.549435, 12.582433, 12.601797, 12.64314, 12.649142, 12.681142, 12.717141, 12.750141, 12.784254, 12.846883, 12.875172, 12.881174, 12.900927, 12.964926, 12.997988, 13.033957, 13.070938, 13.077929, 13.08398, 13.164099, 13.199602, 13.266198, 13.316222, 13.396254, 13.43024, 13.465024, 13.515879, 13.559097, 13.566132, 13.600093, 13.650031, 13.69722, 13.761631, 13.796059, 13.829983, 13.865045, 13.913233, 13.964444, 14.031192, 14.096115, 14.162747, 14.210811, 14.245953, 14.278972, 14.31319, 14.371054, 14.377051, 14.412258, 14.439276, 14.446288, 14.480323, 14.544215, 14.576196, 14.626878, 14.6968, 14.745957, 14.811249, 14.872427, 14.893255, 14.928254, 14.993306, 15.027231, 15.06137, 15.095007, 15.160916, 15.194957, 15.242929, 15.295928, 15.344957, 15.394006, 15.442006, 15.477009, 15.527657, 15.575276, 15.61165, 15.659996, 15.694853, 15.759141, 15.791224, 15.824998, 15.859369, 15.885152, 15.892172, 15.924173, 15.95885, 15.99241, 16.026442, 16.078017, 16.131988, 16.139983, 16.174985, 16.223009, 16.259008, 16.309404, 16.356927, 16.407214, 16.44304, 16.492221, 16.540243, 16.57522, 16.608301, 16.689201, 16.726198, 16.773933, 16.806938, 16.840988, 16.862945, 16.875464, 16.938927, 16.973897, 17.008901, 17.058131, 17.108122, 17.156986, 17.207069, 17.241094, 17.289131, 17.324129, 17.407127, 17.471153, 17.507185, 17.588086, 17.638921, 17.673451, 17.722971, 17.787998, 17.839027, 17.881113, 17.888113,`\n\n`17.938115, 17.986224, 18.023232, 18.070484, 18.105426, 18.172536, 18.239465, 18.304669, 18.355735, 18.401502, 18.452121, 18.488053, 18.555025, 18.603056, 18.644143, 18.652537, 18.68915, 18.753149, 18.818375, 18.854348, 18.898943, 18.920024, 18.967945, 19.003944, 19.067947, 19.102967, 19.152996, 19.201997, 19.253219, 19.303357, 19.369405, 19.418109, 19.468083, 19.519139, 19.553164, 19.603037, 19.642833, 19.650834, 19.715833, 19.751834, 19.817834, 19.866834, 19.91786, 19.98334, 20.050848, 20.099849, 20.142097, 20.150096, 20.201271, 20.249886, 20.315205, 20.366261, 20.415998, 20.468387, 20.517025, 20.596874, 20.648995, 20.698035, 20.749014, 20.799032, 20.847086,`\n\n`20.898838, 20.933925, 20.998227, 21.063493, 21.099021, 21.141637, 21.163191, 21.213214, 21.279844, 21.315845, 21.366468, 21.432188, 21.498972, 21.548041, 21.59528, 21.663983, 21.714913, 21.763873, 21.81289, 21.862874, 21.927918, 21.964929, 22.014916, 22.064826, 22.130826, 22.180826, 22.261946, 22.312869, 22.362847, 22.426321, 22.463895, 22.51287, 22.56187, 22.611902, 22.662993, 22.726853, 22.777898, 22.842853, 22.896873, 22.976485, 23.029913, 23.079066, 23.143257, 23.193878, 23.243957, 23.291977, 23.341977, 23.393011, 23.443726, 23.491152, 23.544885, 23.609842, 23.661368, 23.711368, 23.809851, 23.86005, 23.92505, 23.975053, 24.02605, 24.093997, 24.158564, 24.209714, 24.259895, 24.326029, 24.374029, 24.442848, 24.524285, 24.574008, 24.624286, 24.707886, 24.775651, 24.838039, 24.921235, 24.990098, 25.040862, 25.091897, 25.188123, 25.240209, 25.289191, 25.355819, 25.413186, 25.422181, 25.458182, 25.518908, 25.556857, 25.606297, 25.673476, 25.738836, 25.788355, 25.838075, 25.906028, 25.973012, 26.040013, 26.102012, 26.168845, 26.236988, 26.287988, 26.369035, 26.415952, 26.453034, 26.505033, 26.55596, 26.62007, 26.685697, 26.751048, 26.801293, 26.838227, 26.903295, 26.953379, 27.00367, 27.066581, 27.118684, 27.217879, 27.279561, 27.289839, 27.300836, 27.384017, 27.53471]`\n\n`axis5_pwm118_over_actuaction_range=[]`\n\n`axis5_pwm127_over_actuaction_range=[]`\n\n`axis5_pwm58_over_actuaction_range=[]`\n\n`for i in (range(-10,94)):`\n\n`tmp=(95-i)/(np.interp(i,axis5_arr_118[::-1],time_rec_arr_118[::-1]))`\n\n`axis5_pwm118_over_actuaction_range.append(tmp)`\n\n`tmp=(95-i)/(np.interp(i,axis5_arr_127[::-1],time_rec_arr_127[::-1]))`\n\n`axis5_pwm127_over_actuaction_range.append(tmp)`\n\n`tmp=(95-i)/(np.interp(i,axis5_arr_58[::-1],time_rec_arr_58[::-1]))`\n\n`axis5_pwm58_over_actuaction_range.append(tmp)`\n\n`fig4= plt.figure(figsize=(10,6))`\n\n`ax4=fig4.add_subplot(111)`\n\n`print (axis5_pwm118_over_actuaction_range)`\n\n`ax4.plot(range(-10,94),axis5_pwm118_over_actuaction_range,label='pwm118')`\n\n`ax4.plot(range(-10,94),axis5_pwm127_over_actuaction_range,label='pwm127')`\n\n`ax4.plot(range(-10,94),axis5_pwm58_over_actuaction_range,label='pwm58')`\n\n[`plt.show`](https://plt.show/)`()`", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/juo21f/how_to_build_a_regression_model_for_the_given/',)", "identifyer": 5751556, "year": "2020"}, {"autor": "justryingtoohelp", "date": 1605384310000, "content": "First attempt at Machine Learning /!/ This is my first attempt at machine learning I've watched some intro videos and just jumped straight in and have a few questions. Any resources are also welcome.\n\n**Data set:** [https://www.kaggle.com/rajan1002/hotel-booking](https://www.kaggle.com/rajan1002/hotel-booking)\n\n**Link to the code:**\n[https://github.com/justtryingtoohelp/Machine-Learning/blob/main/data-hotel_model-SVC](https://github.com/justtryingtoohelp/Machine-Learning/blob/main/data-hotel_model-SVC)\n\n**My Goal:** To predict whether a booking is likely to be cancelled.\n**Result:** 76.6% Accuracy.\n\n**My questions:**\n1. Is the SVC the best model for this task? I used a skit-learn algorithm cheat-sheet to select this model as: &gt;50 samples, predicting category, have labelled data, and it said to choose SGD Classifier but I went with Linear SVC (If I'm honest I read it wrong), why would SGD Classifier be better?\n\n2. What is the best way to determine which features to use in the model other than intuition? (Assume plot some data, if so what's the best methodology for this?)\n\n3. When I squish the data between 0 and 1, was this necessary for this type of model?\n\n4. I think that some of my functions might already be provided within libraries, is this the case?\n\n5. The hyper-parameters, the C value, kernel and gamma mainly. How should I alter these to increase my result? What is the thought process behind altering these? I have looked up what each do, but I can't figure out how I would know which way to adjust them without it being a trial and error approach.\n\nThank you for your help. Any comments on code structure / readability are also welcomed, I am a beginner.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ju8aib/first_attempt_at_machine_learning/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "first attempt at machine learning /!/ this is my first attempt at machine learning i've watched some intro videos and just jumped straight in and have a few questions. any resources are also welcome.\n\n**data set:** [https://www.kaggle.com/rajan1002/hotel-booking](https://www.kaggle.com/rajan1002/hotel-booking)\n\n**link to the code:**\n[https://github.com/justtryingtoohelp/machine-learning/blob/main/data-hotel_model-svc](https://github.com/justtryingtoohelp/machine-learning/blob/main/data-hotel_model-svc)\n\n**my goal:** to predict whether a booking is likely to be cancelled.\n**result:** 76.6% accuracy.\n\n**my questions:**\n1. is the svc the best model for this task? i used a skit-learn algorithm cheat-sheet to select this model as: &gt;50 samples, predicting category, have labelled data, and it said to choose sgd classifier but i went with linear svc (if i'm honest i read it wrong), why would sgd classifier be better?\n\n2. what is the best way to determine which features to use in the model other than intuition? (assume -----> plot !!!  some data, if so what's the best methodology for this?)\n\n3. when i squish the data between 0 and 1, was this necessary for this type of model?\n\n4. i think that some of my functions might already be provided within libraries, is this the case?\n\n5. the hyper-parameters, the c value, kernel and gamma mainly. how should i alter these to increase my result? what is the thought process behind altering these? i have looked up what each do, but i can't figure out how i would know which way to adjust them without it being a trial and error approach.\n\nthank you for your help. any comments on code structure / readability are also welcomed, i am a beginner.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 7, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/ju8aib/first_attempt_at_machine_learning/',)", "identifyer": 5751587, "year": "2020"}, {"autor": "HarissaForte", "date": 1585759152000, "content": "Using sample weights on Keras to \"correct\" class imbalance: not accuracy gain with loss reduction. /!/ Hi!\n\nI'm training a CNN for classification on Keras, and I have 2 very unbalanced classes. I  use an `ImageDataGenerator` for both the train and validation dataset and generate my batches with `flow_from_dataframe` method (with some data augmentation on the fly for the training dataset).\n\nOne approach I'm trying to reduce the impact of the class imbalance is using sample weights. The weights are calculated with `sklearn.class_weight.compute_sample_weight` and added in a new column of the dataframe. I checked the results: it's fine.\n\nThen I set the `'weight_col'` parameter of `flow_from_dataframe` to point to this column. So far so good, it was actually very convenient!\n\nBut now\u2026 the reduction of the loss does not transfer to any gain of accuracy (be it weighted or non-weighted).\n\nHere's a scatter plot of weighted accuracy Vs loss at end the of epochs: https://drive.google.com/file/d/1QG_b79rI_B3eIKdYGtXmeRXOHCAtdb3A/view?usp=sharing\n\nTo compare, here's the \"happy\" scatter plot I get when I use over-sampling (`imblearn.RandomOverSampler`) instead of sample weight: https://drive.google.com/file/d/1-5aJsrMujOkhgyqjsAZvsuEjNDgWFGAf/view?usp=sharing\n\nDoes anyone know what's going on?\nI wonder if it is linked with the fact that I use batches. Can it be the cause?\n\nThank you!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ft37up/using_sample_weights_on_keras_to_correct_class/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "using sample weights on keras to \"correct\" class imbalance: not accuracy gain with loss reduction. /!/ hi!\n\ni'm training a cnn for classification on keras, and i have 2 very unbalanced classes. i  use an `imagedatagenerator` for both the train and validation dataset and generate my batches with `flow_from_dataframe` method (with some data augmentation on the fly for the training dataset).\n\none approach i'm trying to reduce the impact of the class imbalance is using sample weights. the weights are calculated with `sklearn.class_weight.compute_sample_weight` and added in a new column of the dataframe. i checked the results: it's fine.\n\nthen i set the `'weight_col'` parameter of `flow_from_dataframe` to point to this column. so far so good, it was actually very convenient!\n\nbut now\u2026 the reduction of the loss does not transfer to any gain of accuracy (be it weighted or non-weighted).\n\nhere's a scatter -----> plot !!!  of weighted accuracy vs loss at end the of epochs: https://drive.google.com/file/d/1qg_b79ri_b3eikdygtxmerxohcatdb3a/view?usp=sharing\n\nto compare, here's the \"happy\" scatter -----> plot !!!  i get when i use over-sampling (`imblearn.randomoversampler`) instead of sample weight: https://drive.google.com/file/d/1-5ajsrmujokhgyqjsazvsuejndgwfgaf/view?usp=sharing\n\ndoes anyone know what's going on?\ni wonder if it is linked with the fact that i use batches. can it be the cause?\n\nthank you!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 3, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/ft37up/using_sample_weights_on_keras_to_correct_class/',)", "identifyer": 5751651, "year": "2020"}, {"autor": "Accomplished-Bag-637", "date": 1595351173000, "content": "Checking Model Performance : Plot of Training and Test errors /!/  \n\nHello Community,\n\nI'm doing supervised learning with logistic regression to predict default/not default using a loan data set. My Model produces similar scores on the training data and test data for each value of the regularisation (C ) parameter .\n\nA sample of scores on the training set are \\[0.839019310294312, 0.8665205961080051, 0.908692100565758, 0.9418031964611859,\\]\n\nsample of scores on the test are:\\[ 0.838786383598493, 0.8663736410721319, 0.9079384423324144, 0.9413515341115974, 0.960177747393608 \\]\n\nI tried to do a plot of the training and test errors and the they are superimposed (on top) over each other.\n\nWhat does this mean? Is my model good/overfitting or underfitting the data? Please help.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hvb9p1/checking_model_performance_plot_of_training_and/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "checking model performance : -----> plot !!!  of training and test errors /!/  \n\nhello community,\n\ni'm doing supervised learning with logistic regression to predict default/not default using a loan data set. my model produces similar scores on the training data and test data for each value of the regularisation (c ) parameter .\n\na sample of scores on the training set are \\[0.839019310294312, 0.8665205961080051, 0.908692100565758, 0.9418031964611859,\\]\n\nsample of scores on the test are:\\[ 0.838786383598493, 0.8663736410721319, 0.9079384423324144, 0.9413515341115974, 0.960177747393608 \\]\n\ni tried to do a plot of the training and test errors and the they are superimposed (on top) over each other.\n\nwhat does this mean? is my model good/overfitting or underfitting the data? please help.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/hvb9p1/checking_model_performance_plot_of_training_and/',)", "identifyer": 5751751, "year": "2020"}, {"autor": "Cabinet-Particular", "date": 1606436398000, "content": "How to interpret results of K-Means clustering?? /!/  The data set is found here:\n\n[https://vicroadsopendatastorehouse.vicroads.vic.gov.au/opendata/Traffic\\_Measurement/TYPICAL\\_HOURLY\\_VOLUME\\_DATA.csv](https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fvicroadsopendatastorehouse.vicroads.vic.gov.au%2Fopendata%2FTraffic_Measurement%2FTYPICAL_HOURLY_VOLUME_DATA.csv&amp;data=04%7C01%7CVenugopal.Bukkala%40umlaut.com%7C760e793d758b4e6da3e708d891c1e1a1%7C58d65043cd9d409f828b59b40ef5f919%7C0%7C0%7C637419609181941888%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&amp;sdata=u67QYM7vXbqBolhGUoroBY5Qb7et5QznjtX%2F5ifMc2o%3D&amp;reserved=0) \n\nI have clustered the data. My code is here:\n\n `import os`\n\n`os.chdir('C:/Users/veua/Downloads/')`\n\n`df_raw = pd.read_csv('TYPICAL_HOURLY_VOLUME_DATA.csv')`\n\n`df_clus = df_raw.dropna(axis=0, how='any')`\n\n`df_clus = df_raw.loc[:, '0:00':'23:00']`\n\n`df_clus.head()`\n\nFinding Optimal Number of Clusters using silhouette score plot\n\n`from sklearn.cluster import KMeans`\n\n`from sklearn.metrics import silhouette_score`\n\n`silhouette_scores = []`\n\n`range_n_clusters = list (range(2,10))`\n\n`for n_clusters in range_n_clusters:`\n\n`clusterer = KMeans(n_clusters=n_clusters)`\n\n`preds = clusterer.fit_predict(df_clus)`\n\n`centers = clusterer.cluster_centers_`\n\n`score = silhouette_score(df_clus, preds, sample_size=100)`\n\n`silhouette_scores.append(score)`\n\n`print(\"For n_clusters = {}, silhouette score is {:.2f})\".format(n_clusters, score))`\n\n`plt.figure(figsize=(16,8))`\n\n`plt.plot(range_n_clusters, silhouette_scores, 'bx-')`\n\n`plt.xlabel('k (number of clusters)')`\n\n`plt.ylabel('Silhouette score')`\n\n`plt.title('Selecting the number of clusters using the silhouette score')`\n\n[`plt.show`](https://plt.show)`()`\n\nI selected two clusters as it has the largest silhouette score.\n\nI cannot use K-Means from sklearn straight away as it the has more observations (&gt;&gt;10000).\n\nMini Batch KMeans\n\n`from sklearn.cluster import MiniBatchKMeans`\n\n`kmeans = MiniBatchKMeans(n_clusters=2, random_state=0,batch_size=100)`\n\n`kmeans1 = kmeans.partial_fit(df_clus.iloc[0:int(len(df_clus)),:])`\n\n`centroids = kmeans1.cluster_centers_`\n\n`labels = kmeans1.labels_`\n\n`pred_clusters = kmeans1.predict(df_clus)`\n\n`print(\"Number of clusters:\", len(centroids))`\n\n`print(\"Predicted Clusters:\", pred_clusters)`\n\n`print(\"Printing Cluster Centroids\", centroids)`\n\nHere I plotted cluster centroids as I can\u2019t visualize clusters.\n\n`for center in centroids:`\n\n`plt.plot(center)`\n\n`plt.xlabel(\"hour of the day\")`\n\n`plt.xticks(range(0, 24))`\n\n`plt.ylabel(\"Avg traffic volume per hour\")`\n\n`plt.title(\"Clusters\")`\n\nSo can anybody help me interpret the results of cluster centroids and the above graph with respect to the data. What inferences can I draw about??\n\n\\###Silhouette score\n\n`from sklearn.metrics import silhouette_score`\n\n`silhouette_score(df_clus, labels)`\n\nPlease do interpret the above metric.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k1qolp/how_to_interpret_results_of_kmeans_clustering/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "how to interpret results of k-means clustering?? /!/  the data set is found here:\n\n[https://vicroadsopendatastorehouse.vicroads.vic.gov.au/opendata/traffic\\_measurement/typical\\_hourly\\_volume\\_data.csv](https://eur01.safelinks.protection.outlook.com/?url=https%3a%2f%2fvicroadsopendatastorehouse.vicroads.vic.gov.au%2fopendata%2ftraffic_measurement%2ftypical_hourly_volume_data.csv&amp;data=04%7c01%7cvenugopal.bukkala%40umlaut.com%7c760e793d758b4e6da3e708d891c1e1a1%7c58d65043cd9d409f828b59b40ef5f919%7c0%7c0%7c637419609181941888%7cunknown%7ctwfpbgzsb3d8eyjwijoimc4wljawmdailcjqijoiv2lumziilcjbtii6ik1hawwilcjxvci6mn0%3d%7c1000&amp;sdata=u67qym7vxbqbolhguoroby5qb7et5qznjtx%2f5ifmc2o%3d&amp;reserved=0) \n\ni have clustered the data. my code is here:\n\n `import os`\n\n`os.chdir('c:/users/veua/downloads/')`\n\n`df_raw = pd.read_csv('typical_hourly_volume_data.csv')`\n\n`df_clus = df_raw.dropna(axis=0, how='any')`\n\n`df_clus = df_raw.loc[:, '0:00':'23:00']`\n\n`df_clus.head()`\n\nfinding optimal number of clusters using silhouette score -----> plot !!! \n\n`from sklearn.cluster import kmeans`\n\n`from sklearn.metrics import silhouette_score`\n\n`silhouette_scores = []`\n\n`range_n_clusters = list (range(2,10))`\n\n`for n_clusters in range_n_clusters:`\n\n`clusterer = kmeans(n_clusters=n_clusters)`\n\n`preds = clusterer.fit_predict(df_clus)`\n\n`centers = clusterer.cluster_centers_`\n\n`score = silhouette_score(df_clus, preds, sample_size=100)`\n\n`silhouette_scores.append(score)`\n\n`print(\"for n_clusters = {}, silhouette score is {:.2f})\".format(n_clusters, score))`\n\n`plt.figure(figsize=(16,8))`\n\n`plt.plot(range_n_clusters, silhouette_scores, 'bx-')`\n\n`plt.xlabel('k (number of clusters)')`\n\n`plt.ylabel('silhouette score')`\n\n`plt.title('selecting the number of clusters using the silhouette score')`\n\n[`plt.show`](https://plt.show)`()`\n\ni selected two clusters as it has the largest silhouette score.\n\ni cannot use k-means from sklearn straight away as it the has more observations (&gt;&gt;10000).\n\nmini batch kmeans\n\n`from sklearn.cluster import minibatchkmeans`\n\n`kmeans = minibatchkmeans(n_clusters=2, random_state=0,batch_size=100)`\n\n`kmeans1 = kmeans.partial_fit(df_clus.iloc[0:int(len(df_clus)),:])`\n\n`centroids = kmeans1.cluster_centers_`\n\n`labels = kmeans1.labels_`\n\n`pred_clusters = kmeans1.predict(df_clus)`\n\n`print(\"number of clusters:\", len(centroids))`\n\n`print(\"predicted clusters:\", pred_clusters)`\n\n`print(\"printing cluster centroids\", centroids)`\n\nhere i plotted cluster centroids as i can\u2019t visualize clusters.\n\n`for center in centroids:`\n\n`plt.plot(center)`\n\n`plt.xlabel(\"hour of the day\")`\n\n`plt.xticks(range(0, 24))`\n\n`plt.ylabel(\"avg traffic volume per hour\")`\n\n`plt.title(\"clusters\")`\n\nso can anybody help me interpret the results of cluster centroids and the above graph with respect to the data. what inferences can i draw about??\n\n\\###silhouette score\n\n`from sklearn.metrics import silhouette_score`\n\n`silhouette_score(df_clus, labels)`\n\nplease do interpret the above metric.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/k1qolp/how_to_interpret_results_of_kmeans_clustering/',)", "identifyer": 5752079, "year": "2020"}, {"autor": "Plyad1", "date": 1606433931000, "content": "Didnt manage to do an MCA on python /!/ Hi, \n\nSo I ve been wanting to plot an MCA in order to check the links between the variables that I have access to.\n\nBut I didnt manage to do a proper [MCA](https://en.m.wikipedia.org/wiki/Multiple_correspondence_analysis)\n\nFirst off, it seems that the prince package is the \"go to\" one to do that on python. (if someone could confirm, I'd be grateful)\n\nI ve manage to do a \"basic\"-ish mca with it. Basically I managed to compute the components, eigenvalues and coordinates.\n\nGreat, BUT I didnt manage to plot it properly. It appears that I must plot the exact same variables as the ones I ve used for the fit. This is not possible in my case as I have about 25 variables. (too many for a plot)\n\nInstead I'd like to plot some of them but it returns \"dimension mismatch\". \n\nI would also like to see the coordinates of some \"supplementary variables\" aka variables not used in the MCA but I didnt find any way to do so.\n\nNB: I am used to doing all of this on R with factomineR so I guess it should be possible on python as well (but maybe I am wrong)\n\n&amp;#x200B;\n\nAny help would be appreciated\n\nThank you in advance", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k1q0r8/didnt_manage_to_do_an_mca_on_python/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "didnt manage to do an mca on python /!/ hi, \n\nso i ve been wanting to -----> plot !!!  an mca in order to check the links between the variables that i have access to.\n\nbut i didnt manage to do a proper [mca](https://en.m.wikipedia.org/wiki/multiple_correspondence_analysis)\n\nfirst off, it seems that the prince package is the \"go to\" one to do that on python. (if someone could confirm, i'd be grateful)\n\ni ve manage to do a \"basic\"-ish mca with it. basically i managed to compute the components, eigenvalues and coordinates.\n\ngreat, but i didnt manage to plot it properly. it appears that i must plot the exact same variables as the ones i ve used for the fit. this is not possible in my case as i have about 25 variables. (too many for a plot)\n\ninstead i'd like to plot some of them but it returns \"dimension mismatch\". \n\ni would also like to see the coordinates of some \"supplementary variables\" aka variables not used in the mca but i didnt find any way to do so.\n\nnb: i am used to doing all of this on r with factominer so i guess it should be possible on python as well (but maybe i am wrong)\n\n&amp;#x200b;\n\nany help would be appreciated\n\nthank you in advance", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/k1q0r8/didnt_manage_to_do_an_mca_on_python/',)", "identifyer": 5752080, "year": "2020"}, {"autor": "SQL_beginner", "date": 1607119343000, "content": "Plot ROC Curves in R /!/  I am working with a computer that does not have internet access or a USB port - I only have R with limited libraries (e.g. I do not have access to the \"pec\" library). I am following a github tutorial where a \"survival analysis\" statistical model is fit on some data, and then a ROC (Receiver Operator Curve) is plotted to measure the performance of the model.\n\nI am following this tutorial over here: [https://gist.github.com/thomasmooon/6eb87964ea663f4a7441cc2b2b730bd4](https://gist.github.com/thomasmooon/6eb87964ea663f4a7441cc2b2b730bd4)\n\nEverything runs perfectly on my personal computer - however, on my work computer I am unable to download the \"pec\" library. This means, I can not plot the ROC. I do however have libraries such as \"caret\", \"dplyr\", \"ggplot2\". I was thinking that it might be possible to plot the ROC using other libraries in R such as base R graphics and ggplot2.\n\n    model &lt;- list(\"rsf\"=container_pred[[i]]$survival[,times_overlap])    \n         pec_ &lt;-  pec(   \n      object = model,     \n     formula = Surv(time = time, event = status) ~ 1, \n     # Kaplan-Meier  \n        traindata = train_data, \n         data = eval_data,     \n     exact = F,    \n     times &lt;- times     )       \n    \n     # store results in container   \n    container_pec[[i]] &lt;- pec_ }  \n     # some summary stats \n    lapply(container_pec,crps) \n    # plot\n    \n     lapply(container_pec, plot) ```'' \n\nDoes anyone know if it is possible to run the above code without using the \"pec\" library? I have the libraries \"ROCR\" and \"mlmetrics\".\n\nThanks\n\nI posted this on stackexchange as well: [https://stackoverflow.com/questions/65137064/r-plotting-roc-curves-without-the-pec-library](https://stackoverflow.com/questions/65137064/r-plotting-roc-curves-without-the-pec-library)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k6un6w/plot_roc_curves_in_r/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "-----> plot !!!  roc curves in r /!/  i am working with a computer that does not have internet access or a usb port - i only have r with limited libraries (e.g. i do not have access to the \"pec\" library). i am following a github tutorial where a \"survival analysis\" statistical model is fit on some data, and then a roc (receiver operator curve) is plotted to measure the performance of the model.\n\ni am following this tutorial over here: [https://gist.github.com/thomasmooon/6eb87964ea663f4a7441cc2b2b730bd4](https://gist.github.com/thomasmooon/6eb87964ea663f4a7441cc2b2b730bd4)\n\neverything runs perfectly on my personal computer - however, on my work computer i am unable to download the \"pec\" library. this means, i can not plot the roc. i do however have libraries such as \"caret\", \"dplyr\", \"ggplot2\". i was thinking that it might be possible to plot the roc using other libraries in r such as base r graphics and ggplot2.\n\n    model &lt;- list(\"rsf\"=container_pred[[i]]$survival[,times_overlap])    \n         pec_ &lt;-  pec(   \n      object = model,     \n     formula = surv(time = time, event = status) ~ 1, \n     # kaplan-meier  \n        traindata = train_data, \n         data = eval_data,     \n     exact = f,    \n     times &lt;- times     )       \n    \n     # store results in container   \n    container_pec[[i]] &lt;- pec_ }  \n     # some summary stats \n    lapply(container_pec,crps) \n    # plot\n    \n     lapply(container_pec, plot) ```'' \n\ndoes anyone know if it is possible to run the above code without using the \"pec\" library? i have the libraries \"rocr\" and \"mlmetrics\".\n\nthanks\n\ni posted this on stackexchange as well: [https://stackoverflow.com/questions/65137064/r-plotting-roc-curves-without-the-pec-library](https://stackoverflow.com/questions/65137064/r-plotting-roc-curves-without-the-pec-library)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/k6un6w/plot_roc_curves_in_r/',)", "identifyer": 5752167, "year": "2020"}, {"autor": "iurykai", "date": 1600108124000, "content": "Andrew Ng programming exercises kinda weird /!/ I'm doing Andrew's Machine Learning course on Coursera and I'm really demotivated when it comes to the programming parts. \n\nI'm good with the math and the coding. When it comes to writing a cost function on Octave, I just sit a while to translate the equation to code, and it's done. \n\nBut some exercises seem like a puzzle, just to understand what am I supposed to do. \n\nOn week 6, ex5 learningCurve for example. It's supposed to be a simple learning curve plot. But there's so many exceptions and \"oh you should do just one sample here and change lambda there because idk just be like that\". Not to mention times that the calculus I learned just didn't work in the code (Theta transpose X never worked, I had to write X times Theta) . I was trying to understand wtf I had to do for 40min until I just googled a solution. \n\nIt's the only thing that really holds me back from wanting to finish the course. Everything else is perfect. I want to know if there's somewhere I can better understand these exercises, like some ytb channel. And if I'm the only one that has this problem.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/isqtix/andrew_ng_programming_exercises_kinda_weird/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "andrew ng programming exercises kinda weird /!/ i'm doing andrew's machine learning course on coursera and i'm really demotivated when it comes to the programming parts. \n\ni'm good with the math and the coding. when it comes to writing a cost function on octave, i just sit a while to translate the equation to code, and it's done. \n\nbut some exercises seem like a puzzle, just to understand what am i supposed to do. \n\non week 6, ex5 learningcurve for example. it's supposed to be a simple learning curve -----> plot !!! . but there's so many exceptions and \"oh you should do just one sample here and change lambda there because idk just be like that\". not to mention times that the calculus i learned just didn't work in the code (theta transpose x never worked, i had to write x times theta) . i was trying to understand wtf i had to do for 40min until i just googled a solution. \n\nit's the only thing that really holds me back from wanting to finish the course. everything else is perfect. i want to know if there's somewhere i can better understand these exercises, like some ytb channel. and if i'm the only one that has this problem.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 52, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/isqtix/andrew_ng_programming_exercises_kinda_weird/',)", "identifyer": 5752458, "year": "2020"}, {"autor": "Longtursk", "date": 1607708640000, "content": "How to transform time series weather data into non-stationary data to fit ARIMA model /!/ I have a sample of a dataset of weather data from a sensor that took temperature readings every hour for about 100 hours that I need to run a time series analysis on. The data is non-stationary as it contains trends and seasons which can be seen in the first photo. I need to transform this data into non-stationary data to fit an ARIMA model but my efforts have proven futile. How may I fit an ARIMA model to this data since none of the transformations I tried worked?\n\n&amp;#x200B;\n\n1. This is a plot of the weather data with trends and seasons\n\n2. ACF and PCF\n\n3. First order differencing\n\n4. Second order differencing\n\n5. log transform and first order differencing\n\n6. sqrt transform and first order differencing\n\n&amp;#x200B;\n\n\\[!\\[This is a plot of the weather data\\]\\[1\\]\\]\\[1\\]\n\n&amp;#x200B;\n\n\\[!\\[ACF and PCF\\]\\[2\\]\\]\\[2\\]\n\n&amp;#x200B;\n\n\\[!\\[First order differencing\\]\\[3\\]\\]\\[3\\]\n\n&amp;#x200B;\n\n\\[!\\[Second order differencing\\]\\[4\\]\\]\\[4\\]\n\n&amp;#x200B;\n\n\\[!\\[log transform and first order differencing\\]\\[5\\]\\]\\[5\\]\n\n&amp;#x200B;\n\n\\[!\\[sqrt transform and first order differencing\\]\\[6\\]\\]\\[6\\]\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n  \\[1\\]: [https://i.stack.imgur.com/fgcYW.png](https://i.stack.imgur.com/fgcYW.png)\n\n  \\[2\\]: [https://i.stack.imgur.com/gSTqa.png](https://i.stack.imgur.com/gSTqa.png)\n\n  \\[3\\]: [https://i.stack.imgur.com/yozsU.png](https://i.stack.imgur.com/yozsU.png)\n\n  \\[4\\]: [https://i.stack.imgur.com/QNdkw.png](https://i.stack.imgur.com/QNdkw.png)\n\n  \\[5\\]: [https://i.stack.imgur.com/XkxhF.png](https://i.stack.imgur.com/XkxhF.png)\n\n  \\[6\\]: [https://i.stack.imgur.com/DcfTy.png](https://i.stack.imgur.com/DcfTy.png)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kb7fim/how_to_transform_time_series_weather_data_into/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "how to transform time series weather data into non-stationary data to fit arima model /!/ i have a sample of a dataset of weather data from a sensor that took temperature readings every hour for about 100 hours that i need to run a time series analysis on. the data is non-stationary as it contains trends and seasons which can be seen in the first photo. i need to transform this data into non-stationary data to fit an arima model but my efforts have proven futile. how may i fit an arima model to this data since none of the transformations i tried worked?\n\n&amp;#x200b;\n\n1. this is a -----> plot !!!  of the weather data with trends and seasons\n\n2. acf and pcf\n\n3. first order differencing\n\n4. second order differencing\n\n5. log transform and first order differencing\n\n6. sqrt transform and first order differencing\n\n&amp;#x200b;\n\n\\[!\\[this is a plot of the weather data\\]\\[1\\]\\]\\[1\\]\n\n&amp;#x200b;\n\n\\[!\\[acf and pcf\\]\\[2\\]\\]\\[2\\]\n\n&amp;#x200b;\n\n\\[!\\[first order differencing\\]\\[3\\]\\]\\[3\\]\n\n&amp;#x200b;\n\n\\[!\\[second order differencing\\]\\[4\\]\\]\\[4\\]\n\n&amp;#x200b;\n\n\\[!\\[log transform and first order differencing\\]\\[5\\]\\]\\[5\\]\n\n&amp;#x200b;\n\n\\[!\\[sqrt transform and first order differencing\\]\\[6\\]\\]\\[6\\]\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n&amp;#x200b;\n\n  \\[1\\]: [https://i.stack.imgur.com/fgcyw.png](https://i.stack.imgur.com/fgcyw.png)\n\n  \\[2\\]: [https://i.stack.imgur.com/gstqa.png](https://i.stack.imgur.com/gstqa.png)\n\n  \\[3\\]: [https://i.stack.imgur.com/yozsu.png](https://i.stack.imgur.com/yozsu.png)\n\n  \\[4\\]: [https://i.stack.imgur.com/qndkw.png](https://i.stack.imgur.com/qndkw.png)\n\n  \\[5\\]: [https://i.stack.imgur.com/xkxhf.png](https://i.stack.imgur.com/xkxhf.png)\n\n  \\[6\\]: [https://i.stack.imgur.com/dcfty.png](https://i.stack.imgur.com/dcfty.png)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 5, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/kb7fim/how_to_transform_time_series_weather_data_into/',)", "identifyer": 5752584, "year": "2020"}, {"autor": "ottawalanguages", "date": 1592958378000, "content": "Plotting one class svm in R /!/ Does anyone know if its possible to plot one class svm in R using the \"one_classification\" option?\n\nI have two files: svm_0 and svm_1 \nY is the binary response variable \n\nI tried the following code : \n\nTune.one &lt;- svm(y ~., data = svm_0, type = \"one-classification\", kernel = 'radial', nu = 0.05, scale = TRUE)\n\nPlot(tune.one, svm_0)\n\nI get an error message: error in plot.svm (tune.one, svm_1) : missing formula \n\nDoes anyone know what I am doing wrong?\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/heqlrr/plotting_one_class_svm_in_r/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "plotting one class svm in r /!/ does anyone know if its possible to -----> plot !!!  one class svm in r using the \"one_classification\" option?\n\ni have two files: svm_0 and svm_1 \ny is the binary response variable \n\ni tried the following code : \n\ntune.one &lt;- svm(y ~., data = svm_0, type = \"one-classification\", kernel = 'radial', nu = 0.05, scale = true)\n\nplot(tune.one, svm_0)\n\ni get an error message: error in plot.svm (tune.one, svm_1) : missing formula \n\ndoes anyone know what i am doing wrong?\n\nthanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/heqlrr/plotting_one_class_svm_in_r/',)", "identifyer": 5752741, "year": "2020"}, {"autor": "ottawalanguages", "date": 1592957450000, "content": "Plotting one class svm in R /!/ Does anyone know if its possible to plot one class svm in R using the \"one_classification\" option?\n\nI have two files: svm_0 and svm_1 \nY is the binary response variable \n\nI tried the following code : \n\nTune.one &lt;- svm(y ~., data = svm_0, type = \"one-classification\", kernel = 'radial', nu = 0.05, scale = TRUE)\n\nPlot(tune.one, svm_0)\n\nI get an error message: error in plot.svm (tune.one, svm_1) : missing formula \n\nDoes anyone know what I am doing wrong?\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/heqcvf/plotting_one_class_svm_in_r/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "plotting one class svm in r /!/ does anyone know if its possible to -----> plot !!!  one class svm in r using the \"one_classification\" option?\n\ni have two files: svm_0 and svm_1 \ny is the binary response variable \n\ni tried the following code : \n\ntune.one &lt;- svm(y ~., data = svm_0, type = \"one-classification\", kernel = 'radial', nu = 0.05, scale = true)\n\nplot(tune.one, svm_0)\n\ni get an error message: error in plot.svm (tune.one, svm_1) : missing formula \n\ndoes anyone know what i am doing wrong?\n\nthanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/heqcvf/plotting_one_class_svm_in_r/',)", "identifyer": 5752743, "year": "2020"}, {"autor": "mialalae", "date": 1592826176000, "content": "Newton's Method in Multivariable Optimization /!/ [Newton's Method in Optimization Wikipedia](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization)\n\n[Additional Helpful Article w/3D graph](https://suzyahyah.github.io/calculus/optimization/2018/04/06/Taylor-Series-Newtons-Method.html)\n\nSo I have a complex number (a + bj) and I pass it into a black box function (I don't have a formula for it) and I get an real output (A single number). I then make a 3D plot of this function by plotting the real and imaginary parts of the input number on the x and y axes and its corresponding output on the z-axis. My problem is that I want to find the minimum output of these functions after starting with an initial guess. Producing this initial guess is taken care of in another part of the program, so right now I am simply trying to minimize the output of a function with a two variable input.\n\nRight now I am doing a sort of \"grid search\" in which I shift around the real and imaginary components by 0.05 to make a 3x3 grid and then find the lowest output value. I then switch to this new output value and re-run the grid search algorithm. [I've made an album with 4 pictures which describes this process in detail here](https://imgur.com/a/TgYiCBz).\n\nNow in order to make a more efficient algorithm, I'm trying to use Newton's Method in Multivariable Optimization instead of this static grid search algorithm. I know that I'm supposed to use [this formula in which I take my current point xk and subtract from it the inverse of the Hessian multiplied by the gradient of the function](https://imgur.com/a/mBJ7TR2). My issue is trying to implement this with a black box function which means I only have discrete points to work with.\n\n[Although I figured out how to get the first partial derivatives (\u2202f/\u2202x and \u2202f/\u2202y) along with the second partial derivatives (\u2202^2 f/\u2202x^2  and \u2202^2 f/\u2202y^2 ) as shown in this picture](https://imgur.com/a/iicfZe6), I still don't know how to get the second partial derivatives of one variable deriving the other (\u2202^2 /(\u2202x\u2202y) and \u2202^2 /(\u2202y\u2202x)). In other words, I don't know how to take the partial derivative of a function with respect to one variable and then take the partial derivative of *that* partial derivative with respect to the other variable in the function given only discrete data.\n\nI need to find out how to find those last two partial derivatives in order to fully complete the Hessian matrix. If anyone could provide any ideas on how to do so, I would appreciate it.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hdqu6s/newtons_method_in_multivariable_optimization/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "newton's method in multivariable optimization /!/ [newton's method in optimization wikipedia](https://en.wikipedia.org/wiki/newton%27s_method_in_optimization)\n\n[additional helpful article w/3d graph](https://suzyahyah.github.io/calculus/optimization/2018/04/06/taylor-series-newtons-method.html)\n\nso i have a complex number (a + bj) and i pass it into a black box function (i don't have a formula for it) and i get an real output (a single number). i then make a 3d -----> plot !!!  of this function by plotting the real and imaginary parts of the input number on the x and y axes and its corresponding output on the z-axis. my problem is that i want to find the minimum output of these functions after starting with an initial guess. producing this initial guess is taken care of in another part of the program, so right now i am simply trying to minimize the output of a function with a two variable input.\n\nright now i am doing a sort of \"grid search\" in which i shift around the real and imaginary components by 0.05 to make a 3x3 grid and then find the lowest output value. i then switch to this new output value and re-run the grid search algorithm. [i've made an album with 4 pictures which describes this process in detail here](https://imgur.com/a/tgyicbz).\n\nnow in order to make a more efficient algorithm, i'm trying to use newton's method in multivariable optimization instead of this static grid search algorithm. i know that i'm supposed to use [this formula in which i take my current point xk and subtract from it the inverse of the hessian multiplied by the gradient of the function](https://imgur.com/a/mbj7tr2). my issue is trying to implement this with a black box function which means i only have discrete points to work with.\n\n[although i figured out how to get the first partial derivatives (\u2202f/\u2202x and \u2202f/\u2202y) along with the second partial derivatives (\u2202^2 f/\u2202x^2  and \u2202^2 f/\u2202y^2 ) as shown in this picture](https://imgur.com/a/iicfze6), i still don't know how to get the second partial derivatives of one variable deriving the other (\u2202^2 /(\u2202x\u2202y) and \u2202^2 /(\u2202y\u2202x)). in other words, i don't know how to take the partial derivative of a function with respect to one variable and then take the partial derivative of *that* partial derivative with respect to the other variable in the function given only discrete data.\n\ni need to find out how to find those last two partial derivatives in order to fully complete the hessian matrix. if anyone could provide any ideas on how to do so, i would appreciate it.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/hdqu6s/newtons_method_in_multivariable_optimization/',)", "identifyer": 5752826, "year": "2020"}, {"autor": "BryGuy81", "date": 1592102385000, "content": "Linear vs Logistic Regression /!/ In Excel I\u2019ve used a scatter plot to plot my linear regression data of a continuous variable. When applying a regression line in Excel I can select a few options (linear, logarithmic, polynomial, etc). The data increases fairly quickly and then level off about halfway.\n\nThe Logarithmic trend line fits well to this model. Is this just a coincidence. I thought Logarithmic was for binary classification regression. \n\nThe data does not exponentially increase, so I assume I should not use and exponential trend line. Looking at R2 the Logarithmic fits better than the Polynomial, Linear or Exponential. Can you use a Logarithmic regression on continuous variable?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/h8llmr/linear_vs_logistic_regression/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "linear vs logistic regression /!/ in excel i\u2019ve used a scatter -----> plot !!!  to -----> plot !!!  my linear regression data of a continuous variable. when applying a regression line in excel i can select a few options (linear, logarithmic, polynomial, etc). the data increases fairly quickly and then level off about halfway.\n\nthe logarithmic trend line fits well to this model. is this just a coincidence. i thought logarithmic was for binary classification regression. \n\nthe data does not exponentially increase, so i assume i should not use and exponential trend line. looking at r2 the logarithmic fits better than the polynomial, linear or exponential. can you use a logarithmic regression on continuous variable?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/h8llmr/linear_vs_logistic_regression/',)", "identifyer": 5752844, "year": "2020"}, {"autor": "znite", "date": 1584831338000, "content": "For any python data scientists out there, here's an interactive dashboard demo of what you can build with very few lines of code /!/ Here's a demo of a simple Python Interactive Dataviz Covid-19 Dashboard \n\n[https://covid19-dash.herokuapp.com](https://covid19-dash.herokuapp.com/)\n\nIt's open source (see the github link), has very little code, true interactive features including animating timeline &amp; plot interaction (see 'about this dashboard), John Hopkins data, updates daily - open for contribution and ideas, or happy to do a demo/tutorial article or webinar if any interest. App is Plotly Dash, and includes a Jupyter notebook with Plotly Express interactive viz.   \n\n\nhttps://i.redd.it/x5clxuepv3o41.gif", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fmoulw/for_any_python_data_scientists_out_there_heres_an/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "for any python data scientists out there, here's an interactive dashboard demo of what you can build with very few lines of code /!/ here's a demo of a simple python interactive dataviz covid-19 dashboard \n\n[https://covid19-dash.herokuapp.com](https://covid19-dash.herokuapp.com/)\n\nit's open source (see the github link), has very little code, true interactive features including animating timeline &amp; -----> plot !!!  interaction (see 'about this dashboard), john hopkins data, updates daily - open for contribution and ideas, or happy to do a demo/tutorial article or webinar if any interest. app is plotly dash, and includes a jupyter notebook with plotly express interactive viz.   \n\n\nhttps://i.redd.it/x5clxuepv3o41.gif", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 9, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/fmoulw/for_any_python_data_scientists_out_there_heres_an/',)", "identifyer": 5753045, "year": "2020"}, {"autor": "MasterDrake97", "date": 1584813440000, "content": "Backpropagation error /!/ Hi, I'm trying to learn how to write from scratch a ANN with Java. I don't want to use it( python or 3th party library) but I want to understand how it works and be able to write it on my own.  \nI'm trying to train to solve the XOR problem but the output is not correct and the MSE keeps increasing and fluctuating.   \n\n\nI was wondering if there was somebody who could look into my code and tell me what I'm doing wrong and what I haven't understood.\n\nThe backpropagation method and the mse calculus are the parts where I think there is some mistakes.\n\nEverytime I try to change maxEpochs, learningRate or targetRate it changes the MSE plot drastically.\n\n [Here's the repository.](https://github.com/MasterDrake/ArtificialNeuralNetwork)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fmj262/backpropagation_error/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "backpropagation error /!/ hi, i'm trying to learn how to write from scratch a ann with java. i don't want to use it( python or 3th party library) but i want to understand how it works and be able to write it on my own.  \ni'm trying to train to solve the xor problem but the output is not correct and the mse keeps increasing and fluctuating.   \n\n\ni was wondering if there was somebody who could look into my code and tell me what i'm doing wrong and what i haven't understood.\n\nthe backpropagation method and the mse calculus are the parts where i think there is some mistakes.\n\neverytime i try to change maxepochs, learningrate or targetrate it changes the mse -----> plot !!!  drastically.\n\n [here's the repository.](https://github.com/masterdrake/artificialneuralnetwork)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/fmj262/backpropagation_error/',)", "identifyer": 5753055, "year": "2020"}, {"autor": "Jandevries101", "date": 1584743848000, "content": "I can\u2019t see any details in this plot (the dot locations). How can I plot the data so that it is in higher details (without manual zoom or selectors)? I tried dpi 200 + bigger fig_size. Any tips?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fm49jp/i_cant_see_any_details_in_this_plot_the_dot/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "i can\u2019t see any details in this -----> plot !!!  (the dot locations). how can i plot the data so that it is in higher details (without manual zoom or selectors)? i tried dpi 200 + bigger fig_size. any tips?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 7, "media": "('image',)", "medialink": "('https://i.redd.it/r8nwt7qknwn41.jpg',)", "identifyer": 5753085, "year": "2020"}, {"autor": "sckulp", "date": 1589426869000, "content": "Trying to replicate Matlab NN regression performance in Keras /!/ I've done a fair amount of ML work using Matlab's deep learning toolbox, but for an upcoming project I need to start using Keras instead. So right now, I'm trying to get a simple 1D regression analysis working, in order to better understand how Keras works.\n\nMy dataset is here: https://www.dropbox.com/s/062updofukikb74/co2_records.csv?dl=0\n\nThe first column is year before 1950, and the second column is the CO2 concentration measured in ice sheet cores. The dataset has roughly 1900 samples.\n\nIn Matlab, I used a very straightforward neural network, using the Neural Network Fitting app. It's got just one hidden layer, with 150 units, trained with Bayesian Regularization backpropogation. The model trained in 1000 epochs (a couple of minutes of compute time), and the results were pretty good: https://www.dropbox.com/s/wc652pks0xn6h6j/nn_c02_predictions_150hu.png?dl=0\n\nWith Keras, though, I've really been struggling. I've tried a bunch of different settings, but here's what it looks like now:\n\n    from numpy import loadtxt\n    import tensorflow.keras\n    from keras.models import Sequential\n    from keras.layers import Dense\n    from sklearn.model_selection import train_test_split\n    import matplotlib.pyplot as plt\n    plt.style.use('seaborn-whitegrid')\n    import numpy as np;\n    \n    dataset = loadtxt('co2_records.csv', delimiter=',',skiprows=1);\n    origX=dataset[:,0];\n    origY=dataset[:,1];\n    \n    x=origX\n    y=origY\n    \n    XTraining, XValidation, YTraining, YValidation = train_test_split(x,y,test_size=0.1, random_state=0) # before model building\n    \n    model = Sequential();\n    model.add(Dense(150,input_dim=1,activation='sigmoid'));\n    model.add(Dense(1, activation='linear'))\n    \n    opt = tensorflow.keras.optimizers.Adam(learning_rate=1e-2)\n    model.compile(loss='mean_squared_error',optimizer=opt);\n    \n    model.fit(XTraining,YTraining,epochs=1000,batch_size=1711);\n    \n    predictions = model.predict(XValidation);\n    targets = YValidation;\n    deviations = predictions-targets;\n    \n    #RMSE\n    print(np.sqrt(np.mean(np.square(deviations))));\n    \n    #Plot graph\n    xQueries = np.linspace(-69,800000,10000);\n    yQueries = model.predict(xQueries);\n    \n    plt.scatter(origX, origY, marker='o');\n    plt.plot(xQueries, yQueries, color='red');\n    \n    plt.show() \n\nThe resulting graph looks like this, just a straight line through the center: https://www.dropbox.com/s/xe2hch520vi1ksz/keras_test.png?dl=0\n\nI looked at the weights generated, and they are all nearly uniform and small (roughly 4-5), while in Matlab, the trained weights could vary a lot from -200 to +200 or so.\n\nNow, I know there are strategies to make this run better, like normalizing the inputs and outputs, increasing the network size, etc. I have indeed tried them, and while they help a tiny bit, I can never get to the same performance as what I got with the Matlab toolbox. I really want to try to replicate what I got in Matlab (using the same # of hidden units and epochs), and then optimize the results from there.\n\nAny thoughts to what I'm missing?\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gjeeb3/trying_to_replicate_matlab_nn_regression/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "trying to replicate matlab nn regression performance in keras /!/ i've done a fair amount of ml work using matlab's deep learning toolbox, but for an upcoming project i need to start using keras instead. so right now, i'm trying to get a simple 1d regression analysis working, in order to better understand how keras works.\n\nmy dataset is here: https://www.dropbox.com/s/062updofukikb74/co2_records.csv?dl=0\n\nthe first column is year before 1950, and the second column is the co2 concentration measured in ice sheet cores. the dataset has roughly 1900 samples.\n\nin matlab, i used a very straightforward neural network, using the neural network fitting app. it's got just one hidden layer, with 150 units, trained with bayesian regularization backpropogation. the model trained in 1000 epochs (a couple of minutes of compute time), and the results were pretty good: https://www.dropbox.com/s/wc652pks0xn6h6j/nn_c02_predictions_150hu.png?dl=0\n\nwith keras, though, i've really been struggling. i've tried a bunch of different settings, but here's what it looks like now:\n\n    from numpy import loadtxt\n    import tensorflow.keras\n    from keras.models import sequential\n    from keras.layers import dense\n    from sklearn.model_selection import train_test_split\n    import matplotlib.pyplot as plt\n    plt.style.use('seaborn-whitegrid')\n    import numpy as np;\n    \n    dataset = loadtxt('co2_records.csv', delimiter=',',skiprows=1);\n    origx=dataset[:,0];\n    origy=dataset[:,1];\n    \n    x=origx\n    y=origy\n    \n    xtraining, xvalidation, ytraining, yvalidation = train_test_split(x,y,test_size=0.1, random_state=0) # before model building\n    \n    model = sequential();\n    model.add(dense(150,input_dim=1,activation='sigmoid'));\n    model.add(dense(1, activation='linear'))\n    \n    opt = tensorflow.keras.optimizers.adam(learning_rate=1e-2)\n    model.compile(loss='mean_squared_error',optimizer=opt);\n    \n    model.fit(xtraining,ytraining,epochs=1000,batch_size=1711);\n    \n    predictions = model.predict(xvalidation);\n    targets = yvalidation;\n    deviations = predictions-targets;\n    \n    #rmse\n    print(np.sqrt(np.mean(np.square(deviations))));\n    \n    #-----> plot !!!  graph\n    xqueries = np.linspace(-69,800000,10000);\n    yqueries = model.predict(xqueries);\n    \n    plt.scatter(origx, origy, marker='o');\n    plt.plot(xqueries, yqueries, color='red');\n    \n    plt.show() \n\nthe resulting graph looks like this, just a straight line through the center: https://www.dropbox.com/s/xe2hch520vi1ksz/keras_test.png?dl=0\n\ni looked at the weights generated, and they are all nearly uniform and small (roughly 4-5), while in matlab, the trained weights could vary a lot from -200 to +200 or so.\n\nnow, i know there are strategies to make this run better, like normalizing the inputs and outputs, increasing the network size, etc. i have indeed tried them, and while they help a tiny bit, i can never get to the same performance as what i got with the matlab toolbox. i really want to try to replicate what i got in matlab (using the same # of hidden units and epochs), and then optimize the results from there.\n\nany thoughts to what i'm missing?\n\nthanks!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/gjeeb3/trying_to_replicate_matlab_nn_regression/',)", "identifyer": 5753368, "year": "2020"}, {"autor": "8329417966", "date": 1597335676000, "content": "Data Visualization using Matrix Plot | Python| Seaborn /!/ https://youtu.be/XU6NLA_U6_E", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i92y6n/data_visualization_using_matrix_plot_python/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "data visualization using matrix -----> plot !!!  | python| seaborn /!/ https://youtu.be/xu6nla_u6_e", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/i92y6n/data_visualization_using_matrix_plot_python/',)", "identifyer": 5753621, "year": "2020"}, {"autor": "GuilhermeLoWa", "date": 1590438613000, "content": "Relationship between batch size and test's loss and accuracy /!/ tldr:   \n**Why would batch size determine the relationship between Test Loss x Test Accuracy?**  \n**Why batch size 8 yields lower loss, 32 yields higher loss and 256/512 medium loss with predicable behaviour?**\n\nFirst I'll present the experiment, the results with questions and then my attempts at answering those questions.\n\n# The experiment\n\nHi! Thanks for taking your time to read me. I'm beginning in NLP and built a simple multilayer perceptron to Text Classification on the AG NEWS dataset - a balanced dataset, 4 classes, 120k training samples and 7,6k test. I'm training on 114k samples and validating on 6k.\n\nThe model has 1 embedding layer (dim 32), 1 hidden layer (32 neurons, relu activation function) and the output layer (4 neurons). I'm using SGD with initial learning rate 4.0 (and adjusting it each epoch), cross entropy as my loss function and I'm training it for 100 epochs. Below is the code for the model, [built upon the PyTorch Text Classification example](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html). I can give full code if needed.\n\n    class TextSentiment(nn.Module):\n        def __init__(self, vocab_size, embed_dim, num_hidden, num_class):\n            super().__init__()\n            self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n            self.fc = nn.Linear(embed_dim, num_hidden)\n            self.fc2 = nn.Linear(num_hidden, num_class)\n            self.init_weights()\n    \n        def init_weights(self):\n            initrange = 0.5\n            self.embedding.weight.data.uniform_(-initrange, initrange)\n            self.fc.weight.data.uniform_(-initrange, initrange)\n            self.fc2.weight.data.uniform_(-initrange, initrange)\n            self.fc.bias.data.zero_()\n            self.fc2.bias.data.zero_()\n    \n        def forward(self, text, offsets):\n            out = self.embedding(text, offsets)\n            out = F.relu(self.fc(out))\n            out = self.fc2(out)\n            return out\n\nAnd here is the training function:\n\n    def train_func(sub_train_, model, batch_size, criterion, optimizer, scheduler):\n    model.train()\n    train_loss = 0\n    train_acc = 0\n    data = DataLoader(sub_train_, batch_size=batch_size, shuffle=True,\n                      collate_fn=generate_batch)\n for i, (text, offsets, cls) in enumerate(data):\n        optimizer.zero_grad()\n        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n        output = model(text, offsets)\n        loss = criterion(output, cls)\n        train_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n        train_acc += (output.argmax(1) == cls).sum().item()\n\n\n  scheduler.step()\n\n  return train_loss / len(sub_train_), train_acc / len(sub_train_)\n\nIn order to better understand NNs I'm testing different configurations for hyperparameters. The first one I tested was batch size. For this experiment, I tried 4 different values of batch sizes (512, 256, 32, 8) running them 20 times each.\n\n# Results and questions\n\nThe full results can be found in [this Tensorboard Dev](https://tensorboard.dev/experiment/7JXjFPNoQQ6kPzHpt1KaXQ/) (ignore total\\_time and best\\_epoch. Unfortunately HParams, which is where I got the following plots from, do not appear in the Tensorboard Dev, don't know why).   \n\n\nThe first surprise came with the Test Accuracy x Loss scatter plot across different runs:\n\n  \n\n\n[Test Loss x Test Acc across runs](https://preview.redd.it/omtad91szy051.png?width=374&amp;format=png&amp;auto=webp&amp;s=44e8b5c05d62409d749f302ff6183082878575a9)\n\nIt is possible to see the tendency of test loss going down while accuracy increases (purple dots). However, there seems to be a very disperse cluster above this tendency (redish dots) and another one below (blueish dots). The curious thing to me is what separates them: batch size. The cluster above (red dots) all have batch size 32 (except for one bs8\\_run1 who seems to be an outlier of this group). Batch size 32 has only one run inside the tendency, run14, which seems to be outlier in its group.\n\n&amp;#x200B;\n\n[Batch size 32 runs](https://preview.redd.it/2en1v2570z051.png?width=388&amp;format=png&amp;auto=webp&amp;s=d16365e3211f37b19e3112a3eff106c8b3218668)\n\nThe cluster below all have batch size 8.  BS 8 only two, runs 2 and 9, who also seem to form a cluster in its group. \n\n&amp;#x200B;\n\n[Batch size 8 runs](https://preview.redd.it/sqzx3buo0z051.png?width=333&amp;format=png&amp;auto=webp&amp;s=d4b1921d14747723851d7ddc4b1d105b6320157a)\n\n&amp;#x200B;\n\n**Why would batch size determine the relationship between Test Loss x Test Accuracy? - Question 1**  \n\n\nHere is the plot of Loss x Batch Size, to better understand what I mean by outliers and cluster (sorry about the order. Apparently Tensorboard sorts in \"alphabetical\" order instead of numerical?):\n\n&amp;#x200B;\n\nhttps://preview.redd.it/qb4qijvwzy051.png?width=439&amp;format=png&amp;auto=webp&amp;s=97121348b3774e97a441356c6fb9baca482928ea\n\nThis plot also shows well the second curious relationship I can't explain: loss seems to be much lower for BS 8, around the same for 512 and 256 and much higher and disperse for BS 32. **Why batch size 8 yields lower loss, 32 yields higher loss and 256/512 medium loss with predicable behaviour? - Question 2**\n\n# Discussion\n\nThose two questions seems to be linked, but I can't find an intuition about why is that so.\n\nI understand that having a smaller batch size you can actually correct your function to more specific cases (since your batch is smaller, you can adjust a smaller gradient to specific cases). This could explain both why BS 8 runs have lower loss and why they tend to get stuck on local minimum (9/20 runs). They also yield higher accuracy on training set, which might be explained by the same reasoning. But then why would BS 32 loss be higher than 256/512?\n\nI don't know how the scheduler or the cross entropy loss works precisely, so might have to do with this. I think I'll take Andrew Ng's course and study this more profoundly.\n\n\\[EXTRA\\] Do you know any NN playground, [such as this one](http://playground.tensorflow.org/), that also shows the training process and what's going on?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gqiox3/relationship_between_batch_size_and_tests_loss/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "relationship between batch size and test's loss and accuracy /!/ tldr:   \n**why would batch size determine the relationship between test loss x test accuracy?**  \n**why batch size 8 yields lower loss, 32 yields higher loss and 256/512 medium loss with predicable behaviour?**\n\nfirst i'll present the experiment, the results with questions and then my attempts at answering those questions.\n\n# the experiment\n\nhi! thanks for taking your time to read me. i'm beginning in nlp and built a simple multilayer perceptron to text classification on the ag news dataset - a balanced dataset, 4 classes, 120k training samples and 7,6k test. i'm training on 114k samples and validating on 6k.\n\nthe model has 1 embedding layer (dim 32), 1 hidden layer (32 neurons, relu activation function) and the output layer (4 neurons). i'm using sgd with initial learning rate 4.0 (and adjusting it each epoch), cross entropy as my loss function and i'm training it for 100 epochs. below is the code for the model, [built upon the pytorch text classification example](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html). i can give full code if needed.\n\n    class textsentiment(nn.module):\n        def __init__(self, vocab_size, embed_dim, num_hidden, num_class):\n            super().__init__()\n            self.embedding = nn.embeddingbag(vocab_size, embed_dim, sparse=true)\n            self.fc = nn.linear(embed_dim, num_hidden)\n            self.fc2 = nn.linear(num_hidden, num_class)\n            self.init_weights()\n    \n        def init_weights(self):\n            initrange = 0.5\n            self.embedding.weight.data.uniform_(-initrange, initrange)\n            self.fc.weight.data.uniform_(-initrange, initrange)\n            self.fc2.weight.data.uniform_(-initrange, initrange)\n            self.fc.bias.data.zero_()\n            self.fc2.bias.data.zero_()\n    \n        def forward(self, text, offsets):\n            out = self.embedding(text, offsets)\n            out = f.relu(self.fc(out))\n            out = self.fc2(out)\n            return out\n\nand here is the training function:\n\n    def train_func(sub_train_, model, batch_size, criterion, optimizer, scheduler):\n    model.train()\n    train_loss = 0\n    train_acc = 0\n    data = dataloader(sub_train_, batch_size=batch_size, shuffle=true,\n                      collate_fn=generate_batch)\n for i, (text, offsets, cls) in enumerate(data):\n        optimizer.zero_grad()\n        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n        output = model(text, offsets)\n        loss = criterion(output, cls)\n        train_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n        train_acc += (output.argmax(1) == cls).sum().item()\n\n\n  scheduler.step()\n\n  return train_loss / len(sub_train_), train_acc / len(sub_train_)\n\nin order to better understand nns i'm testing different configurations for hyperparameters. the first one i tested was batch size. for this experiment, i tried 4 different values of batch sizes (512, 256, 32, 8) running them 20 times each.\n\n# results and questions\n\nthe full results can be found in [this tensorboard dev](https://tensorboard.dev/experiment/7jxjfpnoqq6kpzhpt1kaxq/) (ignore total\\_time and best\\_epoch. unfortunately hparams, which is where i got the following plots from, do not appear in the tensorboard dev, don't know why).   \n\n\nthe first surprise came with the test accuracy x loss scatter -----> plot !!!  across different runs:\n\n  \n\n\n[test loss x test acc across runs](https://preview.redd.it/omtad91szy051.png?width=374&amp;format=png&amp;auto=webp&amp;s=44e8b5c05d62409d749f302ff6183082878575a9)\n\nit is possible to see the tendency of test loss going down while accuracy increases (purple dots). however, there seems to be a very disperse cluster above this tendency (redish dots) and another one below (blueish dots). the curious thing to me is what separates them: batch size. the cluster above (red dots) all have batch size 32 (except for one bs8\\_run1 who seems to be an outlier of this group). batch size 32 has only one run inside the tendency, run14, which seems to be outlier in its group.\n\n&amp;#x200b;\n\n[batch size 32 runs](https://preview.redd.it/2en1v2570z051.png?width=388&amp;format=png&amp;auto=webp&amp;s=d16365e3211f37b19e3112a3eff106c8b3218668)\n\nthe cluster below all have batch size 8.  bs 8 only two, runs 2 and 9, who also seem to form a cluster in its group. \n\n&amp;#x200b;\n\n[batch size 8 runs](https://preview.redd.it/sqzx3buo0z051.png?width=333&amp;format=png&amp;auto=webp&amp;s=d4b1921d14747723851d7ddc4b1d105b6320157a)\n\n&amp;#x200b;\n\n**why would batch size determine the relationship between test loss x test accuracy? - question 1**  \n\n\nhere is the plot of loss x batch size, to better understand what i mean by outliers and cluster (sorry about the order. apparently tensorboard sorts in \"alphabetical\" order instead of numerical?):\n\n&amp;#x200b;\n\nhttps://preview.redd.it/qb4qijvwzy051.png?width=439&amp;format=png&amp;auto=webp&amp;s=97121348b3774e97a441356c6fb9baca482928ea\n\nthis plot also shows well the second curious relationship i can't explain: loss seems to be much lower for bs 8, around the same for 512 and 256 and much higher and disperse for bs 32. **why batch size 8 yields lower loss, 32 yields higher loss and 256/512 medium loss with predicable behaviour? - question 2**\n\n# discussion\n\nthose two questions seems to be linked, but i can't find an intuition about why is that so.\n\ni understand that having a smaller batch size you can actually correct your function to more specific cases (since your batch is smaller, you can adjust a smaller gradient to specific cases). this could explain both why bs 8 runs have lower loss and why they tend to get stuck on local minimum (9/20 runs). they also yield higher accuracy on training set, which might be explained by the same reasoning. but then why would bs 32 loss be higher than 256/512?\n\ni don't know how the scheduler or the cross entropy loss works precisely, so might have to do with this. i think i'll take andrew ng's course and study this more profoundly.\n\n\\[extra\\] do you know any nn playground, [such as this one](http://playground.tensorflow.org/), that also shows the training process and what's going on?", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/gqiox3/relationship_between_batch_size_and_tests_loss/',)", "identifyer": 5753992, "year": "2020"}, {"autor": "jsinghdata", "date": 1606775564000, "content": "Resolve Non-linearity issues in Regression by Variable Transformation /!/ Hello,\n\nI am working on linear regression problem for the Airfoil self Noise Dataset.; [data link](https://archive.ics.uci.edu/ml/datasets/Airfoil+Self-Noise). After some basic data exploration I found that relationship between the response variable (i.e. \\`decibel\\`) and some of the predictors is not linear. For example, I have attached the scatter plot between \\`decibel\\` and \\`Angle\\`. \n\nhttps://preview.redd.it/yc4d7xr1fg261.png?width=1118&amp;format=png&amp;auto=webp&amp;s=596de5e91479597f15923b213c4ff63cc218d689\n\nI was wondering is it possible to use some sort of variable transformation which can be used to get roughly linear plot. Ideas/feedback is appreciated.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k471ka/resolve_nonlinearity_issues_in_regression_by/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "resolve non-linearity issues in regression by variable transformation /!/ hello,\n\ni am working on linear regression problem for the airfoil self noise dataset.; [data link](https://archive.ics.uci.edu/ml/datasets/airfoil+self-noise). after some basic data exploration i found that relationship between the response variable (i.e. \\`decibel\\`) and some of the predictors is not linear. for example, i have attached the scatter -----> plot !!!  between \\`decibel\\` and \\`angle\\`. \n\nhttps://preview.redd.it/yc4d7xr1fg261.png?width=1118&amp;format=png&amp;auto=webp&amp;s=596de5e91479597f15923b213c4ff63cc218d689\n\ni was wondering is it possible to use some sort of variable transformation which can be used to get roughly linear plot. ideas/feedback is appreciated.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 2, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/k471ka/resolve_nonlinearity_issues_in_regression_by/',)", "identifyer": 5754384, "year": "2020"}, {"autor": "biohacker_tobe", "date": 1606771482000, "content": "Model Interpretation: Prediction Intervals /!/ Hey community, I'm exploring the subject of prediction intervals. I'm trying to interpret the following plot but am a bit stuck on what to comment additionally.\n\nMin and Max: Blue and Green Lines\n\nPredicted Values: Black Points\n\nTrue Values: Red Line\n\n&amp;#x200B;\n\nhttps://preview.redd.it/ou6rr4uz2g261.png?width=685&amp;format=png&amp;auto=webp&amp;s=f22ebe610febc28d3f7220c1e0f40240f5b2466b", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k45mof/model_interpretation_prediction_intervals/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "model interpretation: prediction intervals /!/ hey community, i'm exploring the subject of prediction intervals. i'm trying to interpret the following -----> plot !!!  but am a bit stuck on what to comment additionally.\n\nmin and max: blue and green lines\n\npredicted values: black points\n\ntrue values: red line\n\n&amp;#x200b;\n\nhttps://preview.redd.it/ou6rr4uz2g261.png?width=685&amp;format=png&amp;auto=webp&amp;s=f22ebe610febc28d3f7220c1e0f40240f5b2466b", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/k45mof/model_interpretation_prediction_intervals/',)", "identifyer": 5754386, "year": "2020"}, {"autor": "post_hazanko", "date": 1608316233000, "content": "Could I use ML for automatically finding random objects based on histograms(1D/2D) and contours? /!/ I'm working on a personal ground-based navigation vision project. It's basic, not a lot of compute eg. running on a RaspberryPi but it's not running in real time eg. frame by frame.\n\nCurrently I'm working on figuring out some kind of algorithm that based on a photo's histogram(1d for light intensity for v), histogram(2d for h and s ranges) ; I can use these values to generate the HSV masks to apply to an image. Then find the largest contour areas.\n\nWhile I have worked it out roughly how it works(from the plot outputs) and then it can be turned into math eventually by ranges... I'm wondering if this process could train something, is it worth it?\n\nI would have to manually take a lot of pictures(how many) and apply the expected masks and I don't know how exact that would be, or at least I could do the desired output which is the bounding squares.\n\nThis is not your cliche \"find the red cup\" thing. That's the hard part is the dynamic objects(clothes/plastic bags/irregular shapes) and random color/lighting.\n\nBut manually based on those histograms I can isolate almost everything in an image eventually.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kfqpo7/could_i_use_ml_for_automatically_finding_random/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "could i use ml for automatically finding random objects based on histograms(1d/2d) and contours? /!/ i'm working on a personal ground-based navigation vision project. it's basic, not a lot of compute eg. running on a raspberrypi but it's not running in real time eg. frame by frame.\n\ncurrently i'm working on figuring out some kind of algorithm that based on a photo's histogram(1d for light intensity for v), histogram(2d for h and s ranges) ; i can use these values to generate the hsv masks to apply to an image. then find the largest contour areas.\n\nwhile i have worked it out roughly how it works(from the -----> plot !!!  outputs) and then it can be turned into math eventually by ranges... i'm wondering if this process could train something, is it worth it?\n\ni would have to manually take a lot of pictures(how many) and apply the expected masks and i don't know how exact that would be, or at least i could do the desired output which is the bounding squares.\n\nthis is not your cliche \"find the red cup\" thing. that's the hard part is the dynamic objects(clothes/plastic bags/irregular shapes) and random color/lighting.\n\nbut manually based on those histograms i can isolate almost everything in an image eventually.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 4, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/kfqpo7/could_i_use_ml_for_automatically_finding_random/',)", "identifyer": 5754428, "year": "2020"}, {"autor": "blueest", "date": 1608276239000, "content": "Time series forecasting with irregular patterns /!/ Suppose you are interested in forecasting future values of a univariate time series (you are interested in monthly forecasts and you have the past 25 years of data). Let's say there have been several major and minor \"shocks\" (e.g. corona virus, housing crisis, 9/11, etc) that have taken place over the years (you can visually observe these when looking at a plot of the time series). \n\nUsing standard time series forecasting models, is it still possible to produce \"decent\" estimates for future values of this time series? \n\nDoes anyone have any information on checking how \"good\" a time series model is? Do you just use the aic/bic, check the value of parameter estimates and the rmse on a \"test set\" (occuring at a chronologically later point in time)? Are there any standard time series \"cross validation\" and \"training procedures\" similar to standard machine learning problems?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kfgjny/time_series_forecasting_with_irregular_patterns/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "time series forecasting with irregular patterns /!/ suppose you are interested in forecasting future values of a univariate time series (you are interested in monthly forecasts and you have the past 25 years of data). let's say there have been several major and minor \"shocks\" (e.g. corona virus, housing crisis, 9/11, etc) that have taken place over the years (you can visually observe these when looking at a -----> plot !!!  of the time series). \n\nusing standard time series forecasting models, is it still possible to produce \"decent\" estimates for future values of this time series? \n\ndoes anyone have any information on checking how \"good\" a time series model is? do you just use the aic/bic, check the value of parameter estimates and the rmse on a \"test set\" (occuring at a chronologically later point in time)? are there any standard time series \"cross validation\" and \"training procedures\" similar to standard machine learning problems?", "sortedWord": "None", "removed": "('nan',)", "score": 2, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/kfgjny/time_series_forecasting_with_irregular_patterns/',)", "identifyer": 5754454, "year": "2020"}], "name": "plotlearnmachinelearning2020"}
{"interestingcomments": [{"autor": "CrownSparrow", "date": 1597981964000, "content": "Having trouble with my backpropagation function [Help] [Project] /!/ **Intro:**\n\nHello! So I'm a ML noob, and I've been working on my very first neural network class library from scratch. So far it's been fair sailing, except for my backpropagation function. It's not working at all. So I came here looking for some help. Before I continue I should say three things: 1) as I say, I'm a ML noob. 2) I don't understand any of the math behind neural networks. I just put what ever math I need into my function and don't question it. Maybe that's stupid, but I haven't learned calculus. And 3) This is a very basic neural network implementation. I'm using stuff like the Sigmoid function and stuff.\n\n&amp;#x200B;\n\n**My implementation outline:**\n\nRight, so the first thing I think I should do is outline how I've made my network. I'm using an OOP approach. That way things are much easier to think about. Oh, and the language is C#. The main object I have is a neuron object. It acts as, well, a neuron. It has an activation &amp; backprop function (plus a few properties and other functions, but those don't matter for this). Then, I have a neural layer class, which is simply an array of neuron objects. Last, I have a network object that is simply an array of layer objects. This means that all the real functionality is down to the neuron object. For example, the activation function of the network merely loops over all the layers and calls their activation functions. And each layer merely loops over all its neurons and calls their activation function. There's a bit more to it than that, but that's 95% of it. The backprop works the same way: network calls layers, layers calls neurons' backprop function.\n\n&amp;#x200B;\n\n**The issue and what I think is wrong:**\n\nSo, here's the issue: when performing backpropagation, the little nudges the network calculates for all the weights and biases are absurd. Like, stuff is being \"nudged\" by -8.9 **million**. That can't be right. Now, here is what I think is wrong: when calculating the expected output for a hidden layer, I'm using a net of changes each neuron in the current layers wants to make to the next layer (I'm looping backwards). And I think that's wrong. I mean, I should't be using the net changes as the expected output of the next layer, right? One other thing is this: when I do calculate the net changes, they can sometimes come out negative. And because I'm using said net changes as the expected output, that means the next layer is adjusting for it's outputs to be negative. But a neuron's output can't be any lower than 0. So that can't be right either.\n\n&amp;#x200B;\n\n**Links:**\n\n [My code for the backpropagation of the neurons.](https://pastebin.com/7VNtfQ21) \n\n [I have a 20 minute video explaining what I just talked about, if you for some reason you'd watch that.](https://www.youtube.com/watch?v=mLnZGltp-p4) \n\n[This is a tutorial I've used as a guide when trying to figure out what math I use where.](https://www.youtube.com/watch?v=-WjKICvAOsY)\n\n&amp;#x200B;\n\nSorry for the long post!\n\n(Note: I'm posting right before bed, so if you have questions I might be a while in replying)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/idpkd5/having_trouble_with_my_backpropagation_function/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "bed", "selectorShort": "bed", "MarkedSent": "having trouble with my backpropagation function [help] [project] /!/ **intro:**\n\nhello! so i'm a ml noob, and i've been working on my very first neural network class library from scratch. so far it's been fair sailing, except for my backpropagation function. it's not working at all. so i came here looking for some help. before i continue i should say three things: 1) as i say, i'm a ml noob. 2) i don't understand any of the math behind neural networks. i just put what ever math i need into my function and don't question it. maybe that's stupid, but i haven't learned calculus. and 3) this is a very basic neural network implementation. i'm using stuff like the sigmoid function and stuff.\n\n&amp;#x200b;\n\n**my implementation outline:**\n\nright, so the first thing i think i should do is outline how i've made my network. i'm using an oop approach. that way things are much easier to think about. oh, and the language is c#. the main object i have is a neuron object. it acts as, well, a neuron. it has an activation &amp; backprop function (plus a few properties and other functions, but those don't matter for this). then, i have a neural layer class, which is simply an array of neuron objects. last, i have a network object that is simply an array of layer objects. this means that all the real functionality is down to the neuron object. for example, the activation function of the network merely loops over all the layers and calls their activation functions. and each layer merely loops over all its neurons and calls their activation function. there's a bit more to it than that, but that's 95% of it. the backprop works the same way: network calls layers, layers calls neurons' backprop function.\n\n&amp;#x200b;\n\n**the issue and what i think is wrong:**\n\nso, here's the issue: when performing backpropagation, the little nudges the network calculates for all the weights and biases are absurd. like, stuff is being \"nudged\" by -8.9 **million**. that can't be right. now, here is what i think is wrong: when calculating the expected output for a hidden layer, i'm using a net of changes each neuron in the current layers wants to make to the next layer (i'm looping backwards). and i think that's wrong. i mean, i should't be using the net changes as the expected output of the next layer, right? one other thing is this: when i do calculate the net changes, they can sometimes come out negative. and because i'm using said net changes as the expected output, that means the next layer is adjusting for it's outputs to be negative. but a neuron's output can't be any lower than 0. so that can't be right either.\n\n&amp;#x200b;\n\n**links:**\n\n [my code for the backpropagation of the neurons.](https://pastebin.com/7vntfq21) \n\n [i have a 20 minute video explaining what i just talked about, if you for some reason you'd watch that.](https://www.youtube.com/watch?v=mlnzgltp-p4) \n\n[this is a tutorial i've used as a guide when trying to figure out what math i use where.](https://www.youtube.com/watch?v=-wjkicvaosy)\n\n&amp;#x200b;\n\nsorry for the long post!\n\n(note: i'm posting right before -----> bed !!! , so if you have questions i might be a while in replying)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 7, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/idpkd5/having_trouble_with_my_backpropagation_function/',)", "identifyer": 5743161, "year": "2020"}, {"autor": "TrueBirch", "date": 1607404252000, "content": "Conceptual ideas for random forest prediction speed? /!/ I'm a data scientist and for some reason I find myself thinking about random forests instead of going to bed. I'm well versed in their theory and I regularly use them at work. Maybe I'm overthinking here but I'm curious if there are ways to speed up predictions. \n\n\nPredicting the output for a new entry requires feeding the entry's features through every decision tree in the forest and combining the results. Some forests have lots of trees. This takes time. There are techniques to make things run faster, but I'm curious why every tree has to vote on every row of data. I have three specific questions:\n\n\n1. Let's think about classification for example. In the extreme case, if you've polled 51% of the trees and they've all agreed on a class, could you save time by making a prediction without asking the remaining 49% for their opinion? Obviously you'd lose the class probabilities, but there are applications where the final output is what matters, not the probabilities. \n\n\n2. Is there a way to find trees that are virtually identical and combine them? One of the two identical trees would make a prediction and that prediction would have double the influence. \n\n\n3. Adding complexity, could you train a fast model to predict the outcome based on the predictions of a handful of highly influential trees? You'd run your new data through a dozen (or however many) trees and then feed the 12 predictions as features into the downstream model to make the final prediction.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k8y2b5/conceptual_ideas_for_random_forest_prediction/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "bed", "selectorShort": "bed", "MarkedSent": "conceptual ideas for random forest prediction speed? /!/ i'm a data scientist and for some reason i find myself thinking about random forests instead of going to -----> bed !!! . i'm well versed in their theory and i regularly use them at work. maybe i'm overthinking here but i'm curious if there are ways to speed up predictions. \n\n\npredicting the output for a new entry requires feeding the entry's features through every decision tree in the forest and combining the results. some forests have lots of trees. this takes time. there are techniques to make things run faster, but i'm curious why every tree has to vote on every row of data. i have three specific questions:\n\n\n1. let's think about classification for example. in the extreme case, if you've polled 51% of the trees and they've all agreed on a class, could you save time by making a prediction without asking the remaining 49% for their opinion? obviously you'd lose the class probabilities, but there are applications where the final output is what matters, not the probabilities. \n\n\n2. is there a way to find trees that are virtually identical and combine them? one of the two identical trees would make a prediction and that prediction would have double the influence. \n\n\n3. adding complexity, could you train a fast model to predict the outcome based on the predictions of a handful of highly influential trees? you'd run your new data through a dozen (or however many) trees and then feed the 12 predictions as features into the downstream model to make the final prediction.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/k8y2b5/conceptual_ideas_for_random_forest_prediction/',)", "identifyer": 5744981, "year": "2020"}, {"autor": "5QkR3NZw8NjD", "date": 1584528806000, "content": "Tensorflow embeddings layer. /!/ Hi,\n\nI have a sentiment analysis assignment and just need something cleared up in my head.  When training the embeddings layer for sentiment analysis(on reviews, positive or negative) will the resulting cosine distance of the word vectors represent how close the words are in terms of their sentiment? E.g. \"Bed\" and \"car\" will be close if they are always used with the same sentiment.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fkno0n/tensorflow_embeddings_layer/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "bed", "selectorShort": "bed", "MarkedSent": "tensorflow embeddings layer. /!/ hi,\n\ni have a sentiment analysis assignment and just need something cleared up in my head.  when training the embeddings layer for sentiment analysis(on reviews, positive or negative) will the resulting cosine distance of the word vectors represent how close the words are in terms of their sentiment? e.g. \"-----> bed !!! \" and \"car\" will be close if they are always used with the same sentiment.", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 1, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/fkno0n/tensorflow_embeddings_layer/',)", "identifyer": 5745168, "year": "2020"}, {"autor": "KodlaK1593", "date": 1597888460000, "content": "Regarding Jupyter Notebooks - Is it possible to save code that has already been run in memory? /!/ Question is in the title, but I will elaborate a little bit. I am fairly new to this, and I am practicing with classification methods using some data I found on Kaggle. I am trying to iterate through a large set of images as part of the data processing process, and the loop I am using will take hours to finish. Ultimately I will need to go to bed and turn off my computer tonight, and I would very much like to not  have to run the code and wait all over again tomorrow so I can continue with my project. I would think there are people out there running more time intensive pieces of code than this one, and I find it hard to believe people doing this for a living are willing to sacrifice so much time each day to continue with their work. \n\nI have noticed that I need to re-run my code from the beginning each time I open a notebook in order to continue from where I left off. Is it possible to store the code that has already run in memory so that I do not have to run it again when restarting? Or is this just the way it is and I need to make my code more efficient? How do data scientists typically handle this issue? \n\nI hope that makes some sense and I am wording all this correctly, thanks in advance for the help!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/id1e8k/regarding_jupyter_notebooks_is_it_possible_to/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "bed", "selectorShort": "bed", "MarkedSent": "regarding jupyter notebooks - is it possible to save code that has already been run in memory? /!/ question is in the title, but i will elaborate a little bit. i am fairly new to this, and i am practicing with classification methods using some data i found on kaggle. i am trying to iterate through a large set of images as part of the data processing process, and the loop i am using will take hours to finish. ultimately i will need to go to -----> bed !!!  and turn off my computer tonight, and i would very much like to not  have to run the code and wait all over again tomorrow so i can continue with my project. i would think there are people out there running more time intensive pieces of code than this one, and i find it hard to believe people doing this for a living are willing to sacrifice so much time each day to continue with their work. \n\ni have noticed that i need to re-run my code from the beginning each time i open a notebook in order to continue from where i left off. is it possible to store the code that has already run in memory so that i do not have to run it again when restarting? or is this just the way it is and i need to make my code more efficient? how do data scientists typically handle this issue? \n\ni hope that makes some sense and i am wording all this correctly, thanks in advance for the help!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 8, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/id1e8k/regarding_jupyter_notebooks_is_it_possible_to/',)", "identifyer": 5745403, "year": "2020"}, {"autor": "DeadliestToast", "date": 1590565165000, "content": "Question: What are some ML algorithms for discrete event analysis? /!/ I have a dataset with a number of discrete events indexed by timestamp. For instance:\n\n&gt; 10:00 AM | Event Code 1 | Toast had some tea\n&gt; 11:52 AM | Event Code 5 | Toast went for a walk\n&gt; 11:59 AM | Event Code 1 | Toast had some tea\n\nI'm looking for algorithms which help me predict what the likely next 'state' of the system will be, given the previous events (i.e. if there is a run of Event Code 8 - Toast yawns, and an Event Code 9 - Toast brushes his teeth, this probably means Toast is about to go to bed).\n\nApologies if there's a name for this, but a quick google didn't return anything that looked obvious. I thought about both doing clustering and looking at some kind of RNN, and wondered if anyone else had come across something like this before, and what worked for them?\n\nBest,\nToast", "link": "https://www.reddit.com/r/learnmachinelearning/comments/grf7kc/question_what_are_some_ml_algorithms_for_discrete/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "bed", "selectorShort": "bed", "MarkedSent": "question: what are some ml algorithms for discrete event analysis? /!/ i have a dataset with a number of discrete events indexed by timestamp. for instance:\n\n&gt; 10:00 am | event code 1 | toast had some tea\n&gt; 11:52 am | event code 5 | toast went for a walk\n&gt; 11:59 am | event code 1 | toast had some tea\n\ni'm looking for algorithms which help me predict what the likely next 'state' of the system will be, given the previous events (i.e. if there is a run of event code 8 - toast yawns, and an event code 9 - toast brushes his teeth, this probably means toast is about to go to -----> bed !!! ).\n\napologies if there's a name for this, but a quick google didn't return anything that looked obvious. i thought about both doing clustering and looking at some kind of rnn, and wondered if anyone else had come across something like this before, and what worked for them?\n\nbest,\ntoast", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/grf7kc/question_what_are_some_ml_algorithms_for_discrete/',)", "identifyer": 5749194, "year": "2020"}, {"autor": "vladiliescu", "date": 1605183693000, "content": "Hey there Reddit! I'm writing a blog post on Azure Machine Learning, and I need your feedback! /!/ Hi girls and guys! I've written an article on getting started with Azure Automated ML to post on my [website](https://vladiliescu.net), but then my insecurities started kicking in and I decided to come and ask for feedback from you wonderful people. What do you think? Does it make sense to write about stuff like this, or it's not needed at all since everything si too obvious?\n\nThanks for taking the time to look at this!\n\nBombs away!\n\n\\---\n\nI\u2019m gonna start with an inconvenient truth: Machine learning is hard. It used to be harder though, and I feel like ML is getting more and more accessible each day. But acquiring the right background needed to understand what\u2019s going on under the hood of PyTorch or scikit-learn or whatever library you happen to be using is, well, still hard. It requires a lot of work, as the brilliant [A Super Harsh Guide to Machine Learning](https://www.reddit.com/r/MachineLearning/comments/5z8110/d_a_super_harsh_guide_to_machine_learning/) likes to remind us:\n\n*- First, read f\\\\***\\\\***\\\\\\*ing Hastie, Tibshirani, and whoever. Chapters 1-4 and 7-8. If you don\u2019t understand it, keep reading it until you do.*\n\n*- You can read the rest of the book if you want. You probably should, but I\u2019ll assume you know all of it.*\n\n*- Take Andrew Ng\u2019s Coursera. Do all the exercises in Python and R. Make sure you get the same answers with all of them.*\n\n*- Now forget all of that and read the deep learning book. Put TensorFlow and PyTorch on a Linux box and run examples until you get it. Do stuff with CNNs and RNNs and just feed forward NNs.*\n\n*- Once you do all of that, go on arXiv and read the most recent useful papers. The literature changes every few months, so keep up.*\n\nNo wonder machine learning intimidates people: you need to invest significant effort just to understand what\u2019s going on, what you can and cannot do. And then, once things start to make sense, you need to work twice as hard just to keep up with all the research being published all the time. [Red Queen\u2019s race](https://en.wikipedia.org/wiki/Red_Queen%27s_race) anyone?\n\n\u201cBut what if\u201d \u2014 you\u2019ll say \u2014 \u201cwhat if we could outsource this whole machine learning thing, at least partially? What if it were somebody else\u2019s problem?\u201d That would be nice, wouldn\u2019t it?\n\nJust imagine, handing over to someone whatever data you managed to gather, going to bed with a grateful heart, and waking up the next day to a shiny new trained-and-tested model, ready to be deployed and integrated and whatnot. Gee, that would be absolutely splendid \ud83d\ude31 wouldn\u2019t it?\n\nWell, apparently, other people \u2014 engineers, no doubt about it thought the same thing, and decided to solve this problem once and for all. You know how they like to automate this and that, so it was only a matter of time before they automated machine learning too \u2014 from [Wikipedia](https://en.wikipedia.org/wiki/Automated_machine_learning):\n\n**Automated machine learning** *(***AutoML***) is the process of* [*automating*](https://en.wikipedia.org/wiki/Automation) *the process of applying* [*machine learning*](https://en.wikipedia.org/wiki/Machine_learning) *to real-world problems. AutoML covers the complete pipeline from the raw dataset to the deployable machine learning model. AutoML was proposed as an* [*artificial intelligence*](https://en.wikipedia.org/wiki/Artificial_intelligence) *-based solution to the ever-growing challenge of applying machine learning. The high degree of automation in AutoML allows non-experts to make use of machine learning models and techniques without requiring becoming an expert in the field first.*\n\nIt goes on.\n\n*Automating the process of applying machine learning end-to-end additionally offers the advantages of producing simpler solutions, faster creation of those solutions, and models that often outperform hand-designed models.*\n\nWow. Models that often outperform hand-designed models?!? Well, sign me up with my main email! This is what we\u2019ve been looking for isn\u2019t it?\n\nThe truth is, there are several tools &amp; libraries &amp; online services that promise to help in this regard: [H2O.ai](https://www.h2o.ai/), [Microsoft\u2019s Automated Machine Learning](https://azure.microsoft.com/en-us/services/machine-learning/automatedml/), [Google\u2019s AutoML](https://cloud.google.com/automl), [auto-sklearn](https://automl.github.io/auto-sklearn/master/), [TPOT](https://github.com/EpistasisLab/tpot), and [Auto-PyTorch](https://github.com/automl/Auto-PyTorch) to name just a few. Each one with its own strengths and weaknesses, depending on your particular needs and background. Comparing them is somewhat outside the scope of this article, but I strongly suggest you give a try to at least a few of them and see how they stack up against each other.\n\nFor me, my current personal favorite is [Automated ML](https://azure.microsoft.com/en-us/services/machine-learning/automatedml/) \u2014 a cloud service-slash-library that acts like a recommender engine, looking at your data, checking its quirks and stats and whether it can work around them or not by, say, imputing or normalizing fields. It then uses those inputs to recommend a series of ML algorithms, selecting the best-performing one in the process. Let me show you how to use it.\n\n**Using Automated Machine Learning**\n\nAt a high level, all automated ml needs is some labeled data and a computer to run on, and this can be either your local computer or some machine in the cloud.\n\n&amp;#x200B;\n\n[The map is not the territory but it sure helps](https://preview.redd.it/ccua17rdwsy51.png?width=638&amp;format=png&amp;auto=webp&amp;s=75a591ac8ee1bf44f58eccce53f3de53117f7fdc)\n\nOnce started, Automated ML will use the compute you hand it over to run multiple experiments on your data, trying out various combinations of algorithms &amp; hyper-parameters, until it trains a good-enough model, which you can then use and integrate in whatever app you might be building.\n\nFirst things first though. As we all know, machine learning doesn\u2019t exist in a vacuum - we need to have a higher purpose for this whole \u201cget the data train the model\u201d thing. For example, let\u2019s say we\u2019re building a startup focused on real-estate investments, and a crucial functionality is the ability to forecast house prices for different cities, blocks, etc. This sounds like a great opportunity to use machine learning, maybe even add a bit of blockchain for good measure \ud83d\ude0b. This should be good enough to secure an initial round of funding, and then we\u2019re off to the races. \n\nIncidentally, this vision helps us start with the most difficult part of this process - finding and assembling the data. In order to automagically train a model that can forecast house prices, we need a dataset with house prices. Luckily, there are such open datasets, including the one from Kaggle\u2019s famous [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) competition. We could use that, but since we\u2019re not interested competing we can go directly for its source, the [entire Ames dataset](http://jse.amstat.org/v19n3/decock.pdf). From the description:\n\n*This paper presents a data set describing the sale of individual residential property in Ames, Iowa from 2006 to 2010. The data set contains 2930 observations and a large number of explanatory variables (23 nominal, 23 ordinal, 14 discrete, and 20 continuous) involved in assessing home values.*\n\nLooks good, let\u2019s use it! Using it instead of the one provided by Kaggle should give our model twice the data to train on, which in turn should yield better results. One thing you should not do though, is train a model using the entire Ames dataset, and use it to compete in the House Prices competition. That just spoils the fun for everyone.\n\nAnyways, now that we\u2019ve found a training dataset, we\u2019re ready to start using Automated ML. You will need an [Azure subscription](https://azure.microsoft.com/en-us/free/), along with an [Azure Machine Learning Workspace](https://azure.microsoft.com/en-us/services/machine-learning/) before you continue.\n\n**Registering a dataset**\n\nWhat we\u2019re gonna do first is load our dataset in Azure Machine Learning to be able to reference it later. We\u2019ll do that by going to [Azure Machine Learning Studio](https://ml.azure.com/) and choosing to Create a dataset from web files from the Datasets menu.\n\n&amp;#x200B;\n\n[Creating a new dataset](https://preview.redd.it/jxr8je4fwsy51.png?width=1680&amp;format=png&amp;auto=webp&amp;s=656ecb347a086990b4f5b72fb79a73ec8fc51f9b)\n\nThis is the simplest option really, you could also have uploaded the dataset from your computer, or referenced a dataset already uploaded on our datastore (we\u2019ll get to what this means in a moment). All you need to do now is enter the dataset\u2019s address (in our case it\u2019s http://jse.amstat.org/v19n3/decock/AmesHousing.txt), pick a good name (NotAmesHousing is always a winner in my book), and make sure the type of the dataset is Tabular and not some other option like File .\n\nNow that we\u2019ve told AML where the data is, we need to also describe it a little. It should deduce itself that this is a tab-delimited file, encoded as UTF-8, but it most likely won\u2019t know to read the first row as headers. Because who puts the headers in the first row, right? Anyways, make sure to set the Column headers as Use headers from the first file, which works even though in our case the first file is the only file. You\u2019ll also get the chance to review the data, and make sure it\u2019s parsed correctly (ignore the Id field column, that\u2019s just there to tell you the row number, it won\u2019t get added to the dataset).\n\nThe next step is a bit more challenging.\n\n&amp;#x200B;\n\n[Selecting features](https://preview.redd.it/c3eq14viwsy51.png?width=2810&amp;format=png&amp;auto=webp&amp;s=0554db7aac9ec9b48cf5c99a69be63313874959e)\n\nYou\u2019ll probably be wondering what\u2019s with the Path column, since this wasn\u2019t mentioned anywhere so far? Actually that would be useful if we had imported several files, each with its own path, but with just one file it\u2019s rather useless. You can leave it unchecked, while all other columns should be left checked. Except maybe PID, that looks useless - it\u2019s the Parcel Identification Number, as per the [Data Documentation](http://jse.amstat.org/v19n3/decock/DataDocumentation.txt). Let\u2019s leave it in though, and see if Automated ML can figure that out by itself \ud83d\ude08.\n\nIn the end, your settings should look like this:\n\n&amp;#x200B;\n\n[Dataset creation summary](https://preview.redd.it/0z3lelbkwsy51.png?width=2086&amp;format=png&amp;auto=webp&amp;s=bb375638fbe2a3b49b1d0bcfe7b28eb0d04ad8ca)\n\nWe won\u2019t look at profiling datasets today, but you should know that checking this will calculate statistics such as mean, standard deviation, etc. for your entire dataset, as opposed to just getting them for a smaller subset. This won\u2019t be needed for now, and we\u2019ll be able to generate profiling data later on anyway.\n\nAnd that\u2019s it \ud83c\udf89! We now have a dataset, ready to be parsed and processed and used for training. The only thing standing between us and a bathtub full of VC money is finding out a way to run this automated ml thing - and we\u2019ll do just that by configuring a compute resource.\n\n&amp;#x200B;\n\n[Data is done, now check the rest](https://preview.redd.it/jaj76mimwsy51.png?width=638&amp;format=png&amp;auto=webp&amp;s=70f08f6f2fad85aa0ec0875094258398b9cf137c)\n\n**Configuring a compute resource**\n\nConfiguring a new compute in Azure ML is quite easy actually - what we need here is a Compute cluster. As you can see below, in the Compute menu you can create several types of compute, such as compute instances (useful for running notebooks in the cloud), inference clusters (useful for running your trained models and making predictions with them), and also attached compute (a bring-your-own-compute deal, in which you can attach existing HDInsight or Databricks clusters even virtual machines, and use them as compute targets). We\u2019ll stick to an Azure-managed Compute cluster though.\n\n&amp;#x200B;\n\n[Creating a new compute cluster](https://preview.redd.it/8nmgduiowsy51.png?width=1942&amp;format=png&amp;auto=webp&amp;s=0c4ac36e0d666ca6d946ec98721da494d6be8449)\n\nWe\u2019ll need to pay some attention to the next step here, since the compute size can and will significantly influence the time (and implicitly money) Automated ML needs to spend in order to train a good model.\n\nLooking at our data, at 2930 rows and 82 columns this won\u2019t take up that much RAM to load and process, so we can ignore RAM and focus on getting the best CPU we can afford. In our case this means something from the compute-optimized [Fsv2-series](https://azure.microsoft.com/en-us/pricing/details/virtual-machines/windows/#compute-optimized-tab-content) of machines, I went for Standard\\_F4s\\_v2 myself. Looking at the specs, it offers the same CPU performance as the default, general purpose Standard\\_DS3\\_v2 machine, but at a **30%** discount (you can check the pricing [here](https://azure.microsoft.com/en-us/pricing/details/machine-learning/), too). Not too shabby, if I do say so myself. \n\nYou could also go for an even faster machine, but that will fill up your core quota and you won\u2019t be able to start as many VMs in parallel. And generally you want to start as many VMs in parallel as possible, in order to allow Automated ML to explore as many options, as fast as possible. \n\n&amp;#x200B;\n\n[Configuring a compute cluster's size](https://preview.redd.it/hh93d6spwsy51.png?width=2738&amp;format=png&amp;auto=webp&amp;s=b60e82af8ccfcaf8e7b695cbc12f062bf430f116)\n\nOnce you select the best machine money (and quota) can buy, you only need to give it a good name (I named mine Spock \ud83e\udd13) and select a minimum and a maximum number of nodes. Considering our Automated Machine Learning scenario, I\u2019d set the minimum to 0 (we don\u2019t want to pre-allocate and implicitly pay for VMs if we\u2019re not gonna use them), and the maximum to whatever our quota allows (for Standard\\_F4s\\_v2 and my quota of 24, that means 6 nodes). As I said before, the more nodes we can allocate the faster Automated ML will train a suitable model.\n\n&amp;#x200B;\n\n[Two down, one to go](https://preview.redd.it/45id0iduwsy51.png?width=638&amp;format=png&amp;auto=webp&amp;s=d0f1f03be0efb3df3927f02cede52c9ea56082e7)\n\n**Might and Magic: Automated Machine Learning**\n\nThis is the fun part - we\u2019ll start off by going to the aptly named Automated ML menu, and choosing to create a new Automated ML run. We\u2019ll select our dataset, and then configure a few options: \n\n&amp;#x200B;\n\n[Creating a new Automated ML run](https://preview.redd.it/gejqwwgvwsy51.png?width=2886&amp;format=png&amp;auto=webp&amp;s=9ff0a6f1eb3a0d0e79222b4f668a6871511736d7)\n\nDon\u2019t worry too much about having to create a new Experiment, that\u2019s just the entity Azure ML uses to group Runs (and don\u2019t worry too much about what a Run is either, we\u2019ll talk about that some other time \ud83d\ude05). We\u2019ll need to tell Automated ML which column to predict - SalePrice, and on which compute resource to run - Spock. \n\nNext up, we\u2019ll need to tell it what kind of problem we\u2019re facing - do we need to predict a category (Classification), a continuous numeric value (Regression), or something based on time (Time series)? Now, we know that we want to predict house prices, which is to say numeric continuous values, so we\u2019re gonna go for Regression.\n\n&amp;#x200B;\n\n[Selecting the task type](https://preview.redd.it/n3vz72txwsy51.png?width=2838&amp;format=png&amp;auto=webp&amp;s=f89f3af0fe7343f731075571a950a05f7755585f)\n\nWe also have the change to fine-tune Automated ML\u2019s configuration settings, which control how it approaches the whole find-a-good-model-and-then-stop process, and its featurization settings, which control how it transforms the data. We\u2019ll only look at the configuration settings for now.\n\n&amp;#x200B;\n\n[Additional configuration settings](https://preview.redd.it/i5pmynxywsy51.png?width=1264&amp;format=png&amp;auto=webp&amp;s=51631ed0ff8d96fd0c41557304f0e73ec4ab076b)\n\nIn my case, I\u2019ve chosen to evaluate the automatically trained models using Normalized root mean squared error, since I want to be able to know how far off my predictions are. I don\u2019t want Automated ML to take more than one hour to find a suitable model, because I want my costs to be **really** predictable. I want to evaluate the models using a 5-fold cross validation to make sure it\u2019s not overfitting my data, and finally, I want to evaluate as many models as possible so I chose to have 6 concurrent iterations (remember, that\u2019s the maximum for my compute quota), effectively enabling it to try out 6 potential models in parallel.\n\n&amp;#x200B;\n\n[Run 1](https://preview.redd.it/464rw1a0xsy51.png?width=2130&amp;format=png&amp;auto=webp&amp;s=814b2520e026a3bdcc0c2be707d86b3907596d93)\n\nOnce started, you\u2019ll notice a new experiment has been created which has a run, well, running. This run, lovingly called Run 1, contains all there is to know about our automl run - most importantly any and all data issues it detected and their fixes in the Data guardrails tab, and also a growing list of potential models currently evaluated in the Child runs tab.\n\n&amp;#x200B;\n\n[Data guardrails](https://preview.redd.it/qf25ru41xsy51.png?width=3428&amp;format=png&amp;auto=webp&amp;s=0c63ea4e2270eed6bc5ddb919325a5927c001a8f)\n\n&amp;#x200B;\n\n[Child runs](https://preview.redd.it/wd2eti52xsy51.png?width=3210&amp;format=png&amp;auto=webp&amp;s=09188e79c6f4efd59b3b32cb8d32e4ad1381a373)\n\nYou\u2019ll notice a bit of a delay when you first start an AutoML run \u2014 this is because of mainly two reasons:\n\n&amp;#x200B;\n\n1. On the one hand, before it starts, Azure ML needs to configure a Docker image with all the Python packages needed to run. This takes a few minutes, but it ensures further reproducibility (plus, it\u2019s really cool)\n2. On the other hand, our compute nodes each need to be allocated and then they need to pull those automl-enabled Docker images created earlier, taking a few minutes more \u2014 you can check the status of the compute allocation at any time in the Compute menu, but I suggest brewing a coffee instead and only take a look afterwards to make sure there\u2019s something to see.\n\n&amp;#x200B;\n\n[Compute clusters being allocated](https://preview.redd.it/20c7e924xsy51.png?width=2894&amp;format=png&amp;auto=webp&amp;s=6714a7ad5cb64ea369122d19fbb84babd929cf29)\n\nAfter what I can only hope were no more than three espressos and a latte, we\u2019ll start seeing some interesting results. For my setup, in less than 40 minutes Automated ML explored 65 possible models, including stacked and voting ensembles of the best performing models, and identified a Voting Ensemble as being the best of the best.\n\n&amp;#x200B;\n\n[Run 1 completed](https://preview.redd.it/ux2zocw6xsy51.png?width=1536&amp;format=png&amp;auto=webp&amp;s=d8832f8ea14b34e0fff080e5f6bd6b87a3faad16)\n\nYou can take a look at its details of course, there\u2019s a lot of interesting stuff here including but not limited to the generated explanations and metrics - you\u2019ll notice AutoML calculated a lot of metrics apart from our primary one - Normalized root mean squared error. These include Explained variance, Mean absolute error, R2 score, Spearman correlation, even Root mean squared error. The special thing about the primary metric is that it\u2019s used to identify the best model - had you chosen a different metric, you may have gotten a different model.\n\nOnce you decide which model to use, there\u2019s also a simple way for you to  Deploy your model directly in Azure ML, or even Download it and deploy it to the cloud of your liking.\n\n&amp;#x200B;\n\n[Best model details](https://preview.redd.it/j6ducc9ixsy51.png?width=1202&amp;format=png&amp;auto=webp&amp;s=3997863fea45a1b05d6db5abf3cde6088ce48c5b)\n\nAnd, that\u2019s it for a quick intro to Azure Automated Machine Learning. Let\u2019s review the steps here: you\u2019ve defined a problem, found meaningful data that can be used to solve said problem, configured the necessary Azure resources that enabled you to automatically train a model on the data, and now you\u2019re ready to make use of this model, be it for further evaluation and improvement, or for deploying it as a REST endpoint and integrating it in your AI-powered, blockchain-enabled startup.\n\nThanks for reading so far!\n\n&amp;#x200B;\n\n[The End](https://preview.redd.it/6f89tf1kxsy51.png?width=958&amp;format=png&amp;auto=webp&amp;s=16273d0f3a691ab8a411634a7a4bb2f2081f8759)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jstoh8/hey_there_reddit_im_writing_a_blog_post_on_azure/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "bed", "selectorShort": "bed", "MarkedSent": "hey there reddit! i'm writing a blog post on azure machine learning, and i need your feedback! /!/ hi girls and guys! i've written an article on getting started with azure automated ml to post on my [website](https://vladiliescu.net), but then my insecurities started kicking in and i decided to come and ask for feedback from you wonderful people. what do you think? does it make sense to write about stuff like this, or it's not needed at all since everything si too obvious?\n\nthanks for taking the time to look at this!\n\nbombs away!\n\n\\---\n\ni\u2019m gonna start with an inconvenient truth: machine learning is hard. it used to be harder though, and i feel like ml is getting more and more accessible each day. but acquiring the right background needed to understand what\u2019s going on under the hood of pytorch or scikit-learn or whatever library you happen to be using is, well, still hard. it requires a lot of work, as the brilliant [a super harsh guide to machine learning](https://www.reddit.com/r/machinelearning/comments/5z8110/d_a_super_harsh_guide_to_machine_learning/) likes to remind us:\n\n*- first, read f\\\\***\\\\***\\\\\\*ing hastie, tibshirani, and whoever. chapters 1-4 and 7-8. if you don\u2019t understand it, keep reading it until you do.*\n\n*- you can read the rest of the book if you want. you probably should, but i\u2019ll assume you know all of it.*\n\n*- take andrew ng\u2019s coursera. do all the exercises in python and r. make sure you get the same answers with all of them.*\n\n*- now forget all of that and read the deep learning book. put tensorflow and pytorch on a linux box and run examples until you get it. do stuff with cnns and rnns and just feed forward nns.*\n\n*- once you do all of that, go on arxiv and read the most recent useful papers. the literature changes every few months, so keep up.*\n\nno wonder machine learning intimidates people: you need to invest significant effort just to understand what\u2019s going on, what you can and cannot do. and then, once things start to make sense, you need to work twice as hard just to keep up with all the research being published all the time. [red queen\u2019s race](https://en.wikipedia.org/wiki/red_queen%27s_race) anyone?\n\n\u201cbut what if\u201d \u2014 you\u2019ll say \u2014 \u201cwhat if we could outsource this whole machine learning thing, at least partially? what if it were somebody else\u2019s problem?\u201d that would be nice, wouldn\u2019t it?\n\njust imagine, handing over to someone whatever data you managed to gather, going to -----> bed !!!  with a grateful heart, and waking up the next day to a shiny new trained-and-tested model, ready to be deployed and integrated and whatnot. gee, that would be absolutely splendid \ud83d\ude31 wouldn\u2019t it?\n\nwell, apparently, other people \u2014 engineers, no doubt about it thought the same thing, and decided to solve this problem once and for all. you know how they like to automate this and that, so it was only a matter of time before they automated machine learning too \u2014 from [wikipedia](https://en.wikipedia.org/wiki/automated_machine_learning):\n\n**automated machine learning** *(***automl***) is the process of* [*automating*](https://en.wikipedia.org/wiki/automation) *the process of applying* [*machine learning*](https://en.wikipedia.org/wiki/machine_learning) *to real-world problems. automl covers the complete pipeline from the raw dataset to the deployable machine learning model. automl was proposed as an* [*artificial intelligence*](https://en.wikipedia.org/wiki/artificial_intelligence) *-based solution to the ever-growing challenge of applying machine learning. the high degree of automation in automl allows non-experts to make use of machine learning models and techniques without requiring becoming an expert in the field first.*\n\nit goes on.\n\n*automating the process of applying machine learning end-to-end additionally offers the advantages of producing simpler solutions, faster creation of those solutions, and models that often outperform hand-designed models.*\n\nwow. models that often outperform hand-designed models?!? well, sign me up with my main email! this is what we\u2019ve been looking for isn\u2019t it?\n\nthe truth is, there are several tools &amp; libraries &amp; online services that promise to help in this regard: [h2o.ai](https://www.h2o.ai/), [microsoft\u2019s automated machine learning](https://azure.microsoft.com/en-us/services/machine-learning/automatedml/), [google\u2019s automl](https://cloud.google.com/automl), [auto-sklearn](https://automl.github.io/auto-sklearn/master/), [tpot](https://github.com/epistasislab/tpot), and [auto-pytorch](https://github.com/automl/auto-pytorch) to name just a few. each one with its own strengths and weaknesses, depending on your particular needs and background. comparing them is somewhat outside the scope of this article, but i strongly suggest you give a try to at least a few of them and see how they stack up against each other.\n\nfor me, my current personal favorite is [automated ml](https://azure.microsoft.com/en-us/services/machine-learning/automatedml/) \u2014 a cloud service-slash-library that acts like a recommender engine, looking at your data, checking its quirks and stats and whether it can work around them or not by, say, imputing or normalizing fields. it then uses those inputs to recommend a series of ml algorithms, selecting the best-performing one in the process. let me show you how to use it.\n\n**using automated machine learning**\n\nat a high level, all automated ml needs is some labeled data and a computer to run on, and this can be either your local computer or some machine in the cloud.\n\n&amp;#x200b;\n\n[the map is not the territory but it sure helps](https://preview.redd.it/ccua17rdwsy51.png?width=638&amp;format=png&amp;auto=webp&amp;s=75a591ac8ee1bf44f58eccce53f3de53117f7fdc)\n\nonce started, automated ml will use the compute you hand it over to run multiple experiments on your data, trying out various combinations of algorithms &amp; hyper-parameters, until it trains a good-enough model, which you can then use and integrate in whatever app you might be building.\n\nfirst things first though. as we all know, machine learning doesn\u2019t exist in a vacuum - we need to have a higher purpose for this whole \u201cget the data train the model\u201d thing. for example, let\u2019s say we\u2019re building a startup focused on real-estate investments, and a crucial functionality is the ability to forecast house prices for different cities, blocks, etc. this sounds like a great opportunity to use machine learning, maybe even add a bit of blockchain for good measure \ud83d\ude0b. this should be good enough to secure an initial round of funding, and then we\u2019re off to the races. \n\nincidentally, this vision helps us start with the most difficult part of this process - finding and assembling the data. in order to automagically train a model that can forecast house prices, we need a dataset with house prices. luckily, there are such open datasets, including the one from kaggle\u2019s famous [house prices: advanced regression techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) competition. we could use that, but since we\u2019re not interested competing we can go directly for its source, the [entire ames dataset](http://jse.amstat.org/v19n3/decock.pdf). from the description:\n\n*this paper presents a data set describing the sale of individual residential property in ames, iowa from 2006 to 2010. the data set contains 2930 observations and a large number of explanatory variables (23 nominal, 23 ordinal, 14 discrete, and 20 continuous) involved in assessing home values.*\n\nlooks good, let\u2019s use it! using it instead of the one provided by kaggle should give our model twice the data to train on, which in turn should yield better results. one thing you should not do though, is train a model using the entire ames dataset, and use it to compete in the house prices competition. that just spoils the fun for everyone.\n\nanyways, now that we\u2019ve found a training dataset, we\u2019re ready to start using automated ml. you will need an [azure subscription](https://azure.microsoft.com/en-us/free/), along with an [azure machine learning workspace](https://azure.microsoft.com/en-us/services/machine-learning/) before you continue.\n\n**registering a dataset**\n\nwhat we\u2019re gonna do first is load our dataset in azure machine learning to be able to reference it later. we\u2019ll do that by going to [azure machine learning studio](https://ml.azure.com/) and choosing to create a dataset from web files from the datasets menu.\n\n&amp;#x200b;\n\n[creating a new dataset](https://preview.redd.it/jxr8je4fwsy51.png?width=1680&amp;format=png&amp;auto=webp&amp;s=656ecb347a086990b4f5b72fb79a73ec8fc51f9b)\n\nthis is the simplest option really, you could also have uploaded the dataset from your computer, or referenced a dataset already uploaded on our datastore (we\u2019ll get to what this means in a moment). all you need to do now is enter the dataset\u2019s address (in our case it\u2019s http://jse.amstat.org/v19n3/decock/ameshousing.txt), pick a good name (notameshousing is always a winner in my book), and make sure the type of the dataset is tabular and not some other option like file .\n\nnow that we\u2019ve told aml where the data is, we need to also describe it a little. it should deduce itself that this is a tab-delimited file, encoded as utf-8, but it most likely won\u2019t know to read the first row as headers. because who puts the headers in the first row, right? anyways, make sure to set the column headers as use headers from the first file, which works even though in our case the first file is the only file. you\u2019ll also get the chance to review the data, and make sure it\u2019s parsed correctly (ignore the id field column, that\u2019s just there to tell you the row number, it won\u2019t get added to the dataset).\n\nthe next step is a bit more challenging.\n\n&amp;#x200b;\n\n[selecting features](https://preview.redd.it/c3eq14viwsy51.png?width=2810&amp;format=png&amp;auto=webp&amp;s=0554db7aac9ec9b48cf5c99a69be63313874959e)\n\nyou\u2019ll probably be wondering what\u2019s with the path column, since this wasn\u2019t mentioned anywhere so far? actually that would be useful if we had imported several files, each with its own path, but with just one file it\u2019s rather useless. you can leave it unchecked, while all other columns should be left checked. except maybe pid, that looks useless - it\u2019s the parcel identification number, as per the [data documentation](http://jse.amstat.org/v19n3/decock/datadocumentation.txt). let\u2019s leave it in though, and see if automated ml can figure that out by itself \ud83d\ude08.\n\nin the end, your settings should look like this:\n\n&amp;#x200b;\n\n[dataset creation summary](https://preview.redd.it/0z3lelbkwsy51.png?width=2086&amp;format=png&amp;auto=webp&amp;s=bb375638fbe2a3b49b1d0bcfe7b28eb0d04ad8ca)\n\nwe won\u2019t look at profiling datasets today, but you should know that checking this will calculate statistics such as mean, standard deviation, etc. for your entire dataset, as opposed to just getting them for a smaller subset. this won\u2019t be needed for now, and we\u2019ll be able to generate profiling data later on anyway.\n\nand that\u2019s it \ud83c\udf89! we now have a dataset, ready to be parsed and processed and used for training. the only thing standing between us and a bathtub full of vc money is finding out a way to run this automated ml thing - and we\u2019ll do just that by configuring a compute resource.\n\n&amp;#x200b;\n\n[data is done, now check the rest](https://preview.redd.it/jaj76mimwsy51.png?width=638&amp;format=png&amp;auto=webp&amp;s=70f08f6f2fad85aa0ec0875094258398b9cf137c)\n\n**configuring a compute resource**\n\nconfiguring a new compute in azure ml is quite easy actually - what we need here is a compute cluster. as you can see below, in the compute menu you can create several types of compute, such as compute instances (useful for running notebooks in the cloud), inference clusters (useful for running your trained models and making predictions with them), and also attached compute (a bring-your-own-compute deal, in which you can attach existing hdinsight or databricks clusters even virtual machines, and use them as compute targets). we\u2019ll stick to an azure-managed compute cluster though.\n\n&amp;#x200b;\n\n[creating a new compute cluster](https://preview.redd.it/8nmgduiowsy51.png?width=1942&amp;format=png&amp;auto=webp&amp;s=0c4ac36e0d666ca6d946ec98721da494d6be8449)\n\nwe\u2019ll need to pay some attention to the next step here, since the compute size can and will significantly influence the time (and implicitly money) automated ml needs to spend in order to train a good model.\n\nlooking at our data, at 2930 rows and 82 columns this won\u2019t take up that much ram to load and process, so we can ignore ram and focus on getting the best cpu we can afford. in our case this means something from the compute-optimized [fsv2-series](https://azure.microsoft.com/en-us/pricing/details/virtual-machines/windows/#compute-optimized-tab-content) of machines, i went for standard\\_f4s\\_v2 myself. looking at the specs, it offers the same cpu performance as the default, general purpose standard\\_ds3\\_v2 machine, but at a **30%** discount (you can check the pricing [here](https://azure.microsoft.com/en-us/pricing/details/machine-learning/), too). not too shabby, if i do say so myself. \n\nyou could also go for an even faster machine, but that will fill up your core quota and you won\u2019t be able to start as many vms in parallel. and generally you want to start as many vms in parallel as possible, in order to allow automated ml to explore as many options, as fast as possible. \n\n&amp;#x200b;\n\n[configuring a compute cluster's size](https://preview.redd.it/hh93d6spwsy51.png?width=2738&amp;format=png&amp;auto=webp&amp;s=b60e82af8ccfcaf8e7b695cbc12f062bf430f116)\n\nonce you select the best machine money (and quota) can buy, you only need to give it a good name (i named mine spock \ud83e\udd13) and select a minimum and a maximum number of nodes. considering our automated machine learning scenario, i\u2019d set the minimum to 0 (we don\u2019t want to pre-allocate and implicitly pay for vms if we\u2019re not gonna use them), and the maximum to whatever our quota allows (for standard\\_f4s\\_v2 and my quota of 24, that means 6 nodes). as i said before, the more nodes we can allocate the faster automated ml will train a suitable model.\n\n&amp;#x200b;\n\n[two down, one to go](https://preview.redd.it/45id0iduwsy51.png?width=638&amp;format=png&amp;auto=webp&amp;s=d0f1f03be0efb3df3927f02cede52c9ea56082e7)\n\n**might and magic: automated machine learning**\n\nthis is the fun part - we\u2019ll start off by going to the aptly named automated ml menu, and choosing to create a new automated ml run. we\u2019ll select our dataset, and then configure a few options: \n\n&amp;#x200b;\n\n[creating a new automated ml run](https://preview.redd.it/gejqwwgvwsy51.png?width=2886&amp;format=png&amp;auto=webp&amp;s=9ff0a6f1eb3a0d0e79222b4f668a6871511736d7)\n\ndon\u2019t worry too much about having to create a new experiment, that\u2019s just the entity azure ml uses to group runs (and don\u2019t worry too much about what a run is either, we\u2019ll talk about that some other time \ud83d\ude05). we\u2019ll need to tell automated ml which column to predict - saleprice, and on which compute resource to run - spock. \n\nnext up, we\u2019ll need to tell it what kind of problem we\u2019re facing - do we need to predict a category (classification), a continuous numeric value (regression), or something based on time (time series)? now, we know that we want to predict house prices, which is to say numeric continuous values, so we\u2019re gonna go for regression.\n\n&amp;#x200b;\n\n[selecting the task type](https://preview.redd.it/n3vz72txwsy51.png?width=2838&amp;format=png&amp;auto=webp&amp;s=f89f3af0fe7343f731075571a950a05f7755585f)\n\nwe also have the change to fine-tune automated ml\u2019s configuration settings, which control how it approaches the whole find-a-good-model-and-then-stop process, and its featurization settings, which control how it transforms the data. we\u2019ll only look at the configuration settings for now.\n\n&amp;#x200b;\n\n[additional configuration settings](https://preview.redd.it/i5pmynxywsy51.png?width=1264&amp;format=png&amp;auto=webp&amp;s=51631ed0ff8d96fd0c41557304f0e73ec4ab076b)\n\nin my case, i\u2019ve chosen to evaluate the automatically trained models using normalized root mean squared error, since i want to be able to know how far off my predictions are. i don\u2019t want automated ml to take more than one hour to find a suitable model, because i want my costs to be **really** predictable. i want to evaluate the models using a 5-fold cross validation to make sure it\u2019s not overfitting my data, and finally, i want to evaluate as many models as possible so i chose to have 6 concurrent iterations (remember, that\u2019s the maximum for my compute quota), effectively enabling it to try out 6 potential models in parallel.\n\n&amp;#x200b;\n\n[run 1](https://preview.redd.it/464rw1a0xsy51.png?width=2130&amp;format=png&amp;auto=webp&amp;s=814b2520e026a3bdcc0c2be707d86b3907596d93)\n\nonce started, you\u2019ll notice a new experiment has been created which has a run, well, running. this run, lovingly called run 1, contains all there is to know about our automl run - most importantly any and all data issues it detected and their fixes in the data guardrails tab, and also a growing list of potential models currently evaluated in the child runs tab.\n\n&amp;#x200b;\n\n[data guardrails](https://preview.redd.it/qf25ru41xsy51.png?width=3428&amp;format=png&amp;auto=webp&amp;s=0c63ea4e2270eed6bc5ddb919325a5927c001a8f)\n\n&amp;#x200b;\n\n[child runs](https://preview.redd.it/wd2eti52xsy51.png?width=3210&amp;format=png&amp;auto=webp&amp;s=09188e79c6f4efd59b3b32cb8d32e4ad1381a373)\n\nyou\u2019ll notice a bit of a delay when you first start an automl run \u2014 this is because of mainly two reasons:\n\n&amp;#x200b;\n\n1. on the one hand, before it starts, azure ml needs to configure a docker image with all the python packages needed to run. this takes a few minutes, but it ensures further reproducibility (plus, it\u2019s really cool)\n2. on the other hand, our compute nodes each need to be allocated and then they need to pull those automl-enabled docker images created earlier, taking a few minutes more \u2014 you can check the status of the compute allocation at any time in the compute menu, but i suggest brewing a coffee instead and only take a look afterwards to make sure there\u2019s something to see.\n\n&amp;#x200b;\n\n[compute clusters being allocated](https://preview.redd.it/20c7e924xsy51.png?width=2894&amp;format=png&amp;auto=webp&amp;s=6714a7ad5cb64ea369122d19fbb84babd929cf29)\n\nafter what i can only hope were no more than three espressos and a latte, we\u2019ll start seeing some interesting results. for my setup, in less than 40 minutes automated ml explored 65 possible models, including stacked and voting ensembles of the best performing models, and identified a voting ensemble as being the best of the best.\n\n&amp;#x200b;\n\n[run 1 completed](https://preview.redd.it/ux2zocw6xsy51.png?width=1536&amp;format=png&amp;auto=webp&amp;s=d8832f8ea14b34e0fff080e5f6bd6b87a3faad16)\n\nyou can take a look at its details of course, there\u2019s a lot of interesting stuff here including but not limited to the generated explanations and metrics - you\u2019ll notice automl calculated a lot of metrics apart from our primary one - normalized root mean squared error. these include explained variance, mean absolute error, r2 score, spearman correlation, even root mean squared error. the special thing about the primary metric is that it\u2019s used to identify the best model - had you chosen a different metric, you may have gotten a different model.\n\nonce you decide which model to use, there\u2019s also a simple way for you to  deploy your model directly in azure ml, or even download it and deploy it to the cloud of your liking.\n\n&amp;#x200b;\n\n[best model details](https://preview.redd.it/j6ducc9ixsy51.png?width=1202&amp;format=png&amp;auto=webp&amp;s=3997863fea45a1b05d6db5abf3cde6088ce48c5b)\n\nand, that\u2019s it for a quick intro to azure automated machine learning. let\u2019s review the steps here: you\u2019ve defined a problem, found meaningful data that can be used to solve said problem, configured the necessary azure resources that enabled you to automatically train a model on the data, and now you\u2019re ready to make use of this model, be it for further evaluation and improvement, or for deploying it as a rest endpoint and integrating it in your ai-powered, blockchain-enabled startup.\n\nthanks for reading so far!\n\n&amp;#x200b;\n\n[the end](https://preview.redd.it/6f89tf1kxsy51.png?width=958&amp;format=png&amp;auto=webp&amp;s=16273d0f3a691ab8a411634a7a4bb2f2081f8759)", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('nan',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/jstoh8/hey_there_reddit_im_writing_a_blog_post_on_azure/',)", "identifyer": 5751501, "year": "2020"}, {"autor": "Moritz_W", "date": 1603115920000, "content": "How to build an ML-nailbiting detector? \ud83e\udd1e\ud83d\ude80 /!/ Hi, I want to build a simple web-app that you can run in the background while working that uses your webcam to detect and alert when you move your hand to your face and bite your nails or your nail beds.\n\n**My skill level:** \n\n* Programming: I have about 2 years of experience in Web development, mostly in frontend. \n* AI and ML:\u00a0I have studied the basics for about two months and worked a bit with pandas going through 10% of [Complete Machine Learning and Data Science from Zero to Mastery](https://www.udemy.com/course/complete-machine-learning-and-data-science-zero-to-mastery/). I've also watched a few of the [Harvard CS50 AI lectures](https://cs50.harvard.edu/ai/). \n\nMy Progress: I know that I can train the model using tools like Teachable Machine, what I already did. This seems to work great. However, I would not know how to structure the frontend to communicate with the model and where to deploy such a model, etc. \n\nI would like to ask for advice and guidance on how to structure my learning journey and building this project. E.g. how could I build this and what could I learn by doing it in this way. It's not particularly important to learn about the fundamental basics. My priority is to get my hands dirty and build some cool projects.\n\nLooking forward to any suggestions!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/je244n/how_to_build_an_mlnailbiting_detector/", "origin": "Reddit", "suborigin": "learnmachinelearning", "result": true, "Selector": "beds", "selectorShort": "bed", "MarkedSent": "how to build an ml-nailbiting detector? \ud83e\udd1e\ud83d\ude80 /!/ hi, i want to build a simple web-app that you can run in the background while working that uses your webcam to detect and alert when you move your hand to your face and bite your nails or your nail -----> beds !!! .\n\n**my skill level:** \n\n* programming: i have about 2 years of experience in web development, mostly in frontend. \n* ai and ml:\u00a0i have studied the basics for about two months and worked a bit with pandas going through 10% of [complete machine learning and data science from zero to mastery](https://www.udemy.com/course/complete-machine-learning-and-data-science-zero-to-mastery/). i've also watched a few of the [harvard cs50 ai lectures](https://cs50.harvard.edu/ai/). \n\nmy progress: i know that i can train the model using tools like teachable machine, what i already did. this seems to work great. however, i would not know how to structure the frontend to communicate with the model and where to deploy such a model, etc. \n\ni would like to ask for advice and guidance on how to structure my learning journey and building this project. e.g. how could i build this and what could i learn by doing it in this way. it's not particularly important to learn about the fundamental basics. my priority is to get my hands dirty and build some cool projects.\n\nlooking forward to any suggestions!", "sortedWord": "None", "removed": "('nan',)", "score": 1, "comments": 0, "media": "('self',)", "medialink": "('https://www.reddit.com/r/learnmachinelearning/comments/je244n/how_to_build_an_mlnailbiting_detector/',)", "identifyer": 5751892, "year": "2020"}], "name": "bedlearnmachinelearning2020"}
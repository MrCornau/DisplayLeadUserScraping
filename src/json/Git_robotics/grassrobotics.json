{"interestingcomments": [{"Unnamed: 0": 1219, "autor": 199, "date": null, "content": "extrinsic_lidar_camera_calibration\n[Release Note July 2020] This work has been accepted by IEEE Access and has been uploaded to arXiv.\n[Release Note March 2020] This is the new master branch from March 2020. The current master branch supports a revised version of the arXiv paper, namely paper. The original master branch from Oct 2019 to March 2020 is now moved to v1-2019 branch, and it supports the functions associated with the first version of the Extrinsic Calibration paper that we placed on the arXiv, namely paper. Please be aware that there are functions in the older branch that have been removed from the current master branch.\nOverview\nThis is a package for extrinsic calibration between a 3D LiDAR and a camera, described in paper: Improvements to Target-Based 3D LiDAR to Camera Calibration (PDF). We evaluated our proposed methods and compared them with other approaches in a round-robin validation study, including qualitative results and quantitative results, where we use image corners as ground truth to evaluate our projection accuracy.\nAuthors: Bruce JK Huang and Jessy W. Grizzle\nMaintainer: Bruce JK Huang, brucejkh[at]gmail.com\nAffiliation: The Biped Lab, the University of Michigan\nThis package has been tested under MATLAB 2019a and Ubuntu 16.04.\n[Issues] If you encounter any issues, I would be happy to help. If you cannot find a related one in the existing issues, please open a new one. I will try my best to help!\n[Super Super Quick Start] Just to see the results, please clone this repo, download the process/optimized data into load_all_vertices folder and change the path.load_dir to load_all_vertices folder in justCalibrate.m, and then hit run!\n[Super Quick Start] If you would like to see how the LiDAR vertices are optimized, please place the test datasets in folders, change the two paths (path.bag_file_path and path.mat_file_path) in justCalibrate.m, and then hit run!\n[Developers and Calibrators] Please follow more detail instruction as below.\nAbstract\nThe rigid-body transformation between a LiDAR and monocular camera is required for sensor fusion tasks, such as SLAM. While determining such a transformation is not considered glamorous in any sense of the word, it is nonetheless crucial for many modern autonomous systems. Indeed, an error of a few degrees in rotation or a few percent in translation can lead to 20 cm reprojection errors at a distance of 5 m when overlaying a LiDAR image on a camera image. The biggest impediments to determining the transformation accurately are the relative sparsity of LiDAR point clouds and systematic errors in their distance measurements. This paper proposes (1) the use of targets of known dimension and geometry to ameliorate target pose estimation in face of the quantization and systematic errors inherent in a LiDAR image of a target, (2) a fitting method for the LiDAR to monocular camera transformation that avoids the tedious task of target edge extraction from the point could, and (3) a \u201ccross-validation study\u201d based on projection of the 3D LiDAR target vertices to the corresponding corners in the camera image. The end result is a 50% reduction in projection error and a 70% reduction in its variance.\nPerformance\nThis is a short summary from the paper; see PDF for more detail. This table compares mean and standard deviation for baseline and our approach as a function of the number of targets used in training. Units are pixel per corner.\n# Tag 2 4 6 8\nBaseline (previous state-of-the-art) mean 10.3773 4.9645 4.3789 3.9940\nProposed method - PnP mean 3.8523 1.8939 1.6817 1.7547\nProposed method - IoU mean 4.9019 2.2442 1.7631 1.7837\nBaseline (previous state-of-the-art) std 7.0887 1.9532 1.7771 2.0467\nProposed method - PnP std 2.4155 0.5609 0.5516 0.5419\nProposed method - IoU std 2.5060 0.7162 0.5070 0.4566\nApplication Videos\nThe 3D-LiDAR map shown in the videos used this package to calibrate the LiDAR to camera (to get the transformatoin between the LiDAR and camera). Briefly speaking, we project point coulds from the LiDAR back to the semantic labeled images using the obtained transformation and then associate labels with the point to build the 3D LiDAR semantic map.\nHalloween Edition: Cassie Autonomy\nAutonomous Navigation and 3D Semantic Mapping on Bipedal Robot Cassie Blue (Shorter Version)\nAutonomous Navigation and 3D Semantic Mapping on Bipedal Robot Cassie Blue (Longer Version)\nQuick View\nUsing the obtained transformation, LiDAR points are mapped onto a semantically segmented image. Each point is associated with the label of a pixel. The road is marked as white; static objects such buildings as orange; the grass as yellow-green, and dark green indicates trees.\nWhy important?\nA calibration result is not usable if it has few degrees of rotation error and a few percent of translation error. The below shows that a calibration result with little disturbance from the well-aigned image.\nPresentation and Video\nhttps://www.brucerobot.com/calibration\nCalibration Targets\nAny square targets would be fine. The dimensions are assumed known. We use fiducial tags that can be detected both from LiDARs and cameras. Physically, they are the same tags. However, if the tag is detected from LiDARs, we call it LiDARTag and on the other hand, if is is detected from cameras, it is called AprilTag. Please check out this link to download the target images. If you use these targets as you LiDAR targets, please cite\n@article{huang2019lidartag,\ntitle={LiDARTag: A Real-Time Fiducial Tag using Point Clouds},\nauthor={Huang, Jiunn-Kai and Ghaffari, Maani and Hartley, Ross and Gan, Lu and Eustice, Ryan M and Grizzle, Jessy W},\njournal={arXiv preprint arXiv:1908.10349},\nyear={2019}\n}\nnote: You can place any number of targets with different size in different datasets.\nInstallation\nWhich toolboxes are used in this package:\nMATLAB 2019a\noptimization_toolbox\nphased_array_system_toolbox\nrobotics_system_toolbox\nsignal_blocks\nDataset: download from here.\nDataset\nPlease download optimized LiDAR vertices from here and put them into ALL_LiDAR_vertices folder.\nPlease download point cloud mat files from here and put them into LiDARTag_data folder.\nPlease download bagfiles from here and put them into bagfiles folder.\nRunning\n[Super Super Quick Start] Just to see the results, please clone this repo, download the process/optimized data into load_all_vertices folder and change the path.load_dir to load_all_vertices folder in justCalibrate.m, and then hit run!\n[Super Quick Start] If you would like to see how the LiDAR vertices are optimized, please place the test datasets in folders, change the two paths (path.bag_file_path and path.mat_file_path) in justCalibrate.m, and then hit run!\n[Calibrators]\nPlease first try the [Super Super Quick Start] section to ensure you can run this code.\nUse justCalibrate.m file\nFind out your camera intrinsic matrix and write them in the justCalibrate.m file.\nGive initial guess to the LiDAR to camera transformation\nEdit the trained_ids and skip_indices (ids are from getBagData.m).\nIf you have more validation dataset (containing targets), set the validation_flag to 1 and then use put the related information to getBagData.m.\nPlace several square boards with known dimensions. When placing boards, make sure the left corner is taller than the right corner. We use fiducial tags that can be detected both from LiDARs and cameras. Physically, they are the same tags. However, if the tag is detected from LiDARs, we call it LiDARTag and on the other hand, if is is detected from cameras, it is called AprilTag. Please check out this link to download the target images. If you use these targets as you LiDAR targets, please cite\n@article{huang2019lidartag,\ntitle={LiDARTag: A Real-Time Fiducial Tag using Point Clouds},\nauthor={Huang, Jiunn-Kai and Ghaffari, Maani and Hartley, Ross and Gan, Lu and Eustice, Ryan M and Grizzle, Jessy W},\njournal={arXiv preprint arXiv:1908.10349},\nyear={2019}\n}\nUse you favorite methods to extract corners of camera targets and then write them in getBagData.m. When writing the corners, Please follow top-left-right-bottom order.\nGiven point patches of LiDAR targets, saved them into .mat files and also put them getBagData.m. Please make sure you have correctly match your lidar_target with camera_target.\nIf you have trouble extracting patches of LiDAR targets, or converting bagfiles to mat-files, I have also provided another python script to conver a bagfile to a mat-file and extract patches. Please check out bag2mat.py.\nRUN justCalibrate.m! That's it!\nnote: You can place any number of targets with different size in different datasets.\n[Developers] Please download all datasets if you like to play around.\n[Dataset structure] Put ALL information of datasets into getBagData.m. This funciton returns two data structure: TestData and BagData.\nTestData contains bagfile and pc_file, where bagfile is the name of the bagfile and pc_file is mat files of FULL scan of point cloud.\nBagData contatins:\nbagfile: name of the bagfile\nnum_tag: how many tags in this dataset\nlidar_target\npc_file: the name of the mat file of this target of point cloud\ntag_size: size of this target\ncamera_target\ncorners: corner coordinates of the camera targets\nQualitative results\nFor the method GL_1-R trained on S_1, the LiDAR point cloud has been projected into the image plane for the other data sets and marked in green. The red circles highlight various poles, door edges, desk legs, monitors, and sidewalk curbs where the quality of the alignment can be best judged. The reader may find other areas of interest. Enlarge in your browser for best viewing.\nQuantitative results\nFor the method GL_1-R, five sets of estimated LiDAR vertices for each target have been projected into the image plane and marked in green, while the target's point cloud has been marked in red. Blowing up the image allows the numbers reported in the table to be visualized. The vertices are key.\nCitations\nThe detail is described in: Jiunn-Kai Huang and J. Grizzle, \"Improvements to Target-Based 3D LiDAR to Camera Calibration\" (PDF)(arXiv)\n@article{huang2020improvements,\nauthor={J. {Huang} and J. W. {Grizzle}},\njournal={IEEE Access},\ntitle={Improvements to Target-Based 3D LiDAR to Camera Calibration},\nyear={2020},\nvolume={8},\nnumber={},\npages={134101-134110},}\nIf you use LiDARTag as you LiDAR targets, please cite\n@article{huang2019lidartag,\ntitle={LiDARTag: A Real-Time Fiducial Tag using Point Clouds},\nauthor={Huang, Jiunn-Kai and Ghaffari, Maani and Hartley, Ross and Gan, Lu and Eustice, Ryan M and Grizzle, Jessy W},\njournal={arXiv preprint arXiv:1908.10349},\nyear={2019}\n}", "link": "https://github.com/UMich-BipedLab/extrinsic_lidar_camera_calibration", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "grass", "selectorShort": "grass", "MarkedSent": "extrinsic_lidar_camera_calibration\n[release note july 2020] this work has been accepted by ieee access and has been uploaded to arxiv.\n[release note march 2020] this is the new master branch from march 2020. the current master branch supports a revised version of the arxiv paper, namely paper. the original master branch from oct 2019 to march 2020 is now moved to v1-2019 branch, and it supports the functions associated with the first version of the extrinsic calibration paper that we placed on the arxiv, namely paper. please be aware that there are functions in the older branch that have been removed from the current master branch.\noverview\nthis is a package for extrinsic calibration between a 3d lidar and a camera, described in paper: improvements to target-based 3d lidar to camera calibration (pdf). we evaluated our proposed methods and compared them with other approaches in a round-robin validation study, including qualitative results and quantitative results, where we use image corners as ground truth to evaluate our projection accuracy.\nauthors: bruce jk huang and jessy w. grizzle\nmaintainer: bruce jk huang, brucejkh[at]gmail.com\naffiliation: the biped lab, the university of michigan\nthis package has been tested under matlab 2019a and ubuntu 16.04.\n[issues] if you encounter any issues, i would be happy to help. if you cannot find a related one in the existing issues, please open a new one. i will try my best to help!\n[super super quick start] just to see the results, please clone this repo, download the process/optimized data into load_all_vertices folder and change the path.load_dir to load_all_vertices folder in justcalibrate.m, and then hit run!\n[super quick start] if you would like to see how the lidar vertices are optimized, please place the test datasets in folders, change the two paths (path.bag_file_path and path.mat_file_path) in justcalibrate.m, and then hit run!\n[developers and calibrators] please follow more detail instruction as below.\nabstract\nthe rigid-body transformation between a lidar and monocular camera is required for sensor fusion tasks, such as slam. while determining such a transformation is not considered glamorous in any sense of the word, it is nonetheless crucial for many modern autonomous systems. indeed, an error of a few degrees in rotation or a few percent in translation can lead to 20 cm reprojection errors at a distance of 5 m when overlaying a lidar image on a camera image. the biggest impediments to determining the transformation accurately are the relative sparsity of lidar point clouds and systematic errors in their distance measurements. this paper proposes (1) the use of targets of known dimension and geometry to ameliorate target pose estimation in face of the quantization and systematic errors inherent in a lidar image of a target, (2) a fitting method for the lidar to monocular camera transformation that avoids the tedious task of target edge extraction from the point could, and (3) a \u201ccross-validation study\u201d based on projection of the 3d lidar target vertices to the corresponding corners in the camera image. the end result is a 50% reduction in projection error and a 70% reduction in its variance.\nperformance\nthis is a short summary from the paper; see pdf for more detail. this table compares mean and standard deviation for baseline and our approach as a function of the number of targets used in training. units are pixel per corner.\n# tag 2 4 6 8\nbaseline (previous state-of-the-art) mean 10.3773 4.9645 4.3789 3.9940\nproposed method - pnp mean 3.8523 1.8939 1.6817 1.7547\nproposed method - iou mean 4.9019 2.2442 1.7631 1.7837\nbaseline (previous state-of-the-art) std 7.0887 1.9532 1.7771 2.0467\nproposed method - pnp std 2.4155 0.5609 0.5516 0.5419\nproposed method - iou std 2.5060 0.7162 0.5070 0.4566\napplication videos\nthe 3d-lidar map shown in the videos used this package to calibrate the lidar to camera (to get the transformatoin between the lidar and camera). briefly speaking, we project point coulds from the lidar back to the semantic labeled images using the obtained transformation and then associate labels with the point to build the 3d lidar semantic map.\nhalloween edition: cassie autonomy\nautonomous navigation and 3d semantic mapping on bipedal robot cassie blue (shorter version)\nautonomous navigation and 3d semantic mapping on bipedal robot cassie blue (longer version)\nquick view\nusing the obtained transformation, lidar points are mapped onto a semantically segmented image. each point is associated with the label of a pixel. the road is marked as white; static objects such buildings as orange; the -----> grass !!!  as yellow-green, and dark green indicates trees.\nwhy important?\na calibration result is not usable if it has few degrees of rotation error and a few percent of translation error. the below shows that a calibration result with little disturbance from the well-aigned image.\npresentation and video\nhttps://www.brucerobot.com/calibration\ncalibration targets\nany square targets would be fine. the dimensions are assumed known. we use fiducial tags that can be detected both from lidars and cameras. physically, they are the same tags. however, if the tag is detected from lidars, we call it lidartag and on the other hand, if is is detected from cameras, it is called apriltag. please check out this link to download the target images. if you use these targets as you lidar targets, please cite\n@article{huang2019lidartag,\ntitle={lidartag: a real-time fiducial tag using point clouds},\nauthor={huang, jiunn-kai and ghaffari, maani and hartley, ross and gan, lu and eustice, ryan m and grizzle, jessy w},\njournal={arxiv preprint arxiv:1908.10349},\nyear={2019}\n}\nnote: you can place any number of targets with different size in different datasets.\ninstallation\nwhich toolboxes are used in this package:\nmatlab 2019a\noptimization_toolbox\nphased_array_system_toolbox\nrobotics_system_toolbox\nsignal_blocks\ndataset: download from here.\ndataset\nplease download optimized lidar vertices from here and put them into all_lidar_vertices folder.\nplease download point cloud mat files from here and put them into lidartag_data folder.\nplease download bagfiles from here and put them into bagfiles folder.\nrunning\n[super super quick start] just to see the results, please clone this repo, download the process/optimized data into load_all_vertices folder and change the path.load_dir to load_all_vertices folder in justcalibrate.m, and then hit run!\n[super quick start] if you would like to see how the lidar vertices are optimized, please place the test datasets in folders, change the two paths (path.bag_file_path and path.mat_file_path) in justcalibrate.m, and then hit run!\n[calibrators]\nplease first try the [super super quick start] section to ensure you can run this code.\nuse justcalibrate.m file\nfind out your camera intrinsic matrix and write them in the justcalibrate.m file.\ngive initial guess to the lidar to camera transformation\nedit the trained_ids and skip_indices (ids are from getbagdata.m).\nif you have more validation dataset (containing targets), set the validation_flag to 1 and then use put the related information to getbagdata.m.\nplace several square boards with known dimensions. when placing boards, make sure the left corner is taller than the right corner. we use fiducial tags that can be detected both from lidars and cameras. physically, they are the same tags. however, if the tag is detected from lidars, we call it lidartag and on the other hand, if is is detected from cameras, it is called apriltag. please check out this link to download the target images. if you use these targets as you lidar targets, please cite\n@article{huang2019lidartag,\ntitle={lidartag: a real-time fiducial tag using point clouds},\nauthor={huang, jiunn-kai and ghaffari, maani and hartley, ross and gan, lu and eustice, ryan m and grizzle, jessy w},\njournal={arxiv preprint arxiv:1908.10349},\nyear={2019}\n}\nuse you favorite methods to extract corners of camera targets and then write them in getbagdata.m. when writing the corners, please follow top-left-right-bottom order.\ngiven point patches of lidar targets, saved them into .mat files and also put them getbagdata.m. please make sure you have correctly match your lidar_target with camera_target.\nif you have trouble extracting patches of lidar targets, or converting bagfiles to mat-files, i have also provided another python script to conver a bagfile to a mat-file and extract patches. please check out bag2mat.py.\nrun justcalibrate.m! that's it!\nnote: you can place any number of targets with different size in different datasets.\n[developers] please download all datasets if you like to play around.\n[dataset structure] put all information of datasets into getbagdata.m. this funciton returns two data structure: testdata and bagdata.\ntestdata contains bagfile and pc_file, where bagfile is the name of the bagfile and pc_file is mat files of full scan of point cloud.\nbagdata contatins:\nbagfile: name of the bagfile\nnum_tag: how many tags in this dataset\nlidar_target\npc_file: the name of the mat file of this target of point cloud\ntag_size: size of this target\ncamera_target\ncorners: corner coordinates of the camera targets\nqualitative results\nfor the method gl_1-r trained on s_1, the lidar point cloud has been projected into the image plane for the other data sets and marked in green. the red circles highlight various poles, door edges, desk legs, monitors, and sidewalk curbs where the quality of the alignment can be best judged. the reader may find other areas of interest. enlarge in your browser for best viewing.\nquantitative results\nfor the method gl_1-r, five sets of estimated lidar vertices for each target have been projected into the image plane and marked in green, while the target's point cloud has been marked in red. blowing up the image allows the numbers reported in the table to be visualized. the vertices are key.\ncitations\nthe detail is described in: jiunn-kai huang and j. grizzle, \"improvements to target-based 3d lidar to camera calibration\" (pdf)(arxiv)\n@article{huang2020improvements,\nauthor={j. {huang} and j. w. {grizzle}},\njournal={ieee access},\ntitle={improvements to target-based 3d lidar to camera calibration},\nyear={2020},\nvolume={8},\nnumber={},\npages={134101-134110},}\nif you use lidartag as you lidar targets, please cite\n@article{huang2019lidartag,\ntitle={lidartag: a real-time fiducial tag using point clouds},\nauthor={huang, jiunn-kai and ghaffari, maani and hartley, ross and gan, lu and eustice, ryan m and grizzle, jessy w},\njournal={arxiv preprint arxiv:1908.10349},\nyear={2019}\n}", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000199, "year": null}], "name": "grassrobotics"}
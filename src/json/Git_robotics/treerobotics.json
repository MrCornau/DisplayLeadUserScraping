{"interestingcomments": [{"Unnamed: 0": 1021, "autor": 1, "date": null, "content": "PythonRobotics\nPython codes for robotics algorithm.\nTable of Contents\nWhat is this?\nRequirements\nDocumentation\nHow to use\nLocalization\nExtended Kalman Filter localization\nParticle filter localization\nHistogram filter localization\nMapping\nGaussian grid map\nRay casting grid map\nLidar to grid map\nk-means object clustering\nRectangle fitting\nSLAM\nIterative Closest Point (ICP) Matching\nFastSLAM 1.0\nPath Planning\nDynamic Window Approach\nGrid based search\nDijkstra algorithm\nA* algorithm\nD* algorithm\nD* Lite algorithm\nPotential Field algorithm\nGrid based coverage path planning\nState Lattice Planning\nBiased polar sampling\nLane sampling\nProbabilistic Road-Map (PRM) planning\nRapidly-Exploring Random Trees (RRT)\nRRT*\nRRT* with reeds-shepp path\nLQR-RRT*\nQuintic polynomials planning\nReeds Shepp planning\nLQR based path planning\nOptimal Trajectory in a Frenet Frame\nPath Tracking\nmove to a pose control\nStanley control\nRear wheel feedback control\nLinear\u2013quadratic regulator (LQR) speed and steering control\nModel predictive speed and steering control\nNonlinear Model predictive control with C-GMRES\nArm Navigation\nN joint arm to point control\nArm navigation with obstacle avoidance\nAerial Navigation\ndrone 3d trajectory following\nrocket powered landing\nBipedal\nbipedal planner with inverted pendulum\nLicense\nUse-case\nContribution\nCiting\nSupport\nSponsors\nJetBrains\nAuthors\nWhat is this?\nThis is a Python code collection of robotics algorithms.\nFeatures:\nEasy to read for understanding each algorithm's basic idea.\nWidely used and practical algorithms are selected.\nMinimum dependency.\nSee this paper for more details:\n[1808.10703] PythonRobotics: a Python code collection of robotics algorithms (BibTeX)\nRequirements\nFor running each sample code:\nPython 3.9.x\nNumPy\nSciPy\nMatplotlib\npandas\ncvxpy\nFor development:\npytest (for unit tests)\npytest-xdist (for parallel unit tests)\nmypy (for type check)\nsphinx (for document generation)\npycodestyle (for code style check)\nDocumentation\nThis README only shows some examples of this project.\nIf you are interested in other examples or mathematical backgrounds of each algorithm,\nYou can check the full documentation online: https://pythonrobotics.readthedocs.io/\nAll animation gifs are stored here: AtsushiSakai/PythonRoboticsGifs: Animation gifs of PythonRobotics\nHow to use\nClone this repo.\ngit clone https://github.com/AtsushiSakai/PythonRobotics.git\nInstall the required libraries.\nusing conda :\nconda env create -f environment.yml\nusing pip :\npip install -r requirements.txt\nExecute python script in each directory.\nAdd star to this repo if you like it \ud83d\ude03.\nLocalization\nExtended Kalman Filter localization\nDocumentation: Notebook\nParticle filter localization\nThis is a sensor fusion localization with Particle Filter(PF).\nThe blue line is true trajectory, the black line is dead reckoning trajectory,\nand the red line is an estimated trajectory with PF.\nIt is assumed that the robot can measure a distance from landmarks (RFID).\nThese measurements are used for PF localization.\nRef:\nPROBABILISTIC ROBOTICS\nHistogram filter localization\nThis is a 2D localization example with Histogram filter.\nThe red cross is true position, black points are RFID positions.\nThe blue grid shows a position probability of histogram filter.\nIn this simulation, x,y are unknown, yaw is known.\nThe filter integrates speed input and range observations from RFID for localization.\nInitial position is not needed.\nRef:\nPROBABILISTIC ROBOTICS\nMapping\nGaussian grid map\nThis is a 2D Gaussian grid mapping example.\nRay casting grid map\nThis is a 2D ray casting grid mapping example.\nLidar to grid map\nThis example shows how to convert a 2D range measurement to a grid map.\nk-means object clustering\nThis is a 2D object clustering with k-means algorithm.\nRectangle fitting\nThis is a 2D rectangle fitting for vehicle detection.\nSLAM\nSimultaneous Localization and Mapping(SLAM) examples\nIterative Closest Point (ICP) Matching\nThis is a 2D ICP matching example with singular value decomposition.\nIt can calculate a rotation matrix, and a translation vector between points and points.\nRef:\nIntroduction to Mobile Robotics: Iterative Closest Point Algorithm\nFastSLAM 1.0\nThis is a feature based SLAM example using FastSLAM 1.0.\nThe blue line is ground truth, the black line is dead reckoning, the red line is the estimated trajectory with FastSLAM.\nThe red points are particles of FastSLAM.\nBlack points are landmarks, blue crosses are estimated landmark positions by FastSLAM.\nRef:\nPROBABILISTIC ROBOTICS\nSLAM simulations by Tim Bailey\nPath Planning\nDynamic Window Approach\nThis is a 2D navigation sample code with Dynamic Window Approach.\nThe Dynamic Window Approach to Collision Avoidance\nGrid based search\nDijkstra algorithm\nThis is a 2D grid based the shortest path planning with Dijkstra's algorithm.\nIn the animation, cyan points are searched nodes.\nA* algorithm\nThis is a 2D grid based the shortest path planning with A star algorithm.\nIn the animation, cyan points are searched nodes.\nIts heuristic is 2D Euclid distance.\nD* algorithm\nThis is a 2D grid based the shortest path planning with D star algorithm.\nThe animation shows a robot finding its path avoiding an obstacle using the D* search algorithm.\nRef:\nD* Algorithm Wikipedia\nD* Lite algorithm\nThis algorithm finds the shortest path between two points while rerouting when obstacles are discovered. It has been implemented here for a 2D grid.\nThe animation shows a robot finding its path and rerouting to avoid obstacles as they are discovered using the D* Lite search algorithm.\nRefs:\nD* Lite\nImproved Fast Replanning for Robot Navigation in Unknown Terrain\nPotential Field algorithm\nThis is a 2D grid based path planning with Potential Field algorithm.\nIn the animation, the blue heat map shows potential value on each grid.\nRef:\nRobotic Motion Planning:Potential Functions\nGrid based coverage path planning\nThis is a 2D grid based coverage path planning simulation.\nState Lattice Planning\nThis script is a path planning code with state lattice planning.\nThis code uses the model predictive trajectory generator to solve boundary problem.\nRef:\nOptimal rough terrain trajectory generation for wheeled mobile robots\nState Space Sampling of Feasible Motions for High-Performance Mobile Robot Navigation in Complex Environments\nBiased polar sampling\nLane sampling\nProbabilistic Road-Map (PRM) planning\nThis PRM planner uses Dijkstra method for graph search.\nIn the animation, blue points are sampled points,\nCyan crosses means searched points with Dijkstra method,\nThe red line is the final path of PRM.\nRef:\nProbabilistic roadmap - Wikipedia\nRapidly-Exploring Random Trees (RRT)\nRRT*\nThis is a path planning code with RRT*\nBlack circles are obstacles, green line is a searched tree, red crosses are start and goal positions.\nRef:\nIncremental Sampling-based Algorithms for Optimal Motion Planning\nSampling-based Algorithms for Optimal Motion Planning\nRRT* with reeds-shepp path\n)\nPath planning for a car robot with RRT* and reeds shepp path planner.\nLQR-RRT*\nThis is a path planning simulation with LQR-RRT*.\nA double integrator motion model is used for LQR local planner.\nRef:\nLQR-RRT*: Optimal Sampling-Based Motion Planning with Automatically Derived Extension Heuristics\nMahanFathi/LQR-RRTstar: LQR-RRT* method is used for random motion planning of a simple pendulum in its phase plot\nQuintic polynomials planning\nMotion planning with quintic polynomials.\nIt can calculate a 2D path, velocity, and acceleration profile based on quintic polynomials.\nRef:\nLocal Path Planning And Motion Control For Agv In Positioning\nReeds Shepp planning\nA sample code with Reeds Shepp path planning.\nRef:\n15.3.2 Reeds-Shepp Curves\noptimal paths for a car that goes both forwards and backwards\nghliu/pyReedsShepp: Implementation of Reeds Shepp curve.\nLQR based path planning\nA sample code using LQR based path planning for double integrator model.\nOptimal Trajectory in a Frenet Frame\nThis is optimal trajectory generation in a Frenet Frame.\nThe cyan line is the target course and black crosses are obstacles.\nThe red line is the predicted path.\nRef:\nOptimal Trajectory Generation for Dynamic Street Scenarios in a Frenet Frame\nOptimal trajectory generation for dynamic street scenarios in a Frenet Frame\nPath Tracking\nmove to a pose control\nThis is a simulation of moving to a pose control\nRef:\nP. I. Corke, \"Robotics, Vision and Control\" | SpringerLink p102\nStanley control\nPath tracking simulation with Stanley steering control and PID speed control.\nRef:\nStanley: The robot that won the DARPA grand challenge\nAutomatic Steering Methods for Autonomous Automobile Path Tracking\nRear wheel feedback control\nPath tracking simulation with rear wheel feedback steering control and PID speed control.\nRef:\nA Survey of Motion Planning and Control Techniques for Self-driving Urban Vehicles\nLinear\u2013quadratic regulator (LQR) speed and steering control\nPath tracking simulation with LQR speed and steering control.\nRef:\nTowards fully autonomous driving: Systems and algorithms - IEEE Conference Publication\nModel predictive speed and steering control\nPath tracking simulation with iterative linear model predictive speed and steering control.\nRef:\nnotebook\nReal-time Model Predictive Control (MPC), ACADO, Python | Work-is-Playing\nNonlinear Model predictive control with C-GMRES\nA motion planning and path tracking simulation with NMPC of C-GMRES\nRef:\nnotebook\nArm Navigation\nN joint arm to point control\nN joint arm to a point control simulation.\nThis is an interactive simulation.\nYou can set the goal position of the end effector with left-click on the plotting area.\nIn this simulation N = 10, however, you can change it.\nArm navigation with obstacle avoidance\nArm navigation with obstacle avoidance simulation.\nAerial Navigation\ndrone 3d trajectory following\nThis is a 3d trajectory following simulation for a quadrotor.\nrocket powered landing\nThis is a 3d trajectory generation simulation for a rocket powered landing.\nRef:\nnotebook\nBipedal\nbipedal planner with inverted pendulum\nThis is a bipedal planner for modifying footsteps for an inverted pendulum.\nYou can set the footsteps, and the planner will modify those automatically.\nLicense\nMIT\nUse-case\nIf this project helps your robotics project, please let me know with creating an issue.\nYour robot's video, which is using PythonRobotics, is very welcome!!\nThis is a list of user's comment and references:users_comments\nContribution\nAny contribution is welcome!!\nPlease check this document:How to contribute\nCiting\nIf you use this project's code for your academic work, we encourage you to cite our papers\nIf you use this project's code in industry, we'd love to hear from you as well; feel free to reach out to the developers directly.\nSupporting this project\nIf you or your company would like to support this project, please consider:\nSponsor @AtsushiSakai on GitHub Sponsors\nBecome a backer or sponsor on Patreon\nOne-time donation via PayPal\nIf you would like to support us in some other way, please contact with creating an issue.\nSponsors\nJetBrains\nThey are providing a free license of their IDEs for this OSS development.\nAuthors\nContributors to AtsushiSakai/PythonRobotics", "link": "https://github.com/AtsushiSakai/PythonRobotics", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "pythonrobotics\npython codes for robotics algorithm.\ntable of contents\nwhat is this?\nrequirements\ndocumentation\nhow to use\nlocalization\nextended kalman filter localization\nparticle filter localization\nhistogram filter localization\nmapping\ngaussian grid map\nray casting grid map\nlidar to grid map\nk-means object clustering\nrectangle fitting\nslam\niterative closest point (icp) matching\nfastslam 1.0\npath planning\ndynamic window approach\ngrid based search\ndijkstra algorithm\na* algorithm\nd* algorithm\nd* lite algorithm\npotential field algorithm\ngrid based coverage path planning\nstate lattice planning\nbiased polar sampling\nlane sampling\nprobabilistic road-map (prm) planning\nrapidly-exploring random trees (rrt)\nrrt*\nrrt* with reeds-shepp path\nlqr-rrt*\nquintic polynomials planning\nreeds shepp planning\nlqr based path planning\noptimal trajectory in a frenet frame\npath tracking\nmove to a pose control\nstanley control\nrear wheel feedback control\nlinear\u2013quadratic regulator (lqr) speed and steering control\nmodel predictive speed and steering control\nnonlinear model predictive control with c-gmres\narm navigation\nn joint arm to point control\narm navigation with obstacle avoidance\naerial navigation\ndrone 3d trajectory following\nrocket powered landing\nbipedal\nbipedal planner with inverted pendulum\nlicense\nuse-case\ncontribution\nciting\nsupport\nsponsors\njetbrains\nauthors\nwhat is this?\nthis is a python code collection of robotics algorithms.\nfeatures:\neasy to read for understanding each algorithm's basic idea.\nwidely used and practical algorithms are selected.\nminimum dependency.\nsee this paper for more details:\n[1808.10703] pythonrobotics: a python code collection of robotics algorithms (bibtex)\nrequirements\nfor running each sample code:\npython 3.9.x\nnumpy\nscipy\nmatplotlib\npandas\ncvxpy\nfor development:\npytest (for unit tests)\npytest-xdist (for parallel unit tests)\nmypy (for type check)\nsphinx (for document generation)\npycodestyle (for code style check)\ndocumentation\nthis readme only shows some examples of this project.\nif you are interested in other examples or mathematical backgrounds of each algorithm,\nyou can check the full documentation online: https://pythonrobotics.readthedocs.io/\nall animation gifs are stored here: atsushisakai/pythonroboticsgifs: animation gifs of pythonrobotics\nhow to use\nclone this repo.\ngit clone https://github.com/atsushisakai/pythonrobotics.git\ninstall the required libraries.\nusing conda :\nconda env create -f environment.yml\nusing pip :\npip install -r requirements.txt\nexecute python script in each directory.\nadd star to this repo if you like it \ud83d\ude03.\nlocalization\nextended kalman filter localization\ndocumentation: notebook\nparticle filter localization\nthis is a sensor fusion localization with particle filter(pf).\nthe blue line is true trajectory, the black line is dead reckoning trajectory,\nand the red line is an estimated trajectory with pf.\nit is assumed that the robot can measure a distance from landmarks (rfid).\nthese measurements are used for pf localization.\nref:\nprobabilistic robotics\nhistogram filter localization\nthis is a 2d localization example with histogram filter.\nthe red cross is true position, black points are rfid positions.\nthe blue grid shows a position probability of histogram filter.\nin this simulation, x,y are unknown, yaw is known.\nthe filter integrates speed input and range observations from rfid for localization.\ninitial position is not needed.\nref:\nprobabilistic robotics\nmapping\ngaussian grid map\nthis is a 2d gaussian grid mapping example.\nray casting grid map\nthis is a 2d ray casting grid mapping example.\nlidar to grid map\nthis example shows how to convert a 2d range measurement to a grid map.\nk-means object clustering\nthis is a 2d object clustering with k-means algorithm.\nrectangle fitting\nthis is a 2d rectangle fitting for vehicle detection.\nslam\nsimultaneous localization and mapping(slam) examples\niterative closest point (icp) matching\nthis is a 2d icp matching example with singular value decomposition.\nit can calculate a rotation matrix, and a translation vector between points and points.\nref:\nintroduction to mobile robotics: iterative closest point algorithm\nfastslam 1.0\nthis is a feature based slam example using fastslam 1.0.\nthe blue line is ground truth, the black line is dead reckoning, the red line is the estimated trajectory with fastslam.\nthe red points are particles of fastslam.\nblack points are landmarks, blue crosses are estimated landmark positions by fastslam.\nref:\nprobabilistic robotics\nslam simulations by tim bailey\npath planning\ndynamic window approach\nthis is a 2d navigation sample code with dynamic window approach.\nthe dynamic window approach to collision avoidance\ngrid based search\ndijkstra algorithm\nthis is a 2d grid based the shortest path planning with dijkstra's algorithm.\nin the animation, cyan points are searched nodes.\na* algorithm\nthis is a 2d grid based the shortest path planning with a star algorithm.\nin the animation, cyan points are searched nodes.\nits heuristic is 2d euclid distance.\nd* algorithm\nthis is a 2d grid based the shortest path planning with d star algorithm.\nthe animation shows a robot finding its path avoiding an obstacle using the d* search algorithm.\nref:\nd* algorithm wikipedia\nd* lite algorithm\nthis algorithm finds the shortest path between two points while rerouting when obstacles are discovered. it has been implemented here for a 2d grid.\nthe animation shows a robot finding its path and rerouting to avoid obstacles as they are discovered using the d* lite search algorithm.\nrefs:\nd* lite\nimproved fast replanning for robot navigation in unknown terrain\npotential field algorithm\nthis is a 2d grid based path planning with potential field algorithm.\nin the animation, the blue heat map shows potential value on each grid.\nref:\nrobotic motion planning:potential functions\ngrid based coverage path planning\nthis is a 2d grid based coverage path planning simulation.\nstate lattice planning\nthis script is a path planning code with state lattice planning.\nthis code uses the model predictive trajectory generator to solve boundary problem.\nref:\noptimal rough terrain trajectory generation for wheeled mobile robots\nstate space sampling of feasible motions for high-performance mobile robot navigation in complex environments\nbiased polar sampling\nlane sampling\nprobabilistic road-map (prm) planning\nthis prm planner uses dijkstra method for graph search.\nin the animation, blue points are sampled points,\ncyan crosses means searched points with dijkstra method,\nthe red line is the final path of prm.\nref:\nprobabilistic roadmap - wikipedia\nrapidly-exploring random trees (rrt)\nrrt*\nthis is a path planning code with rrt*\nblack circles are obstacles, green line is a searched -----> tree !!! , red crosses are start and goal positions.\nref:\nincremental sampling-based algorithms for optimal motion planning\nsampling-based algorithms for optimal motion planning\nrrt* with reeds-shepp path\n)\npath planning for a car robot with rrt* and reeds shepp path planner.\nlqr-rrt*\nthis is a path planning simulation with lqr-rrt*.\na double integrator motion model is used for lqr local planner.\nref:\nlqr-rrt*: optimal sampling-based motion planning with automatically derived extension heuristics\nmahanfathi/lqr-rrtstar: lqr-rrt* method is used for random motion planning of a simple pendulum in its phase plot\nquintic polynomials planning\nmotion planning with quintic polynomials.\nit can calculate a 2d path, velocity, and acceleration profile based on quintic polynomials.\nref:\nlocal path planning and motion control for agv in positioning\nreeds shepp planning\na sample code with reeds shepp path planning.\nref:\n15.3.2 reeds-shepp curves\noptimal paths for a car that goes both forwards and backwards\nghliu/pyreedsshepp: implementation of reeds shepp curve.\nlqr based path planning\na sample code using lqr based path planning for double integrator model.\noptimal trajectory in a frenet frame\nthis is optimal trajectory generation in a frenet frame.\nthe cyan line is the target course and black crosses are obstacles.\nthe red line is the predicted path.\nref:\noptimal trajectory generation for dynamic street scenarios in a frenet frame\noptimal trajectory generation for dynamic street scenarios in a frenet frame\npath tracking\nmove to a pose control\nthis is a simulation of moving to a pose control\nref:\np. i. corke, \"robotics, vision and control\" | springerlink p102\nstanley control\npath tracking simulation with stanley steering control and pid speed control.\nref:\nstanley: the robot that won the darpa grand challenge\nautomatic steering methods for autonomous automobile path tracking\nrear wheel feedback control\npath tracking simulation with rear wheel feedback steering control and pid speed control.\nref:\na survey of motion planning and control techniques for self-driving urban vehicles\nlinear\u2013quadratic regulator (lqr) speed and steering control\npath tracking simulation with lqr speed and steering control.\nref:\ntowards fully autonomous driving: systems and algorithms - ieee conference publication\nmodel predictive speed and steering control\npath tracking simulation with iterative linear model predictive speed and steering control.\nref:\nnotebook\nreal-time model predictive control (mpc), acado, python | work-is-playing\nnonlinear model predictive control with c-gmres\na motion planning and path tracking simulation with nmpc of c-gmres\nref:\nnotebook\narm navigation\nn joint arm to point control\nn joint arm to a point control simulation.\nthis is an interactive simulation.\nyou can set the goal position of the end effector with left-click on the plotting area.\nin this simulation n = 10, however, you can change it.\narm navigation with obstacle avoidance\narm navigation with obstacle avoidance simulation.\naerial navigation\ndrone 3d trajectory following\nthis is a 3d trajectory following simulation for a quadrotor.\nrocket powered landing\nthis is a 3d trajectory generation simulation for a rocket powered landing.\nref:\nnotebook\nbipedal\nbipedal planner with inverted pendulum\nthis is a bipedal planner for modifying footsteps for an inverted pendulum.\nyou can set the footsteps, and the planner will modify those automatically.\nlicense\nmit\nuse-case\nif this project helps your robotics project, please let me know with creating an issue.\nyour robot's video, which is using pythonrobotics, is very welcome!!\nthis is a list of user's comment and references:users_comments\ncontribution\nany contribution is welcome!!\nplease check this document:how to contribute\nciting\nif you use this project's code for your academic work, we encourage you to cite our papers\nif you use this project's code in industry, we'd love to hear from you as well; feel free to reach out to the developers directly.\nsupporting this project\nif you or your company would like to support this project, please consider:\nsponsor @atsushisakai on github sponsors\nbecome a backer or sponsor on patreon\none-time donation via paypal\nif you would like to support us in some other way, please contact with creating an issue.\nsponsors\njetbrains\nthey are providing a free license of their ides for this oss development.\nauthors\ncontributors to atsushisakai/pythonrobotics", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000001, "year": null}, {"Unnamed: 0": 1035, "autor": 15, "date": null, "content": "Reading List for Topics in Multimodal Machine Learning\nBy Paul Liang (pliang@cs.cmu.edu), Machine Learning Department and Language Technologies Institute, CMU, with help from members of the MultiComp Lab at LTI, CMU. If there are any areas, papers, and datasets I missed, please let me know!\nRecent Content\nSocial Intelligence in Humans and Robots @ ICRA 2021\nLANTERN 2021: The Third Workshop Beyond Vision and LANguage: inTEgrating Real-world kNowledge @ EACL 2021\nMultimodal workshops @ CVPR 2021: Multimodal Learning and Applications, Sight and Sound, Visual Question Answering, Embodied AI, Language for 3D Scenes.\nMultimodal workshops @ NAACL 2021: MAI-Workshop, ALVR, ViGIL.\nICLR 2021 workshop on Embodied Multimodal Learning.\nMicrosoft Research's work on VinVL: blog, paper.\nOpenAI's work on DALL\u00b7E and CLIP: blog, paper, and code.\nFollow our course 11-777 Multimodal Machine Learning, Fall 2020 @ CMU.\nNeurIPS 2020 workshop on Wordplay: When Language Meets Games.\nACL 2020 workshops on Multimodal Language (proceedings) and Advances in Language and Vision Research.\nMultimodal workshops @ ECCV 2020: EVAL, CAMP, and MVA.\nTable of Contents\nSurvey Papers\nCore Areas\nMultimodal Representations\nMultimodal Fusion\nMultimodal Alignment\nMultimodal Pretraining\nMultimodal Translation\nCrossmodal Retrieval\nMissing or Imperfect Modalities\nAnalysis of Multimodal Models\nKnowledge Graphs and Knowledge Bases\nIntepretable Learning\nGenerative Learning\nSemi-supervised Learning\nSelf-supervised Learning\nLanguage Models\nAdversarial Attacks\nFew-Shot Learning\nBias and Fairness\nHuman in the Loop Learning\nArchitectures\nMultimodal Transformers\nMultimodal Memory\nApplications and Datasets\nLanguage and Visual QA\nLanguage Grounding in Vision\nLanguage Grouding in Navigation\nMultimodal Machine Translation\nMulti-agent Communication\nCommonsense Reasoning\nMultimodal Reinforcement Learning\nMultimodal Dialog\nLanguage and Audio\nAudio and Visual\nMedia Description\nVideo Generation from Text\nAffect Recognition and Multimodal Language\nHealthcare\nRobotics\nAutonomous Driving\nFinance\nHuman AI Interaction\nWorkshops\nTutorials\nCourses\nResearch Papers\nSurvey Papers\nTrends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods, JAIR 2021\nExperience Grounds Language, EMNLP 2020\nA Survey of Reinforcement Learning Informed by Natural Language, IJCAI 2019\nMultimodal Machine Learning: A Survey and Taxonomy, TPAMI 2019\nMultimodal Intelligence: Representation Learning, Information Fusion, and Applications, arXiv 2019\nDeep Multimodal Representation Learning: A Survey, arXiv 2019\nGuest Editorial: Image and Language Understanding, IJCV 2017\nRepresentation Learning: A Review and New Perspectives, TPAMI 2013\nA Survey of Socially Interactive Robots, 2003\nCore Areas\nMultimodal Representations\nMultiBench: Multiscale Benchmarks for Multimodal Representation Learning, NeurIPS 2021 [code]\nPerceiver: General Perception with Iterative Attention, ICML 2021 [code]\nLearning Transferable Visual Models From Natural Language Supervision, arXiv 2020 [blog] [code]\n12-in-1: Multi-Task Vision and Language Representation Learning, CVPR 2020 [code]\nWatching the World Go By: Representation Learning from Unlabeled Videos, arXiv 2020\nLearning Video Representations using Contrastive Bidirectional Transformer, arXiv 2019\nVisual Concept-Metaconcept Learning, NeurIPS 2019 [code]\nOmniNet: A Unified Architecture for Multi-modal Multi-task Learning, arXiv 2019 [code]\nLearning Representations by Maximizing Mutual Information Across Views, arXiv 2019 [code]\nViCo: Word Embeddings from Visual Co-occurrences, ICCV 2019 [code]\nUnified Visual-Semantic Embeddings: Bridging Vision and Language With Structured Meaning Representations, CVPR 2019\nMulti-Task Learning of Hierarchical Vision-Language Representation, CVPR 2019\nLearning Factorized Multimodal Representations, ICLR 2019 [code]\nA Probabilistic Framework for Multi-view Feature Learning with Many-to-many Associations via Neural Networks, ICML 2018\nDo Neural Network Cross-Modal Mappings Really Bridge Modalities?, ACL 2018\nLearning Robust Visual-Semantic Embeddings, ICCV 2017\nDeep Multimodal Representation Learning from Temporal Data, CVPR 2017\nIs an Image Worth More than a Thousand Words? On the Fine-Grain Semantic Differences between Visual and Linguistic Representations, COLING 2016\nCombining Language and Vision with a Multimodal Skip-gram Model, NAACL 2015\nDeep Fragment Embeddings for Bidirectional Image Sentence Mapping, NIPS 2014\nMultimodal Learning with Deep Boltzmann Machines, JMLR 2014\nLearning Grounded Meaning Representations with Autoencoders, ACL 2014\nDeViSE: A Deep Visual-Semantic Embedding Model, NeurIPS 2013\nMultimodal Deep Learning, ICML 2011\nMultimodal Fusion\nAttention Bottlenecks for Multimodal Fusion, NeurIPS 2021\nTrusted Multi-View Classification, ICLR 2021 [code]\nDeep-HOSeq: Deep Higher-Order Sequence Fusion for Multimodal Sentiment Analysis, ICDM 2020\nRemoving Bias in Multi-modal Classifiers: Regularization by Maximizing Functional Entropies, NeurIPS 2020 [code]\nDeep Multimodal Fusion by Channel Exchanging, NeurIPS 2020 [code]\nWhat Makes Training Multi-Modal Classification Networks Hard?, CVPR 2020\nDynamic Fusion for Multimodal Data, arXiv 2019\nDeepCU: Integrating Both Common and Unique Latent Information for Multimodal Sentiment Analysis, IJCAI 2019 [code]\nDeep Multimodal Multilinear Fusion with High-order Polynomial Pooling, NeurIPS 2019\nXFlow: Cross-modal Deep Neural Networks for Audiovisual Classification, IEEE TNNLS 2019 [code]\nMFAS: Multimodal Fusion Architecture Search, CVPR 2019\nThe Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision, ICLR 2019 [code]\nUnifying and merging well-trained deep neural networks for inference stage, IJCAI 2018 [code]\nEfficient Low-rank Multimodal Fusion with Modality-Specific Factors, ACL 2018 [code]\nMemory Fusion Network for Multi-view Sequential Learning, AAAI 2018 [code]\nTensor Fusion Network for Multimodal Sentiment Analysis, EMNLP 2017 [code]\nJointly Modeling Deep Video and Compositional Text to Bridge Vision and Language in a Unified Framework, AAAI 2015\nMultimodal Alignment\nCoMIR: Contrastive Multimodal Image Representation for Registration, NeurIPS 2020 [code]\nMultimodal Transformer for Unaligned Multimodal Language Sequences, ACL 2019 [code]\nTemporal Cycle-Consistency Learning, CVPR 2019 [code]\nSee, Hear, and Read: Deep Aligned Representations, arXiv 2017\nOn Deep Multi-View Representation Learning, ICML 2015\nUnsupervised Alignment of Natural Language Instructions with Video Segments, AAAI 2014\nMultimodal Alignment of Videos, MM 2014\nDeep Canonical Correlation Analysis, ICML 2013 [code]\nMultimodal Pretraining\nLess is More: ClipBERT for Video-and-Language Learning via Sparse Sampling, CVPR 2021 [code]\nTransformer is All You Need: Multimodal Multitask Learning with a Unified Transformer, arXiv 2021\nLarge-Scale Adversarial Training for Vision-and-Language Representation Learning, NeurIPS 2020 [code]\nVokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision, EMNLP 2020 [code]\nIntegrating Multimodal Information in Large Pretrained Transformers, ACL 2020\nVL-BERT: Pre-training of Generic Visual-Linguistic Representations, arXiv 2019 [code]\nVisualBERT: A Simple and Performant Baseline for Vision and Language, arXiv 2019 [code]\nViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks, NeurIPS 2019 [code]\nUnicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training, arXiv 2019\nLXMERT: Learning Cross-Modality Encoder Representations from Transformers, EMNLP 2019 [code]\nM-BERT: Injecting Multimodal Information in the BERT Structure, arXiv 2019\nVideoBERT: A Joint Model for Video and Language Representation Learning, ICCV 2019\nMultimodal Translation\nTranslate-to-Recognize Networks for RGB-D Scene Recognition, CVPR 2019 [code]\nLanguage2Pose: Natural Language Grounded Pose Forecasting, 3DV 2019 [code]\nReconstructing Faces from Voices, NeurIPS 2019 [code]\nSpeech2Face: Learning the Face Behind a Voice, CVPR 2019 [code]\nFound in Translation: Learning Robust Joint Representations by Cyclic Translations Between Modalities, AAAI 2019 [code]\nNatural TTS Synthesis by Conditioning Wavenet on Mel Spectrogram Predictions, ICASSP 2018 [code]\nCrossmodal Retrieval\nMURAL: Multimodal, Multitask Retrieval Across Languages, arXiv 2021\nSelf-Supervised Learning from Web Data for Multimodal Retrieval, arXiv 2019\nLook, Imagine and Match: Improving Textual-Visual Cross-Modal Retrieval with Generative Models, CVPR 2018\nMissing or Imperfect Modalities\nA Variational Information Bottleneck Approach to Multi-Omics Data Integration, AISTATS 2021 [code]\nSMIL: Multimodal Learning with Severely Missing Modality, AAAI 2021\nFactorized Inference in Deep Markov Models for Incomplete Multimodal Time Series, arXiv 2019\nLearning Representations from Imperfect Time Series Data via Tensor Rank Regularization, ACL 2019\nMultimodal Deep Learning for Robust RGB-D Object Recognition, IROS 2015\nAnalysis of Multimodal Models\nDecoupling the Role of Data, Attention, and Losses in Multimodal Transformers, TACL 2021\nDoes my multimodal model learn cross-modal interactions? It\u2019s harder to tell than you might think!, EMNLP 2020\nBlindfold Baselines for Embodied QA, NIPS 2018 Visually-Grounded Interaction and Language Workshop\nAnalyzing the Behavior of Visual Question Answering Models, EMNLP 2016\nKnowledge Graphs and Knowledge Bases\nMMKG: Multi-Modal Knowledge Graphs, ESWC 2019\nAnswering Visual-Relational Queries in Web-Extracted Knowledge Graphs, AKBC 2019\nEmbedding Multimodal Relational Data for Knowledge Base Completion, EMNLP 2018\nA Multimodal Translation-Based Approach for Knowledge Graph Representation Learning, SEM 2018 [code]\nOrder-Embeddings of Images and Language, ICLR 2016 [code]\nBuilding a Large-scale Multimodal Knowledge Base System for Answering Visual Queries, arXiv 2015\nIntepretable Learning\nMultimodal Explanations by Predicting Counterfactuality in Videos, CVPR 2019\nMultimodal Explanations: Justifying Decisions and Pointing to the Evidence, CVPR 2018 [code]\nDo Explanations make VQA Models more Predictable to a Human?, EMNLP 2018\nTowards Transparent AI Systems: Interpreting Visual Question Answering Models, ICML Workshop on Visualization for Deep Learning 2016\nGenerative Learning\nGeneralized Multimodal ELBO, ICLR 2021 [code]\nVariational Mixture-of-Experts Autoencodersfor Multi-Modal Deep Generative Models, NeurIPS 2019 [code]\nFew-shot Video-to-Video Synthesis, NeurIPS 2019 [code]\nMultimodal Generative Models for Scalable Weakly-Supervised Learning, NeurIPS 2018 [code1] [code2]\nThe Multi-Entity Variational Autoencoder, NeurIPS 2017\nSemi-supervised Learning\nSemi-supervised Vision-language Mapping via Variational Learning, ICRA 2017\nSemi-supervised Multimodal Hashing, arXiv 2017\nSemi-Supervised Multimodal Deep Learning for RGB-D Object Recognition, IJCAI 2016\nMultimodal Semi-supervised Learning for Image Classification, CVPR 2010\nSelf-supervised Learning\nDABS: A Domain-Agnostic Benchmark for Self-Supervised Learning, NeurIPS 2021 Datasets & Benchmarks Track [code]\nSelf-Supervised Learning by Cross-Modal Audio-Video Clustering, NeurIPS 2020 [code]\nSelf-Supervised MultiModal Versatile Networks, NeurIPS 2020 [code]\nLabelling Unlabelled Videos from Scratch with Multi-modal Self-supervision, NeurIPS 2020 [code]\nSelf-Supervised Learning of Visual Features through Embedding Images into Text Topic Spaces, CVPR 2017\nMultimodal Dynamics : Self-supervised Learning in Perceptual and Motor Systems, 2016\nLanguage Models\nNeural Language Modeling with Visual Features, arXiv 2019\nLearning Multi-Modal Word Representation Grounded in Visual Context, AAAI 2018\nVisual Word2Vec (vis-w2v): Learning Visually Grounded Word Embeddings Using Abstract Scenes, CVPR 2016\nUnifying Visual-Semantic Embeddings with Multimodal Neural Language Models, ICML 2014 [code]\nAdversarial Attacks\nAttend and Attack: Attention Guided Adversarial Attacks on Visual Question Answering Models, NeurIPS Workshop on Visually Grounded Interaction and Language 2018\nAttacking Visual Language Grounding with Adversarial Examples: A Case Study on Neural Image Captioning, ACL 2018 [code]\nFooling Vision and Language Models Despite Localization and Attention Mechanism, CVPR 2018\nFew-Shot Learning\nLanguage to Network: Conditional Parameter Adaptation with Natural Language Descriptions, ACL 2020\nShaping Visual Representations with Language for Few-shot Classification, ACL 2020\nZero-Shot Learning - The Good, the Bad and the Ugly, CVPR 2017\nZero-Shot Learning Through Cross-Modal Transfer, NIPS 2013\nBias and Fairness\nWorst of Both Worlds: Biases Compound in Pre-trained Vision-and-Language Models, arXiv 2021\nTowards Debiasing Sentence Representations, ACL 2020 [code]\nFairCVtest Demo: Understanding Bias in Multimodal Learning with a Testbed in Fair Automatic Recruitment, ICMI 2020 [code]\nModel Cards for Model Reporting, FAccT 2019\nBlack is to Criminal as Caucasian is to Police: Detecting and Removing Multiclass Bias in Word Embeddings, NAACL 2019 [code]\nGender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification, FAccT 2018\nDatasheets for Datasets, arXiv 2018\nMan is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings, NeurIPS 2016\nHuman in the Loop Learning\nHuman in the Loop Dialogue Systems, NeurIPS 2020 workshop\nHuman And Machine in-the-Loop Evaluation and Learning Strategies, NeurIPS 2020 workshop\nHuman-centric dialog training via offline reinforcement learning, EMNLP 2020 [code]\nHuman-In-The-Loop Machine Learning with Intelligent Multimodal Interfaces, ICML 2017 workshop\nArchitectures\nMultimodal Transformers\nFLAVA: A Foundational Language And Vision Alignment Model, arXiv 2021\nPolyViT: Co-training Vision Transformers on Images, Videos and Audio, arXiv 2021\nVATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text, NeurIPS 2021 [code]\nParameter Efficient Multimodal Transformers for Video Representation Learning, ICLR 2021 [code]\nMultimodal Memory\nApplications and Datasets\nLanguage and Visual QA\nMultiModalQA: complex question answering over text, tables and images, ICLR 2021\nManyModalQA: Modality Disambiguation and QA over Diverse Inputs, AAAI 2020 [code]\nIterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA, CVPR 2020\nInteractive Language Learning by Question Answering, EMNLP 2019 [code]\nFusion of Detected Objects in Text for Visual Question Answering, arXiv 2019\nRUBi: Reducing Unimodal Biases in Visual Question Answering, NeurIPS 2019 [code]\nGQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering, CVPR 2019 [code]\nOK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge, CVPR 2019 [code]\nMUREL: Multimodal Relational Reasoning for Visual Question Answering, CVPR 2019 [code]\nSocial-IQ: A Question Answering Benchmark for Artificial Social Intelligence, CVPR 2019 [code]\nProbabilistic Neural-symbolic Models for Interpretable Visual Question Answering, ICML 2019 [code]\nLearning to Count Objects in Natural Images for Visual Question Answering, ICLR 2018, [code]\nOvercoming Language Priors in Visual Question Answering with Adversarial Regularization, NeurIPS 2018\nNeural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding, NeurIPS 2018 [code]\nRecipeQA: A Challenge Dataset for Multimodal Comprehension of Cooking Recipes, EMNLP 2018 [code]\nTVQA: Localized, Compositional Video Question Answering, EMNLP 2018 [code]\nBottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering, CVPR 2018 [code]\nDon't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering, CVPR 2018 [code]\nStacked Latent Attention for Multimodal Reasoning, CVPR 2018\nLearning to Reason: End-to-End Module Networks for Visual Question Answering, ICCV 2017 [code]\nCLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning, CVPR 2017 [code] [dataset generation]\nAre You Smarter Than A Sixth Grader? Textbook Question Answering for Multimodal Machine Comprehension, CVPR 2017 [code]\nMultimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding, EMNLP 2016 [code]\nMovieQA: Understanding Stories in Movies through Question-Answering, CVPR 2016 [code]\nVQA: Visual Question Answering, ICCV 2015 [code]\nLanguage Grounding in Vision\nMaRVL: Multicultural Reasoning over Vision and Language, EMNLP 2021 [code]\nGrounding 'Grounding' in NLP, ACL 2021\nThe Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes, NeurIPS 2020 [code]\nWhat Does BERT with Vision Look At?, ACL 2020\nVisual Grounding in Video for Unsupervised Word Translation, CVPR 2020 [code]\nVIOLIN: A Large-Scale Dataset for Video-and-Language Inference, CVPR 2020 [code]\nGrounded Video Description, CVPR 2019\nShow, Control and Tell: A Framework for Generating Controllable and Grounded Captions, CVPR 2019\nMultilevel Language and Vision Integration for Text-to-Clip Retrieval, AAAI 2019 [code]\nBinary Image Selection (BISON): Interpretable Evaluation of Visual Grounding, arXiv 2019 [code]\nFinding \u201cIt\u201d: Weakly-Supervised Reference-Aware Visual Grounding in Instructional Videos, CVPR 2018\nSCAN: Learning Hierarchical Compositional Visual Concepts, ICLR 2018\nVisual Coreference Resolution in Visual Dialog using Neural Module Networks, ECCV 2018 [code]\nGated-Attention Architectures for Task-Oriented Language Grounding, AAAI 2018 [code]\nUsing Syntax to Ground Referring Expressions in Natural Images, AAAI 2018 [code]\nGrounding language acquisition by training semantic parsers using captioned videos, ACL 2018\nInterpretable and Globally Optimal Prediction for Textual Grounding using Image Concepts, NeurIPS 2017\nLocalizing Moments in Video with Natural Language, ICCV 2017\nWhat are you talking about? Text-to-Image Coreference, CVPR 2014\nGrounded Language Learning from Video Described with Sentences, ACL 2013\nGrounded Compositional Semantics for Finding and Describing Images with Sentences, TACL 2013\nLanguage Grouding in Navigation\nALFWorld: Aligning Text and Embodied Environments for Interactive Learning, ICLR 2021 [code]\nHierarchical Cross-Modal Agent for Robotics Vision-and-Language Navigation, ICRA 2021, [code], [video], [project page]\nImproving Vision-and-Language Navigation with Image-Text Pairs from the Web, ECCV 2020\nTowards Learning a Generic Agent for Vision-and-Language Navigation via Pre-training, CVPR 2020 [code]\nVideoNavQA: Bridging the Gap between Visual and Embodied Question Answering, BMVC 2019 [code]\nVision-and-Dialog Navigation, arXiv 2019 [code]\nHierarchical Decision Making by Generating and Following Natural Language Instructions, arXiv 2019 [code]\nStay on the Path: Instruction Fidelity in Vision-and-Language Navigation, ACL 2019\nAre You Looking? Grounding to Multiple Modalities in Vision-and-Language Navigation, ACL 2019\nTouchdown: Natural Language Navigation and Spatial Reasoning in Visual Street Environments, CVPR 2019 [code]\nReinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation, CVPR 2019\nThe Regretful Navigation Agent for Vision-and-Language Navigation, CVPR 2019 [code]\nTactical Rewind: Self-Correction via Backtracking in Vision-and-Language Navigation, CVPR 2019 [code]\nMulti-modal Discriminative Model for Vision-and-Language Navigation, NAACL SpLU-RoboNLP Workshop 2019\nSelf-Monitoring Navigation Agent via Auxiliary Progress Estimation, ICLR 2019 [code]\nFrom Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following, ICLR 2019\nRead, Watch, and Move: Reinforcement Learning for Temporally Grounding Natural Language Descriptions in Videos, AAAI 2019\nLearning to Navigate Unseen Environments: Back Translation with Environmental Dropout, NAACL 2019 [code]\nAttention Based Natural Language Grounding by Navigating Virtual Environment, IEEE WACV 2019\nMapping Instructions to Actions in 3D Environments with Visual Goal Prediction, EMNLP 2018 [code]\nVision-and-Language Navigation: Interpreting Visually-Grounded Navigation Instructions in Real Environments, CVPR 2018 [code]\nEmbodied Question Answering, CVPR 2018 [code]\nLook Before You Leap: Bridging Model-Free and Model-Based Reinforcement Learning for Planned-Ahead Vision-and-Language Navigation, ECCV 2018\nMultimodal Machine Translation\nUnsupervised Multimodal Neural Machine Translation with Pseudo Visual Pivoting, ACL 2020\nMultimodal Transformer for Multimodal Machine Translation, ACL 2020\nNeural Machine Translation with Universal Visual Representation, ICLR 2020 [code]\nVisual Agreement Regularized Training for Multi-Modal Machine Translation, AAAI 2020\nVATEX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research, ICCV 2019 [code]\nLatent Variable Model for Multi-modal Translation, ACL 2019\nDistilling Translations with Visual Awareness, ACL 2019\nProbing the Need for Visual Context in Multimodal Machine Translation, NAACL 2019\nEmergent Translation in Multi-Agent Communication, ICLR 2018\nZero-Resource Neural Machine Translation with Multi-Agent Communication Game, AAAI 2018\nLearning Translations via Images with a Massively Multilingual Image Dataset, ACL 2018\nA Visual Attention Grounding Neural Model for Multimodal Machine Translation, EMNLP 2018\nAdversarial Evaluation of Multimodal Machine Translation, EMNLP 2018\nDoubly-Attentive Decoder for Multi-modal Neural Machine Translation, ACL 2017 [code]\nAn empirical study on the effectiveness of images in Multimodal Neural Machine Translation, EMNLP 2017\nIncorporating Global Visual Features into Attention-based Neural Machine Translation, EMNLP 2017 [code]\nMultimodal Pivots for Image Caption Translation, ACL 2016\nMulti30K: Multilingual English-German Image Descriptions, ACL Workshop on Language and Vision 2016 [code]\nDoes Multimodality Help Human and Machine for Translation and Image Captioning?, ACL WMT 2016\nMulti-agent Communication\nMulti-agent Communication meets Natural Language: Synergies between Functional and Structural Language Learning, ACL 2020\nEmergence of Compositional Language with Deep Generational Transmission, ICML 2019\nOn the Pitfalls of Measuring Emergent Communication, AAMAS 2019 [code]\nEmergent Translation in Multi-Agent Communication, ICLR 2018 [code]\nEmergent Communication in a Multi-Modal, Multi-Step Referential Game, ICLR 2018 [code]\nEmergence of Linguistic Communication From Referential Games with Symbolic and Pixel Input, ICLR 2018\nEmergent Communication through Negotiation, ICLR 2018 [code]\nEmergence of Grounded Compositional Language in Multi-Agent Populations, AAAI 2018\nEmergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols, NeurIPS 2017\nNatural Language Does Not Emerge 'Naturally' in Multi-Agent Dialog, EMNLP 2017 [code1] [code2]\nLearning Cooperative Visual Dialog Agents with Deep Reinforcement Learning, ICCV 2017 code\nMulti-agent Cooperation and the Emergence of (natural) Language, ICLR 2017\nLearning to Communicate with Deep Multi-agent Reinforcement Learning, NIPS 2016.\nLearning multiagent communication with backpropagation, NIPS 2016.\nThe Emergence of Compositional Structures in Perceptually Grounded Language Games, AI 2005\nCommonsense Reasoning\nAdventures in Flatland: Perceiving Social Interactions Under Physical Dynamics, CogSci 2020\nA Logical Model for Supporting Social Commonsense Knowledge Acquisition, arXiv 2019\nHeterogeneous Graph Learning for Visual Commonsense Reasoning, NeurIPS 2019\nSocialIQA: Commonsense Reasoning about Social Interactions, arXiv 2019\nFrom Recognition to Cognition: Visual Commonsense Reasoning, CVPR 2019 [code]\nCommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge, NAACL 2019\nMultimodal Reinforcement Learning\nMiniHack the Planet: A Sandbox for Open-Ended Reinforcement Learning Research, NeurIPS 2021 [code]\nImitating Interactive Intelligence, arXiv 2020\nGrounded Language Learning Fast and Slow, ICLR 2021\nRTFM: Generalising to Novel Environment Dynamics via Reading, ICLR 2020 [code]\nEmbodied Multimodal Multitask Learning, IJCAI 2020\nLearning to Speak and Act in a Fantasy Text Adventure Game, arXiv 2019 [code]\nLanguage as an Abstraction for Hierarchical Deep Reinforcement Learning, NeurIPS 2019\nHierarchical Decision Making by Generating and Following Natural Language Instructions, NeurIPS 2019 [code]\nHabitat: A Platform for Embodied AI Research, ICCV 2019 [code]\nMultimodal Hierarchical Reinforcement Learning Policy for Task-Oriented Visual Dialog, SIGDIAL 2018\nMapping Instructions and Visual Observations to Actions with Reinforcement Learning, EMNLP 2017\nReinforcement Learning for Mapping Instructions to Actions, ACL 2009\nMultimodal Dialog\nTwo Causal Principles for Improving Visual Dialog, CVPR 2020\nMELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations, ACL 2019 [code]\nCLEVR-Dialog: A Diagnostic Dataset for Multi-Round Reasoning in Visual Dialog, NAACL 2019 [code]\nTalk the Walk: Navigating New York City through Grounded Dialogue, arXiv 2018\nDialog-based Interactive Image Retrieval, NeurIPS 2018 [code]\nTowards Building Large Scale Multimodal Domain-Aware Conversation Systems, arXiv 2017 [code]\nVisual Dialog, CVPR 2017 [code]\nLanguage and Audio\nLattice Transformer for Speech Translation, ACL 2019\nExploring Phoneme-Level Speech Representations for End-to-End Speech Translation, ACL 2019\nAudio Caption: Listen and Tell, ICASSP 2019\nAudio-Linguistic Embeddings for Spoken Sentences, ICASSP 2019\nFrom Semi-supervised to Almost-unsupervised Speech Recognition with Very-low Resource by Jointly Learning Phonetic Structures from Audio and Text Embeddings, arXiv 2019\nFrom Audio to Semantics: Approaches To End-to-end Spoken Language Understanding, arXiv 2018\nNatural TTS Synthesis by Conditioning Wavenet on Mel Spectrogram Predictions, ICASSP 2018 [code]\nDeep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning, ICLR 2018\nDeep Voice 2: Multi-Speaker Neural Text-to-Speech, NeurIPS 2017\nDeep Voice: Real-time Neural Text-to-Speech, ICML 2017\nText-to-Speech Synthesis, 2009\nAudio and Visual\nMusic Gesture for Visual Sound Separation, CVPR 2020\nCo-Compressing and Unifying Deep CNN Models for Efficient Human Face and Speaker Recognition, CVPRW 2019\nLearning Individual Styles of Conversational Gesture, CVPR 2019 [code]\nCapture, Learning, and Synthesis of 3D Speaking Styles, CVPR 2019 [code]\nDisjoint Mapping Network for Cross-modal Matching of Voices and Faces, ICLR 2019\nWav2Pix: Speech-conditioned Face Generation using Generative Adversarial Networks, ICASSP 2019 [code]\nJointly Discovering Visual Objects and Spoken Words from Raw Sensory Input, ECCV 2018 [code]\nSeeing Voices and Hearing Faces: Cross-modal Biometric Matching, CVPR 2018 [code]\nLearning to Separate Object Sounds by Watching Unlabeled Video, CVPR 2018\nDeep Audio-Visual Speech Recognition, IEEE TPAMI 2018\nLook, Listen and Learn, ICCV 2017\nUnsupervised Learning of Spoken Language with Visual Context, NeurIPS 2016\nSoundNet: Learning Sound Representations from Unlabeled Video, NeurIPS 2016 [code]\nMedia Description\nTowards Unsupervised Image Captioning with Shared Multimodal Embeddings, ICCV 2019\nVideo Relationship Reasoning using Gated Spatio-Temporal Energy Graph, CVPR 2019 [code]\nJoint Event Detection and Description in Continuous Video Streams, WACVW 2019\nLearning to Compose and Reason with Language Tree Structures for Visual Grounding, TPAMI 2019\nNeural Baby Talk, CVPR 2018 [code]\nGrounding Referring Expressions in Images by Variational Context, CVPR 2018\nVideo Captioning via Hierarchical Reinforcement Learning, CVPR 2018\nCharades-Ego: A Large-Scale Dataset of Paired Third and First Person Videos, CVPR 2018 [code]\nNeural Motifs: Scene Graph Parsing with Global Context, CVPR 2018 [code]\nNo Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling, ACL 2018\nGenerating Descriptions with Grounded and Co-Referenced People, CVPR 2017\nDenseCap: Fully Convolutional Localization Networks for Dense Captioning, CVPR 2016\nReview Networks for Caption Generation, NeurIPS 2016 [code]\nHollywood in Homes: Crowdsourcing Data Collection for Activity Understanding, ECCV 2016 [code]\nShow and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge, TPAMI 2016 [code]\nShow, Attend and Tell: Neural Image Caption Generation with Visual Attention, ICML 2015 [code]\nDeep Visual-Semantic Alignments for Generating Image Descriptions, CVPR 2015 [code]\nShow and Tell: A Neural Image Caption Generator, CVPR 2015 [code]\nA Dataset for Movie Description, CVPR 2015 [code]\nWhat\u2019s Cookin\u2019? Interpreting Cooking Videos using Text, Speech and Vision, NAACL 2015 [code]\nMicrosoft COCO: Common Objects in Context, ECCV 2014 [code]\nVideo Generation from Text\nImage Generation from Scene Graphs, CVPR 2018\nLearning to Color from Language, NAACL 2018\nGenerative Adversarial Text to Image Synthesis, ICML 2016\nAffect Recognition and Multimodal Language\nEnd-to-end Facial and Physiological Model for Affective Computing and Applications, arXiv 2019\nAffective Computing for Large-Scale Heterogeneous Multimedia Data: A Survey, ACM TOMM 2019\nTowards Multimodal Sarcasm Detection (An Obviously_Perfect Paper), ACL 2019 [code]\nMulti-modal Approach for Affective Computing, EMBC 2018\nMultimodal Language Analysis with Recurrent Multistage Fusion, EMNLP 2018\nMultimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph, ACL 2018 [code]\nMulti-attention Recurrent Network for Human Communication Comprehension, AAAI 2018 [code]\nEnd-to-End Multimodal Emotion Recognition using Deep Neural Networks, arXiv 2017\nAMHUSE - A Multimodal dataset for HUmor SEnsing, ICMI 2017 [code]\nDecoding Children\u2019s Social Behavior, CVPR 2013 [code]\nCollecting Large, Richly Annotated Facial-Expression Databases from Movies, IEEE Multimedia 2012 [code]\nThe Interactive Emotional Dyadic Motion Capture (IEMOCAP) Database, 2008 [code]\nHealthcare\nLeveraging Medical Visual Question Answering with Supporting Facts, arXiv 2019\nUnsupervised Multimodal Representation Learning across Medical Images and Reports, ML4H 2018\nMultimodal Medical Image Retrieval based on Latent Topic Modeling, ML4H 2018\nImproving Hospital Mortality Prediction with Medical Named Entities and Multimodal Learning, ML4H 2018\nKnowledge-driven Generative Subspaces for Modeling Multi-view Dependencies in Medical Data, ML4H 2018\nMultimodal Depression Detection: Fusion Analysis of Paralinguistic, Head Pose and Eye Gaze Behaviors, TAC 2018\nLearning the Joint Representation of Heterogeneous Temporal Events for Clinical Endpoint Prediction, AAAI 2018\nUnderstanding Coagulopathy using Multi-view Data in the Presence of Sub-Cohorts: A Hierarchical Subspace Approach, MLHC 2017\nMachine Learning in Multimodal Medical Imaging, 2017\nCross-modal Recurrent Models for Weight Objective Prediction from Multimodal Time-series Data, ML4H 2017\nSimSensei Kiosk: A Virtual Human Interviewer for Healthcare Decision Support, AAMAS 2014\nDyadic Behavior Analysis in Depression Severity Assessment Interviews, ICMI 2014\nAudiovisual Behavior Descriptors for Depression Assessment, ICMI 2013\nRobotics\nDetect, Reject, Correct: Crossmodal Compensation of Corrupted Sensors, ICRA 2021\nMultimodal sensor fusion with differentiable filters, IROS 2020\nConcept2Robot: Learning Manipulation Concepts from Instructions and Human Demonstrations, RSS 2020\nSee, Feel, Act: Hierarchical Learning for Complex Manipulation Skills with Multi-sensory Fusion, Science Robotics 2019\nEarly Fusion for Goal Directed Robotic Vision, IROS 2019\nSimultaneously Learning Vision and Feature-based Control Policies for Real-world Ball-in-a-Cup, RSS 2019\nProbabilistic Multimodal Modeling for Human-Robot Interaction Tasks, RSS 2019\nMaking Sense of Vision and Touch: Self-Supervised Learning of Multimodal Representations for Contact-Rich Tasks, ICRA 2019\nEvolving Multimodal Robot Behavior via Many Stepping Stones with the Combinatorial Multi-Objective Evolutionary Algorithm , arXiv 2018\nMulti-modal Predicate Identification using Dynamically Learned Robot Controllers, IJCAI 2018\nMultimodal Probabilistic Model-Based Planning for Human-Robot Interaction, arXiv 2017\nPerching and Vertical Climbing: Design of a Multimodal Robot, ICRA 2014\nMulti-Modal Scene Understanding for Robotic Grasping, 2011\nStrategies for Multi-Modal Scene Exploration, IROS 2010\nAutonomous Driving\nDeep Multi-modal Object Detection and Semantic Segmentation for Autonomous Driving: Datasets, Methods, and Challenges, IEEE TITS 2020 [website]\nnuScenes: A multimodal dataset for autonomous driving, CVPR 2020 [dataset]\nMultimodal End-to-End Autonomous Driving, arXiv 2020\nFinance\nA Multimodal Event-driven LSTM Model for Stock Prediction Using Online News, TKDE 2020\nMultimodal Deep Learning for Finance: Integrating and Forecasting International Stock Markets, 2019\nMultimodal deep learning for short-term stock volatility prediction, 2018\nHuman AI Interaction\nMultimodal Human Computer Interaction: A Survey, HCI 2005\nAffective multimodal human-computer interaction, Multimedia 2005\nBuilding a multimodal human-robot interface, IEEE Intelligent Systems 2001\nWorkshops\nGrand Challenge and Workshop on Human Multimodal Language, ACL 2020, ACL 2018\nAdvances in Language and Vision Research, ACL 2020\nVisually Grounded Interaction and Language, NeurIPS 2019, NeurIPS 2018\nEmergent Communication: Towards Natural Language, NeurIPS 2019\nWorkshop on Multimodal Understanding and Learning for Embodied Applications, ACM Multimedia 2019\nBeyond Vision and Language: Integrating Real-World Knowledge, EMNLP 2019\nThe How2 Challenge: New Tasks for Vision & Language, ICML 2019\nVisual Question Answering and Dialog, CVPR 2019, CVPR 2017\nMulti-modal Learning from Videos, CVPR 2019\nMultimodal Learning and Applications Workshop, CVPR 2019, ECCV 2018\nHabitat: Embodied Agents Challenge and Workshop, CVPR 2019\nClosing the Loop Between Vision and Language & LSMD Challenge, ICCV 2019\nMulti-modal Video Analysis and Moments in Time Challenge, ICCV 2019\nCross-Modal Learning in Real World, ICCV 2019\nSpatial Language Understanding and Grounded Communication for Robotics, NAACL 2019\nYouTube-8M Large-Scale Video Understanding, ICCV 2019, ECCV 2018, CVPR 2017\nLanguage and Vision Workshop, CVPR 2019, CVPR 2018, CVPR 2017, CVPR 2015\nSight and Sound, CVPR 2019, CVPR 2018\nThe Large Scale Movie Description Challenge (LSMDC), ICCV 2019, ICCV 2017\nWordplay: Reinforcement and Language Learning in Text-based Games, NeurIPS 2018\nInterpretability and Robustness in Audio, Speech, and Language, NeurIPS 2018\nMultimodal Robot Perception, ICRA 2018\nWMT18: Shared Task on Multimodal Machine Translation, EMNLP 2018\nShortcomings in Vision and Language, ECCV 2018\nComputational Approaches to Subjectivity, Sentiment and Social Media Analysis, EMNLP 2018, EMNLP 2017, NAACL-HLT 2016, EMNLP 2015, ACL 2014, NAACL-HLT 2013\nVisual Understanding Across Modalities, CVPR 2017\nInternational Workshop on Computer Vision for Audio-Visual Media, ICCV 2017\nLanguage Grounding for Robotics, ACL 2017\nComputer Vision for Audio-visual Media, ECCV 2016\nLanguage and Vision, ACL 2016, EMNLP 2015\nTutorials\nRecent Advances in Vision-and-Language Research, CVPR 2020\nConnecting Language and Vision to Actions, ACL 2018\nMachine Learning for Clinicians: Advances for Multi-Modal Health Data, MLHC 2018\nMultimodal Machine Learning, ACL 2017, CVPR 2016, ICMI 2016\nVision and Language: Bridging Vision and Language with Deep Learning, ICIP 2017\nCourses\nCMU 05-618, Human-AI Interaction\nCMU 11-777, Advanced Multimodal Machine Learning\nStanford CS422: Interactive and Embodied Learning\nCMU 16-785, Integrated Intelligence in Robotics: Vision, Language, and Planning\nCMU 10-808, Language Grounding to Vision and Control\nCMU 11-775, Large-Scale Multimedia Analysis\nMIT 6.882, Embodied Intelligence\nGeorgia Tech CS 8803, Vision and Language\nVirginia Tech CS 6501-004, Vision & Language", "link": "https://github.com/pliang279/awesome-multimodal-ml", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "reading list for topics in multimodal machine learning\nby paul liang (pliang@cs.cmu.edu), machine learning department and language technologies institute, cmu, with help from members of the multicomp lab at lti, cmu. if there are any areas, papers, and datasets i missed, please let me know!\nrecent content\nsocial intelligence in humans and robots @ icra 2021\nlantern 2021: the third workshop beyond vision and language: integrating real-world knowledge @ eacl 2021\nmultimodal workshops @ cvpr 2021: multimodal learning and applications, sight and sound, visual question answering, embodied ai, language for 3d scenes.\nmultimodal workshops @ naacl 2021: mai-workshop, alvr, vigil.\niclr 2021 workshop on embodied multimodal learning.\nmicrosoft research's work on vinvl: blog, paper.\nopenai's work on dall\u00b7e and clip: blog, paper, and code.\nfollow our course 11-777 multimodal machine learning, fall 2020 @ cmu.\nneurips 2020 workshop on wordplay: when language meets games.\nacl 2020 workshops on multimodal language (proceedings) and advances in language and vision research.\nmultimodal workshops @ eccv 2020: eval, camp, and mva.\ntable of contents\nsurvey papers\ncore areas\nmultimodal representations\nmultimodal fusion\nmultimodal alignment\nmultimodal pretraining\nmultimodal translation\ncrossmodal retrieval\nmissing or imperfect modalities\nanalysis of multimodal models\nknowledge graphs and knowledge bases\nintepretable learning\ngenerative learning\nsemi-supervised learning\nself-supervised learning\nlanguage models\nadversarial attacks\nfew-shot learning\nbias and fairness\nhuman in the loop learning\narchitectures\nmultimodal transformers\nmultimodal memory\napplications and datasets\nlanguage and visual qa\nlanguage grounding in vision\nlanguage grouding in navigation\nmultimodal machine translation\nmulti-agent communication\ncommonsense reasoning\nmultimodal reinforcement learning\nmultimodal dialog\nlanguage and audio\naudio and visual\nmedia description\nvideo generation from text\naffect recognition and multimodal language\nhealthcare\nrobotics\nautonomous driving\nfinance\nhuman ai interaction\nworkshops\ntutorials\ncourses\nresearch papers\nsurvey papers\ntrends in integration of vision and language research: a survey of tasks, datasets, and methods, jair 2021\nexperience grounds language, emnlp 2020\na survey of reinforcement learning informed by natural language, ijcai 2019\nmultimodal machine learning: a survey and taxonomy, tpami 2019\nmultimodal intelligence: representation learning, information fusion, and applications, arxiv 2019\ndeep multimodal representation learning: a survey, arxiv 2019\nguest editorial: image and language understanding, ijcv 2017\nrepresentation learning: a review and new perspectives, tpami 2013\na survey of socially interactive robots, 2003\ncore areas\nmultimodal representations\nmultibench: multiscale benchmarks for multimodal representation learning, neurips 2021 [code]\nperceiver: general perception with iterative attention, icml 2021 [code]\nlearning transferable visual models from natural language supervision, arxiv 2020 [blog] [code]\n12-in-1: multi-task vision and language representation learning, cvpr 2020 [code]\nwatching the world go by: representation learning from unlabeled videos, arxiv 2020\nlearning video representations using contrastive bidirectional transformer, arxiv 2019\nvisual concept-metaconcept learning, neurips 2019 [code]\nomninet: a unified architecture for multi-modal multi-task learning, arxiv 2019 [code]\nlearning representations by maximizing mutual information across views, arxiv 2019 [code]\nvico: word embeddings from visual co-occurrences, iccv 2019 [code]\nunified visual-semantic embeddings: bridging vision and language with structured meaning representations, cvpr 2019\nmulti-task learning of hierarchical vision-language representation, cvpr 2019\nlearning factorized multimodal representations, iclr 2019 [code]\na probabilistic framework for multi-view feature learning with many-to-many associations via neural networks, icml 2018\ndo neural network cross-modal mappings really bridge modalities?, acl 2018\nlearning robust visual-semantic embeddings, iccv 2017\ndeep multimodal representation learning from temporal data, cvpr 2017\nis an image worth more than a thousand words? on the fine-grain semantic differences between visual and linguistic representations, coling 2016\ncombining language and vision with a multimodal skip-gram model, naacl 2015\ndeep fragment embeddings for bidirectional image sentence mapping, nips 2014\nmultimodal learning with deep boltzmann machines, jmlr 2014\nlearning grounded meaning representations with autoencoders, acl 2014\ndevise: a deep visual-semantic embedding model, neurips 2013\nmultimodal deep learning, icml 2011\nmultimodal fusion\nattention bottlenecks for multimodal fusion, neurips 2021\ntrusted multi-view classification, iclr 2021 [code]\ndeep-hoseq: deep higher-order sequence fusion for multimodal sentiment analysis, icdm 2020\nremoving bias in multi-modal classifiers: regularization by maximizing functional entropies, neurips 2020 [code]\ndeep multimodal fusion by channel exchanging, neurips 2020 [code]\nwhat makes training multi-modal classification networks hard?, cvpr 2020\ndynamic fusion for multimodal data, arxiv 2019\ndeepcu: integrating both common and unique latent information for multimodal sentiment analysis, ijcai 2019 [code]\ndeep multimodal multilinear fusion with high-order polynomial pooling, neurips 2019\nxflow: cross-modal deep neural networks for audiovisual classification, ieee tnnls 2019 [code]\nmfas: multimodal fusion architecture search, cvpr 2019\nthe neuro-symbolic concept learner: interpreting scenes, words, and sentences from natural supervision, iclr 2019 [code]\nunifying and merging well-trained deep neural networks for inference stage, ijcai 2018 [code]\nefficient low-rank multimodal fusion with modality-specific factors, acl 2018 [code]\nmemory fusion network for multi-view sequential learning, aaai 2018 [code]\ntensor fusion network for multimodal sentiment analysis, emnlp 2017 [code]\njointly modeling deep video and compositional text to bridge vision and language in a unified framework, aaai 2015\nmultimodal alignment\ncomir: contrastive multimodal image representation for registration, neurips 2020 [code]\nmultimodal transformer for unaligned multimodal language sequences, acl 2019 [code]\ntemporal cycle-consistency learning, cvpr 2019 [code]\nsee, hear, and read: deep aligned representations, arxiv 2017\non deep multi-view representation learning, icml 2015\nunsupervised alignment of natural language instructions with video segments, aaai 2014\nmultimodal alignment of videos, mm 2014\ndeep canonical correlation analysis, icml 2013 [code]\nmultimodal pretraining\nless is more: clipbert for video-and-language learning via sparse sampling, cvpr 2021 [code]\ntransformer is all you need: multimodal multitask learning with a unified transformer, arxiv 2021\nlarge-scale adversarial training for vision-and-language representation learning, neurips 2020 [code]\nvokenization: improving language understanding with contextualized, visual-grounded supervision, emnlp 2020 [code]\nintegrating multimodal information in large pretrained transformers, acl 2020\nvl-bert: pre-training of generic visual-linguistic representations, arxiv 2019 [code]\nvisualbert: a simple and performant baseline for vision and language, arxiv 2019 [code]\nvilbert: pretraining task-agnostic visiolinguistic representations for vision-and-language tasks, neurips 2019 [code]\nunicoder-vl: a universal encoder for vision and language by cross-modal pre-training, arxiv 2019\nlxmert: learning cross-modality encoder representations from transformers, emnlp 2019 [code]\nm-bert: injecting multimodal information in the bert structure, arxiv 2019\nvideobert: a joint model for video and language representation learning, iccv 2019\nmultimodal translation\ntranslate-to-recognize networks for rgb-d scene recognition, cvpr 2019 [code]\nlanguage2pose: natural language grounded pose forecasting, 3dv 2019 [code]\nreconstructing faces from voices, neurips 2019 [code]\nspeech2face: learning the face behind a voice, cvpr 2019 [code]\nfound in translation: learning robust joint representations by cyclic translations between modalities, aaai 2019 [code]\nnatural tts synthesis by conditioning wavenet on mel spectrogram predictions, icassp 2018 [code]\ncrossmodal retrieval\nmural: multimodal, multitask retrieval across languages, arxiv 2021\nself-supervised learning from web data for multimodal retrieval, arxiv 2019\nlook, imagine and match: improving textual-visual cross-modal retrieval with generative models, cvpr 2018\nmissing or imperfect modalities\na variational information bottleneck approach to multi-omics data integration, aistats 2021 [code]\nsmil: multimodal learning with severely missing modality, aaai 2021\nfactorized inference in deep markov models for incomplete multimodal time series, arxiv 2019\nlearning representations from imperfect time series data via tensor rank regularization, acl 2019\nmultimodal deep learning for robust rgb-d object recognition, iros 2015\nanalysis of multimodal models\ndecoupling the role of data, attention, and losses in multimodal transformers, tacl 2021\ndoes my multimodal model learn cross-modal interactions? it\u2019s harder to tell than you might think!, emnlp 2020\nblindfold baselines for embodied qa, nips 2018 visually-grounded interaction and language workshop\nanalyzing the behavior of visual question answering models, emnlp 2016\nknowledge graphs and knowledge bases\nmmkg: multi-modal knowledge graphs, eswc 2019\nanswering visual-relational queries in web-extracted knowledge graphs, akbc 2019\nembedding multimodal relational data for knowledge base completion, emnlp 2018\na multimodal translation-based approach for knowledge graph representation learning, sem 2018 [code]\norder-embeddings of images and language, iclr 2016 [code]\nbuilding a large-scale multimodal knowledge base system for answering visual queries, arxiv 2015\nintepretable learning\nmultimodal explanations by predicting counterfactuality in videos, cvpr 2019\nmultimodal explanations: justifying decisions and pointing to the evidence, cvpr 2018 [code]\ndo explanations make vqa models more predictable to a human?, emnlp 2018\ntowards transparent ai systems: interpreting visual question answering models, icml workshop on visualization for deep learning 2016\ngenerative learning\ngeneralized multimodal elbo, iclr 2021 [code]\nvariational mixture-of-experts autoencodersfor multi-modal deep generative models, neurips 2019 [code]\nfew-shot video-to-video synthesis, neurips 2019 [code]\nmultimodal generative models for scalable weakly-supervised learning, neurips 2018 [code1] [code2]\nthe multi-entity variational autoencoder, neurips 2017\nsemi-supervised learning\nsemi-supervised vision-language mapping via variational learning, icra 2017\nsemi-supervised multimodal hashing, arxiv 2017\nsemi-supervised multimodal deep learning for rgb-d object recognition, ijcai 2016\nmultimodal semi-supervised learning for image classification, cvpr 2010\nself-supervised learning\ndabs: a domain-agnostic benchmark for self-supervised learning, neurips 2021 datasets & benchmarks track [code]\nself-supervised learning by cross-modal audio-video clustering, neurips 2020 [code]\nself-supervised multimodal versatile networks, neurips 2020 [code]\nlabelling unlabelled videos from scratch with multi-modal self-supervision, neurips 2020 [code]\nself-supervised learning of visual features through embedding images into text topic spaces, cvpr 2017\nmultimodal dynamics : self-supervised learning in perceptual and motor systems, 2016\nlanguage models\nneural language modeling with visual features, arxiv 2019\nlearning multi-modal word representation grounded in visual context, aaai 2018\nvisual word2vec (vis-w2v): learning visually grounded word embeddings using abstract scenes, cvpr 2016\nunifying visual-semantic embeddings with multimodal neural language models, icml 2014 [code]\nadversarial attacks\nattend and attack: attention guided adversarial attacks on visual question answering models, neurips workshop on visually grounded interaction and language 2018\nattacking visual language grounding with adversarial examples: a case study on neural image captioning, acl 2018 [code]\nfooling vision and language models despite localization and attention mechanism, cvpr 2018\nfew-shot learning\nlanguage to network: conditional parameter adaptation with natural language descriptions, acl 2020\nshaping visual representations with language for few-shot classification, acl 2020\nzero-shot learning - the good, the bad and the ugly, cvpr 2017\nzero-shot learning through cross-modal transfer, nips 2013\nbias and fairness\nworst of both worlds: biases compound in pre-trained vision-and-language models, arxiv 2021\ntowards debiasing sentence representations, acl 2020 [code]\nfaircvtest demo: understanding bias in multimodal learning with a testbed in fair automatic recruitment, icmi 2020 [code]\nmodel cards for model reporting, facct 2019\nblack is to criminal as caucasian is to police: detecting and removing multiclass bias in word embeddings, naacl 2019 [code]\ngender shades: intersectional accuracy disparities in commercial gender classification, facct 2018\ndatasheets for datasets, arxiv 2018\nman is to computer programmer as woman is to homemaker? debiasing word embeddings, neurips 2016\nhuman in the loop learning\nhuman in the loop dialogue systems, neurips 2020 workshop\nhuman and machine in-the-loop evaluation and learning strategies, neurips 2020 workshop\nhuman-centric dialog training via offline reinforcement learning, emnlp 2020 [code]\nhuman-in-the-loop machine learning with intelligent multimodal interfaces, icml 2017 workshop\narchitectures\nmultimodal transformers\nflava: a foundational language and vision alignment model, arxiv 2021\npolyvit: co-training vision transformers on images, videos and audio, arxiv 2021\nvatt: transformers for multimodal self-supervised learning from raw video, audio and text, neurips 2021 [code]\nparameter efficient multimodal transformers for video representation learning, iclr 2021 [code]\nmultimodal memory\napplications and datasets\nlanguage and visual qa\nmultimodalqa: complex question answering over text, tables and images, iclr 2021\nmanymodalqa: modality disambiguation and qa over diverse inputs, aaai 2020 [code]\niterative answer prediction with pointer-augmented multimodal transformers for textvqa, cvpr 2020\ninteractive language learning by question answering, emnlp 2019 [code]\nfusion of detected objects in text for visual question answering, arxiv 2019\nrubi: reducing unimodal biases in visual question answering, neurips 2019 [code]\ngqa: a new dataset for real-world visual reasoning and compositional question answering, cvpr 2019 [code]\nok-vqa: a visual question answering benchmark requiring external knowledge, cvpr 2019 [code]\nmurel: multimodal relational reasoning for visual question answering, cvpr 2019 [code]\nsocial-iq: a question answering benchmark for artificial social intelligence, cvpr 2019 [code]\nprobabilistic neural-symbolic models for interpretable visual question answering, icml 2019 [code]\nlearning to count objects in natural images for visual question answering, iclr 2018, [code]\novercoming language priors in visual question answering with adversarial regularization, neurips 2018\nneural-symbolic vqa: disentangling reasoning from vision and language understanding, neurips 2018 [code]\nrecipeqa: a challenge dataset for multimodal comprehension of cooking recipes, emnlp 2018 [code]\ntvqa: localized, compositional video question answering, emnlp 2018 [code]\nbottom-up and top-down attention for image captioning and visual question answering, cvpr 2018 [code]\ndon't just assume; look and answer: overcoming priors for visual question answering, cvpr 2018 [code]\nstacked latent attention for multimodal reasoning, cvpr 2018\nlearning to reason: end-to-end module networks for visual question answering, iccv 2017 [code]\nclevr: a diagnostic dataset for compositional language and elementary visual reasoning, cvpr 2017 [code] [dataset generation]\nare you smarter than a sixth grader? textbook question answering for multimodal machine comprehension, cvpr 2017 [code]\nmultimodal compact bilinear pooling for visual question answering and visual grounding, emnlp 2016 [code]\nmovieqa: understanding stories in movies through question-answering, cvpr 2016 [code]\nvqa: visual question answering, iccv 2015 [code]\nlanguage grounding in vision\nmarvl: multicultural reasoning over vision and language, emnlp 2021 [code]\ngrounding 'grounding' in nlp, acl 2021\nthe hateful memes challenge: detecting hate speech in multimodal memes, neurips 2020 [code]\nwhat does bert with vision look at?, acl 2020\nvisual grounding in video for unsupervised word translation, cvpr 2020 [code]\nviolin: a large-scale dataset for video-and-language inference, cvpr 2020 [code]\ngrounded video description, cvpr 2019\nshow, control and tell: a framework for generating controllable and grounded captions, cvpr 2019\nmultilevel language and vision integration for text-to-clip retrieval, aaai 2019 [code]\nbinary image selection (bison): interpretable evaluation of visual grounding, arxiv 2019 [code]\nfinding \u201cit\u201d: weakly-supervised reference-aware visual grounding in instructional videos, cvpr 2018\nscan: learning hierarchical compositional visual concepts, iclr 2018\nvisual coreference resolution in visual dialog using neural module networks, eccv 2018 [code]\ngated-attention architectures for task-oriented language grounding, aaai 2018 [code]\nusing syntax to ground referring expressions in natural images, aaai 2018 [code]\ngrounding language acquisition by training semantic parsers using captioned videos, acl 2018\ninterpretable and globally optimal prediction for textual grounding using image concepts, neurips 2017\nlocalizing moments in video with natural language, iccv 2017\nwhat are you talking about? text-to-image coreference, cvpr 2014\ngrounded language learning from video described with sentences, acl 2013\ngrounded compositional semantics for finding and describing images with sentences, tacl 2013\nlanguage grouding in navigation\nalfworld: aligning text and embodied environments for interactive learning, iclr 2021 [code]\nhierarchical cross-modal agent for robotics vision-and-language navigation, icra 2021, [code], [video], [project page]\nimproving vision-and-language navigation with image-text pairs from the web, eccv 2020\ntowards learning a generic agent for vision-and-language navigation via pre-training, cvpr 2020 [code]\nvideonavqa: bridging the gap between visual and embodied question answering, bmvc 2019 [code]\nvision-and-dialog navigation, arxiv 2019 [code]\nhierarchical decision making by generating and following natural language instructions, arxiv 2019 [code]\nstay on the path: instruction fidelity in vision-and-language navigation, acl 2019\nare you looking? grounding to multiple modalities in vision-and-language navigation, acl 2019\ntouchdown: natural language navigation and spatial reasoning in visual street environments, cvpr 2019 [code]\nreinforced cross-modal matching and self-supervised imitation learning for vision-language navigation, cvpr 2019\nthe regretful navigation agent for vision-and-language navigation, cvpr 2019 [code]\ntactical rewind: self-correction via backtracking in vision-and-language navigation, cvpr 2019 [code]\nmulti-modal discriminative model for vision-and-language navigation, naacl splu-robonlp workshop 2019\nself-monitoring navigation agent via auxiliary progress estimation, iclr 2019 [code]\nfrom language to goals: inverse reinforcement learning for vision-based instruction following, iclr 2019\nread, watch, and move: reinforcement learning for temporally grounding natural language descriptions in videos, aaai 2019\nlearning to navigate unseen environments: back translation with environmental dropout, naacl 2019 [code]\nattention based natural language grounding by navigating virtual environment, ieee wacv 2019\nmapping instructions to actions in 3d environments with visual goal prediction, emnlp 2018 [code]\nvision-and-language navigation: interpreting visually-grounded navigation instructions in real environments, cvpr 2018 [code]\nembodied question answering, cvpr 2018 [code]\nlook before you leap: bridging model-free and model-based reinforcement learning for planned-ahead vision-and-language navigation, eccv 2018\nmultimodal machine translation\nunsupervised multimodal neural machine translation with pseudo visual pivoting, acl 2020\nmultimodal transformer for multimodal machine translation, acl 2020\nneural machine translation with universal visual representation, iclr 2020 [code]\nvisual agreement regularized training for multi-modal machine translation, aaai 2020\nvatex: a large-scale, high-quality multilingual dataset for video-and-language research, iccv 2019 [code]\nlatent variable model for multi-modal translation, acl 2019\ndistilling translations with visual awareness, acl 2019\nprobing the need for visual context in multimodal machine translation, naacl 2019\nemergent translation in multi-agent communication, iclr 2018\nzero-resource neural machine translation with multi-agent communication game, aaai 2018\nlearning translations via images with a massively multilingual image dataset, acl 2018\na visual attention grounding neural model for multimodal machine translation, emnlp 2018\nadversarial evaluation of multimodal machine translation, emnlp 2018\ndoubly-attentive decoder for multi-modal neural machine translation, acl 2017 [code]\nan empirical study on the effectiveness of images in multimodal neural machine translation, emnlp 2017\nincorporating global visual features into attention-based neural machine translation, emnlp 2017 [code]\nmultimodal pivots for image caption translation, acl 2016\nmulti30k: multilingual english-german image descriptions, acl workshop on language and vision 2016 [code]\ndoes multimodality help human and machine for translation and image captioning?, acl wmt 2016\nmulti-agent communication\nmulti-agent communication meets natural language: synergies between functional and structural language learning, acl 2020\nemergence of compositional language with deep generational transmission, icml 2019\non the pitfalls of measuring emergent communication, aamas 2019 [code]\nemergent translation in multi-agent communication, iclr 2018 [code]\nemergent communication in a multi-modal, multi-step referential game, iclr 2018 [code]\nemergence of linguistic communication from referential games with symbolic and pixel input, iclr 2018\nemergent communication through negotiation, iclr 2018 [code]\nemergence of grounded compositional language in multi-agent populations, aaai 2018\nemergence of language with multi-agent games: learning to communicate with sequences of symbols, neurips 2017\nnatural language does not emerge 'naturally' in multi-agent dialog, emnlp 2017 [code1] [code2]\nlearning cooperative visual dialog agents with deep reinforcement learning, iccv 2017 code\nmulti-agent cooperation and the emergence of (natural) language, iclr 2017\nlearning to communicate with deep multi-agent reinforcement learning, nips 2016.\nlearning multiagent communication with backpropagation, nips 2016.\nthe emergence of compositional structures in perceptually grounded language games, ai 2005\ncommonsense reasoning\nadventures in flatland: perceiving social interactions under physical dynamics, cogsci 2020\na logical model for supporting social commonsense knowledge acquisition, arxiv 2019\nheterogeneous graph learning for visual commonsense reasoning, neurips 2019\nsocialiqa: commonsense reasoning about social interactions, arxiv 2019\nfrom recognition to cognition: visual commonsense reasoning, cvpr 2019 [code]\ncommonsenseqa: a question answering challenge targeting commonsense knowledge, naacl 2019\nmultimodal reinforcement learning\nminihack the planet: a sandbox for open-ended reinforcement learning research, neurips 2021 [code]\nimitating interactive intelligence, arxiv 2020\ngrounded language learning fast and slow, iclr 2021\nrtfm: generalising to novel environment dynamics via reading, iclr 2020 [code]\nembodied multimodal multitask learning, ijcai 2020\nlearning to speak and act in a fantasy text adventure game, arxiv 2019 [code]\nlanguage as an abstraction for hierarchical deep reinforcement learning, neurips 2019\nhierarchical decision making by generating and following natural language instructions, neurips 2019 [code]\nhabitat: a platform for embodied ai research, iccv 2019 [code]\nmultimodal hierarchical reinforcement learning policy for task-oriented visual dialog, sigdial 2018\nmapping instructions and visual observations to actions with reinforcement learning, emnlp 2017\nreinforcement learning for mapping instructions to actions, acl 2009\nmultimodal dialog\ntwo causal principles for improving visual dialog, cvpr 2020\nmeld: a multimodal multi-party dataset for emotion recognition in conversations, acl 2019 [code]\nclevr-dialog: a diagnostic dataset for multi-round reasoning in visual dialog, naacl 2019 [code]\ntalk the walk: navigating new york city through grounded dialogue, arxiv 2018\ndialog-based interactive image retrieval, neurips 2018 [code]\ntowards building large scale multimodal domain-aware conversation systems, arxiv 2017 [code]\nvisual dialog, cvpr 2017 [code]\nlanguage and audio\nlattice transformer for speech translation, acl 2019\nexploring phoneme-level speech representations for end-to-end speech translation, acl 2019\naudio caption: listen and tell, icassp 2019\naudio-linguistic embeddings for spoken sentences, icassp 2019\nfrom semi-supervised to almost-unsupervised speech recognition with very-low resource by jointly learning phonetic structures from audio and text embeddings, arxiv 2019\nfrom audio to semantics: approaches to end-to-end spoken language understanding, arxiv 2018\nnatural tts synthesis by conditioning wavenet on mel spectrogram predictions, icassp 2018 [code]\ndeep voice 3: scaling text-to-speech with convolutional sequence learning, iclr 2018\ndeep voice 2: multi-speaker neural text-to-speech, neurips 2017\ndeep voice: real-time neural text-to-speech, icml 2017\ntext-to-speech synthesis, 2009\naudio and visual\nmusic gesture for visual sound separation, cvpr 2020\nco-compressing and unifying deep cnn models for efficient human face and speaker recognition, cvprw 2019\nlearning individual styles of conversational gesture, cvpr 2019 [code]\ncapture, learning, and synthesis of 3d speaking styles, cvpr 2019 [code]\ndisjoint mapping network for cross-modal matching of voices and faces, iclr 2019\nwav2pix: speech-conditioned face generation using generative adversarial networks, icassp 2019 [code]\njointly discovering visual objects and spoken words from raw sensory input, eccv 2018 [code]\nseeing voices and hearing faces: cross-modal biometric matching, cvpr 2018 [code]\nlearning to separate object sounds by watching unlabeled video, cvpr 2018\ndeep audio-visual speech recognition, ieee tpami 2018\nlook, listen and learn, iccv 2017\nunsupervised learning of spoken language with visual context, neurips 2016\nsoundnet: learning sound representations from unlabeled video, neurips 2016 [code]\nmedia description\ntowards unsupervised image captioning with shared multimodal embeddings, iccv 2019\nvideo relationship reasoning using gated spatio-temporal energy graph, cvpr 2019 [code]\njoint event detection and description in continuous video streams, wacvw 2019\nlearning to compose and reason with language -----> tree !!!  structures for visual grounding, tpami 2019\nneural baby talk, cvpr 2018 [code]\ngrounding referring expressions in images by variational context, cvpr 2018\nvideo captioning via hierarchical reinforcement learning, cvpr 2018\ncharades-ego: a large-scale dataset of paired third and first person videos, cvpr 2018 [code]\nneural motifs: scene graph parsing with global context, cvpr 2018 [code]\nno metrics are perfect: adversarial reward learning for visual storytelling, acl 2018\ngenerating descriptions with grounded and co-referenced people, cvpr 2017\ndensecap: fully convolutional localization networks for dense captioning, cvpr 2016\nreview networks for caption generation, neurips 2016 [code]\nhollywood in homes: crowdsourcing data collection for activity understanding, eccv 2016 [code]\nshow and tell: lessons learned from the 2015 mscoco image captioning challenge, tpami 2016 [code]\nshow, attend and tell: neural image caption generation with visual attention, icml 2015 [code]\ndeep visual-semantic alignments for generating image descriptions, cvpr 2015 [code]\nshow and tell: a neural image caption generator, cvpr 2015 [code]\na dataset for movie description, cvpr 2015 [code]\nwhat\u2019s cookin\u2019? interpreting cooking videos using text, speech and vision, naacl 2015 [code]\nmicrosoft coco: common objects in context, eccv 2014 [code]\nvideo generation from text\nimage generation from scene graphs, cvpr 2018\nlearning to color from language, naacl 2018\ngenerative adversarial text to image synthesis, icml 2016\naffect recognition and multimodal language\nend-to-end facial and physiological model for affective computing and applications, arxiv 2019\naffective computing for large-scale heterogeneous multimedia data: a survey, acm tomm 2019\ntowards multimodal sarcasm detection (an obviously_perfect paper), acl 2019 [code]\nmulti-modal approach for affective computing, embc 2018\nmultimodal language analysis with recurrent multistage fusion, emnlp 2018\nmultimodal language analysis in the wild: cmu-mosei dataset and interpretable dynamic fusion graph, acl 2018 [code]\nmulti-attention recurrent network for human communication comprehension, aaai 2018 [code]\nend-to-end multimodal emotion recognition using deep neural networks, arxiv 2017\namhuse - a multimodal dataset for humor sensing, icmi 2017 [code]\ndecoding children\u2019s social behavior, cvpr 2013 [code]\ncollecting large, richly annotated facial-expression databases from movies, ieee multimedia 2012 [code]\nthe interactive emotional dyadic motion capture (iemocap) database, 2008 [code]\nhealthcare\nleveraging medical visual question answering with supporting facts, arxiv 2019\nunsupervised multimodal representation learning across medical images and reports, ml4h 2018\nmultimodal medical image retrieval based on latent topic modeling, ml4h 2018\nimproving hospital mortality prediction with medical named entities and multimodal learning, ml4h 2018\nknowledge-driven generative subspaces for modeling multi-view dependencies in medical data, ml4h 2018\nmultimodal depression detection: fusion analysis of paralinguistic, head pose and eye gaze behaviors, tac 2018\nlearning the joint representation of heterogeneous temporal events for clinical endpoint prediction, aaai 2018\nunderstanding coagulopathy using multi-view data in the presence of sub-cohorts: a hierarchical subspace approach, mlhc 2017\nmachine learning in multimodal medical imaging, 2017\ncross-modal recurrent models for weight objective prediction from multimodal time-series data, ml4h 2017\nsimsensei kiosk: a virtual human interviewer for healthcare decision support, aamas 2014\ndyadic behavior analysis in depression severity assessment interviews, icmi 2014\naudiovisual behavior descriptors for depression assessment, icmi 2013\nrobotics\ndetect, reject, correct: crossmodal compensation of corrupted sensors, icra 2021\nmultimodal sensor fusion with differentiable filters, iros 2020\nconcept2robot: learning manipulation concepts from instructions and human demonstrations, rss 2020\nsee, feel, act: hierarchical learning for complex manipulation skills with multi-sensory fusion, science robotics 2019\nearly fusion for goal directed robotic vision, iros 2019\nsimultaneously learning vision and feature-based control policies for real-world ball-in-a-cup, rss 2019\nprobabilistic multimodal modeling for human-robot interaction tasks, rss 2019\nmaking sense of vision and touch: self-supervised learning of multimodal representations for contact-rich tasks, icra 2019\nevolving multimodal robot behavior via many stepping stones with the combinatorial multi-objective evolutionary algorithm , arxiv 2018\nmulti-modal predicate identification using dynamically learned robot controllers, ijcai 2018\nmultimodal probabilistic model-based planning for human-robot interaction, arxiv 2017\nperching and vertical climbing: design of a multimodal robot, icra 2014\nmulti-modal scene understanding for robotic grasping, 2011\nstrategies for multi-modal scene exploration, iros 2010\nautonomous driving\ndeep multi-modal object detection and semantic segmentation for autonomous driving: datasets, methods, and challenges, ieee tits 2020 [website]\nnuscenes: a multimodal dataset for autonomous driving, cvpr 2020 [dataset]\nmultimodal end-to-end autonomous driving, arxiv 2020\nfinance\na multimodal event-driven lstm model for stock prediction using online news, tkde 2020\nmultimodal deep learning for finance: integrating and forecasting international stock markets, 2019\nmultimodal deep learning for short-term stock volatility prediction, 2018\nhuman ai interaction\nmultimodal human computer interaction: a survey, hci 2005\naffective multimodal human-computer interaction, multimedia 2005\nbuilding a multimodal human-robot interface, ieee intelligent systems 2001\nworkshops\ngrand challenge and workshop on human multimodal language, acl 2020, acl 2018\nadvances in language and vision research, acl 2020\nvisually grounded interaction and language, neurips 2019, neurips 2018\nemergent communication: towards natural language, neurips 2019\nworkshop on multimodal understanding and learning for embodied applications, acm multimedia 2019\nbeyond vision and language: integrating real-world knowledge, emnlp 2019\nthe how2 challenge: new tasks for vision & language, icml 2019\nvisual question answering and dialog, cvpr 2019, cvpr 2017\nmulti-modal learning from videos, cvpr 2019\nmultimodal learning and applications workshop, cvpr 2019, eccv 2018\nhabitat: embodied agents challenge and workshop, cvpr 2019\nclosing the loop between vision and language & lsmd challenge, iccv 2019\nmulti-modal video analysis and moments in time challenge, iccv 2019\ncross-modal learning in real world, iccv 2019\nspatial language understanding and grounded communication for robotics, naacl 2019\nyoutube-8m large-scale video understanding, iccv 2019, eccv 2018, cvpr 2017\nlanguage and vision workshop, cvpr 2019, cvpr 2018, cvpr 2017, cvpr 2015\nsight and sound, cvpr 2019, cvpr 2018\nthe large scale movie description challenge (lsmdc), iccv 2019, iccv 2017\nwordplay: reinforcement and language learning in text-based games, neurips 2018\ninterpretability and robustness in audio, speech, and language, neurips 2018\nmultimodal robot perception, icra 2018\nwmt18: shared task on multimodal machine translation, emnlp 2018\nshortcomings in vision and language, eccv 2018\ncomputational approaches to subjectivity, sentiment and social media analysis, emnlp 2018, emnlp 2017, naacl-hlt 2016, emnlp 2015, acl 2014, naacl-hlt 2013\nvisual understanding across modalities, cvpr 2017\ninternational workshop on computer vision for audio-visual media, iccv 2017\nlanguage grounding for robotics, acl 2017\ncomputer vision for audio-visual media, eccv 2016\nlanguage and vision, acl 2016, emnlp 2015\ntutorials\nrecent advances in vision-and-language research, cvpr 2020\nconnecting language and vision to actions, acl 2018\nmachine learning for clinicians: advances for multi-modal health data, mlhc 2018\nmultimodal machine learning, acl 2017, cvpr 2016, icmi 2016\nvision and language: bridging vision and language with deep learning, icip 2017\ncourses\ncmu 05-618, human-ai interaction\ncmu 11-777, advanced multimodal machine learning\nstanford cs422: interactive and embodied learning\ncmu 16-785, integrated intelligence in robotics: vision, language, and planning\ncmu 10-808, language grounding to vision and control\ncmu 11-775, large-scale multimedia analysis\nmit 6.882, embodied intelligence\ngeorgia tech cs 8803, vision and language\nvirginia tech cs 6501-004, vision & language", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000015, "year": null}, {"Unnamed: 0": 1053, "autor": 33, "date": null, "content": "Awesome Robotics Libraries\nA curated list of robotics simulators and libraries.\nTable of Contents\nSimulators\nLibraries\nDynamics Simulation\nInverse Kinematics\nMachine Learning\nMotion Planning and Control\nOptimization\nRobot Modeling\nRobot Platform\nSLAM\nVision\nFluid\nMultiphysics\nMath\nETC\nOther Awesome Lists\nContributing\nSimulators\nFree or Open Source\nAI2-THOR - Python framework with a Unity backend, providing interaction, navigation, and manipulation support for household based robotic agents [github ]\nAirSim - Simulator based on Unreal Engine for autonomous vehicles [github ]\nARGoS - Physics-based simulator designed to simulate large-scale robot swarms [github ]\nARTE - Matlab toolbox focussed on robotic manipulators [github ]\nCARLA - Open-source simulator for autonomous driving research [github ]\nCoppeliaSim - Formaly V-REP. Virtual robot experimentation platform [github ]\nGazebo - Dynamic multi-robot simulator [github ]\nGraspIt! - Simulator for grasping research that can accommodate arbitrary hand and robot designs [github ]\nHabitat-Sim - Simulation platform for research in embodied artificial intelligence [github ]\nHexapod Robot Simulator - Open-source hexapod robot inverse kinematics and gaits visualizer [github ]\nIgnition Gazebo - Open source robotics simulator [github ]\nIsaac - Nvidia's virtual simulator for robots\nMORSE - Modular open robots simulation engine [github ]\nNeurorobotics Platform - Internet-accessible simulation of robots controlled by spiking neural networks [bitbucket]\nPyBullet - An easy to use simulator for robotics and deep reinforcement learning [github ]\nRobot Gui - A three.js based 3D robot interface [github ]\nSimbad - A Java 3D robot simulator, enables to write own robot controller with modifying environment using available sensors.\nUnity - Popular game engine that now offers open-source tools, tutorials, and resources for robotics simulation [github ]\nWebots - Robot simulator that provides a complete development environment [github ]\nCommercial\nActin Simulation\nArtiminds - Planning, programming, operation, analysis and optimization\nKineo - Path planning and trajectory optimization for industrial robotics and digital mock-up review applications\nRobotDK - Simulation and OLP for robots\nRobotStudio\nRobot Virtual Worlds\nVirtual Robotics Toolkit\nVisual Components\nCloud\nAWS RoboMaker - Service that makes it easy to develop, test, and deploy intelligent robotics applications at scale\nLibraries\nDynamics Simulation\n\u26a0\ufe0f The following table is not complete. Please feel free to report if you find something incorrect or missing.\nName Models Features Languages Licenses Code Popularity\nARCSim soft C++\nBullet rigid, soft ik, id, urdf, sdf C++, Python Zlib github\nCHRONO::ENGINE rigid, soft, granular, fluid ik, urdf C++, Python BSD-3-Clause github\nDART rigid, soft ik, id, plan, urdf, sdf C++, Python BSD-2-Clause github\nDrake rigid, aero, fluid ik, trj-opt, plan C++, Matlab BSD-3-Clause github\nFlex rigid, soft, particle, fluid C++ github\nFROST rigid MATLAB BSD-3-Clause github\nIBDS rigid, particle C++ Zlib\nidyntree rigid id C++, Python, Matlab, Lua LGPL-2.1 github\nKDL rigid ik C++ LGPL-2.1 github\nkindr rigid (todo) C++, Matlab BSD-3-Clause github\nKlampt (todo) (todo) C++, Python BSD-3-Clause github\nLibrePilot uav, vehicles (todo) C++ GPL-3.0 bitbucket, github\nMARS (todo) (todo) C++, Python LGPL-3.0 github\nMBDyn (todo) (todo) C++ GPL-2.1 download\nMBSim (todo) (todo) C++ (not specified) github\nMBSlib (todo) (todo) C++ LGPL-3.0 github\nmetapod (todo) (todo) C++ LGPL-3.0 github\nMoby rigid id C++ GPL-2.0 github\nmrpt vehicle slam, cv C++, Python, Matlab BSD-3-Clause github\nMuJoCo (todo) id C++, Python licenses closed source\nmvsim vehicle (todo) C++ GPL-3.0 github\nNewton Dynamics (todo) (todo) C++ Zlib github\nnphysics (todo) (todo) Rust BSD-3-Clause github\nODE rigid C++ LGPL-2.1 or BSD-3-Clause bitbucket\nOpenRAVE (todo) (todo) C++, Python LGPL-3.0 github\npinocchio rigid ik, id, urdf, analytical derivatives, code generation C++, Python BSD-2-Clause github\nPositionBasedDynamics (todo) (todo) C++ MIT github\nPhysX (todo) (todo) C++ unknown github\nPyDy (todo) (todo) Python BSD-3-Clause github\nRBDL rigid ik,id,urdf C++, Python Zlib github\nRBDyn rigid (todo) C++, Python LGPL-3.0 github\nRaiSim (todo) (todo) C++ custom github\nReactPhysics3d (todo) (todo) C++ Zlib github\nRigidBodyDynamics.jl rigid (todo) Julia MIT \"Expat\" github\nRigs of Rods rigid, soft, vehicle (todo) C++ GPL-3.0 github\nRobopy (todo) (todo) Python 3 MIT github\nRobotics Library (todo) (todo) C++ GPL-3.0 or BSD-2-Clause github\nRobWork (todo) (todo) C++ Apache-2.0 gitlab\nsiconos (todo) (todo) C++, Python Apache-2.0 github\nSimbody rigid, molecules id, urdf C++ Apache-2.0 github\nSOFA rigid, soft, medical (todo) C++ LGPL-2.1 github\nTiny Differentiable Simulator rigid (todo) C++, Python Apache-2.0 github\ntrep rigid dm, trj-opt C, Python GPL-3.0 github\nqu3e rigid - C++ Zlib github\nFor simplicity, shortened names are used to represent the supported models and features as\nSupported Models\nrigid: rigid bodies\nsoft: soft bodies\naero: aerodynamics\ngranular: granular materials (like sand)\nfluid: fluid dynamics\nvehicles\nuav: unmanned aerial vehicles (like drones)\nmedical\nmolecules\nparallel: parallel mechanism (like Stewart platform)\nFeatures on Simulation, Analysis, Planning, Control Design\ndm: discrete mechanics\nik: inverse kinematics solvers (please find IK specialized packages in this list)\nid: inverse dynamics\nslam: simultaneous localization and mapping\ntrj-opt: trajectory optimization\nplan: motion planning algorithms\ncv: computer vision\nurdf: urdf parser\nsdf: sdf parser\nInverse Kinematics\nIKBT - A python package to solve robot arm inverse kinematics in symbolic form [github ]\nRelaxedIK - Real-time Synthesis of Accurate and Feasible Robot Arm Motion [github ]\nTrip - A python package that solves inverse kinematics of parallel-, serial- or hybrid-robots [github ]\nMachine Learning\nAllenAct - Python/PyTorch-based Research Framework for Embodied AI [github ]\nDLL - Deep Learning Library (DLL) for C++ [github ]\nDyNet - The Dynamic Neural Network Toolkit [github ]\nFido - Lightweight C++ machine learning library for embedded electronics and robotics [github ]\nMiniDNN - A header-only C++ library for deep neural networks [github ]\nmlpack - Scalable C++ machine learning library [github ]\nOpenAI Gym - Developing and comparing reinforcement learning algorithms [github ]\ngym-dart [github ]\ngym-gazebo [github ]\nRLLib - Temporal-difference learning algorithms in reinforcement learning [github ]\ntiny-dnn - Header only, dependency-free deep learning framework in C++14 [github ]\nMotion Planning and Control\nAIKIDO - Solving robotic motion planning and decision making problems. [github ]\nCuiKSuite - Applications to solve position analysis and path planning problems\nControl Toolbox - Open-Source C++ Library for Robotics, Optimal and Model Predictive Control [github ]\nCrocoddyl - Optimal control library for robot control under contact sequence [github ]\nGPMP2 - Gaussian Process Motion Planner 2 [github ]\nHPP - Path planning for kinematic chains in environments cluttered with obstacles [github]\nMoveIt! - Motion planning framework [github ]\nOMPL - Open motion planning library [bitbucket, github ]\nOCS2 - Efficient continuous and discrete time optimal control implementation [bitbucket]\npymanoid - Humanoid robotics prototyping environment based on OpenRAVE [github ]\nROS Behavior Tree - [github ]\nRuckig - Real-time, time-optimal and jerk-constrained online trajectory generation. [github ]\nThe Kautham Project - A robot simulation toolkit for motion planning [github ]\nTOPP-RA - Time-parameterizing robot trajectories subject to kinematic and dynamic constraints [github ]\nMotion Optimizer\nTopiCo - Time-optimal Trajectory Generation and Control [github ]\ntowr - A light-weight, Eigen-based C++ library for trajectory optimization for legged robots [github ]\nTrajectoryOptimization - A fast trajectory optimization library written in Julia [github ]\ntrajopt - Framework for generating robot trajectories by local optimization [github ]\nNearest Neighbor\nCover-Tree - Cover tree data structure for quick k-nearest-neighbor search [github ]\nFaster cover trees by Mike Izbicki et al., ICML 2015.\nFLANN - Fast Library for Approximate Nearest Neighbors [github ]\nnanoflann - Nearest Neighbor search with KD-trees [github ]\n3D Mapping\nlibpointmatcher - Iterative Closest Point library for 2-D/3-D mapping in Robotics [github ]\nOctree - Fast radius neighbor search with an Octree [github ]\nOctoMap - Efficient Probabilistic 3D Mapping Framework Based on Octrees [github ]\nPCL - 2D/3D image and point cloud processing [github ]\nTreexy - Brutally fast, sparse, 3D Voxel Grid [github ]\nvoxblox - Flexible voxel-based mapping focusing on truncated and Euclidean signed distance fields [github ]\nUtility Software\nGoxel - Free and open source 3D voxel editor [github ]\nOptimization\nCasADi - Symbolic framework for algorithmic differentiation and numeric optimization [github ]\nCeres Solver - Large scale nonlinear optimization library [github ]\neigen-qld - Interface to use the QLD QP solver with the Eigen3 library [github ]\nEXOTica - Generic optimisation toolset for robotics platforms [github ]\nhpipm - High-performance interior-point-method QP solvers (Ipopt, Snopt) [github ]\nHYPRE - Parallel solvers for sparse linear systems featuring multigrid methods [github ]\nifopt - An Eigen-based, light-weight C++ Interface to Nonlinear Programming Solvers (Ipopt, Snopt) [github ]\nIpopt - Large scale nonlinear optimization library [github ]\nlibcmaes - Blackbox stochastic optimization using the CMA-ES algorithm [github ]\nlimbo - Gaussian processes and Bayesian optimization of black-box functions [github ]\nlpsolvers - Linear Programming solvers in Python with a unified API [github ]\nNLopt - Nonlinear optimization [github ]\nOptimLib - Lightweight C++ library of numerical optimization methods for nonlinear functions [github ]\nOSQP - The Operator Splitting QP Solver [github ]\nPagmo - Scientific library for massively parallel optimization [github ]\npymoo - Multi-objective Optimization in Python [github ]\nqpsolvers - Quadratic Programming solvers in Python with a unified API [github ]\nRobOptim - Numerical Optimization for Robotics. [github ]\nSCS - Numerical optimization for solving large-scale convex cone problems [github ]\nsferes2 - Evolutionary computation [github ]\nRobot Modeling\nRobot Model Description Format\nSDF - XML format that describes objects and environments for robot simulators, visualization, and control (bitbucket)\nurdf - XML format for representing a robot model [github ]\nUtility to Build Robot Models\nphobos - Add-on for Blender creating URDF and SMURF robot models [github ]\nRobot Platform\nAutoRally - High-performance testbed for advanced perception and control research [github ]\nLinorobot - ROS compatible ground robots [github ]\nonine - Service Robot based on Linorobot and Braccio Arm [github ]\nRock - Software framework for robotic systems\nROS - Flexible framework for writing robot software [github repos]\nROS 2 - Version 2.0 of the Robot Operating System (ROS) software stack [github repos]\nYARP - Communication and device interfaces applicable from humanoids to embedded devices [github ]\nSLAM\nAprilSAM - Real-time smoothing and mapping [github ]\nCartographer - Real-time SLAM in 2D and 3D across multiple platforms and sensor configurations [github ]\nDSO - Novel direct and sparse formulation for Visual Odometry [github ]\nElasticFusion - Real-time dense visual SLAM system [github ]\nfiducials - Simultaneous localization and mapping using fiducial markers [github ]\nGTSAM - Smoothing and mapping (SAM) in robotics and vision [github ]\nKintinuous - Real-time large scale dense visual SLAM system [github ]\nLSD-SLAM - Real-time monocular SLAM [github ]\nORB-SLAM2 - Real-time SLAM library for Monocular, Stereo and RGB-D cameras [github ]\nRTAP-Map - RGB-D Graph SLAM approach based on a global Bayesian loop closure detector [github ]\nSRBA - Solving SLAM/BA in relative coordinates with flexibility for different submapping strategies [github ]\nSLAM Dataset\nAwesome SLAM Datasets\nVision\nViSP - Visual Servoing Platform [github ]\nFluid\nFluid Engine Dev - Jet - Fluid simulation engine for computer graphics applications [github ]\nMultiphysics\nKratos - Framework for building parallel multi-disciplinary simulation software [github ]\nMath\nFastor - Light-weight high performance tensor algebra framework in C++11/14/17 [github ]\nlinalg.h - Single header public domain linear algebra library for C++11 [github ]\nmanif - Small c++11 header-only library for Lie theory. [github ]\nSophus - Lie groups using Eigen [github ]\nSpaceVelAlg - Spatial vector algebra with the Eigen3 [github ]\nETC\nfuse - General architecture for performing sensor fusion live on a robot [github ]\nOther Awesome Lists\nAwesome Robotics (Kiloreux)\nAwesome Robotics (ahundt)\nAwesome Robotic Tooling\nAwesome Artificial Intelligence\nAwesome Collision Detection\nAwesome Computer Vision\nAwesome Machine Learning\nAwesome Deep Learning\nAwesome Gazebo\nAwesome Grasping\nAwesome Human Robot Interaction\nPythonRobotics - Python sample codes for robotics algorithms\nRobotics Coursework - A list of robotics courses you can take online\nContributing\nContributions are very welcome! Please read the contribution guidelines first. Also, please feel free to report any error.\nLicense", "link": "https://github.com/jslee02/awesome-robotics-libraries", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "awesome robotics libraries\na curated list of robotics simulators and libraries.\ntable of contents\nsimulators\nlibraries\ndynamics simulation\ninverse kinematics\nmachine learning\nmotion planning and control\noptimization\nrobot modeling\nrobot platform\nslam\nvision\nfluid\nmultiphysics\nmath\netc\nother awesome lists\ncontributing\nsimulators\nfree or open source\nai2-thor - python framework with a unity backend, providing interaction, navigation, and manipulation support for household based robotic agents [github ]\nairsim - simulator based on unreal engine for autonomous vehicles [github ]\nargos - physics-based simulator designed to simulate large-scale robot swarms [github ]\narte - matlab toolbox focussed on robotic manipulators [github ]\ncarla - open-source simulator for autonomous driving research [github ]\ncoppeliasim - formaly v-rep. virtual robot experimentation platform [github ]\ngazebo - dynamic multi-robot simulator [github ]\ngraspit! - simulator for grasping research that can accommodate arbitrary hand and robot designs [github ]\nhabitat-sim - simulation platform for research in embodied artificial intelligence [github ]\nhexapod robot simulator - open-source hexapod robot inverse kinematics and gaits visualizer [github ]\nignition gazebo - open source robotics simulator [github ]\nisaac - nvidia's virtual simulator for robots\nmorse - modular open robots simulation engine [github ]\nneurorobotics platform - internet-accessible simulation of robots controlled by spiking neural networks [bitbucket]\npybullet - an easy to use simulator for robotics and deep reinforcement learning [github ]\nrobot gui - a three.js based 3d robot interface [github ]\nsimbad - a java 3d robot simulator, enables to write own robot controller with modifying environment using available sensors.\nunity - popular game engine that now offers open-source tools, tutorials, and resources for robotics simulation [github ]\nwebots - robot simulator that provides a complete development environment [github ]\ncommercial\nactin simulation\nartiminds - planning, programming, operation, analysis and optimization\nkineo - path planning and trajectory optimization for industrial robotics and digital mock-up review applications\nrobotdk - simulation and olp for robots\nrobotstudio\nrobot virtual worlds\nvirtual robotics toolkit\nvisual components\ncloud\naws robomaker - service that makes it easy to develop, test, and deploy intelligent robotics applications at scale\nlibraries\ndynamics simulation\n\u26a0\ufe0f the following table is not complete. please feel free to report if you find something incorrect or missing.\nname models features languages licenses code popularity\narcsim soft c++\nbullet rigid, soft ik, id, urdf, sdf c++, python zlib github\nchrono::engine rigid, soft, granular, fluid ik, urdf c++, python bsd-3-clause github\ndart rigid, soft ik, id, plan, urdf, sdf c++, python bsd-2-clause github\ndrake rigid, aero, fluid ik, trj-opt, plan c++, matlab bsd-3-clause github\nflex rigid, soft, particle, fluid c++ github\nfrost rigid matlab bsd-3-clause github\nibds rigid, particle c++ zlib\nidyntree rigid id c++, python, matlab, lua lgpl-2.1 github\nkdl rigid ik c++ lgpl-2.1 github\nkindr rigid (todo) c++, matlab bsd-3-clause github\nklampt (todo) (todo) c++, python bsd-3-clause github\nlibrepilot uav, vehicles (todo) c++ gpl-3.0 bitbucket, github\nmars (todo) (todo) c++, python lgpl-3.0 github\nmbdyn (todo) (todo) c++ gpl-2.1 download\nmbsim (todo) (todo) c++ (not specified) github\nmbslib (todo) (todo) c++ lgpl-3.0 github\nmetapod (todo) (todo) c++ lgpl-3.0 github\nmoby rigid id c++ gpl-2.0 github\nmrpt vehicle slam, cv c++, python, matlab bsd-3-clause github\nmujoco (todo) id c++, python licenses closed source\nmvsim vehicle (todo) c++ gpl-3.0 github\nnewton dynamics (todo) (todo) c++ zlib github\nnphysics (todo) (todo) rust bsd-3-clause github\node rigid c++ lgpl-2.1 or bsd-3-clause bitbucket\nopenrave (todo) (todo) c++, python lgpl-3.0 github\npinocchio rigid ik, id, urdf, analytical derivatives, code generation c++, python bsd-2-clause github\npositionbaseddynamics (todo) (todo) c++ mit github\nphysx (todo) (todo) c++ unknown github\npydy (todo) (todo) python bsd-3-clause github\nrbdl rigid ik,id,urdf c++, python zlib github\nrbdyn rigid (todo) c++, python lgpl-3.0 github\nraisim (todo) (todo) c++ custom github\nreactphysics3d (todo) (todo) c++ zlib github\nrigidbodydynamics.jl rigid (todo) julia mit \"expat\" github\nrigs of rods rigid, soft, vehicle (todo) c++ gpl-3.0 github\nrobopy (todo) (todo) python 3 mit github\nrobotics library (todo) (todo) c++ gpl-3.0 or bsd-2-clause github\nrobwork (todo) (todo) c++ apache-2.0 gitlab\nsiconos (todo) (todo) c++, python apache-2.0 github\nsimbody rigid, molecules id, urdf c++ apache-2.0 github\nsofa rigid, soft, medical (todo) c++ lgpl-2.1 github\ntiny differentiable simulator rigid (todo) c++, python apache-2.0 github\ntrep rigid dm, trj-opt c, python gpl-3.0 github\nqu3e rigid - c++ zlib github\nfor simplicity, shortened names are used to represent the supported models and features as\nsupported models\nrigid: rigid bodies\nsoft: soft bodies\naero: aerodynamics\ngranular: granular materials (like sand)\nfluid: fluid dynamics\nvehicles\nuav: unmanned aerial vehicles (like drones)\nmedical\nmolecules\nparallel: parallel mechanism (like stewart platform)\nfeatures on simulation, analysis, planning, control design\ndm: discrete mechanics\nik: inverse kinematics solvers (please find ik specialized packages in this list)\nid: inverse dynamics\nslam: simultaneous localization and mapping\ntrj-opt: trajectory optimization\nplan: motion planning algorithms\ncv: computer vision\nurdf: urdf parser\nsdf: sdf parser\ninverse kinematics\nikbt - a python package to solve robot arm inverse kinematics in symbolic form [github ]\nrelaxedik - real-time synthesis of accurate and feasible robot arm motion [github ]\ntrip - a python package that solves inverse kinematics of parallel-, serial- or hybrid-robots [github ]\nmachine learning\nallenact - python/pytorch-based research framework for embodied ai [github ]\ndll - deep learning library (dll) for c++ [github ]\ndynet - the dynamic neural network toolkit [github ]\nfido - lightweight c++ machine learning library for embedded electronics and robotics [github ]\nminidnn - a header-only c++ library for deep neural networks [github ]\nmlpack - scalable c++ machine learning library [github ]\nopenai gym - developing and comparing reinforcement learning algorithms [github ]\ngym-dart [github ]\ngym-gazebo [github ]\nrllib - temporal-difference learning algorithms in reinforcement learning [github ]\ntiny-dnn - header only, dependency-free deep learning framework in c++14 [github ]\nmotion planning and control\naikido - solving robotic motion planning and decision making problems. [github ]\ncuiksuite - applications to solve position analysis and path planning problems\ncontrol toolbox - open-source c++ library for robotics, optimal and model predictive control [github ]\ncrocoddyl - optimal control library for robot control under contact sequence [github ]\ngpmp2 - gaussian process motion planner 2 [github ]\nhpp - path planning for kinematic chains in environments cluttered with obstacles [github]\nmoveit! - motion planning framework [github ]\nompl - open motion planning library [bitbucket, github ]\nocs2 - efficient continuous and discrete time optimal control implementation [bitbucket]\npymanoid - humanoid robotics prototyping environment based on openrave [github ]\nros behavior -----> tree !!!  - [github ]\nruckig - real-time, time-optimal and jerk-constrained online trajectory generation. [github ]\nthe kautham project - a robot simulation toolkit for motion planning [github ]\ntopp-ra - time-parameterizing robot trajectories subject to kinematic and dynamic constraints [github ]\nmotion optimizer\ntopico - time-optimal trajectory generation and control [github ]\ntowr - a light-weight, eigen-based c++ library for trajectory optimization for legged robots [github ]\ntrajectoryoptimization - a fast trajectory optimization library written in julia [github ]\ntrajopt - framework for generating robot trajectories by local optimization [github ]\nnearest neighbor\ncover-tree - cover tree data structure for quick k-nearest-neighbor search [github ]\nfaster cover trees by mike izbicki et al., icml 2015.\nflann - fast library for approximate nearest neighbors [github ]\nnanoflann - nearest neighbor search with kd-trees [github ]\n3d mapping\nlibpointmatcher - iterative closest point library for 2-d/3-d mapping in robotics [github ]\noctree - fast radius neighbor search with an octree [github ]\noctomap - efficient probabilistic 3d mapping framework based on octrees [github ]\npcl - 2d/3d image and point cloud processing [github ]\ntreexy - brutally fast, sparse, 3d voxel grid [github ]\nvoxblox - flexible voxel-based mapping focusing on truncated and euclidean signed distance fields [github ]\nutility software\ngoxel - free and open source 3d voxel editor [github ]\noptimization\ncasadi - symbolic framework for algorithmic differentiation and numeric optimization [github ]\nceres solver - large scale nonlinear optimization library [github ]\neigen-qld - interface to use the qld qp solver with the eigen3 library [github ]\nexotica - generic optimisation toolset for robotics platforms [github ]\nhpipm - high-performance interior-point-method qp solvers (ipopt, snopt) [github ]\nhypre - parallel solvers for sparse linear systems featuring multigrid methods [github ]\nifopt - an eigen-based, light-weight c++ interface to nonlinear programming solvers (ipopt, snopt) [github ]\nipopt - large scale nonlinear optimization library [github ]\nlibcmaes - blackbox stochastic optimization using the cma-es algorithm [github ]\nlimbo - gaussian processes and bayesian optimization of black-box functions [github ]\nlpsolvers - linear programming solvers in python with a unified api [github ]\nnlopt - nonlinear optimization [github ]\noptimlib - lightweight c++ library of numerical optimization methods for nonlinear functions [github ]\nosqp - the operator splitting qp solver [github ]\npagmo - scientific library for massively parallel optimization [github ]\npymoo - multi-objective optimization in python [github ]\nqpsolvers - quadratic programming solvers in python with a unified api [github ]\nroboptim - numerical optimization for robotics. [github ]\nscs - numerical optimization for solving large-scale convex cone problems [github ]\nsferes2 - evolutionary computation [github ]\nrobot modeling\nrobot model description format\nsdf - xml format that describes objects and environments for robot simulators, visualization, and control (bitbucket)\nurdf - xml format for representing a robot model [github ]\nutility to build robot models\nphobos - add-on for blender creating urdf and smurf robot models [github ]\nrobot platform\nautorally - high-performance testbed for advanced perception and control research [github ]\nlinorobot - ros compatible ground robots [github ]\nonine - service robot based on linorobot and braccio arm [github ]\nrock - software framework for robotic systems\nros - flexible framework for writing robot software [github repos]\nros 2 - version 2.0 of the robot operating system (ros) software stack [github repos]\nyarp - communication and device interfaces applicable from humanoids to embedded devices [github ]\nslam\naprilsam - real-time smoothing and mapping [github ]\ncartographer - real-time slam in 2d and 3d across multiple platforms and sensor configurations [github ]\ndso - novel direct and sparse formulation for visual odometry [github ]\nelasticfusion - real-time dense visual slam system [github ]\nfiducials - simultaneous localization and mapping using fiducial markers [github ]\ngtsam - smoothing and mapping (sam) in robotics and vision [github ]\nkintinuous - real-time large scale dense visual slam system [github ]\nlsd-slam - real-time monocular slam [github ]\norb-slam2 - real-time slam library for monocular, stereo and rgb-d cameras [github ]\nrtap-map - rgb-d graph slam approach based on a global bayesian loop closure detector [github ]\nsrba - solving slam/ba in relative coordinates with flexibility for different submapping strategies [github ]\nslam dataset\nawesome slam datasets\nvision\nvisp - visual servoing platform [github ]\nfluid\nfluid engine dev - jet - fluid simulation engine for computer graphics applications [github ]\nmultiphysics\nkratos - framework for building parallel multi-disciplinary simulation software [github ]\nmath\nfastor - light-weight high performance tensor algebra framework in c++11/14/17 [github ]\nlinalg.h - single header public domain linear algebra library for c++11 [github ]\nmanif - small c++11 header-only library for lie theory. [github ]\nsophus - lie groups using eigen [github ]\nspacevelalg - spatial vector algebra with the eigen3 [github ]\netc\nfuse - general architecture for performing sensor fusion live on a robot [github ]\nother awesome lists\nawesome robotics (kiloreux)\nawesome robotics (ahundt)\nawesome robotic tooling\nawesome artificial intelligence\nawesome collision detection\nawesome computer vision\nawesome machine learning\nawesome deep learning\nawesome gazebo\nawesome grasping\nawesome human robot interaction\npythonrobotics - python sample codes for robotics algorithms\nrobotics coursework - a list of robotics courses you can take online\ncontributing\ncontributions are very welcome! please read the contribution guidelines first. also, please feel free to report any error.\nlicense", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000033, "year": null}, {"Unnamed: 0": 1063, "autor": 43, "date": null, "content": "BehaviorTree.CPP\nThis C++ 14 library provides a framework to create BehaviorTrees. It was designed to be flexible, easy to use, reactive and fast.\nEven if our main use-case is robotics, you can use this library to build AI for games, or to replace Finite State Machines in your application.\nThere are few features that make BehaviorTree.CPP unique, when compared to other implementations:\nIt makes asynchronous Actions, i.e. non-blocking, a first-class citizen.\nYou can build reactive behaviors that execute multiple Actions concurrently.\nTrees are defined using a Domain Specific Scripting scripting language (based on XML), and can be loaded at run-time; in other words, even if written in C++, Trees are not hard-coded.\nYou can statically link your custom TreeNodes or convert them into plugins which can be loaded at run-time.\nIt provides a type-safe and flexible mechanism to do Dataflow between Nodes of the Tree.\nIt includes a logging/profiling infrastructure that allows the user to visualize, record, replay and analyze state transitions.\nLast but not least: it is well documented!\nDocumentation\nYou can learn about the main concepts, the API and the tutorials here: https://www.behaviortree.dev/\nTo find more details about the conceptual ideas that make this implementation different from others, you can read the final deliverable of the project MOOD2Be.\nDoes your company use BehaviorTree.CPP?\nNo company, institution or public/private funding is currently supporting the development of BehaviorTree.CPP and Groot. As a consequence, my time to support BehaviorTree.CPP is very limited and I decided won't spend any time at all supporting Groot. Pull Requests are welcome and will be reviewed, even if with some delay.\nIf your company use this software, consider becoming a sponsor to support bug fixing and development of new features. You can find contact details in package.xml.\nDesign principles\nThe main goal of this project is to create a Behavior Tree implementation that uses the principles of Model Driven Development to separate the role of the Component Developer from the Behavior Designer.\nIn practice, this means that:\nCustom TreeNodes must be reusable building blocks. You should be able to implement them once and reuse them to build many behaviors.\nTo build a Behavior Tree out of TreeNodes, the Behavior Designer must not need to read nor modify the C++ source code of a given TreeNode.\nComplex Behaviours must be composable using Subtrees\nMany of the features and, sometimes, the apparent limitations of this library, might be a consequence of this design principle.\nFor instance, having a scoped BlackBoard, visible only in a portion of the tree, is particularly important to avoid \"name pollution\" and allow the creation of large scale trees.\nGUI Editor\nEditing a BehaviorTree is as simple as editing a XML file in your favourite text editor.\nIf you are looking for a more fancy graphical user interface (and I know you do) check Groot out.\nWatch Groot and BehaviorTree.CPP in action\nClick on the following image to see a short video of how the C++ library and the graphic user interface are used to design and monitor a Behavior Tree.\nHow to compile (plain old cmake)\nOn Ubuntu, you are encourage to install the following dependencies:\nsudo apt-get install libzmq3-dev libboost-dev\nOther dependencies are already included in the 3rdparty folder.\nTo compile and install the library, from the BehaviorTree.CPP folder, execute:\nmkdir build; cd build\ncmake ..\nmake\nsudo make install\nIf you want to use BT.CPp in your application a typical CMakeLists.txt file will look like this:\ncmake_minimum_required(VERSION 3.5)\nproject(hello_BT)\nset(CMAKE_CXX_STANDARD 14)\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\nfind_package(BehaviorTreeV3)\nadd_executable(${PROJECT_NAME} \"hello_BT.cpp\")\ntarget_link_libraries(${PROJECT_NAME} BT::behaviortree_cpp_v3)\nROS1 or ROS2 users (Catkin/Ament)\nYou can easily install the package with the command\nsudo apt-get install ros-$ROS_DISTRO-behaviortree-cpp-v3\nIf you want to compile it with catkin, you must include this package to your catkin workspace.\nAcknowledgement\nThis library was initially developed at Eurecat - https://eurecat.org/en/ (main author, Davide Faconti) in a joint effort with the Italian Institute of Technology (Michele Colledanchise).\nThis software is one of the main components of MOOD2Be, which is one of the six Integrated Technical Projects (ITPs) selected from the RobMoSys first open call. Therefore, MOOD2Be has been supported by the European Horizon2020 project RobMoSys. This software is RobMoSys conformant.\nFurther readings\nIntroductory article: Behavior trees for AI: How they work\nHow Behavior Trees Modularize Hybrid Control Systems and Generalize Sequential Behavior Compositions, the Subsumption Architecture, and Decision Trees. Michele Colledanchise and Petter Ogren. IEEE Transaction on Robotics 2017.\nBehavior Trees in Robotics and AI, published by CRC Press Taylor & Francis, available for purchase (ebook and hardcover) on the CRC Press Store or Amazon.\nThe Preprint version (free) is available here: https://arxiv.org/abs/1709.00084\nLicense\nThe MIT License (MIT)\nCopyright (c) 2014-2018 Michele Colledanchise Copyright (c) 2018-2021 Davide Faconti\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.", "link": "https://github.com/BehaviorTree/BehaviorTree.CPP", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "behaviortree.cpp\nthis c++ 14 library provides a framework to create behaviortrees. it was designed to be flexible, easy to use, reactive and fast.\neven if our main use-case is robotics, you can use this library to build ai for games, or to replace finite state machines in your application.\nthere are few features that make behaviortree.cpp unique, when compared to other implementations:\nit makes asynchronous actions, i.e. non-blocking, a first-class citizen.\nyou can build reactive behaviors that execute multiple actions concurrently.\ntrees are defined using a domain specific scripting scripting language (based on xml), and can be loaded at run-time; in other words, even if written in c++, trees are not hard-coded.\nyou can statically link your custom treenodes or convert them into plugins which can be loaded at run-time.\nit provides a type-safe and flexible mechanism to do dataflow between nodes of the -----> tree !!! .\nit includes a logging/profiling infrastructure that allows the user to visualize, record, replay and analyze state transitions.\nlast but not least: it is well documented!\ndocumentation\nyou can learn about the main concepts, the api and the tutorials here: https://www.behaviortree.dev/\nto find more details about the conceptual ideas that make this implementation different from others, you can read the final deliverable of the project mood2be.\ndoes your company use behaviortree.cpp?\nno company, institution or public/private funding is currently supporting the development of behaviortree.cpp and groot. as a consequence, my time to support behaviortree.cpp is very limited and i decided won't spend any time at all supporting groot. pull requests are welcome and will be reviewed, even if with some delay.\nif your company use this software, consider becoming a sponsor to support bug fixing and development of new features. you can find contact details in package.xml.\ndesign principles\nthe main goal of this project is to create a behavior tree implementation that uses the principles of model driven development to separate the role of the component developer from the behavior designer.\nin practice, this means that:\ncustom treenodes must be reusable building blocks. you should be able to implement them once and reuse them to build many behaviors.\nto build a behavior tree out of treenodes, the behavior designer must not need to read nor modify the c++ source code of a given treenode.\ncomplex behaviours must be composable using subtrees\nmany of the features and, sometimes, the apparent limitations of this library, might be a consequence of this design principle.\nfor instance, having a scoped blackboard, visible only in a portion of the tree, is particularly important to avoid \"name pollution\" and allow the creation of large scale trees.\ngui editor\nediting a behaviortree is as simple as editing a xml file in your favourite text editor.\nif you are looking for a more fancy graphical user interface (and i know you do) check groot out.\nwatch groot and behaviortree.cpp in action\nclick on the following image to see a short video of how the c++ library and the graphic user interface are used to design and monitor a behavior tree.\nhow to compile (plain old cmake)\non ubuntu, you are encourage to install the following dependencies:\nsudo apt-get install libzmq3-dev libboost-dev\nother dependencies are already included in the 3rdparty folder.\nto compile and install the library, from the behaviortree.cpp folder, execute:\nmkdir build; cd build\ncmake ..\nmake\nsudo make install\nif you want to use bt.cpp in your application a typical cmakelists.txt file will look like this:\ncmake_minimum_required(version 3.5)\nproject(hello_bt)\nset(cmake_cxx_standard 14)\nset(cmake_cxx_standard_required on)\nfind_package(behaviortreev3)\nadd_executable(${project_name} \"hello_bt.cpp\")\ntarget_link_libraries(${project_name} bt::behaviortree_cpp_v3)\nros1 or ros2 users (catkin/ament)\nyou can easily install the package with the command\nsudo apt-get install ros-$ros_distro-behaviortree-cpp-v3\nif you want to compile it with catkin, you must include this package to your catkin workspace.\nacknowledgement\nthis library was initially developed at eurecat - https://eurecat.org/en/ (main author, davide faconti) in a joint effort with the italian institute of technology (michele colledanchise).\nthis software is one of the main components of mood2be, which is one of the six integrated technical projects (itps) selected from the robmosys first open call. therefore, mood2be has been supported by the european horizon2020 project robmosys. this software is robmosys conformant.\nfurther readings\nintroductory article: behavior trees for ai: how they work\nhow behavior trees modularize hybrid control systems and generalize sequential behavior compositions, the subsumption architecture, and decision trees. michele colledanchise and petter ogren. ieee transaction on robotics 2017.\nbehavior trees in robotics and ai, published by crc press taylor & francis, available for purchase (ebook and hardcover) on the crc press store or amazon.\nthe preprint version (free) is available here: https://arxiv.org/abs/1709.00084\nlicense\nthe mit license (mit)\ncopyright (c) 2014-2018 michele colledanchise copyright (c) 2018-2021 davide faconti\npermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"software\"), to deal in the software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the software, and to permit persons to whom the software is furnished to do so, subject to the following conditions:\nthe above copyright notice and this permission notice shall be included in all copies or substantial portions of the software.\nthe software is provided \"as is\", without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose and noninfringement. in no event shall the authors or copyright holders be liable for any claim, damages or other liability, whether in an action of contract, tort or otherwise, arising from, out of or in connection with the software or the use or other dealings in the software.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000043, "year": null}, {"Unnamed: 0": 1097, "autor": 77, "date": null, "content": "Robot-Centric Elevation Mapping\nOverview\nThis is a ROS package developed for elevation mapping with a mobile robot. The software is designed for (local) navigation tasks with robots which are equipped with a pose estimation (e.g. IMU & odometry) and a distance sensor (e.g. structured light (Kinect, RealSense), laser range sensor, stereo camera). The provided elevation map is limited around the robot and reflects the pose uncertainty that is aggregated through the motion of the robot (robot-centric mapping). This method is developed to explicitly handle drift of the robot pose estimation.\nThis is research code, expect that it changes often and any fitness for a particular purpose is disclaimed.\nThe source code is released under a BSD 3-Clause license.\nAuthor: P\u00e9ter Fankhauser\nCo-Author: Maximilian Wulf\nAffiliation: ANYbotics\nMaintainer: Maximilian Wulf, mwulf@anybotics.com, Magnus G\u00e4rtner, mgaertner@anybotics.com\nThis projected was initially developed at ETH Zurich (Autonomous Systems Lab & Robotic Systems Lab).\nThis work is conducted as part of ANYmal Research, a community to advance legged robotics.\nVideos of the elevation mapping software in use:\nCiting\nThe robot-centric elevation mapping methods used in this software are described in the following paper (available here). If you use this work in an academic context, please cite the following publication(s):\nP. Fankhauser, M. Bloesch, and M. Hutter, \"Probabilistic Terrain Mapping for Mobile Robots with Uncertain Localization\", in IEEE Robotics and Automation Letters (RA-L), vol. 3, no. 4, pp. 3019\u20133026, 2018. (PDF)\n@article{Fankhauser2018ProbabilisticTerrainMapping,\nauthor = {Fankhauser, P{\\'{e}}ter and Bloesch, Michael and Hutter, Marco},\ndoi = {10.1109/LRA.2018.2849506},\ntitle = {Probabilistic Terrain Mapping for Mobile Robots with Uncertain Localization},\njournal = {IEEE Robotics and Automation Letters (RA-L)},\nvolume = {3},\nnumber = {4},\npages = {3019--3026},\nyear = {2018}\n}\nP. Fankhauser, M. Bloesch, C. Gehring, M. Hutter, and R. Siegwart, \"Robot-Centric Elevation Mapping with Uncertainty Estimates\", in International Conference on Climbing and Walking Robots (CLAWAR), 2014. (PDF)\n@inproceedings{Fankhauser2014RobotCentricElevationMapping,\nauthor = {Fankhauser, P\\'{e}ter and Bloesch, Michael and Gehring, Christian and Hutter, Marco and Siegwart, Roland},\ntitle = {Robot-Centric Elevation Mapping with Uncertainty Estimates},\nbooktitle = {International Conference on Climbing and Walking Robots (CLAWAR)},\nyear = {2014}\n}\nInstallation\nDependencies\nThis software is built on the Robotic Operating System (ROS), which needs to be installed first. Additionally, the Robot-Centric Elevation Mapping depends on following software:\nGrid Map (grid map library for mobile robots)\nkindr (kinematics and dynamics library for robotics),\nkindr_ros (ROS wrapper for kindr),\nPoint Cloud Library (PCL) (point cloud processing),\nEigen (linear algebra library).\nBuilding\nIn order to install the Robot-Centric Elevation Mapping, clone the latest version from this repository into your catkin workspace and compile the package using ROS.\ncd catkin_workspace/src\ngit clone https://github.com/anybotics/elevation_mapping.git\ncd ../\ncatkin config --cmake-args -DCMAKE_BUILD_TYPE=Release\ncatkin build\nUnit Tests\nBuild tests with\nroscd elevation_mapping\ncatkin build --catkin-make-args run_tests -- --this\nRun the tests with\nrostest elevation_mapping elevation_mapping.test -t\nBasic Usage\nIn order to get the Robot-Centric Elevation Mapping to run with your robot, you will need to adapt a few parameters. It is the easiest if duplicate and adapt all the parameter files that you need to change from the elevation_mapping_demos package (e.g. the simple_demo example). These are specifically the parameter files in config and the launch file from the launch folder.\nTurtleBot3 Waffle Simulation\nA running example is provided, making use of the Turtlebot3 simulation environment. This example can be used to test elevation mapping, as a starting point for further integration.\nTo start with, the Turtlebot3 simulation dependencies need to be installed:\nsudo apt install ros-melodic-turtlebot3*\nThe elevation mapping demo together with the turtlebot3 simulation can be started with\nroslaunch elevation_mapping_demos turtlesim3_waffle_demo.launch\nTo control the robot with a keyboard, a new terminal window needs to be opened (remember to source your ROS environment). Then run\nexport TURTLEBOT3_MODEL=waffle\nroslaunch turtlebot3_teleop turtlebot3_teleop_key.launch\nVelocity inputs can be sent to the robot by pressing the keys a, w,d, x. To stop the robot completely, press s.\nSimple Demo & Ground Truth Demo\nA .ply is published as static pointcloud, elevation_mapping subscribes to it and publishes the elevation map. You can visualize it through rviz. For visualization, select /elevation_mapping/elevation_map_raw.\nNote. You might need to toggle the visibility of the grid_map_plugin to visualize it.\nroslaunch elevation_mapping_demos ground_truth_demo.launch\nWhile ground truth demo estimates the height in map frame, simple demo sets up a more realistic deployment scenario. Here, the elevation_map is configured to track a base frame. To get started, we suggest to play around and also visualize other published topics, such as /elevation_mapping/elevation_map_raw and change the height layer to another layer, e.g elevation_inpainted.\nNodes\nNode: elevation_mapping\nThis is the main Robot-Centric Elevation Mapping node. It uses the distance sensor measurements and the pose and covariance of the robot to generate an elevation map with variance estimates.\nSubscribed Topics\n/points (sensor_msgs/PointCloud2)\nThe distance measurements.\n/pose (geometry_msgs/PoseWithCovarianceStamped)\nThe robot pose and covariance.\n/tf (tf/tfMessage)\nThe transformation tree.\nPublished Topics\nelevation_map (grid_map_msgs/GridMap)\nThe entire (fused) elevation map. It is published periodically (see fused_map_publishing_rate parameter) or after the trigger_fusion service is called.\nelevation_map_raw (grid_map_msgs/GridMap)\nThe entire (raw) elevation map before the fusion step.\nServices\ntrigger_fusion (std_srvs/Empty)\nTrigger the fusing process for the entire elevation map and publish it. For example, you can trigger the map fusion step from the console with\nrosservice call /elevation_mapping/trigger_fusion\nget_submap (grid_map_msgs/GetGridMap)\nGet a fused elevation submap for a requested position and size. For example, you can get the fused elevation submap at position (-0.5, 0.0) and size (0.5, 1.2) described in the odom frame and save it to a text file form the console with\nrosservice call -- /elevation_mapping/get_submap odom -0.5 0.0 0.5 1.2 []\nget_raw_submap (grid_map_msgs/GetGridMap)\nGet a raw elevation submap for a requested position and size. For example, you can get the raw elevation submap at position (-0.5, 0.0) and size (0.5, 1.2) described in the odom frame and save it to a text file form the console with\nrosservice call -- /elevation_mapping/get_raw_submap odom -0.5 0.0 0.5 1.2 []\nclear_map (std_srvs/Empty)\nInitiates clearing of the entire map for resetting purposes. Trigger the map clearing with\nrosservice call /elevation_mapping/clear_map\nmasked_replace ([grid_map_msgs/SetGridMap])\nAllows for setting the individual layers of the elevation map through a service call. The layer mask can be used to only set certain cells and not the entire map. Cells containing NAN in the mask are not set, all the others are set. If the layer mask is not supplied, the entire map will be set in the intersection of both maps. The provided map can be of different size and position than the map that will be altered. An example service call to set some cells marked with a mask in the elevation layer to 0.5 is\nrosservice call /elevation_mapping/masked_replace \"map:\ninfo:\nheader:\nseq: 3\nstamp: {secs: 3, nsecs: 80000000}\nframe_id: 'odom'\nresolution: 0.1\nlength_x: 0.3\nlength_y: 0.3\npose:\nposition: {x: 5.0, y: 0.0, z: 0.0}\norientation: {x: 0.0, y: 0.0, z: 0.0, w: 0.0}\nlayers: [elevation,mask]\nbasic_layers: [elevation]\ndata:\n- layout:\ndim:\n- {label: 'column_index', size: 3, stride: 9}\n- {label: 'row_index', size: 3, stride: 3}\ndata_offset: 0\ndata: [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n- layout:\ndim:\n- {label: 'column_index', size: 3, stride: 9}\n- {label: 'row_index', size: 3, stride: 3}\ndata_offset: 0\ndata: [0, 0, 0, .NAN, .NAN, .NAN, 0, 0, 0]\nouter_start_index: 0\ninner_start_index: 0\"\nsave_map (grid_map_msgs/ProcessFile)\nSaves the current fused grid map and raw grid map to rosbag files. Field topic_name must be a base name, i.e. no leading slash character (/). If field topic_name is empty, then elevation_map is used per default. Example with default topic name\nrosservice call /elevation_mapping/save_map \"file_path: '/home/integration/elevation_map.bag' topic_name: ''\"\nload_map (grid_map_msgs/ProcessFile)\nLoads the fused grid map and raw grid map from rosbag files. Field topic_name must be a base name, i.e. no leading slash character (/). If field topic_name is empty, then elevation_map is used per default. Example with default topic name\nrosservice call /elevation_mapping/load_map \"file_path: '/home/integration/elevation_map.bag' topic_name: ''\"\ndisable_updates (std_srvs/Empty)\nStops updating the elevation map with sensor input. Trigger the update stopping with\nrosservice call /elevation_mapping/disable_updates {}\nenable_updates (std_srvs/Empty)\nStart updating the elevation map with sensor input. Trigger the update starting with\nrosservice call /elevation_mapping/enable_updates {}\nParameters\nDEPRECATED point_cloud_topic (string, default: \"/points\")\nThe name of the distance measurements topic. Use input_sources instead.\ninput_sources (list of input sources, default: none)\nHere you specify your inputs to elevation mapping, currently \"pointcloud\" inputs are supported.\nExample configuration:\ninput_sources:\nfront: # A name to identify the input source\ntype: pointcloud # Supported types: pointcloud\ntopic: /lidar_front/depth/points\nqueue_size: 1\npublish_on_update: true # Wheter to publish the elevation map after a callback from this source.\nrear:\ntype: pointcloud\ntopic: /lidar_rear/depth/points\nqueue_size: 5\npublish_on_update: false\nNo input sources can be configured with an empty array:\ninput_sources: []\nrobot_pose_topic (string, default: \"/robot_state/pose\")\nThe name of the robot pose and covariance topic.\nbase_frame_id (string, default: \"/robot\")\nThe id of the robot base tf frame.\nmap_frame_id (string, default: \"/map\")\nThe id of the tf frame of the elevation map.\ntrack_point_frame_id (string, default: \"/robot\")\nThe elevation map is moved along with the robot following a track point. This is the id of the tf frame in which the track point is defined.\ntrack_point_x, track_point_y, track_point_z (double, default: 0.0, 0.0, 0.0)\nThe elevation map is moved along with the robot following a track point. This is the position of the track point in the track_point_frame_id.\nrobot_pose_cache_size (int, default: 200, min: 0)\nThe size of the robot pose cache.\nmin_update_rate (double, default: 2.0)\nThe mininum update rate (in Hz) at which the elevation map is updated either from new measurements or the robot pose estimates.\nfused_map_publishing_rate (double, default: 1.0)\nThe rate for publishing the entire (fused) elevation map.\nrelocate_rate (double, default: 3.0)\nThe rate (in Hz) at which the elevation map is checked for relocation following the tracking point.\nlength_in_x, length_in_y (double, default: 1.5, min: 0.0)\nThe size (in m) of the elevation map.\nposition_x, position_y (double, default: 0.0)\nThe position of the elevation map center, in the elevation map frame. This parameter sets the planar position offsets between the generated elevation map and the frame in which it is published (map_frame_id). It is only useful if no track_point_frame_id parameter is used.\nresolution (double, default: 0.01, min: 0.0)\nThe resolution (cell size in m/cell) of the elevation map.\nmin_variance, max_variance (double, default: 9.0e-6, 0.01)\nThe minimum and maximum values for the elevation map variance data.\nmahalanobis_distance_threshold (double, default: 2.5)\nEach cell in the elevation map has an uncertainty for its height value. Depending on the Mahalonobis distance of the existing height distribution and the new measurements, the incoming data is fused with the existing estimate, overwritten, or ignored. This parameter determines the threshold on the Mahalanobis distance which determines how the incoming measurements are processed.\nsensor_processor/ignore_points_above (double, default: inf) A hard threshold on the height of points introduced by the depth sensor. Points with a height over this threshold will not be considered valid during the data collection step.\nsensor_processor/ignore_points_below (double, default: -inf) A hard threshold on the height of points introduced by the depth sensor. Points with a height below this threshold will not be considered valid during the data collection step.\nmulti_height_noise (double, default: 9.0e-7)\nNoise added to measurements that are higher than the current elevation map at that particular position. This noise-adding process is only performed if a point falls over the Mahalanobis distance threshold. A higher value is useful to adapt faster to dynamic environments (e.g., moving objects), but might cause more noise in the height estimation.\nmin_horizontal_variance, max_horizontal_variance (double, default: pow(resolution / 2.0, 2), 0.5)\nThe minimum and maximum values for the elevation map horizontal variance data.\nenable_visibility_cleanup (bool, default: true)\nEnable/disable a separate thread that removes elements from the map which are not visible anymore, by means of ray-tracing, originating from the sensor frame.\nvisibility_cleanup_rate (double, default: 1.0)\nThe rate (in Hz) at which the visibility clean-up is performed.\nenable_continuous_cleanup (bool, default: false)\nEnable/disable a continuous clean-up of the elevation map. If enabled, on arrival of each new sensor data the elevation map will be cleared and filled up only with the latest data from the sensor. When continuous clean-up is enabled, visibility clean-up will automatically be disabled since it is not needed in this case.\nnum_callback_threads (int, default: 1, min: 1) The number of threads to use for processing callbacks. More threads results in higher throughput, at cost of more resource usage.\npostprocessor_pipeline_name (string, default: postprocessor_pipeline)\nThe name of the pipeline to execute for postprocessing. It expects a pipeline configuration to be loaded in the private namespace of the node under this name. E.g.:\n<node pkg=\"elevation_mapping\" type=\"elevation_mapping\" name=\"elevation_mapping\" output=\"screen\">\n...\n<rosparam command=\"load\" file=\"$(find elevation_mapping_demos)/config/postprocessor_pipeline.yaml\" />\n</node>\nA pipeline is a grid_map_filter chain, see grid_map_demos/filters_demo.yaml and ros / filters for more information.\npostprocessor_num_threads (int, default: 1, min: 1)\nThe number of threads to use for asynchronous postprocessing. More threads results in higher throughput, at cost of more resource usage.\nscanning_duration (double, default: 1.0)\nThe sensor's scanning duration (in s) which is used for the visibility cleanup. Set this roughly to the duration it takes between two consecutive full scans (e.g. 0.033 for a ToF camera with 30 Hz, or 3 s for a rotating laser scanner). Depending on how dense or sparse your scans are, increase or reduce the scanning duration. Smaller values lead to faster dynamic object removal and bigger values help to reduce faulty map cleanups.\nsensor_cutoff_min_depth, sensor_cutoff_max_depth (double, default: 0.2, 2.0)\nThe minimum and maximum values for the length of the distance sensor measurements. Measurements outside this interval are ignored.\nsensor_model_normal_factor_a, sensor_model_normal_factor_b, sensor_model_normal_factor_c, sensor_model_lateral_factor (double)\nThe data for the sensor noise model.\nChangelog\nSee Changelog\nBugs & Feature Requests\nPlease report bugs and request features using the Issue Tracker.", "link": "https://github.com/ANYbotics/elevation_mapping", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "robot-centric elevation mapping\noverview\nthis is a ros package developed for elevation mapping with a mobile robot. the software is designed for (local) navigation tasks with robots which are equipped with a pose estimation (e.g. imu & odometry) and a distance sensor (e.g. structured light (kinect, realsense), laser range sensor, stereo camera). the provided elevation map is limited around the robot and reflects the pose uncertainty that is aggregated through the motion of the robot (robot-centric mapping). this method is developed to explicitly handle drift of the robot pose estimation.\nthis is research code, expect that it changes often and any fitness for a particular purpose is disclaimed.\nthe source code is released under a bsd 3-clause license.\nauthor: p\u00e9ter fankhauser\nco-author: maximilian wulf\naffiliation: anybotics\nmaintainer: maximilian wulf, mwulf@anybotics.com, magnus g\u00e4rtner, mgaertner@anybotics.com\nthis projected was initially developed at eth zurich (autonomous systems lab & robotic systems lab).\nthis work is conducted as part of anymal research, a community to advance legged robotics.\nvideos of the elevation mapping software in use:\nciting\nthe robot-centric elevation mapping methods used in this software are described in the following paper (available here). if you use this work in an academic context, please cite the following publication(s):\np. fankhauser, m. bloesch, and m. hutter, \"probabilistic terrain mapping for mobile robots with uncertain localization\", in ieee robotics and automation letters (ra-l), vol. 3, no. 4, pp. 3019\u20133026, 2018. (pdf)\n@article{fankhauser2018probabilisticterrainmapping,\nauthor = {fankhauser, p{\\'{e}}ter and bloesch, michael and hutter, marco},\ndoi = {10.1109/lra.2018.2849506},\ntitle = {probabilistic terrain mapping for mobile robots with uncertain localization},\njournal = {ieee robotics and automation letters (ra-l)},\nvolume = {3},\nnumber = {4},\npages = {3019--3026},\nyear = {2018}\n}\np. fankhauser, m. bloesch, c. gehring, m. hutter, and r. siegwart, \"robot-centric elevation mapping with uncertainty estimates\", in international conference on climbing and walking robots (clawar), 2014. (pdf)\n@inproceedings{fankhauser2014robotcentricelevationmapping,\nauthor = {fankhauser, p\\'{e}ter and bloesch, michael and gehring, christian and hutter, marco and siegwart, roland},\ntitle = {robot-centric elevation mapping with uncertainty estimates},\nbooktitle = {international conference on climbing and walking robots (clawar)},\nyear = {2014}\n}\ninstallation\ndependencies\nthis software is built on the robotic operating system (ros), which needs to be installed first. additionally, the robot-centric elevation mapping depends on following software:\ngrid map (grid map library for mobile robots)\nkindr (kinematics and dynamics library for robotics),\nkindr_ros (ros wrapper for kindr),\npoint cloud library (pcl) (point cloud processing),\neigen (linear algebra library).\nbuilding\nin order to install the robot-centric elevation mapping, clone the latest version from this repository into your catkin workspace and compile the package using ros.\ncd catkin_workspace/src\ngit clone https://github.com/anybotics/elevation_mapping.git\ncd ../\ncatkin config --cmake-args -dcmake_build_type=release\ncatkin build\nunit tests\nbuild tests with\nroscd elevation_mapping\ncatkin build --catkin-make-args run_tests -- --this\nrun the tests with\nrostest elevation_mapping elevation_mapping.test -t\nbasic usage\nin order to get the robot-centric elevation mapping to run with your robot, you will need to adapt a few parameters. it is the easiest if duplicate and adapt all the parameter files that you need to change from the elevation_mapping_demos package (e.g. the simple_demo example). these are specifically the parameter files in config and the launch file from the launch folder.\nturtlebot3 waffle simulation\na running example is provided, making use of the turtlebot3 simulation environment. this example can be used to test elevation mapping, as a starting point for further integration.\nto start with, the turtlebot3 simulation dependencies need to be installed:\nsudo apt install ros-melodic-turtlebot3*\nthe elevation mapping demo together with the turtlebot3 simulation can be started with\nroslaunch elevation_mapping_demos turtlesim3_waffle_demo.launch\nto control the robot with a keyboard, a new terminal window needs to be opened (remember to source your ros environment). then run\nexport turtlebot3_model=waffle\nroslaunch turtlebot3_teleop turtlebot3_teleop_key.launch\nvelocity inputs can be sent to the robot by pressing the keys a, w,d, x. to stop the robot completely, press s.\nsimple demo & ground truth demo\na .ply is published as static pointcloud, elevation_mapping subscribes to it and publishes the elevation map. you can visualize it through rviz. for visualization, select /elevation_mapping/elevation_map_raw.\nnote. you might need to toggle the visibility of the grid_map_plugin to visualize it.\nroslaunch elevation_mapping_demos ground_truth_demo.launch\nwhile ground truth demo estimates the height in map frame, simple demo sets up a more realistic deployment scenario. here, the elevation_map is configured to track a base frame. to get started, we suggest to play around and also visualize other published topics, such as /elevation_mapping/elevation_map_raw and change the height layer to another layer, e.g elevation_inpainted.\nnodes\nnode: elevation_mapping\nthis is the main robot-centric elevation mapping node. it uses the distance sensor measurements and the pose and covariance of the robot to generate an elevation map with variance estimates.\nsubscribed topics\n/points (sensor_msgs/pointcloud2)\nthe distance measurements.\n/pose (geometry_msgs/posewithcovariancestamped)\nthe robot pose and covariance.\n/tf (tf/tfmessage)\nthe transformation -----> tree !!! .\npublished topics\nelevation_map (grid_map_msgs/gridmap)\nthe entire (fused) elevation map. it is published periodically (see fused_map_publishing_rate parameter) or after the trigger_fusion service is called.\nelevation_map_raw (grid_map_msgs/gridmap)\nthe entire (raw) elevation map before the fusion step.\nservices\ntrigger_fusion (std_srvs/empty)\ntrigger the fusing process for the entire elevation map and publish it. for example, you can trigger the map fusion step from the console with\nrosservice call /elevation_mapping/trigger_fusion\nget_submap (grid_map_msgs/getgridmap)\nget a fused elevation submap for a requested position and size. for example, you can get the fused elevation submap at position (-0.5, 0.0) and size (0.5, 1.2) described in the odom frame and save it to a text file form the console with\nrosservice call -- /elevation_mapping/get_submap odom -0.5 0.0 0.5 1.2 []\nget_raw_submap (grid_map_msgs/getgridmap)\nget a raw elevation submap for a requested position and size. for example, you can get the raw elevation submap at position (-0.5, 0.0) and size (0.5, 1.2) described in the odom frame and save it to a text file form the console with\nrosservice call -- /elevation_mapping/get_raw_submap odom -0.5 0.0 0.5 1.2 []\nclear_map (std_srvs/empty)\ninitiates clearing of the entire map for resetting purposes. trigger the map clearing with\nrosservice call /elevation_mapping/clear_map\nmasked_replace ([grid_map_msgs/setgridmap])\nallows for setting the individual layers of the elevation map through a service call. the layer mask can be used to only set certain cells and not the entire map. cells containing nan in the mask are not set, all the others are set. if the layer mask is not supplied, the entire map will be set in the intersection of both maps. the provided map can be of different size and position than the map that will be altered. an example service call to set some cells marked with a mask in the elevation layer to 0.5 is\nrosservice call /elevation_mapping/masked_replace \"map:\ninfo:\nheader:\nseq: 3\nstamp: {secs: 3, nsecs: 80000000}\nframe_id: 'odom'\nresolution: 0.1\nlength_x: 0.3\nlength_y: 0.3\npose:\nposition: {x: 5.0, y: 0.0, z: 0.0}\norientation: {x: 0.0, y: 0.0, z: 0.0, w: 0.0}\nlayers: [elevation,mask]\nbasic_layers: [elevation]\ndata:\n- layout:\ndim:\n- {label: 'column_index', size: 3, stride: 9}\n- {label: 'row_index', size: 3, stride: 3}\ndata_offset: 0\ndata: [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n- layout:\ndim:\n- {label: 'column_index', size: 3, stride: 9}\n- {label: 'row_index', size: 3, stride: 3}\ndata_offset: 0\ndata: [0, 0, 0, .nan, .nan, .nan, 0, 0, 0]\nouter_start_index: 0\ninner_start_index: 0\"\nsave_map (grid_map_msgs/processfile)\nsaves the current fused grid map and raw grid map to rosbag files. field topic_name must be a base name, i.e. no leading slash character (/). if field topic_name is empty, then elevation_map is used per default. example with default topic name\nrosservice call /elevation_mapping/save_map \"file_path: '/home/integration/elevation_map.bag' topic_name: ''\"\nload_map (grid_map_msgs/processfile)\nloads the fused grid map and raw grid map from rosbag files. field topic_name must be a base name, i.e. no leading slash character (/). if field topic_name is empty, then elevation_map is used per default. example with default topic name\nrosservice call /elevation_mapping/load_map \"file_path: '/home/integration/elevation_map.bag' topic_name: ''\"\ndisable_updates (std_srvs/empty)\nstops updating the elevation map with sensor input. trigger the update stopping with\nrosservice call /elevation_mapping/disable_updates {}\nenable_updates (std_srvs/empty)\nstart updating the elevation map with sensor input. trigger the update starting with\nrosservice call /elevation_mapping/enable_updates {}\nparameters\ndeprecated point_cloud_topic (string, default: \"/points\")\nthe name of the distance measurements topic. use input_sources instead.\ninput_sources (list of input sources, default: none)\nhere you specify your inputs to elevation mapping, currently \"pointcloud\" inputs are supported.\nexample configuration:\ninput_sources:\nfront: # a name to identify the input source\ntype: pointcloud # supported types: pointcloud\ntopic: /lidar_front/depth/points\nqueue_size: 1\npublish_on_update: true # wheter to publish the elevation map after a callback from this source.\nrear:\ntype: pointcloud\ntopic: /lidar_rear/depth/points\nqueue_size: 5\npublish_on_update: false\nno input sources can be configured with an empty array:\ninput_sources: []\nrobot_pose_topic (string, default: \"/robot_state/pose\")\nthe name of the robot pose and covariance topic.\nbase_frame_id (string, default: \"/robot\")\nthe id of the robot base tf frame.\nmap_frame_id (string, default: \"/map\")\nthe id of the tf frame of the elevation map.\ntrack_point_frame_id (string, default: \"/robot\")\nthe elevation map is moved along with the robot following a track point. this is the id of the tf frame in which the track point is defined.\ntrack_point_x, track_point_y, track_point_z (double, default: 0.0, 0.0, 0.0)\nthe elevation map is moved along with the robot following a track point. this is the position of the track point in the track_point_frame_id.\nrobot_pose_cache_size (int, default: 200, min: 0)\nthe size of the robot pose cache.\nmin_update_rate (double, default: 2.0)\nthe mininum update rate (in hz) at which the elevation map is updated either from new measurements or the robot pose estimates.\nfused_map_publishing_rate (double, default: 1.0)\nthe rate for publishing the entire (fused) elevation map.\nrelocate_rate (double, default: 3.0)\nthe rate (in hz) at which the elevation map is checked for relocation following the tracking point.\nlength_in_x, length_in_y (double, default: 1.5, min: 0.0)\nthe size (in m) of the elevation map.\nposition_x, position_y (double, default: 0.0)\nthe position of the elevation map center, in the elevation map frame. this parameter sets the planar position offsets between the generated elevation map and the frame in which it is published (map_frame_id). it is only useful if no track_point_frame_id parameter is used.\nresolution (double, default: 0.01, min: 0.0)\nthe resolution (cell size in m/cell) of the elevation map.\nmin_variance, max_variance (double, default: 9.0e-6, 0.01)\nthe minimum and maximum values for the elevation map variance data.\nmahalanobis_distance_threshold (double, default: 2.5)\neach cell in the elevation map has an uncertainty for its height value. depending on the mahalonobis distance of the existing height distribution and the new measurements, the incoming data is fused with the existing estimate, overwritten, or ignored. this parameter determines the threshold on the mahalanobis distance which determines how the incoming measurements are processed.\nsensor_processor/ignore_points_above (double, default: inf) a hard threshold on the height of points introduced by the depth sensor. points with a height over this threshold will not be considered valid during the data collection step.\nsensor_processor/ignore_points_below (double, default: -inf) a hard threshold on the height of points introduced by the depth sensor. points with a height below this threshold will not be considered valid during the data collection step.\nmulti_height_noise (double, default: 9.0e-7)\nnoise added to measurements that are higher than the current elevation map at that particular position. this noise-adding process is only performed if a point falls over the mahalanobis distance threshold. a higher value is useful to adapt faster to dynamic environments (e.g., moving objects), but might cause more noise in the height estimation.\nmin_horizontal_variance, max_horizontal_variance (double, default: pow(resolution / 2.0, 2), 0.5)\nthe minimum and maximum values for the elevation map horizontal variance data.\nenable_visibility_cleanup (bool, default: true)\nenable/disable a separate thread that removes elements from the map which are not visible anymore, by means of ray-tracing, originating from the sensor frame.\nvisibility_cleanup_rate (double, default: 1.0)\nthe rate (in hz) at which the visibility clean-up is performed.\nenable_continuous_cleanup (bool, default: false)\nenable/disable a continuous clean-up of the elevation map. if enabled, on arrival of each new sensor data the elevation map will be cleared and filled up only with the latest data from the sensor. when continuous clean-up is enabled, visibility clean-up will automatically be disabled since it is not needed in this case.\nnum_callback_threads (int, default: 1, min: 1) the number of threads to use for processing callbacks. more threads results in higher throughput, at cost of more resource usage.\npostprocessor_pipeline_name (string, default: postprocessor_pipeline)\nthe name of the pipeline to execute for postprocessing. it expects a pipeline configuration to be loaded in the private namespace of the node under this name. e.g.:\n<node pkg=\"elevation_mapping\" type=\"elevation_mapping\" name=\"elevation_mapping\" output=\"screen\">\n...\n<rosparam command=\"load\" file=\"$(find elevation_mapping_demos)/config/postprocessor_pipeline.yaml\" />\n</node>\na pipeline is a grid_map_filter chain, see grid_map_demos/filters_demo.yaml and ros / filters for more information.\npostprocessor_num_threads (int, default: 1, min: 1)\nthe number of threads to use for asynchronous postprocessing. more threads results in higher throughput, at cost of more resource usage.\nscanning_duration (double, default: 1.0)\nthe sensor's scanning duration (in s) which is used for the visibility cleanup. set this roughly to the duration it takes between two consecutive full scans (e.g. 0.033 for a tof camera with 30 hz, or 3 s for a rotating laser scanner). depending on how dense or sparse your scans are, increase or reduce the scanning duration. smaller values lead to faster dynamic object removal and bigger values help to reduce faulty map cleanups.\nsensor_cutoff_min_depth, sensor_cutoff_max_depth (double, default: 0.2, 2.0)\nthe minimum and maximum values for the length of the distance sensor measurements. measurements outside this interval are ignored.\nsensor_model_normal_factor_a, sensor_model_normal_factor_b, sensor_model_normal_factor_c, sensor_model_lateral_factor (double)\nthe data for the sensor noise model.\nchangelog\nsee changelog\nbugs & feature requests\nplease report bugs and request features using the issue tracker.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000077, "year": null}, {"Unnamed: 0": 1101, "autor": 81, "date": null, "content": "Pinocchio instantiates the state-of-the-art Rigid Body Algorithms for poly-articulated systems based on revisited Roy Featherstone's algorithms. Besides, Pinocchio provides the analytical derivatives of the main Rigid-Body Algorithms like the Recursive Newton-Euler Algorithm or the Articulated-Body Algorithm.\nPinocchio is first tailored for robotics applications, but it can be used in extra contexts (biomechanics, computer graphics, vision, etc.). It is built upon Eigen for linear algebra and FCL for collision detection. Pinocchio comes with a Python interface for fast code prototyping, directly accessible through Conda.\nPinocchio is now at the heart of various robotics softwares as Crocoddyl, an open-source and efficient Differential Dynamic Programming solver for robotics, the Stack-of-Tasks, an open-source and versatile hierarchical controller framework or the Humanoid Path Planner, an open-source software for Motion and Manipulation Planning.\nIf you want to learn more on Pinocchio internal behaviors and main features, we invite you to read the related paper.\nIf you want to directly dive into Pinocchio, only one single line is sufficient (assuming you have Conda):\nconda install pinocchio -c conda-forge\nor via pip (currently only available on Linux):\npip install pin\nPinocchio main features\nPinocchio is fast:\nC++ template library,\ncache friendly,\nautomatic code generation support via CppADCodeGen.\nPinocchio is versatile, implementing basic and more advanced rigid body dynamics algorithms:\nforward kinematics and its analytical derivatives,\nforward/inverse dynamics and their analytical derivatives,\ncentroidal dynamics and its analytical derivatives,\nsupport of multiple precision arithmetic via Boost.Multiprecision or any similar framework,\ncomputations of kinematic and dynamic regressors for system identification and more,\nand much more with the support of modern and open source Automatic Differentiation frameworks like CppAD or CasADi.\nPinocchio is flexible:\nheader only,\nC++ 98/03/11/14/17/20 compliant.\nPinocchio is extensible. Pinocchio is multi-thread friendly. Pinocchio is reliable and extensively tested (unit-tests, simulations and real world robotics applications). Pinocchio is supported and tested on Windows, Mac OS X, Unix and Linux (see build status here).\nPinocchio continuous integrations\nPinocchio is constantly tested for several platforms and distributions, as reported below:\nCI on ROS\nCI on Linux via APT\nCI on OSX via Conda\nCI on Windows via Conda\nCI on Linux via Robotpkg\nPerformances\nPinocchio exploits at best the sparsity induced by the kinematic tree of robotics systems. Thanks to modern programming language paradigms, Pinocchio is able to unroll most of the computations directly at compile time, allowing to achieve impressive performances for a large range of robots, as illustrated by the plot below, obtained on a standard laptop equipped with an Intel Core i7 CPU @ 2.4 GHz.\nFor other benchmarks, and mainly the capacity of Pinocchio to exploit at best your CPU capacities using advanced code generation techniques, we refer to the technical paper. In addition, the introspection done here may also help you to understand and compare the performances of the modern rigid body dynamics librairies.\nOngoing developments\nIf you want to follow the current developments, you can directly refer to the devel branch. The master branch only contains latest release. Any new Pull Request should then be submitted on the devel branch.\nInstallation\nPinocchio can be easily installed on various Linux (Ubuntu, Fedora, etc.) and Unix distributions (Mac OS X, BSD, etc.). Please refer to the installation procedure.\nIf you only need the Python bindings of Pinocchio, you may prefer to install it through Conda. Please follow the procedure described here.\nPinocchio is also deployed on ROS, you may follow its deployment status on Melodic or Kinetic.\nDocumentation\nThe online Pinocchio documentation of the last release is available here.\nExamples\nWe provide some basic examples on how to use Pinocchio in Python in the examples/python directory. Additional examples introducing Pinocchio are also available in the documentation.\nTutorials\nPinocchio comes with a large bunch of tutorials aiming at introducing the basic tools for robot control. The content of the tutorials is described here. Additionnal materials were presented during the Memmo summer school and available here.\nVisualization\nPinocchio provides support for many open-source and free visualizers:\nGepetto Viewer: a C++ viewer based on OpenSceneGraph with Python bindings and Blender export. See here for a C++ example on mixing Pinocchio and Gepetto Viewer.\nMeshcat: supporting visualization in Python and which can be embeded inside any browser.\nPanda3d: supporting visualization in Python and which can be embeded inside any browser.\nRViz: supporting visualization in Python and which can interact with other ROS packages.\nMany external viewers can also be integrated. See example here for more information.\nCiting Pinocchio\nTo cite Pinocchio in your academic research, please use the following bibtex lines:\n@misc{pinocchioweb,\nauthor = {Justin Carpentier and Florian Valenza and Nicolas Mansard and others},\ntitle = {Pinocchio: fast forward and inverse dynamics for poly-articulated systems},\nhowpublished = {https://stack-of-tasks.github.io/pinocchio},\nyear = {2015--2021}\n}\nand the following one for the reference to the paper introducing Pinocchio:\n@inproceedings{carpentier2019pinocchio,\ntitle={The Pinocchio C++ library -- A fast and flexible implementation of rigid body dynamics algorithms and their analytical derivatives},\nauthor={Carpentier, Justin and Saurel, Guilhem and Buondonno, Gabriele and Mirabel, Joseph and Lamiraux, Florent and Stasse, Olivier and Mansard, Nicolas},\nbooktitle={IEEE International Symposium on System Integrations (SII)},\nyear={2019}\n}\nThe algorithms for the analytical derivatives of rigid-body dynamics algorithms are detailed here:\n@inproceedings{carpentier2018analytical,\ntitle = {Analytical Derivatives of Rigid Body Dynamics Algorithms},\nauthor = {Carpentier, Justin and Mansard, Nicolas},\nbooktitle = {Robotics: Science and Systems},\nyear = {2018}\n}\nQuestions and Issues\nYou have a question or an issue? You may either directly open a new issue or use the mailing list pinocchio@laas.fr.\nCredits\nThe following people have been involved in the development of Pinocchio and are warmly thanked for their contributions:\nJustin Carpentier (Inria): main developer and manager of the project\nNicolas Mansard (LAAS-CNRS): initial project instructor\nGuilhem Saurel (LAAS-CNRS): continuous integration and deployment\nJoseph Mirabel (LAAS-CNRS): Lie groups support\nAntonio El Khoury (Wandercraft): bug fixes\nGabriele Buondono (LAAS-CNRS): features extension, bug fixes and Python bindings\nFlorian Valenza (Astek): core developments and FCL support\nWolfgang Merkt (University of Edinburgh): ROS integration and support\nRohan Budhiraja (LAAS-CNRS): features extension\nLo\u00efc Est\u00e8ve (Inria): Conda integration and support\nIgor Kalevatykh (Inria): Panda3d viewer support\nMatthieu Vigne (Wandercraft): MeshCat viewer support\nRobin Strudel (Inria): features extension\nFran\u00e7ois Keith (CEA): Windows support\nAamr El Kazdadi (Inria): multiprecision arithmetic support\nNicolas Torres Alberto (Inria): features extension\nEtienne Arlaud (Inria): RViz viewer support\nIf you have taken part to the development of Pinocchio, feel free to add your name and contribution in this list.\nAcknowledgments\nThe development of Pinocchio is actively supported by the Gepetto team @LAAS-CNRS and the Willow team @INRIA.", "link": "https://github.com/stack-of-tasks/pinocchio", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "pinocchio instantiates the state-of-the-art rigid body algorithms for poly-articulated systems based on revisited roy featherstone's algorithms. besides, pinocchio provides the analytical derivatives of the main rigid-body algorithms like the recursive newton-euler algorithm or the articulated-body algorithm.\npinocchio is first tailored for robotics applications, but it can be used in extra contexts (biomechanics, computer graphics, vision, etc.). it is built upon eigen for linear algebra and fcl for collision detection. pinocchio comes with a python interface for fast code prototyping, directly accessible through conda.\npinocchio is now at the heart of various robotics softwares as crocoddyl, an open-source and efficient differential dynamic programming solver for robotics, the stack-of-tasks, an open-source and versatile hierarchical controller framework or the humanoid path planner, an open-source software for motion and manipulation planning.\nif you want to learn more on pinocchio internal behaviors and main features, we invite you to read the related paper.\nif you want to directly dive into pinocchio, only one single line is sufficient (assuming you have conda):\nconda install pinocchio -c conda-forge\nor via pip (currently only available on linux):\npip install pin\npinocchio main features\npinocchio is fast:\nc++ template library,\ncache friendly,\nautomatic code generation support via cppadcodegen.\npinocchio is versatile, implementing basic and more advanced rigid body dynamics algorithms:\nforward kinematics and its analytical derivatives,\nforward/inverse dynamics and their analytical derivatives,\ncentroidal dynamics and its analytical derivatives,\nsupport of multiple precision arithmetic via boost.multiprecision or any similar framework,\ncomputations of kinematic and dynamic regressors for system identification and more,\nand much more with the support of modern and open source automatic differentiation frameworks like cppad or casadi.\npinocchio is flexible:\nheader only,\nc++ 98/03/11/14/17/20 compliant.\npinocchio is extensible. pinocchio is multi-thread friendly. pinocchio is reliable and extensively tested (unit-tests, simulations and real world robotics applications). pinocchio is supported and tested on windows, mac os x, unix and linux (see build status here).\npinocchio continuous integrations\npinocchio is constantly tested for several platforms and distributions, as reported below:\nci on ros\nci on linux via apt\nci on osx via conda\nci on windows via conda\nci on linux via robotpkg\nperformances\npinocchio exploits at best the sparsity induced by the kinematic -----> tree !!!  of robotics systems. thanks to modern programming language paradigms, pinocchio is able to unroll most of the computations directly at compile time, allowing to achieve impressive performances for a large range of robots, as illustrated by the plot below, obtained on a standard laptop equipped with an intel core i7 cpu @ 2.4 ghz.\nfor other benchmarks, and mainly the capacity of pinocchio to exploit at best your cpu capacities using advanced code generation techniques, we refer to the technical paper. in addition, the introspection done here may also help you to understand and compare the performances of the modern rigid body dynamics librairies.\nongoing developments\nif you want to follow the current developments, you can directly refer to the devel branch. the master branch only contains latest release. any new pull request should then be submitted on the devel branch.\ninstallation\npinocchio can be easily installed on various linux (ubuntu, fedora, etc.) and unix distributions (mac os x, bsd, etc.). please refer to the installation procedure.\nif you only need the python bindings of pinocchio, you may prefer to install it through conda. please follow the procedure described here.\npinocchio is also deployed on ros, you may follow its deployment status on melodic or kinetic.\ndocumentation\nthe online pinocchio documentation of the last release is available here.\nexamples\nwe provide some basic examples on how to use pinocchio in python in the examples/python directory. additional examples introducing pinocchio are also available in the documentation.\ntutorials\npinocchio comes with a large bunch of tutorials aiming at introducing the basic tools for robot control. the content of the tutorials is described here. additionnal materials were presented during the memmo summer school and available here.\nvisualization\npinocchio provides support for many open-source and free visualizers:\ngepetto viewer: a c++ viewer based on openscenegraph with python bindings and blender export. see here for a c++ example on mixing pinocchio and gepetto viewer.\nmeshcat: supporting visualization in python and which can be embeded inside any browser.\npanda3d: supporting visualization in python and which can be embeded inside any browser.\nrviz: supporting visualization in python and which can interact with other ros packages.\nmany external viewers can also be integrated. see example here for more information.\nciting pinocchio\nto cite pinocchio in your academic research, please use the following bibtex lines:\n@misc{pinocchioweb,\nauthor = {justin carpentier and florian valenza and nicolas mansard and others},\ntitle = {pinocchio: fast forward and inverse dynamics for poly-articulated systems},\nhowpublished = {https://stack-of-tasks.github.io/pinocchio},\nyear = {2015--2021}\n}\nand the following one for the reference to the paper introducing pinocchio:\n@inproceedings{carpentier2019pinocchio,\ntitle={the pinocchio c++ library -- a fast and flexible implementation of rigid body dynamics algorithms and their analytical derivatives},\nauthor={carpentier, justin and saurel, guilhem and buondonno, gabriele and mirabel, joseph and lamiraux, florent and stasse, olivier and mansard, nicolas},\nbooktitle={ieee international symposium on system integrations (sii)},\nyear={2019}\n}\nthe algorithms for the analytical derivatives of rigid-body dynamics algorithms are detailed here:\n@inproceedings{carpentier2018analytical,\ntitle = {analytical derivatives of rigid body dynamics algorithms},\nauthor = {carpentier, justin and mansard, nicolas},\nbooktitle = {robotics: science and systems},\nyear = {2018}\n}\nquestions and issues\nyou have a question or an issue? you may either directly open a new issue or use the mailing list pinocchio@laas.fr.\ncredits\nthe following people have been involved in the development of pinocchio and are warmly thanked for their contributions:\njustin carpentier (inria): main developer and manager of the project\nnicolas mansard (laas-cnrs): initial project instructor\nguilhem saurel (laas-cnrs): continuous integration and deployment\njoseph mirabel (laas-cnrs): lie groups support\nantonio el khoury (wandercraft): bug fixes\ngabriele buondono (laas-cnrs): features extension, bug fixes and python bindings\nflorian valenza (astek): core developments and fcl support\nwolfgang merkt (university of edinburgh): ros integration and support\nrohan budhiraja (laas-cnrs): features extension\nlo\u00efc est\u00e8ve (inria): conda integration and support\nigor kalevatykh (inria): panda3d viewer support\nmatthieu vigne (wandercraft): meshcat viewer support\nrobin strudel (inria): features extension\nfran\u00e7ois keith (cea): windows support\naamr el kazdadi (inria): multiprecision arithmetic support\nnicolas torres alberto (inria): features extension\netienne arlaud (inria): rviz viewer support\nif you have taken part to the development of pinocchio, feel free to add your name and contribution in this list.\nacknowledgments\nthe development of pinocchio is actively supported by the gepetto team @laas-cnrs and the willow team @inria.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000081, "year": null}, {"Unnamed: 0": 1136, "autor": 116, "date": null, "content": "Deep Reinforcement Learning survey\nThis paper list is a bit different from others. I'll put some opinion and summary on it. However, to understand the whole paper, you still have to read it by yourself!\nSurely, any pull request or discussion are welcomed!\nBefore Jump into Deep Reinforcement Learning\nIf you're a newbie in deep reinforcement learning, I suggest you to read the blog post and open course first.\nOutline\nReinforcement Learning Papers\nHuman-level control through deep reinforcement learning\nMastering the game of Go with deep neural networks and tree search\nDeep Successor Reinforcement Learning\nAction-Conditional Video Prediction using Deep Networks in Atari Games\nPolicy Distillation\nLearning Tetris Using the Noisy Cross-Entropy Method, with code\nContinuous Deep Q-Learning with Model-based Acceleration\nValue Iteration Networks\nLearning Modular Neural Network Policies for Multi-Task and Multi-Robot Transfer\nStochastic Neural Network For Hierarchical Reinforcement Learning\nNoisy Networks for Exploration\nImproving Stochastic Policy Gradients in Continuous Control with Deep Reinforcement Learning using the Beta Distribution\nHigh-Dimensional Continuous Control Using Generalized Advantage Estimation\nGeneralizing Skills with Semi-Supervised Reinforcement Learning\nUnsupervised Perceptual Rewards for Imitation Learning\nTowards Deep Symbolic Reinforcement Learning\nothers\nOpen Source\nPython users\nLua users\nCourses\nTextbook\nMisc\n[Useful] Learn Reinforcement Learning\n\ud83d\udc49 dennybritz/reinforcement-learning\n\ud83d\udc49 Daivd Silver's course about policy gradient\n\ud83d\udc49 Deep Reinforcement Learning", "link": "https://github.com/andrewliao11/Deep-Reinforcement-Learning-Survey", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "deep reinforcement learning survey\nthis paper list is a bit different from others. i'll put some opinion and summary on it. however, to understand the whole paper, you still have to read it by yourself!\nsurely, any pull request or discussion are welcomed!\nbefore jump into deep reinforcement learning\nif you're a newbie in deep reinforcement learning, i suggest you to read the blog post and open course first.\noutline\nreinforcement learning papers\nhuman-level control through deep reinforcement learning\nmastering the game of go with deep neural networks and -----> tree !!!  search\ndeep successor reinforcement learning\naction-conditional video prediction using deep networks in atari games\npolicy distillation\nlearning tetris using the noisy cross-entropy method, with code\ncontinuous deep q-learning with model-based acceleration\nvalue iteration networks\nlearning modular neural network policies for multi-task and multi-robot transfer\nstochastic neural network for hierarchical reinforcement learning\nnoisy networks for exploration\nimproving stochastic policy gradients in continuous control with deep reinforcement learning using the beta distribution\nhigh-dimensional continuous control using generalized advantage estimation\ngeneralizing skills with semi-supervised reinforcement learning\nunsupervised perceptual rewards for imitation learning\ntowards deep symbolic reinforcement learning\nothers\nopen source\npython users\nlua users\ncourses\ntextbook\nmisc\n[useful] learn reinforcement learning\n\ud83d\udc49 dennybritz/reinforcement-learning\n\ud83d\udc49 daivd silver's course about policy gradient\n\ud83d\udc49 deep reinforcement learning", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000116, "year": null}, {"Unnamed: 0": 1159, "autor": 139, "date": null, "content": "PySwip\nInstalling the Latest Version\nThe latest SWI-Prolog supported by Ubuntu 18.04 and 20.04 are 7.6.4. We generally want to support LTS releases of Ubuntu. The compatibility of PySwip with 7.6.4 on master is broken, so we are not able to release a new version until this is fixed. In the meantime, you can use the following to install PySwip from the master branch:\npip install git+https://github.com/yuce/pyswip@master#egg=pyswip\nThe End of Python 2 Support\nPython 2 has reached end of life on January 1st, 2020 as documented here. So, PySwip 0.2.10 will be the last version which officially supports Python 2.\nDo you still require Python 2 support? Let us know at: https://github.com/yuce/pyswip/issues/94\nWhat's New?\nSee the CHANGELOG.\nWARNING! PySwip has no Windows installers! If you are a Windows user, see INSTALL. There are some \"free download\" sites that claim to be hosting PySwip installers. DO NOT TRUST THEM!\nThanks to all contributors.\nIntroduction\nPySwip is a Python - SWI-Prolog bridge enabling to query SWI-Prolog in your Python programs. It features an (incomplete) SWI-Prolog foreign language interface, a utility class that makes it easy querying with Prolog and also a Pythonic interface.\nSince PySwip uses SWI-Prolog as a shared library and ctypes to access it, it doesn't require compilation to be installed.\nRequirements:\nPython 3.6 and higher.\nPyPy is currently not supported.\nSWI-Prolog 8.2 and higher.\nlibswipl as a shared library. This is the default on most platforms.\nWorks on Linux, Windows, MacOS and FreeBSD. Should work on other POSIX.\nInstall\nIMPORTANT: Make sure the SWI-Prolog architecture is the same as the Python architecture. If you are using a 64bit build of Python, use a 64bit build of SWI-Prolog, etc.\nSee INSTALL for instructions.\nExamples\nUsing Prolog\nfrom pyswip import Prolog\nprolog = Prolog()\nprolog.assertz(\"father(michael,john)\")\nprolog.assertz(\"father(michael,gina)\")\nlist(prolog.query(\"father(michael,X)\")) == [{'X': 'john'}, {'X': 'gina'}]\nfor soln in prolog.query(\"father(X,Y)\"):\nprint(soln[\"X\"], \"is the father of\", soln[\"Y\"])\n# michael is the father of john\n# michael is the father of gina\nAn existing knowledge base stored in a Prolog file can also be consulted, and queried. Assuming the filename \"knowledge_base.pl\" and the Python is being run in the same working directory, it is consulted like so:\n>>> from pyswip import Prolog\n>>> prolog = Prolog()\n>>> prolog.consult(\"knowledge_base.pl\")\nForeign Functions\nfrom __future__ import print_function\nfrom pyswip import Prolog, registerForeign\ndef hello(t):\nprint(\"Hello,\", t)\nhello.arity = 1\nregisterForeign(hello)\nprolog = Prolog()\nprolog.assertz(\"father(michael,john)\")\nprolog.assertz(\"father(michael,gina)\")\nprint(list(prolog.query(\"father(michael,X), hello(X)\")))\nPythonic interface (Experimental)\nfrom __future__ import print_function\nfrom pyswip import Functor, Variable, Query, call\nassertz = Functor(\"assertz\", 1)\nfather = Functor(\"father\", 2)\ncall(assertz(father(\"michael\",\"john\")))\ncall(assertz(father(\"michael\",\"gina\")))\nX = Variable()\nq = Query(father(\"michael\",X))\nwhile q.nextSolution():\nprint(\"Hello,\", X.value)\nq.closeQuery()\n# Outputs:\n# Hello, john\n# Hello, gina\nThe core functionality of Prolog.query is based on Nathan Denny's public domain prolog.py.\nHelp!\nSupport Forum\nStack Overflow\nProjects/Publications that Use or Reference PySwip\nDo you have a project, video or publication that uses/mentions PySwip? file an issue or send a pull request.\nIf you would like to reference PySwip in a LaTeX document, you can use the provided BibTeX file.\nBooks\nBeginning Artificial Intelligence with the Raspberry Pi\nPublications\nAssessment of Graph Databases as a Viable Materiel Solution for the Army's Dynamic Force Structure (DFS) Portal Implementation: Part 3, Risks, Mitigation Approach, and Roadmap\nTackling Complexity in High Performance Computing Applications\nSocial Human-Robot Interaction: A New Cognitive and Affective Interaction-Oriented Architecture\nA Planning Module for a ROS-Based Ubiquitous Robot Control System (PDF)\nA pilot framework developed as a common platform integrating diverse elements of computer aided fixture design\nIntegration von Prolog und ClioPatria in Python (PDF, German)\nSELECTSCRIPT: A Query Language for Robotic World Models and Simulations\nA Concept for Declarative Information Acquisition in Smart Environments (PDF)\nImplementation on ADHD Diagnostic Expert System based on DSM Diagnostic Criteria (PDF, Korean)\nWie sehen Krebsmolekule aus? Vergleich der Gute der Klassifizierung potenziell krebserregender Molekule durch induktiv logische und merkmalsbasierte Lernverfahren (PDF, German)\nCompanion Robots Behaving with Style: Towards Plasticity in Social Human-Robot Interaction (PDF)\nSemi-automatically Augmenting Attack Trees using an Annotated Attack Tree Library\nA Learning Framework for Tool Creation by a Robot (PDF)\nConceptual Maps as the First Step in an Ontology Construction Method\nFact-Based Expert System for Supplier Selection with ERP Data\nInteractive Text Graph Mining with a Prolog-based Dialog Engine\nThe Detection Of Conflicts In The Requirements Specification Based On An Ontological Model And A Production Rule System\nDependency-based Text Graphs for Keyphrase and Summary Extraction with Applications to Interactive Content Retrieval (PDF)\nInformation Retrieval Based on Knowledge-Enhanced Word Embedding Through Dialog: A Case Study\nExploring the world of declarative programming\nVideos\nAI - Blocks world solver interactive planner\nPySwip, Prolog, Javascript and HTML (Spanish)\nGet out of the maze with Prolog and Python (Spanish)\nLes robots deviennent (vraiment) intelligents ! (NAO discute avec Kylo Ren)\nProjects\nnoworkflow Supporting infrastructure to run scientific experiments without a scientific workflow management system. http://gems-uff.github.io/noworkflow\nSuper Pacman\nPokemon Weak Detector\nFood Recommendations in Hyderabad, India Food Recommendation AI Expert System using a GUI hosted on Flask and a backend developed with PYSWIP and native Prolog.\npyswip_envctrl An environment control module expert system written in PySwip.\ntic-tac-toe Tic-tac-toe game with AI in Prolog and GUI in Python (kivy framework + pyswip).\nTBM1 - \"Getting to Know My Home\"\nProlog natural language parsing component to control a Scribbler II robot over bluetooth\nCosmos A new logic programming language.\nlib-annotated-attack-trees Scripts and resources for creating a library of annotated attack trees and using it to refine an annotated attack tree.\nClIDE Command-line Intelligent Development Environment\nArtificial Intelligence INF1771 @ PUC-Rio Projects for the Artificial Intelligence class @ PUC-Rio\nAutomobileAdvisor Projekt na systemy ekspertowe pomagaj\u0105cy wybra\u0107 odpowiedni samoch\u00f3d dla danego klienta na podstawie preferencji (Polish)\nProlog Tetris AI\nJupyter SWI Prolog A Jupyter Kernel for SWI-Prolog.\nBlocks World Planner A program that allows users to solve the blocks world problem interacting only using the natural language.\nDeepTalk A Python+Prolog based Dialog Engine using the Python package text_graph_crafts that extracts the highest ranked sentences answering a query.\nDeepRank The system uses dependency links for building Text Graphs, that with help of a centrality algorithm like PageRank, extract relevant keyphrases, summaries and relations from text documents.\nProlog Tic-tac-toe A full-stack tic-tac-toe game with AI in Prolog, backend in Python3 (+Flask) and frontend in Vue.js 3.\nMIDSI Project Solution for data discovery in projects applicable to the Semantic Web, enabling the loading of ontologies and inference of results using the WSML language.\nPopper An inductive logic programming system.\nBlog Posts\nCalling Prolog from Python\nPython v. Prolog: Round 1: Fight!\nPath Follower: Arduino+Rasp on ROS and its Project code\n10 minutes to make a GUI for your SWI-Prolog App via Python\nCompanies using PySwip\nMagazino GmbH Magazino develops and builds intelligent, mobile robots for intralogistics.\nLicense\nCopyright (c) 2007-2020 Y\u00fcce Tekol and PySwip contributors\nPermission is hereby granted, free of charge, to any person obtaining a copy of\nthis software and associated documentation files (the \"Software\"), to deal in\nthe Software without restriction, including without limitation the rights to\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\nthe Software, and to permit persons to whom the Software is furnished to do so,\nsubject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\nFOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\nCOPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\nIN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\nCONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.", "link": "https://github.com/yuce/pyswip", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "pyswip\ninstalling the latest version\nthe latest swi-prolog supported by ubuntu 18.04 and 20.04 are 7.6.4. we generally want to support lts releases of ubuntu. the compatibility of pyswip with 7.6.4 on master is broken, so we are not able to release a new version until this is fixed. in the meantime, you can use the following to install pyswip from the master branch:\npip install git+https://github.com/yuce/pyswip@master#egg=pyswip\nthe end of python 2 support\npython 2 has reached end of life on january 1st, 2020 as documented here. so, pyswip 0.2.10 will be the last version which officially supports python 2.\ndo you still require python 2 support? let us know at: https://github.com/yuce/pyswip/issues/94\nwhat's new?\nsee the changelog.\nwarning! pyswip has no windows installers! if you are a windows user, see install. there are some \"free download\" sites that claim to be hosting pyswip installers. do not trust them!\nthanks to all contributors.\nintroduction\npyswip is a python - swi-prolog bridge enabling to query swi-prolog in your python programs. it features an (incomplete) swi-prolog foreign language interface, a utility class that makes it easy querying with prolog and also a pythonic interface.\nsince pyswip uses swi-prolog as a shared library and ctypes to access it, it doesn't require compilation to be installed.\nrequirements:\npython 3.6 and higher.\npypy is currently not supported.\nswi-prolog 8.2 and higher.\nlibswipl as a shared library. this is the default on most platforms.\nworks on linux, windows, macos and freebsd. should work on other posix.\ninstall\nimportant: make sure the swi-prolog architecture is the same as the python architecture. if you are using a 64bit build of python, use a 64bit build of swi-prolog, etc.\nsee install for instructions.\nexamples\nusing prolog\nfrom pyswip import prolog\nprolog = prolog()\nprolog.assertz(\"father(michael,john)\")\nprolog.assertz(\"father(michael,gina)\")\nlist(prolog.query(\"father(michael,x)\")) == [{'x': 'john'}, {'x': 'gina'}]\nfor soln in prolog.query(\"father(x,y)\"):\nprint(soln[\"x\"], \"is the father of\", soln[\"y\"])\n# michael is the father of john\n# michael is the father of gina\nan existing knowledge base stored in a prolog file can also be consulted, and queried. assuming the filename \"knowledge_base.pl\" and the python is being run in the same working directory, it is consulted like so:\n>>> from pyswip import prolog\n>>> prolog = prolog()\n>>> prolog.consult(\"knowledge_base.pl\")\nforeign functions\nfrom __future__ import print_function\nfrom pyswip import prolog, registerforeign\ndef hello(t):\nprint(\"hello,\", t)\nhello.arity = 1\nregisterforeign(hello)\nprolog = prolog()\nprolog.assertz(\"father(michael,john)\")\nprolog.assertz(\"father(michael,gina)\")\nprint(list(prolog.query(\"father(michael,x), hello(x)\")))\npythonic interface (experimental)\nfrom __future__ import print_function\nfrom pyswip import functor, variable, query, call\nassertz = functor(\"assertz\", 1)\nfather = functor(\"father\", 2)\ncall(assertz(father(\"michael\",\"john\")))\ncall(assertz(father(\"michael\",\"gina\")))\nx = variable()\nq = query(father(\"michael\",x))\nwhile q.nextsolution():\nprint(\"hello,\", x.value)\nq.closequery()\n# outputs:\n# hello, john\n# hello, gina\nthe core functionality of prolog.query is based on nathan denny's public domain prolog.py.\nhelp!\nsupport forum\nstack overflow\nprojects/publications that use or reference pyswip\ndo you have a project, video or publication that uses/mentions pyswip? file an issue or send a pull request.\nif you would like to reference pyswip in a latex document, you can use the provided bibtex file.\nbooks\nbeginning artificial intelligence with the raspberry pi\npublications\nassessment of graph databases as a viable materiel solution for the army's dynamic force structure (dfs) portal implementation: part 3, risks, mitigation approach, and roadmap\ntackling complexity in high performance computing applications\nsocial human-robot interaction: a new cognitive and affective interaction-oriented architecture\na planning module for a ros-based ubiquitous robot control system (pdf)\na pilot framework developed as a common platform integrating diverse elements of computer aided fixture design\nintegration von prolog und cliopatria in python (pdf, german)\nselectscript: a query language for robotic world models and simulations\na concept for declarative information acquisition in smart environments (pdf)\nimplementation on adhd diagnostic expert system based on dsm diagnostic criteria (pdf, korean)\nwie sehen krebsmolekule aus? vergleich der gute der klassifizierung potenziell krebserregender molekule durch induktiv logische und merkmalsbasierte lernverfahren (pdf, german)\ncompanion robots behaving with style: towards plasticity in social human-robot interaction (pdf)\nsemi-automatically augmenting attack trees using an annotated attack -----> tree !!!  library\na learning framework for tool creation by a robot (pdf)\nconceptual maps as the first step in an ontology construction method\nfact-based expert system for supplier selection with erp data\ninteractive text graph mining with a prolog-based dialog engine\nthe detection of conflicts in the requirements specification based on an ontological model and a production rule system\ndependency-based text graphs for keyphrase and summary extraction with applications to interactive content retrieval (pdf)\ninformation retrieval based on knowledge-enhanced word embedding through dialog: a case study\nexploring the world of declarative programming\nvideos\nai - blocks world solver interactive planner\npyswip, prolog, javascript and html (spanish)\nget out of the maze with prolog and python (spanish)\nles robots deviennent (vraiment) intelligents ! (nao discute avec kylo ren)\nprojects\nnoworkflow supporting infrastructure to run scientific experiments without a scientific workflow management system. http://gems-uff.github.io/noworkflow\nsuper pacman\npokemon weak detector\nfood recommendations in hyderabad, india food recommendation ai expert system using a gui hosted on flask and a backend developed with pyswip and native prolog.\npyswip_envctrl an environment control module expert system written in pyswip.\ntic-tac-toe tic-tac-toe game with ai in prolog and gui in python (kivy framework + pyswip).\ntbm1 - \"getting to know my home\"\nprolog natural language parsing component to control a scribbler ii robot over bluetooth\ncosmos a new logic programming language.\nlib-annotated-attack-trees scripts and resources for creating a library of annotated attack trees and using it to refine an annotated attack tree.\nclide command-line intelligent development environment\nartificial intelligence inf1771 @ puc-rio projects for the artificial intelligence class @ puc-rio\nautomobileadvisor projekt na systemy ekspertowe pomagaj\u0105cy wybra\u0107 odpowiedni samoch\u00f3d dla danego klienta na podstawie preferencji (polish)\nprolog tetris ai\njupyter swi prolog a jupyter kernel for swi-prolog.\nblocks world planner a program that allows users to solve the blocks world problem interacting only using the natural language.\ndeeptalk a python+prolog based dialog engine using the python package text_graph_crafts that extracts the highest ranked sentences answering a query.\ndeeprank the system uses dependency links for building text graphs, that with help of a centrality algorithm like pagerank, extract relevant keyphrases, summaries and relations from text documents.\nprolog tic-tac-toe a full-stack tic-tac-toe game with ai in prolog, backend in python3 (+flask) and frontend in vue.js 3.\nmidsi project solution for data discovery in projects applicable to the semantic web, enabling the loading of ontologies and inference of results using the wsml language.\npopper an inductive logic programming system.\nblog posts\ncalling prolog from python\npython v. prolog: round 1: fight!\npath follower: arduino+rasp on ros and its project code\n10 minutes to make a gui for your swi-prolog app via python\ncompanies using pyswip\nmagazino gmbh magazino develops and builds intelligent, mobile robots for intralogistics.\nlicense\ncopyright (c) 2007-2020 y\u00fcce tekol and pyswip contributors\npermission is hereby granted, free of charge, to any person obtaining a copy of\nthis software and associated documentation files (the \"software\"), to deal in\nthe software without restriction, including without limitation the rights to\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\nthe software, and to permit persons to whom the software is furnished to do so,\nsubject to the following conditions:\nthe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the software.\nthe software is provided \"as is\", without warranty of any kind, express or\nimplied, including but not limited to the warranties of merchantability, fitness\nfor a particular purpose and noninfringement. in no event shall the authors or\ncopyright holders be liable for any claim, damages or other liability, whether\nin an action of contract, tort or otherwise, arising from, out of or in\nconnection with the software or the use or other dealings in the software.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000139, "year": null}, {"Unnamed: 0": 1162, "autor": 142, "date": null, "content": "Introduction\nSkiffOS is a lightweight operating system for any Linux-compatible computer, ranging from RPi, Odroid, NVIDIA Jetson, to Desktop PCs, Laptops (i.e. Apple MacBook), Phones (PinePhone), Containers, or Cloud VMs. It is:\nAdoptable: any userspace can be imported/exported to/from container images.\nFamiliar: uses simple Makefile and KConfig language for configuration.\nFlexible: supports all major OS distributions inside containers.\nPortable: containers can be moved between machines of similar CPU type.\nReliable: changes inside user environments cannot break the host boot-up.\nReproducible: a given Skiff Git tree will always produce identical output.\nUses Buildroot to produce a minimal \"single-file\" host OS as a standardized base cross-platform operating system \"shim\" for hosting containers. Most Linux platforms have widely varying requirements for kernel, firmware, and additional hardware support packages. The immutable SkiffOS host system contains everything needed to support the hardware, cleanly separated from the applications.\nGetting started\nThe SKIFF_CONFIG comma-separated environment variable selects which configuration layers should be merged together to configure the build.\n$ make # lists all available layers\n$ export SKIFF_CONFIG=pi/4,skiff/core\n$ make configure # configure the system\n$ make compile # build the system\nAfter you run make configure Skiff will remember what you selected in SKIFF_CONFIG. The compile command instructs Skiff to build the system.\nAdjustments can be made to configuration layers, and make compile can be called again to re-pack the system image without re-building everything.\nYou can add your SSH public key to the target image by adding it to overrides/root_overlay/etc/skiff/authorized_keys/my-key.pub, or by adding it to your own custom configuration package.\nThe example above uses pi/4, which can be replaced with any of the hardware support packages listed in the Supported Systems table.\nOnce the build is complete, it's time to flash the system to a SD card. You will need to switch to sudo bash for this on most systems.\n$ sudo bash # switch to root\n$ blkid # look for your SD card's device file\n$ export PI_SD=/dev/sdz # make sure this is right!\n$ make cmd/pi/common/format # tell skiff to format the device\n$ make cmd/pi/common/install # tell skiff to install the os\nThe device needs to be formatted only one time, after which, the install command can be used to update the SkiffOS images without clearing the persistent state. The persist partition is not touched in this step, so anything you save there, including Docker state and system configuration, will not be modified.\nThere are many other utility commands made available by Buildroot, which can be listed using make br/help, some examples:\n$ make br/menuconfig # optionally explore Buildroot config\n$ make br/sdk # build relocatable SDK for target\n$ make br/graph-size # graph the target packages sizes\nConnect using SSH to root@my-ip-address to access the SkiffOS system, and connect to core@my-ip-address to access the \"Core\" system container. The mapping between users and containers can be edited in the /mnt/persist/skiff/core/config.yaml file.\nThe system can then be upgraded over-the-air (OTA) using the rsync script:\n$ ./scripts/push_image.bash root@my-ip-address\nThe SkiffOS upgrade (or downgrade) will take effect on next reboot.\nSupported Systems\nSkiffOS is based on Buildroot, which can compile operating systems for any Linux-compatible machine.\nHere are the boards/systems currently supported:\nBoard Config Package Bootloader Kernel Notes\nDocker Img virt/docker N/A N/A Run in Docker\nQemu virt/qemu N/A \u2714 5.15.8 Run in QEmu\nWSL on Windows virt/wsl N/A \u2714 msft-5.4.72 Run in WSL2\nApple Macbook apple/macbook \u2714 rEFInd \u2714 5.15.8 \u2714 Tested\nBananaPi M1 bananapi/m1 \u2714 U-Boot 2020.10 \u2714 5.15.8 \u26a0 Obsolete\nBananaPi M1+/Pro bananapi/m1plus \u2714 U-Boot 2020.10 \u2714 5.15.8 \u26a0 Obsolete\nBananaPi M2+ bananapi/m2plus \u2714 U-Boot 2020.10 \u2714 5.15.8\nBananaPi M3 bananapi/m3 \u2714 U-Boot 2020.10 \u2714 5.15.8 \u2714 Tested\nIntel x86/64 intel/x64 \u2714 Grub \u2714 5.15.8 \u2714 Tested\nNVIDIA Jetson Nano jetson/nano \u2714 U-Boot \u2714 4.9.253 \u2714 Tested\nNVIDIA Jetson TX2 jetson/tx2 \u2714 U-Boot \u2714 4.9.253 \u2714 Tested\nOdroid C2 odroid/c2 \u2714 U-Boot 2020.10 \u2714 tb-5.15.3 \u26a0 Obsolete\nOdroid C4 odroid/c4 \u2714 U-Boot 2021.01 \u2714 tb-5.15.3 \u2714 Tested\nOdroid U odroid/u \u2714 U-Boot 2016.03 \u2714 tb-5.15.3 \u26a0 Obsolete\nOdroid HC1 odroid/xu \u2714 U-Boot 2019.04 \u2714 tb-5.15.3 \u2714 Tested\nOdroid HC2 odroid/xu \u2714 U-Boot 2019.04 \u2714 tb-5.15.3 \u2714 Tested\nOdroid XU3 odroid/xu \u2714 U-Boot 2019.04 \u2714 tb-5.15.3 \u26a0 Obsolete\nOdroid XU4 odroid/xu \u2714 U-Boot 2019.04 \u2714 tb-5.15.3 \u2714 Tested\nOrangePi Lite orangepi/lite \u2714 U-Boot 2018.05 \u2714 5.15.8\nOrangePi Zero orangepi/zero \u2714 U-Boot 2018.07 \u2714 5.15.8\nPcDuino 3 pcduino/3 \u2714 U-Boot 2019.07 \u2714 5.15.8\nPcEngines APU2 pcengines/apu2 \u2714 CoreBoot \u2714 5.15.8\nPi 0 pi/0 N/A \u2714 rpi-5.10.83 \u2714 Tested\nPi 1 pi/1 N/A \u2714 rpi-5.10.83\nPi 3 + 1, 2 pi/3 N/A \u2714 rpi-5.10.83 \u2714 Tested\nPi 4 pi/4 N/A \u2714 rpi-5.10.83 \u2714 Tested\nPi 4 (32bit mode) pi/4x32 N/A \u2714 rpi-5.10.83 \u2714 Tested\nPine64 H64 pine64/h64 \u2714 U-Boot \u2714 pine64-5.8.0 \u2714 Tested\nPineBook Pro pine64/book \u2714 U-Boot (bin) \u2714 ayufan-5.13.0 \u2714 Tested\nPinePhone pine64/phone \u2714 U-Boot (bin) \u2714 megi-5.15.2 \u2714 Tested\nRockPro64 pine64/rockpro64 \u2714 U-Boot (bin) \u2714 ayufan-5.13.0 \u2714 Tested\nUSBArmory Mk2 usbarmory/mk2 \u2714 U-Boot 2020.10 \u2714 5.15.8 \u2714 Tested\nAll targets marked \"tested\" use automated end-to-end testing on real hardware. Targets marked \"Obsolete\" are discontinued by their manufacturer but still have a corresponding SkiffOS configuration and should still work.\nAdding support for a board involves creating a Skiff configuration package for the board, as described above. If you have a device that is not yet supported by SkiffOS, please open an issue.\nSkiff Core\nThe Skiff Core subsystem, enabled with the skiff/core layer or by selecting any of the core environment packages, automatically configures mappings between users and containerized environments. It maps incoming SSH sessions accordingly:\nConfigured using a YAML configuration file skiff-core.yaml.\nThe container image is either pulled or built from scratch.\nsystemd and/or other init systems operate as PID 1 inside the container.\nThis allows virtually any workflow to be migrated to Skiff. The config file structure is flexible, and allows for any number of containers, users, and images to be defined and built.\nEnvironment Presets\nAny existing GNU/Linux system with compatibility with the running kernel version can be loaded as a Docker image with the docker import command.\nAll core configurations work with all target platforms:\nDistribution Config Package Notes\nAlpine Linux core/alpine OpenRC\nDebian Bullseye core/debian\nDietPi core/dietpi DietPi applications tool\nGentoo core/gentoo Based on latest stage3\nNASA cFS Framework core/nasa_cfs Flight software framework\nNASA Fprime Framework core/nasa_fprime Flight software framework\nNixOS core/nixos\nNixOS for PinePhone core/pinephone_nixos\nNixOS with XFCE core/nixos_xfce\nUbuntu skiff/core With minimal desktop GUI\nPineBook Manjaro KDE core/pinebook_manjaro_kde KDE Variant\nPinePhone KDE Neon core/pinephone_neon Ubuntu-based KDE Neon\nPinePhone Manjaro KDE core/pinephone_manjaro_kde KDE Variant\nPinePhone Manjaro Lomiri core/pinephone_manjaro_lomiri Lomiri variant\nPinePhone Manjaro Phosh core/pinephone_manjaro_phosh Phosh variant\nPinePhone UBTouch core/pinephone_ubtouch Ubuntu touch\nCustomize Config\nThe default configuration creates a user named \"core\" mapped into a container, but this can be adjusted with the skiff-core.yaml configuration file:\ncontainers:\ncore:\nimage: skiffos/skiff-core-gentoo:latest\n[...]\nusers:\ncore:\ncontainer: core\ncontainerUser: core\n[...]\nThe full example config is in the skiff/core package.\nTo customize a running system, edit /mnt/persist/skiff/core/config.yaml and run systemctl restart skiff-core to apply. You may need to delete existing containers and restart skiff-core to re-create them after changing their config.\nThe config format is defined in the skiff-core repo.\nRelease Channels\nThere are three release channels: next, master, and stable.\nSkiff can be upgraded or downgraded (rolled back) independently from the persistent storage partition. This allows for easy OTA, and significant improvements in confidence when upgrading system components.\nConfiguration Packages/Layers\nSkiff supports modular configuration packages. A configuration directory contains kernel configs, buildroot configs, system overlays, etc.\nThese packages are denoted as namespace/name. For example, an ODROID XU4 configuration would be odroid/xu.\nConfiguration package directories should have a depth of 2, where the first directory is the category name and the second is the package name.\nPackage Layout\nA configuration package is laid out into the following directories:\n\u251c\u2500\u2500 cflags: compiler flags in files\n\u251c\u2500\u2500 buildroot: buildroot configuration fragments\n\u251c\u2500\u2500 buildroot_ext: buildroot extensions (extra packages)\n\u251c\u2500\u2500 buildroot_patches: extra Buildroot global patches\n\u2502 \u251c\u2500\u2500 <packagename>: patch files for Buildroot <packagename>\n\u2502 \u2514\u2500\u2500 <packagename>/<version>: patches for package version\n\u251c\u2500\u2500 extensions: extra commands to add to the build system\n\u2502 \u2514\u2500\u2500 Makefile\n\u251c\u2500\u2500 hooks: scripts hooking pre/post build steps\n\u2502 \u251c\u2500\u2500 post.sh\n\u2502 \u2514\u2500\u2500 pre.sh\n\u251c\u2500\u2500 kernel: kernel configuration fragments\n\u251c\u2500\u2500 kernel_patches: kernel .patch files\n\u251c\u2500\u2500 root_overlay: root overlay files\n\u251c\u2500\u2500 metadata: metadata files\n\u2502 \u251c\u2500\u2500 commands\n\u2502 \u251c\u2500\u2500 dependencies\n\u2502 \u251c\u2500\u2500 description\n\u2502 \u2514\u2500\u2500 unlisted\n\u251c\u2500\u2500 resources: files used by the configuration package\n\u251c\u2500\u2500 scripts: any scripts used by the extensions\n\u251c\u2500\u2500 uboot: u-boot configuration fragments\n\u2514\u2500\u2500 uboot_patches: u-boot .patch files\nAll files are optional.\nOut-of-tree configuration packages\nYou can set the following env variables to control this process:\nSKIFF_CONFIG_PATH_ODROID_XU: Set the path for the ODROID_XU config package. You can set this to add new packages or override old ones.\nSKIFF_EXTRA_CONFIGS_PATH: Colon separated list of paths to look for config packages.\nSKIFF_CONFIG: Name of skiff config to use, or comma separated list to overlay, with the later options taking precedence\nThese packages will be available in the Skiff system.\nLocal Overrides\nIt's often useful to be able to adjust the buildroot, kernel, or other configurations locally during development without actually creating a new configuration layer. This can be easily done with the overrides system.\nThe overrides directory, as well as the overrides/workspaces/$SKIFF_WORKSPACE directory, are automatically used as additional Skiff configuration packages. You can follow the Skiff configuration package format as defined below to override any of the settings in Buildroot or the Linux kernel, add extra Buildroot packages, add build hooks, etc.\nWorkspaces\nWorkspaces allow you to configure and compile multiple systems at a time.\nSet SKIFF_WORKSPACE to the name of the workspace you want to use. The Buildroot setup will be constructed in workspaces/$SKIFF_WORKSPACE. You can also place configuration files in overrides/workspaces/$SKIFF_WORKSPACE/ to override settings for that particular workspace locally.\nVirtualization\nThe virt/ packages are designed for running Skiff in various virtualized environments.\nQemu\nHere is a minimal working example of running Skiff in Qemu:\n$ SKIFF_CONFIG=virt/qemu make configure compile\n$ make cmd/virt/qemu/run\nDocker\nHere is a minimal working example of running Skiff in Docker:\n$ SKIFF_CONFIG=virt/docker,skiff/core make configure compile\n$ make cmd/virt/docker/buildimage\n$ make cmd/virt/docker/run\n# inside container\n$ su - core\nThe build command compiles the image, and run executes it.\nYou can execute a shell inside the container with:\n$ make cmd/virt/docker/exec\n# alternatively\n$ docker exec -it skiff sh\nAlternatively, run the latest demo release on Docker Hub:\ndocker run -t -d --name=skiff \\\n--privileged \\\n--cap-add=NET_ADMIN \\\n--security-opt seccomp=unconfined \\\n--stop-signal=SIGRTMIN+3 \\\n-v /sys/fs/cgroup:/sys/fs/cgroup:ro \\\n-v $(pwd)/skiff-persist:/mnt/persist \\\nskiffos/skiffos:latest\nConfiguration\nSkiffOS includes a systemd-based configuration and a standard partition layout, with boot files separated from the persistent data, on default. This can be disabled, overridden, and/or customized by other configuration packages.\nNetworkManager\nSkiff uses NetworkManager to manage network connections.\nNetwork configurations are loaded from /etc/NetworkManager/system-connections and from skiff/connections on the persist partition.\nThe configuration file format for these connections is documented here with examples.\nYou can use nmcli on the device to manage NetworkManager, and any connection definitions written by nmcli device wifi connect or similar will automatically be written to the persist partition and persisted to future boots.\nHostname\nYou can set the hostname by placing the desired hostname in the skiff/hostname file on the persist partition. You could also set this in one of your config packages by writing the desired hostname to /etc/hostname.\nSSH Keys\nThe system on boot will generate the authorized_keys file for root.\nIt takes SSH public key files (*.pub) from these locations:\n/etc/skiff/authorized_keys from inside the image\nskiff/keys from inside the persist partition\nMount a Disk to a Container\nTo mount a Linux disk, for example an ext4 partition, to a path inside a Docker container, you can use the Docker Volumes feature:\n# create a volume for the storage drive\ndocker volume create --driver=local --opt device=/dev/disk/by-label/storage storage\n# run a temporary container to view the contents\ndocker run --rm -it -v storage:/storage --workdir /storage alpine:edge sh\nThe volume can be mounted into a Skiff Core container by adding to the mounts list in /mnt/persist/skiff/core/config.yaml:\ncontainers:\ncore:\nimage: skiffos/skiff-core-gentoo:latest\nmounts:\n- storage:/mnt/storage\nAfter adding the mount, delete and re-create the container:\ndocker rm -f core\nsystemctl restart skiff-core\nBuild in Docker\nYou can build Skiff inside Docker if you encounter any incompatibility with your build host operating system.\nSupport\nIf you encounter issues or questions at any point when using Skiff, please file a GitHub issue and/or Join Discord.", "link": "https://github.com/skiffos/SkiffOS", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "introduction\nskiffos is a lightweight operating system for any linux-compatible computer, ranging from rpi, odroid, nvidia jetson, to desktop pcs, laptops (i.e. apple macbook), phones (pinephone), containers, or cloud vms. it is:\nadoptable: any userspace can be imported/exported to/from container images.\nfamiliar: uses simple makefile and kconfig language for configuration.\nflexible: supports all major os distributions inside containers.\nportable: containers can be moved between machines of similar cpu type.\nreliable: changes inside user environments cannot break the host boot-up.\nreproducible: a given skiff git -----> tree !!!  will always produce identical output.\nuses buildroot to produce a minimal \"single-file\" host os as a standardized base cross-platform operating system \"shim\" for hosting containers. most linux platforms have widely varying requirements for kernel, firmware, and additional hardware support packages. the immutable skiffos host system contains everything needed to support the hardware, cleanly separated from the applications.\ngetting started\nthe skiff_config comma-separated environment variable selects which configuration layers should be merged together to configure the build.\n$ make # lists all available layers\n$ export skiff_config=pi/4,skiff/core\n$ make configure # configure the system\n$ make compile # build the system\nafter you run make configure skiff will remember what you selected in skiff_config. the compile command instructs skiff to build the system.\nadjustments can be made to configuration layers, and make compile can be called again to re-pack the system image without re-building everything.\nyou can add your ssh public key to the target image by adding it to overrides/root_overlay/etc/skiff/authorized_keys/my-key.pub, or by adding it to your own custom configuration package.\nthe example above uses pi/4, which can be replaced with any of the hardware support packages listed in the supported systems table.\nonce the build is complete, it's time to flash the system to a sd card. you will need to switch to sudo bash for this on most systems.\n$ sudo bash # switch to root\n$ blkid # look for your sd card's device file\n$ export pi_sd=/dev/sdz # make sure this is right!\n$ make cmd/pi/common/format # tell skiff to format the device\n$ make cmd/pi/common/install # tell skiff to install the os\nthe device needs to be formatted only one time, after which, the install command can be used to update the skiffos images without clearing the persistent state. the persist partition is not touched in this step, so anything you save there, including docker state and system configuration, will not be modified.\nthere are many other utility commands made available by buildroot, which can be listed using make br/help, some examples:\n$ make br/menuconfig # optionally explore buildroot config\n$ make br/sdk # build relocatable sdk for target\n$ make br/graph-size # graph the target packages sizes\nconnect using ssh to root@my-ip-address to access the skiffos system, and connect to core@my-ip-address to access the \"core\" system container. the mapping between users and containers can be edited in the /mnt/persist/skiff/core/config.yaml file.\nthe system can then be upgraded over-the-air (ota) using the rsync script:\n$ ./scripts/push_image.bash root@my-ip-address\nthe skiffos upgrade (or downgrade) will take effect on next reboot.\nsupported systems\nskiffos is based on buildroot, which can compile operating systems for any linux-compatible machine.\nhere are the boards/systems currently supported:\nboard config package bootloader kernel notes\ndocker img virt/docker n/a n/a run in docker\nqemu virt/qemu n/a \u2714 5.15.8 run in qemu\nwsl on windows virt/wsl n/a \u2714 msft-5.4.72 run in wsl2\napple macbook apple/macbook \u2714 refind \u2714 5.15.8 \u2714 tested\nbananapi m1 bananapi/m1 \u2714 u-boot 2020.10 \u2714 5.15.8 \u26a0 obsolete\nbananapi m1+/pro bananapi/m1plus \u2714 u-boot 2020.10 \u2714 5.15.8 \u26a0 obsolete\nbananapi m2+ bananapi/m2plus \u2714 u-boot 2020.10 \u2714 5.15.8\nbananapi m3 bananapi/m3 \u2714 u-boot 2020.10 \u2714 5.15.8 \u2714 tested\nintel x86/64 intel/x64 \u2714 grub \u2714 5.15.8 \u2714 tested\nnvidia jetson nano jetson/nano \u2714 u-boot \u2714 4.9.253 \u2714 tested\nnvidia jetson tx2 jetson/tx2 \u2714 u-boot \u2714 4.9.253 \u2714 tested\nodroid c2 odroid/c2 \u2714 u-boot 2020.10 \u2714 tb-5.15.3 \u26a0 obsolete\nodroid c4 odroid/c4 \u2714 u-boot 2021.01 \u2714 tb-5.15.3 \u2714 tested\nodroid u odroid/u \u2714 u-boot 2016.03 \u2714 tb-5.15.3 \u26a0 obsolete\nodroid hc1 odroid/xu \u2714 u-boot 2019.04 \u2714 tb-5.15.3 \u2714 tested\nodroid hc2 odroid/xu \u2714 u-boot 2019.04 \u2714 tb-5.15.3 \u2714 tested\nodroid xu3 odroid/xu \u2714 u-boot 2019.04 \u2714 tb-5.15.3 \u26a0 obsolete\nodroid xu4 odroid/xu \u2714 u-boot 2019.04 \u2714 tb-5.15.3 \u2714 tested\norangepi lite orangepi/lite \u2714 u-boot 2018.05 \u2714 5.15.8\norangepi zero orangepi/zero \u2714 u-boot 2018.07 \u2714 5.15.8\npcduino 3 pcduino/3 \u2714 u-boot 2019.07 \u2714 5.15.8\npcengines apu2 pcengines/apu2 \u2714 coreboot \u2714 5.15.8\npi 0 pi/0 n/a \u2714 rpi-5.10.83 \u2714 tested\npi 1 pi/1 n/a \u2714 rpi-5.10.83\npi 3 + 1, 2 pi/3 n/a \u2714 rpi-5.10.83 \u2714 tested\npi 4 pi/4 n/a \u2714 rpi-5.10.83 \u2714 tested\npi 4 (32bit mode) pi/4x32 n/a \u2714 rpi-5.10.83 \u2714 tested\npine64 h64 pine64/h64 \u2714 u-boot \u2714 pine64-5.8.0 \u2714 tested\npinebook pro pine64/book \u2714 u-boot (bin) \u2714 ayufan-5.13.0 \u2714 tested\npinephone pine64/phone \u2714 u-boot (bin) \u2714 megi-5.15.2 \u2714 tested\nrockpro64 pine64/rockpro64 \u2714 u-boot (bin) \u2714 ayufan-5.13.0 \u2714 tested\nusbarmory mk2 usbarmory/mk2 \u2714 u-boot 2020.10 \u2714 5.15.8 \u2714 tested\nall targets marked \"tested\" use automated end-to-end testing on real hardware. targets marked \"obsolete\" are discontinued by their manufacturer but still have a corresponding skiffos configuration and should still work.\nadding support for a board involves creating a skiff configuration package for the board, as described above. if you have a device that is not yet supported by skiffos, please open an issue.\nskiff core\nthe skiff core subsystem, enabled with the skiff/core layer or by selecting any of the core environment packages, automatically configures mappings between users and containerized environments. it maps incoming ssh sessions accordingly:\nconfigured using a yaml configuration file skiff-core.yaml.\nthe container image is either pulled or built from scratch.\nsystemd and/or other init systems operate as pid 1 inside the container.\nthis allows virtually any workflow to be migrated to skiff. the config file structure is flexible, and allows for any number of containers, users, and images to be defined and built.\nenvironment presets\nany existing gnu/linux system with compatibility with the running kernel version can be loaded as a docker image with the docker import command.\nall core configurations work with all target platforms:\ndistribution config package notes\nalpine linux core/alpine openrc\ndebian bullseye core/debian\ndietpi core/dietpi dietpi applications tool\ngentoo core/gentoo based on latest stage3\nnasa cfs framework core/nasa_cfs flight software framework\nnasa fprime framework core/nasa_fprime flight software framework\nnixos core/nixos\nnixos for pinephone core/pinephone_nixos\nnixos with xfce core/nixos_xfce\nubuntu skiff/core with minimal desktop gui\npinebook manjaro kde core/pinebook_manjaro_kde kde variant\npinephone kde neon core/pinephone_neon ubuntu-based kde neon\npinephone manjaro kde core/pinephone_manjaro_kde kde variant\npinephone manjaro lomiri core/pinephone_manjaro_lomiri lomiri variant\npinephone manjaro phosh core/pinephone_manjaro_phosh phosh variant\npinephone ubtouch core/pinephone_ubtouch ubuntu touch\ncustomize config\nthe default configuration creates a user named \"core\" mapped into a container, but this can be adjusted with the skiff-core.yaml configuration file:\ncontainers:\ncore:\nimage: skiffos/skiff-core-gentoo:latest\n[...]\nusers:\ncore:\ncontainer: core\ncontaineruser: core\n[...]\nthe full example config is in the skiff/core package.\nto customize a running system, edit /mnt/persist/skiff/core/config.yaml and run systemctl restart skiff-core to apply. you may need to delete existing containers and restart skiff-core to re-create them after changing their config.\nthe config format is defined in the skiff-core repo.\nrelease channels\nthere are three release channels: next, master, and stable.\nskiff can be upgraded or downgraded (rolled back) independently from the persistent storage partition. this allows for easy ota, and significant improvements in confidence when upgrading system components.\nconfiguration packages/layers\nskiff supports modular configuration packages. a configuration directory contains kernel configs, buildroot configs, system overlays, etc.\nthese packages are denoted as namespace/name. for example, an odroid xu4 configuration would be odroid/xu.\nconfiguration package directories should have a depth of 2, where the first directory is the category name and the second is the package name.\npackage layout\na configuration package is laid out into the following directories:\n\u251c\u2500\u2500 cflags: compiler flags in files\n\u251c\u2500\u2500 buildroot: buildroot configuration fragments\n\u251c\u2500\u2500 buildroot_ext: buildroot extensions (extra packages)\n\u251c\u2500\u2500 buildroot_patches: extra buildroot global patches\n\u2502 \u251c\u2500\u2500 <packagename>: patch files for buildroot <packagename>\n\u2502 \u2514\u2500\u2500 <packagename>/<version>: patches for package version\n\u251c\u2500\u2500 extensions: extra commands to add to the build system\n\u2502 \u2514\u2500\u2500 makefile\n\u251c\u2500\u2500 hooks: scripts hooking pre/post build steps\n\u2502 \u251c\u2500\u2500 post.sh\n\u2502 \u2514\u2500\u2500 pre.sh\n\u251c\u2500\u2500 kernel: kernel configuration fragments\n\u251c\u2500\u2500 kernel_patches: kernel .patch files\n\u251c\u2500\u2500 root_overlay: root overlay files\n\u251c\u2500\u2500 metadata: metadata files\n\u2502 \u251c\u2500\u2500 commands\n\u2502 \u251c\u2500\u2500 dependencies\n\u2502 \u251c\u2500\u2500 description\n\u2502 \u2514\u2500\u2500 unlisted\n\u251c\u2500\u2500 resources: files used by the configuration package\n\u251c\u2500\u2500 scripts: any scripts used by the extensions\n\u251c\u2500\u2500 uboot: u-boot configuration fragments\n\u2514\u2500\u2500 uboot_patches: u-boot .patch files\nall files are optional.\nout-of-tree configuration packages\nyou can set the following env variables to control this process:\nskiff_config_path_odroid_xu: set the path for the odroid_xu config package. you can set this to add new packages or override old ones.\nskiff_extra_configs_path: colon separated list of paths to look for config packages.\nskiff_config: name of skiff config to use, or comma separated list to overlay, with the later options taking precedence\nthese packages will be available in the skiff system.\nlocal overrides\nit's often useful to be able to adjust the buildroot, kernel, or other configurations locally during development without actually creating a new configuration layer. this can be easily done with the overrides system.\nthe overrides directory, as well as the overrides/workspaces/$skiff_workspace directory, are automatically used as additional skiff configuration packages. you can follow the skiff configuration package format as defined below to override any of the settings in buildroot or the linux kernel, add extra buildroot packages, add build hooks, etc.\nworkspaces\nworkspaces allow you to configure and compile multiple systems at a time.\nset skiff_workspace to the name of the workspace you want to use. the buildroot setup will be constructed in workspaces/$skiff_workspace. you can also place configuration files in overrides/workspaces/$skiff_workspace/ to override settings for that particular workspace locally.\nvirtualization\nthe virt/ packages are designed for running skiff in various virtualized environments.\nqemu\nhere is a minimal working example of running skiff in qemu:\n$ skiff_config=virt/qemu make configure compile\n$ make cmd/virt/qemu/run\ndocker\nhere is a minimal working example of running skiff in docker:\n$ skiff_config=virt/docker,skiff/core make configure compile\n$ make cmd/virt/docker/buildimage\n$ make cmd/virt/docker/run\n# inside container\n$ su - core\nthe build command compiles the image, and run executes it.\nyou can execute a shell inside the container with:\n$ make cmd/virt/docker/exec\n# alternatively\n$ docker exec -it skiff sh\nalternatively, run the latest demo release on docker hub:\ndocker run -t -d --name=skiff \\\n--privileged \\\n--cap-add=net_admin \\\n--security-opt seccomp=unconfined \\\n--stop-signal=sigrtmin+3 \\\n-v /sys/fs/cgroup:/sys/fs/cgroup:ro \\\n-v $(pwd)/skiff-persist:/mnt/persist \\\nskiffos/skiffos:latest\nconfiguration\nskiffos includes a systemd-based configuration and a standard partition layout, with boot files separated from the persistent data, on default. this can be disabled, overridden, and/or customized by other configuration packages.\nnetworkmanager\nskiff uses networkmanager to manage network connections.\nnetwork configurations are loaded from /etc/networkmanager/system-connections and from skiff/connections on the persist partition.\nthe configuration file format for these connections is documented here with examples.\nyou can use nmcli on the device to manage networkmanager, and any connection definitions written by nmcli device wifi connect or similar will automatically be written to the persist partition and persisted to future boots.\nhostname\nyou can set the hostname by placing the desired hostname in the skiff/hostname file on the persist partition. you could also set this in one of your config packages by writing the desired hostname to /etc/hostname.\nssh keys\nthe system on boot will generate the authorized_keys file for root.\nit takes ssh public key files (*.pub) from these locations:\n/etc/skiff/authorized_keys from inside the image\nskiff/keys from inside the persist partition\nmount a disk to a container\nto mount a linux disk, for example an ext4 partition, to a path inside a docker container, you can use the docker volumes feature:\n# create a volume for the storage drive\ndocker volume create --driver=local --opt device=/dev/disk/by-label/storage storage\n# run a temporary container to view the contents\ndocker run --rm -it -v storage:/storage --workdir /storage alpine:edge sh\nthe volume can be mounted into a skiff core container by adding to the mounts list in /mnt/persist/skiff/core/config.yaml:\ncontainers:\ncore:\nimage: skiffos/skiff-core-gentoo:latest\nmounts:\n- storage:/mnt/storage\nafter adding the mount, delete and re-create the container:\ndocker rm -f core\nsystemctl restart skiff-core\nbuild in docker\nyou can build skiff inside docker if you encounter any incompatibility with your build host operating system.\nsupport\nif you encounter issues or questions at any point when using skiff, please file a github issue and/or join discord.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000142, "year": null}, {"Unnamed: 0": 1196, "autor": 176, "date": null, "content": "IROS2019-paper-list\nThe 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2019) has been held on November 4 \u2013 8, 2019 in The Venetian Macao, Macau, China. IROS is one of the largest and most impacting robotics research conferences worldwide. It brings an international community of researchers, educators and practitioners to explore the frontier of science and technology in intelligent robots and systems, and discuss the latest advancements in this fast growing and exciting field.\nThis list is edited by PaopaoRobot, \u6ce1\u6ce1\u673a\u5668\u4eba , the Chinese academic nonprofit organization. Recently we will classify these papers by topics. Welcome to follow our github and our WeChat Public Platform Account ( paopaorobot_slam ). Of course, you could contact with Yvon Shong.\ntitle index\nOutlier-Robust Manifold Pre-Integration for INS/GPS Fusion 0688\nAutonomous Steering of Concentric Tube Robots for Enhanced Force/Velocity Manipulability 0813\nA 2-piece six-axis force/torque sensor capable of measuring loads applied to tools of complex shapes 1744\nAutonomous Hybrid Ground/Aerial Mobility in Unknown Environments 2028\nA Hybrid Active/Passive Wrist Approach for Increasing Virtual Fixture Stiffness in Comanipulated Robotic Minimally Invasive Surgery 2255\n6-Axis Hybrid Sensing and Estimation of Tip Forces/Torques on a Hyper-Redundant Robotic Surgical Instrument 2324\nHybrid Force/Motion Control and Implementation of an Aerial Manipulator towards Sustained Contact Operations 2586\nXL-laser: Large-Scale Cable-Driven Laser Cutting/Engraving Robot 2742\n2D Contour Following with an Unmanned Aerial Manipulator: Towards Tactile-Based Aerial Navigation 1391\n2-DOF Transformable Wheel Design for Various Sized Stair and Step Climbing 2643\n2-Entity RANSAC for Robust Visual Localization in Changing Environment 0982\n3D Canonical Pose Estimation and Abnormal Gait Recognition with a Single RGB-D Camera 2323\n3D Deformable Object Manipulation Using Deep Neural Networks 2307\n3D LiDAR and Stereo Fusion Using Stereo Matching Network with Conditional Cost Volume Normalization 1026\n3D Micromanipulation of Particle Swarm Using a Hexapole Magnetic Tweezer 1134\n3D Move to See: Multi-Perspective Visual Servoing towards the Next Best View within Unstructured and Occluded Environments 1719\n3-DOF Gravity Compensation Mechanism for Robot Waists with the Variations of Center of Mass 0992\n3-DOF Manipulator Design for a Slender-Shaped Wide End-Effector 2630\n3D Point Cloud Data Acquisition Using a Synchronized In-Air Imaging Sonar Sensor Network 0969\n3D Printed Single Incision Laparoscopic Manipulator System Adapted to the Required Forces in Laparoscopic Surgery 0964\n3D Reconstruction by Single Camera Omnidirectional Multi-Stereo System 0095\n3D Shape Control of Linear Deformable Objects by Robot Manipulator 2664\nA 3D Static Modeling Method and Experimental Verification of Continuum Robots Based on Pseudo-Rigid Body Theory 1362\nA Behavior Driven Approach for Sampling Rare-Event Situations for Autonomous Vehicles 0265\nA Behavior Tree Cognitive Assistant System for Emergency Medical Services 0449\nA Benchmark for Visual-Inertial Odometry Systems Employing Onboard Illumination 1770\nA Bi-Directional Multiple Timescales LSTM Model for Grounding of Actions and Verbs 0344\nAbsolute Localization through Orbital Maps and Surface Perspective Imagery: A Synthetic Lunar Dataset and Neural Network Approach 1712\nA Capability-Aware Role Allocation Approach to Industrial Assembly Tasks 2472\nAccelerated Visual Inertial Navigation Via Fragmented Structure Updates 0363\nAccelerating the Construction of Boundaries of Feasibility in Three Classes of Robot Design Problems 1434\nAccurate Pouring Using Model Predictive Control Enabled by Recurrent Neural Network 1981\nA Challenge of Deformation Control for Cloth Actuators 2740\nAchievement of Online Agile Manipulation Task for Aerial Transformable Multilink Robot 1396\nA Compact Laser-Steering End-Effector for Transoral Robotic Surgery 1804\nA Compact Soft Articulated Parallel Wrist for Grasping in Narrow Spaces 2170\nA Comparative Analysis on the Use of Autoencoders for Robot Security Anomaly Detection 0417\nA Comparison of Action Spaces for Learning Manipulation Tasks 1513\nA Comparison of Visual Servoing from Features Velocity and Acceleration Interaction Models 0827\nA Convex-Combinatorial Model for Planar Caging 0896\nA Convolutional Network for Joint Deraining and Dehazing from A Single Image for Autonomous Driving in Rain 0696\nA Convolutional Neural Network Feature Detection Approach to Autonomous Quadrotor Indoor Navigation 1540\nAcoustic Length Sensor for Soft Extensible Pneumatic Actuators with a Frequency Characteristics Model 2535\nAction Recognition Based on 3D Skeleton and RGB Frame Fusion 1903\nActive Incremental Learning of a Contextual Skill Model 0480\nActive Infrared Coded Target Design and Pose Estimation for Multiple Objects 0310\nActive Inverse Model Learning with Error and Reachable Set Estimates 1235\nActive Learning of Reward Dynamics from Hierarchical Queries 1879\nActive SLAM Using Connectivity Graphs As Priors 0630\nActive Whisker Placement and Exploration for Rapid Object Recognition 1329\nActuation and Stiffening in Fluid-Driven Soft Robots Using Low-Melting-Point Material 0489\nAdapting Weed Growth Predictions for Mechanical Weeding Agbots 2616\nAdaptive Adversarial Videos on Roadside Billboards: Dynamically Modifying Trajectories of Autonomous Vehicles 1939\nAdaptive Assist-As-Needed Control Based on Actor-Critic Reinforcement Learning 1191\nAdaptive Deep Path: Efficient Coverage of a Known Environment under Various Configurations 1142\nAdaptive Dynamic Control for Magnetically Actuated Medical Robots 2435\nAdaptive Leader-Follower Formation Control and Obstacle Avoidance Via Deep Reinforcement Learning 1241\nAdaptive Loss Balancing for Multitask Learning of Object Instance Recognition and 3D Pose Estimation 0698\nAdaptive Navigation Scheme for Optimal Deep-Sea Localization Using Multimodal Perception Cues 1565\nAdaptive Neural Admittance Control for Collision Avoidance in Human-Robot Collaborative Tasks 0510\nAdaptive Outcome Selection for Planning with Reduced Models 1181\nAdaptive Risk-Based Replanning for Human-Aware Multi-Robot Task Allocation with Local Perception 2188\nAdaptive Swept Volumes Generation for Human-Robot Coexistence Using Gaussian Processes 0789\nAdaptive Trajectory Planning and Optimization at Limits of Handling 1698\nAdaptive Unscented Kalman Filter-Based Disturbance Rejection with Application to High Precision Hydraulic Robotic Control 0817\nAdaptive Vision-Based Control for Rope-Climbing Robot Manipulator 0712\nA Data-Driven Framework for Learning Dexterous Manipulation of Unknown Objects 1014\nA Deep Learning Approach for Multi-View Engagement Estimation of Children in a Child-Robot Joint Attention Task 0522\nA Deep Learning Approach for Probabilistic Security in Multi-Robot Teams 2576\nA Deep Learning Approach for Robust Corridor Following 1158\nA Density Map Estimation Model with DropBlock Regularization for Clustered-Fruit Counting 1774\nA Development of Inertial-2D LiDAR SLAM on Manifolds towards AGV 2764\nA Distributed Reconfiguration Planning Algorithm for SMORES-EP, a Modular Robot 2565\nAdjusting Weight of Action Decision in Exploration for Logistics Warehouse Picking Learning 0213\nAdmittance Control Based on Stiffness Ellipse for Collision Force Control of Object Manipulation 2758\nAdvanced Autonomy on a Low-Cost Educational Drone Platform 1806\nAdvection and Diffusion Effects towards a Bio-Inspired Artificial Pheromone System 2626\nA Dynamic Optimization Approach for Sloshing Free Transport of Liquid Filled Containers Using an Industrial Robot 1405\nAerial Animal Biometrics: Individual Friesian Cattle Recovery and Visual Identification Via an Autonomous UAV with Onboard Deep Inference 0851\nAerial Robot Control in Close Proximity to Ceiling: A Force Estimation-Based Nonlinear MPC 0450\nAerial Robots with Advanced Manipulation Capabilities for Inspection and Maintenance: The AEROARMS Project 2610\nAerodynamic Model Identification of a Quadrotor Subject to Rotor Failures in the High-Speed Flight Regime 2433\nA-EXP4: Online Social Policy Learning for Adaptive Robot-Pedestrian Interaction 0034\nA Fast Free-Viewpoint Video Synthesis Algorithm for Sports Scenes 0318\nA Fast Heuristic Path Planning Algorithm for Mobile Robots 2747\nA fast online frequency adaptation mechanism for CPG-based robot motion control 2346\nAffordance Learning for End-To-End Visuomotor Robot Control 1257\nA Flexible Sensor for Suture Training 2545\nA Force-Controlled Robotic Wrist Module for the Macro-Micro Manipulation of Industrial Robots 2707\nA Framework for Depth Estimation and Relative Localization of Ground Robots Using Computer Vision 0115\nA Fully-Integrated Sensing and Control System for High-Accuracy Mobile Robotic Building Construction 1219\nA Gear-Driven Prosthetic Hand with Major Grasp Functions for Toddlers 1869\nA Generative Model of Underwater Images for Active Landmark Detection and Docking 0113\nAgent Prioritization for Autonomous Navigation 0560\nAgile Standing-Up Control of Humanoids: Energy-Based Reactive Contact Wrench Optimization with Strict Dynamic Consistency 1646\nA GPS-Aided Omnidirectional Visual-Inertial State Estimator in Ubiquitous Environments 2174\nA Handheld Master Controller for Robot-Assisted Microsurgery 1992\nAir to Ground Collaboration for Energy-Efficient Path Planing for Ground Robots 1520\nA Joint Optimization Approach of LiDAR-Camera Fusion for Accurate Dense 3D Reconstructions 2268\nA Kernelized Approach for Learning and Adapting Symmetric Positive Definite Profiles 2684\nA Learning-Based Inverse Kinematics Solver for Two-Segment Continuum Robot Models 2750\nA Linear Series Elastic Actuator for Accurate Force and Impedance Control with High Torque-To-Rotor-Inertia Ratios 2724\nALTRO: A Fast Solver for Constrained Trajectory Optimization 1603\nA Magnetically Transduced Whisker for Angular Displacement and Moment Sensing 2105\nAmbiguity Poses Estimation for Objects with Symmetry 2773\nA Mechanical Approach to Suppress the Oscillation of a Long Continuum Robot Flying with Water Jets 2564\nA Method for Designing Low-Profile Compliant Transmission Mechanisms 2463\nA Method for Guiding a Person Combining Robot Movement and Projection 0155\nA Methodology for Formulating and Exploiting Innovative Technologies for Collaborative Robots in a Manufacturing Setting 2683\nA Mobile Extendable Robot Arm: Singularity Analysis and Design 1526\nA Model-Based Human Activity Recognition for Human-Robot Collaboration 0569\nA Model for Simulating the Robotic Pushing of Dirt 0860\nA Multi-Channel Embedded DSP Closed-Loop Control System for Musical Robots 0096\nA Multiclass EEG Signal Classification Model Using Spatial Feature Extraction and XGBoost Algorithm 0926\nA Multi-DOF Human-Powered Robot Using Regenerative Clutches and Constant-Force Springs 0914\nA Multimodal Human-Robot Interaction Manager for Assistive Robots 1728\nA Multimodal Soft Crawling-Climbing Robot with the Controllable Horizontal Plane to Slope Transition 1676\nA Multi-Task Convolutional Neural Network for Autonomous Robotic Grasping in Object Stacking Scenes 0395\nA Multi-Trainee Architecture for Haptic Hands-On Training 0762\nAn Adaptive Velocity Obstacle Avoidance Algorithm for Autonomous Surface Vehicles 0979\nAnalysis and Exploitation of Synchronized Parallel Executions in Behavior Trees 0845\nAnalysis of Ground Effect for Small-Scale UAVs in Forward Flight 2392\nAnalyzing Liquid Pouring Sequences Via Audio-Visual Neural Networks 0041\nAn Approach of Facilitated Investigation of Active Self-healing Tension Transmission System Oriented for Legged Robots 1813\nAn Approximation-Free Simple Control Scheme for Uncertain Quadrotor Systems: Theory and Validations 1074\nAn Assisted Telemanipulation Approach: Combining Autonomous Grasp Planning with Haptic Cues 0501\nAn Assistive Low-Vision Platform That Augments Spatial Cognition through Proprioceptive Guidance: Point-To-Tell-And-Touch 1252\nAn Asynchronous Multi-Body Simulation Framework for Real-Time Dynamics, Haptics and Learning with Application to Surgical Robots 2113\nAn Augmented Reality Interface for Human-Robot Interaction in Unconstrained Environments 1080\nAn Automated Learning-Based Procedure for Large-Scale Vehicle Dynamics Modeling on Baidu Apollo Platform 0508\nAn Autonomous Exploration Algorithm Using Environment-Robot Interacted Traversability Analysis 0723\nAn Autonomous Quadrotor System for Robust High-Speed Flight through Cluttered Environments without GPS 2166\nAn Educational Robotic Platform with Multimodal Perception for Teaching Sensor Servoing Controls 2629\nAn Efficient and Accurate Algorithm for the Perspective-N-Point Problem 1972\nAn Efficient Scheduling Algorithm for Multi-Robot Task Allocation in Assembling Aircraft Structures 2571\nA Neurologically Inspired Sequence Processing Model for Mobile Robot Place Recognition 2115\nAn Evaluation of Robot-To-Human Handover Configurations for Commercial Robots 0766\nA New Time-Varying Feedback RISE Control of PKMs: Theory and Application 0088\nAn Experimental Study of Parameters Influencing Physical Human-Robot Negotiation in Comanipulative Tracking Task 0224\nAngle of Arrival Estimation Based on Channel Impulse Response Measurements 2093\nAn In-Pipe Inspection Module with an Omnidirectional Bent-Pipe Self-Adaptation Mechanism Using a Joint Torque Control 0551\nAn Integrated Delta Manipulator for Aerial Repair: A New Aerial Robotic System 2608\nAn Interactive Indoor Drone Assistant 1038\nAn Interactive Method for Virtual Fixture Generation in Unstructured Environments 2704\nAn Interactive Physically-Based Model for Active Suction Phenomenon Simulation 1385\nAn Intuitive, Affordances Oriented Telemanipulation Framework for a Dual Robot Arm Hand System: On the Execution of Bimanual Tasks 1177\nAnkle Torque During Mid-Stance Does Not Lower Energy Requirements of Steady Gaits 1265\nAn Object Attribute Guided Framework for Robot Learning Manipulations from Human Demonstration Videos 0936\nAnonymous Hedonic Game for Task Allocation in a Large-Scale Multiple Agent System 2598\nAn Open-Source 7-Axis, Robotic Platform to Enable Dexterous Procedures within CT Scanners 1428\nAn Optimal Algorithm to Solve the Combined Task Allocation and Path Finding Problem 0471\nAn Optimization Framework for Simulation and Kinematic Control of Constrained Collaborative Mobile Agents (CCMA) System 2393\nA Novel 4-DoF Robotic Link Mechanism with E-CoSMo : Kinematics Based Torque Analysis 0775\nA Novel Approach for Outlier Detection and Robust Sensory Data Model Learning 0340\nA Novel Capabilities of Quadruped Robot Moving through Vertical Ladder without Handrail Support 0329\nA Novel Four-Degree-Of-Freedom versus a Conventional Foot Interface for Controlling a Robotic Assistive Arm in Surgery 2658\nA novel rescue system using multi-agent SLAM framework 2759\nA Novel Robust Approach for Correspondence-Free Extrinsic Calibration 1541\nA Novel Semi-Autonomous Control Framework for Retina Confocal Endomicroscopy Scanning 0483\nA Novel Small-Scale Turtle-Inspired Amphibious Spherical Robot 1467\nANYexo: A Versatile and Dynamic Upper-Limb Rehabilitation Robot 2123\nA Parallel Gripper with a Universal Fingertip Device Using Optical Sensing and Jamming Transition for Maintaining Stable Grasps 1496\nA Passive Closing, Tendon Driven, Adaptive Robot Hand for Ultra-Fast, Aerial Grasping and Perching 0015\nA Penetration Metric for Deforming Tetrahedra Using Object Norm 0064\nApplication of Digging Control based on the Center-of-Mass Velocity of the Attachment of a Hydraulic Excavator 0719\nApplying the Interaction of Walking-Emotion to an Assistive Device for Rehabilitation and Exercise 0238\nApproximating Cfree Space Topology by Constructing Vietoris-Rips Complex 1486\nA Pressure Field Model for Fast, Robust Approximation of Net Contact Force and Moment between Nominally Rigid Objects 1597\nA Probabilistic Approach to Human-Robot Communication 1359\nA Real-Time Dynamic Simulator and an Associated Front-End Representation Format for Simulating Complex Robots and Environments 1766\nA Real-Time V2X Enabled Dynamic Path Planning System for Autonomous Vehicles in Road Blockage Test Scenarios 2715\nA Reliable Gravity Compensation Control Strategy for dVRK Robotic Arms with Nonlinear Disturbance Forces 2340\nAre You Hearing or Listening? the Effect of Task Performance in Verbal Behavior with Smart Speaker 0332\nAre You with Me? Determining the Associationof Individuals and the Collective Social Space 0902\nArguing Security of Autonomous Robots 0158\nA Ring Network Protocol for Articulated Robots 1955\nARMCL: ARM Contact Point Localization Via Monte Carlo Localization 0132\nA Robotic Surgery Approach to Mitochondrial Transfer Amongst Single Cells 0604\nA Robust Biped Locomotion Based on Linear-Quadratic-Gaussian Controller and Divergent Component of Motion 0243\nA Robust Extrinsic Calibration Framework for Vehicles with Unscaled Sensors 1578\nA Robust Laser-Inertial Odometry and Mapping Method for Large-Scale Highway Environments 1670\nA Robustness Analysis of Inverse Optimal Control of Bipedal Walking 2583\nA Robust Position and Posture Measurement System Using Visual Markers and an Inertia Measurement Unit 0584\nA Robust Stereo Semi-Direct SLAM System Based on Hybrid Pyramid 1883\nArticulated Multi-Perspective Cameras and Their Application to Truck Motion Estimation 1440\nArtificial Intelligent Navigation Technology for a Robotic Vacuum Cleaner in an Indoor Environment 2749\nArtificial Lateral Line Based Longitudinal Separation Sensing for Two Swimming Robotic Fish with Leader-Follower Formation 0629\nA RUGD Dataset for Autonomous Navigation and Visual Perception in Unstructured Outdoor Environments 1126\nA Simple Approach on Global Control of a Class of Underactuated Mechanical Robotic Systems 1681\nA Soft Exoglove Equipped with a Wearable Muscle-Machine Interface Based on Forcemyography and Electromyography 2234\nA Spring-Aided Two-Dimensional Electromechanical Spine Architecture for Bio-Inspired Robots 2000\nA Stabilization Analysis of Omni-Mobile Manipulator with 4K Camera 2650\nA Study of a Class of Vibration-Driven Robots: Modeling, Analysis, Control and Design of the Brushbot 1866\nA Study on the Electric Wheelchair-Humanoid Collaboration for Clothing Assistance of the Elderly 2769\nA Sweeping and Grinding Methods Combined Hybrid Sampler for Asteroid Exploration 1759\nAsynchronous Behavior Trees with Memory Aimed at Aerial Vehicles with Redundancy in Flight Controller 1485\nA Systematic Comparison of Affective Robot Expression Modalities 0607\nA Tactile Stimulation System for Robot-Assisted Hand Rehabilitation 2751\nA Taxonomy for Characterizing Modes of Interactions in Goal-Driven, Human-Robot Teams 1548\nA Teleoperated Hexapod Robot for Imitation Learning Task Training 0454\nA Teleoperation Interface for Loco-Manipulation Control of MObile Collaborative Robotic Assistant (MOCA) 2469\nA Testbed for Haptic and Magnetic Resonance Imaging-Guided Percutaneous Needle Biopsy 2479\nAtomic force microscope tip localization and tracking through deep learning based vision inside an electron microscope 1025\nAttention-Based Hierarchical Deep Reinforcement Learning for Lane Change Behaviors in Autonomous Driving 0397\nAttitude- and Cruise Control of a VTOL Tiltwing UAV 2142\nA Two-DOF Bipedal Robot Utilizing the Reuleaux Triangle Drive Mechanism 0464\nAudio-Visual Sensing from a Quadcopter: Dataset and Baselines for Source Localization and Sound Enhancement 0889\nAugmented Reality Controlled Smart Wheelchair Using Dynamic Signifiers for Affordance Representation 0901\nAugmenting Knowledge through Statistical, Goal-Oriented Human-Robot Dialog 1182\nA Unified Active Assistance Control Framework of Hip Exoskeleton for Walking and Balance Assistance 1816\nA Unified Formulation for Visual Odometry 0165\nAutomated Boxwood Topiary Trimming with a Robotic Arm and Integrated Stereo Vision 1053\nAutomated Macro-Micro Manipulation for Robotic Microinjection with Computer Vision 0609\nAutomated Single-Particle Micropatterning System Using Dielectrophoresis 2755\nAutomated Sorting of Rare Cells Based on Autofocusing Visual Feedback in Fluorescence Microscopy 0599\nAutomatic Annotation for Semantic Segmentation in Indoor Scenes 0469\nAutomatic Calibration of Multiple 3D LiDARs in Urban Environments 0822\nAutomatic Cell Assembly by Two-Fingered Microhand 0726\nAutomatic Coverage Selection for Surface-Based Visual Localization 2201\nAutomatic Multi-Sensor Extrinsic Calibration for Mobile Robots 2110\nAutomatic Spatial Template Generation for Realistic 3D Modeling of Large-Scale Indoor Spaces 1666\nAutonomous 3D reconstruction, mapping and exploration of indoor environments with a robotic arm 2383\nAutonomous Detection of PV Panels Using Unmanned Aerial Vehicles 2729\nAutonomous Free-Form Trenching Using a Walking Excavator 2171\nAutonomous Human-Aware Navigation in Dense Crowds 2673\nAutonomous Landing on Pipes Using Soft Gripper for Inspection and Maintenance in Outdoor Environments 1291\nAutonomous Mobile Manipulation Framework for Intelligent Home Service Robots 2694\nAutonomous Photogrammetry Process for Managing Stockpile Inventory with Unmanned Aerial Vehicle 2780\nAutonomous Safe Locomotion System for Bipedal Robot Applying Vision and Sole Reaction Force to Footstep Planning 0121\nAutonomous Search for Sources of Gamma Radiation 2737\nA Variable Stiffness Elbow Joint for Upper Limb Prosthesis 2266\nA Video Data-Driven Approach for the Development of Active Guidance in Robot-Assisted Minimally Invasive Surgical Training 2791\nA Virtual Reality Interface for an Autonomous Spray Painting UAV 2494\nAvoiding Obstacles during Push Recovery Using Real-Time Vision Feedback 0628\nA VR System for Immersive Teleoperation and Live Exploration with a Mobile Robot 1046\nBag of Semantic Visual Words 2762\nBasic Performance of Planar Omnidirectional Crawler During Direction Switching Using Disturbance Degree of Ground Evaluation Method 2461\nBayesian Gaussian Mixture Model for Robotic Policy Imitation 2512\nBayesian Optimization for Policy Search in High-Dimensional Systems Via Automatic Domain Selection 0091\nBeBOT: Bernstein Polynomial Toolkit for Trajectory Generation 1490\nBee+: A 95-Mg Four-Winged Insect-Scale Flying Robot Driven by Twinned Unimorph Actuators 2421\nBehavior Change Based on Stiffness for Haptic Interface 2679\nBelief-Driven Control Policy of a Drone with Microphones for Multiple Sound Source Search 1785\nBelief Space Metareasoning for Exception Recovery 0836\nBenchmarking and Workload Analysis of Robot Dynamics Algorithms 1641\nBetter Lost in Transition Than Lost in Space: SLAM State Machine 1450\nBidirectional Heuristic Search for Motion Planning with an Extend Operator 1851\nBi-Modal Hemispherical Sensor: A Unifying Solution for Three Axis Force and Contact Angle Measurement 1523\nBiomimetic Wrinkled MXene Pressure Sensors towards Collision-Aware Robots 2781\nBiped Robot Pelvis Kinematics Estimation Based on the Touch-Point Updating Method 0364\nBlack Block Recorder: Immutable Black Box Logging for Robots Via Blockchain 2303\nBoosting SLAM: Combining SLAM Methodologies for Robust Localization 2746\nBoundary Effect-Aware Visual Tracking for UAV with Online Enhanced Background Learning and Multi-Frame Consensus Verification 0650\nBounded-Error LQR-Trees 1519\nBP Neural Network Based On-Board Training for Real-Time Locomotion Mode Recognition in Robotic Transtibial Prostheses 0393\nBTEL: A Binary Tree Encoding Approach for Visual Localization 2502\nBuckling-Induced Shape Morphing Using Dielectric Elastomer Actuators Patterned with Spatially-Varying Electrodes 1849\nCable-Driven 4-DOF Upper Limb Rehabilitation Robot 1599\nCALC2.0: Combining Appearance, Semantic and Geometric Information for Robust and Efficient Visual Loop Closure 2006\nCamera Exposure Control for Robust Robot Vision with Noise-Aware Image Quality Assessment 2210\nCamera Pose Estimation Based on PnL with a Known Vertical Direction 2378\nCamera Pose Estimation with Semantic 3D Model 0984\nCamera Zoom Control of Integrated Control Platform for Advancement of Performance Shooting System 2651\nCan a Robot Become a Movie Director? Learning Artistic Principles for Aerial Cinematography 1472\nCan a Robot Hear the Shape and Dimensions of a Room? 1987\nCan a Social Robot Encourage Children's Self-Study? 0656\nCannot Avoid Penalty for Fluctuating Order Arrival Rate? Let's Minimize 0071\nCan User-Centered Reinforcement Learning Allow a Robot to Attract Passersby without Causing Discomfort? 0691\nCapillary Force Gripper for Complex Shaped Micro Objects with Fast Droplet Forming by On-Off Control of a Piston Slider 2169\nCarpie: A Soft, Mechanically-Reconfigurable Worm Robot 0195\nCascaded Gaussian Processes for Data-Efficient Robot Dynamics Learning 1399\nCentralized Control Architecture for Cooperative Object Transportation Using Multiple Omnidirectional AGVs 0646\nChance-Constrained Trajectory Optimization for Non-Linear Systems with Unknown Stochastic Dynamics 1216\nCharacterizing Environmental Interactions for Soft Growing Robots 0632\nCharacterizing Nanoparticle Swarms with Tuneable Concentrations for Enhanced Imaging Contrast 2203\nCheating with Robots: how at ease do they make us feel? 0757\nClock-Torqued Rolling SLIP Model and Its Application to Variable-Speed Running in a Hexapod Robot 2592\nClone Swarms: Learning to Predict and Control Multi-Robot Systems by Imitation 1337\nClosed-Form Equations and Experimental Verification for Soft Robot Arm Based on Cosserat Theory 0854\nClosed-Loop Force Control of a Pneumatic Gripper Actuated by Two Pressure Regulators 1068\nCOBRA: COllaborative Bot with multi-Rotor Actuation 0533\nCognitive Robotic Architecture for Semi-Autonomous Execution of Manipulation Tasks in a Surgical Environment 1349\nCollaborative Human Augmented SLAM 0359\nCollaborative Mapping with Pose Uncertainties using different Radio Frequencies and Communication Modules 0042\nCollaborative Needle Insertion with Active Tissue Deformation Control 2038\nCollaborative Robot Assistant for the Ergonomic Manipulation of Cumbersome Objects 0169\nCollision Detection and Isolation on a Robot Using Joint Torque Sensing 1378\nCombined Optimization of Gripper Finger Design and Pose Estimation Processes for Advanced Industrial Assembly 0870\nCombined Task and Action Learning from Human Demonstrations for Mobile Manipulation Applications 0406\nCombining Spiking Motor Primitives with a Behavior-Based Architecture to Model Locomotion for Six-Legged Robots 0494\nCombining Stochastic Optimization and Frontiers for Aerial Multi-Robot Exploration of 3D Terrains 1102\nCommercialization of Robot Navigation Technology for a Guidance Service in a Large and Highly-Crowded Airport 2748\nCommon Dimensional Autoencoder for Learning Redundant Muscle-Posture Mappings of Complex Musculoskeletal Robots 1375\nCommunication Constrained Cloud-Based Long-Term Visual Localization in Real Time 1016\nCompact Reachability Map for Excavator Motion Planning 1922\nComparing Swimming Performances of flexible and Helical Magnetic Swimmers 0058\nComparison of Deep Reinforcement Learning Policies to Formal Methods for Moving Obstacle Avoidance 1823\nComplexity Conditioned Goals for Reinforcement Learning Agents 2786\nComplex Stiffness Model of Physical Human-Robot Interaction: Implications for Control of Performance Augmentation Exoskeletons 1342\nComponent Modularized Design of Musculoskeletal Humanoid Platform Musashi to Investigate Learning Control Systems 0124\nComputational Design of Statically Balanced Planar Spring Mechanisms 2462\nComputationally Efficient MPC for Cable-Driven Robot 2718\nComputing 3D From-Region Visibility Using Visibility Integrity 2220\nComputing a Minimal Set of T-Spanning Motion Primitives for Lattice Planners 1507\nConcept and Validation of a Large-Scale Human-Machine Safety System Based on Real-Time UWB Indoor Localization 0785\nConditional Generative Neural System for Probabilistic Trajectory Prediction 0952\nConfiguration Modeling of a Soft Robotic Element with Selectable Bending Axes 1287\nConfiguration Transition Control of a Continuum Surgical Manipulator for Improved Kinematic Performance 2457\nConnectivity-Preserving Swarm Teleoperation with a Tree Network 0477\nConstrained Heterogeneous Vehicle Path Planning for Large-Area Coverage 1465\nConstructing a Highly Interactive Vehicle Motion Dataset 1827\nContact-Based Bridge Inspection Multirotors: Design, Modelling and Control Considering the Ceiling Effect 2390\nContact Distance Estimation by Soft Active Bio-Whisker Sensor Based on Morphological Computation 2612\nContactGrasp: Functional Multi-Finger Grasp Synthesis from Contact 1159\nContact-Implicit Trajectory Optimization for Dynamic Object Manipulation 0803\nContact Skill Imitation Learning for Robot-Independent Assembly Programming 0488\nContamination Detection and Classification for an Automated Fa\u00c3\u00a7ade Cleaning Operation 2631\nContext and Intention Aware Planning for Urban Driving 0640\nContext-Dependent Search for Generating Paths for Redundant Manipulators in Cluttered Environments 1424\nContinuous Close-Range 3D Object Pose Estimation 1380\nContinuous Collision Detection for a Robotic Arm Mounted on a Cable-Driven Parallel Robot 1054\nContinuous Mechanical Indexing of Single Cell Spheroids Using a Robot Integrated Microfluidic Chip 2204\nContinuous Modeling of Affordances in a Symbolic Knowledge Base 1245\nContinuous Neural Control Based on Integration of BCI and Adaptive Controller for Steering a Vehicle 2721\nContinuous Relaxation of Symbolic Planner for One-Shot Imitation Learning 0565\nContinuous-Time Collision Avoidance for Trajectory Optimization in Dynamic Environments 1904\nContour Based Reconstruction of Underwater Structures Using Sonar, Visual, Inertial, and Depth Sensor 1537\nControl and Perception Framework for Deep Sea Mining Exploration 0999\nControl of Nonprehensile Planar Rolling Manipulation: A Passivity-Based Approach 2607\nConvolutional autoencoder for feature extraction in tactile sensing 2519\nCooperative Audio-Visual System for Localizing Micro Aerial Robots 1366\nCooperative Decentralised Circumnavigation with Application to Algalbloom Tracking 0864\nCooperative Range-Only SLAM Based on Sum of Gaussian Filter in Dynamic Environments 0842\nCooperative Schedule-Driven Intersection Control with Connected and Autonomous Vehicles 1677\nCo-Simulation of Mechanical Systems with Hydraulic Actuators 2743\nCoupling Disturbance Compensated MIMO Control of Parallel Ankle Rehabilitation Robot Actuated by Pneumatic Muscles 0745\nCovariance Pre-Integration for Delayed Measurements in Multi-Sensor Fusion 1557\nCoverage Path Planning Using Path Primitive Sampling and Primitive Coverage Graph for Visual Inspection 0337\nCoverage Sampling Planner for UAV-Enabled Environmental Exploration and Field Mapping 1408\nCriteria for Maintaining Desired Contacts for Quasi-Static Systems 1558\nCrowd-sourced Semantic Edge Mapping for Autonomous Vehicles 1701\nCubeSLAM: Monocular 3-D Object SLAM 2611\nCuriosity Driven Exploration for Classification in the Dark Using Tactile Sensing 2761\nCurved-Voxel Clustering for Accurate Segmentation of 3D LiDAR Point Clouds with Real-Time Performance 1780\nData Association Aware Semantic Mapping and Localization Via a Viewpoint-Dependent Classifier Model 0627\nData-Based Modeling of Contact State in Robotic Assembly 2698\nData-Driven Model Predictive Control for Trajectory Tracking with a Robotic Arm 2403\nData Flow ORB-SLAM for Real-Time Performance on Embedded GPU Boards 0413\nDecentralized Control for 3D M-Blocks for Path Following, Line Formation, and Light Gradient Aggregation 1308\nDecentralized Pose Control of Modular Reconfigurable Robots Operating in Liquid Environments 1996\nDecentralized Visual-Inertial Localization and Mapping on Mobile Devices for Augmented Reality 0461\nDecoding the Perceived Difficulty of Communicated Contents by Older People: Toward Conversational Robot-Assistive Elderly Care 2507\nDEDUCE: Diverse scEne Detection Methods in Unseen Challenging Environments 0590\nDeepControl: Energy-Efficient Control of a Quadrotor Using a Deep Neural Network 1839\nDeep Dive into Faces: Pose & Illumination Invariant Multi-Face Emotion Recognition System 0742\nDeep Generative Modeling of LiDAR Data 1601\nDeep Imitation Learning for Autonomous Driving in Generic Urban Scenarios with Enhanced Safety 1371\nDeep Lagrangian Networks for End-To-End Learning of Energy-Based Control for Under-Actuated Systems 1042\nDeep Learning-Based Mutual Detection and Collaborative Localization for Mobile Robot Fleets Using Solely 2D LIDAR Sensors 1175\nDeep Learning Based Robotic Tool Detection and Articulation Estimation with Spatio-Temporal Layers 2384\nDeep Learning of Proprioceptive Models for Robotic Force Estimation 0372\nDeepLocNet: Deep Observation Classification and Ranging Bias Regression for Radio Positioning Systems 1961\nDeep Multi-Task Learning for Anomalous Driving Detection Using CAN Bus Scalar Sensor Data 1665\nDeep Neural Network Approach in Electrical Impedance Tomography-Based Real-Time Soft Tactile Sensor 0574\nDeep Neural Network Based Visual Inspection with 3D Metric Measurement of Concrete Defects Using Wall-Climbing Robot 2008\nDeep Orientation: Fast and Robust Upper Body Orientation Estimation for Mobile Robotic Applications 1070\nDeepPCO: End-To-End Point Cloud Odometry through Deep Parallel Neural Network 1056\nDeep Predictive Autonomous Driving Using Multi-Agent Joint Trajectory Prediction and Traffic Rules 2013\nDeep Reinforcement Learning for Robotic Pushing and Picking in Cluttered Environment 1325\nDeep Sensor Fusion for Real-Time Odometry Estimation 0421\nDeep Supervised Hashing with Similar Hierarchy for Place Recognition 0365\nDeepVIO: Self-Supervised Deep Learning of Monocular Visual Inertial Odometry Using 3D Geometric Constraints 0978\nDeep Visual MPC-Policy Learning for Navigation 2120\nDegeneracy-Aware Factors with Applications to Underwater SLAM 1187\nDegeneracy in Self-Calibration Revisited and a Deep Learning Solution for Uncalibrated SLAM 1031\nDelayed Output Feedback Control for Gait Assistance and Resistance Using a Robotic Exoskeleton 2338\nDelayed Output Feedback Control for Gait Assistance with a Robotic Hip Exoskeleton 2609\nDelivering Cognitive Behavioral Therapy Using a Conversational Social Robot 1394\nDempster Shafer Grid-Based Hybrid Fusion of Virtual Lanes for Autonomous Driving 0437\nDense 3D Reconstruction for Visual Tunnel Inspection Using Unmanned Aerial Vehicle 0336\nDensePeds: Pedestrian Tracking in Dense Crowds Using FRVO and Sparse Features 1295\nDense, Sonar-Based Reconstruction of Underwater Scenes 1471\nDepth-Image-Based Textureless-Object Picking by DCNN and Visual Servoing 2619\nDesign a Dexterous Hand for the Logistic Robot in Bin Picking 2730\nDesign and Analysis of a New 3-DOF Active-Type Constant-Force Compliant Parallel Stage 0545\nDesign and Analysis of the All-In-One Actuation Module with Multi-Sensors 2670\nDesign and Characterization of a Fully Autonomous Under-Actuated Soft Batoid-Like Robot 1374\nDesign and Comparative Analysis of 1D Hopping Robots 1831\nDesign and Control of Aerial Modules for Inflight Self-disassembly 2389\nDesign and Control of a High-Torque and Highly-Backdrivable Hybrid Soft Exoskeleton for Knee Injury Prevention during Squatting 2493\nDesign and Control of a Multifunctional Ankle Exoskeleton Powered by Magnetorheological Actuators to Assist Walking, Jumping and Landing 2227\nDesign and Development of Compactly Folding Parallel Open-Close Gripper with Wide Stroke 1926\nDesign and Experiment of Dragonfly Inspired Flexible Blade to Improve Safety of Drones 2331\nDesign and Implementation of a Contact Aerial Manipulator System for Glass-Wall Inspection Tasks 0740\nDesign and Take-Off Flight of a Samara-Inspired Revolving-Wing Robot 0797\nDesign and Verification of a Gravity Compensated Tool Handler for Supporting an Automatic Hair Implanting Device 2367\nDesign and Verification of a Portable Master Manipulator Based on an Effective Workspace Analysis Framework 2096\nDesign, Characterization, and Mechanical Programming of Fabric-Reinforced Textile Actuators for a Soft Robotic Hand 1585\nDesign, Fabrication, and Characterization of an Untethered Amphibious Sea Urchin-Inspired Robot 2276\nDesign for Cobot-Assisted Manufacturing and Assembly (DFcoMA) 2738\nDesigning a Mechanical Tool for Robots with 2-Finger Parallel Grippers 2293\nDesign, Modeling and Control of Fully Actuated 2D Transformable Aerial Robot with 1 DoF Thrust Vectorable Link Module 1461\nDesign, Modeling and Testing of a Flagellum-Inspired Soft Underwater Propeller Exploiting Passive Elasticity 0641\nDesign, Modelling and Adaptive Control of a Novel Autonomous Underwater Vehicle Equipped with Vectored Thrusters 2660\nDesign, modelling and control of a novel agricultural robot with iInterlock drive system 2195\nDesign of a 3-DOF Linkage-Driven Underactuated Finger for Multiple Grasping 1645\nDesign of a Ballistically-Launched Foldable Multirotor 0511\nDesign of a Bipedal Hopping Robot with Morphable Inertial Tail for Agile Locomotion 2717\nDesign of a Compact SMA-Actuated MRI-Compatible Steerable Neurosurgical Robot 2768\nDesign of a Fail-Safe Wearable Robot with Novel Extendable Arms for Ergonomic Accommodation During Floor Work 1696\nDesign of a Growing Robot Inspired by Plant Growth 1653\nDesign of a Mobile Robot for the Treatment, Reuse and Removal of Manure with Monitoring of Environmental Variables for Poultry Farms 2783\nDesign of a Modular Continuum-Articulated Laparoscopic Robotic Tool with Decoupled Kinematics 2231\nDesign of an Adhesion-Aware Fa\u00c3\u00a7ade Cleaning Robot 0151\nDesign of a Novel Gripper System with 3D and Inkjet-Printed Multimodal Sensors for Automated Grasping of a Forestry Robot 2306\nDesign of a Novel Leg for a Small Tree Climbing Robot Driven by Shape Memory Alloy 2709\nDesign of a Semi-Humanoid Telepresence Robot for Plant Disaster Response and Prevention 1936\nDesign of a Variable Counterbalance Mechanism to Minimize Required Torque of Robot Arm 2622\nDesign of Compact Variable Gravity Compensator (CVGC) Based on Cam and Variable Pivot of a Lever Mechanism 0541\nDesign of Robot Leg with Variable Reduction Ratio Crossed Four-bar Linkage Mechanism 1784\nDesign of Soft Flexible Wire-Driven Finger Mechanism for Contact Pressure Distribution 0909\nDesign of Wearable Robot Focused on Contact State with Wearer 2756\nDESK: A Robotic Activity Dataset for Dexterous Surgical Skills Transfer to Medical Robots 1757\nDetecting Layered Structures of Partially Occluded Objects for Bin Picking 1934\nDevelopment of 3DoF Manipulators with Cable-Hydraulic Driven Actuation Modules for Large Workspace and High Payload-To-Weight 2771\nDevelopment of a Continuous Vertical-Pulling Automatic Doffing Robot for the Ring Spinning 1921\nDevelopment of Adjustable Knee Assist Device for Wearable Robot Based on Linkage and Rolling Joint 0387\nDevelopment of a Location Finding System for Minute Sound Source by Using Human Acoustic System with Stochastic Resonance 1857\nDevelopment of an adaptive hexapod robot based on Follow-the-contact-point gait control and Timekeeper control 0559\nDevelopment of an Arm Curl Machine with Variable Resistance Using Pneumatic Artificial Rubber Muscle 1928\nDevelopment of an Autonomous Sanding Robot with Structured-Light Technology 0660\nDevelopment of a Navigation Algorithm for Optimal Path Planning for Autonomous Electric Vehicles 1110\nDevelopment of a Steel Bridge Climbing Robot 1315\nDevelopment of Flexible Dual-Type Proximity Sensor with Resonant Frequency for Robotic Applications 0821\nDevelopment of Immersive VR Interface of Finger Motion without Restriction of Real Environment 2676\nDevelopment of Joint Module with Two-Speed Gear Transmission and Joint Lock Mechanism during Driving for Task Adaptable Robot 0296\nDevelopment of Load Weight and Height Classifier in Lifting-Up Task Using Body Motion Metrics 2555\nDevelopment of Micro Ultrasonic Actuator and Micro Rotor Blade for Micro Aerial Vehicle 0623\nDevelopment of Novel Bevel-Geared 5mm Articulating Wrist for Micro-Laparoscopy Instrument 2238\nDid You Miss the Sign? a False Negative Alarm System for Traffic Sign Detectors 1156\nDirectional TSDF: Modeling Surface Orientation for Coherent Meshes 1151\nDisaster Response Robot's Autonomous Manipulation of Valves in Disaster Sites Based on Visual Analyses of RGBD Images 0514\nDISC: A Large-Scale Virtual Dataset for Simulating Disaster Scenarios 0018\nDISCOMAN: Dataset of Indoor SCenes for Odometry, Mapping and Navigation 1019\nDiscrete N-Dimensional Entropy of Behavior: DNDEB 1377\nDISR: Deep Infrared Spectral Restoration Algorithm for Robot Sensing and Intelligent Visual Tracking Systems 0403\nDistance-Based Cooperative Relative Localization for Leader-Following Control of MAVs 2397\nDistributed Dynamic Sensor Assignment of Multiple Mobile Targets 0716\nDisturbance Estimation and Rejection for High-Precision Multirotor Position Control 2361\nDLD: A Deep Learning Based Line Descriptor for Line Feature Matching 0049\nDo Intermediate Gaits Matter When Rapidly Accelerating? 2284\nDomain-Independent Unsupervised Detection of Grasp Regions to grasp Novel Objects 1689\nDot-to-Dot: Explainable Hierarchical Reinforcement Learning for Robotic Manipulation 0887\nDouble Refinement Network for Efficient Monocular Depth Estimation 1041\nDriving with Style: Inverse Reinforcement Learning in General-Purpose Planning for Automated Driving 1356\nDual-Arm Assembly Planning Considering Gravitational Constraints 1859\nDuckiepond: An Open Education and Research Environment for a Fleet of Autonomous Maritime Vehicles 2301\nDynamic Control for Soft Robots with Internal Constraints in the Presence of Obstacles 1384\nDynamic Density Topological Structure Generation for Real-Time Ladder Affordance Detection 0831\nDynamic Flex-And-Flip Manipulation of Deformable Linear Objects 1515\nDynamic Identification of the Franka Emika Panda Robot with Retrieval of Feasible Parameters Using Penalty-based Optimization 2443\nDynamic Input for Deep Reinforcement Learning in Autonomous Driving 1229\nDynamic Locomotion on Slippery Ground 2528\nDynamic Spatiotemporal Pattern Identification and Analysis Using a Fingertip-Based Electro-Tactile Display Array 1716\nDynamic Task Control Method of a Flexible Manipulator Using a Deep Recurrent Neural Network 0566\nDynamic Whole-Body Control of Unstable Wheeled Humanoid Robots 2252\nEarly Fusion for Goal Directed Robotic Vision 1150\nEdge-Preserving Camera Trajectories for Improved Optical Character Recognition on Static Scenes with Text 2280\nEffective Estimation of Contact Force and Torque for Vision-based Tactile Sensors with Helmholtz-Hodge Decomposition 2464\nEffect of Arm Swinging and Trunk Twisting on Bipedal Locomotion 0557\nEffect of Planning Period on MPC-Based Navigation for a Biped Robot in a Crowd 0382\nEffect of Vibration on Twisted String Actuation through Conduit at High Bending Angles 1529\nEffects of a Bio-mimicked Flapping Path on Propulsion Efficiency of Two-segmental Fish Robots 1034\nEffects of a Person-Following Light-Touch Device During Overground Walking with Visual Perturbations in a Virtual Reality Environment 2498\nEffects of Limb Morphology on Transient Locomotion in Quadruped Robots 2336\nEfficient and Accurate Operational Space Control with Dual-Haptic Interface for Large Workspace Teleoperation 2708\nEfficient and Guaranteed Planar Pose Graph Optimization Using the Complex Number Representation 0987\nEfficient Autonomous Robotic Exploration with Semantic Road Map in Indoor Environments 2225\nEfficient Environment Guided Approach for Exploration of Complex Environments 1085\nEfficient Grasp Planning and Execution with Multi-Fingered Hands by Surface Fitting 2424\nEfficient Quadrupedal Walking Via Decentralized Coordination Mechanism between Limbs and Neck 0606\nEigen-Factors: Plane Estimation for Multi-Frame and Time-Continuous Point Cloud Alignment 1499\nElectrical Bio-Impedance Proximity Sensing for Vitreo-Retinal Micro-Surgery 2320\nElevateNet: A Convolutional Neural Network for Estimating the Missing Dimension in 2D Underwater Sonar Images 1249\nEmpirical Characterization of a High-Performance Exterior-Rotor Type Brushless DC Motor and Drive 2477\nEmploying Magnets to Improve the Force Exertion Capabilities of Adaptive Robot Hands in Precision Grasps 0290\nEmploying Whole-Body Control in Assistive Robotics 1269\nEmpowered Optical Inspection by Using Robotic Manipulator in Industrial Applications 1004\nEnabling human-like task identification from natural conversation 0079\nEndoscopic Bi-Manual Robotic Instrument Design Using a Genetic Algorithm 0816\nEnd-To-End Driving Model for Steering Control of Autonomous Vehicles with Future Spatiotemporal Features 1524\nEnd-To-End Sensorimotor Control Problems of AUVs with Deep Reinforcement Learning 0245\nEnergy-Based Adaptive Control and Learning for Patient-Aware Rehabilitation 1918\nEnergy-Based Hybrid Control of a Ball-Dribbling Robot 2739\nEnergy-Efficient Locomotion Strategies and Performance Benchmarks Using Point Mass Tensegrity Dynamics 1919\nEnergy Harvesting across Temporal Temperature Gradients Using Vaporization 0714\nEnsembleDAgger: A Bayesian Approach to Safe Imitation Learning 0294\nEnthusiastic Robots Make Better Contact 0924\nEntropic Risk Measure in Policy Search 1298\nEnvironmental Sound Segmentation Utilizing Mask U-Net 1814\nEpisodic Learning with Control Lyapunov Functions for Uncertain Robotic Systems 1943\nEPN: Edge-Aware PointNet for Object Recognition from Multi-View 2.5D Point Clouds 1436\nErgodic Flocking 1807\nEscaping Local Minima in Search-Based Planning Using Soft Duplicate Detection 1931\nESKO6d - a Binocular and RGB-D Dataset of Stored Kitchen Objects with 6d Poses 1055\nEstablishing Safer Human-Vehicle Visual Interaction at Night 2659\nEstimating Metric Scale Visual Odometry from Videos Using 3D Convolutional Networks 1923\nEstimating the Center of Mass and the Angular Momentum Derivative for Legged Locomotion ' a Recursive Approach 2585\nEvaluating the Acceptability of Assistive Robots for Early Detection of Mild Cognitive Impairment 0468\nEvaluation of a Large-Scale Event-Driven Robot Skin 2181\nEvaluation of Hopping Robot Performance with Novel Foot Pad Design on Natural Terrain for Hopper Development 2326\nEvaluation System for Hydraulic Excavator Operation Skill Using Remote Controlled Excavator and Virtual Reality 2465\nEV-IMO: Motion Segmentation Dataset and Learning Pipeline for Event Cameras 1351\nExecuting Underspecified Actions in Real World Based on Online Projection 1112\nExo Wrist: A Soft Tendon Driven Wrist Wearable Robot with Active Anchor for Dart Throwing Motion in Hemiplegic Patients 2482\nExperience Reuse with Probabilistic Movement Primitives 0485\nExperimental Analysis of the Influence of Olfactory Property on Chemical Plume Tracing Performance 2126\nExperimental Comparison of Open Source Visual-Inertial-Based State Estimation Algorithms in the Underwater Domain 0500\nExperimental Study on Microfluidic Mixing with Trapezoidal Obstacles in a 1000-Fold Span of Reynolds Number 0099\nExperimental Study on the Parameters of High-Pressure Water-Jet Cleaning on a Facade 2637\nExperimental Validation of Hydraulic Interlocking Drive System for Biped Humanoid Robot 2460\nExplainable One-Shot Meta-Learning to Imitate Motion Segments of Unseen Human-Robot Interactions 2765\nExploiting linearity in dynamics solvers for the design of composable robotic manipulation architectures 1717\nExploiting Sparse Semantic HD Maps for Self-Driving Vehicle Localization 0272\nExplore, Approach, and Terminate: Evaluating Subtasks in Active Visual Object Search Based on Deep Reinforcement Learning 1123\nExploring Logical Consistency and Viewport Sensitivity in Compositional VQA Models 1250\nExploring Low-Level and High-Level Transfer Learning for Multi-Task Facial Recognition with a Semi-Supervised Neural Network 0765\nExtending Monocular Visual Odometry to Stereo Camera Systems by Scale Optimization 1173\nExternal Force Estimation of Human-Cooperative Robot During Object Manipulation Using Recurrent Neural Network 2614\nExtrinsic Calibration of Thermal IR Camera and mmWave Radar by Exploiting Depth from RGB-D Camera 2697\nFA-Harris: A Fast and Asynchronous Corner Detector for Event Cameras 1843\nFast Adaptation with Meta-Reinforcement Learning for Trust Modelling in Human-Robot Interaction 1576\nFast and Incremental Loop Closure Detection Using Proximity Graphs 0330\nFast and Robust 3-D Sound Source Localization with DSVD-PHAT 0271\nFast and Safe Policy Adaptation via Alignment-based Transfer 0563\nFASTER: Fast and Safe Trajectory Planner for Flights in Unknown Environments 1577\nFast Handovers with a Robot Character: Small Sensorimotor Delays Improve Perceived Qualities 1613\nFast Manipulability Maximization Using Continuous-Time Trajectory Optimization 2317\nFast Motion Planning via Free C-space Estimation Based on Deep Neural Network 0832\nFast Perception, Planning, and Execution for a Robotic Butler: Wheeled Humanoid M-Hubo 1637\nFast Run-Time Monitoring, Replanning, and Recovery for Safe Autonomous System Operations 1274\nFast Time-Optimal Avoidance of Moving Obstacles for High-Speed MAV Flight 0030\nFast Trajectory Planning for Multiple Quadrotors Using Relative Safe Flight Corridor 1460\nFault-Tolerant Force Tracking for a Multi-Legged Robot 2672\nFeasibility of Gait Entrainment to Hip Mechanical Perturbation for Locomotor Rehabilitation 0462\nFeasibility of Wireless Power Transfer for Mobile Robots 2655\nFeasibility Test of Exoskeleton Ankle Robot for Gait Training on Stairs for Sub-Acute Stroke Patients 2779\nFeedback-Based Fabric Strip Folding 0932\nFeedback MPC for Torque-Controlled Legged Robots 1021\nFiber Optic Fabry-Perot Interferometry for a Biopsy Needle with Tip Force Sensing 2710\nFIESTA: Fast Incremental Euclidean Distance Fields for Online Quadrotor Motion Planning 1758\nFilter Early, Match Late: Improving Network-Based Visual Place Recognition 0618\nFirst Steps towards Full Model Based Motion Planning and Control of Quadrupeds: A Hybrid Zero Dynamics Approach 1761\nFLAME: Feature-Likelihood Based Mapping and Localization for Autonomous Vehicles 0770\nFlexible Layouts for Fiducial Tags 1503\nFlexible Trinocular: Non-rigid Multi-Camera-IMU Dense Reconstruction for UAV Navigation and Mapping 0615\nFlexure Mechanisms with Variable Stiffness and Damping Using Layer Jamming 1292\nFlightGoggles: Photorealistic Sensor Simulation for Perception-Driven Robotics Using Photogrammetry and Virtual Reality 1834\nFlight Recovery of MAVs with Compromised IMU 1352\nFlower Interaction Subsystem for a Precision Pollination Robot 1332\nFluid Lubricated Dexterous Finger Mechanism for Human-Like Impact Absorbing Capability 2404\nFlying through a Narrow Gap Using Neural Network: An End-To-End Planning and Control Approach 0784\nFollow the Robot: Modeling Coupled Human-Robot Dyads During Navigation 0878\nFoot with a Core-shell Structural Six-axis Force Sensor for Pedal Depressing and Recovering from Foot Slipping during Pedal Pushing Toward Autonomous Driving by Humanoids 0763\nForce-And-Motion Constrained Planning for Tool Use 1619\nForce Field-Based Indirect Manipulation Of UAV Flight Trajectories 1531\nForce Sensitive Robotic End-Effector Using Embedded Fiber Optics andDeep Learning Characterization for Dexterous Remote Manipulation 2179\nForecasting Time-To-Collision from Monocular Video: Feasibility, Dataset, and Challenges 1358\nForest Tree Detection and Segmentation Using High Resolution Airborne LiDAR 0536\nFormation of PVDF Piezoelectric Film on 3D Bellows Surface of Robotic Suction Cup for Providing Force Sensing Ability -Feasibility Study on Two Methods of Dip-Coating and Lamination 1347\nFree-Space Features: Global Localization in 2D Laser SLAM Using Distance Function Maps 0438\nFrom Pixels to Buildings: End-To-End Probabilistic Deep Networks for Large-Scale Semantic Mapping 1737\nFrustum ConvNet: Sliding Frustums to Aggregate Local Point-Wise Features for Amodal 3D Object Detection 0835\nFusing Body Posture with Facial Expressions for Joint Recognition of Affect in Child-Robot Interaction 2449\nFusing Lidar Data and Aerial Imagery with Perspective Correction for Precise Localization in Urban Canyons 1311\nFusion of Passive Ferromagnetic Sensors with the Navigational Data for the Improvement of the Detection of Underwater Metal-Containing Objects 0918\nGAPLE: Generalizable Approaching Policy LEarning for Robotic Object Searching in Indoor Environment 2275\nGaussian Mixture Model (GMM) Based Object Detection and Tracking Using Dynamic Patch Estimation 1013\nGaze-Based Intention Anticipation Over Driving Manoeuvres in Semi-Autonomous Vehicles 1667\nGaze Training by Modulated Dropout Improves Imitation Learning 0474\nGeneral Hand Guidance Framework Using Microsoft HoloLens 0814\nGeneralized Contact Constraints of Hybrid Trajectory Optimization for Different Terrains and Analysis of Sensitivity to Randomized Initial Guesses 1704\nGeneralized Multiple Correlation Coefficient As a Similarity Measurement between Trajectories 1020\nGeneralized Ray-Based Lattice Generation and Graph Representation of Wrench-Closure Workspace for Arbitrary Cable-Driven Robots 2589\nGenerate What You Can't See - a View-Dependent Image Generation 1170\nGenerating a Key Pose Sequence Based on Kinematics and Statics Optimization for Manipulating a Heavy Object by a Humanoid Robot 1048\nGenerating an Image of an Object's Appearance from Somatosensory Information During Haptic Exploration 1339\nGenerating Coordinated Reach-Grasp Motions with Neural Networks 2794\nGenerating Grasp Poses for a High-DOF Gripper Using Neural Networks 0196\nGeometric and Physical Constraints for Drone-Based Head Plane Crowd Density Estimation 0262\nGeo-Referenced Semantic Point Cloud Map Using the USyd Campus Dataset 2690\nGlassLoc: Plenoptic Grasp Pose Detection in Transparent Clutter 1636\nGLFP: Global Localization from a Floor Plan 0727\nGlobal Vision-Based Impedance Control for Robotic Wall Polishing 0558\nGoal-Directed Behavior under Variational Predictive Coding: Dynamic Organization of Visual Attention and Working Memory 0133\nGPU Accelerated Robust Scene Reconstruction 0279\nGQ-STN: Optimizing One-Shot Grasp Detection Based on Robustness Classifier 1065\nGraph-Based Design of Hierarchical Reinforcement Learning Agents 1226\nGraph-Based Path Planning for Autonomous Robotic Exploration in Subterranean Environments 1749\nGraph Element Networks: A Flexible Model for Robotic Applications 2726\nGrasping Unknown Objects Based on Gripper Workspace Spheres 1656\nGRIP: Generative Robust Inference and Perception for Semantic Robot Manipulation in Adversarial Environments 1598\nGrounding Language Attributes to Objects Using Bayesian Eigenobjects 1368\nGuinea Fowl Jumping Robot with Balance Control Mechanism: Modeling, Simulation, and Experiment Results 0309\nHand-eye calibration with a remote centre of motion 2216\nHand Movement Intention Recognition Based on EMG Intensity Map and Convolutional Neural Networks 2728\nHandover Process of Autonomous Driver Assist Systems - a Call for Critical Performance Assessment 2693\nHaptic Guidance for Robot-Assisted Endovascular Procedures: Implementation and Evaluation on Surgical Simulator 0495\nHaptic-Guided Shared Control for Needle Grasping Optimization in Minimally Invasive Robotic Surgery 1234\nHaptic Perception of Liquids Enclosed in Containers 1169\nHaptic Shared-Control Methods for Robotic Cutting under Nonholonomic Constraints 2480\nHaptiCube: A Compact 5-DoF Finger-Wearable Tactile Interface 0603\nHarmonious Sampling for Mobile Manipulation Planning 1306\nHeuristic-Based Multiple Mobile Depots Route Planning for Recharging Persistent Surveillance Robots 1402\nHierarchical Reinforcement Learning for Concurrent Discovery of Compound and Composable Policies 0988\nHierarchical Reinforcement Learning for Quadruped Locomotion 1783\nHierarchical Segmentation of Continuous Motions through sEMG Signal Analysis 2525\nHigh-Dimensional Motion Segmentation by Variational Autoencoder and Gaussian Processes 0297\nHigh-Fidelity Dexterous Tactile Telerobot for Intuitive Teleoperation 2727\nHigh-Speed Humanoid Robot Arm for Badminton Using Pneumatic-Electric Hybrid Actuators 2311\nHigh-Speed On-Chip Mixing by Micro-Vortex Generated by Controlling Local Jet Flow Using Dual Membrane Pumps 2104\nHigh-Speed Sliding Locomotion Generation on Slippery Surface of an Indirectly Controlled Robot With Viscoelastic Body 2251\nHomography-Based Deep Visual Servoing Methods for Planar Grasps 1691\nHong Hu - an Efficient and Versatile Tail-Sitter VTOL UAV Platform: Design, Implementation and Control 2678\nHouseExpo: A Large-Scale 2D Indoor Layout Dataset for Learning-Based Algorithms 2719\nHovering Control of a TTURT with Thrust Vector Decomposition Technique 2640\nHow Can Robot's Gaze Ratio and Gaze Type Show an Awareness of Power Dynamics to the People with Whom It Is Interacting? 2506\nHTetran ' a Polyabolo Inspired Self Reconfigurable Tiling Robot 1790\nHuman Intention Inference and On-Line Human Hand Motion Prediction for Human-Robot Collaboration 1604\nHuman Interactive Motion Planning for Shared Teleoperation 2703\nHumanoid Robot's Force-Based Heavy Manipulation Tasks with Torque-Controlled Arms and Wrist Force Sensors 0776\nHumanoid Robot Next Best View Planning under Occlusions Using Body Movement Primitives 2250\nHuman Robot Visual Interface for 3D Steering of a Flexible, Bio-Inspired Needle for Brain Surgery 0392\nHybrid Visual Servoing for Autonomous Robotic Laser Tattoo Removal 1208\nHysteresis Compensator with Learning-Based Pose Estimation for a Flexible Endoscopic Surgery Robot 0934\n'Why Don't You Have a Wife?!' Free Format Dialogue in CRI* 2753\nIdentification of Rat Ultrasonic Vocalizations from Mix Sounds of a Robotic Rat in Non-Silent Environments 0746\nIdentification of Time-Varying and Time-Scalable Synergies from Continuous Electromyographic Patterns 2237\nIdentifying Opportunities for Relationship-Focused Robotic Interventions in Strained Hierarchical Relationships 0139\nIgnorance Is Not Bliss: An Analysis of Central-Place Foraging Algorithms 1591\nImplementation of a Natural Dynamic Controller on an Under-Actuated Compass-Biped Robot 1259\nImplementing Regularized Predictive Control for Simultaneous Real-Time Footstep and Ground Reaction Force Optimization 0946\nImproved Energy Efficiency Via Parallel Elastic Elements for theStraight-Legged Vertically-Compliant Robot SLIDER 2734\nImproved Exploration through Latent Trajectory Optimization in Deep Deterministic Policy Gradient 1584\nImproved Learning Accuracy for Learning Stable Control from Human Demonstrations 0589\nImproved Mechanical Design and Simplified Motion Planning of Hybrid Active and Passive Cable-Driven Segmented Manipulator with Coupled Motion 0760\nImproved Planetary Rover Inertial Navigation and Wheel Odometry Performance through Periodic Use of Zero-Type Constraints 0886\nImproving 3D Object Detection for Pedestrians with Virtual Multi-View Synthesis Orientation Estimation 1574\nImproving Learning-Based Ego-Motion Estimation with Homomorphism-Based Losses and Drift Correction 1310\nImproving Local Trajectory Optimization by Probabilistic Movement Primitives 1128\nImproving Robot Success Detection Using Static Object Data 0143\nImproving Task-Parameterised Movement Learning Generalisation with Frame-Weighted Trajectory Generation 1448\nIMU-Based Spectrogram Approach with Deep Convolutional Neural Networks for Gait Classification 2775\nInchworm-Inspired Soft Climbing Robot Using Microspine Arrays 0327\nInertial-Based Motion Capturing and Smart Training System 0587\nInference of User-Intention in Remote Robot Wheelchair Assistance Using Multimodal Interfaces 1710\nINFER: INtermediate representations for FuturE pRediction 1417\nInferring Distributions Over Depth from a Single Image 0118\nInfluence of Parameters Uncertainties on the Positioning of Cable-Driven Parallel Robots 1005\nInformation Filter Occupancy Mapping Using Decomposable Radial Kernels 0695\nInformation-Guided Robotic Maximum Seek-And-Sample in Partially Observable Continuous Environments 2385\nInformed Region Selection for Efficient UAV-Based Object Detectors: Altitude-Aware Vehicle Detection with CyCAR Dataset 1463\nInfrastructure-Free NLoS Obstacle Detection for Autonomous Cars 0031\nInteger Programming As a General Solution Methodology for Path-Based Optimization in Robotics: Principles, Best Practices, and Applications 0849\nIntention-Driven Shoulder Rehabilitation for Targeted Neuro-Muscular Training Using an Exo-Musculoskeletal Robot 2777\nInteraction-Aware Decision Making with Adaptive Strategies under Merging Scenarios 1650\nInteractions with an Empathetic Agent: Regulating Emotions and Improving Engagement in Autism 2606\nInteraction Templates for Multi-Robot Systems 2292\nInteractive Trajectory Adaptation through Force-Guided Bayesian Optimization 0811\nIntroducing a Scalable and Modular Control Framework for Low-Cost Monocular Robots in Hazardous Environments 0983\nIntuitive Control of a Robotic Arm and Hand System with Pneumatic Haptic Feedback 2587\nInverse Dynamics Modeling of Robotic Manipulator with Hierarchical Recurrent Network 1999\nInverse Kinematics and Sensitivity Minimization of an N-Stack Stewart Platform 1063\nInverse Optimal Planning for Air Traffic Control 1258\nIRonCub: Towards Aerial Humanoid Robotics 2733\nIterative Learning Control for Fast and Accurate Position Tracking with an Articulated Soft Robotic Arm 0235\nIVOA: Introspective Vision for Obstacle Avoidance 1560\nJISAP: Joint Inference for Surgeon Attributes Prediction During Robot-Assisted Surgery 0540\nJointly Learnable Behavior and Trajectory Planning for Self-Driving Vehicles 1301\nJoint Offset Optimization of Hip Joints in Humanoid Robots 2701\nJoint Torque Estimation Toward Dynamic and Compliant Control for Gear-Driven Torque Sensorless Quadruped Robot 0201\nJoint Velocity and Acceleration Estimation in Serial Chain Rigid Body and Flexible Joint Manipulators 1959\nKinematically Redundant Hybrid Robots with Simple Singularity Conditions and Analytical Inverse Kinematic Solutions 2402\nKinematic Modeling of a Soft Pneumatic Actuator Using Cubic Hermite Splines 0717\nKinematics, Design and Experimental Validation of a Novel Parallel Robot for Two-Fingered Dexterous Manipulation 0900\nKinematic Synthesis of a Serial Robotic Manipulator by Using Generalized Differential Inverse Kinematics 2579\nLambda-Field: A Continuous Counterpart of the Bayesian Occupancy Grid for Risk Assessment 0423\nLaminated Foam-Based Soft Actuator for Actuatable Flexible Structure 0089\nLanding of a Multirotor Aerial Vehicle on an Uneven Surface Using Multiple On-Board Manipulators 0804\nLane Marking Learning Based on Crowdsourced Data 1148\nLarge-Scale 6D Object Pose Estimation Dataset for Industrial Bin-Picking 0857\nLazy Compilation of Variants of Multi-Robot Path Planning with Satisfiability Modulo Theory (SMT) Approach 1422\nLDLS: 3-D Object Segmentation through Label Diffusion from 2-D Images 2164\nLearning 2D to 3D Lifting for Object Detection in 3D for Autonomous Vehicles 1917\nLearning Actions from Human Demonstration Video for Robotic Manipulation 0994\nLearning Barrier Functions for Constrained Motion Planning with Dynamical Systems 2374\nLearning-Based Model Predictive Control for Autonomous Racing 2310\nLearning-Based Nonlinear Model Predictive Control of Reconfigurable Autonomous Robotic Boats: Roboats 0476\nLearning Based Robotic Bin-Picking for Potentially Tangled Objects 2408\nLearning by Demonstration and Robust Control of Dexterous In-Hand Robotic Manipulation Skills 1966\nLearning Continuous Time Control Policies by Minimizing the Hamilton-Jacobi-Bellman Residual 2685\nLearning Event-Based Height from Plane and Parallax 1144\nLearning Footstep Planning on Irregular Surfaces with Partial Placements 0597\nLearning from Demonstration Based on a Mechanism to Utilize an Object's Invisibility 2516\nLearning Generalisable Coupling Terms for Obstacle Avoidance Via Low-Dimensional Geometric Descriptors 2418\nLearning Generative Socially-Aware Models of Pedestrian Motion 2395\nLearning Grasp Affordance Reasoning through Semantic Relations 2527\nLearning Intention Aware Online Adaptation of Movement Primitives 2468\nLearning Interactive Behaviors for Musculoskeletal Robots Using Bayesian Interaction Primitives 0481\nLearning Local Feature Descriptor with Motion Attribute for Vision-based Localization 0823\nLearning Multimodal Representations for Sample-Efficient Recognition of Human Actions 0975\nLearning Multiple Sensorimotor Units to Complete Compound Tasks Using an RNN with Multiple Attractors 2092\nLearning Object Models for Non-Prehensile Manipulation 1178\nLearning Physics-Based Manipulation in Clutter: Combining Image-Based Generalization and Look-Ahead Planning 1536\nLearning Q-Network for Active Information Acquisition 1810\nLearning Real-Time Closed Loop Robotic Reaching from Monocular Vision by Exploiting a Control Lyapunov Function Structure 0583\nLearning Real-World Robot Policies by Dreaming 1155\nLearning Residual Flow as Dynamic Motion from Stereo Videos 1898\nLearning Safe Unlabeled Multi-Robot Planning with Motion Constraints 2531\nLearning Singularity Avoidance 1516\nLearning State-Dependent Sensor Measurement Models for Localization 1870\nLearning the Scope of Applicability for Task Planning Knowledge in Experience-Based Planning Domains 0230\nLearning to Augment Synthetic Images for Sim2Real Policy Transfer 0729\nLearning to Estimate Centers of Mass of Arbitrary Objects 1592\nLearning to Estimate Pose and Shape of Hand-Held Objects from RGB Images 0937\nLearning to Explore in Motion and Interaction Tasks 1281\nLearning to Generate Unambiguous Spatial Referring Expressions for Real-World Environments 1082\nLearning to Grasp Arbitrary Household Objects from a Single Demonstration 1051\nLearning Topometric Semantic Maps from Occupancy Grids 1776\nLearning to Sequence Multiple Tasks with Competing Constraints 1238\nLearning Via-Point Movement Primitives with Inter and Extrapolation Capabilities 1076\nLearning Virtual Borders through Semantic Scene Understanding and Augmented Reality 0253\nLearning Virtual Grasp with Failed Demonstrations via Bayesian Inverse Reinforcement Learning 0490\nLearn to Adapt to Human Walking: A Model-Based Reinforcement Learning Approach for a Robotic Assistant Rollator 2489\nLEGO: Leveraging Experience in Roadmap Generation for Sampling-Based Planning 1481\nLIC-Fusion: LiDAR-Inertial-Camera Odometry 2005\nLiDAR Based Navigable Region Detection for Unmanned Surface Vehicles 0582\nLiDAR-Flow: Dense Scene Flow Estimation from Sparse LiDAR and Stereo Images 0972\nLifelong Federated Reinforcement Learning: A Learning Architecture for Navigation in Cloud Robotic Systems 2090\nLine-Based Absolute and Relative Camera Pose Estimation in Structured Environments 0898\nLoam_livox: A Robust LiDAR Odemetry and Mapping (LOAM) Package for Livox LiDAR 2663\nLocalization and Mapping Using Instance-Specific Mesh Models 0572\nLocal Online Motor Babbling: Learning Motor Abundance of a Musculoskeletal Robot Arm 0351\nLocal Pose Optimization with an Attention-Based Neural Network 1439\nLocomotion Planning of a Variable Geometry Robot Based on Polygon-Shaped Ground Contacts 2639\nLong Range Neural Navigation Policies for the Real World 2264\nLong-Term Community Social Robots to Promote Social Connectedness among Older Adults 2580\nLong-Term Prediction of Motion Trajectories Using Path Homology Clusters 0667\nLong-Term Visual Inertial SLAM Based on Time Series Map Prediction 0893\nLong-Time Self-Body Image Acquisition and Its Application to the Control of Musculoskeletal Structures 2344\nLook Further to Recognize Better: Learning Shared Topics and Category-Specific Dictionaries for Open-Ended 3D Object Recognition 0127\nLow-Cost Sonar Navigation System 1459\nLow Level Control of a Quadrotor with Deep Model-Based Reinforcement Learning 2530\nLSwarm: Efficient Collision Avoidance for Large Swarms with Coverage Constraints in Complex Urban Scenes 2370\nMacro-Micro Multi-Arm Robot for Single-Port Access Surgery 2456\nMagnetic-Needle-Assisted Micromanipulation of Dynamically Self-Assembled Magnetic Droplets for Cargo Transportation 2205\nMagnetic Needle Steering Model Identification Using Expectation-Maximization 1099\nMagnetic Sensor Based Probe for Microrobot Detection and Localization 2784\nMaking Sense of Audio Vibration for Liquid Height Estimation in Robotic Pouring 1118\nManipulation Motion Taxonomy and Coding for Robots 1733\nManipulation Planning with Soft Orientation Constraints Based on Composite Configuration Space 2788\nManipulation Purpose Underwater Agent Vehicle for Ghost Net Recovery Mission 1625\nMap-Aware SLAM with Sparse Map Features 0178\nMap Based Human Motion Prediction for People Tracking 0167\nMapping for Planetary Rovers from Terramechanics Perspective 1168\nMathematic Modeling and Optimal Design of a Magneto-Rheological Clutch for the Compliant Actuator in Physical Robot Interactions 2478\nMAVNet: An Effective Semantic Segmentation Micro-Network for MAV-Based Tasks 2330\nMaximum Information Bounds for Planning Active Sensing Trajectories 1273\nMaximum Likelihood Path Planning for Fast Aerial Maneuvers and Collision Avoidance 0106\nMeasuring Engagement Elicited by Eye Contact in Human-Robot Interaction 0949\nMeta-Learning for Multi-Objective Reinforcement Learning 0929\nMeta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning 2686\nMetric Monocular Localization Using Signed Distance Fields 0840\nMiniaturization of MR Safe Pneumatic Rotational Stepper Motors 0892\nMinimal Sensor Setup in Lower Limb Exoskeletons for Motion Classification Based on Multi-Modal Sensor Data 0722\nMinimum $k$-Connectivity Maintenance for Robust Multi-Robot Systems 1935\nMisalignment Recognition Using Markov Random Fields with Fully Connected Latent Variables for Detecting Localization Failures 2263\nMixed Reality Control of the Humanoid Robot Eve 2735\nMobile Robot Learning from Human Demonstrations with Nonlinear Model Predictive Control 1451\nMobile Robot Localization with Reinforcement Learning Map Update Decision Aided by an Absolute Indoor Positioning System 1223\nModel Free Calibration of Wheeled Robots Using Gaussian Process 1795\nModeling and Force Control of a Terramechanical Wheel-Soil Contact for a Robotic Manipulator Used in the Planetary Rover Design Process 0754\nModeling and Identification for the Design of a Rotary Soft Actuator Based on Wren Mechanism 0652\nModeling, Learning and Prediction of Longitudinal Behaviors of Human-Driven Vehicles by Incorporating Internal Human Decision-Making Process Using Inverse Model Predictive Control 1284\nModeling Novel Soft Mechanosensors based on Air-Flow Measurements 2568\nModeling, Simulation and Experimental Validation of Tendon-Driven Soft-Arm Robot Configuration - a Continuum Mechanics Approach 1091\nModel-Less Active Compliance for Continuum Robots Using Recurrent Neural Networks 2319\nModelling and Dynamic Tracking Control of Industrial Vehicles with Tractor-trailer Structure 1947\nModelling of Uniaxial EGaIn-Based Strain Sensors for Proprioceptive Sensing of Soft Robots 1933\nModel Predictive Contouring Control for Collision Avoidance in Unstructured Dynamic Environments 2387\nModel Predictive Control Based Dynamic Path Tracking of a Four-Wheel Steering Mobile Robot 1060\nModel Simplification for Dynamic Control of Series-Parallel Hybrid Robots - a Representative Study on the Effects of Neglected Dynamics 1049\nModular Volumetric Actuators Using Motorized Auxetics 1643\nMonocular Depth Estimation in New Environments with Absolute Scale 0759\nMonocular Object and Plane SLAM in Structured Environments 2277\nMonocular Outdoor Semantic Mapping with a Multi-Task Network 0174\nMonocular Plan View Networks for Autonomous Driving 1829\nMorphing Structure for Changing Hydrodynamic Characteristics of a Soft Underwater Walking Robot 2540\nMotion Decoupling and Composition Via Reduced Order Model Optimization for Dynamic Humanoid Walking with CLF-QP Based Active Force Control 0625\nMotion Direction Decoding of Upper Limb from EEG Signals with a Cognitive Distraction Task 2675\nMotion Planning for a Continuum Robotic Mobile Lamp: Defining and Navigating the Configuration Space 1370\nMotor-Propeller Matching of Aerial Propulsion Systems for Direct Aerial-Aquatic Operation 0416\nMoving Onto High Steps for a Four-Limbed Robot with Torso Contact 1965\nMPERL : Hardware and Software Co-Design for Robotic Manipulators 1211\nMRLift: A Semi-Active Lower Back Support Exoskeleton Based on MR Fluid and Force Retention Technology 1388\nMT-RRT: A General Purpose Multithreading Library for Path Planning 0806\nMulti-Agent Image Classification Via Reinforcement Learning 2003\nMulticamera 3D Reconstruction of Dynamic Surgical Cavities: Non-Rigid Registration and Point Classi\ufb01cation 0585\nMulti-Contact Stabilization of a Humanoid Robot for Realizing Dynamic Contact Transitions on Non-Coplanar Surfaces 1098\nMulti-Controller Multi-Objective Locomotion Planning for Legged Robots 0701\nMulti-DoF Force Characterization of Soft Actuators 2554\nMulti-Hand Direct Manipulation of Complex Constrained Virtual Objects 0687\nMulti-Layer Environmental Affordance Map for Robust Indoor Localization, Event Detection and Social Friendly Navigation 1075\nMultilevel Incremental Roadmap Spanners for Reactive Motion Planning 1878\nMultimodal Uncertainty Reduction for Intention Recognition in Human-Robot Interaction 0432\nMultiple Hypothesis Semantic Mapping for Robust Data Association 2279\nMulti-Robot Assembly Sequencing Via Discrete Optimization 1747\nMultirobot Charging Strategies: A Game-Theoretic Approach 2125\nMulti-Robot Distributed Digital Printing System 2795\nMulti Robot Route Planning (MRRP): Extended Spatial-Temporal Prioritized Planning 0163\nMulti-Sensor 6-DoF Localization for Aerial Robots in Complex GNSS-Denied Environments 1328\nMulti-step Pick-And-Place Tasks Using Object-centric Dense Correspondences 1421\nMulti-Vehicle Cooperative Local Mapping Using Split Covariance Intersection Filter 0869\nMuSe: Multi-Sensor Integration Strategies Applied to Sequential Monte Carlo Methods 0043\nNear-Contact Grasping Strategies from Awkward Poses: When Simply Closing Your Fingers Is Not Enough 0637\nNeural Control with an Artificial Hormone System for Energy-Efficient Compliant Terrain Locomotion and Adaptation of Walking Robots 2584\nNeural-Learning Trajectory Tracking Control of Flexible-Joint Robot Manipulators with Unknown Dynamics 1590\nNeural Network Based Heterogeneous Sensor Fusion for Robot Motion Planning 0459\nNeural Path Planning: Fixed Time, Near-Optimal Path Generation Via Oracle Imitation 0194\nNeuroTrajectory: A Neuroevolutionary Approach to Local State Trajectory Learning for Autonomous Vehicles 2219\nn-MeRCI: A New Metric to Evaluate the Correlation between Predictive Uncertainty and True Error 1624\nNon-Contact Sensing of Respiratory Signals 2754\nNonlinear Optimization of Step Duration and Step Location 0950\nNon-Myopic Planetary Exploration Combining in Situ and Remote Measurements 1367\nNon-Parametric Mixed-Manifold Products Using Multiscale Kernel Densities 1893\nNon-Uniform Robot Densities in Vibration Driven Swarms Using Phase Separation Theory 1874\nNormal Distribution Mixture Matching Based Model Free Object Tracking Using 2D LIDAR 1307\nNovel Lockable and Stackable Compliant Actuation Unit for Modular +SPEA Actuators 2600\nObject Placement Planning and Optimization for Robot Manipulators 1756\nObject Proposal Algorithms in the Wild: Are They Generalizable to Robot Perception? 1538\nObject Rearrangement with Nested Nonprehensile Manipulation Actions 1431\nObject Singulation Via Nonlinear Pushing for Robotic Grasping 0428\nObservability Analysis of Position Estimation for Quadrotors with Modified Dynamics and Range Measurements 1690\nObstacle Avoidance Using a Capacitive Skin for Safe Human-Robot Interaction 0903\nObstacle Climbing by a Humanoid Robot Using Standing Jump Motion 2702\nObstacle Overcoming on a Fa\u00c3\u00a7ade: Novel Design of a Rotating Leg Mechanism 2634\nOcclusion-Robust Deformable Object Tracking without Physics Simulation 1309\nOlder People Prefrontal Cortex Activation Estimates Their Perceived Difficulty of a Humanoid-Mediated Conversation 2281\nOmnipush: Accurate, Diverse, Real-World Dataset of Pushing Dynamics with RGB-D Video 1101\nOnboard Marker-Less Detection and Localization of Non-Cooperating Drones for Their Safe Interception by an Autonomous Aerial System 2327\nOn-Chip Three-Dimension Cell Rotation Using Whirling Flows Generated by Oscillating Asymmetrical Microstructures 0313\nOn Data Sharing Strategy for Decentralized Collaborative Visual-Inertial Simultaneous Localization and Mapping 0465\nOn Enhancing Ground Surface Detection from Sparse Lidar Point Cloud 0013\nOne-Shot Composition of Vision-Based Skills from Demonstration 0561\nOne-Shot Object Localization Using Learnt Visual Cues Via Siamese Networks 0353\nOn Flying Backwards: Preventing Run-Away of Small, Low-Speed Fixed-Wing UAVs in Strong Winds 1693\nOnline Active Safety for Robotic Manipulators 1732\nOnline and Consistent Occupancy Grid Mapping for Planning in Unknown Environments 1487\nOnline Motion Planning Over Multiple Homotopy Classes with Gaussian Process Inference 1340\nOnline Optimal Impedance Planning for Legged Robots 1763\nOnline Performance Prediction and Profiling of Human Activities by Observation 1033\nOnline Planning for Autonomous Underwater Vehicles Performing Information Gathering Tasks in Large Subsea Environments 0997\nOnline Relative Footstep Optimization for Legged Robots Dynamic Walking Using Discrete-Time Model Predictive Control 1001\nOnline System Identification Algorithm without Persistent Excitation for Robotic Systems: Application to Reconfigurable Autonomous Vessels 1069\nOnline Trajectory Generation of a MAV for Chasing a Moving Target in 3D Dense Environments 1511\nOnline Trajectory Generation: Reactive Control with Return Inside an Admissible Kinematic Domain 0877\nOn Model-Based Adhesion Control of a Vortex Climbing Robot 1401\nOn Modeling the Effects of Auditory Annoyance on Driving Style and Passenger Comfort 1415\nOn the Bayes Filter for Shared Autonomy 2304\nOn the Covariance of X in AX = XB 2597\nOn the Effect of Semielliptical Foot Shape on the Energetic Efficiency of Passive Bipedal Gait 0873\nOn the Feasibility of Multi-Degree-Of-Freedom Haptic Devices Using Passive Actuators 1453\nOn the Tunable Sparse Graph Solver for Pose Graph Optimization in Visual SLAM Problems 1595\nOn Training Flexible Robots Using Deep Reinforcement Learning 1855\nOperational Space Control Framework for Torque Controlled Humanoid Robots with Joint Elasticity 2529\nOperation of a Pneumatic Soft Manipulator Using a Wearable Interface with Flexible Strain Sensors 2132\nOpposite Treatments on Null Space: Null Space Projection vs Null Space Avoidance 2711\nOptical Coherence Tomography Guided Robotic Device for Autonomous Needle Insertion in Cornea Transplant Surgery 1539\nOptimal Solving of Constrained Path-Planning Problems with Graph Convolutional Networks and Optimized Tree Search 1095\nOptimal Temporal Logic Planning for Multi-Robot Systems in Uncertain Semantic Maps 1083\nOptimal Temporal Logic Planning with Cascading Soft Constraints 1824\nOptimization Based Motion Planning for Multi-Limbed Vertical Climbing Robots 0601\nOptimization Based Trajectory Planning of Mobile Cable-Driven Parallel Robots 1137\nOptimization Model for Planning Precision Grasps with Multi-Fingered Hands 1892\nOptimized Locomotion for Energy-Efficient Quadrupedal Robot Over Rough Terrain 2646\nOptimizing Motion-Planning Problem Setup Via Bounded Evaluation with Application to Following Surgical Trajectories 1635\nOrbit Characterization, Stabilization and Composition of 3D Underactuated Bipedal Walking Via Hybrid Passive Linear Inverted Pendulum Model 0624\nORBSLAM-Atlas: A Robust and Accurate Multi-Map System 0830\nOREOS: Oriented Recognition of 3D Point Clouds in Outdoor Scenarios 1242\nOutlier-Robust Spatial Perception: Hardness, General-Purpose Algorithms, and Guarantees 1688\nOutlier-Robust State Estimation for Humanoid Robots 0433\nPanopticFusion: Online Volumetric Semantic Mapping at the Level of Stuff and Things 0278\nPaper-Based Modular Origami Gripper 0282\nPartial Caging: A Clearance-Based Definition and Deep Learning 2217\nPASS3D: Precise and Accelerated Semantic Segmentation for 3D Point Cloud 1842\nPassive Inverted Ultra-Short Baseline (piUSBL) Localization: An Experimental Evaluation of Accuracy 1268\nPassive Model Reduction and Switching for Fast Soft Object Simulation with Intermittent Contacts 1953\nPath Planning Algorithm for a Transformation of a Shape-Morphing Wheel for a Step-Climbing 2642\nPath Planning for Surgery Robot with Bidirectional Continuous Tree Search and Neural Network 0812\nPath Planning with Incremental Roadmap Update for Visibility-Based Target Tracking 0673\nPD based Robust Quadratic Programs for Robotic Systems 1039\nPedestrian Density Prediction for Efficient Mobile Robot Exploration 0166\nPeople's V-Formation and Side-by-Side Model Adapted to Accompany Groups of People by Social Robots 0472\nPerception As Prediction Using General Value Functions in Autonomous Driving Applications 1442\nPerception of Pedestrian Avoidance Strategies of a Self-Balancing Mobile Robot 2510\nPerception System Design for Low-Cost Commercial Ground Robots: Sensor Configurations, Calibration, Localization, and Mapping 2209\nPerformance Guarantees for Receding Horizon Search with Terminal Cost 1438\nPeriodic Trajectory Planning and Robust Output Zeroing Control for Underactuated Bipedal Robots with Predicted Disturbances 1623\nPermanent Magnets Based Actuator for Microrobots Navigation 0839\nPerson-Following for Telepresence Robots Using Web Cameras 0396\nPhysical Fatigue Analysis of Assistive Robot Teleoperation Via Whole-Body Motion Mapping 1567\nPhysical Orienteering Problem for Unmanned Aerial Vehicle Data Collection Planning in Environments with Obstacles 2146\nPiecewise Rigid Scene Flow with Implicit Motion Segmentation 1418\nPixel-Attentive Policy Gradient for Multi-Fingered Grasping in Cluttered Scenes 0252\nPixels to Plans: Learning Non-Prehensile Manipulation by Imitating a Planner 1386\nPlanning Beyond the Sensing Horizon Using a Learned Context 1586\nPlanning High-Quality Motions for Concentric Tube Robots in Point Clouds via Parallel Sampling and Optimization 1365\nPlanning in Stochastic Environments with Goal Uncertainty 0943\nPlanning Reactive Manipulation in Dynamic Environments 0643\nPlant Phenotyping by Deep-Learning Based Planner for Multi-Robots 2176\nPlasticity in Collective Decision-Making for Robots: Creating Global Reference Frames, Detecting Dynamic Environments, and Preventing Lock-Ins 1968\nPnS: A Perspective-n-Spheres Algorithm for Laparoscope Calibration in Minimally Invasive Surgery 0752\nPointAtrousNet: Point Atrous Convolution for Point Cloud Analysis 2290\nPolicy Distillation and Value Matching in Multiagent Reinforcement Learning 1867\nPort-Hamiltonian Passivity-Based Control on SE(3) of a Fully-Actuated UAV for Aerial Physical Interaction Near-Hovering 2434\nPose-Aware Placing with Semantic Labels - Brandname-Based Affordance Prediction and Cooperative Dual-Arm Active Manipulation 0208\nPose Estimation for Omni-Directional Cameras Using Sinusoid Fitting 1527\nPose-Graph Based Indoor Navigation Test for Unmanned Underwater Vehicle Navigation 2760\nPosition-Based Control of Under-Constrained Haptics: A System for the Dexmo Glove 2496\nPosition-Based Monocular Visual Servoing of an Unknown Target Using Online Self-Supervised Learning 1902\nPosition Control of Wire-Suspended Hand for Long-Reach Aerial Manipulation 2674\nPPRNet: Point-Wise Pose Regression Network for Instance Segmentation and 6D Pose Estimation in Bin-Picking Scenarios 0342\nPrecise Correntropy-Based 3D Object Modelling with Geometrical Traffic Prior 1593\nPrecision Modeling and Optimally-Safe Design of Quadcopters for Controlled Crash Landing in Case of Rotor Failure 1618\nPrecision Pouring into Unknown Containers by Service Robots 0756\nPredicting Grasp Success with a Soft Sensing Skin and Shape-Memory Actuated Gripper 2309\nPrediction of Human Arm Target for Robot Reaching Movements 0219\nPredictive and Adaptive Maps for Long-Term Visual Navigation in Changing Environments 0005\nPredictive Inverse Kinematics for Redundant Manipulators with Task Scaling and Kinematic Constraints 2601\nPredictive Inverse Kinematics: Optimizing Future Trajectory through Implicit Time Integration and Future Jacobian Estimation 0519\nPredictive Optimization of Assistive Force on Admittance Control-Based Mobile Walking Support System 2487\nPreliminary Evaluation of an Orbital Camera for Teleoperation of Remote Manipulators 1088\nPreliminary Investigation about Relationship between Perceived Intimacy and Touch Characteristics 2671\nPreliminary Results of Active Compression Sleeve Using Wire and Fabric Mechanism 2700\nPreliminary Study for Developing a Vision-Based Detection System of Unmanned Surface Vessels 2705\nPressure-Driven Body Compliance Using Robot Skin 2425\nPrinting-While-Moving: A New Paradigm for Large-Scale Robotic 3D Printing 0347\nPrivacy-Preserving Robot Vision with Anonymized Faces by Extreme Low Resolution 1845\nProbabilistic Risk Metrics for Navigating Occluded Intersections 2394\nProposal of a Peristaltic Motion Type Duct Cleaning Robot for Traveling in a Flexible Pipe 2533\nProto-Object Based Saliency for Event-Driven Cameras 1195\nqpSWIFT : A Real-Time Sparse Quadratic Program Solver for Robotic Applications 2157\nQuaternion-based Smooth Trajectory Generator for Via Poses in SE(3) Considering Kinematic Limits in Cartesian Space 2386\nQuickly Inserting Pegs into Uncertain Holes Using Multi-View Images and Deep Network Trained on Synthetic Data 2308\nRadar SLAM for Indoor Disaster Environments Via Multi-Modal Registration to Prior LiDAR Map 0355\nRandomized Sensor Selection for Nonlinear Systems with Application to Target Localization 2504\nRange-Limited, Distributed Algorithms on Higher-Order Voronoi Partitions in Multi-Robot Systems 0509\nRangeNet++: Fast and Accurate LiDAR Semantic Segmentation 0346\nRapid and Robust Monocular Visual-Inertial Initialization with Gravity Estimation via Vertical Edges 1687\nRapid Collision Detection for Multicopter Trajectories 1449\nRapid Design of Mechanical Logic Based on Quasi-Static Electromechanical Modeling 1304\nRapid Estimation of Optical Properties for Simulation-Based Evaluation of Pose Estimation Performance 0671\nRapid Trajectory Optimization Using C-FROST with Illustration on a Cassie-Series Dynamic Walking Biped 1651\nReactive Interaction through Body Motion and the Phase-State-Machine 0427\nReal-Time 6D Object Pose Estimation on CPU 0189\nReal-Time Biped Walking-Pattern Generation by Spline Collocation 2706\nRealtime Contact Dynamics for Continuum Arms Using Physics Engines 2776\nReal-Time Dense Depth Estimation Using Semantically-Guided LIDAR Data Propagation and Motion Stereo 2260\nReal-Time Detection of Distracted Driving Using Dual Cameras 2668\nReal-Time Global Registration for Globally Consistent RGB-D SLAM 2577\nReal-Time Model-Based Image Color Correction for Underwater Robots 1775\nReal-Time Monitoring of Human Task Advancement 0888\nReal-Time Quad-Rotor Path Planning Using Convex Optimization and Compound State-Triggered Constraints 2315\nReal-Time Sampling-Based Optimization on FPGA for Accurate Grid Map Merging in Embedded Robotic Systems 2714\nRebellion and Obedience: The Effects of Intention Prediction in Cooperative Handheld Robots 0968\nRecalling Candidates of Grasping Method from an Object Image Using Neural Network 0875\nReconfiguration Motion Planning for Variable Topology Truss 1830\nReconstructing Endovascular Catheter Interaction Forces in 3D Using Multicore Optical Shape Sensors 2136\nRecurrent Convolutional Fusion for RGB-D Object Recognition 2138\nRedundant Resolution Method of an Underwater Manipulation for Disturbance Rejection 2641\nReFusion: 3D Reconstruction in Dynamic Environments for RGB-D Cameras Exploiting Residuals 0135\nRegion-Wise Polynomial Regression for 3D Mobile Gaze Estimation 0223\nRegressing Noisy Joint States from Visual Data Using CNN 2665\nReinforcement Learning Boat Autopilot: A Sample-Efficient and Model Predictive Control Based Approach 2313\nReinforcement Learning of Trajectory Distributions: Applications in Assisted Teleoperation and Motion Planning 2550\nRelaxing the Conservatism of Passivity Condition for Impedance Controlled Series Elastic Actuators 2312\nRemote Center Motion of a Surgical Assisted Robot for In-Situ Collaboration 2654\nRepresentation Learning via Parallel Subset Reconstruction for 3D Point Cloud Generation 0053\nRepresenting Robot Task Plans As Robust Logical-Dynamical Systems 1703\nResearch on Finite Ground Effect of a Rotor 1709\nResFlow: Multi-Tasking of Sequentially Pooling Spatiotemporal Features for Action Recognition and Optical Flow Estimation 0410\nResilience by Reconfiguration: Exploiting Heterogeneity in Robot Teams 2108\nResolving Elevation Ambiguity in 1-D Radar Array Measurements Using Deep Learning 2086\nResponsive Joint Attention in Human-Robot Interaction 0028\nResQbot 2.0: A Mobile Stretcher Bed Robot with Neck Securing Device for Safe Casualty Extraction 2716\nRetrieval-Based Localization Based on Domain-Invariant Feature Learning under Changing Environments 1695\nRGB-To-TSDF: Direct TSDF Prediction from a Single RGB Image for Dense 3D Reconstruction 0378\nRight of Way, Assertiveness and Social Recognition in Human-Robot Doorway Interaction 1564\nRINS-W: Robust Inertial Navigation System on Wheels 0067\nRISE-SLAM: A Resource-Aware Inverse Schmidt Estimator for SLAM 0283\nRisk-Aware Motion Planning and Control Using CVaR-Constrained Optimization 2316\nRiverine Coverage with an Autonomous Surface Vehicle Over Known Environments 0502\nRoboat: An Autonomous Surface Vehicle for Urban Waterways 0188\nRobot-Assisted Composite Manufacturing Using Deep Learning and Multi-View Computer Vision 2732\nRobot Audition Approaches to Field Observation of Bird Songs 2682\nRobot-Based Machining of Unmodeled Objects via Feature Detection in Dense Point Clouds 0960\nRobot-Based Strategy for Objective Assessment of Motor Impairments 2649\nRobot-Enhanced Therapy: Development and Validation of a Supervised Autonomous Robotic System for Autism Spectrum Disorders Therapy 2500\nRobot Finger with Remote Center of Motion Mechanism for Covering Joints with Thick Skin 2151\nRobot'Robot Gesturing for Anchoring Representations 2605\nRobotic Cutting of Solids Based on Fracture Mechanics and FEM 1501\nRobotic Laparoendoscopic Single-Site Surgery Platform on dVRK 2772\nRobotic Tracking Control with Kernel Trick-Based Reinforcement Learning 0381\nRobotic Ultrasound for Catheter Navigation in Endovascular Procedures 0487\nRobot Learning of Shifting Objects for Grasping in Cluttered Environments 0482\nRobot Learning Via Human Adversarial Games 1400\nRobot Localization in Floor Plans Using a Room Layout Edge Extraction Network 1403\nRobot Localization Via Odometry-Assisted Ultra-Wideband Ranging with Stochastic Guarantees 1037\nRobots That Take Advantage of Human Trust 1277\nRobust and Adaptive Lower Limb Prosthesis Control Via Extended Kalman Filter-Based Phase Estimation 2097\nRobust and Efficient Quadrotor Trajectory Generation for Fast Autonomous Flight 2377\nRobust and Efficient Vehicles Motion Estimation with Low-Cost Multi-Camera and Odometer-Gyroscope 1803\nRobust, Compliant Assembly with Elastic Parts and Model Uncertainty 1059\nRobust Deformation Model Approximation for Robotic Cable Manipulation 1626\nRobust Grasp Planning Over Uncertain Shape Completions 0761\nRobust Hand-Eye Calibration via Iteratively Re-weighted Rank-Constrained Semi-Definite Programming 2026\nRobust High Accuracy Visual-Inertial-Laser SLAM System 0971\nRobust Impedance Shaping of Redundant Teleoperators with Time-Delay Via Sliding Mode Control 1203\nRobust Legged Robot State Estimation Using Factor Graph Optimization 2582\nRobust Loop Closure Detection Based on Bag of SuperPoints and Graph Verification 1544\nRobust Moving Path Following Control for Robotic Vehicles: Theory and Experiments 2295\nRobust Non-Rigid Point Set Registration Algorithm Considering Anisotropic Uncertainties Based on Coherent Point Drift 1673\nRobust Outdoor Self-Localization in Changing Environments 1163\nRobust Real-Time RGB-D Visual Odometry in Dynamic Environments Via Rigid Motion Model 1254\nRobust Trajectory Planning for a Multirotor against Disturbance based on Hamilton-Jacobi Reachability Analysis 1801\nRobust UAV Localization Around the Large Scale Facilities with Multiple Subsidiary UAVs 2757\nRobust UAV Position and Attitude Estimation Using Multiple GNSS Receivers for Laser-Based 3D Mapping 0577\nRoFICoM -- First Open-Hardware Connector for Metamorphic Robots 0354\nROI-Based Robotic Grasp Detection for Object Overlapping Scenes 0190\nRolling-Shutter Modelling for Direct Visual-Inertial Odometry 0350\nRONet: Real-Time Range-Only Indoor Localization Via Stacked Bidirectional LSTM with Residual Attention 2267\nRouting a Fleet of Automated Vehicles in a Capacitated Transportation Network 0931\nSafe Path Planning with Gaussian Process Regulated Risk Map 0706\nSafe Physical HRI: Toward a Unified Treatment of Speed and Separation Monitoring Together with Power and Force Limiting 2490\nSailMAV: Design and Implementation of a Novel Multi-Modal Flying Sailing Robot 2599\nSample-Efficient Deep Reinforcement Learning with Imaginary Rollouts for Human-Robot Interaction 0933\nSample Efficient Interactive End-To-End Deep Learning for Self-Driving Cars with Selective Multi-Class Safe Dataset Aggregation 1303\nSampling-Based Motion Planning for Aerial Pick-And-Place 1504\nSampling-Based Motion Planning of 3D Solid Objects Guided by Multiple Approximate Solutions 1108\nSampling-Based Path Planning for Cooperative Autonomous Maritime Vehicles to Reduce Uncertainty in Range-Only Localisation 2223\nScaffold-Based Asynchronous Distributed Self-Reconfiguration by Continuous Module Flow 0990\nScaling Robot Supervision to Hundreds of Hours with RoboTurk: Robotic Manipulation Dataset through Human Reasoning and Dexterity 1251\nScheduling of Mobile Workstations for Overlapping Production Time and Delivery Time 0586\nSeeing behind Things: Extending Semantic Segmentation to Occluded Regions 1433\nSeeing Beyond Appearance ' Mapping Real Images into Geometrical Domains for Unsupervised CAD-Based Recognition 1124\nSeeking the Analytical Approximation of the Stance Dynamics of the 3D Spring-Loaded Inverted Pendulum Model by Using Perturbation Approach 0633\nSegregation and Flow of Modules in a Robot Swarm Utilising the Brazil Nut Effect 0622\nSelf-Calibration and Learning on Chip: Towards Neuromorphic Robots 2752\nSelf-Collision Detection and Avoidance for Dual-Arm Concentric Tube Robots 2523\nSelf-Modeling Tracking Control of Crawler Fire Fighting Robot Based on Causal Network 0419\nSelf-Organised Flocking in Robotic Swarm Based on Active Elastic Sheet 2625\nSelf-Organized Adaptive Paths in Multi-Robot Manufacturing: Reconfigurable and Pattern-Independent Fibre Deployment 1952\nSelf-Specialization of General Robot Plans Based on Experience 2508\nSelf-Supervised 3D Shape and Viewpoint Estimation from Single Images for Robotics 0758\nSelf-Supervised Transfer Learning for Instance Segmentation through Physical Interaction 1890\nSemantically Assisted Loop Closure in SLAM Using NDT Histograms 0879\nSemantic Mates: Intuitive Geometric Constraints for Efficient Assembly Specifications 0478\nSemantic Segmentation Using GAN and Weakly Supervised Based on Deep Transfer Learning 2618\nSemi-Autonomous Interventional Manipulation Using Pneumatically Attachable Flexible Rails 0871\nSensitivity of Legged Balance Control to Uncertainties and Sampling Period 2349\nSensor Installation and Retrieval Operations Using an Unmanned Aerial Manipulator 2558\nSensorless Estimation of the Planar Distal Shape of a Tip-Actuated Endoscope 2497\nSeqLPD: Sequence Matching Enhanced Loop-Closure Detection Based on Large-Scale Point Cloud Description for Self-Driving Vehicles 1907\nSequential Clustering for Tactile Image Compression to Enable Direct Adaptive Feedback 1971\nSetup and Method for Remote Center of Motion Positioning Guidance During Robot-Assisted Surgery 2447\nSGANVO: Unsupervised Deep Visual Odometry and Depth Estimation with Stacked Generative Adversarial Networks 2427\nShared Autonomy of a Flexible Manipulator in Constrained Endoluminal Surgical Tasks 2232\nShared Controller for Obstacle Avoidance of Manipulator for Teleopeartion System 2662\nSharing Is Caring: Socially-Compliant Autonomous Intersection Negotiation 1136\nSiamese Convolutional Neural Network for Sub-Millimeter-Accurate Camera Pose Estimation and Visual Servoing 1514\nSilhoNet: An RGB Method for 6D Object Pose Estimation 2334\nSIMDop: SIMD Optimized Bounding Volume Hierarchies for Collision Detection 0940\nSimitate: A Hybrid Imitation Learning Benchmark 0497\nSim-To-(Multi)-Real: Transfer of Low-Level Robust Control Policies to Multiple Quadrotors 1589\nSim-To-Real Learning for Casualty Detection from Ground Projected Point Cloud Data 1239\nSim-To-Real Transfer for Biped Locomotion 1040\nSimulation-Based Physics Reasoning for Consistent Scene Estimation in an HRI Context 1050\nSimultaneous Drone Localisation and Wind Turbine Model Fitting During Autonomous Surface Inspection 0648\nSimultaneous Transparent and Non-Transparent Objects Segmentation with Multispectral Scenes 0694\nSingle-Hand Movement Direction Decoding from EEG Signals under Opposite-Hand Movement Distraction 2627\nSingle Motor-Based Bidirectional Twisted String Actuation with Variable Radius Pulleys 2412\nSituation Awareness for Proactive Robots in HRI 0967\nSix DoF Pose Estimation for a Tendon-Driven Continuum Mechanism without a Deformation Model 2347\nSkill Interaction Categories for Communication in Flexible Human-Robot Teams 0338\nSmall-Scale Compliant Dual Arm with Tail for Winged Aerial Robots 1409\nSoft Action Particle Deep Reinforcement Learning for a Continuous Action Space 0588\nSoft Pneumatic Helical Actuator with High Contraction Ratio 2538\nSoft Polymer-Electrolyte-Fuel-Cell Tube Realizing Air-Hose-Free Thin McKibben Muscles 2520\nSP2 (spherically-Stratified-Points Projection): Generating Novel Images for 3D Point Cloud Segmentation 2696\nSparse-3D Lidar Outdoor Map-Based Autonomous Vehicle Localization 0595\nSparse Depth Enhanced Direct Thermal-Infrared SLAM Beyond the Visible Spectrum 2278\nSpatiotemporal Learning of Directional Uncertainty in Urban Environments with Kernel Recurrent Mixture Density Networks 2197\nSpatiotemporal Representation of Dynamic Scenes 1228\nSpecification-Based Maneuvering of Quadcopters through Hoops 1297\nSpecifying and Synthesizing Human-Robot Handovers 1397\nSpiking Neural Network on Neuromorphic Hardware for Energy-Efficient Unidimensional SLAM 1452\nSpineBot: Pneumatically Actuated Muscle 2620\nSpine-Inspired Continuum Soft Exoskeleton for Stoop Lifting Assistance 2588\nSpiral Zipper Manipulator for Aerial Grasping and Manipulation 1937\nStability and Gait Switching of Underactuated Biped Walkers 0520\nStair Environment Mapping and Walk-Able Plane Detecting Algorithm for Quadrupedal Robot's Locomotion 2712\nStarNet: Pedestrian Trajectory Prediction Using Deep Neural Network in Star Topology 0199\nState Representation Learning with Robotic Priors for Partially Observable Environments 1302\nStatic Analysis on the Modular Detachable Climbing Robot for All Wall-To-Wall Transitions 2633\nStatistical Coverage Control of Mobile Sensor Networks 2604\nStereo Visual Inertial LiDAR Simultaneous Localization and Mapping 2562\nStereo Visual-Inertial SLAM Using Graph-Based Optimization 2621\nStiffness Bounds for Resilient and Stable Physical Interaction of Articulated Soft Robots 2429\nStochastic Path Planning for Autonomous Underwater Gliders with Safety Constraints 1317\nStochastic Sampling Simulation for Pedestrian Trajectory Prediction 2419\nStructured Classification of Locomotion Modes for Wearable Robot Control 2653\nStructured Reward Shaping Using Signal Temporal Logic Specifications 1818\nStudy on Elastic Elements Allocation for Energy-Efficient Robotic Cheetah Leg 0928\nStudy on Performance of Marker Detection Via Training Data Augmentation of Partial Distortion in Underwater Sonar Image 2687\nStudy on Stumbles of the Elderly from a Depth Perception Dependency Test 1812\nSubmodular Optimization for Coupled Task Allocation and Intermittent Deployment Problems 2134\nSubspace-Based Direct Visual Servoing 2161\nSuMa++: Efficient LiDAR-Based Semantic SLAM 0228\nSVIn2: An Underwater SLAM System Using Sonar, Visual, Inertial, and Depth Sensor 0269\nSynchronizing Virtual Constraints and Preview Controller: A Walking Pattern Generator for the Humanoid Robot COMAN+ 1061\nSynergy-Based Control for Multi-Fingered Hands Using Selected Joint Spaces 2688\nSynthesizing Robot Manipulation Programs from a Single Observed Human Demonstration 0328\nSystematic Benchmarking for Reproducibility of Computer Vision Algorithms for Real-Time Systems: The Example of Optic Flow Estimation 0506\nTactile-Based Insertion for Dense Box-Packing 1881\nTactile Localization for Unknown and Known Objects 2628\nTalk to the Vehicle: Language Conditioned Autonomous Navigation of Self Driving Cars 1765\nTarget Classification and Prediction of Unguided Rocket Trajectories Using Deep Neural Networks 2723\nTarget Tracking of Moving and Rotating Object by High-Speed Monocular Active Vision 2692\nTask-Motion Planning with Reinforcement Learning for Adaptable Mobile Service Robots 1443\nTask-Oriented Grasping in Object Stacking Scenes with CRF-Based Semantic Model 0663\nTask-Specific Self-Body Controller Acquisition by Musculoskeletal Humanoids: Application to Pedal Control in Autonomous Driving 0109\nTeaching a Drone to Accompany a Person from Demonstrations Using Non-Linear ASFM 1107\nTeleoperating Robots from the International Space Station: Microgravity Effects on Performance with Force Feedback 0370\nTendencyRL: Multi-Stage Discriminative Hints for Ef\ufb01cient Goal-Oriented Reverse Curriculum Learning 0532\nTerrainFusion: Real-Time Digital Surface Model Reconstruction Based on Monocular SLAM 0786\nThe ANBOT: An Intelligent Robotic Co-Worker for Industrial Abrasive Blasting 1497\nThe ARMM System: Demonstrating Clinical Feasibility in Steering Magnetically Actuated Catheters in Endovascular Applications 2648\nThe Combination Function for Multi-Leg Modular Robot, Bio-Mimicked from Ant's Behavior 2657\nThe Compliant Joint Toolbox for MATLAB: An Introduction with Examples 2574\nThe CoSTAR Block Stacking Dataset: Learning with Workspace Constraints 1236\nThe Impact of Domain Randomization on Object Detection: Case Study on Parametric Shapes and Synthetic Textures 1333\nThe MaSTr1325 Dataset for Training Deep USV Obstacle Detection Models 1480\nTheoretical Foundation for Design of Friction-Tunable Soft Finger with Wrinkle's Morphology 2518\nThe RGB-D Triathlon: Towards Agile Visual Toolboxes for Robots 2357\nThermal-Inertial Odometry for Autonomous Flight Throughout the Night 1819\nThe Road is Enough! Extrinsic Calibration of Non-overlapping Stereo Camera and LiDAR using Road Information 2213\nThe Robot Show Must Go On: Effective Responses to Robot Failures 1280\nThe Role of Robot Payload in the Safety Map Framework 1984\nThe \"Smellicopter, \" a Bio-Hybrid Odor Localizing Nano Air Vehicle 2245\nThe Stability of Human Supervisory Control Operator Behavioral Models Using Hidden Markov Models 1015\nThree-Degrees-Of-Freedom Passive Gravity Compensation Mechanism Applicable to Robotic Arm with Remote Center of Motion for Minimally Invasive Surgery 2239\nTimed-Elastic Bands for Manipulation Motion Planning 2371\nTimed-Elastic Smooth Curve Optimization for Mobile-Base Motion Planning 1549\nTime-Delay Compensation Using Energy Tank for Satellite Dynamics Robotic Simulators 1116\nTime-Optimal Path Tracking for Jerk Controlled Robots 2375\nTime-Optimal Trajectory Generation for Dynamic Vehicles: A Bilevel Optimization Approach 1850\nTimepix Radiation Detector for Autonomous Radiation Localization and Mapping by Micro Unmanned Vehicles 2593\nTime Series Motion Generation Considering Long Short-Term Motion 1608\nTime-Varying Graph Patrolling against Attackers with Locally Limited and Imperfect Observation Models 0553\nTIMTAM: Tunnel-Image Texturally-Accorded Mosaic for Location Refinement of Underground Vehicles with a Single Camera 2561\nTIP Model: A Combination of Unstable Subsystems for Lateral Balance in Walking 1354\nTorso-Mounted Vibrotactile Interface to Experimentally Induce Illusory Own-Body Perceptions 0367\nToward a Ballbot for Physically Leading People: A Human-Centered Approach 0920\nToward a Bipedal Robot with Variable Gait Styles: Sagittal Forces Analysis in a Planar Simulation and a Prototype Ball-Tray Mechanism 1446\nToward Achieving Formal Guarantees for Human-Aware Controllers in Human-Robot Interaction 1096\nToward Affordance Detection and Ranking on Novel Objects for Real-World Robotic Manipulation 2318\nToward a Human-Machine Interface Based on Electrical Impedance Tomography for Robotic Manipulator Control 0596\nToward an Efficient Hybrid Interaction Paradigm for Object Manipulation in Optical See-Through Mixed Reality 0084\nToward a Versatile Robotic Platform for Fluoroscopy and MRI-Guided Endovascular Interventions: A Pre-Clinical Study 1894\nToward Controllable Morphogenesis in Large Robot Swarms 2193\nToward Improving Patient Safety and Surgeon Comfort in a Synergic Robot-Assisted Eye Surgery: A Comparative Study 2256\nToward Model-Based Benchmarking of Robot Components 1224\nTowards Active Stabilization of Probe-Based Confocal Laser Endomicroscopy Using a Handheld Micromanipulator 2680\nTowards a General Framework for Generating Stable and Flexible Locomotion Skills 2736\nTowards a Generic in Vivo in Situ Camera Lens Cleaning Module for Laparoscopic Surgery 0073\nTowards an Assisted Robotic Platform for Soft Neural Tissue Interaction 2770\nTowards a Natural Motion Generator: A Pipeline to Control a Humanoid Based on Motion Data 1573\nTowards an Autonomous Unwrapping System for Intralogistics 2444\nTowards an Open-Source Micro Robot Oceanarium: A Low-Cost, Modular, and Mobile Underwater Motion-Capture System 1925\nTowards a Robot Architecture for Situated Lifelong Object Learning 1429\nTowards a Robust Aerial Cinematography Platform: Localizing and Tracking Moving Targets in Unstructured Environments 1607\nTowards Autonomous Industrial-Scale Bathymetric Surveying 1353\nTowards Ergonomic Control of Collaborative Effort in Multi-Human Mobile-Robot Teams 0899\nTowards Explainable Shared Control Using Augmented Reality 0448\nTowards Generalizing Sensorimotor Control Across Weather Conditions 0954\nTowards Jumping Locomotion for Quadruped Robots on the Moon 0801\nTowards Learning Trajectory Segmentation through Semi-Supervised Learning 2767\nTowards More Realistic Human-Robot Conversation: A Seq2Seq-Based Body Gesture Interaction System 1420\nTowards Reversible Dynamic Movement Primitives 0753\nTowards the Design and Development of a Pediatric Neuroendoscope Tool 1125\nT-PFC: A Trajectory-Optimized Perturbation Feedback Control Approach 2388\nTracking Control of Fully-Constrained Cable-Driven Parallel Robots Using Adaptive Dynamic Programming 0777\nTraining in Task Space to Speed Up and Guide Reinforcement Learning 0186\nTrajectory Estimation for Geo-Fencing Applications on Small-Size Fixed-Wing UAVs 1209\nTrajectory Optimization for Legged Robots with Slipping Motions 2145\nTrajectory Optimization for Unknown Constrained Systems Using Reinforcement Learning 1077\nTrajectory Planning for a Bat-Like Flapping Wing Robot 0863\nTransferable Trial-Minimizing Progressive Peg-In-Hole Model 0232\nTransfer learning for vision-based tactile sensing 0248\nTrust but Verify: A Distributed Algorithm for Multi-Robot Wireframe Exploration and Mapping 0720\nTunable Contact Conditions and Grasp Hydrodynamics Using Gentle Fingertip Suction 2575\nTwin Kinematics Approach for Robotic-Assisted Tele-Echography 1979\nTwo-View Fusion Based Convolutional Neural Network for Urban Road Detection 0198\nTZC: Efficient Inter-Process Communication for Robotics Middleware with Partial Serialization 0386\nUAV Landing at an Unknown Location Marked by a Radio Beacon 1512\nUncertainty-Aware Imitation Learning Using Kernelized Movement Primitives 0255\nUnderactuated Gripper with Forearm Roll Estimation for Human Limbs Manipulation in Rescue Robotics 1320\nUnderstanding Multi-Robot Systems: On the Concept of Legibility 0435\nUnderstanding Natural Language Instructions for Fetching Daily Objects Using GAN-Based Multimodal Target-Source Classification 2288\nUnified Balance Control for Biped Robots Including Modification of Footsteps with Angular Momentum and Falling Detection Based on Capturability 0126\nUnified Human-Robot Shared Control with Application to Haptic Telemanipulation 1748\nUnstructured Terrain Navigation and Topographic Mapping with a Low-Cost Mobile Cuboid Robot 0164\nUnsupervised Task Segmentation Approach for Bimanual Surgical Tasks Using Spatiotemporal and Variance Properties 1244\nUnsupervised Traffic Accident Detection in First-Person Videos 1278\nUntethered Quadrupedal Hopping on a Trampoline 2677\nUpper-Limb Joint Angle Estimation Method with Commercial Depth Sensor for Planar Robot-Aided Reaching Movement 2766\nUpper Limb Motion Simulation Algorithm for Prosthesis Prescription and Training 1052\nUrban Street Trajectory Prediction with Multi-Class LSTM Networks 2691\nUse of Deep Learning Based on Recurrent Neural Network for Modeling of Characteristics of a Pneumatic Artificial Muscle 2617\nVariable Configuration Planner for Legged-Rolling Obstacle Negotiation Locomotion: Application on the CENTAURO Robot 0134\nVariable Impedance in End-Effector Space: An Action Space for Reinforcement Learning in Contact-Rich Tasks 1610\nVarious Sized Obstacle and Stair Climbing Robot by Wheel Transformation: Prototype and Experimental Results 2644\nVehicular Multi-Camera Sensor System for Automated Visual Inspection of Electric Power Distribution Equipment 1736\nView Management for Lifelong Visual Maps 0649\nView Sharing to Enhance Driving Safety through Vehicle-To-Vehicle Communication 2667\nViLiVO: Virtual LiDAR-Visual Odometry for an Autonomous Vehicle with a Multi-Camera System 2025\nVirtual Lane Boundary Generation for Human-Compatible Autonomous Driving: A Tight Coupling between Perception and Planning 1364\nVirtual Maps for Autonomous Exploration with Pose SLAM 1215\nVirtual-Mass-Ellipsoid Inverted Pendulum Model and Its Applications to 3D Bipedal Locomotion on Uneven Terrains 0739\nVirtual Region Based Multi-Robot Path Planning in an Unknown Occluded Environment 2127\nVision-Aided Localization for Ground Robots 0314\nVision-Based Automatic Control of a 5-Fingered Simulated Assistive Robotic Manipulator for Activities of Daily Living 0805\nVision-Based Estimation of Driving Energy for Planetary Rovers Using Deep Learning and Terramechanics 2329\nVision-Based Magnetic Platform for Actuator Positioning and Wireless Control of Microrobots 0479\nVision-Based Virtual Fixtures Generation for Robotic-Assisted Polyp Dissection Procedures 1167\nVisual-Based Autonomous Driving Deployment from a Stochastic and Uncertainty-Aware Perspective 0055\nVisual Domain Adaptation Exploiting Confidence-Samples 0222\nVisual-Inertial Localization with Prior LiDAR Map Constraints 2271\nVisual-Inertial Odometry Tightly Coupled with Wheel Encoder Adopting Robust Initialization and Online Extrinsic Calibration 0614\nVisual-Inertial Odometry with Point and Line Features 1809\nVisual-Inertial On-Board Throw-And-Go Initialization for Micro Air Vehicles 1957\nVisual Servo Control of a Novel Magnetic Actuated Endoscope for Uniportal Video Assisted Thoracic Surgery 2228\nVisual Servoing of Miniature Magnetic Film Swimming Robots for 3D Arbitrary Path Following 2521\nVoice-Controlled Flexible Exotendon (FLEXotendon) Glove for Hand Rehabilitation 1130\nVolumetric Instance-Aware Semantic Mapping and 3D Object Discovery 2118\nVolumetric Tree*: Adaptive Sparse Graph for Effective Exploration of Homotopy Classes 1616\nWalking with Augmented Reality Real-Time Visual Feedback Wearing a Cable-Driven Active Leg Exoskeleton (C-ALEX) 2485\nWalking with Confidence: Safety Regulation for Full Order Biped Models 2526\nWall-Mounted Robot Arm Equipped with 3-DOF Roll-Pitch-Pitch Counterbalance Mechanism 2411\nWarped Hypertime Representations for Long-Term Autonomy of Mobile Robots 2131\nWearable Activity Recognition for Robust Human-Robot Teaming in Safety-Critical Environments Via Hybrid Neural Networks 1950\nWhole-Body Control of Humanoid Robot in 3D Multi-Contact under Contact Wrench Constraints Including Joint Load Reduction with Self-Collision and Internal Wrench Distribution 0670\nWhole-Body Control with (Self) Collision Avoidance Using Vector Field Inequalities 2432\nWhole-Body Locomotion and Posture Control on a Torque-Controlled Hydraulic Rover 2175\nWhole-Body Motion and Landing Force Control for Quadrupedal Stair Climbing 1470\nWhole-Body Motion Planning for Walking Excavators 0991\nWhole-Body MPC for a Dynamically Stable Mobile Manipulator 2422\nWhole-Body Postural Control Approach Based on Multiple ZMP Evaluation in Humanoid Robots 2638\nWide Aperture Imaging Sonar Reconstruction Using Generative Models 0138\nWith Proximity Servoing towards Safe Human-Robot-Interaction 1369\nWLR-II, a Hose-Less Hydraulic Wheel-Legged Robot 0226\nWord2vec to Behavior: Morphology Facilitates the Grounding of Language in Machines 1517\nWSRender: A Workspace Analysis and Visualization Toolbox for Robotic System Design and Verification 2499\nYouWasps: Towards Autonomous Multi-Robot Mobile Deposition for Construction 1282", "link": "https://github.com/PaoPaoRobot/IROS2019-paper-list", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "iros2019-paper-list\nthe 2019 ieee/rsj international conference on intelligent robots and systems (iros 2019) has been held on november 4 \u2013 8, 2019 in the venetian macao, macau, china. iros is one of the largest and most impacting robotics research conferences worldwide. it brings an international community of researchers, educators and practitioners to explore the frontier of science and technology in intelligent robots and systems, and discuss the latest advancements in this fast growing and exciting field.\nthis list is edited by paopaorobot, \u6ce1\u6ce1\u673a\u5668\u4eba , the chinese academic nonprofit organization. recently we will classify these papers by topics. welcome to follow our github and our wechat public platform account ( paopaorobot_slam ). of course, you could contact with yvon shong.\ntitle index\noutlier-robust manifold pre-integration for ins/gps fusion 0688\nautonomous steering of concentric tube robots for enhanced force/velocity manipulability 0813\na 2-piece six-axis force/torque sensor capable of measuring loads applied to tools of complex shapes 1744\nautonomous hybrid ground/aerial mobility in unknown environments 2028\na hybrid active/passive wrist approach for increasing virtual fixture stiffness in comanipulated robotic minimally invasive surgery 2255\n6-axis hybrid sensing and estimation of tip forces/torques on a hyper-redundant robotic surgical instrument 2324\nhybrid force/motion control and implementation of an aerial manipulator towards sustained contact operations 2586\nxl-laser: large-scale cable-driven laser cutting/engraving robot 2742\n2d contour following with an unmanned aerial manipulator: towards tactile-based aerial navigation 1391\n2-dof transformable wheel design for various sized stair and step climbing 2643\n2-entity ransac for robust visual localization in changing environment 0982\n3d canonical pose estimation and abnormal gait recognition with a single rgb-d camera 2323\n3d deformable object manipulation using deep neural networks 2307\n3d lidar and stereo fusion using stereo matching network with conditional cost volume normalization 1026\n3d micromanipulation of particle swarm using a hexapole magnetic tweezer 1134\n3d move to see: multi-perspective visual servoing towards the next best view within unstructured and occluded environments 1719\n3-dof gravity compensation mechanism for robot waists with the variations of center of mass 0992\n3-dof manipulator design for a slender-shaped wide end-effector 2630\n3d point cloud data acquisition using a synchronized in-air imaging sonar sensor network 0969\n3d printed single incision laparoscopic manipulator system adapted to the required forces in laparoscopic surgery 0964\n3d reconstruction by single camera omnidirectional multi-stereo system 0095\n3d shape control of linear deformable objects by robot manipulator 2664\na 3d static modeling method and experimental verification of continuum robots based on pseudo-rigid body theory 1362\na behavior driven approach for sampling rare-event situations for autonomous vehicles 0265\na behavior -----> tree !!!  cognitive assistant system for emergency medical services 0449\na benchmark for visual-inertial odometry systems employing onboard illumination 1770\na bi-directional multiple timescales lstm model for grounding of actions and verbs 0344\nabsolute localization through orbital maps and surface perspective imagery: a synthetic lunar dataset and neural network approach 1712\na capability-aware role allocation approach to industrial assembly tasks 2472\naccelerated visual inertial navigation via fragmented structure updates 0363\naccelerating the construction of boundaries of feasibility in three classes of robot design problems 1434\naccurate pouring using model predictive control enabled by recurrent neural network 1981\na challenge of deformation control for cloth actuators 2740\nachievement of online agile manipulation task for aerial transformable multilink robot 1396\na compact laser-steering end-effector for transoral robotic surgery 1804\na compact soft articulated parallel wrist for grasping in narrow spaces 2170\na comparative analysis on the use of autoencoders for robot security anomaly detection 0417\na comparison of action spaces for learning manipulation tasks 1513\na comparison of visual servoing from features velocity and acceleration interaction models 0827\na convex-combinatorial model for planar caging 0896\na convolutional network for joint deraining and dehazing from a single image for autonomous driving in rain 0696\na convolutional neural network feature detection approach to autonomous quadrotor indoor navigation 1540\nacoustic length sensor for soft extensible pneumatic actuators with a frequency characteristics model 2535\naction recognition based on 3d skeleton and rgb frame fusion 1903\nactive incremental learning of a contextual skill model 0480\nactive infrared coded target design and pose estimation for multiple objects 0310\nactive inverse model learning with error and reachable set estimates 1235\nactive learning of reward dynamics from hierarchical queries 1879\nactive slam using connectivity graphs as priors 0630\nactive whisker placement and exploration for rapid object recognition 1329\nactuation and stiffening in fluid-driven soft robots using low-melting-point material 0489\nadapting weed growth predictions for mechanical weeding agbots 2616\nadaptive adversarial videos on roadside billboards: dynamically modifying trajectories of autonomous vehicles 1939\nadaptive assist-as-needed control based on actor-critic reinforcement learning 1191\nadaptive deep path: efficient coverage of a known environment under various configurations 1142\nadaptive dynamic control for magnetically actuated medical robots 2435\nadaptive leader-follower formation control and obstacle avoidance via deep reinforcement learning 1241\nadaptive loss balancing for multitask learning of object instance recognition and 3d pose estimation 0698\nadaptive navigation scheme for optimal deep-sea localization using multimodal perception cues 1565\nadaptive neural admittance control for collision avoidance in human-robot collaborative tasks 0510\nadaptive outcome selection for planning with reduced models 1181\nadaptive risk-based replanning for human-aware multi-robot task allocation with local perception 2188\nadaptive swept volumes generation for human-robot coexistence using gaussian processes 0789\nadaptive trajectory planning and optimization at limits of handling 1698\nadaptive unscented kalman filter-based disturbance rejection with application to high precision hydraulic robotic control 0817\nadaptive vision-based control for rope-climbing robot manipulator 0712\na data-driven framework for learning dexterous manipulation of unknown objects 1014\na deep learning approach for multi-view engagement estimation of children in a child-robot joint attention task 0522\na deep learning approach for probabilistic security in multi-robot teams 2576\na deep learning approach for robust corridor following 1158\na density map estimation model with dropblock regularization for clustered-fruit counting 1774\na development of inertial-2d lidar slam on manifolds towards agv 2764\na distributed reconfiguration planning algorithm for smores-ep, a modular robot 2565\nadjusting weight of action decision in exploration for logistics warehouse picking learning 0213\nadmittance control based on stiffness ellipse for collision force control of object manipulation 2758\nadvanced autonomy on a low-cost educational drone platform 1806\nadvection and diffusion effects towards a bio-inspired artificial pheromone system 2626\na dynamic optimization approach for sloshing free transport of liquid filled containers using an industrial robot 1405\naerial animal biometrics: individual friesian cattle recovery and visual identification via an autonomous uav with onboard deep inference 0851\naerial robot control in close proximity to ceiling: a force estimation-based nonlinear mpc 0450\naerial robots with advanced manipulation capabilities for inspection and maintenance: the aeroarms project 2610\naerodynamic model identification of a quadrotor subject to rotor failures in the high-speed flight regime 2433\na-exp4: online social policy learning for adaptive robot-pedestrian interaction 0034\na fast free-viewpoint video synthesis algorithm for sports scenes 0318\na fast heuristic path planning algorithm for mobile robots 2747\na fast online frequency adaptation mechanism for cpg-based robot motion control 2346\naffordance learning for end-to-end visuomotor robot control 1257\na flexible sensor for suture training 2545\na force-controlled robotic wrist module for the macro-micro manipulation of industrial robots 2707\na framework for depth estimation and relative localization of ground robots using computer vision 0115\na fully-integrated sensing and control system for high-accuracy mobile robotic building construction 1219\na gear-driven prosthetic hand with major grasp functions for toddlers 1869\na generative model of underwater images for active landmark detection and docking 0113\nagent prioritization for autonomous navigation 0560\nagile standing-up control of humanoids: energy-based reactive contact wrench optimization with strict dynamic consistency 1646\na gps-aided omnidirectional visual-inertial state estimator in ubiquitous environments 2174\na handheld master controller for robot-assisted microsurgery 1992\nair to ground collaboration for energy-efficient path planing for ground robots 1520\na joint optimization approach of lidar-camera fusion for accurate dense 3d reconstructions 2268\na kernelized approach for learning and adapting symmetric positive definite profiles 2684\na learning-based inverse kinematics solver for two-segment continuum robot models 2750\na linear series elastic actuator for accurate force and impedance control with high torque-to-rotor-inertia ratios 2724\naltro: a fast solver for constrained trajectory optimization 1603\na magnetically transduced whisker for angular displacement and moment sensing 2105\nambiguity poses estimation for objects with symmetry 2773\na mechanical approach to suppress the oscillation of a long continuum robot flying with water jets 2564\na method for designing low-profile compliant transmission mechanisms 2463\na method for guiding a person combining robot movement and projection 0155\na methodology for formulating and exploiting innovative technologies for collaborative robots in a manufacturing setting 2683\na mobile extendable robot arm: singularity analysis and design 1526\na model-based human activity recognition for human-robot collaboration 0569\na model for simulating the robotic pushing of dirt 0860\na multi-channel embedded dsp closed-loop control system for musical robots 0096\na multiclass eeg signal classification model using spatial feature extraction and xgboost algorithm 0926\na multi-dof human-powered robot using regenerative clutches and constant-force springs 0914\na multimodal human-robot interaction manager for assistive robots 1728\na multimodal soft crawling-climbing robot with the controllable horizontal plane to slope transition 1676\na multi-task convolutional neural network for autonomous robotic grasping in object stacking scenes 0395\na multi-trainee architecture for haptic hands-on training 0762\nan adaptive velocity obstacle avoidance algorithm for autonomous surface vehicles 0979\nanalysis and exploitation of synchronized parallel executions in behavior trees 0845\nanalysis of ground effect for small-scale uavs in forward flight 2392\nanalyzing liquid pouring sequences via audio-visual neural networks 0041\nan approach of facilitated investigation of active self-healing tension transmission system oriented for legged robots 1813\nan approximation-free simple control scheme for uncertain quadrotor systems: theory and validations 1074\nan assisted telemanipulation approach: combining autonomous grasp planning with haptic cues 0501\nan assistive low-vision platform that augments spatial cognition through proprioceptive guidance: point-to-tell-and-touch 1252\nan asynchronous multi-body simulation framework for real-time dynamics, haptics and learning with application to surgical robots 2113\nan augmented reality interface for human-robot interaction in unconstrained environments 1080\nan automated learning-based procedure for large-scale vehicle dynamics modeling on baidu apollo platform 0508\nan autonomous exploration algorithm using environment-robot interacted traversability analysis 0723\nan autonomous quadrotor system for robust high-speed flight through cluttered environments without gps 2166\nan educational robotic platform with multimodal perception for teaching sensor servoing controls 2629\nan efficient and accurate algorithm for the perspective-n-point problem 1972\nan efficient scheduling algorithm for multi-robot task allocation in assembling aircraft structures 2571\na neurologically inspired sequence processing model for mobile robot place recognition 2115\nan evaluation of robot-to-human handover configurations for commercial robots 0766\na new time-varying feedback rise control of pkms: theory and application 0088\nan experimental study of parameters influencing physical human-robot negotiation in comanipulative tracking task 0224\nangle of arrival estimation based on channel impulse response measurements 2093\nan in-pipe inspection module with an omnidirectional bent-pipe self-adaptation mechanism using a joint torque control 0551\nan integrated delta manipulator for aerial repair: a new aerial robotic system 2608\nan interactive indoor drone assistant 1038\nan interactive method for virtual fixture generation in unstructured environments 2704\nan interactive physically-based model for active suction phenomenon simulation 1385\nan intuitive, affordances oriented telemanipulation framework for a dual robot arm hand system: on the execution of bimanual tasks 1177\nankle torque during mid-stance does not lower energy requirements of steady gaits 1265\nan object attribute guided framework for robot learning manipulations from human demonstration videos 0936\nanonymous hedonic game for task allocation in a large-scale multiple agent system 2598\nan open-source 7-axis, robotic platform to enable dexterous procedures within ct scanners 1428\nan optimal algorithm to solve the combined task allocation and path finding problem 0471\nan optimization framework for simulation and kinematic control of constrained collaborative mobile agents (ccma) system 2393\na novel 4-dof robotic link mechanism with e-cosmo : kinematics based torque analysis 0775\na novel approach for outlier detection and robust sensory data model learning 0340\na novel capabilities of quadruped robot moving through vertical ladder without handrail support 0329\na novel four-degree-of-freedom versus a conventional foot interface for controlling a robotic assistive arm in surgery 2658\na novel rescue system using multi-agent slam framework 2759\na novel robust approach for correspondence-free extrinsic calibration 1541\na novel semi-autonomous control framework for retina confocal endomicroscopy scanning 0483\na novel small-scale turtle-inspired amphibious spherical robot 1467\nanyexo: a versatile and dynamic upper-limb rehabilitation robot 2123\na parallel gripper with a universal fingertip device using optical sensing and jamming transition for maintaining stable grasps 1496\na passive closing, tendon driven, adaptive robot hand for ultra-fast, aerial grasping and perching 0015\na penetration metric for deforming tetrahedra using object norm 0064\napplication of digging control based on the center-of-mass velocity of the attachment of a hydraulic excavator 0719\napplying the interaction of walking-emotion to an assistive device for rehabilitation and exercise 0238\napproximating cfree space topology by constructing vietoris-rips complex 1486\na pressure field model for fast, robust approximation of net contact force and moment between nominally rigid objects 1597\na probabilistic approach to human-robot communication 1359\na real-time dynamic simulator and an associated front-end representation format for simulating complex robots and environments 1766\na real-time v2x enabled dynamic path planning system for autonomous vehicles in road blockage test scenarios 2715\na reliable gravity compensation control strategy for dvrk robotic arms with nonlinear disturbance forces 2340\nare you hearing or listening? the effect of task performance in verbal behavior with smart speaker 0332\nare you with me? determining the associationof individuals and the collective social space 0902\narguing security of autonomous robots 0158\na ring network protocol for articulated robots 1955\narmcl: arm contact point localization via monte carlo localization 0132\na robotic surgery approach to mitochondrial transfer amongst single cells 0604\na robust biped locomotion based on linear-quadratic-gaussian controller and divergent component of motion 0243\na robust extrinsic calibration framework for vehicles with unscaled sensors 1578\na robust laser-inertial odometry and mapping method for large-scale highway environments 1670\na robustness analysis of inverse optimal control of bipedal walking 2583\na robust position and posture measurement system using visual markers and an inertia measurement unit 0584\na robust stereo semi-direct slam system based on hybrid pyramid 1883\narticulated multi-perspective cameras and their application to truck motion estimation 1440\nartificial intelligent navigation technology for a robotic vacuum cleaner in an indoor environment 2749\nartificial lateral line based longitudinal separation sensing for two swimming robotic fish with leader-follower formation 0629\na rugd dataset for autonomous navigation and visual perception in unstructured outdoor environments 1126\na simple approach on global control of a class of underactuated mechanical robotic systems 1681\na soft exoglove equipped with a wearable muscle-machine interface based on forcemyography and electromyography 2234\na spring-aided two-dimensional electromechanical spine architecture for bio-inspired robots 2000\na stabilization analysis of omni-mobile manipulator with 4k camera 2650\na study of a class of vibration-driven robots: modeling, analysis, control and design of the brushbot 1866\na study on the electric wheelchair-humanoid collaboration for clothing assistance of the elderly 2769\na sweeping and grinding methods combined hybrid sampler for asteroid exploration 1759\nasynchronous behavior trees with memory aimed at aerial vehicles with redundancy in flight controller 1485\na systematic comparison of affective robot expression modalities 0607\na tactile stimulation system for robot-assisted hand rehabilitation 2751\na taxonomy for characterizing modes of interactions in goal-driven, human-robot teams 1548\na teleoperated hexapod robot for imitation learning task training 0454\na teleoperation interface for loco-manipulation control of mobile collaborative robotic assistant (moca) 2469\na testbed for haptic and magnetic resonance imaging-guided percutaneous needle biopsy 2479\natomic force microscope tip localization and tracking through deep learning based vision inside an electron microscope 1025\nattention-based hierarchical deep reinforcement learning for lane change behaviors in autonomous driving 0397\nattitude- and cruise control of a vtol tiltwing uav 2142\na two-dof bipedal robot utilizing the reuleaux triangle drive mechanism 0464\naudio-visual sensing from a quadcopter: dataset and baselines for source localization and sound enhancement 0889\naugmented reality controlled smart wheelchair using dynamic signifiers for affordance representation 0901\naugmenting knowledge through statistical, goal-oriented human-robot dialog 1182\na unified active assistance control framework of hip exoskeleton for walking and balance assistance 1816\na unified formulation for visual odometry 0165\nautomated boxwood topiary trimming with a robotic arm and integrated stereo vision 1053\nautomated macro-micro manipulation for robotic microinjection with computer vision 0609\nautomated single-particle micropatterning system using dielectrophoresis 2755\nautomated sorting of rare cells based on autofocusing visual feedback in fluorescence microscopy 0599\nautomatic annotation for semantic segmentation in indoor scenes 0469\nautomatic calibration of multiple 3d lidars in urban environments 0822\nautomatic cell assembly by two-fingered microhand 0726\nautomatic coverage selection for surface-based visual localization 2201\nautomatic multi-sensor extrinsic calibration for mobile robots 2110\nautomatic spatial template generation for realistic 3d modeling of large-scale indoor spaces 1666\nautonomous 3d reconstruction, mapping and exploration of indoor environments with a robotic arm 2383\nautonomous detection of pv panels using unmanned aerial vehicles 2729\nautonomous free-form trenching using a walking excavator 2171\nautonomous human-aware navigation in dense crowds 2673\nautonomous landing on pipes using soft gripper for inspection and maintenance in outdoor environments 1291\nautonomous mobile manipulation framework for intelligent home service robots 2694\nautonomous photogrammetry process for managing stockpile inventory with unmanned aerial vehicle 2780\nautonomous safe locomotion system for bipedal robot applying vision and sole reaction force to footstep planning 0121\nautonomous search for sources of gamma radiation 2737\na variable stiffness elbow joint for upper limb prosthesis 2266\na video data-driven approach for the development of active guidance in robot-assisted minimally invasive surgical training 2791\na virtual reality interface for an autonomous spray painting uav 2494\navoiding obstacles during push recovery using real-time vision feedback 0628\na vr system for immersive teleoperation and live exploration with a mobile robot 1046\nbag of semantic visual words 2762\nbasic performance of planar omnidirectional crawler during direction switching using disturbance degree of ground evaluation method 2461\nbayesian gaussian mixture model for robotic policy imitation 2512\nbayesian optimization for policy search in high-dimensional systems via automatic domain selection 0091\nbebot: bernstein polynomial toolkit for trajectory generation 1490\nbee+: a 95-mg four-winged insect-scale flying robot driven by twinned unimorph actuators 2421\nbehavior change based on stiffness for haptic interface 2679\nbelief-driven control policy of a drone with microphones for multiple sound source search 1785\nbelief space metareasoning for exception recovery 0836\nbenchmarking and workload analysis of robot dynamics algorithms 1641\nbetter lost in transition than lost in space: slam state machine 1450\nbidirectional heuristic search for motion planning with an extend operator 1851\nbi-modal hemispherical sensor: a unifying solution for three axis force and contact angle measurement 1523\nbiomimetic wrinkled mxene pressure sensors towards collision-aware robots 2781\nbiped robot pelvis kinematics estimation based on the touch-point updating method 0364\nblack block recorder: immutable black box logging for robots via blockchain 2303\nboosting slam: combining slam methodologies for robust localization 2746\nboundary effect-aware visual tracking for uav with online enhanced background learning and multi-frame consensus verification 0650\nbounded-error lqr-trees 1519\nbp neural network based on-board training for real-time locomotion mode recognition in robotic transtibial prostheses 0393\nbtel: a binary tree encoding approach for visual localization 2502\nbuckling-induced shape morphing using dielectric elastomer actuators patterned with spatially-varying electrodes 1849\ncable-driven 4-dof upper limb rehabilitation robot 1599\ncalc2.0: combining appearance, semantic and geometric information for robust and efficient visual loop closure 2006\ncamera exposure control for robust robot vision with noise-aware image quality assessment 2210\ncamera pose estimation based on pnl with a known vertical direction 2378\ncamera pose estimation with semantic 3d model 0984\ncamera zoom control of integrated control platform for advancement of performance shooting system 2651\ncan a robot become a movie director? learning artistic principles for aerial cinematography 1472\ncan a robot hear the shape and dimensions of a room? 1987\ncan a social robot encourage children's self-study? 0656\ncannot avoid penalty for fluctuating order arrival rate? let's minimize 0071\ncan user-centered reinforcement learning allow a robot to attract passersby without causing discomfort? 0691\ncapillary force gripper for complex shaped micro objects with fast droplet forming by on-off control of a piston slider 2169\ncarpie: a soft, mechanically-reconfigurable worm robot 0195\ncascaded gaussian processes for data-efficient robot dynamics learning 1399\ncentralized control architecture for cooperative object transportation using multiple omnidirectional agvs 0646\nchance-constrained trajectory optimization for non-linear systems with unknown stochastic dynamics 1216\ncharacterizing environmental interactions for soft growing robots 0632\ncharacterizing nanoparticle swarms with tuneable concentrations for enhanced imaging contrast 2203\ncheating with robots: how at ease do they make us feel? 0757\nclock-torqued rolling slip model and its application to variable-speed running in a hexapod robot 2592\nclone swarms: learning to predict and control multi-robot systems by imitation 1337\nclosed-form equations and experimental verification for soft robot arm based on cosserat theory 0854\nclosed-loop force control of a pneumatic gripper actuated by two pressure regulators 1068\ncobra: collaborative bot with multi-rotor actuation 0533\ncognitive robotic architecture for semi-autonomous execution of manipulation tasks in a surgical environment 1349\ncollaborative human augmented slam 0359\ncollaborative mapping with pose uncertainties using different radio frequencies and communication modules 0042\ncollaborative needle insertion with active tissue deformation control 2038\ncollaborative robot assistant for the ergonomic manipulation of cumbersome objects 0169\ncollision detection and isolation on a robot using joint torque sensing 1378\ncombined optimization of gripper finger design and pose estimation processes for advanced industrial assembly 0870\ncombined task and action learning from human demonstrations for mobile manipulation applications 0406\ncombining spiking motor primitives with a behavior-based architecture to model locomotion for six-legged robots 0494\ncombining stochastic optimization and frontiers for aerial multi-robot exploration of 3d terrains 1102\ncommercialization of robot navigation technology for a guidance service in a large and highly-crowded airport 2748\ncommon dimensional autoencoder for learning redundant muscle-posture mappings of complex musculoskeletal robots 1375\ncommunication constrained cloud-based long-term visual localization in real time 1016\ncompact reachability map for excavator motion planning 1922\ncomparing swimming performances of flexible and helical magnetic swimmers 0058\ncomparison of deep reinforcement learning policies to formal methods for moving obstacle avoidance 1823\ncomplexity conditioned goals for reinforcement learning agents 2786\ncomplex stiffness model of physical human-robot interaction: implications for control of performance augmentation exoskeletons 1342\ncomponent modularized design of musculoskeletal humanoid platform musashi to investigate learning control systems 0124\ncomputational design of statically balanced planar spring mechanisms 2462\ncomputationally efficient mpc for cable-driven robot 2718\ncomputing 3d from-region visibility using visibility integrity 2220\ncomputing a minimal set of t-spanning motion primitives for lattice planners 1507\nconcept and validation of a large-scale human-machine safety system based on real-time uwb indoor localization 0785\nconditional generative neural system for probabilistic trajectory prediction 0952\nconfiguration modeling of a soft robotic element with selectable bending axes 1287\nconfiguration transition control of a continuum surgical manipulator for improved kinematic performance 2457\nconnectivity-preserving swarm teleoperation with a tree network 0477\nconstrained heterogeneous vehicle path planning for large-area coverage 1465\nconstructing a highly interactive vehicle motion dataset 1827\ncontact-based bridge inspection multirotors: design, modelling and control considering the ceiling effect 2390\ncontact distance estimation by soft active bio-whisker sensor based on morphological computation 2612\ncontactgrasp: functional multi-finger grasp synthesis from contact 1159\ncontact-implicit trajectory optimization for dynamic object manipulation 0803\ncontact skill imitation learning for robot-independent assembly programming 0488\ncontamination detection and classification for an automated fa\u00e3\u00a7ade cleaning operation 2631\ncontext and intention aware planning for urban driving 0640\ncontext-dependent search for generating paths for redundant manipulators in cluttered environments 1424\ncontinuous close-range 3d object pose estimation 1380\ncontinuous collision detection for a robotic arm mounted on a cable-driven parallel robot 1054\ncontinuous mechanical indexing of single cell spheroids using a robot integrated microfluidic chip 2204\ncontinuous modeling of affordances in a symbolic knowledge base 1245\ncontinuous neural control based on integration of bci and adaptive controller for steering a vehicle 2721\ncontinuous relaxation of symbolic planner for one-shot imitation learning 0565\ncontinuous-time collision avoidance for trajectory optimization in dynamic environments 1904\ncontour based reconstruction of underwater structures using sonar, visual, inertial, and depth sensor 1537\ncontrol and perception framework for deep sea mining exploration 0999\ncontrol of nonprehensile planar rolling manipulation: a passivity-based approach 2607\nconvolutional autoencoder for feature extraction in tactile sensing 2519\ncooperative audio-visual system for localizing micro aerial robots 1366\ncooperative decentralised circumnavigation with application to algalbloom tracking 0864\ncooperative range-only slam based on sum of gaussian filter in dynamic environments 0842\ncooperative schedule-driven intersection control with connected and autonomous vehicles 1677\nco-simulation of mechanical systems with hydraulic actuators 2743\ncoupling disturbance compensated mimo control of parallel ankle rehabilitation robot actuated by pneumatic muscles 0745\ncovariance pre-integration for delayed measurements in multi-sensor fusion 1557\ncoverage path planning using path primitive sampling and primitive coverage graph for visual inspection 0337\ncoverage sampling planner for uav-enabled environmental exploration and field mapping 1408\ncriteria for maintaining desired contacts for quasi-static systems 1558\ncrowd-sourced semantic edge mapping for autonomous vehicles 1701\ncubeslam: monocular 3-d object slam 2611\ncuriosity driven exploration for classification in the dark using tactile sensing 2761\ncurved-voxel clustering for accurate segmentation of 3d lidar point clouds with real-time performance 1780\ndata association aware semantic mapping and localization via a viewpoint-dependent classifier model 0627\ndata-based modeling of contact state in robotic assembly 2698\ndata-driven model predictive control for trajectory tracking with a robotic arm 2403\ndata flow orb-slam for real-time performance on embedded gpu boards 0413\ndecentralized control for 3d m-blocks for path following, line formation, and light gradient aggregation 1308\ndecentralized pose control of modular reconfigurable robots operating in liquid environments 1996\ndecentralized visual-inertial localization and mapping on mobile devices for augmented reality 0461\ndecoding the perceived difficulty of communicated contents by older people: toward conversational robot-assistive elderly care 2507\ndeduce: diverse scene detection methods in unseen challenging environments 0590\ndeepcontrol: energy-efficient control of a quadrotor using a deep neural network 1839\ndeep dive into faces: pose & illumination invariant multi-face emotion recognition system 0742\ndeep generative modeling of lidar data 1601\ndeep imitation learning for autonomous driving in generic urban scenarios with enhanced safety 1371\ndeep lagrangian networks for end-to-end learning of energy-based control for under-actuated systems 1042\ndeep learning-based mutual detection and collaborative localization for mobile robot fleets using solely 2d lidar sensors 1175\ndeep learning based robotic tool detection and articulation estimation with spatio-temporal layers 2384\ndeep learning of proprioceptive models for robotic force estimation 0372\ndeeplocnet: deep observation classification and ranging bias regression for radio positioning systems 1961\ndeep multi-task learning for anomalous driving detection using can bus scalar sensor data 1665\ndeep neural network approach in electrical impedance tomography-based real-time soft tactile sensor 0574\ndeep neural network based visual inspection with 3d metric measurement of concrete defects using wall-climbing robot 2008\ndeep orientation: fast and robust upper body orientation estimation for mobile robotic applications 1070\ndeeppco: end-to-end point cloud odometry through deep parallel neural network 1056\ndeep predictive autonomous driving using multi-agent joint trajectory prediction and traffic rules 2013\ndeep reinforcement learning for robotic pushing and picking in cluttered environment 1325\ndeep sensor fusion for real-time odometry estimation 0421\ndeep supervised hashing with similar hierarchy for place recognition 0365\ndeepvio: self-supervised deep learning of monocular visual inertial odometry using 3d geometric constraints 0978\ndeep visual mpc-policy learning for navigation 2120\ndegeneracy-aware factors with applications to underwater slam 1187\ndegeneracy in self-calibration revisited and a deep learning solution for uncalibrated slam 1031\ndelayed output feedback control for gait assistance and resistance using a robotic exoskeleton 2338\ndelayed output feedback control for gait assistance with a robotic hip exoskeleton 2609\ndelivering cognitive behavioral therapy using a conversational social robot 1394\ndempster shafer grid-based hybrid fusion of virtual lanes for autonomous driving 0437\ndense 3d reconstruction for visual tunnel inspection using unmanned aerial vehicle 0336\ndensepeds: pedestrian tracking in dense crowds using frvo and sparse features 1295\ndense, sonar-based reconstruction of underwater scenes 1471\ndepth-image-based textureless-object picking by dcnn and visual servoing 2619\ndesign a dexterous hand for the logistic robot in bin picking 2730\ndesign and analysis of a new 3-dof active-type constant-force compliant parallel stage 0545\ndesign and analysis of the all-in-one actuation module with multi-sensors 2670\ndesign and characterization of a fully autonomous under-actuated soft batoid-like robot 1374\ndesign and comparative analysis of 1d hopping robots 1831\ndesign and control of aerial modules for inflight self-disassembly 2389\ndesign and control of a high-torque and highly-backdrivable hybrid soft exoskeleton for knee injury prevention during squatting 2493\ndesign and control of a multifunctional ankle exoskeleton powered by magnetorheological actuators to assist walking, jumping and landing 2227\ndesign and development of compactly folding parallel open-close gripper with wide stroke 1926\ndesign and experiment of dragonfly inspired flexible blade to improve safety of drones 2331\ndesign and implementation of a contact aerial manipulator system for glass-wall inspection tasks 0740\ndesign and take-off flight of a samara-inspired revolving-wing robot 0797\ndesign and verification of a gravity compensated tool handler for supporting an automatic hair implanting device 2367\ndesign and verification of a portable master manipulator based on an effective workspace analysis framework 2096\ndesign, characterization, and mechanical programming of fabric-reinforced textile actuators for a soft robotic hand 1585\ndesign, fabrication, and characterization of an untethered amphibious sea urchin-inspired robot 2276\ndesign for cobot-assisted manufacturing and assembly (dfcoma) 2738\ndesigning a mechanical tool for robots with 2-finger parallel grippers 2293\ndesign, modeling and control of fully actuated 2d transformable aerial robot with 1 dof thrust vectorable link module 1461\ndesign, modeling and testing of a flagellum-inspired soft underwater propeller exploiting passive elasticity 0641\ndesign, modelling and adaptive control of a novel autonomous underwater vehicle equipped with vectored thrusters 2660\ndesign, modelling and control of a novel agricultural robot with iinterlock drive system 2195\ndesign of a 3-dof linkage-driven underactuated finger for multiple grasping 1645\ndesign of a ballistically-launched foldable multirotor 0511\ndesign of a bipedal hopping robot with morphable inertial tail for agile locomotion 2717\ndesign of a compact sma-actuated mri-compatible steerable neurosurgical robot 2768\ndesign of a fail-safe wearable robot with novel extendable arms for ergonomic accommodation during floor work 1696\ndesign of a growing robot inspired by plant growth 1653\ndesign of a mobile robot for the treatment, reuse and removal of manure with monitoring of environmental variables for poultry farms 2783\ndesign of a modular continuum-articulated laparoscopic robotic tool with decoupled kinematics 2231\ndesign of an adhesion-aware fa\u00e3\u00a7ade cleaning robot 0151\ndesign of a novel gripper system with 3d and inkjet-printed multimodal sensors for automated grasping of a forestry robot 2306\ndesign of a novel leg for a small tree climbing robot driven by shape memory alloy 2709\ndesign of a semi-humanoid telepresence robot for plant disaster response and prevention 1936\ndesign of a variable counterbalance mechanism to minimize required torque of robot arm 2622\ndesign of compact variable gravity compensator (cvgc) based on cam and variable pivot of a lever mechanism 0541\ndesign of robot leg with variable reduction ratio crossed four-bar linkage mechanism 1784\ndesign of soft flexible wire-driven finger mechanism for contact pressure distribution 0909\ndesign of wearable robot focused on contact state with wearer 2756\ndesk: a robotic activity dataset for dexterous surgical skills transfer to medical robots 1757\ndetecting layered structures of partially occluded objects for bin picking 1934\ndevelopment of 3dof manipulators with cable-hydraulic driven actuation modules for large workspace and high payload-to-weight 2771\ndevelopment of a continuous vertical-pulling automatic doffing robot for the ring spinning 1921\ndevelopment of adjustable knee assist device for wearable robot based on linkage and rolling joint 0387\ndevelopment of a location finding system for minute sound source by using human acoustic system with stochastic resonance 1857\ndevelopment of an adaptive hexapod robot based on follow-the-contact-point gait control and timekeeper control 0559\ndevelopment of an arm curl machine with variable resistance using pneumatic artificial rubber muscle 1928\ndevelopment of an autonomous sanding robot with structured-light technology 0660\ndevelopment of a navigation algorithm for optimal path planning for autonomous electric vehicles 1110\ndevelopment of a steel bridge climbing robot 1315\ndevelopment of flexible dual-type proximity sensor with resonant frequency for robotic applications 0821\ndevelopment of immersive vr interface of finger motion without restriction of real environment 2676\ndevelopment of joint module with two-speed gear transmission and joint lock mechanism during driving for task adaptable robot 0296\ndevelopment of load weight and height classifier in lifting-up task using body motion metrics 2555\ndevelopment of micro ultrasonic actuator and micro rotor blade for micro aerial vehicle 0623\ndevelopment of novel bevel-geared 5mm articulating wrist for micro-laparoscopy instrument 2238\ndid you miss the sign? a false negative alarm system for traffic sign detectors 1156\ndirectional tsdf: modeling surface orientation for coherent meshes 1151\ndisaster response robot's autonomous manipulation of valves in disaster sites based on visual analyses of rgbd images 0514\ndisc: a large-scale virtual dataset for simulating disaster scenarios 0018\ndiscoman: dataset of indoor scenes for odometry, mapping and navigation 1019\ndiscrete n-dimensional entropy of behavior: dndeb 1377\ndisr: deep infrared spectral restoration algorithm for robot sensing and intelligent visual tracking systems 0403\ndistance-based cooperative relative localization for leader-following control of mavs 2397\ndistributed dynamic sensor assignment of multiple mobile targets 0716\ndisturbance estimation and rejection for high-precision multirotor position control 2361\ndld: a deep learning based line descriptor for line feature matching 0049\ndo intermediate gaits matter when rapidly accelerating? 2284\ndomain-independent unsupervised detection of grasp regions to grasp novel objects 1689\ndot-to-dot: explainable hierarchical reinforcement learning for robotic manipulation 0887\ndouble refinement network for efficient monocular depth estimation 1041\ndriving with style: inverse reinforcement learning in general-purpose planning for automated driving 1356\ndual-arm assembly planning considering gravitational constraints 1859\nduckiepond: an open education and research environment for a fleet of autonomous maritime vehicles 2301\ndynamic control for soft robots with internal constraints in the presence of obstacles 1384\ndynamic density topological structure generation for real-time ladder affordance detection 0831\ndynamic flex-and-flip manipulation of deformable linear objects 1515\ndynamic identification of the franka emika panda robot with retrieval of feasible parameters using penalty-based optimization 2443\ndynamic input for deep reinforcement learning in autonomous driving 1229\ndynamic locomotion on slippery ground 2528\ndynamic spatiotemporal pattern identification and analysis using a fingertip-based electro-tactile display array 1716\ndynamic task control method of a flexible manipulator using a deep recurrent neural network 0566\ndynamic whole-body control of unstable wheeled humanoid robots 2252\nearly fusion for goal directed robotic vision 1150\nedge-preserving camera trajectories for improved optical character recognition on static scenes with text 2280\neffective estimation of contact force and torque for vision-based tactile sensors with helmholtz-hodge decomposition 2464\neffect of arm swinging and trunk twisting on bipedal locomotion 0557\neffect of planning period on mpc-based navigation for a biped robot in a crowd 0382\neffect of vibration on twisted string actuation through conduit at high bending angles 1529\neffects of a bio-mimicked flapping path on propulsion efficiency of two-segmental fish robots 1034\neffects of a person-following light-touch device during overground walking with visual perturbations in a virtual reality environment 2498\neffects of limb morphology on transient locomotion in quadruped robots 2336\nefficient and accurate operational space control with dual-haptic interface for large workspace teleoperation 2708\nefficient and guaranteed planar pose graph optimization using the complex number representation 0987\nefficient autonomous robotic exploration with semantic road map in indoor environments 2225\nefficient environment guided approach for exploration of complex environments 1085\nefficient grasp planning and execution with multi-fingered hands by surface fitting 2424\nefficient quadrupedal walking via decentralized coordination mechanism between limbs and neck 0606\neigen-factors: plane estimation for multi-frame and time-continuous point cloud alignment 1499\nelectrical bio-impedance proximity sensing for vitreo-retinal micro-surgery 2320\nelevatenet: a convolutional neural network for estimating the missing dimension in 2d underwater sonar images 1249\nempirical characterization of a high-performance exterior-rotor type brushless dc motor and drive 2477\nemploying magnets to improve the force exertion capabilities of adaptive robot hands in precision grasps 0290\nemploying whole-body control in assistive robotics 1269\nempowered optical inspection by using robotic manipulator in industrial applications 1004\nenabling human-like task identification from natural conversation 0079\nendoscopic bi-manual robotic instrument design using a genetic algorithm 0816\nend-to-end driving model for steering control of autonomous vehicles with future spatiotemporal features 1524\nend-to-end sensorimotor control problems of auvs with deep reinforcement learning 0245\nenergy-based adaptive control and learning for patient-aware rehabilitation 1918\nenergy-based hybrid control of a ball-dribbling robot 2739\nenergy-efficient locomotion strategies and performance benchmarks using point mass tensegrity dynamics 1919\nenergy harvesting across temporal temperature gradients using vaporization 0714\nensembledagger: a bayesian approach to safe imitation learning 0294\nenthusiastic robots make better contact 0924\nentropic risk measure in policy search 1298\nenvironmental sound segmentation utilizing mask u-net 1814\nepisodic learning with control lyapunov functions for uncertain robotic systems 1943\nepn: edge-aware pointnet for object recognition from multi-view 2.5d point clouds 1436\nergodic flocking 1807\nescaping local minima in search-based planning using soft duplicate detection 1931\nesko6d - a binocular and rgb-d dataset of stored kitchen objects with 6d poses 1055\nestablishing safer human-vehicle visual interaction at night 2659\nestimating metric scale visual odometry from videos using 3d convolutional networks 1923\nestimating the center of mass and the angular momentum derivative for legged locomotion ' a recursive approach 2585\nevaluating the acceptability of assistive robots for early detection of mild cognitive impairment 0468\nevaluation of a large-scale event-driven robot skin 2181\nevaluation of hopping robot performance with novel foot pad design on natural terrain for hopper development 2326\nevaluation system for hydraulic excavator operation skill using remote controlled excavator and virtual reality 2465\nev-imo: motion segmentation dataset and learning pipeline for event cameras 1351\nexecuting underspecified actions in real world based on online projection 1112\nexo wrist: a soft tendon driven wrist wearable robot with active anchor for dart throwing motion in hemiplegic patients 2482\nexperience reuse with probabilistic movement primitives 0485\nexperimental analysis of the influence of olfactory property on chemical plume tracing performance 2126\nexperimental comparison of open source visual-inertial-based state estimation algorithms in the underwater domain 0500\nexperimental study on microfluidic mixing with trapezoidal obstacles in a 1000-fold span of reynolds number 0099\nexperimental study on the parameters of high-pressure water-jet cleaning on a facade 2637\nexperimental validation of hydraulic interlocking drive system for biped humanoid robot 2460\nexplainable one-shot meta-learning to imitate motion segments of unseen human-robot interactions 2765\nexploiting linearity in dynamics solvers for the design of composable robotic manipulation architectures 1717\nexploiting sparse semantic hd maps for self-driving vehicle localization 0272\nexplore, approach, and terminate: evaluating subtasks in active visual object search based on deep reinforcement learning 1123\nexploring logical consistency and viewport sensitivity in compositional vqa models 1250\nexploring low-level and high-level transfer learning for multi-task facial recognition with a semi-supervised neural network 0765\nextending monocular visual odometry to stereo camera systems by scale optimization 1173\nexternal force estimation of human-cooperative robot during object manipulation using recurrent neural network 2614\nextrinsic calibration of thermal ir camera and mmwave radar by exploiting depth from rgb-d camera 2697\nfa-harris: a fast and asynchronous corner detector for event cameras 1843\nfast adaptation with meta-reinforcement learning for trust modelling in human-robot interaction 1576\nfast and incremental loop closure detection using proximity graphs 0330\nfast and robust 3-d sound source localization with dsvd-phat 0271\nfast and safe policy adaptation via alignment-based transfer 0563\nfaster: fast and safe trajectory planner for flights in unknown environments 1577\nfast handovers with a robot character: small sensorimotor delays improve perceived qualities 1613\nfast manipulability maximization using continuous-time trajectory optimization 2317\nfast motion planning via free c-space estimation based on deep neural network 0832\nfast perception, planning, and execution for a robotic butler: wheeled humanoid m-hubo 1637\nfast run-time monitoring, replanning, and recovery for safe autonomous system operations 1274\nfast time-optimal avoidance of moving obstacles for high-speed mav flight 0030\nfast trajectory planning for multiple quadrotors using relative safe flight corridor 1460\nfault-tolerant force tracking for a multi-legged robot 2672\nfeasibility of gait entrainment to hip mechanical perturbation for locomotor rehabilitation 0462\nfeasibility of wireless power transfer for mobile robots 2655\nfeasibility test of exoskeleton ankle robot for gait training on stairs for sub-acute stroke patients 2779\nfeedback-based fabric strip folding 0932\nfeedback mpc for torque-controlled legged robots 1021\nfiber optic fabry-perot interferometry for a biopsy needle with tip force sensing 2710\nfiesta: fast incremental euclidean distance fields for online quadrotor motion planning 1758\nfilter early, match late: improving network-based visual place recognition 0618\nfirst steps towards full model based motion planning and control of quadrupeds: a hybrid zero dynamics approach 1761\nflame: feature-likelihood based mapping and localization for autonomous vehicles 0770\nflexible layouts for fiducial tags 1503\nflexible trinocular: non-rigid multi-camera-imu dense reconstruction for uav navigation and mapping 0615\nflexure mechanisms with variable stiffness and damping using layer jamming 1292\nflightgoggles: photorealistic sensor simulation for perception-driven robotics using photogrammetry and virtual reality 1834\nflight recovery of mavs with compromised imu 1352\nflower interaction subsystem for a precision pollination robot 1332\nfluid lubricated dexterous finger mechanism for human-like impact absorbing capability 2404\nflying through a narrow gap using neural network: an end-to-end planning and control approach 0784\nfollow the robot: modeling coupled human-robot dyads during navigation 0878\nfoot with a core-shell structural six-axis force sensor for pedal depressing and recovering from foot slipping during pedal pushing toward autonomous driving by humanoids 0763\nforce-and-motion constrained planning for tool use 1619\nforce field-based indirect manipulation of uav flight trajectories 1531\nforce sensitive robotic end-effector using embedded fiber optics anddeep learning characterization for dexterous remote manipulation 2179\nforecasting time-to-collision from monocular video: feasibility, dataset, and challenges 1358\nforest tree detection and segmentation using high resolution airborne lidar 0536\nformation of pvdf piezoelectric film on 3d bellows surface of robotic suction cup for providing force sensing ability -feasibility study on two methods of dip-coating and lamination 1347\nfree-space features: global localization in 2d laser slam using distance function maps 0438\nfrom pixels to buildings: end-to-end probabilistic deep networks for large-scale semantic mapping 1737\nfrustum convnet: sliding frustums to aggregate local point-wise features for amodal 3d object detection 0835\nfusing body posture with facial expressions for joint recognition of affect in child-robot interaction 2449\nfusing lidar data and aerial imagery with perspective correction for precise localization in urban canyons 1311\nfusion of passive ferromagnetic sensors with the navigational data for the improvement of the detection of underwater metal-containing objects 0918\ngaple: generalizable approaching policy learning for robotic object searching in indoor environment 2275\ngaussian mixture model (gmm) based object detection and tracking using dynamic patch estimation 1013\ngaze-based intention anticipation over driving manoeuvres in semi-autonomous vehicles 1667\ngaze training by modulated dropout improves imitation learning 0474\ngeneral hand guidance framework using microsoft hololens 0814\ngeneralized contact constraints of hybrid trajectory optimization for different terrains and analysis of sensitivity to randomized initial guesses 1704\ngeneralized multiple correlation coefficient as a similarity measurement between trajectories 1020\ngeneralized ray-based lattice generation and graph representation of wrench-closure workspace for arbitrary cable-driven robots 2589\ngenerate what you can't see - a view-dependent image generation 1170\ngenerating a key pose sequence based on kinematics and statics optimization for manipulating a heavy object by a humanoid robot 1048\ngenerating an image of an object's appearance from somatosensory information during haptic exploration 1339\ngenerating coordinated reach-grasp motions with neural networks 2794\ngenerating grasp poses for a high-dof gripper using neural networks 0196\ngeometric and physical constraints for drone-based head plane crowd density estimation 0262\ngeo-referenced semantic point cloud map using the usyd campus dataset 2690\nglassloc: plenoptic grasp pose detection in transparent clutter 1636\nglfp: global localization from a floor plan 0727\nglobal vision-based impedance control for robotic wall polishing 0558\ngoal-directed behavior under variational predictive coding: dynamic organization of visual attention and working memory 0133\ngpu accelerated robust scene reconstruction 0279\ngq-stn: optimizing one-shot grasp detection based on robustness classifier 1065\ngraph-based design of hierarchical reinforcement learning agents 1226\ngraph-based path planning for autonomous robotic exploration in subterranean environments 1749\ngraph element networks: a flexible model for robotic applications 2726\ngrasping unknown objects based on gripper workspace spheres 1656\ngrip: generative robust inference and perception for semantic robot manipulation in adversarial environments 1598\ngrounding language attributes to objects using bayesian eigenobjects 1368\nguinea fowl jumping robot with balance control mechanism: modeling, simulation, and experiment results 0309\nhand-eye calibration with a remote centre of motion 2216\nhand movement intention recognition based on emg intensity map and convolutional neural networks 2728\nhandover process of autonomous driver assist systems - a call for critical performance assessment 2693\nhaptic guidance for robot-assisted endovascular procedures: implementation and evaluation on surgical simulator 0495\nhaptic-guided shared control for needle grasping optimization in minimally invasive robotic surgery 1234\nhaptic perception of liquids enclosed in containers 1169\nhaptic shared-control methods for robotic cutting under nonholonomic constraints 2480\nhapticube: a compact 5-dof finger-wearable tactile interface 0603\nharmonious sampling for mobile manipulation planning 1306\nheuristic-based multiple mobile depots route planning for recharging persistent surveillance robots 1402\nhierarchical reinforcement learning for concurrent discovery of compound and composable policies 0988\nhierarchical reinforcement learning for quadruped locomotion 1783\nhierarchical segmentation of continuous motions through semg signal analysis 2525\nhigh-dimensional motion segmentation by variational autoencoder and gaussian processes 0297\nhigh-fidelity dexterous tactile telerobot for intuitive teleoperation 2727\nhigh-speed humanoid robot arm for badminton using pneumatic-electric hybrid actuators 2311\nhigh-speed on-chip mixing by micro-vortex generated by controlling local jet flow using dual membrane pumps 2104\nhigh-speed sliding locomotion generation on slippery surface of an indirectly controlled robot with viscoelastic body 2251\nhomography-based deep visual servoing methods for planar grasps 1691\nhong hu - an efficient and versatile tail-sitter vtol uav platform: design, implementation and control 2678\nhouseexpo: a large-scale 2d indoor layout dataset for learning-based algorithms 2719\nhovering control of a tturt with thrust vector decomposition technique 2640\nhow can robot's gaze ratio and gaze type show an awareness of power dynamics to the people with whom it is interacting? 2506\nhtetran ' a polyabolo inspired self reconfigurable tiling robot 1790\nhuman intention inference and on-line human hand motion prediction for human-robot collaboration 1604\nhuman interactive motion planning for shared teleoperation 2703\nhumanoid robot's force-based heavy manipulation tasks with torque-controlled arms and wrist force sensors 0776\nhumanoid robot next best view planning under occlusions using body movement primitives 2250\nhuman robot visual interface for 3d steering of a flexible, bio-inspired needle for brain surgery 0392\nhybrid visual servoing for autonomous robotic laser tattoo removal 1208\nhysteresis compensator with learning-based pose estimation for a flexible endoscopic surgery robot 0934\n'why don't you have a wife?!' free format dialogue in cri* 2753\nidentification of rat ultrasonic vocalizations from mix sounds of a robotic rat in non-silent environments 0746\nidentification of time-varying and time-scalable synergies from continuous electromyographic patterns 2237\nidentifying opportunities for relationship-focused robotic interventions in strained hierarchical relationships 0139\nignorance is not bliss: an analysis of central-place foraging algorithms 1591\nimplementation of a natural dynamic controller on an under-actuated compass-biped robot 1259\nimplementing regularized predictive control for simultaneous real-time footstep and ground reaction force optimization 0946\nimproved energy efficiency via parallel elastic elements for thestraight-legged vertically-compliant robot slider 2734\nimproved exploration through latent trajectory optimization in deep deterministic policy gradient 1584\nimproved learning accuracy for learning stable control from human demonstrations 0589\nimproved mechanical design and simplified motion planning of hybrid active and passive cable-driven segmented manipulator with coupled motion 0760\nimproved planetary rover inertial navigation and wheel odometry performance through periodic use of zero-type constraints 0886\nimproving 3d object detection for pedestrians with virtual multi-view synthesis orientation estimation 1574\nimproving learning-based ego-motion estimation with homomorphism-based losses and drift correction 1310\nimproving local trajectory optimization by probabilistic movement primitives 1128\nimproving robot success detection using static object data 0143\nimproving task-parameterised movement learning generalisation with frame-weighted trajectory generation 1448\nimu-based spectrogram approach with deep convolutional neural networks for gait classification 2775\ninchworm-inspired soft climbing robot using microspine arrays 0327\ninertial-based motion capturing and smart training system 0587\ninference of user-intention in remote robot wheelchair assistance using multimodal interfaces 1710\ninfer: intermediate representations for future prediction 1417\ninferring distributions over depth from a single image 0118\ninfluence of parameters uncertainties on the positioning of cable-driven parallel robots 1005\ninformation filter occupancy mapping using decomposable radial kernels 0695\ninformation-guided robotic maximum seek-and-sample in partially observable continuous environments 2385\ninformed region selection for efficient uav-based object detectors: altitude-aware vehicle detection with cycar dataset 1463\ninfrastructure-free nlos obstacle detection for autonomous cars 0031\ninteger programming as a general solution methodology for path-based optimization in robotics: principles, best practices, and applications 0849\nintention-driven shoulder rehabilitation for targeted neuro-muscular training using an exo-musculoskeletal robot 2777\ninteraction-aware decision making with adaptive strategies under merging scenarios 1650\ninteractions with an empathetic agent: regulating emotions and improving engagement in autism 2606\ninteraction templates for multi-robot systems 2292\ninteractive trajectory adaptation through force-guided bayesian optimization 0811\nintroducing a scalable and modular control framework for low-cost monocular robots in hazardous environments 0983\nintuitive control of a robotic arm and hand system with pneumatic haptic feedback 2587\ninverse dynamics modeling of robotic manipulator with hierarchical recurrent network 1999\ninverse kinematics and sensitivity minimization of an n-stack stewart platform 1063\ninverse optimal planning for air traffic control 1258\nironcub: towards aerial humanoid robotics 2733\niterative learning control for fast and accurate position tracking with an articulated soft robotic arm 0235\nivoa: introspective vision for obstacle avoidance 1560\njisap: joint inference for surgeon attributes prediction during robot-assisted surgery 0540\njointly learnable behavior and trajectory planning for self-driving vehicles 1301\njoint offset optimization of hip joints in humanoid robots 2701\njoint torque estimation toward dynamic and compliant control for gear-driven torque sensorless quadruped robot 0201\njoint velocity and acceleration estimation in serial chain rigid body and flexible joint manipulators 1959\nkinematically redundant hybrid robots with simple singularity conditions and analytical inverse kinematic solutions 2402\nkinematic modeling of a soft pneumatic actuator using cubic hermite splines 0717\nkinematics, design and experimental validation of a novel parallel robot for two-fingered dexterous manipulation 0900\nkinematic synthesis of a serial robotic manipulator by using generalized differential inverse kinematics 2579\nlambda-field: a continuous counterpart of the bayesian occupancy grid for risk assessment 0423\nlaminated foam-based soft actuator for actuatable flexible structure 0089\nlanding of a multirotor aerial vehicle on an uneven surface using multiple on-board manipulators 0804\nlane marking learning based on crowdsourced data 1148\nlarge-scale 6d object pose estimation dataset for industrial bin-picking 0857\nlazy compilation of variants of multi-robot path planning with satisfiability modulo theory (smt) approach 1422\nldls: 3-d object segmentation through label diffusion from 2-d images 2164\nlearning 2d to 3d lifting for object detection in 3d for autonomous vehicles 1917\nlearning actions from human demonstration video for robotic manipulation 0994\nlearning barrier functions for constrained motion planning with dynamical systems 2374\nlearning-based model predictive control for autonomous racing 2310\nlearning-based nonlinear model predictive control of reconfigurable autonomous robotic boats: roboats 0476\nlearning based robotic bin-picking for potentially tangled objects 2408\nlearning by demonstration and robust control of dexterous in-hand robotic manipulation skills 1966\nlearning continuous time control policies by minimizing the hamilton-jacobi-bellman residual 2685\nlearning event-based height from plane and parallax 1144\nlearning footstep planning on irregular surfaces with partial placements 0597\nlearning from demonstration based on a mechanism to utilize an object's invisibility 2516\nlearning generalisable coupling terms for obstacle avoidance via low-dimensional geometric descriptors 2418\nlearning generative socially-aware models of pedestrian motion 2395\nlearning grasp affordance reasoning through semantic relations 2527\nlearning intention aware online adaptation of movement primitives 2468\nlearning interactive behaviors for musculoskeletal robots using bayesian interaction primitives 0481\nlearning local feature descriptor with motion attribute for vision-based localization 0823\nlearning multimodal representations for sample-efficient recognition of human actions 0975\nlearning multiple sensorimotor units to complete compound tasks using an rnn with multiple attractors 2092\nlearning object models for non-prehensile manipulation 1178\nlearning physics-based manipulation in clutter: combining image-based generalization and look-ahead planning 1536\nlearning q-network for active information acquisition 1810\nlearning real-time closed loop robotic reaching from monocular vision by exploiting a control lyapunov function structure 0583\nlearning real-world robot policies by dreaming 1155\nlearning residual flow as dynamic motion from stereo videos 1898\nlearning safe unlabeled multi-robot planning with motion constraints 2531\nlearning singularity avoidance 1516\nlearning state-dependent sensor measurement models for localization 1870\nlearning the scope of applicability for task planning knowledge in experience-based planning domains 0230\nlearning to augment synthetic images for sim2real policy transfer 0729\nlearning to estimate centers of mass of arbitrary objects 1592\nlearning to estimate pose and shape of hand-held objects from rgb images 0937\nlearning to explore in motion and interaction tasks 1281\nlearning to generate unambiguous spatial referring expressions for real-world environments 1082\nlearning to grasp arbitrary household objects from a single demonstration 1051\nlearning topometric semantic maps from occupancy grids 1776\nlearning to sequence multiple tasks with competing constraints 1238\nlearning via-point movement primitives with inter and extrapolation capabilities 1076\nlearning virtual borders through semantic scene understanding and augmented reality 0253\nlearning virtual grasp with failed demonstrations via bayesian inverse reinforcement learning 0490\nlearn to adapt to human walking: a model-based reinforcement learning approach for a robotic assistant rollator 2489\nlego: leveraging experience in roadmap generation for sampling-based planning 1481\nlic-fusion: lidar-inertial-camera odometry 2005\nlidar based navigable region detection for unmanned surface vehicles 0582\nlidar-flow: dense scene flow estimation from sparse lidar and stereo images 0972\nlifelong federated reinforcement learning: a learning architecture for navigation in cloud robotic systems 2090\nline-based absolute and relative camera pose estimation in structured environments 0898\nloam_livox: a robust lidar odemetry and mapping (loam) package for livox lidar 2663\nlocalization and mapping using instance-specific mesh models 0572\nlocal online motor babbling: learning motor abundance of a musculoskeletal robot arm 0351\nlocal pose optimization with an attention-based neural network 1439\nlocomotion planning of a variable geometry robot based on polygon-shaped ground contacts 2639\nlong range neural navigation policies for the real world 2264\nlong-term community social robots to promote social connectedness among older adults 2580\nlong-term prediction of motion trajectories using path homology clusters 0667\nlong-term visual inertial slam based on time series map prediction 0893\nlong-time self-body image acquisition and its application to the control of musculoskeletal structures 2344\nlook further to recognize better: learning shared topics and category-specific dictionaries for open-ended 3d object recognition 0127\nlow-cost sonar navigation system 1459\nlow level control of a quadrotor with deep model-based reinforcement learning 2530\nlswarm: efficient collision avoidance for large swarms with coverage constraints in complex urban scenes 2370\nmacro-micro multi-arm robot for single-port access surgery 2456\nmagnetic-needle-assisted micromanipulation of dynamically self-assembled magnetic droplets for cargo transportation 2205\nmagnetic needle steering model identification using expectation-maximization 1099\nmagnetic sensor based probe for microrobot detection and localization 2784\nmaking sense of audio vibration for liquid height estimation in robotic pouring 1118\nmanipulation motion taxonomy and coding for robots 1733\nmanipulation planning with soft orientation constraints based on composite configuration space 2788\nmanipulation purpose underwater agent vehicle for ghost net recovery mission 1625\nmap-aware slam with sparse map features 0178\nmap based human motion prediction for people tracking 0167\nmapping for planetary rovers from terramechanics perspective 1168\nmathematic modeling and optimal design of a magneto-rheological clutch for the compliant actuator in physical robot interactions 2478\nmavnet: an effective semantic segmentation micro-network for mav-based tasks 2330\nmaximum information bounds for planning active sensing trajectories 1273\nmaximum likelihood path planning for fast aerial maneuvers and collision avoidance 0106\nmeasuring engagement elicited by eye contact in human-robot interaction 0949\nmeta-learning for multi-objective reinforcement learning 0929\nmeta-world: a benchmark and evaluation for multi-task and meta reinforcement learning 2686\nmetric monocular localization using signed distance fields 0840\nminiaturization of mr safe pneumatic rotational stepper motors 0892\nminimal sensor setup in lower limb exoskeletons for motion classification based on multi-modal sensor data 0722\nminimum $k$-connectivity maintenance for robust multi-robot systems 1935\nmisalignment recognition using markov random fields with fully connected latent variables for detecting localization failures 2263\nmixed reality control of the humanoid robot eve 2735\nmobile robot learning from human demonstrations with nonlinear model predictive control 1451\nmobile robot localization with reinforcement learning map update decision aided by an absolute indoor positioning system 1223\nmodel free calibration of wheeled robots using gaussian process 1795\nmodeling and force control of a terramechanical wheel-soil contact for a robotic manipulator used in the planetary rover design process 0754\nmodeling and identification for the design of a rotary soft actuator based on wren mechanism 0652\nmodeling, learning and prediction of longitudinal behaviors of human-driven vehicles by incorporating internal human decision-making process using inverse model predictive control 1284\nmodeling novel soft mechanosensors based on air-flow measurements 2568\nmodeling, simulation and experimental validation of tendon-driven soft-arm robot configuration - a continuum mechanics approach 1091\nmodel-less active compliance for continuum robots using recurrent neural networks 2319\nmodelling and dynamic tracking control of industrial vehicles with tractor-trailer structure 1947\nmodelling of uniaxial egain-based strain sensors for proprioceptive sensing of soft robots 1933\nmodel predictive contouring control for collision avoidance in unstructured dynamic environments 2387\nmodel predictive control based dynamic path tracking of a four-wheel steering mobile robot 1060\nmodel simplification for dynamic control of series-parallel hybrid robots - a representative study on the effects of neglected dynamics 1049\nmodular volumetric actuators using motorized auxetics 1643\nmonocular depth estimation in new environments with absolute scale 0759\nmonocular object and plane slam in structured environments 2277\nmonocular outdoor semantic mapping with a multi-task network 0174\nmonocular plan view networks for autonomous driving 1829\nmorphing structure for changing hydrodynamic characteristics of a soft underwater walking robot 2540\nmotion decoupling and composition via reduced order model optimization for dynamic humanoid walking with clf-qp based active force control 0625\nmotion direction decoding of upper limb from eeg signals with a cognitive distraction task 2675\nmotion planning for a continuum robotic mobile lamp: defining and navigating the configuration space 1370\nmotor-propeller matching of aerial propulsion systems for direct aerial-aquatic operation 0416\nmoving onto high steps for a four-limbed robot with torso contact 1965\nmperl : hardware and software co-design for robotic manipulators 1211\nmrlift: a semi-active lower back support exoskeleton based on mr fluid and force retention technology 1388\nmt-rrt: a general purpose multithreading library for path planning 0806\nmulti-agent image classification via reinforcement learning 2003\nmulticamera 3d reconstruction of dynamic surgical cavities: non-rigid registration and point classi\ufb01cation 0585\nmulti-contact stabilization of a humanoid robot for realizing dynamic contact transitions on non-coplanar surfaces 1098\nmulti-controller multi-objective locomotion planning for legged robots 0701\nmulti-dof force characterization of soft actuators 2554\nmulti-hand direct manipulation of complex constrained virtual objects 0687\nmulti-layer environmental affordance map for robust indoor localization, event detection and social friendly navigation 1075\nmultilevel incremental roadmap spanners for reactive motion planning 1878\nmultimodal uncertainty reduction for intention recognition in human-robot interaction 0432\nmultiple hypothesis semantic mapping for robust data association 2279\nmulti-robot assembly sequencing via discrete optimization 1747\nmultirobot charging strategies: a game-theoretic approach 2125\nmulti-robot distributed digital printing system 2795\nmulti robot route planning (mrrp): extended spatial-temporal prioritized planning 0163\nmulti-sensor 6-dof localization for aerial robots in complex gnss-denied environments 1328\nmulti-step pick-and-place tasks using object-centric dense correspondences 1421\nmulti-vehicle cooperative local mapping using split covariance intersection filter 0869\nmuse: multi-sensor integration strategies applied to sequential monte carlo methods 0043\nnear-contact grasping strategies from awkward poses: when simply closing your fingers is not enough 0637\nneural control with an artificial hormone system for energy-efficient compliant terrain locomotion and adaptation of walking robots 2584\nneural-learning trajectory tracking control of flexible-joint robot manipulators with unknown dynamics 1590\nneural network based heterogeneous sensor fusion for robot motion planning 0459\nneural path planning: fixed time, near-optimal path generation via oracle imitation 0194\nneurotrajectory: a neuroevolutionary approach to local state trajectory learning for autonomous vehicles 2219\nn-merci: a new metric to evaluate the correlation between predictive uncertainty and true error 1624\nnon-contact sensing of respiratory signals 2754\nnonlinear optimization of step duration and step location 0950\nnon-myopic planetary exploration combining in situ and remote measurements 1367\nnon-parametric mixed-manifold products using multiscale kernel densities 1893\nnon-uniform robot densities in vibration driven swarms using phase separation theory 1874\nnormal distribution mixture matching based model free object tracking using 2d lidar 1307\nnovel lockable and stackable compliant actuation unit for modular +spea actuators 2600\nobject placement planning and optimization for robot manipulators 1756\nobject proposal algorithms in the wild: are they generalizable to robot perception? 1538\nobject rearrangement with nested nonprehensile manipulation actions 1431\nobject singulation via nonlinear pushing for robotic grasping 0428\nobservability analysis of position estimation for quadrotors with modified dynamics and range measurements 1690\nobstacle avoidance using a capacitive skin for safe human-robot interaction 0903\nobstacle climbing by a humanoid robot using standing jump motion 2702\nobstacle overcoming on a fa\u00e3\u00a7ade: novel design of a rotating leg mechanism 2634\nocclusion-robust deformable object tracking without physics simulation 1309\nolder people prefrontal cortex activation estimates their perceived difficulty of a humanoid-mediated conversation 2281\nomnipush: accurate, diverse, real-world dataset of pushing dynamics with rgb-d video 1101\nonboard marker-less detection and localization of non-cooperating drones for their safe interception by an autonomous aerial system 2327\non-chip three-dimension cell rotation using whirling flows generated by oscillating asymmetrical microstructures 0313\non data sharing strategy for decentralized collaborative visual-inertial simultaneous localization and mapping 0465\non enhancing ground surface detection from sparse lidar point cloud 0013\none-shot composition of vision-based skills from demonstration 0561\none-shot object localization using learnt visual cues via siamese networks 0353\non flying backwards: preventing run-away of small, low-speed fixed-wing uavs in strong winds 1693\nonline active safety for robotic manipulators 1732\nonline and consistent occupancy grid mapping for planning in unknown environments 1487\nonline motion planning over multiple homotopy classes with gaussian process inference 1340\nonline optimal impedance planning for legged robots 1763\nonline performance prediction and profiling of human activities by observation 1033\nonline planning for autonomous underwater vehicles performing information gathering tasks in large subsea environments 0997\nonline relative footstep optimization for legged robots dynamic walking using discrete-time model predictive control 1001\nonline system identification algorithm without persistent excitation for robotic systems: application to reconfigurable autonomous vessels 1069\nonline trajectory generation of a mav for chasing a moving target in 3d dense environments 1511\nonline trajectory generation: reactive control with return inside an admissible kinematic domain 0877\non model-based adhesion control of a vortex climbing robot 1401\non modeling the effects of auditory annoyance on driving style and passenger comfort 1415\non the bayes filter for shared autonomy 2304\non the covariance of x in ax = xb 2597\non the effect of semielliptical foot shape on the energetic efficiency of passive bipedal gait 0873\non the feasibility of multi-degree-of-freedom haptic devices using passive actuators 1453\non the tunable sparse graph solver for pose graph optimization in visual slam problems 1595\non training flexible robots using deep reinforcement learning 1855\noperational space control framework for torque controlled humanoid robots with joint elasticity 2529\noperation of a pneumatic soft manipulator using a wearable interface with flexible strain sensors 2132\nopposite treatments on null space: null space projection vs null space avoidance 2711\noptical coherence tomography guided robotic device for autonomous needle insertion in cornea transplant surgery 1539\noptimal solving of constrained path-planning problems with graph convolutional networks and optimized tree search 1095\noptimal temporal logic planning for multi-robot systems in uncertain semantic maps 1083\noptimal temporal logic planning with cascading soft constraints 1824\noptimization based motion planning for multi-limbed vertical climbing robots 0601\noptimization based trajectory planning of mobile cable-driven parallel robots 1137\noptimization model for planning precision grasps with multi-fingered hands 1892\noptimized locomotion for energy-efficient quadrupedal robot over rough terrain 2646\noptimizing motion-planning problem setup via bounded evaluation with application to following surgical trajectories 1635\norbit characterization, stabilization and composition of 3d underactuated bipedal walking via hybrid passive linear inverted pendulum model 0624\norbslam-atlas: a robust and accurate multi-map system 0830\noreos: oriented recognition of 3d point clouds in outdoor scenarios 1242\noutlier-robust spatial perception: hardness, general-purpose algorithms, and guarantees 1688\noutlier-robust state estimation for humanoid robots 0433\npanopticfusion: online volumetric semantic mapping at the level of stuff and things 0278\npaper-based modular origami gripper 0282\npartial caging: a clearance-based definition and deep learning 2217\npass3d: precise and accelerated semantic segmentation for 3d point cloud 1842\npassive inverted ultra-short baseline (piusbl) localization: an experimental evaluation of accuracy 1268\npassive model reduction and switching for fast soft object simulation with intermittent contacts 1953\npath planning algorithm for a transformation of a shape-morphing wheel for a step-climbing 2642\npath planning for surgery robot with bidirectional continuous tree search and neural network 0812\npath planning with incremental roadmap update for visibility-based target tracking 0673\npd based robust quadratic programs for robotic systems 1039\npedestrian density prediction for efficient mobile robot exploration 0166\npeople's v-formation and side-by-side model adapted to accompany groups of people by social robots 0472\nperception as prediction using general value functions in autonomous driving applications 1442\nperception of pedestrian avoidance strategies of a self-balancing mobile robot 2510\nperception system design for low-cost commercial ground robots: sensor configurations, calibration, localization, and mapping 2209\nperformance guarantees for receding horizon search with terminal cost 1438\nperiodic trajectory planning and robust output zeroing control for underactuated bipedal robots with predicted disturbances 1623\npermanent magnets based actuator for microrobots navigation 0839\nperson-following for telepresence robots using web cameras 0396\nphysical fatigue analysis of assistive robot teleoperation via whole-body motion mapping 1567\nphysical orienteering problem for unmanned aerial vehicle data collection planning in environments with obstacles 2146\npiecewise rigid scene flow with implicit motion segmentation 1418\npixel-attentive policy gradient for multi-fingered grasping in cluttered scenes 0252\npixels to plans: learning non-prehensile manipulation by imitating a planner 1386\nplanning beyond the sensing horizon using a learned context 1586\nplanning high-quality motions for concentric tube robots in point clouds via parallel sampling and optimization 1365\nplanning in stochastic environments with goal uncertainty 0943\nplanning reactive manipulation in dynamic environments 0643\nplant phenotyping by deep-learning based planner for multi-robots 2176\nplasticity in collective decision-making for robots: creating global reference frames, detecting dynamic environments, and preventing lock-ins 1968\npns: a perspective-n-spheres algorithm for laparoscope calibration in minimally invasive surgery 0752\npointatrousnet: point atrous convolution for point cloud analysis 2290\npolicy distillation and value matching in multiagent reinforcement learning 1867\nport-hamiltonian passivity-based control on se(3) of a fully-actuated uav for aerial physical interaction near-hovering 2434\npose-aware placing with semantic labels - brandname-based affordance prediction and cooperative dual-arm active manipulation 0208\npose estimation for omni-directional cameras using sinusoid fitting 1527\npose-graph based indoor navigation test for unmanned underwater vehicle navigation 2760\nposition-based control of under-constrained haptics: a system for the dexmo glove 2496\nposition-based monocular visual servoing of an unknown target using online self-supervised learning 1902\nposition control of wire-suspended hand for long-reach aerial manipulation 2674\npprnet: point-wise pose regression network for instance segmentation and 6d pose estimation in bin-picking scenarios 0342\nprecise correntropy-based 3d object modelling with geometrical traffic prior 1593\nprecision modeling and optimally-safe design of quadcopters for controlled crash landing in case of rotor failure 1618\nprecision pouring into unknown containers by service robots 0756\npredicting grasp success with a soft sensing skin and shape-memory actuated gripper 2309\nprediction of human arm target for robot reaching movements 0219\npredictive and adaptive maps for long-term visual navigation in changing environments 0005\npredictive inverse kinematics for redundant manipulators with task scaling and kinematic constraints 2601\npredictive inverse kinematics: optimizing future trajectory through implicit time integration and future jacobian estimation 0519\npredictive optimization of assistive force on admittance control-based mobile walking support system 2487\npreliminary evaluation of an orbital camera for teleoperation of remote manipulators 1088\npreliminary investigation about relationship between perceived intimacy and touch characteristics 2671\npreliminary results of active compression sleeve using wire and fabric mechanism 2700\npreliminary study for developing a vision-based detection system of unmanned surface vessels 2705\npressure-driven body compliance using robot skin 2425\nprinting-while-moving: a new paradigm for large-scale robotic 3d printing 0347\nprivacy-preserving robot vision with anonymized faces by extreme low resolution 1845\nprobabilistic risk metrics for navigating occluded intersections 2394\nproposal of a peristaltic motion type duct cleaning robot for traveling in a flexible pipe 2533\nproto-object based saliency for event-driven cameras 1195\nqpswift : a real-time sparse quadratic program solver for robotic applications 2157\nquaternion-based smooth trajectory generator for via poses in se(3) considering kinematic limits in cartesian space 2386\nquickly inserting pegs into uncertain holes using multi-view images and deep network trained on synthetic data 2308\nradar slam for indoor disaster environments via multi-modal registration to prior lidar map 0355\nrandomized sensor selection for nonlinear systems with application to target localization 2504\nrange-limited, distributed algorithms on higher-order voronoi partitions in multi-robot systems 0509\nrangenet++: fast and accurate lidar semantic segmentation 0346\nrapid and robust monocular visual-inertial initialization with gravity estimation via vertical edges 1687\nrapid collision detection for multicopter trajectories 1449\nrapid design of mechanical logic based on quasi-static electromechanical modeling 1304\nrapid estimation of optical properties for simulation-based evaluation of pose estimation performance 0671\nrapid trajectory optimization using c-frost with illustration on a cassie-series dynamic walking biped 1651\nreactive interaction through body motion and the phase-state-machine 0427\nreal-time 6d object pose estimation on cpu 0189\nreal-time biped walking-pattern generation by spline collocation 2706\nrealtime contact dynamics for continuum arms using physics engines 2776\nreal-time dense depth estimation using semantically-guided lidar data propagation and motion stereo 2260\nreal-time detection of distracted driving using dual cameras 2668\nreal-time global registration for globally consistent rgb-d slam 2577\nreal-time model-based image color correction for underwater robots 1775\nreal-time monitoring of human task advancement 0888\nreal-time quad-rotor path planning using convex optimization and compound state-triggered constraints 2315\nreal-time sampling-based optimization on fpga for accurate grid map merging in embedded robotic systems 2714\nrebellion and obedience: the effects of intention prediction in cooperative handheld robots 0968\nrecalling candidates of grasping method from an object image using neural network 0875\nreconfiguration motion planning for variable topology truss 1830\nreconstructing endovascular catheter interaction forces in 3d using multicore optical shape sensors 2136\nrecurrent convolutional fusion for rgb-d object recognition 2138\nredundant resolution method of an underwater manipulation for disturbance rejection 2641\nrefusion: 3d reconstruction in dynamic environments for rgb-d cameras exploiting residuals 0135\nregion-wise polynomial regression for 3d mobile gaze estimation 0223\nregressing noisy joint states from visual data using cnn 2665\nreinforcement learning boat autopilot: a sample-efficient and model predictive control based approach 2313\nreinforcement learning of trajectory distributions: applications in assisted teleoperation and motion planning 2550\nrelaxing the conservatism of passivity condition for impedance controlled series elastic actuators 2312\nremote center motion of a surgical assisted robot for in-situ collaboration 2654\nrepresentation learning via parallel subset reconstruction for 3d point cloud generation 0053\nrepresenting robot task plans as robust logical-dynamical systems 1703\nresearch on finite ground effect of a rotor 1709\nresflow: multi-tasking of sequentially pooling spatiotemporal features for action recognition and optical flow estimation 0410\nresilience by reconfiguration: exploiting heterogeneity in robot teams 2108\nresolving elevation ambiguity in 1-d radar array measurements using deep learning 2086\nresponsive joint attention in human-robot interaction 0028\nresqbot 2.0: a mobile stretcher bed robot with neck securing device for safe casualty extraction 2716\nretrieval-based localization based on domain-invariant feature learning under changing environments 1695\nrgb-to-tsdf: direct tsdf prediction from a single rgb image for dense 3d reconstruction 0378\nright of way, assertiveness and social recognition in human-robot doorway interaction 1564\nrins-w: robust inertial navigation system on wheels 0067\nrise-slam: a resource-aware inverse schmidt estimator for slam 0283\nrisk-aware motion planning and control using cvar-constrained optimization 2316\nriverine coverage with an autonomous surface vehicle over known environments 0502\nroboat: an autonomous surface vehicle for urban waterways 0188\nrobot-assisted composite manufacturing using deep learning and multi-view computer vision 2732\nrobot audition approaches to field observation of bird songs 2682\nrobot-based machining of unmodeled objects via feature detection in dense point clouds 0960\nrobot-based strategy for objective assessment of motor impairments 2649\nrobot-enhanced therapy: development and validation of a supervised autonomous robotic system for autism spectrum disorders therapy 2500\nrobot finger with remote center of motion mechanism for covering joints with thick skin 2151\nrobot'robot gesturing for anchoring representations 2605\nrobotic cutting of solids based on fracture mechanics and fem 1501\nrobotic laparoendoscopic single-site surgery platform on dvrk 2772\nrobotic tracking control with kernel trick-based reinforcement learning 0381\nrobotic ultrasound for catheter navigation in endovascular procedures 0487\nrobot learning of shifting objects for grasping in cluttered environments 0482\nrobot learning via human adversarial games 1400\nrobot localization in floor plans using a room layout edge extraction network 1403\nrobot localization via odometry-assisted ultra-wideband ranging with stochastic guarantees 1037\nrobots that take advantage of human trust 1277\nrobust and adaptive lower limb prosthesis control via extended kalman filter-based phase estimation 2097\nrobust and efficient quadrotor trajectory generation for fast autonomous flight 2377\nrobust and efficient vehicles motion estimation with low-cost multi-camera and odometer-gyroscope 1803\nrobust, compliant assembly with elastic parts and model uncertainty 1059\nrobust deformation model approximation for robotic cable manipulation 1626\nrobust grasp planning over uncertain shape completions 0761\nrobust hand-eye calibration via iteratively re-weighted rank-constrained semi-definite programming 2026\nrobust high accuracy visual-inertial-laser slam system 0971\nrobust impedance shaping of redundant teleoperators with time-delay via sliding mode control 1203\nrobust legged robot state estimation using factor graph optimization 2582\nrobust loop closure detection based on bag of superpoints and graph verification 1544\nrobust moving path following control for robotic vehicles: theory and experiments 2295\nrobust non-rigid point set registration algorithm considering anisotropic uncertainties based on coherent point drift 1673\nrobust outdoor self-localization in changing environments 1163\nrobust real-time rgb-d visual odometry in dynamic environments via rigid motion model 1254\nrobust trajectory planning for a multirotor against disturbance based on hamilton-jacobi reachability analysis 1801\nrobust uav localization around the large scale facilities with multiple subsidiary uavs 2757\nrobust uav position and attitude estimation using multiple gnss receivers for laser-based 3d mapping 0577\nroficom -- first open-hardware connector for metamorphic robots 0354\nroi-based robotic grasp detection for object overlapping scenes 0190\nrolling-shutter modelling for direct visual-inertial odometry 0350\nronet: real-time range-only indoor localization via stacked bidirectional lstm with residual attention 2267\nrouting a fleet of automated vehicles in a capacitated transportation network 0931\nsafe path planning with gaussian process regulated risk map 0706\nsafe physical hri: toward a unified treatment of speed and separation monitoring together with power and force limiting 2490\nsailmav: design and implementation of a novel multi-modal flying sailing robot 2599\nsample-efficient deep reinforcement learning with imaginary rollouts for human-robot interaction 0933\nsample efficient interactive end-to-end deep learning for self-driving cars with selective multi-class safe dataset aggregation 1303\nsampling-based motion planning for aerial pick-and-place 1504\nsampling-based motion planning of 3d solid objects guided by multiple approximate solutions 1108\nsampling-based path planning for cooperative autonomous maritime vehicles to reduce uncertainty in range-only localisation 2223\nscaffold-based asynchronous distributed self-reconfiguration by continuous module flow 0990\nscaling robot supervision to hundreds of hours with roboturk: robotic manipulation dataset through human reasoning and dexterity 1251\nscheduling of mobile workstations for overlapping production time and delivery time 0586\nseeing behind things: extending semantic segmentation to occluded regions 1433\nseeing beyond appearance ' mapping real images into geometrical domains for unsupervised cad-based recognition 1124\nseeking the analytical approximation of the stance dynamics of the 3d spring-loaded inverted pendulum model by using perturbation approach 0633\nsegregation and flow of modules in a robot swarm utilising the brazil nut effect 0622\nself-calibration and learning on chip: towards neuromorphic robots 2752\nself-collision detection and avoidance for dual-arm concentric tube robots 2523\nself-modeling tracking control of crawler fire fighting robot based on causal network 0419\nself-organised flocking in robotic swarm based on active elastic sheet 2625\nself-organized adaptive paths in multi-robot manufacturing: reconfigurable and pattern-independent fibre deployment 1952\nself-specialization of general robot plans based on experience 2508\nself-supervised 3d shape and viewpoint estimation from single images for robotics 0758\nself-supervised transfer learning for instance segmentation through physical interaction 1890\nsemantically assisted loop closure in slam using ndt histograms 0879\nsemantic mates: intuitive geometric constraints for efficient assembly specifications 0478\nsemantic segmentation using gan and weakly supervised based on deep transfer learning 2618\nsemi-autonomous interventional manipulation using pneumatically attachable flexible rails 0871\nsensitivity of legged balance control to uncertainties and sampling period 2349\nsensor installation and retrieval operations using an unmanned aerial manipulator 2558\nsensorless estimation of the planar distal shape of a tip-actuated endoscope 2497\nseqlpd: sequence matching enhanced loop-closure detection based on large-scale point cloud description for self-driving vehicles 1907\nsequential clustering for tactile image compression to enable direct adaptive feedback 1971\nsetup and method for remote center of motion positioning guidance during robot-assisted surgery 2447\nsganvo: unsupervised deep visual odometry and depth estimation with stacked generative adversarial networks 2427\nshared autonomy of a flexible manipulator in constrained endoluminal surgical tasks 2232\nshared controller for obstacle avoidance of manipulator for teleopeartion system 2662\nsharing is caring: socially-compliant autonomous intersection negotiation 1136\nsiamese convolutional neural network for sub-millimeter-accurate camera pose estimation and visual servoing 1514\nsilhonet: an rgb method for 6d object pose estimation 2334\nsimdop: simd optimized bounding volume hierarchies for collision detection 0940\nsimitate: a hybrid imitation learning benchmark 0497\nsim-to-(multi)-real: transfer of low-level robust control policies to multiple quadrotors 1589\nsim-to-real learning for casualty detection from ground projected point cloud data 1239\nsim-to-real transfer for biped locomotion 1040\nsimulation-based physics reasoning for consistent scene estimation in an hri context 1050\nsimultaneous drone localisation and wind turbine model fitting during autonomous surface inspection 0648\nsimultaneous transparent and non-transparent objects segmentation with multispectral scenes 0694\nsingle-hand movement direction decoding from eeg signals under opposite-hand movement distraction 2627\nsingle motor-based bidirectional twisted string actuation with variable radius pulleys 2412\nsituation awareness for proactive robots in hri 0967\nsix dof pose estimation for a tendon-driven continuum mechanism without a deformation model 2347\nskill interaction categories for communication in flexible human-robot teams 0338\nsmall-scale compliant dual arm with tail for winged aerial robots 1409\nsoft action particle deep reinforcement learning for a continuous action space 0588\nsoft pneumatic helical actuator with high contraction ratio 2538\nsoft polymer-electrolyte-fuel-cell tube realizing air-hose-free thin mckibben muscles 2520\nsp2 (spherically-stratified-points projection): generating novel images for 3d point cloud segmentation 2696\nsparse-3d lidar outdoor map-based autonomous vehicle localization 0595\nsparse depth enhanced direct thermal-infrared slam beyond the visible spectrum 2278\nspatiotemporal learning of directional uncertainty in urban environments with kernel recurrent mixture density networks 2197\nspatiotemporal representation of dynamic scenes 1228\nspecification-based maneuvering of quadcopters through hoops 1297\nspecifying and synthesizing human-robot handovers 1397\nspiking neural network on neuromorphic hardware for energy-efficient unidimensional slam 1452\nspinebot: pneumatically actuated muscle 2620\nspine-inspired continuum soft exoskeleton for stoop lifting assistance 2588\nspiral zipper manipulator for aerial grasping and manipulation 1937\nstability and gait switching of underactuated biped walkers 0520\nstair environment mapping and walk-able plane detecting algorithm for quadrupedal robot's locomotion 2712\nstarnet: pedestrian trajectory prediction using deep neural network in star topology 0199\nstate representation learning with robotic priors for partially observable environments 1302\nstatic analysis on the modular detachable climbing robot for all wall-to-wall transitions 2633\nstatistical coverage control of mobile sensor networks 2604\nstereo visual inertial lidar simultaneous localization and mapping 2562\nstereo visual-inertial slam using graph-based optimization 2621\nstiffness bounds for resilient and stable physical interaction of articulated soft robots 2429\nstochastic path planning for autonomous underwater gliders with safety constraints 1317\nstochastic sampling simulation for pedestrian trajectory prediction 2419\nstructured classification of locomotion modes for wearable robot control 2653\nstructured reward shaping using signal temporal logic specifications 1818\nstudy on elastic elements allocation for energy-efficient robotic cheetah leg 0928\nstudy on performance of marker detection via training data augmentation of partial distortion in underwater sonar image 2687\nstudy on stumbles of the elderly from a depth perception dependency test 1812\nsubmodular optimization for coupled task allocation and intermittent deployment problems 2134\nsubspace-based direct visual servoing 2161\nsuma++: efficient lidar-based semantic slam 0228\nsvin2: an underwater slam system using sonar, visual, inertial, and depth sensor 0269\nsynchronizing virtual constraints and preview controller: a walking pattern generator for the humanoid robot coman+ 1061\nsynergy-based control for multi-fingered hands using selected joint spaces 2688\nsynthesizing robot manipulation programs from a single observed human demonstration 0328\nsystematic benchmarking for reproducibility of computer vision algorithms for real-time systems: the example of optic flow estimation 0506\ntactile-based insertion for dense box-packing 1881\ntactile localization for unknown and known objects 2628\ntalk to the vehicle: language conditioned autonomous navigation of self driving cars 1765\ntarget classification and prediction of unguided rocket trajectories using deep neural networks 2723\ntarget tracking of moving and rotating object by high-speed monocular active vision 2692\ntask-motion planning with reinforcement learning for adaptable mobile service robots 1443\ntask-oriented grasping in object stacking scenes with crf-based semantic model 0663\ntask-specific self-body controller acquisition by musculoskeletal humanoids: application to pedal control in autonomous driving 0109\nteaching a drone to accompany a person from demonstrations using non-linear asfm 1107\nteleoperating robots from the international space station: microgravity effects on performance with force feedback 0370\ntendencyrl: multi-stage discriminative hints for ef\ufb01cient goal-oriented reverse curriculum learning 0532\nterrainfusion: real-time digital surface model reconstruction based on monocular slam 0786\nthe anbot: an intelligent robotic co-worker for industrial abrasive blasting 1497\nthe armm system: demonstrating clinical feasibility in steering magnetically actuated catheters in endovascular applications 2648\nthe combination function for multi-leg modular robot, bio-mimicked from ant's behavior 2657\nthe compliant joint toolbox for matlab: an introduction with examples 2574\nthe costar block stacking dataset: learning with workspace constraints 1236\nthe impact of domain randomization on object detection: case study on parametric shapes and synthetic textures 1333\nthe mastr1325 dataset for training deep usv obstacle detection models 1480\ntheoretical foundation for design of friction-tunable soft finger with wrinkle's morphology 2518\nthe rgb-d triathlon: towards agile visual toolboxes for robots 2357\nthermal-inertial odometry for autonomous flight throughout the night 1819\nthe road is enough! extrinsic calibration of non-overlapping stereo camera and lidar using road information 2213\nthe robot show must go on: effective responses to robot failures 1280\nthe role of robot payload in the safety map framework 1984\nthe \"smellicopter, \" a bio-hybrid odor localizing nano air vehicle 2245\nthe stability of human supervisory control operator behavioral models using hidden markov models 1015\nthree-degrees-of-freedom passive gravity compensation mechanism applicable to robotic arm with remote center of motion for minimally invasive surgery 2239\ntimed-elastic bands for manipulation motion planning 2371\ntimed-elastic smooth curve optimization for mobile-base motion planning 1549\ntime-delay compensation using energy tank for satellite dynamics robotic simulators 1116\ntime-optimal path tracking for jerk controlled robots 2375\ntime-optimal trajectory generation for dynamic vehicles: a bilevel optimization approach 1850\ntimepix radiation detector for autonomous radiation localization and mapping by micro unmanned vehicles 2593\ntime series motion generation considering long short-term motion 1608\ntime-varying graph patrolling against attackers with locally limited and imperfect observation models 0553\ntimtam: tunnel-image texturally-accorded mosaic for location refinement of underground vehicles with a single camera 2561\ntip model: a combination of unstable subsystems for lateral balance in walking 1354\ntorso-mounted vibrotactile interface to experimentally induce illusory own-body perceptions 0367\ntoward a ballbot for physically leading people: a human-centered approach 0920\ntoward a bipedal robot with variable gait styles: sagittal forces analysis in a planar simulation and a prototype ball-tray mechanism 1446\ntoward achieving formal guarantees for human-aware controllers in human-robot interaction 1096\ntoward affordance detection and ranking on novel objects for real-world robotic manipulation 2318\ntoward a human-machine interface based on electrical impedance tomography for robotic manipulator control 0596\ntoward an efficient hybrid interaction paradigm for object manipulation in optical see-through mixed reality 0084\ntoward a versatile robotic platform for fluoroscopy and mri-guided endovascular interventions: a pre-clinical study 1894\ntoward controllable morphogenesis in large robot swarms 2193\ntoward improving patient safety and surgeon comfort in a synergic robot-assisted eye surgery: a comparative study 2256\ntoward model-based benchmarking of robot components 1224\ntowards active stabilization of probe-based confocal laser endomicroscopy using a handheld micromanipulator 2680\ntowards a general framework for generating stable and flexible locomotion skills 2736\ntowards a generic in vivo in situ camera lens cleaning module for laparoscopic surgery 0073\ntowards an assisted robotic platform for soft neural tissue interaction 2770\ntowards a natural motion generator: a pipeline to control a humanoid based on motion data 1573\ntowards an autonomous unwrapping system for intralogistics 2444\ntowards an open-source micro robot oceanarium: a low-cost, modular, and mobile underwater motion-capture system 1925\ntowards a robot architecture for situated lifelong object learning 1429\ntowards a robust aerial cinematography platform: localizing and tracking moving targets in unstructured environments 1607\ntowards autonomous industrial-scale bathymetric surveying 1353\ntowards ergonomic control of collaborative effort in multi-human mobile-robot teams 0899\ntowards explainable shared control using augmented reality 0448\ntowards generalizing sensorimotor control across weather conditions 0954\ntowards jumping locomotion for quadruped robots on the moon 0801\ntowards learning trajectory segmentation through semi-supervised learning 2767\ntowards more realistic human-robot conversation: a seq2seq-based body gesture interaction system 1420\ntowards reversible dynamic movement primitives 0753\ntowards the design and development of a pediatric neuroendoscope tool 1125\nt-pfc: a trajectory-optimized perturbation feedback control approach 2388\ntracking control of fully-constrained cable-driven parallel robots using adaptive dynamic programming 0777\ntraining in task space to speed up and guide reinforcement learning 0186\ntrajectory estimation for geo-fencing applications on small-size fixed-wing uavs 1209\ntrajectory optimization for legged robots with slipping motions 2145\ntrajectory optimization for unknown constrained systems using reinforcement learning 1077\ntrajectory planning for a bat-like flapping wing robot 0863\ntransferable trial-minimizing progressive peg-in-hole model 0232\ntransfer learning for vision-based tactile sensing 0248\ntrust but verify: a distributed algorithm for multi-robot wireframe exploration and mapping 0720\ntunable contact conditions and grasp hydrodynamics using gentle fingertip suction 2575\ntwin kinematics approach for robotic-assisted tele-echography 1979\ntwo-view fusion based convolutional neural network for urban road detection 0198\ntzc: efficient inter-process communication for robotics middleware with partial serialization 0386\nuav landing at an unknown location marked by a radio beacon 1512\nuncertainty-aware imitation learning using kernelized movement primitives 0255\nunderactuated gripper with forearm roll estimation for human limbs manipulation in rescue robotics 1320\nunderstanding multi-robot systems: on the concept of legibility 0435\nunderstanding natural language instructions for fetching daily objects using gan-based multimodal target-source classification 2288\nunified balance control for biped robots including modification of footsteps with angular momentum and falling detection based on capturability 0126\nunified human-robot shared control with application to haptic telemanipulation 1748\nunstructured terrain navigation and topographic mapping with a low-cost mobile cuboid robot 0164\nunsupervised task segmentation approach for bimanual surgical tasks using spatiotemporal and variance properties 1244\nunsupervised traffic accident detection in first-person videos 1278\nuntethered quadrupedal hopping on a trampoline 2677\nupper-limb joint angle estimation method with commercial depth sensor for planar robot-aided reaching movement 2766\nupper limb motion simulation algorithm for prosthesis prescription and training 1052\nurban street trajectory prediction with multi-class lstm networks 2691\nuse of deep learning based on recurrent neural network for modeling of characteristics of a pneumatic artificial muscle 2617\nvariable configuration planner for legged-rolling obstacle negotiation locomotion: application on the centauro robot 0134\nvariable impedance in end-effector space: an action space for reinforcement learning in contact-rich tasks 1610\nvarious sized obstacle and stair climbing robot by wheel transformation: prototype and experimental results 2644\nvehicular multi-camera sensor system for automated visual inspection of electric power distribution equipment 1736\nview management for lifelong visual maps 0649\nview sharing to enhance driving safety through vehicle-to-vehicle communication 2667\nvilivo: virtual lidar-visual odometry for an autonomous vehicle with a multi-camera system 2025\nvirtual lane boundary generation for human-compatible autonomous driving: a tight coupling between perception and planning 1364\nvirtual maps for autonomous exploration with pose slam 1215\nvirtual-mass-ellipsoid inverted pendulum model and its applications to 3d bipedal locomotion on uneven terrains 0739\nvirtual region based multi-robot path planning in an unknown occluded environment 2127\nvision-aided localization for ground robots 0314\nvision-based automatic control of a 5-fingered simulated assistive robotic manipulator for activities of daily living 0805\nvision-based estimation of driving energy for planetary rovers using deep learning and terramechanics 2329\nvision-based magnetic platform for actuator positioning and wireless control of microrobots 0479\nvision-based virtual fixtures generation for robotic-assisted polyp dissection procedures 1167\nvisual-based autonomous driving deployment from a stochastic and uncertainty-aware perspective 0055\nvisual domain adaptation exploiting confidence-samples 0222\nvisual-inertial localization with prior lidar map constraints 2271\nvisual-inertial odometry tightly coupled with wheel encoder adopting robust initialization and online extrinsic calibration 0614\nvisual-inertial odometry with point and line features 1809\nvisual-inertial on-board throw-and-go initialization for micro air vehicles 1957\nvisual servo control of a novel magnetic actuated endoscope for uniportal video assisted thoracic surgery 2228\nvisual servoing of miniature magnetic film swimming robots for 3d arbitrary path following 2521\nvoice-controlled flexible exotendon (flexotendon) glove for hand rehabilitation 1130\nvolumetric instance-aware semantic mapping and 3d object discovery 2118\nvolumetric tree*: adaptive sparse graph for effective exploration of homotopy classes 1616\nwalking with augmented reality real-time visual feedback wearing a cable-driven active leg exoskeleton (c-alex) 2485\nwalking with confidence: safety regulation for full order biped models 2526\nwall-mounted robot arm equipped with 3-dof roll-pitch-pitch counterbalance mechanism 2411\nwarped hypertime representations for long-term autonomy of mobile robots 2131\nwearable activity recognition for robust human-robot teaming in safety-critical environments via hybrid neural networks 1950\nwhole-body control of humanoid robot in 3d multi-contact under contact wrench constraints including joint load reduction with self-collision and internal wrench distribution 0670\nwhole-body control with (self) collision avoidance using vector field inequalities 2432\nwhole-body locomotion and posture control on a torque-controlled hydraulic rover 2175\nwhole-body motion and landing force control for quadrupedal stair climbing 1470\nwhole-body motion planning for walking excavators 0991\nwhole-body mpc for a dynamically stable mobile manipulator 2422\nwhole-body postural control approach based on multiple zmp evaluation in humanoid robots 2638\nwide aperture imaging sonar reconstruction using generative models 0138\nwith proximity servoing towards safe human-robot-interaction 1369\nwlr-ii, a hose-less hydraulic wheel-legged robot 0226\nword2vec to behavior: morphology facilitates the grounding of language in machines 1517\nwsrender: a workspace analysis and visualization toolbox for robotic system design and verification 2499\nyouwasps: towards autonomous multi-robot mobile deposition for construction 1282", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000176, "year": null}, {"Unnamed: 0": 1199, "autor": 179, "date": null, "content": "IROS2018 SLAM Collections\nIntroduction\nThis repository contains SLAM papers from IROS2018. Thanks for the efforts from PaoPaoRobot\nReference\n[\u6ce1\u6ce1\u524d\u6cbf\u8ffd\u8e2a]\u8ddf\u8e2aSLAM\u524d\u6cbf\u52a8\u6001\u7cfb\u5217\u4e4bIROS2018\nFields\nVisual Inertial Odometry\nA Tutorial on Quantitative Trajectory Evaluation for Visual(-Inertial) Odometry, Zichao Zhang, Davide Scaramuzza [pdf] [code]\nChallenges in Monocular Visual Odometry Photometric Calibration, Motion Bias and Rolling Shutter Effect, Nan Yang , Rui Wang, Xiang Gao, and Daniel Cremers [pdf]\nCVI-SLAM \u2013 Collaborative Visual-Inertial SLAM, Marco Karrer, Patrik Schmuck, Margarita Chli [pdf]\nEmbedding Temporally Consistent Depth Recovery for Real-time Dense Mapping in Visual-inertial Odometry, Hui Cheng, Zhuoqi Zheng, Jinhao He, Chongyu Chen, Keze Wang, and Liang Lin\nHybrid Contact Preintegration for Visual-Inertial-Contact State Estimation Using Factor Graphs, Ross Hartley, Maani Ghaffari Jadidi, Lu Gan, Jiunn-Kai Huang, Jessy W. Grizzle, and Ryan M. Eustice [pdf] [video]\nInformation Sparsi\ufb01cation in Visual-Inertial Odometry, Jerry Hsiung, Ming Hsiao, Eric Westman, Rafael Valencia, and Michael Kaess [pdf]\nKey-Frame Strategy During Fast Image-Scale Changes and Zero Motion in VIO Without Persistent Features, Eren Allak, Alexander Hardt-Stremayr, and Stephan Weiss\nOn the Comparison of Gauge Freedom Handling in Optimization-based Visual-Inertial State Estimation, Zichao Zhang, Guillermo Gallego, Davide Scaramuzza [pdf]\nOnline Temporal Calibration for Monocular Visual-Inertial Systems, Tong Qin and Shaojie Shen [pdf] [code]\nReal-Time Fully Incremental Scene Understanding on Mobile Platforms, Johanna Wald, Keisuke Tateno, Jurgen Sturm, Nassir Navab, and Federico Tombari [pdf] [project]\nThe TUM VI Benchmark for Evaluating Visual-Inertial Odometry, David Schubert, Thore Goll, Nikolaus Demmel, Vladyslav Usenko, Jorg Stuckler and Daniel Cremers [pdf] [dataset]\nTrifo-VIO: Robust and Efficient Stereo Visual Inertial Odometry using Points and Lines, Feng Zheng, Grace Tsai, Zhe Zhang, Shaoshan Liu, Chen-Chi Chu, and Hongbing Hu [pdf] [dataset]\n\u03c0-SoC Heterogeneous SoC Architecture for Visual Inertial SLAM Applications, Jie Tang, Bo Yu, Shaoshan Liu, Zhe Zhang, Weikang Fang, and Yanjun Zhang\nRobocentric Visual-Inertial Odometry, Zheng Huai, Guoquan Huang [pdf]\nRobust Visual-Inertial State Estimation with Multiple Odometries and Efficient Mapping on an MAV with Ultra-Wide FOV Stereo Vision, M. G. M\u00fcller, F. Steidle, M. J. Schuster, P. Lutz, M. Maier, S. Stoneman, T. Tomic, W. St\u00fcrzl\nUnscented Kalman Filter on Lie Groups for Visual_Inertial Odometry, Martin Brossard, Silvere Bonnabel, Axel Barrau [pdf]\nRGBD Camera\nPerception Based Locomotion System for a Humanoid Robot with Adaptive Footstep Compensation under Task Constraints, Iori Kumagai, Mitsuharu Morisawa, Shin\u2019ichiro Nakaoka, Takeshi Sakaguchi, Hiroshi Kaminaga, Kenji Kaneko, Fumio Kanehiro\nReal-time 3D Reconstruction Using a Combination of Point-based and Volumetric Fusion , Maik Keller, Damien Lefloch, Martin Lambers, Shahram Izadi, Tim Weyrich, Andreas Kolb [pdf]\nEdge-based Robust RGB-D Visual Odometry_Using 2-D Edge Divergence Minimization, Changhyeon Kim, Pyojin Kim, Sangil Lee and H. Jin Kim\nLiDAR\nA Maximum Likelihood Approach to Extract Polylines from 2-D Laser Range Scans, Alexander Schaefer, Daniel Buscher, Lukas Luft, Wolfram Burgard [pdf] [code]\nA robust pose graph approach for city scale LiDAR mapping, Sheng Yang, Xiaoling Zhu, Xing Nian, Lu Feng, Xiaozhi Qu and Teng Ma\nDynamic Scaling Factors of Covariances for Accurate 3D Normal Distributions Transform Registration , Hyunki Hong and B. H. Lee\nIntegrating Deep Semantic Segmentation into 3D Point Cloud Registration Anestis Zaganidis, Li Sun, Tom Duckett, Grzegorz Cielniak [pdf]\nLIMO Lidar-Monocular Visual Odometry, Johannes Graeter, Alexander Wilczynski and Martin Lauer [pdf] [code]\nLIPS LiDAR-Inertial 3D Plane SLAM, Patrick Geneva, Kevin Eckenhoff, Yulin Yang and Guoquan Huang [pdf] [code]\nPoseMap Lifelong, Multi-Environment 3D LiDAR Localization, Philipp Egger, Paulo V K Borges, Gavin Catt, Andreas Pfrunder, Roland Siegwart, Renaud Dub\u00e9 [pdf] [video1] [video2]\nStabilize an Unsupervised Feature Learning for LiDAR-based Place Recognition, Peng Yin, Lingyun Xu, Zhe Liu, Lu Li, Hadi Salman, Yuqing He, Weiliang Xu, Hesheng Wang and Howie Choset\nStereo Camera Localization in 3D LiDAR Maps, Youngji Kim, Jinyong Jeong and Ayoung Kim [video]\nLeGO-LOAM_ Lightweight and Ground-Optimized_Lidar Odometry and Mapping on Variable Terrain, Tixiao Shan and Brendan Englot [pdf] [code] [video]\nLiDAR and Camera Calibration using Motions Estimated by Sensor_Fusion Odometry, Ryoichi Ishikawa, Takeshi Oishi and Katsushi Ikeuchi [pdf]\nScan Context: Egocentric Spatial Descriptor for Place Recognition within 3D Point Cloud Map, Giseop Kim and Ayoung Kim\nEvent Camera\nAsynchronousCornerDetectionandTrackingforEventCamerasinReal-Time, Ignacio Alzugaray and Margarita Chli [pdf] [video]\nMapping\nA B-spline Mapping Framework for Long-Term Autonomous Operations, R\u00f4mulo T. Rodrigues, A. Pedro Aguiar and Ant\u00f3nio Pascoal [video]\nC-blox A Scalable and Consistent TSDF-based Dense Mapping Approach, Alexander Millane, Zachary Taylor, Helen Oleynikova, Juan Nieto, Roland Siegwart, C\u00e9sar Cadena [pdf] [code]\nEfficient Long-term Mapping in Dynamic Environments, Mar\u0131\u0301a T. L\u00e1zaro, Roberto Capobianco and Giorgio Grisetti [code]\nHMAPs \u2013 Hybrid Height-Voxel Maps for Environment Representation, Lu\u0131\u0301s Garrote, Cristiano Premebida, David Silva, Urbano J. Nunes\nHuman-in-the-loop Augmented Mapping, Abbas Sidaoui, Imad H. Elhajj, and Daniel Asmar\nStructured Skip List A Compact Data Structure for 3D Reconstruction, Shi-Jie Li, Ming-Ming Cheng, Yun Liu, Shao-Ping Lu, YaHui Wang, Victor Adrian Prisacariu [pdf]\nSLAM System\nArthroSLAM: multi-sensor robust visual localization for minimally invasive orthopedic surgery, Andres Marmol, Peter Corke, Thierry Peynot\nDynaSLAM Tracking, Mapping and Inpainting in Dynamic Scenes, Berta Bescos, Jos\u00e9 M. F\u00e1cil, Javier Civera and Jos\u00e9 Neira [pdf] [video]\nImproving Repeatability of Experiments by Automatic Evaluation of SLAM Algorithms, Francesco Amigoni, Valerio Castelli, and Matteo Luperto [code]\nIndoor Mapping and Localization for Pedestrians using Opportunistic Sensing with Smartphones, Qing Liang, Lujia Wang, Youfu Li and Ming Liu [video]\nMIS-SLAM Real-time Large Scale Dense Deformable SLAM System in Minimal Invasive Surgery Based on Heterogeneous Computing, Jingwei Song, Jun Wang, Liang Zhao, Shoudong Huang and Gamini Dissanayake [pdf]\nOmnidirectional DSO Direct Sparse Odometry with Fisheye Cameras, Hidenobu Matsuki, Lukas von Stumberg, Vladyslav Usenko, J\u00f6rg St\u00fcckler and Daniel Cremers [pdf] [video]\nOpenSeqSLAM2.0 An Open Source Toolbox for Visual Place Recognition Under Changing Conditions, Ben Talbot, Sourav Garg, and Michael Milford [pdf] [code]\nProbabilistic Dense Reconstruction from a Moving Camera, Yonggen Ling, Kaixuan Wang and Shaojie Shen [code] [video]\nRobust Long-Term Registration of UAV Images of Crop Fields for Precision Agriculture, Nived Chebrolu Thomas L\u00e4be and Cyrill Stachniss [pdf]\nSemi-Supervised SLAM Leveraging Low-Cost Sensors on Underground Autonomous Vehicles for Position Tracking, Adam Jacobson, Fan Zeng, David Smith, Nigel Boswell, Thierry Peynot and Michael Milford\nTowards Robust Visual Odometry with a Multi-Camera System, Peidong Liu, Marcel Geppert, Lionel Heng, Torsten Sattler, Andreas Geiger and Marc Pollefeys [pdf] [code]\nFast Cylinder and Plane Extraction from Depth Cameras for Visual_Odometry, Pedro F. Proen\u00e7a and Yang Gao [pdf] [code] [video]\nGeometric-based Line Segment Tracking for HDR Stereo Sequences, Ruben Gomez-Ojeda, Javier Gonzalez-Jimenez [pdf] [code]\nMultimotion Visual Odometry (MVO): Simultaneous Estimation of Camera and Third-Party Motions, Kevin M. Judd and Jonathan D. Gammell and Paul Newman [pdf]\nReliable fusion of black-box estimates of underwater localization, Hendry Ferreira Chame, Matheus Machado dos Santos, S\u0131\u0301lvia Silva da Costa Botelho [code]\nStereo Visual Odometry and Semantics based Localization of Aerial_Robots in Indoor Environments, Hriday Bavle, Stephan Manthe, Paloma de la Puente, Alejandro Rodriguez-Ramos, Carlos Sampedro, Pascual Campoy [video]\nBackend\nLDSO Direct Sparse Odometry with Loop Closure, Xiang Gao, Rui Wang, Nikolaus Demmel and Daniel Cremers [pdf] [code] [video]\nPredicting Objective Function Change in Pose-Graph Optimization, Fang Bai, Teresa Vidal-Calleja, Shoudong Huang, and Rong Xiong\nScan Similarity-based Pose Graph Construction Method for Graph SLAM,Wonsok Yoo, Hanjun Kim, Hyunki Hong, and Beom H. Lee [pdf]\nSubmap-based Pose-graph Visual SLAM A Robust Visual Exploration and Localization System, Weinan Chen, Lei Zhu, Yisheng Guan, C. Ronald Kube, Hong Zhang [pdf]\nVirtual Occupancy Grid Map for Submap-based Pose Graph SLAM and Planning in 3D Environments, Bing-Jui Ho, Paloma Sodhi, Pedro Teixeira, Ming Hsiao, Tushar Kusnur, and Michael Kaess [pdf]\nFrontend\nA Combined RGB and Depth Deor for SLAM with Humanoids, Rasha Sheikh, Stefan O\u00dfwald and Maren Bennewitz [pdf] [code]\nGood Feature Selection for Least Squares Pose Optimization in VOVSLAM, Yipu Zhao and Patricio A. Vela\nHBST A Hamming Distance embedding Binary Search Tree for Feature-based Visual Place Recognition, Dominik Schlegel and Giorgio Grisetti [pdf] [code]\nKeyframe-based Photometric Online Calibration and Color Correction, Jan Quenzel, Jannis Horn, Sebastian Houben and Sven Behnke [pdf] [video]\nOptimized Contrast Enhancements to Improve Robustness of Visual Tracking in a SLAM Relocalisation Context, Xi Wang, Marc Christie, Eric Marchand [pdf]\nRobust Camera Pose Estimation via Consensus on Ray Bundle and Vector Field, Haoang Li, Ji Zhao, Jean-Charles Bazin, Lei Luo, Junlin Wu and Jian Yao\nUnit Quaternion-based Parameterization for Point Features in Visual Navigation, James Maley and Guoquan Huang\nPerspective Correcting Visual Odometry for Agile MAVs using a Pixel Processor Array, Colin Greatwood, Laurie Bose, Thomas Richardson, Walterio Mayol-Cuevas, Jianing Chen, Stephen J. Carey and Piotr Dudek [pdf] [video]\nDeep SLAM\nA Variational Feature Encoding Method of 3D Object for Probabilistic Semantic SLAM, H. W. Yu and B. H. Lee [pdf]\nBayesian Information Recovery from CNN for Probabilistic Inference, Dmitry Kopitkov and Vadim Indelman [pdf]\nDS-SLAM A Semantic Visual SLAM towards Dynamic Environments, Chao Yu, Zuxin Liu, Xin-Jun Liu, Fugui Xie, Yi Yang, Qi Wei, Qiao Fei [pdf]\nFast and Accurate Semantic Mapping through Geometric-based Incremental Segmentation, Yoshikatsu Nakajima, Keisuke Tateno, Federico Tombari and Hideo Saito [pdf]\nIntegrating Deep Semantic Segmentation into 3D Point Cloud Registration, Anestis Zaganidis, LiSun, Tom Duckett, and Grzegorz Cielniak [pdf]\nLearning monocular visual odometry with dense 3D mapping from dense 3D \ufb02ow, Cheng Zhao, Li Sun, Pulak Purkait, Tom Duckett and Rustam Stolkin [pdf]\nLocalization of Classi\ufb01ed Objects in SLAM using Nonparametric Statistics and Clustering, Asif Iqbal 1 and Nicholas R. Gans\nPose Estimation and Map Formation with Spiking Neural Networks towards Neuromorphic SLAM, Raphaela Kreiser, Panin Pienroj, Alpha Renner, and Yulia Sandamirskaya [pdf]\nRobust Exploration with Multiple Hypothesis Data Association, Jinkun Wang and Brendan Englot [pdf]\nSemantic Monocular SLAM for Highly Dynamic Environments, Nikolas Brasch, Aljaz Bozic, Joe Lallemand, Federico Tombari\nUnsupervised Odometry and Depth Learning for Endoscopic Capsule Robots, Mehmet Turan, Evin Pinar Ornek, Nail Ibrahimli, Can Giracoglu, Yasin Almalioglu,Mehmet Fatih Yanik, and Metin Sitti [pdf]", "link": "https://github.com/mengyuest/iros2018-slam-papers", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "iros2018 slam collections\nintroduction\nthis repository contains slam papers from iros2018. thanks for the efforts from paopaorobot\nreference\n[\u6ce1\u6ce1\u524d\u6cbf\u8ffd\u8e2a]\u8ddf\u8e2aslam\u524d\u6cbf\u52a8\u6001\u7cfb\u5217\u4e4biros2018\nfields\nvisual inertial odometry\na tutorial on quantitative trajectory evaluation for visual(-inertial) odometry, zichao zhang, davide scaramuzza [pdf] [code]\nchallenges in monocular visual odometry photometric calibration, motion bias and rolling shutter effect, nan yang , rui wang, xiang gao, and daniel cremers [pdf]\ncvi-slam \u2013 collaborative visual-inertial slam, marco karrer, patrik schmuck, margarita chli [pdf]\nembedding temporally consistent depth recovery for real-time dense mapping in visual-inertial odometry, hui cheng, zhuoqi zheng, jinhao he, chongyu chen, keze wang, and liang lin\nhybrid contact preintegration for visual-inertial-contact state estimation using factor graphs, ross hartley, maani ghaffari jadidi, lu gan, jiunn-kai huang, jessy w. grizzle, and ryan m. eustice [pdf] [video]\ninformation sparsi\ufb01cation in visual-inertial odometry, jerry hsiung, ming hsiao, eric westman, rafael valencia, and michael kaess [pdf]\nkey-frame strategy during fast image-scale changes and zero motion in vio without persistent features, eren allak, alexander hardt-stremayr, and stephan weiss\non the comparison of gauge freedom handling in optimization-based visual-inertial state estimation, zichao zhang, guillermo gallego, davide scaramuzza [pdf]\nonline temporal calibration for monocular visual-inertial systems, tong qin and shaojie shen [pdf] [code]\nreal-time fully incremental scene understanding on mobile platforms, johanna wald, keisuke tateno, jurgen sturm, nassir navab, and federico tombari [pdf] [project]\nthe tum vi benchmark for evaluating visual-inertial odometry, david schubert, thore goll, nikolaus demmel, vladyslav usenko, jorg stuckler and daniel cremers [pdf] [dataset]\ntrifo-vio: robust and efficient stereo visual inertial odometry using points and lines, feng zheng, grace tsai, zhe zhang, shaoshan liu, chen-chi chu, and hongbing hu [pdf] [dataset]\n\u03c0-soc heterogeneous soc architecture for visual inertial slam applications, jie tang, bo yu, shaoshan liu, zhe zhang, weikang fang, and yanjun zhang\nrobocentric visual-inertial odometry, zheng huai, guoquan huang [pdf]\nrobust visual-inertial state estimation with multiple odometries and efficient mapping on an mav with ultra-wide fov stereo vision, m. g. m\u00fcller, f. steidle, m. j. schuster, p. lutz, m. maier, s. stoneman, t. tomic, w. st\u00fcrzl\nunscented kalman filter on lie groups for visual_inertial odometry, martin brossard, silvere bonnabel, axel barrau [pdf]\nrgbd camera\nperception based locomotion system for a humanoid robot with adaptive footstep compensation under task constraints, iori kumagai, mitsuharu morisawa, shin\u2019ichiro nakaoka, takeshi sakaguchi, hiroshi kaminaga, kenji kaneko, fumio kanehiro\nreal-time 3d reconstruction using a combination of point-based and volumetric fusion , maik keller, damien lefloch, martin lambers, shahram izadi, tim weyrich, andreas kolb [pdf]\nedge-based robust rgb-d visual odometry_using 2-d edge divergence minimization, changhyeon kim, pyojin kim, sangil lee and h. jin kim\nlidar\na maximum likelihood approach to extract polylines from 2-d laser range scans, alexander schaefer, daniel buscher, lukas luft, wolfram burgard [pdf] [code]\na robust pose graph approach for city scale lidar mapping, sheng yang, xiaoling zhu, xing nian, lu feng, xiaozhi qu and teng ma\ndynamic scaling factors of covariances for accurate 3d normal distributions transform registration , hyunki hong and b. h. lee\nintegrating deep semantic segmentation into 3d point cloud registration anestis zaganidis, li sun, tom duckett, grzegorz cielniak [pdf]\nlimo lidar-monocular visual odometry, johannes graeter, alexander wilczynski and martin lauer [pdf] [code]\nlips lidar-inertial 3d plane slam, patrick geneva, kevin eckenhoff, yulin yang and guoquan huang [pdf] [code]\nposemap lifelong, multi-environment 3d lidar localization, philipp egger, paulo v k borges, gavin catt, andreas pfrunder, roland siegwart, renaud dub\u00e9 [pdf] [video1] [video2]\nstabilize an unsupervised feature learning for lidar-based place recognition, peng yin, lingyun xu, zhe liu, lu li, hadi salman, yuqing he, weiliang xu, hesheng wang and howie choset\nstereo camera localization in 3d lidar maps, youngji kim, jinyong jeong and ayoung kim [video]\nlego-loam_ lightweight and ground-optimized_lidar odometry and mapping on variable terrain, tixiao shan and brendan englot [pdf] [code] [video]\nlidar and camera calibration using motions estimated by sensor_fusion odometry, ryoichi ishikawa, takeshi oishi and katsushi ikeuchi [pdf]\nscan context: egocentric spatial descriptor for place recognition within 3d point cloud map, giseop kim and ayoung kim\nevent camera\nasynchronouscornerdetectionandtrackingforeventcamerasinreal-time, ignacio alzugaray and margarita chli [pdf] [video]\nmapping\na b-spline mapping framework for long-term autonomous operations, r\u00f4mulo t. rodrigues, a. pedro aguiar and ant\u00f3nio pascoal [video]\nc-blox a scalable and consistent tsdf-based dense mapping approach, alexander millane, zachary taylor, helen oleynikova, juan nieto, roland siegwart, c\u00e9sar cadena [pdf] [code]\nefficient long-term mapping in dynamic environments, mar\u0131\u0301a t. l\u00e1zaro, roberto capobianco and giorgio grisetti [code]\nhmaps \u2013 hybrid height-voxel maps for environment representation, lu\u0131\u0301s garrote, cristiano premebida, david silva, urbano j. nunes\nhuman-in-the-loop augmented mapping, abbas sidaoui, imad h. elhajj, and daniel asmar\nstructured skip list a compact data structure for 3d reconstruction, shi-jie li, ming-ming cheng, yun liu, shao-ping lu, yahui wang, victor adrian prisacariu [pdf]\nslam system\narthroslam: multi-sensor robust visual localization for minimally invasive orthopedic surgery, andres marmol, peter corke, thierry peynot\ndynaslam tracking, mapping and inpainting in dynamic scenes, berta bescos, jos\u00e9 m. f\u00e1cil, javier civera and jos\u00e9 neira [pdf] [video]\nimproving repeatability of experiments by automatic evaluation of slam algorithms, francesco amigoni, valerio castelli, and matteo luperto [code]\nindoor mapping and localization for pedestrians using opportunistic sensing with smartphones, qing liang, lujia wang, youfu li and ming liu [video]\nmis-slam real-time large scale dense deformable slam system in minimal invasive surgery based on heterogeneous computing, jingwei song, jun wang, liang zhao, shoudong huang and gamini dissanayake [pdf]\nomnidirectional dso direct sparse odometry with fisheye cameras, hidenobu matsuki, lukas von stumberg, vladyslav usenko, j\u00f6rg st\u00fcckler and daniel cremers [pdf] [video]\nopenseqslam2.0 an open source toolbox for visual place recognition under changing conditions, ben talbot, sourav garg, and michael milford [pdf] [code]\nprobabilistic dense reconstruction from a moving camera, yonggen ling, kaixuan wang and shaojie shen [code] [video]\nrobust long-term registration of uav images of crop fields for precision agriculture, nived chebrolu thomas l\u00e4be and cyrill stachniss [pdf]\nsemi-supervised slam leveraging low-cost sensors on underground autonomous vehicles for position tracking, adam jacobson, fan zeng, david smith, nigel boswell, thierry peynot and michael milford\ntowards robust visual odometry with a multi-camera system, peidong liu, marcel geppert, lionel heng, torsten sattler, andreas geiger and marc pollefeys [pdf] [code]\nfast cylinder and plane extraction from depth cameras for visual_odometry, pedro f. proen\u00e7a and yang gao [pdf] [code] [video]\ngeometric-based line segment tracking for hdr stereo sequences, ruben gomez-ojeda, javier gonzalez-jimenez [pdf] [code]\nmultimotion visual odometry (mvo): simultaneous estimation of camera and third-party motions, kevin m. judd and jonathan d. gammell and paul newman [pdf]\nreliable fusion of black-box estimates of underwater localization, hendry ferreira chame, matheus machado dos santos, s\u0131\u0301lvia silva da costa botelho [code]\nstereo visual odometry and semantics based localization of aerial_robots in indoor environments, hriday bavle, stephan manthe, paloma de la puente, alejandro rodriguez-ramos, carlos sampedro, pascual campoy [video]\nbackend\nldso direct sparse odometry with loop closure, xiang gao, rui wang, nikolaus demmel and daniel cremers [pdf] [code] [video]\npredicting objective function change in pose-graph optimization, fang bai, teresa vidal-calleja, shoudong huang, and rong xiong\nscan similarity-based pose graph construction method for graph slam,wonsok yoo, hanjun kim, hyunki hong, and beom h. lee [pdf]\nsubmap-based pose-graph visual slam a robust visual exploration and localization system, weinan chen, lei zhu, yisheng guan, c. ronald kube, hong zhang [pdf]\nvirtual occupancy grid map for submap-based pose graph slam and planning in 3d environments, bing-jui ho, paloma sodhi, pedro teixeira, ming hsiao, tushar kusnur, and michael kaess [pdf]\nfrontend\na combined rgb and depth deor for slam with humanoids, rasha sheikh, stefan o\u00dfwald and maren bennewitz [pdf] [code]\ngood feature selection for least squares pose optimization in vovslam, yipu zhao and patricio a. vela\nhbst a hamming distance embedding binary search -----> tree !!!  for feature-based visual place recognition, dominik schlegel and giorgio grisetti [pdf] [code]\nkeyframe-based photometric online calibration and color correction, jan quenzel, jannis horn, sebastian houben and sven behnke [pdf] [video]\noptimized contrast enhancements to improve robustness of visual tracking in a slam relocalisation context, xi wang, marc christie, eric marchand [pdf]\nrobust camera pose estimation via consensus on ray bundle and vector field, haoang li, ji zhao, jean-charles bazin, lei luo, junlin wu and jian yao\nunit quaternion-based parameterization for point features in visual navigation, james maley and guoquan huang\nperspective correcting visual odometry for agile mavs using a pixel processor array, colin greatwood, laurie bose, thomas richardson, walterio mayol-cuevas, jianing chen, stephen j. carey and piotr dudek [pdf] [video]\ndeep slam\na variational feature encoding method of 3d object for probabilistic semantic slam, h. w. yu and b. h. lee [pdf]\nbayesian information recovery from cnn for probabilistic inference, dmitry kopitkov and vadim indelman [pdf]\nds-slam a semantic visual slam towards dynamic environments, chao yu, zuxin liu, xin-jun liu, fugui xie, yi yang, qi wei, qiao fei [pdf]\nfast and accurate semantic mapping through geometric-based incremental segmentation, yoshikatsu nakajima, keisuke tateno, federico tombari and hideo saito [pdf]\nintegrating deep semantic segmentation into 3d point cloud registration, anestis zaganidis, lisun, tom duckett, and grzegorz cielniak [pdf]\nlearning monocular visual odometry with dense 3d mapping from dense 3d \ufb02ow, cheng zhao, li sun, pulak purkait, tom duckett and rustam stolkin [pdf]\nlocalization of classi\ufb01ed objects in slam using nonparametric statistics and clustering, asif iqbal 1 and nicholas r. gans\npose estimation and map formation with spiking neural networks towards neuromorphic slam, raphaela kreiser, panin pienroj, alpha renner, and yulia sandamirskaya [pdf]\nrobust exploration with multiple hypothesis data association, jinkun wang and brendan englot [pdf]\nsemantic monocular slam for highly dynamic environments, nikolas brasch, aljaz bozic, joe lallemand, federico tombari\nunsupervised odometry and depth learning for endoscopic capsule robots, mehmet turan, evin pinar ornek, nail ibrahimli, can giracoglu, yasin almalioglu,mehmet fatih yanik, and metin sitti [pdf]", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000179, "year": null}, {"Unnamed: 0": 1200, "autor": 180, "date": null, "content": "Py Trees\n[About][Docs & Demos][Releases][Installation][PyTrees-Ros Ecosystem]\nAbout\nPyTrees is a python implementation of behaviour trees designed to facilitate the rapid development of medium sized decision making engines for use in fields like robotics. Brief feature list:\nSequence, Selector, Parallel composites\nBlackboards for data sharing\nPython generators for smarter ticking over the tree graph\nPython decorators for enabling meta behaviours\nRender trees to dot graphs or visualise with ascii graphs on stdout\nDocs and Demos\nCore API documentation (also includes some explanation concerning the demo scripts):\nIf you're really looking for something more edifying than hello world examples, walk through the ros tutorials which incrementally step through the process of building a scenario handling layer for a robot.\nThere are also runtime visualisation tools - refer to the py_trees_ros_viewer/README as an example implementation of the underlying py_trees_js library.\nReleases\n0.y.x - first open source releases\n1.0.x - first stable release\n1.1.x - improvements\n1.2.x - improvements\n2.0.x - blackboards v2 with namespaces, access permissions and key tracking\n2.1.x - Chooser deprecated, api housekeeping\nDevel 2.1.x 2.0.x 1.2.x 0.7.x 0.6.x\nSources\nCompatibility\nCI\nDocumentation\nInstallation\nFrom ppa on Ubuntu/Bionic:\n$ sudo apt install python3-py-trees\nFrom pypi:\n$ pip3 install py_trees\nIn a Python Virtual Environment:\n$ git clone https://github.com/splintered-reality/py_trees\n$ cd py_trees\n$ source ./venv.bash\nBuild your own python3 deb:\n$ git clone https://github.com/splintered-reality/py_trees\n$ cd py_trees\n$ source ./venv.bash\n$ make deb\nFrom the ROS2 ecosystem:\n$ sudo apt install ros-<rosdistro>-py-trees\nPyTrees-ROS Ecosystem\nSee the py_trees_ros README for the latest information on pytrees packages in the ROS ecosystem and their status.", "link": "https://github.com/splintered-reality/py_trees", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "py trees\n[about][docs & demos][releases][installation][pytrees-ros ecosystem]\nabout\npytrees is a python implementation of behaviour trees designed to facilitate the rapid development of medium sized decision making engines for use in fields like robotics. brief feature list:\nsequence, selector, parallel composites\nblackboards for data sharing\npython generators for smarter ticking over the -----> tree !!!  graph\npython decorators for enabling meta behaviours\nrender trees to dot graphs or visualise with ascii graphs on stdout\ndocs and demos\ncore api documentation (also includes some explanation concerning the demo scripts):\nif you're really looking for something more edifying than hello world examples, walk through the ros tutorials which incrementally step through the process of building a scenario handling layer for a robot.\nthere are also runtime visualisation tools - refer to the py_trees_ros_viewer/readme as an example implementation of the underlying py_trees_js library.\nreleases\n0.y.x - first open source releases\n1.0.x - first stable release\n1.1.x - improvements\n1.2.x - improvements\n2.0.x - blackboards v2 with namespaces, access permissions and key tracking\n2.1.x - chooser deprecated, api housekeeping\ndevel 2.1.x 2.0.x 1.2.x 0.7.x 0.6.x\nsources\ncompatibility\nci\ndocumentation\ninstallation\nfrom ppa on ubuntu/bionic:\n$ sudo apt install python3-py-trees\nfrom pypi:\n$ pip3 install py_trees\nin a python virtual environment:\n$ git clone https://github.com/splintered-reality/py_trees\n$ cd py_trees\n$ source ./venv.bash\nbuild your own python3 deb:\n$ git clone https://github.com/splintered-reality/py_trees\n$ cd py_trees\n$ source ./venv.bash\n$ make deb\nfrom the ros2 ecosystem:\n$ sudo apt install ros-<rosdistro>-py-trees\npytrees-ros ecosystem\nsee the py_trees_ros readme for the latest information on pytrees packages in the ros ecosystem and their status.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000180, "year": null}, {"Unnamed: 0": 1213, "autor": 193, "date": null, "content": "Awesome MATLAB and Simulink Robotics\nThis is a list of awesome demos, tutorials, utilities and overall resources for the robotics community that use MATLAB\u00ae and Simulink\u00ae. For more information and to get your projects included in this list, reach out to roboticsarena@mathworks.com\nBy Applications Areas\nGround Vehicles and Mobile Robotics\nManipulation\nLegged Locomotion\nRobot Modeling\nPerception\nMapping, Localization and SLAM\nMotion Planning and Path Planning\nMotion Control\nUnmanned Aerial Vehicles (UAV)\nMarine Robotics & AUV\nAutomated Driving\nBy Common Tools\nSimulators\nROS and Middleware\nHardware and Connectivity\nBy Relevant MATLAB Toolboxes\nBy Applications Areas\nGround Vehicles and Mobile Robotics\nDeveloping Navigation Stacks for Mobile Robots and UGV\nKinematic motion models for simulation\nControl and simulation of warehouse robots\nProgramming of soccer robot behavior (Video)\nSimulation and programming of robot swarm (Video)\nMapping, Localization and SLAM (See Section Below)\nMotion Planning and Path Planning (See Section Below)\nMobile Robotics Simulation Toolbox (Video)\nRobotics Playground (Robotics Education - Video)\nManipulation\nTools for rigid body tree dynamics and analysis\nInverse Kinematics (Blog and GitHub Repo)\nInverse kinematics with spatial constraints\nInteractive Inverse Kinematics\nCollision checking (Self-Collisions, Environment Collisions)\nTrajectory Generation (Blog, GitHub Repo)\nSafe trajectory planning (Impedance based control)\nPick and place workflows (Using Gazebo)\nLegged Locomotion\nModeling and simulation of walking robots (GitHub Repo)\nPattern Generation for Walking Robots (Video)\nLinear Inverted Pendulum Model (LIPM)for humanoid walking (Video)\nDeep Reinforcement Learning for Walking Robots (Video)\nModeling of quadruped robot running (Files)\nQuadruped Robot Locomotion Using DDPG Agent\nRobot Modeling\nSimscape\u2122 Tools for Modeling and Simulation of Physical Systems\nSimulate Manipulator Actuators and Tune Control Parameters\nAlgorithm Verification Using Robot Models\nImport Robots to MATLAB from URDF Files\nImport Robots from CAD and URDF Files\nPerception\nDeep Learning and Machine Learning\nLidar and 3D Point Cloud Processing\n3D Vision and Stereo Vision\nFeature Detection, Extraction, and Matching\nObject Tracking and Motion Estimation\nOrientation Estimation from Inertial Sensors\nDrift Reduction for Visual Odometry\nMapping, Localization and SLAM\n2D Lidar SLAM Implementations (Offline, Online)\n3D Lidar SLAM Implementation\nSLAM Map Builder Application\nOccupancy Grid Utilities\nMonte Carlo Localization\nEgo-Centric (Near Field) Occupancy Maps\nMotion Planning and Path Planning\nMotion Planners (RRT, PRM, Hybrid A*)\nRRT Planners for Manipulators\nRRT Planners for Mobile Robots\nPath Planning Using Probabilistic Road Maps\nPath Following with Obstacle Avoidance\nDynamic Re-planning of Paths\nChoosing Path Planning Algorithms\nMotion Control\nObstacle Avoidance for Mobile Robots Using Reinforcement Learning\nDeep Reinforcement Learning for Walking Robots (Video)\nModel Predictive Control for collision-free manipulation trajectories\nModel Predictive Control for holonomic robot navigation\nMulti-Loop PI Control Tuning for Robotic Arm Actuators\nUnmanned Aerial Vehicles (UAV)\nSimulation Library for Fixed-Wing and Multi-Rotor UAVs\nTune Waypoint Follower for Fixed-Wing UAV\nApproximate High-Fidelity UAV models\nLoad and Playback MAVLink TLOG\nUse a MAVLink Parameter Protocol for Tuning UAV Parameters in MATLAB\nSupport for Parrot Drones\nSupport for PX4 Autopilots\nMarine Robotics & AUV\nAUV Development with MATLAB and Simulink (Webinar, Videos, GitHub)\nEstimating Direction of Arrival for AUV\u2019s\nSystem Identification for Blue Robotics Thrusters\nLQR Control of an AUV\nDynamics and Control of AUV\u2019s (GitHub Repo)\nModeling Robotic Boats in Simulink\nSimulation and Control of RobotX Challenge WAM-V Boats\nAutomated Driving\nLane Following Control with Sensor Fusion and Lane Detection\nAutomate Ground Truth Labeling for Semantic Segmentation\nTrack Vehicles Using Lidar: From Point Cloud to Track List\nTrack-Level Fusion of Radar and Lidar Data\nVisualize Automated Parking Valet Using 3D Simulation\nDesign Lidar SLAM Algorithm Using 3D Simulation Environment\nImplementing an Adaptive Cruise Controller with Simulink\nBy Common Tools\nSimulators\nROS \ufffd Based Simulators (See Section Below)\nGazebo Co-Simulation\nUNREAL-Engine-Based Scenarios for Automated Driving\nMobile Robotics Simulation Toolbox (GitHub Repo)\nRobotics Playground (GitHub Repo)\nROS and Middleware\nGetting Started with MATLAB, Simulink and ROS\nMATLAB support for ROS and ROS 2\nSimulink Support for ROS and ROS 2\nSupport for ROS Custom Messages\nAutomatic ROS Node Generation from Simulink\nROS Node Generation for Raspberry Pi\nHardware and Connectivity\nAny Robot Running ROS (See ROS Section)\nRobotics System Toolbox Support for Manipulators\nToyota HSR Examples\nTurtleBot Robots\nVEX Robotics\nRaspberry Pi\nBeagleBone Blue\nLEGO Mindstorms\nMATLAB and Simulink Hardware Support Packages\nBy Relevant MATLAB Toolboxes\nRobotics System Toolbox\u2122\nROS Toolbox\nNavigation Toolbox\u2122\nSensor Fusion and Tracking Toolbox\u2122\nComputer Vision Toolbox\u2122\nAutomated Driving Toolbox\u2122\nRoadRunner\nDeep Learning Toolbox\u2122\nReinforcement Learning Toolbox\u2122\nControl System Toolbox\u2122\nSimscape\u2122\nCopyright 2020 The MathWorks, Inc", "link": "https://github.com/mathworks-robotics/awesome-matlab-robotics", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "awesome matlab and simulink robotics\nthis is a list of awesome demos, tutorials, utilities and overall resources for the robotics community that use matlab\u00ae and simulink\u00ae. for more information and to get your projects included in this list, reach out to roboticsarena@mathworks.com\nby applications areas\nground vehicles and mobile robotics\nmanipulation\nlegged locomotion\nrobot modeling\nperception\nmapping, localization and slam\nmotion planning and path planning\nmotion control\nunmanned aerial vehicles (uav)\nmarine robotics & auv\nautomated driving\nby common tools\nsimulators\nros and middleware\nhardware and connectivity\nby relevant matlab toolboxes\nby applications areas\nground vehicles and mobile robotics\ndeveloping navigation stacks for mobile robots and ugv\nkinematic motion models for simulation\ncontrol and simulation of warehouse robots\nprogramming of soccer robot behavior (video)\nsimulation and programming of robot swarm (video)\nmapping, localization and slam (see section below)\nmotion planning and path planning (see section below)\nmobile robotics simulation toolbox (video)\nrobotics playground (robotics education - video)\nmanipulation\ntools for rigid body -----> tree !!!  dynamics and analysis\ninverse kinematics (blog and github repo)\ninverse kinematics with spatial constraints\ninteractive inverse kinematics\ncollision checking (self-collisions, environment collisions)\ntrajectory generation (blog, github repo)\nsafe trajectory planning (impedance based control)\npick and place workflows (using gazebo)\nlegged locomotion\nmodeling and simulation of walking robots (github repo)\npattern generation for walking robots (video)\nlinear inverted pendulum model (lipm)for humanoid walking (video)\ndeep reinforcement learning for walking robots (video)\nmodeling of quadruped robot running (files)\nquadruped robot locomotion using ddpg agent\nrobot modeling\nsimscape\u2122 tools for modeling and simulation of physical systems\nsimulate manipulator actuators and tune control parameters\nalgorithm verification using robot models\nimport robots to matlab from urdf files\nimport robots from cad and urdf files\nperception\ndeep learning and machine learning\nlidar and 3d point cloud processing\n3d vision and stereo vision\nfeature detection, extraction, and matching\nobject tracking and motion estimation\norientation estimation from inertial sensors\ndrift reduction for visual odometry\nmapping, localization and slam\n2d lidar slam implementations (offline, online)\n3d lidar slam implementation\nslam map builder application\noccupancy grid utilities\nmonte carlo localization\nego-centric (near field) occupancy maps\nmotion planning and path planning\nmotion planners (rrt, prm, hybrid a*)\nrrt planners for manipulators\nrrt planners for mobile robots\npath planning using probabilistic road maps\npath following with obstacle avoidance\ndynamic re-planning of paths\nchoosing path planning algorithms\nmotion control\nobstacle avoidance for mobile robots using reinforcement learning\ndeep reinforcement learning for walking robots (video)\nmodel predictive control for collision-free manipulation trajectories\nmodel predictive control for holonomic robot navigation\nmulti-loop pi control tuning for robotic arm actuators\nunmanned aerial vehicles (uav)\nsimulation library for fixed-wing and multi-rotor uavs\ntune waypoint follower for fixed-wing uav\napproximate high-fidelity uav models\nload and playback mavlink tlog\nuse a mavlink parameter protocol for tuning uav parameters in matlab\nsupport for parrot drones\nsupport for px4 autopilots\nmarine robotics & auv\nauv development with matlab and simulink (webinar, videos, github)\nestimating direction of arrival for auv\u2019s\nsystem identification for blue robotics thrusters\nlqr control of an auv\ndynamics and control of auv\u2019s (github repo)\nmodeling robotic boats in simulink\nsimulation and control of robotx challenge wam-v boats\nautomated driving\nlane following control with sensor fusion and lane detection\nautomate ground truth labeling for semantic segmentation\ntrack vehicles using lidar: from point cloud to track list\ntrack-level fusion of radar and lidar data\nvisualize automated parking valet using 3d simulation\ndesign lidar slam algorithm using 3d simulation environment\nimplementing an adaptive cruise controller with simulink\nby common tools\nsimulators\nros \ufffd based simulators (see section below)\ngazebo co-simulation\nunreal-engine-based scenarios for automated driving\nmobile robotics simulation toolbox (github repo)\nrobotics playground (github repo)\nros and middleware\ngetting started with matlab, simulink and ros\nmatlab support for ros and ros 2\nsimulink support for ros and ros 2\nsupport for ros custom messages\nautomatic ros node generation from simulink\nros node generation for raspberry pi\nhardware and connectivity\nany robot running ros (see ros section)\nrobotics system toolbox support for manipulators\ntoyota hsr examples\nturtlebot robots\nvex robotics\nraspberry pi\nbeaglebone blue\nlego mindstorms\nmatlab and simulink hardware support packages\nby relevant matlab toolboxes\nrobotics system toolbox\u2122\nros toolbox\nnavigation toolbox\u2122\nsensor fusion and tracking toolbox\u2122\ncomputer vision toolbox\u2122\nautomated driving toolbox\u2122\nroadrunner\ndeep learning toolbox\u2122\nreinforcement learning toolbox\u2122\ncontrol system toolbox\u2122\nsimscape\u2122\ncopyright 2020 the mathworks, inc", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000193, "year": null}, {"Unnamed: 0": 1216, "autor": 196, "date": null, "content": "RRT\nC++ RRT (Rapidly-exploring Random Tree) implementation\nInteractive RRT Viewer\nThis project contains an interactive RRT viewer. The source and destination points can be dragged with a mouse. Here's a screenshot:\nDependencies\nThe following are required in order to build this project:\ncmake >= 3.2.0\nQt 5.5+\n(The following dependencies are only needed for the rrt viewer)\nQtDeclarative 5\nQtQuick2.5+\nQtQuick Controls\nQtQuick Dialogs\nEigen\nninja\nccache\nlibflann-dev\nlibboost-all-dev\nTo install all needed dependencies on Ubuntu 16.04, run:\nsudo apt-get -y install qt5-default libeigen3-dev g++ ninja-build cmake clang-format-3.6 ccache libflann-dev qtdeclarative5-dev qtdeclarative5-qtquick2-plugin qml-module-qtquick-{controls,dialogs} libboost-all-dev\nBuilding\nRun make in the main directory to build the rrt-viewer program and the test-runner program, which are placed in the build/ folder.\ngit submodule update --init\nmake\nbuild/rrt-viewer\nResources\nHere are some good resources for learning more about RRTs:\nhttp://msl.cs.uiuc.edu/rrt/\nWikipedia\nhttp://www.cs.cmu.edu/~15780/readings/02iros-errt.pdf\nLicense\nThis project is licensed under the Apache License v2.0. See the LICENSE file for more information.", "link": "https://github.com/RoboJackets/rrt", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "rrt\nc++ rrt (rapidly-exploring random -----> tree !!! ) implementation\ninteractive rrt viewer\nthis project contains an interactive rrt viewer. the source and destination points can be dragged with a mouse. here's a screenshot:\ndependencies\nthe following are required in order to build this project:\ncmake >= 3.2.0\nqt 5.5+\n(the following dependencies are only needed for the rrt viewer)\nqtdeclarative 5\nqtquick2.5+\nqtquick controls\nqtquick dialogs\neigen\nninja\nccache\nlibflann-dev\nlibboost-all-dev\nto install all needed dependencies on ubuntu 16.04, run:\nsudo apt-get -y install qt5-default libeigen3-dev g++ ninja-build cmake clang-format-3.6 ccache libflann-dev qtdeclarative5-dev qtdeclarative5-qtquick2-plugin qml-module-qtquick-{controls,dialogs} libboost-all-dev\nbuilding\nrun make in the main directory to build the rrt-viewer program and the test-runner program, which are placed in the build/ folder.\ngit submodule update --init\nmake\nbuild/rrt-viewer\nresources\nhere are some good resources for learning more about rrts:\nhttp://msl.cs.uiuc.edu/rrt/\nwikipedia\nhttp://www.cs.cmu.edu/~15780/readings/02iros-errt.pdf\nlicense\nthis project is licensed under the apache license v2.0. see the license file for more information.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000196, "year": null}, {"Unnamed: 0": 1222, "autor": 202, "date": null, "content": "Compact UAVCAN/CAN v1 in C\nLibcanard is a compact implementation of the UAVCAN/CAN protocol stack in C99/C11 for high-integrity real-time embedded systems.\nUAVCAN is an open lightweight data bus standard designed for reliable intravehicular communication in aerospace and robotic applications via CAN bus, Ethernet, and other robust transports. The acronym UAVCAN stands for Uncomplicated Application-level Vehicular Computing And Networking.\nRead the docs in libcanard/canard.h.\nFind examples, starters, tutorials on the UAVCAN forum.\nIf you want to contribute, please read CONTRIBUTING.md.\nFeatures\nFull test coverage and extensive static analysis.\nCompliance with automatically enforceable MISRA C rules (reach out to https://forum.uavcan.org for details).\nDetailed time complexity and memory requirement models for the benefit of real-time high-integrity applications.\nPurely reactive API without the need for background servicing.\nSupport for the Classic CAN and CAN FD.\nSupport for redundant transports.\nCompatibility with 8/16/32/64-bit platforms.\nCompatibility with extremely resource-constrained baremetal environments starting from 32K ROM and 8K RAM.\nImplemented in \u22481000 lines of code.\nPlatforms\nThe library is designed to be usable out of the box with any conventional 8/16/32/64-bit platform, including deeply embedded baremetal platforms, as long as there is a standard-compliant compiler available. The platform-specific media IO layer (driver) is supposed to be provided by the application:\n+---------------------------------+\n| Application |\n+-------+-----------------+-------+\n| |\n+-------+-------+ +-------+-------+\n| Libcanard | | Media layer |\n+---------------+ +-------+-------+\n|\n+-------+-------+\n| Hardware |\n+---------------+\nThe UAVCAN Development Team maintains a collection of various platform-specific components in a separate repository at https://github.com/UAVCAN/platform_specific_components. Users are encouraged to search through that repository for drivers, examples, and other pieces that may be reused in the target application to speed up the design of the media IO layer (driver) for the application.\nExample\nThe example augments the documentation but does not replace it.\nThe library requires a constant-complexity deterministic dynamic memory allocator. We could use the standard C heap, but most implementations are not constant-complexity, so let's suppose that we're using O1Heap instead. We are going to need basic wrappers:\nstatic void* memAllocate(CanardInstance* const canard, const size_t amount)\n{\n(void) canard;\nreturn o1heapAllocate(my_allocator, amount);\n}\nstatic void memFree(CanardInstance* const canard, void* const pointer)\n{\n(void) canard;\no1heapFree(my_allocator, pointer);\n}\nInit a library instance:\nCanardInstance canard = canardInit(&memAllocate, &memFree);\ncanard.node_id = 42; // Defaults to anonymous; can be set up later at any point.\nIn order to be able to send transfers over the network, we will need one transmission queue per redundant CAN interface:\nCanardTxQueue queue = canardTxInit(100, // Limit the size of the queue at 100 frames.\nCANARD_MTU_CAN_FD); // Set MTU = 64 bytes. There is also CANARD_MTU_CAN_CLASSIC.\nPublish a message (message serialization not shown):\nstatic uint8_t my_message_transfer_id; // Must be static or heap-allocated to retain state between calls.\nconst CanardTransferMetadata transfer_metadata = {\n.priority = CanardPriorityNominal,\n.transfer_kind = CanardTransferKindMessage,\n.port_id = 1234, // This is the subject-ID.\n.remote_node_id = CANARD_NODE_ID_UNSET, // Messages cannot be unicast, so use UNSET.\n.transfer_id = my_message_transfer_id,\n};\n++my_message_transfer_id; // The transfer-ID shall be incremented after every transmission on this subject.\nint32_t result = canardTxPush(&queue, // Call this once per redundant CAN interface (queue).\n&canard,\ntx_deadline_usec, // Zero if transmission deadline is not limited.\n&transfer_metadata,\n47, // Size of the message payload (see Nunavut transpiler).\n\"\\x2D\\x00\" \"Sancho, it strikes me thou art in great fear.\");\nif (result < 0)\n{\n// An error has occurred: either an argument is invalid, the TX queue is full, or we've run out of memory.\n// It is possible to statically prove that an out-of-memory will never occur for a given application if the\n// heap is sized correctly; for background, refer to the Robson's Proof and the documentation for O1Heap.\n}\nUse Nunavut to automatically generate (de)serialization code from DSDL definitions.\nThe CAN frames generated from the message transfer are now stored in the queue. We need to pick them out one by one and have them transmitted. Normally, the following fragment should be invoked periodically to unload the CAN frames from the prioritized transmission queue into the CAN driver (or several, if redundant interfaces are used):\nfor (const CanardTxQueueItem* ti = NULL; (ti = canardTxPeek(&queue)) != NULL;) // Peek at the top of the queue.\n{\nif ((0U == ti->tx_deadline_usec) || (ti->tx_deadline_usec > getCurrentMicroseconds())) // Check the deadline.\n{\nif (!pleaseTransmit(ti)) // Send the frame over this redundant CAN iface.\n{\nbreak; // If the driver is busy, break and retry later.\n}\n}\n// After the frame is transmitted or if it has timed out while waiting, pop it from the queue and deallocate:\ncanard.memory_free(&canard, canardTxPop(&queue, ti));\n}\nTransfer reception is done by feeding frames into the transfer reassembly state machine from any of the redundant interfaces. But first, we need to subscribe:\nCanardRxSubscription heartbeat_subscription;\n(void) canardRxSubscribe(&canard, // Subscribe to messages uavcan.node.Heartbeat.\nCanardTransferKindMessage,\n7509, // The fixed Subject-ID of the Heartbeat message type (see DSDL definition).\n16, // The extent (the maximum possible payload size) provided by Nunavut.\nCANARD_DEFAULT_TRANSFER_ID_TIMEOUT_USEC,\n&heartbeat_subscription);\nCanardRxSubscription my_service_subscription;\n(void) canardRxSubscribe(&canard, // Subscribe to an arbitrary service response.\nCanardTransferKindResponse, // Specify that we want service responses, not requests.\n123, // The Service-ID whose responses we will receive.\n1024, // The extent (see above).\nCANARD_DEFAULT_TRANSFER_ID_TIMEOUT_USEC,\n&my_service_subscription);\nThe \"extent\" refers to the minimum amount of memory required to hold any serialized representation of any compatible version of the data type; or, in other words, it is the maximum possible size of received objects. This parameter is determined by the data type author at the data type definition time. It is typically larger than the maximum object size in order to allow the data type author to introduce more fields in the future versions of the type; for example, MyMessage.1.0 may have the maximum size of 100 bytes and the extent 200 bytes; a revised version MyMessage.1.1 may have the maximum size anywhere between 0 and 200 bytes. Extent values are provided per data type by DSDL transcompilers such as Nunavut.\nIn Libcanard we use the term \"subscription\" not only for subjects (messages), but also for services, for simplicity.\nWe can subscribe and unsubscribe at runtime as many times as we want. Normally, however, an embedded application would subscribe once and roll with it. Okay, this is how we receive transfers:\nCanardRxTransfer transfer;\nconst int8_t result = canardRxAccept(&canard,\nrx_timestamp_usec, // When the frame was received, in microseconds.\n&received_frame, // The CAN frame received from the bus.\nredundant_interface_index, // If the transport is not redundant, use 0.\n&transfer,\nNULL);\nif (result < 0)\n{\n// An error has occurred: either an argument is invalid or we've ran out of memory.\n// It is possible to statically prove that an out-of-memory will never occur for a given application if\n// the heap is sized correctly; for background, refer to the Robson's Proof and the documentation for O1Heap.\n// Reception of an invalid frame is NOT an error.\n}\nelse if (result == 1)\n{\nprocessReceivedTransfer(redundant_interface_index, &transfer); // A transfer has been received, process it.\ncanard.memory_free(&canard, transfer.payload); // Deallocate the dynamic memory afterwards.\n}\nelse\n{\n// Nothing to do.\n// The received frame is either invalid or it's a non-last frame of a multi-frame transfer.\n// Reception of an invalid frame is NOT reported as an error because it is not an error.\n}\nA simple API for generating CAN hardware acceptance filter configurations is also provided. Acceptance filters are generated in an extended 29-bit ID + mask scheme and can be used to minimize the number of irrelevant transfers processed in software.\n// Generate an acceptance filter to receive only uavcan.node.Heartbeat.1.0 messages (fixed port-ID 7509):\nCanardFilter heartbeat_config = canardMakeFilterForSubject(7509);\n// And to receive only uavcan.register.Access.1.0 service transfers (fixed port-ID 384):\nCanardFilter register_access_config = canardMakeFilterForService(384, ins.node_id);\n// You can also combine the two filter configurations into one (may also accept irrelevant messages).\n// This allows consolidating a large set of configurations to fit the number of hardware filters.\n// For more information on the optimal subset of configurations to consolidate to minimize wasted CPU,\n// see the UAVCAN specification.\nCanardFilter combined_config =\ncanardConsolidateFilters(&heartbeat_config, &register_access_config);\nconfigureHardwareFilters(combined_config.extended_can_id, combined_config.extended_mask);\nFull API specification is available in the documentation. If you find the examples to be unclear or incorrect, please, open a ticket.\nRevisions\nv2.0\nDedicated transmission queues per redundant CAN interface with depth limits. The application is now expected to instantiate CanardTxQueue (or several in case of redundant transport) manually.\nReplace O(n) linked lists with fast O(log n) AVL trees (Cavl library is distributed with libcanard). Traversing the list of RX subscriptions now requires recursive traversal of the tree.\nManual DSDL serialization helpers removed; use Nunavut instead.\nReplace bitwise CRC computation with much faster static table by default (#185). This can be disabled by setting CANARD_CRC_TABLE=0, which is expected to save ca. 500 bytes of ROM.\nFixed issues with const-correctness in the API (#175).\ncanardRxAccept2() renamed to canardRxAccept().\nSupport build configuration headers via CANARD_CONFIG_HEADER.\nAdd API for generating CAN hardware acceptance filter configurations (#169).\nv1.1\nAdd new API function canardRxAccept2(), deprecate canardRxAccept().\nProvide user references in CanardRxSubscription.\nPromote certain internal fields to the public API to allow introspection.\nv1.0\nThe initial release.", "link": "https://github.com/UAVCAN/libcanard", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "compact uavcan/can v1 in c\nlibcanard is a compact implementation of the uavcan/can protocol stack in c99/c11 for high-integrity real-time embedded systems.\nuavcan is an open lightweight data bus standard designed for reliable intravehicular communication in aerospace and robotic applications via can bus, ethernet, and other robust transports. the acronym uavcan stands for uncomplicated application-level vehicular computing and networking.\nread the docs in libcanard/canard.h.\nfind examples, starters, tutorials on the uavcan forum.\nif you want to contribute, please read contributing.md.\nfeatures\nfull test coverage and extensive static analysis.\ncompliance with automatically enforceable misra c rules (reach out to https://forum.uavcan.org for details).\ndetailed time complexity and memory requirement models for the benefit of real-time high-integrity applications.\npurely reactive api without the need for background servicing.\nsupport for the classic can and can fd.\nsupport for redundant transports.\ncompatibility with 8/16/32/64-bit platforms.\ncompatibility with extremely resource-constrained baremetal environments starting from 32k rom and 8k ram.\nimplemented in \u22481000 lines of code.\nplatforms\nthe library is designed to be usable out of the box with any conventional 8/16/32/64-bit platform, including deeply embedded baremetal platforms, as long as there is a standard-compliant compiler available. the platform-specific media io layer (driver) is supposed to be provided by the application:\n+---------------------------------+\n| application |\n+-------+-----------------+-------+\n| |\n+-------+-------+ +-------+-------+\n| libcanard | | media layer |\n+---------------+ +-------+-------+\n|\n+-------+-------+\n| hardware |\n+---------------+\nthe uavcan development team maintains a collection of various platform-specific components in a separate repository at https://github.com/uavcan/platform_specific_components. users are encouraged to search through that repository for drivers, examples, and other pieces that may be reused in the target application to speed up the design of the media io layer (driver) for the application.\nexample\nthe example augments the documentation but does not replace it.\nthe library requires a constant-complexity deterministic dynamic memory allocator. we could use the standard c heap, but most implementations are not constant-complexity, so let's suppose that we're using o1heap instead. we are going to need basic wrappers:\nstatic void* memallocate(canardinstance* const canard, const size_t amount)\n{\n(void) canard;\nreturn o1heapallocate(my_allocator, amount);\n}\nstatic void memfree(canardinstance* const canard, void* const pointer)\n{\n(void) canard;\no1heapfree(my_allocator, pointer);\n}\ninit a library instance:\ncanardinstance canard = canardinit(&memallocate, &memfree);\ncanard.node_id = 42; // defaults to anonymous; can be set up later at any point.\nin order to be able to send transfers over the network, we will need one transmission queue per redundant can interface:\ncanardtxqueue queue = canardtxinit(100, // limit the size of the queue at 100 frames.\ncanard_mtu_can_fd); // set mtu = 64 bytes. there is also canard_mtu_can_classic.\npublish a message (message serialization not shown):\nstatic uint8_t my_message_transfer_id; // must be static or heap-allocated to retain state between calls.\nconst canardtransfermetadata transfer_metadata = {\n.priority = canardprioritynominal,\n.transfer_kind = canardtransferkindmessage,\n.port_id = 1234, // this is the subject-id.\n.remote_node_id = canard_node_id_unset, // messages cannot be unicast, so use unset.\n.transfer_id = my_message_transfer_id,\n};\n++my_message_transfer_id; // the transfer-id shall be incremented after every transmission on this subject.\nint32_t result = canardtxpush(&queue, // call this once per redundant can interface (queue).\n&canard,\ntx_deadline_usec, // zero if transmission deadline is not limited.\n&transfer_metadata,\n47, // size of the message payload (see nunavut transpiler).\n\"\\x2d\\x00\" \"sancho, it strikes me thou art in great fear.\");\nif (result < 0)\n{\n// an error has occurred: either an argument is invalid, the tx queue is full, or we've run out of memory.\n// it is possible to statically prove that an out-of-memory will never occur for a given application if the\n// heap is sized correctly; for background, refer to the robson's proof and the documentation for o1heap.\n}\nuse nunavut to automatically generate (de)serialization code from dsdl definitions.\nthe can frames generated from the message transfer are now stored in the queue. we need to pick them out one by one and have them transmitted. normally, the following fragment should be invoked periodically to unload the can frames from the prioritized transmission queue into the can driver (or several, if redundant interfaces are used):\nfor (const canardtxqueueitem* ti = null; (ti = canardtxpeek(&queue)) != null;) // peek at the top of the queue.\n{\nif ((0u == ti->tx_deadline_usec) || (ti->tx_deadline_usec > getcurrentmicroseconds())) // check the deadline.\n{\nif (!pleasetransmit(ti)) // send the frame over this redundant can iface.\n{\nbreak; // if the driver is busy, break and retry later.\n}\n}\n// after the frame is transmitted or if it has timed out while waiting, pop it from the queue and deallocate:\ncanard.memory_free(&canard, canardtxpop(&queue, ti));\n}\ntransfer reception is done by feeding frames into the transfer reassembly state machine from any of the redundant interfaces. but first, we need to subscribe:\ncanardrxsubscription heartbeat_subscription;\n(void) canardrxsubscribe(&canard, // subscribe to messages uavcan.node.heartbeat.\ncanardtransferkindmessage,\n7509, // the fixed subject-id of the heartbeat message type (see dsdl definition).\n16, // the extent (the maximum possible payload size) provided by nunavut.\ncanard_default_transfer_id_timeout_usec,\n&heartbeat_subscription);\ncanardrxsubscription my_service_subscription;\n(void) canardrxsubscribe(&canard, // subscribe to an arbitrary service response.\ncanardtransferkindresponse, // specify that we want service responses, not requests.\n123, // the service-id whose responses we will receive.\n1024, // the extent (see above).\ncanard_default_transfer_id_timeout_usec,\n&my_service_subscription);\nthe \"extent\" refers to the minimum amount of memory required to hold any serialized representation of any compatible version of the data type; or, in other words, it is the maximum possible size of received objects. this parameter is determined by the data type author at the data type definition time. it is typically larger than the maximum object size in order to allow the data type author to introduce more fields in the future versions of the type; for example, mymessage.1.0 may have the maximum size of 100 bytes and the extent 200 bytes; a revised version mymessage.1.1 may have the maximum size anywhere between 0 and 200 bytes. extent values are provided per data type by dsdl transcompilers such as nunavut.\nin libcanard we use the term \"subscription\" not only for subjects (messages), but also for services, for simplicity.\nwe can subscribe and unsubscribe at runtime as many times as we want. normally, however, an embedded application would subscribe once and roll with it. okay, this is how we receive transfers:\ncanardrxtransfer transfer;\nconst int8_t result = canardrxaccept(&canard,\nrx_timestamp_usec, // when the frame was received, in microseconds.\n&received_frame, // the can frame received from the bus.\nredundant_interface_index, // if the transport is not redundant, use 0.\n&transfer,\nnull);\nif (result < 0)\n{\n// an error has occurred: either an argument is invalid or we've ran out of memory.\n// it is possible to statically prove that an out-of-memory will never occur for a given application if\n// the heap is sized correctly; for background, refer to the robson's proof and the documentation for o1heap.\n// reception of an invalid frame is not an error.\n}\nelse if (result == 1)\n{\nprocessreceivedtransfer(redundant_interface_index, &transfer); // a transfer has been received, process it.\ncanard.memory_free(&canard, transfer.payload); // deallocate the dynamic memory afterwards.\n}\nelse\n{\n// nothing to do.\n// the received frame is either invalid or it's a non-last frame of a multi-frame transfer.\n// reception of an invalid frame is not reported as an error because it is not an error.\n}\na simple api for generating can hardware acceptance filter configurations is also provided. acceptance filters are generated in an extended 29-bit id + mask scheme and can be used to minimize the number of irrelevant transfers processed in software.\n// generate an acceptance filter to receive only uavcan.node.heartbeat.1.0 messages (fixed port-id 7509):\ncanardfilter heartbeat_config = canardmakefilterforsubject(7509);\n// and to receive only uavcan.register.access.1.0 service transfers (fixed port-id 384):\ncanardfilter register_access_config = canardmakefilterforservice(384, ins.node_id);\n// you can also combine the two filter configurations into one (may also accept irrelevant messages).\n// this allows consolidating a large set of configurations to fit the number of hardware filters.\n// for more information on the optimal subset of configurations to consolidate to minimize wasted cpu,\n// see the uavcan specification.\ncanardfilter combined_config =\ncanardconsolidatefilters(&heartbeat_config, &register_access_config);\nconfigurehardwarefilters(combined_config.extended_can_id, combined_config.extended_mask);\nfull api specification is available in the documentation. if you find the examples to be unclear or incorrect, please, open a ticket.\nrevisions\nv2.0\ndedicated transmission queues per redundant can interface with depth limits. the application is now expected to instantiate canardtxqueue (or several in case of redundant transport) manually.\nreplace o(n) linked lists with fast o(log n) avl trees (cavl library is distributed with libcanard). traversing the list of rx subscriptions now requires recursive traversal of the -----> tree !!! .\nmanual dsdl serialization helpers removed; use nunavut instead.\nreplace bitwise crc computation with much faster static table by default (#185). this can be disabled by setting canard_crc_table=0, which is expected to save ca. 500 bytes of rom.\nfixed issues with const-correctness in the api (#175).\ncanardrxaccept2() renamed to canardrxaccept().\nsupport build configuration headers via canard_config_header.\nadd api for generating can hardware acceptance filter configurations (#169).\nv1.1\nadd new api function canardrxaccept2(), deprecate canardrxaccept().\nprovide user references in canardrxsubscription.\npromote certain internal fields to the public api to allow introspection.\nv1.0\nthe initial release.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000202, "year": null}, {"Unnamed: 0": 1289, "autor": 269, "date": null, "content": "RoboDK API\nThis repository is the implementation of the RoboDK API in different programming languages for simulation and offline programming. The RoboDK API allows you to program any industrial robot arm using your preferred programming language.\nThe RoboDK API is available in the following programming languages:\nPython\nC#\nMatlab\nC++\nVisual Basic\nThe RoboDK API allows creating simulations for industrial robots, specific mechanisms and generating vendor-specific programs for robots. While RoboDK's graphical user interface can be used to create programs, it is possible to extend the robot controller limitations by using a universal programming language such as Python.\nWith the RoboDK API it is possible to simulate and program any industrial robot using your preferred programming language. This avoids using vendor-specific programming languages.\nEach package implements the following modules/classes:\nThe robolink module is the link to RoboDK.\nThe Item module: any item from the RoboDK item tree can be retrieved. Items are represented by the object Item (RoboDK.Item). An item can be a robot, a reference frame, a tool, an object or a specific project.\nThe robodk module includes a robotics toolbox for pose operations. This toolbox is inspired from Peter Corke's Robotics Toolbox.\nThe following website provides an overview of the RoboDK API: https://robodk.com/offline-programming\nRoboDK can be used for a wide range of applications, such as 3D printing, robot machining, synchronizing multiple robots, pick and place...\nIndustrial Robot application examples\nRoboDK Blog\nImportant: The RoboDK API is not the same as the RoboDK Plug-In interface.\nRequirements\nRoboDK must be installed:\nRoboDK Simulation Software: https://robodk.com/download\nThe RoboDK API can be used with a free RoboDK license.\nDocumentation\nIntroduction to the RoboDK API\nRoboDK API reference (based on Python)\nIntroduction to RoboDK for robot simulation and offline programming\nC++ version of the API\nC# version of the API (NuGet)\nPlugIn interface (C++)\nExample\nThe following script (Python) shows an example that uses the RoboDK API for robot simulation and offline programming::\nfrom robolink import * # RoboDK's API\nfrom robodk import * # Math toolbox for robots\n# Start the RoboDK API:\nRDK = Robolink()\n# Get the robot item by name:\nrobot = RDK.Item('Fanuc LR Mate 200iD', ITEM_TYPE_ROBOT)\n# Get the reference target by name:\ntarget = RDK.Item('Target 1')\ntarget_pose = target.Pose()\nxyz_ref = target_pose.Pos()\n# Move the robot to the reference point:\nrobot.MoveJ(target)\n# Draw a hexagon around the reference target:\nfor i in range(7):\nang = i*2*pi/6 #ang = 0, 60, 120, ..., 360\n# Calculate the new position around the reference:\nx = xyz_ref[0] + R*cos(ang) # new X coordinate\ny = xyz_ref[1] + R*sin(ang) # new Y coordinate\nz = xyz_ref[2] # new Z coordinate\ntarget_pos.setPos([x,y,z])\n# Move to the new target:\nrobot.MoveL(target_pos)\n# Trigger a program call at the end of the movement\nrobot.RunCode('Program_Done')\n# Move back to the reference target:\nrobot.MoveL(target)\nThe same script used for simulation can be used for offline programming, this means that the corresponding program can be generated for the robot controller. RoboDK supports a large number of robot controllers and it is easy to include compatibility for new robot controllers using Post Processors.\nFor more Examples:\nhttps://robodk.com/doc/en/PythonAPI/examples.html\nFor more information about robot post processors:\nQuick introduction to RoboDK post processors\nHow to use Post Processors\nTechnical Reference\nSupported robots\nThe following list includes the robot controllers supported by RoboDK:\nABB RAPID IRC5: for ABB IRC5 robot controllers\nABB RAPID S4C: for ABB S4C robot controllers\nAdept Vplus: for Adept V+ programming language\nAllen Bradley Logix5000: for Allen Bradley Logix5000 PCL\nCLOOS: for cloos robot controllers\nComau C5G: for Comau C5G robot controllers\nDenso PAC: for Denso RC7 (and older) robot controllers (PAC programming language)\nDenso RC8: for Denso RC8 (and newer) robot controllers (PacScript programming language)\nDobot: for educational Dobot robots\nFanuc R30iA: for Fanuc R30iA and R30iB robot controllers\nFanuc R30iA Arc: for Fanuc Arc welding\nFanuc RJ3: for Fanuc RJ3 robot controllers\nGCode BnR: for B&R robot controllers\nGSK: for GSK robots\nHIWIN HRSS: for HIWIN robots\nKAIRO: for Keba Kairo robot controllers\nKUKA IIWA: for KUKA IIWA sunrise programming in Java\nKUKA KRC2: for KUKA KRC2 robot controllers\nKUKA KRC2 CamRob: for KUKA CamRob milling option\nKUKA KRC2 DAT: for KUKA KRC2 robot controllers including DAT data files\nKUKA KRC4: for KUKA KRC4 robot controllers\nKUKA KRC4 Config: for KUKA KRC4 robot controllers with configuration data in each line\nKUKA KRC4 DAT: for KUKA KRC4 robot controllers including DAT data files\nKawasaki: for Kawasaki AS robot controllers\nMecademic: for Mecademic Meca500 robot\nMotoman/Yaskawa: For Motoman robot controllers (JBI II and JBI III programming)\nMitsubishi: for Mitsubishi robot controllers\nNachi AX FD: for Nachi AX and FD robot controllers\nDaihen OTC: for Daihen OTC robot controllers\nPrecise: for Precise Scara robots\nSiemens Sinumerik: for Siemens Sinumerik ROBX robot controller\nStaubli VAL3: for Staubli VAL3 robot programs (CS8 controllers and later)\nStaubli VAL3 InlineMove: to generate Staubli VAL3 programs with inline movement data\nStaubli S6: for Staubli S6 robot controllers\nToshiba: for Toshiba robots\nUniversal Robots: for UR robots, generates linear movements as pose targets\nUniversal Robots RobotiQ: for UR robots including support for RobotiQ gripper\nUniversal Robots joints: for UR robots, generates linear movements as joint targets\nYamaha: for Yamaha robots", "link": "https://github.com/RoboDK/RoboDK-API", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "robodk api\nthis repository is the implementation of the robodk api in different programming languages for simulation and offline programming. the robodk api allows you to program any industrial robot arm using your preferred programming language.\nthe robodk api is available in the following programming languages:\npython\nc#\nmatlab\nc++\nvisual basic\nthe robodk api allows creating simulations for industrial robots, specific mechanisms and generating vendor-specific programs for robots. while robodk's graphical user interface can be used to create programs, it is possible to extend the robot controller limitations by using a universal programming language such as python.\nwith the robodk api it is possible to simulate and program any industrial robot using your preferred programming language. this avoids using vendor-specific programming languages.\neach package implements the following modules/classes:\nthe robolink module is the link to robodk.\nthe item module: any item from the robodk item -----> tree !!!  can be retrieved. items are represented by the object item (robodk.item). an item can be a robot, a reference frame, a tool, an object or a specific project.\nthe robodk module includes a robotics toolbox for pose operations. this toolbox is inspired from peter corke's robotics toolbox.\nthe following website provides an overview of the robodk api: https://robodk.com/offline-programming\nrobodk can be used for a wide range of applications, such as 3d printing, robot machining, synchronizing multiple robots, pick and place...\nindustrial robot application examples\nrobodk blog\nimportant: the robodk api is not the same as the robodk plug-in interface.\nrequirements\nrobodk must be installed:\nrobodk simulation software: https://robodk.com/download\nthe robodk api can be used with a free robodk license.\ndocumentation\nintroduction to the robodk api\nrobodk api reference (based on python)\nintroduction to robodk for robot simulation and offline programming\nc++ version of the api\nc# version of the api (nuget)\nplugin interface (c++)\nexample\nthe following script (python) shows an example that uses the robodk api for robot simulation and offline programming::\nfrom robolink import * # robodk's api\nfrom robodk import * # math toolbox for robots\n# start the robodk api:\nrdk = robolink()\n# get the robot item by name:\nrobot = rdk.item('fanuc lr mate 200id', item_type_robot)\n# get the reference target by name:\ntarget = rdk.item('target 1')\ntarget_pose = target.pose()\nxyz_ref = target_pose.pos()\n# move the robot to the reference point:\nrobot.movej(target)\n# draw a hexagon around the reference target:\nfor i in range(7):\nang = i*2*pi/6 #ang = 0, 60, 120, ..., 360\n# calculate the new position around the reference:\nx = xyz_ref[0] + r*cos(ang) # new x coordinate\ny = xyz_ref[1] + r*sin(ang) # new y coordinate\nz = xyz_ref[2] # new z coordinate\ntarget_pos.setpos([x,y,z])\n# move to the new target:\nrobot.movel(target_pos)\n# trigger a program call at the end of the movement\nrobot.runcode('program_done')\n# move back to the reference target:\nrobot.movel(target)\nthe same script used for simulation can be used for offline programming, this means that the corresponding program can be generated for the robot controller. robodk supports a large number of robot controllers and it is easy to include compatibility for new robot controllers using post processors.\nfor more examples:\nhttps://robodk.com/doc/en/pythonapi/examples.html\nfor more information about robot post processors:\nquick introduction to robodk post processors\nhow to use post processors\ntechnical reference\nsupported robots\nthe following list includes the robot controllers supported by robodk:\nabb rapid irc5: for abb irc5 robot controllers\nabb rapid s4c: for abb s4c robot controllers\nadept vplus: for adept v+ programming language\nallen bradley logix5000: for allen bradley logix5000 pcl\ncloos: for cloos robot controllers\ncomau c5g: for comau c5g robot controllers\ndenso pac: for denso rc7 (and older) robot controllers (pac programming language)\ndenso rc8: for denso rc8 (and newer) robot controllers (pacscript programming language)\ndobot: for educational dobot robots\nfanuc r30ia: for fanuc r30ia and r30ib robot controllers\nfanuc r30ia arc: for fanuc arc welding\nfanuc rj3: for fanuc rj3 robot controllers\ngcode bnr: for b&r robot controllers\ngsk: for gsk robots\nhiwin hrss: for hiwin robots\nkairo: for keba kairo robot controllers\nkuka iiwa: for kuka iiwa sunrise programming in java\nkuka krc2: for kuka krc2 robot controllers\nkuka krc2 camrob: for kuka camrob milling option\nkuka krc2 dat: for kuka krc2 robot controllers including dat data files\nkuka krc4: for kuka krc4 robot controllers\nkuka krc4 config: for kuka krc4 robot controllers with configuration data in each line\nkuka krc4 dat: for kuka krc4 robot controllers including dat data files\nkawasaki: for kawasaki as robot controllers\nmecademic: for mecademic meca500 robot\nmotoman/yaskawa: for motoman robot controllers (jbi ii and jbi iii programming)\nmitsubishi: for mitsubishi robot controllers\nnachi ax fd: for nachi ax and fd robot controllers\ndaihen otc: for daihen otc robot controllers\nprecise: for precise scara robots\nsiemens sinumerik: for siemens sinumerik robx robot controller\nstaubli val3: for staubli val3 robot programs (cs8 controllers and later)\nstaubli val3 inlinemove: to generate staubli val3 programs with inline movement data\nstaubli s6: for staubli s6 robot controllers\ntoshiba: for toshiba robots\nuniversal robots: for ur robots, generates linear movements as pose targets\nuniversal robots robotiq: for ur robots including support for robotiq gripper\nuniversal robots joints: for ur robots, generates linear movements as joint targets\nyamaha: for yamaha robots", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000269, "year": null}, {"Unnamed: 0": 1301, "autor": 281, "date": null, "content": "IKBT\nA python based system for generating closed-form solutions to the manipulator inverse kinematics problem using behavior trees for action selection. Solutions are fully symbolic and are output as LaTex, Python, and C++.\nCurrent News\nDec 2021\nLatex output has been refactored. New Latex procedure is simplified as follows:\nrun IKBT\ncd LaTex\npdflatex ik_solutions_ROBOTNAME.tex\nA new top-level is provided to do Forward Kinematics and Jacobian matrix only.\npython fkOnly.py ROBOTNAME\ncd LaTex\npdflatex fk_equations_ROBOTNAME.tex\nNov 2021\nWe've accumulated experience from many installations with the help of students in ECE 543 (University of Washington). One issue that came up is that with python 3.6.X something fails in the complex graph of imports. This works fine with python 3.8.X. We haven't attempted to fix this.\nRecent commits to main have corrected some remaining python 2 hangovers in Latex outout and python output (even python 3 code can generate pytyon 2 print statements!).\nSept 2021 Version 2.0\nAt long last the 3-way sum of angles feature is upgraded to python3 and merged into main branch. All tests are passing, UR5 and Puma both solve all unknowns. USE MAIN BRANCH.\nJuly 2021\nUnit test programs were broken for python3. This is now fixed. Test programs work and all pass.\nMarch 2021\nUpgraded to Python3 (mostly just adding parens to print statements and new python3-sympy). New \"main\" branch for more respectful terminology.\nFeb 2020\nBH has fixed the UR5 regression - it now works again and solution output no longer shows sum-of-angles variables that are not needed in the actual solution.\nUse branch RepairUR5Regression for latest and greatest version.\nFeb 2020\nBH has fixed the UR5 regression - it now works again and solution output no longer shows sum-of-angles variables that are not needed in the actual solution.\nUse branch RepairUR5Regression for latest and greatest version.\nJuly 2019\nWork aimed at #15 and #18 is still ongoing (it's tough!). But a few smaller bugs have been fixed which are now in the 'testing' branch. Please try that branch and post issues. THX.\nMay 2019\nBH is working on a new implementation of the solution graph and generating the list of solutions (much trickier that it seems at first!). I've taken this work to a private repo fork to decluter this page, but will merge and commit shortly. This work is aimed at issues #15 and #18. Thanks to you new issue posters!\nAug 2018\nSum-of-Angles transform now works for the case of three angles (corresponds to three parallel axes in the mechanism). IKBT can now solve the UR5 and similar robots!\nIKBT Overview\nOur contributions to automate closed-form kinematics solving are:\nWe built an autonomous inverse kinematics solver (IKBT) using a behavior tree to organize solution algorithms.\nWe incorporated knowledge frequently used (by human experts) when solving inverse kinematics into a behavior tree. These rule-based solvers applicable to any serial-chain, non-redundant, robot arm.\nIKBT generates a dependency {\\it graph} of joint variables after solving, generating all possible solutions.\nIKBT provides convenience features such as automatic documentation of the solution in \\LaTeX and automatic code generation in Python and C++.\nImplementation in a modern open-source, cross-platform, programming language (Python) with minimal dependencies outside of the standard Python distribution ({\\tt sympy}).\nVideos\nIntroductory Video (6min)\nHow to set up IKBT for your own robot arm (6.5min)\nDetails\nHow to cite:\nZhang, Dianmu, and Blake Hannaford. \"IKBT: solving symbolic inverse kinematics with behavior tree.\" Journal of Artificial Intelligence Research 65 (2019): 457-486. Link\nZhang, Dianmu, and Blake Hannaford. \"IKBT: solving closed-form Inverse Kinematics with Behavior Tree.\" arXiv preprint arXiv:1711.05412 (2017). Link\nInstallation Dependencies\nYou need the following to be installed to run IKBT:\nPython 2.7.x (Python Installation)\nSympy python package (Installation instructions for all OS)\nLatex package (for nice equation output - highly recommended) (Install Latex)\nTested robots, DH parameters & other technical details to reproduce the results\nA list of all DH parameters tested in the paper: ['Puma', 'Chair_Helper', 'Wrist', 'MiniDD', 'Olson13','Stanford', 'Sims11', 'Srisuan11', 'Axtman13', 'Mackler13', 'Minder13', 'Palm13', 'Parkman13', 'Frei13', 'Wachtveitl', 'Bartell', 'DZhang', 'Khat6DOF'.]\nWe suggest you first run the Wrist since it is relatively fast:\npython ikSolver.py Wrist\nTo solve your own problem open the file ikbtfunctions/ik_robots.py and create an entry for your robot. You should copy an entry for an existing robot and edit it's entries. Create an \"unknown\" for each joint variable and package them into the vector \"variables\". Enter the DH parameters in matrix form. Also, enter the name of your robot into the list of valid names (ikbtfunctions/ik_robots.py, line 31).\nDH parameters explained: The vector \"vv\" encodes whether each joint is rotary (1) or prismatic (0). If your robot is less than 6 DOF, create empty rows: [ 0 , 0, 0, 0 ], in the DH table so that it has six rows. Many standard symbols in robot kinematics are pre-defined for you but if you use any new ones, be sure to define them using sp.var(). See \"Wrist\" for an example in which the three joint variables \"A, B, C\" are set up for sympy by sp.var('A B C'). \"pvals\" is where you can put in the numerical values for all parameters, for result verification purposes.\nPre-computed forward kinematics.\nSometimes computation of the forward kinematic equations (and their subsequent simplification) can be time consuming. When debugging an inverse kinematics solution (for example modifying the BT), it can slow the cycle if these have to be redone each time. Therefore, the software has a mechanism using Python \"pickle\" files, to cache the forward kinematics computation and not repeat it. Forward kinematics pickle files are stored in the directory fk_eqns/. This directory will be automatically created if you don't have it. In some cases you may have to delete the pickle file for your robot. To do that, >rm fk_eqns/NAME_pickle.p. IKBT will generally tell you when you should do this, but it is OK to just >rm -rf fk_eqns/ .", "link": "https://github.com/uw-biorobotics/IKBT", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "ikbt\na python based system for generating closed-form solutions to the manipulator inverse kinematics problem using behavior trees for action selection. solutions are fully symbolic and are output as latex, python, and c++.\ncurrent news\ndec 2021\nlatex output has been refactored. new latex procedure is simplified as follows:\nrun ikbt\ncd latex\npdflatex ik_solutions_robotname.tex\na new top-level is provided to do forward kinematics and jacobian matrix only.\npython fkonly.py robotname\ncd latex\npdflatex fk_equations_robotname.tex\nnov 2021\nwe've accumulated experience from many installations with the help of students in ece 543 (university of washington). one issue that came up is that with python 3.6.x something fails in the complex graph of imports. this works fine with python 3.8.x. we haven't attempted to fix this.\nrecent commits to main have corrected some remaining python 2 hangovers in latex outout and python output (even python 3 code can generate pytyon 2 print statements!).\nsept 2021 version 2.0\nat long last the 3-way sum of angles feature is upgraded to python3 and merged into main branch. all tests are passing, ur5 and puma both solve all unknowns. use main branch.\njuly 2021\nunit test programs were broken for python3. this is now fixed. test programs work and all pass.\nmarch 2021\nupgraded to python3 (mostly just adding parens to print statements and new python3-sympy). new \"main\" branch for more respectful terminology.\nfeb 2020\nbh has fixed the ur5 regression - it now works again and solution output no longer shows sum-of-angles variables that are not needed in the actual solution.\nuse branch repairur5regression for latest and greatest version.\nfeb 2020\nbh has fixed the ur5 regression - it now works again and solution output no longer shows sum-of-angles variables that are not needed in the actual solution.\nuse branch repairur5regression for latest and greatest version.\njuly 2019\nwork aimed at #15 and #18 is still ongoing (it's tough!). but a few smaller bugs have been fixed which are now in the 'testing' branch. please try that branch and post issues. thx.\nmay 2019\nbh is working on a new implementation of the solution graph and generating the list of solutions (much trickier that it seems at first!). i've taken this work to a private repo fork to decluter this page, but will merge and commit shortly. this work is aimed at issues #15 and #18. thanks to you new issue posters!\naug 2018\nsum-of-angles transform now works for the case of three angles (corresponds to three parallel axes in the mechanism). ikbt can now solve the ur5 and similar robots!\nikbt overview\nour contributions to automate closed-form kinematics solving are:\nwe built an autonomous inverse kinematics solver (ikbt) using a behavior -----> tree !!!  to organize solution algorithms.\nwe incorporated knowledge frequently used (by human experts) when solving inverse kinematics into a behavior tree. these rule-based solvers applicable to any serial-chain, non-redundant, robot arm.\nikbt generates a dependency {\\it graph} of joint variables after solving, generating all possible solutions.\nikbt provides convenience features such as automatic documentation of the solution in \\latex and automatic code generation in python and c++.\nimplementation in a modern open-source, cross-platform, programming language (python) with minimal dependencies outside of the standard python distribution ({\\tt sympy}).\nvideos\nintroductory video (6min)\nhow to set up ikbt for your own robot arm (6.5min)\ndetails\nhow to cite:\nzhang, dianmu, and blake hannaford. \"ikbt: solving symbolic inverse kinematics with behavior tree.\" journal of artificial intelligence research 65 (2019): 457-486. link\nzhang, dianmu, and blake hannaford. \"ikbt: solving closed-form inverse kinematics with behavior tree.\" arxiv preprint arxiv:1711.05412 (2017). link\ninstallation dependencies\nyou need the following to be installed to run ikbt:\npython 2.7.x (python installation)\nsympy python package (installation instructions for all os)\nlatex package (for nice equation output - highly recommended) (install latex)\ntested robots, dh parameters & other technical details to reproduce the results\na list of all dh parameters tested in the paper: ['puma', 'chair_helper', 'wrist', 'minidd', 'olson13','stanford', 'sims11', 'srisuan11', 'axtman13', 'mackler13', 'minder13', 'palm13', 'parkman13', 'frei13', 'wachtveitl', 'bartell', 'dzhang', 'khat6dof'.]\nwe suggest you first run the wrist since it is relatively fast:\npython iksolver.py wrist\nto solve your own problem open the file ikbtfunctions/ik_robots.py and create an entry for your robot. you should copy an entry for an existing robot and edit it's entries. create an \"unknown\" for each joint variable and package them into the vector \"variables\". enter the dh parameters in matrix form. also, enter the name of your robot into the list of valid names (ikbtfunctions/ik_robots.py, line 31).\ndh parameters explained: the vector \"vv\" encodes whether each joint is rotary (1) or prismatic (0). if your robot is less than 6 dof, create empty rows: [ 0 , 0, 0, 0 ], in the dh table so that it has six rows. many standard symbols in robot kinematics are pre-defined for you but if you use any new ones, be sure to define them using sp.var(). see \"wrist\" for an example in which the three joint variables \"a, b, c\" are set up for sympy by sp.var('a b c'). \"pvals\" is where you can put in the numerical values for all parameters, for result verification purposes.\npre-computed forward kinematics.\nsometimes computation of the forward kinematic equations (and their subsequent simplification) can be time consuming. when debugging an inverse kinematics solution (for example modifying the bt), it can slow the cycle if these have to be redone each time. therefore, the software has a mechanism using python \"pickle\" files, to cache the forward kinematics computation and not repeat it. forward kinematics pickle files are stored in the directory fk_eqns/. this directory will be automatically created if you don't have it. in some cases you may have to delete the pickle file for your robot. to do that, >rm fk_eqns/name_pickle.p. ikbt will generally tell you when you should do this, but it is ok to just >rm -rf fk_eqns/ .", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000281, "year": null}, {"Unnamed: 0": 1319, "autor": 299, "date": null, "content": "SCRIMMAGE Multi-Agent Simulator\nWelcome to SCRIMMAGE\nSCRIMMAGE is a multi-agent simulator for robotics research. It has been used to conduct studies in multi-agent task assignment, differential game theory, novel controllers, and reinforcement learning.\nSCRIMMAGE Demonstration Video\nOnline Documentation\nTutorials\nCitation\nIf you use SCRIMMAGE in your research, please cite our research paper:\n@inproceedings{demarco2018,\ntitle={Simulating Collaborative Robots in a Massive Multi-Agent Game Environment ({SCRIMMAGE})},\nauthor={DeMarco, Kevin and Squires, Eric and Day, Michael and Pippin, Charles},\nbooktitle={Int. Symp. on Distributed Autonomous Robotic Systems},\nyear={2018},\n}\nBuild SCRIMMAGE\nDirectory Setup\nSCRIMMAGE developers and users may use multiple scrimmage-related projects and repositories. Thus, it is recommended to group your scrimmage-related projects under a single directory, but it is not necessary. To create a directory to hold your scrimmage projects and clone this scrimmage repo, run the following commands:\n$ mkdir -p ~/scrimmage && cd ~/scrimmage\n$ git clone https://github.com/gtri/scrimmage.git\nInstall Binary Dependencies\nA list of the Ubuntu packages required is provided in ./setup/install-binaries.sh in the \"DEPS_DPKG\" array. Run our automated installer to install the required packages:\n$ cd scrimmage\n$ sudo ./setup/install-binaries.sh -e 0 -p 3\nThe first argument -e 0 says to install all dependencies for all features in SCRIMMAGE (you would use -e 1 if you wanted to run SCRIMMAGE as part of an embedded system). The second argument -p 3 says to install python3 dependencies (use -p a for both python2 and 3 or -p 2 for just python2 dependencies).\nInstall Custom Built Binary Dependencies\nSome of SCRIMMAGE's dependencies have to be custom built from source. We provide debian package binaries for both Ubuntu 16.04 (xenial) and 18.04 (bionic) via the SCRIMMAGE PPA on Launchpad for these custom built packages. For Ubuntu 16.04 (xenial) and 18.04 (bionic), add the following PPA to your apt-get sources:\n$ sudo add-apt-repository ppa:kevin-demarco/scrimmage\nFor both distributions, update your sources list:\n$ sudo apt-get update\nNow, install the SCRIMMAGE custom built binary dependencies:\n$ sudo apt-get install scrimmage-dependencies scrimmage-jsbsim\nRun the SCRIMMAGE setup script, which adds the ~/.scrimmage directory to your local system and sets up some environment variables:\n$ source /opt/scrimmage/*/setup.sh\nNote: If you need to build the dependencies from source or generate binary packages, see Build Dependencies from Source\nBuild SCRIMMAGE Core\n$ mkdir build && cd build\n$ cmake ..\n$ make\n$ source ~/.scrimmage/setup.bash\nEnvironment Setup\nWhenever you want to use scrimmage, you need to source the ~/.scrimmage/setup.bash file or you can place a line in your ~/.bashrc file to source it automatically:\n$ echo \"source ~/.scrimmage/setup.bash\" >> ~/.bashrc\nRun SCRIMMAGE\nOpen a new terminal, change to the scrimmage directory, and execute a mission.\n$ cd </path/to/>scrimmage\n$ scrimmage ./missions/straight.xml\nYou should see the visualization GUI open up and display the simulation.\nGUI Commands\nThe GUI responds to the following input keys:\n'q' : Quit the simulation\n'b' : (Break) Pauses and unpauses the simulation.\n'space bar' : When paused, take a single simulation step.\n'a' : Rotate through the camera views\n'right/left arrows' : Change the aircraft to follow\n'[' : Decrease simulation warp speed\n']' : Increase simulation warp speed\n'+' : Increase visual scale of all entities\n'-' : Decrease visual scale of all entities\n'r' : Reset visual scale and reset camera position\n'z' : Zoom out from entity\n'Z' : Zoom in to entity (z+shift)\n'w' : Display wireframe\n's' : Display solids (vs. wireframe)\n'CTRL + Left Click' : Rotate world\n'SHIFT + Left Click' : Translate camera through world\nThe GUI's camera can operate in three modes (cycle with 'a' key):\nFollow the entity and point towards the entity's heading\nFree floating camera\nFollow the entity from a fixed viewpoint\nNote: If all of the terrain data does not appear, click on the GUI window with your mouse.\nBuilding on macOS\nSupport for macOS has been deprecated. Refer here for legacy homebrew instructions, but this is not guaranteed to build without intervention from the user. Any user wishing to develop on macOS is free to do so and make pull requests for patches, but the platform is no longer supported by SCRIMMAGE core developers.\nPython Bindings\nIt is recommended to build scrimmage's Python bindings in a python virtual environment:\n$ cd /path/to/scrimmage\n$ sudo apt-get install python3 libpython3-dev python3-venv\n$ python3 -m venv env\n$ source ./env/bin/activate\nInstall the python dependencies with specific version numbers:\n(env)$ pip install -r ./python/requirements.txt\nRe-build the scrimmage project within the virtual environment:\n(env)$ cmake .. -DPYTHON_MIN_VERSION=3.6\n(env)$ make\nInstall SCRIMMAGE Python Bindings\nTo install scrimmage's python bindings:\n(env)$ cd /path/to/scrimmage/python\n(env)$ python setup.py develop\nBuild SCRIMMAGE Documentation\n$ cd build\n$ cmake .. -DBUILD_DOCS=ON\n$ make docs\nView SCRIMMAGE API (Doxygen) Documentation\n$ firefox ./docs/doxygen/html/index.html\nView SCRIMMAGE Tutorial (Sphinx) Documentation\n$ firefox ./docs/sphinx/html/index.html\nBuild and Run Tests\n$ cmake .. -DBUILD_TESTS=ON\n$ make\n$ make test\nCleaning SCRIMMAGE\nThe scrimmage source code can be cleaned with the standard clean command:\n$ make clean\nHowever, if you want to clean everything, you can remove your build directory:\n$ cd /path/to/scrimmage && rm -rf build\nROS Integration\nTo build SCRIMMAGE's ROS plugins, you must have ROS installed, the ROS environment sourced, and the BUILD_ROS_PLUGINS cmake variable must be set:\n$ sudo apt-get install ros-${ROS_VERSION}-desktop-full ros-${ROS_VERSION}-mavros-msgs\n$ source /opt/ros/${ROS_VERSION}/setup.sh\n$ cmake .. -DBUILD_ROS_PLUGINS=ON\nThe ${ROS_VERSION} should be substituted with an appropriate ROS version (e.g., \"kinetic\", \"melodic\", etc.).\nAn example of using SCRIMMAGE to simulate robots running the ROS 2D Navigation stack can be found in the scrimmage_ros package.\nMOOS Integration\nIf you want to use MOOS with SCRIMMAGE, you will first need to download and build MOOS/MOOS-IVP according to the instructions at: http://oceanai.mit.edu/moos-ivp/pmwiki/pmwiki.php?n=Site.Download\nThe MOOSAutonomy plugin interacts with the MOOSDB to synchronize time, exchange contact information, and receive desired state from the IvP Helm. To build MOOSAutonomy, you have to provide cmake with the path to the moos-ivp source tree:\n$ cmake .. -DMOOSIVP_SOURCE_TREE_BASE=/path/to/moos-ivp\nFlightGear Multiplayer Server (FGMS) Integration\nIf you want to use FGMS with SCRIMMAGE, you will first need to download and build FGMS according to the instructions at: https://github.com/FlightGear/fgms\nClone the flight gear multiplayer server repository and build it:\n$ git clone https://github.com/FlightGear/fgms.git\n$ cd fgms\n$ git checkout 6669ac222b9f6ca34b0d56ba1bc6cac9cc0324b2\n$ mkdir build && cd build\n$ cmake .. -DBUILD_SHARED_LIB=ON\n$ make\nThe FGMS plugin interacts with SCRIMMAGE to receive the state variables of each entity. To build FGMS, you have to provide SCRIMMAGE's CMake project the path to the FGMS root source:\n$ cmake .. -DFGMS_SOURCE_TREE_BASE=/path/to/fgms\nRunning SCRIMMAGE inside of Docker\nThe SCRIMMAGE docker image is pushed to a public repository after a successful build on Travis. If docker is installed on your machine, you can obtain the SCRIMMAGE docker image by running the following command:\n$ docker pull syllogismrxs/scrimmage:latest\nYou can pass mission files from your host machine to the scrimmage executable inside of the docker container with the following command:\n$ cd /path/to/scrimmage/missions\n$ docker run --name my-scrimmage \\\n-v ${PWD}/straight_jsbsim.xml:/straight_jsbsim.xml \\\nsyllogismrxs/scrimmage:latest /straight_jsbsim.xml\nThe previous command mounts the straight_jsbsim.xml mission file on your host machine into the scrimmage container and then the /straight_jsbsim.xml portion at the end of the command overwrites the default docker CMD, which is defined in the Dockerfile. Finally, the scrimmage executable is passed the /straight_jsbsim.xml mission file.\nSince we provided a name for our container, we can easily extract the SCRIMMAGE log files from the docker container:\n$ docker cp my-scrimmage:/root/.scrimmage/logs .\nIf you need to drop into a shell inside of the scrimmage container, you will need to overwrite the docker image's ENTRYPOINT.\n$ docker run -it --entrypoint=\"/bin/bash\" syllogismrxs/scrimmage:latest\nOnce inside of the container, you will need to source the setup.bash file manually before running a mission.\n$ source ~/.scrimmage/setup.bash\n$ scrimmage ./missions/straight-no-gui.xml\nBuilding SCRIMMAGE for CentOS or RedHat\nThis repository contains a Dockerfile that builds a compiler with C++14 support, SCRIMMAGE's dependencies, and SCRIMMAGE for CentOS6 or RedHat6. RPMs are built inside of the docker image and they can be extracted and install on a CentOS or RedHat system. The user can change the package install prefix for all RPMs by specifying the PKG_PREFIX docker build argument. Building the docker image can take several hours:\n$ cd /path/to/scrimmage/ci/dockerfiles\n$ docker build --build-arg PKG_PREFIX=/opt/scrimmage \\\n--tag scrimmage/centos6:latest \\\n--file centos6 .\nExtract the RPMs that were built to the host's rpms folder:\n$ docker create --name mycontainer scrimmage/centos6:latest\n$ docker cp mycontainer:/root/rpms ./rpms # extract the rpms\n$ docker rm mycontainer # clean up container\nCopy the rpms folder to your CentOS or RedHat system and install the run-time dependencies:\n$ cd /path/to/rpms\n$ rpm -ivh scrimmage_gcc*.rpm \\\nscrimmage_python*.rpm \\\nscrimmage_boost*.rpm \\\nscrimmage_geographiclib*.rpm \\\nscrimmage_jsbsim*.rpm \\\nscrimmage_grpc*.rpm \\\nscrimmage_protobuf*.rpm \\\nscrimmage_0.2.0*.rpm\nTo test that SCRIMMAGE was installed correctly, run the following command:\n$ export JSBSIM_ROOT=/opt/scrimmage/etc/JSBSim \\\n&& source /opt/scrimmage/etc/scrimmage/env/scrimmage-setenv \\\n&& scrimmage /opt/scrimmage/share/scrimmage/missions/straight-no-gui.xml\nInstalling and Configuring Open Grid Engine\nInstructions modified from: https://scidom.wordpress.com/2012/01/18/sge-on-single-pc/ http://www.bu.edu/tech/support/research/system-usage/running-jobs/tracking-jobs/\nInstall Grid Engine:\n$ sudo apt-get install gridengine-master gridengine-exec \\\ngridengine-common gridengine-qmon gridengine-client\nNote that you can configure how qsub is called with a .sge_request in your home directory. Further, you can set the number of available slots (cores available) when running grid engine under the Queue Control tab.\nInstalling and Configuring PostgreSQL\nInstall PostgreSQL and configure the database scrimmage, create user scrimmage with password scrimmage, and add that user to the scrimmage database:\n$ sudo apt-get install postgresql postgresql-contrib\n$ sudo update-rc.d postgresql enable &&\\\nsudo service postgresql restart &&\\\nsudo -u postgres createdb scrimmage &&\\\nsudo -u postgres psql -c \"CREATE USER scrimmage with password 'scrimmage';\" &&\\\nsudo -u postgres psql -c \"alter user scrimmage with encrypted password 'scrimmage'\" &&\\\nsudo -u postgres psql -c \"grant all privileges on database scrimmage to scrimmage;\"\nGo into /etc/postgresql/9.5/main/pg_hba.conf (or similar path to your postgres install) and change the line:\nlocal all all peer\nto\nlocal all all md5\nThen run:\n$ sudo service postgresql restart\nThis will allow us to authenticate the scrimmage user on postgres with the password scrimmage that we created.\nTo use the python scripts for pulling .csv files to postgres, install psycopg2, the python interface for postgres:\n$ pip install psycopg2\nThe scripts are located in the scripts directory.\nTroubleshooting\nProblem: I can't run the SCRIMMAGE GUI in a Virtual Machine (VirtualBox)\nThere are some OpenGL issues with VTK6 in Virtualbox. To run SCRIMMAGE in VirtualBox with VTK5, run the following commands:\n$ sudo apt-get install libvtk5-dev\n$ cd ~/scrimmage/scrimmage/build # Note: Path may vary\n$ cmake ..\nAt this point, cmake should output a message about finding VTK Version 5. Now, you have to rebuild SCRIMMAGE:\n$ make\nProblem: I cannot load python libraries through scrimmage\nMake sure that when you run the cmake command it is using the version of python that you want to use with the following:\n$ cmake -DPYTHON_EXECUTABLE:FILEPATH=/usr/bin/python \\ # adjust path to your needs\n-DPYTHON_INCLUDE_DIR:PATH=/usr/include/python2.7 \\ # adjust path to your needs\n-DPYTHON_LIBRARY:FILEPATH=/usr/lib/libpython2.7.so # adjust path to your needs\nProblem: vtkRenderingPythonTkWidgets cmake Warning\nWhen running cmake, the user gets the cmake warning:\n-- The imported target \"vtkRenderingPythonTkWidgets\" references the file\n\"/usr/lib/x86_64-linux-gnu/libvtkRenderingPythonTkWidgets.so\"\nbut this file does not exist. Possible reasons include:\n* The file was deleted, renamed, or moved to another location.\n* An install or uninstall procedure did not complete successfully.\n* The installation package was faulty and contained\n\"/usr/lib/cmake/vtk-6.2/VTKTargets.cmake\"\nbut not all the files it references.\nThis is a VTK6 Ubuntu package bug. It can be ignored.\nProblem: I do not see building extrusions in the SCRIMMAGE GUI\nSCRIMMAGE uses vtkGeoJSONReader to load polygon extrusion data from a GeoJSON file. This VTK feature was added in VTK7. Since this feature is not available in older versions, SCRIMMAGE does not load building data if the installed VTK version is less than 7. Therefore, the remedy is to upgrade VTK. To install VTK7 on Ubuntu, run the following command:\n$ sudo apt-get install libvtk7-dev\nProblem: Docker Container Can't Access Internet\nDocker can have DNS issues. If you can ping a public ip address within a docker image (such as 8.8.8.8), but you can't ping archive.ubuntu.com, create the file /etc/docker/daemon.json with the following contents:\n{\n\"dns\": [\"<DNS-IP>\", \"8.8.8.8\"]\n}\nWhere <DNS-IP> is the first DNS IP address and is a network interface with internet access from the commands:\n$ nmcli dev list | grep 'IP4.DNS' # Ubuntu <= 14\n$ nmcli device show <interfacename> | grep IP4.DNS # Ubuntu >= 15\nRestart docker:\n$ sudo service docker restart", "link": "https://github.com/gtri/scrimmage", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "scrimmage multi-agent simulator\nwelcome to scrimmage\nscrimmage is a multi-agent simulator for robotics research. it has been used to conduct studies in multi-agent task assignment, differential game theory, novel controllers, and reinforcement learning.\nscrimmage demonstration video\nonline documentation\ntutorials\ncitation\nif you use scrimmage in your research, please cite our research paper:\n@inproceedings{demarco2018,\ntitle={simulating collaborative robots in a massive multi-agent game environment ({scrimmage})},\nauthor={demarco, kevin and squires, eric and day, michael and pippin, charles},\nbooktitle={int. symp. on distributed autonomous robotic systems},\nyear={2018},\n}\nbuild scrimmage\ndirectory setup\nscrimmage developers and users may use multiple scrimmage-related projects and repositories. thus, it is recommended to group your scrimmage-related projects under a single directory, but it is not necessary. to create a directory to hold your scrimmage projects and clone this scrimmage repo, run the following commands:\n$ mkdir -p ~/scrimmage && cd ~/scrimmage\n$ git clone https://github.com/gtri/scrimmage.git\ninstall binary dependencies\na list of the ubuntu packages required is provided in ./setup/install-binaries.sh in the \"deps_dpkg\" array. run our automated installer to install the required packages:\n$ cd scrimmage\n$ sudo ./setup/install-binaries.sh -e 0 -p 3\nthe first argument -e 0 says to install all dependencies for all features in scrimmage (you would use -e 1 if you wanted to run scrimmage as part of an embedded system). the second argument -p 3 says to install python3 dependencies (use -p a for both python2 and 3 or -p 2 for just python2 dependencies).\ninstall custom built binary dependencies\nsome of scrimmage's dependencies have to be custom built from source. we provide debian package binaries for both ubuntu 16.04 (xenial) and 18.04 (bionic) via the scrimmage ppa on launchpad for these custom built packages. for ubuntu 16.04 (xenial) and 18.04 (bionic), add the following ppa to your apt-get sources:\n$ sudo add-apt-repository ppa:kevin-demarco/scrimmage\nfor both distributions, update your sources list:\n$ sudo apt-get update\nnow, install the scrimmage custom built binary dependencies:\n$ sudo apt-get install scrimmage-dependencies scrimmage-jsbsim\nrun the scrimmage setup script, which adds the ~/.scrimmage directory to your local system and sets up some environment variables:\n$ source /opt/scrimmage/*/setup.sh\nnote: if you need to build the dependencies from source or generate binary packages, see build dependencies from source\nbuild scrimmage core\n$ mkdir build && cd build\n$ cmake ..\n$ make\n$ source ~/.scrimmage/setup.bash\nenvironment setup\nwhenever you want to use scrimmage, you need to source the ~/.scrimmage/setup.bash file or you can place a line in your ~/.bashrc file to source it automatically:\n$ echo \"source ~/.scrimmage/setup.bash\" >> ~/.bashrc\nrun scrimmage\nopen a new terminal, change to the scrimmage directory, and execute a mission.\n$ cd </path/to/>scrimmage\n$ scrimmage ./missions/straight.xml\nyou should see the visualization gui open up and display the simulation.\ngui commands\nthe gui responds to the following input keys:\n'q' : quit the simulation\n'b' : (break) pauses and unpauses the simulation.\n'space bar' : when paused, take a single simulation step.\n'a' : rotate through the camera views\n'right/left arrows' : change the aircraft to follow\n'[' : decrease simulation warp speed\n']' : increase simulation warp speed\n'+' : increase visual scale of all entities\n'-' : decrease visual scale of all entities\n'r' : reset visual scale and reset camera position\n'z' : zoom out from entity\n'z' : zoom in to entity (z+shift)\n'w' : display wireframe\n's' : display solids (vs. wireframe)\n'ctrl + left click' : rotate world\n'shift + left click' : translate camera through world\nthe gui's camera can operate in three modes (cycle with 'a' key):\nfollow the entity and point towards the entity's heading\nfree floating camera\nfollow the entity from a fixed viewpoint\nnote: if all of the terrain data does not appear, click on the gui window with your mouse.\nbuilding on macos\nsupport for macos has been deprecated. refer here for legacy homebrew instructions, but this is not guaranteed to build without intervention from the user. any user wishing to develop on macos is free to do so and make pull requests for patches, but the platform is no longer supported by scrimmage core developers.\npython bindings\nit is recommended to build scrimmage's python bindings in a python virtual environment:\n$ cd /path/to/scrimmage\n$ sudo apt-get install python3 libpython3-dev python3-venv\n$ python3 -m venv env\n$ source ./env/bin/activate\ninstall the python dependencies with specific version numbers:\n(env)$ pip install -r ./python/requirements.txt\nre-build the scrimmage project within the virtual environment:\n(env)$ cmake .. -dpython_min_version=3.6\n(env)$ make\ninstall scrimmage python bindings\nto install scrimmage's python bindings:\n(env)$ cd /path/to/scrimmage/python\n(env)$ python setup.py develop\nbuild scrimmage documentation\n$ cd build\n$ cmake .. -dbuild_docs=on\n$ make docs\nview scrimmage api (doxygen) documentation\n$ firefox ./docs/doxygen/html/index.html\nview scrimmage tutorial (sphinx) documentation\n$ firefox ./docs/sphinx/html/index.html\nbuild and run tests\n$ cmake .. -dbuild_tests=on\n$ make\n$ make test\ncleaning scrimmage\nthe scrimmage source code can be cleaned with the standard clean command:\n$ make clean\nhowever, if you want to clean everything, you can remove your build directory:\n$ cd /path/to/scrimmage && rm -rf build\nros integration\nto build scrimmage's ros plugins, you must have ros installed, the ros environment sourced, and the build_ros_plugins cmake variable must be set:\n$ sudo apt-get install ros-${ros_version}-desktop-full ros-${ros_version}-mavros-msgs\n$ source /opt/ros/${ros_version}/setup.sh\n$ cmake .. -dbuild_ros_plugins=on\nthe ${ros_version} should be substituted with an appropriate ros version (e.g., \"kinetic\", \"melodic\", etc.).\nan example of using scrimmage to simulate robots running the ros 2d navigation stack can be found in the scrimmage_ros package.\nmoos integration\nif you want to use moos with scrimmage, you will first need to download and build moos/moos-ivp according to the instructions at: http://oceanai.mit.edu/moos-ivp/pmwiki/pmwiki.php?n=site.download\nthe moosautonomy plugin interacts with the moosdb to synchronize time, exchange contact information, and receive desired state from the ivp helm. to build moosautonomy, you have to provide cmake with the path to the moos-ivp source -----> tree !!! :\n$ cmake .. -dmoosivp_source_tree_base=/path/to/moos-ivp\nflightgear multiplayer server (fgms) integration\nif you want to use fgms with scrimmage, you will first need to download and build fgms according to the instructions at: https://github.com/flightgear/fgms\nclone the flight gear multiplayer server repository and build it:\n$ git clone https://github.com/flightgear/fgms.git\n$ cd fgms\n$ git checkout 6669ac222b9f6ca34b0d56ba1bc6cac9cc0324b2\n$ mkdir build && cd build\n$ cmake .. -dbuild_shared_lib=on\n$ make\nthe fgms plugin interacts with scrimmage to receive the state variables of each entity. to build fgms, you have to provide scrimmage's cmake project the path to the fgms root source:\n$ cmake .. -dfgms_source_tree_base=/path/to/fgms\nrunning scrimmage inside of docker\nthe scrimmage docker image is pushed to a public repository after a successful build on travis. if docker is installed on your machine, you can obtain the scrimmage docker image by running the following command:\n$ docker pull syllogismrxs/scrimmage:latest\nyou can pass mission files from your host machine to the scrimmage executable inside of the docker container with the following command:\n$ cd /path/to/scrimmage/missions\n$ docker run --name my-scrimmage \\\n-v ${pwd}/straight_jsbsim.xml:/straight_jsbsim.xml \\\nsyllogismrxs/scrimmage:latest /straight_jsbsim.xml\nthe previous command mounts the straight_jsbsim.xml mission file on your host machine into the scrimmage container and then the /straight_jsbsim.xml portion at the end of the command overwrites the default docker cmd, which is defined in the dockerfile. finally, the scrimmage executable is passed the /straight_jsbsim.xml mission file.\nsince we provided a name for our container, we can easily extract the scrimmage log files from the docker container:\n$ docker cp my-scrimmage:/root/.scrimmage/logs .\nif you need to drop into a shell inside of the scrimmage container, you will need to overwrite the docker image's entrypoint.\n$ docker run -it --entrypoint=\"/bin/bash\" syllogismrxs/scrimmage:latest\nonce inside of the container, you will need to source the setup.bash file manually before running a mission.\n$ source ~/.scrimmage/setup.bash\n$ scrimmage ./missions/straight-no-gui.xml\nbuilding scrimmage for centos or redhat\nthis repository contains a dockerfile that builds a compiler with c++14 support, scrimmage's dependencies, and scrimmage for centos6 or redhat6. rpms are built inside of the docker image and they can be extracted and install on a centos or redhat system. the user can change the package install prefix for all rpms by specifying the pkg_prefix docker build argument. building the docker image can take several hours:\n$ cd /path/to/scrimmage/ci/dockerfiles\n$ docker build --build-arg pkg_prefix=/opt/scrimmage \\\n--tag scrimmage/centos6:latest \\\n--file centos6 .\nextract the rpms that were built to the host's rpms folder:\n$ docker create --name mycontainer scrimmage/centos6:latest\n$ docker cp mycontainer:/root/rpms ./rpms # extract the rpms\n$ docker rm mycontainer # clean up container\ncopy the rpms folder to your centos or redhat system and install the run-time dependencies:\n$ cd /path/to/rpms\n$ rpm -ivh scrimmage_gcc*.rpm \\\nscrimmage_python*.rpm \\\nscrimmage_boost*.rpm \\\nscrimmage_geographiclib*.rpm \\\nscrimmage_jsbsim*.rpm \\\nscrimmage_grpc*.rpm \\\nscrimmage_protobuf*.rpm \\\nscrimmage_0.2.0*.rpm\nto test that scrimmage was installed correctly, run the following command:\n$ export jsbsim_root=/opt/scrimmage/etc/jsbsim \\\n&& source /opt/scrimmage/etc/scrimmage/env/scrimmage-setenv \\\n&& scrimmage /opt/scrimmage/share/scrimmage/missions/straight-no-gui.xml\ninstalling and configuring open grid engine\ninstructions modified from: https://scidom.wordpress.com/2012/01/18/sge-on-single-pc/ http://www.bu.edu/tech/support/research/system-usage/running-jobs/tracking-jobs/\ninstall grid engine:\n$ sudo apt-get install gridengine-master gridengine-exec \\\ngridengine-common gridengine-qmon gridengine-client\nnote that you can configure how qsub is called with a .sge_request in your home directory. further, you can set the number of available slots (cores available) when running grid engine under the queue control tab.\ninstalling and configuring postgresql\ninstall postgresql and configure the database scrimmage, create user scrimmage with password scrimmage, and add that user to the scrimmage database:\n$ sudo apt-get install postgresql postgresql-contrib\n$ sudo update-rc.d postgresql enable &&\\\nsudo service postgresql restart &&\\\nsudo -u postgres createdb scrimmage &&\\\nsudo -u postgres psql -c \"create user scrimmage with password 'scrimmage';\" &&\\\nsudo -u postgres psql -c \"alter user scrimmage with encrypted password 'scrimmage'\" &&\\\nsudo -u postgres psql -c \"grant all privileges on database scrimmage to scrimmage;\"\ngo into /etc/postgresql/9.5/main/pg_hba.conf (or similar path to your postgres install) and change the line:\nlocal all all peer\nto\nlocal all all md5\nthen run:\n$ sudo service postgresql restart\nthis will allow us to authenticate the scrimmage user on postgres with the password scrimmage that we created.\nto use the python scripts for pulling .csv files to postgres, install psycopg2, the python interface for postgres:\n$ pip install psycopg2\nthe scripts are located in the scripts directory.\ntroubleshooting\nproblem: i can't run the scrimmage gui in a virtual machine (virtualbox)\nthere are some opengl issues with vtk6 in virtualbox. to run scrimmage in virtualbox with vtk5, run the following commands:\n$ sudo apt-get install libvtk5-dev\n$ cd ~/scrimmage/scrimmage/build # note: path may vary\n$ cmake ..\nat this point, cmake should output a message about finding vtk version 5. now, you have to rebuild scrimmage:\n$ make\nproblem: i cannot load python libraries through scrimmage\nmake sure that when you run the cmake command it is using the version of python that you want to use with the following:\n$ cmake -dpython_executable:filepath=/usr/bin/python \\ # adjust path to your needs\n-dpython_include_dir:path=/usr/include/python2.7 \\ # adjust path to your needs\n-dpython_library:filepath=/usr/lib/libpython2.7.so # adjust path to your needs\nproblem: vtkrenderingpythontkwidgets cmake warning\nwhen running cmake, the user gets the cmake warning:\n-- the imported target \"vtkrenderingpythontkwidgets\" references the file\n\"/usr/lib/x86_64-linux-gnu/libvtkrenderingpythontkwidgets.so\"\nbut this file does not exist. possible reasons include:\n* the file was deleted, renamed, or moved to another location.\n* an install or uninstall procedure did not complete successfully.\n* the installation package was faulty and contained\n\"/usr/lib/cmake/vtk-6.2/vtktargets.cmake\"\nbut not all the files it references.\nthis is a vtk6 ubuntu package bug. it can be ignored.\nproblem: i do not see building extrusions in the scrimmage gui\nscrimmage uses vtkgeojsonreader to load polygon extrusion data from a geojson file. this vtk feature was added in vtk7. since this feature is not available in older versions, scrimmage does not load building data if the installed vtk version is less than 7. therefore, the remedy is to upgrade vtk. to install vtk7 on ubuntu, run the following command:\n$ sudo apt-get install libvtk7-dev\nproblem: docker container can't access internet\ndocker can have dns issues. if you can ping a public ip address within a docker image (such as 8.8.8.8), but you can't ping archive.ubuntu.com, create the file /etc/docker/daemon.json with the following contents:\n{\n\"dns\": [\"<dns-ip>\", \"8.8.8.8\"]\n}\nwhere <dns-ip> is the first dns ip address and is a network interface with internet access from the commands:\n$ nmcli dev list | grep 'ip4.dns' # ubuntu <= 14\n$ nmcli device show <interfacename> | grep ip4.dns # ubuntu >= 15\nrestart docker:\n$ sudo service docker restart", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000299, "year": null}, {"Unnamed: 0": 1322, "autor": 302, "date": null, "content": "Deepbots is a simple framework which is used as \"middleware\" between the free and open-source Cyberbotics' Webots robot simulator and Reinforcement Learning algorithms. When it comes to Reinforcement Learning the OpenAI gym environment has been established as the most used interface between the actual application and the RL algorithm. Deepbots is a framework which follows the OpenAI gym environment interface logic in order to be used by Webots applications.\nInstallation\nPrerequisites\nInstall Webots\nWindows\nLinux\nmacOS\nInstall Python version 3.X (please refer to Using Python to select the proper Python version for your system)\nFollow the Using Python guide provided by Webots\nWebots provides a basic code editor, but if you want to use PyCharm as your IDE refer to using PyCharm IDE provided by Webots\nYou will probably also need a backend library to implement the neural networks, such as PyTorch or TensorFlow. Deepbots interfaces with RL agents using the OpenAI gym logic, so it can work with any backend library you choose to implement the agent with and any agent that already works with gym.\nInstall deepbots\nDeepbots can be installed through the package installer pip running the following command:\npip install deepbots\nOfficial resources\nOn the deepbots-tutorials repository you can find the official tutorials for deepbots\nOn the deepworlds repository you can find examples of deepbots being used.\nFeel free to contribute your own!\nCitation\nConference paper (AIAI2020): https://link.springer.com/chapter/10.1007/978-3-030-49186-4_6\n@InProceedings{10.1007/978-3-030-49186-4_6,\nauthor=\"Kirtas, M.\nand Tsampazis, K.\nand Passalis, N.\nand Tefas, A.\",\ntitle=\"Deepbots: A Webots-Based Deep Reinforcement Learning Framework for Robotics\",\nbooktitle=\"Artificial Intelligence Applications and Innovations\",\nyear=\"2020\",\npublisher=\"Springer International Publishing\",\naddress=\"Cham\",\npages=\"64--75\",\nisbn=\"978-3-030-49186-4\"\n}\nHow it works\nFirst of all let's set up a simple glossary:\nWorld: Webots uses a tree structure to represent the different entities in the scene. The World is the root entity which contains all the entities/nodes. For example, the world contains the Supervisor and Robot entities as well as other objects which might be included in the scene.\nSupervisor: The Supervisor is an entity which has access to all other entities of the world, while having no physical presence in it. For example, the Supervisor knows the exact position of all the entities of the world and can manipulate them. Additionally, the Supervisor has the Supervisor Controller as one of its child nodes.\nSupervisor Controller: The Supervisor Controller is a python script which is responsible for the Supervisor. For example, in the Supervisor Controller script the distance between two entities in the world can be calculated.\nRobot: The Robot is an entity that represents a robot in the world. It might have sensors and other active components, like motors, etc. as child entities. Also, one of its children is the Robot Controller. For example, epuck and TIAGo are robots.\nRobot Controller: The Robot Controller is a python script which is responsible for the Robot's movement and sensors. With the Robot Controller it is possible to observe the world and act accordingly.\nEnvironment: The Environment is the interface as described by the OpenAI gym. The Environment interface has the following methods:\nget_observations(): Return the observations of the robot. For example, metrics from sensors, a camera image etc.\nstep(action): Each timestep, the agent chooses an action, and the environment returns the observation, the reward and the state of the problem (done or not).\nget_reward(action): The reward the agent receives as a result of their action.\nis_done(): Whether it\u2019s time to reset the environment. Most (but not all) tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. For example, if a robot has the task to reach a goal, then the done condition might happen when the robot \"touches\" the goal.\nreset(): Used to reset the world to the initial state.\nIn order to set up a task in Deepbots it is necessary to understand the intention of the OpenAI gym environment. According to the OpenAI gym documentation, the framework follows the classic \u201cagent-environment loop\u201d. \"Each timestep, the agent chooses an action, and the environment returns an observation and a reward. The process gets started by calling reset(), which returns an initial observation.\"\nDeepbots follows this exact agent-environment loop with the only difference being that the agent, which is responsible to choose an action, runs on the Supervisor and the observations are acquired by the robot. The goal of the deepbots framework is to hide this communication from the user, especially from those who are familiar with the OpenAI gym environment. More specifically, SupervisorEnv is the interface which is used by the Reinforcement Learning algorithms and follows the OpenAI Gym environment logic. The Deepbots framework provides different levels of abstraction according to the user's needs. Moreover, a goal of the framework is to provide different wrappers for a wide range of robots.\nDeepbots also provides a default implementation of the reset() method, leveraging Webots' built-in simulation reset functions, removing the need for the user to implement reset procedures for simpler use-cases. It is always possible to override this method and implement any custom reset procedure, as needed.\nEmitter - receiver scheme\nCurrently, the communication between the Supervisor and the Robot is achieved via an emitter and a receiver. Separating the Supervisor from the Robot, deepbots can fit a variety of use-cases, e.g. multiple Robots collecting experience and a Supervisor controlling them with a single agent. The way Webots implements emitter/receiver communication requires messages to be packed and unpacked, which introduces an overhead that becomes prohibiting in use-cases where the observations are high-dimensional or long, such as camera images. Deepbots provides another partially abstract class that combines the Supervisor and the Robot into one controller and circumvents that issue, while being less flexible, which is discussed later.\nOn one hand, the emitter is an entity which is provided by Webots, that broadcasts messages to the world. On the other hand, the receiver is an entity that is used to receive messages from the World. Consequently, the agent-environment loop is transformed accordingly. Firstly, the Robot uses its sensors to retrieve the observation from the World and in turn uses the emitter component to broadcast this observation. Secondly, the Supervisor receives the observation via the receiver component and in turn, the agent uses it to choose an action. It should be noted that the observation the agent uses might be extended from the Supervisor. For example, a model might use LiDAR sensors installed on the Robot, but also the Euclidean distance between the Robot and an object. As it is expected, the Robot does not know the Euclidean distance, only the Supervisor can calculate it, because it has access to all entities in the World.\nYou can follow the emitter-receiver scheme tutorial to get started and work your way up from there.\nCombined Robot-Supervisor scheme\nAs mentioned earlier, in use-cases where the observation transmitted between the Robot and the Supervisor is high-dimensional or long, e.g. high resolution images taken from a camera, a significant overhead is introduced. This is circumvented by inheriting and implementing the partially abstract RobotSupervisor that combines the Robot controller and the Supervisor Controller into one, forgoing all emitter/receiver communication. This new controller runs on the Robot, but requires Supervisor privileges and is limited to one Robot, one Supervisor.\nYou can follow the robot-supervisor scheme tutorial to get started and work your way up from there. We recommended this tutorial to get started with deepbots.\nAbstraction Levels\nThe deepbots framework has been created mostly for educational purposes. The aim of the framework is to enable people to use Reinforcement Learning in Webots. More specifically, we can consider deepbots as a wrapper of Webots exposing an OpenAI gym style interface. For this reason there are multiple levels of abstraction. For example, a user can choose if they want to use CSV emitter/receiver or if they want to make an implementation from scratch. In the top level of the abstraction hierarchy is the SupervisorEnv which is the OpenAI gym interface. Below that level there are partially implemented classes with common functionality. These implementations aim to hide the communication between the Supervisor and the Robot, as described in the two different schemes ealier. Similarly, in the emitter/receiver scheme the Robot also has different abstraction levels. According to their needs, users can choose either to process the messages received from the Supervisor themselves or use the existing implementations.\nAcknowledgments\nThis project has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 871449 (OpenDR). This publication reflects the authors\u2019 views only. The European Commission is not responsible for any use that may be made of the information it contains.\nContributors \u2728\nThanks goes to these wonderful people (emoji key):\nManos Kirtas\n\ud83d\udcbb\nKostas Tsampazis\n\ud83d\udcbb\nJiun Kai Yang\n\ud83d\udcbb\nMentalGear\n\ud83e\udd14\nDreamtale\n\ud83d\udc1b\nNikolaos Kokkinis-Ntrenis\n\ud83d\udcbb \ud83d\udcd6 \ud83e\udd14\nThis project follows the all-contributors specification. Contributions of any kind welcome!\nSpecial thanks to Papanikolaou Evangelia for designing project's logo!", "link": "https://github.com/aidudezzz/deepbots", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "deepbots is a simple framework which is used as \"middleware\" between the free and open-source cyberbotics' webots robot simulator and reinforcement learning algorithms. when it comes to reinforcement learning the openai gym environment has been established as the most used interface between the actual application and the rl algorithm. deepbots is a framework which follows the openai gym environment interface logic in order to be used by webots applications.\ninstallation\nprerequisites\ninstall webots\nwindows\nlinux\nmacos\ninstall python version 3.x (please refer to using python to select the proper python version for your system)\nfollow the using python guide provided by webots\nwebots provides a basic code editor, but if you want to use pycharm as your ide refer to using pycharm ide provided by webots\nyou will probably also need a backend library to implement the neural networks, such as pytorch or tensorflow. deepbots interfaces with rl agents using the openai gym logic, so it can work with any backend library you choose to implement the agent with and any agent that already works with gym.\ninstall deepbots\ndeepbots can be installed through the package installer pip running the following command:\npip install deepbots\nofficial resources\non the deepbots-tutorials repository you can find the official tutorials for deepbots\non the deepworlds repository you can find examples of deepbots being used.\nfeel free to contribute your own!\ncitation\nconference paper (aiai2020): https://link.springer.com/chapter/10.1007/978-3-030-49186-4_6\n@inproceedings{10.1007/978-3-030-49186-4_6,\nauthor=\"kirtas, m.\nand tsampazis, k.\nand passalis, n.\nand tefas, a.\",\ntitle=\"deepbots: a webots-based deep reinforcement learning framework for robotics\",\nbooktitle=\"artificial intelligence applications and innovations\",\nyear=\"2020\",\npublisher=\"springer international publishing\",\naddress=\"cham\",\npages=\"64--75\",\nisbn=\"978-3-030-49186-4\"\n}\nhow it works\nfirst of all let's set up a simple glossary:\nworld: webots uses a -----> tree !!!  structure to represent the different entities in the scene. the world is the root entity which contains all the entities/nodes. for example, the world contains the supervisor and robot entities as well as other objects which might be included in the scene.\nsupervisor: the supervisor is an entity which has access to all other entities of the world, while having no physical presence in it. for example, the supervisor knows the exact position of all the entities of the world and can manipulate them. additionally, the supervisor has the supervisor controller as one of its child nodes.\nsupervisor controller: the supervisor controller is a python script which is responsible for the supervisor. for example, in the supervisor controller script the distance between two entities in the world can be calculated.\nrobot: the robot is an entity that represents a robot in the world. it might have sensors and other active components, like motors, etc. as child entities. also, one of its children is the robot controller. for example, epuck and tiago are robots.\nrobot controller: the robot controller is a python script which is responsible for the robot's movement and sensors. with the robot controller it is possible to observe the world and act accordingly.\nenvironment: the environment is the interface as described by the openai gym. the environment interface has the following methods:\nget_observations(): return the observations of the robot. for example, metrics from sensors, a camera image etc.\nstep(action): each timestep, the agent chooses an action, and the environment returns the observation, the reward and the state of the problem (done or not).\nget_reward(action): the reward the agent receives as a result of their action.\nis_done(): whether it\u2019s time to reset the environment. most (but not all) tasks are divided up into well-defined episodes, and done being true indicates the episode has terminated. for example, if a robot has the task to reach a goal, then the done condition might happen when the robot \"touches\" the goal.\nreset(): used to reset the world to the initial state.\nin order to set up a task in deepbots it is necessary to understand the intention of the openai gym environment. according to the openai gym documentation, the framework follows the classic \u201cagent-environment loop\u201d. \"each timestep, the agent chooses an action, and the environment returns an observation and a reward. the process gets started by calling reset(), which returns an initial observation.\"\ndeepbots follows this exact agent-environment loop with the only difference being that the agent, which is responsible to choose an action, runs on the supervisor and the observations are acquired by the robot. the goal of the deepbots framework is to hide this communication from the user, especially from those who are familiar with the openai gym environment. more specifically, supervisorenv is the interface which is used by the reinforcement learning algorithms and follows the openai gym environment logic. the deepbots framework provides different levels of abstraction according to the user's needs. moreover, a goal of the framework is to provide different wrappers for a wide range of robots.\ndeepbots also provides a default implementation of the reset() method, leveraging webots' built-in simulation reset functions, removing the need for the user to implement reset procedures for simpler use-cases. it is always possible to override this method and implement any custom reset procedure, as needed.\nemitter - receiver scheme\ncurrently, the communication between the supervisor and the robot is achieved via an emitter and a receiver. separating the supervisor from the robot, deepbots can fit a variety of use-cases, e.g. multiple robots collecting experience and a supervisor controlling them with a single agent. the way webots implements emitter/receiver communication requires messages to be packed and unpacked, which introduces an overhead that becomes prohibiting in use-cases where the observations are high-dimensional or long, such as camera images. deepbots provides another partially abstract class that combines the supervisor and the robot into one controller and circumvents that issue, while being less flexible, which is discussed later.\non one hand, the emitter is an entity which is provided by webots, that broadcasts messages to the world. on the other hand, the receiver is an entity that is used to receive messages from the world. consequently, the agent-environment loop is transformed accordingly. firstly, the robot uses its sensors to retrieve the observation from the world and in turn uses the emitter component to broadcast this observation. secondly, the supervisor receives the observation via the receiver component and in turn, the agent uses it to choose an action. it should be noted that the observation the agent uses might be extended from the supervisor. for example, a model might use lidar sensors installed on the robot, but also the euclidean distance between the robot and an object. as it is expected, the robot does not know the euclidean distance, only the supervisor can calculate it, because it has access to all entities in the world.\nyou can follow the emitter-receiver scheme tutorial to get started and work your way up from there.\ncombined robot-supervisor scheme\nas mentioned earlier, in use-cases where the observation transmitted between the robot and the supervisor is high-dimensional or long, e.g. high resolution images taken from a camera, a significant overhead is introduced. this is circumvented by inheriting and implementing the partially abstract robotsupervisor that combines the robot controller and the supervisor controller into one, forgoing all emitter/receiver communication. this new controller runs on the robot, but requires supervisor privileges and is limited to one robot, one supervisor.\nyou can follow the robot-supervisor scheme tutorial to get started and work your way up from there. we recommended this tutorial to get started with deepbots.\nabstraction levels\nthe deepbots framework has been created mostly for educational purposes. the aim of the framework is to enable people to use reinforcement learning in webots. more specifically, we can consider deepbots as a wrapper of webots exposing an openai gym style interface. for this reason there are multiple levels of abstraction. for example, a user can choose if they want to use csv emitter/receiver or if they want to make an implementation from scratch. in the top level of the abstraction hierarchy is the supervisorenv which is the openai gym interface. below that level there are partially implemented classes with common functionality. these implementations aim to hide the communication between the supervisor and the robot, as described in the two different schemes ealier. similarly, in the emitter/receiver scheme the robot also has different abstraction levels. according to their needs, users can choose either to process the messages received from the supervisor themselves or use the existing implementations.\nacknowledgments\nthis project has received funding from the european union's horizon 2020 research and innovation programme under grant agreement no 871449 (opendr). this publication reflects the authors\u2019 views only. the european commission is not responsible for any use that may be made of the information it contains.\ncontributors \u2728\nthanks goes to these wonderful people (emoji key):\nmanos kirtas\n\ud83d\udcbb\nkostas tsampazis\n\ud83d\udcbb\njiun kai yang\n\ud83d\udcbb\nmentalgear\n\ud83e\udd14\ndreamtale\n\ud83d\udc1b\nnikolaos kokkinis-ntrenis\n\ud83d\udcbb \ud83d\udcd6 \ud83e\udd14\nthis project follows the all-contributors specification. contributions of any kind welcome!\nspecial thanks to papanikolaou evangelia for designing project's logo!", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000302, "year": null}, {"Unnamed: 0": 1382, "autor": 362, "date": null, "content": "k: Kinematics library for rust-lang\nk has below functionalities.\nForward kinematics\nInverse kinematics\nURDF Loader\nk uses nalgebra as math library.\nSee Document and examples/ for more details.\nAPI is unstable.\nRequirements to build examples\nsudo apt install g++ cmake xorg-dev libglu1-mesa-dev\nIK example with GUI\ncargo run --release --example interactive_ik\nPush below keys to move the end of the manipulator.\nf: forward\nb: backward\np: up\nn: down\nl: left\nr: right\nz: reset the manipulator state.\nCreate link tree from urdf and solve IK\nuse k::prelude::*;\nfn main() {\n// Load urdf file\nlet chain = k::Chain::<f32>::from_urdf_file(\"urdf/sample.urdf\").unwrap();\nprintln!(\"chain: {}\", chain);\n// Set initial joint angles\nlet angles = vec![0.2, 0.2, 0.0, -1.0, 0.0, 0.0, 0.2, 0.2, 0.0, -1.0, 0.0, 0.0];\nchain.set_joint_positions(&angles).unwrap();\nprintln!(\"initial angles={:?}\", chain.joint_positions());\nlet target_link = chain.find(\"l_wrist_pitch\").unwrap();\n// Get the transform of the end of the manipulator (forward kinematics)\nchain.update_transforms();\nlet mut target = target_link.world_transform().unwrap();\nprintln!(\"initial target pos = {}\", target.translation);\nprintln!(\"move z: +0.1\");\ntarget.translation.vector.z += 0.1;\n// Create IK solver with default settings\nlet solver = k::JacobianIkSolver::default();\n// Create a set of joints from end joint\nlet arm = k::SerialChain::from_end(target_link);\n// solve and move the manipulator angles\nsolver.solve(&arm, &target).unwrap();\nprintln!(\"solved angles={:?}\", chain.joint_positions());\nchain.update_transforms();\nlet solved_pose = target_link.world_transform().unwrap();\nprintln!(\"solved target pos = {}\", solved_pose.translation);\n}\nStructure of API\nTop level interface is Chain struct. It contains Nodes and they have the relations between nodes (parent/children). Actual data (joint angle(position), transform between nodes) is stored in Joint object inside nodes.\nYou can get local/world transform of nodes. See below figure to understand what is the node's local_transform() and world_transform().", "link": "https://github.com/openrr/k", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "k: kinematics library for rust-lang\nk has below functionalities.\nforward kinematics\ninverse kinematics\nurdf loader\nk uses nalgebra as math library.\nsee document and examples/ for more details.\napi is unstable.\nrequirements to build examples\nsudo apt install g++ cmake xorg-dev libglu1-mesa-dev\nik example with gui\ncargo run --release --example interactive_ik\npush below keys to move the end of the manipulator.\nf: forward\nb: backward\np: up\nn: down\nl: left\nr: right\nz: reset the manipulator state.\ncreate link -----> tree !!!  from urdf and solve ik\nuse k::prelude::*;\nfn main() {\n// load urdf file\nlet chain = k::chain::<f32>::from_urdf_file(\"urdf/sample.urdf\").unwrap();\nprintln!(\"chain: {}\", chain);\n// set initial joint angles\nlet angles = vec![0.2, 0.2, 0.0, -1.0, 0.0, 0.0, 0.2, 0.2, 0.0, -1.0, 0.0, 0.0];\nchain.set_joint_positions(&angles).unwrap();\nprintln!(\"initial angles={:?}\", chain.joint_positions());\nlet target_link = chain.find(\"l_wrist_pitch\").unwrap();\n// get the transform of the end of the manipulator (forward kinematics)\nchain.update_transforms();\nlet mut target = target_link.world_transform().unwrap();\nprintln!(\"initial target pos = {}\", target.translation);\nprintln!(\"move z: +0.1\");\ntarget.translation.vector.z += 0.1;\n// create ik solver with default settings\nlet solver = k::jacobianiksolver::default();\n// create a set of joints from end joint\nlet arm = k::serialchain::from_end(target_link);\n// solve and move the manipulator angles\nsolver.solve(&arm, &target).unwrap();\nprintln!(\"solved angles={:?}\", chain.joint_positions());\nchain.update_transforms();\nlet solved_pose = target_link.world_transform().unwrap();\nprintln!(\"solved target pos = {}\", solved_pose.translation);\n}\nstructure of api\ntop level interface is chain struct. it contains nodes and they have the relations between nodes (parent/children). actual data (joint angle(position), transform between nodes) is stored in joint object inside nodes.\nyou can get local/world transform of nodes. see below figure to understand what is the node's local_transform() and world_transform().", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000362, "year": null}, {"Unnamed: 0": 1404, "autor": 384, "date": null, "content": "MATLAB implementation of RRT, RRT* and RRT*FN algorithms.\nWhat is RRT, RRT* and RRT*FN\nRRT (Rapidly-Exploring Random Tree) is a sampling-based algorithm for solving path planning problem. RRT provides feasable solution if time of RRT tends to infinity.\nRRT* is a sampling-based algorithm for solving motion planning problem, which is an probabilistically optimal variant of RRT. RRT* converges to the optimal solution asymptotically.\nRRT*FN is a sampling-based algorithm based on RRT*. RRT*FN inherents asymptotical convergence to the optimal solution, however RRT*FN implements it using less memory.\nHow to use\nThe original package contains 3 files containing algorithm\nrrt.m\nrrt_star.m\nrrt_star_fn.m\nand 2 files containing classes that enable algorithm to solve problems for simple 2D mobile robot model and nDOF Redundant Manipulator\nFNSimple2D.m\nFNRedundantManipulator.m\nOne can add other additional models implementing all methods mentioned in rrt.m, rrt_star.m, rrt_star_fn.m\nAuthors\nOlzhas Adiyatov oadiyatov[at]nu.edu.kz\nAtakan Varol\n(c) 2013", "link": "https://github.com/olzhas/rrt_toolbox", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "matlab implementation of rrt, rrt* and rrt*fn algorithms.\nwhat is rrt, rrt* and rrt*fn\nrrt (rapidly-exploring random -----> tree !!! ) is a sampling-based algorithm for solving path planning problem. rrt provides feasable solution if time of rrt tends to infinity.\nrrt* is a sampling-based algorithm for solving motion planning problem, which is an probabilistically optimal variant of rrt. rrt* converges to the optimal solution asymptotically.\nrrt*fn is a sampling-based algorithm based on rrt*. rrt*fn inherents asymptotical convergence to the optimal solution, however rrt*fn implements it using less memory.\nhow to use\nthe original package contains 3 files containing algorithm\nrrt.m\nrrt_star.m\nrrt_star_fn.m\nand 2 files containing classes that enable algorithm to solve problems for simple 2d mobile robot model and ndof redundant manipulator\nfnsimple2d.m\nfnredundantmanipulator.m\none can add other additional models implementing all methods mentioned in rrt.m, rrt_star.m, rrt_star_fn.m\nauthors\nolzhas adiyatov oadiyatov[at]nu.edu.kz\natakan varol\n(c) 2013", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000384, "year": null}, {"Unnamed: 0": 1452, "autor": 432, "date": null, "content": "CLOiSim : Multi-Robot Simulator\nHappy to announce CLOiSim. It is a new multi-robot simulator that uses an SDF file containing 3d world environemnts and robot descriptions.\nThe simulator is based on Unity 3D. It may look similar to Gazebo, where, unfortunately, we encountered performance problems while loading multiple robots equipped with multiple sensors. Hence, CLOiSim.\nThis project consists of\nSDF Parser for C#\nSDF Robot Implementation for Unity -> Visual / Collision / Sensor / Physics for joints\nSDF Plugins for Unity3D\nUI modules -> Module for on-screen information\nNetwork modules -> Module for transporting sensor data or control signal\nWeb service -> Module for controling simulation through a web interface\nvideo link\nFeatures\nSensors\nThe current release includes the features only for marked items in the list below. Other sensor models are work in progress. Here are the list of items that is implemented(marked) or planned to be implemented.\nJoint models\n2-Wheeled Motor driving\nJoint control\nSensor models\nLiDAR Sensor\n2D\n3D\nSonar sensor\nIMU\nContact\nCamera\nCamera intrinsic parameter\nDepth Camera\nMulti-camera\nRealSense (RGB + IR1 + IR2 + Depth)\nGPS sensor\nSensor noise models\nGaussian\nGPS, IMU\nLidar\nCamera\nCustom\nPhysics\nSupport all physics parameters in SDF specification\nSupport <Joint type=\"revolute2\">\nWorlds\nActors\ninterpolate_x in <animation>\nLights\nsupporting <specular>, <attenuation/linear>, <attenuation/contant>, <attenuation/quadratic>, <spot/falloff>\nSpherical Coordinates\nPlus, SDF works on the essential elements such as <model>, <link>, <visual>, <collision>, <joint>, etc. It does not support optional elmenets like <wind>, <audio>, <state>, <atmosphere>, <magnetic_field>, <scene>, <road>, <population>.\nCurrently, geometry mesh type is supporting only 'Wavefront(.obj) with material', 'Collada(.dae) including animation' and 'STL(.stl)'. <ambient> elements in <materal> and ambient properies in mesh files are not support in CLOiSim.\nvideo link\nSensor Plugins\nIt called 'CLOiSimPlugin'. And below plugins could be utilized through write an element on SDF.\nPlugin name should be written in filename attribute and it's case sensitive.\nFor example,\n<plugin name=\"actor_plugin\" filename=\"libActorPlugin.so\" />\nmore details in here).\nModel Specific\nLaserPlugin: help to publish 2D or 3D lidar data\nCameraPlugin: help to publish 2D color image data or depth image data\nMultiCameraPlugin: help to publish multiple color image data\nRealSensePlugin: can handle ir1(left), ir2(right), depth, color\nMicomPlugin: control micom input/output(sensor)\nGpsPlugin: gps position in world\nActorPlugin: add actor control functionality using AI(Unity) components\nWorld Specific\nElevatorSystemPlugin: control(lifting, cal) elevators\nGroundTruthPlugin: retrieve all information(position, size, velocity) for objects\nActorControlPlugin: controls actor using AI(Unity) components(actor which loaded ActorPlugin)\nHow it works\nRefer to core codes in 'Assets/Scripts'.\nLoad SDF file -> Parse SDF(simulation description) -> Implement and realize description\nShaders are also used to get depth buffer information in a few sensor model.\nDefault physics engine 'Nvidia PhysX' is used for physics. And it retrieves some of physics parameters from <ode> in sdf. 'SDFPlugins' help physics tricky handling for jointing <link> ojbects by <joint> element.\nWe've deceided to change a solver type of physics engine since new solver \"TGS(Temporal Gauss Seidel)\" is intorduced recently(PhysX 4.1).\nSo there is NO more constaints for rigidbodies by PGS(Projected Gauss Seidel) solver type since latest version(CLOiSim-1.11.0).\nFor the performance in terms of collision handling, designing collision geometry properly may important.\nan aspect of rendering\nif <name> element of <script> element in <material> element contains \"tree\" words, CLOiSim applies \"URP/Nature/SpeedTree\" Shader as a rendering material.\nGetting Started\nMinimum requirement\nProcessor: testing and looking for the minimum\nMemory: testing and looking for the minimum\nGraphics: testing and looking for the minimum\nTested environement\nLatest Unity Editor Version: '2020.3.17f1 (LTS)'.\nLinux Machine\nOS: Ubuntu 20.04.2 LTS\nProcessor: AMD\u00ae Ryzen 9 3900x 12-core processor \u00d7 24\nMemory: 32GB\nGraphics: NVIDIA Corporation [GeForce RTX 3090]\nWindows Machine\nOS: Windows 10 20H2\nProcessor: AMD\u00ae Ryzen 9 5900HS 8-core processor x 16\nMemory: 32GB\nGraphics: NVIDIA GeForce RTX3060 Laptop GPU\nRelease version\nIf you don't want to build a project, just USE a release binary(Download linux version). And just refer to 'Usage' section.\nIn terms of branch, 'main' is release(stable) version, and 'develop' is used for development(on-going).\nIf you want to build a project\nPlease visit here build guide.\nUsage\nRun 'CLOiSim'\nSet environment path like below. You can find the sample resources here\nMultiple path can be set by :(colon).\nexport CLOISIM_FILES_PATH=\"/home/Unity/cloisim/sample_resources/media\"\nexport CLOISIM_MODEL_PATH=\"/home/Unity/cloisim/sample_resources/models:/home/Unity/cloisim/another_resources/models\"\nexport CLOISIM_WORLD_PATH=\"/home/Unity/cloisim/sample_resources/worlds\"\nRun CLOiSim\n./CLOiSim.x86_64 -world lg_seocho.world\nor you can execute './run.sh' script in release binary version.\n./run.sh cloisim.world\nRun 'cloisim_ros' after running CLOiSim\nYou need to run this package in order to publish sensor data in ROS2.\nRun bringup node in 'cloisim_ros' ros2 packages\nThat's it. Have fun!!!\nDebugging log\ntail -f ~/.config/unity3d/lge-arlab/CLOiSim/Player.log\nControl and external UI service\nCLOiSim supports web-based control service through websocket as an external interface.\nwebsocket service path: ws://127.0.0.1:8080/{service-name}\nYou can add markers like line, text, box, or sphere point and reset simulation by just sending a request data as a JSON format.\nRead detail guide\nvideo link\nFuture Plan\nNew features or functions shall be developed on demand.\nFully support to keep up with 'SDF specifiaction version 1.8'\nAdd new sensor models and enhance sensor performance\nintroduce programmable c++ plugin\nPerformance optimization for sensors (Use DOT by unity?)\nUpgrade quality of graphical elements\nIf you have any troubles or issues, please don't hesitate to create a new issue on 'Issues'. https://github.com/lge-ros2/cloisim/issues\n\uac10\uc0ac\ud569\ub2c8\ub2e4. Thank you", "link": "https://github.com/lge-ros2/cloisim", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "cloisim : multi-robot simulator\nhappy to announce cloisim. it is a new multi-robot simulator that uses an sdf file containing 3d world environemnts and robot descriptions.\nthe simulator is based on unity 3d. it may look similar to gazebo, where, unfortunately, we encountered performance problems while loading multiple robots equipped with multiple sensors. hence, cloisim.\nthis project consists of\nsdf parser for c#\nsdf robot implementation for unity -> visual / collision / sensor / physics for joints\nsdf plugins for unity3d\nui modules -> module for on-screen information\nnetwork modules -> module for transporting sensor data or control signal\nweb service -> module for controling simulation through a web interface\nvideo link\nfeatures\nsensors\nthe current release includes the features only for marked items in the list below. other sensor models are work in progress. here are the list of items that is implemented(marked) or planned to be implemented.\njoint models\n2-wheeled motor driving\njoint control\nsensor models\nlidar sensor\n2d\n3d\nsonar sensor\nimu\ncontact\ncamera\ncamera intrinsic parameter\ndepth camera\nmulti-camera\nrealsense (rgb + ir1 + ir2 + depth)\ngps sensor\nsensor noise models\ngaussian\ngps, imu\nlidar\ncamera\ncustom\nphysics\nsupport all physics parameters in sdf specification\nsupport <joint type=\"revolute2\">\nworlds\nactors\ninterpolate_x in <animation>\nlights\nsupporting <specular>, <attenuation/linear>, <attenuation/contant>, <attenuation/quadratic>, <spot/falloff>\nspherical coordinates\nplus, sdf works on the essential elements such as <model>, <link>, <visual>, <collision>, <joint>, etc. it does not support optional elmenets like <wind>, <audio>, <state>, <atmosphere>, <magnetic_field>, <scene>, <road>, <population>.\ncurrently, geometry mesh type is supporting only 'wavefront(.obj) with material', 'collada(.dae) including animation' and 'stl(.stl)'. <ambient> elements in <materal> and ambient properies in mesh files are not support in cloisim.\nvideo link\nsensor plugins\nit called 'cloisimplugin'. and below plugins could be utilized through write an element on sdf.\nplugin name should be written in filename attribute and it's case sensitive.\nfor example,\n<plugin name=\"actor_plugin\" filename=\"libactorplugin.so\" />\nmore details in here).\nmodel specific\nlaserplugin: help to publish 2d or 3d lidar data\ncameraplugin: help to publish 2d color image data or depth image data\nmulticameraplugin: help to publish multiple color image data\nrealsenseplugin: can handle ir1(left), ir2(right), depth, color\nmicomplugin: control micom input/output(sensor)\ngpsplugin: gps position in world\nactorplugin: add actor control functionality using ai(unity) components\nworld specific\nelevatorsystemplugin: control(lifting, cal) elevators\ngroundtruthplugin: retrieve all information(position, size, velocity) for objects\nactorcontrolplugin: controls actor using ai(unity) components(actor which loaded actorplugin)\nhow it works\nrefer to core codes in 'assets/scripts'.\nload sdf file -> parse sdf(simulation description) -> implement and realize description\nshaders are also used to get depth buffer information in a few sensor model.\ndefault physics engine 'nvidia physx' is used for physics. and it retrieves some of physics parameters from <ode> in sdf. 'sdfplugins' help physics tricky handling for jointing <link> ojbects by <joint> element.\nwe've deceided to change a solver type of physics engine since new solver \"tgs(temporal gauss seidel)\" is intorduced recently(physx 4.1).\nso there is no more constaints for rigidbodies by pgs(projected gauss seidel) solver type since latest version(cloisim-1.11.0).\nfor the performance in terms of collision handling, designing collision geometry properly may important.\nan aspect of rendering\nif <name> element of <script> element in <material> element contains \"-----> tree !!! \" words, cloisim applies \"urp/nature/speedtree\" shader as a rendering material.\ngetting started\nminimum requirement\nprocessor: testing and looking for the minimum\nmemory: testing and looking for the minimum\ngraphics: testing and looking for the minimum\ntested environement\nlatest unity editor version: '2020.3.17f1 (lts)'.\nlinux machine\nos: ubuntu 20.04.2 lts\nprocessor: amd\u00ae ryzen 9 3900x 12-core processor \u00d7 24\nmemory: 32gb\ngraphics: nvidia corporation [geforce rtx 3090]\nwindows machine\nos: windows 10 20h2\nprocessor: amd\u00ae ryzen 9 5900hs 8-core processor x 16\nmemory: 32gb\ngraphics: nvidia geforce rtx3060 laptop gpu\nrelease version\nif you don't want to build a project, just use a release binary(download linux version). and just refer to 'usage' section.\nin terms of branch, 'main' is release(stable) version, and 'develop' is used for development(on-going).\nif you want to build a project\nplease visit here build guide.\nusage\nrun 'cloisim'\nset environment path like below. you can find the sample resources here\nmultiple path can be set by :(colon).\nexport cloisim_files_path=\"/home/unity/cloisim/sample_resources/media\"\nexport cloisim_model_path=\"/home/unity/cloisim/sample_resources/models:/home/unity/cloisim/another_resources/models\"\nexport cloisim_world_path=\"/home/unity/cloisim/sample_resources/worlds\"\nrun cloisim\n./cloisim.x86_64 -world lg_seocho.world\nor you can execute './run.sh' script in release binary version.\n./run.sh cloisim.world\nrun 'cloisim_ros' after running cloisim\nyou need to run this package in order to publish sensor data in ros2.\nrun bringup node in 'cloisim_ros' ros2 packages\nthat's it. have fun!!!\ndebugging log\ntail -f ~/.config/unity3d/lge-arlab/cloisim/player.log\ncontrol and external ui service\ncloisim supports web-based control service through websocket as an external interface.\nwebsocket service path: ws://127.0.0.1:8080/{service-name}\nyou can add markers like line, text, box, or sphere point and reset simulation by just sending a request data as a json format.\nread detail guide\nvideo link\nfuture plan\nnew features or functions shall be developed on demand.\nfully support to keep up with 'sdf specifiaction version 1.8'\nadd new sensor models and enhance sensor performance\nintroduce programmable c++ plugin\nperformance optimization for sensors (use dot by unity?)\nupgrade quality of graphical elements\nif you have any troubles or issues, please don't hesitate to create a new issue on 'issues'. https://github.com/lge-ros2/cloisim/issues\n\uac10\uc0ac\ud569\ub2c8\ub2e4. thank you", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000432, "year": null}, {"Unnamed: 0": 1468, "autor": 448, "date": null, "content": "IncrementalInference.jl\nClick on badges to follow links:\nStable Release Dev branch Coverage Documentation\nOptimization routines for incremental non-parametric and parametric solutions based on factor graphs and the Bayes (Junction) tree implemented in the Julia language (and JuliaPro).\nIntroduction\nThis package implements a few different non-Gaussian factor graph inference algorithms, primarily\nMulti-Modal iSAM (MM-iSAM) (see references) which does hybrid non-parametric and parametric inference/state-estimation over large factor graphs.\nBatch Parametric (akin to conventional \"non-linear least squares\"),\nMax-mixtures parametric,\nOther multiparametric and non-Gaussian algorithms are in the works and will be announced in due course.\nFundamentally, inference is performed via the Bayes (junction) tree where Chapman-Kolmogorov transit integral solutions are based on marginal-joint belief estimation (a sum-product / belief-propagation approximation algorithm). Many benefits such as clique recycling are also available. See the common Caesar.jl documenation for more details.\nThis package forms a cardinal piece of the Caesar.jl robotics toolkit, including 3D visualization and database interaction, which can serve as a base station for a robotic platform. A standalone Robot Motion Estimate (RoME.jl) package extends the available variables, factors, and utilities for use in robotic navigation.\nNote, that IncrementalInference.jl does not have to be used with RoME.jl / Caesar.jl -- IncrementalInference.jl only implements the algebraic inference operations against mathematical abstractions such as Manifolds.jl.\nFurthermore, please contact info@navability.io for more formal support on this package, NavAbility.io.\nInstallation\nInstall the package from inside Julia\n(v1.6) pkg> add IncrementalInference\nPre-install the following packages system wide packages[, and easily draw factor graph and Bayes tree]:\nsudo apt-get install hdf5-tools\nsudo apt-get install graphviz xdot # optional\nExamples\nSee the common Caesar.jl documenation for more details . Further examples can be found in the examples and test folders.\nCite and Contributors\nWe are grateful for many, many contributions within the Julia package ecosystem -- see the Project.toml files for a far reaching list of upstream packages and contributions.\nConsider citing our work using the common reference at Caesar.jl Citation with IncrementalInference.jl DOI\nGet Involved, and Code of Conduct\nThis project adheres to the JuliaRobotics code of conduct, and we invite contributions or comments from the community. Use the slack channel, Julia Discourse, or Github issues to get in touch.\nReferences\nSee references of interest here", "link": "https://github.com/JuliaRobotics/IncrementalInference.jl", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "incrementalinference.jl\nclick on badges to follow links:\nstable release dev branch coverage documentation\noptimization routines for incremental non-parametric and parametric solutions based on factor graphs and the bayes (junction) -----> tree !!!  implemented in the julia language (and juliapro).\nintroduction\nthis package implements a few different non-gaussian factor graph inference algorithms, primarily\nmulti-modal isam (mm-isam) (see references) which does hybrid non-parametric and parametric inference/state-estimation over large factor graphs.\nbatch parametric (akin to conventional \"non-linear least squares\"),\nmax-mixtures parametric,\nother multiparametric and non-gaussian algorithms are in the works and will be announced in due course.\nfundamentally, inference is performed via the bayes (junction) tree where chapman-kolmogorov transit integral solutions are based on marginal-joint belief estimation (a sum-product / belief-propagation approximation algorithm). many benefits such as clique recycling are also available. see the common caesar.jl documenation for more details.\nthis package forms a cardinal piece of the caesar.jl robotics toolkit, including 3d visualization and database interaction, which can serve as a base station for a robotic platform. a standalone robot motion estimate (rome.jl) package extends the available variables, factors, and utilities for use in robotic navigation.\nnote, that incrementalinference.jl does not have to be used with rome.jl / caesar.jl -- incrementalinference.jl only implements the algebraic inference operations against mathematical abstractions such as manifolds.jl.\nfurthermore, please contact info@navability.io for more formal support on this package, navability.io.\ninstallation\ninstall the package from inside julia\n(v1.6) pkg> add incrementalinference\npre-install the following packages system wide packages[, and easily draw factor graph and bayes tree]:\nsudo apt-get install hdf5-tools\nsudo apt-get install graphviz xdot # optional\nexamples\nsee the common caesar.jl documenation for more details . further examples can be found in the examples and test folders.\ncite and contributors\nwe are grateful for many, many contributions within the julia package ecosystem -- see the project.toml files for a far reaching list of upstream packages and contributions.\nconsider citing our work using the common reference at caesar.jl citation with incrementalinference.jl doi\nget involved, and code of conduct\nthis project adheres to the juliarobotics code of conduct, and we invite contributions or comments from the community. use the slack channel, julia discourse, or github issues to get in touch.\nreferences\nsee references of interest here", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000448, "year": null}, {"Unnamed: 0": 1486, "autor": 466, "date": null, "content": "flame_ros\nFLaME (Fast Lightweight Mesh Estimation) is a lightweight, CPU-only method for dense online monocular depth estimation. Given a sequence of camera images with known poses, FLaME is able to reconstruct dense 3D meshes of the environment by posing the depth estimation problem as a variational optimization over a Delaunay graph that can be solved at framerate, even on computationally constrained platforms.\nThe flame_ros repository contains the ROS bindings, visualization code, and offline frontends for the algorithm. The core library can be found here.\nRelated Publications:\nFLaME: Fast Lightweight Mesh Estimation using Variational Smoothing on Delaunay Graphs, W. Nicholas Greene and Nicholas Roy, ICCV 2017.\nAuthor\nW. Nicholas Greene (wng@csail.mit.edu)\nQuickstart\nBuild the provided Docker image and run an example dataset (requires nvidia-docker for rviz):\n# Build the image.\ncd flame_ros\ndocker build --rm -t flame -f scripts/Dockerfile .\n# Run an example dataset.\n./scripts/flame_docker_example.sh\nYou may need to run xhost +local:root in order to forward rviz outside the container.\nDependencies\nUbuntu 16.04\nROS Kinetic\nOpenCV 3.2\nBoost 1.54\nPCL 1.7\nEigen 3.2\nSophus (SHA: b474f05f839c0f63c281aa4e7ece03145729a2cd)\nflame\ncatkin_tools (optional)\nInstallation\nNOTE: These instructions assume you are running ROS Kinetic on Ubuntu 16.04 and are interested in installing both flame and flame_ros. See the installation instructions for flame if you only wish to install flame.\nInstall apt dependencies:\nsudo apt-get install libboost-all-dev libpcl-dev python-catkin-tools\nCreate a Catkin workspace using catkin_tools:\n# Source ROS.\nsource /opt/ros/kinetic/setup.sh\n# Create workspace source folder.\nmkdir -p flame_ws/src\n# Checkout flame and flame_ros into workspace.\ncd flame_ws/src\ngit clone https://github.com/robustrobotics/flame.git\ngit clone https://github.com/robustrobotics/flame_ros.git\n# Initialize workspace.\ncd ..\ncatkin init\n# Install ROS dependencies using rosdep.\nrosdep install -iy --from-paths ./src\nInstall Eigen 3.2 and Sophus using the scripts provided with flame:\n# Create a dependencies folder.\nmkdir -p flame_ws/dependencies/src\n# Checkout Eigen and Sophus into ./dependencies/src and install into ./dependencies.\ncd flame_ws\n./src/flame/scripts/eigen.sh ./dependencies/src ./dependencies\n./src/flame/scripts/sophus.sh ./dependencies/src ./dependencies\n# Copy and source environment variable script:\ncp ./src/flame/scripts/env.sh ./dependencies/\nsource ./dependencies/env.sh\nBuild workspace:\n# Build!\ncatkin build\n# Source workspace.\nsource ./devel/setup.sh\nOffline Processing\nTwo types of offline nodes are provided, one to process video from the EuRoC MAV Dataset and one to process video from the TUM RGB-D SLAM Benchmark.\nEuRoC Data\nFirst, download and extract one of the ASL datasets here (the Vicon Room datasets should work well).\nNext, update the parameter file in flame_ros/cfg/flame_offline_asl.yaml to point to where you extracted the data:\npose_path: <path_to_dataset>/mav0/state_groundtruth_estimate0\nrgb_path: <path_to_dataset>/mav0/cam0\nFinally, to process the data launch:\nroslaunch flame_ros flame_offline_asl.launch\nThe mesh should be published on the /flame/mesh topic. To visualize this topic in rviz, consult the the Visualization section.\nTUM Data\nFirst, download and extract one of the datasets here (fr3/structure_texture_far or fr3/long_office_household should work well). Use the associate.py script here to associate the pose (groundtruth.txt) and RGB (rgb.txt) files (you can associate the depthmaps as well).\nA ROS-compliant camera calibration YAML file will be needed. You can use the one provided in flame_ros/cfg/kinect.yaml, which has the default parameters for the Microsoft Kinect used to collect the TUM datasets.\nNext, update the parameter file in flame_ros/cfg/flame_offline_tum.yaml to point to where you extracted the data:\ninput_file: <path_to_dataset>/groundtruth_rgb.txt\ncalib_file: <path_to_flame_ros>/cfg/kinect.yaml\nFinally, to process the data launch:\nroslaunch flame_ros flame_offline_tum.launch\nThe mesh should be published on the /flame/mesh topic. To visualize this topic in rviz, consult the the Visualization section.\nOnline Processing\nThe online nodelet can be launched using flame_nodelet.launch:\nroslaunch flame_ros flame_nodelet.launch image:=/image\nwhere /image is your live rectified/undistorted image stream. The frame_id of this topic must correspond to a Right-Down-Forward frame attached to the camera. The tf tree must be complete such that the pose of the camera in the world frame can be resolved by tf.\nThe mesh should be published on the /flame/mesh topic. To visualize this topic in rviz, consult the the Visualization section.\nThe flame_nodelet.launch launch file loads the parameters listed in flame_ros/cfg/flame_nodelet.yaml. You may need to update the input/camera_frame_id param for your data. See the Parameters section for more parameter information.\nVisualization\nYou can use the provided configuration file (flame_ros/cfg/flame.rviz) to visualize the output data in rviz. This approach uses a custom rviz plugin to render the /flame/mesh messages. flame_ros can also publish the depth data in other formats (e.g. as a sensor_msgs/PointCloud2 or a sensor_msgs/Image), which can be visualized by enabling the corresponding plugins.\nParameters\nThere are many parameters that control processing, but only a handful are particularly important:\noutput/*: Controls what type of output messages are produced. If you are concerned about speed, you should prefer publishing only the mesh data (output/mesh: True), but other types of output can be enabled here.\ndebug/*: Controls what type of debug images are published. Creating these images is relatively expensive, so disable for real-time operation.\nthreading/openmp/*: Controls OpenMP-accelerated sections. You may wish to tune the number of threads per parallel section (threading/openmp/num_threads) or the number of chunks per thread (threading/openmp/chunk_size) for your processor.\nfeatures/detection/min_grad_mag: Controls the minimum gradient magnitude for detected features.\nfeatures/detection/win_size: Features are detected by dividing the image domain into win_size x win_size blocks and selecting the best trackable pixel inside each block. Set to a large number (e.g. 32) for coarse, but fast reconstructions, and a small number (e.g. 8) for finer reconstructions.\nregularization/nltgv2/data_factor: Controls the balance between smoothing and data-fitting in the regularizer. It should be set in relation to the detection window size. Some good values are 0.1-0.25.\nPerformance Tips\nFor best results use a high framerate (>= 30 Hz) camera with VGA-sized images. Higher resolution images will require more accurate poses. The feature detection window size (features/detection/win_size) and the data scaling term (regularization/nltgv2/data_factor) are the primary knobs for tuning performance. The default parameters should work well in most cases, but you may need to tune for your specific data.\nBy default, flame_ros will publish several debug images. While helpful to observe during operation, they will slow down the pipeline. Disable them if you are trying to increase throughput.\nThe usual tips for monocular SLAM/depth estimation systems also apply:\nPrefer slow translational motion\nAvoid fast rotations when possible\nUse an accurate pose source (e.g. one of the many available visual odometry/SLAM packages)\nPrefer texture-rich environments\nPrefer environments with even lighting", "link": "https://github.com/robustrobotics/flame_ros", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "flame_ros\nflame (fast lightweight mesh estimation) is a lightweight, cpu-only method for dense online monocular depth estimation. given a sequence of camera images with known poses, flame is able to reconstruct dense 3d meshes of the environment by posing the depth estimation problem as a variational optimization over a delaunay graph that can be solved at framerate, even on computationally constrained platforms.\nthe flame_ros repository contains the ros bindings, visualization code, and offline frontends for the algorithm. the core library can be found here.\nrelated publications:\nflame: fast lightweight mesh estimation using variational smoothing on delaunay graphs, w. nicholas greene and nicholas roy, iccv 2017.\nauthor\nw. nicholas greene (wng@csail.mit.edu)\nquickstart\nbuild the provided docker image and run an example dataset (requires nvidia-docker for rviz):\n# build the image.\ncd flame_ros\ndocker build --rm -t flame -f scripts/dockerfile .\n# run an example dataset.\n./scripts/flame_docker_example.sh\nyou may need to run xhost +local:root in order to forward rviz outside the container.\ndependencies\nubuntu 16.04\nros kinetic\nopencv 3.2\nboost 1.54\npcl 1.7\neigen 3.2\nsophus (sha: b474f05f839c0f63c281aa4e7ece03145729a2cd)\nflame\ncatkin_tools (optional)\ninstallation\nnote: these instructions assume you are running ros kinetic on ubuntu 16.04 and are interested in installing both flame and flame_ros. see the installation instructions for flame if you only wish to install flame.\ninstall apt dependencies:\nsudo apt-get install libboost-all-dev libpcl-dev python-catkin-tools\ncreate a catkin workspace using catkin_tools:\n# source ros.\nsource /opt/ros/kinetic/setup.sh\n# create workspace source folder.\nmkdir -p flame_ws/src\n# checkout flame and flame_ros into workspace.\ncd flame_ws/src\ngit clone https://github.com/robustrobotics/flame.git\ngit clone https://github.com/robustrobotics/flame_ros.git\n# initialize workspace.\ncd ..\ncatkin init\n# install ros dependencies using rosdep.\nrosdep install -iy --from-paths ./src\ninstall eigen 3.2 and sophus using the scripts provided with flame:\n# create a dependencies folder.\nmkdir -p flame_ws/dependencies/src\n# checkout eigen and sophus into ./dependencies/src and install into ./dependencies.\ncd flame_ws\n./src/flame/scripts/eigen.sh ./dependencies/src ./dependencies\n./src/flame/scripts/sophus.sh ./dependencies/src ./dependencies\n# copy and source environment variable script:\ncp ./src/flame/scripts/env.sh ./dependencies/\nsource ./dependencies/env.sh\nbuild workspace:\n# build!\ncatkin build\n# source workspace.\nsource ./devel/setup.sh\noffline processing\ntwo types of offline nodes are provided, one to process video from the euroc mav dataset and one to process video from the tum rgb-d slam benchmark.\neuroc data\nfirst, download and extract one of the asl datasets here (the vicon room datasets should work well).\nnext, update the parameter file in flame_ros/cfg/flame_offline_asl.yaml to point to where you extracted the data:\npose_path: <path_to_dataset>/mav0/state_groundtruth_estimate0\nrgb_path: <path_to_dataset>/mav0/cam0\nfinally, to process the data launch:\nroslaunch flame_ros flame_offline_asl.launch\nthe mesh should be published on the /flame/mesh topic. to visualize this topic in rviz, consult the the visualization section.\ntum data\nfirst, download and extract one of the datasets here (fr3/structure_texture_far or fr3/long_office_household should work well). use the associate.py script here to associate the pose (groundtruth.txt) and rgb (rgb.txt) files (you can associate the depthmaps as well).\na ros-compliant camera calibration yaml file will be needed. you can use the one provided in flame_ros/cfg/kinect.yaml, which has the default parameters for the microsoft kinect used to collect the tum datasets.\nnext, update the parameter file in flame_ros/cfg/flame_offline_tum.yaml to point to where you extracted the data:\ninput_file: <path_to_dataset>/groundtruth_rgb.txt\ncalib_file: <path_to_flame_ros>/cfg/kinect.yaml\nfinally, to process the data launch:\nroslaunch flame_ros flame_offline_tum.launch\nthe mesh should be published on the /flame/mesh topic. to visualize this topic in rviz, consult the the visualization section.\nonline processing\nthe online nodelet can be launched using flame_nodelet.launch:\nroslaunch flame_ros flame_nodelet.launch image:=/image\nwhere /image is your live rectified/undistorted image stream. the frame_id of this topic must correspond to a right-down-forward frame attached to the camera. the tf -----> tree !!!  must be complete such that the pose of the camera in the world frame can be resolved by tf.\nthe mesh should be published on the /flame/mesh topic. to visualize this topic in rviz, consult the the visualization section.\nthe flame_nodelet.launch launch file loads the parameters listed in flame_ros/cfg/flame_nodelet.yaml. you may need to update the input/camera_frame_id param for your data. see the parameters section for more parameter information.\nvisualization\nyou can use the provided configuration file (flame_ros/cfg/flame.rviz) to visualize the output data in rviz. this approach uses a custom rviz plugin to render the /flame/mesh messages. flame_ros can also publish the depth data in other formats (e.g. as a sensor_msgs/pointcloud2 or a sensor_msgs/image), which can be visualized by enabling the corresponding plugins.\nparameters\nthere are many parameters that control processing, but only a handful are particularly important:\noutput/*: controls what type of output messages are produced. if you are concerned about speed, you should prefer publishing only the mesh data (output/mesh: true), but other types of output can be enabled here.\ndebug/*: controls what type of debug images are published. creating these images is relatively expensive, so disable for real-time operation.\nthreading/openmp/*: controls openmp-accelerated sections. you may wish to tune the number of threads per parallel section (threading/openmp/num_threads) or the number of chunks per thread (threading/openmp/chunk_size) for your processor.\nfeatures/detection/min_grad_mag: controls the minimum gradient magnitude for detected features.\nfeatures/detection/win_size: features are detected by dividing the image domain into win_size x win_size blocks and selecting the best trackable pixel inside each block. set to a large number (e.g. 32) for coarse, but fast reconstructions, and a small number (e.g. 8) for finer reconstructions.\nregularization/nltgv2/data_factor: controls the balance between smoothing and data-fitting in the regularizer. it should be set in relation to the detection window size. some good values are 0.1-0.25.\nperformance tips\nfor best results use a high framerate (>= 30 hz) camera with vga-sized images. higher resolution images will require more accurate poses. the feature detection window size (features/detection/win_size) and the data scaling term (regularization/nltgv2/data_factor) are the primary knobs for tuning performance. the default parameters should work well in most cases, but you may need to tune for your specific data.\nby default, flame_ros will publish several debug images. while helpful to observe during operation, they will slow down the pipeline. disable them if you are trying to increase throughput.\nthe usual tips for monocular slam/depth estimation systems also apply:\nprefer slow translational motion\navoid fast rotations when possible\nuse an accurate pose source (e.g. one of the many available visual odometry/slam packages)\nprefer texture-rich environments\nprefer environments with even lighting", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000466, "year": null}, {"Unnamed: 0": 1499, "autor": 479, "date": null, "content": "Node.js Language Binding for ev3dev\nThis is a Node.js module that exposes the features of the ev3dev API in an easy-to-use structure. It is part of the \"unified\" bindings project for ev3dev, which means it implements our abstract API specification. This specification is implemented in multiple languages so that one can easily carry the same code concepts from one language to another, and all the core ev3dev APIs are supported universally.\nWARNING\nDue to the fact that Node.js has dropped support for the processor in the EV3, this library will no longer be supported nor operable starting with the upcoming ev3dev-stretch release. I suggest looking into Python as an alternative language choice.\nCurrent supported kernel version: *-11-ev3dev-*\nQuickstart\nInstall the module from npm:\n$ npm install ev3dev-lang\nNow add a require statement to your .js file:\nvar ev3dev = require('ev3dev-lang');\nNow check out the online documentation to see what you can do. Note that all the classes listed in the docs are available in the ev3dev object that you imported above.\nGetting the Module\nInstalling the latest release from npm\nThe easiest way to get the module is to install it through npm:\n$ npm install ev3dev-lang\nAnd then require() it for use in your code.\nDownloading the source code and compiling yourself\nYou can also download the source from GitHub directly, either from the releases page or via git. If you do it this way, you will need to follow the building instructions below to make it usable from Node.\nThis module is written in TypeScript, which means it cannot be directly used from JavaScript or Node.js. If you would like to make changes to the module or use a copy of the module from GitHub, you will need to follow these steps to build the module from source. The below steps should work on any modern OS, including Linux, OSX and Windows.\nFirst, you will need to install some tools. Both building and running the module will require Node.js and npm, so make sure that you have both installed. Then install grunt, the task runner that we use to build the library:\n$ npm install -g grunt-cli\nOnce you have done this, run grunt --version to make sure that everything was installed correctly (you may have to restart your terminal window first). Next you'll need to get the source code. You can git clone it to get the most recent changes, or download a release from the releases page. The following commands will need to be executed from the root directory of the source tree so cd in to that directory before continuing.\nNow we will install the last few tools that we need. The list of dependencies for the module is pre-defined in the package.json file, so all we need to do is to tell npm to install them for us:\n$ npm install\nThe final step is to run the build job. We can invoke the task runner that we installed earlier to do this:\n$ grunt tsc\nThe build job should have put the generated JavaScript in the bin folder.\nGetting started with the API\nWe recommend that you start by running the files in the examples/ subdirectory of the repo so that you can make sure that your system is set up correctly. Assuming you don't get any errors, you can create your own js file and require the ev3dev-lang module to start writing your own code. For reference, you can take a look at the example scripts or check out the online documentation.\nExecuting your Node.js scripts\nThe simplest way is to run your code from the command line with the node command. This can be done over an SSH session or directly on the brick. To run a .js file, execute:\n$ node path/to/file.js\nIf you want to be able to execute your scripts from brickman's file browser, you can add a shebang and make it executable. You first must add the following code to the top of your .js file:\n#!/usr/bin/env node\nYou can then make it executable from the command line:\n$ chmod +x path/to/file.js\nYou should now be able to execute it directly from brickman.\nUse cases for JavaScript on the EV3\nJavaScript is asynchronous by nature. There is practically no way to \"sleep\" your code for a certain amount of time, or wait for the operation to finish. This is by design, and both restricts the use cases for Node and JS as well as opens up new scenarios to explore.\nSituations to use JavaScript:\nServers\nProgramming a web interface, where you need to serve files\nResponding to commands sent by an external controller (maybe a PC and browser)\nContinuously taking input\nRunning a job on a timer\nRunning any code that only occasionally \"wakes up\"\nSituations in which you should use other languages:\nSequential actions that must run in a specific order\nPrecise timing and delay\nCoordinating multiple motors, sensors, or other hardware devices", "link": "https://github.com/WasabiFan/ev3dev-lang-js", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "node.js language binding for ev3dev\nthis is a node.js module that exposes the features of the ev3dev api in an easy-to-use structure. it is part of the \"unified\" bindings project for ev3dev, which means it implements our abstract api specification. this specification is implemented in multiple languages so that one can easily carry the same code concepts from one language to another, and all the core ev3dev apis are supported universally.\nwarning\ndue to the fact that node.js has dropped support for the processor in the ev3, this library will no longer be supported nor operable starting with the upcoming ev3dev-stretch release. i suggest looking into python as an alternative language choice.\ncurrent supported kernel version: *-11-ev3dev-*\nquickstart\ninstall the module from npm:\n$ npm install ev3dev-lang\nnow add a require statement to your .js file:\nvar ev3dev = require('ev3dev-lang');\nnow check out the online documentation to see what you can do. note that all the classes listed in the docs are available in the ev3dev object that you imported above.\ngetting the module\ninstalling the latest release from npm\nthe easiest way to get the module is to install it through npm:\n$ npm install ev3dev-lang\nand then require() it for use in your code.\ndownloading the source code and compiling yourself\nyou can also download the source from github directly, either from the releases page or via git. if you do it this way, you will need to follow the building instructions below to make it usable from node.\nthis module is written in typescript, which means it cannot be directly used from javascript or node.js. if you would like to make changes to the module or use a copy of the module from github, you will need to follow these steps to build the module from source. the below steps should work on any modern os, including linux, osx and windows.\nfirst, you will need to install some tools. both building and running the module will require node.js and npm, so make sure that you have both installed. then install grunt, the task runner that we use to build the library:\n$ npm install -g grunt-cli\nonce you have done this, run grunt --version to make sure that everything was installed correctly (you may have to restart your terminal window first). next you'll need to get the source code. you can git clone it to get the most recent changes, or download a release from the releases page. the following commands will need to be executed from the root directory of the source -----> tree !!!  so cd in to that directory before continuing.\nnow we will install the last few tools that we need. the list of dependencies for the module is pre-defined in the package.json file, so all we need to do is to tell npm to install them for us:\n$ npm install\nthe final step is to run the build job. we can invoke the task runner that we installed earlier to do this:\n$ grunt tsc\nthe build job should have put the generated javascript in the bin folder.\ngetting started with the api\nwe recommend that you start by running the files in the examples/ subdirectory of the repo so that you can make sure that your system is set up correctly. assuming you don't get any errors, you can create your own js file and require the ev3dev-lang module to start writing your own code. for reference, you can take a look at the example scripts or check out the online documentation.\nexecuting your node.js scripts\nthe simplest way is to run your code from the command line with the node command. this can be done over an ssh session or directly on the brick. to run a .js file, execute:\n$ node path/to/file.js\nif you want to be able to execute your scripts from brickman's file browser, you can add a shebang and make it executable. you first must add the following code to the top of your .js file:\n#!/usr/bin/env node\nyou can then make it executable from the command line:\n$ chmod +x path/to/file.js\nyou should now be able to execute it directly from brickman.\nuse cases for javascript on the ev3\njavascript is asynchronous by nature. there is practically no way to \"sleep\" your code for a certain amount of time, or wait for the operation to finish. this is by design, and both restricts the use cases for node and js as well as opens up new scenarios to explore.\nsituations to use javascript:\nservers\nprogramming a web interface, where you need to serve files\nresponding to commands sent by an external controller (maybe a pc and browser)\ncontinuously taking input\nrunning a job on a timer\nrunning any code that only occasionally \"wakes up\"\nsituations in which you should use other languages:\nsequential actions that must run in a specific order\nprecise timing and delay\ncoordinating multiple motors, sensors, or other hardware devices", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000479, "year": null}, {"Unnamed: 0": 1547, "autor": 527, "date": null, "content": "FloBaRoID\n(FLOating BAse RObot dynamical IDentification)\nFloBaRoID is a python toolkit for parameter identification of floating-base rigid body tree-structures such as humanoid robots. It aims to provide a complete solution for obtaining physical consistent identified dynamics parameters.\nTools:\ntrajectory.py: generate optimized trajectories\nexcite.py: send trajectory to control the robot movement and record the resulting measurements\nidentify.py: identify dynamical parameters (mass, COM and rotational inertia) starting from an URDF description and from torque and force measurements\nvisualize.py: show 3D robot model of URDF, trajectory motion\nFeatures:\nfind optimized excitation trajectories with non-linear global optimization (as parameters of fourier-series for periodic soft trajectories)\ndata preprocessing\nderive velocity and acceleration values from position readings\ndata is zero-phase low-pass filtered from supplied measurements\nit is possible to only select a combination of data blocks to yield a better condition number (Venture, 2009)\nvalidation with other measurement files\nexcitation of robots, using ROS/MoveIt! or Yarp\nimplemented estimation methods:\nordinary least squares, OLS\nweighted least squares (Zak, 1994)\nestimation of parameter error using previously known CAD values (Gautier, 2013)\nessential standard parameters (Pham, Gautier, 2013), estimating only those that are most certain for the measurement data and leaving the others unchanged\nidentification problem formulation with constraints as linear convex SDP problem to get optimal physical consistent standard parameters (Sousa, 2014)\nnon-linear optimization within consistent parameter space (Traversaro, 2016)\nvisualization of trajectories\nplotting of measured and estimated joint state and torques (interactive, HTML, PDF or Tikz)\noutput of the identified parameters directly into URDF\nrequirements for identification module:\npython 2.7 or >= 3.3\npython modules\nnumpy (> 1.8), scipy, sympy (== 1.0), pyyaml, trimesh, cvxopt, pylmi-sdp, matplotlib (>= 1.4), colorama, palettable, humanize, tqdm\niDynTree, e.g. from iDynTree superbuild (with enabled python binding)\nwhen using Python 2.7: future\nwhen using Python < 3.5: typing\ndsdp5 (command line executable)\noptional:\npyglet, pyOpenGL (for visualizer)\nsymengine.py (to speedup SDP)\nmpld3, jinja2 (for html plots)\nmatplotlib2tikz (for tikz plots)\nrbdl (alternative for inverse dynamics)\nrequirements for excitation module:\nfor ros, python modules: ros, moveit_msg, moveit_commander\nfor yarp: c compiler, installed robotology-superbuild, python modules: yarp\nfor other robots, new modules might have to be written\nrequirements for optimization module:\noptimization: python modules: iDynTree, pyOpt (fork at https://github.com/kjyv/pyOpt is recommended)\npyipopt from https://github.com/xuy/pyipopt (plus cmd line ipopt/libipopt with libhsl/coin-hsl)\nmpi4py / mpirun (for parallel trajectory optimization)\nfcl 0.5.0 and python-fcl (from https://github.com/jf---/python-fcl) (possibly disable octomap if there are errors)\nYou can do pip install -r requirements.txt for most of them but you will need to check for the correct versions of each library. You might have to install some library dependencies if you get compile errors. If you're using on Ubuntu and also have ros installed, it is recommended to install with pip within a virtualenv.\nAlso see the Tutorial.\nKnown limitations:\ntrajectory optimization is limited to fixed-base robots (full simulation, balance criterion etc. not implemented)\nYARP excitation module is not very generic (ROS should be)\nusing position control over YARP is not realtime safe and can expose timing issues (especially with python to C bridge)\nSince preparing SDP matrices uses sympy expressions, most of the time for solving the identification problem is spent in symbolic manipulations rather than the actual convex optimization solver. Possibly the time demands can be reduced.\nSDP optimization code is based on or uses parts from cdsousa/wam7_dyn_ident\nUsage is licensed under the LGPL 3.0, see License.md. Please quote the following publication if you're using this software for any project: S. Bethge, J. Malzahn, N. Tsagarakis, D. Caldwell: \"FloBaRoID \u2014 A Software Package for the Identification of Robot Dynamics Parameters\", 26th International Conference on Robotics in Alpe-Adria-Danube Region (RAAD), 2017", "link": "https://github.com/kjyv/FloBaRoID", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "flobaroid\n(floating base robot dynamical identification)\nflobaroid is a python toolkit for parameter identification of floating-base rigid body -----> tree !!! -structures such as humanoid robots. it aims to provide a complete solution for obtaining physical consistent identified dynamics parameters.\ntools:\ntrajectory.py: generate optimized trajectories\nexcite.py: send trajectory to control the robot movement and record the resulting measurements\nidentify.py: identify dynamical parameters (mass, com and rotational inertia) starting from an urdf description and from torque and force measurements\nvisualize.py: show 3d robot model of urdf, trajectory motion\nfeatures:\nfind optimized excitation trajectories with non-linear global optimization (as parameters of fourier-series for periodic soft trajectories)\ndata preprocessing\nderive velocity and acceleration values from position readings\ndata is zero-phase low-pass filtered from supplied measurements\nit is possible to only select a combination of data blocks to yield a better condition number (venture, 2009)\nvalidation with other measurement files\nexcitation of robots, using ros/moveit! or yarp\nimplemented estimation methods:\nordinary least squares, ols\nweighted least squares (zak, 1994)\nestimation of parameter error using previously known cad values (gautier, 2013)\nessential standard parameters (pham, gautier, 2013), estimating only those that are most certain for the measurement data and leaving the others unchanged\nidentification problem formulation with constraints as linear convex sdp problem to get optimal physical consistent standard parameters (sousa, 2014)\nnon-linear optimization within consistent parameter space (traversaro, 2016)\nvisualization of trajectories\nplotting of measured and estimated joint state and torques (interactive, html, pdf or tikz)\noutput of the identified parameters directly into urdf\nrequirements for identification module:\npython 2.7 or >= 3.3\npython modules\nnumpy (> 1.8), scipy, sympy (== 1.0), pyyaml, trimesh, cvxopt, pylmi-sdp, matplotlib (>= 1.4), colorama, palettable, humanize, tqdm\nidyntree, e.g. from idyntree superbuild (with enabled python binding)\nwhen using python 2.7: future\nwhen using python < 3.5: typing\ndsdp5 (command line executable)\noptional:\npyglet, pyopengl (for visualizer)\nsymengine.py (to speedup sdp)\nmpld3, jinja2 (for html plots)\nmatplotlib2tikz (for tikz plots)\nrbdl (alternative for inverse dynamics)\nrequirements for excitation module:\nfor ros, python modules: ros, moveit_msg, moveit_commander\nfor yarp: c compiler, installed robotology-superbuild, python modules: yarp\nfor other robots, new modules might have to be written\nrequirements for optimization module:\noptimization: python modules: idyntree, pyopt (fork at https://github.com/kjyv/pyopt is recommended)\npyipopt from https://github.com/xuy/pyipopt (plus cmd line ipopt/libipopt with libhsl/coin-hsl)\nmpi4py / mpirun (for parallel trajectory optimization)\nfcl 0.5.0 and python-fcl (from https://github.com/jf---/python-fcl) (possibly disable octomap if there are errors)\nyou can do pip install -r requirements.txt for most of them but you will need to check for the correct versions of each library. you might have to install some library dependencies if you get compile errors. if you're using on ubuntu and also have ros installed, it is recommended to install with pip within a virtualenv.\nalso see the tutorial.\nknown limitations:\ntrajectory optimization is limited to fixed-base robots (full simulation, balance criterion etc. not implemented)\nyarp excitation module is not very generic (ros should be)\nusing position control over yarp is not realtime safe and can expose timing issues (especially with python to c bridge)\nsince preparing sdp matrices uses sympy expressions, most of the time for solving the identification problem is spent in symbolic manipulations rather than the actual convex optimization solver. possibly the time demands can be reduced.\nsdp optimization code is based on or uses parts from cdsousa/wam7_dyn_ident\nusage is licensed under the lgpl 3.0, see license.md. please quote the following publication if you're using this software for any project: s. bethge, j. malzahn, n. tsagarakis, d. caldwell: \"flobaroid \u2014 a software package for the identification of robot dynamics parameters\", 26th international conference on robotics in alpe-adria-danube region (raad), 2017", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000527, "year": null}, {"Unnamed: 0": 1578, "autor": 558, "date": null, "content": "motion-planners\nFlexible python implementations of several robotic motion planners.\nCitation\nCaelan Reed Garrett. Motion Planners. https://github.com/caelan/motion-planners. 2017.\nExample\nA simple motion planning problem in 2D workspace and 2D configuration space that only depends on Tkinter.\nmotion-planners$ python -m motion_planners.tkinter.run\nAlgorithms\nSingle-Query\nSampling-Based:\nRapidly-Exploring Random Tree (RRT)\nBidirectional RRT (BiRRT/RRT-Connect)\nMultiRRT\nRRT*\nGrid Search\nBreadth-First Search (BFS)\nA*\nLattice\nOther\nStraight-Line Path\nLinear Shortcutting\nMulti-Query\nSampling-based:\nProbabilistic Roadmap (PRM)\nLazy PRM\nAPI\nEach motion planner takes as input python functions that perform its primitive operations, allowing them to be flexibly used in many software environments.\n:param distance_fn: Distance function - distance_fn(q1, q2)->float\n:param sample_fn: Sample function - sample_fn()->conf\n:param extend_fn: Extension function - extend_fn(q1, q2)->[q', ..., q\"]\n:param collision_fn: Collision function - collision_fn(q)->bool\nApplications\nPyBullet Motion Planning - https://github.com/caelan/pybullet-planning\nPyBullet Task and Motion Planning (TAMP) - https://github.com/caelan/pddlstream", "link": "https://github.com/caelan/motion-planners", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "motion-planners\nflexible python implementations of several robotic motion planners.\ncitation\ncaelan reed garrett. motion planners. https://github.com/caelan/motion-planners. 2017.\nexample\na simple motion planning problem in 2d workspace and 2d configuration space that only depends on tkinter.\nmotion-planners$ python -m motion_planners.tkinter.run\nalgorithms\nsingle-query\nsampling-based:\nrapidly-exploring random -----> tree !!!  (rrt)\nbidirectional rrt (birrt/rrt-connect)\nmultirrt\nrrt*\ngrid search\nbreadth-first search (bfs)\na*\nlattice\nother\nstraight-line path\nlinear shortcutting\nmulti-query\nsampling-based:\nprobabilistic roadmap (prm)\nlazy prm\napi\neach motion planner takes as input python functions that perform its primitive operations, allowing them to be flexibly used in many software environments.\n:param distance_fn: distance function - distance_fn(q1, q2)->float\n:param sample_fn: sample function - sample_fn()->conf\n:param extend_fn: extension function - extend_fn(q1, q2)->[q', ..., q\"]\n:param collision_fn: collision function - collision_fn(q)->bool\napplications\npybullet motion planning - https://github.com/caelan/pybullet-planning\npybullet task and motion planning (tamp) - https://github.com/caelan/pddlstream", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000558, "year": null}, {"Unnamed: 0": 1611, "autor": 591, "date": null, "content": "Robot software stack\nThis repo contains all the software used on our robots:\ncan-io-firmware contains the firmware that runs on the IO board\nmotor-control-firmware contains the firmware that runs on the motor board\nproximity-beacon-firmware contains the firmware that runs on the proximity beacon, it's the same code as the motor board but with a different application that is tailored to the needs of our proximity beacon module\nmaster-firmware contains the software that runs on the master board, it interfaces all the other boards over CAN and runs the robot's \"intelligence\".\neurobot contains documentation and cofiguration files specific to the Eurobot competition\nsensor-firmware contains code running on the sensor board\nuwb-beacon-firmware contains code and documentation that runs on the UWB beacon board\nhitl contains code to support Hardware In The Loop testing of the master board firmware.\nOther important software components can be found in this repo:\nlib contains all the libraries and building blocks we use on multiple boards, which includes:\nlib/can-bootloader the bootloader that allows us to update our boards (IO and motor) over CAN\nlib/ChibiOS the RTOS/HAL we use on all our boards\nlib/uavcan the CAN communication library we use on all our boards\nlib/error a logging library\nlib/parameter a library to create and manage configurations of boards\nlib/msgbus a publish/subscribe library for inter thread communication\nand more.\ntools groups all tools we use to develop on the robot including:\ntools/pid-tuner a GUI to tune PID gains of motor boards over CAN, written using Python and Qt\ntools/studio a set of introspection tools written in Python and Qt to debug our robots\nuavcan_data_types contains the custom message definitions (DSDL) for the UAVCAN communication protocol\nci groups scripts and Docker files for our continuous integration server\nuser-guide contains high-level documentation about software and electronics components used on our robots\nThe user guide is generated using mdbook from doc/user-guide\nBuilding with CMake\nTo build one platform with cmake, for example here our UWB beacon board:\nmkdir build-uwb && cd build-uwb\ncmake .. -DCMAKE_TOOLCHAIN_FILE=../cmake/uwb-beacon.cmake\n# WARNING: Make all or make without target does not work.\nmake uwb-beacon-firmware.elf\n# The resulting firmware is at `uwb-beacon-firmware/uwb-beacon-firmware.elf`.\nTo build the unit tests for all the boards:\nmkdir build\ncd build\ncmake ..\nmake all test\nCoding style\nWe use clang-format (tested with version 7 or greater) to enforce proper source code formatting. You can use the format-all.sh script in the root directory to format the whole source tree. You can also use clang-format -i --style=file src/foo.c to format a particular file. Finally, some editors include support for clang-format through plugin, check for yourself.", "link": "https://github.com/cvra/robot-software", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "robot software stack\nthis repo contains all the software used on our robots:\ncan-io-firmware contains the firmware that runs on the io board\nmotor-control-firmware contains the firmware that runs on the motor board\nproximity-beacon-firmware contains the firmware that runs on the proximity beacon, it's the same code as the motor board but with a different application that is tailored to the needs of our proximity beacon module\nmaster-firmware contains the software that runs on the master board, it interfaces all the other boards over can and runs the robot's \"intelligence\".\neurobot contains documentation and cofiguration files specific to the eurobot competition\nsensor-firmware contains code running on the sensor board\nuwb-beacon-firmware contains code and documentation that runs on the uwb beacon board\nhitl contains code to support hardware in the loop testing of the master board firmware.\nother important software components can be found in this repo:\nlib contains all the libraries and building blocks we use on multiple boards, which includes:\nlib/can-bootloader the bootloader that allows us to update our boards (io and motor) over can\nlib/chibios the rtos/hal we use on all our boards\nlib/uavcan the can communication library we use on all our boards\nlib/error a logging library\nlib/parameter a library to create and manage configurations of boards\nlib/msgbus a publish/subscribe library for inter thread communication\nand more.\ntools groups all tools we use to develop on the robot including:\ntools/pid-tuner a gui to tune pid gains of motor boards over can, written using python and qt\ntools/studio a set of introspection tools written in python and qt to debug our robots\nuavcan_data_types contains the custom message definitions (dsdl) for the uavcan communication protocol\nci groups scripts and docker files for our continuous integration server\nuser-guide contains high-level documentation about software and electronics components used on our robots\nthe user guide is generated using mdbook from doc/user-guide\nbuilding with cmake\nto build one platform with cmake, for example here our uwb beacon board:\nmkdir build-uwb && cd build-uwb\ncmake .. -dcmake_toolchain_file=../cmake/uwb-beacon.cmake\n# warning: make all or make without target does not work.\nmake uwb-beacon-firmware.elf\n# the resulting firmware is at `uwb-beacon-firmware/uwb-beacon-firmware.elf`.\nto build the unit tests for all the boards:\nmkdir build\ncd build\ncmake ..\nmake all test\ncoding style\nwe use clang-format (tested with version 7 or greater) to enforce proper source code formatting. you can use the format-all.sh script in the root directory to format the whole source -----> tree !!! . you can also use clang-format -i --style=file src/foo.c to format a particular file. finally, some editors include support for clang-format through plugin, check for yourself.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000591, "year": null}, {"Unnamed: 0": 1649, "autor": 629, "date": null, "content": "CSMM.103x - ColumbiaX | Robotics | edX\nproject1: publish and subscribe custom messages.\ntf2_examples: basic example using the tf2 package for coordinate system's transformations.\nproject2_solution: program that uses tf to describe the poses between a robot, its camera and an object.\nmarker_publisher: used in project2_solution to visualize the transformations. It's invoked by project2_solution.\nproject3: packages that analyze the forward dynamics of a Kuka LBR iiwa arm.\nproject4/cartesian_control: also with the package robot_sim from project3, uses the differential kinematics of the Kuka LBR iiwa robot arm.\nproject5: In order for the robot end-effector to reach a desired pose without collisions with any obstacles present in its environment it's necessary to implement motion planning. In this project is used a Rapidly-exploring Random Tree (RRT) motion planner for the same 7-jointed robot arm of the previous lectures.", "link": "https://github.com/eborghi10/ColumbiaX-Robotics", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "csmm.103x - columbiax | robotics | edx\nproject1: publish and subscribe custom messages.\ntf2_examples: basic example using the tf2 package for coordinate system's transformations.\nproject2_solution: program that uses tf to describe the poses between a robot, its camera and an object.\nmarker_publisher: used in project2_solution to visualize the transformations. it's invoked by project2_solution.\nproject3: packages that analyze the forward dynamics of a kuka lbr iiwa arm.\nproject4/cartesian_control: also with the package robot_sim from project3, uses the differential kinematics of the kuka lbr iiwa robot arm.\nproject5: in order for the robot end-effector to reach a desired pose without collisions with any obstacles present in its environment it's necessary to implement motion planning. in this project is used a rapidly-exploring random -----> tree !!!  (rrt) motion planner for the same 7-jointed robot arm of the previous lectures.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000629, "year": null}, {"Unnamed: 0": 1675, "autor": 655, "date": null, "content": "sra-board-component\nAn ESP-IDF Component for SRA Board\nExplore the docs \u00bb\nView Demo \u00b7 Report Bug \u00b7 Request Feature \u00b7 Send a Pull Request\nTable of Contents\nAbout the Project\nBuilt With\nGetting Started\nPrerequisites\nInstallation\nUsage\nExamples\nRoadmap\nContributing\nLicense\nContact\nAcknowledgements\nAbout The Project\nSRA ESP-IDF component provides an abstraction\nBuilt With\nC\nToolchain & Supported SDK\nESP-IDF v4.2-release\nDocs\nDoxygen\nGetting Started\nHardware Required : SRA Development Board\nRecommended ESP-IDF v4.2-release .\nRefer espressif-docs for intallation guide\nPrerequisites\nA stable release of ESP-IDF\nProject Tree containing components dir\n- myProject/\n- CMakeLists.txt\n- sdkconfig\n- components/ - component1/\n- component2/\n- main/ - CMakeLists.txt\n- src.c\n- build/\nInstallation\ncd <your_esp_idf_project>\nmkdir components\ncd components\ngit clone https://github.com/SRA-VJTI/sra-board-component.git\nUsage\nVisit examples\nFor more examples, please refer to the Documentation\nExamples\nExample Link Description\nBar Graph Bar graph led can be used for testing purposes\nLight sensing array LSA aka Light Sensing Array is used mainly in line following bot\nMotor driver - normal- parallel Example for using two motor drivers present on SRA Board in both parallel and normal mode\nMPU6050 MPU6050 combines 3-axis Gyroscope, 3-axis Accelerometer and Motion Processor all in small package. This example is for reading values from mpu6050 accelerometer\nServos Servos are used in the locomotion of bots like Wall-E . This example is for using the servo port on SRA Board to run servos\nSwitches Manual Switches provide the traditional comfort to toggle the input variables. This example is for using four user switches present on SRA Board\nRoadmap\nSee the open issues for a list of proposed features (and known issues).\nContributing\nContributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are extremely appreciated.\nFork the Project\nCreate your Feature Branch (git checkout -b feature/AmazingFeature)\nCommit your Changes (git commit -m 'Add some AmazingFeature')\nPush to the Branch (git push origin feature/AmazingFeature)\nOpen a Pull Request\nLicense\nDistributed under the MIT License.\nContact\n@SRAVJTI - https://www.sravjti.in/\nSRA-VJTI Website", "link": "https://github.com/SRA-VJTI/sra-board-component", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "sra-board-component\nan esp-idf component for sra board\nexplore the docs \u00bb\nview demo \u00b7 report bug \u00b7 request feature \u00b7 send a pull request\ntable of contents\nabout the project\nbuilt with\ngetting started\nprerequisites\ninstallation\nusage\nexamples\nroadmap\ncontributing\nlicense\ncontact\nacknowledgements\nabout the project\nsra esp-idf component provides an abstraction\nbuilt with\nc\ntoolchain & supported sdk\nesp-idf v4.2-release\ndocs\ndoxygen\ngetting started\nhardware required : sra development board\nrecommended esp-idf v4.2-release .\nrefer espressif-docs for intallation guide\nprerequisites\na stable release of esp-idf\nproject -----> tree !!!  containing components dir\n- myproject/\n- cmakelists.txt\n- sdkconfig\n- components/ - component1/\n- component2/\n- main/ - cmakelists.txt\n- src.c\n- build/\ninstallation\ncd <your_esp_idf_project>\nmkdir components\ncd components\ngit clone https://github.com/sra-vjti/sra-board-component.git\nusage\nvisit examples\nfor more examples, please refer to the documentation\nexamples\nexample link description\nbar graph bar graph led can be used for testing purposes\nlight sensing array lsa aka light sensing array is used mainly in line following bot\nmotor driver - normal- parallel example for using two motor drivers present on sra board in both parallel and normal mode\nmpu6050 mpu6050 combines 3-axis gyroscope, 3-axis accelerometer and motion processor all in small package. this example is for reading values from mpu6050 accelerometer\nservos servos are used in the locomotion of bots like wall-e . this example is for using the servo port on sra board to run servos\nswitches manual switches provide the traditional comfort to toggle the input variables. this example is for using four user switches present on sra board\nroadmap\nsee the open issues for a list of proposed features (and known issues).\ncontributing\ncontributions are what make the open source community such an amazing place to be learn, inspire, and create. any contributions you make are extremely appreciated.\nfork the project\ncreate your feature branch (git checkout -b feature/amazingfeature)\ncommit your changes (git commit -m 'add some amazingfeature')\npush to the branch (git push origin feature/amazingfeature)\nopen a pull request\nlicense\ndistributed under the mit license.\ncontact\n@sravjti - https://www.sravjti.in/\nsra-vjti website", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000655, "year": null}, {"Unnamed: 0": 1678, "autor": 658, "date": null, "content": "Closed Chain Inverse Kinematics\nA generalized inverse kinematics solver that supports closed chains for parallel kinematics systems, dynamic reconfiguration, and arbitrary joint configuration based on damped least squares error minimization techniques. Supports all variety of joints including combinations of rotation and translation degrees of freedom and is agnostic to visualization framework. Inspired by Marty Vona's MSim research work and using techniques outlined in this 2009 paper by Samuel Buss. Developed with some aid and advice from Marty Vona.\nSolver being used on JPL's ATHLETE robot for full body IK\nATHLETE and Robonaut demo here!\nVR demo here!\nPartial degrees of freedom Goal demo here!\nModel License Information\nRobitics models used in the project are for demonstration purposes only and subject to the licenses of their respective projects.\nATHLETE\nRobonaut\nCuriosity\nStaubli\nUse\nSimple 2 DoF System\nimport { Solver, Joint, Link, Goal, DOF } from 'closed-chain-ik';\n// Create links and joints\nconst link1 = new Link();\nconst joint1 = new Joint();\njoint.setDoF( DOF.EZ );\njoint.setPosition( 0, 1, 0 );\njoint.setDoFValues( Math.PI / 4 );\nconst link2 = new Link();\nconst joint2 = new Joint();\njoint.setDoF( DOF.EX );\njoint.setPosition( 0, 1, 0 );\njoint.setDoFValues( Math.PI / 4 );\nconst link3 = new Link();\nlink3.setPosition( 0, 1, 0 );\n// Create the goal\nconst goal = new Goal();\nlink3.getWorldPosition( goal.position );\nlink3.getWorldQuaternion( goal.quaternion );\n// Create structure\nlink1.addChild( joint1 );\njoint1.addChild( link2 );\nlink2.addChild( joint2 );\njoint2.addChild( link3 );\ngoal.makeClosure( link3 );\n// create solver\nconst solver = new Solver( link1 );\n// ...\n// move the goal around and solve\nsolver.solve();\nUsing a WebWorker Solver\nimport { WorkerSolve, Joint, Link, Goal, DOF } from 'closed-chain-ik';\n// ... instantiate kinematic system...\nconst solver = new WorkerSolver( link1 );\n// ...\n// move the goal around and solve asynchronously\nsolver.solve();\nCaveats\nThe web worker implementation uses ShareArrayBuffers which are not available on some platforms (Safari, Chrome for Android). See issue #44.\nSmoothing out 3DoF non closure ball joint behavior is in progress. See issue #22.\nEnabling SVD on the Solver can cause divergence on solvable systems and stutter. See #76.\nAPI\nConstants\nDOF\nEnumerated fields representing different degrees of freedom for Joints.\n// Translation DoF\nDOF.X, DOF.Y, DOF.Z,\n// Euler Rotation DoF\nDOF.EX, DOF.EY, DOF.EZ,\nDOF_NAMES\nAn array of strings representing the names of the above degrees of freedom.\nSOLVE_STATUS\nEnumerated fields representing the state of a solve result.\n// Error for all goals are within\n// the threshold.\nSOLVE_STATUS.CONVERGED,\n// Error for the goals has begun\n// to diverge.\nSOLVE_STATUS.DIVERGED,\n// Resulting angles has not changed\n// significantly enough to reach the\n// stall threshold.\nSOLVE_STATUS.STALLED,\n// The solve has reached the maximum\n// number of allowed iterations.\nSOLVE_STATUS.TIMEOUT,\nSOLVE_STATUS_NAMES\nAn array of strings representing the names of the above solve statuses.\nFunctions\nSet of utility functions including some for creating an ik system from and working with results from URDFLoader.\nfindRoots\nfindRoots( frames : Array<Frame> ) : Array<Frame>\nTakes an array of frames to traverse including the closure joints and links and finds a set of unique nodes to treat as the roots of the connected trees for use in solving.\nurdfRobotToIKRoot\nurdfRobotToIKRoot( robot : URDFRobot, trimUnused : Boolean = false ) : Joint\nGenerates an IK three based on the provided URDFRobot with the root joint having a all 6 degrees of freedom set. Returns the root joint. If trimUnused is true then any dangling links that do not connect to non-fixed joints will be removed from the system.\nsetUrdfFromIK\nsetUrdfFromIK( robot : URDFRobot, ikRoot : Joint ) : void\nCopies the joint values from robot onto ikRoot based on joint names.\nsetIKFromUrdf\nsetIKFromUrdf( ikRoot : Joint, robot : URDFRobot ) : void\nCopies the joint values from ikRoot onto robot based on joint names.\nFrame\nA base class for Link, Joint, and Goal representing a frame defined by a position and rotation in space.\n.position\nposition : Float32Array[ 3 ]\nThe position of the frame. If this is modified directly setMatrixNeedsUpdate() must be called.\n.quaternion\nquaternion : Float32Array[ 4 ]\nThe orientation of the frame. If this is modified directly setMatrixNeedsUpdate() must be called.\n.matrix\nreadonly matrix : Float32Array[ 16 ]\nThe local transform matrix composed from the position and quaternion.\n.matrixWorld\nreadonly matrixWorld : Float32Array[ 16 ]\nThe world transform matrix computed based on the parent matrixWorld and this local matrix.\n.parent\nreadonly parent : Frame\nThe parent frame this frame is a child of.\n.children\nreadonly children : Array<Frame>\nThe set of child frames this frame is a parent of.\n.setPosition\nsetPosition( x : Number, y : Number, z : Number ) : void\nSets the position of the frame.\n.setWorldPosition\nsetWorldPosition( x : Number, y : Number, z : Number ) : void\nSets the positon of the frame in world space. Automatically computes the local position relative to the parent.\n.getWorldPosition\ngetWorldPosition( target : FloatArray[ 3 ] ) : void\nGets the position of the frame in the world in the target argument.\n.setQuaternion\nsetQuaternion( x : Number, y : Number, z : Number, w : Number ) : void\nSets the orientation of the frame.\n.setWorldQuaternion\nsetWorldQuaternion( x : Number, y : Number, z : Number, w : Number ) : void\nSets the orientation of the frame in world space. Automatically computes the local orientation relative to the parent.\n.getWorldQuaternion\ngetWorldQuaternion( target : FloatArray[ 4 ] ) : void\nGets the quaternion of the frame in the world in the target argument.\n.traverseParents\ntraverseParents( callback : ( parent : Frame ) => Boolean ) : void\nFires the given callback for every parent starting with the closest. If callback returns true then the traversal is stopped.\n.traverse\ntraverse( callback : ( child : Frame ) => Boolean ) : void\nFires the given callback for every child recursively in breadth first order. If callback returns true then the traversal is stopped.\n.addChild\naddChild( child : Frame ) : void\nAdds a child to this frame and sets the childs parent to this frame. Throws an error if the child already has a parent.\n.removeChild\nremoveChild( child : Frame ) : void\nRemoves the given child from this frame. Throws an error if the given frame is not a child of this frame.\n.attachChild\nattachChild( child : Frame ) : void\nAdds the given frame as a child of this frame while preserving the world position of the child.\n.detachChild\ndetachChild( child : Frame ) : void\nRemoves the given frame as a child of this frame while preserving the world position of the child.\n.updateMatrix\nupdateMatrix() : void\nUpdates the local .matrix field if it needs to be updated.\n.updateMatrixWorld\nupdateMatrixWorld( includeChildren : Boolean = true ) : void\nUpdates the local .matrix and .worldMatrix fields if they need to be updated. Ensures parent matrices are up to date.\n.setMatrixNeedsUpdate\nsetMatrixNeedsUpdate() : void\nFlags this frame as needing a matrix and matrix world update.\n.setMatrixWorldNeedsUpdate\nsetMatrixNeedsUpdate() : void\nFlags this frame and all its children as needing a matrix world update.\nLink\nextends Frame\nA Frame modeling a fixed connection between two Joints. Only Joints may be added as children.\n.closureJoints\nclosureJoints : Array<Joint>\nThe set of joints that are connected to this indirectly via Joint.makeClosure.\nJoint\nextends Frame\nA dynamic Frame representing a kinematic joint arbitrarily defineable degrees of freedom. A degree of freedom indicates an offset value can be set. Only Links may be added as children and a Joint may only have a single child.\n.child\nreadonly child : Link = null\nReference to the joint child.\n.isClosure\nreadonly isClosure : Boolean = false\nWhether or not the child relationship is a closure or not.\n.dof\nreadonly dof : Array<DOF>\nA list of all the free degrees of freedom.\n.dofFlags\nreadonly dofFlags : Uint8Array[6]\nA list of 0 and 1 flags with 1 corresonding to a field in dof.\n.dofValues\nreadonly dofValues : Float32Array[6]\nThe current joint values for all joint degrees of freedom.\n.dofTarget\nreadonly dofTarget : Float32Array[6]\nThe joint value targets for each degree of freedom. Solver will attempt to solve for these targets if targetSet is true.\n.dofRestPose\nreadonly dofRestPose : Float32Array[6]\nThe rest pose for each joint degree of freedom. Solver will attempt to move the joint towards this position when it does not compromise solving for the other goals and when restPoseSet is true.\n.minDoFLimit\nreadonly minDoFLimit : Float32Array[6]\nThe minimum value limits for each joint degree of freeom.\n.maxDoFLimit\nreadonly maxDoFLimit : Float32Array[6]\nThe maximum value limits for each joint degree of freeom.\n.matrixDoF\nreadonly matrixDoF : Float32Array[16]\nThe matrix representing the transformation offset due to the current joint values.\n.targetSet\ntargetSet : Boolean = false\nWhen set to true Solver will try to move this joints dofValues towards the target values.\n.restPoseSet\nrestPoseSet : Boolean = false\nWhen set to true Solver will try to move this joints dofValues towards the rest pose values without compromising the other goals.\n.setDoF\nsetDoF( ...dof : Array<DOF> ) : void\nSets the degrees of freedom of the joint. Arguments must be passed in X, Y, Z, EX, EY, EZ order without duplicate values. All relatd degree of freedom values are reset.\n.clearDoF\nclearDoF() : void\nClears all degrees of freedom.\n.set[ * ]Values\nsetDoFValues( ...values : Array<Number> ) : void;\nsetRestPoseValues( ...values : Array<Number> ) : void;\nsetTargetValues( ...values : Array<Number> ) : void;\nsetMinLimits( ...values : Array<Number> ) : void;\nsetMaxLimits( ...values : Array<Number> ) : void;\nThe number of arguments must match the number of degrees of freedom of the joint.\n.set[ * ]Value\nsetDoFValue( dof : DOF, value : Number ) : Boolean\nsetRestPoseValue( dof : DOF, value : Number ) : Boolean\nsetTargetValue( dof : DOF, value : Number ) : Boolean\nsetMinLimit( dof : DOF, value : Number ) : Boolean\nsetMaxLimit( dof : DOF, value : Number ) : Boolean\n.get[ * ]Value\ngetDoFValue( dof : DOF ) : Number\ngetRestPoseValue( dof : DOF ) : Number\ngetTargetValue( dof : DOF ) : Number\ngetMinLimit( dof : DOF ) : Number\ngetMaxLimit( dof : DOF ) : Number\n.makeClosure\nmakeClosure( child : Link ) : void\nDeclares the relationship between this joint and the given child link is a closure meaning there is no direct parent child relationship but the Solver will treat the closure link as a target for this joint to keep them closed.\nNote that when making a closure connection between a Joint and a Link the link will not be added to the Joints children array and instead will only be available on the child field. The Joint will be appended to the Links closureJoints array.\nGoal\nextends Joint\nA Frame representing a goal to achieve for a connected Link. Set degrees of freedom represent fixed goals for a link to achieve as opposed to moveable degrees of freedom defined for Joints. A goal cannot have children and only be used to make a closure.\nSolver\nClass for solving the closure and target joint constraints of a sytem. As well as the listed fields a set of \"options\" are set on the object which are listed here:\n// Whether or not to use the SVD when calculating the pseudo inverse of the jacobian\n// which can result in a more numerically stable calculation. If the SVD cannot be calculated\n// then the transpose method is used.\nuseSVD = true;\n// The max amount of iterations to try to solve for. The solve will terminate\n// with SOLVE_STATUS.TIMEOUT if this limit is exceeded.\nmaxIterations = 5;\n// The threshold under which a joint is not considered to have really moved. If\n// no joint is moved more than this threshold then the solve will terminate with\n// SOLVE_STATUS.STALLED.\nstallThreshold = 1e-4;\n// The threshold for comparing how much error has changed between solve iterations.\n// If the error has grown by more than this threshold then the solve will terminate\n// with SOLVE_STATUS.DIVERGED.\ndivergeThreshold = 0.01;\n// The fixed damping factor to use in the DLS calculation.\ndampingFactor = 0.001;\n// The factor with which to move the joints towards the rest pose if set.\nrestPoseFactor = 0.01;\n// The thresholds with which to compute whether or not the translation or rotation\n// goals have been met. If the error between target and goal is under these\n// thresholds then the solve will terminate with SOLVE_STATUS.CONVERGED.\ntranslationConvergeThreshold = 1e-3;\nrotationConvergeThreshold = 1e-5;\n// Factors to apply to the translation and rotation error in the error vector.\n// Useful for weighting rotation higher than translation if they do not seem to\n// be solved \"evenly\". Values are expected to be in the range [ 0, 1 ].\ntranslationFactor = 1;\nrotationFactor = 1;\n// The amount to move a joint when calculating the change in error a joint has\n// for a jacobian.\ntranslationStep = 1e-3;\nrotationStep = 1e-3;\n// The step to take towards the IK goals when solving. Setting this to a larger value\n// may solve more quickly but may lead also lead to divergence.\ntranslationErrorClamp = 0.1;\nrotationErrorClamp = 0.1;\n.roots\nroots : Array<Frame>\nThe list of roots that should be accounted for in a solve. When .updateStructure is called the series of roots are traversed including closure joints to find all connected link hierarchies to use in the solve.\n.constructor\nconstructor( roots : Array<Frame> )\nConstructor takes a list of roots to solve for.\n.solve\nsolve() : Array<SOLVE_STATUS>\nTraverses the given set of roots to find joint chains to solve for and attempts to solve for the error in the system goals. A result is returned for each independent chain found in the system.\n.updateStructure\nupdateStructure() : void\nMust be called whenever parent child relationships and structural changes related to the tree change or .roots is modified.\nWorkerSolver\nImplements the interface defined by Solver but runs the solve asynchronously on in a WebWorker. Results are automatically copied to the joint system being solved for.\n\u26a0\ufe0f When SharedArrayBuffers are not available new copies of ArrayBuffer are created every update and from the worker.\n.results\nresults : Array<Solve_STATUS>\nThe list of the last results from the solve copied over from the WebWorker.\n.updateSolverSettings\nupdateSolverSettings( settings : Object ) : void\nSets the solver settings in the WebWorker to the values in the given object. Valid \"options\" values are listed in the Solver docs.\n.updateFrameState\nupdateFrameState( ...jointsToUpdate : Array<Joint> = [] ) : void\nCopies the joint settings for the given joints to the WebWorker for a solve. \"Joint settings\" include everything except for joint values (that the solver would be solving for) and parent child relationships. If joint values or parent child relationhips change then updateStructure must be called.\n.solve\nsolve() : void\nStarts a solve in the WebWorker if one is not active. The solve will terminate automatically if none of the results are SOLVE_STATUS.TIMEOUT.\n.stop\nstop() : void\nTerminate any active solve in the WebWorker.\n.dispose\ndispose() : void\nTerminates the WebWorker and sets members to null.\nIKRootsHelper\nextends THREE.Group\nA helper class for rendering the joints and links in a three.js scene. Renders frame relationships as lines and joints degrees of freedom with indicators based on the joint type.\n.roots\nroots : Array<Frame>\nSet of roots to render in the helper visualization. If this is changed then .updateStructure must be called.\n.constructor\nconstructor( roots : Array<Frame> )\nTakes the set of roots to visualize.\n.setJointScale\nsetJointScale( scale : Number ) : this\nSets the scale of the joint indicators.\n.setColor\nsetColor( color : Color | String | Number ) : this\nSets the color of the helper.\n.setDrawThrough\nsetDrawThrough( drawThrough : Boolean ) : this\nSets whether the helper will draw through the environment.\n.setResolution\nsetResolution( width : Number, height : Number ) : this\nSets the resolution of the renderer so the 2d lines can be rendered at the appropriate thickness.\n.updateStructure\nupdateStructure() : void\nMust be called if the structure of the IK system being visualized has changed.\n.dispose\ndispose() : void\nCalls dispose on all created materials and geometry in the tree.", "link": "https://github.com/gkjohnson/closed-chain-ik-js", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "closed chain inverse kinematics\na generalized inverse kinematics solver that supports closed chains for parallel kinematics systems, dynamic reconfiguration, and arbitrary joint configuration based on damped least squares error minimization techniques. supports all variety of joints including combinations of rotation and translation degrees of freedom and is agnostic to visualization framework. inspired by marty vona's msim research work and using techniques outlined in this 2009 paper by samuel buss. developed with some aid and advice from marty vona.\nsolver being used on jpl's athlete robot for full body ik\nathlete and robonaut demo here!\nvr demo here!\npartial degrees of freedom goal demo here!\nmodel license information\nrobitics models used in the project are for demonstration purposes only and subject to the licenses of their respective projects.\nathlete\nrobonaut\ncuriosity\nstaubli\nuse\nsimple 2 dof system\nimport { solver, joint, link, goal, dof } from 'closed-chain-ik';\n// create links and joints\nconst link1 = new link();\nconst joint1 = new joint();\njoint.setdof( dof.ez );\njoint.setposition( 0, 1, 0 );\njoint.setdofvalues( math.pi / 4 );\nconst link2 = new link();\nconst joint2 = new joint();\njoint.setdof( dof.ex );\njoint.setposition( 0, 1, 0 );\njoint.setdofvalues( math.pi / 4 );\nconst link3 = new link();\nlink3.setposition( 0, 1, 0 );\n// create the goal\nconst goal = new goal();\nlink3.getworldposition( goal.position );\nlink3.getworldquaternion( goal.quaternion );\n// create structure\nlink1.addchild( joint1 );\njoint1.addchild( link2 );\nlink2.addchild( joint2 );\njoint2.addchild( link3 );\ngoal.makeclosure( link3 );\n// create solver\nconst solver = new solver( link1 );\n// ...\n// move the goal around and solve\nsolver.solve();\nusing a webworker solver\nimport { workersolve, joint, link, goal, dof } from 'closed-chain-ik';\n// ... instantiate kinematic system...\nconst solver = new workersolver( link1 );\n// ...\n// move the goal around and solve asynchronously\nsolver.solve();\ncaveats\nthe web worker implementation uses sharearraybuffers which are not available on some platforms (safari, chrome for android). see issue #44.\nsmoothing out 3dof non closure ball joint behavior is in progress. see issue #22.\nenabling svd on the solver can cause divergence on solvable systems and stutter. see #76.\napi\nconstants\ndof\nenumerated fields representing different degrees of freedom for joints.\n// translation dof\ndof.x, dof.y, dof.z,\n// euler rotation dof\ndof.ex, dof.ey, dof.ez,\ndof_names\nan array of strings representing the names of the above degrees of freedom.\nsolve_status\nenumerated fields representing the state of a solve result.\n// error for all goals are within\n// the threshold.\nsolve_status.converged,\n// error for the goals has begun\n// to diverge.\nsolve_status.diverged,\n// resulting angles has not changed\n// significantly enough to reach the\n// stall threshold.\nsolve_status.stalled,\n// the solve has reached the maximum\n// number of allowed iterations.\nsolve_status.timeout,\nsolve_status_names\nan array of strings representing the names of the above solve statuses.\nfunctions\nset of utility functions including some for creating an ik system from and working with results from urdfloader.\nfindroots\nfindroots( frames : array<frame> ) : array<frame>\ntakes an array of frames to traverse including the closure joints and links and finds a set of unique nodes to treat as the roots of the connected trees for use in solving.\nurdfrobottoikroot\nurdfrobottoikroot( robot : urdfrobot, trimunused : boolean = false ) : joint\ngenerates an ik three based on the provided urdfrobot with the root joint having a all 6 degrees of freedom set. returns the root joint. if trimunused is true then any dangling links that do not connect to non-fixed joints will be removed from the system.\nseturdffromik\nseturdffromik( robot : urdfrobot, ikroot : joint ) : void\ncopies the joint values from robot onto ikroot based on joint names.\nsetikfromurdf\nsetikfromurdf( ikroot : joint, robot : urdfrobot ) : void\ncopies the joint values from ikroot onto robot based on joint names.\nframe\na base class for link, joint, and goal representing a frame defined by a position and rotation in space.\n.position\nposition : float32array[ 3 ]\nthe position of the frame. if this is modified directly setmatrixneedsupdate() must be called.\n.quaternion\nquaternion : float32array[ 4 ]\nthe orientation of the frame. if this is modified directly setmatrixneedsupdate() must be called.\n.matrix\nreadonly matrix : float32array[ 16 ]\nthe local transform matrix composed from the position and quaternion.\n.matrixworld\nreadonly matrixworld : float32array[ 16 ]\nthe world transform matrix computed based on the parent matrixworld and this local matrix.\n.parent\nreadonly parent : frame\nthe parent frame this frame is a child of.\n.children\nreadonly children : array<frame>\nthe set of child frames this frame is a parent of.\n.setposition\nsetposition( x : number, y : number, z : number ) : void\nsets the position of the frame.\n.setworldposition\nsetworldposition( x : number, y : number, z : number ) : void\nsets the positon of the frame in world space. automatically computes the local position relative to the parent.\n.getworldposition\ngetworldposition( target : floatarray[ 3 ] ) : void\ngets the position of the frame in the world in the target argument.\n.setquaternion\nsetquaternion( x : number, y : number, z : number, w : number ) : void\nsets the orientation of the frame.\n.setworldquaternion\nsetworldquaternion( x : number, y : number, z : number, w : number ) : void\nsets the orientation of the frame in world space. automatically computes the local orientation relative to the parent.\n.getworldquaternion\ngetworldquaternion( target : floatarray[ 4 ] ) : void\ngets the quaternion of the frame in the world in the target argument.\n.traverseparents\ntraverseparents( callback : ( parent : frame ) => boolean ) : void\nfires the given callback for every parent starting with the closest. if callback returns true then the traversal is stopped.\n.traverse\ntraverse( callback : ( child : frame ) => boolean ) : void\nfires the given callback for every child recursively in breadth first order. if callback returns true then the traversal is stopped.\n.addchild\naddchild( child : frame ) : void\nadds a child to this frame and sets the childs parent to this frame. throws an error if the child already has a parent.\n.removechild\nremovechild( child : frame ) : void\nremoves the given child from this frame. throws an error if the given frame is not a child of this frame.\n.attachchild\nattachchild( child : frame ) : void\nadds the given frame as a child of this frame while preserving the world position of the child.\n.detachchild\ndetachchild( child : frame ) : void\nremoves the given frame as a child of this frame while preserving the world position of the child.\n.updatematrix\nupdatematrix() : void\nupdates the local .matrix field if it needs to be updated.\n.updatematrixworld\nupdatematrixworld( includechildren : boolean = true ) : void\nupdates the local .matrix and .worldmatrix fields if they need to be updated. ensures parent matrices are up to date.\n.setmatrixneedsupdate\nsetmatrixneedsupdate() : void\nflags this frame as needing a matrix and matrix world update.\n.setmatrixworldneedsupdate\nsetmatrixneedsupdate() : void\nflags this frame and all its children as needing a matrix world update.\nlink\nextends frame\na frame modeling a fixed connection between two joints. only joints may be added as children.\n.closurejoints\nclosurejoints : array<joint>\nthe set of joints that are connected to this indirectly via joint.makeclosure.\njoint\nextends frame\na dynamic frame representing a kinematic joint arbitrarily defineable degrees of freedom. a degree of freedom indicates an offset value can be set. only links may be added as children and a joint may only have a single child.\n.child\nreadonly child : link = null\nreference to the joint child.\n.isclosure\nreadonly isclosure : boolean = false\nwhether or not the child relationship is a closure or not.\n.dof\nreadonly dof : array<dof>\na list of all the free degrees of freedom.\n.dofflags\nreadonly dofflags : uint8array[6]\na list of 0 and 1 flags with 1 corresonding to a field in dof.\n.dofvalues\nreadonly dofvalues : float32array[6]\nthe current joint values for all joint degrees of freedom.\n.doftarget\nreadonly doftarget : float32array[6]\nthe joint value targets for each degree of freedom. solver will attempt to solve for these targets if targetset is true.\n.dofrestpose\nreadonly dofrestpose : float32array[6]\nthe rest pose for each joint degree of freedom. solver will attempt to move the joint towards this position when it does not compromise solving for the other goals and when restposeset is true.\n.mindoflimit\nreadonly mindoflimit : float32array[6]\nthe minimum value limits for each joint degree of freeom.\n.maxdoflimit\nreadonly maxdoflimit : float32array[6]\nthe maximum value limits for each joint degree of freeom.\n.matrixdof\nreadonly matrixdof : float32array[16]\nthe matrix representing the transformation offset due to the current joint values.\n.targetset\ntargetset : boolean = false\nwhen set to true solver will try to move this joints dofvalues towards the target values.\n.restposeset\nrestposeset : boolean = false\nwhen set to true solver will try to move this joints dofvalues towards the rest pose values without compromising the other goals.\n.setdof\nsetdof( ...dof : array<dof> ) : void\nsets the degrees of freedom of the joint. arguments must be passed in x, y, z, ex, ey, ez order without duplicate values. all relatd degree of freedom values are reset.\n.cleardof\ncleardof() : void\nclears all degrees of freedom.\n.set[ * ]values\nsetdofvalues( ...values : array<number> ) : void;\nsetrestposevalues( ...values : array<number> ) : void;\nsettargetvalues( ...values : array<number> ) : void;\nsetminlimits( ...values : array<number> ) : void;\nsetmaxlimits( ...values : array<number> ) : void;\nthe number of arguments must match the number of degrees of freedom of the joint.\n.set[ * ]value\nsetdofvalue( dof : dof, value : number ) : boolean\nsetrestposevalue( dof : dof, value : number ) : boolean\nsettargetvalue( dof : dof, value : number ) : boolean\nsetminlimit( dof : dof, value : number ) : boolean\nsetmaxlimit( dof : dof, value : number ) : boolean\n.get[ * ]value\ngetdofvalue( dof : dof ) : number\ngetrestposevalue( dof : dof ) : number\ngettargetvalue( dof : dof ) : number\ngetminlimit( dof : dof ) : number\ngetmaxlimit( dof : dof ) : number\n.makeclosure\nmakeclosure( child : link ) : void\ndeclares the relationship between this joint and the given child link is a closure meaning there is no direct parent child relationship but the solver will treat the closure link as a target for this joint to keep them closed.\nnote that when making a closure connection between a joint and a link the link will not be added to the joints children array and instead will only be available on the child field. the joint will be appended to the links closurejoints array.\ngoal\nextends joint\na frame representing a goal to achieve for a connected link. set degrees of freedom represent fixed goals for a link to achieve as opposed to moveable degrees of freedom defined for joints. a goal cannot have children and only be used to make a closure.\nsolver\nclass for solving the closure and target joint constraints of a sytem. as well as the listed fields a set of \"options\" are set on the object which are listed here:\n// whether or not to use the svd when calculating the pseudo inverse of the jacobian\n// which can result in a more numerically stable calculation. if the svd cannot be calculated\n// then the transpose method is used.\nusesvd = true;\n// the max amount of iterations to try to solve for. the solve will terminate\n// with solve_status.timeout if this limit is exceeded.\nmaxiterations = 5;\n// the threshold under which a joint is not considered to have really moved. if\n// no joint is moved more than this threshold then the solve will terminate with\n// solve_status.stalled.\nstallthreshold = 1e-4;\n// the threshold for comparing how much error has changed between solve iterations.\n// if the error has grown by more than this threshold then the solve will terminate\n// with solve_status.diverged.\ndivergethreshold = 0.01;\n// the fixed damping factor to use in the dls calculation.\ndampingfactor = 0.001;\n// the factor with which to move the joints towards the rest pose if set.\nrestposefactor = 0.01;\n// the thresholds with which to compute whether or not the translation or rotation\n// goals have been met. if the error between target and goal is under these\n// thresholds then the solve will terminate with solve_status.converged.\ntranslationconvergethreshold = 1e-3;\nrotationconvergethreshold = 1e-5;\n// factors to apply to the translation and rotation error in the error vector.\n// useful for weighting rotation higher than translation if they do not seem to\n// be solved \"evenly\". values are expected to be in the range [ 0, 1 ].\ntranslationfactor = 1;\nrotationfactor = 1;\n// the amount to move a joint when calculating the change in error a joint has\n// for a jacobian.\ntranslationstep = 1e-3;\nrotationstep = 1e-3;\n// the step to take towards the ik goals when solving. setting this to a larger value\n// may solve more quickly but may lead also lead to divergence.\ntranslationerrorclamp = 0.1;\nrotationerrorclamp = 0.1;\n.roots\nroots : array<frame>\nthe list of roots that should be accounted for in a solve. when .updatestructure is called the series of roots are traversed including closure joints to find all connected link hierarchies to use in the solve.\n.constructor\nconstructor( roots : array<frame> )\nconstructor takes a list of roots to solve for.\n.solve\nsolve() : array<solve_status>\ntraverses the given set of roots to find joint chains to solve for and attempts to solve for the error in the system goals. a result is returned for each independent chain found in the system.\n.updatestructure\nupdatestructure() : void\nmust be called whenever parent child relationships and structural changes related to the -----> tree !!!  change or .roots is modified.\nworkersolver\nimplements the interface defined by solver but runs the solve asynchronously on in a webworker. results are automatically copied to the joint system being solved for.\n\u26a0\ufe0f when sharedarraybuffers are not available new copies of arraybuffer are created every update and from the worker.\n.results\nresults : array<solve_status>\nthe list of the last results from the solve copied over from the webworker.\n.updatesolversettings\nupdatesolversettings( settings : object ) : void\nsets the solver settings in the webworker to the values in the given object. valid \"options\" values are listed in the solver docs.\n.updateframestate\nupdateframestate( ...jointstoupdate : array<joint> = [] ) : void\ncopies the joint settings for the given joints to the webworker for a solve. \"joint settings\" include everything except for joint values (that the solver would be solving for) and parent child relationships. if joint values or parent child relationhips change then updatestructure must be called.\n.solve\nsolve() : void\nstarts a solve in the webworker if one is not active. the solve will terminate automatically if none of the results are solve_status.timeout.\n.stop\nstop() : void\nterminate any active solve in the webworker.\n.dispose\ndispose() : void\nterminates the webworker and sets members to null.\nikrootshelper\nextends three.group\na helper class for rendering the joints and links in a three.js scene. renders frame relationships as lines and joints degrees of freedom with indicators based on the joint type.\n.roots\nroots : array<frame>\nset of roots to render in the helper visualization. if this is changed then .updatestructure must be called.\n.constructor\nconstructor( roots : array<frame> )\ntakes the set of roots to visualize.\n.setjointscale\nsetjointscale( scale : number ) : this\nsets the scale of the joint indicators.\n.setcolor\nsetcolor( color : color | string | number ) : this\nsets the color of the helper.\n.setdrawthrough\nsetdrawthrough( drawthrough : boolean ) : this\nsets whether the helper will draw through the environment.\n.setresolution\nsetresolution( width : number, height : number ) : this\nsets the resolution of the renderer so the 2d lines can be rendered at the appropriate thickness.\n.updatestructure\nupdatestructure() : void\nmust be called if the structure of the ik system being visualized has changed.\n.dispose\ndispose() : void\ncalls dispose on all created materials and geometry in the tree.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000658, "year": null}, {"Unnamed: 0": 1726, "autor": 706, "date": null, "content": "go-behaviortree\nPackage behaviortree provides a simple and powerful Go implementation of behavior trees without fluff.\nGo doc: https://godoc.org/github.com/joeycumines/go-behaviortree\nWikipedia: Behavior tree - AI, robotics, and control\ntype (\n// Node represents an node in a tree, that can be ticked\nNode func() (Tick, []Node)\n// Tick represents the logic for a node, which may or may not be stateful\nTick func(children []Node) (Status, error)\n// Status is a type with three valid values, Running, Success, and Failure, the three possible states for BTs\nStatus int\n)\n// Tick runs the node's tick function with it's children\nfunc (n Node) Tick() (Status, error)\nFeatures\nCore behavior tree implementation (the types above + Sequence and Selector)\nTools to aide implementation of \"reactive\" behavior trees (Memorize, Async, Sync)\nImplementations to run and manage behavior trees (NewManager, NewTicker)\nCollection of Tick implementations / wrappers (targeting various use cases)\nContext-like mechanism to attach metadata to Node values that can transit API boundaries / encapsulation\nBasic tree debugging capabilities via implementation of fmt.Stringer (see also DefaultPrinter, Node.Frame)\nExperimental support for the PA-BT planning algorithm via github.com/joeycumines/go-pabt\nDesign\nThis library provides an implementation of the generalised behavior tree pattern. The three types above along with the stateless Tick functions Selector and Sequence (line for line from Wikipedia) make up the entirety of the core functionality. Additional features have been derived by means of iterating against specific, real-world problem cases.\nIt is recommended that anyone interested in understanding behavior trees read from the start of chapter 1 of Colledanchise, Michele & Ogren, Petter. (2018). Behavior Trees in Robotics and AI: An Introduction. 10.1201/9780429489105. until at least (the end of) 1.3.2 - Control Flow Nodes with Memory. Further context should (hopefully) go a long way in aiding newcomers to gain a firm grasp of the relevant behavior and associated patterns, in order to start building automations using behavior trees.\nN.B. Though it does appear to be an excellent resource, Behavior Trees in Robotics and AI: An Introduction. was only brought to my attention well after the release of v1.3.0. In particular, concepts and terms are unlikely to be entirely consistent.\nReactivity\nExecuting a behavior tree sequentially (i.e. without the use of nodes that return Running) can be an effective way to decompose complicated switching logic. Though effective, this design suffers from limitations common to traditional finite state machines, which tends to cripple the modularity of any interruptable operations, as each tick must manage it's own exit condition(s). Golang's context doesn't really help in that case, either, being a communication mechanism, rather than a control mechanism. As an alternative, implementations may use pre-conditions (preceding child(ren) in a sequence), guarding an asynchronous tick. Context support may be desirable, and may be implemented as Tick implementations(s). This package provides a Context implementation to address several common use cases, such as operations with timeouts. Context support is peripheral in that it's only relevant to a subset of implementations, and was deliberately omitted from the core Tick type.\nModularity\nThis library only concerns itself with the task of actually running behavior trees. It is deliberately designed to make it straightforward to plug in external implementations. External implementations may be integrated as a fully-fledged recursively generated tree of Node. Tree-aware debug tracing could be implemented using a similar mechanism. At the end of the day though, implementations just need to Tick.\nImplementation\nThis section attempts to provide better insight into the intent of certain implementation details. Part of this involves offering implementation guidelines, based on imagined use cases (beyond those explored as part of each implementation). It ain't gospel.\nExecuting BTs\nUse explicit exit conditions\nReturn error to handle failures that should terminate tree execution\nBTs that \"run until completion\" may be implemented using NewTickerStopOnFailure (internally it just returns an error then strips any occurrence of that that error from the ticker's result)\nPanics may be used as normal, and are suitable for cases such unrecoverable errors that shouldn't happen\nShared state\nTick implementations should be grouped in a way that makes sense, in the context of any shared state\nIt may be convenient to expose a group of Tick prototypes with shared state as methods of a struct\nGlobal state should be avoided for all the regular reasons, but especially since it defeats a lot of the point of having modular, composable behavior\nEncapsulate Tick implementations, rather than Node\nChildren may be may be modified, but only until the (outer) Tick returns it's next non-running status\nThis mechanism theoretically facilitates dynamically generated trees while simultaneously supporting more complex / concurrency-heavy implementations made up of reusable building blocks\nRoadmap\nI am actively maintaining this project, and will be for the foreseeable future. It has been \"feature complete\" for some time though, so additional functionality will assessed on a case-by-case basis.\nExample Usage\nThe examples below are straight from example_test.go.\n// ExampleNewTickerStopOnFailure_counter demonstrates the use of NewTickerStopOnFailure to implement more complex \"run\n// to completion\" behavior using the simple modular building blocks provided by this package\nfunc ExampleNewTickerStopOnFailure_counter() {\nvar (\n// counter is the shared state used by this example\ncounter = 0\n// printCounter returns a node that will print the counter prefixed with the given name then succeed\nprintCounter = func(name string) Node {\nreturn New(\nfunc(children []Node) (Status, error) {\nfmt.Printf(\"%s: %d\\n\", name, counter)\nreturn Success, nil\n},\n)\n}\n// incrementCounter is a node that will increment counter then succeed\nincrementCounter = New(\nfunc(children []Node) (Status, error) {\ncounter++\nreturn Success, nil\n},\n)\n// ticker is what actually runs this example and will tick the behavior tree defined by a given node at a given\n// rate and will stop after the first failed tick or error or context cancel\nticker = NewTickerStopOnFailure(\ncontext.Background(),\ntime.Millisecond,\nNew(\nSelector, // runs each child sequentially until one succeeds (success) or all fail (failure)\nNew(\nSequence, // runs each child in order until one fails (failure) or they all succeed (success)\nNew(\nfunc(children []Node) (Status, error) { // succeeds while counter is less than 10\nif counter < 10 {\nreturn Success, nil\n}\nreturn Failure, nil\n},\n),\nincrementCounter,\nprintCounter(\"< 10\"),\n),\nNew(\nSequence,\nNew(\nfunc(children []Node) (Status, error) { // succeeds while counter is less than 20\nif counter < 20 {\nreturn Success, nil\n}\nreturn Failure, nil\n},\n),\nincrementCounter,\nprintCounter(\"< 20\"),\n),\n), // if both children failed (counter is >= 20) the root node will also fail\n)\n)\n// waits until ticker stops, which will be on the first failure of it's root node\n<-ticker.Done()\n// every Tick may return an error which would automatically cause a failure and propagation of the error\nif err := ticker.Err(); err != nil {\npanic(err)\n}\n// Output:\n// < 10: 1\n// < 10: 2\n// < 10: 3\n// < 10: 4\n// < 10: 5\n// < 10: 6\n// < 10: 7\n// < 10: 8\n// < 10: 9\n// < 10: 10\n// < 20: 11\n// < 20: 12\n// < 20: 13\n// < 20: 14\n// < 20: 15\n// < 20: 16\n// < 20: 17\n// < 20: 18\n// < 20: 19\n// < 20: 20\n}\n// ExampleMemorize_cancellationWithContextCancel demonstrates how support for reactive logic that uses context can\n// be implemented\nfunc ExampleMemorize_cancellationWithContextCancel() {\nvar (\nctx context.Context\ncancel context.CancelFunc\ndebug = func(label string, tick Tick) Tick {\nreturn func(children []Node) (status Status, err error) {\nstatus, err = tick(children)\nfmt.Printf(\"%s returned (%v, %v)\\n\", label, status, err)\nreturn\n}\n}\nrecorded = func(statuses ...Status) Tick {\nreturn func([]Node) (status Status, err error) {\nstatus = statuses[0]\nstatuses = statuses[1:]\nreturn\n}\n}\ncounter int\nticker = NewTickerStopOnFailure(\ncontext.Background(),\ntime.Millisecond,\nNew(\nAll,\nNew(\nMemorize(debug(`memorized`, All)),\nNew(func([]Node) (Status, error) {\ncounter++\nctx, cancel = context.WithCancel(context.WithValue(context.Background(), `n`, counter))\nreturn Success, nil\n}), // prepare the context\nNew(\ndebug(`sequence`, Sequence),\nNew(debug(`guard`, recorded(\nSuccess,\nSuccess,\nSuccess,\nSuccess,\nFailure,\n))),\nNew(func([]Node) (Status, error) {\nfmt.Printf(\"[start action] context #%d's err=%v\\n\", ctx.Value(`n`), ctx.Err())\nreturn Success, nil\n}),\nNew(debug(`action`, recorded(\nRunning,\nRunning,\nSuccess,\nRunning,\n))),\n),\nNew(func([]Node) (Status, error) {\ncancel()\nreturn Success, nil\n}), // cancel the context\n),\nNew(func([]Node) (Status, error) {\nfmt.Printf(\"[end memorized] context #%d's err=%v\\n\", ctx.Value(`n`), ctx.Err())\nreturn Success, nil\n}),\n),\n)\n)\n<-ticker.Done()\nif err := ticker.Err(); err != nil {\npanic(err)\n}\n//output:\n//guard returned (success, <nil>)\n//[start action] context #1's err=<nil>\n//action returned (running, <nil>)\n//sequence returned (running, <nil>)\n//memorized returned (running, <nil>)\n//guard returned (success, <nil>)\n//[start action] context #1's err=<nil>\n//action returned (running, <nil>)\n//sequence returned (running, <nil>)\n//memorized returned (running, <nil>)\n//guard returned (success, <nil>)\n//[start action] context #1's err=<nil>\n//action returned (success, <nil>)\n//sequence returned (success, <nil>)\n//memorized returned (success, <nil>)\n//[end memorized] context #1's err=context canceled\n//guard returned (success, <nil>)\n//[start action] context #2's err=<nil>\n//action returned (running, <nil>)\n//sequence returned (running, <nil>)\n//memorized returned (running, <nil>)\n//guard returned (failure, <nil>)\n//sequence returned (failure, <nil>)\n//memorized returned (failure, <nil>)\n//[end memorized] context #2's err=context canceled\n}\n// ExampleBackground_asyncJobQueue implements a basic example of backgrounding of long-running tasks that may be\n// performed concurrently, see ExampleNewTickerStopOnFailure_counter for an explanation of the ticker\nfunc ExampleBackground_asyncJobQueue() {\ntype (\nJob struct {\nName string\nDuration time.Duration\nDone chan struct{}\n}\n)\nvar (\n// doWorker performs the actual \"work\" for a Job\ndoWorker = func(job Job) {\nfmt.Printf(\"[worker] job \\\"%s\\\" STARTED\\n\", job.Name)\ntime.Sleep(job.Duration)\nfmt.Printf(\"[worker] job \\\"%s\\\" FINISHED\\n\", job.Name)\nclose(job.Done)\n}\n// queue be sent jobs, which will be received within the ticker\nqueue = make(chan Job, 50)\n// doClient sends and waits for a job\ndoClient = func(name string, duration time.Duration) {\njob := Job{name, duration, make(chan struct{})}\nts := time.Now()\nfmt.Printf(\"[client] job \\\"%s\\\" STARTED\\n\", job.Name)\nqueue <- job\n<-job.Done\nfmt.Printf(\"[client] job \\\"%s\\\" FINISHED\\n\", job.Name)\nt := time.Now().Sub(ts)\nd := t - job.Duration\nif d < 0 {\nd *= -1\n}\nif d > time.Millisecond*50 {\npanic(fmt.Errorf(`job \"%s\" expected %s actual %s`, job.Name, job.Duration.String(), t.String()))\n}\n}\n// running keeps track of the number of running jobs\nrunning = func() func(delta int64) int64 {\nvar (\nvalue int64\nmutex sync.Mutex\n)\nreturn func(delta int64) int64 {\nmutex.Lock()\ndefer mutex.Unlock()\nvalue += delta\nreturn value\n}\n}()\n// done will be closed when it's time to exit the ticker\ndone = make(chan struct{})\nticker = NewTickerStopOnFailure(\ncontext.Background(),\ntime.Millisecond,\nNew(\nSequence,\nNew(func(children []Node) (Status, error) {\nselect {\ncase <-done:\nreturn Failure, nil\ndefault:\nreturn Success, nil\n}\n}),\nfunc() Node {\n// the tick is initialised once, and is stateful (though the tick it's wrapping isn't)\ntick := Background(func() Tick { return Selector })\nreturn func() (Tick, []Node) {\n// this block will be refreshed each time that a new job is started\nvar (\njob Job\n)\nreturn tick, []Node{\nNew(\nSequence,\nSync([]Node{\nNew(func(children []Node) (Status, error) {\nselect {\ncase job = <-queue:\nrunning(1)\nreturn Success, nil\ndefault:\nreturn Failure, nil\n}\n}),\nNew(Async(func(children []Node) (Status, error) {\ndefer running(-1)\ndoWorker(job)\nreturn Success, nil\n})),\n})...,\n),\n// no job available - success\nNew(func(children []Node) (Status, error) {\nreturn Success, nil\n}),\n}\n}\n}(),\n),\n)\nwg sync.WaitGroup\n)\nwg.Add(1)\nrun := func(name string, duration time.Duration) {\nwg.Add(1)\ndefer wg.Done()\ndoClient(name, duration)\n}\nfmt.Printf(\"running jobs: %d\\n\", running(0))\ngo run(`1. 120ms`, time.Millisecond*120)\ntime.Sleep(time.Millisecond * 25)\ngo run(`2. 70ms`, time.Millisecond*70)\ntime.Sleep(time.Millisecond * 25)\nfmt.Printf(\"running jobs: %d\\n\", running(0))\ndoClient(`3. 150ms`, time.Millisecond*150)\ntime.Sleep(time.Millisecond * 50)\nfmt.Printf(\"running jobs: %d\\n\", running(0))\ntime.Sleep(time.Millisecond * 50)\nwg.Done()\nwg.Wait()\nclose(done)\n<-ticker.Done()\nif err := ticker.Err(); err != nil {\npanic(err)\n}\n//output:\n//running jobs: 0\n//[client] job \"1. 120ms\" STARTED\n//[worker] job \"1. 120ms\" STARTED\n//[client] job \"2. 70ms\" STARTED\n//[worker] job \"2. 70ms\" STARTED\n//running jobs: 2\n//[client] job \"3. 150ms\" STARTED\n//[worker] job \"3. 150ms\" STARTED\n//[worker] job \"2. 70ms\" FINISHED\n//[client] job \"2. 70ms\" FINISHED\n//[worker] job \"1. 120ms\" FINISHED\n//[client] job \"1. 120ms\" FINISHED\n//[worker] job \"3. 150ms\" FINISHED\n//[client] job \"3. 150ms\" FINISHED\n//running jobs: 0\n}\n// ExampleContext demonstrates how the Context implementation may be used to integrate with the context package\nfunc ExampleContext() {\nctx, cancel := context.WithCancel(context.Background())\ndefer cancel()\nvar (\nbtCtx = new(Context).WithTimeout(ctx, time.Millisecond*100)\ndebug = func(args ...interface{}) Tick {\nreturn func([]Node) (Status, error) {\nfmt.Println(args...)\nreturn Success, nil\n}\n}\ncounter int\ncounterEqual = func(v int) Tick {\nreturn func([]Node) (Status, error) {\nif counter == v {\nreturn Success, nil\n}\nreturn Failure, nil\n}\n}\ncounterInc Tick = func([]Node) (Status, error) {\ncounter++\n//fmt.Printf(\"counter = %d\\n\", counter)\nreturn Success, nil\n}\nticker = NewTicker(ctx, time.Millisecond, New(\nSequence,\nNew(\nSelector,\nNew(Not(btCtx.Err)),\nNew(\nSequence,\nNew(debug(`(re)initialising btCtx...`)),\nNew(btCtx.Init),\nNew(Not(btCtx.Err)),\n),\n),\nNew(\nSelector,\nNew(\nSequence,\nNew(counterEqual(0)),\nNew(debug(`blocking on context-enabled tick...`)),\nNew(\nbtCtx.Tick(func(ctx context.Context, children []Node) (Status, error) {\nfmt.Printf(\"NOTE children (%d) passed through\\n\", len(children))\n<-ctx.Done()\nreturn Success, nil\n}),\nNew(Sequence),\nNew(Sequence),\n),\nNew(counterInc),\n),\nNew(\nSequence,\nNew(counterEqual(1)),\nNew(debug(`blocking on done...`)),\nNew(btCtx.Done),\nNew(counterInc),\n),\nNew(\nSequence,\nNew(counterEqual(2)),\nNew(debug(`canceling local then rechecking the above...`)),\nNew(btCtx.Cancel),\nNew(btCtx.Err),\nNew(btCtx.Tick(func(ctx context.Context, children []Node) (Status, error) {\n<-ctx.Done()\nreturn Success, nil\n})),\nNew(btCtx.Done),\nNew(counterInc),\n),\nNew(\nSequence,\nNew(counterEqual(3)),\nNew(debug(`canceling parent then rechecking the above...`)),\nNew(func([]Node) (Status, error) {\ncancel()\nreturn Success, nil\n}),\nNew(btCtx.Err),\nNew(btCtx.Tick(func(ctx context.Context, children []Node) (Status, error) {\n<-ctx.Done()\nreturn Success, nil\n})),\nNew(btCtx.Done),\nNew(debug(`exiting...`)),\n),\n),\n))\n)\n<-ticker.Done()\n//output:\n//(re)initialising btCtx...\n//blocking on context-enabled tick...\n//NOTE children (2) passed through\n//(re)initialising btCtx...\n//blocking on done...\n//(re)initialising btCtx...\n//canceling local then rechecking the above...\n//(re)initialising btCtx...\n//canceling parent then rechecking the above...\n//exiting...\n}", "link": "https://github.com/joeycumines/go-behaviortree", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "go-behaviortree\npackage behaviortree provides a simple and powerful go implementation of behavior trees without fluff.\ngo doc: https://godoc.org/github.com/joeycumines/go-behaviortree\nwikipedia: behavior -----> tree !!!  - ai, robotics, and control\ntype (\n// node represents an node in a -----> tree !!! , that can be ticked\nnode func() (tick, []node)\n// tick represents the logic for a node, which may or may not be stateful\ntick func(children []node) (status, error)\n// status is a type with three valid values, running, success, and failure, the three possible states for bts\nstatus int\n)\n// tick runs the node's tick function with it's children\nfunc (n node) tick() (status, error)\nfeatures\ncore behavior tree implementation (the types above + sequence and selector)\ntools to aide implementation of \"reactive\" behavior trees (memorize, async, sync)\nimplementations to run and manage behavior trees (newmanager, newticker)\ncollection of tick implementations / wrappers (targeting various use cases)\ncontext-like mechanism to attach metadata to node values that can transit api boundaries / encapsulation\nbasic tree debugging capabilities via implementation of fmt.stringer (see also defaultprinter, node.frame)\nexperimental support for the pa-bt planning algorithm via github.com/joeycumines/go-pabt\ndesign\nthis library provides an implementation of the generalised behavior tree pattern. the three types above along with the stateless tick functions selector and sequence (line for line from wikipedia) make up the entirety of the core functionality. additional features have been derived by means of iterating against specific, real-world problem cases.\nit is recommended that anyone interested in understanding behavior trees read from the start of chapter 1 of colledanchise, michele & ogren, petter. (2018). behavior trees in robotics and ai: an introduction. 10.1201/9780429489105. until at least (the end of) 1.3.2 - control flow nodes with memory. further context should (hopefully) go a long way in aiding newcomers to gain a firm grasp of the relevant behavior and associated patterns, in order to start building automations using behavior trees.\nn.b. though it does appear to be an excellent resource, behavior trees in robotics and ai: an introduction. was only brought to my attention well after the release of v1.3.0. in particular, concepts and terms are unlikely to be entirely consistent.\nreactivity\nexecuting a behavior tree sequentially (i.e. without the use of nodes that return running) can be an effective way to decompose complicated switching logic. though effective, this design suffers from limitations common to traditional finite state machines, which tends to cripple the modularity of any interruptable operations, as each tick must manage it's own exit condition(s). golang's context doesn't really help in that case, either, being a communication mechanism, rather than a control mechanism. as an alternative, implementations may use pre-conditions (preceding child(ren) in a sequence), guarding an asynchronous tick. context support may be desirable, and may be implemented as tick implementations(s). this package provides a context implementation to address several common use cases, such as operations with timeouts. context support is peripheral in that it's only relevant to a subset of implementations, and was deliberately omitted from the core tick type.\nmodularity\nthis library only concerns itself with the task of actually running behavior trees. it is deliberately designed to make it straightforward to plug in external implementations. external implementations may be integrated as a fully-fledged recursively generated tree of node. tree-aware debug tracing could be implemented using a similar mechanism. at the end of the day though, implementations just need to tick.\nimplementation\nthis section attempts to provide better insight into the intent of certain implementation details. part of this involves offering implementation guidelines, based on imagined use cases (beyond those explored as part of each implementation). it ain't gospel.\nexecuting bts\nuse explicit exit conditions\nreturn error to handle failures that should terminate tree execution\nbts that \"run until completion\" may be implemented using newtickerstoponfailure (internally it just returns an error then strips any occurrence of that that error from the ticker's result)\npanics may be used as normal, and are suitable for cases such unrecoverable errors that shouldn't happen\nshared state\ntick implementations should be grouped in a way that makes sense, in the context of any shared state\nit may be convenient to expose a group of tick prototypes with shared state as methods of a struct\nglobal state should be avoided for all the regular reasons, but especially since it defeats a lot of the point of having modular, composable behavior\nencapsulate tick implementations, rather than node\nchildren may be may be modified, but only until the (outer) tick returns it's next non-running status\nthis mechanism theoretically facilitates dynamically generated trees while simultaneously supporting more complex / concurrency-heavy implementations made up of reusable building blocks\nroadmap\ni am actively maintaining this project, and will be for the foreseeable future. it has been \"feature complete\" for some time though, so additional functionality will assessed on a case-by-case basis.\nexample usage\nthe examples below are straight from example_test.go.\n// examplenewtickerstoponfailure_counter demonstrates the use of newtickerstoponfailure to implement more complex \"run\n// to completion\" behavior using the simple modular building blocks provided by this package\nfunc examplenewtickerstoponfailure_counter() {\nvar (\n// counter is the shared state used by this example\ncounter = 0\n// printcounter returns a node that will print the counter prefixed with the given name then succeed\nprintcounter = func(name string) node {\nreturn new(\nfunc(children []node) (status, error) {\nfmt.printf(\"%s: %d\\n\", name, counter)\nreturn success, nil\n},\n)\n}\n// incrementcounter is a node that will increment counter then succeed\nincrementcounter = new(\nfunc(children []node) (status, error) {\ncounter++\nreturn success, nil\n},\n)\n// ticker is what actually runs this example and will tick the behavior tree defined by a given node at a given\n// rate and will stop after the first failed tick or error or context cancel\nticker = newtickerstoponfailure(\ncontext.background(),\ntime.millisecond,\nnew(\nselector, // runs each child sequentially until one succeeds (success) or all fail (failure)\nnew(\nsequence, // runs each child in order until one fails (failure) or they all succeed (success)\nnew(\nfunc(children []node) (status, error) { // succeeds while counter is less than 10\nif counter < 10 {\nreturn success, nil\n}\nreturn failure, nil\n},\n),\nincrementcounter,\nprintcounter(\"< 10\"),\n),\nnew(\nsequence,\nnew(\nfunc(children []node) (status, error) { // succeeds while counter is less than 20\nif counter < 20 {\nreturn success, nil\n}\nreturn failure, nil\n},\n),\nincrementcounter,\nprintcounter(\"< 20\"),\n),\n), // if both children failed (counter is >= 20) the root node will also fail\n)\n)\n// waits until ticker stops, which will be on the first failure of it's root node\n<-ticker.done()\n// every tick may return an error which would automatically cause a failure and propagation of the error\nif err := ticker.err(); err != nil {\npanic(err)\n}\n// output:\n// < 10: 1\n// < 10: 2\n// < 10: 3\n// < 10: 4\n// < 10: 5\n// < 10: 6\n// < 10: 7\n// < 10: 8\n// < 10: 9\n// < 10: 10\n// < 20: 11\n// < 20: 12\n// < 20: 13\n// < 20: 14\n// < 20: 15\n// < 20: 16\n// < 20: 17\n// < 20: 18\n// < 20: 19\n// < 20: 20\n}\n// examplememorize_cancellationwithcontextcancel demonstrates how support for reactive logic that uses context can\n// be implemented\nfunc examplememorize_cancellationwithcontextcancel() {\nvar (\nctx context.context\ncancel context.cancelfunc\ndebug = func(label string, tick tick) tick {\nreturn func(children []node) (status status, err error) {\nstatus, err = tick(children)\nfmt.printf(\"%s returned (%v, %v)\\n\", label, status, err)\nreturn\n}\n}\nrecorded = func(statuses ...status) tick {\nreturn func([]node) (status status, err error) {\nstatus = statuses[0]\nstatuses = statuses[1:]\nreturn\n}\n}\ncounter int\nticker = newtickerstoponfailure(\ncontext.background(),\ntime.millisecond,\nnew(\nall,\nnew(\nmemorize(debug(`memorized`, all)),\nnew(func([]node) (status, error) {\ncounter++\nctx, cancel = context.withcancel(context.withvalue(context.background(), `n`, counter))\nreturn success, nil\n}), // prepare the context\nnew(\ndebug(`sequence`, sequence),\nnew(debug(`guard`, recorded(\nsuccess,\nsuccess,\nsuccess,\nsuccess,\nfailure,\n))),\nnew(func([]node) (status, error) {\nfmt.printf(\"[start action] context #%d's err=%v\\n\", ctx.value(`n`), ctx.err())\nreturn success, nil\n}),\nnew(debug(`action`, recorded(\nrunning,\nrunning,\nsuccess,\nrunning,\n))),\n),\nnew(func([]node) (status, error) {\ncancel()\nreturn success, nil\n}), // cancel the context\n),\nnew(func([]node) (status, error) {\nfmt.printf(\"[end memorized] context #%d's err=%v\\n\", ctx.value(`n`), ctx.err())\nreturn success, nil\n}),\n),\n)\n)\n<-ticker.done()\nif err := ticker.err(); err != nil {\npanic(err)\n}\n//output:\n//guard returned (success, <nil>)\n//[start action] context #1's err=<nil>\n//action returned (running, <nil>)\n//sequence returned (running, <nil>)\n//memorized returned (running, <nil>)\n//guard returned (success, <nil>)\n//[start action] context #1's err=<nil>\n//action returned (running, <nil>)\n//sequence returned (running, <nil>)\n//memorized returned (running, <nil>)\n//guard returned (success, <nil>)\n//[start action] context #1's err=<nil>\n//action returned (success, <nil>)\n//sequence returned (success, <nil>)\n//memorized returned (success, <nil>)\n//[end memorized] context #1's err=context canceled\n//guard returned (success, <nil>)\n//[start action] context #2's err=<nil>\n//action returned (running, <nil>)\n//sequence returned (running, <nil>)\n//memorized returned (running, <nil>)\n//guard returned (failure, <nil>)\n//sequence returned (failure, <nil>)\n//memorized returned (failure, <nil>)\n//[end memorized] context #2's err=context canceled\n}\n// examplebackground_asyncjobqueue implements a basic example of backgrounding of long-running tasks that may be\n// performed concurrently, see examplenewtickerstoponfailure_counter for an explanation of the ticker\nfunc examplebackground_asyncjobqueue() {\ntype (\njob struct {\nname string\nduration time.duration\ndone chan struct{}\n}\n)\nvar (\n// doworker performs the actual \"work\" for a job\ndoworker = func(job job) {\nfmt.printf(\"[worker] job \\\"%s\\\" started\\n\", job.name)\ntime.sleep(job.duration)\nfmt.printf(\"[worker] job \\\"%s\\\" finished\\n\", job.name)\nclose(job.done)\n}\n// queue be sent jobs, which will be received within the ticker\nqueue = make(chan job, 50)\n// doclient sends and waits for a job\ndoclient = func(name string, duration time.duration) {\njob := job{name, duration, make(chan struct{})}\nts := time.now()\nfmt.printf(\"[client] job \\\"%s\\\" started\\n\", job.name)\nqueue <- job\n<-job.done\nfmt.printf(\"[client] job \\\"%s\\\" finished\\n\", job.name)\nt := time.now().sub(ts)\nd := t - job.duration\nif d < 0 {\nd *= -1\n}\nif d > time.millisecond*50 {\npanic(fmt.errorf(`job \"%s\" expected %s actual %s`, job.name, job.duration.string(), t.string()))\n}\n}\n// running keeps track of the number of running jobs\nrunning = func() func(delta int64) int64 {\nvar (\nvalue int64\nmutex sync.mutex\n)\nreturn func(delta int64) int64 {\nmutex.lock()\ndefer mutex.unlock()\nvalue += delta\nreturn value\n}\n}()\n// done will be closed when it's time to exit the ticker\ndone = make(chan struct{})\nticker = newtickerstoponfailure(\ncontext.background(),\ntime.millisecond,\nnew(\nsequence,\nnew(func(children []node) (status, error) {\nselect {\ncase <-done:\nreturn failure, nil\ndefault:\nreturn success, nil\n}\n}),\nfunc() node {\n// the tick is initialised once, and is stateful (though the tick it's wrapping isn't)\ntick := background(func() tick { return selector })\nreturn func() (tick, []node) {\n// this block will be refreshed each time that a new job is started\nvar (\njob job\n)\nreturn tick, []node{\nnew(\nsequence,\nsync([]node{\nnew(func(children []node) (status, error) {\nselect {\ncase job = <-queue:\nrunning(1)\nreturn success, nil\ndefault:\nreturn failure, nil\n}\n}),\nnew(async(func(children []node) (status, error) {\ndefer running(-1)\ndoworker(job)\nreturn success, nil\n})),\n})...,\n),\n// no job available - success\nnew(func(children []node) (status, error) {\nreturn success, nil\n}),\n}\n}\n}(),\n),\n)\nwg sync.waitgroup\n)\nwg.add(1)\nrun := func(name string, duration time.duration) {\nwg.add(1)\ndefer wg.done()\ndoclient(name, duration)\n}\nfmt.printf(\"running jobs: %d\\n\", running(0))\ngo run(`1. 120ms`, time.millisecond*120)\ntime.sleep(time.millisecond * 25)\ngo run(`2. 70ms`, time.millisecond*70)\ntime.sleep(time.millisecond * 25)\nfmt.printf(\"running jobs: %d\\n\", running(0))\ndoclient(`3. 150ms`, time.millisecond*150)\ntime.sleep(time.millisecond * 50)\nfmt.printf(\"running jobs: %d\\n\", running(0))\ntime.sleep(time.millisecond * 50)\nwg.done()\nwg.wait()\nclose(done)\n<-ticker.done()\nif err := ticker.err(); err != nil {\npanic(err)\n}\n//output:\n//running jobs: 0\n//[client] job \"1. 120ms\" started\n//[worker] job \"1. 120ms\" started\n//[client] job \"2. 70ms\" started\n//[worker] job \"2. 70ms\" started\n//running jobs: 2\n//[client] job \"3. 150ms\" started\n//[worker] job \"3. 150ms\" started\n//[worker] job \"2. 70ms\" finished\n//[client] job \"2. 70ms\" finished\n//[worker] job \"1. 120ms\" finished\n//[client] job \"1. 120ms\" finished\n//[worker] job \"3. 150ms\" finished\n//[client] job \"3. 150ms\" finished\n//running jobs: 0\n}\n// examplecontext demonstrates how the context implementation may be used to integrate with the context package\nfunc examplecontext() {\nctx, cancel := context.withcancel(context.background())\ndefer cancel()\nvar (\nbtctx = new(context).withtimeout(ctx, time.millisecond*100)\ndebug = func(args ...interface{}) tick {\nreturn func([]node) (status, error) {\nfmt.println(args...)\nreturn success, nil\n}\n}\ncounter int\ncounterequal = func(v int) tick {\nreturn func([]node) (status, error) {\nif counter == v {\nreturn success, nil\n}\nreturn failure, nil\n}\n}\ncounterinc tick = func([]node) (status, error) {\ncounter++\n//fmt.printf(\"counter = %d\\n\", counter)\nreturn success, nil\n}\nticker = newticker(ctx, time.millisecond, new(\nsequence,\nnew(\nselector,\nnew(not(btctx.err)),\nnew(\nsequence,\nnew(debug(`(re)initialising btctx...`)),\nnew(btctx.init),\nnew(not(btctx.err)),\n),\n),\nnew(\nselector,\nnew(\nsequence,\nnew(counterequal(0)),\nnew(debug(`blocking on context-enabled tick...`)),\nnew(\nbtctx.tick(func(ctx context.context, children []node) (status, error) {\nfmt.printf(\"note children (%d) passed through\\n\", len(children))\n<-ctx.done()\nreturn success, nil\n}),\nnew(sequence),\nnew(sequence),\n),\nnew(counterinc),\n),\nnew(\nsequence,\nnew(counterequal(1)),\nnew(debug(`blocking on done...`)),\nnew(btctx.done),\nnew(counterinc),\n),\nnew(\nsequence,\nnew(counterequal(2)),\nnew(debug(`canceling local then rechecking the above...`)),\nnew(btctx.cancel),\nnew(btctx.err),\nnew(btctx.tick(func(ctx context.context, children []node) (status, error) {\n<-ctx.done()\nreturn success, nil\n})),\nnew(btctx.done),\nnew(counterinc),\n),\nnew(\nsequence,\nnew(counterequal(3)),\nnew(debug(`canceling parent then rechecking the above...`)),\nnew(func([]node) (status, error) {\ncancel()\nreturn success, nil\n}),\nnew(btctx.err),\nnew(btctx.tick(func(ctx context.context, children []node) (status, error) {\n<-ctx.done()\nreturn success, nil\n})),\nnew(btctx.done),\nnew(debug(`exiting...`)),\n),\n),\n))\n)\n<-ticker.done()\n//output:\n//(re)initialising btctx...\n//blocking on context-enabled tick...\n//note children (2) passed through\n//(re)initialising btctx...\n//blocking on done...\n//(re)initialising btctx...\n//canceling local then rechecking the above...\n//(re)initialising btctx...\n//canceling parent then rechecking the above...\n//exiting...\n}", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000706, "year": null}, {"Unnamed: 0": 1749, "autor": 729, "date": null, "content": "TinyCog\nA collection of speech, vision, and movement functionalities aimed at small or toy robots on embedded systems, such as the Raspberry Pi computer. High level reasoning, language understanding, language gneration and movement planning is provided by OpenCog.\nThe current hardware platform requires an RPI3 computer, a Pi Camera V2 and a USB Microphone; other sensor/detector components are planned.\nThe current software functions include face detection, emotion recognition, gesture analysis, speech-to-text and text-to-speech subsystems. All high-level founction is provided by OpenCog, and specifically by the Ghost scripting system -- ghost is able to process sensory input, and provide coordinated chatbot and movement abilities.\nSetup\nEverything here is meant to run on a rpi3 computer; one can also compile everything on a standard Linux desktop computer.\nA fully prepped raspbian image is available here\nUse xzcat to clone the image as shown here replacing sdX with your device.\nxzcat oc-debian-stretch-arm64.img.xz | sudo dd of=/dev/sdX\nWhen you first boot with this image, and login with the default credentials, it automatically expands the filesystem to occupy the entire / partition and then it reboots. This is a 64bit Debian Stretch OS for RPI3 which means that PiCamera driver is not available. A USB camera should be used for this image.\nThe default credentials:\nUsername: oc\nPassword: opencog\nThe image contains the opencog version at the time of it's building and other libraries such as opencv and dlib (3.4 and 19.15). To see the opencog commit version, pkg-config can be used.\npkg-config --variable=cogutil opencog #shows cogutil commit version\npkg-config --variable=atomspace opencog #shows opencog commit version\npkg-config --variable=opencog opencog #shows opencog commit version\nThere is a problem with this image, no driver for piCamera as it's not available as a 64bit binary.\nInstall\nNeed to have these whether on desktop or rpi\nOpencv-3.2\nOpencv_contrib-3.2\nDlib\nFestival\nGuile 2.2\nPocketSphinx On rpi3 only\nRaspicam > v1.5\nWiringPi\nUse cmake for building. Default build mode is Debug mode. Set CMAKE_BUILD_TYPE to Release to disable debug mode. For the emotion recognition service you should set the variable SERVER_ADDRESS to \"34.216.72.29:6205\"\ncd to TinyCog dir\nmkdir build\ncd build\ncmake .. # -DCMAKE_BUILD_TYPE=Release -DSERVER_ADDRESS=\"34.216.72.29:6205\"\nmake\nTesting\nTo test the sensors from the guile shell, run the following from within the build dir which opens up the camera and does a live view of the camera with markings for the sensors.\n$ ./TestDrRoboto.scm\nTo test from a video file instead of a camera, run the folloiwng way\n$ ./TestDrRoboto.scm -- <video_file_path>\nRunning\nStart relex server\nrun run.sh\n$ guile -l dr-roboto.scm\nIn another terminal, connect to port 5555 via telnet to input speech\n$ telnet localhost 5555\nImplementation\nOverall Description\nThe dr-roboto.cpp file is compiled to a guile extension which is loaded with the scheme dr-roboto.scm file. This guile extension is written in C++ and it's main job is to open the camera and sense stuff. When the scheme program loads the extension, the first thing it does is it sends the address of its atomspace to the extension so that the two can share an atomspace. Then the sensors are started which is a loop run in a separate thread that just collects information and places them in the atomspace. Most sensory values are stored with Atomspace Values in the following format:\nValue\nConceptNode \"position\"\nConceptNode \"face_x\"\nFloatValue X Y\nThe scheme program dr-roboto.scm includes the behavior/behavior.scm code that contains a very small model of OpenCog behavior tree. The behavior is a looking behavior which first goes through checking if there is a face, if there is only one then just look at that one, if there are more than one then check if one of them is smiling, if not, check if any of them has a non-neutral facial expression, if not just look at one of the faces randomly. If there are no faces in view then just look at the salient point. The behavior tree calls functions that simply check the atomspace for the information they require. The behavior/behavior.scm file also loads the ghost scripts located here When the behavior program wants to command the Professor Einstein robot of Hanson Robotics, it calls functions defined in cmd_einstein.scm This program connects to the Professor Einstein robot through its socket api and sends it commands. The fuctions.scm file contains some utility functions used by other scheme source files such as converting ghost results which are a list of WordNodes to a single string to be spoken by the robot and mapping of values between the image dimensions and the robot's pan/tilt limit.\nSensors\nSensors are a camera and microphones. The camera for face detection -> face landmark -> facial expression and emotion and also for hand detection -> convexity defects -> fingers count and gestures. The microphone for STT and sound source localization. Some of the sensor programs such as the face, hand and voice activity detection can run on the PI without much stress on the hardware but other functionalities like emotion and and speech recogntion should be implemented as services from a server possibly from singnet. Currently, STT is implemented using pocketsphinx. It's not ideal but can be used for a very limited range of commands and simple conversation.\nAct\nFor Hanson Robotic's Professor Einstein robot, the cmd_einstein.scm file contains the code necessary to command it. The robot should also act as well as sense. It must speak and move around. The speech synthesis utilizes festival. The code is in act/audio. Movement was intended to be with SPI communication with the hardware but that has changed. However the spi interface is in comm/spi\nBehavior\nCode connecting the behavior module to the STT is found in socket/socket.scm\nBehavior rules are in behavior/\nGhost rules that list all syntax found in ghost/\nFor non-speech triggered actions, an OpenCog behavior tree is used.\nEvents and event monitoring functions are in beavior/events.scm\nFunctions/actions used by various scheme code are found at behavior/functions.scm\nScheme-C++ binding in dr-roboto/dr-roboto.cpp\nScratch interface in behavior/scratch\nToDo\nImprove STT\nGhost rules\nStories for a specific identity we need the robot to have", "link": "https://github.com/opencog/TinyCog", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "tinycog\na collection of speech, vision, and movement functionalities aimed at small or toy robots on embedded systems, such as the raspberry pi computer. high level reasoning, language understanding, language gneration and movement planning is provided by opencog.\nthe current hardware platform requires an rpi3 computer, a pi camera v2 and a usb microphone; other sensor/detector components are planned.\nthe current software functions include face detection, emotion recognition, gesture analysis, speech-to-text and text-to-speech subsystems. all high-level founction is provided by opencog, and specifically by the ghost scripting system -- ghost is able to process sensory input, and provide coordinated chatbot and movement abilities.\nsetup\neverything here is meant to run on a rpi3 computer; one can also compile everything on a standard linux desktop computer.\na fully prepped raspbian image is available here\nuse xzcat to clone the image as shown here replacing sdx with your device.\nxzcat oc-debian-stretch-arm64.img.xz | sudo dd of=/dev/sdx\nwhen you first boot with this image, and login with the default credentials, it automatically expands the filesystem to occupy the entire / partition and then it reboots. this is a 64bit debian stretch os for rpi3 which means that picamera driver is not available. a usb camera should be used for this image.\nthe default credentials:\nusername: oc\npassword: opencog\nthe image contains the opencog version at the time of it's building and other libraries such as opencv and dlib (3.4 and 19.15). to see the opencog commit version, pkg-config can be used.\npkg-config --variable=cogutil opencog #shows cogutil commit version\npkg-config --variable=atomspace opencog #shows opencog commit version\npkg-config --variable=opencog opencog #shows opencog commit version\nthere is a problem with this image, no driver for picamera as it's not available as a 64bit binary.\ninstall\nneed to have these whether on desktop or rpi\nopencv-3.2\nopencv_contrib-3.2\ndlib\nfestival\nguile 2.2\npocketsphinx on rpi3 only\nraspicam > v1.5\nwiringpi\nuse cmake for building. default build mode is debug mode. set cmake_build_type to release to disable debug mode. for the emotion recognition service you should set the variable server_address to \"34.216.72.29:6205\"\ncd to tinycog dir\nmkdir build\ncd build\ncmake .. # -dcmake_build_type=release -dserver_address=\"34.216.72.29:6205\"\nmake\ntesting\nto test the sensors from the guile shell, run the following from within the build dir which opens up the camera and does a live view of the camera with markings for the sensors.\n$ ./testdrroboto.scm\nto test from a video file instead of a camera, run the folloiwng way\n$ ./testdrroboto.scm -- <video_file_path>\nrunning\nstart relex server\nrun run.sh\n$ guile -l dr-roboto.scm\nin another terminal, connect to port 5555 via telnet to input speech\n$ telnet localhost 5555\nimplementation\noverall description\nthe dr-roboto.cpp file is compiled to a guile extension which is loaded with the scheme dr-roboto.scm file. this guile extension is written in c++ and it's main job is to open the camera and sense stuff. when the scheme program loads the extension, the first thing it does is it sends the address of its atomspace to the extension so that the two can share an atomspace. then the sensors are started which is a loop run in a separate thread that just collects information and places them in the atomspace. most sensory values are stored with atomspace values in the following format:\nvalue\nconceptnode \"position\"\nconceptnode \"face_x\"\nfloatvalue x y\nthe scheme program dr-roboto.scm includes the behavior/behavior.scm code that contains a very small model of opencog behavior -----> tree !!! . the behavior is a looking behavior which first goes through checking if there is a face, if there is only one then just look at that one, if there are more than one then check if one of them is smiling, if not, check if any of them has a non-neutral facial expression, if not just look at one of the faces randomly. if there are no faces in view then just look at the salient point. the behavior tree calls functions that simply check the atomspace for the information they require. the behavior/behavior.scm file also loads the ghost scripts located here when the behavior program wants to command the professor einstein robot of hanson robotics, it calls functions defined in cmd_einstein.scm this program connects to the professor einstein robot through its socket api and sends it commands. the fuctions.scm file contains some utility functions used by other scheme source files such as converting ghost results which are a list of wordnodes to a single string to be spoken by the robot and mapping of values between the image dimensions and the robot's pan/tilt limit.\nsensors\nsensors are a camera and microphones. the camera for face detection -> face landmark -> facial expression and emotion and also for hand detection -> convexity defects -> fingers count and gestures. the microphone for stt and sound source localization. some of the sensor programs such as the face, hand and voice activity detection can run on the pi without much stress on the hardware but other functionalities like emotion and and speech recogntion should be implemented as services from a server possibly from singnet. currently, stt is implemented using pocketsphinx. it's not ideal but can be used for a very limited range of commands and simple conversation.\nact\nfor hanson robotic's professor einstein robot, the cmd_einstein.scm file contains the code necessary to command it. the robot should also act as well as sense. it must speak and move around. the speech synthesis utilizes festival. the code is in act/audio. movement was intended to be with spi communication with the hardware but that has changed. however the spi interface is in comm/spi\nbehavior\ncode connecting the behavior module to the stt is found in socket/socket.scm\nbehavior rules are in behavior/\nghost rules that list all syntax found in ghost/\nfor non-speech triggered actions, an opencog behavior tree is used.\nevents and event monitoring functions are in beavior/events.scm\nfunctions/actions used by various scheme code are found at behavior/functions.scm\nscheme-c++ binding in dr-roboto/dr-roboto.cpp\nscratch interface in behavior/scratch\ntodo\nimprove stt\nghost rules\nstories for a specific identity we need the robot to have", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000729, "year": null}, {"Unnamed: 0": 1751, "autor": 731, "date": null, "content": "ROBIN\nA ROS-CODESYS shared memory bridge to map CODESYS variables to ROS topics.\nThis bridge is the result of the ROBIN project, a Focused Technical Project (FTP) of the ROSIN European project.\nGetting started\nThe bridge is made up of two components:\nA ROS package that doesn't require any manual configuration other than the installation of its dependencies. The package contains a ROS node that reads/writes data from/to shared memory spaces and publishes/receives messages to/from ROS topics.\nA CODESYS library to be used in a CODESYS project created by the user. An example project is provided in robin_updater/src/robin_updater/cfg/codesys_project.xml. The library contains a Robin function block that reads/writes data from/to shared memory spaces and writes/reads it to CODESYS user-defined variables.\nThe following IEC 61131-3 data types are currently supported:\nBOOL\nBYTE\nSINT, INT, DINT, LINT, USINT, UINT, UDINT, ULINT\nREAL, LREAL\nCHAR, STRING\nAs well as arrays and custom structs. The following standard ROS message packages are already defined as CODESYS structs and available on the Robin CODESYS library:\nstd_msgs\ngeometry_msgs\nThese variables have to be defined on both the CODESYS project and the ROS package. For arrays or for structs with string or array members, because these data types are handled as non-POD (Plain Old Data) objects in C++, the mapping between the C++ variables and the ROS messages has to be explicitly defined. However, an updater application was developed to automate most of this process. The user simply needs to define its desired variables on the CODESYS project and run the updater.\nPrerequisites\nUbuntu 18.04/16.04 system (may work on other distros as well) with:\nSSH server\nROS Melodic/Kinetic\nCODESYS Control SoftPLC application:\nDebian/Ubuntu\nRaspberry Pi\nBeagleBone\nWindows system with:\nCODESYS Development System V3 (developed and tested with version 3.5.15.0)\nWindows OpenSSH\nInstallation\nInstall CODESYS library:\nOpen CODESYS Development System V3\nGo to Tools->Library Repository->Install\nFind and select robin_bridge/src/robin.library\nClose the Library Repository dialog\nCreate catkin workspace (if non-existent):\nmkdir -p ~/catkin_ws/src\ncd ~/catkin_ws\ncatkin_make # or 'catkin build'\nsource ~/catkin_ws/devel/setup.bash\nClone repository into catkin workspace (eg. ~/catkin_ws):\ncd ~/catkin_ws/src\ngit clone https://github.com/ScalABLE40/robin\nInstall updater package dependencies:\nrosdep install robin_updater\nCompile bridge package:\ncd ~/catkin_ws\ncatkin_make robin # or 'catkin build robin'\nUsage\nCreate CODESYS project. You can either:\nCreate your own project and add the Robin library to it.\nIn the Devices tree, double click Library Manager and open the Add Library dialog\nFind and select the previously installed Robin library and click OK\nYou can now use the Robin function block as shown in the Examples section\nCreate a new empty project and import the example project from codesys_project.xml.\nGo to Project->Import PLCopenXML...\nFind and select the XML file\nSelect all items and click OK\nVariable length arrays are only partially supported in CODESYS. To make the updater interpret a regular fixed length array as a ROS variable length array, preceed its declaration with the line: {attribute 'robin_var_len'}.\nMake sure you can establish connection with the PLC. Go to the Devices tree, double click the Device and then:\nScan Network... for your PLC device.\nOr add it manually Device->Options->Manage Favourite Devices...\nGo to Windows Search Bar->Services and make sure Windows OpenSSH Authentication Agent service is running (Startup type: Automatic).\nRun the updater application:\nGo to Tools->Scripting->Execute Script File...\nOpen the script file robin_updater/src/robin_updater/src/robin_updater/start_update.py\nIf you don't have access to it from CODESYS, first copy it to your Windows system\nInput the requested information (target address and password) and follow the script's execution\nNOTE: Password will be asked again during the script\nLaunch the robin ROS node. Will restart codesyscontrol service and then launch the node:\nTo avoid having to manually restart codesyscontrol after each update run:\necho \"$USER ALL=(ALL:ALL) NOPASSWD: /bin/systemctl * codesyscontrol\" | sudo EDITOR=\"tee\" visudo -f /etc/sudoers.d/allow_restart_codesyscontrol\nThis will allow the command systemctl start/stop codesyscontrol to be run with sudo without having to input a password. The user must be in the sudo group.\nIf your system does not have systemctl:\necho \"$USER ALL=(ALL:ALL) NOPASSWD: /usr/sbin/service codesyscontrol *\" | sudo EDITOR=\"tee\" visudo -f /etc/sudoers.d/allow_restart_codesyscontrol\nThis will allow the command service codesyscontrol start/stop to be run with sudo without having to input a password. The user must be in the sudo group.\nroslaunch robin_bridge_generated run.launch\nIf you prefer not to give those permissions run the node manually:\nrosrun robin_bridge_generated robin_node_generated\nExamples\nLicense\nSupported by ROSIN - ROS-Industrial Quality-Assured Robot Software Components.\nMore information: rosin-project.eu\nThis project has received funding from the European Union\u2019s Horizon 2020\nresearch and innovation programme under grant agreement no. 732287.", "link": "https://github.com/ScalABLE40/robin", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "robin\na ros-codesys shared memory bridge to map codesys variables to ros topics.\nthis bridge is the result of the robin project, a focused technical project (ftp) of the rosin european project.\ngetting started\nthe bridge is made up of two components:\na ros package that doesn't require any manual configuration other than the installation of its dependencies. the package contains a ros node that reads/writes data from/to shared memory spaces and publishes/receives messages to/from ros topics.\na codesys library to be used in a codesys project created by the user. an example project is provided in robin_updater/src/robin_updater/cfg/codesys_project.xml. the library contains a robin function block that reads/writes data from/to shared memory spaces and writes/reads it to codesys user-defined variables.\nthe following iec 61131-3 data types are currently supported:\nbool\nbyte\nsint, int, dint, lint, usint, uint, udint, ulint\nreal, lreal\nchar, string\nas well as arrays and custom structs. the following standard ros message packages are already defined as codesys structs and available on the robin codesys library:\nstd_msgs\ngeometry_msgs\nthese variables have to be defined on both the codesys project and the ros package. for arrays or for structs with string or array members, because these data types are handled as non-pod (plain old data) objects in c++, the mapping between the c++ variables and the ros messages has to be explicitly defined. however, an updater application was developed to automate most of this process. the user simply needs to define its desired variables on the codesys project and run the updater.\nprerequisites\nubuntu 18.04/16.04 system (may work on other distros as well) with:\nssh server\nros melodic/kinetic\ncodesys control softplc application:\ndebian/ubuntu\nraspberry pi\nbeaglebone\nwindows system with:\ncodesys development system v3 (developed and tested with version 3.5.15.0)\nwindows openssh\ninstallation\ninstall codesys library:\nopen codesys development system v3\ngo to tools->library repository->install\nfind and select robin_bridge/src/robin.library\nclose the library repository dialog\ncreate catkin workspace (if non-existent):\nmkdir -p ~/catkin_ws/src\ncd ~/catkin_ws\ncatkin_make # or 'catkin build'\nsource ~/catkin_ws/devel/setup.bash\nclone repository into catkin workspace (eg. ~/catkin_ws):\ncd ~/catkin_ws/src\ngit clone https://github.com/scalable40/robin\ninstall updater package dependencies:\nrosdep install robin_updater\ncompile bridge package:\ncd ~/catkin_ws\ncatkin_make robin # or 'catkin build robin'\nusage\ncreate codesys project. you can either:\ncreate your own project and add the robin library to it.\nin the devices -----> tree !!! , double click library manager and open the add library dialog\nfind and select the previously installed robin library and click ok\nyou can now use the robin function block as shown in the examples section\ncreate a new empty project and import the example project from codesys_project.xml.\ngo to project->import plcopenxml...\nfind and select the xml file\nselect all items and click ok\nvariable length arrays are only partially supported in codesys. to make the updater interpret a regular fixed length array as a ros variable length array, preceed its declaration with the line: {attribute 'robin_var_len'}.\nmake sure you can establish connection with the plc. go to the devices tree, double click the device and then:\nscan network... for your plc device.\nor add it manually device->options->manage favourite devices...\ngo to windows search bar->services and make sure windows openssh authentication agent service is running (startup type: automatic).\nrun the updater application:\ngo to tools->scripting->execute script file...\nopen the script file robin_updater/src/robin_updater/src/robin_updater/start_update.py\nif you don't have access to it from codesys, first copy it to your windows system\ninput the requested information (target address and password) and follow the script's execution\nnote: password will be asked again during the script\nlaunch the robin ros node. will restart codesyscontrol service and then launch the node:\nto avoid having to manually restart codesyscontrol after each update run:\necho \"$user all=(all:all) nopasswd: /bin/systemctl * codesyscontrol\" | sudo editor=\"tee\" visudo -f /etc/sudoers.d/allow_restart_codesyscontrol\nthis will allow the command systemctl start/stop codesyscontrol to be run with sudo without having to input a password. the user must be in the sudo group.\nif your system does not have systemctl:\necho \"$user all=(all:all) nopasswd: /usr/sbin/service codesyscontrol *\" | sudo editor=\"tee\" visudo -f /etc/sudoers.d/allow_restart_codesyscontrol\nthis will allow the command service codesyscontrol start/stop to be run with sudo without having to input a password. the user must be in the sudo group.\nroslaunch robin_bridge_generated run.launch\nif you prefer not to give those permissions run the node manually:\nrosrun robin_bridge_generated robin_node_generated\nexamples\nlicense\nsupported by rosin - ros-industrial quality-assured robot software components.\nmore information: rosin-project.eu\nthis project has received funding from the european union\u2019s horizon 2020\nresearch and innovation programme under grant agreement no. 732287.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000731, "year": null}, {"Unnamed: 0": 1758, "autor": 738, "date": null, "content": "SPART\nSPART is an open-source modeling and control toolkit for mobile-base robotic multibody systems with kinematic tree topologies (i.e., open-loop multi-branched systems). SPART is MATLAB-based and ROS-compatible, allowing to prototype in simulation and deploy to hardware controllers for robotic systems.\nGiven a URDF description of a multibody system, SPART computes the system's:\nKinematics -- pose of the links and joints (i.e., rotation matrices and position vectors).\nDifferential kinematics -- operational space velocities and accelerations, as well as the geometric Jacobians and their time derivatives.\nDynamics -- generalized inertia and convective inertia matrices.\nForward/Inverse dynamics -- solves both problems, including the floating-base case.\nSPART supports symbolic computation and analytic expressions for all kinematic and dynamic quantities can be obtained.\nInstallation\nJust clone or download the toolkit and run the SPART2path.m script. This will add all the SPART MATLAB functions and the Simulink library to the path and save it. Then you can use it as any other MATLAB toolbox.\nTo run an example go to Examples/URDF_Tutorial and run:\nURDF_Tutorial\nDocumentation\nUp to date documentation and tutorials are available at spart.readthedocs.org.\nExamples\nHere is an example of a planar Desired-Reaction-Maneuver, where the kinematic redundancy of a manipulator is exploited to make the floating base point towards the end-effector, while this one is moving along a prescribed path.\nSPART can also be used to control real manipulators. Here is an example of resolved motion-rate control of the R5D3 manipulator (the tip of the end-effector is describing a triangle in space).\nLicense\nThis software is released under the LGPLv3 license.", "link": "https://github.com/NPS-SRL/SPART", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "spart\nspart is an open-source modeling and control toolkit for mobile-base robotic multibody systems with kinematic -----> tree !!!  topologies (i.e., open-loop multi-branched systems). spart is matlab-based and ros-compatible, allowing to prototype in simulation and deploy to hardware controllers for robotic systems.\ngiven a urdf description of a multibody system, spart computes the system's:\nkinematics -- pose of the links and joints (i.e., rotation matrices and position vectors).\ndifferential kinematics -- operational space velocities and accelerations, as well as the geometric jacobians and their time derivatives.\ndynamics -- generalized inertia and convective inertia matrices.\nforward/inverse dynamics -- solves both problems, including the floating-base case.\nspart supports symbolic computation and analytic expressions for all kinematic and dynamic quantities can be obtained.\ninstallation\njust clone or download the toolkit and run the spart2path.m script. this will add all the spart matlab functions and the simulink library to the path and save it. then you can use it as any other matlab toolbox.\nto run an example go to examples/urdf_tutorial and run:\nurdf_tutorial\ndocumentation\nup to date documentation and tutorials are available at spart.readthedocs.org.\nexamples\nhere is an example of a planar desired-reaction-maneuver, where the kinematic redundancy of a manipulator is exploited to make the floating base point towards the end-effector, while this one is moving along a prescribed path.\nspart can also be used to control real manipulators. here is an example of resolved motion-rate control of the r5d3 manipulator (the tip of the end-effector is describing a triangle in space).\nlicense\nthis software is released under the lgplv3 license.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000738, "year": null}, {"Unnamed: 0": 1799, "autor": 779, "date": null, "content": "rrt\nRRT (Rapidly-exploring Random Tree) library in Rust\nUsing this crate\nIn your Cargo.tml, add below.\n[dependencies]\nrrt = \"0.5\"\nExamples\nThere are an example to solve collision avoid problem.\ncargo run --release --example collision_avoid\nBelow is the simplest example. It search the path from [-1.2, 0.0] to [1.2, 0.0] avoiding [-1, -1] - [1, 1] region. There are only one function dual_rrt_connect, which takes start, goal, is free function, random generation function, unit length of extend, max repeat num.\nuse rand::distributions::{Distribution, Uniform};\nlet result = rrt::dual_rrt_connect(\n&[-1.2, 0.0],\n&[1.2, 0.0],\n|p: &[f64]| !(p[0].abs() < 1.0 && p[1].abs() < 1.0),\n|| {\nlet between = Uniform::new(-2.0, 2.0);\nlet mut rng = rand::thread_rng();\nvec![between.sample(&mut rng), between.sample(&mut rng)]\n},\n0.2,\n1000,\n)\n.unwrap();\nprintln!(\"{:?}\", result);\nassert!(result.len() >= 4);", "link": "https://github.com/openrr/rrt", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "rrt\nrrt (rapidly-exploring random -----> tree !!! ) library in rust\nusing this crate\nin your cargo.tml, add below.\n[dependencies]\nrrt = \"0.5\"\nexamples\nthere are an example to solve collision avoid problem.\ncargo run --release --example collision_avoid\nbelow is the simplest example. it search the path from [-1.2, 0.0] to [1.2, 0.0] avoiding [-1, -1] - [1, 1] region. there are only one function dual_rrt_connect, which takes start, goal, is free function, random generation function, unit length of extend, max repeat num.\nuse rand::distributions::{distribution, uniform};\nlet result = rrt::dual_rrt_connect(\n&[-1.2, 0.0],\n&[1.2, 0.0],\n|p: &[f64]| !(p[0].abs() < 1.0 && p[1].abs() < 1.0),\n|| {\nlet between = uniform::new(-2.0, 2.0);\nlet mut rng = rand::thread_rng();\nvec![between.sample(&mut rng), between.sample(&mut rng)]\n},\n0.2,\n1000,\n)\n.unwrap();\nprintln!(\"{:?}\", result);\nassert!(result.len() >= 4);", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000779, "year": null}, {"Unnamed: 0": 1801, "autor": 781, "date": null, "content": "airsim_ros_client\nOverview\nThis repository is meant to integrate ROS and AirSim plugin using the python APIs available for the simulator.\nThe airsim_ros_client package has been tested under ROS Kinetic and Ubuntu 16.04LTS. The source code is released under MIT License.\nThe mesh of DJI M-100 was downloaded from DJI website (link). DJI reserves all rights to this material.\nInstallation\nDependencies\nInstall the python depenencies:\n# AirSim APIs\npip install airsim\nBuilding\nTo build from source, clone the latest version from this repository into your catkin workspace\nmkdir -p ~/catkin_ws/src\ncd ~/catkin_ws/src\ngit clone https://github.com/Mayankm96/airsim_ros_client.git\nTo compile the package:\ncd ~/catkin_ws\ncatkin_make\nsource devel/setup.bash\nRunning AirSim in Unreal Engine\nBefore running the nodes in the package, you need to run Airsim plugin in the Unreal Engine. In case you are unfamiliar on how to do so, refer to the tutorials available here.\nA sample settings.json file used to run the simulator with the ROS package is available here. Copy it to the ~/Documents/AirSim directory to use the package without any further modifications.\nUsage\nRunning the tf publisher of drone model (DJI M100)\nTo use the urdf model of the drone used in AirSim simulator, then run:\nroslaunch airsim_ros_client publish_tf.launch\nNOTE: In the modified blueprint of the drone for UE4, all cameras are downward-facing.\nRunning image publisher\nChange the IP and Port configurations in pubImages.launch to match the settings in which Airsim is running. Then:\nroslaunch airsim_ros_client pubImages.launch\nNodes\nairsim_img_publisher\nThis is a client node at (img_publisher.py) interfaces with the AirSim plugin to retrieve the drone's pose and camera images (rgb, depth).\nPublished Topics\n/airsim/rgb/image_raw (sensor_msgs/Image)\nThe rgb camera images in rgba8 encoding.\n/airsim/depth (sensor_msgs/Image)\nThe depth camera images in 32FC1 encoding.\n/airsim/camera_info (sensor_msgs/CameraInfo)\nThe rgb camera paramters.\n/airsim/depth/camera_info (sensor_msgs/CameraInfo)\nThe depth camera paramters.\n/airsim/pose (geometry_msgs/PoseStamped)\nThe position/orientation of the quadcoper (base_link)\n/odom (nav_msgs/Odometry)\nThe odometry of the quadcoper (base_link) in the world frame\n/tf\ntf tree with the origin (world), the position/orientation of the quadcoper (base_link)\nParameters\nCamera parameters: Fx, Fy, cx, cz, width, height\nPublishing frequency: loop_rate\nairsim_follow_trajectory\nThis is a client node at (follow_trajectory.py) interfaces with the AirSim plugin to follow a trajectory.\nSubscribed Topics\n/trajectory/spline_marker_array (visualization_msgs/MarkerArray)\nAn array of waypoints to follow.\nParameters\nVelocity: velocity", "link": "https://github.com/Mayankm96/airsim_ros_client", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "airsim_ros_client\noverview\nthis repository is meant to integrate ros and airsim plugin using the python apis available for the simulator.\nthe airsim_ros_client package has been tested under ros kinetic and ubuntu 16.04lts. the source code is released under mit license.\nthe mesh of dji m-100 was downloaded from dji website (link). dji reserves all rights to this material.\ninstallation\ndependencies\ninstall the python depenencies:\n# airsim apis\npip install airsim\nbuilding\nto build from source, clone the latest version from this repository into your catkin workspace\nmkdir -p ~/catkin_ws/src\ncd ~/catkin_ws/src\ngit clone https://github.com/mayankm96/airsim_ros_client.git\nto compile the package:\ncd ~/catkin_ws\ncatkin_make\nsource devel/setup.bash\nrunning airsim in unreal engine\nbefore running the nodes in the package, you need to run airsim plugin in the unreal engine. in case you are unfamiliar on how to do so, refer to the tutorials available here.\na sample settings.json file used to run the simulator with the ros package is available here. copy it to the ~/documents/airsim directory to use the package without any further modifications.\nusage\nrunning the tf publisher of drone model (dji m100)\nto use the urdf model of the drone used in airsim simulator, then run:\nroslaunch airsim_ros_client publish_tf.launch\nnote: in the modified blueprint of the drone for ue4, all cameras are downward-facing.\nrunning image publisher\nchange the ip and port configurations in pubimages.launch to match the settings in which airsim is running. then:\nroslaunch airsim_ros_client pubimages.launch\nnodes\nairsim_img_publisher\nthis is a client node at (img_publisher.py) interfaces with the airsim plugin to retrieve the drone's pose and camera images (rgb, depth).\npublished topics\n/airsim/rgb/image_raw (sensor_msgs/image)\nthe rgb camera images in rgba8 encoding.\n/airsim/depth (sensor_msgs/image)\nthe depth camera images in 32fc1 encoding.\n/airsim/camera_info (sensor_msgs/camerainfo)\nthe rgb camera paramters.\n/airsim/depth/camera_info (sensor_msgs/camerainfo)\nthe depth camera paramters.\n/airsim/pose (geometry_msgs/posestamped)\nthe position/orientation of the quadcoper (base_link)\n/odom (nav_msgs/odometry)\nthe odometry of the quadcoper (base_link) in the world frame\n/tf\ntf -----> tree !!!  with the origin (world), the position/orientation of the quadcoper (base_link)\nparameters\ncamera parameters: fx, fy, cx, cz, width, height\npublishing frequency: loop_rate\nairsim_follow_trajectory\nthis is a client node at (follow_trajectory.py) interfaces with the airsim plugin to follow a trajectory.\nsubscribed topics\n/trajectory/spline_marker_array (visualization_msgs/markerarray)\nan array of waypoints to follow.\nparameters\nvelocity: velocity", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000781, "year": null}, {"Unnamed: 0": 1834, "autor": 814, "date": null, "content": "Trajectory planner\nThis planner can calculate movement trajectory from start to goal positions on a ros::OccupancyGrid using A* or BFS algorithms for a robot primitive that can perform a set of simple movements.\nFeatures\n2D planning\nPlanning works on a 2D occupancy grid (ros::OccupancyGrid), which is just a 2D array, where each cell represents a small area and can be in one of three states: occupied, free, unknown. It is a good map data structure for small wheel platforms and simple walking robots. You can get it from Rtabmap, hector_mapping or gmapping SLAM algorithms.\nrospy based\nThis package is based on ROS and built using Python 2, so you don't need to compile it. To run the package, move it into catkin workspace and run planning on predefined map: roslaunch trajectory_planner_py static_planning.launch\nDescription\nHow planner works\nThe planner is searching in the state space, where State is a vector of position and orientation of a robot. The Robot is a rectangular primitive with width and height parameters and a set of simple moves, which are described as vectors (length, dtheta) and represent rotation and moving forward/backward. New states are derived from the previous ones by applying move transformations.\nWe also check intermediate sub states on collisions with obstacles by simulating moves with a small step.\nTo start planning we need a map, start and goal positions, robot parameters and a set of available moves. Then we can get a tree of states by applying moves to the start state and repeating this for its \"children\". We use A* or BFS to search for a goal state in that tree. I got intuition for A* implementation from this nice publication.\nROS API\nSubscribed topics\ninitialpose (geometry_msgs/PoseWithCovarianceStamped)\nStart pose for planner. You can set initialpose from RVIZ\ngoal (geometry_msgs/PoseStamped)\nGoal pose for planner\nPublished topics\ntrajectory (visualization_msgs/MarkerArray)\nArray of robot's poses\nDisclaimer\nThis package is in development now and may contain bugs.", "link": "https://github.com/LetsPlayNow/TrajectoryPlanner", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "trajectory planner\nthis planner can calculate movement trajectory from start to goal positions on a ros::occupancygrid using a* or bfs algorithms for a robot primitive that can perform a set of simple movements.\nfeatures\n2d planning\nplanning works on a 2d occupancy grid (ros::occupancygrid), which is just a 2d array, where each cell represents a small area and can be in one of three states: occupied, free, unknown. it is a good map data structure for small wheel platforms and simple walking robots. you can get it from rtabmap, hector_mapping or gmapping slam algorithms.\nrospy based\nthis package is based on ros and built using python 2, so you don't need to compile it. to run the package, move it into catkin workspace and run planning on predefined map: roslaunch trajectory_planner_py static_planning.launch\ndescription\nhow planner works\nthe planner is searching in the state space, where state is a vector of position and orientation of a robot. the robot is a rectangular primitive with width and height parameters and a set of simple moves, which are described as vectors (length, dtheta) and represent rotation and moving forward/backward. new states are derived from the previous ones by applying move transformations.\nwe also check intermediate sub states on collisions with obstacles by simulating moves with a small step.\nto start planning we need a map, start and goal positions, robot parameters and a set of available moves. then we can get a -----> tree !!!  of states by applying moves to the start state and repeating this for its \"children\". we use a* or bfs to search for a goal state in that tree. i got intuition for a* implementation from this nice publication.\nros api\nsubscribed topics\ninitialpose (geometry_msgs/posewithcovariancestamped)\nstart pose for planner. you can set initialpose from rviz\ngoal (geometry_msgs/posestamped)\ngoal pose for planner\npublished topics\ntrajectory (visualization_msgs/markerarray)\narray of robot's poses\ndisclaimer\nthis package is in development now and may contain bugs.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000814, "year": null}, {"Unnamed: 0": 1867, "autor": 847, "date": null, "content": "GenNav\nStatus: Under Development\nA python package for robot navigation algorithms.\nInstallation\nThe package is currently under development so we suggest installing from source.\nFrom Source (Recommended)\ngit clone https://github.com/ERC-BPGC/gennav.git\ncd gennav\npython -m pip install .\nUsing pip\npip install gennav\nUsage\nTo plan a path using the Rapidly-exploring random tree algorithm in a polygon based environment representation.\nfrom gennav.planners import RRT\nfrom gennav.envs import PolygonEnv\nfrom gennav.utils import RobotState\nfrom gennav.utils.geometry import Point\nfrom gennav.utils.samplers import UniformRectSampler\nobstacles = [] # obstacles can be added here\nenv = PolygonEnv() # polygon environment to keep track of obstacles\nenv.update(obstacles) # updating environment with obstacles\nsampler = UniformRectSampler(-5, 15, -5, 15) # for sampling random states\nplanner = RRT(sampler=sampler, expand_dis=0.1) # creating the planner\nstart = RobotState(position=Point(1, 1)) # starting state\ngoal = RobotState(position=Point(10, 10)) # goal state\npath, info_dict = planner.plan(start, goal, env) # planning path through obstacles\nNote that the environment have been left blank empty here, they should be updated as per use case.\nFor more details you can refer to our documentation.\nROS Integration\nIf you wish to use gennav in a ROS based stack, check out gennav_ros.\nContributions\nContributions are always welcome. We reccomend you check out contribution guidelines and view the docs beforehand.", "link": "https://github.com/ERC-BPGC/gennav", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "gennav\nstatus: under development\na python package for robot navigation algorithms.\ninstallation\nthe package is currently under development so we suggest installing from source.\nfrom source (recommended)\ngit clone https://github.com/erc-bpgc/gennav.git\ncd gennav\npython -m pip install .\nusing pip\npip install gennav\nusage\nto plan a path using the rapidly-exploring random -----> tree !!!  algorithm in a polygon based environment representation.\nfrom gennav.planners import rrt\nfrom gennav.envs import polygonenv\nfrom gennav.utils import robotstate\nfrom gennav.utils.geometry import point\nfrom gennav.utils.samplers import uniformrectsampler\nobstacles = [] # obstacles can be added here\nenv = polygonenv() # polygon environment to keep track of obstacles\nenv.update(obstacles) # updating environment with obstacles\nsampler = uniformrectsampler(-5, 15, -5, 15) # for sampling random states\nplanner = rrt(sampler=sampler, expand_dis=0.1) # creating the planner\nstart = robotstate(position=point(1, 1)) # starting state\ngoal = robotstate(position=point(10, 10)) # goal state\npath, info_dict = planner.plan(start, goal, env) # planning path through obstacles\nnote that the environment have been left blank empty here, they should be updated as per use case.\nfor more details you can refer to our documentation.\nros integration\nif you wish to use gennav in a ros based stack, check out gennav_ros.\ncontributions\ncontributions are always welcome. we reccomend you check out contribution guidelines and view the docs beforehand.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000847, "year": null}, {"Unnamed: 0": 1894, "autor": 874, "date": null, "content": "CLF_reactive_planning_system\nOverview\nThis package provides a CLF-based reactive planning system, described in the paper: Efficient Anytime CLF Reactive Planning System for a Bipedal Robot on Undulating Terrain (PDF)(arXiv). The reactive planning system consists of a 5-Hz planning thread to guide a robot to a distant goal and a 300-Hz Control-Lyapunov-Function-based (CLF-based) reactive thread to cope with robot deviations. The planning system allowed Cassie Blue to autonomously traverse sinusoidally varying terrain. More experiments are still being conducted and this repo and the paper will be updated accordingly.\nAuthor: Jiunn-Kai (Bruce) Huang and Jessy W. Grizzle\nMaintainer: Bruce JK Huang, brucejkh[at]gmail.com\nAffiliation: The Biped Lab, the University of Michigan\nThis package has been tested under [ROS] Melodic and Ubuntu 18.04.\n[Note] More detailed introduction will be updated shortly. Sorry for the inconvenient!\n[Issues] If you encounter any issues, I would be happy to help. If you cannot find a related one in the existing issues, please open a new one. I will try my best to help!\nAbstract\nWe propose and experimentally demonstrate a reactive planning system for bipedal robots on unexplored, challenging terrains. The system consists of a low-frequency planning thread (5 Hz) to find an asymptotically optimal path and a high-frequency reactive thread (300 Hz) to accommodate robot deviation. The planning thread includes: a multi-layer local map to compute traversability for the robot on the terrain; an anytime omnidirectional Control Lyapunov Function (CLF) for use with a Rapidly Exploring Random Tree Star (RRT*) that generates a vector field for specifying motion between nodes; a sub-goal finder when the final goal is outside of the current map; and a finite-state machine to handle high-level mission decisions. The system also includes a reactive thread to obviate the non-smooth motions that arise with traditional RRT* algorithms when performing path following. The reactive thread copes with robot deviation while eliminating non-smooth motions via a vector field (defined by a closed-loop feedback policy) that provides real-time control commands to the robot's gait controller as a function of instantaneous robot pose. The system is evaluated on various challenging outdoor terrains and cluttered indoor scenes in both simulation and experiment on Cassie Blue, a bipedal robot with 20 degrees of freedom. All implementations are coded in C++ with the Robot Operating System (ROS) and are available at https://github.com/UMich-BipedLab/CLF_reactive_planning_system.\nVideo\nPlease checkout the introduction video. It highlights some important keypoints in the paper!\nQuick View\nCassie Blue autonomously traverses the Wave Field via the proposed reactive planning system. More Cassie experiments are still being conducted. More images will be updated!\nWhat does This Reactive Planning System Contain?\nThis reactive planning system consists of a low-frequency (5 Hz) planning thread and a high-frequency (300 Hz) reactive thread (see here). The planning thread involves a multi-layer local map to compute traversability, a sub-goal finder, and an omnidirectional Control Lyapunov Function RRT*. Instead of a common waypoint-following or path-tracking strategy, the reactive thread copes with robot deviation while eliminating non-smooth motions via a vector field (defined by a closed-loop feedback policy) that provides real-time control commands to the robot's gait controller as a function of instantaneous robot pose.\nWhat is Reactive and Why\nA reactive planning replaces the concept of trajectory with a vector field arising as the gradient of a potential function. Therefore, instead of a common waypoint-following or path-tracking strategy, the reactive thread copes with robot deviation while eliminating non-smooth motions via a vector field (defined by a closed-loop feedback policy) that provides real-time control commands to the robot's gait controller as a function of instantaneous robot pose. In other words, the reactive planner automatically adjusts the control commands to handle any deviations the robot may incur!\nOverall Pipeline for Autonomy\nIllustration of how the various processes in the overall autonomy system are distributed and their computation frequencies. The larger boxes indicate various modules such as Data Acquisition, Planning, and Control. The smaller boxes are colored according to the processor that runs them.\nResults\nBoth simulation and experimental results are provided. All the followings results are shown as GIFs, which are generated by extracting 300 frames from the original videos and then are played back by 10 FPS. The real videos with 1x speed can be found at here.\nExperimental Results\nMore experiments are still being conducted! More visual will be added soon.\nCassie Blue on Undulating Terrain\nThe experimental terrain is the Wave Field is located on the North Campus of the University of Michigan. The Wave Field consists of sinusoidal humps with a depth of approximately 1.5 m from the bottom of the valleys to the crest of the humps; there is a second sinusoidal pattern running orthogonal to the main pattern, which adds 25 cm ripples peak-to-peak even in the valleys.\nWave Field on the North Campus of the University of Michigan\nSimulation Results of an ALIP Model Robot on the Wave Field\nThe simulated robot is based on the ALIP model and accepts piece-wise constant inputs at the beginning of each step. The highlighted areas show the local maps being provided to the robot.\nSimulation Results of an ALIP Model Robot in Synthetic, Noise, and Patchy Maps\nThe simulated robot is based on the ALIP model and accepts piece-wise constant inputs at the beginning of each step. The highlighted areas show the local maps being provided to the robot.\n-- cluttered indoor scenes with obstacles and holes\n-- noisy undulating outdoor terrains\n-- high-level missions\nSimulation Results of a Whole-body Cassie Simulator in a Perfect Map\nTo ensure the control commands from the reactive planning system are feasible for Cassie-series bipedal robots, we sent the commands via User Datagram Protocol (UDP) from ROS C++ to Matlab-Simmechanics, which simulates a 20 DoF of Cassie, using footfalls on the specified terrain. The simulator then sent back the pose of the simulated Cassie robot to the planning system to plan for the optimal path via UDP. The planner system successfully took the simulated Cassie to the goal without falling!\nRequired Libraries / Packages\nPlease install ROS Melodic.\nPlease install this package.\nPlease install planner_msgs.\nPlease install grid_map.\nThe structure should be catkin\n\u2514\u2500\u2500 src\n\u251c\u2500\u2500 CLF_reactive_planning_system (This package)\n\u251c\u2500\u2500 planner_msgs\n\u2514\u2500\u2500 customized_grid_map\nDatasets\nAll the datasets (bagfiles) are available on Google Drive: please download them here\nRunning\nUsing provided bagfiles:\nPlease download at least one bagfile from here.\ncatkin_make the package in the catkin folder.\nsource devel/setup.bash\nroslaunch cassie_planning fake_robot.launch\nroslaunch cassie_planning wavefield.launch\nrosbag play -l -q bagfile.bag\nTo see the results, rosrun rviz rviz. You can directly open wavefield.rviz under the CLF_reactive_planning_system/rviz/ folder.\nOnboard processing:\ncatkin_make the package in the catkin folder.\nsource devel/setup.bash\nroslaunch cassie_planning wavefield.launch\nensure you have robot pose and map information published\nTo see the results, rosrun rviz rviz. You can directly open wavefield.rviz under the CLF_reactive_planning_system/rviz/ folder.\nNote: More detailed explanation about how to use configuration files (*.yaml) with launch files will come soon.\nCitations\nThe detail is described in: Efficient Anytime CLF Reactive Planning System for a Bipedal Robot on Undulating Terrain, Jiunn-Kai Huang and Jessy W. Grizzle. (PDF) (arXiv)\n@article{huang2021efficient,\ntitle={{Efficient Anytime CLF Reactive Planning System for a Bipedal Robot on Undulating Terrain},\nauthor={Jiunn-Kai Huang and Jessy W. Grizzle}},\nyear={2021},\njournal={arXiv preprint arXiv:2108.06699},\nprimaryClass={cs.RO}\n}", "link": "https://github.com/UMich-BipedLab/CLF_reactive_planning_system", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "clf_reactive_planning_system\noverview\nthis package provides a clf-based reactive planning system, described in the paper: efficient anytime clf reactive planning system for a bipedal robot on undulating terrain (pdf)(arxiv). the reactive planning system consists of a 5-hz planning thread to guide a robot to a distant goal and a 300-hz control-lyapunov-function-based (clf-based) reactive thread to cope with robot deviations. the planning system allowed cassie blue to autonomously traverse sinusoidally varying terrain. more experiments are still being conducted and this repo and the paper will be updated accordingly.\nauthor: jiunn-kai (bruce) huang and jessy w. grizzle\nmaintainer: bruce jk huang, brucejkh[at]gmail.com\naffiliation: the biped lab, the university of michigan\nthis package has been tested under [ros] melodic and ubuntu 18.04.\n[note] more detailed introduction will be updated shortly. sorry for the inconvenient!\n[issues] if you encounter any issues, i would be happy to help. if you cannot find a related one in the existing issues, please open a new one. i will try my best to help!\nabstract\nwe propose and experimentally demonstrate a reactive planning system for bipedal robots on unexplored, challenging terrains. the system consists of a low-frequency planning thread (5 hz) to find an asymptotically optimal path and a high-frequency reactive thread (300 hz) to accommodate robot deviation. the planning thread includes: a multi-layer local map to compute traversability for the robot on the terrain; an anytime omnidirectional control lyapunov function (clf) for use with a rapidly exploring random -----> tree !!!  star (rrt*) that generates a vector field for specifying motion between nodes; a sub-goal finder when the final goal is outside of the current map; and a finite-state machine to handle high-level mission decisions. the system also includes a reactive thread to obviate the non-smooth motions that arise with traditional rrt* algorithms when performing path following. the reactive thread copes with robot deviation while eliminating non-smooth motions via a vector field (defined by a closed-loop feedback policy) that provides real-time control commands to the robot's gait controller as a function of instantaneous robot pose. the system is evaluated on various challenging outdoor terrains and cluttered indoor scenes in both simulation and experiment on cassie blue, a bipedal robot with 20 degrees of freedom. all implementations are coded in c++ with the robot operating system (ros) and are available at https://github.com/umich-bipedlab/clf_reactive_planning_system.\nvideo\nplease checkout the introduction video. it highlights some important keypoints in the paper!\nquick view\ncassie blue autonomously traverses the wave field via the proposed reactive planning system. more cassie experiments are still being conducted. more images will be updated!\nwhat does this reactive planning system contain?\nthis reactive planning system consists of a low-frequency (5 hz) planning thread and a high-frequency (300 hz) reactive thread (see here). the planning thread involves a multi-layer local map to compute traversability, a sub-goal finder, and an omnidirectional control lyapunov function rrt*. instead of a common waypoint-following or path-tracking strategy, the reactive thread copes with robot deviation while eliminating non-smooth motions via a vector field (defined by a closed-loop feedback policy) that provides real-time control commands to the robot's gait controller as a function of instantaneous robot pose.\nwhat is reactive and why\na reactive planning replaces the concept of trajectory with a vector field arising as the gradient of a potential function. therefore, instead of a common waypoint-following or path-tracking strategy, the reactive thread copes with robot deviation while eliminating non-smooth motions via a vector field (defined by a closed-loop feedback policy) that provides real-time control commands to the robot's gait controller as a function of instantaneous robot pose. in other words, the reactive planner automatically adjusts the control commands to handle any deviations the robot may incur!\noverall pipeline for autonomy\nillustration of how the various processes in the overall autonomy system are distributed and their computation frequencies. the larger boxes indicate various modules such as data acquisition, planning, and control. the smaller boxes are colored according to the processor that runs them.\nresults\nboth simulation and experimental results are provided. all the followings results are shown as gifs, which are generated by extracting 300 frames from the original videos and then are played back by 10 fps. the real videos with 1x speed can be found at here.\nexperimental results\nmore experiments are still being conducted! more visual will be added soon.\ncassie blue on undulating terrain\nthe experimental terrain is the wave field is located on the north campus of the university of michigan. the wave field consists of sinusoidal humps with a depth of approximately 1.5 m from the bottom of the valleys to the crest of the humps; there is a second sinusoidal pattern running orthogonal to the main pattern, which adds 25 cm ripples peak-to-peak even in the valleys.\nwave field on the north campus of the university of michigan\nsimulation results of an alip model robot on the wave field\nthe simulated robot is based on the alip model and accepts piece-wise constant inputs at the beginning of each step. the highlighted areas show the local maps being provided to the robot.\nsimulation results of an alip model robot in synthetic, noise, and patchy maps\nthe simulated robot is based on the alip model and accepts piece-wise constant inputs at the beginning of each step. the highlighted areas show the local maps being provided to the robot.\n-- cluttered indoor scenes with obstacles and holes\n-- noisy undulating outdoor terrains\n-- high-level missions\nsimulation results of a whole-body cassie simulator in a perfect map\nto ensure the control commands from the reactive planning system are feasible for cassie-series bipedal robots, we sent the commands via user datagram protocol (udp) from ros c++ to matlab-simmechanics, which simulates a 20 dof of cassie, using footfalls on the specified terrain. the simulator then sent back the pose of the simulated cassie robot to the planning system to plan for the optimal path via udp. the planner system successfully took the simulated cassie to the goal without falling!\nrequired libraries / packages\nplease install ros melodic.\nplease install this package.\nplease install planner_msgs.\nplease install grid_map.\nthe structure should be catkin\n\u2514\u2500\u2500 src\n\u251c\u2500\u2500 clf_reactive_planning_system (this package)\n\u251c\u2500\u2500 planner_msgs\n\u2514\u2500\u2500 customized_grid_map\ndatasets\nall the datasets (bagfiles) are available on google drive: please download them here\nrunning\nusing provided bagfiles:\nplease download at least one bagfile from here.\ncatkin_make the package in the catkin folder.\nsource devel/setup.bash\nroslaunch cassie_planning fake_robot.launch\nroslaunch cassie_planning wavefield.launch\nrosbag play -l -q bagfile.bag\nto see the results, rosrun rviz rviz. you can directly open wavefield.rviz under the clf_reactive_planning_system/rviz/ folder.\nonboard processing:\ncatkin_make the package in the catkin folder.\nsource devel/setup.bash\nroslaunch cassie_planning wavefield.launch\nensure you have robot pose and map information published\nto see the results, rosrun rviz rviz. you can directly open wavefield.rviz under the clf_reactive_planning_system/rviz/ folder.\nnote: more detailed explanation about how to use configuration files (*.yaml) with launch files will come soon.\ncitations\nthe detail is described in: efficient anytime clf reactive planning system for a bipedal robot on undulating terrain, jiunn-kai huang and jessy w. grizzle. (pdf) (arxiv)\n@article{huang2021efficient,\ntitle={{efficient anytime clf reactive planning system for a bipedal robot on undulating terrain},\nauthor={jiunn-kai huang and jessy w. grizzle}},\nyear={2021},\njournal={arxiv preprint arxiv:2108.06699},\nprimaryclass={cs.ro}\n}", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000874, "year": null}, {"Unnamed: 0": 1916, "autor": 896, "date": null, "content": "Awesome ROS Tools\nThe list of tools and packages for Robot Operating System development!\nTable of Contents\nBest practices\nDevelopment tools\nDocumentation\nBag files\nVisualization\nCode testing\nSimulation\nHardware\nSensors\nWeb tools\nOther\nBest Practices\nros_best_practices - loose collection of best practices, conventions, and tricks for using the Robot Operating System (ROS). It builds up on the official ROS documentation and other resources and is meant as summary and overview\nROS Hacks Repo - the repository is designed to make ROS developer's life easier. After the installation usefull aliases and functions will be added to the terminal.\nDevelopment Tools\ncatkin_tools - command line tools for working with catkin github.\nROSCpp Code Format - the repo contains an auto formatting script for the ROS C++ Style Guidelines.\nswri_profiler - is a lightweight profiling framework for C++ ROS nodes. It allows you to selectively measure how much time is spent in various scopes. Profiling data is generated and published to a ROS topic where it can be recorded or monitored in real time. The profiler was designed to be lightweight enough that it can be left in during normal operation so that performance data can be monitored at any time.\npal_statistics - provides a way of gathering, aggregating, storing and visualizing statistics from arbitrary sources in a flexible and real-time safe way in ROS. From internal variables values to high level statistics about, but not limited to, robot performance pal_statistics Wiki.\nROS Logs & ELK Stack - demo of collecting ROS Logs (from ROS Containers) with Filebeat which are then sent to Logstash indexed by Elasticsearch and can then be viewed and visualized at Kibana. All logs are stored in the roslogs volume.\nros2-migration-tools - contains a set of tools for migrating a ROS1 package to a ROS2 package. The C++ source code migration uses libclang8 and its corresponding python bindings.\nROS Qt Creator Plug-in - plug-in for Qt Creator to work with ROS workspaces, analyse performance and debug code.\nDocumentation\nrosdoc_lite - ROS package wraps documentation tools like doxygen, sphinx, and epydoc, making it convenient to generate ROS package documentation. It also generates online documentation for the ROS wiki. github.\nrosautodoc - the rosautodoc project provides a python executable that can automatically generate documentation for ROS nodes that are running on the system.\nroslaunch_to_dot - convert a roslaunch XML file into a dot file containing a graph of the launch tree.\nBag files\nrosbag_fancy - fancy terminal UI frontend for the venerable rosbag tool\nrosbag-sliding-windows-annotator - Providing a way to annotate rosbag files by using the method of sliding windows (use a video image topic as a guide for tagging numerical data)\nrosbag_editor - GUI-app to remove one or more topics from a rosbag, change the duration of the rosbag, rename a topic, change the compression type etc.\nbag-database - web-based application that monitors a directory for ROS bag files, parses their metadata, and provides a friendly web interface for searching for bags and downloading them. Its goal is to make it easy to catalog thousands of bag files, search through them for relevant data such as topic names and message types, view information about them, and download them.\nrbb_core - tool to index/visualize/manage rosbags on remote storage systems. Additionally it provides a web interface and framework for automated simulations.\nmarv-robotics - MARV Robotics is an extensible data management platform for robot logs. New robot logs are found by a scanner and configured nodes are run to extract, filter and process data from them. The robot logs are visualized in a web-based application that features a listing view with filters and summary, and detail views of individual log files.\nRosbagPandas - create Python pandas data frame from a ros bag file.\nrosbag_metadata - tool for collecting and writing metadata to ROS bagfiles or to accompanying yaml files.\nbag_tools - set of useful bag processing tools (make_video from topic, change_frame_id, change_camera_info, extract_stereo_images etc.). bag_tools Wiki.\nrosbag_compress - a python command line tool for compression or decompression of multiple ROS bag files. This tool searchs bag files recrusively,compress or compress them at same time. It is executed in parallel process, so the task is done fastly..\nVisualization\nrosshow - Displays various sensor messages in a useful fashion using Unicode Braille art in the terminal\nwebviz - Web-based application for playback and visualization of ROS bag files. This repository also contains some libraries that can be used independently to build web-based visualization tools\nmapviz - ROS based visualization tool with a plug-in system similar to RVIZ focused on visualizing 2D data\nrqt_multiplot_plugin - GUI rqt plugin for visualizing numeric values in multiple 2D plots using the Qwt plotting backend\nPlotJuggler - QT5 based application to display time series in plots, using an intuitive \"drag and drop\" interface It can be used either to load static data from file or connect to live streaming of data\nrviz_satellite - Plugin for rviz for displaying satellite maps loaded from the internet.\nCode Testing\nhypothesis-ros - data generators for Property Based Testing and Fuzzy Testing of ROS nodes (Unmantained!)\nros1_fuzzer - Fuzzer aims to help developers and researchers to find bugs and vulnerabilities in ROS nodes by performing fuzz tests over topics that the target nodes process\nroschaos - functionality for process reliability/fault recovery testing in ROS\ncode_coverage - ROS package to run coverage testing. Introduction.\nRos-Test-Example - a ROS Workspace containing an example car simulation to show GTest and Rostest.\nSimulation\nCARLA - an open-source simulator for autonomous driving research. github\nWorld Construction Tool - automatic tool for gazebo world construction: from a grayscale image to a 3d solid model.\nDataset of Gazebo Worlds Models and Maps - a set of Gazebo worlds models and maps.\nHardware\nOpenCR - Open Source Control Module for ROS. OpenCR is developed for ROS embedded systems to provide completely open-source hardware and software.\nSensors\nimu_tools - IMU-related filters and visualizers including Madgwick filter, Complementary filter and rviz IMU plugin for visualizing sensor_msgs::Imu messages.\nros_imu_covariance_calculator - ROS Package to estimate the variance of the inertial data from an IMU to be used to populate the error covariance matrix.\nkalibr - Kalibr is a toolbox that solves the following calibration problems:\nMultiple camera calibration: intrinsic and extrinsic calibration of a camera-systems with non-globally shared overlapping fields of view\nVisual-inertial calibration calibration (camera-IMU): spatial and temporal calibration of an IMU w.r.t a camera-system\nRolling Shutter Camera calibration: full intrinsic calibration (projection, distortion and shutter parameters) of rolling shutter cameras.\nWeb Tools\nRobot Web Tools - robot web tools is a collection of open-source modules and tools for building web-based robot apps.\nweb_video_server - HTTP Streaming of ROS Image Topics in Multiple Formats.\nOther\nmultimaster_fkie - the ROS stack of fkie_multimaster offers a complete solution for using ROS with multicores. In addition, Node Manager with a daemon provide a GUI-based management environment that is very useful to manage ROS-launch configurations and control running nodes, also in a single-core system.\nrosmon - rosmon is a drop-in replacement for the standard roslaunch tool. Rather unlike roslaunch, rosmon is focused on (remote) process monitoring. rosmon Wiki.\nLicense", "link": "https://github.com/oleg-Shipitko/awesome-ros-tools", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "awesome ros tools\nthe list of tools and packages for robot operating system development!\ntable of contents\nbest practices\ndevelopment tools\ndocumentation\nbag files\nvisualization\ncode testing\nsimulation\nhardware\nsensors\nweb tools\nother\nbest practices\nros_best_practices - loose collection of best practices, conventions, and tricks for using the robot operating system (ros). it builds up on the official ros documentation and other resources and is meant as summary and overview\nros hacks repo - the repository is designed to make ros developer's life easier. after the installation usefull aliases and functions will be added to the terminal.\ndevelopment tools\ncatkin_tools - command line tools for working with catkin github.\nroscpp code format - the repo contains an auto formatting script for the ros c++ style guidelines.\nswri_profiler - is a lightweight profiling framework for c++ ros nodes. it allows you to selectively measure how much time is spent in various scopes. profiling data is generated and published to a ros topic where it can be recorded or monitored in real time. the profiler was designed to be lightweight enough that it can be left in during normal operation so that performance data can be monitored at any time.\npal_statistics - provides a way of gathering, aggregating, storing and visualizing statistics from arbitrary sources in a flexible and real-time safe way in ros. from internal variables values to high level statistics about, but not limited to, robot performance pal_statistics wiki.\nros logs & elk stack - demo of collecting ros logs (from ros containers) with filebeat which are then sent to logstash indexed by elasticsearch and can then be viewed and visualized at kibana. all logs are stored in the roslogs volume.\nros2-migration-tools - contains a set of tools for migrating a ros1 package to a ros2 package. the c++ source code migration uses libclang8 and its corresponding python bindings.\nros qt creator plug-in - plug-in for qt creator to work with ros workspaces, analyse performance and debug code.\ndocumentation\nrosdoc_lite - ros package wraps documentation tools like doxygen, sphinx, and epydoc, making it convenient to generate ros package documentation. it also generates online documentation for the ros wiki. github.\nrosautodoc - the rosautodoc project provides a python executable that can automatically generate documentation for ros nodes that are running on the system.\nroslaunch_to_dot - convert a roslaunch xml file into a dot file containing a graph of the launch -----> tree !!! .\nbag files\nrosbag_fancy - fancy terminal ui frontend for the venerable rosbag tool\nrosbag-sliding-windows-annotator - providing a way to annotate rosbag files by using the method of sliding windows (use a video image topic as a guide for tagging numerical data)\nrosbag_editor - gui-app to remove one or more topics from a rosbag, change the duration of the rosbag, rename a topic, change the compression type etc.\nbag-database - web-based application that monitors a directory for ros bag files, parses their metadata, and provides a friendly web interface for searching for bags and downloading them. its goal is to make it easy to catalog thousands of bag files, search through them for relevant data such as topic names and message types, view information about them, and download them.\nrbb_core - tool to index/visualize/manage rosbags on remote storage systems. additionally it provides a web interface and framework for automated simulations.\nmarv-robotics - marv robotics is an extensible data management platform for robot logs. new robot logs are found by a scanner and configured nodes are run to extract, filter and process data from them. the robot logs are visualized in a web-based application that features a listing view with filters and summary, and detail views of individual log files.\nrosbagpandas - create python pandas data frame from a ros bag file.\nrosbag_metadata - tool for collecting and writing metadata to ros bagfiles or to accompanying yaml files.\nbag_tools - set of useful bag processing tools (make_video from topic, change_frame_id, change_camera_info, extract_stereo_images etc.). bag_tools wiki.\nrosbag_compress - a python command line tool for compression or decompression of multiple ros bag files. this tool searchs bag files recrusively,compress or compress them at same time. it is executed in parallel process, so the task is done fastly..\nvisualization\nrosshow - displays various sensor messages in a useful fashion using unicode braille art in the terminal\nwebviz - web-based application for playback and visualization of ros bag files. this repository also contains some libraries that can be used independently to build web-based visualization tools\nmapviz - ros based visualization tool with a plug-in system similar to rviz focused on visualizing 2d data\nrqt_multiplot_plugin - gui rqt plugin for visualizing numeric values in multiple 2d plots using the qwt plotting backend\nplotjuggler - qt5 based application to display time series in plots, using an intuitive \"drag and drop\" interface it can be used either to load static data from file or connect to live streaming of data\nrviz_satellite - plugin for rviz for displaying satellite maps loaded from the internet.\ncode testing\nhypothesis-ros - data generators for property based testing and fuzzy testing of ros nodes (unmantained!)\nros1_fuzzer - fuzzer aims to help developers and researchers to find bugs and vulnerabilities in ros nodes by performing fuzz tests over topics that the target nodes process\nroschaos - functionality for process reliability/fault recovery testing in ros\ncode_coverage - ros package to run coverage testing. introduction.\nros-test-example - a ros workspace containing an example car simulation to show gtest and rostest.\nsimulation\ncarla - an open-source simulator for autonomous driving research. github\nworld construction tool - automatic tool for gazebo world construction: from a grayscale image to a 3d solid model.\ndataset of gazebo worlds models and maps - a set of gazebo worlds models and maps.\nhardware\nopencr - open source control module for ros. opencr is developed for ros embedded systems to provide completely open-source hardware and software.\nsensors\nimu_tools - imu-related filters and visualizers including madgwick filter, complementary filter and rviz imu plugin for visualizing sensor_msgs::imu messages.\nros_imu_covariance_calculator - ros package to estimate the variance of the inertial data from an imu to be used to populate the error covariance matrix.\nkalibr - kalibr is a toolbox that solves the following calibration problems:\nmultiple camera calibration: intrinsic and extrinsic calibration of a camera-systems with non-globally shared overlapping fields of view\nvisual-inertial calibration calibration (camera-imu): spatial and temporal calibration of an imu w.r.t a camera-system\nrolling shutter camera calibration: full intrinsic calibration (projection, distortion and shutter parameters) of rolling shutter cameras.\nweb tools\nrobot web tools - robot web tools is a collection of open-source modules and tools for building web-based robot apps.\nweb_video_server - http streaming of ros image topics in multiple formats.\nother\nmultimaster_fkie - the ros stack of fkie_multimaster offers a complete solution for using ros with multicores. in addition, node manager with a daemon provide a gui-based management environment that is very useful to manage ros-launch configurations and control running nodes, also in a single-core system.\nrosmon - rosmon is a drop-in replacement for the standard roslaunch tool. rather unlike roslaunch, rosmon is focused on (remote) process monitoring. rosmon wiki.\nlicense", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000896, "year": null}, {"Unnamed: 0": 1934, "autor": 914, "date": null, "content": "Rapidly Exploring Random Tree (RRT) Planner\nROS package for a 2D path planner using the Rapidly Exploring Random Trees (RRT) algorithm. This repository contains two packages.\nroslaunch planner rrt_planner.launch\nInstallation\nClone this repository in a catkin workspace and run the following command to build the packages:\ncatkin build\nMapping\nThe mapping package provides a minimalistic GUI to draw obstacles in a 2D environment and choose starting position and goal.\nTo run the mapping node:\nroslaunch mapping mapping_node.launch\nThe following parameters can be set in the launch file:\nmap_topic (default: \"/map\"): Topic where the map should be published (type: nav_msgs/OccupancyGrid)\ngoal_topic (default: \"/goal\"): Topic where goal should be published (type: geometry_msgs/Pose2D)\npose_topic (default: \"/pose\"): Topic where starting position should be published (type: geometry_msgs/Pose2D)\nheight (default: 500): Height of map\nwidth (default: 500): Width of map\nresolution (default: 0.05): Resolution of map\nsave_map (default: \"none\"): If this parameter is set, the created map will be stored to this path\nload_map (default: \"none\"): If this parameter is set, map will be loaded from an image file at this path\nWhen running the mapping node:\nPress 'q' to quit\nPress 's' to save and move on\nPress numbers to the keyboard to change width of paint brush\nRight-click to toggle between eraser and paint brush\nPlanner\nThe planner package provides an implementation of the RRT algorithm.\nTo run the rrt planner node:\nroslaunch planner rrt_planner_node.launch\nThe following parameters can be set in the launch file:\nmap_topic (default: \"/map\"): Topic where the map is published (type: nav_msgs/OccupancyGrid)\ngoal_topic (default: \"/goal\"): Topic where goal is published (type: geometry_msgs/Pose2D)\npose_topic (default: \"/pose\"): Topic where starting position is published (type: geometry_msgs/Pose2D)\nmax_vertices (default: 1500): Maximum vertices to be added to graph after which goal search should be stopped\nstep_size (default: 20): Size of steps. Bigger steps means smaller graphs but more time and vice versa", "link": "https://github.com/naiveHobo/RRTPlanner", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "rapidly exploring random -----> tree !!!  (rrt) planner\nros package for a 2d path planner using the rapidly exploring random trees (rrt) algorithm. this repository contains two packages.\nroslaunch planner rrt_planner.launch\ninstallation\nclone this repository in a catkin workspace and run the following command to build the packages:\ncatkin build\nmapping\nthe mapping package provides a minimalistic gui to draw obstacles in a 2d environment and choose starting position and goal.\nto run the mapping node:\nroslaunch mapping mapping_node.launch\nthe following parameters can be set in the launch file:\nmap_topic (default: \"/map\"): topic where the map should be published (type: nav_msgs/occupancygrid)\ngoal_topic (default: \"/goal\"): topic where goal should be published (type: geometry_msgs/pose2d)\npose_topic (default: \"/pose\"): topic where starting position should be published (type: geometry_msgs/pose2d)\nheight (default: 500): height of map\nwidth (default: 500): width of map\nresolution (default: 0.05): resolution of map\nsave_map (default: \"none\"): if this parameter is set, the created map will be stored to this path\nload_map (default: \"none\"): if this parameter is set, map will be loaded from an image file at this path\nwhen running the mapping node:\npress 'q' to quit\npress 's' to save and move on\npress numbers to the keyboard to change width of paint brush\nright-click to toggle between eraser and paint brush\nplanner\nthe planner package provides an implementation of the rrt algorithm.\nto run the rrt planner node:\nroslaunch planner rrt_planner_node.launch\nthe following parameters can be set in the launch file:\nmap_topic (default: \"/map\"): topic where the map is published (type: nav_msgs/occupancygrid)\ngoal_topic (default: \"/goal\"): topic where goal is published (type: geometry_msgs/pose2d)\npose_topic (default: \"/pose\"): topic where starting position is published (type: geometry_msgs/pose2d)\nmax_vertices (default: 1500): maximum vertices to be added to graph after which goal search should be stopped\nstep_size (default: 20): size of steps. bigger steps means smaller graphs but more time and vice versa", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000914, "year": null}, {"Unnamed: 0": 1947, "autor": 927, "date": null, "content": "Awesome Reinforcement Learning\nA curated list of resources dedicated to reinforcement learning.\nWe have pages for other topics: awesome-rnn, awesome-deep-vision, awesome-random-forest\nMaintainers:Zhimin Hou, Hyunsoo Kim, Jiwon Kim\nWe are looking for more contributors and maintainers!\nContributing\nPlease feel free to pull requests\nTable of Contents\nCodes\nTheory\nLectures\nBooks\nSurveys\nPapers / Thesis\nApplications\nGame Playing\nRobotics\nControl\nOperations Research\nHuman Computer Interaction\nTutorials / Websites\nOnline Demos\nOpen Source Reinforcement Learning Platforms\nCodes\nCodes for examples and exercises in Richard Sutton and Andrew Barto's Reinforcement Learning: An Introduction\nPython Code\nMATLAB Code\nC/Lisp Code\nBook\nSimulation code for Reinforcement Learning Control Problems\nPole-Cart Problem\nQ-learning Controller\nMATLAB Environment and GUI for Reinforcement Learning\nReinforcement Learning Repository - University of Massachusetts, Amherst\nBrown-UMBC Reinforcement Learning and Planning Library (Java)\nReinforcement Learning in R (MDP, Value Iteration)\nReinforcement Learning Environment in Python and MATLAB\nRL-Glue (standard interface for RL) and RL-Glue Library\nPyBrain Library - Python-Based Reinforcement learning, Artificial intelligence, and Neural network\nRLPy Framework - Value-Function-Based Reinforcement Learning Framework for Education and Research\nMaja - Machine learning framework for problems in Reinforcement Learning in python\nTeachingBox - Java based Reinforcement Learning framework\nPolicy Gradient Reinforcement Learning Toolbox for MATLAB\nPIQLE - Platform Implementing Q-Learning and other RL algorithms\nBeliefBox - Bayesian reinforcement learning library and toolkit\nDeep Q-Learning with Tensor Flow - A deep Q learning demonstration using Google Tensorflow\nAtari - Deep Q-networks and asynchronous agents in Torch\nAgentNet - A python library for deep reinforcement learning and custom recurrent networks using Theano+Lasagne.\nReinforcement Learning Examples by RLCode - A Collection of minimal and clean reinforcement learning examples\nPyTorch Deep RL - Popular deep RL algorithm implementations with PyTorch\nBlack-DROPS - Modular and generic code for the model-based policy search Black-DROPS algorithm (IROS 2017 paper) and easy integration with the DART simulator\nBrowse state-of-the-art - all the state-of-art projects include CV, NLP, Robotics\nCode for real robot examples - implemented in real physical robots\nBenchmarking of model-based reinforcement learning - all the model-based reinforcement learning algorithms\nTheory\nBlog\n[Paper reading] endtoendAI\nLectures\n[UCL] COMPM050/COMPGI13 Reinforcement Learning by David Silver\n[UC Berkeley] CS188 Artificial Intelligence by Pieter Abbeel\nLecture 8: Markov Decision Processes 1\nLecture 9: Markov Decision Processes 2\nLecture 10: Reinforcement Learning 1\nLecture 11: Reinforcement Learning 2\n[Udacity (Georgia Tech.)] CS7642 Reinforcement Learning\n[Stanford] CS229 Machine Learning - Lecture 16: Reinforcement Learning by Andrew Ng\n[UC Berkeley] Deep RL Bootcamp\n[UC Berkeley] CS294 Deep Reinforcement Learning by John Schulman and Pieter Abbeel\n[CMU] 10703: Deep Reinforcement Learning and Control, Spring 2017\n[MIT] 6.S094: Deep Learning for Self-Driving Cars\nLecture 2: Deep Reinforcement Learning for Motion Planning\n[DLRL summer school] [Lectures for deep learning and reinforcement learning]\n2016 DLRL summer school in Montreal\n2017 DLRL summer school in Montreal\n2018 DLRL summer school in Toronto\n[CMU] Domain knowledge intergrate into deep learning process\n[Chinese version].Intro to Reinforcement Learning (\u5f3a\u5316\u5b66\u4e60\u7eb2\u8981)\nBooks\nRichard Sutton and Andrew Barto, Reinforcement Learning: An Introduction (1st Edition, 1998) [Book] [Code]\nRichard Sutton and Andrew Barto, Reinforcement Learning: An Introduction (2nd Edition, in progress, 2018) [Book] [Code]\nCsaba Szepesvari, Algorithms for Reinforcement Learning [Book]\nDavid Poole and Alan Mackworth, Artificial Intelligence: Foundations of Computational Agents [Book Chapter]\nDimitri P. Bertsekas and John N. Tsitsiklis, Neuro-Dynamic Programming [Book (Amazon)] [Summary]\nMykel J. Kochenderfer, Decision Making Under Uncertainty: Theory and Application [Book (Amazon)]\nSurveys\nLeslie Pack Kaelbling, Michael L. Littman, Andrew W. Moore, Reinforcement Learning: A Survey, JAIR, 1996. [Paper]\nS. S. Keerthi and B. Ravindran, A Tutorial Survey of Reinforcement Learning, Sadhana, 1994. [Paper]\nMatthew E. Taylor, Peter Stone, Transfer Learning for Reinforcement Learning Domains: A Survey, JMLR, 2009. [Paper]\nJens Kober, J. Andrew Bagnell, Jan Peters, Reinforcement Learning in Robotics, A Survey, IJRR, 2013. [Paper]\nMichael L. Littman, \"Reinforcement learning improves behaviour from evaluative feedback.\" Nature 521.7553 (2015): 445-451. [Paper]\nMarc P. Deisenroth, Gerhard Neumann, Jan Peter, A Survey on Policy Search for Robotics, Foundations and Trends in Robotics, 2014. [Book]\nPapers / Thesis\nFoundational Papers\nMarvin Minsky, Steps toward Artificial Intelligence, Proceedings of the IRE, 1961. [Paper] (discusses issues in RL such as the \"credit assignment problem\")\nIan H. Witten, An Adaptive Optimal Controller for Discrete-Time Markov Environments, Information and Control, 1977. [Paper] (earliest publication on temporal-difference (TD) learning rule)\nMethods\nDynamic Programming (DP):\nChristopher J. C. H. Watkins, Learning from Delayed Rewards, Ph.D. Thesis, Cambridge University, 1989. [Thesis]\nMonte Carlo:\nAndrew Barto, Michael Duff, Monte Carlo Inversion and Reinforcement Learning, NIPS, 1994. [Paper]\nSatinder P. Singh, Richard S. Sutton, Reinforcement Learning with Replacing Eligibility Traces, Machine Learning, 1996. [Paper]\nTemporal-Difference:\nRichard S. Sutton, Learning to predict by the methods of temporal differences. Machine Learning 3: 9-44, 1988. [Paper]\nQ-Learning (Off-policy TD algorithm):\nChris Watkins, Learning from Delayed Rewards, Cambridge, 1989. [Thesis]\nSarsa (On-policy TD algorithm):\nG.A. Rummery, M. Niranjan, On-line Q-learning using connectionist systems, Technical Report, Cambridge Univ., 1994. [Report]\nRichard S. Sutton, Generalization in Reinforcement Learning: Successful examples using sparse coding, NIPS, 1996. [Paper]\nR-Learning (learning of relative values)\nAndrew Schwartz, A Reinforcement Learning Method for Maximizing Undiscounted Rewards, ICML, 1993. [Paper-Google Scholar]\nFunction Approximation methods (Least-Square Temporal Difference, Least-Square Policy Iteration)\nSteven J. Bradtke, Andrew G. Barto, Linear Least-Squares Algorithms for Temporal Difference Learning, Machine Learning, 1996. [Paper]\nMichail G. Lagoudakis, Ronald Parr, Model-Free Least Squares Policy Iteration, NIPS, 2001. [Paper] [Code]\nPolicy Search / Policy Gradient\nRichard Sutton, David McAllester, Satinder Singh, Yishay Mansour, Policy Gradient Methods for Reinforcement Learning with Function Approximation, NIPS, 1999. [Paper]\nJan Peters, Sethu Vijayakumar, Stefan Schaal, Natural Actor-Critic, ECML, 2005. [Paper]\nJens Kober, Jan Peters, Policy Search for Motor Primitives in Robotics, NIPS, 2009. [Paper]\nJan Peters, Katharina Mulling, Yasemin Altun, Relative Entropy Policy Search, AAAI, 2010. [Paper]\nFreek Stulp, Olivier Sigaud, Path Integral Policy Improvement with Covariance Matrix Adaptation, ICML, 2012. [Paper]\nNate Kohl, Peter Stone, Policy Gradient Reinforcement Learning for Fast Quadrupedal Locomotion, ICRA, 2004. [Paper]\nMarc Deisenroth, Carl Rasmussen, PILCO: A Model-Based and Data-Efficient Approach to Policy Search, ICML, 2011. [Paper]\nScott Kuindersma, Roderic Grupen, Andrew Barto, Learning Dynamic Arm Motions for Postural Recovery, Humanoids, 2011. [Paper]\nKonstantinos Chatzilygeroudis, Roberto Rama, Rituraj Kaushik, Dorian Goepp, Vassilis Vassiliades, Jean-Baptiste Mouret, Black-Box Data-efficient Policy Search for Robotics, IROS, 2017. [Paper]\nHierarchical RL\nRichard Sutton, Doina Precup, Satinder Singh, Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning, Artificial Intelligence, 1999. [Paper]\nGeorge Konidaris, Andrew Barto, Building Portable Options: Skill Transfer in Reinforcement Learning, IJCAI, 2007. [Paper]\nDeep Learning + Reinforcement Learning (A sample of recent works on DL+RL)\nV. Mnih, et. al., Human-level Control through Deep Reinforcement Learning, Nature, 2015. [Paper]\nXiaoxiao Guo, Satinder Singh, Honglak Lee, Richard Lewis, Xiaoshi Wang, Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning, NIPS, 2014. [Paper]\nSergey Levine, Chelsea Finn, Trevor Darrel, Pieter Abbeel, End-to-End Training of Deep Visuomotor Policies. ArXiv, 16 Oct 2015. [ArXiv]\nTom Schaul, John Quan, Ioannis Antonoglou, David Silver, Prioritized Experience Replay, ArXiv, 18 Nov 2015. [ArXiv]\nHado van Hasselt, Arthur Guez, David Silver, Deep Reinforcement Learning with Double Q-Learning, ArXiv, 22 Sep 2015. [ArXiv]\nVolodymyr Mnih, Adri\u00e0 Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu, Asynchronous Methods for Deep Reinforcement Learning, ArXiv, 4 Feb 2016. [ArXiv]\nKalman Filter\nThe basic lecture\nApplications\nGame Playing\nTraditional Games\nBackgammon - \"TD-Gammon\" game play using TD(\u03bb) (Tesauro, ACM 1995) [Paper]\nChess - \"KnightCap\" program using TD(\u03bb) (Baxter, arXiv 1999) [arXiv]\nChess - Giraffe: Using deep reinforcement learning to play chess (Lai, arXiv 2015) [arXiv]\nComputer Games\nHuman-level Control through Deep Reinforcement Learning (Mnih, Nature 2015) [Paper] [Code] [Video]\nFlappy Bird Reinforcement Learning [Video]\nMarI/O - learning to play Mario with evolutionary reinforcement learning using artificial neural networks (Stanley, Evolutionary Computation 2002) [Paper] [Video]\nRobotics\nPolicy Gradient Reinforcement Learning for Fast Quadrupedal Locomotion (Kohl, ICRA 2004) [Paper]\nRobot Motor SKill Coordination with EM-based Reinforcement Learning (Kormushev, IROS 2010) [Paper] [Video]\nGeneralized Model Learning for Reinforcement Learning on a Humanoid Robot (Hester, ICRA 2010) [Paper] [Video]\nAutonomous Skill Acquisition on a Mobile Manipulator (Konidaris, AAAI 2011) [Paper] [Video]\nPILCO: A Model-Based and Data-Efficient Approach to Policy Search (Deisenroth, ICML 2011) [Paper]\nIncremental Semantically Grounded Learning from Demonstration (Niekum, RSS 2013) [Paper]\nEfficient Reinforcement Learning for Robots using Informative Simulated Priors (Cutler, ICRA 2015) [Paper] [Video]\nRobots that can adapt like animals (Cully, Nature 2015) [Paper] [Video] [Code]\nBlack-Box Data-efficient Policy Search for Robotics (Chatzilygeroudis, IROS 2017) [Paper] [Video] [Code]\nModel-driven DDPG with fuzzy reward signals for robotic peg-in-hole assembly(IEEE Transactions on Industrial Informatics) [Paper]\nControl\nAn Application of Reinforcement Learning to Aerobatic Helicopter Flight (Abbeel, NIPS 2006) [Paper] [Video]\nAutonomous helicopter control using Reinforcement Learning Policy Search Methods (Bagnell, ICRA 2001) [Paper]\nOperations Research\nScaling Average-reward Reinforcement Learning for Product Delivery (Proper, AAAI 2004) [Paper]\nCross Channel Optimized Marketing by Reinforcement Learning (Abe, KDD 2004) [Paper]\nHuman Computer Interaction\nOptimizing Dialogue Management with Reinforcement Learning: Experiments with the NJFun System (Singh, JAIR 2002) [Paper]\nTutorials / Websites\nMance Harmon and Stephanie Harmon, Reinforcement Learning: A Tutorial\nC. Igel, M.A. Riedmiller, et al., Reinforcement Learning in a Nutshell, ESANN, 2007. [Paper]\nUNSW - Reinforcement Learning\nIntroduction\nTD-Learning\nQ-Learning and SARSA\nApplet for \"Cat and Mouse\" Game\nROS Reinforcement Learning Tutorial\nPOMDP for Dummies\nScholarpedia articles on:\nReinforcement Learning\nTemporal Difference Learning\nRepository with useful MATLAB Software, presentations, and demo videos\nBibliography on Reinforcement Learning\nUC Berkeley - CS 294: Deep Reinforcement Learning, Fall 2015 (John Schulman, Pieter Abbeel) [Class Website]\nBlog posts on Reinforcement Learning, Parts 1-4 by Travis DeWolf\nThe Arcade Learning Environment - Atari 2600 games environment for developing AI agents\nDeep Reinforcement Learning: Pong from Pixels by Andrej Karpathy\nDemystifying Deep Reinforcement Learning\nLet\u2019s make a DQN\nSimple Reinforcement Learning with Tensorflow, Parts 0-8 by Arthur Juliani\nPractical_RL - github-based course in reinforcement learning in the wild (lectures, coding labs, projects)\nPrinciples of Deep RL by David Silver (Deep Learning Indaba)\nSuccess Stories of Deep RL by David Silver (Deep leanring Indaba)\nOnline Demos\nReal-world demonstrations of Reinforcement Learning\nDeep Q-Learning Demo - A deep Q learning demonstration using ConvNetJS\nDeep Q-Learning with Tensor Flow - A deep Q learning demonstration using Google Tensorflow\nReinforcement Learning Demo - A reinforcement learning demo using reinforcejs by Andrej Karpathy\nOpen Source Reinforcement Learning Platforms\nOpenAI gym - A toolkit for developing and comparing reinforcement learning algorithms\nOpenAI universe - A software platform for measuring and training an AI's general intelligence across the world's supply of games, websites and other applications\nDeepMind Lab - A customisable 3D platform for agent-based AI research\nProject Malmo - A platform for Artificial Intelligence experimentation and research built on top of Minecraft by Microsoft\nViZDoom - Doom-based AI research platform for reinforcement learning from raw visual information\nRetro Learning Environment - An AI platform for reinforcement learning based on video game emulators. Currently supports SNES and Sega Genesis. Compatible with OpenAI gym.\ntorch-twrl - A package that enables reinforcement learning in Torch by Twitter\nUETorch - A Torch plugin for Unreal Engine 4 by Facebook\nTorchCraft - Connecting Torch to StarCraft\nrllab - A framework for developing and evaluating reinforcement learning algorithms, fully compatible with OpenAI Gym\nTensorForce - Practical deep reinforcement learning on TensorFlow with Gitter support and OpenAI Gym/Universe/DeepMind Lab integration.\nOpenAI lab - An experimentation system for Reinforcement Learning using OpenAI Gym, Tensorflow, and Keras.\nkeras-rl - State-of-the art deep reinforcement learning algorithms in Keras designed for compatibility with OpenAI.\nBURLAP - Brown-UMBC Reinforcement Learning and Planning, a library written in Java\nMAgent - A Platform for Many-agent Reinforcement Learning.\nRay RLlib - Ray RLlib is a reinforcement learning library that aims to provide both performance and composability.\nMIT Autonomous Driving Lab - This include a deep traffic and deep learning for autonomous driving.", "link": "https://github.com/hzm2016/sources-of-reinforcement-learning", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "awesome reinforcement learning\na curated list of resources dedicated to reinforcement learning.\nwe have pages for other topics: awesome-rnn, awesome-deep-vision, awesome-random-forest\nmaintainers:zhimin hou, hyunsoo kim, jiwon kim\nwe are looking for more contributors and maintainers!\ncontributing\nplease feel free to pull requests\ntable of contents\ncodes\ntheory\nlectures\nbooks\nsurveys\npapers / thesis\napplications\ngame playing\nrobotics\ncontrol\noperations research\nhuman computer interaction\ntutorials / websites\nonline demos\nopen source reinforcement learning platforms\ncodes\ncodes for examples and exercises in richard sutton and andrew barto's reinforcement learning: an introduction\npython code\nmatlab code\nc/lisp code\nbook\nsimulation code for reinforcement learning control problems\npole-cart problem\nq-learning controller\nmatlab environment and gui for reinforcement learning\nreinforcement learning repository - university of massachusetts, amherst\nbrown-umbc reinforcement learning and planning library (java)\nreinforcement learning in r (mdp, value iteration)\nreinforcement learning environment in python and matlab\nrl-glue (standard interface for rl) and rl-glue library\npybrain library - python-based reinforcement learning, artificial intelligence, and neural network\nrlpy framework - value-function-based reinforcement learning framework for education and research\nmaja - machine learning framework for problems in reinforcement learning in python\nteachingbox - java based reinforcement learning framework\npolicy gradient reinforcement learning toolbox for matlab\npiqle - platform implementing q-learning and other rl algorithms\nbeliefbox - bayesian reinforcement learning library and toolkit\ndeep q-learning with tensor flow - a deep q learning demonstration using google tensorflow\natari - deep q-networks and asynchronous agents in torch\nagentnet - a python library for deep reinforcement learning and custom recurrent networks using theano+lasagne.\nreinforcement learning examples by rlcode - a collection of minimal and clean reinforcement learning examples\npytorch deep rl - popular deep rl algorithm implementations with pytorch\nblack-drops - modular and generic code for the model-based policy search black-drops algorithm (iros 2017 paper) and easy integration with the dart simulator\nbrowse state-of-the-art - all the state-of-art projects include cv, nlp, robotics\ncode for real robot examples - implemented in real physical robots\nbenchmarking of model-based reinforcement learning - all the model-based reinforcement learning algorithms\ntheory\nblog\n[paper reading] endtoendai\nlectures\n[ucl] compm050/compgi13 reinforcement learning by david silver\n[uc berkeley] cs188 artificial intelligence by pieter abbeel\nlecture 8: markov decision processes 1\nlecture 9: markov decision processes 2\nlecture 10: reinforcement learning 1\nlecture 11: reinforcement learning 2\n[udacity (georgia tech.)] cs7642 reinforcement learning\n[stanford] cs229 machine learning - lecture 16: reinforcement learning by andrew ng\n[uc berkeley] deep rl bootcamp\n[uc berkeley] cs294 deep reinforcement learning by john schulman and pieter abbeel\n[cmu] 10703: deep reinforcement learning and control, spring 2017\n[mit] 6.s094: deep learning for self-driving cars\nlecture 2: deep reinforcement learning for motion planning\n[dlrl summer school] [lectures for deep learning and reinforcement learning]\n2016 dlrl summer school in montreal\n2017 dlrl summer school in montreal\n2018 dlrl summer school in toronto\n[cmu] domain knowledge intergrate into deep learning process\n[chinese version].intro to reinforcement learning (\u5f3a\u5316\u5b66\u4e60\u7eb2\u8981)\nbooks\nrichard sutton and andrew barto, reinforcement learning: an introduction (1st edition, 1998) [book] [code]\nrichard sutton and andrew barto, reinforcement learning: an introduction (2nd edition, in progress, 2018) [book] [code]\ncsaba szepesvari, algorithms for reinforcement learning [book]\ndavid poole and alan mackworth, artificial intelligence: foundations of computational agents [book chapter]\ndimitri p. bertsekas and john n. tsitsiklis, neuro-dynamic programming [book (amazon)] [summary]\nmykel j. kochenderfer, decision making under uncertainty: theory and application [book (amazon)]\nsurveys\nleslie pack kaelbling, michael l. littman, andrew w. moore, reinforcement learning: a survey, jair, 1996. [paper]\ns. s. keerthi and b. ravindran, a tutorial survey of reinforcement learning, sadhana, 1994. [paper]\nmatthew e. taylor, peter stone, transfer learning for reinforcement learning domains: a survey, jmlr, 2009. [paper]\njens kober, j. andrew bagnell, jan peters, reinforcement learning in robotics, a survey, ijrr, 2013. [paper]\nmichael l. littman, \"reinforcement learning improves behaviour from evaluative feedback.\" nature 521.7553 (2015): 445-451. [paper]\nmarc p. deisenroth, gerhard neumann, jan peter, a survey on policy search for robotics, foundations and trends in robotics, 2014. [book]\npapers / thesis\nfoundational papers\nmarvin minsky, steps toward artificial intelligence, proceedings of the ire, 1961. [paper] (discusses issues in rl such as the \"credit assignment problem\")\nian h. witten, an adaptive optimal controller for discrete-time markov environments, information and control, 1977. [paper] (earliest publication on temporal-difference (td) learning rule)\nmethods\ndynamic programming (dp):\nchristopher j. c. h. watkins, learning from delayed rewards, ph.d. thesis, cambridge university, 1989. [thesis]\nmonte carlo:\nandrew barto, michael duff, monte carlo inversion and reinforcement learning, nips, 1994. [paper]\nsatinder p. singh, richard s. sutton, reinforcement learning with replacing eligibility traces, machine learning, 1996. [paper]\ntemporal-difference:\nrichard s. sutton, learning to predict by the methods of temporal differences. machine learning 3: 9-44, 1988. [paper]\nq-learning (off-policy td algorithm):\nchris watkins, learning from delayed rewards, cambridge, 1989. [thesis]\nsarsa (on-policy td algorithm):\ng.a. rummery, m. niranjan, on-line q-learning using connectionist systems, technical report, cambridge univ., 1994. [report]\nrichard s. sutton, generalization in reinforcement learning: successful examples using sparse coding, nips, 1996. [paper]\nr-learning (learning of relative values)\nandrew schwartz, a reinforcement learning method for maximizing undiscounted rewards, icml, 1993. [paper-google scholar]\nfunction approximation methods (least-square temporal difference, least-square policy iteration)\nsteven j. bradtke, andrew g. barto, linear least-squares algorithms for temporal difference learning, machine learning, 1996. [paper]\nmichail g. lagoudakis, ronald parr, model-free least squares policy iteration, nips, 2001. [paper] [code]\npolicy search / policy gradient\nrichard sutton, david mcallester, satinder singh, yishay mansour, policy gradient methods for reinforcement learning with function approximation, nips, 1999. [paper]\njan peters, sethu vijayakumar, stefan schaal, natural actor-critic, ecml, 2005. [paper]\njens kober, jan peters, policy search for motor primitives in robotics, nips, 2009. [paper]\njan peters, katharina mulling, yasemin altun, relative entropy policy search, aaai, 2010. [paper]\nfreek stulp, olivier sigaud, path integral policy improvement with covariance matrix adaptation, icml, 2012. [paper]\nnate kohl, peter stone, policy gradient reinforcement learning for fast quadrupedal locomotion, icra, 2004. [paper]\nmarc deisenroth, carl rasmussen, pilco: a model-based and data-efficient approach to policy search, icml, 2011. [paper]\nscott kuindersma, roderic grupen, andrew barto, learning dynamic arm motions for postural recovery, humanoids, 2011. [paper]\nkonstantinos chatzilygeroudis, roberto rama, rituraj kaushik, dorian goepp, vassilis vassiliades, jean-baptiste mouret, black-box data-efficient policy search for robotics, iros, 2017. [paper]\nhierarchical rl\nrichard sutton, doina precup, satinder singh, between mdps and semi-mdps: a framework for temporal abstraction in reinforcement learning, artificial intelligence, 1999. [paper]\ngeorge konidaris, andrew barto, building portable options: skill transfer in reinforcement learning, ijcai, 2007. [paper]\ndeep learning + reinforcement learning (a sample of recent works on dl+rl)\nv. mnih, et. al., human-level control through deep reinforcement learning, nature, 2015. [paper]\nxiaoxiao guo, satinder singh, honglak lee, richard lewis, xiaoshi wang, deep learning for real-time atari game play using offline monte-carlo -----> tree !!!  search planning, nips, 2014. [paper]\nsergey levine, chelsea finn, trevor darrel, pieter abbeel, end-to-end training of deep visuomotor policies. arxiv, 16 oct 2015. [arxiv]\ntom schaul, john quan, ioannis antonoglou, david silver, prioritized experience replay, arxiv, 18 nov 2015. [arxiv]\nhado van hasselt, arthur guez, david silver, deep reinforcement learning with double q-learning, arxiv, 22 sep 2015. [arxiv]\nvolodymyr mnih, adri\u00e0 puigdom\u00e8nech badia, mehdi mirza, alex graves, timothy p. lillicrap, tim harley, david silver, koray kavukcuoglu, asynchronous methods for deep reinforcement learning, arxiv, 4 feb 2016. [arxiv]\nkalman filter\nthe basic lecture\napplications\ngame playing\ntraditional games\nbackgammon - \"td-gammon\" game play using td(\u03bb) (tesauro, acm 1995) [paper]\nchess - \"knightcap\" program using td(\u03bb) (baxter, arxiv 1999) [arxiv]\nchess - giraffe: using deep reinforcement learning to play chess (lai, arxiv 2015) [arxiv]\ncomputer games\nhuman-level control through deep reinforcement learning (mnih, nature 2015) [paper] [code] [video]\nflappy bird reinforcement learning [video]\nmari/o - learning to play mario with evolutionary reinforcement learning using artificial neural networks (stanley, evolutionary computation 2002) [paper] [video]\nrobotics\npolicy gradient reinforcement learning for fast quadrupedal locomotion (kohl, icra 2004) [paper]\nrobot motor skill coordination with em-based reinforcement learning (kormushev, iros 2010) [paper] [video]\ngeneralized model learning for reinforcement learning on a humanoid robot (hester, icra 2010) [paper] [video]\nautonomous skill acquisition on a mobile manipulator (konidaris, aaai 2011) [paper] [video]\npilco: a model-based and data-efficient approach to policy search (deisenroth, icml 2011) [paper]\nincremental semantically grounded learning from demonstration (niekum, rss 2013) [paper]\nefficient reinforcement learning for robots using informative simulated priors (cutler, icra 2015) [paper] [video]\nrobots that can adapt like animals (cully, nature 2015) [paper] [video] [code]\nblack-box data-efficient policy search for robotics (chatzilygeroudis, iros 2017) [paper] [video] [code]\nmodel-driven ddpg with fuzzy reward signals for robotic peg-in-hole assembly(ieee transactions on industrial informatics) [paper]\ncontrol\nan application of reinforcement learning to aerobatic helicopter flight (abbeel, nips 2006) [paper] [video]\nautonomous helicopter control using reinforcement learning policy search methods (bagnell, icra 2001) [paper]\noperations research\nscaling average-reward reinforcement learning for product delivery (proper, aaai 2004) [paper]\ncross channel optimized marketing by reinforcement learning (abe, kdd 2004) [paper]\nhuman computer interaction\noptimizing dialogue management with reinforcement learning: experiments with the njfun system (singh, jair 2002) [paper]\ntutorials / websites\nmance harmon and stephanie harmon, reinforcement learning: a tutorial\nc. igel, m.a. riedmiller, et al., reinforcement learning in a nutshell, esann, 2007. [paper]\nunsw - reinforcement learning\nintroduction\ntd-learning\nq-learning and sarsa\napplet for \"cat and mouse\" game\nros reinforcement learning tutorial\npomdp for dummies\nscholarpedia articles on:\nreinforcement learning\ntemporal difference learning\nrepository with useful matlab software, presentations, and demo videos\nbibliography on reinforcement learning\nuc berkeley - cs 294: deep reinforcement learning, fall 2015 (john schulman, pieter abbeel) [class website]\nblog posts on reinforcement learning, parts 1-4 by travis dewolf\nthe arcade learning environment - atari 2600 games environment for developing ai agents\ndeep reinforcement learning: pong from pixels by andrej karpathy\ndemystifying deep reinforcement learning\nlet\u2019s make a dqn\nsimple reinforcement learning with tensorflow, parts 0-8 by arthur juliani\npractical_rl - github-based course in reinforcement learning in the wild (lectures, coding labs, projects)\nprinciples of deep rl by david silver (deep learning indaba)\nsuccess stories of deep rl by david silver (deep leanring indaba)\nonline demos\nreal-world demonstrations of reinforcement learning\ndeep q-learning demo - a deep q learning demonstration using convnetjs\ndeep q-learning with tensor flow - a deep q learning demonstration using google tensorflow\nreinforcement learning demo - a reinforcement learning demo using reinforcejs by andrej karpathy\nopen source reinforcement learning platforms\nopenai gym - a toolkit for developing and comparing reinforcement learning algorithms\nopenai universe - a software platform for measuring and training an ai's general intelligence across the world's supply of games, websites and other applications\ndeepmind lab - a customisable 3d platform for agent-based ai research\nproject malmo - a platform for artificial intelligence experimentation and research built on top of minecraft by microsoft\nvizdoom - doom-based ai research platform for reinforcement learning from raw visual information\nretro learning environment - an ai platform for reinforcement learning based on video game emulators. currently supports snes and sega genesis. compatible with openai gym.\ntorch-twrl - a package that enables reinforcement learning in torch by twitter\nuetorch - a torch plugin for unreal engine 4 by facebook\ntorchcraft - connecting torch to starcraft\nrllab - a framework for developing and evaluating reinforcement learning algorithms, fully compatible with openai gym\ntensorforce - practical deep reinforcement learning on tensorflow with gitter support and openai gym/universe/deepmind lab integration.\nopenai lab - an experimentation system for reinforcement learning using openai gym, tensorflow, and keras.\nkeras-rl - state-of-the art deep reinforcement learning algorithms in keras designed for compatibility with openai.\nburlap - brown-umbc reinforcement learning and planning, a library written in java\nmagent - a platform for many-agent reinforcement learning.\nray rllib - ray rllib is a reinforcement learning library that aims to provide both performance and composability.\nmit autonomous driving lab - this include a deep traffic and deep learning for autonomous driving.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000927, "year": null}, {"Unnamed: 0": 1948, "autor": 928, "date": null, "content": "The classification of articles may be inaccurate due to personal limited knowledge level.\nContent\nData Structure\nTree\nMachine Learning\nAlgorithm\nNeural Network\nFramework\nNetwork Architecture\nNeural Network Component\nOptimizer\nComputer Vision\nDataset\n2D Object Detection\nAlgorithm\n3D Object Detection\nAlgorithm\n2D Segmentation\nAlgorithm\n3D Segmentation\nAlgorithm\n2D Pose\nAlgorithm\n3D Pose\nAlgorithm\nVideo\nSegmentation\nMotion Representation\nGenerative Model\nVAE (Variational Auto-Encoder)\nModels\nApplications\nGAN (Generative Adversarial Networks)\nModels\nApplications\nReinforcement Learning\nEnvironment\nAlgorithm\nRL in Games\nDistributional RL\nTransfer Learning & Meta Learning\nAlgorithm or Model\nZero Shot Learning\nRobot\nDataset\nHardware\nGrasping\nGrasping with RL\nGrasping Unknown Objects\nGrasping in Cluttered Environment\nGrasping via Segmentation\nGrasping Points Selection\nMachine Vision\nActive Perception\nMotion Prediction\nInteractive Perception\nData Structure\nTree\n\u3010Kd-tree\u3011Multidimensional binary search trees used for associative searching (1975)\n\u3010Oc-tree\u3011Octree encoding: A new technique for the representation, manipulation and display of arbitrary 3-d objects by computer (1980)\nMachine Learning\nAlgorithm\n\u3010SVM\u3011Least squares support vector machine classifiers (Springer 1999)\n\u3010PCA\u3011Singular value decomposition and principal component analysis (Springer 2003)\nNeural Network\nFramework\nTorch7: A MATLAB-like environment for machine learning (NIPS workshop 2011)\nCaffe: Convolutional Architecture for Fast Feature Embedding (arxiv 2014)\nTensorFlow: A System for Large-Scale Machine Learning (OSDI 2016)\nNetwork Architecture\nLong Short Term Memory Network (Journals 1997)\n\u3010VGG16\u3011Very Deep Convolutional Networks for Large-Scale Image Recognition (arxiv 2014)\nDueling Network Architectures for Deep Reinforcement Learning (arxiv 2015)\n\u3010STN\u3011Recurrent Spatial Transformer Networks (arxiv 2015)\n\u3010FCN\u3011Fully Convolutional Networks for Semantic Segmentation (cv-foundation 2015)\nFaceNet: A Unified Embedding for Face Recognition and Clustering (cv-foundation 2015)\n\u3010C3D\u3011Learning Spatiotemporal Features with 3D Convolutional Networks (ICCV 2015)\n\u3010ResNet\u3011Deep Residual Learning for Image Recognition (openaccess.thecvf 2016)\n\u3010GCN\u3011Semi-supervised classification with graph convolutional networks (ICLR 2017)\nNon-local Neural Networks (arxiv 2017)\n\u3010SPN\u3011Learning Affinity via Spatial Propagation Networks (NIPS 2017)\n\u3010Capsule\u3011Dynamic Routing Between Capsules (NIPS 2017)\nMobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications (arxiv 2017)\nPackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning (arxiv 2017)\nPoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes (arxiv 2017)\n\u3010PSMNet\u3011Pyramid Stereo Matching Network (CVPR 2018)\nPlaneNet: Piece-wise Planar Reconstruction from a Single RGB Image (CVPR 2018)\nSBNet: Sparse Blocks Network for Fast Inference (arxiv 2018)\nNeural Network Component\n\u3010ReLu\u3011Rectified Linear Units Improve Restricted Boltzmann Machines (ICML 2010)\nDropout: a simple way to prevent neural networks from overfitting (jmlr 2014)\nBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (ICML 2015)\n\u3010center loss\u3011A Discriminative Feature Learning Approach for Deep Face Recognition (ECCV 2016)\nGroup Normalization (arxiv 2018)\nOptimizer\n\u3010ASGD\u3011Acceleration of stochastic approximation by averaging (Journals 1992)\n\u3010Adagrad\u3011Adaptive Subgradient Methods for Online Learning and Stochastic Optimization (jmlr 2011)\nADADELTA: An Adaptive Learning Rate Method (arxiv 2012)\n\u3010RMSprop\u3011Generating Sequences With Recurrent Neural Networks (arxiv 2013)\nAdam: A Method for Stochastic Optimization (ICLR 2015)\nLearning to learn by gradient descent by gradient descent (NIPS 2016)\nComputer Vision\nDataset\nImageNet: A large-scale hierarchical image database (CVPR 2009)\n\u3010NYUV2\u3011Indoor segmentation and support inference from rgbd images (ECCV 2012)\n\u3010KITTI\u3011Vision meets robotics: The KITTI dataset (IJRR 2013)\n\u3010Daimler Urban Segmentation\u3011Efficient Multi-Cue Scene Segmentation (GCPR 2013)\n\u3010Pascal Context\u3011The Role of Context for Object Detection and Semantic Segmentation in the Wild (CVPR 2014)\n\u3010Pascal VOC\u3011The Pascal Visual Object Classes Challenge: A Retrospective (IJCV 2014)\n\u3010COCO\u3011Microsoft COCO: Common Objects in Context (ECCV 2014)\n\u3010ILSVRC\u3011ImageNet Large Scale Visual Recognition Challenge (IJCV 2015)\n\u3010ModelNet40\u30113D ShapeNets: A Deep Representation for Volumetric Shapes (CVPR 2015) Dataset is available at [website].\n\u3010Cityscapes\u3011The Cityscapes Dataset for Semantic Urban Scene Understanding (CVPR 2016)\n\u3010S3DIS\u30113d semantic parsing of largescale indoor spaces (CVPR 2016)\n\u3010ScanNet\u3011ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes (CVPR workshop 2018)\n2D Object Detection\nAlgorithm\n\u3010R-CNN\u3011Rich feature hierarchies for accurate object detection and semantic segmentation (CVPR 2014)\nFast R-CNN (ICCV 2015)\nFaster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks (NIPS 2015)\nSSD: Single Shot MultiBox Detector (ECCV 2016)\nR-FCN: Object Detection via Region-based Fully Convolutional Networks (NIPS 2016)\n\u3010YOLO\u3011You Only Look Once: Unified, Real-Time Object Detection (CVPR 2016)\nYOLO9000: Better, Faster, Stronger (openaccess.thecvf 2017)\nFSSD: Feature Fusion Single Shot Multibox Detector (arxiv 2017)\n\u3010RFB-SSD\u3011Receptive Field Block Net for Accurate and Fast Object Detection (arxiv 2017)\n\u3010RefineDet\u3011Single-Shot Refinement Neural Network for Object Detection (arxiv 2017)\nMegDet: A Large Mini-Batch Object Detector (arxiv 2017)\nLight-Head R-CNN: In Defense of Two-Stage Object Detector (arxiv 2017)\n\u3010RetinaNet / Focal Loss\u3011Focal Loss for Dense Object Detection (ICCV 2017)\nYOLOv3: An Incremental Improvement (?? 2018)\n3D Object Detection\nAlgorithm\nSSD-6D: Making RGB-based 3D detection and 6D pose estimation great again (openaccess.thecvf 2017)\n\u3010Frustum PointNets\u3011Frustum PointNets for 3D Object Detection from RGB-D Data (arxiv 2017)\n2D Segmentation\nAlgorithm\n\u3010U-Net\u3011U-net: Convolutional networks for biomedical image segmentation (arxiv 2015)\n\u3010DeepMask\u3011Learning to Segment Object Candidates (arxiv 2015)\nInstance-aware Semantic Segmentation via Multi-task Network Cascades (CVPR 2016)\nMask R-CNN (ICCV 2017)\n\u3010W-Net\u3011W-Net: A Deep Model for Fully Unsupervised Image Segmentation (arxiv 2017)\n\u3010RefineNet\u3011RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation (CVPR 2017)\nSemantic Instance Segmentation with a Discriminative Loss Function (CVPR workshop 2017)\nDeep Extreme Cut: From Extreme Points to Object Segmentation (CVPR 2018)\nWeakly Supervised Instance Segmentation using Class Peak Response (CVPR 2018)\n\u3010Mask^X RCNN\u3011Learning to Segment Every Thing (CVPR 2018)\n3D Segmentation\nAlgorithm\nDeep learning with sets and point clouds (ICLR 2017)\nPointNet: Deep Learning on Point Sets for 3D Classification and Segmentation (CVPR 2017)\n3DContextNet: K-d Tree Guided Hierarchical Learning of Point Clouds Using Local and Global Contextual Cues (CVPR 2017)\nPointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space (NIPS 2017)\nEscape from cells: Deep kd-networks for the recognition of 3d point cloud models (ICCV 2017)\n\u3010PointSIFT\u3011PointSIFT: A SIFT-like Network Module for 3D Point Cloud Semantic Segmentation (arxiv 2018)\n\u3010Kd-network\u3011PointCNN (arxiv 2018)\n\u3010SO-Net\u3011SO-Net: Self-Organizing Network for Point Cloud Analysis (CVPR 2018)\n\u3010SGPN\u3011SGPN: Similarity Group Proposal Network for 3D Point Cloud Instance Segmentation (CVPR 2018)\n\u3010DGCNN\u3011Dynamic Graph CNN for Learning on Point Clouds\n2D Pose\nAlgorithm\n\u3010open pose\u3011Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields (CVPR 2017)\n\u3010G-RMI\u3011Towards accurate multi-person pose estimation in the wild (CVPR 2017)\nJoint Multi-Person Pose Estimation and Semantic Part Segmentation (CVPR 2017)\nRMPE: Regional Multi-Person Pose Estimation (ICCV 2017)\nVnect: Real-time 3d human pose estimation with a single rgb camera (SIGGRAPH 2017)\n\u3010CPN\u3011Cascaded Pyramid Network for Multi-Person Pose Estimation (CVPR 2018)\n3D Pose\nAlgorithm\nCoarse-to-Fine Volumetric Prediction for Single-Image 3D Human Pose (CVPR 2017)\nVideo\nSegmentation\nLearning to Segment Instances in Videos with Spatial Propagation Network (CVPR workshop 2017)\nSegFlow: Joint Learning for Video Object Segmentation and Optical Flow (CVPR 2017)\nLearning Features by Watching Objects Move (CVPR 2017)\nMotion Representation\n\u3010TVnet\u3011End-to-End Learning of Motion Representation for Video Understanding (CVPR 2018)\nGenerative Model\nVAE (Variational Auto-Encoder)\nModels\nApplications\nAttribute2Image: Conditional Image Generation from Visual Attributes (ECCV 2016)\nGAN (Generative Adversarial Networks)\nModels\n\u3010GAN\u3011Generative Adversarial Networks (NIPS 2014)\n\u3010CGAN\u3011Conditional Generative Adversarial Nets (arxiv 2014)\n\u3010AAE\u3011Adversarial Autoencoders (arxiv 2015)\n\u3010DCGAN\u3011Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (arxiv 2015)\nApplications\nReinforcement Learning\nEnvironment\nMuJoCo: A physics engine for model-based control (International Conference on Intelligent Robots and Systems 2012)\nOpenAI Gym (arxiv 2016)\n\u3010rllab\u3011Benchmarking Deep Reinforcement Learning for Continuous Control (jmlr 2016)\nDeepMind Lab (arxiv 2016)\nStarCraft II: A New Challenge for Reinforcement Learning (arxiv 2017)\nMAgent: A Many-Agent Reinforcement Learning Platform for Artificial Collective Intelligence (arxiv 2017)\nAlgorithm\nQ-learning (Springer 1992)\n\u3010DQN\u3011Playing Atari with Deep Reinforcement Learning (NIPS workshop 2013)\n\u3010DPG\u3011Deterministic Policy Gradient Algorithms (ICML 2014)\n\u3010TRPO\u3011Trust Region Policy Optimization (ICML 2015)\n\u3010Double-DQN\u3011Deep Reinforcement Learning with Double Q-learning (AAAI 2016)\n\u3010h-DQN\u3011Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation (NIPS 2016)\n\u3010A3C\u3011Asynchronous Methods for Deep Reinforcement Learning (ICML 2016)\n\u3010DDPG\u3011Continuous control with deep reinforcement learning (ICLR 2016)\n\u3010NAF\u3011Continuous Deep Q-Learning with Model-based Acceleration (arxiv 2016)\n\u3010ACER\u3011Sample Efficient Actor-Critic with Experience Replay (arxiv 2016)\n\u3010GAIL\u3011Generative Adversarial Imitation Learning (NIPS 2016)\nNeural Episodic Control (arxiv 2017)\nQ-PROP: SAMPLE-EFFICIENT POLICY GRADIENT WITH AN OFF-POLICY CRITIC (ICLR 2017)\n\u3010PPO\u3011Proximal Policy Optimization Algorithms (arxiv 2017)\nEmergence of Locomotion Behaviours in Rich Environments (arxiv 2017)\n\u3010ACKTR\u3011Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation (NIPS 2017)\n\u3010HER\u3011Hindsight Experience Replay (NIPS 2017)\nNoisy Networks for Exploration (ICLR 2018)\nRL in Games\nControl of Memory, Active Perception, and Action in Minecraft (arxiv 2016)\nDistributional RL\nA Distributional Perspective on Reinforcement Learning (arxiv 2017)\nCuriosity-Driven RL\nCuriosity-driven Exploration by Self-supervised Prediction (ICML 2017)\nIntrinsically motivated model learning for developing curious robots (Artificial Intelligence 2017)\nComputational Theories of Curiosity-Driven Learning (arxiv 2018)\nEmergence of Structured Behaviors from Curiosity-Based Intrinsic Motivation (arxiv 2018)\nTransfer Learning & Meta Learning\nAlgorithm or Model\n\u3010MAML\u3011Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks (arxiv 2017)\nOPTIMIZATION AS A MODEL FOR FEW-SHOT LEARNING (ICLR 2017)\n\u3010SNAIL\u3011A Simple Neural Attentive Meta-Learner (ICLR 2018)\nZero Shot Learning\nADAPT: Zero-Shot Adaptive Policy Transfer for Stochastic Dynamical Systems (isrr 2017)\nZero-Shot Object Detection (arxiv 2018)\nZero-shot Recognition via Semantic Embeddings and Knowledge Graphs (CVPR 2018)\nRobot\nHardware\nGrasping\nDataset\nDeep Grasp: Detection and Localization of Grasps with Deep Neural Networks (arxiv 2018)\nJacquard: A Large Scale Dataset for Robotic Grasp Detection (arxiv 2018)\nGrasping with RL\nGrasping Unknown Objects\nRanking the good points: A comprehensive method for humanoid robots to grasp unknown objects (International Conference on Advanced Robotics 2013)\nModel-Free Segmentation and Grasp Selection of Unknown Stacked Objects (ECCV 2014)\nPick and Place Without Geometric Object Models (ICRA 2018)\nGrasping in Cluttered Environment\nReal-Time 3D Segmentation of Cluttered Scenes for Robot Grasping (ICHR 2012)\n\u3010GPD\u3011High precision grasp pose detection in dense clutter (IROS 2016)\nRobotic Pick-and-Place of Novel Objects in Clutter with Multi-Affordance Grasping and Cross-Domain Image Matching (arxiv 2017)\nGrasping via Segmentation\nGrasping novel objects with depth segmentation (IROS 2010)\n3D scene segmentation for autonomous robot grasping (IROS 2012)\nGrasping Points Selection\nGP-GPIS-OPT: Grasp planning with shape uncertainty using Gaussian process implicit surfaces and Sequential Convex Programming (ICRA 2015)\nUsing Geometry to Detect Grasp Poses in 3D Point Clouds (ISRR 2015)\nDex-Net 1.0: A Cloud-Based Network of 3D Objects for Robust Grasp Planning Using a Multi-Armed Bandit Model with Correlated Rewards (ICRA 2016)\nDex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics (arxiv 2017)\nDex-Net 3.0: Computing Robust Robot Suction Grasp Targets in Point Clouds using a New Analytic Model and Deep Learning (arxiv 2017)\nMachine Vision\nActive Perception\nActive Perception: Interactive Manipulation for Improving Object Detection (Standford University Journal 2018)\nLearning Instance Segmentation by Interaction (CVPR 2018)\nMotion Prediction\n\u3010SE3-Net\u3011SE3-Nets: Learning Rigid Body Motion using Deep Neural Networks (ICRA 2017)\nInteractive Perception\nBetter Vision through Manipulation (Adaptive Behavior 2003)\nInteractive Perception: Closing the Gap Between Action and Perception (ICRA 2007)\nBIRTH OF THE OBJECT: DETECTION OF OBJECTNESS AND EXTRACTION OF OBJECT SHAPE THROUGH OBJECT\u2013ACTION COMPLEXES (International Journal of Humanoid Robotics 2008)\nInteractive Segmentation for Manipulation in Unstructured Environments (ICRA 2009)\nGenerating Object Hypotheses in Natural Scenes through Human-Robot Interaction (IROS 2011)\nSegmentation and learning of unknown objects through physical interaction (IEEE/RAS Int. Conf. on Humanoid Robots (Humanoids) 2011)\nClearing a Pile of Unknown Objects using Interactive Perception (?? 2012)\nSegmentation of Cluttered Scenes through Interactive Perception (ICRA 2012)\nInteractive singulation of objects from a pile (ICRA 2012)\nTracking-based Interactive Segmentation of Textureless Objects (ICRA 2013)\nProbabilistic Segmentation and Targeted Exploration of Objects in Cluttered Environments (Robotics, IEEE Transactions 2014)\nInteractive perception: Leveraging action in perception and perception in action (arxiv 2016)\nSegmenting objects through an autonomous agnostic exploration conducted by a robot (IRC 2017)\nLearning Instance Segmentation by Interaction (CVPR 2018)", "link": "https://github.com/shinshiner/Paper-Survey", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "the classification of articles may be inaccurate due to personal limited knowledge level.\ncontent\ndata structure\n-----> tree !!! \nmachine learning\nalgorithm\nneural network\nframework\nnetwork architecture\nneural network component\noptimizer\ncomputer vision\ndataset\n2d object detection\nalgorithm\n3d object detection\nalgorithm\n2d segmentation\nalgorithm\n3d segmentation\nalgorithm\n2d pose\nalgorithm\n3d pose\nalgorithm\nvideo\nsegmentation\nmotion representation\ngenerative model\nvae (variational auto-encoder)\nmodels\napplications\ngan (generative adversarial networks)\nmodels\napplications\nreinforcement learning\nenvironment\nalgorithm\nrl in games\ndistributional rl\ntransfer learning & meta learning\nalgorithm or model\nzero shot learning\nrobot\ndataset\nhardware\ngrasping\ngrasping with rl\ngrasping unknown objects\ngrasping in cluttered environment\ngrasping via segmentation\ngrasping points selection\nmachine vision\nactive perception\nmotion prediction\ninteractive perception\ndata structure\ntree\n\u3010kd-tree\u3011multidimensional binary search trees used for associative searching (1975)\n\u3010oc-tree\u3011octree encoding: a new technique for the representation, manipulation and display of arbitrary 3-d objects by computer (1980)\nmachine learning\nalgorithm\n\u3010svm\u3011least squares support vector machine classifiers (springer 1999)\n\u3010pca\u3011singular value decomposition and principal component analysis (springer 2003)\nneural network\nframework\ntorch7: a matlab-like environment for machine learning (nips workshop 2011)\ncaffe: convolutional architecture for fast feature embedding (arxiv 2014)\ntensorflow: a system for large-scale machine learning (osdi 2016)\nnetwork architecture\nlong short term memory network (journals 1997)\n\u3010vgg16\u3011very deep convolutional networks for large-scale image recognition (arxiv 2014)\ndueling network architectures for deep reinforcement learning (arxiv 2015)\n\u3010stn\u3011recurrent spatial transformer networks (arxiv 2015)\n\u3010fcn\u3011fully convolutional networks for semantic segmentation (cv-foundation 2015)\nfacenet: a unified embedding for face recognition and clustering (cv-foundation 2015)\n\u3010c3d\u3011learning spatiotemporal features with 3d convolutional networks (iccv 2015)\n\u3010resnet\u3011deep residual learning for image recognition (openaccess.thecvf 2016)\n\u3010gcn\u3011semi-supervised classification with graph convolutional networks (iclr 2017)\nnon-local neural networks (arxiv 2017)\n\u3010spn\u3011learning affinity via spatial propagation networks (nips 2017)\n\u3010capsule\u3011dynamic routing between capsules (nips 2017)\nmobilenets: efficient convolutional neural networks for mobile vision applications (arxiv 2017)\npacknet: adding multiple tasks to a single network by iterative pruning (arxiv 2017)\nposecnn: a convolutional neural network for 6d object pose estimation in cluttered scenes (arxiv 2017)\n\u3010psmnet\u3011pyramid stereo matching network (cvpr 2018)\nplanenet: piece-wise planar reconstruction from a single rgb image (cvpr 2018)\nsbnet: sparse blocks network for fast inference (arxiv 2018)\nneural network component\n\u3010relu\u3011rectified linear units improve restricted boltzmann machines (icml 2010)\ndropout: a simple way to prevent neural networks from overfitting (jmlr 2014)\nbatch normalization: accelerating deep network training by reducing internal covariate shift (icml 2015)\n\u3010center loss\u3011a discriminative feature learning approach for deep face recognition (eccv 2016)\ngroup normalization (arxiv 2018)\noptimizer\n\u3010asgd\u3011acceleration of stochastic approximation by averaging (journals 1992)\n\u3010adagrad\u3011adaptive subgradient methods for online learning and stochastic optimization (jmlr 2011)\nadadelta: an adaptive learning rate method (arxiv 2012)\n\u3010rmsprop\u3011generating sequences with recurrent neural networks (arxiv 2013)\nadam: a method for stochastic optimization (iclr 2015)\nlearning to learn by gradient descent by gradient descent (nips 2016)\ncomputer vision\ndataset\nimagenet: a large-scale hierarchical image database (cvpr 2009)\n\u3010nyuv2\u3011indoor segmentation and support inference from rgbd images (eccv 2012)\n\u3010kitti\u3011vision meets robotics: the kitti dataset (ijrr 2013)\n\u3010daimler urban segmentation\u3011efficient multi-cue scene segmentation (gcpr 2013)\n\u3010pascal context\u3011the role of context for object detection and semantic segmentation in the wild (cvpr 2014)\n\u3010pascal voc\u3011the pascal visual object classes challenge: a retrospective (ijcv 2014)\n\u3010coco\u3011microsoft coco: common objects in context (eccv 2014)\n\u3010ilsvrc\u3011imagenet large scale visual recognition challenge (ijcv 2015)\n\u3010modelnet40\u30113d shapenets: a deep representation for volumetric shapes (cvpr 2015) dataset is available at [website].\n\u3010cityscapes\u3011the cityscapes dataset for semantic urban scene understanding (cvpr 2016)\n\u3010s3dis\u30113d semantic parsing of largescale indoor spaces (cvpr 2016)\n\u3010scannet\u3011scannet: richly-annotated 3d reconstructions of indoor scenes (cvpr workshop 2018)\n2d object detection\nalgorithm\n\u3010r-cnn\u3011rich feature hierarchies for accurate object detection and semantic segmentation (cvpr 2014)\nfast r-cnn (iccv 2015)\nfaster r-cnn: towards real-time object detection with region proposal networks (nips 2015)\nssd: single shot multibox detector (eccv 2016)\nr-fcn: object detection via region-based fully convolutional networks (nips 2016)\n\u3010yolo\u3011you only look once: unified, real-time object detection (cvpr 2016)\nyolo9000: better, faster, stronger (openaccess.thecvf 2017)\nfssd: feature fusion single shot multibox detector (arxiv 2017)\n\u3010rfb-ssd\u3011receptive field block net for accurate and fast object detection (arxiv 2017)\n\u3010refinedet\u3011single-shot refinement neural network for object detection (arxiv 2017)\nmegdet: a large mini-batch object detector (arxiv 2017)\nlight-head r-cnn: in defense of two-stage object detector (arxiv 2017)\n\u3010retinanet / focal loss\u3011focal loss for dense object detection (iccv 2017)\nyolov3: an incremental improvement (?? 2018)\n3d object detection\nalgorithm\nssd-6d: making rgb-based 3d detection and 6d pose estimation great again (openaccess.thecvf 2017)\n\u3010frustum pointnets\u3011frustum pointnets for 3d object detection from rgb-d data (arxiv 2017)\n2d segmentation\nalgorithm\n\u3010u-net\u3011u-net: convolutional networks for biomedical image segmentation (arxiv 2015)\n\u3010deepmask\u3011learning to segment object candidates (arxiv 2015)\ninstance-aware semantic segmentation via multi-task network cascades (cvpr 2016)\nmask r-cnn (iccv 2017)\n\u3010w-net\u3011w-net: a deep model for fully unsupervised image segmentation (arxiv 2017)\n\u3010refinenet\u3011refinenet: multi-path refinement networks for high-resolution semantic segmentation (cvpr 2017)\nsemantic instance segmentation with a discriminative loss function (cvpr workshop 2017)\ndeep extreme cut: from extreme points to object segmentation (cvpr 2018)\nweakly supervised instance segmentation using class peak response (cvpr 2018)\n\u3010mask^x rcnn\u3011learning to segment every thing (cvpr 2018)\n3d segmentation\nalgorithm\ndeep learning with sets and point clouds (iclr 2017)\npointnet: deep learning on point sets for 3d classification and segmentation (cvpr 2017)\n3dcontextnet: k-d tree guided hierarchical learning of point clouds using local and global contextual cues (cvpr 2017)\npointnet++: deep hierarchical feature learning on point sets in a metric space (nips 2017)\nescape from cells: deep kd-networks for the recognition of 3d point cloud models (iccv 2017)\n\u3010pointsift\u3011pointsift: a sift-like network module for 3d point cloud semantic segmentation (arxiv 2018)\n\u3010kd-network\u3011pointcnn (arxiv 2018)\n\u3010so-net\u3011so-net: self-organizing network for point cloud analysis (cvpr 2018)\n\u3010sgpn\u3011sgpn: similarity group proposal network for 3d point cloud instance segmentation (cvpr 2018)\n\u3010dgcnn\u3011dynamic graph cnn for learning on point clouds\n2d pose\nalgorithm\n\u3010open pose\u3011realtime multi-person 2d pose estimation using part affinity fields (cvpr 2017)\n\u3010g-rmi\u3011towards accurate multi-person pose estimation in the wild (cvpr 2017)\njoint multi-person pose estimation and semantic part segmentation (cvpr 2017)\nrmpe: regional multi-person pose estimation (iccv 2017)\nvnect: real-time 3d human pose estimation with a single rgb camera (siggraph 2017)\n\u3010cpn\u3011cascaded pyramid network for multi-person pose estimation (cvpr 2018)\n3d pose\nalgorithm\ncoarse-to-fine volumetric prediction for single-image 3d human pose (cvpr 2017)\nvideo\nsegmentation\nlearning to segment instances in videos with spatial propagation network (cvpr workshop 2017)\nsegflow: joint learning for video object segmentation and optical flow (cvpr 2017)\nlearning features by watching objects move (cvpr 2017)\nmotion representation\n\u3010tvnet\u3011end-to-end learning of motion representation for video understanding (cvpr 2018)\ngenerative model\nvae (variational auto-encoder)\nmodels\napplications\nattribute2image: conditional image generation from visual attributes (eccv 2016)\ngan (generative adversarial networks)\nmodels\n\u3010gan\u3011generative adversarial networks (nips 2014)\n\u3010cgan\u3011conditional generative adversarial nets (arxiv 2014)\n\u3010aae\u3011adversarial autoencoders (arxiv 2015)\n\u3010dcgan\u3011unsupervised representation learning with deep convolutional generative adversarial networks (arxiv 2015)\napplications\nreinforcement learning\nenvironment\nmujoco: a physics engine for model-based control (international conference on intelligent robots and systems 2012)\nopenai gym (arxiv 2016)\n\u3010rllab\u3011benchmarking deep reinforcement learning for continuous control (jmlr 2016)\ndeepmind lab (arxiv 2016)\nstarcraft ii: a new challenge for reinforcement learning (arxiv 2017)\nmagent: a many-agent reinforcement learning platform for artificial collective intelligence (arxiv 2017)\nalgorithm\nq-learning (springer 1992)\n\u3010dqn\u3011playing atari with deep reinforcement learning (nips workshop 2013)\n\u3010dpg\u3011deterministic policy gradient algorithms (icml 2014)\n\u3010trpo\u3011trust region policy optimization (icml 2015)\n\u3010double-dqn\u3011deep reinforcement learning with double q-learning (aaai 2016)\n\u3010h-dqn\u3011hierarchical deep reinforcement learning: integrating temporal abstraction and intrinsic motivation (nips 2016)\n\u3010a3c\u3011asynchronous methods for deep reinforcement learning (icml 2016)\n\u3010ddpg\u3011continuous control with deep reinforcement learning (iclr 2016)\n\u3010naf\u3011continuous deep q-learning with model-based acceleration (arxiv 2016)\n\u3010acer\u3011sample efficient actor-critic with experience replay (arxiv 2016)\n\u3010gail\u3011generative adversarial imitation learning (nips 2016)\nneural episodic control (arxiv 2017)\nq-prop: sample-efficient policy gradient with an off-policy critic (iclr 2017)\n\u3010ppo\u3011proximal policy optimization algorithms (arxiv 2017)\nemergence of locomotion behaviours in rich environments (arxiv 2017)\n\u3010acktr\u3011scalable trust-region method for deep reinforcement learning using kronecker-factored approximation (nips 2017)\n\u3010her\u3011hindsight experience replay (nips 2017)\nnoisy networks for exploration (iclr 2018)\nrl in games\ncontrol of memory, active perception, and action in minecraft (arxiv 2016)\ndistributional rl\na distributional perspective on reinforcement learning (arxiv 2017)\ncuriosity-driven rl\ncuriosity-driven exploration by self-supervised prediction (icml 2017)\nintrinsically motivated model learning for developing curious robots (artificial intelligence 2017)\ncomputational theories of curiosity-driven learning (arxiv 2018)\nemergence of structured behaviors from curiosity-based intrinsic motivation (arxiv 2018)\ntransfer learning & meta learning\nalgorithm or model\n\u3010maml\u3011model-agnostic meta-learning for fast adaptation of deep networks (arxiv 2017)\noptimization as a model for few-shot learning (iclr 2017)\n\u3010snail\u3011a simple neural attentive meta-learner (iclr 2018)\nzero shot learning\nadapt: zero-shot adaptive policy transfer for stochastic dynamical systems (isrr 2017)\nzero-shot object detection (arxiv 2018)\nzero-shot recognition via semantic embeddings and knowledge graphs (cvpr 2018)\nrobot\nhardware\ngrasping\ndataset\ndeep grasp: detection and localization of grasps with deep neural networks (arxiv 2018)\njacquard: a large scale dataset for robotic grasp detection (arxiv 2018)\ngrasping with rl\ngrasping unknown objects\nranking the good points: a comprehensive method for humanoid robots to grasp unknown objects (international conference on advanced robotics 2013)\nmodel-free segmentation and grasp selection of unknown stacked objects (eccv 2014)\npick and place without geometric object models (icra 2018)\ngrasping in cluttered environment\nreal-time 3d segmentation of cluttered scenes for robot grasping (ichr 2012)\n\u3010gpd\u3011high precision grasp pose detection in dense clutter (iros 2016)\nrobotic pick-and-place of novel objects in clutter with multi-affordance grasping and cross-domain image matching (arxiv 2017)\ngrasping via segmentation\ngrasping novel objects with depth segmentation (iros 2010)\n3d scene segmentation for autonomous robot grasping (iros 2012)\ngrasping points selection\ngp-gpis-opt: grasp planning with shape uncertainty using gaussian process implicit surfaces and sequential convex programming (icra 2015)\nusing geometry to detect grasp poses in 3d point clouds (isrr 2015)\ndex-net 1.0: a cloud-based network of 3d objects for robust grasp planning using a multi-armed bandit model with correlated rewards (icra 2016)\ndex-net 2.0: deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics (arxiv 2017)\ndex-net 3.0: computing robust robot suction grasp targets in point clouds using a new analytic model and deep learning (arxiv 2017)\nmachine vision\nactive perception\nactive perception: interactive manipulation for improving object detection (standford university journal 2018)\nlearning instance segmentation by interaction (cvpr 2018)\nmotion prediction\n\u3010se3-net\u3011se3-nets: learning rigid body motion using deep neural networks (icra 2017)\ninteractive perception\nbetter vision through manipulation (adaptive behavior 2003)\ninteractive perception: closing the gap between action and perception (icra 2007)\nbirth of the object: detection of objectness and extraction of object shape through object\u2013action complexes (international journal of humanoid robotics 2008)\ninteractive segmentation for manipulation in unstructured environments (icra 2009)\ngenerating object hypotheses in natural scenes through human-robot interaction (iros 2011)\nsegmentation and learning of unknown objects through physical interaction (ieee/ras int. conf. on humanoid robots (humanoids) 2011)\nclearing a pile of unknown objects using interactive perception (?? 2012)\nsegmentation of cluttered scenes through interactive perception (icra 2012)\ninteractive singulation of objects from a pile (icra 2012)\ntracking-based interactive segmentation of textureless objects (icra 2013)\nprobabilistic segmentation and targeted exploration of objects in cluttered environments (robotics, ieee transactions 2014)\ninteractive perception: leveraging action in perception and perception in action (arxiv 2016)\nsegmenting objects through an autonomous agnostic exploration conducted by a robot (irc 2017)\nlearning instance segmentation by interaction (cvpr 2018)", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000928, "year": null}, {"Unnamed: 0": 1981, "autor": 961, "date": null, "content": "Behavior Trees\nA simple C# example of Behavior Trees + Editor.\nIntroduction\nBehavior Trees (BT) is a simple, scalable, modular solution that represents complex AI-behaviors and provides easy-to-maintain and configure logic.\nThey have been widely used in robotics and gaming since mid 2000, in particular, such game engines as Unreal, Unity, CryEngine use BT. BT have several advantages over FSM, such as Maintainability (nodes (or subtrees) can be designed independent from each other and can be independent changed), Scalability (complex BT can be decomposed into small sub-trees), Reusability (subtrees can be used as independent nodes)\nYou can learn more about Behavior Trees at the following links:\nBehavior tree (Wikipedia)\nBehavior trees for AI: How they work (Gamasutra)\nUnderstanding Behavior Trees\nBehavior Trees for Next-Gen Game AI (Video)\nAbout project\nThis project demonstrates the concept and working principle of the Behavior Trees. Therefore, I tried to make it as simple and laconic as possible. You can fork, adapt and extend the project to suit your needs.\nThe project includes BehaviorTrees library (C#, .Net Standard) with the main types of nodes: actions, conditions, composites and decorators (20 in total) as well as auxiliary classes. You can add your nodes by inheriting from existing ones. You can use ActionBase base class to create custom actions and use BaseEvent base class to create custom events. Trees can be serialized in json.\nBehaviorTreesEditor (.Net Framework WinForms) allows you to edit trees with a simple TreeList control, to save, to load and to run trees.\nExample 1\nBehaviorTrees.Example1 (.Net Standard) contains simple example of the Behavior Tree with custom node Move:\n[DataContract]\n[BTNode(\"Move\", \"Example\")]\npublic class Move : Node\n{\nPoint _position;\nbool _completed;\n[DataMember]\npublic Point Position\n{\nget { return _position; }\nset {\n_position = value;\nRoot.SendValueChanged(this);\n}\n}\npublic Move(Point pos)\n{\nPosition = pos;\n}\npublic override string NodeParameters => $\"Move To ({Position.X}, {Position.Y})\";\nprotected override void OnActivated()\n{\nbase.OnActivated();\nLog.Write($\"{Owner.Name} moving to {Position}\");\nTask.Delay(1000).ContinueWith(_ => _completed = true);\n}\nprotected override void OnDeactivated()\n{\nbase.OnDeactivated();\nLog.Write($\"{Owner.Name} moving completed \");\n}\nprotected override ExecutingStatus OnExecuted()\n{\nreturn _completed ? ExecutingStatus.Success : ExecutingStatus.Running;\n}\n}\nBehaviorTrees.Example1 demonstrates Behavior Tree creation by using TreeBuilder:\nvar exampleBT = new TreeBuilder<Node>(new Sequence())\n.AddWithChild(new Loop(3))\n.AddWithChild(new Sequence())\n.Add(new Move(new Point(0, 0)))\n.Add(new Move(new Point(20, 0)))\n.AddWithChild(new Delay(2))\n.Add(new Move(new Point(0, 20)))\n.Up()\n.Up()\n.Up()\n.ToTree();\nExample 2 - IronPython nodes\nThe library BehaviorTrees.IronPython is an example of scripting language integration into the Behavior Tree. As an example, two nodes are added: ScriptedAction and ScriptedCondition. The script can be edited both with help of the PropertyGrid and using the special editor implemented in IronPythonEditor project.\nYou can extend the example and add your own API, import your types, both from the host application and from other IronPython scripts. Based on this example you can integrate another scripting language you need.\nOther BT implementations on the Github:\nBehaviorTree.CPP (C++)\nFluent Behavior Tree (Typescript)\nGo Behave (Go)\nFluid Behavior Tree (Unity, C#)\nThird party in this project\nTreeView with Columns by jkristia\nGeneric Tree container by peterchen\nVisual Studio 2015 Image Library. Microsoft Software License Terms\nIronPython and FastColoredTextBox in Example 2\nLicense\nCopyright (c) 2015-2019 Eugeny Novikov. Code under MIT license.", "link": "https://github.com/EugenyN/BehaviorTrees", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "behavior trees\na simple c# example of behavior trees + editor.\nintroduction\nbehavior trees (bt) is a simple, scalable, modular solution that represents complex ai-behaviors and provides easy-to-maintain and configure logic.\nthey have been widely used in robotics and gaming since mid 2000, in particular, such game engines as unreal, unity, cryengine use bt. bt have several advantages over fsm, such as maintainability (nodes (or subtrees) can be designed independent from each other and can be independent changed), scalability (complex bt can be decomposed into small sub-trees), reusability (subtrees can be used as independent nodes)\nyou can learn more about behavior trees at the following links:\nbehavior -----> tree !!!  (wikipedia)\nbehavior trees for ai: how they work (gamasutra)\nunderstanding behavior trees\nbehavior trees for next-gen game ai (video)\nabout project\nthis project demonstrates the concept and working principle of the behavior trees. therefore, i tried to make it as simple and laconic as possible. you can fork, adapt and extend the project to suit your needs.\nthe project includes behaviortrees library (c#, .net standard) with the main types of nodes: actions, conditions, composites and decorators (20 in total) as well as auxiliary classes. you can add your nodes by inheriting from existing ones. you can use actionbase base class to create custom actions and use baseevent base class to create custom events. trees can be serialized in json.\nbehaviortreeseditor (.net framework winforms) allows you to edit trees with a simple treelist control, to save, to load and to run trees.\nexample 1\nbehaviortrees.example1 (.net standard) contains simple example of the behavior tree with custom node move:\n[datacontract]\n[btnode(\"move\", \"example\")]\npublic class move : node\n{\npoint _position;\nbool _completed;\n[datamember]\npublic point position\n{\nget { return _position; }\nset {\n_position = value;\nroot.sendvaluechanged(this);\n}\n}\npublic move(point pos)\n{\nposition = pos;\n}\npublic override string nodeparameters => $\"move to ({position.x}, {position.y})\";\nprotected override void onactivated()\n{\nbase.onactivated();\nlog.write($\"{owner.name} moving to {position}\");\ntask.delay(1000).continuewith(_ => _completed = true);\n}\nprotected override void ondeactivated()\n{\nbase.ondeactivated();\nlog.write($\"{owner.name} moving completed \");\n}\nprotected override executingstatus onexecuted()\n{\nreturn _completed ? executingstatus.success : executingstatus.running;\n}\n}\nbehaviortrees.example1 demonstrates behavior tree creation by using treebuilder:\nvar examplebt = new treebuilder<node>(new sequence())\n.addwithchild(new loop(3))\n.addwithchild(new sequence())\n.add(new move(new point(0, 0)))\n.add(new move(new point(20, 0)))\n.addwithchild(new delay(2))\n.add(new move(new point(0, 20)))\n.up()\n.up()\n.up()\n.totree();\nexample 2 - ironpython nodes\nthe library behaviortrees.ironpython is an example of scripting language integration into the behavior tree. as an example, two nodes are added: scriptedaction and scriptedcondition. the script can be edited both with help of the propertygrid and using the special editor implemented in ironpythoneditor project.\nyou can extend the example and add your own api, import your types, both from the host application and from other ironpython scripts. based on this example you can integrate another scripting language you need.\nother bt implementations on the github:\nbehaviortree.cpp (c++)\nfluent behavior tree (typescript)\ngo behave (go)\nfluid behavior tree (unity, c#)\nthird party in this project\ntreeview with columns by jkristia\ngeneric tree container by peterchen\nvisual studio 2015 image library. microsoft software license terms\nironpython and fastcoloredtextbox in example 2\nlicense\ncopyright (c) 2015-2019 eugeny novikov. code under mit license.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000961, "year": null}, {"Unnamed: 0": 1985, "autor": 965, "date": null, "content": "i_see_pee\ni_see_pee is a 2d localization package. At its core it uses an icp algorithm to determine the current robot position. Its main advantage is that it performs one optimization step linear in time: O(N).\nWhats the difference\nThere are many stellar packages for localization which rely on icp.\nlibpointmatcher\ncartographer\ndynamic_robot_localization\nThese are generic localization/slam packages. They support 2d and 3d environment, offer rich features for sensor processing and what not. Which is nice. i_see_pee is different. It just offers 2d localization. But its smaller. And its way faster.\nThe speed\nA typical icp algorithm consists of two steps:\nk nearest neighbors matching\nsolving the least squares problem.\nThe least squares problem is typically O(N), which is nice. The k nearest neighbors is a little more expensive. Typically the lookup is implemented using a knn-tree. The lookup there comes at O(D log M), where D is the dimension and M the size of the occupied points on the map.\nWhen aiming for a generic setup, there is little one can do. i_see_pee leverages the fact that all grid_maps are discrete and that the registration cloud (map) is known in advance. It pre-computes for cells close to walls the k nearest neighbors and stores them in a hash-table. When a sensor reading falls in one of these cells, all its k nearest neighbors can be retrieved in linear time.\nCompiling\nYou will require Eigen for the compilation. Besides that the package can be compiled as a standard ros-package.\nIf you use catkin tools you can compile the package with:\ncd catkin_ws/src\ngit clone https://github.com/dorezyuk/i_see_pee.git\ncatkin build i_see_pee -DCMAKE_BUILD_TYPE=Release\nHint: make sure to compile the library as release\nDepending on your processor you should enable vectorization for Eigen.\nConfiguring\nSubscriptions\nmap/topic\ntopic under which the static map is published\ntype: string\ndefault: map\nmessage_type: nav_msgs::OccupancyGrid\nscan/topic\ntopic under which the sensor data is published\ntype: string\ndefault: scan\nmessage_type: sensor_msgs::LaserScan\nFrames\nodom/map_frame\nframe_id which defines the global frame.\ntype: string\ndefault: map\nodom/base_frame\nframe_id which defines the base frame. You can use either only odometry, which is provided by the differential_drive controller or fuse imu and odometry to improve the initial guess.\ntype: string\ndefault: base_link\nodom/base_frame\nframe_id of the odom frame of the robot.\ntype: string\ndefault: odom\nMapping parameters\nmap/k\nnumber of neighbors to consider\ntype: int\ndefault: 5 (min: 1, max: 10)\nmap/radius\nnumber of cells to consider from an occupied cell. This massively determines the size of the hash map\ntype: int\ndefault: 5 (min: 0, max: 10)\nOptimization parameters\nicp/t_norm\ntranslation norm in meter defining convergence of the icp: if the generated transform is smaller than icp/t_norm and icp/r_norm the icp is said to be converged.\ntype: float\ndefault: 0.01 (min: 0.01)\nicp/r_norm\nrotation norm defining convergence (see [icp/t_norm])\ntype: float\ndefault: 0.01 (min: 0.01)\nicp/max_iter\ndefines the maximum number of iterations of the icp optimization. The looping stops, if the norm of the generated correction transform is smaller then icp/t_norm and icp/r_norm\ntype: int\ndefault: 100 (min: 1, max: 100)\nicp/stride\nin order to speed up the computation one can reduce the point cloud size. Applying a stride of 'n' will take only every nth point into consideration. The starting point defines hence which points will be drawn. It is drawn at random.\ntype: int\ndefault: 3 (min: 1, max: 100)\nUnderstanding\nWhen you want to play with the code, you should be lucky: i_see_pee is very small (less then 1000 loc currently). As you might find, everything related to specific ros-types (msgs, tf) is shielded away behind an interface.\nThe main files are undoubtedly i_see_pee.hpp/cpp. They contain several namespaces:\nmap: everything related to the static map. This includes a ros-interface and the algorithm to extract knns from a grid and to store them in a hash-map\nscan: everything related to the sensor data. This includes the ros-interface to sensor_msgs::LaserScan and some caching to convert it to some internal representation.\nodom: everything related to frames/odometry.\nicp: everything related to the math part (which was heavily borrowed from libpointmatcher)\nLet me know how your experience was with the package.", "link": "https://github.com/dorezyuk/i_see_pee", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "i_see_pee\ni_see_pee is a 2d localization package. at its core it uses an icp algorithm to determine the current robot position. its main advantage is that it performs one optimization step linear in time: o(n).\nwhats the difference\nthere are many stellar packages for localization which rely on icp.\nlibpointmatcher\ncartographer\ndynamic_robot_localization\nthese are generic localization/slam packages. they support 2d and 3d environment, offer rich features for sensor processing and what not. which is nice. i_see_pee is different. it just offers 2d localization. but its smaller. and its way faster.\nthe speed\na typical icp algorithm consists of two steps:\nk nearest neighbors matching\nsolving the least squares problem.\nthe least squares problem is typically o(n), which is nice. the k nearest neighbors is a little more expensive. typically the lookup is implemented using a knn------> tree !!! . the lookup there comes at o(d log m), where d is the dimension and m the size of the occupied points on the map.\nwhen aiming for a generic setup, there is little one can do. i_see_pee leverages the fact that all grid_maps are discrete and that the registration cloud (map) is known in advance. it pre-computes for cells close to walls the k nearest neighbors and stores them in a hash-table. when a sensor reading falls in one of these cells, all its k nearest neighbors can be retrieved in linear time.\ncompiling\nyou will require eigen for the compilation. besides that the package can be compiled as a standard ros-package.\nif you use catkin tools you can compile the package with:\ncd catkin_ws/src\ngit clone https://github.com/dorezyuk/i_see_pee.git\ncatkin build i_see_pee -dcmake_build_type=release\nhint: make sure to compile the library as release\ndepending on your processor you should enable vectorization for eigen.\nconfiguring\nsubscriptions\nmap/topic\ntopic under which the static map is published\ntype: string\ndefault: map\nmessage_type: nav_msgs::occupancygrid\nscan/topic\ntopic under which the sensor data is published\ntype: string\ndefault: scan\nmessage_type: sensor_msgs::laserscan\nframes\nodom/map_frame\nframe_id which defines the global frame.\ntype: string\ndefault: map\nodom/base_frame\nframe_id which defines the base frame. you can use either only odometry, which is provided by the differential_drive controller or fuse imu and odometry to improve the initial guess.\ntype: string\ndefault: base_link\nodom/base_frame\nframe_id of the odom frame of the robot.\ntype: string\ndefault: odom\nmapping parameters\nmap/k\nnumber of neighbors to consider\ntype: int\ndefault: 5 (min: 1, max: 10)\nmap/radius\nnumber of cells to consider from an occupied cell. this massively determines the size of the hash map\ntype: int\ndefault: 5 (min: 0, max: 10)\noptimization parameters\nicp/t_norm\ntranslation norm in meter defining convergence of the icp: if the generated transform is smaller than icp/t_norm and icp/r_norm the icp is said to be converged.\ntype: float\ndefault: 0.01 (min: 0.01)\nicp/r_norm\nrotation norm defining convergence (see [icp/t_norm])\ntype: float\ndefault: 0.01 (min: 0.01)\nicp/max_iter\ndefines the maximum number of iterations of the icp optimization. the looping stops, if the norm of the generated correction transform is smaller then icp/t_norm and icp/r_norm\ntype: int\ndefault: 100 (min: 1, max: 100)\nicp/stride\nin order to speed up the computation one can reduce the point cloud size. applying a stride of 'n' will take only every nth point into consideration. the starting point defines hence which points will be drawn. it is drawn at random.\ntype: int\ndefault: 3 (min: 1, max: 100)\nunderstanding\nwhen you want to play with the code, you should be lucky: i_see_pee is very small (less then 1000 loc currently). as you might find, everything related to specific ros-types (msgs, tf) is shielded away behind an interface.\nthe main files are undoubtedly i_see_pee.hpp/cpp. they contain several namespaces:\nmap: everything related to the static map. this includes a ros-interface and the algorithm to extract knns from a grid and to store them in a hash-map\nscan: everything related to the sensor data. this includes the ros-interface to sensor_msgs::laserscan and some caching to convert it to some internal representation.\nodom: everything related to frames/odometry.\nicp: everything related to the math part (which was heavily borrowed from libpointmatcher)\nlet me know how your experience was with the package.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000965, "year": null}, {"Unnamed: 0": 2037, "autor": 1017, "date": null, "content": "Deep Learning SotA\nNote: This repository is no longer under support. Please refer to websites such as Paper with Code, which provide more comprehensive and up-to-date information on SOTA models. This repository is in archive mode now.\nThis repository lists the state-of-the-art results for mainstream deep learning tasks. We do our best to keep it up to date. If you do find a task's SotA result is outdated or missing, please raise an issue (with: title of paper, dataset, metric, source code, and year).\nThis summary is categorized into:\nComputer Vision\nSpeech\nNLP\nContact\nComputer Vision\nClassification\nDataset Type Top-1 accuracy Method Paper Code\nImageNet ResNet-50 78.35% ResNet-50 + DropBlock + label smoothing DropBlock: A Regularization Method for Convolutional Neural Networks\nImageNet Single model 82.52% AmoebaNet-B + DropBlock DropBlock: A Regularization Method for Convolutional Neural Networks\nObject Detection\nDataset Type AP Method Paper Code\nMS-COCO 2017 ResNet-101 43.4 D-RFCN + SNIP + ResNet-101 An Analysis of Scale Invariance in Object Detection - SNIP\nMS-COCO 2017 Single model 45.7 D-RFCN + SNIP + DPN-98 An Analysis of Scale Invariance in Object Detection - SNIP\nInstance Segmentation\nDataset Type AP Method Paper Code\nMS-COCO 2018 Ensemble 48.6 mmdet + FishNet, 5 models - PyTorch\nVisual Question Answering\nDataset Type Score Method Paper Code\nVQA Ensemble 72.41 Pythia Pythia v0.1: The Winning Entry to the VQA Challenge 2018 PyTorch\nPerson Re-identification\nDataset Type Rank-1 accuracy Method Paper Code\nMarket-1501 Supervised single-query 91.2% Pixel-level attention + region-level attention + joint feature learning Harmonious Attention Network for Person Re-Identification\nMarket-1501 Supervised multi-query 93.8% Pixel-level attention + region-level attention + joint feature learning + multi-query Harmonious Attention Network for Person Re-Identification\nDukeMTMC-reID Supervised single-query 85.95% SPReID Human Semantic Parsing for Person Re-identification\nNLP\nLanguage Modelling\nDataset Type Perplexity Method Paper Code\nPenn Tree Bank 47.69 MoS Breaking the Softmax Bottleneck: A High-Rank RNN Language Model PyTorch\nWikiText-2 40.68 MoS Breaking the Softmax Bottleneck: A High-Rank RNN Language Model PyTorch\nMachine Translation\nDataset Type BLEU Method Paper Code\nWMT 2014 English-to-French 41.4 Weighted Transformer Weighted Transformer Network for Machine Translation\nWMT 2014 English-to-German 28.9 Weighted Transformer Weighted Transformer Network for Machine Translation\nText Classification\nDataset Type Accuracy Method Paper Code\nYelp 68.6% Learning Structured Text Representations TensorFlow\nNatural Language Inference\nDataset Type Accuracy Method Paper Code\nStanford Natural Language Inference (SNLI) Single 89.9% GPT Improving Language Understanding by Generative Pre-Training\nStanford Natural Language Inference (SNLI) Emsemble 90.1% Semantic Sentence Matching with Densely-Connected Recurrent and Co-Attentive Information\nMultiNLI Emsemble 86.7% BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\nQuestion Answering\nDataset Type F1 Method Paper Code\nSQuAD 2.0 Single model 83.061 BERT-large BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\nNamed Entity Recognition\nDataset Type F1 Method Paper Code\nCoNLL-2003 Single model 92.8 BERT-large BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\nSpeech\nAcoustic Speech Recognition\nDataset Type WER Method Paper Code\nSwitchboard Hub5'00 Ensemble 5.0 biLSTM + CNN + Dense, 8 models The CAPIO 2017 Conversational Speech Recognition System\nContact\nEmail: cmsflash99@gmail.com", "link": "https://github.com/cmsflash/deep-learning-sota", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tree", "selectorShort": "tree", "MarkedSent": "deep learning sota\nnote: this repository is no longer under support. please refer to websites such as paper with code, which provide more comprehensive and up-to-date information on sota models. this repository is in archive mode now.\nthis repository lists the state-of-the-art results for mainstream deep learning tasks. we do our best to keep it up to date. if you do find a task's sota result is outdated or missing, please raise an issue (with: title of paper, dataset, metric, source code, and year).\nthis summary is categorized into:\ncomputer vision\nspeech\nnlp\ncontact\ncomputer vision\nclassification\ndataset type top-1 accuracy method paper code\nimagenet resnet-50 78.35% resnet-50 + dropblock + label smoothing dropblock: a regularization method for convolutional neural networks\nimagenet single model 82.52% amoebanet-b + dropblock dropblock: a regularization method for convolutional neural networks\nobject detection\ndataset type ap method paper code\nms-coco 2017 resnet-101 43.4 d-rfcn + snip + resnet-101 an analysis of scale invariance in object detection - snip\nms-coco 2017 single model 45.7 d-rfcn + snip + dpn-98 an analysis of scale invariance in object detection - snip\ninstance segmentation\ndataset type ap method paper code\nms-coco 2018 ensemble 48.6 mmdet + fishnet, 5 models - pytorch\nvisual question answering\ndataset type score method paper code\nvqa ensemble 72.41 pythia pythia v0.1: the winning entry to the vqa challenge 2018 pytorch\nperson re-identification\ndataset type rank-1 accuracy method paper code\nmarket-1501 supervised single-query 91.2% pixel-level attention + region-level attention + joint feature learning harmonious attention network for person re-identification\nmarket-1501 supervised multi-query 93.8% pixel-level attention + region-level attention + joint feature learning + multi-query harmonious attention network for person re-identification\ndukemtmc-reid supervised single-query 85.95% spreid human semantic parsing for person re-identification\nnlp\nlanguage modelling\ndataset type perplexity method paper code\npenn -----> tree !!!  bank 47.69 mos breaking the softmax bottleneck: a high-rank rnn language model pytorch\nwikitext-2 40.68 mos breaking the softmax bottleneck: a high-rank rnn language model pytorch\nmachine translation\ndataset type bleu method paper code\nwmt 2014 english-to-french 41.4 weighted transformer weighted transformer network for machine translation\nwmt 2014 english-to-german 28.9 weighted transformer weighted transformer network for machine translation\ntext classification\ndataset type accuracy method paper code\nyelp 68.6% learning structured text representations tensorflow\nnatural language inference\ndataset type accuracy method paper code\nstanford natural language inference (snli) single 89.9% gpt improving language understanding by generative pre-training\nstanford natural language inference (snli) emsemble 90.1% semantic sentence matching with densely-connected recurrent and co-attentive information\nmultinli emsemble 86.7% bert: pre-training of deep bidirectional transformers for language understanding\nquestion answering\ndataset type f1 method paper code\nsquad 2.0 single model 83.061 bert-large bert: pre-training of deep bidirectional transformers for language understanding\nnamed entity recognition\ndataset type f1 method paper code\nconll-2003 single model 92.8 bert-large bert: pre-training of deep bidirectional transformers for language understanding\nspeech\nacoustic speech recognition\ndataset type wer method paper code\nswitchboard hub5'00 ensemble 5.0 bilstm + cnn + dense, 8 models the capio 2017 conversational speech recognition system\ncontact\nemail: cmsflash99@gmail.com", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7001017, "year": null}], "name": "treerobotics"}
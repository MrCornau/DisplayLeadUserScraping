{"interestingcomments": [{"Unnamed: 0": 1048, "autor": 28, "date": null, "content": "evo\nPython package for the evaluation of odometry and SLAM\nLinux / macOS / Windows / ROS\nThis package provides executables and a small library for handling, evaluating and comparing the trajectory output of odometry and SLAM algorithms.\nSupported trajectory formats:\n'TUM' trajectory files\n'KITTI' pose files\n'EuRoC MAV' (.csv groundtruth and TUM trajectory file)\nROS bagfile with geometry_msgs/PoseStamped, geometry_msgs/TransformStamped, geometry_msgs/PoseWithCovarianceStamped or nav_msgs/Odometry topics or TF messages\nSee here for more infos about the formats.\nWhy?\nevo has several advantages over other public benchmarking tools:\ncommon tools for different formats\nalgorithmic options for association, alignment, scale adjustment for monocular SLAM etc.\nflexible options for output, plotting or export (e.g. LaTeX plots or Excel tables)\na powerful, configurable CLI that can cover many use cases\nmodular core and tools libraries for custom extensions\nfaster than other established Python-based tools (see here)\nWhat it's not: a 1-to-1 re-implementation of a particular evaluation protocol tailored to a specific dataset.\nInstallation / Upgrade\nInstallation is easy-peasy if you're familiar with this: https://xkcd.com/1987/#\nevo supports Python 3.6+. The last evo version that supports Python 2.7 is 1.12.0. If you want to use the ROS bagfile interface, first check which Python version is used by your ROS installation and install accordingly. You might also want to use a virtual environment.\nFrom PyPi\nIf you just want to use the executables of the latest release version, the easiest way is to run:\npip install evo --upgrade --no-binary evo\nThis will download the package and its dependencies from PyPI and install or upgrade them. Depending on your OS, you might be able to use pip2 or pip3 to specify the Python version you want. Tab completion for Bash terminals is supported via the argcomplete package on most UNIX systems - open a new shell after the installation to use it (without --no-binary evo the tab completion might not be installed properly). If you want, you can subscribe to new releases via https://libraries.io/pypi/evo.\nFrom Source\nRun this in the repository's base folder:\npip install --editable . --upgrade --no-binary evo\nDependencies\nPython packages\nevo has some required dependencies that are automatically resolved during installation with pip. They are specified in the install_requires part of the setup.py file.\nPyQt5 (optional)\nPyQt5 will give you the enhanced GUI for plot figures from the \"Qt5Agg\" matplotlib backend (otherwise: \"TkAgg\"). If PyQt5 is already installed when installing this package, it will be used as a default (see evo_config show). To change the plot backend afterwards, run evo_config set plot_backend Qt5Agg.\nROS (optional)\nAll ROS-related features like reading bagfiles require a ROS installation, see here. We are testing this package with ROS Noetic. Previous versions (<= 1.12.0) work with Melodic, Kinetic and Indigo.\nCommand Line Interface\nAfter installation with setup.py or from pip, the following executables can be called globally from your command-line:\nMetrics:\nevo_ape - absolute pose error\nevo_rpe - relative pose error\nTools:\nevo_traj - tool for analyzing, plotting or exporting one or more trajectories\nevo_res - tool for comparing one or multiple result files from evo_ape or evo_rpe\nevo_fig - (experimental) tool for re-opening serialized plots (saved with --serialize_plot)\nevo_config - tool for global settings and config file manipulation\nCall the commands with --help to see the options, e.g. evo_ape --help. Tab-completion of command line parameters is available on UNIX-like systems.\nMore documentation Check out the Wiki on GitHub.\nExample Workflow\nThere are some example trajectories in the source folder in test/data.\n1.) Plot multiple trajectories\nHere, we plot two KITTI pose files and the ground truth using evo_traj:\ncd test/data\nevo_traj kitti KITTI_00_ORB.txt KITTI_00_SPTAM.txt --ref=KITTI_00_gt.txt -p --plot_mode=xz\n2.) Run a metric on trajectories\nFor example, here we calculate the absolute pose error for two trajectories from ORB-SLAM and S-PTAM using evo_ape (KITTI_00_gt.txt is the reference (ground truth)) and plot and save the individual results to .zip files for evo_res:\nFirst trajectory (ORB Stereo):\nmkdir results\nevo_ape kitti KITTI_00_gt.txt KITTI_00_ORB.txt -va --plot --plot_mode xz --save_results results/ORB.zip\nSecond trajectory (S-PTAM):\nevo_ape kitti KITTI_00_gt.txt KITTI_00_SPTAM.txt -va --plot --plot_mode xz --save_results results/SPTAM.zip\n3.) Process multiple results from a metric\nevo_res can be used to compare multiple result files from the metrics, i.e.:\nprint infos and statistics (default)\nplot the results\nsave the statistics in a table\nHere, we use the results from above to generate a plot and a table:\nevo_res results/*.zip -p --save_table results/table.csv\nIPython / Jupyter Resources\nFor an interactive source code documentation, open the Jupyter notebook metrics_tutorial.ipynb in the notebooks folder of the repository. More infos on Jupyter notebooks: see here\nIf you have IPython installed, you can launch an IPython shell with a custom evo profile with the command evo_ipython.\nContributing Utilities\nA few \"inoffical\" scripts for special use-cases are collected in the contrib/ directory of the repository. They are inofficial in the sense that they don't ship with the package distribution and thus aren't regularly tested in continuous integration.\nTrouble\n\"\ud83d\ude31, this piece of \ud83d\udca9 software doesn't do what I want!!1!1!!\"\nFirst aid:\nappend -h/ --help to your command\ncheck the Wiki\ncheck the previous issues\nopen a new issue\nContributing\nPatches are welcome, preferably as pull requests.\nLicense\nGPL-3.0 or later\nIf you use this package for your research, a footnote with the link to this repository is appreciated: github.com/MichaelGrupp/evo.\n...or, for citation with BibTeX:\n@misc{grupp2017evo,\ntitle={evo: Python package for the evaluation of odometry and SLAM.},\nauthor={Grupp, Michael},\nhowpublished={\\url{https://github.com/MichaelGrupp/evo}},\nyear={2017}\n}", "link": "https://github.com/MichaelGrupp/evo", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "evo\npython package for the evaluation of odometry and slam\nlinux / macos / windows / ros\nthis package provides executables and a small library for handling, evaluating and comparing the trajectory output of odometry and slam algorithms.\nsupported trajectory formats:\n'tum' trajectory files\n'kitti' pose files\n'euroc mav' (.csv groundtruth and tum trajectory file)\nros bagfile with geometry_msgs/posestamped, geometry_msgs/transformstamped, geometry_msgs/posewithcovariancestamped or nav_msgs/odometry topics or tf messages\nsee here for more infos about the formats.\nwhy?\nevo has several advantages over other public benchmarking tools:\ncommon tools for different formats\nalgorithmic options for association, alignment, scale adjustment for monocular slam etc.\nflexible options for output, plotting or export (e.g. latex plots or excel tables)\na powerful, configurable cli that can cover many use cases\nmodular core and tools libraries for custom extensions\nfaster than other established python-based tools (see here)\nwhat it's not: a 1-to-1 re-implementation of a particular evaluation protocol tailored to a specific dataset.\ninstallation / upgrade\ninstallation is easy-peasy if you're familiar with this: https://xkcd.com/1987/#\nevo supports python 3.6+. the last evo version that supports python 2.7 is 1.12.0. if you want to use the ros bagfile interface, first check which python version is used by your ros installation and install accordingly. you might also want to use a virtual environment.\nfrom pypi\nif you just want to use the executables of the latest release version, the easiest way is to run:\npip install evo --upgrade --no-binary evo\nthis will download the package and its dependencies from pypi and install or upgrade them. depending on your os, you might be able to use pip2 or pip3 to specify the python version you want. tab completion for bash terminals is supported via the argcomplete package on most unix systems - open a new shell after the installation to use it (without --no-binary evo the tab completion might not be installed properly). if you want, you can subscribe to new releases via https://libraries.io/pypi/evo.\nfrom source\nrun this in the repository's base folder:\npip install --editable . --upgrade --no-binary evo\ndependencies\npython packages\nevo has some required dependencies that are automatically resolved during installation with pip. they are specified in the install_requires part of the setup.py file.\npyqt5 (optional)\npyqt5 will give you the enhanced gui for -----> plot !!!  figures from the \"qt5agg\" matplotlib backend (otherwise: \"tkagg\"). if pyqt5 is already installed when installing this package, it will be used as a default (see evo_config show). to change the plot backend afterwards, run evo_config set plot_backend qt5agg.\nros (optional)\nall ros-related features like reading bagfiles require a ros installation, see here. we are testing this package with ros noetic. previous versions (<= 1.12.0) work with melodic, kinetic and indigo.\ncommand line interface\nafter installation with setup.py or from pip, the following executables can be called globally from your command-line:\nmetrics:\nevo_ape - absolute pose error\nevo_rpe - relative pose error\ntools:\nevo_traj - tool for analyzing, plotting or exporting one or more trajectories\nevo_res - tool for comparing one or multiple result files from evo_ape or evo_rpe\nevo_fig - (experimental) tool for re-opening serialized plots (saved with --serialize_plot)\nevo_config - tool for global settings and config file manipulation\ncall the commands with --help to see the options, e.g. evo_ape --help. tab-completion of command line parameters is available on unix-like systems.\nmore documentation check out the wiki on github.\nexample workflow\nthere are some example trajectories in the source folder in test/data.\n1.) plot multiple trajectories\nhere, we plot two kitti pose files and the ground truth using evo_traj:\ncd test/data\nevo_traj kitti kitti_00_orb.txt kitti_00_sptam.txt --ref=kitti_00_gt.txt -p --plot_mode=xz\n2.) run a metric on trajectories\nfor example, here we calculate the absolute pose error for two trajectories from orb-slam and s-ptam using evo_ape (kitti_00_gt.txt is the reference (ground truth)) and plot and save the individual results to .zip files for evo_res:\nfirst trajectory (orb stereo):\nmkdir results\nevo_ape kitti kitti_00_gt.txt kitti_00_orb.txt -va --plot --plot_mode xz --save_results results/orb.zip\nsecond trajectory (s-ptam):\nevo_ape kitti kitti_00_gt.txt kitti_00_sptam.txt -va --plot --plot_mode xz --save_results results/sptam.zip\n3.) process multiple results from a metric\nevo_res can be used to compare multiple result files from the metrics, i.e.:\nprint infos and statistics (default)\nplot the results\nsave the statistics in a table\nhere, we use the results from above to generate a plot and a table:\nevo_res results/*.zip -p --save_table results/table.csv\nipython / jupyter resources\nfor an interactive source code documentation, open the jupyter notebook metrics_tutorial.ipynb in the notebooks folder of the repository. more infos on jupyter notebooks: see here\nif you have ipython installed, you can launch an ipython shell with a custom evo profile with the command evo_ipython.\ncontributing utilities\na few \"inoffical\" scripts for special use-cases are collected in the contrib/ directory of the repository. they are inofficial in the sense that they don't ship with the package distribution and thus aren't regularly tested in continuous integration.\ntrouble\n\"\ud83d\ude31, this piece of \ud83d\udca9 software doesn't do what i want!!1!1!!\"\nfirst aid:\nappend -h/ --help to your command\ncheck the wiki\ncheck the previous issues\nopen a new issue\ncontributing\npatches are welcome, preferably as pull requests.\nlicense\ngpl-3.0 or later\nif you use this package for your research, a footnote with the link to this repository is appreciated: github.com/michaelgrupp/evo.\n...or, for citation with bibtex:\n@misc{grupp2017evo,\ntitle={evo: python package for the evaluation of odometry and slam.},\nauthor={grupp, michael},\nhowpublished={\\url{https://github.com/michaelgrupp/evo}},\nyear={2017}\n}", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000028, "year": null}, {"Unnamed: 0": 1088, "autor": 68, "date": null, "content": "Robotics Toolbox for MATLAB\u00ae release 10\nFor support please use the Google group forum rather than GitHub issues. There are more people participating and you'll likely get a quicker response. Checkout the FAQ before you post a question, it covers common problems that arise with incorrect MATLAB paths.\nSynopsis\nThis toolbox brings robotics specific functionality to MATLAB, exploiting the native capabilities of MATLAB (linear algebra, portability, graphics).\nThe Toolbox uses a very general method of representing the kinematics and dynamics of serial-link manipulators as MATLAB\u00ae objects \u2013 robot objects can be created by the user for any serial-link manipulator and a number of examples are provided for well known robots from Kinova, Universal Robotics, Rethink as well as classical robots such as the Puma 560 and the Stanford arm.\nThe toolbox also supports mobile robots with functions for robot motion models (unicycle, bicycle), path planning algorithms (bug, distance transform, D*, PRM), kinodynamic planning (lattice, RRT), localization (EKF, particle filter), map building (EKF) and simultaneous localization and mapping (EKF), and a Simulink model a of non-holonomic vehicle. The Toolbox also including a detailed Simulink model for a quadrotor flying robot.\nAdvantages of the Toolbox are that:\nthe code is mature and provides a point of comparison for other implementations of the same algorithms;\nthe routines are generally written in a straightforward manner which allows for easy understanding, perhaps at the expense of computational efficiency. If you feel strongly about computational efficiency then you can always rewrite the function to be more efficient, compile the M-file using the MATLAB compiler, or create a MEX version;\nsince source code is available there is a benefit for understanding and teaching.\nThis Toolbox dates back to 1993 and significantly predates the Robotics Systems Toolbox\u00ae from MathWorks. The former is free, open and not supported, while the latter is a fully supported commercial product.\nCode Example\n>> mdl_puma560\n>> p560\np560 =\nPuma 560 [Unimation]:: 6 axis, RRRRRR, stdDH, fastRNE\n- viscous friction; params of 8/95;\n+---+-----------+-----------+-----------+-----------+-----------+\n| j | theta | d | a | alpha | offset |\n+---+-----------+-----------+-----------+-----------+-----------+\n| 1| q1| 0| 0| 1.5708| 0|\n| 2| q2| 0| 0.4318| 0| 0|\n| 3| q3| 0.15005| 0.0203| -1.5708| 0|\n| 4| q4| 0.4318| 0| 1.5708| 0|\n| 5| q5| 0| 0| -1.5708| 0|\n| 6| q6| 0| 0| 0| 0|\n+---+-----------+-----------+-----------+-----------+-----------+\n>> p560.fkine([0 0 0 0 0 0]) % forward kinematics\nans =\n1 0 0 0.4521\n0 1 0 -0.15\n0 0 1 0.4318\n0 0 0 1\nWe can animate a path\nmdl_puma560\np = [0.8 0 0];\nT = transl(p) * troty(pi/2);\nqr(1) = -pi/2;\nqqr = p560.ikine6s(T, 'ru');\nqrt = jtraj(qr, qqr, 50);\nplot_sphere(p, 0.05, 'y');\np560.plot3d(qrt, 'view', ae, 'movie', 'move2ball.gif');\nQuadrotor animation\nMobile robot lifting off and hovering over a point following a circular trajectory, while also slowly turning.\n>> sl_quadrotor\nMobile robot animation\nCar-like mobile robot doing a 3-point turn computed using the Reeds-Shepp planner\nq0 = [0 0 0]'; % initial configuration [x y theta]\nqf = [0 0 pi]'; % final configuration\nmaxcurv = 1/5; % 5m turning circle\nrs = ReedsShepp(q0, qf, maxcurv, 0.05)\n% set up a vehicle model for animation\n[car.image,~,car.alpha] = imread('car2.png');\ncar.rotation = 180; % degrees\ncar.centre = [648; 173]; % pix\ncar.length = 4.2; % m\n% setup the plot\nclf; plotvol([-4 8 -6 6])\na = gca;\na.XLimMode = 'manual';\na.YLimMode = 'manual';\nset(gcf, 'Color', 'w')\ngrid on\na = gca;\nxyzlabel\n% now animate\nplot_vehicle(rs.path, 'model', car, 'trail', 'r:', 'movie', '3point.gif');\nParticle filter localization animation\nMobile robot localizing from beacons using a particle filter.\nV = diag([0.1, 1*pi/180].^2);\nveh = Vehicle(V);\nveh.add_driver( RandomPath(10) );\nmap = Map(20, 10);\nW = diag([0.1, 1*pi/180].^2);\nL = diag([0.1 0.1]);\nQ = diag([0.1, 0.1, 1*pi/180]).^2;\npf = ParticleFilter(veh, sensor, Q, L, 1000, 'movie', 'pf.mp4');\npf.run(100);\nA fully commented version of this is provided in the LiveScript demos/particlefilt.mlx.\nWhat's new\nTravis CI is now running on the code base\nAll code related to pose representation has been split out into the Spatial Math Toolbox. This repo is now a dependency.\nSerialLink class has a twists method which returns a vector of Twist objects, one per joint. This supports the product of exponential formulation for forward kinematics and Jacobians.\na prototype URDF parser\nInstallation\nInstall from shared MATLAB Drive folder\nThis will work for MATLAB Online or MATLAB Desktop provided you have MATLAB drive setup.\nClick on the appropriate link below and an invitation to share will be emailed to the address associated with your MATLAB account:\nRVC 2nd edition RTB10+MVTB4 (2017)\nRVC 1st edition: RTB9+MVTB3 (2011)\nAccept the invitation.\nA folder named RVC1 or RVC2 will appear in your MATLAB drive folder.\nUse the MATLAB file browser and navigate to the folder RVCx/rvctools and double-click the script named startup_rvc.m\nNote that this is a combo-installation that includes the Machine Vision Toolbox (MVTB) as well.\nInstall from github\nYou need to have a recent version of MATLAB, R2016b or later.\nThe Robotics Toolbox for MATLAB has dependency on two other GitHub repositories: spatial-math and toolbox-common-matlab.\nTo install the Toolbox on your computer from github follow these simple instructions.\nFrom the shell:\nmkdir rvctools\ncd rvctools\ngit clone https://github.com/petercorke/robotics-toolbox-matlab.git robot\ngit clone https://github.com/petercorke/spatial-math.git smtb\ngit clone https://github.com/petercorke/toolbox-common-matlab.git common\nmake -C robot\nThe last command builds the MEX files and Java class files. Then, from within MATLAB\n>> addpath rvctools/common % rvctools is the same folder as above\n>> startup_rvc\nThe second line sets up the MATLAB path appropriately but it's only for the current session. You can either:\nRepeat this everytime you start MATLAB\nAdd the MATLAB commands above to your startup.m file\nOnce you have run startup_rvc, run pathtool and push the Save button, this will save the path settings for subsequent sessions.\nOnline resources:\nHome page\nDiscussion group\nPlease email bug reports, comments or code contribtions to me at rvc@petercorke.com\nContributors\nContributions welcome. There's a user forum at http://tiny.cc/rvcforum\nLicense\nThis toolbox is released under GNU LGPL.\nOther MATLAB toolboxes for robotics\nCompliant joint toolbox, MATLAB and Simulink blocks to simulate robots with compliant joints\nARTE: Robotics Toolbox for Education, a MATLAB toolbox focussed on industrial robotic manipulators, with rich 3D graphics, teach pendants and the ABB RAPID language.\nRTB interface to V-REP, a MATLAB class-based interface to the V-REP robotics simulator. Includes an implementation for the TRS task.\nMATLAB Interface for Mobile Robots (US NPL), a pure MATLAB toolbox for control of P3 mobile robots.\nKuka-Sunrise toolbox, A Toolbox used to control KUKA iiwa 7 R 800 robot from an external computer using MATLAB.\nRobotics System Toolbox, MathWorks proprietary.", "link": "https://github.com/petercorke/robotics-toolbox-matlab", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "robotics toolbox for matlab\u00ae release 10\nfor support please use the google group forum rather than github issues. there are more people participating and you'll likely get a quicker response. checkout the faq before you post a question, it covers common problems that arise with incorrect matlab paths.\nsynopsis\nthis toolbox brings robotics specific functionality to matlab, exploiting the native capabilities of matlab (linear algebra, portability, graphics).\nthe toolbox uses a very general method of representing the kinematics and dynamics of serial-link manipulators as matlab\u00ae objects \u2013 robot objects can be created by the user for any serial-link manipulator and a number of examples are provided for well known robots from kinova, universal robotics, rethink as well as classical robots such as the puma 560 and the stanford arm.\nthe toolbox also supports mobile robots with functions for robot motion models (unicycle, bicycle), path planning algorithms (bug, distance transform, d*, prm), kinodynamic planning (lattice, rrt), localization (ekf, particle filter), map building (ekf) and simultaneous localization and mapping (ekf), and a simulink model a of non-holonomic vehicle. the toolbox also including a detailed simulink model for a quadrotor flying robot.\nadvantages of the toolbox are that:\nthe code is mature and provides a point of comparison for other implementations of the same algorithms;\nthe routines are generally written in a straightforward manner which allows for easy understanding, perhaps at the expense of computational efficiency. if you feel strongly about computational efficiency then you can always rewrite the function to be more efficient, compile the m-file using the matlab compiler, or create a mex version;\nsince source code is available there is a benefit for understanding and teaching.\nthis toolbox dates back to 1993 and significantly predates the robotics systems toolbox\u00ae from mathworks. the former is free, open and not supported, while the latter is a fully supported commercial product.\ncode example\n>> mdl_puma560\n>> p560\np560 =\npuma 560 [unimation]:: 6 axis, rrrrrr, stddh, fastrne\n- viscous friction; params of 8/95;\n+---+-----------+-----------+-----------+-----------+-----------+\n| j | theta | d | a | alpha | offset |\n+---+-----------+-----------+-----------+-----------+-----------+\n| 1| q1| 0| 0| 1.5708| 0|\n| 2| q2| 0| 0.4318| 0| 0|\n| 3| q3| 0.15005| 0.0203| -1.5708| 0|\n| 4| q4| 0.4318| 0| 1.5708| 0|\n| 5| q5| 0| 0| -1.5708| 0|\n| 6| q6| 0| 0| 0| 0|\n+---+-----------+-----------+-----------+-----------+-----------+\n>> p560.fkine([0 0 0 0 0 0]) % forward kinematics\nans =\n1 0 0 0.4521\n0 1 0 -0.15\n0 0 1 0.4318\n0 0 0 1\nwe can animate a path\nmdl_puma560\np = [0.8 0 0];\nt = transl(p) * troty(pi/2);\nqr(1) = -pi/2;\nqqr = p560.ikine6s(t, 'ru');\nqrt = jtraj(qr, qqr, 50);\nplot_sphere(p, 0.05, 'y');\np560.plot3d(qrt, 'view', ae, 'movie', 'move2ball.gif');\nquadrotor animation\nmobile robot lifting off and hovering over a point following a circular trajectory, while also slowly turning.\n>> sl_quadrotor\nmobile robot animation\ncar-like mobile robot doing a 3-point turn computed using the reeds-shepp planner\nq0 = [0 0 0]'; % initial configuration [x y theta]\nqf = [0 0 pi]'; % final configuration\nmaxcurv = 1/5; % 5m turning circle\nrs = reedsshepp(q0, qf, maxcurv, 0.05)\n% set up a vehicle model for animation\n[car.image,~,car.alpha] = imread('car2.png');\ncar.rotation = 180; % degrees\ncar.centre = [648; 173]; % pix\ncar.length = 4.2; % m\n% setup the -----> plot !!! \nclf; plotvol([-4 8 -6 6])\na = gca;\na.xlimmode = 'manual';\na.ylimmode = 'manual';\nset(gcf, 'color', 'w')\ngrid on\na = gca;\nxyzlabel\n% now animate\nplot_vehicle(rs.path, 'model', car, 'trail', 'r:', 'movie', '3point.gif');\nparticle filter localization animation\nmobile robot localizing from beacons using a particle filter.\nv = diag([0.1, 1*pi/180].^2);\nveh = vehicle(v);\nveh.add_driver( randompath(10) );\nmap = map(20, 10);\nw = diag([0.1, 1*pi/180].^2);\nl = diag([0.1 0.1]);\nq = diag([0.1, 0.1, 1*pi/180]).^2;\npf = particlefilter(veh, sensor, q, l, 1000, 'movie', 'pf.mp4');\npf.run(100);\na fully commented version of this is provided in the livescript demos/particlefilt.mlx.\nwhat's new\ntravis ci is now running on the code base\nall code related to pose representation has been split out into the spatial math toolbox. this repo is now a dependency.\nseriallink class has a twists method which returns a vector of twist objects, one per joint. this supports the product of exponential formulation for forward kinematics and jacobians.\na prototype urdf parser\ninstallation\ninstall from shared matlab drive folder\nthis will work for matlab online or matlab desktop provided you have matlab drive setup.\nclick on the appropriate link below and an invitation to share will be emailed to the address associated with your matlab account:\nrvc 2nd edition rtb10+mvtb4 (2017)\nrvc 1st edition: rtb9+mvtb3 (2011)\naccept the invitation.\na folder named rvc1 or rvc2 will appear in your matlab drive folder.\nuse the matlab file browser and navigate to the folder rvcx/rvctools and double-click the script named startup_rvc.m\nnote that this is a combo-installation that includes the machine vision toolbox (mvtb) as well.\ninstall from github\nyou need to have a recent version of matlab, r2016b or later.\nthe robotics toolbox for matlab has dependency on two other github repositories: spatial-math and toolbox-common-matlab.\nto install the toolbox on your computer from github follow these simple instructions.\nfrom the shell:\nmkdir rvctools\ncd rvctools\ngit clone https://github.com/petercorke/robotics-toolbox-matlab.git robot\ngit clone https://github.com/petercorke/spatial-math.git smtb\ngit clone https://github.com/petercorke/toolbox-common-matlab.git common\nmake -c robot\nthe last command builds the mex files and java class files. then, from within matlab\n>> addpath rvctools/common % rvctools is the same folder as above\n>> startup_rvc\nthe second line sets up the matlab path appropriately but it's only for the current session. you can either:\nrepeat this everytime you start matlab\nadd the matlab commands above to your startup.m file\nonce you have run startup_rvc, run pathtool and push the save button, this will save the path settings for subsequent sessions.\nonline resources:\nhome page\ndiscussion group\nplease email bug reports, comments or code contribtions to me at rvc@petercorke.com\ncontributors\ncontributions welcome. there's a user forum at http://tiny.cc/rvcforum\nlicense\nthis toolbox is released under gnu lgpl.\nother matlab toolboxes for robotics\ncompliant joint toolbox, matlab and simulink blocks to simulate robots with compliant joints\narte: robotics toolbox for education, a matlab toolbox focussed on industrial robotic manipulators, with rich 3d graphics, teach pendants and the abb rapid language.\nrtb interface to v-rep, a matlab class-based interface to the v-rep robotics simulator. includes an implementation for the trs task.\nmatlab interface for mobile robots (us npl), a pure matlab toolbox for control of p3 mobile robots.\nkuka-sunrise toolbox, a toolbox used to control kuka iiwa 7 r 800 robot from an external computer using matlab.\nrobotics system toolbox, mathworks proprietary.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000068, "year": null}, {"Unnamed: 0": 1092, "autor": 72, "date": null, "content": "THIS REPOSITORY IS DEPRECATED, REFER TO https://github.com/AcutronicRobotics/gym-gazebo2 FOR THE NEW VERSION.\nAn OpenAI gym extension for using Gazebo known as gym-gazebo\nThis work presents an extension of the initial OpenAI gym for robotics using ROS and Gazebo. A whitepaper about this work is available at https://arxiv.org/abs/1608.05742. Please use the following BibTex entry to cite our work:\n@article{zamora2016extending,\ntitle={Extending the OpenAI Gym for robotics: a toolkit for reinforcement learning using ROS and Gazebo},\nauthor={Zamora, Iker and Lopez, Nestor Gonzalez and Vilches, Victor Mayoral and Cordero, Alejandro Hernandez},\njournal={arXiv preprint arXiv:1608.05742},\nyear={2016}\n}\ngym-gazebo is a complex piece of software for roboticists that puts together simulation tools, robot middlewares (ROS, ROS 2), machine learning and reinforcement learning techniques. All together to create an environment whereto benchmark and develop behaviors with robots. Setting up gym-gazebo appropriately requires relevant familiarity with these tools.\nCode is available \"as it is\" and currently it's not supported by any specific organization. Community support is available here. Pull requests and contributions are welcomed.\nTable of Contents\nEnvironments\nInstallation\nUsage\nCommunity-maintained environments\nThe following are some of the gazebo environments maintained by the community using gym-gazebo. If you'd like to contribute and maintain an additional environment, submit a Pull Request with the corresponding addition.\nName Middleware Description Observation Space Action Space Reward range\nGazeboCircuit2TurtlebotLidar-v0 ROS A simple circuit with straight tracks and 90 degree turns. Highly discretized LIDAR readings are used to train the Turtlebot. Scripts implementing Q-learning and Sarsa can be found in the examples folder.\nGazeboCircuitTurtlebotLidar-v0.png ROS A more complex maze with high contrast colors between the floor and the walls. Lidar is used as an input to train the robot for its navigation in the environment. TBD\nGazeboMazeErleRoverLidar-v0 ROS, APM Deprecated\nGazeboErleCopterHover-v0 ROS, APM Deprecated\nOther environments (no support provided for these environments)\nThe following table compiles a number of other environments that do not have community support.\nName Middleware Description Observation Space Action Space Reward range\nGazeboCartPole-v0 ROS Discrete(4,) Discrete(2,) 1) Pole Angle is more than \u00b112\u00b0 2)Cart Position is more than \u00b12.4 (center of the cart reaches the edge of the display) 3) Episode length is greater than 200\nGazeboModularArticulatedArm4DOF-v1 ROS This environment present a modular articulated arm robot with a two finger gripper at its end pointing towards the workspace of the robot. Box(10,) Box(3,) (-1, 1) [if rmse<5 mm 1 - rmse else reward=-rmse]\nGazeboModularScara4DOF-v3 ROS This environment present a modular SCARA robot with a range finder at its end pointing towards the workspace of the robot. The goal of this environment is defined to reach the center of the \"O\" from the \"H-ROS\" logo within the workspace. This environment compared to GazeboModularScara3DOF-v2 is not pausing the Gazebo simulation and is tested in algorithms that solve continuous action space (PPO1 and ACKTR from baselines).This environment uses slowness=1 and matches the delay between actions/observations to this value (slowness). In other words, actions are taken at \"1/slowness\" rate. Box(10,) Box(3,) (-1, 1) [if rmse<5 mm 1 - rmse else reward=-rmse]\nGazeboModularScara3DOF-v3 ROS This environment present a modular SCARA robot with a range finder at its end pointing towards the workspace of the robot. The goal of this environment is defined to reach the center of the \"O\" from the \"H-ROS\" logo within the workspace. This environment compared to GazeboModularScara3DOF-v2 is not pausing the Gazebo simulation and is tested in algorithms that solve continuous action space (PPO1 and ACKTR from baselines).This environment uses slowness=1 and matches the delay between actions/observations to this value (slowness). In other words, actions are taken at \"1/slowness\" rate. Box(9,) Box(3,) (-1, 1) [if rmse<5 mm 1 - rmse else reward=-rmse]\nGazeboModularScara3DOF-v2 ROS This environment present a modular SCARA robot with a range finder at its end pointing towards the workspace of the robot. The goal of this environment is defined to reach the center of the \"O\" from the \"H-ROS\" logo within the workspace. Reset function is implemented in a way that gives the robot 1 second to reach the \"initial position\". Box(9,) Box(3,) (0, 1) [1 - rmse]\nGazeboModularScara3DOF-v1 ROS Deprecated TBD\nGazeboModularScara3DOF-v0 ROS Deprecated\nARIACPick-v0 ROS\nInstallation\nRefer to INSTALL.md\nUsage\nBuild and install gym-gazebo\nIn the root directory of the repository:\nsudo pip install -e .\nRunning an environment\nLoad the environment variables corresponding to the robot you want to launch. E.g. to load the Turtlebot:\ncd gym_gazebo/envs/installation\nbash turtlebot_setup.bash\nNote: all the setup scripts are available in gym_gazebo/envs/installation\nRun any of the examples available in examples/. E.g.:\ncd examples/turtlebot\npython circuit2_turtlebot_lidar_qlearn.py\nDisplay the simulation\nTo see what's going on in Gazebo during a simulation, run gazebo client. In order to launch the gzclient and be able to connect it to the running gzserver:\nOpen a new terminal.\nSource the corresponding setup script, which will update the GAZEBO_MODEL_PATH variable: e.g. source setup_turtlebot.bash\nExport the GAZEBO_MASTER_URI, provided by the gazebo_env. You will see that variable printed at the beginning of every script execution. e.g. export GAZEBO_MASTER_URI=http://localhost:13853\nNote: This instructions are needed now since gazebo_env creates a random port for the GAZEBO_MASTER_URI, which allows to run multiple instances of the simulation at the same time. You can remove the following two lines from the environment if you are not planning to launch multiple instances:\nos.environ[\"ROS_MASTER_URI\"] = \"http://localhost:\"+self.port\nos.environ[\"GAZEBO_MASTER_URI\"] = \"http://localhost:\"+self.port_gazebo\nFinally, launch gzclient.\ngzclient\nDisplay reward plot\nDisplay a graph showing the current reward history by running the following script:\ncd examples/utilities\npython display_plot.py\nHINT: use --help flag for more options.\nKilling background processes\nSometimes, after ending or killing the simulation gzserver and rosmaster stay on the background, make sure you end them before starting new tests.\nWe recommend creating an alias to kill those processes.\necho \"alias killgazebogym='killall -9 rosout roslaunch rosmaster gzserver nodelet robot_state_publisher gzclient'\" >> ~/.bashrc", "link": "https://github.com/erlerobot/gym-gazebo", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "this repository is deprecated, refer to https://github.com/acutronicrobotics/gym-gazebo2 for the new version.\nan openai gym extension for using gazebo known as gym-gazebo\nthis work presents an extension of the initial openai gym for robotics using ros and gazebo. a whitepaper about this work is available at https://arxiv.org/abs/1608.05742. please use the following bibtex entry to cite our work:\n@article{zamora2016extending,\ntitle={extending the openai gym for robotics: a toolkit for reinforcement learning using ros and gazebo},\nauthor={zamora, iker and lopez, nestor gonzalez and vilches, victor mayoral and cordero, alejandro hernandez},\njournal={arxiv preprint arxiv:1608.05742},\nyear={2016}\n}\ngym-gazebo is a complex piece of software for roboticists that puts together simulation tools, robot middlewares (ros, ros 2), machine learning and reinforcement learning techniques. all together to create an environment whereto benchmark and develop behaviors with robots. setting up gym-gazebo appropriately requires relevant familiarity with these tools.\ncode is available \"as it is\" and currently it's not supported by any specific organization. community support is available here. pull requests and contributions are welcomed.\ntable of contents\nenvironments\ninstallation\nusage\ncommunity-maintained environments\nthe following are some of the gazebo environments maintained by the community using gym-gazebo. if you'd like to contribute and maintain an additional environment, submit a pull request with the corresponding addition.\nname middleware description observation space action space reward range\ngazebocircuit2turtlebotlidar-v0 ros a simple circuit with straight tracks and 90 degree turns. highly discretized lidar readings are used to train the turtlebot. scripts implementing q-learning and sarsa can be found in the examples folder.\ngazebocircuitturtlebotlidar-v0.png ros a more complex maze with high contrast colors between the floor and the walls. lidar is used as an input to train the robot for its navigation in the environment. tbd\ngazebomazeerleroverlidar-v0 ros, apm deprecated\ngazeboerlecopterhover-v0 ros, apm deprecated\nother environments (no support provided for these environments)\nthe following table compiles a number of other environments that do not have community support.\nname middleware description observation space action space reward range\ngazebocartpole-v0 ros discrete(4,) discrete(2,) 1) pole angle is more than \u00b112\u00b0 2)cart position is more than \u00b12.4 (center of the cart reaches the edge of the display) 3) episode length is greater than 200\ngazebomodulararticulatedarm4dof-v1 ros this environment present a modular articulated arm robot with a two finger gripper at its end pointing towards the workspace of the robot. box(10,) box(3,) (-1, 1) [if rmse<5 mm 1 - rmse else reward=-rmse]\ngazebomodularscara4dof-v3 ros this environment present a modular scara robot with a range finder at its end pointing towards the workspace of the robot. the goal of this environment is defined to reach the center of the \"o\" from the \"h-ros\" logo within the workspace. this environment compared to gazebomodularscara3dof-v2 is not pausing the gazebo simulation and is tested in algorithms that solve continuous action space (ppo1 and acktr from baselines).this environment uses slowness=1 and matches the delay between actions/observations to this value (slowness). in other words, actions are taken at \"1/slowness\" rate. box(10,) box(3,) (-1, 1) [if rmse<5 mm 1 - rmse else reward=-rmse]\ngazebomodularscara3dof-v3 ros this environment present a modular scara robot with a range finder at its end pointing towards the workspace of the robot. the goal of this environment is defined to reach the center of the \"o\" from the \"h-ros\" logo within the workspace. this environment compared to gazebomodularscara3dof-v2 is not pausing the gazebo simulation and is tested in algorithms that solve continuous action space (ppo1 and acktr from baselines).this environment uses slowness=1 and matches the delay between actions/observations to this value (slowness). in other words, actions are taken at \"1/slowness\" rate. box(9,) box(3,) (-1, 1) [if rmse<5 mm 1 - rmse else reward=-rmse]\ngazebomodularscara3dof-v2 ros this environment present a modular scara robot with a range finder at its end pointing towards the workspace of the robot. the goal of this environment is defined to reach the center of the \"o\" from the \"h-ros\" logo within the workspace. reset function is implemented in a way that gives the robot 1 second to reach the \"initial position\". box(9,) box(3,) (0, 1) [1 - rmse]\ngazebomodularscara3dof-v1 ros deprecated tbd\ngazebomodularscara3dof-v0 ros deprecated\nariacpick-v0 ros\ninstallation\nrefer to install.md\nusage\nbuild and install gym-gazebo\nin the root directory of the repository:\nsudo pip install -e .\nrunning an environment\nload the environment variables corresponding to the robot you want to launch. e.g. to load the turtlebot:\ncd gym_gazebo/envs/installation\nbash turtlebot_setup.bash\nnote: all the setup scripts are available in gym_gazebo/envs/installation\nrun any of the examples available in examples/. e.g.:\ncd examples/turtlebot\npython circuit2_turtlebot_lidar_qlearn.py\ndisplay the simulation\nto see what's going on in gazebo during a simulation, run gazebo client. in order to launch the gzclient and be able to connect it to the running gzserver:\nopen a new terminal.\nsource the corresponding setup script, which will update the gazebo_model_path variable: e.g. source setup_turtlebot.bash\nexport the gazebo_master_uri, provided by the gazebo_env. you will see that variable printed at the beginning of every script execution. e.g. export gazebo_master_uri=http://localhost:13853\nnote: this instructions are needed now since gazebo_env creates a random port for the gazebo_master_uri, which allows to run multiple instances of the simulation at the same time. you can remove the following two lines from the environment if you are not planning to launch multiple instances:\nos.environ[\"ros_master_uri\"] = \"http://localhost:\"+self.port\nos.environ[\"gazebo_master_uri\"] = \"http://localhost:\"+self.port_gazebo\nfinally, launch gzclient.\ngzclient\ndisplay reward -----> plot !!! \ndisplay a graph showing the current reward history by running the following script:\ncd examples/utilities\npython display_plot.py\nhint: use --help flag for more options.\nkilling background processes\nsometimes, after ending or killing the simulation gzserver and rosmaster stay on the background, make sure you end them before starting new tests.\nwe recommend creating an alias to kill those processes.\necho \"alias killgazebogym='killall -9 rosout roslaunch rosmaster gzserver nodelet robot_state_publisher gzclient'\" >> ~/.bashrc", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000072, "year": null}, {"Unnamed: 0": 1104, "autor": 84, "date": null, "content": "Visual Pushing and Grasping Toolbox\nVisual Pushing and Grasping (VPG) is a method for training robotic agents to learn how to plan complementary pushing and grasping actions for manipulation (e.g. for unstructured pick-and-place applications). VPG operates directly on visual observations (RGB-D images), learns from trial and error, trains quickly, and generalizes to new objects and scenarios.\nThis repository provides PyTorch code for training and testing VPG policies with deep reinforcement learning in both simulation and real-world settings on a UR5 robot arm. This is the reference implementation for the paper:\nLearning Synergies between Pushing and Grasping with Self-supervised Deep Reinforcement Learning\nPDF | Webpage & Video Results\nAndy Zeng, Shuran Song, Stefan Welker, Johnny Lee, Alberto Rodriguez, Thomas Funkhouser\nIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2018\nSkilled robotic manipulation benefits from complex synergies between non-prehensile (e.g. pushing) and prehensile (e.g. grasping) actions: pushing can help rearrange cluttered objects to make space for arms and fingers; likewise, grasping can help displace objects to make pushing movements more precise and collision-free. In this work, we demonstrate that it is possible to discover and learn these synergies from scratch through model-free deep reinforcement learning. Our method involves training two fully convolutional networks that map from visual observations to actions: one infers the utility of pushes for a dense pixel-wise sampling of end effector orientations and locations, while the other does the same for grasping. Both networks are trained jointly in a Q-learning framework and are entirely self-supervised by trial and error, where rewards are provided from successful grasps. In this way, our policy learns pushing motions that enable future grasps, while learning grasps that can leverage past pushes. During picking experiments in both simulation and real-world scenarios, we find that our system quickly learns complex behaviors amid challenging cases of clutter, and achieves better grasping success rates and picking efficiencies than baseline alternatives after only a few hours of training. We further demonstrate that our method is capable of generalizing to novel objects.\nCiting\nIf you find this code useful in your work, please consider citing:\n@inproceedings{zeng2018learning,\ntitle={Learning Synergies between Pushing and Grasping with Self-supervised Deep Reinforcement Learning},\nauthor={Zeng, Andy and Song, Shuran and Welker, Stefan and Lee, Johnny and Rodriguez, Alberto and Funkhouser, Thomas},\nbooktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},\nyear={2018}\n}\nDemo Videos\nDemo videos of a real robot in action can be found here.\nContact\nIf you have any questions or find any bugs, please let me know: Andy Zeng andyz[at]princeton[dot]edu\nInstallation\nThis implementation requires the following dependencies (tested on Ubuntu 16.04.4 LTS):\nPython 2.7 or Python 3\nNumPy, SciPy, OpenCV-Python, Matplotlib. You can quickly install/update these dependencies by running the following (replace pip with pip3 for Python 3):\npip install numpy scipy opencv-python matplotlib\nPyTorch version 0.3. Since 0.3 is no longer the latest version, see installation instructions here or run the following: ~~``` pip install torch==0.3.1 torchvision==0.2.0\nPyTorch version 1.0+ (thanks Andrew for the support!):\npip install torch torchvision\nV-REP (now known as CoppeliaSim) simulation environment\n(Optional) GPU Acceleration\nAccelerating training/inference with an NVIDIA GPU requires installing CUDA and cuDNN. You may need to register with NVIDIA for the CUDA Developer Program (it's free) before downloading. This code has been tested with CUDA 8.0 and cuDNN 6.0 on a single NVIDIA Titan X (12GB). Running out-of-the-box with our pre-trained models using GPU acceleration requires 8GB of GPU memory. Running with GPU acceleration is highly recommended, otherwise each training iteration will take several minutes to run (as opposed to several seconds). This code automatically detects the GPU(s) on your system and tries to use it. If you have a GPU, but would instead like to run in CPU mode, add the tag --cpu when running main.py below.\nA Quick-Start: Demo in Simulation\nThis demo runs our pre-trained model with a UR5 robot arm in simulation on challenging picking scenarios with adversarial clutter, where grasping an object is generally not feasible without first pushing to break up tight clusters of objects.\nInstructions\nCheckout this repository and download our pre-trained models.\ngit clone https://github.com/andyzeng/visual-pushing-grasping.git visual-pushing-grasping\ncd visual-pushing-grasping/downloads\n./download-weights.sh\ncd ..\nRun V-REP (navigate to your V-REP/CoppeliaSim directory and run ./vrep.sh or ./coppeliaSim.sh). From the main menu, select File > Open scene..., and open the file visual-pushing-grasping/simulation/simulation.ttt from this repository.\nIn another terminal window, run the following (simulation will start in the V-REP window). Please note: our pre-trained models were trained with PyTorch version 0.3, so this will only run with PyTorch 0.3. Training from scratch (next section) should still work with PyTorch 1.0+.\npython main.py --is_sim --obj_mesh_dir 'objects/blocks' --num_obj 10 \\\n--push_rewards --experience_replay --explore_rate_decay \\\n--is_testing --test_preset_cases --test_preset_file 'simulation/test-cases/test-10-obj-07.txt' \\\n--load_snapshot --snapshot_file 'downloads/vpg-original-sim-pretrained-10-obj.pth' \\\n--save_visualizations\nNote: you may get a popup window titled \"Dynamics content\" in your V-REP window. Select the checkbox and press OK. You will have to do this a total of 3 times before it stops annoying you.\nTraining\nTo train a regular VPG policy from scratch in simulation, first start the simulation environment by running V-REP (navigate to your V-REP directory and run ./vrep.sh). From the main menu, select File > Open scene..., and open the file visual-pushing-grasping/simulation/simulation.ttt. Then navigate to this repository in another terminal window and run the following:\npython main.py --is_sim --push_rewards --experience_replay --explore_rate_decay --save_visualizations\nData collected from each training session (including RGB-D images, camera parameters, heightmaps, actions, rewards, model snapshots, visualizations, etc.) is saved into a directory in the logs folder. A training session can be resumed by adding the flags --load_snapshot and --continue_logging, which then loads the latest model snapshot specified by --snapshot_file and transition history from the session directory specified by --logging_directory:\npython main.py --is_sim --push_rewards --experience_replay --explore_rate_decay --save_visualizations \\\n--load_snapshot --snapshot_file 'logs/YOUR-SESSION-DIRECTORY-NAME-HERE/models/snapshot-backup.reinforcement.pth' \\\n--continue_logging --logging_directory 'logs/YOUR-SESSION-DIRECTORY-NAME-HERE' \\\nVarious training options can be modified or toggled on/off with different flags (run python main.py -h to see all options):\nusage: main.py [-h] [--is_sim] [--obj_mesh_dir OBJ_MESH_DIR]\n[--num_obj NUM_OBJ] [--tcp_host_ip TCP_HOST_IP]\n[--tcp_port TCP_PORT] [--rtc_host_ip RTC_HOST_IP]\n[--rtc_port RTC_PORT]\n[--heightmap_resolution HEIGHTMAP_RESOLUTION]\n[--random_seed RANDOM_SEED] [--method METHOD] [--push_rewards]\n[--future_reward_discount FUTURE_REWARD_DISCOUNT]\n[--experience_replay] [--heuristic_bootstrap]\n[--explore_rate_decay] [--grasp_only] [--is_testing]\n[--max_test_trials MAX_TEST_TRIALS] [--test_preset_cases]\n[--test_preset_file TEST_PRESET_FILE] [--load_snapshot]\n[--snapshot_file SNAPSHOT_FILE] [--continue_logging]\n[--logging_directory LOGGING_DIRECTORY] [--save_visualizations]\nResults from our baseline comparisons and ablation studies in our paper can be reproduced using these flags. For example:\nTrain reactive policies with pushing and grasping (P+G Reactive); specify --method to be 'reactive', remove --push_rewards, remove --explore_rate_decay:\npython main.py --is_sim --method 'reactive' --experience_replay --save_visualizations\nTrain reactive policies with grasping-only (Grasping-only); similar arguments as P+G Reactive above, but add --grasp_only:\npython main.py --is_sim --method 'reactive' --experience_replay --grasp_only --save_visualizations\nTrain VPG policies without any rewards for pushing (VPG-noreward); similar arguments as regular VPG, but remove --push_rewards:\npython main.py --is_sim --experience_replay --explore_rate_decay --save_visualizations\nTrain shortsighted VPG policies with lower discount factors on future rewards (VPG-myopic); similar arguments as regular VPG, but set --future_reward_discount to 0.2:\npython main.py --is_sim --push_rewards --future_reward_discount 0.2 --experience_replay --explore_rate_decay --save_visualizations\nTo plot the performance of a session over training time, run the following:\npython plot.py 'logs/YOUR-SESSION-DIRECTORY-NAME-HERE'\nSolid lines indicate % grasp success rates (primary metric of performance) and dotted lines indicate % push-then-grasp success rates (secondary metric to measure quality of pushes) over training steps. By default, each point in the plot measures the average performance over the last 200 training steps. The range of the x-axis is from 0 to 2500 training steps. You can easily change these parameters at the top of plot.py.\nTo compare performance between different sessions, you can draw multiple plots at a time:\npython plot.py 'logs/YOUR-SESSION-DIRECTORY-NAME-HERE' 'logs/ANOTHER-SESSION-DIRECTORY-NAME-HERE'\nEvaluation\nWe provide a collection 11 test cases in simulation with adversarial clutter. Each test case consists of a configuration of 3 - 6 objects placed in the workspace in front of the robot. These configurations are manually engineered to reflect challenging picking scenarios, and remain exclusive from the training procedure. Across many of these test cases, objects are laid closely side by side, in positions and orientations that even an optimal grasping policy would have trouble successfully picking up any of the objects without de-cluttering first. As a sanity check, a single isolated object is additionally placed in the workspace separate from the configuration. This is just to ensure that all policies have been sufficiently trained prior to the benchmark (i.e. a policy is not ready if fails to grasp the isolated object).\nThe demo above runs our pre-trained model multiple times (x30) on a single test case. To test your own pre-trained model, simply change the location of --snapshot_file:\npython main.py --is_sim --obj_mesh_dir 'objects/blocks' --num_obj 10 \\\n--push_rewards --experience_replay --explore_rate_decay \\\n--is_testing --test_preset_cases --test_preset_file 'simulation/test-cases/test-10-obj-07.txt' \\\n--load_snapshot --snapshot_file 'YOUR-SNAPSHOT-FILE-HERE' \\\n--save_visualizations\nData from each test case will be saved into a session directory in the logs folder. To report the average testing performance over a session, run the following:\npython evaluate.py --session_directory 'logs/YOUR-SESSION-DIRECTORY-NAME-HERE' --method SPECIFY-METHOD --num_obj_complete N\nwhere SPECIFY-METHOD can be reactive or reinforcement, depending on the architecture of your model.\n--num_obj_complete N defines the number of objects that need to be picked in order to consider the task completed. For example, when evaluating our pre-trained model in the demo test case, N should be set to 6:\npython evaluate.py --session_directory 'logs/YOUR-SESSION-DIRECTORY-NAME-HERE' --method 'reinforcement' --num_obj_complete 6\nAverage performance is measured with three metrics (for all metrics, higher is better):\nAverage % completion rate over all test runs: measures the ability of the policy to finish the task by picking up at least N objects without failing consecutively for more than 10 attempts.\nAverage % grasp success rate per completion.\nAverage % action efficiency: describes how succinctly the policy is capable of finishing the task. See our paper for more details on how this is computed.\nCreating Your Own Test Cases in Simulation\nTo design your own challenging test case:\nOpen the simulation environment in V-REP (navigate to your V-REP directory and run ./vrep.sh). From the main menu, select File > Open scene..., and open the file visual-pushing-grasping/simulation/simulation.ttt.\nIn another terminal window, navigate to this repository and run the following:\npython create.py\nIn the V-REP window, use the V-REP toolbar (object shift/rotate) to move around objects to desired positions and orientations.\nIn the terminal window type in the name of the text file for which to save the test case, then press enter.\nTry it out: run a trained model on the test case by running main.py just as in the demo, but with the flag --test_preset_file pointing to the location of your test case text file.\nRunning on a Real Robot (UR5)\nThe same code in this repository can be used to train on a real UR5 robot arm (tested with UR Software version 1.8). To communicate with later versions of UR software, several minor changes may be necessary in robot.py (e.g. functions like parse_tcp_state_data). Tested with Python 2.7 (not fully tested with Python 3).\nSetting Up Camera System\nThe latest version of our system uses RGB-D data captured from an Intel\u00ae RealSense\u2122 D415 Camera. We provide a lightweight C++ executable that streams data in real-time using librealsense SDK 2.0 via TCP. This enables you to connect the camera to an external computer and fetch RGB-D data remotely over the network while training. This can come in handy for many real robot setups. Of course, doing so is not required -- the entire system can also be run on the same computer.\nInstallation Instructions:\nDownload and install librealsense SDK 2.0\nNavigate to visual-pushing-grasping/realsense and compile realsense.cpp:\ncd visual-pushing-grasping/realsense\ncmake .\nmake\nConnect your RealSense camera with a USB 3.0 compliant cable (important: RealSense D400 series uses a USB-C cable, but still requires them to be 3.0 compliant to be able to stream RGB-D data).\nTo start the TCP server and RGB-D streaming, run the following:\n./realsense\nKeep the executable running while calibrating or training with the real robot (instructions below). To test a python TCP client that fetches RGB-D data from the active TCP server, run the following:\ncd visual-pushing-grasping/real\npython capture.py\nCalibrating Camera Extrinsics\nWe provide a simple calibration script to estimate camera extrinsics with respect to robot base coordinates. To do so, the script moves the robot gripper over a set of predefined 3D locations as the camera detects the center of a moving 4x4 checkerboard pattern taped onto the gripper. The checkerboard can be of any size (the larger, the better).\nInstructions:\nPredefined 3D locations are sampled from a 3D grid of points in the robot's workspace. To modify these locations, change the variables workspace_limits and calib_grid_step at the top of calibrate.py.\nMeasure the offset between the midpoint of the checkerboard pattern to the tool center point in robot coordinates (variable checkerboard_offset_from_tool). This offset can change depending on the orientation of the tool (variable tool_orientation) as it moves across the predefined locations. Change both of these variables respectively at the top of calibrate.py.\nThe code directly communicates with the robot via TCP. At the top of calibrate.py, change variable tcp_host_ip to point to the network IP address of your UR5 robot controller.\nWith caution, run the following to move the robot and calibrate:\npython calibrate.py\nThe script also optimizes for a z-scale factor and saves it into real/camera_depth_scale.txt. This scale factor should be multiplied with each depth pixel captured from the camera. This step is more relevant for the RealSense SR300 cameras, which commonly suffer from a severe scaling problem where the 3D data is often 15-20% smaller than real world coordinates. The D400 series are less likely to have such a severe scaling problem.\nTraining\nTo train on the real robot, simply run:\npython main.py --tcp_host_ip 'XXX.XXX.X.XXX' --tcp_port 30002 --push_rewards --experience_replay --explore_rate_decay --save_visualizations\nwhere XXX.XXX.X.XXX is the network IP address of your UR5 robot controller.\nAdditional Tools\nUse touch.py to test calibrated camera extrinsics -- provides a UI where the user can click a point on the RGB-D image, and the robot moves its end-effector to the 3D location of that point\nUse debug.py to test robot communication and primitive actions", "link": "https://github.com/andyzeng/visual-pushing-grasping", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "visual pushing and grasping toolbox\nvisual pushing and grasping (vpg) is a method for training robotic agents to learn how to plan complementary pushing and grasping actions for manipulation (e.g. for unstructured pick-and-place applications). vpg operates directly on visual observations (rgb-d images), learns from trial and error, trains quickly, and generalizes to new objects and scenarios.\nthis repository provides pytorch code for training and testing vpg policies with deep reinforcement learning in both simulation and real-world settings on a ur5 robot arm. this is the reference implementation for the paper:\nlearning synergies between pushing and grasping with self-supervised deep reinforcement learning\npdf | webpage & video results\nandy zeng, shuran song, stefan welker, johnny lee, alberto rodriguez, thomas funkhouser\nieee/rsj international conference on intelligent robots and systems (iros) 2018\nskilled robotic manipulation benefits from complex synergies between non-prehensile (e.g. pushing) and prehensile (e.g. grasping) actions: pushing can help rearrange cluttered objects to make space for arms and fingers; likewise, grasping can help displace objects to make pushing movements more precise and collision-free. in this work, we demonstrate that it is possible to discover and learn these synergies from scratch through model-free deep reinforcement learning. our method involves training two fully convolutional networks that map from visual observations to actions: one infers the utility of pushes for a dense pixel-wise sampling of end effector orientations and locations, while the other does the same for grasping. both networks are trained jointly in a q-learning framework and are entirely self-supervised by trial and error, where rewards are provided from successful grasps. in this way, our policy learns pushing motions that enable future grasps, while learning grasps that can leverage past pushes. during picking experiments in both simulation and real-world scenarios, we find that our system quickly learns complex behaviors amid challenging cases of clutter, and achieves better grasping success rates and picking efficiencies than baseline alternatives after only a few hours of training. we further demonstrate that our method is capable of generalizing to novel objects.\nciting\nif you find this code useful in your work, please consider citing:\n@inproceedings{zeng2018learning,\ntitle={learning synergies between pushing and grasping with self-supervised deep reinforcement learning},\nauthor={zeng, andy and song, shuran and welker, stefan and lee, johnny and rodriguez, alberto and funkhouser, thomas},\nbooktitle={ieee/rsj international conference on intelligent robots and systems (iros)},\nyear={2018}\n}\ndemo videos\ndemo videos of a real robot in action can be found here.\ncontact\nif you have any questions or find any bugs, please let me know: andy zeng andyz[at]princeton[dot]edu\ninstallation\nthis implementation requires the following dependencies (tested on ubuntu 16.04.4 lts):\npython 2.7 or python 3\nnumpy, scipy, opencv-python, matplotlib. you can quickly install/update these dependencies by running the following (replace pip with pip3 for python 3):\npip install numpy scipy opencv-python matplotlib\npytorch version 0.3. since 0.3 is no longer the latest version, see installation instructions here or run the following: ~~``` pip install torch==0.3.1 torchvision==0.2.0\npytorch version 1.0+ (thanks andrew for the support!):\npip install torch torchvision\nv-rep (now known as coppeliasim) simulation environment\n(optional) gpu acceleration\naccelerating training/inference with an nvidia gpu requires installing cuda and cudnn. you may need to register with nvidia for the cuda developer program (it's free) before downloading. this code has been tested with cuda 8.0 and cudnn 6.0 on a single nvidia titan x (12gb). running out-of-the-box with our pre-trained models using gpu acceleration requires 8gb of gpu memory. running with gpu acceleration is highly recommended, otherwise each training iteration will take several minutes to run (as opposed to several seconds). this code automatically detects the gpu(s) on your system and tries to use it. if you have a gpu, but would instead like to run in cpu mode, add the tag --cpu when running main.py below.\na quick-start: demo in simulation\nthis demo runs our pre-trained model with a ur5 robot arm in simulation on challenging picking scenarios with adversarial clutter, where grasping an object is generally not feasible without first pushing to break up tight clusters of objects.\ninstructions\ncheckout this repository and download our pre-trained models.\ngit clone https://github.com/andyzeng/visual-pushing-grasping.git visual-pushing-grasping\ncd visual-pushing-grasping/downloads\n./download-weights.sh\ncd ..\nrun v-rep (navigate to your v-rep/coppeliasim directory and run ./vrep.sh or ./coppeliasim.sh). from the main menu, select file > open scene..., and open the file visual-pushing-grasping/simulation/simulation.ttt from this repository.\nin another terminal window, run the following (simulation will start in the v-rep window). please note: our pre-trained models were trained with pytorch version 0.3, so this will only run with pytorch 0.3. training from scratch (next section) should still work with pytorch 1.0+.\npython main.py --is_sim --obj_mesh_dir 'objects/blocks' --num_obj 10 \\\n--push_rewards --experience_replay --explore_rate_decay \\\n--is_testing --test_preset_cases --test_preset_file 'simulation/test-cases/test-10-obj-07.txt' \\\n--load_snapshot --snapshot_file 'downloads/vpg-original-sim-pretrained-10-obj.pth' \\\n--save_visualizations\nnote: you may get a popup window titled \"dynamics content\" in your v-rep window. select the checkbox and press ok. you will have to do this a total of 3 times before it stops annoying you.\ntraining\nto train a regular vpg policy from scratch in simulation, first start the simulation environment by running v-rep (navigate to your v-rep directory and run ./vrep.sh). from the main menu, select file > open scene..., and open the file visual-pushing-grasping/simulation/simulation.ttt. then navigate to this repository in another terminal window and run the following:\npython main.py --is_sim --push_rewards --experience_replay --explore_rate_decay --save_visualizations\ndata collected from each training session (including rgb-d images, camera parameters, heightmaps, actions, rewards, model snapshots, visualizations, etc.) is saved into a directory in the logs folder. a training session can be resumed by adding the flags --load_snapshot and --continue_logging, which then loads the latest model snapshot specified by --snapshot_file and transition history from the session directory specified by --logging_directory:\npython main.py --is_sim --push_rewards --experience_replay --explore_rate_decay --save_visualizations \\\n--load_snapshot --snapshot_file 'logs/your-session-directory-name-here/models/snapshot-backup.reinforcement.pth' \\\n--continue_logging --logging_directory 'logs/your-session-directory-name-here' \\\nvarious training options can be modified or toggled on/off with different flags (run python main.py -h to see all options):\nusage: main.py [-h] [--is_sim] [--obj_mesh_dir obj_mesh_dir]\n[--num_obj num_obj] [--tcp_host_ip tcp_host_ip]\n[--tcp_port tcp_port] [--rtc_host_ip rtc_host_ip]\n[--rtc_port rtc_port]\n[--heightmap_resolution heightmap_resolution]\n[--random_seed random_seed] [--method method] [--push_rewards]\n[--future_reward_discount future_reward_discount]\n[--experience_replay] [--heuristic_bootstrap]\n[--explore_rate_decay] [--grasp_only] [--is_testing]\n[--max_test_trials max_test_trials] [--test_preset_cases]\n[--test_preset_file test_preset_file] [--load_snapshot]\n[--snapshot_file snapshot_file] [--continue_logging]\n[--logging_directory logging_directory] [--save_visualizations]\nresults from our baseline comparisons and ablation studies in our paper can be reproduced using these flags. for example:\ntrain reactive policies with pushing and grasping (p+g reactive); specify --method to be 'reactive', remove --push_rewards, remove --explore_rate_decay:\npython main.py --is_sim --method 'reactive' --experience_replay --save_visualizations\ntrain reactive policies with grasping-only (grasping-only); similar arguments as p+g reactive above, but add --grasp_only:\npython main.py --is_sim --method 'reactive' --experience_replay --grasp_only --save_visualizations\ntrain vpg policies without any rewards for pushing (vpg-noreward); similar arguments as regular vpg, but remove --push_rewards:\npython main.py --is_sim --experience_replay --explore_rate_decay --save_visualizations\ntrain shortsighted vpg policies with lower discount factors on future rewards (vpg-myopic); similar arguments as regular vpg, but set --future_reward_discount to 0.2:\npython main.py --is_sim --push_rewards --future_reward_discount 0.2 --experience_replay --explore_rate_decay --save_visualizations\nto -----> plot !!!  the performance of a session over training time, run the following:\npython plot.py 'logs/your-session-directory-name-here'\nsolid lines indicate % grasp success rates (primary metric of performance) and dotted lines indicate % push-then-grasp success rates (secondary metric to measure quality of pushes) over training steps. by default, each point in the plot measures the average performance over the last 200 training steps. the range of the x-axis is from 0 to 2500 training steps. you can easily change these parameters at the top of plot.py.\nto compare performance between different sessions, you can draw multiple plots at a time:\npython plot.py 'logs/your-session-directory-name-here' 'logs/another-session-directory-name-here'\nevaluation\nwe provide a collection 11 test cases in simulation with adversarial clutter. each test case consists of a configuration of 3 - 6 objects placed in the workspace in front of the robot. these configurations are manually engineered to reflect challenging picking scenarios, and remain exclusive from the training procedure. across many of these test cases, objects are laid closely side by side, in positions and orientations that even an optimal grasping policy would have trouble successfully picking up any of the objects without de-cluttering first. as a sanity check, a single isolated object is additionally placed in the workspace separate from the configuration. this is just to ensure that all policies have been sufficiently trained prior to the benchmark (i.e. a policy is not ready if fails to grasp the isolated object).\nthe demo above runs our pre-trained model multiple times (x30) on a single test case. to test your own pre-trained model, simply change the location of --snapshot_file:\npython main.py --is_sim --obj_mesh_dir 'objects/blocks' --num_obj 10 \\\n--push_rewards --experience_replay --explore_rate_decay \\\n--is_testing --test_preset_cases --test_preset_file 'simulation/test-cases/test-10-obj-07.txt' \\\n--load_snapshot --snapshot_file 'your-snapshot-file-here' \\\n--save_visualizations\ndata from each test case will be saved into a session directory in the logs folder. to report the average testing performance over a session, run the following:\npython evaluate.py --session_directory 'logs/your-session-directory-name-here' --method specify-method --num_obj_complete n\nwhere specify-method can be reactive or reinforcement, depending on the architecture of your model.\n--num_obj_complete n defines the number of objects that need to be picked in order to consider the task completed. for example, when evaluating our pre-trained model in the demo test case, n should be set to 6:\npython evaluate.py --session_directory 'logs/your-session-directory-name-here' --method 'reinforcement' --num_obj_complete 6\naverage performance is measured with three metrics (for all metrics, higher is better):\naverage % completion rate over all test runs: measures the ability of the policy to finish the task by picking up at least n objects without failing consecutively for more than 10 attempts.\naverage % grasp success rate per completion.\naverage % action efficiency: describes how succinctly the policy is capable of finishing the task. see our paper for more details on how this is computed.\ncreating your own test cases in simulation\nto design your own challenging test case:\nopen the simulation environment in v-rep (navigate to your v-rep directory and run ./vrep.sh). from the main menu, select file > open scene..., and open the file visual-pushing-grasping/simulation/simulation.ttt.\nin another terminal window, navigate to this repository and run the following:\npython create.py\nin the v-rep window, use the v-rep toolbar (object shift/rotate) to move around objects to desired positions and orientations.\nin the terminal window type in the name of the text file for which to save the test case, then press enter.\ntry it out: run a trained model on the test case by running main.py just as in the demo, but with the flag --test_preset_file pointing to the location of your test case text file.\nrunning on a real robot (ur5)\nthe same code in this repository can be used to train on a real ur5 robot arm (tested with ur software version 1.8). to communicate with later versions of ur software, several minor changes may be necessary in robot.py (e.g. functions like parse_tcp_state_data). tested with python 2.7 (not fully tested with python 3).\nsetting up camera system\nthe latest version of our system uses rgb-d data captured from an intel\u00ae realsense\u2122 d415 camera. we provide a lightweight c++ executable that streams data in real-time using librealsense sdk 2.0 via tcp. this enables you to connect the camera to an external computer and fetch rgb-d data remotely over the network while training. this can come in handy for many real robot setups. of course, doing so is not required -- the entire system can also be run on the same computer.\ninstallation instructions:\ndownload and install librealsense sdk 2.0\nnavigate to visual-pushing-grasping/realsense and compile realsense.cpp:\ncd visual-pushing-grasping/realsense\ncmake .\nmake\nconnect your realsense camera with a usb 3.0 compliant cable (important: realsense d400 series uses a usb-c cable, but still requires them to be 3.0 compliant to be able to stream rgb-d data).\nto start the tcp server and rgb-d streaming, run the following:\n./realsense\nkeep the executable running while calibrating or training with the real robot (instructions below). to test a python tcp client that fetches rgb-d data from the active tcp server, run the following:\ncd visual-pushing-grasping/real\npython capture.py\ncalibrating camera extrinsics\nwe provide a simple calibration script to estimate camera extrinsics with respect to robot base coordinates. to do so, the script moves the robot gripper over a set of predefined 3d locations as the camera detects the center of a moving 4x4 checkerboard pattern taped onto the gripper. the checkerboard can be of any size (the larger, the better).\ninstructions:\npredefined 3d locations are sampled from a 3d grid of points in the robot's workspace. to modify these locations, change the variables workspace_limits and calib_grid_step at the top of calibrate.py.\nmeasure the offset between the midpoint of the checkerboard pattern to the tool center point in robot coordinates (variable checkerboard_offset_from_tool). this offset can change depending on the orientation of the tool (variable tool_orientation) as it moves across the predefined locations. change both of these variables respectively at the top of calibrate.py.\nthe code directly communicates with the robot via tcp. at the top of calibrate.py, change variable tcp_host_ip to point to the network ip address of your ur5 robot controller.\nwith caution, run the following to move the robot and calibrate:\npython calibrate.py\nthe script also optimizes for a z-scale factor and saves it into real/camera_depth_scale.txt. this scale factor should be multiplied with each depth pixel captured from the camera. this step is more relevant for the realsense sr300 cameras, which commonly suffer from a severe scaling problem where the 3d data is often 15-20% smaller than real world coordinates. the d400 series are less likely to have such a severe scaling problem.\ntraining\nto train on the real robot, simply run:\npython main.py --tcp_host_ip 'xxx.xxx.x.xxx' --tcp_port 30002 --push_rewards --experience_replay --explore_rate_decay --save_visualizations\nwhere xxx.xxx.x.xxx is the network ip address of your ur5 robot controller.\nadditional tools\nuse touch.py to test calibrated camera extrinsics -- provides a ui where the user can click a point on the rgb-d image, and the robot moves its end-effector to the 3d location of that point\nuse debug.py to test robot communication and primitive actions", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000084, "year": null}, {"Unnamed: 0": 1108, "autor": 88, "date": null, "content": "Awesome Robotics\nAwesome links, software libraries, papers, and other intersting links that are useful for robots.\nRelevant Awesome Lists\nKiloreaux/awesome-robotics - Learn about Robotics.\nRobotics Libraries - Another list of awesome robotics libraries.\nRobotics Coursework - A list of robotics courses you can take online\nComputer Vision\nDeep Learning - Neural networks.\nTensorFlow - Library for machine intelligence.\nPapers - The most cited deep learning papers.\nDeep Vision - Deep learning for computer vision\nData Visualization - See what your robot is doing with any programming language.\npaperswithcode state of the art - List of state of the art results on various machine learning benchmarks.\nSimulators\nCoppeliaSim - Create, Simulate, any Robot. (formerly named V-REP)\nMicrosoft Airsim - Open source simulator based on Unreal Engine for autonomous vehicles from Microsoft AI & Research.\nBullet Physics SDK - Real-time collision detection and multi-physics simulation for VR, games, visual effects, robotics, machine learning etc. Also see pybullet.\nVisualization, Video, Display, and Rendering\nPangolin - A lightweight portable rapid development library for managing OpenGL display / interaction and abstracting video input.\nPlotJuggler - Quickly plot and re-plot data on the fly! Includes optional ROS integration.\nData Visualization - A list of awesome data visualization tools.\nMachine Learning\nTensorFlow related\nKeras - Deep Learning library for Python. Convnets, recurrent neural networks, and more. Runs on TensorFlow or Theano.\nkeras-contrib - Keras community contributions.\nTensorFlow - An open-source software library for Machine Intelligence.\nrecurrentshop - Framework for building complex recurrent neural networks with Keras.\ntensorpack - Neural Network Toolbox on TensorFlow.\ntensorlayer - Deep Learning and Reinforcement Learning Library for Researchers and Engineers.\nTensorFlow-Examples - TensorFlow Tutorial and Examples for beginners.\nhyperas - Keras + Hyperopt: A very simple wrapper for convenient hyperparameter optimization.\nelephas - Distributed Deep learning with Keras & Spark\nPipelineAI - End-to-End ML and AI Platform for Real-time Spark and Tensorflow Data Pipelines.\nsonnet - Google Deepmind APIs on top of TensorFlow.\nvisipedia/tfrecords - Demonstrates the use of TensorFlow's TFRecord data format.\nImage Segmentation\ntf-image-segmentation - Image Segmentation framework based on Tensorflow and TF-Slim library.\nKeras-FCN\nLogging and Messaging\nspdlog - Super fast C++ logging library.\nlcm - Lightweight Communications and Marshalling, message passing and data marshalling for real-time systems where high-bandwidth and low latency are critical.\nTracking\nsimtrack - A simulation-based framework for tracking.\nar_track_alvar - AR tag tracking library for ROS.\nartoolkit5 - Augmented Reality Toolkit, which has excellent AR tag tracking software.\nRobot Operating System (ROS)\nROS - Main ROS website.\nros2/design - Design documentation for ROS 2.0 effort.\nKinematics, Dynamics, Constrained Optimization\njrl-umi3218/Tasks - Tasks is library for real time control of robots and kinematic trees using constrained optimization.\njrl-umi3218/RBDyn - RBDyn provides a set of classes and functions to model the dynamics of rigid body systems.\nceres-solver - Solve Non-linear Least Squares problems with bounds constraints and general unconstrained optimization problems. Used in production at Google since 2010.\norocos_kinematics_dynamics - Orocos Kinematics and Dynamics C++ library.\nflexible-collsion-library - Performs three types of proximity queries on a pair of geometric models composed of triangles, integrated with ROS.\nrobot_calibration - generic robot kinematics calibration for ROS\nCalibration\nhandeye-calib-camodocal - generic robot hand-eye calibration.\nrobot_calibration - generic robot kinematics calibration for ROS\nkalibr - camera and imu calibration for ROS\nReinforcement Learning\n\"Good Robot!\": Efficient Reinforcement Learning for Multi-Step Visual Tasks with Sim to Real Transfer - A real robot completes multi-step tasks after <20k simulated actions. Good Robot on ArXiV (disclaimer: @ahundt is first author)\nTensorForce - A TensorFlow library for applied reinforcement learning\ngqcnn - Grasp Quality Convolutional Neural Networks (GQ-CNNs) for grasp planning using training datasets from the Dexterity Network (Dex-Net)\nGuided Policy Search - Guided policy search (gps) algorithm and LQG-based trajectory optimization, meant to help others understand, reuse, and build upon existing work.\nDrivers for Sensors, Devices and Arms\nlibfreenect2 - Open source drivers for the Kinect for Windows v2 and Xbox One devices.\niai_kinect2 - Tools for using the Kinect One (Kinect v2) in ROS.\ngrl - Generic Robotics Library: Cross platform drivers for Kuka iiwa and Atracsys FusionTrack with optional v-rep and ros drivers. Also has cross platform Hand Eye Calibration and Tool Tip Calibration.\nDatasets\nCoSTAR Block Stacking Dataset - Robot stacking colored children's blocks (disclaimer: created by @ahundt)\nshapestacks - simulated stacks of colored children's objects\npascal voc 2012 - The classic reference image segmentation dataset.\nopenimages - Huge imagenet style dataset by Google.\nCOCO - Objects with segmentation, keypoints, and links to many other external datasets.\ncocostuff - COCO additional full scene segmentation including backgrounds and annotator.\nGoogle Brain Robot Data - Robotics datasets including grasping, pushing, and pouring.\nMaterials in Context - Materials Dataset with real world images in 23 categories.\nDex-Net 2.0 - 6.7 million pairs of synthetic point clouds and grasps with robustness labels.\nDataset Collection\nLabelFusion - \"A Pipeline for Generating Ground Truth Labels for Real RGBD Data of Cluttered Scenes\" code\ncocostuff - COCO additional full scene segmentation including backgrounds and annotator.\nLinear Algebra & Geometry\nEigen - Eigen is a C++ template library for linear algebra: matrices, vectors, numerical solvers, and related algorithms.\nBoost.QVM - Quaternions, Vectors, Matrices library for Boost.\nBoost.Geometry - Boost.Geometry contains instantiable geometry classes, but library users can also use their own.\nSpaceVecAlg - Implementation of spatial vector algebra for 3D geometry with the Eigen3 linear algebra library.\nSophus - C++ implementation of Lie Groups which are for 3D Geometry, using Eigen.\nPoint Clouds\nlibpointmatcher - An \"Iterative Closest Point\" library robotics and 2-D/3-D mapping.\nPoint Cloud Library (pcl) - The Point Cloud Library (PCL) is a standalone, large scale, open project for 2D/3D image and point cloud processing.\nSimultaneous Localization and Mapping (SLAM)\nElasticFusion - Real-time dense visual SLAM system.\nco-fusion - Real-time Segmentation, Tracking and Fusion of Multiple Objects. Extends ElasticFusion.\nGoogle Cartographer - Cartographer is a system that provides real-time simultaneous localization and mapping (SLAM) in 2D and 3D across multiple platforms and sensor configurations.\nOctoMap - An Efficient Probabilistic 3D Mapping Framework Based on Octrees. Contains the main OctoMap library, the viewer octovis, and dynamicEDT3D.\nORB_SLAM2 - Real-Time SLAM for Monocular, Stereo and RGB-D Cameras, with Loop Detection and Relocalization Capabilities.\nLicense\nThis work is licensed under a Creative Commons Attribution 4.0 International License.", "link": "https://github.com/ahundt/awesome-robotics", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "awesome robotics\nawesome links, software libraries, papers, and other intersting links that are useful for robots.\nrelevant awesome lists\nkiloreaux/awesome-robotics - learn about robotics.\nrobotics libraries - another list of awesome robotics libraries.\nrobotics coursework - a list of robotics courses you can take online\ncomputer vision\ndeep learning - neural networks.\ntensorflow - library for machine intelligence.\npapers - the most cited deep learning papers.\ndeep vision - deep learning for computer vision\ndata visualization - see what your robot is doing with any programming language.\npaperswithcode state of the art - list of state of the art results on various machine learning benchmarks.\nsimulators\ncoppeliasim - create, simulate, any robot. (formerly named v-rep)\nmicrosoft airsim - open source simulator based on unreal engine for autonomous vehicles from microsoft ai & research.\nbullet physics sdk - real-time collision detection and multi-physics simulation for vr, games, visual effects, robotics, machine learning etc. also see pybullet.\nvisualization, video, display, and rendering\npangolin - a lightweight portable rapid development library for managing opengl display / interaction and abstracting video input.\nplotjuggler - quickly -----> plot !!!  and re------> plot !!!  data on the fly! includes optional ros integration.\ndata visualization - a list of awesome data visualization tools.\nmachine learning\ntensorflow related\nkeras - deep learning library for python. convnets, recurrent neural networks, and more. runs on tensorflow or theano.\nkeras-contrib - keras community contributions.\ntensorflow - an open-source software library for machine intelligence.\nrecurrentshop - framework for building complex recurrent neural networks with keras.\ntensorpack - neural network toolbox on tensorflow.\ntensorlayer - deep learning and reinforcement learning library for researchers and engineers.\ntensorflow-examples - tensorflow tutorial and examples for beginners.\nhyperas - keras + hyperopt: a very simple wrapper for convenient hyperparameter optimization.\nelephas - distributed deep learning with keras & spark\npipelineai - end-to-end ml and ai platform for real-time spark and tensorflow data pipelines.\nsonnet - google deepmind apis on top of tensorflow.\nvisipedia/tfrecords - demonstrates the use of tensorflow's tfrecord data format.\nimage segmentation\ntf-image-segmentation - image segmentation framework based on tensorflow and tf-slim library.\nkeras-fcn\nlogging and messaging\nspdlog - super fast c++ logging library.\nlcm - lightweight communications and marshalling, message passing and data marshalling for real-time systems where high-bandwidth and low latency are critical.\ntracking\nsimtrack - a simulation-based framework for tracking.\nar_track_alvar - ar tag tracking library for ros.\nartoolkit5 - augmented reality toolkit, which has excellent ar tag tracking software.\nrobot operating system (ros)\nros - main ros website.\nros2/design - design documentation for ros 2.0 effort.\nkinematics, dynamics, constrained optimization\njrl-umi3218/tasks - tasks is library for real time control of robots and kinematic trees using constrained optimization.\njrl-umi3218/rbdyn - rbdyn provides a set of classes and functions to model the dynamics of rigid body systems.\nceres-solver - solve non-linear least squares problems with bounds constraints and general unconstrained optimization problems. used in production at google since 2010.\norocos_kinematics_dynamics - orocos kinematics and dynamics c++ library.\nflexible-collsion-library - performs three types of proximity queries on a pair of geometric models composed of triangles, integrated with ros.\nrobot_calibration - generic robot kinematics calibration for ros\ncalibration\nhandeye-calib-camodocal - generic robot hand-eye calibration.\nrobot_calibration - generic robot kinematics calibration for ros\nkalibr - camera and imu calibration for ros\nreinforcement learning\n\"good robot!\": efficient reinforcement learning for multi-step visual tasks with sim to real transfer - a real robot completes multi-step tasks after <20k simulated actions. good robot on arxiv (disclaimer: @ahundt is first author)\ntensorforce - a tensorflow library for applied reinforcement learning\ngqcnn - grasp quality convolutional neural networks (gq-cnns) for grasp planning using training datasets from the dexterity network (dex-net)\nguided policy search - guided policy search (gps) algorithm and lqg-based trajectory optimization, meant to help others understand, reuse, and build upon existing work.\ndrivers for sensors, devices and arms\nlibfreenect2 - open source drivers for the kinect for windows v2 and xbox one devices.\niai_kinect2 - tools for using the kinect one (kinect v2) in ros.\ngrl - generic robotics library: cross platform drivers for kuka iiwa and atracsys fusiontrack with optional v-rep and ros drivers. also has cross platform hand eye calibration and tool tip calibration.\ndatasets\ncostar block stacking dataset - robot stacking colored children's blocks (disclaimer: created by @ahundt)\nshapestacks - simulated stacks of colored children's objects\npascal voc 2012 - the classic reference image segmentation dataset.\nopenimages - huge imagenet style dataset by google.\ncoco - objects with segmentation, keypoints, and links to many other external datasets.\ncocostuff - coco additional full scene segmentation including backgrounds and annotator.\ngoogle brain robot data - robotics datasets including grasping, pushing, and pouring.\nmaterials in context - materials dataset with real world images in 23 categories.\ndex-net 2.0 - 6.7 million pairs of synthetic point clouds and grasps with robustness labels.\ndataset collection\nlabelfusion - \"a pipeline for generating ground truth labels for real rgbd data of cluttered scenes\" code\ncocostuff - coco additional full scene segmentation including backgrounds and annotator.\nlinear algebra & geometry\neigen - eigen is a c++ template library for linear algebra: matrices, vectors, numerical solvers, and related algorithms.\nboost.qvm - quaternions, vectors, matrices library for boost.\nboost.geometry - boost.geometry contains instantiable geometry classes, but library users can also use their own.\nspacevecalg - implementation of spatial vector algebra for 3d geometry with the eigen3 linear algebra library.\nsophus - c++ implementation of lie groups which are for 3d geometry, using eigen.\npoint clouds\nlibpointmatcher - an \"iterative closest point\" library robotics and 2-d/3-d mapping.\npoint cloud library (pcl) - the point cloud library (pcl) is a standalone, large scale, open project for 2d/3d image and point cloud processing.\nsimultaneous localization and mapping (slam)\nelasticfusion - real-time dense visual slam system.\nco-fusion - real-time segmentation, tracking and fusion of multiple objects. extends elasticfusion.\ngoogle cartographer - cartographer is a system that provides real-time simultaneous localization and mapping (slam) in 2d and 3d across multiple platforms and sensor configurations.\noctomap - an efficient probabilistic 3d mapping framework based on octrees. contains the main octomap library, the viewer octovis, and dynamicedt3d.\norb_slam2 - real-time slam for monocular, stereo and rgb-d cameras, with loop detection and relocalization capabilities.\nlicense\nthis work is licensed under a creative commons attribution 4.0 international license.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000088, "year": null}, {"Unnamed: 0": 1120, "autor": 100, "date": null, "content": "This repository is no longer maintained. If you are looking for RL implementations, there is Stable-Baselines3, for a training framework, there is the RL Baselines3 Zoo.\nS-RL Toolbox: Reinforcement Learning (RL) and State Representation Learning (SRL) Toolbox for Robotics\nThis repository was made to evaluate State Representation Learning methods using Reinforcement Learning. It integrates (automatic logging, plotting, saving, loading of trained agent) various RL algorithms (PPO, A2C, ARS, ACKTR, DDPG, DQN, ACER, CMA-ES, SAC, TRPO) along with different SRL methods (see SRL Repo) in an efficient way (1 Million steps in 1 Hour with 8-core cpu and 1 Titan X GPU).\nWe also release customizable Gym environments for working with simulation (Kuka arm, Mobile Robot in PyBullet, running at 250 FPS on a 8-core machine) and real robots (Baxter Robot, Robobo with ROS).\nRelated papers:\n\"Decoupling feature extraction from policy learning: assessing benefits of state representation learning in goal based robotics\" (Raffin et al. 2018) https://arxiv.org/abs/1901.08651\n\"S-RL Toolbox: Environments, Datasets and Evaluation Metrics for State Representation Learning\" (Raffin et al., 2018) https://arxiv.org/abs/1809.09369\nMain Features\n10 RL algorithms (Stable Baselines included)\nlogging / plotting / visdom integration / replay trained agent\nhyperparameter search (hyperband, hyperopt)\nintegration with State Representation Learning (SRL) methods (for feature extraction)\nvisualisation tools (explore latent space, display action proba, live plot in the state space, ...)\nrobotics environments to compare SRL methods\neasy install using anaconda env or Docker images (CPU/GPU)\nDocumentation\nDocumentation is available online: https://s-rl-toolbox.readthedocs.io/\nExample\nHere is a quick example of how to train a PPO2 agent on MobileRobotGymEnv-v0 environment for 10 000 steps using 4 parallel processes:\npython -m rl_baselines.train --algo ppo2 --no-vis --num-cpu 4 --num-timesteps 10000 --env MobileRobotGymEnv-v0\nThe complete command (logs will be saved in logs/ folder):\npython -m rl_baselines.train --algo rl_algo --env env1 --log-dir logs/ --srl-model raw_pixels --num-timesteps 10000 --no-vis\nTo use the robot's position as input instead of pixels, just pass --srl-model ground_truth instead of --srl-model raw_pixels\nInstallation\nPython 3 is required (python 2 is not supported because of OpenAI baselines)\nNote: we are using Stable Baselines, a fork of OpenAI Baselines with unified interface and other improvements (e.g. tensorboard support).\nUsing Anaconda\nDownload the project (note the --recursive argument because we are using git submodules):\ngit clone git@github.com:araffin/robotics-rl-srl.git --recursive\nInstall the swig library:\nsudo apt-get install swig\nInstall the dependencies using environment.yml file (for anaconda users) in the current environment\nconda env create --file environment.yml\nsource activate py35\nPyBullet Documentation\nUsing Docker\nPlease read the documentation for more details.\nReinforcement Learning\nSeveral algorithms from Stable Baselines have been integrated along with some evolution strategies and SAC:\nA2C: A synchronous, deterministic variant of Asynchronous Advantage Actor Critic (A3C).\nACER: Sample Efficient Actor-Critic with Experience Replay\nACKTR: Actor Critic using Kronecker-Factored Trust Region\nARS: Augmented Random Search (https://arxiv.org/abs/1803.07055)\nCMA-ES: Covariance Matrix Adaptation Evolution Strategy\nDDPG: Deep Deterministic Policy Gradients\nDeepQ: DQN and variants (Double, Dueling, prioritized experience replay)\nPPO1: Proximal Policy Optimization (MPI Implementation)\nPPO2: Proximal Policy Optimization (GPU Implementation)\nSAC: Soft Actor Critic\nTRPO: Trust Region Policy Optimization (MPI Implementation)\nPlease read the documentation for more details on how to train/load an agent on discrete/continuous actions, and how to add your own rl algorithm.\nHyperparameter Search\nThis repository also allows hyperparameter search, using hyperband or hyperopt for the implemented RL algorithms\nfor example, here is the command for a hyperband search on PPO2, ground truth on the mobile robot environment:\npython -m rl_baselines.hyperparam_search --optimizer hyperband --algo ppo2 --env MobileRobotGymEnv-v0 --srl-model ground_truth\nEnvironments\nAll the environments we propose follow the OpenAI Gym interface. We also extended this interface (adding extra methods) to work with SRL methods (see State Representation Learning Models).\nAvailable Environments\nKuka environment Mobile Robot environment Racing car environment Omnidirectional robot environment\nName Action space (discrete) Action space (continuous) Rewards ground truth\nKuka\nButton 6 actions (3D cardinal direction) 3 axis (3D cardinal direction) (1) 1 when target reached, -1 when too far from target or when table is hit, otherwise 0 (2) (3) the X,Y,Z position of the effector (4)\nKuka\nRandButton 6 actions (3D cardinal direction) 3 axis (3D cardinal direction) (1) 1 when target reached, -1 when too far from target or when table is hit, otherwise 0 (2) (3) the X,Y,Z position of the effector (4)\nKuka\n2Button 6 actions (3D cardinal direction) 3 axis (3D cardinal direction) (1) 1 when the first target is reached, 1 when the second target is reached, -1 when too far from target or when table is hit, otherwise 0 (2) the X,Y,Z position of the effector (4)\nKuka\nMovingButton 6 actions (3D cardinal direction) 3 axis (3D cardinal direction) (1) 1 when target reached, -1 when too far from target or when table is hit, otherwise 0 (2) (3) the X,Y,Z position of the effector (4)\nMobileRobot\n4 actions (2D cardinal direction) 2 axis (2D cardinal direction) 1 when target reached, -1 for a wall hit, otherwise 0 (2) the X,Y position of the robot (4)\nMobileRobot\n2Target 4 actions (2D cardinal direction) 2 axis (2D cardinal direction) 1 when target reached, -1 for a wall hit, otherwise 0 (2) the X,Y position of the robot (4)\nMobileRobot\n1D 2 actions (1D cardinal direction) 1 axis (1D cardinal direction) 1 when target reached, -1 for a wall hit, otherwise 0 (2) the X position of the robot (4)\nMobileRobot\nLineTarget 4 actions (2D cardinal direction) 2 axis (2D cardinal direction) 1 when target reached, -1 for a wall hit, otherwise 0 (2) the X,Y position of the robot (4)\nCarRacing 4 actions (left, right, accelerate, brake) 3 axis (stearing, accelerate, brake) -100 when out of bounds, otherwise -0.1 the X,Y position of the car (4)\nOmniRobot 4 actions (2D cardinal direction) 2 axis (2D cardinal direction) 1 when target reached, -1 for a wall hit, otherwise 0 (2) the X,Y position of the robot (4)\n1. The action space can use 6 axis arm joints control with the --joints flag\n2. The reward can be the euclidian distance to the target with the --shape-reward flag\n3. When using --shape-reward and --continuous, the reward for hitting the button is 50 and for being out of bounds is -250. This is to prevent the agent hitting the table to stop the environment early and obtaining a higher reward\n4. The ground truth can be relative position from agent to the target by changing the RELATIVE_POS constant in the environment file\nthe available environments are:\nKuka arm: Here we have a Kuka arm which must reach a target, here a button.\nKukaButtonGymEnv-v0: Kuka arm with a single button in front.\nKukaRandButtonGymEnv-v0: Kuka arm with a single button in front, and some randomly positioned objects\nKuka2ButtonGymEnv-v0: Kuka arm with 2 buttons next to each others, they must be pressed in the correct order (lighter button, then darker button).\nKukaMovingButtonGymEnv-v0: Kuka arm with a single button in front, slowly moving left to right.\nMobile robot: Here we have a mobile robot which reach a target position\nMobileRobotGymEnv-v0: A mobile robot on a 2d terrain where it needs to reach a target position.\nMobileRobot2TargetGymEnv-v0: A mobile robot on a 2d terrain where it needs to reach two target positions, in the correct order (lighter target, then darker target).\nMobileRobot1DGymEnv-v0: A mobile robot on a 1d slider where it can only go up and down, it must reach a target position.\nMobileRobotLineTargetGymEnv-v0: A mobile robot on a 2d terrain where it needs to reach a colored band going across the terrain.\nRacing car: Here we have the interface for the Gym racing car environment. It must complete a racing course in the least time possible (only available in a terminal with X running)\nCarRacingGymEnv-v0: A racing car on a racing course, it must complete the racing course in the least time possible.\nBaxter: A baxter robot that must reach a target, with its arms. (see Working With Real Robots: Baxter and Robobo)\nBaxter-v0: A bridge to use a baxter robot with ROS (in simulation, it uses Gazebo)\nRobobo: A Robobo robot that must reach a target position.\nRoboboGymEnv-v0: A bridge to use a Robobo robot with ROS.\nOmniRobot: An Omnidirectional robot on a 2d terrain that must reach a target position (see Working With Real Robots: OmniRobot)\nOmnirobotEnv-v0: Simulator but also a bridge to use an OmniRobot with ROS.\nPlease read the documentation for more details (e.g. adding a custom environment).\nState Representation Learning Models\nPlease look the SRL Repo to learn how to train a state representation model. Then you must edit config/srl_models.yaml and set the right path to use the learned state representations.\nThe available state representation models are:\nground_truth: Hand engineered features (e.g., robot position + target position for mobile robot env)\nraw_pixels: Learning a policy in an end-to-end manner, directly from pixels to actions.\nautoencoder: an autoencoder from the raw pixels\ninverse: an inverse dynamics model\nforward: a forward dynamics model\nvae: a variational autoencoder from the raw pixels\nrandom: random features, the feature extractor, a convolutional network, is fixed after random initialization.\nsrl_combination: a model combining several losses (e.g. vae + forward + inverse...) for SRL\nsupervised: A model trained with Ground Truth states as targets in a supervised setting.\nrobotic_priors: Robotic Priors model\npca: pca applied to the raw pixels\nmulti_view_srl: a SRL model using views from multiple cameras as input, with any of the above losses (e.g triplet and others)\njoints: the arm's joints angles (only for Kuka environments)\njoints_position: the arm's x,y,z position and joints angles (only for Kuka environments)\nPlease read the documentation for more details (e.g. adding a custom SRL model).\nTroubleshooting\nIf a submodule is not downloaded:\ngit submodule update --init\nIf you have troubles installing mpi4py, make sure you the following installed:\nsudo apt-get install libopenmpi-dev openmpi-bin openmpi-doc\nKnown issues\nThe inverse kinematics function has trouble finding a solution when the arm is fully straight and the arm must bend to reach the requested point.\nAcknowledgements\nThis work is supported by the DREAM project through the European Union Horizon 2020 FET research and innovation program under grant agreement No 640891.\nCitation\nIf you use this toolbox, please cite:\n@article{Raffin18,\ntitle={S-RL Toolbox: Environments, Datasets and Evaluation Metrics for State Representation Learning},\nauthor={Raffin, Antonin and Hill, Ashley and Traor{\\'e}, Ren{\\'e} and Lesort, Timoth{\\'e}e and D{\\'\\i}az-Rodr{\\'\\i}guez, Natalia and Filliat, David},\njournal={arXiv preprint arXiv:1809.09369},\nyear={2018}\n}", "link": "https://github.com/araffin/robotics-rl-srl", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "this repository is no longer maintained. if you are looking for rl implementations, there is stable-baselines3, for a training framework, there is the rl baselines3 zoo.\ns-rl toolbox: reinforcement learning (rl) and state representation learning (srl) toolbox for robotics\nthis repository was made to evaluate state representation learning methods using reinforcement learning. it integrates (automatic logging, plotting, saving, loading of trained agent) various rl algorithms (ppo, a2c, ars, acktr, ddpg, dqn, acer, cma-es, sac, trpo) along with different srl methods (see srl repo) in an efficient way (1 million steps in 1 hour with 8-core cpu and 1 titan x gpu).\nwe also release customizable gym environments for working with simulation (kuka arm, mobile robot in pybullet, running at 250 fps on a 8-core machine) and real robots (baxter robot, robobo with ros).\nrelated papers:\n\"decoupling feature extraction from policy learning: assessing benefits of state representation learning in goal based robotics\" (raffin et al. 2018) https://arxiv.org/abs/1901.08651\n\"s-rl toolbox: environments, datasets and evaluation metrics for state representation learning\" (raffin et al., 2018) https://arxiv.org/abs/1809.09369\nmain features\n10 rl algorithms (stable baselines included)\nlogging / plotting / visdom integration / replay trained agent\nhyperparameter search (hyperband, hyperopt)\nintegration with state representation learning (srl) methods (for feature extraction)\nvisualisation tools (explore latent space, display action proba, live -----> plot !!!  in the state space, ...)\nrobotics environments to compare srl methods\neasy install using anaconda env or docker images (cpu/gpu)\ndocumentation\ndocumentation is available online: https://s-rl-toolbox.readthedocs.io/\nexample\nhere is a quick example of how to train a ppo2 agent on mobilerobotgymenv-v0 environment for 10 000 steps using 4 parallel processes:\npython -m rl_baselines.train --algo ppo2 --no-vis --num-cpu 4 --num-timesteps 10000 --env mobilerobotgymenv-v0\nthe complete command (logs will be saved in logs/ folder):\npython -m rl_baselines.train --algo rl_algo --env env1 --log-dir logs/ --srl-model raw_pixels --num-timesteps 10000 --no-vis\nto use the robot's position as input instead of pixels, just pass --srl-model ground_truth instead of --srl-model raw_pixels\ninstallation\npython 3 is required (python 2 is not supported because of openai baselines)\nnote: we are using stable baselines, a fork of openai baselines with unified interface and other improvements (e.g. tensorboard support).\nusing anaconda\ndownload the project (note the --recursive argument because we are using git submodules):\ngit clone git@github.com:araffin/robotics-rl-srl.git --recursive\ninstall the swig library:\nsudo apt-get install swig\ninstall the dependencies using environment.yml file (for anaconda users) in the current environment\nconda env create --file environment.yml\nsource activate py35\npybullet documentation\nusing docker\nplease read the documentation for more details.\nreinforcement learning\nseveral algorithms from stable baselines have been integrated along with some evolution strategies and sac:\na2c: a synchronous, deterministic variant of asynchronous advantage actor critic (a3c).\nacer: sample efficient actor-critic with experience replay\nacktr: actor critic using kronecker-factored trust region\nars: augmented random search (https://arxiv.org/abs/1803.07055)\ncma-es: covariance matrix adaptation evolution strategy\nddpg: deep deterministic policy gradients\ndeepq: dqn and variants (double, dueling, prioritized experience replay)\nppo1: proximal policy optimization (mpi implementation)\nppo2: proximal policy optimization (gpu implementation)\nsac: soft actor critic\ntrpo: trust region policy optimization (mpi implementation)\nplease read the documentation for more details on how to train/load an agent on discrete/continuous actions, and how to add your own rl algorithm.\nhyperparameter search\nthis repository also allows hyperparameter search, using hyperband or hyperopt for the implemented rl algorithms\nfor example, here is the command for a hyperband search on ppo2, ground truth on the mobile robot environment:\npython -m rl_baselines.hyperparam_search --optimizer hyperband --algo ppo2 --env mobilerobotgymenv-v0 --srl-model ground_truth\nenvironments\nall the environments we propose follow the openai gym interface. we also extended this interface (adding extra methods) to work with srl methods (see state representation learning models).\navailable environments\nkuka environment mobile robot environment racing car environment omnidirectional robot environment\nname action space (discrete) action space (continuous) rewards ground truth\nkuka\nbutton 6 actions (3d cardinal direction) 3 axis (3d cardinal direction) (1) 1 when target reached, -1 when too far from target or when table is hit, otherwise 0 (2) (3) the x,y,z position of the effector (4)\nkuka\nrandbutton 6 actions (3d cardinal direction) 3 axis (3d cardinal direction) (1) 1 when target reached, -1 when too far from target or when table is hit, otherwise 0 (2) (3) the x,y,z position of the effector (4)\nkuka\n2button 6 actions (3d cardinal direction) 3 axis (3d cardinal direction) (1) 1 when the first target is reached, 1 when the second target is reached, -1 when too far from target or when table is hit, otherwise 0 (2) the x,y,z position of the effector (4)\nkuka\nmovingbutton 6 actions (3d cardinal direction) 3 axis (3d cardinal direction) (1) 1 when target reached, -1 when too far from target or when table is hit, otherwise 0 (2) (3) the x,y,z position of the effector (4)\nmobilerobot\n4 actions (2d cardinal direction) 2 axis (2d cardinal direction) 1 when target reached, -1 for a wall hit, otherwise 0 (2) the x,y position of the robot (4)\nmobilerobot\n2target 4 actions (2d cardinal direction) 2 axis (2d cardinal direction) 1 when target reached, -1 for a wall hit, otherwise 0 (2) the x,y position of the robot (4)\nmobilerobot\n1d 2 actions (1d cardinal direction) 1 axis (1d cardinal direction) 1 when target reached, -1 for a wall hit, otherwise 0 (2) the x position of the robot (4)\nmobilerobot\nlinetarget 4 actions (2d cardinal direction) 2 axis (2d cardinal direction) 1 when target reached, -1 for a wall hit, otherwise 0 (2) the x,y position of the robot (4)\ncarracing 4 actions (left, right, accelerate, brake) 3 axis (stearing, accelerate, brake) -100 when out of bounds, otherwise -0.1 the x,y position of the car (4)\nomnirobot 4 actions (2d cardinal direction) 2 axis (2d cardinal direction) 1 when target reached, -1 for a wall hit, otherwise 0 (2) the x,y position of the robot (4)\n1. the action space can use 6 axis arm joints control with the --joints flag\n2. the reward can be the euclidian distance to the target with the --shape-reward flag\n3. when using --shape-reward and --continuous, the reward for hitting the button is 50 and for being out of bounds is -250. this is to prevent the agent hitting the table to stop the environment early and obtaining a higher reward\n4. the ground truth can be relative position from agent to the target by changing the relative_pos constant in the environment file\nthe available environments are:\nkuka arm: here we have a kuka arm which must reach a target, here a button.\nkukabuttongymenv-v0: kuka arm with a single button in front.\nkukarandbuttongymenv-v0: kuka arm with a single button in front, and some randomly positioned objects\nkuka2buttongymenv-v0: kuka arm with 2 buttons next to each others, they must be pressed in the correct order (lighter button, then darker button).\nkukamovingbuttongymenv-v0: kuka arm with a single button in front, slowly moving left to right.\nmobile robot: here we have a mobile robot which reach a target position\nmobilerobotgymenv-v0: a mobile robot on a 2d terrain where it needs to reach a target position.\nmobilerobot2targetgymenv-v0: a mobile robot on a 2d terrain where it needs to reach two target positions, in the correct order (lighter target, then darker target).\nmobilerobot1dgymenv-v0: a mobile robot on a 1d slider where it can only go up and down, it must reach a target position.\nmobilerobotlinetargetgymenv-v0: a mobile robot on a 2d terrain where it needs to reach a colored band going across the terrain.\nracing car: here we have the interface for the gym racing car environment. it must complete a racing course in the least time possible (only available in a terminal with x running)\ncarracinggymenv-v0: a racing car on a racing course, it must complete the racing course in the least time possible.\nbaxter: a baxter robot that must reach a target, with its arms. (see working with real robots: baxter and robobo)\nbaxter-v0: a bridge to use a baxter robot with ros (in simulation, it uses gazebo)\nrobobo: a robobo robot that must reach a target position.\nrobobogymenv-v0: a bridge to use a robobo robot with ros.\nomnirobot: an omnidirectional robot on a 2d terrain that must reach a target position (see working with real robots: omnirobot)\nomnirobotenv-v0: simulator but also a bridge to use an omnirobot with ros.\nplease read the documentation for more details (e.g. adding a custom environment).\nstate representation learning models\nplease look the srl repo to learn how to train a state representation model. then you must edit config/srl_models.yaml and set the right path to use the learned state representations.\nthe available state representation models are:\nground_truth: hand engineered features (e.g., robot position + target position for mobile robot env)\nraw_pixels: learning a policy in an end-to-end manner, directly from pixels to actions.\nautoencoder: an autoencoder from the raw pixels\ninverse: an inverse dynamics model\nforward: a forward dynamics model\nvae: a variational autoencoder from the raw pixels\nrandom: random features, the feature extractor, a convolutional network, is fixed after random initialization.\nsrl_combination: a model combining several losses (e.g. vae + forward + inverse...) for srl\nsupervised: a model trained with ground truth states as targets in a supervised setting.\nrobotic_priors: robotic priors model\npca: pca applied to the raw pixels\nmulti_view_srl: a srl model using views from multiple cameras as input, with any of the above losses (e.g triplet and others)\njoints: the arm's joints angles (only for kuka environments)\njoints_position: the arm's x,y,z position and joints angles (only for kuka environments)\nplease read the documentation for more details (e.g. adding a custom srl model).\ntroubleshooting\nif a submodule is not downloaded:\ngit submodule update --init\nif you have troubles installing mpi4py, make sure you the following installed:\nsudo apt-get install libopenmpi-dev openmpi-bin openmpi-doc\nknown issues\nthe inverse kinematics function has trouble finding a solution when the arm is fully straight and the arm must bend to reach the requested point.\nacknowledgements\nthis work is supported by the dream project through the european union horizon 2020 fet research and innovation program under grant agreement no 640891.\ncitation\nif you use this toolbox, please cite:\n@article{raffin18,\ntitle={s-rl toolbox: environments, datasets and evaluation metrics for state representation learning},\nauthor={raffin, antonin and hill, ashley and traor{\\'e}, ren{\\'e} and lesort, timoth{\\'e}e and d{\\'\\i}az-rodr{\\'\\i}guez, natalia and filliat, david},\njournal={arxiv preprint arxiv:1809.09369},\nyear={2018}\n}", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000100, "year": null}, {"Unnamed: 0": 1125, "autor": 105, "date": null, "content": "RL Baselines3 Zoo: A Training Framework for Stable Baselines3 Reinforcement Learning Agents\nRL Baselines3 Zoo is a training framework for Reinforcement Learning (RL), using Stable Baselines3.\nIt provides scripts for training, evaluating agents, tuning hyperparameters, plotting results and recording videos.\nIn addition, it includes a collection of tuned hyperparameters for common environments and RL algorithms, and agents trained with those settings.\nWe are looking for contributors to complete the collection!\nGoals of this repository:\nProvide a simple interface to train and enjoy RL agents\nBenchmark the different Reinforcement Learning algorithms\nProvide tuned hyperparameters for each environment and RL algorithm\nHave fun with the trained agents!\nThis is the SB3 version of the original SB2 rl-zoo.\nTrain an Agent\nThe hyperparameters for each environment are defined in hyperparameters/algo_name.yml.\nIf the environment exists in this file, then you can train an agent using:\npython train.py --algo algo_name --env env_id\nFor example (with tensorboard support):\npython train.py --algo ppo --env CartPole-v1 --tensorboard-log /tmp/stable-baselines/\nEvaluate the agent every 10000 steps using 10 episodes for evaluation (using only one evaluation env):\npython train.py --algo sac --env HalfCheetahBulletEnv-v0 --eval-freq 10000 --eval-episodes 10 --n-eval-envs 1\nSave a checkpoint of the agent every 100000 steps:\npython train.py --algo td3 --env HalfCheetahBulletEnv-v0 --save-freq 100000\nContinue training (here, load pretrained agent for Breakout and continue training for 5000 steps):\npython train.py --algo a2c --env BreakoutNoFrameskip-v4 -i rl-trained-agents/a2c/BreakoutNoFrameskip-v4_1/BreakoutNoFrameskip-v4.zip -n 5000\nWhen using off-policy algorithms, you can also save the replay buffer after training:\npython train.py --algo sac --env Pendulum-v0 --save-replay-buffer\nIt will be automatically loaded if present when continuing training.\nPlot Scripts\nPlot scripts (to be documented, see \"Results\" sections in SB3 documentation):\nscripts/all_plots.py/scripts/plot_from_file.py for plotting evaluations\nscripts/plot_train.py for plotting training reward/success\nExamples (on the current collection)\nPlot training success (y-axis) w.r.t. timesteps (x-axis) with a moving window of 500 episodes for all the Fetch environment with HER algorithm:\npython scripts/plot_train.py -a her -e Fetch -y success -f rl-trained-agents/ -w 500 -x steps\nPlot evaluation reward curve for TQC, SAC and TD3 on the HalfCheetah and Ant PyBullet environments:\npython scripts/all_plots.py -a sac td3 tqc --env HalfCheetah Ant -f rl-trained-agents/\nPlot with the rliable library\nThe RL zoo integrates some of rliable library features. You can find a visual explanation of the tools used by rliable in this blog post.\nFirst, you need to install rliable.\nNote: Python 3.7+ is required in that case.\nThen export your results to a file using the all_plots.py script (see above):\npython scripts/all_plots.py -a sac td3 tqc --env Half Ant -f logs/ -o logs/offpolicy\nYou can now use the plot_from_file.py script with --rliable, --versus and --iqm arguments:\npython scripts/plot_from_file.py -i logs/offpolicy.pkl --skip-timesteps --rliable --versus -l SAC TD3 TQC\nNote: you may need to edit plot_from_file.py, in particular the env_key_to_env_id dictionary and the scripts/score_normalization.py which stores min and max score for each environment.\nRemark: plotting with the --rliable option is usually slow as confidence interval need to be computed using bootstrap sampling.\nCustom Environment\nThe easiest way to add support for a custom environment is to edit utils/import_envs.py and register your environment here. Then, you need to add a section for it in the hyperparameters file (hyperparams/algo.yml).\nEnjoy a Trained Agent\nNote: to download the repo with the trained agents, you must use git clone --recursive https://github.com/DLR-RM/rl-baselines3-zoo in order to clone the submodule too.\nIf the trained agent exists, then you can see it in action using:\npython enjoy.py --algo algo_name --env env_id\nFor example, enjoy A2C on Breakout during 5000 timesteps:\npython enjoy.py --algo a2c --env BreakoutNoFrameskip-v4 --folder rl-trained-agents/ -n 5000\nIf you have trained an agent yourself, you need to do:\n# exp-id 0 corresponds to the last experiment, otherwise, you can specify another ID\npython enjoy.py --algo algo_name --env env_id -f logs/ --exp-id 0\nTo load the best model (when using evaluation environment):\npython enjoy.py --algo algo_name --env env_id -f logs/ --exp-id 1 --load-best\nTo load a checkpoint (here the checkpoint name is rl_model_10000_steps.zip):\npython enjoy.py --algo algo_name --env env_id -f logs/ --exp-id 1 --load-checkpoint 10000\nTo load the latest checkpoint:\npython enjoy.py --algo algo_name --env env_id -f logs/ --exp-id 1 --load-last-checkpoint\nHyperparameter yaml syntax\nThe syntax used in hyperparameters/algo_name.yml for setting hyperparameters (likewise the syntax to overwrite hyperparameters on the cli) may be specialized if the argument is a function. See examples in the hyperparameters/ directory. For example:\nSpecify a linear schedule for the learning rate:\nlearning_rate: lin_0.012486195510232303\nSpecify a different activation function for the network:\npolicy_kwargs: \"dict(activation_fn=nn.ReLU)\"\nHyperparameter Tuning\nWe use Optuna for optimizing the hyperparameters. Not all hyperparameters are tuned, and tuning enforces certain default hyperparameter settings that may be different from the official defaults. See utils/hyperparams_opt.py for the current settings for each agent.\nHyperparameters not specified in utils/hyperparams_opt.py are taken from the associated YAML file and fallback to the default values of SB3 if not present.\nNote: hyperparameters search is not implemented for DQN for now. when using SuccessiveHalvingPruner (\"halving\"), you must specify --n-jobs > 1\nBudget of 1000 trials with a maximum of 50000 steps:\npython train.py --algo ppo --env MountainCar-v0 -n 50000 -optimize --n-trials 1000 --n-jobs 2 \\\n--sampler tpe --pruner median\nDistributed optimization using a shared database is also possible (see the corresponding Optuna documentation):\npython train.py --algo ppo --env MountainCar-v0 -optimize --study-name test --storage sqlite:///example.db\nPrint and save best hyperparameters of an Optuna study:\npython scripts/parse_study.py -i path/to/study.pkl --print-n-best-trials 10 --save-n-best-hyperparameters 10\nHyperparameters search space\nNote that the default hyperparameters used in the zoo when tuning are not always the same as the defaults provided in stable-baselines3. Consult the latest source code to be sure of these settings. For example:\nPPO tuning assumes a network architecture with ortho_init = False when tuning, though it is True by default. You can change that by updating utils/hyperparams_opt.py.\nNon-episodic rollout in TD3 and DDPG assumes gradient_steps = train_freq and so tunes only train_freq to reduce the search space.\nWhen working with continuous actions, we recommend to enable gSDE by uncommenting lines in utils/hyperparams_opt.py.\nEnv normalization\nIn the hyperparameter file, normalize: True means that the training environment will be wrapped in a VecNormalize wrapper.\nNormalization uses the default parameters of VecNormalize, with the exception of gamma which is set to match that of the agent. This can be overridden using the appropriate hyperparameters/algo_name.yml, e.g.\nnormalize: \"{'norm_obs': True, 'norm_reward': False}\"\nEnv Wrappers\nYou can specify in the hyperparameter config one or more wrapper to use around the environment:\nfor one wrapper:\nenv_wrapper: gym_minigrid.wrappers.FlatObsWrapper\nfor multiple, specify a list:\nenv_wrapper:\n- utils.wrappers.DoneOnSuccessWrapper:\nreward_offset: 1.0\n- sb3_contrib.common.wrappers.TimeFeatureWrapper\nNote that you can easily specify parameters too.\nCallbacks\nFollowing the same syntax as env wrappers, you can also add custom callbacks to use during training.\ncallback:\n- utils.callbacks.ParallelTrainCallback:\ngradient_steps: 256\nEnv keyword arguments\nYou can specify keyword arguments to pass to the env constructor in the command line, using --env-kwargs:\npython enjoy.py --algo ppo --env MountainCar-v0 --env-kwargs goal_velocity:10\nOverwrite hyperparameters\nYou can easily overwrite hyperparameters in the command line, using --hyperparams:\npython train.py --algo a2c --env MountainCarContinuous-v0 --hyperparams learning_rate:0.001 policy_kwargs:\"dict(net_arch=[64, 64])\"\nNote: if you want to pass a string, you need to escape it like that: my_string:\"'value'\"\nRecord a Video of a Trained Agent\nRecord 1000 steps with the latest saved model:\npython -m utils.record_video --algo ppo --env BipedalWalkerHardcore-v3 -n 1000\nUse the best saved model instead:\npython -m utils.record_video --algo ppo --env BipedalWalkerHardcore-v3 -n 1000 --load-best\nRecord a video of a checkpoint saved during training (here the checkpoint name is rl_model_10000_steps.zip):\npython -m utils.record_video --algo ppo --env BipedalWalkerHardcore-v3 -n 1000 --load-checkpoint 10000\nRecord a Video of a Training Experiment\nApart from recording videos of specific saved models, it is also possible to record a video of a training experiment where checkpoints have been saved.\nRecord 1000 steps for each checkpoint, latest and best saved models:\npython -m utils.record_training --algo ppo --env CartPole-v1 -n 1000 -f logs --deterministic\nThe previous command will create a mp4 file. To convert this file to gif format as well:\npython -m utils.record_training --algo ppo --env CartPole-v1 -n 1000 -f logs --deterministic --gif\nCurrent Collection: 100+ Trained Agents!\nFinal performance of the trained agents can be found in benchmark.md. To compute them, simply run python -m utils.benchmark.\nNOTE: this is not a quantitative benchmark as it corresponds to only one run (cf issue #38). This benchmark is meant to check algorithm (maximal) performance, find potential bugs and also allow users to have access to pretrained agents.\nAtari Games\n7 atari games from OpenAI benchmark (NoFrameskip-v4 versions).\nRL Algo BeamRider Breakout Enduro Pong Qbert Seaquest SpaceInvaders\nA2C \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nPPO \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nDQN \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nQR-DQN \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nAdditional Atari Games (to be completed):\nRL Algo MsPacman Asteroids RoadRunner\nA2C \u2714\ufe0f \u2714\ufe0f\nPPO \u2714\ufe0f \u2714\ufe0f\nDQN \u2714\ufe0f \u2714\ufe0f\nQR-DQN \u2714\ufe0f \u2714\ufe0f\nClassic Control Environments\nRL Algo CartPole-v1 MountainCar-v0 Acrobot-v1 Pendulum-v0 MountainCarContinuous-v0\nA2C \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nPPO \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nDQN \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f N/A N/A\nQR-DQN \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f N/A N/A\nDDPG N/A N/A N/A \u2714\ufe0f \u2714\ufe0f\nSAC N/A N/A N/A \u2714\ufe0f \u2714\ufe0f\nTD3 N/A N/A N/A \u2714\ufe0f \u2714\ufe0f\nTQC N/A N/A N/A \u2714\ufe0f \u2714\ufe0f\nBox2D Environments\nRL Algo BipedalWalker-v3 LunarLander-v2 LunarLanderContinuous-v2 BipedalWalkerHardcore-v3 CarRacing-v0\nA2C \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nPPO \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nDQN N/A \u2714\ufe0f N/A N/A N/A\nQR-DQN N/A \u2714\ufe0f N/A N/A N/A\nDDPG \u2714\ufe0f N/A \u2714\ufe0f\nSAC \u2714\ufe0f N/A \u2714\ufe0f \u2714\ufe0f\nTD3 \u2714\ufe0f N/A \u2714\ufe0f \u2714\ufe0f\nTQC \u2714\ufe0f N/A \u2714\ufe0f \u2714\ufe0f\nPyBullet Environments\nSee https://github.com/bulletphysics/bullet3/tree/master/examples/pybullet/gym/pybullet_envs. Similar to MuJoCo Envs but with a free (MuJoCo 2.1.0+ is now free!) easy to install simulator: pybullet. We are using BulletEnv-v0 version.\nNote: those environments are derived from Roboschool and are harder than the Mujoco version (see Pybullet issue)\nRL Algo Walker2D HalfCheetah Ant Reacher Hopper Humanoid\nA2C \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nPPO \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nDDPG \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nSAC \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nTD3 \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nTQC \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nPyBullet Envs (Continued)\nRL Algo Minitaur MinitaurDuck InvertedDoublePendulum InvertedPendulumSwingup\nA2C\nPPO\nDDPG\nSAC\nTD3\nTQC\nMuJoCo Environments\nRL Algo Walker2d HalfCheetah Ant Swimmer Hopper Humanoid\nA2C \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nPPO \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nDDPG\nSAC \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nTD3 \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nTQC \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nRobotics Environments\nSee https://gym.openai.com/envs/#robotics and https://github.com/DLR-RM/rl-baselines3-zoo/pull/71\nMuJoCo version: 1.50.1.0 Gym version: 0.18.0\nWe used the v1 environments.\nRL Algo FetchReach FetchPickAndPlace FetchPush FetchSlide\nHER+TQC \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nPanda robot Environments\nSee https://github.com/qgallouedec/panda-gym/.\nSimilar to MuJoCo Robotics Envs but with a free easy to install simulator: pybullet.\nWe used the v1 environments.\nRL Algo PandaReach PandaPickAndPlace PandaPush PandaSlide PandaStack\nHER+TQC \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nTo visualize the result, you can pass --env-kwargs render:True to the enjoy script.\nMiniGrid Envs\nSee https://github.com/maximecb/gym-minigrid A simple, lightweight and fast Gym environments implementation of the famous gridworld.\nRL Algo Empty FourRooms DoorKey MultiRoom Fetch\nA2C\nPPO\nDDPG\nSAC\nTRPO\nThere are 19 environment groups (variations for each) in total.\nNote that you need to specify --gym-packages gym_minigrid with enjoy.py and train.py as it is not a standard Gym environment, as well as installing the custom Gym package module or putting it in python path.\npip install gym-minigrid\npython train.py --algo ppo --env MiniGrid-DoorKey-5x5-v0 --gym-packages gym_minigrid\nThis does the same thing as:\nimport gym_minigrid\nColab Notebook: Try it Online!\nYou can train agents online using colab notebook.\nInstallation\nStable-Baselines3 PyPi Package\nWe recommend using stable-baselines3 and sb3_contrib master versions.\napt-get install swig cmake ffmpeg\npip install -r requirements.txt\nPlease see Stable Baselines3 documentation for alternatives.\nDocker Images\nBuild docker image (CPU):\nmake docker-cpu\nGPU:\nUSE_GPU=True make docker-gpu\nPull built docker image (CPU):\ndocker pull stablebaselines/rl-baselines3-zoo-cpu\nGPU image:\ndocker pull stablebaselines/rl-baselines3-zoo\nRun script in the docker image:\n./scripts/run_docker_cpu.sh python train.py --algo ppo --env CartPole-v1\nTests\nTo run tests, first install pytest, then:\nmake pytest\nSame for type checking with pytype:\nmake type\nCiting the Project\nTo cite this repository in publications:\n@misc{rl-zoo3,\nauthor = {Raffin, Antonin},\ntitle = {RL Baselines3 Zoo},\nyear = {2020},\npublisher = {GitHub},\njournal = {GitHub repository},\nhowpublished = {\\url{https://github.com/DLR-RM/rl-baselines3-zoo}},\n}\nContributing\nIf you trained an agent that is not present in the RL Zoo, please submit a Pull Request (containing the hyperparameters and the score too).\nContributors\nWe would like to thanks our contributors: @iandanforth, @tatsubori @Shade5 @mcres", "link": "https://github.com/DLR-RM/rl-baselines3-zoo", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "rl baselines3 zoo: a training framework for stable baselines3 reinforcement learning agents\nrl baselines3 zoo is a training framework for reinforcement learning (rl), using stable baselines3.\nit provides scripts for training, evaluating agents, tuning hyperparameters, plotting results and recording videos.\nin addition, it includes a collection of tuned hyperparameters for common environments and rl algorithms, and agents trained with those settings.\nwe are looking for contributors to complete the collection!\ngoals of this repository:\nprovide a simple interface to train and enjoy rl agents\nbenchmark the different reinforcement learning algorithms\nprovide tuned hyperparameters for each environment and rl algorithm\nhave fun with the trained agents!\nthis is the sb3 version of the original sb2 rl-zoo.\ntrain an agent\nthe hyperparameters for each environment are defined in hyperparameters/algo_name.yml.\nif the environment exists in this file, then you can train an agent using:\npython train.py --algo algo_name --env env_id\nfor example (with tensorboard support):\npython train.py --algo ppo --env cartpole-v1 --tensorboard-log /tmp/stable-baselines/\nevaluate the agent every 10000 steps using 10 episodes for evaluation (using only one evaluation env):\npython train.py --algo sac --env halfcheetahbulletenv-v0 --eval-freq 10000 --eval-episodes 10 --n-eval-envs 1\nsave a checkpoint of the agent every 100000 steps:\npython train.py --algo td3 --env halfcheetahbulletenv-v0 --save-freq 100000\ncontinue training (here, load pretrained agent for breakout and continue training for 5000 steps):\npython train.py --algo a2c --env breakoutnoframeskip-v4 -i rl-trained-agents/a2c/breakoutnoframeskip-v4_1/breakoutnoframeskip-v4.zip -n 5000\nwhen using off-policy algorithms, you can also save the replay buffer after training:\npython train.py --algo sac --env pendulum-v0 --save-replay-buffer\nit will be automatically loaded if present when continuing training.\n-----> plot !!!  scripts\n-----> plot !!!  scripts (to be documented, see \"results\" sections in sb3 documentation):\nscripts/all_plots.py/scripts/plot_from_file.py for plotting evaluations\nscripts/plot_train.py for plotting training reward/success\nexamples (on the current collection)\n-----> plot !!!  training success (y-axis) w.r.t. timesteps (x-axis) with a moving window of 500 episodes for all the fetch environment with her algorithm:\npython scripts/plot_train.py -a her -e fetch -y success -f rl-trained-agents/ -w 500 -x steps\nplot evaluation reward curve for tqc, sac and td3 on the halfcheetah and ant pybullet environments:\npython scripts/all_plots.py -a sac td3 tqc --env halfcheetah ant -f rl-trained-agents/\nplot with the rliable library\nthe rl zoo integrates some of rliable library features. you can find a visual explanation of the tools used by rliable in this blog post.\nfirst, you need to install rliable.\nnote: python 3.7+ is required in that case.\nthen export your results to a file using the all_plots.py script (see above):\npython scripts/all_plots.py -a sac td3 tqc --env half ant -f logs/ -o logs/offpolicy\nyou can now use the plot_from_file.py script with --rliable, --versus and --iqm arguments:\npython scripts/plot_from_file.py -i logs/offpolicy.pkl --skip-timesteps --rliable --versus -l sac td3 tqc\nnote: you may need to edit plot_from_file.py, in particular the env_key_to_env_id dictionary and the scripts/score_normalization.py which stores min and max score for each environment.\nremark: plotting with the --rliable option is usually slow as confidence interval need to be computed using bootstrap sampling.\ncustom environment\nthe easiest way to add support for a custom environment is to edit utils/import_envs.py and register your environment here. then, you need to add a section for it in the hyperparameters file (hyperparams/algo.yml).\nenjoy a trained agent\nnote: to download the repo with the trained agents, you must use git clone --recursive https://github.com/dlr-rm/rl-baselines3-zoo in order to clone the submodule too.\nif the trained agent exists, then you can see it in action using:\npython enjoy.py --algo algo_name --env env_id\nfor example, enjoy a2c on breakout during 5000 timesteps:\npython enjoy.py --algo a2c --env breakoutnoframeskip-v4 --folder rl-trained-agents/ -n 5000\nif you have trained an agent yourself, you need to do:\n# exp-id 0 corresponds to the last experiment, otherwise, you can specify another id\npython enjoy.py --algo algo_name --env env_id -f logs/ --exp-id 0\nto load the best model (when using evaluation environment):\npython enjoy.py --algo algo_name --env env_id -f logs/ --exp-id 1 --load-best\nto load a checkpoint (here the checkpoint name is rl_model_10000_steps.zip):\npython enjoy.py --algo algo_name --env env_id -f logs/ --exp-id 1 --load-checkpoint 10000\nto load the latest checkpoint:\npython enjoy.py --algo algo_name --env env_id -f logs/ --exp-id 1 --load-last-checkpoint\nhyperparameter yaml syntax\nthe syntax used in hyperparameters/algo_name.yml for setting hyperparameters (likewise the syntax to overwrite hyperparameters on the cli) may be specialized if the argument is a function. see examples in the hyperparameters/ directory. for example:\nspecify a linear schedule for the learning rate:\nlearning_rate: lin_0.012486195510232303\nspecify a different activation function for the network:\npolicy_kwargs: \"dict(activation_fn=nn.relu)\"\nhyperparameter tuning\nwe use optuna for optimizing the hyperparameters. not all hyperparameters are tuned, and tuning enforces certain default hyperparameter settings that may be different from the official defaults. see utils/hyperparams_opt.py for the current settings for each agent.\nhyperparameters not specified in utils/hyperparams_opt.py are taken from the associated yaml file and fallback to the default values of sb3 if not present.\nnote: hyperparameters search is not implemented for dqn for now. when using successivehalvingpruner (\"halving\"), you must specify --n-jobs > 1\nbudget of 1000 trials with a maximum of 50000 steps:\npython train.py --algo ppo --env mountaincar-v0 -n 50000 -optimize --n-trials 1000 --n-jobs 2 \\\n--sampler tpe --pruner median\ndistributed optimization using a shared database is also possible (see the corresponding optuna documentation):\npython train.py --algo ppo --env mountaincar-v0 -optimize --study-name test --storage sqlite:///example.db\nprint and save best hyperparameters of an optuna study:\npython scripts/parse_study.py -i path/to/study.pkl --print-n-best-trials 10 --save-n-best-hyperparameters 10\nhyperparameters search space\nnote that the default hyperparameters used in the zoo when tuning are not always the same as the defaults provided in stable-baselines3. consult the latest source code to be sure of these settings. for example:\nppo tuning assumes a network architecture with ortho_init = false when tuning, though it is true by default. you can change that by updating utils/hyperparams_opt.py.\nnon-episodic rollout in td3 and ddpg assumes gradient_steps = train_freq and so tunes only train_freq to reduce the search space.\nwhen working with continuous actions, we recommend to enable gsde by uncommenting lines in utils/hyperparams_opt.py.\nenv normalization\nin the hyperparameter file, normalize: true means that the training environment will be wrapped in a vecnormalize wrapper.\nnormalization uses the default parameters of vecnormalize, with the exception of gamma which is set to match that of the agent. this can be overridden using the appropriate hyperparameters/algo_name.yml, e.g.\nnormalize: \"{'norm_obs': true, 'norm_reward': false}\"\nenv wrappers\nyou can specify in the hyperparameter config one or more wrapper to use around the environment:\nfor one wrapper:\nenv_wrapper: gym_minigrid.wrappers.flatobswrapper\nfor multiple, specify a list:\nenv_wrapper:\n- utils.wrappers.doneonsuccesswrapper:\nreward_offset: 1.0\n- sb3_contrib.common.wrappers.timefeaturewrapper\nnote that you can easily specify parameters too.\ncallbacks\nfollowing the same syntax as env wrappers, you can also add custom callbacks to use during training.\ncallback:\n- utils.callbacks.paralleltraincallback:\ngradient_steps: 256\nenv keyword arguments\nyou can specify keyword arguments to pass to the env constructor in the command line, using --env-kwargs:\npython enjoy.py --algo ppo --env mountaincar-v0 --env-kwargs goal_velocity:10\noverwrite hyperparameters\nyou can easily overwrite hyperparameters in the command line, using --hyperparams:\npython train.py --algo a2c --env mountaincarcontinuous-v0 --hyperparams learning_rate:0.001 policy_kwargs:\"dict(net_arch=[64, 64])\"\nnote: if you want to pass a string, you need to escape it like that: my_string:\"'value'\"\nrecord a video of a trained agent\nrecord 1000 steps with the latest saved model:\npython -m utils.record_video --algo ppo --env bipedalwalkerhardcore-v3 -n 1000\nuse the best saved model instead:\npython -m utils.record_video --algo ppo --env bipedalwalkerhardcore-v3 -n 1000 --load-best\nrecord a video of a checkpoint saved during training (here the checkpoint name is rl_model_10000_steps.zip):\npython -m utils.record_video --algo ppo --env bipedalwalkerhardcore-v3 -n 1000 --load-checkpoint 10000\nrecord a video of a training experiment\napart from recording videos of specific saved models, it is also possible to record a video of a training experiment where checkpoints have been saved.\nrecord 1000 steps for each checkpoint, latest and best saved models:\npython -m utils.record_training --algo ppo --env cartpole-v1 -n 1000 -f logs --deterministic\nthe previous command will create a mp4 file. to convert this file to gif format as well:\npython -m utils.record_training --algo ppo --env cartpole-v1 -n 1000 -f logs --deterministic --gif\ncurrent collection: 100+ trained agents!\nfinal performance of the trained agents can be found in benchmark.md. to compute them, simply run python -m utils.benchmark.\nnote: this is not a quantitative benchmark as it corresponds to only one run (cf issue #38). this benchmark is meant to check algorithm (maximal) performance, find potential bugs and also allow users to have access to pretrained agents.\natari games\n7 atari games from openai benchmark (noframeskip-v4 versions).\nrl algo beamrider breakout enduro pong qbert seaquest spaceinvaders\na2c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nppo \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\ndqn \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nqr-dqn \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nadditional atari games (to be completed):\nrl algo mspacman asteroids roadrunner\na2c \u2714\ufe0f \u2714\ufe0f\nppo \u2714\ufe0f \u2714\ufe0f\ndqn \u2714\ufe0f \u2714\ufe0f\nqr-dqn \u2714\ufe0f \u2714\ufe0f\nclassic control environments\nrl algo cartpole-v1 mountaincar-v0 acrobot-v1 pendulum-v0 mountaincarcontinuous-v0\na2c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nppo \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\ndqn \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f n/a n/a\nqr-dqn \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f n/a n/a\nddpg n/a n/a n/a \u2714\ufe0f \u2714\ufe0f\nsac n/a n/a n/a \u2714\ufe0f \u2714\ufe0f\ntd3 n/a n/a n/a \u2714\ufe0f \u2714\ufe0f\ntqc n/a n/a n/a \u2714\ufe0f \u2714\ufe0f\nbox2d environments\nrl algo bipedalwalker-v3 lunarlander-v2 lunarlandercontinuous-v2 bipedalwalkerhardcore-v3 carracing-v0\na2c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nppo \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\ndqn n/a \u2714\ufe0f n/a n/a n/a\nqr-dqn n/a \u2714\ufe0f n/a n/a n/a\nddpg \u2714\ufe0f n/a \u2714\ufe0f\nsac \u2714\ufe0f n/a \u2714\ufe0f \u2714\ufe0f\ntd3 \u2714\ufe0f n/a \u2714\ufe0f \u2714\ufe0f\ntqc \u2714\ufe0f n/a \u2714\ufe0f \u2714\ufe0f\npybullet environments\nsee https://github.com/bulletphysics/bullet3/tree/master/examples/pybullet/gym/pybullet_envs. similar to mujoco envs but with a free (mujoco 2.1.0+ is now free!) easy to install simulator: pybullet. we are using bulletenv-v0 version.\nnote: those environments are derived from roboschool and are harder than the mujoco version (see pybullet issue)\nrl algo walker2d halfcheetah ant reacher hopper humanoid\na2c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nppo \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nddpg \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nsac \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\ntd3 \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\ntqc \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\npybullet envs (continued)\nrl algo minitaur minitaurduck inverteddoublependulum invertedpendulumswingup\na2c\nppo\nddpg\nsac\ntd3\ntqc\nmujoco environments\nrl algo walker2d halfcheetah ant swimmer hopper humanoid\na2c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nppo \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nddpg\nsac \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\ntd3 \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\ntqc \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nrobotics environments\nsee https://gym.openai.com/envs/#robotics and https://github.com/dlr-rm/rl-baselines3-zoo/pull/71\nmujoco version: 1.50.1.0 gym version: 0.18.0\nwe used the v1 environments.\nrl algo fetchreach fetchpickandplace fetchpush fetchslide\nher+tqc \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\npanda robot environments\nsee https://github.com/qgallouedec/panda-gym/.\nsimilar to mujoco robotics envs but with a free easy to install simulator: pybullet.\nwe used the v1 environments.\nrl algo pandareach pandapickandplace pandapush pandaslide pandastack\nher+tqc \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nto visualize the result, you can pass --env-kwargs render:true to the enjoy script.\nminigrid envs\nsee https://github.com/maximecb/gym-minigrid a simple, lightweight and fast gym environments implementation of the famous gridworld.\nrl algo empty fourrooms doorkey multiroom fetch\na2c\nppo\nddpg\nsac\ntrpo\nthere are 19 environment groups (variations for each) in total.\nnote that you need to specify --gym-packages gym_minigrid with enjoy.py and train.py as it is not a standard gym environment, as well as installing the custom gym package module or putting it in python path.\npip install gym-minigrid\npython train.py --algo ppo --env minigrid-doorkey-5x5-v0 --gym-packages gym_minigrid\nthis does the same thing as:\nimport gym_minigrid\ncolab notebook: try it online!\nyou can train agents online using colab notebook.\ninstallation\nstable-baselines3 pypi package\nwe recommend using stable-baselines3 and sb3_contrib master versions.\napt-get install swig cmake ffmpeg\npip install -r requirements.txt\nplease see stable baselines3 documentation for alternatives.\ndocker images\nbuild docker image (cpu):\nmake docker-cpu\ngpu:\nuse_gpu=true make docker-gpu\npull built docker image (cpu):\ndocker pull stablebaselines/rl-baselines3-zoo-cpu\ngpu image:\ndocker pull stablebaselines/rl-baselines3-zoo\nrun script in the docker image:\n./scripts/run_docker_cpu.sh python train.py --algo ppo --env cartpole-v1\ntests\nto run tests, first install pytest, then:\nmake pytest\nsame for type checking with pytype:\nmake type\nciting the project\nto cite this repository in publications:\n@misc{rl-zoo3,\nauthor = {raffin, antonin},\ntitle = {rl baselines3 zoo},\nyear = {2020},\npublisher = {github},\njournal = {github repository},\nhowpublished = {\\url{https://github.com/dlr-rm/rl-baselines3-zoo}},\n}\ncontributing\nif you trained an agent that is not present in the rl zoo, please submit a pull request (containing the hyperparameters and the score too).\ncontributors\nwe would like to thanks our contributors: @iandanforth, @tatsubori @shade5 @mcres", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000105, "year": null}, {"Unnamed: 0": 1146, "autor": 126, "date": null, "content": "NOTE\nThis repository's master branch is actively developed, please git pull frequently and feel free to open new issues for any undesired, unexpected, or (presumably) incorrect behavior. Thanks \ud83d\ude4f\ngym-pybullet-drones\nSimple OpenAI Gym environment based on PyBullet for multi-agent reinforcement learning with quadrotors\nThe default DroneModel.CF2X dynamics are based on Bitcraze's Crazyflie 2.x nano-quadrotor\nEverything after a $ is entered on a terminal, everything after >>> is passed to a Python interpreter\nTo better understand how the PyBullet back-end works, refer to its Quickstart Guide\nSuggestions and corrections are very welcome in the form of issues and pull requests, respectively\nWhy Reinforcement Learning of Quadrotor Control\nA lot of recent RL research for continuous actions has focused on policy gradient algorithms and actor-critic architectures. A quadrotor is (i) an easy-to-understand mobile robot platform whose (ii) control can be framed as a continuous states and actions problem but, beyond 1-dimension, (iii) it adds the complexity that many candidate policies lead to unrecoverable states, violating the assumption of the existence of a stationary state distribution on the entailed Markov chain.\nOverview\ngym-pybullet-drones AirSim Flightmare\nPhysics PyBullet FastPhysicsEngine/PhysX Ad hoc/Gazebo\nRendering PyBullet Unreal Engine 4 Unity\nLanguage Python C++/C# C++/Python\nRGB/Depth/Segm. views Yes Yes Yes\nMulti-agent control Yes Yes Yes\nROS interface ROS2/Python ROS/C++ ROS/C++\nHardware-In-The-Loop No Yes No\nFully steppable physics Yes No Yes\nAerodynamic effects Drag, downwash, ground Drag Drag\nOpenAI Gym interface Yes No Yes\nRLlib MultiAgentEnv interface Yes No No\nPerformance\nSimulation speed-up with respect to the wall-clock when using\n240Hz (in simulation clock) PyBullet physics for EACH drone\nAND 48Hz (in simulation clock) PID control of EACH drone\nAND nearby obstacles AND a mildly complex background (see GIFs)\nAND 24FPS (in sim. clock), 64x48 pixel capture of 6 channels (RGBA, depth, segm.) on EACH drone\nLenovo P52 (i7-8850H/Quadro P2000) 2020 MacBook Pro (i7-1068NG7)\nRendering OpenGL CPU-based TinyRenderer\nSingle drone, no vision 15.5x 16.8x\nSingle drone with vision 10.8x 1.3x\nMulti-drone (10), no vision 2.1x 2.3x\nMulti-drone (5) with vision 2.5x 0.2x\n80 drones in 4 env, no vision 0.8x 0.95x\nNote: use gui=False and aggregate_phy_steps=int(SIM_HZ/CTRL_HZ) for better performance\nWhile it is easy to\u2014consciously or not\u2014cherry pick statistics, ~5kHz PyBullet physics (CPU-only) is faster than AirSim (1kHz) and more accurate than Flightmare's 35kHz simple single quadcopter dynamics\nExploiting parallel computation\u2014i.e., multiple (80) drones in multiple (4) environments (see script parallelism.sh)\u2014achieves PyBullet physics updates at ~20kHz\nMulti-agent 6-ch. video capture at ~750kB/s with CPU rendering ((64*48)*(4+4+2)*24*5*0.2) is comparable to Flightmare's 240 RGB frames/s ((32*32)*3*240)\u2014although in more complex Unity environments\u2014and up to an order of magnitude faster on Ubuntu, with OpenGL rendering\nRequirements and Installation\nThe repo was written using Python 3.7 with conda on macOS 10.15 and tested on macOS 11, Ubuntu 18.04\nOn macOS and Ubuntu\nMajor dependencies are gym, pybullet, stable-baselines3, and rllib\npip3 install --upgrade numpy Pillow matplotlib cycler\npip3 install --upgrade gym pybullet stable_baselines3 'ray[rllib]'\nVideo recording requires to have ffmpeg installed, on macOS\n$ brew install ffmpeg\nOn Ubuntu\n$ sudo apt install ffmpeg\nThe repo is structured as a Gym Environment and can be installed with pip install --editable\n$ git clone https://github.com/utiasDSL/gym-pybullet-drones.git\n$ cd gym-pybullet-drones/\n$ pip3 install -e .\nOn Ubuntu and with a GPU available, optionally uncomment line 203 of BaseAviary.py to use the eglPlugin\nOn Windows\nCheck these step-by-step instructions written by Dr. Karime Pereida for Windows 10\nExamples\nThere are 2 basic template scripts in examples/: fly.py and learn.py\nfly.py runs an independent flight using PID control implemented in class DSLPIDControl\n$ cd gym-pybullet-drones/examples/\n$ python3 fly.py # Try 'python3 fly.py -h' to show the script's customizable parameters\nTip: use the GUI's sliders and button Use GUI RPM to override the control with interactive inputs\nlearn.py is an RL example to learn take-off using stable-baselines3's A2C or rllib's PPO\n$ cd gym-pybullet-drones/examples/\n$ python3 learn.py # Try 'python3 learn.py -h' to show the script's customizable parameters\nOther scripts in folder examples/ are\ndownwash.py is a flight script with only 2 drones, to test the downwash model\n$ cd gym-pybullet-drones/examples/\n$ python3 downwash.py # Try 'python3 downwash.py -h' to show the script's customizable parameters\ncompare.py which replays and compare to a trace saved in example_trace.pkl\n$ cd gym-pybullet-drones/examples/\n$ python3 compare.py # Try 'python3 compare.py -h' to show the script's customizable parameters\nExperiments\nFolder experiments/learning contains scripts with template learning pipelines\nFor single agent RL problems, using stable-baselines3, run the training script as\n$ cd gym-pybullet-drones/experiments/learning/\n$ python3 singleagent.py --env <env> --algo <alg> --obs <ObservationType> --act <ActionType> --cpu <cpu_num>\nRun the replay script to visualize the best trained agent(s) as\n$ python3 test_singleagent.py --exp ./results/save-<env>-<algo>-<obs>-<act>-<time-date>\nFor multi-agent RL problems, using rllib run the train script as\n$ cd gym-pybullet-drones/experiments/learning/\n$ python3 multiagent.py --num_drones <num_drones> --env <env> --obs <ObservationType> --act <ActionType> --algo <alg> --num_workers <num_workers>\nRun the replay script to visualize the best trained agent(s) as\n$ python3 test_multiagent.py --exp ./results/save-<env>-<num_drones>-<algo>-<obs>-<act>-<date>\nClass BaseAviary\nA flight arena for one (ore more) quadrotor can be created as a subclass of BaseAviary()\n>>> env = BaseAviary(\n>>> drone_model=DroneModel.CF2X, # See DroneModel Enum class for other quadcopter models\n>>> num_drones=1, # Number of drones\n>>> neighbourhood_radius=np.inf, # Distance at which drones are considered neighbors, only used for multiple drones\n>>> initial_xyzs=None, # Initial XYZ positions of the drones\n>>> initial_rpys=None, # Initial roll, pitch, and yaw of the drones in radians\n>>> physics: Physics=Physics.PYB, # Choice of (PyBullet) physics implementation\n>>> freq=240, # Stepping frequency of the simulation\n>>> aggregate_phy_steps=1, # Number of physics updates within each call to BaseAviary.step()\n>>> gui=True, # Whether to display PyBullet's GUI, only use this for debbuging\n>>> record=False, # Whether to save a .mp4 video (if gui=True) or .png frames (if gui=False) in gym-pybullet-drones/files/, see script /files/videos/ffmpeg_png2mp4.sh for encoding\n>>> obstacles=False, # Whether to add obstacles to the environment\n>>> user_debug_gui=True) # Whether to use addUserDebugLine and addUserDebugParameter calls (it can slow down the GUI)\nAnd instantiated with gym.make()\u2014see learn.py for an example\n>>> env = gym.make('rl-takeoff-aviary-v0') # See learn.py\nThen, the environment can be stepped with\n>>> obs = env.reset()\n>>> for _ in range(10*240):\n>>> obs, reward, done, info = env.step(env.action_space.sample())\n>>> env.render()\n>>> if done: obs = env.reset()\n>>> env.close()\nCreating New Aviaries\nA new RL problem can be created as a subclass of BaseAviary (i.e. class NewAviary(BaseAviary): ...) and implementing the following 7 abstract methods\n>>> #### 1\n>>> def _actionSpace(self):\n>>> # e.g. return spaces.Box(low=np.zeros(4), high=np.ones(4), dtype=np.float32)\n>>> #### 2\n>>> def _observationSpace(self):\n>>> # e.g. return spaces.Box(low=np.zeros(20), high=np.ones(20), dtype=np.float32)\n>>> #### 3\n>>> def _computeObs(self):\n>>> # e.g. return self._getDroneStateVector(0)\n>>> #### 4\n>>> def _preprocessAction(self, action):\n>>> # e.g. return np.clip(action, 0, 1)\n>>> #### 5\n>>> def _computeReward(self):\n>>> # e.g. return -1\n>>> #### 6\n>>> def _computeDone(self):\n>>> # e.g. return False\n>>> #### 7\n>>> def _computeInfo(self):\n>>> # e.g. return {\"answer\": 42} # Calculated by the Deep Thought supercomputer in 7.5M years\nSee CtrlAviary, VisionAviary, HoverAviary, and FlockAviary for examples\nAction Spaces Examples\nThe action space's definition of an environment must be implemented in each subclass of BaseAviary by function\n>>> def _actionSpace(self):\n>>> ...\nIn CtrlAviary and VisionAviary, it is a Dict() of Box(4,) containing the drones' commanded RPMs\nThe dictionary's keys are \"0\", \"1\", .., \"n\"\u2014where n is the number of drones\nEach subclass of BaseAviary also needs to implement a preprocessing step translating actions into RPMs\n>>> def _preprocessAction(self, action):\n>>> ...\nCtrlAviary, VisionAviary, HoverAviary, and FlockAviary all simply clip the inputs to MAX_RPM\nDynAviary's action input to DynAviary.step() is a Dict() of Box(4,) containing\nThe desired thrust along the drone's z-axis\nThe desired torque around the drone's x-axis\nThe desired torque around the drone's y-axis\nThe desired torque around the drone's z-axis\nFrom these, desired RPMs are computed by DynAviary._preprocessAction()\nObservation Spaces Examples\nThe observation space's definition of an environment must be implemented by every subclass of BaseAviary\n>>> def _observationSpace(self):\n>>> ...\nIn CtrlAviary, it is a Dict() of pairs {\"state\": Box(20,), \"neighbors\": MultiBinary(num_drones)}\nThe dictionary's keys are \"0\", \"1\", .., \"n\"\u2014where n is the number of drones\nEach Box(20,) contains the drone's\nX, Y, Z position in WORLD_FRAME (in meters, 3 values)\nQuaternion orientation in WORLD_FRAME (4 values)\nRoll, pitch and yaw angles in WORLD_FRAME (in radians, 3 values)\nThe velocity vector in WORLD_FRAME (in m/s, 3 values)\nAngular velocity in WORLD_FRAME (3 values)\nMotors' speeds (in RPMs, 4 values)\nEach MultiBinary(num_drones) contains the drone's own row of the multi-robot system adjacency matrix\nThe observation space of VisionAviary is the same asCtrlAviary but also includes keys rgb, dep, and seg (in each drone's dictionary) for the matrices containing the drone's RGB, depth, and segmentation views\nTo fill/customize the content of obs, every subclass of BaseAviary needs to implement\n>>> def _computeObs(self, action):\n>>> ...\nSee BaseAviary._exportImage()) and its use in VisionAviary._computeObs() to save frames as PNGs\nObstacles\nObjects can be added to an environment using loadURDF (or loadSDF, loadMJCF) in method _addObstacles()\n>>> def _addObstacles(self):\n>>> ...\n>>> p.loadURDF(\"sphere2.urdf\", [0,0,0], p.getQuaternionFromEuler([0,0,0]), physicsClientId=self.CLIENT)\nDrag, Ground Effect, and Downwash Models\nSimple drag, ground effect, and downwash models can be included in the simulation initializing BaseAviary() with physics=Physics.PYB_GND_DRAG_DW, these are based on the system identification of Forster (2015) (Eq. 4.2), the analytical model used as a baseline for comparison by Shi et al. (2019) (Eq. 15), and DSL's experimental work\nCheck the implementations of _drag(), _groundEffect(), and _downwash() in BaseAviary for more detail\nRGB, Depth, and Segmentation Views\nPID Control\nFolder control contains the implementations of 2 PID controllers\nDSLPIDControl (for DroneModel.CF2X/P) and SimplePIDControl (for DroneModel.HB) can be used as\n>>> ctrl = [DSLPIDControl(drone_model=DroneModel.CF2X) for i in range(num_drones)] # Initialize \"num_drones\" controllers\n>>> ...\n>>> for i in range(num_drones): # Compute control for each drone\n>>> action[str(i)], _, _ = ctrl[i].computeControlFromState(. # Write the action in a dictionary\n>>> control_timestep=env.TIMESTEP,\n>>> state=obs[str(i)][\"state\"],\n>>> target_pos=TARGET_POS)\nFor high-level coordination\u2014using a velocity input\u2014VelocityAviary integrates PID control within a gym.Env.\nMethod setPIDCoefficients can be used to change the coefficients of one of the given PID controllers\u2014and, for example, implement learning problems whose goal is parameter tuning (see TuneAviary).\nLogger\nClass Logger contains helper functions to save and plot simulation data, as in this example\n>>> logger = Logger(logging_freq_hz=freq, num_drones=num_drones) # Initialize the logger\n>>> ...\n>>> for i in range(NUM_DRONES): # Log information for each drone\n>>> logger.log(drone=i,\n>>> timestamp=K/env.SIM_FREQ,\n>>> state= obs[str(i)][\"state\"],\n>>> control=np.hstack([ TARGET_POS, np.zeros(9) ]))\n>>> ...\n>>> logger.save() # Save data to file\n>>> logger.plot() # Plot data\nROS2 Python Wrapper\nWorkspace ros2 contains two ROS2 Foxy Fitzroy Python nodes\nAviaryWrapper is a wrapper node for a single-drone CtrlAviary environment\nRandomControl reads AviaryWrapper's obs topic and publishes random RPMs on topic action\nWith ROS2 installed (on either macOS or Ubuntu, edit ros2_and_pkg_setups.(zsh/bash) accordingly), run\n$ cd gym-pybullet-drones/ros2/\n$ source ros2_and_pkg_setups.zsh # On macOS, on Ubuntu use $ source ros2_and_pkg_setups.bash\n$ colcon build --packages-select ros2_gym_pybullet_drones\n$ source ros2_and_pkg_setups.zsh # On macOS, on Ubuntu use $ source ros2_and_pkg_setups.bash\n$ ros2 run ros2_gym_pybullet_drones aviary_wrapper\nIn a new terminal terminal, run\n$ cd gym-pybullet-drones/ros2/\n$ source ros2_and_pkg_setups.zsh # On macOS, on Ubuntu use $ source ros2_and_pkg_setups.bash\n$ ros2 run ros2_gym_pybullet_drones random_control\nDesiderata/WIP\nTemplate scripts using PyMARL\nGoogle Colaboratory example\nAlternative multi-contribution downwash effect\nCitation\nIf you wish, please cite our work (link) as\n@INPROCEEDINGS{panerati2021learning,\ntitle={Learning to Fly---a Gym Environment with PyBullet Physics for Reinforcement Learning of Multi-agent Quadcopter Control},\nauthor={Jacopo Panerati and Hehui Zheng and SiQi Zhou and James Xu and Amanda Prorok and Angela P. Schoellig},\nbooktitle={2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},\nyear={2021},\nvolume={},\nnumber={},\npages={},\ndoi={}\n}\nReferences\nNathan Michael, Daniel Mellinger, Quentin Lindsey, Vijay Kumar (2010) The GRASP Multiple Micro UAV Testbed\nBenoit Landry (2014) Planning and Control for Quadrotor Flight through Cluttered Environments\nJulian Forster (2015) System Identification of the Crazyflie 2.0 Nano Quadrocopter\nCarlos Luis and Jeroome Le Ny (2016) Design of a Trajectory Tracking Controller for a Nanoquadcopter\nShital Shah, Debadeepta Dey, Chris Lovett, and Ashish Kapoor (2017) AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles\nEric Liang, Richard Liaw, Philipp Moritz, Robert Nishihara, Roy Fox, Ken Goldberg, Joseph E. Gonzalez, Michael I. Jordan, and Ion Stoica (2018) RLlib: Abstractions for Distributed Reinforcement Learning\nAntonin Raffin, Ashley Hill, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, and Noah Dormann (2019) Stable Baselines3\nGuanya Shi, Xichen Shi, Michael O\u2019Connell, Rose Yu, Kamyar Azizzadenesheli, Animashree Anandkumar, Yisong Yue, and Soon-Jo Chung (2019) Neural Lander: Stable Drone Landing Control Using Learned Dynamics\nMikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas Nardelli, Tim G. J. Rudner, Chia-Man Hung, Philip H. S. Torr, Jakob Foerster, and Shimon Whiteson (2019) The StarCraft Multi-Agent Challenge\nC. Karen Liu and Dan Negrut (2020) The Role of Physics-Based Simulators in Robotics\nYunlong Song, Selim Naji, Elia Kaufmann, Antonio Loquercio, and Davide Scaramuzza (2020) Flightmare: A Flexible Quadrotor Simulator\nBonus GIF for scrolling this far\nUniversity of Toronto's Dynamic Systems Lab / Vector Institute / University of Cambridge's Prorok Lab / Mitacs", "link": "https://github.com/utiasDSL/gym-pybullet-drones", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "note\nthis repository's master branch is actively developed, please git pull frequently and feel free to open new issues for any undesired, unexpected, or (presumably) incorrect behavior. thanks \ud83d\ude4f\ngym-pybullet-drones\nsimple openai gym environment based on pybullet for multi-agent reinforcement learning with quadrotors\nthe default dronemodel.cf2x dynamics are based on bitcraze's crazyflie 2.x nano-quadrotor\neverything after a $ is entered on a terminal, everything after >>> is passed to a python interpreter\nto better understand how the pybullet back-end works, refer to its quickstart guide\nsuggestions and corrections are very welcome in the form of issues and pull requests, respectively\nwhy reinforcement learning of quadrotor control\na lot of recent rl research for continuous actions has focused on policy gradient algorithms and actor-critic architectures. a quadrotor is (i) an easy-to-understand mobile robot platform whose (ii) control can be framed as a continuous states and actions problem but, beyond 1-dimension, (iii) it adds the complexity that many candidate policies lead to unrecoverable states, violating the assumption of the existence of a stationary state distribution on the entailed markov chain.\noverview\ngym-pybullet-drones airsim flightmare\nphysics pybullet fastphysicsengine/physx ad hoc/gazebo\nrendering pybullet unreal engine 4 unity\nlanguage python c++/c# c++/python\nrgb/depth/segm. views yes yes yes\nmulti-agent control yes yes yes\nros interface ros2/python ros/c++ ros/c++\nhardware-in-the-loop no yes no\nfully steppable physics yes no yes\naerodynamic effects drag, downwash, ground drag drag\nopenai gym interface yes no yes\nrllib multiagentenv interface yes no no\nperformance\nsimulation speed-up with respect to the wall-clock when using\n240hz (in simulation clock) pybullet physics for each drone\nand 48hz (in simulation clock) pid control of each drone\nand nearby obstacles and a mildly complex background (see gifs)\nand 24fps (in sim. clock), 64x48 pixel capture of 6 channels (rgba, depth, segm.) on each drone\nlenovo p52 (i7-8850h/quadro p2000) 2020 macbook pro (i7-1068ng7)\nrendering opengl cpu-based tinyrenderer\nsingle drone, no vision 15.5x 16.8x\nsingle drone with vision 10.8x 1.3x\nmulti-drone (10), no vision 2.1x 2.3x\nmulti-drone (5) with vision 2.5x 0.2x\n80 drones in 4 env, no vision 0.8x 0.95x\nnote: use gui=false and aggregate_phy_steps=int(sim_hz/ctrl_hz) for better performance\nwhile it is easy to\u2014consciously or not\u2014cherry pick statistics, ~5khz pybullet physics (cpu-only) is faster than airsim (1khz) and more accurate than flightmare's 35khz simple single quadcopter dynamics\nexploiting parallel computation\u2014i.e., multiple (80) drones in multiple (4) environments (see script parallelism.sh)\u2014achieves pybullet physics updates at ~20khz\nmulti-agent 6-ch. video capture at ~750kb/s with cpu rendering ((64*48)*(4+4+2)*24*5*0.2) is comparable to flightmare's 240 rgb frames/s ((32*32)*3*240)\u2014although in more complex unity environments\u2014and up to an order of magnitude faster on ubuntu, with opengl rendering\nrequirements and installation\nthe repo was written using python 3.7 with conda on macos 10.15 and tested on macos 11, ubuntu 18.04\non macos and ubuntu\nmajor dependencies are gym, pybullet, stable-baselines3, and rllib\npip3 install --upgrade numpy pillow matplotlib cycler\npip3 install --upgrade gym pybullet stable_baselines3 'ray[rllib]'\nvideo recording requires to have ffmpeg installed, on macos\n$ brew install ffmpeg\non ubuntu\n$ sudo apt install ffmpeg\nthe repo is structured as a gym environment and can be installed with pip install --editable\n$ git clone https://github.com/utiasdsl/gym-pybullet-drones.git\n$ cd gym-pybullet-drones/\n$ pip3 install -e .\non ubuntu and with a gpu available, optionally uncomment line 203 of baseaviary.py to use the eglplugin\non windows\ncheck these step-by-step instructions written by dr. karime pereida for windows 10\nexamples\nthere are 2 basic template scripts in examples/: fly.py and learn.py\nfly.py runs an independent flight using pid control implemented in class dslpidcontrol\n$ cd gym-pybullet-drones/examples/\n$ python3 fly.py # try 'python3 fly.py -h' to show the script's customizable parameters\ntip: use the gui's sliders and button use gui rpm to override the control with interactive inputs\nlearn.py is an rl example to learn take-off using stable-baselines3's a2c or rllib's ppo\n$ cd gym-pybullet-drones/examples/\n$ python3 learn.py # try 'python3 learn.py -h' to show the script's customizable parameters\nother scripts in folder examples/ are\ndownwash.py is a flight script with only 2 drones, to test the downwash model\n$ cd gym-pybullet-drones/examples/\n$ python3 downwash.py # try 'python3 downwash.py -h' to show the script's customizable parameters\ncompare.py which replays and compare to a trace saved in example_trace.pkl\n$ cd gym-pybullet-drones/examples/\n$ python3 compare.py # try 'python3 compare.py -h' to show the script's customizable parameters\nexperiments\nfolder experiments/learning contains scripts with template learning pipelines\nfor single agent rl problems, using stable-baselines3, run the training script as\n$ cd gym-pybullet-drones/experiments/learning/\n$ python3 singleagent.py --env <env> --algo <alg> --obs <observationtype> --act <actiontype> --cpu <cpu_num>\nrun the replay script to visualize the best trained agent(s) as\n$ python3 test_singleagent.py --exp ./results/save-<env>-<algo>-<obs>-<act>-<time-date>\nfor multi-agent rl problems, using rllib run the train script as\n$ cd gym-pybullet-drones/experiments/learning/\n$ python3 multiagent.py --num_drones <num_drones> --env <env> --obs <observationtype> --act <actiontype> --algo <alg> --num_workers <num_workers>\nrun the replay script to visualize the best trained agent(s) as\n$ python3 test_multiagent.py --exp ./results/save-<env>-<num_drones>-<algo>-<obs>-<act>-<date>\nclass baseaviary\na flight arena for one (ore more) quadrotor can be created as a subclass of baseaviary()\n>>> env = baseaviary(\n>>> drone_model=dronemodel.cf2x, # see dronemodel enum class for other quadcopter models\n>>> num_drones=1, # number of drones\n>>> neighbourhood_radius=np.inf, # distance at which drones are considered neighbors, only used for multiple drones\n>>> initial_xyzs=none, # initial xyz positions of the drones\n>>> initial_rpys=none, # initial roll, pitch, and yaw of the drones in radians\n>>> physics: physics=physics.pyb, # choice of (pybullet) physics implementation\n>>> freq=240, # stepping frequency of the simulation\n>>> aggregate_phy_steps=1, # number of physics updates within each call to baseaviary.step()\n>>> gui=true, # whether to display pybullet's gui, only use this for debbuging\n>>> record=false, # whether to save a .mp4 video (if gui=true) or .png frames (if gui=false) in gym-pybullet-drones/files/, see script /files/videos/ffmpeg_png2mp4.sh for encoding\n>>> obstacles=false, # whether to add obstacles to the environment\n>>> user_debug_gui=true) # whether to use adduserdebugline and adduserdebugparameter calls (it can slow down the gui)\nand instantiated with gym.make()\u2014see learn.py for an example\n>>> env = gym.make('rl-takeoff-aviary-v0') # see learn.py\nthen, the environment can be stepped with\n>>> obs = env.reset()\n>>> for _ in range(10*240):\n>>> obs, reward, done, info = env.step(env.action_space.sample())\n>>> env.render()\n>>> if done: obs = env.reset()\n>>> env.close()\ncreating new aviaries\na new rl problem can be created as a subclass of baseaviary (i.e. class newaviary(baseaviary): ...) and implementing the following 7 abstract methods\n>>> #### 1\n>>> def _actionspace(self):\n>>> # e.g. return spaces.box(low=np.zeros(4), high=np.ones(4), dtype=np.float32)\n>>> #### 2\n>>> def _observationspace(self):\n>>> # e.g. return spaces.box(low=np.zeros(20), high=np.ones(20), dtype=np.float32)\n>>> #### 3\n>>> def _computeobs(self):\n>>> # e.g. return self._getdronestatevector(0)\n>>> #### 4\n>>> def _preprocessaction(self, action):\n>>> # e.g. return np.clip(action, 0, 1)\n>>> #### 5\n>>> def _computereward(self):\n>>> # e.g. return -1\n>>> #### 6\n>>> def _computedone(self):\n>>> # e.g. return false\n>>> #### 7\n>>> def _computeinfo(self):\n>>> # e.g. return {\"answer\": 42} # calculated by the deep thought supercomputer in 7.5m years\nsee ctrlaviary, visionaviary, hoveraviary, and flockaviary for examples\naction spaces examples\nthe action space's definition of an environment must be implemented in each subclass of baseaviary by function\n>>> def _actionspace(self):\n>>> ...\nin ctrlaviary and visionaviary, it is a dict() of box(4,) containing the drones' commanded rpms\nthe dictionary's keys are \"0\", \"1\", .., \"n\"\u2014where n is the number of drones\neach subclass of baseaviary also needs to implement a preprocessing step translating actions into rpms\n>>> def _preprocessaction(self, action):\n>>> ...\nctrlaviary, visionaviary, hoveraviary, and flockaviary all simply clip the inputs to max_rpm\ndynaviary's action input to dynaviary.step() is a dict() of box(4,) containing\nthe desired thrust along the drone's z-axis\nthe desired torque around the drone's x-axis\nthe desired torque around the drone's y-axis\nthe desired torque around the drone's z-axis\nfrom these, desired rpms are computed by dynaviary._preprocessaction()\nobservation spaces examples\nthe observation space's definition of an environment must be implemented by every subclass of baseaviary\n>>> def _observationspace(self):\n>>> ...\nin ctrlaviary, it is a dict() of pairs {\"state\": box(20,), \"neighbors\": multibinary(num_drones)}\nthe dictionary's keys are \"0\", \"1\", .., \"n\"\u2014where n is the number of drones\neach box(20,) contains the drone's\nx, y, z position in world_frame (in meters, 3 values)\nquaternion orientation in world_frame (4 values)\nroll, pitch and yaw angles in world_frame (in radians, 3 values)\nthe velocity vector in world_frame (in m/s, 3 values)\nangular velocity in world_frame (3 values)\nmotors' speeds (in rpms, 4 values)\neach multibinary(num_drones) contains the drone's own row of the multi-robot system adjacency matrix\nthe observation space of visionaviary is the same asctrlaviary but also includes keys rgb, dep, and seg (in each drone's dictionary) for the matrices containing the drone's rgb, depth, and segmentation views\nto fill/customize the content of obs, every subclass of baseaviary needs to implement\n>>> def _computeobs(self, action):\n>>> ...\nsee baseaviary._exportimage()) and its use in visionaviary._computeobs() to save frames as pngs\nobstacles\nobjects can be added to an environment using loadurdf (or loadsdf, loadmjcf) in method _addobstacles()\n>>> def _addobstacles(self):\n>>> ...\n>>> p.loadurdf(\"sphere2.urdf\", [0,0,0], p.getquaternionfromeuler([0,0,0]), physicsclientid=self.client)\ndrag, ground effect, and downwash models\nsimple drag, ground effect, and downwash models can be included in the simulation initializing baseaviary() with physics=physics.pyb_gnd_drag_dw, these are based on the system identification of forster (2015) (eq. 4.2), the analytical model used as a baseline for comparison by shi et al. (2019) (eq. 15), and dsl's experimental work\ncheck the implementations of _drag(), _groundeffect(), and _downwash() in baseaviary for more detail\nrgb, depth, and segmentation views\npid control\nfolder control contains the implementations of 2 pid controllers\ndslpidcontrol (for dronemodel.cf2x/p) and simplepidcontrol (for dronemodel.hb) can be used as\n>>> ctrl = [dslpidcontrol(drone_model=dronemodel.cf2x) for i in range(num_drones)] # initialize \"num_drones\" controllers\n>>> ...\n>>> for i in range(num_drones): # compute control for each drone\n>>> action[str(i)], _, _ = ctrl[i].computecontrolfromstate(. # write the action in a dictionary\n>>> control_timestep=env.timestep,\n>>> state=obs[str(i)][\"state\"],\n>>> target_pos=target_pos)\nfor high-level coordination\u2014using a velocity input\u2014velocityaviary integrates pid control within a gym.env.\nmethod setpidcoefficients can be used to change the coefficients of one of the given pid controllers\u2014and, for example, implement learning problems whose goal is parameter tuning (see tuneaviary).\nlogger\nclass logger contains helper functions to save and -----> plot !!!  simulation data, as in this example\n>>> logger = logger(logging_freq_hz=freq, num_drones=num_drones) # initialize the logger\n>>> ...\n>>> for i in range(num_drones): # log information for each drone\n>>> logger.log(drone=i,\n>>> timestamp=k/env.sim_freq,\n>>> state= obs[str(i)][\"state\"],\n>>> control=np.hstack([ target_pos, np.zeros(9) ]))\n>>> ...\n>>> logger.save() # save data to file\n>>> logger.plot() # plot data\nros2 python wrapper\nworkspace ros2 contains two ros2 foxy fitzroy python nodes\naviarywrapper is a wrapper node for a single-drone ctrlaviary environment\nrandomcontrol reads aviarywrapper's obs topic and publishes random rpms on topic action\nwith ros2 installed (on either macos or ubuntu, edit ros2_and_pkg_setups.(zsh/bash) accordingly), run\n$ cd gym-pybullet-drones/ros2/\n$ source ros2_and_pkg_setups.zsh # on macos, on ubuntu use $ source ros2_and_pkg_setups.bash\n$ colcon build --packages-select ros2_gym_pybullet_drones\n$ source ros2_and_pkg_setups.zsh # on macos, on ubuntu use $ source ros2_and_pkg_setups.bash\n$ ros2 run ros2_gym_pybullet_drones aviary_wrapper\nin a new terminal terminal, run\n$ cd gym-pybullet-drones/ros2/\n$ source ros2_and_pkg_setups.zsh # on macos, on ubuntu use $ source ros2_and_pkg_setups.bash\n$ ros2 run ros2_gym_pybullet_drones random_control\ndesiderata/wip\ntemplate scripts using pymarl\ngoogle colaboratory example\nalternative multi-contribution downwash effect\ncitation\nif you wish, please cite our work (link) as\n@inproceedings{panerati2021learning,\ntitle={learning to fly---a gym environment with pybullet physics for reinforcement learning of multi-agent quadcopter control},\nauthor={jacopo panerati and hehui zheng and siqi zhou and james xu and amanda prorok and angela p. schoellig},\nbooktitle={2021 ieee/rsj international conference on intelligent robots and systems (iros)},\nyear={2021},\nvolume={},\nnumber={},\npages={},\ndoi={}\n}\nreferences\nnathan michael, daniel mellinger, quentin lindsey, vijay kumar (2010) the grasp multiple micro uav testbed\nbenoit landry (2014) planning and control for quadrotor flight through cluttered environments\njulian forster (2015) system identification of the crazyflie 2.0 nano quadrocopter\ncarlos luis and jeroome le ny (2016) design of a trajectory tracking controller for a nanoquadcopter\nshital shah, debadeepta dey, chris lovett, and ashish kapoor (2017) airsim: high-fidelity visual and physical simulation for autonomous vehicles\neric liang, richard liaw, philipp moritz, robert nishihara, roy fox, ken goldberg, joseph e. gonzalez, michael i. jordan, and ion stoica (2018) rllib: abstractions for distributed reinforcement learning\nantonin raffin, ashley hill, maximilian ernestus, adam gleave, anssi kanervisto, and noah dormann (2019) stable baselines3\nguanya shi, xichen shi, michael o\u2019connell, rose yu, kamyar azizzadenesheli, animashree anandkumar, yisong yue, and soon-jo chung (2019) neural lander: stable drone landing control using learned dynamics\nmikayel samvelyan, tabish rashid, christian schroeder de witt, gregory farquhar, nantas nardelli, tim g. j. rudner, chia-man hung, philip h. s. torr, jakob foerster, and shimon whiteson (2019) the starcraft multi-agent challenge\nc. karen liu and dan negrut (2020) the role of physics-based simulators in robotics\nyunlong song, selim naji, elia kaufmann, antonio loquercio, and davide scaramuzza (2020) flightmare: a flexible quadrotor simulator\nbonus gif for scrolling this far\nuniversity of toronto's dynamic systems lab / vector institute / university of cambridge's prorok lab / mitacs", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000126, "year": null}, {"Unnamed: 0": 1147, "autor": 127, "date": null, "content": "IKPy\nDemo\nLive demos of what IKPy can do (click on the image below to see the video):\nAlso, a presentation of IKPy: Presentation.\nFeatures\nWith IKPy, you can:\nCompute the Inverse Kinematics of every existing robot.\nCompute the Inverse Kinematics in position, orientation, or both\nDefine your kinematic chain using arbitrary representations: DH (Denavit\u2013Hartenberg), URDF, custom...\nAutomaticly import a kinematic chain from a URDF file.\nSupport for arbitrary joint types: revolute, prismatic and more to come in the future\nUse pre-configured robots, such as baxter or the poppy-torso\nIKPy is precise (up to 7 digits): the only limitation being your underlying model's precision, and fast: from 7 ms to 50 ms (depending on your precision) for a complete IK computation.\nPlot your kinematic chain: no need to use a real robot (or a simulator) to test your algorithms!\nDefine your own Inverse Kinematics methods.\nUtils to parse and analyze URDF files:\nMoreover, IKPy is a pure-Python library: the install is a matter of seconds, and no compiling is required.\nInstallation\nYou have three options:\nFrom PyPI (recommended) - simply run:\npip install ikpy\nIf you intend to plot your robot, you can install the plotting dependencies (mainly matplotlib):\npip install 'ikpy[plot]'\nFrom source - first download and extract the archive, then run:\npip install ./\nNB: You must have the proper rights to execute this command\nQuickstart\nFollow this IPython notebook.\nGuides and Tutorials\nGo to the wiki. It should introduce you to the basic concepts of IKPy.\nAPI Documentation\nAn extensive documentation of the API can be found here.\nDependencies and compatibility\nStarting with IKPy v3.1, only Python 3 is supported. For versions before v3.1, the library can work with both versions of Python (2.7 and 3.x).\nIn terms of dependencies, it requires numpy and scipy.\nsympy is highly recommended, for fast hybrid computations, that's why it is installed by default.\nmatplotlib is optional: it is used to plot your models (in 3D).\nContributing\nIKPy is designed to be easily customisable: you can add your own IK methods or robot representations (such as DH-Parameters) using a dedicated developer API.\nContributions are welcome: if you have an awesome patented (but also open-source!) IK method, don't hesitate to propose adding it to the library!\nLinks\nIf performance is your main concern, aversive++ has an inverse kinematics module written in C++, which works the same way IKPy does.", "link": "https://github.com/Phylliade/ikpy", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "ikpy\ndemo\nlive demos of what ikpy can do (click on the image below to see the video):\nalso, a presentation of ikpy: presentation.\nfeatures\nwith ikpy, you can:\ncompute the inverse kinematics of every existing robot.\ncompute the inverse kinematics in position, orientation, or both\ndefine your kinematic chain using arbitrary representations: dh (denavit\u2013hartenberg), urdf, custom...\nautomaticly import a kinematic chain from a urdf file.\nsupport for arbitrary joint types: revolute, prismatic and more to come in the future\nuse pre-configured robots, such as baxter or the poppy-torso\nikpy is precise (up to 7 digits): the only limitation being your underlying model's precision, and fast: from 7 ms to 50 ms (depending on your precision) for a complete ik computation.\n-----> plot !!!  your kinematic chain: no need to use a real robot (or a simulator) to test your algorithms!\ndefine your own inverse kinematics methods.\nutils to parse and analyze urdf files:\nmoreover, ikpy is a pure-python library: the install is a matter of seconds, and no compiling is required.\ninstallation\nyou have three options:\nfrom pypi (recommended) - simply run:\npip install ikpy\nif you intend to plot your robot, you can install the plotting dependencies (mainly matplotlib):\npip install 'ikpy[plot]'\nfrom source - first download and extract the archive, then run:\npip install ./\nnb: you must have the proper rights to execute this command\nquickstart\nfollow this ipython notebook.\nguides and tutorials\ngo to the wiki. it should introduce you to the basic concepts of ikpy.\napi documentation\nan extensive documentation of the api can be found here.\ndependencies and compatibility\nstarting with ikpy v3.1, only python 3 is supported. for versions before v3.1, the library can work with both versions of python (2.7 and 3.x).\nin terms of dependencies, it requires numpy and scipy.\nsympy is highly recommended, for fast hybrid computations, that's why it is installed by default.\nmatplotlib is optional: it is used to plot your models (in 3d).\ncontributing\nikpy is designed to be easily customisable: you can add your own ik methods or robot representations (such as dh-parameters) using a dedicated developer api.\ncontributions are welcome: if you have an awesome patented (but also open-source!) ik method, don't hesitate to propose adding it to the library!\nlinks\nif performance is your main concern, aversive++ has an inverse kinematics module written in c++, which works the same way ikpy does.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000127, "year": null}, {"Unnamed: 0": 1165, "autor": 145, "date": null, "content": "C++ Robotics\nA header-only, fully-templated C++ library for control algorithms.\nGoals \u2022 Requirements \u2022 Getting Started \u2022 Features \u2022 TODOs \u2022 References\nGoals\nC++ Robotics has the following goals:\nImplement as many robotics algorithms as possible, without sacrificing quality. These include, e.g., control, path planning, and estimation algorithms.\nBe easy to use and to get started with, thus the header-only format and minimal external dependencies.\nBe fast: by making use of templates, most algorithms use static-size data structures whose size is known at compilation time.\nThis project is inspired by PythonRobotics. Instead of being just an educational repo, this library aims at implementing fast algorithms with a consistent API, to guarantee runtime performance and ease of use.\nWhile this is still a work in progress, I would appreciate it if you left any suggestions or starred the repo. Any help is greatly appreciated!\nRequirements\nC++11 compiler\nCMake 3.14+ (if using Visual Studio on Windows you should be able to import the project as a CMake project)\nThe C++ dependencies will be obtained automatically by CPM.cmake. Note that the library only depends on Eigen, but matplotplusplus is used in the examples folder to plot the results.\nGetting started\nFollowing are some examples to get started. The examples folder contains several examples that are useful to get started.\nCppRobotics aims to be modular, which means:\nOnce you define a dynamical system, most algorithms will be readily available for it to use\nData should flow seamlessly between objects (e.g. estimator -> controller)\nOnce you setup an algorithm, you should be able to change the dynamical system and integrate it directly\nClone this repo\ngit clone https://github.com/giacomo-b/CppRobotics.git\nBuilding and running the examples\nGiven a generic EXAMPLE that you want to run, the following commands build it:\ncmake -S examples/EXAMPLE -B build/EXAMPLE\ncmake --build build/EXAMPLE\nOn Windows, this will default to a Debug configuration. To build the project in release mode, you can add --config=Release after the first command.\nTo run the example on Linux, macOS, and most Unix-based systems:\n./build/EXAMPLE/main\nOn Windows:\n./build/EXAMPLE/CONFIG_TYPE/main\nwhere CONFIG_TYPE is either Debug or Release, depending on how you configured the project.\nUsing the library in your projects\nImporting the library\n#include <robotics/robotics.hpp>\nSystem definition\nSystemBase represents a generic dynamical system. In most cases, you will be using either a LinearSystem or NonlinearSystem.\nSince the library is templated, to define a system you need to define:\nThe number of states\nThe number of inputs (control actions)\nThe number of outputs\ne.g.:\nstatic constexpr int N = 2; // Number of states\nstatic constexpr int M = 1; // Number of inputs\nstatic constexpr int P = 2; // Number of outputs\nType aliases can come handy, and prevent the coder from mixing up the wrong dimensions:\nusing State = Robotics::ColumnVector<N>;\nusing Input = Robotics::ColumnVector<M>;\nusing Output = Robotics::ColumnVector<P>;\nusing StateMatrix = Robotics::SquareMatrix<N>;\nusing InputMatrix = Robotics::Matrix<N, M>;\nusing OutputMatrix = Robotics::Matrix<P, N>;\nusing FeedthroughMatrix = Robotics::Matrix<P, N>;\nLet's define a linear system whose state form is\nx' = A * x + B * u\ny = C * x + D * u\nTo set up a LinearSystem:\nStateMatrix A;\nA << 1, 0,\n0, 1;\nInputMatrix B;\nB << 1, 0;\nOutputMatrix C;\nC << 1, 0,\n0, 1;\nNote that having templates not only improves runtime performance, but also enforces compile-time checking. If you initialize a matrix with the wrong number of elements, the code will not compile.\nMatrices C and D are not required: they are null by default if not provided. In this case, D is null. To define the system:\nRobotics::Model::LinearSystem<N, M, P> system(A, B, C);\nThe initial state is zero by default. You can set a custom initial state as follows:\nsystem.SetInitialState(State(1.0, -1.0));\nFeatures\nCheck out TheLartians/ModernCppStarter if you want to include these features in your project.\nRunning tests\nFrom the root directory:\ncmake -S test -B build/test\ncmake --build build/test\nCTEST_OUTPUT_ON_FAILURE=1 cmake --build build/test --target test\n# or simply call the executable:\n./build/test/RoboticsTests\nTo also collect code coverage information, run CMake with the -DENABLE_TEST_COVERAGE=1 option.\nRunning clang-format for autoformatting\nThis requires clang-format, cmake-format and pyyaml to be installed on the current system.\ncmake -S test -B build/test\n# view changes\ncmake --build build/test --target format\n# apply changes\ncmake --build build/test --target fix-format\nSee Format.cmake for details.\nBuild the documentation\nThe documentation is automatically built and published whenever a GitHub Release is created. To manually build documentation, call the following command.\ncmake -S documentation -B build/doc\ncmake --build build/doc --target GenerateDocs\n# view the docs\nopen build/doc/doxygen/html/index.html\nTo build the documentation locally, you will need Doxygen, jinja2 and Pygments installed in your system.\nBuild everything at once\nThe project also includes an all directory that allows building all targets at the same time. This is useful during development, as it exposes all subprojects to your IDE and avoids redundant builds of the library.\ncmake -S all -B build\ncmake --build build\n# run tests\n./build/test/RoboticsTests\n# format code\ncmake --build build --target fix-format\n# run standalone\n./build/standalone/Robotics --help\n# build docs\ncmake --build build --target GenerateDocs\nAdditional tools\nThe test and standalone subprojects include the tools.cmake file which is used to import additional tools on-demand through CMake configuration arguments. The following are currently supported.\nSanitizers\nSanitizers can be enabled by configuring CMake with -DUSE_SANITIZER=<Address | Memory | MemoryWithOrigins | Undefined | Thread | Leak | 'Address;Undefined'>.\nStatic Analyzers\nStatic Analyzers can be enabled by setting -DUSE_STATIC_ANALYZER=<clang-tidy | iwyu | cppcheck>, or a combination of those in quotation marks, separated by semicolons. By default, analyzers will automatically find configuration files such as .clang-format. Additional arguments can be passed to the analyzers by setting the CLANG_TIDY_ARGS, IWYU_ARGS, or CPPCHECK_ARGS variables.\nCcache\nCcache can be enabled by configuring with -DUSE_CCACHE=<ON | OFF>.\nTODOs\nAdd more algorithms\nAdd support for nonlinear systems automatic differentiation, so that the Jacobians are automatically computed (see autodiff)\nAdd a README.md to each example folder, to explain the theory\nCache the packages downloaded by CPM.CMake (currently everything is re-downloaded every time a new example is built)\nMany more, feel free to add your ideas!\nReferences\nAs mentioned above, this repo was originally inspired by AtsushiSakai/PythonRobotics. So go check it out if you want to see more algorithms (or if you want to help port a few of them!).", "link": "https://github.com/giacomo-b/CppRobotics", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "c++ robotics\na header-only, fully-templated c++ library for control algorithms.\ngoals \u2022 requirements \u2022 getting started \u2022 features \u2022 todos \u2022 references\ngoals\nc++ robotics has the following goals:\nimplement as many robotics algorithms as possible, without sacrificing quality. these include, e.g., control, path planning, and estimation algorithms.\nbe easy to use and to get started with, thus the header-only format and minimal external dependencies.\nbe fast: by making use of templates, most algorithms use static-size data structures whose size is known at compilation time.\nthis project is inspired by pythonrobotics. instead of being just an educational repo, this library aims at implementing fast algorithms with a consistent api, to guarantee runtime performance and ease of use.\nwhile this is still a work in progress, i would appreciate it if you left any suggestions or starred the repo. any help is greatly appreciated!\nrequirements\nc++11 compiler\ncmake 3.14+ (if using visual studio on windows you should be able to import the project as a cmake project)\nthe c++ dependencies will be obtained automatically by cpm.cmake. note that the library only depends on eigen, but matplotplusplus is used in the examples folder to -----> plot !!!  the results.\ngetting started\nfollowing are some examples to get started. the examples folder contains several examples that are useful to get started.\ncpprobotics aims to be modular, which means:\nonce you define a dynamical system, most algorithms will be readily available for it to use\ndata should flow seamlessly between objects (e.g. estimator -> controller)\nonce you setup an algorithm, you should be able to change the dynamical system and integrate it directly\nclone this repo\ngit clone https://github.com/giacomo-b/cpprobotics.git\nbuilding and running the examples\ngiven a generic example that you want to run, the following commands build it:\ncmake -s examples/example -b build/example\ncmake --build build/example\non windows, this will default to a debug configuration. to build the project in release mode, you can add --config=release after the first command.\nto run the example on linux, macos, and most unix-based systems:\n./build/example/main\non windows:\n./build/example/config_type/main\nwhere config_type is either debug or release, depending on how you configured the project.\nusing the library in your projects\nimporting the library\n#include <robotics/robotics.hpp>\nsystem definition\nsystembase represents a generic dynamical system. in most cases, you will be using either a linearsystem or nonlinearsystem.\nsince the library is templated, to define a system you need to define:\nthe number of states\nthe number of inputs (control actions)\nthe number of outputs\ne.g.:\nstatic constexpr int n = 2; // number of states\nstatic constexpr int m = 1; // number of inputs\nstatic constexpr int p = 2; // number of outputs\ntype aliases can come handy, and prevent the coder from mixing up the wrong dimensions:\nusing state = robotics::columnvector<n>;\nusing input = robotics::columnvector<m>;\nusing output = robotics::columnvector<p>;\nusing statematrix = robotics::squarematrix<n>;\nusing inputmatrix = robotics::matrix<n, m>;\nusing outputmatrix = robotics::matrix<p, n>;\nusing feedthroughmatrix = robotics::matrix<p, n>;\nlet's define a linear system whose state form is\nx' = a * x + b * u\ny = c * x + d * u\nto set up a linearsystem:\nstatematrix a;\na << 1, 0,\n0, 1;\ninputmatrix b;\nb << 1, 0;\noutputmatrix c;\nc << 1, 0,\n0, 1;\nnote that having templates not only improves runtime performance, but also enforces compile-time checking. if you initialize a matrix with the wrong number of elements, the code will not compile.\nmatrices c and d are not required: they are null by default if not provided. in this case, d is null. to define the system:\nrobotics::model::linearsystem<n, m, p> system(a, b, c);\nthe initial state is zero by default. you can set a custom initial state as follows:\nsystem.setinitialstate(state(1.0, -1.0));\nfeatures\ncheck out thelartians/moderncppstarter if you want to include these features in your project.\nrunning tests\nfrom the root directory:\ncmake -s test -b build/test\ncmake --build build/test\nctest_output_on_failure=1 cmake --build build/test --target test\n# or simply call the executable:\n./build/test/roboticstests\nto also collect code coverage information, run cmake with the -denable_test_coverage=1 option.\nrunning clang-format for autoformatting\nthis requires clang-format, cmake-format and pyyaml to be installed on the current system.\ncmake -s test -b build/test\n# view changes\ncmake --build build/test --target format\n# apply changes\ncmake --build build/test --target fix-format\nsee format.cmake for details.\nbuild the documentation\nthe documentation is automatically built and published whenever a github release is created. to manually build documentation, call the following command.\ncmake -s documentation -b build/doc\ncmake --build build/doc --target generatedocs\n# view the docs\nopen build/doc/doxygen/html/index.html\nto build the documentation locally, you will need doxygen, jinja2 and pygments installed in your system.\nbuild everything at once\nthe project also includes an all directory that allows building all targets at the same time. this is useful during development, as it exposes all subprojects to your ide and avoids redundant builds of the library.\ncmake -s all -b build\ncmake --build build\n# run tests\n./build/test/roboticstests\n# format code\ncmake --build build --target fix-format\n# run standalone\n./build/standalone/robotics --help\n# build docs\ncmake --build build --target generatedocs\nadditional tools\nthe test and standalone subprojects include the tools.cmake file which is used to import additional tools on-demand through cmake configuration arguments. the following are currently supported.\nsanitizers\nsanitizers can be enabled by configuring cmake with -duse_sanitizer=<address | memory | memorywithorigins | undefined | thread | leak | 'address;undefined'>.\nstatic analyzers\nstatic analyzers can be enabled by setting -duse_static_analyzer=<clang-tidy | iwyu | cppcheck>, or a combination of those in quotation marks, separated by semicolons. by default, analyzers will automatically find configuration files such as .clang-format. additional arguments can be passed to the analyzers by setting the clang_tidy_args, iwyu_args, or cppcheck_args variables.\nccache\nccache can be enabled by configuring with -duse_ccache=<on | off>.\ntodos\nadd more algorithms\nadd support for nonlinear systems automatic differentiation, so that the jacobians are automatically computed (see autodiff)\nadd a readme.md to each example folder, to explain the theory\ncache the packages downloaded by cpm.cmake (currently everything is re-downloaded every time a new example is built)\nmany more, feel free to add your ideas!\nreferences\nas mentioned above, this repo was originally inspired by atsushisakai/pythonrobotics. so go check it out if you want to see more algorithms (or if you want to help port a few of them!).", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000145, "year": null}, {"Unnamed: 0": 1170, "autor": 150, "date": null, "content": "Note: This is a cleaned-up, PyTorch port of the GG-CNN code. For the original Keras implementation, see the RSS2018 branch.\nMain changes are major code clean-ups and documentation, an improved GG-CNN2 model, ability to use the Jacquard dataset and simpler evaluation.\nGenerative Grasping CNN (GG-CNN)\nThe GG-CNN is a lightweight, fully-convolutional network which predicts the quality and pose of antipodal grasps at every pixel in an input depth image. The lightweight and single-pass generative nature of GG-CNN allows for fast execution and closed-loop control, enabling accurate grasping in dynamic environments where objects are moved during the grasp attempt.\nThis repository contains the implementation of the Generative Grasping Convolutional Neural Network (GG-CNN) from the paper:\nClosing the Loop for Robotic Grasping: A Real-time, Generative Grasp Synthesis Approach\nDouglas Morrison, Peter Corke, J\u00fcrgen Leitner\nRobotics: Science and Systems (RSS) 2018\narXiv | Video\nIf you use this work, please cite:\n@inproceedings{morrison2018closing,\ntitle={{Closing the Loop for Robotic Grasping: A Real-time, Generative Grasp Synthesis Approach}},\nauthor={Morrison, Douglas and Corke, Peter and Leitner, J\\\"urgen},\nbooktitle={Proc.\\ of Robotics: Science and Systems (RSS)},\nyear={2018}\n}\nContact\nAny questions or comments contact Doug Morrison.\nInstallation\nThis code was developed with Python 3.6 on Ubuntu 16.04. Python requirements can installed by:\npip install -r requirements.txt\nDatasets\nCurrently, both the Cornell Grasping Dataset and Jacquard Dataset are supported.\nCornell Grasping Dataset\nDownload the and extract Cornell Grasping Dataset.\nConvert the PCD files to depth images by running python -m utils.dataset_processing.generate_cornell_depth <Path To Dataset>\nJacquard Dataset\nDownload and extract the Jacquard Dataset.\nPre-trained Models\nSome example pre-trained models for GG-CNN and GG-CNN2 can be downloaded from here. The models are trained on the Cornell grasping dataset using the depth images. Each zip file contains 1) the full saved model from torch.save(model) and 2) the weights state dict from torch.save(model.state_dict()).\nFor example loading GG-CNN (replace ggcnn with ggcnn2 as required):\n# Enter the directory where you cloned this repo\ncd /path/to/ggcnn\n# Download the weights\nwget https://github.com/dougsm/ggcnn/releases/download/v0.1/ggcnn_weights_cornell.zip\n# Unzip the weights.\nunzip ggcnn_weights_cornell.zip\n# Load the weights in python, e.g.\npython\n>>> import torch\n# Option 1) Load the model directly.\n# (this may print warning based on the installed version of python)\n>>> model = torch.load('ggcnn_weights_cornell/ggcnn_epoch_23_cornell')\n>>> model\nGGCNN(\n(conv1): Conv2d(1, 32, kernel_size=(9, 9), stride=(3, 3), padding=(3, 3))\n(conv2): Conv2d(32, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n(conv3): Conv2d(16, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n(convt1): ConvTranspose2d(8, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n(convt2): ConvTranspose2d(8, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n(convt3): ConvTranspose2d(16, 32, kernel_size=(9, 9), stride=(3, 3), padding=(3, 3), output_padding=(1, 1))\n(pos_output): Conv2d(32, 1, kernel_size=(2, 2), stride=(1, 1))\n(cos_output): Conv2d(32, 1, kernel_size=(2, 2), stride=(1, 1))\n(sin_output): Conv2d(32, 1, kernel_size=(2, 2), stride=(1, 1))\n(width_output): Conv2d(32, 1, kernel_size=(2, 2), stride=(1, 1))\n)\n# Option 2) Instantiate a model and load the weights.\n>>> from models.ggcnn import GGCNN\n>>> model = GGCNN()\n>>> model.load_state_dict(torch.load('ggcnn_weights_cornell/ggcnn_epoch_23_cornell_statedict.pt'))\n<All keys matched successfully>\nTraining\nTraining is done by the train_ggcnn.py script. Run train_ggcnn.py --help to see a full list of options, such as dataset augmentation and validation options.\nSome basic examples:\n# Train GG-CNN on Cornell Dataset\npython train_ggcnn.py --description training_example --network ggcnn --dataset cornell --dataset-path <Path To Dataset>\n# Train GG-CNN2 on Jacquard Datset\npython train_ggcnn.py --description training_example2 --network ggcnn2 --dataset jacquard --dataset-path <Path To Dataset>\nTrained models are saved in output/models by default, with the validation score appended.\nEvaluation/Visualisation\nEvaluation or visualisation of the trained networks are done using the eval_ggcnn.py script. Run eval_ggcnn.py --help for a full set of options.\nImportant flags are:\n--iou-eval to evaluate using the IoU between grasping rectangles metric.\n--jacquard-output to generate output files in the format required for simulated testing against the Jacquard dataset.\n--vis to plot the network output and predicted grasping rectangles.\nFor example:\npython eval_ggcnn.py --network <Path to Trained Network> --dataset jacquard --dataset-path <Path to Dataset> --jacquard-output --iou-eval\nRunning on a Robot\nOur ROS implementation for running the grasping system see https://github.com/dougsm/mvp_grasp.\nThe original implementation for running experiments on a Kinva Mico arm can be found in the repository https://github.com/dougsm/ggcnn_kinova_grasping.", "link": "https://github.com/dougsm/ggcnn", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "note: this is a cleaned-up, pytorch port of the gg-cnn code. for the original keras implementation, see the rss2018 branch.\nmain changes are major code clean-ups and documentation, an improved gg-cnn2 model, ability to use the jacquard dataset and simpler evaluation.\ngenerative grasping cnn (gg-cnn)\nthe gg-cnn is a lightweight, fully-convolutional network which predicts the quality and pose of antipodal grasps at every pixel in an input depth image. the lightweight and single-pass generative nature of gg-cnn allows for fast execution and closed-loop control, enabling accurate grasping in dynamic environments where objects are moved during the grasp attempt.\nthis repository contains the implementation of the generative grasping convolutional neural network (gg-cnn) from the paper:\nclosing the loop for robotic grasping: a real-time, generative grasp synthesis approach\ndouglas morrison, peter corke, j\u00fcrgen leitner\nrobotics: science and systems (rss) 2018\narxiv | video\nif you use this work, please cite:\n@inproceedings{morrison2018closing,\ntitle={{closing the loop for robotic grasping: a real-time, generative grasp synthesis approach}},\nauthor={morrison, douglas and corke, peter and leitner, j\\\"urgen},\nbooktitle={proc.\\ of robotics: science and systems (rss)},\nyear={2018}\n}\ncontact\nany questions or comments contact doug morrison.\ninstallation\nthis code was developed with python 3.6 on ubuntu 16.04. python requirements can installed by:\npip install -r requirements.txt\ndatasets\ncurrently, both the cornell grasping dataset and jacquard dataset are supported.\ncornell grasping dataset\ndownload the and extract cornell grasping dataset.\nconvert the pcd files to depth images by running python -m utils.dataset_processing.generate_cornell_depth <path to dataset>\njacquard dataset\ndownload and extract the jacquard dataset.\npre-trained models\nsome example pre-trained models for gg-cnn and gg-cnn2 can be downloaded from here. the models are trained on the cornell grasping dataset using the depth images. each zip file contains 1) the full saved model from torch.save(model) and 2) the weights state dict from torch.save(model.state_dict()).\nfor example loading gg-cnn (replace ggcnn with ggcnn2 as required):\n# enter the directory where you cloned this repo\ncd /path/to/ggcnn\n# download the weights\nwget https://github.com/dougsm/ggcnn/releases/download/v0.1/ggcnn_weights_cornell.zip\n# unzip the weights.\nunzip ggcnn_weights_cornell.zip\n# load the weights in python, e.g.\npython\n>>> import torch\n# option 1) load the model directly.\n# (this may print warning based on the installed version of python)\n>>> model = torch.load('ggcnn_weights_cornell/ggcnn_epoch_23_cornell')\n>>> model\nggcnn(\n(conv1): conv2d(1, 32, kernel_size=(9, 9), stride=(3, 3), padding=(3, 3))\n(conv2): conv2d(32, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n(conv3): conv2d(16, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n(convt1): convtranspose2d(8, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n(convt2): convtranspose2d(8, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n(convt3): convtranspose2d(16, 32, kernel_size=(9, 9), stride=(3, 3), padding=(3, 3), output_padding=(1, 1))\n(pos_output): conv2d(32, 1, kernel_size=(2, 2), stride=(1, 1))\n(cos_output): conv2d(32, 1, kernel_size=(2, 2), stride=(1, 1))\n(sin_output): conv2d(32, 1, kernel_size=(2, 2), stride=(1, 1))\n(width_output): conv2d(32, 1, kernel_size=(2, 2), stride=(1, 1))\n)\n# option 2) instantiate a model and load the weights.\n>>> from models.ggcnn import ggcnn\n>>> model = ggcnn()\n>>> model.load_state_dict(torch.load('ggcnn_weights_cornell/ggcnn_epoch_23_cornell_statedict.pt'))\n<all keys matched successfully>\ntraining\ntraining is done by the train_ggcnn.py script. run train_ggcnn.py --help to see a full list of options, such as dataset augmentation and validation options.\nsome basic examples:\n# train gg-cnn on cornell dataset\npython train_ggcnn.py --description training_example --network ggcnn --dataset cornell --dataset-path <path to dataset>\n# train gg-cnn2 on jacquard datset\npython train_ggcnn.py --description training_example2 --network ggcnn2 --dataset jacquard --dataset-path <path to dataset>\ntrained models are saved in output/models by default, with the validation score appended.\nevaluation/visualisation\nevaluation or visualisation of the trained networks are done using the eval_ggcnn.py script. run eval_ggcnn.py --help for a full set of options.\nimportant flags are:\n--iou-eval to evaluate using the iou between grasping rectangles metric.\n--jacquard-output to generate output files in the format required for simulated testing against the jacquard dataset.\n--vis to -----> plot !!!  the network output and predicted grasping rectangles.\nfor example:\npython eval_ggcnn.py --network <path to trained network> --dataset jacquard --dataset-path <path to dataset> --jacquard-output --iou-eval\nrunning on a robot\nour ros implementation for running the grasping system see https://github.com/dougsm/mvp_grasp.\nthe original implementation for running experiments on a kinva mico arm can be found in the repository https://github.com/dougsm/ggcnn_kinova_grasping.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000150, "year": null}, {"Unnamed: 0": 1174, "autor": 154, "date": null, "content": "Introduction\nCrocoddyl is an optimal control library for robot control under contact sequence. Its solvers are based on novel and efficient Differential Dynamic Programming (DDP) algorithms. Crocoddyl computes optimal trajectories along with optimal feedback gains. It uses Pinocchio for fast computation of robots dynamics and their analytical derivatives.\nThe source code is released under the BSD 3-Clause license.\nAuthors: Carlos Mastalli and Rohan Budhiraja\nInstructors: Nicolas Mansard\nWith additional support from the Gepetto team at LAAS-CNRS and MEMMO project. For more details see Section Credits\nIf you want to follow the current developments, you can directly refer to the devel branch. If you want to directly dive into Crocoddyl, only one single line is sufficient\nconda install crocoddyl -c conda-forge\nand if you prefer pip (in Python 3 or 2):\npip install --user crocoddyl\nInstallation\nCrocoddyl can be easily installed on various Linux (Ubuntu, Fedora, etc.) and Unix distributions (Mac OS X, BSD, etc.).\nCrocoddyl features\nCrocoddyl is versatible:\nvarious optimal control solvers (DDP, FDDP, BoxDDP, etc) - single and multi-shooting methods\nanalytical and sparse derivatives via Pinocchio\nEuclidian and non-Euclidian geometry friendly via Pinocchio\nhandle autonomous and nonautomous dynamical systems\nnumerical differentiation support\nautomatic differentiation support via CppAD\nCrocoddyl is efficient and flexible:\ncache friendly,\nmulti-thread friendly\nPython bindings (including models and solvers abstractions) via Boost Python\nC++ 14/17/20 compliant\nextensively tested\nautomatic code generation support via CppADCodeGen\nInstallation through robotpkg\nYou can install this package through robotpkg. robotpkg is a package manager tailored for robotics softwares. It greatly simplifies the release of new versions along with the management of their dependencies. You just need to add the robotpkg apt repository to your sources.list and then use sudo apt install robotpkg-py27-crocoddyl (or py3X for python 3.X, depending on your system):\nIf you have never added robotpkg as a softwares repository, please follow first the instructions from 1 to 3; otherwise, go directly to instruction 4. Those instructions are similar to the installation procedures presented in http://robotpkg.openrobots.org/debian.html.\nAdd robotpkg as source repository to apt:\nsudo tee /etc/apt/sources.list.d/robotpkg.list <<EOF\ndeb [arch=amd64] http://robotpkg.openrobots.org/wip/packages/debian/pub $(lsb_release -sc) robotpkg\ndeb [arch=amd64] http://robotpkg.openrobots.org/packages/debian/pub $(lsb_release -sc) robotpkg\nEOF\nRegister the authentication certificate of robotpkg:\ncurl http://robotpkg.openrobots.org/packages/debian/robotpkg.key | sudo apt-key add -\nYou need to run at least once apt update to fetch the package descriptions:\nsudo apt-get update\nThe installation of Crocoddyl:\nsudo apt install robotpkg-py36-crocoddyl # for Python 3\nsudo apt install robotpkg-py27-crocoddyl # for Python 2\nFinally you will need to configure your environment variables, e.g.:\nexport PATH=/opt/openrobots/bin:$PATH\nexport PKG_CONFIG_PATH=/opt/openrobots/lib/pkgconfig:$PKG_CONFIG_PATH\nexport LD_LIBRARY_PATH=/opt/openrobots/lib:$LD_LIBRARY_PATH\nexport PYTHONPATH=/opt/openrobots/lib/python2.7/site-packages:$PYTHONPATH\nBuilding from source\nCrocoddyl is c++ library with Python bindings for versatile and fast prototyping. It has the following dependencies:\npinocchio\nEigen\neigenpy\nBoost\nexample-robot-data (optional for examples, install Python loaders)\nOpenMP (optional for multi-threading installation)\ngepetto-viewer-corba (optional for display)\njupyter (optional for notebooks)\nmatplotlib (optional for examples)\nYou can run examples, unit-tests and benchmarks from your build dir:\ncd build\nmake test\nmake -s examples-quadrupedal_gaits INPUT=\"display plot\" # enable display and plot\nmake -s benchmarks-cpp-quadrupedal_gaits INPUT=\"100 walk\" # number of trials ; type of gait\nAlternatively, you can see the 3D result and/or graphs of your run examples (through gepetto-viewer and matplotlib), you can use\nexport CROCODDYL_DISPLAY=1\nexport CROCODDYL_PLOT=1\nAfter installation, you could run the examples as follows:\npython -m crocoddyl.examples.quadrupedal_gaits \"display\" \"plot\" # enable display and plot\nIf you want to learn about Crocoddyl, take a look at the Jupyter notebooks. Start in the following order.\nexamples/notebooks/unicycle_towards_origin.ipynb\nexamples/notebooks/cartpole_swing_up.ipynb\nexamples/notebooks/arm_manipulation.ipynb\nexamples/notebooks/whole_body_manipulation.ipynb\nexamples/notebooks/bipedal_walking.ipynb\nexamples/notebooks/introduction_to_crocoddyl.ipynb\nDocumentation\nThe documentation of Crocoddyl of its last release is available here.\nCiting Crocoddyl\nTo cite Crocoddyl in your academic research, please use the following bibtex lines:\n@inproceedings{mastalli20crocoddyl,\nauthor={Mastalli, Carlos and Budhiraja, Rohan and Merkt, Wolfgang and Saurel, Guilhem and Hammoud, Bilal\nand Naveau, Maximilien and Carpentier, Justin and Righetti, Ludovic and Vijayakumar, Sethu and Mansard, Nicolas},\ntitle={{Crocoddyl: An Efficient and Versatile Framework for Multi-Contact Optimal Control}},\nbooktitle = {IEEE International Conference on Robotics and Automation (ICRA)},\nyear={2020}\n}\nand the following one to reference this website:\n@misc{crocoddylweb,\nauthor = {Carlos Mastalli, Rohan Budhiraja and Nicolas Mansard and others},\ntitle = {Crocoddyl: a fast and flexible optimal control library for robot control under contact sequence},\nhowpublished = {https://github.com/loco-3d/crocoddyl/wikis/home},\nyear = {2019}\n}\nCrocoddyl contributions go beyond efficient software implementation as well. Please also consider to cite the algorithm contributions of our different solvers and formulations:\nFeasibility-driven DDP (FDDP): [1]\nControl-limited feasibility-driven DDP (Box-FDDP): [2]\nMulti-phase rigid optimal control: [3]\nFinally, please also consider citing Pinocchio, which contributes to the efficient implementation of rigid body algorithms and their derivatives. For more details how to cite Pinocchio visit: https://github.com/stack-of-tasks/pinocchio.\nBelow, there is list of the selected publications that describe different components of Crocoddyl. For a complete list see PUBLICATIONS.md.\nSelected publications\n[1] C. Mastalli, R. Budhiraja, W. Merkt, G. Saurel, B. Hammoud, M. Naveau, J. Carpentier, L. Righetti, S. Vijayakumar and N. Mansard. Crocoddyl: An Efficient and Versatile Framework for Multi-Contact Optimal Control, IEEE International Conference on Robotics and Automation (ICRA), 2020\n[2] C. Mastalli, W. Merkt, J. Marti-Saumell, H. Ferrolho, J. Sola, N. Mansard, S. Vijayakumar. A Direct-Indirect Hybridization Approach to Control-Limited DDP, 2021\n[3] R. Budhiraja, J. Carpentier, C. Mastalli and N. Mansard. Differential Dynamic Programming for Multi-Phase Rigid Contact Dynamics, IEEE RAS International Conference on Humanoid Robots (ICHR), 2018\nQuestions and Issues\nYou have a question or an issue? You may either directly open a new issue or use the mailing list crocoddyl@laas.fr.\nSteering Committee\nCrocoddyl is being managed by a steering committee which meets every two weeks to discuss the ongoing developments.\nThe committee is being led by Carlos Mastalli (University of Edinburgh) and Rohan Budhiraja (LAAS-CNRS). Nicolas Mansard (LAAS-CNRS), Guilhem Saurel (LAAS-CNRS) and Justin Carpentier (INRIA) are other members of the committee.\nCredits\nThe following people have been involved in the development of Crocoddyl:\nNicolas Mansard (LAAS-CNRS): project instructor and main developer\nCarlos Mastalli (University of Edinburgh): main developer\nRohan Budhiraja (LAAS-CNRS): main developer\nJustin Carpentier (INRIA): efficient analytical rigid-body dynamics derivatives, conda integration\nMaximilien Naveau (MPI): unit-test support\nGuilhem Saurel (LAAS-CNRS): continuous integration and deployment\nWolfgang Merkt (University of Oxford): feature extension and debugging\nJosep Mart\u00ed Saumell (IRI: CSIC-UPC): feature extension\nBilal Hammoud (MPI): features extension\nJulian E\u00dfer (DFKI): features extension (contact stability)\nAcknowledgments\nThe development of Crocoddyl is supported by the EU MEMMO project, and the EU RoboCom++ project. It is maintained by the Gepetto team @LAAS-CNRS, the Statistical Machine Learning and Motor Control Group @University of Edinburgh, and the Willow team @INRIA.", "link": "https://github.com/loco-3d/crocoddyl", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "introduction\ncrocoddyl is an optimal control library for robot control under contact sequence. its solvers are based on novel and efficient differential dynamic programming (ddp) algorithms. crocoddyl computes optimal trajectories along with optimal feedback gains. it uses pinocchio for fast computation of robots dynamics and their analytical derivatives.\nthe source code is released under the bsd 3-clause license.\nauthors: carlos mastalli and rohan budhiraja\ninstructors: nicolas mansard\nwith additional support from the gepetto team at laas-cnrs and memmo project. for more details see section credits\nif you want to follow the current developments, you can directly refer to the devel branch. if you want to directly dive into crocoddyl, only one single line is sufficient\nconda install crocoddyl -c conda-forge\nand if you prefer pip (in python 3 or 2):\npip install --user crocoddyl\ninstallation\ncrocoddyl can be easily installed on various linux (ubuntu, fedora, etc.) and unix distributions (mac os x, bsd, etc.).\ncrocoddyl features\ncrocoddyl is versatible:\nvarious optimal control solvers (ddp, fddp, boxddp, etc) - single and multi-shooting methods\nanalytical and sparse derivatives via pinocchio\neuclidian and non-euclidian geometry friendly via pinocchio\nhandle autonomous and nonautomous dynamical systems\nnumerical differentiation support\nautomatic differentiation support via cppad\ncrocoddyl is efficient and flexible:\ncache friendly,\nmulti-thread friendly\npython bindings (including models and solvers abstractions) via boost python\nc++ 14/17/20 compliant\nextensively tested\nautomatic code generation support via cppadcodegen\ninstallation through robotpkg\nyou can install this package through robotpkg. robotpkg is a package manager tailored for robotics softwares. it greatly simplifies the release of new versions along with the management of their dependencies. you just need to add the robotpkg apt repository to your sources.list and then use sudo apt install robotpkg-py27-crocoddyl (or py3x for python 3.x, depending on your system):\nif you have never added robotpkg as a softwares repository, please follow first the instructions from 1 to 3; otherwise, go directly to instruction 4. those instructions are similar to the installation procedures presented in http://robotpkg.openrobots.org/debian.html.\nadd robotpkg as source repository to apt:\nsudo tee /etc/apt/sources.list.d/robotpkg.list <<eof\ndeb [arch=amd64] http://robotpkg.openrobots.org/wip/packages/debian/pub $(lsb_release -sc) robotpkg\ndeb [arch=amd64] http://robotpkg.openrobots.org/packages/debian/pub $(lsb_release -sc) robotpkg\neof\nregister the authentication certificate of robotpkg:\ncurl http://robotpkg.openrobots.org/packages/debian/robotpkg.key | sudo apt-key add -\nyou need to run at least once apt update to fetch the package descriptions:\nsudo apt-get update\nthe installation of crocoddyl:\nsudo apt install robotpkg-py36-crocoddyl # for python 3\nsudo apt install robotpkg-py27-crocoddyl # for python 2\nfinally you will need to configure your environment variables, e.g.:\nexport path=/opt/openrobots/bin:$path\nexport pkg_config_path=/opt/openrobots/lib/pkgconfig:$pkg_config_path\nexport ld_library_path=/opt/openrobots/lib:$ld_library_path\nexport pythonpath=/opt/openrobots/lib/python2.7/site-packages:$pythonpath\nbuilding from source\ncrocoddyl is c++ library with python bindings for versatile and fast prototyping. it has the following dependencies:\npinocchio\neigen\neigenpy\nboost\nexample-robot-data (optional for examples, install python loaders)\nopenmp (optional for multi-threading installation)\ngepetto-viewer-corba (optional for display)\njupyter (optional for notebooks)\nmatplotlib (optional for examples)\nyou can run examples, unit-tests and benchmarks from your build dir:\ncd build\nmake test\nmake -s examples-quadrupedal_gaits input=\"display -----> plot !!! \" # enable display and -----> plot !!! \nmake -s benchmarks-cpp-quadrupedal_gaits input=\"100 walk\" # number of trials ; type of gait\nalternatively, you can see the 3d result and/or graphs of your run examples (through gepetto-viewer and matplotlib), you can use\nexport crocoddyl_display=1\nexport crocoddyl_plot=1\nafter installation, you could run the examples as follows:\npython -m crocoddyl.examples.quadrupedal_gaits \"display\" \"-----> plot !!! \" # enable display and -----> plot !!! \nif you want to learn about crocoddyl, take a look at the jupyter notebooks. start in the following order.\nexamples/notebooks/unicycle_towards_origin.ipynb\nexamples/notebooks/cartpole_swing_up.ipynb\nexamples/notebooks/arm_manipulation.ipynb\nexamples/notebooks/whole_body_manipulation.ipynb\nexamples/notebooks/bipedal_walking.ipynb\nexamples/notebooks/introduction_to_crocoddyl.ipynb\ndocumentation\nthe documentation of crocoddyl of its last release is available here.\nciting crocoddyl\nto cite crocoddyl in your academic research, please use the following bibtex lines:\n@inproceedings{mastalli20crocoddyl,\nauthor={mastalli, carlos and budhiraja, rohan and merkt, wolfgang and saurel, guilhem and hammoud, bilal\nand naveau, maximilien and carpentier, justin and righetti, ludovic and vijayakumar, sethu and mansard, nicolas},\ntitle={{crocoddyl: an efficient and versatile framework for multi-contact optimal control}},\nbooktitle = {ieee international conference on robotics and automation (icra)},\nyear={2020}\n}\nand the following one to reference this website:\n@misc{crocoddylweb,\nauthor = {carlos mastalli, rohan budhiraja and nicolas mansard and others},\ntitle = {crocoddyl: a fast and flexible optimal control library for robot control under contact sequence},\nhowpublished = {https://github.com/loco-3d/crocoddyl/wikis/home},\nyear = {2019}\n}\ncrocoddyl contributions go beyond efficient software implementation as well. please also consider to cite the algorithm contributions of our different solvers and formulations:\nfeasibility-driven ddp (fddp): [1]\ncontrol-limited feasibility-driven ddp (box-fddp): [2]\nmulti-phase rigid optimal control: [3]\nfinally, please also consider citing pinocchio, which contributes to the efficient implementation of rigid body algorithms and their derivatives. for more details how to cite pinocchio visit: https://github.com/stack-of-tasks/pinocchio.\nbelow, there is list of the selected publications that describe different components of crocoddyl. for a complete list see publications.md.\nselected publications\n[1] c. mastalli, r. budhiraja, w. merkt, g. saurel, b. hammoud, m. naveau, j. carpentier, l. righetti, s. vijayakumar and n. mansard. crocoddyl: an efficient and versatile framework for multi-contact optimal control, ieee international conference on robotics and automation (icra), 2020\n[2] c. mastalli, w. merkt, j. marti-saumell, h. ferrolho, j. sola, n. mansard, s. vijayakumar. a direct-indirect hybridization approach to control-limited ddp, 2021\n[3] r. budhiraja, j. carpentier, c. mastalli and n. mansard. differential dynamic programming for multi-phase rigid contact dynamics, ieee ras international conference on humanoid robots (ichr), 2018\nquestions and issues\nyou have a question or an issue? you may either directly open a new issue or use the mailing list crocoddyl@laas.fr.\nsteering committee\ncrocoddyl is being managed by a steering committee which meets every two weeks to discuss the ongoing developments.\nthe committee is being led by carlos mastalli (university of edinburgh) and rohan budhiraja (laas-cnrs). nicolas mansard (laas-cnrs), guilhem saurel (laas-cnrs) and justin carpentier (inria) are other members of the committee.\ncredits\nthe following people have been involved in the development of crocoddyl:\nnicolas mansard (laas-cnrs): project instructor and main developer\ncarlos mastalli (university of edinburgh): main developer\nrohan budhiraja (laas-cnrs): main developer\njustin carpentier (inria): efficient analytical rigid-body dynamics derivatives, conda integration\nmaximilien naveau (mpi): unit-test support\nguilhem saurel (laas-cnrs): continuous integration and deployment\nwolfgang merkt (university of oxford): feature extension and debugging\njosep mart\u00ed saumell (iri: csic-upc): feature extension\nbilal hammoud (mpi): features extension\njulian e\u00dfer (dfki): features extension (contact stability)\nacknowledgments\nthe development of crocoddyl is supported by the eu memmo project, and the eu robocom++ project. it is maintained by the gepetto team @laas-cnrs, the statistical machine learning and motor control group @university of edinburgh, and the willow team @inria.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000154, "year": null}, {"Unnamed: 0": 1181, "autor": 161, "date": null, "content": "Ravens - Transporter Networks\nRavens is a collection of simulated tasks in PyBullet for learning vision-based robotic manipulation, with emphasis on pick and place. It features a Gym-like API with 10 tabletop rearrangement tasks, each with (i) a scripted oracle that provides expert demonstrations (for imitation learning), and (ii) reward functions that provide partial credit (for reinforcement learning).\n(a) block-insertion: pick up the L-shaped red block and place it into the L-shaped fixture.\n(b) place-red-in-green: pick up the red blocks and place them into the green bowls amidst other objects.\n(c) towers-of-hanoi: sequentially move disks from one tower to another\u2014only smaller disks can be on top of larger ones.\n(d) align-box-corner: pick up the randomly sized box and align one of its corners to the L-shaped marker on the tabletop.\n(e) stack-block-pyramid: sequentially stack 6 blocks into a pyramid of 3-2-1 with rainbow colored ordering.\n(f) palletizing-boxes: pick up homogeneous fixed-sized boxes and stack them in transposed layers on the pallet.\n(g) assembling-kits: pick up different objects and arrange them on a board marked with corresponding silhouettes.\n(h) packing-boxes: pick up randomly sized boxes and place them tightly into a container.\n(i) manipulating-rope: rearrange a deformable rope such that it connects the two endpoints of a 3-sided square.\n(j) sweeping-piles: push piles of small objects into a target goal zone marked on the tabletop.\nSome tasks require generalizing to unseen objects (d,g,h), or multi-step sequencing with closed-loop feedback (c,e,f,h,i,j).\nTeam: this repository is developed and maintained by Andy Zeng, Pete Florence, Daniel Seita, Jonathan Tompson, and Ayzaan Wahid. This is the reference repository for the paper:\nTransporter Networks: Rearranging the Visual World for Robotic Manipulation\nProject Website \u2022 PDF \u2022 Conference on Robot Learning (CoRL) 2020\nAndy Zeng, Pete Florence, Jonathan Tompson, Stefan Welker, Jonathan Chien, Maria Attarian, Travis Armstrong,\nIvan Krasin, Dan Duong, Vikas Sindhwani, Johnny Lee\nAbstract. Robotic manipulation can be formulated as inducing a sequence of spatial displacements: where the space being moved can encompass an object, part of an object, or end effector. In this work, we propose the Transporter Network, a simple model architecture that rearranges deep features to infer spatial displacements from visual input\u2014which can parameterize robot actions. It makes no assumptions of objectness (e.g. canonical poses, models, or keypoints), it exploits spatial symmetries, and is orders of magnitude more sample efficient than our benchmarked alternatives in learning vision-based manipulation tasks: from stacking a pyramid of blocks, to assembling kits with unseen objects; from manipulating deformable ropes, to pushing piles of small objects with closed-loop feedback. Our method can represent complex multi-modal policy distributions and generalizes to multi-step sequential tasks, as well as 6DoF pick-and-place. Experiments on 10 simulated tasks show that it learns faster and generalizes better than a variety of end-to-end baselines, including policies that use ground-truth object poses. We validate our methods with hardware in the real world.\nInstallation\nStep 1. Recommended: install Miniconda with Python 3.7.\ncurl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh -b -u\necho $'\\nexport PATH=~/miniconda3/bin:\"${PATH}\"\\n' >> ~/.profile # Add Conda to PATH.\nsource ~/.profile\nconda init\nStep 2. Create and activate Conda environment, then install GCC and Python packages.\ncd ~/ravens\nconda create --name ravens python=3.7 -y\nconda activate ravens\nsudo apt-get update\nsudo apt-get -y install gcc libgl1-mesa-dev\npip install -r requirements.txt\npython setup.py install --user\nStep 3. Recommended: install GPU acceleration with NVIDIA CUDA 10.1 and cuDNN 7.6.5 for Tensorflow.\n./oss_scripts/install_cuda.sh # For Ubuntu 16.04 and 18.04.\nconda install cudatoolkit==10.1.243 -y\nconda install cudnn==7.6.5 -y\nAlternative: Pure pip\nAs an example for Ubuntu 18.04:\n./oss_scipts/install_cuda.sh # For Ubuntu 16.04 and 18.04.\nsudo apt install gcc libgl1-mesa-dev python3.8-venv\npython3.8 -m venv ./venv\nsource ./venv/bin/activate\npip install -U pip\npip install scikit-build\npip install -r ./requirements.txt\nexport PYTHONPATH=${PWD}\nGetting Started\nStep 1. Generate training and testing data (saved locally). Note: remove --disp for headless mode.\npython ravens/demos.py --assets_root=./ravens/environments/assets/ --disp=True --task=block-insertion --mode=train --n=10\npython ravens/demos.py --assets_root=./ravens/environments/assets/ --disp=True --task=block-insertion --mode=test --n=100\nTo run with shared memory, open a separate terminal window and run python3 -m pybullet_utils.runServer. Then add --shared_memory flag to the command above.\nStep 2. Train a model e.g., Transporter Networks model. Model checkpoints are saved to the checkpoints directory. Optional: you may exit training prematurely after 1000 iterations to skip to the next step.\npython ravens/train.py --task=block-insertion --agent=transporter --n_demos=10\nStep 3. Evaluate a Transporter Networks agent using the model trained for 1000 iterations. Results are saved locally into .pkl files.\npython ravens/test.py --assets_root=./ravens/environments/assets/ --disp=True --task=block-insertion --agent=transporter --n_demos=10 --n_steps=1000\nStep 4. Plot and print results.\npython ravens/plot.py --disp=True --task=block-insertion --agent=transporter --n_demos=10\nOptional. Track training and validation losses with Tensorboard.\npython -m tensorboard.main --logdir=logs # Open the browser to where it tells you to.\nDatasets and Pre-Trained Models\nDownload our generated train and test datasets and pre-trained models.\nwget https://storage.googleapis.com/ravens-assets/checkpoints.zip\nwget https://storage.googleapis.com/ravens-assets/block-insertion.zip\nwget https://storage.googleapis.com/ravens-assets/place-red-in-green.zip\nwget https://storage.googleapis.com/ravens-assets/towers-of-hanoi.zip\nwget https://storage.googleapis.com/ravens-assets/align-box-corner.zip\nwget https://storage.googleapis.com/ravens-assets/stack-block-pyramid.zip\nwget https://storage.googleapis.com/ravens-assets/palletizing-boxes.zip\nwget https://storage.googleapis.com/ravens-assets/assembling-kits.zip\nwget https://storage.googleapis.com/ravens-assets/packing-boxes.zip\nwget https://storage.googleapis.com/ravens-assets/manipulating-rope.zip\nwget https://storage.googleapis.com/ravens-assets/sweeping-piles.zip\nThe MDP formulation for each task uses transitions with the following structure:\nObservations: raw RGB-D images and camera parameters (pose and intrinsics).\nActions: a primitive function (to be called by the robot) and parameters.\nRewards: total sum of rewards for a successful episode should be =1.\nInfo: 6D poses, sizes, and colors of objects.", "link": "https://github.com/google-research/ravens", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "ravens - transporter networks\nravens is a collection of simulated tasks in pybullet for learning vision-based robotic manipulation, with emphasis on pick and place. it features a gym-like api with 10 tabletop rearrangement tasks, each with (i) a scripted oracle that provides expert demonstrations (for imitation learning), and (ii) reward functions that provide partial credit (for reinforcement learning).\n(a) block-insertion: pick up the l-shaped red block and place it into the l-shaped fixture.\n(b) place-red-in-green: pick up the red blocks and place them into the green bowls amidst other objects.\n(c) towers-of-hanoi: sequentially move disks from one tower to another\u2014only smaller disks can be on top of larger ones.\n(d) align-box-corner: pick up the randomly sized box and align one of its corners to the l-shaped marker on the tabletop.\n(e) stack-block-pyramid: sequentially stack 6 blocks into a pyramid of 3-2-1 with rainbow colored ordering.\n(f) palletizing-boxes: pick up homogeneous fixed-sized boxes and stack them in transposed layers on the pallet.\n(g) assembling-kits: pick up different objects and arrange them on a board marked with corresponding silhouettes.\n(h) packing-boxes: pick up randomly sized boxes and place them tightly into a container.\n(i) manipulating-rope: rearrange a deformable rope such that it connects the two endpoints of a 3-sided square.\n(j) sweeping-piles: push piles of small objects into a target goal zone marked on the tabletop.\nsome tasks require generalizing to unseen objects (d,g,h), or multi-step sequencing with closed-loop feedback (c,e,f,h,i,j).\nteam: this repository is developed and maintained by andy zeng, pete florence, daniel seita, jonathan tompson, and ayzaan wahid. this is the reference repository for the paper:\ntransporter networks: rearranging the visual world for robotic manipulation\nproject website \u2022 pdf \u2022 conference on robot learning (corl) 2020\nandy zeng, pete florence, jonathan tompson, stefan welker, jonathan chien, maria attarian, travis armstrong,\nivan krasin, dan duong, vikas sindhwani, johnny lee\nabstract. robotic manipulation can be formulated as inducing a sequence of spatial displacements: where the space being moved can encompass an object, part of an object, or end effector. in this work, we propose the transporter network, a simple model architecture that rearranges deep features to infer spatial displacements from visual input\u2014which can parameterize robot actions. it makes no assumptions of objectness (e.g. canonical poses, models, or keypoints), it exploits spatial symmetries, and is orders of magnitude more sample efficient than our benchmarked alternatives in learning vision-based manipulation tasks: from stacking a pyramid of blocks, to assembling kits with unseen objects; from manipulating deformable ropes, to pushing piles of small objects with closed-loop feedback. our method can represent complex multi-modal policy distributions and generalizes to multi-step sequential tasks, as well as 6dof pick-and-place. experiments on 10 simulated tasks show that it learns faster and generalizes better than a variety of end-to-end baselines, including policies that use ground-truth object poses. we validate our methods with hardware in the real world.\ninstallation\nstep 1. recommended: install miniconda with python 3.7.\ncurl -o https://repo.anaconda.com/miniconda/miniconda3-latest-linux-x86_64.sh\nbash miniconda3-latest-linux-x86_64.sh -b -u\necho $'\\nexport path=~/miniconda3/bin:\"${path}\"\\n' >> ~/.profile # add conda to path.\nsource ~/.profile\nconda init\nstep 2. create and activate conda environment, then install gcc and python packages.\ncd ~/ravens\nconda create --name ravens python=3.7 -y\nconda activate ravens\nsudo apt-get update\nsudo apt-get -y install gcc libgl1-mesa-dev\npip install -r requirements.txt\npython setup.py install --user\nstep 3. recommended: install gpu acceleration with nvidia cuda 10.1 and cudnn 7.6.5 for tensorflow.\n./oss_scripts/install_cuda.sh # for ubuntu 16.04 and 18.04.\nconda install cudatoolkit==10.1.243 -y\nconda install cudnn==7.6.5 -y\nalternative: pure pip\nas an example for ubuntu 18.04:\n./oss_scipts/install_cuda.sh # for ubuntu 16.04 and 18.04.\nsudo apt install gcc libgl1-mesa-dev python3.8-venv\npython3.8 -m venv ./venv\nsource ./venv/bin/activate\npip install -u pip\npip install scikit-build\npip install -r ./requirements.txt\nexport pythonpath=${pwd}\ngetting started\nstep 1. generate training and testing data (saved locally). note: remove --disp for headless mode.\npython ravens/demos.py --assets_root=./ravens/environments/assets/ --disp=true --task=block-insertion --mode=train --n=10\npython ravens/demos.py --assets_root=./ravens/environments/assets/ --disp=true --task=block-insertion --mode=test --n=100\nto run with shared memory, open a separate terminal window and run python3 -m pybullet_utils.runserver. then add --shared_memory flag to the command above.\nstep 2. train a model e.g., transporter networks model. model checkpoints are saved to the checkpoints directory. optional: you may exit training prematurely after 1000 iterations to skip to the next step.\npython ravens/train.py --task=block-insertion --agent=transporter --n_demos=10\nstep 3. evaluate a transporter networks agent using the model trained for 1000 iterations. results are saved locally into .pkl files.\npython ravens/test.py --assets_root=./ravens/environments/assets/ --disp=true --task=block-insertion --agent=transporter --n_demos=10 --n_steps=1000\nstep 4. -----> plot !!!  and print results.\npython ravens/plot.py --disp=true --task=block-insertion --agent=transporter --n_demos=10\noptional. track training and validation losses with tensorboard.\npython -m tensorboard.main --logdir=logs # open the browser to where it tells you to.\ndatasets and pre-trained models\ndownload our generated train and test datasets and pre-trained models.\nwget https://storage.googleapis.com/ravens-assets/checkpoints.zip\nwget https://storage.googleapis.com/ravens-assets/block-insertion.zip\nwget https://storage.googleapis.com/ravens-assets/place-red-in-green.zip\nwget https://storage.googleapis.com/ravens-assets/towers-of-hanoi.zip\nwget https://storage.googleapis.com/ravens-assets/align-box-corner.zip\nwget https://storage.googleapis.com/ravens-assets/stack-block-pyramid.zip\nwget https://storage.googleapis.com/ravens-assets/palletizing-boxes.zip\nwget https://storage.googleapis.com/ravens-assets/assembling-kits.zip\nwget https://storage.googleapis.com/ravens-assets/packing-boxes.zip\nwget https://storage.googleapis.com/ravens-assets/manipulating-rope.zip\nwget https://storage.googleapis.com/ravens-assets/sweeping-piles.zip\nthe mdp formulation for each task uses transitions with the following structure:\nobservations: raw rgb-d images and camera parameters (pose and intrinsics).\nactions: a primitive function (to be called by the robot) and parameters.\nrewards: total sum of rewards for a successful episode should be =1.\ninfo: 6d poses, sizes, and colors of objects.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000161, "year": null}, {"Unnamed: 0": 1252, "autor": 232, "date": null, "content": "Stereo-Odometry-SOFT\nThis repository is a MATLAB implementation of the Stereo Odometry based on careful Feature selection and Tracking. The code is released under MIT License.\nThe code has been tested on MATLAB R2018a and depends on the following toolboxes:\nParallel Processing Toolbox\nComputer Vision Toolbox\nOn a laptop with Intel(R) Core(TM) i7-8750H CPU @ 2.20GHz and 16GB RAM, the following average timings were observed:\nTime taken for feature processing (in ms): 261.4\nTime taken for feature matching (in ms): 3650.5\nTime taken for feature selection (in ms): 6.5\nTime taken for motion estimation (in ms): 1.1\nHow to run the repository?\nClone the repository using the following command:\ngit clone https://github.com/Mayankm96/Stereo-Odometry-SOFT.git\nImport the dataset to the folder data. In case you wish to use the KITTI Dataset, such as the Residential dataset, the following command might be useful:\ncd PATH/TO/Stereo-Odometry-SOFT\n## For Reseidential Sequence: 61 (2011_09_46_drive_0061)\n# synced+rectified data\nwget -c https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0009/2011_09_26_drive_0009_sync.zip -P data\n# calib.txt\nwget -c https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_calib.zip -P data\nChange the corresponding paramters in the configuration file configFile.m according to your need\nRun the script main.m to get a plot of the estimated odometry\nProposed Implementation of the Algorithm\nThe input to the algorithm at each time frame, are the left and right images at the current time instant, and the ones at the previous timestamp.\nKeypoints Detection\nIn this section, the keypoints detection and matching is divided into following separate stages:\nfeature processing: each image is searched for locations that are likely to match well in other images\nfeature matching: efficiently searching for likely matching candidates in other images\nfeature tracking: unlike to the second stage, the correspondences are searched in a small neighborhood around each detected feature and across frames at different time steps\nFeature processing\nCorner and blob features are extracted for each image using the following steps:\nFirst, blob and checkerboard kernels are convolved over the input image.\nEfficient Non-Maximum Suppression is applied on the filter responses to produce keypoints that may belong to either of the following classes: blob maxima, blob minima, corner maxima, and corner minima. To speed up the feature matching, correspondences are only found between features belong to the same class.\nThe feature descriptors are constructed by using a set of 16 locations out of an 11 x 11 block around each keypoint in input image's gradients. The gradient images are computed by convolving 5 x 5 sobel kernels across the input image. The descriptor has a total length of 32,\nFeature matching\nThis part of the algorithm is concerned with finding the features for egomotion estimation. It is based on the process mentioned in the paper StereoScan: Dense 3d reconstruction in real-time. The process can be summarized as follows:\nCorrespondences in two images are found by computing the Sum of Absolute Differences (SAD) score between a feature in the first image with the one lying in the second image that belongs to the same class\nThis matching is done in a circular fasion between the left and right frames at time instants t-1 and t as shown below:\nTo remove certain outliers, Normalized Cross-Correlation (NCC) is again in a circular fasion using templates of size 21 x 21 pixels around the features that have been matched successfully in the process mentioned above.\nFeature Selection\nTo ensure a uniform distribution of features across the image, the entire image is divided into buckets of size 50 x 50 pixels and feature selection is done to select only the strongest features present in each bucket.\nEgomotion Estimation\nUsing P3P algorithm along with RANSAC, incremental rotation and translation is estimated.\nTo-Dos\nfix parfor and for loops to enable running without parallel processing\nread the camera calibration parameters from calibration file directly\nadd sub-pixel refinement using parabolic fitting\nadd feature selection based on feature tracking i.e. the age of features\nimplement Nister's algorithm and SLERP for rotation estimation\nuse Gauss-Newton optimization to estimate translation from weighted reprojection error", "link": "https://github.com/Mayankm96/Stereo-Odometry-SOFT", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "stereo-odometry-soft\nthis repository is a matlab implementation of the stereo odometry based on careful feature selection and tracking. the code is released under mit license.\nthe code has been tested on matlab r2018a and depends on the following toolboxes:\nparallel processing toolbox\ncomputer vision toolbox\non a laptop with intel(r) core(tm) i7-8750h cpu @ 2.20ghz and 16gb ram, the following average timings were observed:\ntime taken for feature processing (in ms): 261.4\ntime taken for feature matching (in ms): 3650.5\ntime taken for feature selection (in ms): 6.5\ntime taken for motion estimation (in ms): 1.1\nhow to run the repository?\nclone the repository using the following command:\ngit clone https://github.com/mayankm96/stereo-odometry-soft.git\nimport the dataset to the folder data. in case you wish to use the kitti dataset, such as the residential dataset, the following command might be useful:\ncd path/to/stereo-odometry-soft\n## for reseidential sequence: 61 (2011_09_46_drive_0061)\n# synced+rectified data\nwget -c https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0009/2011_09_26_drive_0009_sync.zip -p data\n# calib.txt\nwget -c https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_calib.zip -p data\nchange the corresponding paramters in the configuration file configfile.m according to your need\nrun the script main.m to get a -----> plot !!!  of the estimated odometry\nproposed implementation of the algorithm\nthe input to the algorithm at each time frame, are the left and right images at the current time instant, and the ones at the previous timestamp.\nkeypoints detection\nin this section, the keypoints detection and matching is divided into following separate stages:\nfeature processing: each image is searched for locations that are likely to match well in other images\nfeature matching: efficiently searching for likely matching candidates in other images\nfeature tracking: unlike to the second stage, the correspondences are searched in a small neighborhood around each detected feature and across frames at different time steps\nfeature processing\ncorner and blob features are extracted for each image using the following steps:\nfirst, blob and checkerboard kernels are convolved over the input image.\nefficient non-maximum suppression is applied on the filter responses to produce keypoints that may belong to either of the following classes: blob maxima, blob minima, corner maxima, and corner minima. to speed up the feature matching, correspondences are only found between features belong to the same class.\nthe feature descriptors are constructed by using a set of 16 locations out of an 11 x 11 block around each keypoint in input image's gradients. the gradient images are computed by convolving 5 x 5 sobel kernels across the input image. the descriptor has a total length of 32,\nfeature matching\nthis part of the algorithm is concerned with finding the features for egomotion estimation. it is based on the process mentioned in the paper stereoscan: dense 3d reconstruction in real-time. the process can be summarized as follows:\ncorrespondences in two images are found by computing the sum of absolute differences (sad) score between a feature in the first image with the one lying in the second image that belongs to the same class\nthis matching is done in a circular fasion between the left and right frames at time instants t-1 and t as shown below:\nto remove certain outliers, normalized cross-correlation (ncc) is again in a circular fasion using templates of size 21 x 21 pixels around the features that have been matched successfully in the process mentioned above.\nfeature selection\nto ensure a uniform distribution of features across the image, the entire image is divided into buckets of size 50 x 50 pixels and feature selection is done to select only the strongest features present in each bucket.\negomotion estimation\nusing p3p algorithm along with ransac, incremental rotation and translation is estimated.\nto-dos\nfix parfor and for loops to enable running without parallel processing\nread the camera calibration parameters from calibration file directly\nadd sub-pixel refinement using parabolic fitting\nadd feature selection based on feature tracking i.e. the age of features\nimplement nister's algorithm and slerp for rotation estimation\nuse gauss-newton optimization to estimate translation from weighted reprojection error", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000232, "year": null}, {"Unnamed: 0": 1387, "autor": 367, "date": null, "content": "Accompanying repository of Master's thesis at TU Berlin / Aalborg University. No longer under active development. Developed in my earlier Python days, please forgive the unformatted spaghetti code.\nDeep Reinforcement Learning for robotic pick and place applications using purely visual observations\nAuthor: Paul Daniel (paudan22@gmail.com)\nTraits of this environment: Very large and multi-discrete actionspace, very high sample-cost, visual observations, binary reward.\nTrained agent in action Example of predicted grasp chances\nSetup iteration Relevant changes\nIT5 - Many more objects, randomly piled\n- Actionspace now multi-discrete, with second dimension being a rotation action\nIT4 - Z-coordinate for grasping now calculated using depth data\n- Objects now vary in size\nIT3 - New two-finger gripper implemented\nIT2 - Grasp success check now after moving to drop location (1000 steps)\nIT1 (Baseline) - Grasp success check after moving straight up (500 steps of trying to close the gripper)\n- Fixed z-coordinate for grasping\n- Objects of equal size\nThis repository provides several python classes for control of robotic arms in MuJoCo:\nMJ_Controller: This class can be used as a standalone class for basic robot control in MuJoCo. This can be useful for trying out models and their grasping capabilities. Alternatively, its methods can also be used by any other class (like a Gym environment) to provide some more functionality. One example of this might be to move the robot back into a certain position after every episode of training, which might be preferable compared to just resetting all the joint angles and velocities. The controller currently also holds the methods for image transformations, which might be put into another separate class at some point.\nGraspEnv: A Gym environment for training reinforcement learning agents. The task to master is a pick & place task. The difference to most other MuJoCo Gym environments is that the observation returned is a camera image instead of a state vector of the simulation. This is meant to resemble a real world setup more closely.\nThe robot configuration used in this setup (Universal Robots UR5 + Robotiq S Model 3 Finger Gripper) is based on this resource. It has since been heavily modified. Most current XML-file: UR5gripper_2_finger.xml\nThe python bindings used come from mujoco_py.\nThe PID controllers implemented are based on simple_pid.\nA simple inverse kinematics solver for translating end-effector positions into joint angles has been implemented using ikpy.\nThe required modules can be installed either manually or using the provided requirements.txt - file.\nSetup\nDownload and install MuJoCo from here. Set up a license and activate it here.\nThen clone this repo:\ngit clone https://github.com/PaulDanielML/MuJoCo_RL_UR5.git\nThen change into the newly created directory:\ncd MuJoCo_RL_UR5/\nIf desired, activate a virtual environment, then run\npip install -r requirements.txt\nThis will install all required packages using pip. The first time you run a script that uses the Mujoco_UR5_controller class some more setup might happen, which can take a few moments. This is all the setup required to use this repo.\nUsage\nGraspEnv - class:\nGym-environment for training agents to use RGB-D data for predicting pixel-wise grasp success chances.\nThe file example_agent.py demonstrates the use of a random agent for this environment.\nThe file Grasping_Agent.py gives an example of training a shortsighted DQN-agent in the environment to predict pixel-wise grasping success (PyTorch). The created environment has an associated controller object, which provides all the functionality of the MJ_Controller - class to it.\nAction space: Pixel space, can be specified by setting height and width. Current defaults: 200x200. This means there are 40.000 possible actions. This resolution translates to a picking accuracy of ~ 4mm.\nState space: The states / observations provided are dictionaries containing two arrays: An RGB-image and a depth-image, both of the same resolution as the action space\nReward function: The environment has been updated to a binary reward structure:\n0 for choosing a pixel that does not lead to a successful grasp.\n+1 for choosing a pixel that leads to a successful grasp.\nThe user gets a summary of each step performed in the console. It is recommended to train agents without rendering, as this will speed up training significantly.\nThe rgb part of the last captured observation will be shown and updated in an extra window.\nMJ_Controller - class:\nExample usage of some of the class methods is demonstrated in the file example.py.\nThe class MJ_Controller offers high and low level methods for controlling the robot in MuJoCo.\nmove_ee : High level, moves the endeffector of the arm to the desired XYZ position (in world coordinates). This is done using very simple inverse kinematics, just obeying the joint limits. Currently there is not collision avoidance implemented. Since this whole repo is created with grasping in mind, the delivered pose will always be so that the gripper is oriented in a vertical way (for top down grasps).\nactuate_joint_group : Low level, lets the user specify motor activations for a specified group\ngrasp : Uses the specified gripper group to attempt a grasp. A simple check is done to determine weather the grasp was successful or not and the result will be output blinking in the console.\nUpdates\nTrials for Offline RL: The folder Offline RL contains scripts for generating and learning from a dataset of (state, action, reward)-transitions. generate_data.py can be used to generate as many files as required, each file containing 12 transitions.\nNew gripper model available: A new, less bulky, 2-finger gripper was implemented in the model in training setup iteration 3.\nImage normalization: Added script normalize.py, which samples 100 images from the environment and writes the mean values and standard deviations of all channels to a file.\nReset shuffle: Calling the environments step method now rearranges all the pickable objects to random positions on the table.\nRecord grasps: The step method of the GraspingEnv now has the optional parameter record_grasps. If set to True, it will capture a side camera image every time a grasp is made that is deemed successful by the environment. This allows for \"quality control\" of the grasps, without having to watch all the failed attempts. The captured images can also be useful for fine tuning grasping parameters.\nPoint clouds: The controller class was provided with new methods for image transformations.\ndepth_2_meters: Converts the normalized depth values returned by mujoco_py into m.\ncreate_camera_data: Constructs a camera matrix, focal length and sets the camera's position and rotation based on a provided camera name and desired image width and depth.\nworld_2_pixel: Accepts a XYZ world position and returns the corresponding x-y pixel coordinates\npixel_2_world: Accepts x-y pixel coordinates and a depth value, returns the XYZ world position. This method can be used to construct point clouds out of the data returned by the controllers get_image_data method.\nJoint plots: All methods that move joints now have an optional plot parameter. If set to True, a .png-file will be created in the local directory. It will show plots for each joint involved in the trajectory, containing the joint angles over time, as well as the target values. This can be used to determine which joints overshoot, oscillate etc. and adjust the controller gains based on that.\nThe tolerance used for the trajectory are plotted in red, so it can easily be determined how many steps each of the joints needs to reach a value within tolerance.", "link": "https://github.com/PaulDanielML/MuJoCo_RL_UR5", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "accompanying repository of master's thesis at tu berlin / aalborg university. no longer under active development. developed in my earlier python days, please forgive the unformatted spaghetti code.\ndeep reinforcement learning for robotic pick and place applications using purely visual observations\nauthor: paul daniel (paudan22@gmail.com)\ntraits of this environment: very large and multi-discrete actionspace, very high sample-cost, visual observations, binary reward.\ntrained agent in action example of predicted grasp chances\nsetup iteration relevant changes\nit5 - many more objects, randomly piled\n- actionspace now multi-discrete, with second dimension being a rotation action\nit4 - z-coordinate for grasping now calculated using depth data\n- objects now vary in size\nit3 - new two-finger gripper implemented\nit2 - grasp success check now after moving to drop location (1000 steps)\nit1 (baseline) - grasp success check after moving straight up (500 steps of trying to close the gripper)\n- fixed z-coordinate for grasping\n- objects of equal size\nthis repository provides several python classes for control of robotic arms in mujoco:\nmj_controller: this class can be used as a standalone class for basic robot control in mujoco. this can be useful for trying out models and their grasping capabilities. alternatively, its methods can also be used by any other class (like a gym environment) to provide some more functionality. one example of this might be to move the robot back into a certain position after every episode of training, which might be preferable compared to just resetting all the joint angles and velocities. the controller currently also holds the methods for image transformations, which might be put into another separate class at some point.\ngraspenv: a gym environment for training reinforcement learning agents. the task to master is a pick & place task. the difference to most other mujoco gym environments is that the observation returned is a camera image instead of a state vector of the simulation. this is meant to resemble a real world setup more closely.\nthe robot configuration used in this setup (universal robots ur5 + robotiq s model 3 finger gripper) is based on this resource. it has since been heavily modified. most current xml-file: ur5gripper_2_finger.xml\nthe python bindings used come from mujoco_py.\nthe pid controllers implemented are based on simple_pid.\na simple inverse kinematics solver for translating end-effector positions into joint angles has been implemented using ikpy.\nthe required modules can be installed either manually or using the provided requirements.txt - file.\nsetup\ndownload and install mujoco from here. set up a license and activate it here.\nthen clone this repo:\ngit clone https://github.com/pauldanielml/mujoco_rl_ur5.git\nthen change into the newly created directory:\ncd mujoco_rl_ur5/\nif desired, activate a virtual environment, then run\npip install -r requirements.txt\nthis will install all required packages using pip. the first time you run a script that uses the mujoco_ur5_controller class some more setup might happen, which can take a few moments. this is all the setup required to use this repo.\nusage\ngraspenv - class:\ngym-environment for training agents to use rgb-d data for predicting pixel-wise grasp success chances.\nthe file example_agent.py demonstrates the use of a random agent for this environment.\nthe file grasping_agent.py gives an example of training a shortsighted dqn-agent in the environment to predict pixel-wise grasping success (pytorch). the created environment has an associated controller object, which provides all the functionality of the mj_controller - class to it.\naction space: pixel space, can be specified by setting height and width. current defaults: 200x200. this means there are 40.000 possible actions. this resolution translates to a picking accuracy of ~ 4mm.\nstate space: the states / observations provided are dictionaries containing two arrays: an rgb-image and a depth-image, both of the same resolution as the action space\nreward function: the environment has been updated to a binary reward structure:\n0 for choosing a pixel that does not lead to a successful grasp.\n+1 for choosing a pixel that leads to a successful grasp.\nthe user gets a summary of each step performed in the console. it is recommended to train agents without rendering, as this will speed up training significantly.\nthe rgb part of the last captured observation will be shown and updated in an extra window.\nmj_controller - class:\nexample usage of some of the class methods is demonstrated in the file example.py.\nthe class mj_controller offers high and low level methods for controlling the robot in mujoco.\nmove_ee : high level, moves the endeffector of the arm to the desired xyz position (in world coordinates). this is done using very simple inverse kinematics, just obeying the joint limits. currently there is not collision avoidance implemented. since this whole repo is created with grasping in mind, the delivered pose will always be so that the gripper is oriented in a vertical way (for top down grasps).\nactuate_joint_group : low level, lets the user specify motor activations for a specified group\ngrasp : uses the specified gripper group to attempt a grasp. a simple check is done to determine weather the grasp was successful or not and the result will be output blinking in the console.\nupdates\ntrials for offline rl: the folder offline rl contains scripts for generating and learning from a dataset of (state, action, reward)-transitions. generate_data.py can be used to generate as many files as required, each file containing 12 transitions.\nnew gripper model available: a new, less bulky, 2-finger gripper was implemented in the model in training setup iteration 3.\nimage normalization: added script normalize.py, which samples 100 images from the environment and writes the mean values and standard deviations of all channels to a file.\nreset shuffle: calling the environments step method now rearranges all the pickable objects to random positions on the table.\nrecord grasps: the step method of the graspingenv now has the optional parameter record_grasps. if set to true, it will capture a side camera image every time a grasp is made that is deemed successful by the environment. this allows for \"quality control\" of the grasps, without having to watch all the failed attempts. the captured images can also be useful for fine tuning grasping parameters.\npoint clouds: the controller class was provided with new methods for image transformations.\ndepth_2_meters: converts the normalized depth values returned by mujoco_py into m.\ncreate_camera_data: constructs a camera matrix, focal length and sets the camera's position and rotation based on a provided camera name and desired image width and depth.\nworld_2_pixel: accepts a xyz world position and returns the corresponding x-y pixel coordinates\npixel_2_world: accepts x-y pixel coordinates and a depth value, returns the xyz world position. this method can be used to construct point clouds out of the data returned by the controllers get_image_data method.\njoint plots: all methods that move joints now have an optional -----> plot !!!  parameter. if set to true, a .png-file will be created in the local directory. it will show plots for each joint involved in the trajectory, containing the joint angles over time, as well as the target values. this can be used to determine which joints overshoot, oscillate etc. and adjust the controller gains based on that.\nthe tolerance used for the trajectory are plotted in red, so it can easily be determined how many steps each of the joints needs to reach a value within tolerance.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000367, "year": null}, {"Unnamed: 0": 1465, "autor": 445, "date": null, "content": "\"Good Robot!\" Efficient Reinforcement Learning for Multi-Step Visual Tasks with Sim to Real Transfer\nAndrew Hundt, Benjamin Killeen, Nicholas Greene, Hongtao Wu, Heeyeon Kwon, Chris Paxton, and Gregory D. Hager\nClick the image to watch the video:\nPaper, Abstract, and Citations\nGood Robot! Paper on IEEE Xplore, Good Robot! Paper on ArXiV\n@article{hundt2020good,\ntitle=\"\u201cGood Robot!\u201d: Efficient Reinforcement Learning for Multi-Step Visual Tasks with Sim to Real Transfer\",\nauthor=\"Andrew {Hundt} and Benjamin {Killeen} and Nicholas {Greene} and Hongtao {Wu} and Heeyeon {Kwon} and Chris {Paxton} and Gregory D. {Hager}\",\njournal=\"IEEE Robotics and Automation Letters (RA-L)\",\nvolume=\"5\",\nnumber=\"4\",\npages=\"6724--6731\",\nyear=\"2020\",\nurl={https://arxiv.org/abs/1909.11730}\n}\nAbstract\u2014 Current Reinforcement Learning (RL) algorithms struggle with long-horizon tasks where time can be wasted exploring dead ends and task progress may be easily reversed. We develop the SPOT framework, which explores within action safety zones, learns about unsafe regions without exploring them, and prioritizes experiences that reverse earlier progress to learn with remarkable efficiency.\nThe SPOT framework successfully completes simulated trials of a variety of tasks, improving a baseline trial success rate from 13% to 100% when stacking 4 cubes, from 13% to 99% when creating rows of 4 cubes, and from 84% to 95% when clearing toys arranged in adversarial patterns. Efficiency with respect to actions per trial typically improves by 30% or more, while training takes just 1-20k actions, depending on the task.\nFurthermore, we demonstrate direct sim to real transfer. We are able to create real stacks in 100% of trials with 61% efficiency and real rows in 100% of trials with 59% efficiency by directly loading the simulation-trained model on the real robot with no additional real-world fine-tuning. To our knowledge, this is the first instance of reinforcement learning with successful sim to real transfer applied to long term multi-step tasks such as block-stacking and row-making with consideration of progress reversal. Code is available at https://github.com/jhu-lcsr/good_robot.\nTraining Row Making, Stack Making, and Table Clearing Tasks\nDetails of our specific training and test runs, command line commands, pre-trained models, and logged data with images are on the good_robot github releases page.\nStarting the V-REP Simulation\nDownload CoppeliaSim (formerly V-REP) and run it to start the simulation. Uou may need to adjust the paths below to match your V-REP folder, and it should be run from the costar_visual_stacking repository directory:\ncd ~/src/real_good_robot\n~/src/CoppeliaSim_Edu_V4_0_0_Ubuntu18_04/coppeliaSim.sh -gREMOTEAPISERVERSERVICE_19997_FALSE_TRUE -s ~/src/real_good_robot/simulation/simulation.ttt\nCube Stacking\nCube Stack Training\nexport CUDA_VISIBLE_DEVICES=\"0\" && python3 main.py --is_sim --obj_mesh_dir objects/blocks --num_obj 4 --push_rewards --experience_replay --explore_rate_decay --check_row --tcp_port 19997 --place --future_reward_discount 0.65 --max_train_actions 20000 --random_actions --common_sense --trial_reward\nAll training results will go in the path/to/good_robot/logs/<training_run> folder. To use SPOT Progress Reward remove --trial_reward from this command. To disable the \"common sense\" dynamic action space regions remove --common_sense from this command. This command will automatically run in test mode once training is complete, and the testing results directory will be moved into the training results directory.\nManual Cube Stack Testing\nRemember to first train the model or download the snapshot file from the release page (ex: v0.12 release) and update the command line --snapshot_file FILEPATH:\nexport CUDA_VISIBLE_DEVICES=\"0\" && python3 main.py --is_sim --obj_mesh_dir 'objects/blocks' --num_obj 4 --push_rewards --experience_replay --explore_rate_decay --place --load_snapshot --snapshot_file ~/Downloads/snapshot.reinforcement-best-stack-rate.pth --random_seed 1238 --is_testing --save_visualizations --disable_situation_removal\nSim to Real Stack Testing\nYou will need a fully configured real robot, we have some instructions in this readme. You will also need to update the snapshot file path to wherever your version is:\nexport CUDA_VISIBLE_DEVICES=\"0\" && python3 main.py --num_obj 8 --push_rewards --experience_replay --explore_rate_decay --trial_reward --common_sense --check_z_height --place --future_reward_discount 0.65 --is_testing --random_seed 1238 --max_test_trials 10 --save_visualizations --random_actions --snapshot_file /media/costar/f5f1f858-3666-4832-beea-b743127f1030/real_good_robot/logs/2020-05-13-12-51-39_Sim-Stack-SPOT-Trial-Reward-Masked-Training/models/snapshot.reinforcement_action_efficiency_best_value.pth\nRow of 4 Cubes\nRow Testing Video:\nRow v0.12 release page and pretrained models.\nRow Training\nCUDA_VISIBLE_DEVICES=\"0\" && python3 main.py --is_sim --obj_mesh_dir objects/blocks --num_obj 4 --push_rewards --experience_replay --explore_rate_decay --check_row --tcp_port 19997 --place --future_reward_discount 0.65 --max_train_actions 20000 --random_actions --common_sense\nTesting will automatically run after training is complete.\nManual Row Testing\nexport CUDA_VISIBLE_DEVICES=\"0\" && python3 main.py --is_sim --obj_mesh_dir 'objects/blocks' --num_obj 4 --push_rewards --experience_replay --explore_rate_decay --trial_reward --future_reward_discount 0.65 --place --check_row --is_testing --tcp_port 19996 --load_snapshot --snapshot_file '/home/costar/Downloads/snapshot-backup.reinforcement-best-stack-rate.pth' --random_seed 1238 --disable_situation_removal --save_visualizations\nSim to Real Row Testing\nYou will need a fully configured real robot, we have some instructions in this readme. You will also need to update the snapshot file path to wherever your version is:\nexport CUDA_VISIBLE_DEVICES=\"0\" && python3 main.py --num_obj 4 --push_rewards --experience_replay --explore_rate_decay --check_row --check_z_height --place --future_reward_discount 0.65 --is_testing --random_seed 1238 --max_test_trials 10 --random_actions --save_visualizations --common_sense --snapshot_file \"/home/costar/src/real_good_robot/logs/2020-06-03-12-05-28_Sim-Rows-Two-Step-Reward-Masked-Training/models/snapshot.reinforcement_trial_success_rate_best_value.pth\"\nPush + Grasp\nWe provide backwards compatibility with the Visual Pushing Grasping (VPG) GitHub Repository, and evaluate on their pushing and grasping task as a baseline from which to compare our algorithms.\nPush + Grasp Training\nexport CUDA_VISIBLE_DEVICES=\"0\" && python3 main.py --is_sim --obj_mesh_dir 'objects/toys' --num_obj 10 --push_rewards --experience_replay --explore_rate_decay\nYou can also run without --trial_reward and with the default --future_reward_discount 0.5.\nPush + Grasp Random Object Location Testing\nexport CUDA_VISIBLE_DEVICES=\"0\" && python3 main.py --is_sim --obj_mesh_dir 'objects/toys' --num_obj 10 --push_rewards --experience_replay --explore_rate_decay --load_snapshot --snapshot_file '/home/costar/src/costar_visual_stacking/logs/2019-08-17.20:54:32-train-grasp-place-split-efficientnet-21k-acc-0.80/models/snapshot.reinforcement.pth' --random_seed 1238 --is_testing --save_visualizations\nPush + Grasp Adversarial Object Location Testing\nAdversarial pushing and grasping release v0.3.2 video:\nexport CUDA_VISIBLE_DEVICES=\"0\" && python3 main.py --is_sim --obj_mesh_dir 'objects/toys' --num_obj 10 --push_rewards --experience_replay --explore_rate_decay --trial_reward --future_reward_discount 0.65 --tcp_port 19996 --is_testing --random_seed 1238 --load_snapshot --snapshot_file '/home/ahundt/src/costar_visual_stacking/logs/2019-09-12.18:21:37-push-grasp-16k-trial-reward/models/snapshot.reinforcement.pth' --max_test_trials 10 --test_preset_cases\nAfter Running the test you need to summarize the results:\npython3 evaluate.py --session_directory /home/ahundt/src/costar_visual_stacking/logs/2019-09-16.02:11:25 --method reinforcement --num_obj_complete 6 --preset\nTroubleshooting\nDuring training, be sure to check the simulator doesn't encounter physics bugs, such as blocks permanently bonded to the gripper. If this happens, you will notice one action will never succeed, and the other might always succeed, and multi-step tasks will no longer progress. If you catch it quickly, you can directly remove the object in the simulator with the scene editing tools, or simply stop the simulation and the training function should detect the problem and reset.\nData Collection and Data Release Checklist (v0.1, checklist creation is in progress)\nThis is a checklist for all the steps you should follow to collect data consistently.\nSave the following data about the run you are going to do into a txt file for your records: 1. Describe the configuration you are running 2. Git commit hash of the code version you will run 2. Exact command line command you will run 3. Current CoppeliaSim (aka V-REP) ip address port (if simulation mode) 4. GPU number you will use on the computer\nStart recording: If sim, start V-REP & Start V-REP Recording. If running the real robot, start a real camera recording the robot.\nStart the run.\nAfter the run completes:\nDo a random testing run for a reasonable number of trials like 10-100.\nIf pushing and grasping do an adversarial scenario testing run.\nMake note of the directories with results, and include dir path in your txt file.\nCreate a github release.\nAdd the model file binary to the release binaries so it is easier to download and utilize.\nIncorporate the log folder name in the release name.\nProvide a good description in english of what the run was trying to achieve.\nFind several representative images, and add them directly to the release page for easy viewing.\nZip visualization directory separately because they tend to be larger files & upload to the github release.\nMove the visualization directory out and then zip the remainder of the log directory and upload to the github release.\nZip up the random testing directories and adversarial testing directories (if applicable) upload to the github release binaries.\nUpload the video recording to a website such as youtube, and add a link to the github release.\nMake sure the github release commit lines up with the commit you actually ran your experiment on.\nSubmit the github release.\nCostar Visual Stacking Execution Details\nRunning Multiple Simulations in Parallel\nIt is possible to do multiple runs on different GPUs on the same machine. First, start an instance of V-Rep as below,\n~/src/V-REP_PRO_EDU_V3_6_2_Ubuntu16_04/vrep.sh -gREMOTEAPISERVERSERVICE_19997_FALSE_TRUE -s simulation/simulation.ttt\nbeing careful to use V-Rep 3.6.2 wherever it is installed locally. The port number, here 19997 which is the usual default, is the important point, as we will cahnge it in subsequent runs.\nStart the simulation as usual, but now specify --tcp_port 19997.\nStart another V-Rep session.\n~/src/V-REP_PRO_EDU_V3_6_2_Ubuntu16_04/vrep.sh -gREMOTEAPISERVERSERVICE_19996_FALSE_TRUE -s simulation/simulation.ttt\nFor some reason, the port number is important here, and should be selected to be lower than already running sessions.\nWhen you start training, be sure to specify a different GPU. For example, if previously you set\nexport CUDA_VISIBLE_DEVICES=\"0\"\nthen you should likely set\nexport CUDA_VISIBLE_DEVICES=\"1\"\nand specify the corresponding tcp port --tcp_port 19996.\nAdditional runs in parallel should use ports 19995, 19994, etc.\nGenerating plots\nTo generate plots run python3 plot.py, you can manually edit the bottom part of plot.py to call plot_it() for your folder.\nlog_dir = './logs/2020-02-03-16-57-28_Sim-Stack-Trial-Reward-Common-Sense-Training'\nplot_it(log_dir,'Sim Stack, SPOT Reward, Common Sense, Training', window=window, max_iter=None)\nThe configuration of the plot should be done automatically from the folder.\nRunning on a Real UR5 with ROS Based Image Collection\nROS Based Image Collection Setup\nWe require python3, so you'll need to ensure export ROS_PYTHON_VERSION=3 is set for the build. A couple additional steps below will need to be added in the middle. We advise installing in the folder:\n~/ros_catkin_ws\nFollow instructions in the ROS Melodic steps to build ros from source.\nIn particular fix up this command:\nexport ROS_PYTHON_VERSION=3 && rosinstall_generator desktop_full --rosdistro melodic --deps --tar > melodic-desktop-full.rosinstall && wstool init -j8 src melodic-desktop-full.rosinstall\nFor the primesense camera add in the openni2_launch, and rgbd_launch repositories, and for handeye calibration between the camera and robot add UbiquityRobotics/fiducials:\ncd ~/catkin_ros_ws\ngit clone https://github.com/ros-drivers/openni2_launch.git\ngit clone https://github.com/ros-drivers/rgbd_launch.git\ngit clone https://github.com/UbiquityRobotics/fiducials.git\nRun the build and install.\ncd ~/ros_catkin_ws\nrosdep install --from-paths src --ignore-src --rosdistro melodic -y && ./src/catkin/bin/catkin_make_isolated --install\nSource the ros setup so you get access to the launch commands:\nsource ~/ros_catkin_ws/install_isolated/setup.zsh\nRunning ROS with depth image processing:\ntaskset 0x00000FFF roslaunch openni2_launch openni2.launch depth_registration:=true\nWe use the linux taskset command (examples) to limit ROS to utilizing 8 cores or fewer, so other cores are available for training.\nIn a separate tab run our small test script:\npython test_ros_images.py\nRunning RVIZ to look at the images:\nrosrun rviz rviz\nThe correct images, as done in the JHU costar dataset class collector, are from the following ROS topics:\nself.rgb_topic = \"/camera/rgb/image_rect_color\"\n# raw means it is in the format provided by the openi drivers, 16 bit int\nself.depth_topic = \"/camera/depth_registered/hw_registered/image_rect\"\nCalibrating Camera Intrincics\nYou must first calibrate your rgb and depth camera intrinsics and rectify your images to ensure you can accurately convert camera positions to robot poses. We do this using camera_calibration in the ros-perception/image_pipeline library.\nYou will need to generate and load a calibration yaml file which goes in a location like ~/.ros/camera_info/rgb_PS1080_PrimeSense.yaml. We have an examle from our robot in this repository saved at real/rgb_PS1080_PrimeSense.yaml.\nCalibrating Camera Extrinsics\nPrint an ArUco Tag, we use 70mm tags with the 4x4 design (dictionary 1), so it can fit in the gripper. Make sure the ArUco dictionary id in the launch files is correct. Attach the ArUco Tag on the robot.\nEdit the fiducials ROS package aruco_detect.launch file in ~/ros_catkin_ws/src/fiducials/aruco_detect/launch/aruco_detect.launch from the fiducials github repository you cloned earlier, see the fiducials wiki for reference. Modify the launch file in [fiducials/aruco to detect your markers and receive images from your sensor. Here is our configuration:\n<!-- Run the aruco_detect node -->\n<launch>\n<!-- namespace for camera input -->\n<!-- /camera/rgb/image_rect_color/compressed -->\n<arg name=\"camera\" default=\"/camera/rgb\"/>\n<arg name=\"image\" default=\"image_rect_color\"/>\n<arg name=\"transport\" default=\"compressed\"/>\n<arg name=\"fiducial_len\" default=\"0.07\"/>\n<arg name=\"dictionary\" default=\"1\"/>\n<arg name=\"do_pose_estimation\" default=\"true\"/>\n<arg name=\"ignore_fiducials\" default=\"\" />\n<arg name=\"fiducial_len_override\" default=\"\" />\n<node pkg=\"aruco_detect\" name=\"aruco_detect\"\ntype=\"aruco_detect\" output=\"screen\" respawn=\"false\">\n<param name=\"image_transport\" value=\"$(arg transport)\"/>\n<param name=\"publish_images\" value=\"true\" />\n<param name=\"fiducial_len\" value=\"$(arg fiducial_len)\"/>\n<param name=\"dictionary\" value=\"$(arg dictionary)\"/>\n<param name=\"do_pose_estimation\" value=\"$(arg do_pose_estimation)\"/>\n<param name=\"ignore_fiducials\" value=\"$(arg ignore_fiducials)\"/>\n<param name=\"fiducial_len_override\" value=\"$(arg fiducial_len_override)\"/>\n<remap from=\"/camera/compressed\"\nto=\"$(arg camera)/$(arg image)/$(arg transport)\"/>\n<remap from=\"/camera_info\" to=\"$(arg camera)/camera_info\"/>\n</node>\n</launch>\nYou must predefine the workspace_limits python variables in the calibration_ros.py, touch.py, and main.py, and robot.py. To modify these locations, change the variables workspace_limits at the end of calibrate_ros.py. You may define it in the Calibrate class or in the function collect_data for data collection.\nThe code directly communicates with the robot via TCP. At the top of calibrate_ros.py, change variable tcp_host_ip to point to the network IP address of your UR5 robot controller.\nRoslaunch the camera with, for example:\ntaskset 0x00000FFF roslaunch openni2_launch openni2.launch depth_registration:=true\nWe use the [linux taskset command](https://linux.die.net/man/1/taskset) ([examples](https://www.howtoforge.com/linux-taskset-command/)) to limit ROS to utilizing 8 cores or fewer, so other cores are available for training.\nThe script is subscribed to the rostopic /fiducial_transform to get the pose of the tag in the camera frame. Roslaunch aruco_detect:\ntaskset 0x00000FFF roslaunch aruco_detect aruco_detect.launch\nThe robot will move suddenly and rapidly. Users must be ready to push the emergency stop button at any time.\nCAREFULLY run python touch.py to start the arm, it will move suddenly!\nCenter the AR tag on the gripper manually using the teach mode button on the robot.\nClick on the title bar of the color image window, do not click on the general color area the robot may move suddenly!\nPress - to close the gripper (= will open it), and check that the center of the AR tag is where you want your gripper center to be defined.\nPress k to calibrate, and after going to a number of positions you should see a calibration result like the following:\nTotal number of poses: 26\nInvalid poses number: 0\nRobot Base to Camera:\n[[ 0.1506513 0.87990966 -0.45062533 0.79319678]\n[ 0.98857761 -0.13210593 0.07254191 -0.14601768]\n[ 0.00430005 -0.45640664 -0.88976092 0.55173518]\n[ 0. 0. 0. 1. ]]\nTotal number of poses: 26\nInvalid poses number: 0\nTool Tip to AR Tag:\n[[ 0.18341198 -0.01617267 -0.98290309 0.0050482 ]\n[ 0.03295954 0.99940367 -0.01029385 0.01899328]\n[ 0.98248344 -0.03050802 0.18383565 0.10822485]\n[ 0. 0. 0. 1. ]]\nBackup procedure (in place of the steps 6 and later from above): with caution, run the following to move the robot and calibrate:\nThe robot will move suddenly and rapidly. Users must be ready to push the emergency stop button at any time.\npython calibrate_ros.py\nThe script will record the pose of the robot and the ArUco tag in the camera frame with correspondence. Then it uses the Park and Martin Method to solve the AX=XB problem for the hand-eye calibration. And the method is implemented in the utils.py. The script will generate a robot_base_to_camera_pose.txt in real/. This txt basically is the pose of the camera in the robot base frame.\nIf you already have corresponded pose file of the robot and the ArUco tag, you can also use the calibrate() function in the calibrate_ros.py to directly calculate the pose of the camera without the data collection step.\nCollecting the Background heightmap\nThe real robot also uses a background heightmap of the scene with no objects present.\nCompletely clear the table or working surface.\nBack up and remove the current real/background_heightmap.depth.png.\nRun pushing and grasping data collection with the --show_heightmap flag.\nView the heightmap images until you see one with no holes (black spots), and save the iteration number at the top.\nCopy the good heightmap from logs/<run_folder>/data/depth_heightmaps/<iteration>.0.depth.png and rename it to real/background_heightmap.depth.png.\nStop and re-run pushing and grasping with the --show_heightmap flag.\nHere is an example of the matplotlib visualization of a good depth heightmap, there are no black specks aside from one corner which is out of the camera's field of view:\nYour updated depth heightmaps should be good to go!\n\"Good Robot!\" is forked from the Visual Pushing and Grasping Toolbox\nOriginal Visual Pushing Grasping (VPG) Repository. Edits have been made to the text below to reflect some configuration and code updates needed to reproduce the previous VPG paper's original behavior:\nVisual Pushing and Grasping (VPG) is a method for training robotic agents to learn how to plan complementary pushing and grasping actions for manipulation (e.g. for unstructured pick-and-place applications). VPG operates directly on visual observations (RGB-D images), learns from trial and error, trains quickly, and generalizes to new objects and scenarios.\nThis repository provides PyTorch code for training and testing VPG policies with deep reinforcement learning in both simulation and real-world settings on a UR5 robot arm. This is the reference implementation for the paper:\nLearning Synergies between Pushing and Grasping with Self-supervised Deep Reinforcement Learning\nPDF | Webpage & Video Results\nAndy Zeng, Shuran Song, Stefan Welker, Johnny Lee, Alberto Rodriguez, Thomas Funkhouser\nIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2018\nSkilled robotic manipulation benefits from complex synergies between non-prehensile (e.g. pushing) and prehensile (e.g. grasping) actions: pushing can help rearrange cluttered objects to make space for arms and fingers; likewise, grasping can help displace objects to make pushing movements more precise and collision-free. In this work, we demonstrate that it is possible to discover and learn these synergies from scratch through model-free deep reinforcement learning. Our method involves training two fully convolutional networks that map from visual observations to actions: one infers the utility of pushes for a dense pixel-wise sampling of end effector orientations and locations, while the other does the same for grasping. Both networks are trained jointly in a Q-learning framework and are entirely self-supervised by trial and error, where rewards are provided from successful grasps. In this way, our policy learns pushing motions that enable future grasps, while learning grasps that can leverage past pushes. During picking experiments in both simulation and real-world scenarios, we find that our system quickly learns complex behaviors amid challenging cases of clutter, and achieves better grasping success rates and picking efficiencies than baseline alternatives after only a few hours of training. We further demonstrate that our method is capable of generalizing to novel objects.\nCiting\nIf you find this code useful in your work, please consider citing:\n@inproceedings{zeng2018learning,\ntitle={Learning Synergies between Pushing and Grasping with Self-supervised Deep Reinforcement Learning},\nauthor={Zeng, Andy and Song, Shuran and Welker, Stefan and Lee, Johnny and Rodriguez, Alberto and Funkhouser, Thomas},\nbooktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},\nyear={2018}\n}\nDemo Videos\nDemo videos of a real robot in action can be found here.\nContact\nThe contact for CoSTAR Visual Stacking is Andrew Hundt. The contact for the original Visual Pushing Grasping repository is Andy Zeng andyz[at]princeton[dot]edu\nInstallation\nThis implementation requires the following dependencies (tested on Ubuntu 16.04.4 LTS):\nPython 2.7 or Python 3\nNumPy, SciPy, OpenCV-Python, Matplotlib. You can quickly install/update these dependencies by running the following (replace pip with pip3 for Python 3):\npip3 install numpy scipy opencv-python matplotlib\nPyTorch version 1.2:\npip3 install torch==1.2 torchvision==0.4.0\nV-REP (simulation environment)\n(Optional) GPU Acceleration\nAccelerating training/inference with an NVIDIA GPU requires installing CUDA and cuDNN. You may need to register with NVIDIA for the CUDA Developer Program (it's free) before downloading. This code has been tested with CUDA 8.0 and cuDNN 6.0 on a single NVIDIA Titan X (12GB). Running out-of-the-box with our pre-trained models using GPU acceleration requires 8GB of GPU memory. Running with GPU acceleration is highly recommended, otherwise each training iteration will take several minutes to run (as opposed to several seconds). This code automatically detects the GPU(s) on your system and tries to use it. If you have a GPU, but would instead like to run in CPU mode, add the tag --cpu when running main.py below.\nA Quick-Start: Demo in Simulation\nThis demo runs our pre-trained model with a UR5 robot arm in simulation on challenging picking scenarios with adversarial clutter, where grasping an object is generally not feasible without first pushing to break up tight clusters of objects.\nInstructions\nCheckout this repository and download our pre-trained models.\ngit clone https://github.com/jhu-lcsr/costar_visual_stacking.git visual-pushing-grasping\ncd visual-pushing-grasping/downloads\n./download-weights.sh\ncd ..\nRun V-REP (navigate to your V-REP directory and run ./vrep.sh). From the main menu, select File > Open scene..., and open the file visual-pushing-grasping/simulation/simulation.ttt from this repository.\nIn another terminal window, run the following (simulation will start in the V-REP window):\npython main.py --is_sim --obj_mesh_dir 'objects/blocks' --num_obj 10 \\\n--push_rewards --experience_replay --explore_rate_decay \\\n--is_testing --test_preset_cases --test_preset_file 'simulation/test-cases/test-10-obj-07.txt' \\\n--load_snapshot --snapshot_file 'downloads/vpg-original-sim-pretrained-10-obj.pth' \\\n--save_visualizations --nn densenet\nNote: you may get a popup window titled \"Dynamics content\" in your V-REP window. Select the checkbox and press OK. You will have to do this a total of 3 times before it stops annoying you.\nTraining\nTo train a regular VPG policy from scratch in simulation, first start the simulation environment by running V-REP (navigate to your V-REP directory and run ./vrep.sh). From the main menu, select File > Open scene..., and open the file visual-pushing-grasping/simulation/simulation.ttt. Then navigate to this repository in another terminal window and run the following:\npython main.py --is_sim --push_rewards --experience_replay --explore_rate_decay --save_visualizations\nData collected from each training session (including RGB-D images, camera parameters, heightmaps, actions, rewards, model snapshots, visualizations, etc.) is saved into a directory in the logs folder. A training session can be resumed by adding the --resume flag, which then loads the model snapshot specified by --snapshot_file and transition history from the provided session directory:\npython main.py --is_sim --push_rewards --experience_replay --explore_rate_decay --save_visualizations \\\n--snapshot_file 'logs/YOUR-SESSION-DIRECTORY-NAME-HERE/models/snapshot-backup.reinforcement.pth' \\\n--resume 'logs/YOUR-SESSION-DIRECTORY-NAME-HERE' \\\nIf the --resume flag is raised without an argument, the most recent log file is used. If --resume is raised but no --snapshot-file is specified, the snapshot file from the implied or specified logging directory is used.\nVarious training options can be modified or toggled on/off with different flags (run python main.py -h to see all options):\nusage: main.py [-h] [--is_sim] [--obj_mesh_dir OBJ_MESH_DIR]\n[--num_obj NUM_OBJ] [--num_extra_obj NUM_EXTRA_OBJ]\n[--tcp_host_ip TCP_HOST_IP] [--tcp_port TCP_PORT]\n[--rtc_host_ip RTC_HOST_IP] [--rtc_port RTC_PORT]\n[--heightmap_resolution HEIGHTMAP_RESOLUTION]\n[--random_seed RANDOM_SEED] [--cpu] [--flops] [--method METHOD]\n[--push_rewards]\n[--future_reward_discount FUTURE_REWARD_DISCOUNT]\n[--experience_replay] [--heuristic_bootstrap]\n[--explore_rate_decay] [--grasp_only] [--check_row]\n[--random_weights] [--max_iter MAX_ITER] [--place]\n[--no_height_reward] [--grasp_color_task]\n[--grasp_count GRASP_COUT] [--transfer_grasp_to_place]\n[--check_z_height] [--trial_reward]\n[--check_z_height_goal CHECK_Z_HEIGHT_GOAL]\n[--disable_situation_removal] [--is_testing]\n[--evaluate_random_objects] [--max_test_trials MAX_TEST_TRIALS]\n[--test_preset_cases] [--test_preset_file TEST_PRESET_FILE]\n[--test_preset_dir TEST_PRESET_DIR]\n[--show_preset_cases_then_exit] [--load_snapshot]\n[--snapshot_file SNAPSHOT_FILE] [--nn NN] [--continue_logging]\n[--logging_directory LOGGING_DIRECTORY] [--save_visualizations]\nTrain robotic agents to learn how to plan complementary pushing, grasping, and placing as well as multi-step tasks\nfor manipulation with deep reinforcement learning in PyTorch.\noptional arguments:\n-h, --help show this help message and exit\n--is_sim run in simulation?\n--obj_mesh_dir OBJ_MESH_DIR\ndirectory containing 3D mesh files (.obj) of objects\nto be added to simulation\n--num_obj NUM_OBJ number of objects to add to simulation\n--num_extra_obj NUM_EXTRA_OBJ\nnumber of secondary objects, like distractors, to add\nto simulation\n--tcp_host_ip TCP_HOST_IP\nIP address to robot arm as TCP client (UR5)\n--tcp_port TCP_PORT port to robot arm as TCP client (UR5)\n--rtc_host_ip RTC_HOST_IP\nIP address to robot arm as real-time client (UR5)\n--rtc_port RTC_PORT port to robot arm as real-time client (UR5)\n--heightmap_resolution HEIGHTMAP_RESOLUTION\nmeters per pixel of heightmap\n--random_seed RANDOM_SEED\nrandom seed for simulation and neural net\ninitialization\n--cpu force code to run in CPU mode\n--flops calculate floating point operations of a forward pass\nthen exit\n--method METHOD set to 'reactive' (supervised learning) or\n'reinforcement' (reinforcement learning ie Q-learning)\n--push_rewards use immediate rewards (from change detection) for\npushing?\n--future_reward_discount FUTURE_REWARD_DISCOUNT\n--experience_replay use prioritized experience replay?\n--heuristic_bootstrap\nuse handcrafted grasping algorithm when grasping fails\ntoo many times in a row during training?\n--explore_rate_decay\n--grasp_only\n--check_row check for placed rows instead of stacks\n--random_weights use random weights rather than weights pretrained on\nImageNet\n--max_iter MAX_ITER max iter for training. -1 (default) trains\nindefinitely.\n--place enable placing of objects\n--no_height_reward disable stack height reward multiplier\n--grasp_color_task enable grasping specific colored objects\n--grasp_count GRASP_COUT\nnumber of successful task based grasps\n--transfer_grasp_to_place\nLoad the grasping weights as placing weights.\n--check_z_height use check_z_height instead of check_stacks for any\nstacks\n--trial_reward Experience replay delivers rewards for the whole\ntrial, not just next step.\n--check_z_height_goal CHECK_Z_HEIGHT_GOAL\ncheck_z_height goal height, a value of 2.0 is 0.1\nmeters, and a value of 4.0 is 0.2 meters\n--disable_situation_removal\nDisables situation removal, where rewards are set to 0\nand a reset is triggerd upon reveral of task progress.\n--is_testing\n--evaluate_random_objects\nEvaluate trials with random block positions, for\nexample testing frequency of random rows.\n--max_test_trials MAX_TEST_TRIALS\nmaximum number of test runs per case/scenario\n--test_preset_cases\n--test_preset_file TEST_PRESET_FILE\n--test_preset_dir TEST_PRESET_DIR\n--show_preset_cases_then_exit\njust show all the preset cases so you can have a look,\nthen exit\n--load_snapshot load pre-trained snapshot of model?\n--snapshot_file SNAPSHOT_FILE\n--nn NN Neural network architecture choice, options are\nefficientnet, densenet\n--continue_logging continue logging from previous session?\n--logging_directory LOGGING_DIRECTORY\n--save_visualizations\nsave visualizations of FCN predictions?\nResults from our baseline comparisons and ablation studies in our paper can be reproduced using these flags. For example:\nTrain reactive policies with pushing and grasping (P+G Reactive); specify --method to be 'reactive', remove --push_rewards, remove --explore_rate_decay:\npython main.py --is_sim --method 'reactive' --experience_replay --save_visualizations\nTrain reactive policies with grasping-only (Grasping-only); similar arguments as P+G Reactive above, but add --grasp_only:\npython main.py --is_sim --method 'reactive' --experience_replay --grasp_only --save_visualizations\nTrain VPG policies without any rewards for pushing (VPG-noreward); similar arguments as regular VPG, but remove --push_rewards:\npython main.py --is_sim --experience_replay --explore_rate_decay --save_visualizations\nTrain shortsighted VPG policies with lower discount factors on future rewards (VPG-myopic); similar arguments as regular VPG, but set --future_reward_discount to 0.2:\npython main.py --is_sim --push_rewards --future_reward_discount 0.2 --experience_replay --explore_rate_decay --save_visualizations\nTo plot the performance of a session over training time, run the following:\npython plot.py 'logs/YOUR-SESSION-DIRECTORY-NAME-HERE'\nSolid lines indicate % grasp success rates (primary metric of performance) and dotted lines indicate % push-then-grasp success rates (secondary metric to measure quality of pushes) over training steps. By default, each point in the plot measures the average performance over the last 200 training steps. The range of the x-axis is from 0 to 2500 training steps. You can easily change these parameters at the top of plot.py.\nTo compare performance between different sessions, you can draw multiple plots at a time:\npython plot.py 'logs/YOUR-SESSION-DIRECTORY-NAME-HERE' 'logs/ANOTHER-SESSION-DIRECTORY-NAME-HERE'\nEvaluation\nWe provide a collection 11 test cases in simulation with adversarial clutter. Each test case consists of a configuration of 3 - 6 objects placed in the workspace in front of the robot. These configurations are manually engineered to reflect challenging picking scenarios, and remain exclusive from the training procedure. Across many of these test cases, objects are laid closely side by side, in positions and orientations that even an optimal grasping policy would have trouble successfully picking up any of the objects without de-cluttering first. As a sanity check, a single isolated object is additionally placed in the workspace separate from the configuration. This is just to ensure that all policies have been sufficiently trained prior to the benchmark (i.e. a policy is not ready if fails to grasp the isolated object).\nThe demo above runs our pre-trained model multiple times (x30) on a single test case. To test your own pre-trained model, simply change the location of --snapshot_file:\nexport CUDA_VISIBLE_DEVICES=\"0\" && python3 main.py --is_sim --obj_mesh_dir 'objects/toys' --num_obj 10 --push_rewards --experience_replay --explore_rate_decay --load_snapshot --snapshot_file '/home/$USER/Downloads/snapshot.reinforcement.pth' --random_seed 1238 --is_testing --save_visualizations --test_preset_cases --test_preset_dir 'simulation/test-cases/' --max_test_trials 10\nData from each test case will be saved into a session directory in the logs folder. To report the average testing performance over a session, run the following:\npython evaluate.py --session_directory 'logs/YOUR-SESSION-DIRECTORY-NAME-HERE' --method SPECIFY-METHOD --num_obj_complete N\nwhere SPECIFY-METHOD can be reactive or reinforcement, depending on the architecture of your model.\n--num_obj_complete N defines the number of objects that need to be picked in order to consider the task completed. For example, when evaluating our pre-trained model in the demo test case, N should be set to 6:\npython evaluate.py --session_directory 'logs/YOUR-SESSION-DIRECTORY-NAME-HERE' --method 'reinforcement' --num_obj_complete 6\nAverage performance is measured with three metrics (for all metrics, higher is better):\nAverage % completion rate over all test runs: measures the ability of the policy to finish the task by picking up at least N objects without failing consecutively for more than 10 attempts.\nAverage % grasp success rate per completion.\nAverage % action efficiency: describes how succinctly the policy is capable of finishing the task. See our paper for more details on how this is computed.\nCreating Your Own Test Cases in Simulation\nTo design your own challenging test case:\nOpen the simulation environment in V-REP (navigate to your V-REP directory and run ./vrep.sh). From the main menu, select File > Open scene..., and open the file visual-pushing-grasping/simulation/simulation.ttt.\nIn another terminal window, navigate to this repository and run the following:\npython create.py\nIn the V-REP window, use the V-REP toolbar (object shift/rotate) to move around objects to desired positions and orientations.\nIn the terminal window type in the name of the text file for which to save the test case, then press enter.\nTry it out: run a trained model on the test case by running main.py just as in the demo, but with the flag --test_preset_file pointing to the location of your test case text file.\nRunning on a Real Robot (UR5)\nThe same code in this repository can be used to train on a real UR5 robot arm (tested with UR Software version 1.8). To communicate with later versions of UR software, several minor changes may be necessary in robot.py (e.g. functions like parse_tcp_state_data). Tested with Python 3.\nSetting Up Camera System\nThe PrimeSense Camera can be used with the perception packages from the Berkeley Automation Lab.\nAlternatively, the latest version of our system uses RGB-D data captured from an Intel\u00ae RealSense\u2122 D415 Camera. We provide a lightweight C++ executable that streams data in real-time using librealsense SDK 2.0 via TCP. This enables you to connect the camera to an external computer and fetch RGB-D data remotely over the network while training. This can come in handy for many real robot setups. Of course, doing so is not required -- the entire system can also be run on the same computer.\nInstallation Instructions:\nDownload and install librealsense SDK 2.0\nNavigate to visual-pushing-grasping/realsense and compile realsense.cpp:\ncd visual-pushing-grasping/realsense\ncmake .\nmake\nConnect your RealSense camera with a USB 3.0 compliant cable (important: RealSense D400 series uses a USB-C cable, but still requires them to be 3.0 compliant to be able to stream RGB-D data).\nTo start the TCP server and RGB-D streaming, run the following:\n./realsense\nKeep the executable running while calibrating or training with the real robot (instructions below). To test a python TCP client that fetches RGB-D data from the active TCP server, run the following:\ncd visual-pushing-grasping/real\npython capture.py\nCalibrating Camera Extrinsics\nWe provide a simple calibration script calibration_ros.py to estimate camera extrinsics with respect to robot base coordinates. In this project, we are dealing with an eye-on-base calibration (see more explanation of eye-on-base vs eye-on-hand here). To do so, the script move the robot to several random positions and orientations within the workspace.\nWe are using the PrimeSense Carmine 1.08 for this project. If you are using other cameras which need calibration for the depth scale (e.g. Intel RealSense D415, you may refer the calibration method provided by Andy Zeng here.\nTraining\nTo train on the real robot, simply run:\npython main.py --tcp_host_ip 'XXX.XXX.X.XXX' --tcp_port 30002 --push_rewards --experience_replay --explore_rate_decay --save_visualizations\nwhere XXX.XXX.X.XXX is the network IP address of your UR5 robot controller.\nAdditional Tools\nUse touch.py to test calibrated camera extrinsics -- provides a UI where the user can click a point on the RGB-D image, and the robot moves its end-effector to the 3D location of that point\nUse debug.py to test robot communication and primitive actions", "link": "https://github.com/jhu-lcsr/good_robot", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "\"good robot!\" efficient reinforcement learning for multi-step visual tasks with sim to real transfer\nandrew hundt, benjamin killeen, nicholas greene, hongtao wu, heeyeon kwon, chris paxton, and gregory d. hager\nclick the image to watch the video:\npaper, abstract, and citations\ngood robot! paper on ieee xplore, good robot! paper on arxiv\n@article{hundt2020good,\ntitle=\"\u201cgood robot!\u201d: efficient reinforcement learning for multi-step visual tasks with sim to real transfer\",\nauthor=\"andrew {hundt} and benjamin {killeen} and nicholas {greene} and hongtao {wu} and heeyeon {kwon} and chris {paxton} and gregory d. {hager}\",\njournal=\"ieee robotics and automation letters (ra-l)\",\nvolume=\"5\",\nnumber=\"4\",\npages=\"6724--6731\",\nyear=\"2020\",\nurl={https://arxiv.org/abs/1909.11730}\n}\nabstract\u2014 current reinforcement learning (rl) algorithms struggle with long-horizon tasks where time can be wasted exploring dead ends and task progress may be easily reversed. we develop the spot framework, which explores within action safety zones, learns about unsafe regions without exploring them, and prioritizes experiences that reverse earlier progress to learn with remarkable efficiency.\nthe spot framework successfully completes simulated trials of a variety of tasks, improving a baseline trial success rate from 13% to 100% when stacking 4 cubes, from 13% to 99% when creating rows of 4 cubes, and from 84% to 95% when clearing toys arranged in adversarial patterns. efficiency with respect to actions per trial typically improves by 30% or more, while training takes just 1-20k actions, depending on the task.\nfurthermore, we demonstrate direct sim to real transfer. we are able to create real stacks in 100% of trials with 61% efficiency and real rows in 100% of trials with 59% efficiency by directly loading the simulation-trained model on the real robot with no additional real-world fine-tuning. to our knowledge, this is the first instance of reinforcement learning with successful sim to real transfer applied to long term multi-step tasks such as block-stacking and row-making with consideration of progress reversal. code is available at https://github.com/jhu-lcsr/good_robot.\ntraining row making, stack making, and table clearing tasks\ndetails of our specific training and test runs, command line commands, pre-trained models, and logged data with images are on the good_robot github releases page.\nstarting the v-rep simulation\ndownload coppeliasim (formerly v-rep) and run it to start the simulation. uou may need to adjust the paths below to match your v-rep folder, and it should be run from the costar_visual_stacking repository directory:\ncd ~/src/real_good_robot\n~/src/coppeliasim_edu_v4_0_0_ubuntu18_04/coppeliasim.sh -gremoteapiserverservice_19997_false_true -s ~/src/real_good_robot/simulation/simulation.ttt\ncube stacking\ncube stack training\nexport cuda_visible_devices=\"0\" && python3 main.py --is_sim --obj_mesh_dir objects/blocks --num_obj 4 --push_rewards --experience_replay --explore_rate_decay --check_row --tcp_port 19997 --place --future_reward_discount 0.65 --max_train_actions 20000 --random_actions --common_sense --trial_reward\nall training results will go in the path/to/good_robot/logs/<training_run> folder. to use spot progress reward remove --trial_reward from this command. to disable the \"common sense\" dynamic action space regions remove --common_sense from this command. this command will automatically run in test mode once training is complete, and the testing results directory will be moved into the training results directory.\nmanual cube stack testing\nremember to first train the model or download the snapshot file from the release page (ex: v0.12 release) and update the command line --snapshot_file filepath:\nexport cuda_visible_devices=\"0\" && python3 main.py --is_sim --obj_mesh_dir 'objects/blocks' --num_obj 4 --push_rewards --experience_replay --explore_rate_decay --place --load_snapshot --snapshot_file ~/downloads/snapshot.reinforcement-best-stack-rate.pth --random_seed 1238 --is_testing --save_visualizations --disable_situation_removal\nsim to real stack testing\nyou will need a fully configured real robot, we have some instructions in this readme. you will also need to update the snapshot file path to wherever your version is:\nexport cuda_visible_devices=\"0\" && python3 main.py --num_obj 8 --push_rewards --experience_replay --explore_rate_decay --trial_reward --common_sense --check_z_height --place --future_reward_discount 0.65 --is_testing --random_seed 1238 --max_test_trials 10 --save_visualizations --random_actions --snapshot_file /media/costar/f5f1f858-3666-4832-beea-b743127f1030/real_good_robot/logs/2020-05-13-12-51-39_sim-stack-spot-trial-reward-masked-training/models/snapshot.reinforcement_action_efficiency_best_value.pth\nrow of 4 cubes\nrow testing video:\nrow v0.12 release page and pretrained models.\nrow training\ncuda_visible_devices=\"0\" && python3 main.py --is_sim --obj_mesh_dir objects/blocks --num_obj 4 --push_rewards --experience_replay --explore_rate_decay --check_row --tcp_port 19997 --place --future_reward_discount 0.65 --max_train_actions 20000 --random_actions --common_sense\ntesting will automatically run after training is complete.\nmanual row testing\nexport cuda_visible_devices=\"0\" && python3 main.py --is_sim --obj_mesh_dir 'objects/blocks' --num_obj 4 --push_rewards --experience_replay --explore_rate_decay --trial_reward --future_reward_discount 0.65 --place --check_row --is_testing --tcp_port 19996 --load_snapshot --snapshot_file '/home/costar/downloads/snapshot-backup.reinforcement-best-stack-rate.pth' --random_seed 1238 --disable_situation_removal --save_visualizations\nsim to real row testing\nyou will need a fully configured real robot, we have some instructions in this readme. you will also need to update the snapshot file path to wherever your version is:\nexport cuda_visible_devices=\"0\" && python3 main.py --num_obj 4 --push_rewards --experience_replay --explore_rate_decay --check_row --check_z_height --place --future_reward_discount 0.65 --is_testing --random_seed 1238 --max_test_trials 10 --random_actions --save_visualizations --common_sense --snapshot_file \"/home/costar/src/real_good_robot/logs/2020-06-03-12-05-28_sim-rows-two-step-reward-masked-training/models/snapshot.reinforcement_trial_success_rate_best_value.pth\"\npush + grasp\nwe provide backwards compatibility with the visual pushing grasping (vpg) github repository, and evaluate on their pushing and grasping task as a baseline from which to compare our algorithms.\npush + grasp training\nexport cuda_visible_devices=\"0\" && python3 main.py --is_sim --obj_mesh_dir 'objects/toys' --num_obj 10 --push_rewards --experience_replay --explore_rate_decay\nyou can also run without --trial_reward and with the default --future_reward_discount 0.5.\npush + grasp random object location testing\nexport cuda_visible_devices=\"0\" && python3 main.py --is_sim --obj_mesh_dir 'objects/toys' --num_obj 10 --push_rewards --experience_replay --explore_rate_decay --load_snapshot --snapshot_file '/home/costar/src/costar_visual_stacking/logs/2019-08-17.20:54:32-train-grasp-place-split-efficientnet-21k-acc-0.80/models/snapshot.reinforcement.pth' --random_seed 1238 --is_testing --save_visualizations\npush + grasp adversarial object location testing\nadversarial pushing and grasping release v0.3.2 video:\nexport cuda_visible_devices=\"0\" && python3 main.py --is_sim --obj_mesh_dir 'objects/toys' --num_obj 10 --push_rewards --experience_replay --explore_rate_decay --trial_reward --future_reward_discount 0.65 --tcp_port 19996 --is_testing --random_seed 1238 --load_snapshot --snapshot_file '/home/ahundt/src/costar_visual_stacking/logs/2019-09-12.18:21:37-push-grasp-16k-trial-reward/models/snapshot.reinforcement.pth' --max_test_trials 10 --test_preset_cases\nafter running the test you need to summarize the results:\npython3 evaluate.py --session_directory /home/ahundt/src/costar_visual_stacking/logs/2019-09-16.02:11:25 --method reinforcement --num_obj_complete 6 --preset\ntroubleshooting\nduring training, be sure to check the simulator doesn't encounter physics bugs, such as blocks permanently bonded to the gripper. if this happens, you will notice one action will never succeed, and the other might always succeed, and multi-step tasks will no longer progress. if you catch it quickly, you can directly remove the object in the simulator with the scene editing tools, or simply stop the simulation and the training function should detect the problem and reset.\ndata collection and data release checklist (v0.1, checklist creation is in progress)\nthis is a checklist for all the steps you should follow to collect data consistently.\nsave the following data about the run you are going to do into a txt file for your records: 1. describe the configuration you are running 2. git commit hash of the code version you will run 2. exact command line command you will run 3. current coppeliasim (aka v-rep) ip address port (if simulation mode) 4. gpu number you will use on the computer\nstart recording: if sim, start v-rep & start v-rep recording. if running the real robot, start a real camera recording the robot.\nstart the run.\nafter the run completes:\ndo a random testing run for a reasonable number of trials like 10-100.\nif pushing and grasping do an adversarial scenario testing run.\nmake note of the directories with results, and include dir path in your txt file.\ncreate a github release.\nadd the model file binary to the release binaries so it is easier to download and utilize.\nincorporate the log folder name in the release name.\nprovide a good description in english of what the run was trying to achieve.\nfind several representative images, and add them directly to the release page for easy viewing.\nzip visualization directory separately because they tend to be larger files & upload to the github release.\nmove the visualization directory out and then zip the remainder of the log directory and upload to the github release.\nzip up the random testing directories and adversarial testing directories (if applicable) upload to the github release binaries.\nupload the video recording to a website such as youtube, and add a link to the github release.\nmake sure the github release commit lines up with the commit you actually ran your experiment on.\nsubmit the github release.\ncostar visual stacking execution details\nrunning multiple simulations in parallel\nit is possible to do multiple runs on different gpus on the same machine. first, start an instance of v-rep as below,\n~/src/v-rep_pro_edu_v3_6_2_ubuntu16_04/vrep.sh -gremoteapiserverservice_19997_false_true -s simulation/simulation.ttt\nbeing careful to use v-rep 3.6.2 wherever it is installed locally. the port number, here 19997 which is the usual default, is the important point, as we will cahnge it in subsequent runs.\nstart the simulation as usual, but now specify --tcp_port 19997.\nstart another v-rep session.\n~/src/v-rep_pro_edu_v3_6_2_ubuntu16_04/vrep.sh -gremoteapiserverservice_19996_false_true -s simulation/simulation.ttt\nfor some reason, the port number is important here, and should be selected to be lower than already running sessions.\nwhen you start training, be sure to specify a different gpu. for example, if previously you set\nexport cuda_visible_devices=\"0\"\nthen you should likely set\nexport cuda_visible_devices=\"1\"\nand specify the corresponding tcp port --tcp_port 19996.\nadditional runs in parallel should use ports 19995, 19994, etc.\ngenerating plots\nto generate plots run python3 plot.py, you can manually edit the bottom part of plot.py to call plot_it() for your folder.\nlog_dir = './logs/2020-02-03-16-57-28_sim-stack-trial-reward-common-sense-training'\nplot_it(log_dir,'sim stack, spot reward, common sense, training', window=window, max_iter=none)\nthe configuration of the -----> plot !!!  should be done automatically from the folder.\nrunning on a real ur5 with ros based image collection\nros based image collection setup\nwe require python3, so you'll need to ensure export ros_python_version=3 is set for the build. a couple additional steps below will need to be added in the middle. we advise installing in the folder:\n~/ros_catkin_ws\nfollow instructions in the ros melodic steps to build ros from source.\nin particular fix up this command:\nexport ros_python_version=3 && rosinstall_generator desktop_full --rosdistro melodic --deps --tar > melodic-desktop-full.rosinstall && wstool init -j8 src melodic-desktop-full.rosinstall\nfor the primesense camera add in the openni2_launch, and rgbd_launch repositories, and for handeye calibration between the camera and robot add ubiquityrobotics/fiducials:\ncd ~/catkin_ros_ws\ngit clone https://github.com/ros-drivers/openni2_launch.git\ngit clone https://github.com/ros-drivers/rgbd_launch.git\ngit clone https://github.com/ubiquityrobotics/fiducials.git\nrun the build and install.\ncd ~/ros_catkin_ws\nrosdep install --from-paths src --ignore-src --rosdistro melodic -y && ./src/catkin/bin/catkin_make_isolated --install\nsource the ros setup so you get access to the launch commands:\nsource ~/ros_catkin_ws/install_isolated/setup.zsh\nrunning ros with depth image processing:\ntaskset 0x00000fff roslaunch openni2_launch openni2.launch depth_registration:=true\nwe use the linux taskset command (examples) to limit ros to utilizing 8 cores or fewer, so other cores are available for training.\nin a separate tab run our small test script:\npython test_ros_images.py\nrunning rviz to look at the images:\nrosrun rviz rviz\nthe correct images, as done in the jhu costar dataset class collector, are from the following ros topics:\nself.rgb_topic = \"/camera/rgb/image_rect_color\"\n# raw means it is in the format provided by the openi drivers, 16 bit int\nself.depth_topic = \"/camera/depth_registered/hw_registered/image_rect\"\ncalibrating camera intrincics\nyou must first calibrate your rgb and depth camera intrinsics and rectify your images to ensure you can accurately convert camera positions to robot poses. we do this using camera_calibration in the ros-perception/image_pipeline library.\nyou will need to generate and load a calibration yaml file which goes in a location like ~/.ros/camera_info/rgb_ps1080_primesense.yaml. we have an examle from our robot in this repository saved at real/rgb_ps1080_primesense.yaml.\ncalibrating camera extrinsics\nprint an aruco tag, we use 70mm tags with the 4x4 design (dictionary 1), so it can fit in the gripper. make sure the aruco dictionary id in the launch files is correct. attach the aruco tag on the robot.\nedit the fiducials ros package aruco_detect.launch file in ~/ros_catkin_ws/src/fiducials/aruco_detect/launch/aruco_detect.launch from the fiducials github repository you cloned earlier, see the fiducials wiki for reference. modify the launch file in [fiducials/aruco to detect your markers and receive images from your sensor. here is our configuration:\n<!-- run the aruco_detect node -->\n<launch>\n<!-- namespace for camera input -->\n<!-- /camera/rgb/image_rect_color/compressed -->\n<arg name=\"camera\" default=\"/camera/rgb\"/>\n<arg name=\"image\" default=\"image_rect_color\"/>\n<arg name=\"transport\" default=\"compressed\"/>\n<arg name=\"fiducial_len\" default=\"0.07\"/>\n<arg name=\"dictionary\" default=\"1\"/>\n<arg name=\"do_pose_estimation\" default=\"true\"/>\n<arg name=\"ignore_fiducials\" default=\"\" />\n<arg name=\"fiducial_len_override\" default=\"\" />\n<node pkg=\"aruco_detect\" name=\"aruco_detect\"\ntype=\"aruco_detect\" output=\"screen\" respawn=\"false\">\n<param name=\"image_transport\" value=\"$(arg transport)\"/>\n<param name=\"publish_images\" value=\"true\" />\n<param name=\"fiducial_len\" value=\"$(arg fiducial_len)\"/>\n<param name=\"dictionary\" value=\"$(arg dictionary)\"/>\n<param name=\"do_pose_estimation\" value=\"$(arg do_pose_estimation)\"/>\n<param name=\"ignore_fiducials\" value=\"$(arg ignore_fiducials)\"/>\n<param name=\"fiducial_len_override\" value=\"$(arg fiducial_len_override)\"/>\n<remap from=\"/camera/compressed\"\nto=\"$(arg camera)/$(arg image)/$(arg transport)\"/>\n<remap from=\"/camera_info\" to=\"$(arg camera)/camera_info\"/>\n</node>\n</launch>\nyou must predefine the workspace_limits python variables in the calibration_ros.py, touch.py, and main.py, and robot.py. to modify these locations, change the variables workspace_limits at the end of calibrate_ros.py. you may define it in the calibrate class or in the function collect_data for data collection.\nthe code directly communicates with the robot via tcp. at the top of calibrate_ros.py, change variable tcp_host_ip to point to the network ip address of your ur5 robot controller.\nroslaunch the camera with, for example:\ntaskset 0x00000fff roslaunch openni2_launch openni2.launch depth_registration:=true\nwe use the [linux taskset command](https://linux.die.net/man/1/taskset) ([examples](https://www.howtoforge.com/linux-taskset-command/)) to limit ros to utilizing 8 cores or fewer, so other cores are available for training.\nthe script is subscribed to the rostopic /fiducial_transform to get the pose of the tag in the camera frame. roslaunch aruco_detect:\ntaskset 0x00000fff roslaunch aruco_detect aruco_detect.launch\nthe robot will move suddenly and rapidly. users must be ready to push the emergency stop button at any time.\ncarefully run python touch.py to start the arm, it will move suddenly!\ncenter the ar tag on the gripper manually using the teach mode button on the robot.\nclick on the title bar of the color image window, do not click on the general color area the robot may move suddenly!\npress - to close the gripper (= will open it), and check that the center of the ar tag is where you want your gripper center to be defined.\npress k to calibrate, and after going to a number of positions you should see a calibration result like the following:\ntotal number of poses: 26\ninvalid poses number: 0\nrobot base to camera:\n[[ 0.1506513 0.87990966 -0.45062533 0.79319678]\n[ 0.98857761 -0.13210593 0.07254191 -0.14601768]\n[ 0.00430005 -0.45640664 -0.88976092 0.55173518]\n[ 0. 0. 0. 1. ]]\ntotal number of poses: 26\ninvalid poses number: 0\ntool tip to ar tag:\n[[ 0.18341198 -0.01617267 -0.98290309 0.0050482 ]\n[ 0.03295954 0.99940367 -0.01029385 0.01899328]\n[ 0.98248344 -0.03050802 0.18383565 0.10822485]\n[ 0. 0. 0. 1. ]]\nbackup procedure (in place of the steps 6 and later from above): with caution, run the following to move the robot and calibrate:\nthe robot will move suddenly and rapidly. users must be ready to push the emergency stop button at any time.\npython calibrate_ros.py\nthe script will record the pose of the robot and the aruco tag in the camera frame with correspondence. then it uses the park and martin method to solve the ax=xb problem for the hand-eye calibration. and the method is implemented in the utils.py. the script will generate a robot_base_to_camera_pose.txt in real/. this txt basically is the pose of the camera in the robot base frame.\nif you already have corresponded pose file of the robot and the aruco tag, you can also use the calibrate() function in the calibrate_ros.py to directly calculate the pose of the camera without the data collection step.\ncollecting the background heightmap\nthe real robot also uses a background heightmap of the scene with no objects present.\ncompletely clear the table or working surface.\nback up and remove the current real/background_heightmap.depth.png.\nrun pushing and grasping data collection with the --show_heightmap flag.\nview the heightmap images until you see one with no holes (black spots), and save the iteration number at the top.\ncopy the good heightmap from logs/<run_folder>/data/depth_heightmaps/<iteration>.0.depth.png and rename it to real/background_heightmap.depth.png.\nstop and re-run pushing and grasping with the --show_heightmap flag.\nhere is an example of the matplotlib visualization of a good depth heightmap, there are no black specks aside from one corner which is out of the camera's field of view:\nyour updated depth heightmaps should be good to go!\n\"good robot!\" is forked from the visual pushing and grasping toolbox\noriginal visual pushing grasping (vpg) repository. edits have been made to the text below to reflect some configuration and code updates needed to reproduce the previous vpg paper's original behavior:\nvisual pushing and grasping (vpg) is a method for training robotic agents to learn how to plan complementary pushing and grasping actions for manipulation (e.g. for unstructured pick-and-place applications). vpg operates directly on visual observations (rgb-d images), learns from trial and error, trains quickly, and generalizes to new objects and scenarios.\nthis repository provides pytorch code for training and testing vpg policies with deep reinforcement learning in both simulation and real-world settings on a ur5 robot arm. this is the reference implementation for the paper:\nlearning synergies between pushing and grasping with self-supervised deep reinforcement learning\npdf | webpage & video results\nandy zeng, shuran song, stefan welker, johnny lee, alberto rodriguez, thomas funkhouser\nieee/rsj international conference on intelligent robots and systems (iros) 2018\nskilled robotic manipulation benefits from complex synergies between non-prehensile (e.g. pushing) and prehensile (e.g. grasping) actions: pushing can help rearrange cluttered objects to make space for arms and fingers; likewise, grasping can help displace objects to make pushing movements more precise and collision-free. in this work, we demonstrate that it is possible to discover and learn these synergies from scratch through model-free deep reinforcement learning. our method involves training two fully convolutional networks that map from visual observations to actions: one infers the utility of pushes for a dense pixel-wise sampling of end effector orientations and locations, while the other does the same for grasping. both networks are trained jointly in a q-learning framework and are entirely self-supervised by trial and error, where rewards are provided from successful grasps. in this way, our policy learns pushing motions that enable future grasps, while learning grasps that can leverage past pushes. during picking experiments in both simulation and real-world scenarios, we find that our system quickly learns complex behaviors amid challenging cases of clutter, and achieves better grasping success rates and picking efficiencies than baseline alternatives after only a few hours of training. we further demonstrate that our method is capable of generalizing to novel objects.\nciting\nif you find this code useful in your work, please consider citing:\n@inproceedings{zeng2018learning,\ntitle={learning synergies between pushing and grasping with self-supervised deep reinforcement learning},\nauthor={zeng, andy and song, shuran and welker, stefan and lee, johnny and rodriguez, alberto and funkhouser, thomas},\nbooktitle={ieee/rsj international conference on intelligent robots and systems (iros)},\nyear={2018}\n}\ndemo videos\ndemo videos of a real robot in action can be found here.\ncontact\nthe contact for costar visual stacking is andrew hundt. the contact for the original visual pushing grasping repository is andy zeng andyz[at]princeton[dot]edu\ninstallation\nthis implementation requires the following dependencies (tested on ubuntu 16.04.4 lts):\npython 2.7 or python 3\nnumpy, scipy, opencv-python, matplotlib. you can quickly install/update these dependencies by running the following (replace pip with pip3 for python 3):\npip3 install numpy scipy opencv-python matplotlib\npytorch version 1.2:\npip3 install torch==1.2 torchvision==0.4.0\nv-rep (simulation environment)\n(optional) gpu acceleration\naccelerating training/inference with an nvidia gpu requires installing cuda and cudnn. you may need to register with nvidia for the cuda developer program (it's free) before downloading. this code has been tested with cuda 8.0 and cudnn 6.0 on a single nvidia titan x (12gb). running out-of-the-box with our pre-trained models using gpu acceleration requires 8gb of gpu memory. running with gpu acceleration is highly recommended, otherwise each training iteration will take several minutes to run (as opposed to several seconds). this code automatically detects the gpu(s) on your system and tries to use it. if you have a gpu, but would instead like to run in cpu mode, add the tag --cpu when running main.py below.\na quick-start: demo in simulation\nthis demo runs our pre-trained model with a ur5 robot arm in simulation on challenging picking scenarios with adversarial clutter, where grasping an object is generally not feasible without first pushing to break up tight clusters of objects.\ninstructions\ncheckout this repository and download our pre-trained models.\ngit clone https://github.com/jhu-lcsr/costar_visual_stacking.git visual-pushing-grasping\ncd visual-pushing-grasping/downloads\n./download-weights.sh\ncd ..\nrun v-rep (navigate to your v-rep directory and run ./vrep.sh). from the main menu, select file > open scene..., and open the file visual-pushing-grasping/simulation/simulation.ttt from this repository.\nin another terminal window, run the following (simulation will start in the v-rep window):\npython main.py --is_sim --obj_mesh_dir 'objects/blocks' --num_obj 10 \\\n--push_rewards --experience_replay --explore_rate_decay \\\n--is_testing --test_preset_cases --test_preset_file 'simulation/test-cases/test-10-obj-07.txt' \\\n--load_snapshot --snapshot_file 'downloads/vpg-original-sim-pretrained-10-obj.pth' \\\n--save_visualizations --nn densenet\nnote: you may get a popup window titled \"dynamics content\" in your v-rep window. select the checkbox and press ok. you will have to do this a total of 3 times before it stops annoying you.\ntraining\nto train a regular vpg policy from scratch in simulation, first start the simulation environment by running v-rep (navigate to your v-rep directory and run ./vrep.sh). from the main menu, select file > open scene..., and open the file visual-pushing-grasping/simulation/simulation.ttt. then navigate to this repository in another terminal window and run the following:\npython main.py --is_sim --push_rewards --experience_replay --explore_rate_decay --save_visualizations\ndata collected from each training session (including rgb-d images, camera parameters, heightmaps, actions, rewards, model snapshots, visualizations, etc.) is saved into a directory in the logs folder. a training session can be resumed by adding the --resume flag, which then loads the model snapshot specified by --snapshot_file and transition history from the provided session directory:\npython main.py --is_sim --push_rewards --experience_replay --explore_rate_decay --save_visualizations \\\n--snapshot_file 'logs/your-session-directory-name-here/models/snapshot-backup.reinforcement.pth' \\\n--resume 'logs/your-session-directory-name-here' \\\nif the --resume flag is raised without an argument, the most recent log file is used. if --resume is raised but no --snapshot-file is specified, the snapshot file from the implied or specified logging directory is used.\nvarious training options can be modified or toggled on/off with different flags (run python main.py -h to see all options):\nusage: main.py [-h] [--is_sim] [--obj_mesh_dir obj_mesh_dir]\n[--num_obj num_obj] [--num_extra_obj num_extra_obj]\n[--tcp_host_ip tcp_host_ip] [--tcp_port tcp_port]\n[--rtc_host_ip rtc_host_ip] [--rtc_port rtc_port]\n[--heightmap_resolution heightmap_resolution]\n[--random_seed random_seed] [--cpu] [--flops] [--method method]\n[--push_rewards]\n[--future_reward_discount future_reward_discount]\n[--experience_replay] [--heuristic_bootstrap]\n[--explore_rate_decay] [--grasp_only] [--check_row]\n[--random_weights] [--max_iter max_iter] [--place]\n[--no_height_reward] [--grasp_color_task]\n[--grasp_count grasp_cout] [--transfer_grasp_to_place]\n[--check_z_height] [--trial_reward]\n[--check_z_height_goal check_z_height_goal]\n[--disable_situation_removal] [--is_testing]\n[--evaluate_random_objects] [--max_test_trials max_test_trials]\n[--test_preset_cases] [--test_preset_file test_preset_file]\n[--test_preset_dir test_preset_dir]\n[--show_preset_cases_then_exit] [--load_snapshot]\n[--snapshot_file snapshot_file] [--nn nn] [--continue_logging]\n[--logging_directory logging_directory] [--save_visualizations]\ntrain robotic agents to learn how to plan complementary pushing, grasping, and placing as well as multi-step tasks\nfor manipulation with deep reinforcement learning in pytorch.\noptional arguments:\n-h, --help show this help message and exit\n--is_sim run in simulation?\n--obj_mesh_dir obj_mesh_dir\ndirectory containing 3d mesh files (.obj) of objects\nto be added to simulation\n--num_obj num_obj number of objects to add to simulation\n--num_extra_obj num_extra_obj\nnumber of secondary objects, like distractors, to add\nto simulation\n--tcp_host_ip tcp_host_ip\nip address to robot arm as tcp client (ur5)\n--tcp_port tcp_port port to robot arm as tcp client (ur5)\n--rtc_host_ip rtc_host_ip\nip address to robot arm as real-time client (ur5)\n--rtc_port rtc_port port to robot arm as real-time client (ur5)\n--heightmap_resolution heightmap_resolution\nmeters per pixel of heightmap\n--random_seed random_seed\nrandom seed for simulation and neural net\ninitialization\n--cpu force code to run in cpu mode\n--flops calculate floating point operations of a forward pass\nthen exit\n--method method set to 'reactive' (supervised learning) or\n'reinforcement' (reinforcement learning ie q-learning)\n--push_rewards use immediate rewards (from change detection) for\npushing?\n--future_reward_discount future_reward_discount\n--experience_replay use prioritized experience replay?\n--heuristic_bootstrap\nuse handcrafted grasping algorithm when grasping fails\ntoo many times in a row during training?\n--explore_rate_decay\n--grasp_only\n--check_row check for placed rows instead of stacks\n--random_weights use random weights rather than weights pretrained on\nimagenet\n--max_iter max_iter max iter for training. -1 (default) trains\nindefinitely.\n--place enable placing of objects\n--no_height_reward disable stack height reward multiplier\n--grasp_color_task enable grasping specific colored objects\n--grasp_count grasp_cout\nnumber of successful task based grasps\n--transfer_grasp_to_place\nload the grasping weights as placing weights.\n--check_z_height use check_z_height instead of check_stacks for any\nstacks\n--trial_reward experience replay delivers rewards for the whole\ntrial, not just next step.\n--check_z_height_goal check_z_height_goal\ncheck_z_height goal height, a value of 2.0 is 0.1\nmeters, and a value of 4.0 is 0.2 meters\n--disable_situation_removal\ndisables situation removal, where rewards are set to 0\nand a reset is triggerd upon reveral of task progress.\n--is_testing\n--evaluate_random_objects\nevaluate trials with random block positions, for\nexample testing frequency of random rows.\n--max_test_trials max_test_trials\nmaximum number of test runs per case/scenario\n--test_preset_cases\n--test_preset_file test_preset_file\n--test_preset_dir test_preset_dir\n--show_preset_cases_then_exit\njust show all the preset cases so you can have a look,\nthen exit\n--load_snapshot load pre-trained snapshot of model?\n--snapshot_file snapshot_file\n--nn nn neural network architecture choice, options are\nefficientnet, densenet\n--continue_logging continue logging from previous session?\n--logging_directory logging_directory\n--save_visualizations\nsave visualizations of fcn predictions?\nresults from our baseline comparisons and ablation studies in our paper can be reproduced using these flags. for example:\ntrain reactive policies with pushing and grasping (p+g reactive); specify --method to be 'reactive', remove --push_rewards, remove --explore_rate_decay:\npython main.py --is_sim --method 'reactive' --experience_replay --save_visualizations\ntrain reactive policies with grasping-only (grasping-only); similar arguments as p+g reactive above, but add --grasp_only:\npython main.py --is_sim --method 'reactive' --experience_replay --grasp_only --save_visualizations\ntrain vpg policies without any rewards for pushing (vpg-noreward); similar arguments as regular vpg, but remove --push_rewards:\npython main.py --is_sim --experience_replay --explore_rate_decay --save_visualizations\ntrain shortsighted vpg policies with lower discount factors on future rewards (vpg-myopic); similar arguments as regular vpg, but set --future_reward_discount to 0.2:\npython main.py --is_sim --push_rewards --future_reward_discount 0.2 --experience_replay --explore_rate_decay --save_visualizations\nto plot the performance of a session over training time, run the following:\npython plot.py 'logs/your-session-directory-name-here'\nsolid lines indicate % grasp success rates (primary metric of performance) and dotted lines indicate % push-then-grasp success rates (secondary metric to measure quality of pushes) over training steps. by default, each point in the plot measures the average performance over the last 200 training steps. the range of the x-axis is from 0 to 2500 training steps. you can easily change these parameters at the top of plot.py.\nto compare performance between different sessions, you can draw multiple plots at a time:\npython plot.py 'logs/your-session-directory-name-here' 'logs/another-session-directory-name-here'\nevaluation\nwe provide a collection 11 test cases in simulation with adversarial clutter. each test case consists of a configuration of 3 - 6 objects placed in the workspace in front of the robot. these configurations are manually engineered to reflect challenging picking scenarios, and remain exclusive from the training procedure. across many of these test cases, objects are laid closely side by side, in positions and orientations that even an optimal grasping policy would have trouble successfully picking up any of the objects without de-cluttering first. as a sanity check, a single isolated object is additionally placed in the workspace separate from the configuration. this is just to ensure that all policies have been sufficiently trained prior to the benchmark (i.e. a policy is not ready if fails to grasp the isolated object).\nthe demo above runs our pre-trained model multiple times (x30) on a single test case. to test your own pre-trained model, simply change the location of --snapshot_file:\nexport cuda_visible_devices=\"0\" && python3 main.py --is_sim --obj_mesh_dir 'objects/toys' --num_obj 10 --push_rewards --experience_replay --explore_rate_decay --load_snapshot --snapshot_file '/home/$user/downloads/snapshot.reinforcement.pth' --random_seed 1238 --is_testing --save_visualizations --test_preset_cases --test_preset_dir 'simulation/test-cases/' --max_test_trials 10\ndata from each test case will be saved into a session directory in the logs folder. to report the average testing performance over a session, run the following:\npython evaluate.py --session_directory 'logs/your-session-directory-name-here' --method specify-method --num_obj_complete n\nwhere specify-method can be reactive or reinforcement, depending on the architecture of your model.\n--num_obj_complete n defines the number of objects that need to be picked in order to consider the task completed. for example, when evaluating our pre-trained model in the demo test case, n should be set to 6:\npython evaluate.py --session_directory 'logs/your-session-directory-name-here' --method 'reinforcement' --num_obj_complete 6\naverage performance is measured with three metrics (for all metrics, higher is better):\naverage % completion rate over all test runs: measures the ability of the policy to finish the task by picking up at least n objects without failing consecutively for more than 10 attempts.\naverage % grasp success rate per completion.\naverage % action efficiency: describes how succinctly the policy is capable of finishing the task. see our paper for more details on how this is computed.\ncreating your own test cases in simulation\nto design your own challenging test case:\nopen the simulation environment in v-rep (navigate to your v-rep directory and run ./vrep.sh). from the main menu, select file > open scene..., and open the file visual-pushing-grasping/simulation/simulation.ttt.\nin another terminal window, navigate to this repository and run the following:\npython create.py\nin the v-rep window, use the v-rep toolbar (object shift/rotate) to move around objects to desired positions and orientations.\nin the terminal window type in the name of the text file for which to save the test case, then press enter.\ntry it out: run a trained model on the test case by running main.py just as in the demo, but with the flag --test_preset_file pointing to the location of your test case text file.\nrunning on a real robot (ur5)\nthe same code in this repository can be used to train on a real ur5 robot arm (tested with ur software version 1.8). to communicate with later versions of ur software, several minor changes may be necessary in robot.py (e.g. functions like parse_tcp_state_data). tested with python 3.\nsetting up camera system\nthe primesense camera can be used with the perception packages from the berkeley automation lab.\nalternatively, the latest version of our system uses rgb-d data captured from an intel\u00ae realsense\u2122 d415 camera. we provide a lightweight c++ executable that streams data in real-time using librealsense sdk 2.0 via tcp. this enables you to connect the camera to an external computer and fetch rgb-d data remotely over the network while training. this can come in handy for many real robot setups. of course, doing so is not required -- the entire system can also be run on the same computer.\ninstallation instructions:\ndownload and install librealsense sdk 2.0\nnavigate to visual-pushing-grasping/realsense and compile realsense.cpp:\ncd visual-pushing-grasping/realsense\ncmake .\nmake\nconnect your realsense camera with a usb 3.0 compliant cable (important: realsense d400 series uses a usb-c cable, but still requires them to be 3.0 compliant to be able to stream rgb-d data).\nto start the tcp server and rgb-d streaming, run the following:\n./realsense\nkeep the executable running while calibrating or training with the real robot (instructions below). to test a python tcp client that fetches rgb-d data from the active tcp server, run the following:\ncd visual-pushing-grasping/real\npython capture.py\ncalibrating camera extrinsics\nwe provide a simple calibration script calibration_ros.py to estimate camera extrinsics with respect to robot base coordinates. in this project, we are dealing with an eye-on-base calibration (see more explanation of eye-on-base vs eye-on-hand here). to do so, the script move the robot to several random positions and orientations within the workspace.\nwe are using the primesense carmine 1.08 for this project. if you are using other cameras which need calibration for the depth scale (e.g. intel realsense d415, you may refer the calibration method provided by andy zeng here.\ntraining\nto train on the real robot, simply run:\npython main.py --tcp_host_ip 'xxx.xxx.x.xxx' --tcp_port 30002 --push_rewards --experience_replay --explore_rate_decay --save_visualizations\nwhere xxx.xxx.x.xxx is the network ip address of your ur5 robot controller.\nadditional tools\nuse touch.py to test calibrated camera extrinsics -- provides a ui where the user can click a point on the rgb-d image, and the robot moves its end-effector to the 3d location of that point\nuse debug.py to test robot communication and primitive actions", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000445, "year": null}, {"Unnamed: 0": 1512, "autor": 492, "date": null, "content": "A Python block diagram simulation package\nGitHub repository\nDocumentation\nWiki (examples and details)\nInstallation\nDependencies: `numpy`, `scipy`, `matplotlib`, `spatialmath`, `ansitable`, `colored`, `ffmpeg` (if rendering animations as a movie)\nBlock diagram simulation\nThis Python package enables modelling and simulation of dynamic systems conceptualized in block diagram form, but represented in terms of Python class and method calls. Unlike Simulink or LabView we write Python code rather than drawing boxes and wires. Wires can communicate any Python type such as scalars, lists, numpy arrays, other objects, and even functions.\nWe first sketch the dynamic system we want to simulate as a block diagram, for example this simple first-order system\nwhich we can express concisely with bdsim as (see bdsim/examples/eg1.py\n1 #!/usr/bin/env python3\n2\n3 import bdsim\n4\n5\n6 sim = bdsim.BDSim(animation=True) # create simulator\n7 print(sim)\n8 bd = sim.blockdiagram() # create an empty block diagram\n9\n10 # define the blocks\n11 demand = bd.STEP(T=1, pos=(0,0), name='demand')\n12 sum = bd.SUM('+-', pos=(1,0))\n13 gain = bd.GAIN(10, pos=(1.5,0))\n14 plant = bd.LTI_SISO(0.5, [2, 1], name='plant', pos=(3,0))\n15 scope = bd.SCOPE(styles=['k', 'r--'], pos=(4,0))\n16\n17 # connect the blocks\n18 bd.connect(demand, sum[0], scope[1])\n19 bd.connect(plant, sum[1])\n20 bd.connect(sum, gain)\n21 bd.connect(gain, plant)\n22 bd.connect(plant, scope[0])\n23\n24 bd.compile() # check the diagram\n25 bd.report() # list all blocks and wires\n26\n27 out = sim.run(bd, 5, watch=[plant, demand]) # simulate for 5s\n28\n29 sim.savefig(scope, 'scope0')\n30 sim.done(bd, block=True)\nwhich is just 20 lines actual of code.\nThe red block annotations in the diagram are the names of blocks, and have become names of instances of object that represent those blocks. The blocks can also have names which are used in diagnostics and as labels in plots.\nIn bdsim all wires are point to point, a one-to-many connection is implemented by many wires.\nPorts are designated using Python indexing and slicing notation, for example sum[0]. Whether it is an input or output port depends on context. Blocks are connected by connect(from, to_1, to_2, ...) so an index on the first argument refers to an output port, while on the second (or subsequent) arguments refers to an input port. If a port has only a single port then no index is required.\nA bundle of wires can be denoted using slice notation, for example block[2:4] refers to ports 2 and 3. When connecting slices of ports the number of wires in each slice must be consistent. You could even do a cross over by connecting block1[2:4] to block2[5:2:-1].\nLine 25 generates a report, in tabular form, showing all the blocks and wires in the diagram.\nBlocks::\n\u250c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502id \u2502 name \u2502 nin \u2502 nout \u2502 nstate \u2502 ndstate \u2502 type \u2502\n\u251c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 0 \u2502 demand \u2502 0 \u2502 1 \u2502 0 \u2502 0 \u2502 step \u2502\n\u2502 1 \u2502 sum.0 \u2502 2 \u2502 1 \u2502 0 \u2502 0 \u2502 sum \u2502\n\u2502 2 \u2502 gain.0 \u2502 1 \u2502 1 \u2502 0 \u2502 0 \u2502 gain \u2502\n\u2502 3 \u2502 plant \u2502 1 \u2502 1 \u2502 1 \u2502 0 \u2502 LTI \u2502\n\u2502 4 \u2502 scope.0 \u2502 2 \u2502 0 \u2502 0 \u2502 0 \u2502 scope \u2502\n\u2514\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nWires::\n\u250c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502id \u2502 from \u2502 to \u2502 description \u2502 type \u2502\n\u251c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 0 \u2502 0[0] \u2502 1[0] \u2502 demand[0] --> sum.0[0] \u2502 int \u2502\n\u2502 1 \u2502 0[0] \u2502 4[1] \u2502 demand[0] --> scope.0[1] \u2502 int \u2502\n\u2502 2 \u2502 3[0] \u2502 1[1] \u2502 plant[0] --> sum.0[1] \u2502 float64 \u2502\n\u2502 3 \u2502 1[0] \u2502 2[0] \u2502 sum.0[0] --> gain.0[0] \u2502 float64 \u2502\n\u2502 4 \u2502 2[0] \u2502 3[0] \u2502 gain.0[0] --> plant[0] \u2502 float64 \u2502\n\u2502 5 \u2502 3[0] \u2502 4[0] \u2502 plant[0] --> scope.0[0] \u2502 float64 \u2502\n\u2514\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nwhere nstate is the number of continuous states and ndstate is the number of discrete states.\nLine 27 runs the simulation for 5 seconds using the default variable-step RK45 solver and saves output values at least every 0.1s. The scope block pops up a graph\nLine 29 saves the content causes the graphs in all displayed figures to be saved in the specified format, in this case the file would be called scope.b4.pdf.\nLine 28 blocks the script until all figure windows are closed, or the script is killed with SIGINT.\nThe result out is effectively a structure with elements\n>>> out\nresults:\nt | ndarray (67,)\nx | ndarray (67, 1)\nxnames | list\ny0 | ndarray (67,)\ny1 | ndarray (67,)\nynames | list\nwhere\nt the time vector: ndarray, shape=(M,)\nx is the state vector: ndarray, shape=(M,N), one row per timestep\nxnames is a list of the names of the states corresponding to columns of x, eg. \"plant.x0\"\nThe watch argument is a list of outputs to log, in this case plant defaults to output port 0. This information is saved in additional variables y0, y1 etc. ynames is a list of the names of the watched variables.\nLine 29 saves the scope graphics as a PDF file.\nLine 30 blocks until the last figure is dismissed.\nA Graphviz .dot file can be generated by\nbd.dotfile('demo.dot')\nwhich you can compile and display\n% dot -Tpng -o demo.png demo.dot\nor neato\n% neato -Tpng -o demo.png demo.dot\nWhile this is topologically correct, it's not quite the way we would expect the diagram to be drawn. dot ignores the pos options on the blocks while neato respects them, but is prone to drawing all the lines on top of each other.\nSources are shown as 3D boxes, sinks as folders, functions as boxes (apart from gains which are triangles and summing junctions which are points), and transfer functions as connectors (look's like a gate). To create a decent looking plot you need to manually place the blocks using the pos argument to place them. Unit spacing in the x- and y-directions is generally sufficient.\nThe sim object can do these operations in a convenient shorthand\nsim.showgraph(bd)\nand display the result via your webbrowser.\nOther examples\nIn the folder bdsim/examples you can find a few other runnable examples:\neg1.py the example given above\nwaveform.py two signal generators connected to two scopes\nExamples from Chapter four of Robotics, Vision & Control (2017):\nrvc4_2.py Fig 4.2 - a car-like vehicle with bicycle kinematics driven by a rectangular pulse steering signal\nrvc4_4.py Fig 4.4 - a car-like vehicle driving to a point\nrvc4_6.py Fig 4.6 - a car-like vehicle driving to/along a line\nrvc4_8.py Fig 4.8 - a car-like vehicle using pure-pursuit trajectory following\nrvc4_11.py Fig 4.11 a car-like vehicle driving to a pose\nFigs 4.8 (pure pursuit) and Fig 4.21 (quadrotor control) are yet to be done.\nLimitations\nThere are lots! The biggest is that bdsim is based on a very standard variable-step integrator from the scipy library. For discontinuous inputs (step, square wave, triangle wave, piecewise constant) the transitions get missed. This also makes it inaccurate to simulate hybrid discrete-continuous time systems. We really need a better integrator, perhaps odedc from SciLab could be integrated.", "link": "https://github.com/petercorke/bdsim", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "a python block diagram simulation package\ngithub repository\ndocumentation\nwiki (examples and details)\ninstallation\ndependencies: `numpy`, `scipy`, `matplotlib`, `spatialmath`, `ansitable`, `colored`, `ffmpeg` (if rendering animations as a movie)\nblock diagram simulation\nthis python package enables modelling and simulation of dynamic systems conceptualized in block diagram form, but represented in terms of python class and method calls. unlike simulink or labview we write python code rather than drawing boxes and wires. wires can communicate any python type such as scalars, lists, numpy arrays, other objects, and even functions.\nwe first sketch the dynamic system we want to simulate as a block diagram, for example this simple first-order system\nwhich we can express concisely with bdsim as (see bdsim/examples/eg1.py\n1 #!/usr/bin/env python3\n2\n3 import bdsim\n4\n5\n6 sim = bdsim.bdsim(animation=true) # create simulator\n7 print(sim)\n8 bd = sim.blockdiagram() # create an empty block diagram\n9\n10 # define the blocks\n11 demand = bd.step(t=1, pos=(0,0), name='demand')\n12 sum = bd.sum('+-', pos=(1,0))\n13 gain = bd.gain(10, pos=(1.5,0))\n14 plant = bd.lti_siso(0.5, [2, 1], name='plant', pos=(3,0))\n15 scope = bd.scope(styles=['k', 'r--'], pos=(4,0))\n16\n17 # connect the blocks\n18 bd.connect(demand, sum[0], scope[1])\n19 bd.connect(plant, sum[1])\n20 bd.connect(sum, gain)\n21 bd.connect(gain, plant)\n22 bd.connect(plant, scope[0])\n23\n24 bd.compile() # check the diagram\n25 bd.report() # list all blocks and wires\n26\n27 out = sim.run(bd, 5, watch=[plant, demand]) # simulate for 5s\n28\n29 sim.savefig(scope, 'scope0')\n30 sim.done(bd, block=true)\nwhich is just 20 lines actual of code.\nthe red block annotations in the diagram are the names of blocks, and have become names of instances of object that represent those blocks. the blocks can also have names which are used in diagnostics and as labels in plots.\nin bdsim all wires are point to point, a one-to-many connection is implemented by many wires.\nports are designated using python indexing and slicing notation, for example sum[0]. whether it is an input or output port depends on context. blocks are connected by connect(from, to_1, to_2, ...) so an index on the first argument refers to an output port, while on the second (or subsequent) arguments refers to an input port. if a port has only a single port then no index is required.\na bundle of wires can be denoted using slice notation, for example block[2:4] refers to ports 2 and 3. when connecting slices of ports the number of wires in each slice must be consistent. you could even do a cross over by connecting block1[2:4] to block2[5:2:-1].\nline 25 generates a report, in tabular form, showing all the blocks and wires in the diagram.\nblocks::\n\u250c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502id \u2502 name \u2502 nin \u2502 nout \u2502 nstate \u2502 ndstate \u2502 type \u2502\n\u251c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 0 \u2502 demand \u2502 0 \u2502 1 \u2502 0 \u2502 0 \u2502 step \u2502\n\u2502 1 \u2502 sum.0 \u2502 2 \u2502 1 \u2502 0 \u2502 0 \u2502 sum \u2502\n\u2502 2 \u2502 gain.0 \u2502 1 \u2502 1 \u2502 0 \u2502 0 \u2502 gain \u2502\n\u2502 3 \u2502 plant \u2502 1 \u2502 1 \u2502 1 \u2502 0 \u2502 lti \u2502\n\u2502 4 \u2502 scope.0 \u2502 2 \u2502 0 \u2502 0 \u2502 0 \u2502 scope \u2502\n\u2514\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nwires::\n\u250c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502id \u2502 from \u2502 to \u2502 description \u2502 type \u2502\n\u251c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 0 \u2502 0[0] \u2502 1[0] \u2502 demand[0] --> sum.0[0] \u2502 int \u2502\n\u2502 1 \u2502 0[0] \u2502 4[1] \u2502 demand[0] --> scope.0[1] \u2502 int \u2502\n\u2502 2 \u2502 3[0] \u2502 1[1] \u2502 plant[0] --> sum.0[1] \u2502 float64 \u2502\n\u2502 3 \u2502 1[0] \u2502 2[0] \u2502 sum.0[0] --> gain.0[0] \u2502 float64 \u2502\n\u2502 4 \u2502 2[0] \u2502 3[0] \u2502 gain.0[0] --> plant[0] \u2502 float64 \u2502\n\u2502 5 \u2502 3[0] \u2502 4[0] \u2502 plant[0] --> scope.0[0] \u2502 float64 \u2502\n\u2514\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nwhere nstate is the number of continuous states and ndstate is the number of discrete states.\nline 27 runs the simulation for 5 seconds using the default variable-step rk45 solver and saves output values at least every 0.1s. the scope block pops up a graph\nline 29 saves the content causes the graphs in all displayed figures to be saved in the specified format, in this case the file would be called scope.b4.pdf.\nline 28 blocks the script until all figure windows are closed, or the script is killed with sigint.\nthe result out is effectively a structure with elements\n>>> out\nresults:\nt | ndarray (67,)\nx | ndarray (67, 1)\nxnames | list\ny0 | ndarray (67,)\ny1 | ndarray (67,)\nynames | list\nwhere\nt the time vector: ndarray, shape=(m,)\nx is the state vector: ndarray, shape=(m,n), one row per timestep\nxnames is a list of the names of the states corresponding to columns of x, eg. \"plant.x0\"\nthe watch argument is a list of outputs to log, in this case plant defaults to output port 0. this information is saved in additional variables y0, y1 etc. ynames is a list of the names of the watched variables.\nline 29 saves the scope graphics as a pdf file.\nline 30 blocks until the last figure is dismissed.\na graphviz .dot file can be generated by\nbd.dotfile('demo.dot')\nwhich you can compile and display\n% dot -tpng -o demo.png demo.dot\nor neato\n% neato -tpng -o demo.png demo.dot\nwhile this is topologically correct, it's not quite the way we would expect the diagram to be drawn. dot ignores the pos options on the blocks while neato respects them, but is prone to drawing all the lines on top of each other.\nsources are shown as 3d boxes, sinks as folders, functions as boxes (apart from gains which are triangles and summing junctions which are points), and transfer functions as connectors (look's like a gate). to create a decent looking -----> plot !!!  you need to manually place the blocks using the pos argument to place them. unit spacing in the x- and y-directions is generally sufficient.\nthe sim object can do these operations in a convenient shorthand\nsim.showgraph(bd)\nand display the result via your webbrowser.\nother examples\nin the folder bdsim/examples you can find a few other runnable examples:\neg1.py the example given above\nwaveform.py two signal generators connected to two scopes\nexamples from chapter four of robotics, vision & control (2017):\nrvc4_2.py fig 4.2 - a car-like vehicle with bicycle kinematics driven by a rectangular pulse steering signal\nrvc4_4.py fig 4.4 - a car-like vehicle driving to a point\nrvc4_6.py fig 4.6 - a car-like vehicle driving to/along a line\nrvc4_8.py fig 4.8 - a car-like vehicle using pure-pursuit trajectory following\nrvc4_11.py fig 4.11 a car-like vehicle driving to a pose\nfigs 4.8 (pure pursuit) and fig 4.21 (quadrotor control) are yet to be done.\nlimitations\nthere are lots! the biggest is that bdsim is based on a very standard variable-step integrator from the scipy library. for discontinuous inputs (step, square wave, triangle wave, piecewise constant) the transitions get missed. this also makes it inaccurate to simulate hybrid discrete-continuous time systems. we really need a better integrator, perhaps odedc from scilab could be integrated.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000492, "year": null}, {"Unnamed: 0": 1565, "autor": 545, "date": null, "content": "trajectory_tracking\nThis is a ROS application to test trajectory tracking algorithms for mobile robots using Gazebo as simulator. The robot that has been used for this project is Turtlebot 2 which is an affordable mobile robot widely used for research.\nAlgorithms that have been used and are working properly so far:\nNumerical method controller using Euler's approximation.\nPID controller\nThese algorithms have been tested, and are properly working for the following trajectories:\nTrajectory Euler Controller PID Controller\nlinear \u2705 \u2705\ncircular \u2705 \u274c\nsquared \u2705 \u274c\nlemniscate \u2705 \u274c\nepitrochoid \u2705 \u274c\nlissajous \u2705 \u274c\nInstallation\nBefore cloning this repository, you should install ROS and Gazebo. It is highly recommended to use Ubuntu as OS. I also suggest installing Gazebo before ROS in order to avoid Gazebo version conflicts.\nInstalling Gazebo\nIn order to install Gazebo, you can follow the instructions provided in this tutorial, or you can just execute this command line:\n$ curl -ssL http://get.gazebosim.org | sh\nInstalling ROS\nThe ROS version used in this project is kinect. In order to install ROS, you can follow the instruction in this tutorial, or you can execute the following command lines:\n$ sudo sh -c 'echo \"deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main\" > /etc/apt/sources.list.d/ros-latest.list'\n$ sudo apt-key adv --keyserver hkp://ha.pool.sks-keyservers.net:80 --recv-key 421C365BD9FF1F717815A3895523BAEEB01FA116\n$ sudo apt-get update\n$ sudo apt-get install ros-kinetic-desktop-full\n$ sudo rosdep init\n$ rosdep update\nFinally you should source the enviroment variables to avoid doing it every time you want to use ROS:\n$ echo \"source /opt/ros/kinetic/setup.bash\" >> ~/.bashrc\n$ source ~/.bashrc\nYou could optionally install rosinstall in order to download packages easily:\n$ sudo apt-get install python-rosinstall\nInstalling Turtlebot package\nOnce ROS and Gazebo have been installed, you have to install the Turtlebot package. Run the following command line:\n$ sudo apt-get install ros-kinetic-turtlebot-gazebo\nIn order to make sure that the installation process was successful, execute the following command line, which will open a Gazebo world with some objects and a Turtlebot between them:\n$ roslaunch turtlebot_gazebo turtlebot_world.launch\nIn case you get an error when executing the previous command, just restart your computer and try again.\nCreating a workspace\nNow, it is time to create a workspace, so you can create it in your home directory by executing the following commands, in this case the workspace will be named turtlebot_ws\n$ mkdir -p ~/turtlebot_ws/src\n$ cd ~/turtlebot_ws/src/\n$ catkin_init_workspace\n$ cd ~/turtlebot_ws/\n$ catkin_make\nOnce the workspace has been created source it:\n$ source ~/turtlebot_ws/devel/setup.bash\nCloning repository\nNow it is possible to clone this repository. Because this repository is a ROS package, it should be cloned inside ~/turtlebot_ws/src/:\n$ cd ~/turtlebot_ws/src/\n$ git clone https://github.com/bit0001/trajectory_tracking.git\nAfter cloning the repo, you have to run catkin_make again:\n$ cd ~/turtlebot/\n$ catkin_make\nFinally, this package is ready to be used.\nUsage\nPlotting a trajectory\nIt is possible to visualize the trajectories that the mobile robot can follow:\n$ cd ~/turtlebot_ws/src/trajectory_tracking/src/\n$ python -m trajectory\nThis will run a console application in which one can list all available trajectories, and one can also plot a trajectory.\nRunning a simulation\nOpen a terminal window, and you have to source the workspace, and execute a launch file in order to initialize Gazebo:\n$ source ~/turtlebot_ws/devel/setup.bash\n$ roslaunch trajectory_tracking turtlebot_world.launch\nThis will open Gazebo in a world where a turtlebot is shown in the middle of a room. Now, open a new terminal, source again the workspace, and run the file trajectory_tracking twist.py:\n$ source ~/turtlebot_ws/devel/setup.bash\n$ rosrun trajectory_tracking twist.py cmd_vel:=cmd_vel_mux/input/teleop\nFinally, open again a new terminal, source the workspace and run the file control.py:\n$ source ~/turtlebot_ws/devel/setup.bash\n$ rosrun trajectory_tracking control.py <controller_name> <trajectory_name> [simulation_time]\nWhere controller_name could be either euler or pid, and trajectory_name could be either linear, circular, or squared. The simulation_time is a positive number, and if it is not given, a default time is used. Example:\n$ rosrun trajectory_tracking control.py euler squared 60.0\nThe previous command uses the the euler method controller to follow a squared trajectory during 60 seconds. Once an experiment is completed, results are plotted and shown, and when plot windows are closed, simulation data is stored in a database, which is stored in the root directory of this project, creating a table which name follows the following format: controller_trajectory_YYYY_mm_dd_HH_MM_SS.\nPlotting results of the last simulation\nOnce a simulation has been completed, it is possible to plot again the results obtained in that simulation. In order to achive this, one has to run the module plotter and pass the absolute path to the database results.db as an argument of the command-line:\n$ cd ~/turtlebot_ws/src/trajectory_tracking/src/\n$ python -m plotter /absolute/path/to/database/results.db\nPlotting results of a simulation specifying its name\nIt is also possible to plot again the results of a simulation by specifying its name:\n$ cd ~/turtlebot_ws/src/trajectory_tracking/src/\n$ python -m plotter /absolute/path/to/database/results.db simulation_name\nIn order to see the list of simulation names use the --sims flag:\n$ cd ~/turtlebot_ws/src/trajectory_tracking/src/\n$ python -m plotter /absolute/path/to/database/results.db --sims\nComparing the results of two simulations\nIt is possible to compare the obtained results in two different simulations. There is one mandatory requirement that is comparing two simulations of the same trajectory, i.e., the controller can be different.\nAlthough at first glance it seems that the time that both simulations lasted should be the same, it is possible to force a comparison. When a comparison is forced, the smallest time of the two simulations is taken, and the other simulation is limited to the smallest time.\nTo compare two simulations use:\n$ cd ~/turtlebot_ws/src/trajectory_tracking/src/\n$ python -m plotter /absolute/path/to/database/results.db sim1_name sim2_name\nTo force the plot comparison of two simulations use:\n$ cd ~/turtlebot_ws/src/trajectory_tracking/src/\n$ python -m plotter /absolute/path/to/database/results.db sim1_name sim2_name --f", "link": "https://github.com/lmiguelvargasf/trajectory_tracking", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "trajectory_tracking\nthis is a ros application to test trajectory tracking algorithms for mobile robots using gazebo as simulator. the robot that has been used for this project is turtlebot 2 which is an affordable mobile robot widely used for research.\nalgorithms that have been used and are working properly so far:\nnumerical method controller using euler's approximation.\npid controller\nthese algorithms have been tested, and are properly working for the following trajectories:\ntrajectory euler controller pid controller\nlinear \u2705 \u2705\ncircular \u2705 \u274c\nsquared \u2705 \u274c\nlemniscate \u2705 \u274c\nepitrochoid \u2705 \u274c\nlissajous \u2705 \u274c\ninstallation\nbefore cloning this repository, you should install ros and gazebo. it is highly recommended to use ubuntu as os. i also suggest installing gazebo before ros in order to avoid gazebo version conflicts.\ninstalling gazebo\nin order to install gazebo, you can follow the instructions provided in this tutorial, or you can just execute this command line:\n$ curl -ssl http://get.gazebosim.org | sh\ninstalling ros\nthe ros version used in this project is kinect. in order to install ros, you can follow the instruction in this tutorial, or you can execute the following command lines:\n$ sudo sh -c 'echo \"deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main\" > /etc/apt/sources.list.d/ros-latest.list'\n$ sudo apt-key adv --keyserver hkp://ha.pool.sks-keyservers.net:80 --recv-key 421c365bd9ff1f717815a3895523baeeb01fa116\n$ sudo apt-get update\n$ sudo apt-get install ros-kinetic-desktop-full\n$ sudo rosdep init\n$ rosdep update\nfinally you should source the enviroment variables to avoid doing it every time you want to use ros:\n$ echo \"source /opt/ros/kinetic/setup.bash\" >> ~/.bashrc\n$ source ~/.bashrc\nyou could optionally install rosinstall in order to download packages easily:\n$ sudo apt-get install python-rosinstall\ninstalling turtlebot package\nonce ros and gazebo have been installed, you have to install the turtlebot package. run the following command line:\n$ sudo apt-get install ros-kinetic-turtlebot-gazebo\nin order to make sure that the installation process was successful, execute the following command line, which will open a gazebo world with some objects and a turtlebot between them:\n$ roslaunch turtlebot_gazebo turtlebot_world.launch\nin case you get an error when executing the previous command, just restart your computer and try again.\ncreating a workspace\nnow, it is time to create a workspace, so you can create it in your home directory by executing the following commands, in this case the workspace will be named turtlebot_ws\n$ mkdir -p ~/turtlebot_ws/src\n$ cd ~/turtlebot_ws/src/\n$ catkin_init_workspace\n$ cd ~/turtlebot_ws/\n$ catkin_make\nonce the workspace has been created source it:\n$ source ~/turtlebot_ws/devel/setup.bash\ncloning repository\nnow it is possible to clone this repository. because this repository is a ros package, it should be cloned inside ~/turtlebot_ws/src/:\n$ cd ~/turtlebot_ws/src/\n$ git clone https://github.com/bit0001/trajectory_tracking.git\nafter cloning the repo, you have to run catkin_make again:\n$ cd ~/turtlebot/\n$ catkin_make\nfinally, this package is ready to be used.\nusage\nplotting a trajectory\nit is possible to visualize the trajectories that the mobile robot can follow:\n$ cd ~/turtlebot_ws/src/trajectory_tracking/src/\n$ python -m trajectory\nthis will run a console application in which one can list all available trajectories, and one can also -----> plot !!!  a trajectory.\nrunning a simulation\nopen a terminal window, and you have to source the workspace, and execute a launch file in order to initialize gazebo:\n$ source ~/turtlebot_ws/devel/setup.bash\n$ roslaunch trajectory_tracking turtlebot_world.launch\nthis will open gazebo in a world where a turtlebot is shown in the middle of a room. now, open a new terminal, source again the workspace, and run the file trajectory_tracking twist.py:\n$ source ~/turtlebot_ws/devel/setup.bash\n$ rosrun trajectory_tracking twist.py cmd_vel:=cmd_vel_mux/input/teleop\nfinally, open again a new terminal, source the workspace and run the file control.py:\n$ source ~/turtlebot_ws/devel/setup.bash\n$ rosrun trajectory_tracking control.py <controller_name> <trajectory_name> [simulation_time]\nwhere controller_name could be either euler or pid, and trajectory_name could be either linear, circular, or squared. the simulation_time is a positive number, and if it is not given, a default time is used. example:\n$ rosrun trajectory_tracking control.py euler squared 60.0\nthe previous command uses the the euler method controller to follow a squared trajectory during 60 seconds. once an experiment is completed, results are plotted and shown, and when plot windows are closed, simulation data is stored in a database, which is stored in the root directory of this project, creating a table which name follows the following format: controller_trajectory_yyyy_mm_dd_hh_mm_ss.\nplotting results of the last simulation\nonce a simulation has been completed, it is possible to plot again the results obtained in that simulation. in order to achive this, one has to run the module plotter and pass the absolute path to the database results.db as an argument of the command-line:\n$ cd ~/turtlebot_ws/src/trajectory_tracking/src/\n$ python -m plotter /absolute/path/to/database/results.db\nplotting results of a simulation specifying its name\nit is also possible to plot again the results of a simulation by specifying its name:\n$ cd ~/turtlebot_ws/src/trajectory_tracking/src/\n$ python -m plotter /absolute/path/to/database/results.db simulation_name\nin order to see the list of simulation names use the --sims flag:\n$ cd ~/turtlebot_ws/src/trajectory_tracking/src/\n$ python -m plotter /absolute/path/to/database/results.db --sims\ncomparing the results of two simulations\nit is possible to compare the obtained results in two different simulations. there is one mandatory requirement that is comparing two simulations of the same trajectory, i.e., the controller can be different.\nalthough at first glance it seems that the time that both simulations lasted should be the same, it is possible to force a comparison. when a comparison is forced, the smallest time of the two simulations is taken, and the other simulation is limited to the smallest time.\nto compare two simulations use:\n$ cd ~/turtlebot_ws/src/trajectory_tracking/src/\n$ python -m plotter /absolute/path/to/database/results.db sim1_name sim2_name\nto force the plot comparison of two simulations use:\n$ cd ~/turtlebot_ws/src/trajectory_tracking/src/\n$ python -m plotter /absolute/path/to/database/results.db sim1_name sim2_name --f", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000545, "year": null}, {"Unnamed: 0": 1652, "autor": 632, "date": null, "content": "Quarl: Quantization For Reinforcement Learning\nCode for QuaRL, a framework for evaluating the effects of quantization on reinforcement learning policies across different environments, training algorithms and quantization methods and ActorQ a quantized distributed RL training setup exhibiting speedups of upto 2.5x.\nTable of Contents\nIntroduction\nQuickstart\nResults\nCitations\nIntroduction\nDeep reinforcement learning has achieved significant milestones, however, the computational demands of reinforcement learning training and inference remain substantial. Using quantization techniques such as Post Training Quantization and Quantization Aware Training, a well-known technique in reducing computation costs, we perform a systematic study of Reinforcement Learning Algorithms such as A2C, DDPG, DQN, PPO and D4PG on common environments.\nMotivated by the effectiveness of PTQ, we propose ActorQ, a quantized actor-learner distributed training system that runs learners in full precision and actors in quantized precision (fp16, int8). We demonstrated end-to-end speedups of 1.5x - 2.5x in reinforcement learning training with no loss in reward. Further, we breakdown the various runtime costs in distributed reinforcement learning training and show the effects of quantization on each.\nThe framework currently support the following environments, RL algorithms and quantization methods.\nEnvironments\nAtari Learning Environment (through OpenAI Gym)\nOpenAI Gym\nPyBullet\nMujoco\nDeepmind Control Suite\nRL Algorithms\nProximal Policy Optimization (PPO)\nActor Critic (A2C)\nDeep Deterministic Policy Gradients (DDPG)\nDQN (Deep Q Networks)\nD4PG (Distributed Distributional Deep Deterministic Gradients)\nQuantization Methods\nPost-training Quantization (Located in baseline)\nQuantization Aware Training (Located in baseline)\nActorQ (for distributed RL) (Located in actorQ)\nRead the paper here for more information: https://arxiv.org/abs/1910.01055\nQuickstart\nWe suggest that you create an environment using conda first\nconda create --name quarl python=3.6\nconda activate quarl\nFor ubuntu:\n./setup_ubuntu.sh\ncd baseline\nFor MacOS:\n./setup_mac.sh\ncd baseline\nBaseline\n8-bit Post-training Quantization:\nThe PTQ algorithms require pre-trained models through rl-baselines-zoo.\npython new_ptq.py ppo2 MountainCarContinuous-v0 8\npython rl-baselines-zoo/enjoy.py --algo ppo2 --env MountainCarContinuous-v0 --no-render --folder quantized/8/ -n 50000\nNote: PTQ can be run on any numerical precision between 2-32\nfp16 Post-training Quantization:\npython new_ptq.py ppo2 MountainCarContinuous-v0 fp16\npython rl-baselines-zoo/enjoy.py --algo ppo2 --env MountainCarContinuous-v0 --no-render --folder quantized/8/ -n 50000\nBenchmark a particular PTQ over an Algorithm, Environment\n# Format ./benchmark.sh {algo} {env} {precision}\n./benchmark.sh ppo2 MountainCarContinuous-v0\n# Benchmark A2C MsPacman\n./benchmark.sh a2c MsPacmanNoFrameskip-v4\n# Create Sweetspot PNG\npython collate.py a2c MsPacmanNoFrameskip-v4\nBenchmark all algorithms over all environments and collect PNGs for all\n./all.sh\n./create_all_pngs.sh\n8-bit Quantization Aware Training and testing:\nQAT usually requires training a model from scratch. We suggest setting quant-delay as half the total number of training steps. The official TF guidelines suggest finetuning min, max quantization ranges after the model has fully converged but in the case of RL over-training usually results in bad performance. QAT results also vary a lot depending on training so exact rewards as mentioned in the paper are not always guaranteed.\npython qat.py --algo a2c --env BreakoutNoFrameskip-v4 -q 7 --quant-delay 5000000 -n 10000000\nActorQ\nSince our ActorQ implementation requires TF 2.0, we suggest creating a new environment.\nconda create --name actorq python=3.6\nconda activate actorq\ncd actorQ/d4pg\nSetup\nActorQ is dependent on Reverb for creating a parameter server, so ActorQ is only supported on Ubuntu at the moment. A setup script is located in the actorQ directory which assumes a working installation of Mujoco. The rough steps are also desribed below:\nInstall Mujoco\nInstall Deepmind ACME, Reverb\nInstall zlib\nRunning ActorQ\nActorQ currently supports two algorithms, Distributed Distributional Deep Deterministic Gradients (D4PG) and Deep Q Networks (DQN). Codes for them can be found in the directories actorQ/d4pg and actorQ/dqn.\nThere are three main processes that need to be run for ActorQ:\nLearner\nParameter Server\nActors\nA basic example of ActorQ:\nn_actors=4\n# Launch the Learner:\npython learner_main.py --actor_update_period 300 \\\n--taskstr gym,MountainCarContinuous-v0 --model_str 2048,2048,2048\\\n--num_episodes 100000 --quantize_communication 32 --quantize 8\\\n--replay_table_max_times_sampled 16 --batch_size 256 \\\n--n_actors 4 --logpath logs/logfiles/ \\\n--weight_compress 0 > logs/logs_stdout/learner_out\nmaster_pid = $!\n# Launch the Parameter Server:\npython parameter_broadcaster.py --actor_update_period 300\\\n--taskstr gym,MountainCarContinuous-v0 --model_str 2048,2048,2048\\\n--num_episodes 100000 --quantize_communication 32 \\\n--quantize 8 --replay_table_max_times_sampled 16 \\\n--batch_size 256 --n_actors 4 --logpath logs/logfiles/ \\\n--weight_compress 0 > logs/logs_stdout/broadcaster_out\n# Launch the actors:\nfor i in `seq 1 $n_actors`; do\npython actor_main.py --actor_update_period 300 \\\n--taskstr gym,MountainCarContinuous-v0 --model_str 2048,2048,2048 \\\n--num_episodes 100000 --n_actors 4 --quantize_communication 32 \\\n--quantize 8 --replay_table_max_times_sampled 16 --batch_size 256 --actor_id $i \\\n--logpath logs/logfiles/ --weight_compress 0 > logs/logs_stdout/actorid=${i}_out &\necho $!\ndone\n# Wait\nwait master_pid\nAn example with all hyperparameter settings of running ActorQ with 5 actors:\ncd actorQ/d4pg/\n./run_multiprocess.sh logs MountainCarContinuous-v0 2048,2048,2048 100000 4 256 16 32 32 30 0\n# Arguments\n# 1: Logdir: \"{path}\"\n# E.g. logs\n# 2: Task string: \"f{suite},{env}\"\n# E.g. gym,MountainCarContinuous-v0\n# dm_control,cartpole_balance\n# 3: Model Architecture: \"{layer 1},{layer2}, ...}\"\n# E.g. 2048,2048,2048\n# 4: Number of Episodes: int\n# E.g. 100000\n# 5: Number of Actors: int\n# E.g. 4\n# 6: Batch size: int\n# E.g. 256\n# 7: Samples per Insert: int\n# E.g. 16\n# 8: Quantized Actor Precision: int\n# E.g. 8\n# 9: Quantized Communication Precision: int\n# E.g. 32\n# 10: Actor Update Period: int\n# E.g. 20\n# 11: Weight Compress: Bool in int\n# E.g. 0 or 1\nVisualization\nVisualizing the model's parameter (weight & bias) distribution. These scripts can be found in baseline directory.\nIf the saved model is in '.pb' format, please run\npython visualize_pb.py -f <folder> -b <num_bits>\nor: python visualize_pb.py --folder=<folder> --num_bits=<num_bits>\nIf the saved model is in '.pkl' format, please run\npython visualize_pkl.py -f <folder> -b <num_bits>\nor: python visualize_pkl.py --folder=<folder> --num_bits=<num_bits>\nThe parameter distribution plot will be saved under <folder>, and the detailed statistical information will be saved in output.txt under <folder>.\nFor example, here is an example of visualizing the weights distribution for breakout envionment trained using DQN, PPO, and A2C:\nResults\nActorQ end-to-end speedups\nDistributed RL Training Breakdown\nFor more results, please check our paper.\nCitations\nTo cite this repository in publications:\n@misc{lam2021quantized,\ntitle={Quantized Reinforcement Learning (QUARL)},\nauthor={Maximilian Lam and Sharad Chitlangia and Srivatsan Krishnan and Zishen Wan and Gabriel Barth-Maron and Aleksandra Faust and Vijay Janapa Reddi},\nyear={2021},\neprint={1910.01055},\narchivePrefix={arXiv},\nprimaryClass={cs.LG}\n}", "link": "https://github.com/harvard-edge/QuaRL", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "quarl: quantization for reinforcement learning\ncode for quarl, a framework for evaluating the effects of quantization on reinforcement learning policies across different environments, training algorithms and quantization methods and actorq a quantized distributed rl training setup exhibiting speedups of upto 2.5x.\ntable of contents\nintroduction\nquickstart\nresults\ncitations\nintroduction\ndeep reinforcement learning has achieved significant milestones, however, the computational demands of reinforcement learning training and inference remain substantial. using quantization techniques such as post training quantization and quantization aware training, a well-known technique in reducing computation costs, we perform a systematic study of reinforcement learning algorithms such as a2c, ddpg, dqn, ppo and d4pg on common environments.\nmotivated by the effectiveness of ptq, we propose actorq, a quantized actor-learner distributed training system that runs learners in full precision and actors in quantized precision (fp16, int8). we demonstrated end-to-end speedups of 1.5x - 2.5x in reinforcement learning training with no loss in reward. further, we breakdown the various runtime costs in distributed reinforcement learning training and show the effects of quantization on each.\nthe framework currently support the following environments, rl algorithms and quantization methods.\nenvironments\natari learning environment (through openai gym)\nopenai gym\npybullet\nmujoco\ndeepmind control suite\nrl algorithms\nproximal policy optimization (ppo)\nactor critic (a2c)\ndeep deterministic policy gradients (ddpg)\ndqn (deep q networks)\nd4pg (distributed distributional deep deterministic gradients)\nquantization methods\npost-training quantization (located in baseline)\nquantization aware training (located in baseline)\nactorq (for distributed rl) (located in actorq)\nread the paper here for more information: https://arxiv.org/abs/1910.01055\nquickstart\nwe suggest that you create an environment using conda first\nconda create --name quarl python=3.6\nconda activate quarl\nfor ubuntu:\n./setup_ubuntu.sh\ncd baseline\nfor macos:\n./setup_mac.sh\ncd baseline\nbaseline\n8-bit post-training quantization:\nthe ptq algorithms require pre-trained models through rl-baselines-zoo.\npython new_ptq.py ppo2 mountaincarcontinuous-v0 8\npython rl-baselines-zoo/enjoy.py --algo ppo2 --env mountaincarcontinuous-v0 --no-render --folder quantized/8/ -n 50000\nnote: ptq can be run on any numerical precision between 2-32\nfp16 post-training quantization:\npython new_ptq.py ppo2 mountaincarcontinuous-v0 fp16\npython rl-baselines-zoo/enjoy.py --algo ppo2 --env mountaincarcontinuous-v0 --no-render --folder quantized/8/ -n 50000\nbenchmark a particular ptq over an algorithm, environment\n# format ./benchmark.sh {algo} {env} {precision}\n./benchmark.sh ppo2 mountaincarcontinuous-v0\n# benchmark a2c mspacman\n./benchmark.sh a2c mspacmannoframeskip-v4\n# create sweetspot png\npython collate.py a2c mspacmannoframeskip-v4\nbenchmark all algorithms over all environments and collect pngs for all\n./all.sh\n./create_all_pngs.sh\n8-bit quantization aware training and testing:\nqat usually requires training a model from scratch. we suggest setting quant-delay as half the total number of training steps. the official tf guidelines suggest finetuning min, max quantization ranges after the model has fully converged but in the case of rl over-training usually results in bad performance. qat results also vary a lot depending on training so exact rewards as mentioned in the paper are not always guaranteed.\npython qat.py --algo a2c --env breakoutnoframeskip-v4 -q 7 --quant-delay 5000000 -n 10000000\nactorq\nsince our actorq implementation requires tf 2.0, we suggest creating a new environment.\nconda create --name actorq python=3.6\nconda activate actorq\ncd actorq/d4pg\nsetup\nactorq is dependent on reverb for creating a parameter server, so actorq is only supported on ubuntu at the moment. a setup script is located in the actorq directory which assumes a working installation of mujoco. the rough steps are also desribed below:\ninstall mujoco\ninstall deepmind acme, reverb\ninstall zlib\nrunning actorq\nactorq currently supports two algorithms, distributed distributional deep deterministic gradients (d4pg) and deep q networks (dqn). codes for them can be found in the directories actorq/d4pg and actorq/dqn.\nthere are three main processes that need to be run for actorq:\nlearner\nparameter server\nactors\na basic example of actorq:\nn_actors=4\n# launch the learner:\npython learner_main.py --actor_update_period 300 \\\n--taskstr gym,mountaincarcontinuous-v0 --model_str 2048,2048,2048\\\n--num_episodes 100000 --quantize_communication 32 --quantize 8\\\n--replay_table_max_times_sampled 16 --batch_size 256 \\\n--n_actors 4 --logpath logs/logfiles/ \\\n--weight_compress 0 > logs/logs_stdout/learner_out\nmaster_pid = $!\n# launch the parameter server:\npython parameter_broadcaster.py --actor_update_period 300\\\n--taskstr gym,mountaincarcontinuous-v0 --model_str 2048,2048,2048\\\n--num_episodes 100000 --quantize_communication 32 \\\n--quantize 8 --replay_table_max_times_sampled 16 \\\n--batch_size 256 --n_actors 4 --logpath logs/logfiles/ \\\n--weight_compress 0 > logs/logs_stdout/broadcaster_out\n# launch the actors:\nfor i in `seq 1 $n_actors`; do\npython actor_main.py --actor_update_period 300 \\\n--taskstr gym,mountaincarcontinuous-v0 --model_str 2048,2048,2048 \\\n--num_episodes 100000 --n_actors 4 --quantize_communication 32 \\\n--quantize 8 --replay_table_max_times_sampled 16 --batch_size 256 --actor_id $i \\\n--logpath logs/logfiles/ --weight_compress 0 > logs/logs_stdout/actorid=${i}_out &\necho $!\ndone\n# wait\nwait master_pid\nan example with all hyperparameter settings of running actorq with 5 actors:\ncd actorq/d4pg/\n./run_multiprocess.sh logs mountaincarcontinuous-v0 2048,2048,2048 100000 4 256 16 32 32 30 0\n# arguments\n# 1: logdir: \"{path}\"\n# e.g. logs\n# 2: task string: \"f{suite},{env}\"\n# e.g. gym,mountaincarcontinuous-v0\n# dm_control,cartpole_balance\n# 3: model architecture: \"{layer 1},{layer2}, ...}\"\n# e.g. 2048,2048,2048\n# 4: number of episodes: int\n# e.g. 100000\n# 5: number of actors: int\n# e.g. 4\n# 6: batch size: int\n# e.g. 256\n# 7: samples per insert: int\n# e.g. 16\n# 8: quantized actor precision: int\n# e.g. 8\n# 9: quantized communication precision: int\n# e.g. 32\n# 10: actor update period: int\n# e.g. 20\n# 11: weight compress: bool in int\n# e.g. 0 or 1\nvisualization\nvisualizing the model's parameter (weight & bias) distribution. these scripts can be found in baseline directory.\nif the saved model is in '.pb' format, please run\npython visualize_pb.py -f <folder> -b <num_bits>\nor: python visualize_pb.py --folder=<folder> --num_bits=<num_bits>\nif the saved model is in '.pkl' format, please run\npython visualize_pkl.py -f <folder> -b <num_bits>\nor: python visualize_pkl.py --folder=<folder> --num_bits=<num_bits>\nthe parameter distribution -----> plot !!!  will be saved under <folder>, and the detailed statistical information will be saved in output.txt under <folder>.\nfor example, here is an example of visualizing the weights distribution for breakout envionment trained using dqn, ppo, and a2c:\nresults\nactorq end-to-end speedups\ndistributed rl training breakdown\nfor more results, please check our paper.\ncitations\nto cite this repository in publications:\n@misc{lam2021quantized,\ntitle={quantized reinforcement learning (quarl)},\nauthor={maximilian lam and sharad chitlangia and srivatsan krishnan and zishen wan and gabriel barth-maron and aleksandra faust and vijay janapa reddi},\nyear={2021},\neprint={1910.01055},\narchiveprefix={arxiv},\nprimaryclass={cs.lg}\n}", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000632, "year": null}, {"Unnamed: 0": 1674, "autor": 654, "date": null, "content": "Kalman Filter in Python\nThis is a basic example of how Kalman filter works in Python. I do plan on refactoring and expanding this repo in the future.\nA great series on Kalman Filters which I have been following can be found here. The example I'm using can also be found in the same video.\nSimply run:\npython kalman.py\nto get started. A plot should be generated with sensor and predicted values. The true value (assumed to be unknown) is 72.", "link": "https://github.com/enochkan/kalmanpy", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "kalman filter in python\nthis is a basic example of how kalman filter works in python. i do plan on refactoring and expanding this repo in the future.\na great series on kalman filters which i have been following can be found here. the example i'm using can also be found in the same video.\nsimply run:\npython kalman.py\nto get started. a -----> plot !!!  should be generated with sensor and predicted values. the true value (assumed to be unknown) is 72.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000654, "year": null}, {"Unnamed: 0": 1694, "autor": 674, "date": null, "content": "LQR-RRT*\nLQR heuristic as an extension to sample based motion planning algorithms, such as RRT or RRT*, can be a relatively low-cost distance metric and find optimal plans in domains with complex or underactuated dynamics. Below, is the phase plot propagation of the simple pendulum:\nMore on this at: http://people.csail.mit.edu/tlp/pdf/2012/ICRA12_1657_FI.pdf", "link": "https://github.com/MahanFathi/LQR-RRTstar", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "lqr-rrt*\nlqr heuristic as an extension to sample based motion planning algorithms, such as rrt or rrt*, can be a relatively low-cost distance metric and find optimal plans in domains with complex or underactuated dynamics. below, is the phase -----> plot !!!  propagation of the simple pendulum:\nmore on this at: http://people.csail.mit.edu/tlp/pdf/2012/icra12_1657_fi.pdf", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000674, "year": null}, {"Unnamed: 0": 1733, "autor": 713, "date": null, "content": "WolfieMouse\nWolfieMouse is a robotics project to build a micromouse. This project covers three parts:\nRobotics algorithm for maze solving (C++)\nlow-level hardware driver software (C and ARM assembly)\nTools to capture and plot robot's sensor data (Python)\nEmbedded hardware design and PCB design (KiCad, an open-source alternative to Altium)\nThis robot won:\nThe Special Mention Award in 2018 IEEE Region 1 Micromouse Robotics Competition.\nThe 3rd Place Award in 2019 IEEE Region 1 Micromouse Robotics Competition.\nGet started\nIf you don't know what is micromouse competition, this document will help..\nRequires tools:\nGNU Arm Embedded Toolchain and Makefile to build program for the robot.\nOpenOCD to upload the program to the robot and to debug.\nKiCad to design a PCB hardware.\nOtherwise, a Vagrant virtual machine environment is provided to skip installing the above tools.\nIf you whish to start with this project, see get stared document.\nDocumentation\nSee documentation section.\nDirectory descriptions\ndoc: Documentation folder\nfirmware: The robot's program folder, including robotics algorithms and hardware drivers\nsimulation: Programs to test algorithms on a desktop computer\ntools: Contains robot sensor data capturing tools and scripts for Vagrant machine.\nGallery\nTerminal-based simulation program. M stands for the position of the robot, D stands for the position of the goal in the maze, and S stands for the starting position.\nSchematic overview.\nPCB Footprint overview.\nLicense and Credits\nSource code in firmware and simulation folders are licensed under GPLv2.1.\nExternal libraries (FreeRTOS, CMSIS, STM32F4 HAL) under firmware/lib follow their own terms.\nThe fundamental hardware design is inspired by Project Futura by Green Ye.", "link": "https://github.com/kbumsik/WolfieMouse", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "wolfiemouse\nwolfiemouse is a robotics project to build a micromouse. this project covers three parts:\nrobotics algorithm for maze solving (c++)\nlow-level hardware driver software (c and arm assembly)\ntools to capture and -----> plot !!!  robot's sensor data (python)\nembedded hardware design and pcb design (kicad, an open-source alternative to altium)\nthis robot won:\nthe special mention award in 2018 ieee region 1 micromouse robotics competition.\nthe 3rd place award in 2019 ieee region 1 micromouse robotics competition.\nget started\nif you don't know what is micromouse competition, this document will help..\nrequires tools:\ngnu arm embedded toolchain and makefile to build program for the robot.\nopenocd to upload the program to the robot and to debug.\nkicad to design a pcb hardware.\notherwise, a vagrant virtual machine environment is provided to skip installing the above tools.\nif you whish to start with this project, see get stared document.\ndocumentation\nsee documentation section.\ndirectory descriptions\ndoc: documentation folder\nfirmware: the robot's program folder, including robotics algorithms and hardware drivers\nsimulation: programs to test algorithms on a desktop computer\ntools: contains robot sensor data capturing tools and scripts for vagrant machine.\ngallery\nterminal-based simulation program. m stands for the position of the robot, d stands for the position of the goal in the maze, and s stands for the starting position.\nschematic overview.\npcb footprint overview.\nlicense and credits\nsource code in firmware and simulation folders are licensed under gplv2.1.\nexternal libraries (freertos, cmsis, stm32f4 hal) under firmware/lib follow their own terms.\nthe fundamental hardware design is inspired by project futura by green ye.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000713, "year": null}, {"Unnamed: 0": 1744, "autor": 724, "date": null, "content": "Robotlib\nThis is a library of functions to help out in a robotics lab. At present stage, it contains functions for forward kinematics, jacobians, iterative inverse kinematics and for a few robotics related calibration problems. The library also contains a number of functions to convert from various orientation representations and other robotics related helper functions.\nInstall using\nusing Pkg; Pkg.add(\"Robotlib\")\nUsage\nfkine, ikine, jacobian = get_kinematic_functions(\"yumi\") # Replace yumi for your robot model, as long as it's supported\ndata = csv2dict(path) # Read data from a csv-file and store in a dict\nq = getdata(\"robot_0.*posRawAbs\", data, 1, removeNaN = false) # Extract columns from data object using regex like syntax\nFor ABB YuMi, joint angles q must be converted to logical order using e.g. abb2logical!(q) If you use the kinematic functions privided by get_kinematic_functions, the base transform is handled automatically. If you use the standard kinematic functions provided in Robotlib, you must also consider the base transform.\nCase study, calibrate force sensor (or accelerometer)\nusing Robotlib\nusing DSP # For filtfilt\n# Define robot to use, in this case YuMi\ndh = DHYuMi()\nfkine, ikine, jacobian = get_kinematic_functions(\"robotname\")\nq,q\u0307,\u03c4 = load_your_data()\n# Apply gear ratio transformation\nq = q*dh.GR'\nq\u0307 = q\u0307*dh.GR'\n\u03c4 = \u03c4*inv(dh.GR')\n# Filter velocities to get accelerations\nq\u0308 = filtfilt(ones(50),[50.],centralDiff(q\u0307))\n# plot(abs([q\u0307, q\u0308]))\n# Sort out data with low acceleration\nlowAcc = all(abs.(q\u0308) .< 3e-4,2)\nq = q[lowAcc,:]\nq\u0307 = q\u0307[lowAcc,:]\n\u03c4 = \u03c4[lowAcc,:]\nf = f[lowAcc,:]\nN = size(q,1)\n# Apply forward kinematics to get end-effector poses\nT = cat([fkine(q[i,:]) for i = 1:N]..., dims=3)\ntrajplot(T) # Plots a trajectory of R4x4 transformation matrices\n# Perform the force sensor calibration and plot the errors\nRf,m,offset = calib_force(T,f,0.2205,offset=true) # See also calib_force_iterative, calib_force_eigen\nerr = hcat([Rf*f[i,1:3] + offset - T[1:3,1:3,i]'*[0, 0, m*-9.82] for i = 1:N]...)'\nplot(f[:,1:3],lab=\"Force\")\nplot!(err,l=:dash,lab=\"Error\")\nprintln(\"Error: \", round(rms(err), digits=4))\nExported functions\nSee\nnames(Robotlib)\nnames(Robotlib.Frames)\nThe submodule Robotlib.Frames supports creation of frames, simple projections, fitting of planes, lines etc. and has a number of plotting options. It must be separately imported with using Robotlib.Frames.\nKinematics\nThe library has functions for calculation of forward kinematics, inverse kinematics and jacobians. Several versions of all kinematics functions are provided; calculations can be made using either the DH-convention or the (local) product of exponentials formulation. To support a new robot, create an object of the type DH, or provide a matrix with POE-style link twists, for use with the kinematic functions.\nUsage\ndh = DH7600() # ABB Irb 7600\nxi = DH2twistsPOE(dh)\nT = fkinePOE(xi, q)\nor alternatively\ndh = DH7600()\nJn, J0, T, Ti, trans = jacobian(q, dh)\nmany other options exits, check kinematics.jl\nFrames\nThis module is aimed at assisting with the creation of frames for tracking using optical tracking systems. It supports projection of points and lines onto planes, creating frames from features and has some plotting functionality.\nUsage\nThis is an example of how data can be loaded from files and how different geometrical objects can be fitted to data, projected onto other objects etc.\nusing Frames\nimport MAT\nfunction setupframes(path)\npath = Pkg.dir(\"Robotlib\",\"src\",\"applications\",\"frames/\")\n# Add frame names to the dictionary\nadd_frame_name!(\"SEAM\",\"Weld seam frame\")\nadd_frame_name!(\"TAB\",\"Table frame\")\n# Read matrices from file\nT_RB_Tm = MAT.matread(path*\"T_RB_T.mat\")[\"T_RB_T\"]\nT_TF_TCPm = MAT.matread(path*\"T_TF_TCP.mat\")[\"T_TF_TCP\"]\nT_T_TABm = MAT.matread(path*\"T_T_Table.mat\")[\"T_T_Table\"]\n# Create frames from matrices\nT_RB_T = Frame(T_RB_Tm,\"RB\",\"T\")\nT_S_D = Frame(T_TF_TCPm,\"S\",\"D\")\nT_T_TAB = Frame(T_T_TABm,\"T\",\"TAB\")\n# Read point clouds generated by nikon software from file\ncloud_seam = readcloud(path*\"CloudSeam_edge.txt\")\nplane_seam = readplane(path*\"PlaneSeam_edge.txt\")\n# Project points onto plane and fit a line\ncloud_seam_projected = project(plane_seam,cloud_seam)\nline_seam = fitline(cloud_seam_projected)\n# Create a frame from the measured features\nT_T_SEAM = framefromfeatures((\"z+\",line_seam),(\"y-\",plane_seam),cloud_seam_projected[1],\"SEAM\")\nT_RB_SEAM = T_RB_T*T_T_SEAM\nT_RB_TAB = T_RB_T*T_T_TAB\nT_TAB_SEAM = inv(T_T_TAB)*T_T_SEAM\ncloud_seam_RB = T_RB_T*cloud_seam\ncloud_seam_projected_RB = T_RB_T*cloud_seam_projected\nplane_seam_RB = T_RB_T*plane_seam\nline_seam_RB = T_RB_T*line_seam\n# Plot results\nplot(Frame(I4,\"RB\",\"U\"), 200)\nplot!(cloud_seam_RB, c=:blue)\nplot!(cloud_seam_projected_RB, c=:red)\nplot!(line_seam_RB, 500, label=\"Line seam\")\nplot!(plane_seam_RB, 200, label=\"Plane seam\")\nplot!(T_RB_SEAM, 200, label=\"T_RB_SEAM\")\nplot!(T_RB_TAB, 200, label=\"T_RB_TAB\")\nxlabel!(\"x\")\nylabel!(\"y\")\n# zlabel!(\"z\")\n# Write frames to file\nMAT.matwrite(path*\"T_TAB_SEAM.mat\",[\"T_TAB_SEAM\" => T_TAB_SEAM.T])\nMAT.matwrite(path*\"T_T_SEAM.mat\",[\"T_T_SEAM\" => T_T_SEAM.T])\nMAT.matwrite(path*\"T_RB_TAB.mat\",[\"T_RB_TAB\" => T_RB_TAB.T])\nprintln(\"Wrote T_TAB_SEAM, T_T_SEAM, T_RB_TAB to files in $path\")\nend\nCiting\nThis package was developed for the thesis Bagge Carlson, F., \"Machine Learning and System Identification for Estimation in Physical Systems\" (PhD Thesis 2018).\n@thesis{bagge2018,\ntitle = {Machine Learning and System Identification for Estimation in Physical Systems},\nauthor = {Bagge Carlson, Fredrik},\nkeyword = {Machine Learning,System Identification,Robotics,Spectral estimation,Calibration,State estimation},\nmonth = {12},\ntype = {PhD Thesis},\nnumber = {TFRT-1122},\ninstitution = {Dept. Automatic Control, Lund University, Sweden},\nyear = {2018},\nurl = {https://lup.lub.lu.se/search/publication/ffb8dc85-ce12-4f75-8f2b-0881e492f6c0},\n}\nThe algorithm calibNAXP was presented in\n@inproceedings{bagge2015calibration,\ntitle = {Six {DOF} eye-to-hand calibration from {2D} measurements using planar constraints},\nauthor = {Bagge Carlson, Fredrik and Johansson, Rolf and Robertsson, Anders},\nbooktitle = {International Conference on Intelligent Robots and Systems (IROS)},\nyear = {2015},\norganization = {IEEE}\n}\nThe friction model frictionRBFN was presented in\n@inproceedings{bagge2015friction,\ntitle = {Modeling and identification of position and temperature dependent friction phenomena without temperature sensing},\nauthor = {Bagge Carlson, Fredrik and Robertsson, Anders and Johansson, Rolf},\nbooktitle = {International Conference on Intelligent Robots and Systems (IROS)},\nyear = {2015},\norganization = {IEEE}\n}", "link": "https://github.com/baggepinnen/Robotlib.jl", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "robotlib\nthis is a library of functions to help out in a robotics lab. at present stage, it contains functions for forward kinematics, jacobians, iterative inverse kinematics and for a few robotics related calibration problems. the library also contains a number of functions to convert from various orientation representations and other robotics related helper functions.\ninstall using\nusing pkg; pkg.add(\"robotlib\")\nusage\nfkine, ikine, jacobian = get_kinematic_functions(\"yumi\") # replace yumi for your robot model, as long as it's supported\ndata = csv2dict(path) # read data from a csv-file and store in a dict\nq = getdata(\"robot_0.*posrawabs\", data, 1, removenan = false) # extract columns from data object using regex like syntax\nfor abb yumi, joint angles q must be converted to logical order using e.g. abb2logical!(q) if you use the kinematic functions privided by get_kinematic_functions, the base transform is handled automatically. if you use the standard kinematic functions provided in robotlib, you must also consider the base transform.\ncase study, calibrate force sensor (or accelerometer)\nusing robotlib\nusing dsp # for filtfilt\n# define robot to use, in this case yumi\ndh = dhyumi()\nfkine, ikine, jacobian = get_kinematic_functions(\"robotname\")\nq,q\u0307,\u03c4 = load_your_data()\n# apply gear ratio transformation\nq = q*dh.gr'\nq\u0307 = q\u0307*dh.gr'\n\u03c4 = \u03c4*inv(dh.gr')\n# filter velocities to get accelerations\nq\u0308 = filtfilt(ones(50),[50.],centraldiff(q\u0307))\n# plot(abs([q\u0307, q\u0308]))\n# sort out data with low acceleration\nlowacc = all(abs.(q\u0308) .< 3e-4,2)\nq = q[lowacc,:]\nq\u0307 = q\u0307[lowacc,:]\n\u03c4 = \u03c4[lowacc,:]\nf = f[lowacc,:]\nn = size(q,1)\n# apply forward kinematics to get end-effector poses\nt = cat([fkine(q[i,:]) for i = 1:n]..., dims=3)\ntrajplot(t) # plots a trajectory of r4x4 transformation matrices\n# perform the force sensor calibration and -----> plot !!!  the errors\nrf,m,offset = calib_force(t,f,0.2205,offset=true) # see also calib_force_iterative, calib_force_eigen\nerr = hcat([rf*f[i,1:3] + offset - t[1:3,1:3,i]'*[0, 0, m*-9.82] for i = 1:n]...)'\nplot(f[:,1:3],lab=\"force\")\nplot!(err,l=:dash,lab=\"error\")\nprintln(\"error: \", round(rms(err), digits=4))\nexported functions\nsee\nnames(robotlib)\nnames(robotlib.frames)\nthe submodule robotlib.frames supports creation of frames, simple projections, fitting of planes, lines etc. and has a number of plotting options. it must be separately imported with using robotlib.frames.\nkinematics\nthe library has functions for calculation of forward kinematics, inverse kinematics and jacobians. several versions of all kinematics functions are provided; calculations can be made using either the dh-convention or the (local) product of exponentials formulation. to support a new robot, create an object of the type dh, or provide a matrix with poe-style link twists, for use with the kinematic functions.\nusage\ndh = dh7600() # abb irb 7600\nxi = dh2twistspoe(dh)\nt = fkinepoe(xi, q)\nor alternatively\ndh = dh7600()\njn, j0, t, ti, trans = jacobian(q, dh)\nmany other options exits, check kinematics.jl\nframes\nthis module is aimed at assisting with the creation of frames for tracking using optical tracking systems. it supports projection of points and lines onto planes, creating frames from features and has some plotting functionality.\nusage\nthis is an example of how data can be loaded from files and how different geometrical objects can be fitted to data, projected onto other objects etc.\nusing frames\nimport mat\nfunction setupframes(path)\npath = pkg.dir(\"robotlib\",\"src\",\"applications\",\"frames/\")\n# add frame names to the dictionary\nadd_frame_name!(\"seam\",\"weld seam frame\")\nadd_frame_name!(\"tab\",\"table frame\")\n# read matrices from file\nt_rb_tm = mat.matread(path*\"t_rb_t.mat\")[\"t_rb_t\"]\nt_tf_tcpm = mat.matread(path*\"t_tf_tcp.mat\")[\"t_tf_tcp\"]\nt_t_tabm = mat.matread(path*\"t_t_table.mat\")[\"t_t_table\"]\n# create frames from matrices\nt_rb_t = frame(t_rb_tm,\"rb\",\"t\")\nt_s_d = frame(t_tf_tcpm,\"s\",\"d\")\nt_t_tab = frame(t_t_tabm,\"t\",\"tab\")\n# read point clouds generated by nikon software from file\ncloud_seam = readcloud(path*\"cloudseam_edge.txt\")\nplane_seam = readplane(path*\"planeseam_edge.txt\")\n# project points onto plane and fit a line\ncloud_seam_projected = project(plane_seam,cloud_seam)\nline_seam = fitline(cloud_seam_projected)\n# create a frame from the measured features\nt_t_seam = framefromfeatures((\"z+\",line_seam),(\"y-\",plane_seam),cloud_seam_projected[1],\"seam\")\nt_rb_seam = t_rb_t*t_t_seam\nt_rb_tab = t_rb_t*t_t_tab\nt_tab_seam = inv(t_t_tab)*t_t_seam\ncloud_seam_rb = t_rb_t*cloud_seam\ncloud_seam_projected_rb = t_rb_t*cloud_seam_projected\nplane_seam_rb = t_rb_t*plane_seam\nline_seam_rb = t_rb_t*line_seam\n# plot results\nplot(frame(i4,\"rb\",\"u\"), 200)\nplot!(cloud_seam_rb, c=:blue)\nplot!(cloud_seam_projected_rb, c=:red)\nplot!(line_seam_rb, 500, label=\"line seam\")\nplot!(plane_seam_rb, 200, label=\"plane seam\")\nplot!(t_rb_seam, 200, label=\"t_rb_seam\")\nplot!(t_rb_tab, 200, label=\"t_rb_tab\")\nxlabel!(\"x\")\nylabel!(\"y\")\n# zlabel!(\"z\")\n# write frames to file\nmat.matwrite(path*\"t_tab_seam.mat\",[\"t_tab_seam\" => t_tab_seam.t])\nmat.matwrite(path*\"t_t_seam.mat\",[\"t_t_seam\" => t_t_seam.t])\nmat.matwrite(path*\"t_rb_tab.mat\",[\"t_rb_tab\" => t_rb_tab.t])\nprintln(\"wrote t_tab_seam, t_t_seam, t_rb_tab to files in $path\")\nend\nciting\nthis package was developed for the thesis bagge carlson, f., \"machine learning and system identification for estimation in physical systems\" (phd thesis 2018).\n@thesis{bagge2018,\ntitle = {machine learning and system identification for estimation in physical systems},\nauthor = {bagge carlson, fredrik},\nkeyword = {machine learning,system identification,robotics,spectral estimation,calibration,state estimation},\nmonth = {12},\ntype = {phd thesis},\nnumber = {tfrt-1122},\ninstitution = {dept. automatic control, lund university, sweden},\nyear = {2018},\nurl = {https://lup.lub.lu.se/search/publication/ffb8dc85-ce12-4f75-8f2b-0881e492f6c0},\n}\nthe algorithm calibnaxp was presented in\n@inproceedings{bagge2015calibration,\ntitle = {six {dof} eye-to-hand calibration from {2d} measurements using planar constraints},\nauthor = {bagge carlson, fredrik and johansson, rolf and robertsson, anders},\nbooktitle = {international conference on intelligent robots and systems (iros)},\nyear = {2015},\norganization = {ieee}\n}\nthe friction model frictionrbfn was presented in\n@inproceedings{bagge2015friction,\ntitle = {modeling and identification of position and temperature dependent friction phenomena without temperature sensing},\nauthor = {bagge carlson, fredrik and robertsson, anders and johansson, rolf},\nbooktitle = {international conference on intelligent robots and systems (iros)},\nyear = {2015},\norganization = {ieee}\n}", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000724, "year": null}, {"Unnamed: 0": 1757, "autor": 737, "date": null, "content": "Motion Planning Benchmark\nBenchmarking motion planners for wheeled mobile robots in cluttered environments on scenarios close to real-world autonomous driving settings.\nDependencies\nlibccd 1.4+ (because of the chomp implementation used here), included as submodule and automatically built\nOMPL 1.5 - included as submodule, needs to be installed first\nnlohmann/json - not provided, needs to be installed first\nSBPL 1.3.1 - not provided, needs to be installed\nJupyter Lab with Python 3 kernel for plotting and evaluation (see python/README.md)\nThe following boost libraries (version 1.58+) need to be installed:\nboost_serialization\nboost_filesystem\nboost_system\nboost_program_options\nThe provided CHOMP implementation requires, GLUT and other OpenGL libraries to be present, which can be installed through the freeglut3-dev package. PNG via libpng-dev, expat via libexpat1-dev.\nOptionally, to support visual debugging, Qt5 with the Charts and Svg modules needs to be installed.\nThe Python front-end dependencies are defined in python/requirements.txt which can be installed through\npip install -r python/requirements.txt\nUsing Docker\nBuild the Docker image\ndocker build -t mpb .\nRun the image to be able to access the Jupyter Lab instance on port 8888 in your browser from where you can run and evaluate benchmarks:\ndocker run -p 8888:8888 -it mpb\nOptionally, you can mount your local mpb copy to its respective folder inside the docker via\ndocker run -p 8888:8888 -v $(pwd):/root/code/mpb -it mpb\n# use %cd% in place of $(pwd) on Windows\nNow you can edit files from outside the docker and use this container to build and run the experiments.\nYou can connect multiple times to this same running docker, for example if you want to access it from multiple shell instances via\ndocker exec -it $(docker ps -qf \"ancestor=mpb\") bash\nAlternatively, run the provided script ./docker_connect.sh that executes this command.\nBuild instructions\nCheck out the submodules\ngit submodule init && git submodule update\nCreate build and log folders\nmkdir build\nBuild project\ncd build\ncmake ..\ncmake --build . -- -j4\nIf you see an error during the cmake .. command that Qt or one of the Qt modules could not be found, you can ignore this message as this dependency is optional.\nGetting started\nThis project contains several build targets in the experiments/ folder. The main application for benchmarking is the benchmark executable that gets built in the bin/ folder in the project directory.\nRunning a benchmark\n\u26a0 It is recommended to run the benchmarks from the Jupyter front-end.\nRun jupyter lab from the project folder and navigate to the python/ directory where you can find several notebooks that can execute experiments and allow you to plot and analyze the benchmark results.\nAlternatively, you have the option to manually run benchmarks via JSON configuration files that define which planners to execute, and many other settings concerning environments, steer functions, etc.\nIn the bin/ folder, start a benchmark via\n./benchmark configuration.json\nwhere configuration.json is any of the json files in the benchmarks/ folder.\nOptionally, if multiple CPUs are available, multiple benchmarks can be run in parallel using GNU Parallel, e.g., via\nparallel -k ./benchmark ::: ../benchmarks/corridor_radius_*\nThis command will execute the experiments with varying corridor sizes in parallel. For more information, consult the GNU Parallel tutorial.\nThis will eventually output a line similar to\nInfo: Saved path statistics log file <...>\nThe resulting JSON log file can be used for visualizing the planning results and plotting the statistics. To get started, check out the Jupyter notebooks inside the python/ folder where all the plotting tools are provided.\nThird-party libraries\nThis project uses forks from some of the following repositories:\nOpen Motion Planning Library (OMPL)\nSearch-Based Planning Library (SBPL)\nhbanzhaf/steering_functions\nBesides the above contributions, the authors thank Nathan Sturtevant's Moving AI Lab for providing the 2D Pathfinding \"MovingAI\" Datasets.\nDevelopers\nEric Heiden (University of Southern California, Los Angeles, USA)\nLuigi Palmieri (Robert Bosch GmbH, Corporate Research, Stuttgart, Germany)\nLeonard Bruns (KTH Royal Institute of Technology, Stockholm, Sweden)\nZiang Liu (University of Southern California, Los Angeles, USA)\nCitation\nPlease consider citing our corresponding article:\n@article{heiden2021benchmr,\nauthor={Heiden, Eric and Palmieri, Luigi and Bruns, Leonard and Arras, Kai O. and Sukhatme, Gaurav S. and Koenig, Sven},\njournal={IEEE Robotics and Automation Letters},\ntitle={Bench-MR: A Motion Planning Benchmark for Wheeled Mobile Robots},\nyear={2021},\nvolume={6},\nnumber={3},\npages={4536-4543},\ndoi={10.1109/LRA.2021.3068913}}", "link": "https://github.com/robot-motion/bench-mr", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "motion planning benchmark\nbenchmarking motion planners for wheeled mobile robots in cluttered environments on scenarios close to real-world autonomous driving settings.\ndependencies\nlibccd 1.4+ (because of the chomp implementation used here), included as submodule and automatically built\nompl 1.5 - included as submodule, needs to be installed first\nnlohmann/json - not provided, needs to be installed first\nsbpl 1.3.1 - not provided, needs to be installed\njupyter lab with python 3 kernel for plotting and evaluation (see python/readme.md)\nthe following boost libraries (version 1.58+) need to be installed:\nboost_serialization\nboost_filesystem\nboost_system\nboost_program_options\nthe provided chomp implementation requires, glut and other opengl libraries to be present, which can be installed through the freeglut3-dev package. png via libpng-dev, expat via libexpat1-dev.\noptionally, to support visual debugging, qt5 with the charts and svg modules needs to be installed.\nthe python front-end dependencies are defined in python/requirements.txt which can be installed through\npip install -r python/requirements.txt\nusing docker\nbuild the docker image\ndocker build -t mpb .\nrun the image to be able to access the jupyter lab instance on port 8888 in your browser from where you can run and evaluate benchmarks:\ndocker run -p 8888:8888 -it mpb\noptionally, you can mount your local mpb copy to its respective folder inside the docker via\ndocker run -p 8888:8888 -v $(pwd):/root/code/mpb -it mpb\n# use %cd% in place of $(pwd) on windows\nnow you can edit files from outside the docker and use this container to build and run the experiments.\nyou can connect multiple times to this same running docker, for example if you want to access it from multiple shell instances via\ndocker exec -it $(docker ps -qf \"ancestor=mpb\") bash\nalternatively, run the provided script ./docker_connect.sh that executes this command.\nbuild instructions\ncheck out the submodules\ngit submodule init && git submodule update\ncreate build and log folders\nmkdir build\nbuild project\ncd build\ncmake ..\ncmake --build . -- -j4\nif you see an error during the cmake .. command that qt or one of the qt modules could not be found, you can ignore this message as this dependency is optional.\ngetting started\nthis project contains several build targets in the experiments/ folder. the main application for benchmarking is the benchmark executable that gets built in the bin/ folder in the project directory.\nrunning a benchmark\n\u26a0 it is recommended to run the benchmarks from the jupyter front-end.\nrun jupyter lab from the project folder and navigate to the python/ directory where you can find several notebooks that can execute experiments and allow you to -----> plot !!!  and analyze the benchmark results.\nalternatively, you have the option to manually run benchmarks via json configuration files that define which planners to execute, and many other settings concerning environments, steer functions, etc.\nin the bin/ folder, start a benchmark via\n./benchmark configuration.json\nwhere configuration.json is any of the json files in the benchmarks/ folder.\noptionally, if multiple cpus are available, multiple benchmarks can be run in parallel using gnu parallel, e.g., via\nparallel -k ./benchmark ::: ../benchmarks/corridor_radius_*\nthis command will execute the experiments with varying corridor sizes in parallel. for more information, consult the gnu parallel tutorial.\nthis will eventually output a line similar to\ninfo: saved path statistics log file <...>\nthe resulting json log file can be used for visualizing the planning results and plotting the statistics. to get started, check out the jupyter notebooks inside the python/ folder where all the plotting tools are provided.\nthird-party libraries\nthis project uses forks from some of the following repositories:\nopen motion planning library (ompl)\nsearch-based planning library (sbpl)\nhbanzhaf/steering_functions\nbesides the above contributions, the authors thank nathan sturtevant's moving ai lab for providing the 2d pathfinding \"movingai\" datasets.\ndevelopers\neric heiden (university of southern california, los angeles, usa)\nluigi palmieri (robert bosch gmbh, corporate research, stuttgart, germany)\nleonard bruns (kth royal institute of technology, stockholm, sweden)\nziang liu (university of southern california, los angeles, usa)\ncitation\nplease consider citing our corresponding article:\n@article{heiden2021benchmr,\nauthor={heiden, eric and palmieri, luigi and bruns, leonard and arras, kai o. and sukhatme, gaurav s. and koenig, sven},\njournal={ieee robotics and automation letters},\ntitle={bench-mr: a motion planning benchmark for wheeled mobile robots},\nyear={2021},\nvolume={6},\nnumber={3},\npages={4536-4543},\ndoi={10.1109/lra.2021.3068913}}", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000737, "year": null}, {"Unnamed: 0": 1792, "autor": 772, "date": null, "content": "Maddux\nRobot Arm and Simulation Environment\nYou can view the complete documentation here.\nCreated to use in a project for Robert Platt's Robotics Course\nFeatures\nArbitrary Length Arms\nForward Kinematics\nInverse Kinematics\nSimulation Environment (with objects like Balls, Targets, Obstacles)\n3D Environment Animations\n3D Arm Animations\nEnd Effector Position, Velocity\nArm Visualization and Animations\nimport numpy as np\nfrom maddux.objects import Obstacle, Ball\nfrom maddux.environment import Environment\nfrom maddux.robots import simple_human_arm\nobstacles = [Obstacle([1, 2, 1], [2, 2.5, 1.5]),\nObstacle([3, 2, 1], [4, 2.5, 1.5])]\nball = Ball([2.5, 2.5, 2.0], 0.25)\nq0 = np.array([0, 0, 0, np.pi / 2, 0, 0, 0])\nhuman_arm = simple_human_arm(2.0, 2.0, q0, np.array([3.0, 1.0, 0.0]))\nenv = Environment(dimensions=[10.0, 10.0, 20.0],\ndynamic_objects=[ball],\nstatic_objects=obstacles,\nrobot=human_arm)\nq_new = human_arm.ikine(ball.position)\nhuman_arm.update_angles(q_new)\nenv.plot()\nEnvironment Visualizations and Animations\nfrom maddux.environment import Environment\nfrom maddux.objects import Ball, Target\nball = Ball([2, 0, 2], 0.25)\ntarget = Target([2, 10, 2], 0.5)\nenvironment = Environment(dynamic_objects=[ball], static_objects=[target])\nrelease_velocity = np.array([0, 15, 5])\nball.throw(release_velocity)\n# Either run environment for n seconds\nenvironment.run(2.0)\n# And plot the result\nenvironment.plot()\n# Or, you can animate it while running\nenvironment.animate(2.0)\nArm Usage\nfrom maddux.robots.link import Link\nfrom maddux.robots.arm import Arm\n# Create a series of links (each link has one joint)\nL1 = Link(0,0,0,1.571)\nL2 = Link(0,0,0,-1.571)\nL3 = Link(0,0.4318,0,-1.571)\nL4 = Link(0,0,0,1.571)\nL5 = Link(0,0.4318,0,1.571)\nlinks = np.array([L1, L2, L3, L4, L5])\n# Initial arm angle\nq0 = np.array([0, 0, 0, np.pi/2, 0])\n# Create arm\nr = Arm(links, q0, '1-link')\nUse with Deep Q Learning\nThis library was created with the intent of experimenting with reinforcement learning on robot manipulators. nivwusquorum/tensorflow-deepq provides an excellent tool to experiment with Deep Q Learning.\nmaddux/rl_experiments/ provides full reinforcement learning classes and arm environments for doing obstacle avoidance and manipulator control using the above Deep Q Learning framework.\nFor fun, here's some examples\nIteration 0\nIteration 100\nIteration 1000", "link": "https://github.com/bcaine/maddux", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "maddux\nrobot arm and simulation environment\nyou can view the complete documentation here.\ncreated to use in a project for robert platt's robotics course\nfeatures\narbitrary length arms\nforward kinematics\ninverse kinematics\nsimulation environment (with objects like balls, targets, obstacles)\n3d environment animations\n3d arm animations\nend effector position, velocity\narm visualization and animations\nimport numpy as np\nfrom maddux.objects import obstacle, ball\nfrom maddux.environment import environment\nfrom maddux.robots import simple_human_arm\nobstacles = [obstacle([1, 2, 1], [2, 2.5, 1.5]),\nobstacle([3, 2, 1], [4, 2.5, 1.5])]\nball = ball([2.5, 2.5, 2.0], 0.25)\nq0 = np.array([0, 0, 0, np.pi / 2, 0, 0, 0])\nhuman_arm = simple_human_arm(2.0, 2.0, q0, np.array([3.0, 1.0, 0.0]))\nenv = environment(dimensions=[10.0, 10.0, 20.0],\ndynamic_objects=[ball],\nstatic_objects=obstacles,\nrobot=human_arm)\nq_new = human_arm.ikine(ball.position)\nhuman_arm.update_angles(q_new)\nenv.plot()\nenvironment visualizations and animations\nfrom maddux.environment import environment\nfrom maddux.objects import ball, target\nball = ball([2, 0, 2], 0.25)\ntarget = target([2, 10, 2], 0.5)\nenvironment = environment(dynamic_objects=[ball], static_objects=[target])\nrelease_velocity = np.array([0, 15, 5])\nball.throw(release_velocity)\n# either run environment for n seconds\nenvironment.run(2.0)\n# and -----> plot !!!  the result\nenvironment.plot()\n# or, you can animate it while running\nenvironment.animate(2.0)\narm usage\nfrom maddux.robots.link import link\nfrom maddux.robots.arm import arm\n# create a series of links (each link has one joint)\nl1 = link(0,0,0,1.571)\nl2 = link(0,0,0,-1.571)\nl3 = link(0,0.4318,0,-1.571)\nl4 = link(0,0,0,1.571)\nl5 = link(0,0.4318,0,1.571)\nlinks = np.array([l1, l2, l3, l4, l5])\n# initial arm angle\nq0 = np.array([0, 0, 0, np.pi/2, 0])\n# create arm\nr = arm(links, q0, '1-link')\nuse with deep q learning\nthis library was created with the intent of experimenting with reinforcement learning on robot manipulators. nivwusquorum/tensorflow-deepq provides an excellent tool to experiment with deep q learning.\nmaddux/rl_experiments/ provides full reinforcement learning classes and arm environments for doing obstacle avoidance and manipulator control using the above deep q learning framework.\nfor fun, here's some examples\niteration 0\niteration 100\niteration 1000", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000772, "year": null}, {"Unnamed: 0": 1812, "autor": 792, "date": null, "content": "LearnRoboticsCpp\nThis repository contains my implementations of classical robotics algorithms in C++. Inspiration drawn from PythonRobotics and CppRobotics. The CppRobotics repo was very good, but it used OpenCV to plot 2D graphs. I found the process of converting 2D points to pixel coordinates in OpenCV very tedious and it seems like a bit of a hack. This repo uses gnuplot-iostream instead for plotting which makes much prettier graphs than OpenCV and allows for us to easily make 3D plots.\nSome of these implementations will have a tutorial attached to it. It's still a work in progress.\nTable of Contents\nRequirements\nDependencies Installation\nBuild\nLocalization\nExtended Kalman Filter\nUnscented Kalman Filter\nParticle Filter\nPath Planning\nDijkstra\nAStar\nRRT\nRRTstar\nPRM\nPotential Field\nQuintic Polynomial\nCubic Spline\nDWA\nModel Predictive Trajectory Generator\nState Lattice Planner\nPath Tracking\nMove to Pose\nStanely Control\nModel Predictive Control\nRequirments\nTested on Ubuntu 18.04\ncmake\nopencv 3.3 (for KD tree in PRM)\nEigen 3\nBoost 1.4 (for gnuplot-iostream)\ngnuplot\nipoptd (this one is a pickle, install tips borrowed from Udacity )\ncppad\nRunning with Docker\nThe Docker image is about 3GB.\nDeployment\n$ sudo docker build -f Dockerfile -t ctfchan/learn-robotics-cpp:latest .\n$ sudo docker push ctfchan/learn-robotics-cpp:latest\nMake sure -it ctfchan/learn-robotics-cpp goes last when you do docker run.\n$ sudo docker pull ctfchan/learn-robotics-cpp\n$ sudo docker run --name learn-robotics-cpp --mount type=bind,source=\"$(pwd)\",target=/root/LearnRoboticsCpp -it ctfchan/learn-robotics-cpp\nFrom inside the Docker\n$ cd ~/LearnRoboticsCpp\n$ ./bin/state_lattice # or whatever executable you want to run\nThe images will show up in the animations directory.\ndocker stop when you're done. docker rm when you want to get rid of the container to start over or something.\n$ sudo docker stop learn-robotics-cpp\n$ sudo docker rm learn-robotics-cpp\ndocker exec to run it after you stop it.\n$ sudo docker exec learn-robotics-cpp bash\nPath Planning\nDWA\nDijkstra\nA*\nPRM\nRRT\nRRTStar\nPotential Field\nQuintic Polynomial\nExplanation\nCubic Spline\nExplanation\nModel Predictive Trajectory Generation\nExplanation\nState Lattice Planner\nLeft to right: uniform, biased, lane sampling\nTutorial\nPath Tracking\nMove To Pose\nStanley Control\nModel Predictive Control\nTutorial\nLocalization\nExtended Kalman Filter\nTutorial\nUnscented Kalman Filter\nWe can see error ellipse in this demo is a much better approximation of the true distribution. EKF can be biased and inconsistent.\nTutorial\nParticle Filter", "link": "https://github.com/CtfChan/LearnRoboticsCpp", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "learnroboticscpp\nthis repository contains my implementations of classical robotics algorithms in c++. inspiration drawn from pythonrobotics and cpprobotics. the cpprobotics repo was very good, but it used opencv to -----> plot !!!  2d graphs. i found the process of converting 2d points to pixel coordinates in opencv very tedious and it seems like a bit of a hack. this repo uses gnuplot-iostream instead for plotting which makes much prettier graphs than opencv and allows for us to easily make 3d plots.\nsome of these implementations will have a tutorial attached to it. it's still a work in progress.\ntable of contents\nrequirements\ndependencies installation\nbuild\nlocalization\nextended kalman filter\nunscented kalman filter\nparticle filter\npath planning\ndijkstra\nastar\nrrt\nrrtstar\nprm\npotential field\nquintic polynomial\ncubic spline\ndwa\nmodel predictive trajectory generator\nstate lattice planner\npath tracking\nmove to pose\nstanely control\nmodel predictive control\nrequirments\ntested on ubuntu 18.04\ncmake\nopencv 3.3 (for kd tree in prm)\neigen 3\nboost 1.4 (for gnuplot-iostream)\ngnuplot\nipoptd (this one is a pickle, install tips borrowed from udacity )\ncppad\nrunning with docker\nthe docker image is about 3gb.\ndeployment\n$ sudo docker build -f dockerfile -t ctfchan/learn-robotics-cpp:latest .\n$ sudo docker push ctfchan/learn-robotics-cpp:latest\nmake sure -it ctfchan/learn-robotics-cpp goes last when you do docker run.\n$ sudo docker pull ctfchan/learn-robotics-cpp\n$ sudo docker run --name learn-robotics-cpp --mount type=bind,source=\"$(pwd)\",target=/root/learnroboticscpp -it ctfchan/learn-robotics-cpp\nfrom inside the docker\n$ cd ~/learnroboticscpp\n$ ./bin/state_lattice # or whatever executable you want to run\nthe images will show up in the animations directory.\ndocker stop when you're done. docker rm when you want to get rid of the container to start over or something.\n$ sudo docker stop learn-robotics-cpp\n$ sudo docker rm learn-robotics-cpp\ndocker exec to run it after you stop it.\n$ sudo docker exec learn-robotics-cpp bash\npath planning\ndwa\ndijkstra\na*\nprm\nrrt\nrrtstar\npotential field\nquintic polynomial\nexplanation\ncubic spline\nexplanation\nmodel predictive trajectory generation\nexplanation\nstate lattice planner\nleft to right: uniform, biased, lane sampling\ntutorial\npath tracking\nmove to pose\nstanley control\nmodel predictive control\ntutorial\nlocalization\nextended kalman filter\ntutorial\nunscented kalman filter\nwe can see error ellipse in this demo is a much better approximation of the true distribution. ekf can be biased and inconsistent.\ntutorial\nparticle filter", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000792, "year": null}, {"Unnamed: 0": 1843, "autor": 823, "date": null, "content": "DTW - Dynamic Time Warping in Python / C (using ctypes)\nThe Dynamic Time Warping (DTW)[1,2] is a time-normalisation algorithm initially designed to eliminate timing differences between two speech patterns. This normalisation, or correction, is done by warping the time axis of one time series to match the other. The correction (time warping) makes it easier to compare two signals in a similar way to the method human beings use[3].\nExample of 2D trajectory-matching generated by the DTW method. Although looking perfect in the figure on the left, the cardioid was modified to have a constant value zone from time step $50$ to $150$. The DTW correctly matches the values as can be seen as a straight blue line in the Accumulated Distance plot (right).\nAbove is presented an example where a cardioid is compared to a circle. The cardioid also had a time delay inserted (values were kept constant). The DTW calculates the distance (here the Euclidean one) between all the points of the two time series and, then, generates another matrix with the accumulated distances. The total distance defined by the path formed with the minimum values of the accumulated distance (right-hand side of the figure) can be easily applied to compare different shapes.\nThis version of the algorithm uses a C kernel, supporting multidimensional arrays and Euclidean distance, to speed up the calculations with a Python wrapper as the user interface. More details and sample code can be found in this Jupyter notebook:\nDynamic_Time_Warping.ipynb\nif you are not happy with my explanations above, one of the best explanations about how the DTW works I've found on a presentation by Elena Tsiporkova.\nHow to install:\nClone the repository, or download it as a zip file and unzip it.\nInside the directory dtw_python execute make.\nNow, to install the library and make it accessible to all users you need to execute sudo make install.\nAfter that you don't need the dtw_python directory anymore and you can test it using the jupyter notebook.\nRelated works:\nGraceful Degradation under Noise on Brain Inspired Robot Controllers\nDiverse, Noisy and Parallel: a New Spiking Neural Network Approach for Humanoid Robot Control\nShort-Term Plasticity in a Liquid State Machine Biomimetic Robot Arm Controller\nNeurorobotic Simulations on the Degradation of Multiple Column Liquid State Machines\nSensor Fusion Approach Using Liquid StateMachines for Positioning Control\nReferences:\nSakoe, H., and S. Chiba. \u201cDynamic Programming Algorithm Optimization for Spoken Word Recognition.\u201d IEEE Transactions on Acoustics, Speech and Signal Processing 26, no. 1 (February 1978): 43\u201349.\nMeinard M\u00fcller. \u201cDynamic Time Warping.\u201d In Information Retrieval for Music and Motion, ch. 4, 69-82. New York: Springer-Verlag, 2007.\nRatcliff, Roger. \u201cContinuous versus Discrete Information Processing: Modeling Accumulation of Partial Information.\u201d Master Thesis, Radboud University Nijmegen, 2004.\nOther projects you may like to check:\ncolab_utils: Some useful (or not so much) Python stuff for Google Colab notebooks\nExecThatCell: (Re)Execute a Jupyter (colab) notebook cell programmatically by searching for its label.\nMaple-Syrup-Pi-Camera: Low power('ish) AIoT smart camera (3D printed) based on the Raspberry Pi Zero W and Google Coral EdgeTPU\nThe CogniFly Project: Open-source autonomous flying robots robust to collisions and smart enough to do something interesting!\nBee: The Bee simulator is an open source Spiking Neural Network (SNN) simulator, freely available, specialised in Liquid State Machine (LSM) systems with its core functions fully implemented in C.\nhttp://ricardodeazambuja.com/", "link": "https://github.com/ricardodeazambuja/DTW", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "dtw - dynamic time warping in python / c (using ctypes)\nthe dynamic time warping (dtw)[1,2] is a time-normalisation algorithm initially designed to eliminate timing differences between two speech patterns. this normalisation, or correction, is done by warping the time axis of one time series to match the other. the correction (time warping) makes it easier to compare two signals in a similar way to the method human beings use[3].\nexample of 2d trajectory-matching generated by the dtw method. although looking perfect in the figure on the left, the cardioid was modified to have a constant value zone from time step $50$ to $150$. the dtw correctly matches the values as can be seen as a straight blue line in the accumulated distance -----> plot !!!  (right).\nabove is presented an example where a cardioid is compared to a circle. the cardioid also had a time delay inserted (values were kept constant). the dtw calculates the distance (here the euclidean one) between all the points of the two time series and, then, generates another matrix with the accumulated distances. the total distance defined by the path formed with the minimum values of the accumulated distance (right-hand side of the figure) can be easily applied to compare different shapes.\nthis version of the algorithm uses a c kernel, supporting multidimensional arrays and euclidean distance, to speed up the calculations with a python wrapper as the user interface. more details and sample code can be found in this jupyter notebook:\ndynamic_time_warping.ipynb\nif you are not happy with my explanations above, one of the best explanations about how the dtw works i've found on a presentation by elena tsiporkova.\nhow to install:\nclone the repository, or download it as a zip file and unzip it.\ninside the directory dtw_python execute make.\nnow, to install the library and make it accessible to all users you need to execute sudo make install.\nafter that you don't need the dtw_python directory anymore and you can test it using the jupyter notebook.\nrelated works:\ngraceful degradation under noise on brain inspired robot controllers\ndiverse, noisy and parallel: a new spiking neural network approach for humanoid robot control\nshort-term plasticity in a liquid state machine biomimetic robot arm controller\nneurorobotic simulations on the degradation of multiple column liquid state machines\nsensor fusion approach using liquid statemachines for positioning control\nreferences:\nsakoe, h., and s. chiba. \u201cdynamic programming algorithm optimization for spoken word recognition.\u201d ieee transactions on acoustics, speech and signal processing 26, no. 1 (february 1978): 43\u201349.\nmeinard m\u00fcller. \u201cdynamic time warping.\u201d in information retrieval for music and motion, ch. 4, 69-82. new york: springer-verlag, 2007.\nratcliff, roger. \u201ccontinuous versus discrete information processing: modeling accumulation of partial information.\u201d master thesis, radboud university nijmegen, 2004.\nother projects you may like to check:\ncolab_utils: some useful (or not so much) python stuff for google colab notebooks\nexecthatcell: (re)execute a jupyter (colab) notebook cell programmatically by searching for its label.\nmaple-syrup-pi-camera: low power('ish) aiot smart camera (3d printed) based on the raspberry pi zero w and google coral edgetpu\nthe cognifly project: open-source autonomous flying robots robust to collisions and smart enough to do something interesting!\nbee: the bee simulator is an open source spiking neural network (snn) simulator, freely available, specialised in liquid state machine (lsm) systems with its core functions fully implemented in c.\nhttp://ricardodeazambuja.com/", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000823, "year": null}, {"Unnamed: 0": 1866, "autor": 846, "date": null, "content": "LRGNet: Learnable Region Growing for Point Cloud Segmentation\nThis repository contains code for the RAL paper LRGNet: Learnable Region Growing for Class-Agnostic Point Cloud Segmentation.\nPrerequisites\nnumpy\nscipy\nscikit-learn\ntensorflow\nh5py\nnetworkx\nData Staging\nRun the following script to download the necessary point cloud files in H5 format to the data folder.\nbash download_data.sh\nTo use the Semantic KITTI dataset, follow the instructions on the Semantic KITTI website and then run the following script:\npython stage_semantic_kitti.py --dataset semantic_kitti/dataset/ --output data/kitti_train.h5 --sequences 00,01,02,03,04,05,06,07,09,10\npython stage_semantic_kitti.py --dataset semantic_kitti/dataset/ --output data/kitti_val.h5 --sequences 08\nData Visualization\nTo check the data shape/size contained in each H5 file:\npython examine_h5.py data/scannet.h5\n<HDF5 dataset \"count_room\": shape (312,), type \"<i4\">\n<HDF5 dataset \"points\": shape (7924044, 8), type \"<f4\">\nTo convert the H5 data file into individual point cloud files (PLY) in format, run the script as follows. PLY files can be opened using the CloudCompare program\n#Render the point clouds in original RGB color\npython h5_to_ply.py data/s3dis_area3.h5 --rgb\n#Render the point clouds colored according to segmentation ID\npython h5_to_ply.py data/s3dis_area3.h5 --seg\n...\nSaved to data/viz/22.ply: (18464 points)\nSaved to data/viz/23.ply: (20749 points)\nTo plot the instance color legend for a target room:\n#Plot color legend for room #100 in ScanNet\npython h5_to_ply.py data/scannet.h5 --target 100\nBenchmarks\nTrain benchmark networks such as PointNet and PointNet++ (pointnet2).\npython train_pointnet.py --mode pointnet --train-area 1,2,3,4,6 --val-area 5\nRun benchmark algorithms on each dataset. Mode is one of normal, color, curvature, pointnet, pointnet2, edge, smoothness, fpfh, feature.\npython benchmarks.py --mode normal --area 5 --threshold 0.99 --save\nEvaluate the cross-domain performance as follows:\npython train_pointnet.py --mode pointnet --train-area scannet --val-area s3dis --cross-domain\npython benchmarks.py --mode pointnet --train-area scannet --area s3dis --cross-domain\nEvaluate the performance on the Semantic KITTI dataset as follows:\npython benchmarks.py --mode feature --area kitti_val --resolution 0.3\npython train_pointnet.py --mode pointnet --train-area kitti_train --val-area kitti_val\npython benchmarks.py --mode pointnet --train-area kitti_train --area kitti_val --resolution 0.3\nLearn Region Grow (LRGNet)\nRun region growing simulations to stage ground truth data for LRGNet.\npython stage_data.py\n#To apply data augmentation, run stage_data with different random seeds\nfor i in 0 1 2 3 4 5 6 7\ndo\nfor j in s3dis scannet\ndo\npython stage_data.py --seed $i --area $j\ndone\ndone\nTrain LRGNet for each area of the S3DIS dataset.\npython train_region_grow.py --train-area 1,2,3,4,6 --val-area 5\nTest LrgNet and measure the accuracy metrics.\npython test_region_grow.py --area 5 --save\npython test_region_grow.py --area scannet --save\nTest LRGNet using local search methods\npython test_random_restart.py --area 5 --scoring ml\npython test_random_restart.py --area 5 --scoring np\npython test_beam_search.py --area 5 --scoring ml\npython test_beam_search.py --area 5 --scoring np\nEvaluate the cross-domain performance as follows:\npython train_region_grow.py --train-area scannet --cross-domain\npython test_region_grow.py --train-area scannet --area s3dis --cross-domain\npython test_random_restart.py --train-area scannet --area s3dis --cross-domain --scoring np\nEvaluate the performance on the Semantic KITTI dataset as follows:\nfor i in 01 02 03 04 05 06 07 09 10\ndo\npython stage_semantic_kitti.py --dataset semantic_kitti/dataset/ --output data/kitti_train_\"$i\".h5 --sequences $i --skip 1\ndone\nfor i in 0 1 2 3 4 5 6 7 9 10\ndo\npython stage_data.py --area kitti_train --resolution 0.3 --seed $i\ndone\npython train_region_grow.py --train-area kitti_train --val-area kitti_val --multiseed 11\npython test_region_grow.py --area kitti_val --resolution 0.3 --save\nResults\nSegmentation results on S3DIS dataset\nSegmentation results on ScanNet dataset\nSegmentation results on Semantic KITTI dataset\nCitation\n@ARTICLE{chen2021ral,\nauthor={J. {Chen} and Z. {Kira} and Y. K. {Cho}},\njournal={IEEE Robotics and Automation Letters},\ntitle={LRGNet: Learnable Region Growing for Class-Agnostic Point Cloud Segmentation},\nyear={2021},\nvolume={6},\nnumber={2},\npages={2799-2806},\ndoi={10.1109/LRA.2021.3062607},\n}", "link": "https://github.com/jingdao/learn_region_grow", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "lrgnet: learnable region growing for point cloud segmentation\nthis repository contains code for the ral paper lrgnet: learnable region growing for class-agnostic point cloud segmentation.\nprerequisites\nnumpy\nscipy\nscikit-learn\ntensorflow\nh5py\nnetworkx\ndata staging\nrun the following script to download the necessary point cloud files in h5 format to the data folder.\nbash download_data.sh\nto use the semantic kitti dataset, follow the instructions on the semantic kitti website and then run the following script:\npython stage_semantic_kitti.py --dataset semantic_kitti/dataset/ --output data/kitti_train.h5 --sequences 00,01,02,03,04,05,06,07,09,10\npython stage_semantic_kitti.py --dataset semantic_kitti/dataset/ --output data/kitti_val.h5 --sequences 08\ndata visualization\nto check the data shape/size contained in each h5 file:\npython examine_h5.py data/scannet.h5\n<hdf5 dataset \"count_room\": shape (312,), type \"<i4\">\n<hdf5 dataset \"points\": shape (7924044, 8), type \"<f4\">\nto convert the h5 data file into individual point cloud files (ply) in format, run the script as follows. ply files can be opened using the cloudcompare program\n#render the point clouds in original rgb color\npython h5_to_ply.py data/s3dis_area3.h5 --rgb\n#render the point clouds colored according to segmentation id\npython h5_to_ply.py data/s3dis_area3.h5 --seg\n...\nsaved to data/viz/22.ply: (18464 points)\nsaved to data/viz/23.ply: (20749 points)\nto -----> plot !!!  the instance color legend for a target room:\n#-----> plot !!!  color legend for room #100 in scannet\npython h5_to_ply.py data/scannet.h5 --target 100\nbenchmarks\ntrain benchmark networks such as pointnet and pointnet++ (pointnet2).\npython train_pointnet.py --mode pointnet --train-area 1,2,3,4,6 --val-area 5\nrun benchmark algorithms on each dataset. mode is one of normal, color, curvature, pointnet, pointnet2, edge, smoothness, fpfh, feature.\npython benchmarks.py --mode normal --area 5 --threshold 0.99 --save\nevaluate the cross-domain performance as follows:\npython train_pointnet.py --mode pointnet --train-area scannet --val-area s3dis --cross-domain\npython benchmarks.py --mode pointnet --train-area scannet --area s3dis --cross-domain\nevaluate the performance on the semantic kitti dataset as follows:\npython benchmarks.py --mode feature --area kitti_val --resolution 0.3\npython train_pointnet.py --mode pointnet --train-area kitti_train --val-area kitti_val\npython benchmarks.py --mode pointnet --train-area kitti_train --area kitti_val --resolution 0.3\nlearn region grow (lrgnet)\nrun region growing simulations to stage ground truth data for lrgnet.\npython stage_data.py\n#to apply data augmentation, run stage_data with different random seeds\nfor i in 0 1 2 3 4 5 6 7\ndo\nfor j in s3dis scannet\ndo\npython stage_data.py --seed $i --area $j\ndone\ndone\ntrain lrgnet for each area of the s3dis dataset.\npython train_region_grow.py --train-area 1,2,3,4,6 --val-area 5\ntest lrgnet and measure the accuracy metrics.\npython test_region_grow.py --area 5 --save\npython test_region_grow.py --area scannet --save\ntest lrgnet using local search methods\npython test_random_restart.py --area 5 --scoring ml\npython test_random_restart.py --area 5 --scoring np\npython test_beam_search.py --area 5 --scoring ml\npython test_beam_search.py --area 5 --scoring np\nevaluate the cross-domain performance as follows:\npython train_region_grow.py --train-area scannet --cross-domain\npython test_region_grow.py --train-area scannet --area s3dis --cross-domain\npython test_random_restart.py --train-area scannet --area s3dis --cross-domain --scoring np\nevaluate the performance on the semantic kitti dataset as follows:\nfor i in 01 02 03 04 05 06 07 09 10\ndo\npython stage_semantic_kitti.py --dataset semantic_kitti/dataset/ --output data/kitti_train_\"$i\".h5 --sequences $i --skip 1\ndone\nfor i in 0 1 2 3 4 5 6 7 9 10\ndo\npython stage_data.py --area kitti_train --resolution 0.3 --seed $i\ndone\npython train_region_grow.py --train-area kitti_train --val-area kitti_val --multiseed 11\npython test_region_grow.py --area kitti_val --resolution 0.3 --save\nresults\nsegmentation results on s3dis dataset\nsegmentation results on scannet dataset\nsegmentation results on semantic kitti dataset\ncitation\n@article{chen2021ral,\nauthor={j. {chen} and z. {kira} and y. k. {cho}},\njournal={ieee robotics and automation letters},\ntitle={lrgnet: learnable region growing for class-agnostic point cloud segmentation},\nyear={2021},\nvolume={6},\nnumber={2},\npages={2799-2806},\ndoi={10.1109/lra.2021.3062607},\n}", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000846, "year": null}, {"Unnamed: 0": 1885, "autor": 865, "date": null, "content": "Robotics---Computational-Motion-Planning\n1. Graph-based Path Planning:\nImplementing the planning systems that work on 2D grid-like environments. For both sections the input map will be specified as a 2D logical array where the false or zero entries correspond to free cells and the true or non-zero entries correspond to obstacle cells. The goal of these planning routines is to construct a route between the specified start and destination cells. Two algorithms are to be implemented:\n1. Dijkstra planner.\n2. A* planner.\n2. Configuration Space:\nWritinging a program to help guide the two-link robot arm shown in the the figure below from one configuration to another while avoiding the objects in the workspace.\n3. Probabilistic Roadmap:\nwriting a program to help guide the six-link robot shown in the figure below from one one configuration to another while avoiding the objects in the workspace with planning methods based on random sampling. The robot shown in the figure is comprised of six revolute links and its configuration can be specified with a vector (theta1; theta2; theta3; theta4; theta5; theta6) where each entry is an angle in degrees between 0 and 360. Function SixLinkRobot computes the layout of all of the links as a function of the 6 parameters.\n4. Artificial Potential Field:\nDeveloping code to guide a robot from one location to another in a 2-dimensional configuration space using artificial potential fields. The figure below depicts a plot of the energy surface associated with our sample environment and the state of the robot is modeled by the red sphere which we can think of as rolling down the energy surface towards the goal location.", "link": "https://github.com/2wavetech/Robotics---Computational-Motion-Planning", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "robotics---computational-motion-planning\n1. graph-based path planning:\nimplementing the planning systems that work on 2d grid-like environments. for both sections the input map will be specified as a 2d logical array where the false or zero entries correspond to free cells and the true or non-zero entries correspond to obstacle cells. the goal of these planning routines is to construct a route between the specified start and destination cells. two algorithms are to be implemented:\n1. dijkstra planner.\n2. a* planner.\n2. configuration space:\nwritinging a program to help guide the two-link robot arm shown in the the figure below from one configuration to another while avoiding the objects in the workspace.\n3. probabilistic roadmap:\nwriting a program to help guide the six-link robot shown in the figure below from one one configuration to another while avoiding the objects in the workspace with planning methods based on random sampling. the robot shown in the figure is comprised of six revolute links and its configuration can be specified with a vector (theta1; theta2; theta3; theta4; theta5; theta6) where each entry is an angle in degrees between 0 and 360. function sixlinkrobot computes the layout of all of the links as a function of the 6 parameters.\n4. artificial potential field:\ndeveloping code to guide a robot from one location to another in a 2-dimensional configuration space using artificial potential fields. the figure below depicts a -----> plot !!!  of the energy surface associated with our sample environment and the state of the robot is modeled by the red sphere which we can think of as rolling down the energy surface towards the goal location.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000865, "year": null}, {"Unnamed: 0": 1907, "autor": 887, "date": null, "content": "Inverse Kinematic Model OpenCat\nThis is an inverse kinematic model and gait generator for the OpenCat project based on the IKPY library (http://phylliade.github.io/ikpy).\nNybble-Version\nBittle-Version\nUsage and Tweaks\nSimply run the script with python kinematics_bittle.py or kinematics_nybble.py. At first you will see a plot of the vertical and horizontal movement. After closing this window, the script will generate the joint angles, that can be directly used for the instincts.h in OpenCat.\nPlease make sure that you install the required packages in requirements.txt.", "link": "https://github.com/ger01d/kinematic-model-opencat", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "inverse kinematic model opencat\nthis is an inverse kinematic model and gait generator for the opencat project based on the ikpy library (http://phylliade.github.io/ikpy).\nnybble-version\nbittle-version\nusage and tweaks\nsimply run the script with python kinematics_bittle.py or kinematics_nybble.py. at first you will see a -----> plot !!!  of the vertical and horizontal movement. after closing this window, the script will generate the joint angles, that can be directly used for the instincts.h in opencat.\nplease make sure that you install the required packages in requirements.txt.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000887, "year": null}, {"Unnamed: 0": 1962, "autor": 942, "date": null, "content": "Exudyn\nExudyn version = 1.1.42\nA flexible multibody dynamics systems simulation code with Python and C++\nfree, open source and with plenty of documentation and examples\nNOTE: for pure installation, just go to main/distand install your preferred Microsoft installer (.msi) or wheel (.whl) in your according Python environment (win=Windows, py3.7 = Python3.7, ...)\nThis README document is a small part of the complete documentation found as PDF document in docs/theDoc/theDoc.pdf. It is auto-generated from .tex files (sorry for some conversion errors!). Due to limitations for complex formulas and tables in .rst files, details of the reference manual and many other parts of the documentation are only available in theDoc.pdf, see the github page of Exudyn !\nFor license, see LICENSE.txt in the root folder!\nIn addition to the tutorial in the documentation, many ( 100+ ) examples can be found under main/pythonDev/Examples and main/pythonDev/TestModels .\nTutorial videos can be found in the youtube channel of Exudyn !\nEnjoy the Python library for multibody dynamics modeling, simulation, creating large scale systems, parameterized systems, component mode synthesis, optimization, ...\nInstallation and Getting Started\nExudyn is hosted on GitHub :\nweb: https://github.com/jgerstmayr/EXUDYN\nFor any comments, requests, issues, bug reports, send an email to:\nemail: reply.exudyn@gmail.com\nThanks for your contribution!\nGetting started\nThis section will show:\nWhat is Exudyn ?\nWho is developing Exudyn ?\nHow to install Exudyn\nHow to link Exudyn and Python\nGoals of Exudyn\nRun a simple example in Spyder\nFAQ -- Frequently asked questions\nWhat is Exudyn ?\nExudyn -- (flEXible mUltibody DYNamics -- EXtend yoUr DYNamics)\nExudyn is a C++ based Python library for efficient simulation of flexible multibody dynamics systems. It is the follow up code of the previously developed multibody code HOTINT, which Johannes Gerstmayr started during his PhD-thesis. It seemed that the previous code HOTINT reached limits of further (efficient) development and it seemed impossible to continue from this code as it was outdated regarding programming techniques and the numerical formulation at the time Exudyn was started.\nExudyn is designed to easily set up complex multibody models, consisting of rigid and flexible bodies with joints, loads and other components. It shall enable automatized model setup and parameter variations, which are often necessary for system design but also for analysis of technical problems. The broad usability of Python allows to couple a multibody simulation with environments such as optimization, statistics, data analysis, machine learning and others.\nThe multibody formulation is mainly based on redundant coordinates. This means that computational objects (rigid bodies, flexible bodies, ...) are added as independent bodies to the system. Hereafter, connectors (e.g., springs or constraints) are used to interconnect the bodies. The connectors are using Markers on the bodies as interfaces, in order to transfer forces and displacements. For details on the interaction of nodes, objects, markers and loads see theDoc.pdf.\nDevelopers of Exudyn and thanks\nExudyn is currently developed at the University of Innsbruck. In the first phase most of the core code is written by Johannes Gerstmayr, implementing ideas that followed out of the project HOTINT. 15 years of development led to a lot of lessions learned and after 20 years, a code must be re-designed.\nSome important tests for the coupling between C++ and Python have been written by Stefan Holzinger. Stefan also helped to set up the previous upload to GitLab and to test parallelization features. For the interoperability between C++ and Python, we extensively use Pybind11, originally written by Jakob Wenzel, see https://github.com/pybind/pybind11. Without Pybind11 we couldn't have made this project -- Thank's a lot!\nImportant discussions with researchers from the community were important for the design and development of Exudyn , where we like to mention Joachim Sch\"oberl from TU-Vienna who boosted the design of the code with great concepts.\nThe cooperation and funding within the EU H2020-MSCA-ITN project 'Joint Training on Numerical Modelling of Highly Flexible Structures for Industrial Applications' contributes to the development of the code.\nThe following people have contributed to Python and C++ library implementations:\nJoachim Sch\"oberl (Providing specialized NGsolve core library with taskmanager for bf multithreaded parallelization; NGsolve mesh and FE-matrices import; highly efficient eigenvector computations)\nStefan Holzinger (Lie group solvers in Python)\nPeter Manzl (ConvexRoll Python / C++ implementation)\nMartin Sereinig (special robotics functionality)\nThe following people have contributed to the examples:\nStefan Holzinger, Michael Pieber, Manuel Schieferle, Martin Knapp, Lukas March,\nDominik Sponring, David Wibmer, Andreas Zw\"olfer, Peter Manzl\n-- thanks a lot! --\nInstallation instructions\nHow to install Exudyn ?\nIn order to run Exudyn , you need an appropriate Python installation. We currently (2021-07) recommend to use\nAnaconda, 64bit, Python 3.7.7 (Anaconda3 64bit with Python3.7.7 can be downloaded via the repository archive https://repo.anaconda.com/archive/ choosing Anaconda3-2020.02-Windows-x86_64.exe) (but Python 3.8 is also working well!)\nSpyder 4.1.3 (with Python 3.7.7, 64bit), which is included in the Anaconda installation (or 64bit and are compiled up to the same minor version, i.e., 3.7.x. There will be a strange .DLL error, if you mix up 32/64bit. It is possible to install both, Anaconda 32bit and Anaconda 64bit -- then you should follow the recommendations of paths as suggested by Anaconda installer.)\nMany alternative options exist:\nIn case that you have an older CPU, which does not support AVX2, use: Anaconda, 32bit, Python 3.6.5) (Anaconda 32bit with Python3.6 can be downloaded via the repository archive https://repo.anaconda.com/archive/ choosing Anaconda3-5.2.0-Windows-x86.exe.)\nUsers report successful use of Exudyn with Visual Studio Code. Jupyter has been tested with some examples; both environments should work with default settings.\nAnaconda 2020-11 with Python 3.8 and Spyder 4.1.5: no problems up to now (2021-07), TestSuite runs without problems since Exudyn version 1.0.182.\nAlternative option with more stable Spyder (as compared to Spyder 4.1.3): Anaconda, 64bit, Python 3.6.5) (Anaconda 64bit with Python3.6 can be downloaded via the repository archive https://repo.anaconda.com/archive/ choosing Anaconda3-5.2.0-Windows-x86_64.exe for 64bit.)\nIf you plan to extend the C++ code, we recommend to use VS2017 (previously, VS2019 was recommended: However, VS2019 has problems with the library 'Eigen' and therefore leads to erroneous results with the sparse solver. VS2017 can also be configured with Python 3.7 now.) to compile your code, which offers Python 3.7 compatibility. Once again, remember that Python versions and the version of the Exudyn module must be identical (e.g., Python 3.6 32 bit both in the Exudyn module and in Spyder).\nparagraphInstallation without Anaconda: If you do not install Anaconda (e.g., under Linux), make sure that you have the according Python packages installed:\nnumpy (used throughout the code, inevitable)\nmatplotlib (for any plot, also PlotSensor(...))\ntkinter (for interactive dialogs, SolutionViewer, etc.)\nscipy (needed for eigenvalue computation)\nYou can install most of these packages using pip install numpy (Windows) or pip3 install numpy (Linux).\nFor interaction (right-mouse-click, some key-board commands) you need the Python module tkinter. This is included in regular Anaconda distributions (recommended, see below), but on UBUNTU you need to type alike (do not forget the '3', otherwise it installs for Python2 ...):\nsudo apt-get install python3-tk\nsee also common blogs for your operating system.\nInstall with Windows MSI installer\nThe simplest way on Windows 10 (and maybe also Windows 7), which works well if you installed only one Python version and if you installed Anaconda with the option 'Register Anaconda as my default Python 3.x' or similar, then you can use the provided .msi installers in the main/dist directory:\nFor the 64bits Python 3.7 version, double click on (version may differ):\nexudyn-1.0.248.win-amd64-py3.7.msi\nFollow the instructions of the installer\nIf Python / Anaconda is not found by the installer, provide the 'python directory' as the installation directory of Anaconda3, which usually is installed in:\nC:\\ProgramData\\Anaconda3\nInstall from Wheel (UBUNTU and Windows)\nThe standard way to install the Python package Exudyn is to use the so-called 'wheels' (file ending .whl) provided at the directory wheels in the Exudyn repository.\nFor UBUNTU18.04 (which by default uses Python 3.6) this may read (version number 1.0.20 may be different):\nPython 3.6, 64bit: pip3 install distexudyn-1.0.20-cp36-cp36-linux_x86_64.whl\nFor UBUNTU20.04 (which by default uses Python 3.8) this may read (version number 1.0.20 may be different):\nPython 3.8, 64bit: pip3 install distexudyn-1.0.20-cp38-cp38-linux_x86_64.whl\nNOTE that your installation may have environments with different Python versions, so install that Exudyn version appropriately! If the wheel installation does not work on UBUNTU, it is highly recommended to build Exudyn for your specific system as given in theDoc.pdf.\nWindows:\nFirst, open an Anaconda prompt:\nEITHER calling: START->Anaconda->... OR go to anaconda/Scripts folder and call activate.bat\nYou can check your Python version then, by running Python (python3 under UBUNTU 18.04), the output reads like:\nPython 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)] on win32...\n=> type exit() to close Python\n**Go to the folder Exudyn_git/main** (where setup.py lies) and choose the wheel in subdirectory main/dist according to your system (windows/UBUNTU), Python version (3.6 or 3.7) and 32 or 64 bits.\nFor Windows the installation commands may read (version number 1.0.20 may be different):\nPython 3.6, 32bit: pip install distexudyn-1.0.20-cp36-cp36m-win32.whl\nPython 3.6, 64bit: pip install distexudyn-1.0.20-cp36-cp36m-win_amd64.whl\nPython 3.7, 64bit: pip install distexudyn-1.0.20-cp37-cp37m-win_amd64.whl\nWork without installation and editing sys.path\nThe uncommon and old way (=> not recommended for Exudyn versions ge 1.0.0) is to use Python's sys module to link to your exudyn (previously WorkingRelease) directory, for example:\nimport sys\nsys.path.append('C:/DATA/cpp/EXUDYN_git/bin/EXUDYN32bitsPython36')\nThe folder EXUDYN32bitsPython36 needs to be adapted to the location of the according Exudyn package.\nBuild and install Exudyn under Windows 10?\nNote that there are a couple of pre-requisites, depending on your system and installed libraries. For Windows 10, the following steps proved to work:\ninstall your Anaconda distribution including Spyder\nclose all Python programs (e.g. Spyder, Jupyter, ...)\nrun an Anaconda prompt (may need to be run as administrator)\nif you cannot run Anaconda prompt directly, do:\nopen windows shell (cmd.exe) as administrator (START => search for cmd.exe => right click on app => 'run as administrator' if necessary)\ngo to your Scripts folder inside the Anaconda folder (e.g. C:\\ProgramData\\Anaconda\\Scripts)\nrun 'activate.bat'\ngo to 'main' of your cloned github folder of exudyn\nrun: python setup.py install\nread the output; if there are errors, try to solve them by installing appropriate modules\nYou can also create your own wheels, doing the above steps to activate the according Python version and then calling (requires installation of Microsoft Visual Studio; recommended: VS2017):\npython setup.py bdist_wheel\nThis will add a wheel in the dist folder.\nBuild and install Exudyn under Mac OS X?\nInstallation and building on Mac OS X is rarely tested, but first successful compilation including GLFW has been achieved. Requirements are an according Anaconda installation.\nTested configuration:\nMac OS X 10.11.6 'El Capitan', Mac Pro (2010), 3.33GHz 6-Core Intel Xeon, 4GB Memory\nAnaconda Navigator 1.9.7\nPython 3.7.0\nSpyder 3.3.6\nFor a compatible Mac OS X system, you can install the pre-compiled wheel (go to local directory). Go to the main/dist directory in your back terminal and type, e.g.,\npip install exudyn-1.0.218-cp37-cp37m-macosx_10_9_x86_64.whl %\nAlternatively, we tested on:\nMac OS 11.x 'Big Sur', Mac Mini (2021), Apple M1, 16GB Memory\nAnaconda (i368 based with Rosetta 2) with Python 3.8\nthis configuration is currently evaluated but showed general compatibility => pre-compiled wheel: exudyn-1.1.0-cp38-cp38-macosx_11_0_x86_64.whl\nbf Compile from source:\nIf you would like to compile from source, just use a bash terminal on your Mac, and do the following steps inside the main directory of your repository and type\npython setup.py bdist_wheel=> this compiles and takes approx.~5 minutes, depending on your machine => it may produce some errors, depending on your version; if there are some liker errors (saying that there is no '-framework Cocoa' and '-framework OpenGL', just go back in the terminal and copy everything from 'g++ ...' until the end of the last command '-mmacosx-verion-min...' and paste it into the terminal. Calling that again will finalize linking; then run again python setup.py bdist_wheel=> this now creates the wheel (if you want to distribute) in the dist folder alternatively just call\npython setup.py installto install exudyn\nThen just go to the pythonDev/Examples folder and run an example:\npython springDamperUserFunctionTest.py\nIf you have a new system, try to adapt setup.py accordingly, e.g., activating the -std=c++17 support. If there are other issues, we are happy to receive your detailed bug reports.\nNote that you need to run\nexudyn.StartRenderer()exudyn.DoRendererIdleTasks(-1)\nin order to interact with the render window, as there is only a single-threaded version available for Mac OS.\nBuild and install Exudyn under UBUNTU?\nHaving a new UBUNTU 18.04 standard installation (e.g. using a VM virtual box environment), the following steps need to be done (Python 3.6 is already installed on UBUNTU18.04, otherwise use sudo apt install python3) (see also the youtube video: https://www.youtube.com/playlist?list=PLZduTa9mdcmOh5KVUqatD9GzVg_jtl6fx):\nFirst update ...\nsudo apt-get update\nInstall necessary Python libraries and pip3; matplotlib andscipy are not required for installation but used in Exudyn examples:\nsudo dpkg --configure -a\nsudo apt install python3-pip\npip3 install numpy\npip3 install matplotlib\npip3 install scipy\nInstall pybind11 (needed for running the setup.py file derived from the pybind11 example):\npip3 install pybind11\nIf graphics is used (\\#define USE_GLFW_GRAPHICS in BasicDefinitions.h), you must install the according GLFW and OpenGL libs:\nsudo apt-get install freeglut3 freeglut3-dev\nsudo apt-get install mesa-common-dev\nsudo apt-get install libglfw3 libglfw3-dev\nsudo apt-get install libx11-dev xorg-dev libglew1.5 libglew1.5-dev libglu1-mesa libglu1-mesa-dev libgl1-mesa-glx libgl1-mesa-dev\nWith all of these libs, you can run the setup.py installer (go to Exudyn_git/main folder), which takes some minutes for compilation (the --user option is used to install in local user folder):\nsudo python3 setup.py install --user\nCongratulation! Now, run a test example (will also open an OpenGL window if successful):\npython3 pythonDev/Examples/rigid3Dexample.py\nYou can also create a UBUNTU wheel which can be easily installed on the same machine (x64), same operating system (UBUNTU18.04) and with same Python version (e.g., 3.6):\nsudo pip3 install wheelsudo python3 setup.py bdist_wheel\nKNOWN issues for linux builds:\nUsing WSL2 (Windows subsystem for linux), there occur some conflicts during build because of incompatible windows and linux file systems and builds will not be copied to the dist folder; workaround: go to explorer, right click on 'build' directory and set all rights for authenticated user to 'full access'\ncompiler (gcc,g++) conflicts: It seems that Exudyn works well on UBUNTU18.04 with the original Python 3.6.9 and gcc-7.5.0 version as well as with UBUNTU20.04 with Python 3.8.5 and gcc-9.3.0. Upgrading gcc on a linux system with Python 3.6 to, e.g., gcc-8.2 showed us a linker error when loading the Exudyn module in Python -- there are some common restriction using gcc versions different from those with which the Python version has been built. Starting python or python3 on your linux machine shows you the gcc version it had been build with.\nCheck your current gcc version with: gcc --version\nUninstall Exudyn\nTo uninstall exudyn under Windows, run (may require admin rights):\npip uninstall exudyn\nTo uninstall under UBUNTU, run:\nsudo pip3 uninstall exudyn\nIf you upgrade to a newer version, uninstall is usually not necessary!\nHow to install Exudyn and use the C++ source code (advanced)?\nExudyn is still under intensive development of core modules. There are several ways of using the code, but you cannot install Exudyn as compared to other executable programs and apps.\nIn order to make full usage of the C++ code and extending it, you can use:\nWindows / Microsoft Visual Studio 2017 and above:\nget the files from git\nput them into a local directory (recommended: C:/DATA/cpp/EXUDYN_git)\nstart main_sln.sln with Visual Studio\ncompile the code and run main/pythonDev/pytest.py example code\nadapt pytest.py for your applications\nextend the C++ source code\nlink it to your own code\nNOTE: on Linux systems, you mostly need to replace '/' with ''\nLinux, etc.: not fully supported yet; however, all external libraries are Linux-compatible and thus should run with minimum adaptation efforts.\nFurther notes\nGoals of Exudyn\nAfter the first development phase (2019-2020), it shall\nbe a small multibody library, which can be easily linked to other projects,\nallow to efficiently simulate small scale systems (compute 100000s time steps per second for systems with n_DOF<10),\nallow to efficiently simulate medium scaled systems for problems with n_DOF < 1,000,000,\nsafe and widely accessible module for Python,\nallow to add user defined objects in C++,\nallow to add user defined solvers in Python.\nFuture goals are:\nextend tests,\nadd more multi-threaded parallel computing techniques (first trials implemented, improvements planned: Q3 2021),\nadd vectorization,\nadd specific and advanced connectors/constraints (3D revolute joint and prismatic joint instead of generic joint, extended wheels, contact, control connector)\nmore interfaces for robotics,\nadd 3D beams,\nextend floating frame of reference formulation with modal reduction\nFor specific open issues, see trackerlog.html.\nRun a simple example in Spyder\nAfter performing the steps of the previous section, this section shows a simplistic model which helps you to check if Exudyn runs on your computer.\nIn order to start, run the Python interpreter Spyder. For the following example,\nopen myFirstExample.py from your EXUDYN32bitsPython36 (or any other directory according to your Python version) directory\nHereafter, press the play button or F5 in Spyder.\nIf successful, the IPython Console of Spyder will print something like:\nrunfile('C:/DATA/cpp/EXUDYN_git/main/bin/EXUDYN32bitsPython36/myFirstExample.py',\nwdir='C:/DATA/cpp/EXUDYN_git/main/bin/EXUDYN32bitsPython36')\n+++++++++++++++++++++++++++++++\nEXUDYN V1.0.1 solver: implicit second order time integration\nSTEP100, t = 1 sec, timeToGo = 0 sec, Nit/step = 1\nsolver finished after 0.0007824 seconds.\nIf you check your current directory (where myFirstExample.py lies), you will find a new file coordinatesSolution.txt, which contains the results of your computation (with default values for time integration). The beginning and end of the file should look like:\n#Exudyn generalized alpha solver solution file\n#simulation started=2019-11-14,20:35:12\n#columns contain: time, ODE2 displacements, ODE2 velocities, ODE2 accelerations, AE coordinates, ODE2 velocities\n#number of system coordinates [nODE2, nODE1, nAlgebraic, nData] = [2,0,0,0]\n#number of written coordinates [nODE2, nVel2, nAcc2, nODE1, nVel1, nAlgebraic, nData] = [2,2,2,0,0,0,0]\n#total columns exported (excl. time) = 6\n#number of time steps (planned) = 100\n#\n0,0,0,0,0,0.0001,0\n0.02,2e-08,0,2e-06,0,0.0001,0\n0.03,4.5e-08,0,3e-06,0,0.0001,0\n0.04,8e-08,0,4e-06,0,0.0001,0\n0.05,1.25e-07,0,5e-06,0,0.0001,0\n...\n0.96,4.608e-05,0,9.6e-05,0,0.0001,0\n0.97,4.7045e-05,0,9.7e-05,0,0.0001,0\n0.98,4.802e-05,0,9.8e-05,0,0.0001,0\n0.99,4.9005e-05,0,9.9e-05,0,0.0001,0\n1,5e-05,0,0.0001,0,0.0001,0\n#simulation finished=2019-11-14,20:35:12\n#Solver Info: errorOccurred=0,converged=1,solutionDiverged=0,total time steps=100,total Newton iterations=100,total Newton jacobians=100\nWithin this file, the first column shows the simulation time and the following columns provide solution of coordinates, their derivatives and Lagrange multipliers on system level. As expected, the x-coordinate of the point mass has constant acceleration a=f/m=0.001/10=0.0001, the velocity grows up to 0.0001 after 1 second and the point mass moves 0.00005 along the x-axis.\nTrouble shooting and FAQ\nTrouble shooting\nPython import errors:\nSometimes the Exudyn module cannot be loaded into Python. Typical error messages if Python versions are not compatible are:\nTraceback (most recent call last):\nFile \"<ipython-input-14-df2a108166a6>\", line 1, in <module>\nimport exudynCPP\nImportError: Module use of python36.dll conflicts with this version of Python.\nTypical error messages if 32/64 bits versions are mixed:\nTraceback (most recent call last):\nFile \"<ipython-input-2-df2a108166a6>\", line 1, in <module>\nimport exudynCPP\nImportError: DLL load failed: \\%1 is not a valid Win32 application.\nThere are several reasons and workarounds:\n=> You mixed up 32 and 64 bits version (see below)\n=> You are using an exudyn version for Python x_1.y_1 (e.g., 3.6.z_1) different from the Python x_2.y_2 version in your Anaconda (e.g., 3.7.z_2); note that x_1=x_2 and y_1=y_2 must be obeyed while z_1 and z_2 may be different\nModuleNotFoundError: No module named 'exudynCPP':\n=> A known reason is that your CPU does not support AVX2, while Exudyn is compiled with the AVX2 option (not support AVX2, e.g., Intel Celeron G3900, Intel core 2 quad q6600, Intel Pentium Gold G5400T; check the system settings of your computer to find out the processor type; typical CPU manufacturer pages or Wikipedia provide information on this).\n=> workaround to solve the AVX problem: use the Python 3.6 32bits version, which is compiled without AVX2; you can also compile for your specific Python version without AVX if you adjust the setup.py file in the main folder.\n=> The ModuleNotFoundError may also happen if something went wrong during installation (paths, problems with Anaconda, ..) => very often a new installation of Anaconda and Exudyn helps.\nTypical Python errors:\nTypical Python syntax error with missing braces:\nFile \"C:\\DATA\\cpp\\EXUDYN_git\\main\\pythonDev\\Examples\\springDamperTutorial.py\", line 42\nnGround=mbs.AddNode(NodePointGround(referenceCoordinates = [0,0,0]))\n^\nSyntaxError: invalid syntax\n=> such an error points to the line of your code (line 42), but in fact the error may have been caused in previous code, such as in this case there was a missing brace in the line 40, which caused the error:\n38 n1=mbs.AddNode(Point(referenceCoordinates = [L,0,0],\n39 initialCoordinates = [u0,0,0],\n40 initialVelocities= [v0,0,0])\n41 #ground node\n42 nGround=mbs.AddNode(NodePointGround(referenceCoordinates = [0,0,0]))\n43\nTypical Python import error message on Linux / UBUNTU if Python modules are missing:\nPython WARNING [file '/home/johannes/.local/lib/python3.6/site-packages/exudyn/solver.py', line 236]:\nError when executing process ShowVisualizationSettingsDialog':\nModuleNotFoundError: No module named 'tkinter'\n=> see installation instructions to install missing Python modules, theDoc.pdf.\nTypical solver errors:\nSolveDynamic or SolveStatic terminated due to errors:\n=> use flag showHints = True in SolveDynamic or SolveStatic\nVery simple example without loads leads to error: SolveDynamic or SolveStatic terminated due to errors:\n=> if you do not add loads to the system and if there are no forces, the residual nearly gives 0 (due to round off errors). The Newton solver tries to reduce the error by the factor given in simulationSettings.staticSolver.newton.relativeTolerance (for static solver), which is not possible for 0 residual. The absolute tolerance is helping out as a lower bound for the error, given in simulationSettings.staticSolver.newton.absoluteTolerance (for static solver), which is by default rather low (1e-10). Increasing this value helps to solve unloaded problems. Nevertheless, you should usually set this tolerance as low as possible because otherwise, your solution may become inaccurate.\nTypical solver error due to redundant constraints or missing inertia terms, could read as follows:\n=========================================\nSYSTEM ERROR [file 'C:\\ProgramData\\Anaconda3_64b37\\lib\\site-packages\\exudyn\\solver.py', line 207]:\nCSolverBase::Newton: System Jacobian seems to be singular / not invertible!\ntime/load step #1, time = 0.0002\ncausing system equation number (coordinate number) = 42\n=========================================\n=> this solver error shows that equation 42 is not solvable. The according coordinate is shown later in such an error message:\n...\nThe causing system equation 42 belongs to a algebraic variable (Lagrange multiplier)\nPotential object number(s) causing linear solver to fail: [7]\nobject 7, name='object7', type=JointGeneric\n=> object 7 seems to be the reason, possibly there are too much (joint) constraints applied to your system, check this object.\n=> show typical REASONS and SOLUTIONS, by using showHints=True in exu.SolveDynamic(...) or exu.SolveStatic(...)\n=> You can also highlight object 7 by using the following code in the iPython console:\nexu.StartRenderer()\nHighlightItem(SC,mbs,7)\nwhich draws the according object in red and others gray/transparent (but sometimes objects may be hidden inside other objects!). See the command's description for further options, e.g., to highlight nodes.\nTypical solver error if Newton does not converge:\n+++++++++++++++++++++++++++++++\nEXUDYN V1.0.200 solver: implicit second order time integration\nNewton (time/load step #1): convergence failed after 25 iterations; relative error = 0.079958, time = 2\nNewton (time/load step #1): convergence failed after 25 iterations; relative error = 0.0707764, time = 1\nNewton (time/load step #1): convergence failed after 25 iterations; relative error = 0.0185745, time = 0.5\nNewton (time/load step #2): convergence failed after 25 iterations; relative error = 0.332953, time = 0.5\nNewton (time/load step #2): convergence failed after 25 iterations; relative error = 0.0783815, time = 0.375\nNewton (time/load step #2): convergence failed after 25 iterations; relative error = 0.0879718, time = 0.3125\nNewton (time/load step #2): convergence failed after 25 iterations; relative error = 2.84704e-06, time = 0.28125\nNewton (time/load step #3): convergence failed after 25 iterations; relative error = 1.9894e-07, time = 0.28125\nSTEP348, t = 20 sec, timeToGo = 0 sec, Nit/step = 7.00575\nsolver finished after 0.258349 seconds.\n=> this solver error is caused, because the nonlinear system cannot be solved using Newton's method.\n=> the static or dynamic solver by default tries to reduce step size to overcome this problem, but may fail finally (at minimum step size).\n=> possible reasons are: too large time steps (reduce step size by using more steps/second), inappropriate initial conditions, or inappropriate joints or constraints (remove joints to see if they are the reason), usually within a singular configuration. Sometimes a system may be just unsolvable in the way you set it up.\nTypical solver error if (e.g., syntax) error in user function (output may be very long, read always message on top!):\n=========================================\nSYSTEM ERROR [file 'C:\\ProgramData\\Anaconda3_64b37\\lib\\site-packages\\exudyn\\solver.py', line 214]:\nError in python USER FUNCTION 'LoadCoordinate::loadVectorUserFunction' (referred line number my be wrong!):\nNameError: name 'sin' is not defined\nAt:\nC:\\DATA\\cpp\\DocumentationAndInformation\\tests\\springDamperUserFunctionTest.py(48): Sweep\nC:\\DATA\\cpp\\DocumentationAndInformation\\tests\\springDamperUserFunctionTest.py(54): userLoad\nC:\\ProgramData\\Anaconda3_64b37\\lib\\site-packages\\exudyn\\solver.py(214): SolveDynamic\nC:\\DATA\\cpp\\DocumentationAndInformation\\tests\\springDamperUserFunctionTest.py(106): <module>\nC:\\ProgramData\\Anaconda3_64b37\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py(377): exec_code\nC:\\ProgramData\\Anaconda3_64b37\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py(476): runfile\n<ipython-input-14-323569bebfb4>(1): <module>\nC:\\ProgramData\\Anaconda3_64b37\\lib\\site-packages\\IPython\\core\\interactiveshell.py(3331): run_code\n...\n...\n; check your python code!\n=========================================\nSolver stopped! use showHints=True to show helpful information\n=> this indicates an error in the user function LoadCoordinate::loadVectorUserFunction, because sin function has not been defined (must be imported, e.g., from math). It indicates that the error occurred in line 48 in springDamperUserFunctionTest.py within function Sweep, which has been called from function userLoad, etc.\nFAQ\nSome frequently asked questions:\nWhen importing Exudyn in Python (windows) I get an error\n=> see trouble shooting instructions above!\nI do not understand the Python errors -- how can I find the reason of the error or crash?\n=> Read trouble shooting section above!\n=> First, you should read all error messages and warnings: from the very first to the last message. Very often, there is a definite line number which shows the error. Note, that if you are executing a string (or module) as a Python code, the line numbers refer to the local line number inside the script or module.\n=> If everything fails, try to execute only part of the code to find out where the first error occurs. By omiting parts of the code, you should find the according source of the error.\n=> If you think, it is a bug: send an email with a representative code snippet, version, etc.to `` reply.exudyn@gmail.com``\nSpyder console hangs up, does not show error messages, ...:\n=> very often a new start of Spyder helps; most times, it is sufficient to restart the kernel or to just press the 'x' in your IPython console, which closes the current session and restarts the kernel (this is much faster than restarting Spyder)\n=> restarting the IPython console also brings back all error messages\nWhere do I find the '.exe' file?\n=> Exudyn is only available via the Python interface as a module 'exudyn', the C++ code being inside of exudynCPP.pyd, which is located in the exudyn folder where you installed the package. This means that you need to run Python (best: Spyder) and import the Exudyn module.\nI get the error message 'check potential mixing of different (object, node, marker, ...) indices', what does it mean?\n=> probably you used wrong item indexes, see beginning of command interface in theDoc.pdf.\n=> E.g., an object number oNum = mbs.AddObject(...) is used at a place where a NodeIndex is expected, e.g., mbs.AddObject(MassPoint(nodeNumber=oNum, ...))\n=> Usually, this is an ERROR in your code, it does not make sense to mix up these indexes!\n=> In the exceptional case, that you want to convert numbers, see beginning of theDoc.pdf.\nWhy does type auto completion not work for mbs (MainSystem)?\n=> UPDATE 2020-06-01: with Spyder 4, using Python 3.7, type auto completion works much better, but may find too many completions.\n=> most Python environments (e.g., with Spyder 3) only have information up to the first sub-structure, e.g., SC=exu.SystemContainer() provides full access to SC in the type completion, but mbs=SC.AddSystem() is at the second sub-structure of the module and is not accessible.\n=> WORKAROUND: type mbs=MainSystem() before the mbs=SC.AddSystem() command and the interpreter will know what type mbs is. This also works for settings, e.g., simulation settings 'Newton'.\nHow to add graphics?\n=> Graphics (lines, text, 3D triangular / STL mesh) can be added to all BodyGraphicsData items in objects. Graphics objects which are fixed with the background can be attached to a ObjectGround object. Moving objects must be attached to the BodyGraphicsData of a moving body. Other moving bodies can be realized, e.g., by adding a ObjectGround and changing its reference with time. Furthermore, ObjectGround allows to add fully user defined graphics.\nIn GenerateStraightLineANCFCable2D\n=> coordinate constraints can be used to constrain position and rotation, e.g., fixedConstraintsNode0 = [1,1,0,1] for a beam aligned along the global x-axis;\n=> this does not work for beams with arbitrary rotation in reference configuration, e.g.,\n45textdegree. Use a GenericJoint with a rotationMarker instead.\nWhat is the difference between MarkerBodyPosition and MarkerBodyRigid?\n=> Position markers (and nodes) do not have information on the orientation (rotation). For that reason, there is a difference between position based and rigid-body based markers. In case of a rigid body attached to ground with a SpringDamper, you can use both, MarkerBodyPosition or MarkerBodyRigid, markers. For a prismatic joint, you will need a MarkerBodyRigid.\nI get an error in exu.SolveDynamic(mbs, ...) OR in exu.SolveStatic(mbs, ...) but no further information -- how can I solve it?\n=> Typical time integration errors may look like:\nFile \"C:/DATA/cpp/EXUDYN_git/main/pythonDev/...<file name>\", line XXX, in <module>\nsolver.SolveSystem(...)\nSystemError: <built-in method SolveSystem of PyCapsule object at 0x0CC63590> returned a result with an error set\n=> The pre-checks, which are performed to enable a crash-free simulation are insufficient for your model\n=> As a first try, restart the IPython console in order to get all error messages, which may be blocked due to a previous run of Exudyn.\n=> Very likely, you are using Python user functions inside Exudyn : They lead to an internal Python error, which is not always catched by Exudyn ; e.g., a load user function UFload(mbs,~t,~load), which tries to access component load[3] of a load vector with 3 components will fail internally;\n=> Use the print(...) command in Python at many places to find a possible error in user functions (e.g., put print(\"Start user function XYZ\") at the beginning of every user function; test user functions from iPython console\n=> It is also possible, that you are using inconsistent data, which leads to the crash. In that case, you should try to change your model: omit parts and find out which part is causing your error\n=> see also I do not understand the Python errors -- how can I find the cause?\nWhy can't I get the focus of the simulation window on startup (render window hidden)?\n=> Starting Exudyn out of Spyder might not bring the simulation window to front, because of specific settings in Spyder(version 3.2.8), e.g., Tools=>Preferences=>Editor=>Advanced settings: uncheck 'Maintain focus in the Editor after running cells or selections'; Alternatively, set SC.visualizationSettings.window.alwaysOnTop=True before starting the renderer with exu.StartRenderer()\nOverview on Exudyn\nModule structure\nThis section will show:\nOverview of modules\nConventions: dimension of nodes, objects and vectors\nCoordinates: reference coordinates and displacements\nNodes, Objects, Markers and Loads\nFor an introduction to the solvers, see theDoc.pdf.\nOverview of modules\nCurrently, the module structure is simple:\nPython parts:\nitemInterface: contains the interface, which transfers python classes (e.g., of a NodePoint) to dictionaries that can be understood by the C++ module\nexudynUtilities: constains helper classes in Python, which allows simpler working with Exudyn\nC++ parts, see Figs.[theDoc.pdf] and [theDoc.pdf]:\nexudyn: on this level, there are just very few functions: SystemContainer(), StartRenderer(), StopRenderer(), GetVersionString(), SolveStatic(...), SolveDynamic(...), ... as well as system and user variable dictionaries exudyn.variables and exudyn.sys\nSystemContainer: contains the systems (most important), solvers (static, dynamics, ...), visualization settings\nmbs: system created with mbs = SC.AddSystem(), this structure contains everything that defines a solvable multibody system; a large set of nodes, objects, markers, loads can added to the system, see theDoc.pdf;\nmbs.systemData: contains the initial, current, visualization, ... states of the system and holds the items, see [figure in theDoc.pdf]\nConventions: items, indexes, coordinates\nIn this documentation, we will use the term item to identify nodes, objects, markers, loads and sensors:\nitem in node, object, marker, load, sensor\nIndexes: arrays and vector starting with 0:\nAs known from Python, all indexes of arrays, vectors, matrices, ...are starting with 0. This means that the first component of the vector v=[1,2,3] is accessed with v[0] in Python (and also in the C++ part of Exudyn ). The range is usually defined as range(0,3), in which '3' marks the index after the last valid component of an array or vector.\n**Dimensionality of objects and vectors: **\nac2D vs.ac3D\nAs a convention, quantities in Exudyn are 3D, such as nodes, objects, markers, loads, measured quantities, etc. For that reason, we denote planar nodes, objects, etc.with the suffix 2D, but 3D objects do not get this suffix.\nOutput and input to objects, markers, loads, etc.is usually given by 3D vectors (or matrices), such as (local) position, force, torque, rotation, etc. However, initial and reference values for nodes depend on their dimensionality. As an example, consider a NodePoint2D:\nreferenceCoordinates is a 2D vector (but could be any dimension in general nodes)\nmeasuring the current position of NodePoint2D gives a 3D vector\nwhen attaching a MarkerNodePosition and a LoadForceVector, the force will be still a 3D vector\nFurthermore, the local position in 2D objects is provided by a 3D vector. Usually, the dimensionality is given in the reference manual. User errors in the dimensionality will be usually detected either by the python interface (i.e., at the time the item is created) or by the system-preprocessor\nItems: Nodes, Objects, Loads, Markers, Sensors, ...\nIn this section, the most important part of Exudyn are provided. An overview of the interaction of the items is given in [figure in theDoc.pdf]\nNodes\nNodes provide the coordinates (and the degrees of freedom) to the system. They have no mass, stiffness or whatsoever assigned. Without nodes, the system has no unknown coordinates. Adding a node provides (for the system unknown) coordinates. In addition we also need equations for every nodal coordinate -- otherwise the system cannot be computed (NOTE: this is currently not checked by the preprocessor).\nObjects\nObjects are 'computational objects' and they provide equations to your system. Objects often provide derivatives and have measurable quantities (e.g. displacement) and they provide access, which can be used to apply, e.g., forces. Some of this functionality is only available in C++, but not in Python.\nObjects can be a:\ngeneral object (e.g.a controller, user defined object, ...; no example yet)\nbody: has a mass or mass distribution; markers can be placed on bodies; loads can be applied; constraints can be attached via markers; bodies can be:\nground object: has no nodes\nsimple body: has one node (e.g. mass point, rigid body)\nfinite element and more complicated body (e.g. FFRF-object): has more than one node\nconnector: uses markers to connect nodes and/or bodies; adds additional terms to system equations either based on stiffness/damping or with constraints (and Lagrange multipliers). Possible connectors:\nalgebraic constraint (e.g. constrain two coordinates: q_1 = q_2)\nclassical joint\nspring-damper or penalty constraint\nMarkers\nMarkers are interfaces between objects/nodes and constraints/loads. A constraint (which is also an object) or load cannot act directly on a node or object without a marker. As a benefit, the constraint or load does not need to know whether it is applied, e.g., to a node or to a local position of a body.\nTypical situations are:\nNode -- Marker -- Load\nNode -- Marker -- Constraint (object)\nBody(object) -- Marker -- Load\nBody1 -- Marker1 -- Joint(object) -- Marker2 -- Body2\nLoads\nLoads are used to apply forces and torques to the system. The load values are static values. However, you can use Python functionality to modify loads either by linearly increasing them during static computation or by using the 'mbs.SetPreStepUserFunction(...)' structure in order to modify loads in every integration step depending on time or on measured quantities (thus, creating a controller).\nSensors\nSensors are only used to measure output variables (values) in order to simpler generate the requested output quantities. They have a very weak influence on the system, because they are only evaluated after certain solver steps as requested by the user.\nReference coordinates and displacements\nNodes usually have separated reference and initial quantities. Here, referenceCoordinates are the coordinates for which the system is defined upon creation. Reference coordinates are needed, e.g., for definition of joints and for the reference configuration of finite elements. In many cases it marks the undeformed configuration (e.g., with finite elements), but not, e.g., for ObjectConnectorSpringDamper, which has its own reference length.\nInitial displacement (or rotation) values are provided separately, in order to start a system from a configuration different from the reference configuration. As an example, the initial configuration of a NodePoint is given by referenceCoordinates + initialCoordinates, while the initial state of a dynamic system additionally needs initialVelocities.\nExudyn Basics\nThis section will show:\nInteraction with the Exudyn module\nSimulation settings\nVisualization settings\nGenerating output and results\nGraphics pipeline\nGenerating animations\nInteraction with the Exudyn module\nIt is important that the Exudyn module is basically a state machine, where you create items on the C++ side using the Python interface. This helps you to easily set up models using many other Python modules (numpy, sympy, matplotlib, ...) while the computation will be performed in the end on the C++ side in a very efficient manner.\nWhere do objects live?\nWhenever a system container is created with SC = exu.SystemContainer(), the structure SC becomes a variable in the Python interpreter, but it is managed inside the C++ code and it can be modified via the Python interface. Usually, the system container will hold at least one system, usually called mbs. Commands such as mbs.AddNode(...) add objects to the system mbs. The system will be prepared for simulation by mbs.Assemble() and can be solved (e.g., using exu.SolveDynamic(...)) and evaluated hereafter using the results files. Using mbs.Reset() will clear the system and allows to set up a new system. Items can be modified (ModifyObject(...)) after first initialization, even during simulation.\nSimulation settings\nThe simulation settings consists of a couple of substructures, e.g., for solutionSettings, staticSolver, timeIntegration as well as a couple of general options -- for details see Sections [theDoc.pdf] -- [theDoc.pdf].\nSimulation settings are needed for every solver. They contain solver-specific parameters (e.g., the way how load steps are applied), information on how solution files are written, and very specific control parameters, e.g., for the Newton solver.\nThe simulation settings structure is created with\nsimulationSettings = exu.SimulationSettings()\nHereafter, values of the structure can be modified, e.g.,\ntEnd = 10 #10 seconds of simulation time:\nh = 0.01 #step size (gives 1000 steps)\nsimulationSettings.timeIntegration.endTime = tEnd\n#steps for time integration must be integer:\nsimulationSettings.timeIntegration.numberOfSteps = int(tEnd/h)\n#assigns a new tolerance for Newton's method:\nsimulationSettings.timeIntegration.newton.relativeTolerance = 1e-9\n#write some output while the solver is active (SLOWER):\nsimulationSettings.timeIntegration.verboseMode = 2\n#write solution every 0.1 seconds:\nsimulationSettings.solutionSettings.solutionWritePeriod = 0.1\n#use sparse matrix storage and solver (package Eigen):\nsimulationSettings.linearSolverType = exu.LinearSolverType.EigenSparse\nGenerating output and results\nThe solvers provide a number of options in solutionSettings to generate a solution file. As a default, exporting solution to the solution file is activated with a writing period of 0.01 seconds.\nTypical output settings are:\n#create a new simulationSettings structure:\nsimulationSettings = exu.SimulationSettings()\n#activate writing to solution file:\nsimulationSettings.solutionSettings.writeSolutionToFile = True\n#write results every 1ms:\nsimulationSettings.solutionSettings.solutionWritePeriod = 0.001\n#assign new filename to solution file\nsimulationSettings.solutionSettings.coordinatesSolutionFileName= \"myOutput.txt\"\n#do not export certain coordinates:\nsimulationSettings.solutionSettings.exportDataCoordinates = False\nVisualization settings\nVisualization settings are used for user interaction with the model. E.g., the nodes, markers, loads, etc., can be visualized for every model. There are default values, e.g., for the size of nodes, which may be inappropriate for your model. Therefore, you can adjust those parameters. In some cases, huge models require simpler graphics representation, in order not to slow down performance -- e.g., the number of faces to represent a cylinder should be small if there are 10000s of cylinders drawn. Even computation performance can be slowed down, if visualization takes lots of CPU power. However, visualization is performed in a separate thread, which usually does not influence the computation exhaustively. Details on visualization settings and its substructures are provided in Sections [theDoc.pdf] -- [theDoc.pdf].\nThe visualization settings structure can be accessed in the system container SC (access per reference, no copying!), accessing every value or structure directly, e.g.,\nSC.visualizationSettings.nodes.defaultSize = 0.001 #draw nodes very small\n#change openGL parameters; current values can be obtained from SC.GetRenderState()\n#change zoom factor:\nSC.visualizationSettings.openGL.initialZoom = 0.2\n#set the center point of the scene (can be attached to moving object):\nSC.visualizationSettings.openGL.initialCenterPoint = [0.192, -0.0039,-0.075]\n#turn of auto-fit:\nSC.visualizationSettings.general.autoFitScene = False\n#change smoothness of a cylinder:\nSC.visualizationSettings.general.cylinderTiling = 100\n#make round objects flat:\nSC.visualizationSettings.openGL.shadeModelSmooth = False\n#turn on coloured plot, using y-component of displacements:\nSC.visualizationSettings.contour.outputVariable = exu.OutputVariableType.Displacement\nSC.visualizationSettings.contour.outputVariableComponent = 1 #0=x, 1=y, 2=z\nStoring the model view\nThere is a simple way to store the current view (zoom, centerpoint, orientation, etc.) by using SC.GetRenderState() and SC.SetRenderState(). A simple way is to reload the stored render state (model view) after simulating your model once at the end of the simulation ( note that visualizationSettings.general.autoFitScene should be set False if you want to use the stored zoom factor):\nimport exudyn as exu\nSC=exu.SystemContainer()\nSC.visualizationSettings.general.autoFitScene = False #prevent from autozoom\nexu.StartRenderer()\nif 'renderState' in exu.sys:\nSC.SetRenderState(exu.sys['renderState'])\n#+++++++++++++++\n#do simulation here and adjust model view settings with mouse\n#+++++++++++++++\n#store model view for next run:\nStopRenderer() #stores render state in exu.sys['renderState']\nAlternatively, you can obtain the current model view from the console after a simulation, e.g.,\nIn[1] : SC.GetRenderState()\nOut[1]:\n'centerPoint': [1.0, 0.0, 0.0],\n'maxSceneSize': 2.0,\n'zoom': 1.0,\n'currentWindowSize': [1024, 768],\n'modelRotation': [[ 0.34202015, 0. , 0.9396926 ],\n[-0.60402274, 0.76604444, 0.21984631],\n[-0.7198463 , -0.6427876 , 0.26200265]])\nwhich contains the last state of the renderer. Now copy the output and set this with SC.SetRenderState in your Python code to have a fixed model view in every simulation (SC.SetRenderState AFTER exu.StartRenderer()):\nSC.visualizationSettings.general.autoFitScene = False #prevent from autozoom\nexu.StartRenderer()\nrenderState='centerPoint': [1.0, 0.0, 0.0],\n'maxSceneSize': 2.0,\n'zoom': 1.0,\n'currentWindowSize': [1024, 768],\n'modelRotation': [[ 0.34202015, 0. , 0.9396926 ],\n[-0.60402274, 0.76604444, 0.21984631],\n[-0.7198463 , -0.6427876 , 0.26200265]])\nSC.SetRenderState(renderState)\n#.... further code for simulation here\nGraphics pipeline\nThere are basically two loops during simulation, which feed the graphics pipeline. The solver runs a loop:\ncompute new step\nfinish computation step; results are in current state\ncopy current state to visualization state (thread safe)\nsignal graphics pipeline that new visualization data is available\nThe openGL graphics thread (=separate thread) runs the following loop:\nrender openGL scene with a given graphicsData structure (containing lines, faces, text, ...)\ngo idle for some milliseconds\ncheck if openGL rendering needs an update (e.g. due to user interaction) => if update is needed, the visualization of all items is updated -- stored in a graphicsData structure)\ncheck if new visualization data is available and the time since last update is larger than a presribed value, the graphicsData structure is updated with the new visualization state\nGraphics user Python functions\nThere are some user functions in order to customize drawing:\nYou can assign graphicsData to the visualization to most bodies, such as rigid bodies in order to change the shape. Graphics can also be imported from STL files (GraphicsDataFromSTLfileTxt).\nSome objects, e.g., ObjectGenericODE2 or ObjectRigidBody, provide customized a function graphicsDataUserFunction. This user function just returns a list of GraphicsData, see theDoc.pdf. With this function you can change the shape of the body in every step of the computation.\nSpecifically, the graphicsDataUserFunction in ObjectGround can be used to draw any moving background in the scene.\nNote that all kinds of graphicsUserPythonFunctions need to be called from the main (=computation) process as Python functions may not be called from separate threads (GIL). Therefore, the computation thread is interrupted to execute the graphicsDataUserFunction between two time steps, such that the graphics Python user function can be executed. There is a timeout variable for this interruption of the computation with a warning if scenes get too complicated.\nColor and RGBA\nMany functions and objects include color information. In order to allow transparency, all colors contain a list of 4 RGBA values, all values being in the range [0..1]:\nred (R) channel\ngreen (G) channel\nblue (B) channel\nalpha (A) value, representing transparency (A=0: fully transparent, A=1: solid)\nE.g., red color with no transparency is obtained by the color=[1,0,0,1]. Color predefinitions are found in exudynGraphicsDataUtilities.py, e.g., color4red or color4steelblue as well a list of 10 colors color4list, which is convenient to be used in a loop creating objects.\nCamera following objects and interacting with model view\nFor some models, it may be advantageous to track the translation and/or rotation of certain bodies, e.g., for cars, (wheeled) robots or bicycles. To do so, the current render state (SC.GetRenderState(), SC.SetRenderState(...)) can be obtained and modified, in order to always follow a certain position. As this needs to be done during redraw of every frame, it is conveniently done in a graphicsUserFunction, e.g., within the ground body. This is shown in the following example, in which mbs.variables['nTrackNode'] is a node number to be tracked:\n#mbs.variables['nTrackNode'] contains node number\ndef UFgraphics(mbs, objectNum):\nn = mbs.variables['nTrackNode']\np = mbs.GetNodeOutput(n,exu.OutputVariableType.Position,\nconfiguration=exu.ConfigurationType.Visualization)\nrs=SC.GetRenderState() #get current render state\nA = np.array(rs['modelRotation'])\np = A.T @ p #transform point into model view coordinates\nrs['centerPoint']=[p[0],p[1],p[2]]\nSC.SetRenderState(rs) #modify render state\nreturn []\n#add object with graphics user function\noGround2 = mbs.AddObject(ObjectGround(visualization=\nVObjectGround(graphicsDataUserFunction=UFgraphics)))\n#.... further code for simulation here\nSolution viewer\nExudyn offers a convenient WYSIWYS -- 'What you See is What you Simulate' interface, showing you the computation results during simulation. If you are running large models, it may be more convenient to watch results after simulation has been finished. For this, you can use\nutilities.AnimateSolution, see Section [theDoc.pdf]\ninteractive.SolutionViewer, see Section [theDoc.pdf]\ninteractive.AnimateModes, lets you view the animation of computed modes, see Section [theDoc.pdf]\nThe function AnimateSolution allows to directly visualize the stored solution for according stored time frames. The SolutionViewer adds a tkinter interactive dialog, which lets you interact with the model ('Player'). In both methods AnimateSolution and SolutionViewer, the solution needs to be loaded with LoadSolutionFile('coordinatesSolution.txt'), where 'coordinatesSolution.txt' represents the stored solution file, see\nexu.SimulationSettings().solutionSettings.coordinatesSolutionFileName\nYou can call the SolutionViewer either in the model, or at the command line / IPython to load a previous solution (belonging to the same mbs underlying the solution!):\nfrom exudyn.interactive import SolutionViewer\nsol = LoadSolutionFile('coordinatesSolution.txt')\nSolutionViewer(mbs, sol)\nAlternatively, you can just reload the last stored solution (according to your simulationSettings):\nfrom exudyn.interactive import SolutionViewer\nSolutionViewer(mbs)\nAn example for the SolutionViewer is integrated into the Examples/ directory, see solutionViewerTest.py.\nGenerating animations\nIn many dynamics simulations, it is very helpful to create animations in order to better understand the motion of bodies. Specifically, the animation can be used to visualize the model much slower or faster than the model is computed.\nAnimations are created based on a series of images (frames, snapshots) taken during simulation. It is important, that the current view is used to record these images -- this means that the view should not be changed during the recording of images. To turn on recording of images during solving, set the following flag to a positive value\nsimulationSettings.solutionSettings.recordImagesInterval = 0.01\nwhich means, that after every 0.01 seconds of simulation time, an image of the current view is taken and stored in the directory and filename (without filename ending) specified by\nSC.visualizationSettings.exportImages.saveImageFileName = \"myFolder/frame\"\nBy default, a consecutive numbering is generated for the image, e.g., 'frame0000.tga, frame0001.tga,...'. Note that '.tga' files contain raw image data and therefore can become very large.\nTo create animation files, an external tool FFMPEG is used to efficiently convert a series of images into an animation. => see theDoc.pdf !\nC++ Code\nThis section covers some information on the C++ code. For more information see the Open source code and use doxygen.\nExudyn was developed for the efficient simulation of flexible multi-body systems. Exudyn was designed for rapid implementation and testing of new formulations and algorithms in multibody systems, whereby these algorithms can be easily implemented in efficient C++ code. The code is applied to industry-related research projects and applications.\nFocus of the C++ code\nFour principles:\ndeveloper-friendly\nerror minimization\nefficiency\nuser-friendliness\nThe focus is therefore on:\nA developer-friendly basic structure regarding the C++ class library and the possibility to add new components.\nThe basic libraries are slim, but extensively tested; only the necessary components are available\nComplete unit tests are added to new program parts during development; for more complex processes, tests are available in Python\nIn order to implement the sometimes difficult formulations and algorithms without errors, error avoidance is always prioritized.\nTo generate efficient code, classes for parallelization (vectorization and multithreading) are provided. We live the principle that parallelization takes place on multi-core processors with a central main memory, and thus an increase in efficiency through parallelization is only possible with small systems, as long as the program runs largely in the cache of the processor cores. Vectorization is tailored to SIMD commands as they have Intel processors, but could also be extended to GPGPUs in the future.\nThe user interface (Python) provides a 1:1 image of the system and the processes running in it, which can be controlled with the extensive possibilities of Python.\nC++ Code structure\nThe functionality of the code is based on systems (MainSystem/CSystem) representing the multibody system or similar physical systems to be simulated. Parts of the core structure of Exudyn are:\nCSystem / MainSystem: a multibody system which consists of nodes, objects, markers, loads, etc.\nSystemContainer: holds a set of systems; connects to visualization (container)\nnode: used to hold coordinates (unknowns)\n(computational) object: leads to equations, using nodes\nmarker: defines a consistent interface to objects (bodies) and nodes; write access ('AccessFunction') -- provides jacobian and read access ('OutputVariable')\nload: acts on an object or node via a marker\ncomputational objects: efficient objects for computation = bodies, connectors, connectors, loads, nodes, ...\nvisualization objects: interface between computational objects and 3D graphics\nmain (manager) objects: do all tasks (e.g. interface to visualization objects, GUI, python, ...) which are not needed during computation\nstatic solver, kinematic solver, time integration\npython interface via pybind11; items are accessed with a dictionary interface; system structures and settings read/written by direct access to the structure (e.g. SimulationSettings, VisualizationSettings)\ninterfaces to linear solvers; future: optimizer, eigenvalue solver, ... (mostly external or in python)\nC++ Code: Modules\nThe following internal modules are used, which are represented by directories in main/src:\nAutogenerated: item (nodes, objects, markers and loads) classes split into main (management, python connection), visualization and computation\nGraphics: a general data structure for 2D and 3D graphical objects and a tiny openGL visualization; linkage to GLFW\nLinalg: Linear algebra with vectors and matrices; separate classes for small vectors (SlimVector), large vectors (Vector and ResizableVector), vectors without copying data (LinkedDataVector), and vectors with constant size (ConstVector)\nMain: mainly contains SystemContainer, System and ObjectFactory\nObjects: contains the implementation part of the autogenerated items\nPymodules: manually created libraries for linkage to python via pybind; remaining linking to python is located in autogenerated folder\npythonGenerator: contains python files for automatic generation of C++ interfaces and python interfaces of items;\nSolver: contains all solvers for solving a CSystem\nSystem: contains core item files (e.g., MainNode, CNode, MainObject, CObject, ...)\nTests: files for testing of internal linalg (vector/matrix), data structure libraries (array, etc.) and functions\nUtilities: array structures for administrative/managing tasks (indexes of objects ... bodies, forces, connectors, ...); basic classes with templates and definitions\nThe following main external libraries are linked to Exudyn:\nLEST: for testing of internal functions (e.g. linalg)\nGLFW: 3D graphics with openGL; cross-platform capabilities\nEigen: linear algebra for large matrices, linear solvers, sparse matrices and link to special solvers\npybind11: linking of C++ to python\nCode style and conventions\nThis section provides general coding rules and conventions, partly applicable to the C++ and python parts of the code. Many rules follow common conventions (e.g., google code style, but not always -- see notation):\nwrite simple code (no complicated structures or uncommon coding)\nwrite readable code (e.g., variables and functions with names that represent the content or functionality; AVOID abbreviations)\nput a header in every file, according to Doxygen format\nput a comment to every (global) function, member function, data member, template parameter\nALWAYS USE curly brackets for single statements in 'if', 'for', etc.; example: if (i<n) i += 1;\nuse Doxygen-style comments (use '//!' Qt style and '@ date' with '@' instead of '' for commands)\nuse Doxygen (with preceeding '@') 'test' for tests, 'todo' for todos and 'bug' for bugs\nUSE 4-spaces-tab\nuse C++11 standards when appropriate, but not exhaustively\nONE class ONE file rule (except for some collectors of single implementation functions)\nadd complete unit test to every function (every file has link to LEST library)\navoid large classes (>30 member functions; > 15 data members)\nsplit up god classes (>60 member functions)\nmark changed code with your name and date\nREPLACE tabs by spaces: Extras->Options->C/C++->Tabstopps: tab stopp size = 4 (=standard) + KEEP SPACES=YES\nNotation conventions\nThe following notation conventions are applied (no exceptions!):\nuse lowerCamelCase for names of variables (including class member variables), consts, c-define variables, ...; EXCEPTION: for algorithms following formulas, e.g., f = M*q_tt + K*q, GBar, ...\nuse UpperCamelCase for functions, classes, structs, ...\nSpecial cases for CamelCase: write 'ODEsystem', BUT: 'ODE1Equations'\n'[...]Init' ... in arguments, for initialization of variables; e.g. 'valueInit' for initialization of member variable 'value'\nuse American English troughout: Visualization, etc.\nfor (abbreviations) in captial letters, e.g. ODE, use a lower case letter afterwards:\ndo not use consecutive capitalized words, e.g. DO NOT WRITE 'ODEAE'\nfor functions use ODEComputeCoords(), for variables avoid 'ODE' at beginning: use nODE or write odeCoords\ndo not use '_' within variable or function names; exception: derivatives\nuse name which exactly describes the function/variable: 'numberOfItems' instead of 'size' or 'l'\nexamples for variable names: secondOrderSize, massMatrix, mThetaTheta\nexamples for function/class names: SecondOrderSize, EvaluateMassMatrix, Position(const Vector3D\\& localPosition)\nuse the Get/Set...() convention if data is retrieved from a class (Get) or something is set in a class (Set); Use const T\\& Get()/T\\& Get if direct access to variables is needed; Use Get/Set for pybind11\nexample Get/Set: Real* GetDataPointer(), Vector::SetAll(Real), GetTransposed(), SetRotationalParameters(...), SetColor(...), ...\nuse 'Real' instead of double or float: for compatibility, also for AVX with SP/DP\nuse 'Index' for array/vector size and index instead of size_t or int\nitem: object, node, marker, load: anything handled within the computational/visualization systems\nDo not use numbers (3 for 3D or any other number which represents, e.g., the number of rotation parameters). Use const Index or constexpr to define constants.\nNo-abbreviations-rule\nThe code uses a minimum set of abbreviations; however, the following abbreviation rules are used throughout: In general: DO NOT ABBREVIATE function, class or variable names: GetDataPointer() instead of GetPtr(); exception: cnt, i, j, k, x or v in cases where it is really clear (5-line member functions).\nExceptions to the NO-ABBREVIATIONS-RULE:\nODE ... ordinary differential equations;\nODE2 ... marks parts related to second order differential equations (SOS2, EvalF2 in HOTINT)\nODE1 ... marks parts related to first order differential equations (ES, EvalF in HOTINT)\nAE ... algebraic equations (IS, EvalG in HOTINT); write 'AEcoordinates' for 'algebraicEquationsCoordinates'\n'C[...]' ... Computational, e.g. for ComputationalNode ==> use 'CNode'\nmin, max ... minimum and maximum\nwrite time derivatives with underscore: _t, _tt; example: Position_t, Position_tt, ...\nwrite space-wise derivatives ith underscore: _x, _xx, _y, ...\nif a scalar, write coordinate derivative with underscore: _q, _v (derivative w.r.t. velocity coordinates)\nfor components, elements or entries of vectors, arrays, matrices: use 'item' throughout\n'[...]Init' ... in arguments, for initialization of variables; e.g. 'valueInit' for initialization of member variable 'value'\nTutorial\nThis section will show:\nA basic tutorial for a 1D mass and spring-damper with initial displacements, shortest possible model with practically no special settings\nA more advanced rigid-body model, including 3D rigid bodies and revolute joints\nLinks to examples section\nA large number of examples, some of them quite advanced, can be found in:\nmain/pythonDev/Examplesmain/pythonDev/TestModels\nMass-Spring-Damper tutorial\nThe python source code of the first tutorial can be found in the file:\nmain/pythonDev/Examples/springDamperTutorial.py\nThis tutorial will set up a mass point and a spring damper, dynamically compute the solution and evaluate the reference solution.\nWe import the exudyn library and the interface for all nodes, objects, markers, loads and sensors:\nimport exudyn as exu\nfrom exudyn.itemInterface import *\nimport numpy as np #for postprocessing\nNext, we need a SystemContainer, which contains all computable systems and add a new MainSystem mbs. Per default, you always should name your system 'mbs' (multibody system), in order to copy/paste code parts from other examples, tutorials and other projects:\nSC = exu.SystemContainer()\nmbs = SC.AddSystem()\nIn order to check, which version you are using, you can printout the current Exudyn version. This version is in line with the issue tracker and marks the number of open/closed issues added to Exudyn :\nprint('EXUDYN version='+exu.__version__)\nUsing the powerful Python language, we can define some variables for our problem, which will also be used for the analytical solution:\nL=0.5 #reference position of mass\nmass = 1.6 #mass in kg\nspring = 4000 #stiffness of spring-damper in N/m\ndamper = 8 #damping constant in N/(m/s)\nf =80 #force on mass\nFor the simple spring-mass-damper system, we need initial displacements and velocities:\nu0=-0.08 #initial displacement\nv0=1 #initial velocity\nx0=f/spring #static displacement\nprint('resonance frequency = '+str(np.sqrt(spring/mass)))\nprint('static displacement = '+str(x0))\nWe first need to add nodes, which provide the coordinates (and the degrees of freedom) to the system. The following line adds a 3D node for 3D mass point (Note: Point is an abbreviation for NodePoint, defined in itemInterface.py.):\nn1=mbs.AddNode(Point(referenceCoordinates = [L,0,0],\ninitialCoordinates = [u0,0,0],\ninitialVelocities = [v0,0,0]))\nHere, Point (=NodePoint) is a Python class, which takes a number of arguments defined in the reference manual. The arguments here are referenceCoordinates, which are the coordinates for which the system is defined. The initial configuration is given by referenceCoordinates + initialCoordinates, while the initial state additionally gets initialVelocities. The command mbs.AddNode(...) returns a NodeIndex n1, which basically contains an integer, which can only be used as node number. This node number will be used lateron to use the node in the object or in the marker.\nWhile Point adds 3 unknown coordinates to the system, which need to be solved, we also can add ground nodes, which can be used similar to nodes, but they do not have unknown coordinates -- and therefore also have no initial displacements or velocities. The advantage of ground nodes (and ground bodies) is that no constraints are needed to fix these nodes. Such a ground node is added via:\nnGround=mbs.AddNode(NodePointGround(referenceCoordinates = [0,0,0]))\nIn the next step, we add an object (sec:programStructure.), which provides equations for coordinates. The MassPoint needs at least a mass (kg) and a node number to which the mass point is attached. Additionally, graphical objects could be attached:\nmassPoint = mbs.AddObject(MassPoint(physicsMass = mass, nodeNumber = n1))\nIn order to apply constraints and loads, we need markers. These markers are used as local positions (and frames), where we can attach a constraint lateron. In this example, we work on the coordinate level, both for forces as well as for constraints. Markers are attached to the according ground and regular node number, additionally using a coordinate number (0 ... first coordinate):\ngroundMarker=mbs.AddMarker(MarkerNodeCoordinate(nodeNumber= nGround,\ncoordinate = 0))\n#marker for springDamper for first (x-)coordinate:\nnodeMarker = mbs.AddMarker(MarkerNodeCoordinate(nodeNumber= n1,\ncoordinate = 0))\nThis means that loads can be applied to the first coordinate of node n1 via marker with number nodeMarker, which is in fact of type MarkerIndex.\nNow we add a spring-damper to the markers with numbers groundMarker and the nodeMarker, providing stiffness and damping parameters:\nnC = mbs.AddObject(CoordinateSpringDamper(markerNumbers = [groundMarker, nodeMarker],\nstiffness = spring,\ndamping = damper))\nA load is added to marker nodeMarker, with a scalar load with value f:\nnLoad = mbs.AddLoad(LoadCoordinate(markerNumber = nodeMarker,\nload = f))\nFinally, a sensor is added to the coordinate constraint object with number nC, requesting the outputVariableType Force:\nmbs.AddSensor(SensorObject(objectNumber=nC, fileName='groundForce.txt',\noutputVariableType=exu.OutputVariableType.Force))\nNote that sensors can be attached, e.g., to nodes, bodies, objects (constraints) or loads. As our system is fully set, we can print the overall information and assemble the system to make it ready for simulation:\nprint(mbs)\nmbs.Assemble()\nWe will use time integration and therefore define a number of steps (fixed step size; must be provided) and the total time span for the simulation:\ntEnd = 1 #end time of simulation\nh = 0.001 #step size; leads to 1000 steps\nAll settings for simulation, see according reference section, can be provided in a structure given from exu.SimulationSettings(). Note that this structure will contain all default values, and only non-default values need to be provided:\nsimulationSettings = exu.SimulationSettings()\nsimulationSettings.solutionSettings.solutionWritePeriod = 5e-3 #output interval general\nsimulationSettings.solutionSettings.sensorsWritePeriod = 5e-3 #output interval of sensors\nsimulationSettings.timeIntegration.numberOfSteps = int(tEnd/h) #must be integer\nsimulationSettings.timeIntegration.endTime = tEnd\nWe are using a generalized alpha solver, where numerical damping is needed for index 3 constraints. As we have only spring-dampers, we can set the spectral radius to 1, meaning no numerical damping:\nsimulationSettings.timeIntegration.generalizedAlpha.spectralRadius = 1\nIn order to visualize the results online, a renderer can be started. As our computation will be very fast, it is a good idea to wait for the user to press SPACE, before starting the simulation (uncomment second line):\nexu.StartRenderer() #start graphics visualization\n#mbs.WaitForUserToContinue() #wait for pressing SPACE bar to continue (in render window!)\nAs the simulation is still very fast, we will not see the motion of our node. Using e.g.steps=10000000 in the lines above allows you online visualize the resulting oscillations.\nFinally, we start the solver, by telling which system to be solved, solver type and the simulation settings:\nexu.SolveDynamic(mbs, simulationSettings)\nAfter simulation, our renderer needs to be stopped (otherwise it would stay in background and prohibit further simulations). Sometimes you would like to wait until closing the render window, using WaitForRenderEngineStopFlag():\n#SC.WaitForRenderEngineStopFlag()#wait for pressing 'Q' to quit\nexu.StopRenderer() #safely close rendering window!\nThere are several ways to evaluate results, see the reference pages. In the following we take the final value of node n1 and read its 3D position vector:\n#evaluate final (=current) output values\nu = mbs.GetNodeOutput(n1, exu.OutputVariableType.Position)\nprint('displacement=',u)\nThe following code generates a reference (exact) solution for our example:\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nomega0 = np.sqrt(spring/mass) #eigen frequency of undamped system\ndRel = damper/(2*np.sqrt(spring*mass)) #dimensionless damping\nomega = omega0*np.sqrt(1-dRel**2) #eigen freq of damped system\nC1 = u0-x0 #static solution needs to be considered!\nC2 = (v0+omega0*dRel*C1) / omega #C1, C2 are coeffs for solution\nsteps = int(tEnd/h) #use same steps for reference solution\nrefSol = np.zeros((steps+1,2))\nfor i in range(0,steps+1):\nt = tEnd*i/steps\nrefSol[i,0] = t\nrefSol[i,1] = np.exp(-omega0*dRel*t)*(C1*np.cos(omega*t)+C2*np.sin(omega*t))+x0\nplt.plot(refSol[:,0], refSol[:,1], 'r-', label='displacement (m); exact solution')\nNow we can load our results from the default solution file coordinatesSolution.txt, which is in the same directory as your python tutorial file. For convenient reading the file containing commented lines, we use a numpy feature and finally plot the displacement of coordinate 0 or our mass point (data[:,0] contains the simulation time, data[:,1] contains displacement of (global) coordinate 0, data[:,2] contains displacement of (global) coordinate 1, ...)):\ndata = np.loadtxt('coordinatesSolution.txt', comments='#', delimiter=',')\nplt.plot(data[:,0], data[:,1], 'b-', label='displacement (m); numerical solution')\nThe sensor result can be loaded in the same way. The sensor output format contains time in the first column and sensor values in the remaining columns. The number of columns depends on the sensor and the output quantity (scalar, vector, ...):\ndata = np.loadtxt('groundForce.txt', comments='#', delimiter=',')\nplt.plot(data[:,0], data[:,1]*1e-3, 'g-', label='force (kN)')\nIn order to get a nice plot within Spyder, the following options can be used (note, in some environments you need finally the command plt.show()):\nax=plt.gca() # get current axes\nax.grid(True, 'major', 'both')\nax.xaxis.set_major_locator(ticker.MaxNLocator(10))\nax.yaxis.set_major_locator(ticker.MaxNLocator(10))\nplt.legend() #show labels as legend\nplt.tight_layout()\nplt.show()\nThe matplotlib output should look like this:\nRigid body and joints tutorial\nThe python source code of the first tutorial can be found in the file:\nmain/pythonDev/Examples/rigidBodyTutorial3.py\nThis tutorial will set up a multibody system containing a ground, two rigid bodies and two revolute joints driven by gravity, compare a 3D view of the example in the figure above.\nWe first import the exudyn library and the interface for all nodes, objects, markers, loads and sensors:\nimport exudyn as exu\nfrom exudyn.itemInterface import *\nfrom exudyn.utilities import *\nimport numpy as np #for postprocessing\nThe submodule exudyn.utilities contains helper functions for graphics representation, 3D rigid bodies and joints.\nAs in the first tutorial, we need a SystemContainer and add a new MainSystem mbs:\nSC = exu.SystemContainer()\nmbs = SC.AddSystem()\nWe define some geometrical parameters for lateron use.\n#physical parameters\ng = [0,-9.81,0] #gravity\nL = 1 #length\nw = 0.1 #width\nbodyDim=[L,w,w] #body dimensions\np0 = [0,0,0] #origin of pendulum\npMid0 = np.array([L*0.5,0,0]) #center of mass, body0\nWe add an empty ground body, using default values. It's origin is at [0,0,0] and here we use no visualization.\n#ground body\noGround = mbs.AddObject(ObjectGround())\nFor physical parameters of the rigid body, we can use the class RigidBodyInertia, which allows to define mass, center of mass (COM) and inertia parameters, as well as shifting COM or adding inertias. The RigidBodyInertia can be used directly to create rigid bodies. Special derived classes can be use to define rigid body inertias for cylinders, cubes, etc., so we use a cube here:\n#first link:\niCube0 = InertiaCuboid(density=5000, sideLengths=bodyDim)\niCube0 = iCube0.Translated([-0.25*L,0,0]) #transform COM, COM not at reference point!\nNote that the COM is translated in axial direction, while it would be at the body's local position [0,0,0] by default!\nFor visualization, we need to add some graphics for the body defined as a 3D RigidLink object and we additionally draw a basis (three RGB-vectors) at the COM:\n#graphics for body\ngraphicsBody0 = GraphicsDataRigidLink(p0=[-0.5*L,0,0],p1=[0.5*L,0,0],\naxis0=[0,0,1], axis1=[0,0,0], radius=[0.5*w,0.5*w],\nthickness=w, width=[1.2*w,1.2*w], color=color4red)\ngraphicsCOM0 = GraphicsDataBasis(origin=iCube0.com, length=2*w)\nNow we have defined all data for the link (rigid body). We could use mbs.AddNode(NodeRigidBodyEP(...)) and mbs.AddObject(ObjectRigidBody(...)) to create a node and a body, but the exudyn.rigidBodyUtilities offer a much more comfortable function:\n[n0,b0]=AddRigidBody(mainSys = mbs,\ninertia = iCube0, #includes COM\nnodeType = exu.NodeType.RotationEulerParameters,\nposition = pMid0,\nrotationMatrix = np.diag([1,1,1]),\ngravity = g,\ngraphicsDataList = [graphicsBody0, graphicsCOM0])\nwhich also adds a gravity load and could also set initial velocities, if wanted. The nodeType specifies the underlying model for the rigid body node, see theDoc.pdf. We can use\nRotationEulerParameters: for fast computation, but leads to an additional algebraic equation and thus needs an implicit solver\nRotationRxyz: contains a singularity if the second angle reaches +/- 90 degrees, but no algebraic equations\nRotationRotationVector: basically contains a singularity for 0 degrees, but if used in combination with Lie group integrators, singularities are bypassed\nWe now add a revolute joint around the (global) z-axis. We have several possibilities, which are shown in the following. For the first two possibilities only, we need the following markers\n#markers for ground and rigid body (not needed for option 3):\nmarkerGround = mbs.AddMarker(MarkerBodyRigid(bodyNumber=oGround, localPosition=[0,0,0]))\nmarkerBody0J0 = mbs.AddMarker(MarkerBodyRigid(bodyNumber=b0, localPosition=[-0.5*L,0,0]))\nThe very general option 1 is to use the GenericJoint, that can be used to define any kind of joint with translations and rotations fixed or free,\n#revolute joint option 1:\nmbs.AddObject(GenericJoint(markerNumbers=[markerGround, markerBody0J0],\nconstrainedAxes=[1,1,1,1,1,0],\nvisualization=VObjectJointGeneric(axesRadius=0.2*w,\naxesLength=1.4*w)))\nIn addition, transformation matrices (rotationMarker0/1) can be added, see the joint description.\nOption 2 is using the revolute joint, which allows a free rotation around the local z-axis of marker 0 (markerGround in our example)\n#revolute joint option 2:\nmbs.AddObject(ObjectJointRevoluteZ(markerNumbers = [markerGround, markerBody0J0],\nrotationMarker0=np.eye(3),\nrotationMarker1=np.eye(3),\nvisualization=VObjectJointRevoluteZ(axisRadius=0.2*w,\naxisLength=1.4*w)\n))\nAdditional transformation matrices (rotationMarker0/1) can be added in order to chose any rotation axis.\nNote that an error in the definition of markers for the joints can be also detected in the render window (if you completed the example), e.g., if you change the following marker in the lines above,\n#example if wrong marker position is chosen:\nmarkerBody0J0 = mbs.AddMarker(MarkerBodyRigid(bodyNumber=b0, localPosition=[-0.4*L,0,0]))\n=> you will see a misalignment of the two parts of the joint by 0.1*L.\nDue to the fact that the definition of markers for general joints is tedious, there is a utility function, which allows to attach revolute joints immediately to bodies and defining the rotation axis only once for the joint:\n#revolute joint option 3:\nAddRevoluteJoint(mbs, body0=oGround, body1=b0, point=[0,0,0],\naxis=[0,0,1], useGlobalFrame=True, showJoint=True,\naxisRadius=0.2*w, axisLength=1.4*w)\nThe second link and the according joint can be set up in a very similar way:\n#second link:\ngraphicsBody1 = GraphicsDataRigidLink(p0=[0,0,-0.5*L],p1=[0,0,0.5*L],\naxis0=[1,0,0], axis1=[0,0,0], radius=[0.06,0.05],\nthickness = 0.1, width = [0.12,0.12],\ncolor=color4lightgreen)\niCube1 = InertiaCuboid(density=5000, sideLengths=[0.1,0.1,1])\npMid1 = np.array([L,0,0]) + np.array([0,0,0.5*L]) #center of mass, body1\n[n1,b1]=AddRigidBody(mainSys = mbs,\ninertia = iCube1,\nnodeType = exu.NodeType.RotationEulerParameters,\nposition = pMid1,\nrotationMatrix = np.diag([1,1,1]),\nangularVelocity = [0,0,0],\ngravity = g,\ngraphicsDataList = [graphicsBody1])\nThe revolute joint in this case has a free rotation around the global x-axis:\n#revolute joint (free x-axis)\nAddRevoluteJoint(mbs, body0=b0, body1=b1, point=[L,0,0],\naxis=[1,0,0], useGlobalFrame=True, showJoint=True,\naxisRadius=0.2*w, axisLength=1.4*w)\nFinally, we also add a sensor for some output of the double pendulum:\n#position sensor at tip of body1\nsens1=mbs.AddSensor(SensorBody(bodyNumber=b1, localPosition=[0,0,0.5*L],\nfileName='solution/sensorPos.txt',\noutputVariableType = exu.OutputVariableType.Position))\nBefore simulation, we need to call Assemble() for our system, which links objects, nodes, ..., assigns initial values and does further pre-computations and checks:\nmbs.Assemble()\nAfter Assemble(), markers, nodes, objects, etc. are linked and we can analyze the internal structure. First, we can print out useful information, either just typing mbs in the iPython console to print out overal information:\n<systemData:\nNumber of nodes= 2\nNumber of objects = 5\nNumber of markers = 8\nNumber of loads = 2\nNumber of sensors = 1\nNumber of ODE2 coordinates = 14\nNumber of ODE1 coordinates = 0\nNumber of AE coordinates = 12\nNumber of data coordinates = 0\nFor details see mbs.systemData, mbs.sys and mbs.variables\n>\nNote that there are 2 nodes for the two rigid bodies. The five objects are due to ground object, 2 rigid bodies and 2 revolute joints. The meaning of markers can be seen in the graphical representation described below.\nAlternatively we can print the full internal information as a dictionary using:\nmbs.systemData.Info() #show detailed information\nwhich results in the following output:\nnode0:\n'nodeType': 'RigidBodyEP', 'referenceCoordinates': [0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0], 'addConstraintEquation': True, 'initialCoordinates': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'initialVelocities': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'name': 'node0', 'Vshow': True, 'VdrawSize': -1.0, 'Vcolor': [-1.0, -1.0, -1.0, -1.0]\nnode1:\n'nodeType': 'RigidBodyEP', 'referenceCoordinates': [1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0], 'addConstraintEquation': True, 'initialCoordinates': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'initialVelocities': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'name': 'node1', 'Vshow': True, 'VdrawSize': -1.0, 'Vcolor': [-1.0, -1.0, -1.0, -1.0]\nobject0:\n'objectType': 'Ground', 'referencePosition': [0.0, 0.0, 0.0], 'name': 'object0', 'Vshow': True, 'VgraphicsDataUserFunction': 0, 'Vcolor': [-1.0, -1.0, -1.0, -1.0], 'VgraphicsData': 'TODO': 'Get graphics data to be implemented'\nobject1:\n'objectType': 'RigidBody', 'physicsMass': 50.0, 'physicsInertia': [0.08333333333333336, 7.333333333333334, 7.333333333333334, 0.0, 0.0, 0.0], 'physicsCenterOfMass': [-0.25, 0.0, 0.0], 'nodeNumber': 0, 'name': 'object1', 'Vshow': True, 'VgraphicsDataUserFunction': 0, 'VgraphicsData': 'TODO': 'Get graphics data to be implemented'\nobject2:\n'objectType': 'JointRevolute', 'markerNumbers': [3, 4], 'rotationMarker0': [[0.0, 1.0, 0.0], [-1.0, 0.0, 0.0], [0.0, 0.0, 1.0]], 'rotationMarker1': [[0.0, 1.0, 0.0], [-1.0, 0.0, 0.0], [0.0, 0.0, 1.0]], 'activeConnector': True, 'name': 'object2', 'Vshow': True, 'VaxisRadius': 0.019999999552965164, 'VaxisLength': 0.14000000059604645, 'Vcolor': [-1.0, -1.0, -1.0, -1.0]\nobject3:\n...\nA graphical representation of the internal structure of the model can be shown using the command DrawSystemGraph:\nDrawSystemGraph(mbs, useItemTypes=True) #draw nice graph of system\nFor the output see the figure below. Note that obviously, markers are always needed to connect objects (or nodes) as well as loads. We can also see, that 2 markers MarkerBodyRigid1 and MarkerBodyRigid2 are unused, which is no further problem for the model and also does not require additional computational resources (except for some bytes of memory). Having isolated nodes or joints that are not connected (or having too many connections) may indicate that you did something wrong in setting up your model.\nBefore starting our simulation, we should adjust the solver parameters, especially the end time and the step size (no automatic step size for implicit solvers available!):\nsimulationSettings = exu.SimulationSettings() #takes currently set values or default values\ntEnd = 4 #simulation time\nh = 1e-3 #step size\nsimulationSettings.timeIntegration.numberOfSteps = int(tEnd/h)\nsimulationSettings.timeIntegration.endTime = tEnd\nsimulationSettings.timeIntegration.verboseMode = 1\n#simulationSettings.timeIntegration.simulateInRealtime = True\nsimulationSettings.solutionSettings.solutionWritePeriod = 0.005 #store every 5 ms\nThe verboseMode tells the solver the amount of output during solving. Higher values (2, 3, ...) show residual vectors, jacobians, etc. for every time step, but slow down simulation significantly. The option simulateInRealtime is used to view the model during simulation, while setting this false, the simulation finishes after fractions of a second. It should be set to false in general, while solution can be viewed using the SolutionViewer(). With solutionWritePeriod you can adjust the frequency which is used to store the solution of the whole model, which may lead to very large files and may slow down simulation, but is used in the SolutionViewer() to reload the solution after simulation.\nIn order to improve visualization, there are hundreds of options, see Visualization settings in theDoc.pdf, some of them used here:\nSC.visualizationSettings.window.renderWindowSize=[1600,1200]\nSC.visualizationSettings.openGL.multiSampling = 4 #improved OpenGL rendering\nSC.visualizationSettings.general.autoFitScene = False\nSC.visualizationSettings.nodes.drawNodesAsPoint=False\nSC.visualizationSettings.nodes.showBasis=True #shows three RGB (=xyz) lines for node basis\nThe option autoFitScene is used in order to avoid zooming while loading the last saved render state, see below.\nWe can start the 3D visualization (Renderer) now:\nexu.StartRenderer()\nIn order to reload the model view of the last simulation (if there is any), we can use the following commands:\nif 'renderState' in exu.sys: #reload old view\nSC.SetRenderState(exu.sys['renderState'])\nmbs.WaitForUserToContinue() #stop before simulating\nthe function WaitForUserToContinue() waits with simulation until we press SPACE bar. This allows us to make some pre-checks.\nFinally, implicit time integration (simulation) is started with:\nexu.SolveDynamic(mbs, simulationSettings = simulationSettings,\nsolverType=exu.DynamicSolverType.TrapezoidalIndex2)\nAfter simulation, the library would immediately exit (and jump back to iPython or close the terminal window). In order to avoid this, we can use WaitForRenderEngineStopFlag() to wait until we press key 'Q'.\nSC.WaitForRenderEngineStopFlag() #stop before closing\nexu.StopRenderer() #safely close rendering window!\nIf you entered everything correctly, the render window should show a nice animation of the 3D double pendulum after pressing the SPACE key. If we do not stop the renderer (StopRenderer()), it will stay open for further simulations. However, it is safer to always close the renderer at the end.\nAs the simulation will run very fast, if you did not set simulateInRealtime to true. However, you can reload the stored solution and view the stored steps interactively:\nsol = LoadSolutionFile('coordinatesSolution.txt')\nfrom exudyn.interactive import SolutionViewer\nSolutionViewer(mbs, sol)\nFinally, we can plot our sensor, drawing the y-component of the sensor:\nfrom exudyn.plot import PlotSensor\nPlotSensor(mbs, [sens1],[1])\nCongratulations! You completed the rigid body tutorial, which gives you the ability to model multibody systems. Note that much more complicated models are possible, including feedback control or flexible bodies, see the Examples!\nFOR FURTHER INFORMATION GO TO theDoc.pdf !!!", "link": "https://github.com/jgerstmayr/EXUDYN", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "exudyn\nexudyn version = 1.1.42\na flexible multibody dynamics systems simulation code with python and c++\nfree, open source and with plenty of documentation and examples\nnote: for pure installation, just go to main/distand install your preferred microsoft installer (.msi) or wheel (.whl) in your according python environment (win=windows, py3.7 = python3.7, ...)\nthis readme document is a small part of the complete documentation found as pdf document in docs/thedoc/thedoc.pdf. it is auto-generated from .tex files (sorry for some conversion errors!). due to limitations for complex formulas and tables in .rst files, details of the reference manual and many other parts of the documentation are only available in thedoc.pdf, see the github page of exudyn !\nfor license, see license.txt in the root folder!\nin addition to the tutorial in the documentation, many ( 100+ ) examples can be found under main/pythondev/examples and main/pythondev/testmodels .\ntutorial videos can be found in the youtube channel of exudyn !\nenjoy the python library for multibody dynamics modeling, simulation, creating large scale systems, parameterized systems, component mode synthesis, optimization, ...\ninstallation and getting started\nexudyn is hosted on github :\nweb: https://github.com/jgerstmayr/exudyn\nfor any comments, requests, issues, bug reports, send an email to:\nemail: reply.exudyn@gmail.com\nthanks for your contribution!\ngetting started\nthis section will show:\nwhat is exudyn ?\nwho is developing exudyn ?\nhow to install exudyn\nhow to link exudyn and python\ngoals of exudyn\nrun a simple example in spyder\nfaq -- frequently asked questions\nwhat is exudyn ?\nexudyn -- (flexible multibody dynamics -- extend your dynamics)\nexudyn is a c++ based python library for efficient simulation of flexible multibody dynamics systems. it is the follow up code of the previously developed multibody code hotint, which johannes gerstmayr started during his phd-thesis. it seemed that the previous code hotint reached limits of further (efficient) development and it seemed impossible to continue from this code as it was outdated regarding programming techniques and the numerical formulation at the time exudyn was started.\nexudyn is designed to easily set up complex multibody models, consisting of rigid and flexible bodies with joints, loads and other components. it shall enable automatized model setup and parameter variations, which are often necessary for system design but also for analysis of technical problems. the broad usability of python allows to couple a multibody simulation with environments such as optimization, statistics, data analysis, machine learning and others.\nthe multibody formulation is mainly based on redundant coordinates. this means that computational objects (rigid bodies, flexible bodies, ...) are added as independent bodies to the system. hereafter, connectors (e.g., springs or constraints) are used to interconnect the bodies. the connectors are using markers on the bodies as interfaces, in order to transfer forces and displacements. for details on the interaction of nodes, objects, markers and loads see thedoc.pdf.\ndevelopers of exudyn and thanks\nexudyn is currently developed at the university of innsbruck. in the first phase most of the core code is written by johannes gerstmayr, implementing ideas that followed out of the project hotint. 15 years of development led to a lot of lessions learned and after 20 years, a code must be re-designed.\nsome important tests for the coupling between c++ and python have been written by stefan holzinger. stefan also helped to set up the previous upload to gitlab and to test parallelization features. for the interoperability between c++ and python, we extensively use pybind11, originally written by jakob wenzel, see https://github.com/pybind/pybind11. without pybind11 we couldn't have made this project -- thank's a lot!\nimportant discussions with researchers from the community were important for the design and development of exudyn , where we like to mention joachim sch\"oberl from tu-vienna who boosted the design of the code with great concepts.\nthe cooperation and funding within the eu h2020-msca-itn project 'joint training on numerical modelling of highly flexible structures for industrial applications' contributes to the development of the code.\nthe following people have contributed to python and c++ library implementations:\njoachim sch\"oberl (providing specialized ngsolve core library with taskmanager for bf multithreaded parallelization; ngsolve mesh and fe-matrices import; highly efficient eigenvector computations)\nstefan holzinger (lie group solvers in python)\npeter manzl (convexroll python / c++ implementation)\nmartin sereinig (special robotics functionality)\nthe following people have contributed to the examples:\nstefan holzinger, michael pieber, manuel schieferle, martin knapp, lukas march,\ndominik sponring, david wibmer, andreas zw\"olfer, peter manzl\n-- thanks a lot! --\ninstallation instructions\nhow to install exudyn ?\nin order to run exudyn , you need an appropriate python installation. we currently (2021-07) recommend to use\nanaconda, 64bit, python 3.7.7 (anaconda3 64bit with python3.7.7 can be downloaded via the repository archive https://repo.anaconda.com/archive/ choosing anaconda3-2020.02-windows-x86_64.exe) (but python 3.8 is also working well!)\nspyder 4.1.3 (with python 3.7.7, 64bit), which is included in the anaconda installation (or 64bit and are compiled up to the same minor version, i.e., 3.7.x. there will be a strange .dll error, if you mix up 32/64bit. it is possible to install both, anaconda 32bit and anaconda 64bit -- then you should follow the recommendations of paths as suggested by anaconda installer.)\nmany alternative options exist:\nin case that you have an older cpu, which does not support avx2, use: anaconda, 32bit, python 3.6.5) (anaconda 32bit with python3.6 can be downloaded via the repository archive https://repo.anaconda.com/archive/ choosing anaconda3-5.2.0-windows-x86.exe.)\nusers report successful use of exudyn with visual studio code. jupyter has been tested with some examples; both environments should work with default settings.\nanaconda 2020-11 with python 3.8 and spyder 4.1.5: no problems up to now (2021-07), testsuite runs without problems since exudyn version 1.0.182.\nalternative option with more stable spyder (as compared to spyder 4.1.3): anaconda, 64bit, python 3.6.5) (anaconda 64bit with python3.6 can be downloaded via the repository archive https://repo.anaconda.com/archive/ choosing anaconda3-5.2.0-windows-x86_64.exe for 64bit.)\nif you plan to extend the c++ code, we recommend to use vs2017 (previously, vs2019 was recommended: however, vs2019 has problems with the library 'eigen' and therefore leads to erroneous results with the sparse solver. vs2017 can also be configured with python 3.7 now.) to compile your code, which offers python 3.7 compatibility. once again, remember that python versions and the version of the exudyn module must be identical (e.g., python 3.6 32 bit both in the exudyn module and in spyder).\nparagraphinstallation without anaconda: if you do not install anaconda (e.g., under linux), make sure that you have the according python packages installed:\nnumpy (used throughout the code, inevitable)\nmatplotlib (for any -----> plot !!! , also plotsensor(...))\ntkinter (for interactive dialogs, solutionviewer, etc.)\nscipy (needed for eigenvalue computation)\nyou can install most of these packages using pip install numpy (windows) or pip3 install numpy (linux).\nfor interaction (right-mouse-click, some key-board commands) you need the python module tkinter. this is included in regular anaconda distributions (recommended, see below), but on ubuntu you need to type alike (do not forget the '3', otherwise it installs for python2 ...):\nsudo apt-get install python3-tk\nsee also common blogs for your operating system.\ninstall with windows msi installer\nthe simplest way on windows 10 (and maybe also windows 7), which works well if you installed only one python version and if you installed anaconda with the option 'register anaconda as my default python 3.x' or similar, then you can use the provided .msi installers in the main/dist directory:\nfor the 64bits python 3.7 version, double click on (version may differ):\nexudyn-1.0.248.win-amd64-py3.7.msi\nfollow the instructions of the installer\nif python / anaconda is not found by the installer, provide the 'python directory' as the installation directory of anaconda3, which usually is installed in:\nc:\\programdata\\anaconda3\ninstall from wheel (ubuntu and windows)\nthe standard way to install the python package exudyn is to use the so-called 'wheels' (file ending .whl) provided at the directory wheels in the exudyn repository.\nfor ubuntu18.04 (which by default uses python 3.6) this may read (version number 1.0.20 may be different):\npython 3.6, 64bit: pip3 install distexudyn-1.0.20-cp36-cp36-linux_x86_64.whl\nfor ubuntu20.04 (which by default uses python 3.8) this may read (version number 1.0.20 may be different):\npython 3.8, 64bit: pip3 install distexudyn-1.0.20-cp38-cp38-linux_x86_64.whl\nnote that your installation may have environments with different python versions, so install that exudyn version appropriately! if the wheel installation does not work on ubuntu, it is highly recommended to build exudyn for your specific system as given in thedoc.pdf.\nwindows:\nfirst, open an anaconda prompt:\neither calling: start->anaconda->... or go to anaconda/scripts folder and call activate.bat\nyou can check your python version then, by running python (python3 under ubuntu 18.04), the output reads like:\npython 3.6.5 |anaconda, inc.| (default, mar 29 2018, 13:32:41) [msc v.1900 64 bit (amd64)] on win32...\n=> type exit() to close python\n**go to the folder exudyn_git/main** (where setup.py lies) and choose the wheel in subdirectory main/dist according to your system (windows/ubuntu), python version (3.6 or 3.7) and 32 or 64 bits.\nfor windows the installation commands may read (version number 1.0.20 may be different):\npython 3.6, 32bit: pip install distexudyn-1.0.20-cp36-cp36m-win32.whl\npython 3.6, 64bit: pip install distexudyn-1.0.20-cp36-cp36m-win_amd64.whl\npython 3.7, 64bit: pip install distexudyn-1.0.20-cp37-cp37m-win_amd64.whl\nwork without installation and editing sys.path\nthe uncommon and old way (=> not recommended for exudyn versions ge 1.0.0) is to use python's sys module to link to your exudyn (previously workingrelease) directory, for example:\nimport sys\nsys.path.append('c:/data/cpp/exudyn_git/bin/exudyn32bitspython36')\nthe folder exudyn32bitspython36 needs to be adapted to the location of the according exudyn package.\nbuild and install exudyn under windows 10?\nnote that there are a couple of pre-requisites, depending on your system and installed libraries. for windows 10, the following steps proved to work:\ninstall your anaconda distribution including spyder\nclose all python programs (e.g. spyder, jupyter, ...)\nrun an anaconda prompt (may need to be run as administrator)\nif you cannot run anaconda prompt directly, do:\nopen windows shell (cmd.exe) as administrator (start => search for cmd.exe => right click on app => 'run as administrator' if necessary)\ngo to your scripts folder inside the anaconda folder (e.g. c:\\programdata\\anaconda\\scripts)\nrun 'activate.bat'\ngo to 'main' of your cloned github folder of exudyn\nrun: python setup.py install\nread the output; if there are errors, try to solve them by installing appropriate modules\nyou can also create your own wheels, doing the above steps to activate the according python version and then calling (requires installation of microsoft visual studio; recommended: vs2017):\npython setup.py bdist_wheel\nthis will add a wheel in the dist folder.\nbuild and install exudyn under mac os x?\ninstallation and building on mac os x is rarely tested, but first successful compilation including glfw has been achieved. requirements are an according anaconda installation.\ntested configuration:\nmac os x 10.11.6 'el capitan', mac pro (2010), 3.33ghz 6-core intel xeon, 4gb memory\nanaconda navigator 1.9.7\npython 3.7.0\nspyder 3.3.6\nfor a compatible mac os x system, you can install the pre-compiled wheel (go to local directory). go to the main/dist directory in your back terminal and type, e.g.,\npip install exudyn-1.0.218-cp37-cp37m-macosx_10_9_x86_64.whl %\nalternatively, we tested on:\nmac os 11.x 'big sur', mac mini (2021), apple m1, 16gb memory\nanaconda (i368 based with rosetta 2) with python 3.8\nthis configuration is currently evaluated but showed general compatibility => pre-compiled wheel: exudyn-1.1.0-cp38-cp38-macosx_11_0_x86_64.whl\nbf compile from source:\nif you would like to compile from source, just use a bash terminal on your mac, and do the following steps inside the main directory of your repository and type\npython setup.py bdist_wheel=> this compiles and takes approx.~5 minutes, depending on your machine => it may produce some errors, depending on your version; if there are some liker errors (saying that there is no '-framework cocoa' and '-framework opengl', just go back in the terminal and copy everything from 'g++ ...' until the end of the last command '-mmacosx-verion-min...' and paste it into the terminal. calling that again will finalize linking; then run again python setup.py bdist_wheel=> this now creates the wheel (if you want to distribute) in the dist folder alternatively just call\npython setup.py installto install exudyn\nthen just go to the pythondev/examples folder and run an example:\npython springdamperuserfunctiontest.py\nif you have a new system, try to adapt setup.py accordingly, e.g., activating the -std=c++17 support. if there are other issues, we are happy to receive your detailed bug reports.\nnote that you need to run\nexudyn.startrenderer()exudyn.dorendereridletasks(-1)\nin order to interact with the render window, as there is only a single-threaded version available for mac os.\nbuild and install exudyn under ubuntu?\nhaving a new ubuntu 18.04 standard installation (e.g. using a vm virtual box environment), the following steps need to be done (python 3.6 is already installed on ubuntu18.04, otherwise use sudo apt install python3) (see also the youtube video: https://www.youtube.com/playlist?list=plzduta9mdcmoh5kvuqatd9gzvg_jtl6fx):\nfirst update ...\nsudo apt-get update\ninstall necessary python libraries and pip3; matplotlib andscipy are not required for installation but used in exudyn examples:\nsudo dpkg --configure -a\nsudo apt install python3-pip\npip3 install numpy\npip3 install matplotlib\npip3 install scipy\ninstall pybind11 (needed for running the setup.py file derived from the pybind11 example):\npip3 install pybind11\nif graphics is used (\\#define use_glfw_graphics in basicdefinitions.h), you must install the according glfw and opengl libs:\nsudo apt-get install freeglut3 freeglut3-dev\nsudo apt-get install mesa-common-dev\nsudo apt-get install libglfw3 libglfw3-dev\nsudo apt-get install libx11-dev xorg-dev libglew1.5 libglew1.5-dev libglu1-mesa libglu1-mesa-dev libgl1-mesa-glx libgl1-mesa-dev\nwith all of these libs, you can run the setup.py installer (go to exudyn_git/main folder), which takes some minutes for compilation (the --user option is used to install in local user folder):\nsudo python3 setup.py install --user\ncongratulation! now, run a test example (will also open an opengl window if successful):\npython3 pythondev/examples/rigid3dexample.py\nyou can also create a ubuntu wheel which can be easily installed on the same machine (x64), same operating system (ubuntu18.04) and with same python version (e.g., 3.6):\nsudo pip3 install wheelsudo python3 setup.py bdist_wheel\nknown issues for linux builds:\nusing wsl2 (windows subsystem for linux), there occur some conflicts during build because of incompatible windows and linux file systems and builds will not be copied to the dist folder; workaround: go to explorer, right click on 'build' directory and set all rights for authenticated user to 'full access'\ncompiler (gcc,g++) conflicts: it seems that exudyn works well on ubuntu18.04 with the original python 3.6.9 and gcc-7.5.0 version as well as with ubuntu20.04 with python 3.8.5 and gcc-9.3.0. upgrading gcc on a linux system with python 3.6 to, e.g., gcc-8.2 showed us a linker error when loading the exudyn module in python -- there are some common restriction using gcc versions different from those with which the python version has been built. starting python or python3 on your linux machine shows you the gcc version it had been build with.\ncheck your current gcc version with: gcc --version\nuninstall exudyn\nto uninstall exudyn under windows, run (may require admin rights):\npip uninstall exudyn\nto uninstall under ubuntu, run:\nsudo pip3 uninstall exudyn\nif you upgrade to a newer version, uninstall is usually not necessary!\nhow to install exudyn and use the c++ source code (advanced)?\nexudyn is still under intensive development of core modules. there are several ways of using the code, but you cannot install exudyn as compared to other executable programs and apps.\nin order to make full usage of the c++ code and extending it, you can use:\nwindows / microsoft visual studio 2017 and above:\nget the files from git\nput them into a local directory (recommended: c:/data/cpp/exudyn_git)\nstart main_sln.sln with visual studio\ncompile the code and run main/pythondev/pytest.py example code\nadapt pytest.py for your applications\nextend the c++ source code\nlink it to your own code\nnote: on linux systems, you mostly need to replace '/' with ''\nlinux, etc.: not fully supported yet; however, all external libraries are linux-compatible and thus should run with minimum adaptation efforts.\nfurther notes\ngoals of exudyn\nafter the first development phase (2019-2020), it shall\nbe a small multibody library, which can be easily linked to other projects,\nallow to efficiently simulate small scale systems (compute 100000s time steps per second for systems with n_dof<10),\nallow to efficiently simulate medium scaled systems for problems with n_dof < 1,000,000,\nsafe and widely accessible module for python,\nallow to add user defined objects in c++,\nallow to add user defined solvers in python.\nfuture goals are:\nextend tests,\nadd more multi-threaded parallel computing techniques (first trials implemented, improvements planned: q3 2021),\nadd vectorization,\nadd specific and advanced connectors/constraints (3d revolute joint and prismatic joint instead of generic joint, extended wheels, contact, control connector)\nmore interfaces for robotics,\nadd 3d beams,\nextend floating frame of reference formulation with modal reduction\nfor specific open issues, see trackerlog.html.\nrun a simple example in spyder\nafter performing the steps of the previous section, this section shows a simplistic model which helps you to check if exudyn runs on your computer.\nin order to start, run the python interpreter spyder. for the following example,\nopen myfirstexample.py from your exudyn32bitspython36 (or any other directory according to your python version) directory\nhereafter, press the play button or f5 in spyder.\nif successful, the ipython console of spyder will print something like:\nrunfile('c:/data/cpp/exudyn_git/main/bin/exudyn32bitspython36/myfirstexample.py',\nwdir='c:/data/cpp/exudyn_git/main/bin/exudyn32bitspython36')\n+++++++++++++++++++++++++++++++\nexudyn v1.0.1 solver: implicit second order time integration\nstep100, t = 1 sec, timetogo = 0 sec, nit/step = 1\nsolver finished after 0.0007824 seconds.\nif you check your current directory (where myfirstexample.py lies), you will find a new file coordinatessolution.txt, which contains the results of your computation (with default values for time integration). the beginning and end of the file should look like:\n#exudyn generalized alpha solver solution file\n#simulation started=2019-11-14,20:35:12\n#columns contain: time, ode2 displacements, ode2 velocities, ode2 accelerations, ae coordinates, ode2 velocities\n#number of system coordinates [node2, node1, nalgebraic, ndata] = [2,0,0,0]\n#number of written coordinates [node2, nvel2, nacc2, node1, nvel1, nalgebraic, ndata] = [2,2,2,0,0,0,0]\n#total columns exported (excl. time) = 6\n#number of time steps (planned) = 100\n#\n0,0,0,0,0,0.0001,0\n0.02,2e-08,0,2e-06,0,0.0001,0\n0.03,4.5e-08,0,3e-06,0,0.0001,0\n0.04,8e-08,0,4e-06,0,0.0001,0\n0.05,1.25e-07,0,5e-06,0,0.0001,0\n...\n0.96,4.608e-05,0,9.6e-05,0,0.0001,0\n0.97,4.7045e-05,0,9.7e-05,0,0.0001,0\n0.98,4.802e-05,0,9.8e-05,0,0.0001,0\n0.99,4.9005e-05,0,9.9e-05,0,0.0001,0\n1,5e-05,0,0.0001,0,0.0001,0\n#simulation finished=2019-11-14,20:35:12\n#solver info: erroroccurred=0,converged=1,solutiondiverged=0,total time steps=100,total newton iterations=100,total newton jacobians=100\nwithin this file, the first column shows the simulation time and the following columns provide solution of coordinates, their derivatives and lagrange multipliers on system level. as expected, the x-coordinate of the point mass has constant acceleration a=f/m=0.001/10=0.0001, the velocity grows up to 0.0001 after 1 second and the point mass moves 0.00005 along the x-axis.\ntrouble shooting and faq\ntrouble shooting\npython import errors:\nsometimes the exudyn module cannot be loaded into python. typical error messages if python versions are not compatible are:\ntraceback (most recent call last):\nfile \"<ipython-input-14-df2a108166a6>\", line 1, in <module>\nimport exudyncpp\nimporterror: module use of python36.dll conflicts with this version of python.\ntypical error messages if 32/64 bits versions are mixed:\ntraceback (most recent call last):\nfile \"<ipython-input-2-df2a108166a6>\", line 1, in <module>\nimport exudyncpp\nimporterror: dll load failed: \\%1 is not a valid win32 application.\nthere are several reasons and workarounds:\n=> you mixed up 32 and 64 bits version (see below)\n=> you are using an exudyn version for python x_1.y_1 (e.g., 3.6.z_1) different from the python x_2.y_2 version in your anaconda (e.g., 3.7.z_2); note that x_1=x_2 and y_1=y_2 must be obeyed while z_1 and z_2 may be different\nmodulenotfounderror: no module named 'exudyncpp':\n=> a known reason is that your cpu does not support avx2, while exudyn is compiled with the avx2 option (not support avx2, e.g., intel celeron g3900, intel core 2 quad q6600, intel pentium gold g5400t; check the system settings of your computer to find out the processor type; typical cpu manufacturer pages or wikipedia provide information on this).\n=> workaround to solve the avx problem: use the python 3.6 32bits version, which is compiled without avx2; you can also compile for your specific python version without avx if you adjust the setup.py file in the main folder.\n=> the modulenotfounderror may also happen if something went wrong during installation (paths, problems with anaconda, ..) => very often a new installation of anaconda and exudyn helps.\ntypical python errors:\ntypical python syntax error with missing braces:\nfile \"c:\\data\\cpp\\exudyn_git\\main\\pythondev\\examples\\springdampertutorial.py\", line 42\nnground=mbs.addnode(nodepointground(referencecoordinates = [0,0,0]))\n^\nsyntaxerror: invalid syntax\n=> such an error points to the line of your code (line 42), but in fact the error may have been caused in previous code, such as in this case there was a missing brace in the line 40, which caused the error:\n38 n1=mbs.addnode(point(referencecoordinates = [l,0,0],\n39 initialcoordinates = [u0,0,0],\n40 initialvelocities= [v0,0,0])\n41 #ground node\n42 nground=mbs.addnode(nodepointground(referencecoordinates = [0,0,0]))\n43\ntypical python import error message on linux / ubuntu if python modules are missing:\npython warning [file '/home/johannes/.local/lib/python3.6/site-packages/exudyn/solver.py', line 236]:\nerror when executing process showvisualizationsettingsdialog':\nmodulenotfounderror: no module named 'tkinter'\n=> see installation instructions to install missing python modules, thedoc.pdf.\ntypical solver errors:\nsolvedynamic or solvestatic terminated due to errors:\n=> use flag showhints = true in solvedynamic or solvestatic\nvery simple example without loads leads to error: solvedynamic or solvestatic terminated due to errors:\n=> if you do not add loads to the system and if there are no forces, the residual nearly gives 0 (due to round off errors). the newton solver tries to reduce the error by the factor given in simulationsettings.staticsolver.newton.relativetolerance (for static solver), which is not possible for 0 residual. the absolute tolerance is helping out as a lower bound for the error, given in simulationsettings.staticsolver.newton.absolutetolerance (for static solver), which is by default rather low (1e-10). increasing this value helps to solve unloaded problems. nevertheless, you should usually set this tolerance as low as possible because otherwise, your solution may become inaccurate.\ntypical solver error due to redundant constraints or missing inertia terms, could read as follows:\n=========================================\nsystem error [file 'c:\\programdata\\anaconda3_64b37\\lib\\site-packages\\exudyn\\solver.py', line 207]:\ncsolverbase::newton: system jacobian seems to be singular / not invertible!\ntime/load step #1, time = 0.0002\ncausing system equation number (coordinate number) = 42\n=========================================\n=> this solver error shows that equation 42 is not solvable. the according coordinate is shown later in such an error message:\n...\nthe causing system equation 42 belongs to a algebraic variable (lagrange multiplier)\npotential object number(s) causing linear solver to fail: [7]\nobject 7, name='object7', type=jointgeneric\n=> object 7 seems to be the reason, possibly there are too much (joint) constraints applied to your system, check this object.\n=> show typical reasons and solutions, by using showhints=true in exu.solvedynamic(...) or exu.solvestatic(...)\n=> you can also highlight object 7 by using the following code in the ipython console:\nexu.startrenderer()\nhighlightitem(sc,mbs,7)\nwhich draws the according object in red and others gray/transparent (but sometimes objects may be hidden inside other objects!). see the command's description for further options, e.g., to highlight nodes.\ntypical solver error if newton does not converge:\n+++++++++++++++++++++++++++++++\nexudyn v1.0.200 solver: implicit second order time integration\nnewton (time/load step #1): convergence failed after 25 iterations; relative error = 0.079958, time = 2\nnewton (time/load step #1): convergence failed after 25 iterations; relative error = 0.0707764, time = 1\nnewton (time/load step #1): convergence failed after 25 iterations; relative error = 0.0185745, time = 0.5\nnewton (time/load step #2): convergence failed after 25 iterations; relative error = 0.332953, time = 0.5\nnewton (time/load step #2): convergence failed after 25 iterations; relative error = 0.0783815, time = 0.375\nnewton (time/load step #2): convergence failed after 25 iterations; relative error = 0.0879718, time = 0.3125\nnewton (time/load step #2): convergence failed after 25 iterations; relative error = 2.84704e-06, time = 0.28125\nnewton (time/load step #3): convergence failed after 25 iterations; relative error = 1.9894e-07, time = 0.28125\nstep348, t = 20 sec, timetogo = 0 sec, nit/step = 7.00575\nsolver finished after 0.258349 seconds.\n=> this solver error is caused, because the nonlinear system cannot be solved using newton's method.\n=> the static or dynamic solver by default tries to reduce step size to overcome this problem, but may fail finally (at minimum step size).\n=> possible reasons are: too large time steps (reduce step size by using more steps/second), inappropriate initial conditions, or inappropriate joints or constraints (remove joints to see if they are the reason), usually within a singular configuration. sometimes a system may be just unsolvable in the way you set it up.\ntypical solver error if (e.g., syntax) error in user function (output may be very long, read always message on top!):\n=========================================\nsystem error [file 'c:\\programdata\\anaconda3_64b37\\lib\\site-packages\\exudyn\\solver.py', line 214]:\nerror in python user function 'loadcoordinate::loadvectoruserfunction' (referred line number my be wrong!):\nnameerror: name 'sin' is not defined\nat:\nc:\\data\\cpp\\documentationandinformation\\tests\\springdamperuserfunctiontest.py(48): sweep\nc:\\data\\cpp\\documentationandinformation\\tests\\springdamperuserfunctiontest.py(54): userload\nc:\\programdata\\anaconda3_64b37\\lib\\site-packages\\exudyn\\solver.py(214): solvedynamic\nc:\\data\\cpp\\documentationandinformation\\tests\\springdamperuserfunctiontest.py(106): <module>\nc:\\programdata\\anaconda3_64b37\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py(377): exec_code\nc:\\programdata\\anaconda3_64b37\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py(476): runfile\n<ipython-input-14-323569bebfb4>(1): <module>\nc:\\programdata\\anaconda3_64b37\\lib\\site-packages\\ipython\\core\\interactiveshell.py(3331): run_code\n...\n...\n; check your python code!\n=========================================\nsolver stopped! use showhints=true to show helpful information\n=> this indicates an error in the user function loadcoordinate::loadvectoruserfunction, because sin function has not been defined (must be imported, e.g., from math). it indicates that the error occurred in line 48 in springdamperuserfunctiontest.py within function sweep, which has been called from function userload, etc.\nfaq\nsome frequently asked questions:\nwhen importing exudyn in python (windows) i get an error\n=> see trouble shooting instructions above!\ni do not understand the python errors -- how can i find the reason of the error or crash?\n=> read trouble shooting section above!\n=> first, you should read all error messages and warnings: from the very first to the last message. very often, there is a definite line number which shows the error. note, that if you are executing a string (or module) as a python code, the line numbers refer to the local line number inside the script or module.\n=> if everything fails, try to execute only part of the code to find out where the first error occurs. by omiting parts of the code, you should find the according source of the error.\n=> if you think, it is a bug: send an email with a representative code snippet, version, etc.to `` reply.exudyn@gmail.com``\nspyder console hangs up, does not show error messages, ...:\n=> very often a new start of spyder helps; most times, it is sufficient to restart the kernel or to just press the 'x' in your ipython console, which closes the current session and restarts the kernel (this is much faster than restarting spyder)\n=> restarting the ipython console also brings back all error messages\nwhere do i find the '.exe' file?\n=> exudyn is only available via the python interface as a module 'exudyn', the c++ code being inside of exudyncpp.pyd, which is located in the exudyn folder where you installed the package. this means that you need to run python (best: spyder) and import the exudyn module.\ni get the error message 'check potential mixing of different (object, node, marker, ...) indices', what does it mean?\n=> probably you used wrong item indexes, see beginning of command interface in thedoc.pdf.\n=> e.g., an object number onum = mbs.addobject(...) is used at a place where a nodeindex is expected, e.g., mbs.addobject(masspoint(nodenumber=onum, ...))\n=> usually, this is an error in your code, it does not make sense to mix up these indexes!\n=> in the exceptional case, that you want to convert numbers, see beginning of thedoc.pdf.\nwhy does type auto completion not work for mbs (mainsystem)?\n=> update 2020-06-01: with spyder 4, using python 3.7, type auto completion works much better, but may find too many completions.\n=> most python environments (e.g., with spyder 3) only have information up to the first sub-structure, e.g., sc=exu.systemcontainer() provides full access to sc in the type completion, but mbs=sc.addsystem() is at the second sub-structure of the module and is not accessible.\n=> workaround: type mbs=mainsystem() before the mbs=sc.addsystem() command and the interpreter will know what type mbs is. this also works for settings, e.g., simulation settings 'newton'.\nhow to add graphics?\n=> graphics (lines, text, 3d triangular / stl mesh) can be added to all bodygraphicsdata items in objects. graphics objects which are fixed with the background can be attached to a objectground object. moving objects must be attached to the bodygraphicsdata of a moving body. other moving bodies can be realized, e.g., by adding a objectground and changing its reference with time. furthermore, objectground allows to add fully user defined graphics.\nin generatestraightlineancfcable2d\n=> coordinate constraints can be used to constrain position and rotation, e.g., fixedconstraintsnode0 = [1,1,0,1] for a beam aligned along the global x-axis;\n=> this does not work for beams with arbitrary rotation in reference configuration, e.g.,\n45textdegree. use a genericjoint with a rotationmarker instead.\nwhat is the difference between markerbodyposition and markerbodyrigid?\n=> position markers (and nodes) do not have information on the orientation (rotation). for that reason, there is a difference between position based and rigid-body based markers. in case of a rigid body attached to ground with a springdamper, you can use both, markerbodyposition or markerbodyrigid, markers. for a prismatic joint, you will need a markerbodyrigid.\ni get an error in exu.solvedynamic(mbs, ...) or in exu.solvestatic(mbs, ...) but no further information -- how can i solve it?\n=> typical time integration errors may look like:\nfile \"c:/data/cpp/exudyn_git/main/pythondev/...<file name>\", line xxx, in <module>\nsolver.solvesystem(...)\nsystemerror: <built-in method solvesystem of pycapsule object at 0x0cc63590> returned a result with an error set\n=> the pre-checks, which are performed to enable a crash-free simulation are insufficient for your model\n=> as a first try, restart the ipython console in order to get all error messages, which may be blocked due to a previous run of exudyn.\n=> very likely, you are using python user functions inside exudyn : they lead to an internal python error, which is not always catched by exudyn ; e.g., a load user function ufload(mbs,~t,~load), which tries to access component load[3] of a load vector with 3 components will fail internally;\n=> use the print(...) command in python at many places to find a possible error in user functions (e.g., put print(\"start user function xyz\") at the beginning of every user function; test user functions from ipython console\n=> it is also possible, that you are using inconsistent data, which leads to the crash. in that case, you should try to change your model: omit parts and find out which part is causing your error\n=> see also i do not understand the python errors -- how can i find the cause?\nwhy can't i get the focus of the simulation window on startup (render window hidden)?\n=> starting exudyn out of spyder might not bring the simulation window to front, because of specific settings in spyder(version 3.2.8), e.g., tools=>preferences=>editor=>advanced settings: uncheck 'maintain focus in the editor after running cells or selections'; alternatively, set sc.visualizationsettings.window.alwaysontop=true before starting the renderer with exu.startrenderer()\noverview on exudyn\nmodule structure\nthis section will show:\noverview of modules\nconventions: dimension of nodes, objects and vectors\ncoordinates: reference coordinates and displacements\nnodes, objects, markers and loads\nfor an introduction to the solvers, see thedoc.pdf.\noverview of modules\ncurrently, the module structure is simple:\npython parts:\niteminterface: contains the interface, which transfers python classes (e.g., of a nodepoint) to dictionaries that can be understood by the c++ module\nexudynutilities: constains helper classes in python, which allows simpler working with exudyn\nc++ parts, see figs.[thedoc.pdf] and [thedoc.pdf]:\nexudyn: on this level, there are just very few functions: systemcontainer(), startrenderer(), stoprenderer(), getversionstring(), solvestatic(...), solvedynamic(...), ... as well as system and user variable dictionaries exudyn.variables and exudyn.sys\nsystemcontainer: contains the systems (most important), solvers (static, dynamics, ...), visualization settings\nmbs: system created with mbs = sc.addsystem(), this structure contains everything that defines a solvable multibody system; a large set of nodes, objects, markers, loads can added to the system, see thedoc.pdf;\nmbs.systemdata: contains the initial, current, visualization, ... states of the system and holds the items, see [figure in thedoc.pdf]\nconventions: items, indexes, coordinates\nin this documentation, we will use the term item to identify nodes, objects, markers, loads and sensors:\nitem in node, object, marker, load, sensor\nindexes: arrays and vector starting with 0:\nas known from python, all indexes of arrays, vectors, matrices, ...are starting with 0. this means that the first component of the vector v=[1,2,3] is accessed with v[0] in python (and also in the c++ part of exudyn ). the range is usually defined as range(0,3), in which '3' marks the index after the last valid component of an array or vector.\n**dimensionality of objects and vectors: **\nac2d vs.ac3d\nas a convention, quantities in exudyn are 3d, such as nodes, objects, markers, loads, measured quantities, etc. for that reason, we denote planar nodes, objects, etc.with the suffix 2d, but 3d objects do not get this suffix.\noutput and input to objects, markers, loads, etc.is usually given by 3d vectors (or matrices), such as (local) position, force, torque, rotation, etc. however, initial and reference values for nodes depend on their dimensionality. as an example, consider a nodepoint2d:\nreferencecoordinates is a 2d vector (but could be any dimension in general nodes)\nmeasuring the current position of nodepoint2d gives a 3d vector\nwhen attaching a markernodeposition and a loadforcevector, the force will be still a 3d vector\nfurthermore, the local position in 2d objects is provided by a 3d vector. usually, the dimensionality is given in the reference manual. user errors in the dimensionality will be usually detected either by the python interface (i.e., at the time the item is created) or by the system-preprocessor\nitems: nodes, objects, loads, markers, sensors, ...\nin this section, the most important part of exudyn are provided. an overview of the interaction of the items is given in [figure in thedoc.pdf]\nnodes\nnodes provide the coordinates (and the degrees of freedom) to the system. they have no mass, stiffness or whatsoever assigned. without nodes, the system has no unknown coordinates. adding a node provides (for the system unknown) coordinates. in addition we also need equations for every nodal coordinate -- otherwise the system cannot be computed (note: this is currently not checked by the preprocessor).\nobjects\nobjects are 'computational objects' and they provide equations to your system. objects often provide derivatives and have measurable quantities (e.g. displacement) and they provide access, which can be used to apply, e.g., forces. some of this functionality is only available in c++, but not in python.\nobjects can be a:\ngeneral object (e.g.a controller, user defined object, ...; no example yet)\nbody: has a mass or mass distribution; markers can be placed on bodies; loads can be applied; constraints can be attached via markers; bodies can be:\nground object: has no nodes\nsimple body: has one node (e.g. mass point, rigid body)\nfinite element and more complicated body (e.g. ffrf-object): has more than one node\nconnector: uses markers to connect nodes and/or bodies; adds additional terms to system equations either based on stiffness/damping or with constraints (and lagrange multipliers). possible connectors:\nalgebraic constraint (e.g. constrain two coordinates: q_1 = q_2)\nclassical joint\nspring-damper or penalty constraint\nmarkers\nmarkers are interfaces between objects/nodes and constraints/loads. a constraint (which is also an object) or load cannot act directly on a node or object without a marker. as a benefit, the constraint or load does not need to know whether it is applied, e.g., to a node or to a local position of a body.\ntypical situations are:\nnode -- marker -- load\nnode -- marker -- constraint (object)\nbody(object) -- marker -- load\nbody1 -- marker1 -- joint(object) -- marker2 -- body2\nloads\nloads are used to apply forces and torques to the system. the load values are static values. however, you can use python functionality to modify loads either by linearly increasing them during static computation or by using the 'mbs.setprestepuserfunction(...)' structure in order to modify loads in every integration step depending on time or on measured quantities (thus, creating a controller).\nsensors\nsensors are only used to measure output variables (values) in order to simpler generate the requested output quantities. they have a very weak influence on the system, because they are only evaluated after certain solver steps as requested by the user.\nreference coordinates and displacements\nnodes usually have separated reference and initial quantities. here, referencecoordinates are the coordinates for which the system is defined upon creation. reference coordinates are needed, e.g., for definition of joints and for the reference configuration of finite elements. in many cases it marks the undeformed configuration (e.g., with finite elements), but not, e.g., for objectconnectorspringdamper, which has its own reference length.\ninitial displacement (or rotation) values are provided separately, in order to start a system from a configuration different from the reference configuration. as an example, the initial configuration of a nodepoint is given by referencecoordinates + initialcoordinates, while the initial state of a dynamic system additionally needs initialvelocities.\nexudyn basics\nthis section will show:\ninteraction with the exudyn module\nsimulation settings\nvisualization settings\ngenerating output and results\ngraphics pipeline\ngenerating animations\ninteraction with the exudyn module\nit is important that the exudyn module is basically a state machine, where you create items on the c++ side using the python interface. this helps you to easily set up models using many other python modules (numpy, sympy, matplotlib, ...) while the computation will be performed in the end on the c++ side in a very efficient manner.\nwhere do objects live?\nwhenever a system container is created with sc = exu.systemcontainer(), the structure sc becomes a variable in the python interpreter, but it is managed inside the c++ code and it can be modified via the python interface. usually, the system container will hold at least one system, usually called mbs. commands such as mbs.addnode(...) add objects to the system mbs. the system will be prepared for simulation by mbs.assemble() and can be solved (e.g., using exu.solvedynamic(...)) and evaluated hereafter using the results files. using mbs.reset() will clear the system and allows to set up a new system. items can be modified (modifyobject(...)) after first initialization, even during simulation.\nsimulation settings\nthe simulation settings consists of a couple of substructures, e.g., for solutionsettings, staticsolver, timeintegration as well as a couple of general options -- for details see sections [thedoc.pdf] -- [thedoc.pdf].\nsimulation settings are needed for every solver. they contain solver-specific parameters (e.g., the way how load steps are applied), information on how solution files are written, and very specific control parameters, e.g., for the newton solver.\nthe simulation settings structure is created with\nsimulationsettings = exu.simulationsettings()\nhereafter, values of the structure can be modified, e.g.,\ntend = 10 #10 seconds of simulation time:\nh = 0.01 #step size (gives 1000 steps)\nsimulationsettings.timeintegration.endtime = tend\n#steps for time integration must be integer:\nsimulationsettings.timeintegration.numberofsteps = int(tend/h)\n#assigns a new tolerance for newton's method:\nsimulationsettings.timeintegration.newton.relativetolerance = 1e-9\n#write some output while the solver is active (slower):\nsimulationsettings.timeintegration.verbosemode = 2\n#write solution every 0.1 seconds:\nsimulationsettings.solutionsettings.solutionwriteperiod = 0.1\n#use sparse matrix storage and solver (package eigen):\nsimulationsettings.linearsolvertype = exu.linearsolvertype.eigensparse\ngenerating output and results\nthe solvers provide a number of options in solutionsettings to generate a solution file. as a default, exporting solution to the solution file is activated with a writing period of 0.01 seconds.\ntypical output settings are:\n#create a new simulationsettings structure:\nsimulationsettings = exu.simulationsettings()\n#activate writing to solution file:\nsimulationsettings.solutionsettings.writesolutiontofile = true\n#write results every 1ms:\nsimulationsettings.solutionsettings.solutionwriteperiod = 0.001\n#assign new filename to solution file\nsimulationsettings.solutionsettings.coordinatessolutionfilename= \"myoutput.txt\"\n#do not export certain coordinates:\nsimulationsettings.solutionsettings.exportdatacoordinates = false\nvisualization settings\nvisualization settings are used for user interaction with the model. e.g., the nodes, markers, loads, etc., can be visualized for every model. there are default values, e.g., for the size of nodes, which may be inappropriate for your model. therefore, you can adjust those parameters. in some cases, huge models require simpler graphics representation, in order not to slow down performance -- e.g., the number of faces to represent a cylinder should be small if there are 10000s of cylinders drawn. even computation performance can be slowed down, if visualization takes lots of cpu power. however, visualization is performed in a separate thread, which usually does not influence the computation exhaustively. details on visualization settings and its substructures are provided in sections [thedoc.pdf] -- [thedoc.pdf].\nthe visualization settings structure can be accessed in the system container sc (access per reference, no copying!), accessing every value or structure directly, e.g.,\nsc.visualizationsettings.nodes.defaultsize = 0.001 #draw nodes very small\n#change opengl parameters; current values can be obtained from sc.getrenderstate()\n#change zoom factor:\nsc.visualizationsettings.opengl.initialzoom = 0.2\n#set the center point of the scene (can be attached to moving object):\nsc.visualizationsettings.opengl.initialcenterpoint = [0.192, -0.0039,-0.075]\n#turn of auto-fit:\nsc.visualizationsettings.general.autofitscene = false\n#change smoothness of a cylinder:\nsc.visualizationsettings.general.cylindertiling = 100\n#make round objects flat:\nsc.visualizationsettings.opengl.shademodelsmooth = false\n#turn on coloured plot, using y-component of displacements:\nsc.visualizationsettings.contour.outputvariable = exu.outputvariabletype.displacement\nsc.visualizationsettings.contour.outputvariablecomponent = 1 #0=x, 1=y, 2=z\nstoring the model view\nthere is a simple way to store the current view (zoom, centerpoint, orientation, etc.) by using sc.getrenderstate() and sc.setrenderstate(). a simple way is to reload the stored render state (model view) after simulating your model once at the end of the simulation ( note that visualizationsettings.general.autofitscene should be set false if you want to use the stored zoom factor):\nimport exudyn as exu\nsc=exu.systemcontainer()\nsc.visualizationsettings.general.autofitscene = false #prevent from autozoom\nexu.startrenderer()\nif 'renderstate' in exu.sys:\nsc.setrenderstate(exu.sys['renderstate'])\n#+++++++++++++++\n#do simulation here and adjust model view settings with mouse\n#+++++++++++++++\n#store model view for next run:\nstoprenderer() #stores render state in exu.sys['renderstate']\nalternatively, you can obtain the current model view from the console after a simulation, e.g.,\nin[1] : sc.getrenderstate()\nout[1]:\n'centerpoint': [1.0, 0.0, 0.0],\n'maxscenesize': 2.0,\n'zoom': 1.0,\n'currentwindowsize': [1024, 768],\n'modelrotation': [[ 0.34202015, 0. , 0.9396926 ],\n[-0.60402274, 0.76604444, 0.21984631],\n[-0.7198463 , -0.6427876 , 0.26200265]])\nwhich contains the last state of the renderer. now copy the output and set this with sc.setrenderstate in your python code to have a fixed model view in every simulation (sc.setrenderstate after exu.startrenderer()):\nsc.visualizationsettings.general.autofitscene = false #prevent from autozoom\nexu.startrenderer()\nrenderstate='centerpoint': [1.0, 0.0, 0.0],\n'maxscenesize': 2.0,\n'zoom': 1.0,\n'currentwindowsize': [1024, 768],\n'modelrotation': [[ 0.34202015, 0. , 0.9396926 ],\n[-0.60402274, 0.76604444, 0.21984631],\n[-0.7198463 , -0.6427876 , 0.26200265]])\nsc.setrenderstate(renderstate)\n#.... further code for simulation here\ngraphics pipeline\nthere are basically two loops during simulation, which feed the graphics pipeline. the solver runs a loop:\ncompute new step\nfinish computation step; results are in current state\ncopy current state to visualization state (thread safe)\nsignal graphics pipeline that new visualization data is available\nthe opengl graphics thread (=separate thread) runs the following loop:\nrender opengl scene with a given graphicsdata structure (containing lines, faces, text, ...)\ngo idle for some milliseconds\ncheck if opengl rendering needs an update (e.g. due to user interaction) => if update is needed, the visualization of all items is updated -- stored in a graphicsdata structure)\ncheck if new visualization data is available and the time since last update is larger than a presribed value, the graphicsdata structure is updated with the new visualization state\ngraphics user python functions\nthere are some user functions in order to customize drawing:\nyou can assign graphicsdata to the visualization to most bodies, such as rigid bodies in order to change the shape. graphics can also be imported from stl files (graphicsdatafromstlfiletxt).\nsome objects, e.g., objectgenericode2 or objectrigidbody, provide customized a function graphicsdatauserfunction. this user function just returns a list of graphicsdata, see thedoc.pdf. with this function you can change the shape of the body in every step of the computation.\nspecifically, the graphicsdatauserfunction in objectground can be used to draw any moving background in the scene.\nnote that all kinds of graphicsuserpythonfunctions need to be called from the main (=computation) process as python functions may not be called from separate threads (gil). therefore, the computation thread is interrupted to execute the graphicsdatauserfunction between two time steps, such that the graphics python user function can be executed. there is a timeout variable for this interruption of the computation with a warning if scenes get too complicated.\ncolor and rgba\nmany functions and objects include color information. in order to allow transparency, all colors contain a list of 4 rgba values, all values being in the range [0..1]:\nred (r) channel\ngreen (g) channel\nblue (b) channel\nalpha (a) value, representing transparency (a=0: fully transparent, a=1: solid)\ne.g., red color with no transparency is obtained by the color=[1,0,0,1]. color predefinitions are found in exudyngraphicsdatautilities.py, e.g., color4red or color4steelblue as well a list of 10 colors color4list, which is convenient to be used in a loop creating objects.\ncamera following objects and interacting with model view\nfor some models, it may be advantageous to track the translation and/or rotation of certain bodies, e.g., for cars, (wheeled) robots or bicycles. to do so, the current render state (sc.getrenderstate(), sc.setrenderstate(...)) can be obtained and modified, in order to always follow a certain position. as this needs to be done during redraw of every frame, it is conveniently done in a graphicsuserfunction, e.g., within the ground body. this is shown in the following example, in which mbs.variables['ntracknode'] is a node number to be tracked:\n#mbs.variables['ntracknode'] contains node number\ndef ufgraphics(mbs, objectnum):\nn = mbs.variables['ntracknode']\np = mbs.getnodeoutput(n,exu.outputvariabletype.position,\nconfiguration=exu.configurationtype.visualization)\nrs=sc.getrenderstate() #get current render state\na = np.array(rs['modelrotation'])\np = a.t @ p #transform point into model view coordinates\nrs['centerpoint']=[p[0],p[1],p[2]]\nsc.setrenderstate(rs) #modify render state\nreturn []\n#add object with graphics user function\noground2 = mbs.addobject(objectground(visualization=\nvobjectground(graphicsdatauserfunction=ufgraphics)))\n#.... further code for simulation here\nsolution viewer\nexudyn offers a convenient wysiwys -- 'what you see is what you simulate' interface, showing you the computation results during simulation. if you are running large models, it may be more convenient to watch results after simulation has been finished. for this, you can use\nutilities.animatesolution, see section [thedoc.pdf]\ninteractive.solutionviewer, see section [thedoc.pdf]\ninteractive.animatemodes, lets you view the animation of computed modes, see section [thedoc.pdf]\nthe function animatesolution allows to directly visualize the stored solution for according stored time frames. the solutionviewer adds a tkinter interactive dialog, which lets you interact with the model ('player'). in both methods animatesolution and solutionviewer, the solution needs to be loaded with loadsolutionfile('coordinatessolution.txt'), where 'coordinatessolution.txt' represents the stored solution file, see\nexu.simulationsettings().solutionsettings.coordinatessolutionfilename\nyou can call the solutionviewer either in the model, or at the command line / ipython to load a previous solution (belonging to the same mbs underlying the solution!):\nfrom exudyn.interactive import solutionviewer\nsol = loadsolutionfile('coordinatessolution.txt')\nsolutionviewer(mbs, sol)\nalternatively, you can just reload the last stored solution (according to your simulationsettings):\nfrom exudyn.interactive import solutionviewer\nsolutionviewer(mbs)\nan example for the solutionviewer is integrated into the examples/ directory, see solutionviewertest.py.\ngenerating animations\nin many dynamics simulations, it is very helpful to create animations in order to better understand the motion of bodies. specifically, the animation can be used to visualize the model much slower or faster than the model is computed.\nanimations are created based on a series of images (frames, snapshots) taken during simulation. it is important, that the current view is used to record these images -- this means that the view should not be changed during the recording of images. to turn on recording of images during solving, set the following flag to a positive value\nsimulationsettings.solutionsettings.recordimagesinterval = 0.01\nwhich means, that after every 0.01 seconds of simulation time, an image of the current view is taken and stored in the directory and filename (without filename ending) specified by\nsc.visualizationsettings.exportimages.saveimagefilename = \"myfolder/frame\"\nby default, a consecutive numbering is generated for the image, e.g., 'frame0000.tga, frame0001.tga,...'. note that '.tga' files contain raw image data and therefore can become very large.\nto create animation files, an external tool ffmpeg is used to efficiently convert a series of images into an animation. => see thedoc.pdf !\nc++ code\nthis section covers some information on the c++ code. for more information see the open source code and use doxygen.\nexudyn was developed for the efficient simulation of flexible multi-body systems. exudyn was designed for rapid implementation and testing of new formulations and algorithms in multibody systems, whereby these algorithms can be easily implemented in efficient c++ code. the code is applied to industry-related research projects and applications.\nfocus of the c++ code\nfour principles:\ndeveloper-friendly\nerror minimization\nefficiency\nuser-friendliness\nthe focus is therefore on:\na developer-friendly basic structure regarding the c++ class library and the possibility to add new components.\nthe basic libraries are slim, but extensively tested; only the necessary components are available\ncomplete unit tests are added to new program parts during development; for more complex processes, tests are available in python\nin order to implement the sometimes difficult formulations and algorithms without errors, error avoidance is always prioritized.\nto generate efficient code, classes for parallelization (vectorization and multithreading) are provided. we live the principle that parallelization takes place on multi-core processors with a central main memory, and thus an increase in efficiency through parallelization is only possible with small systems, as long as the program runs largely in the cache of the processor cores. vectorization is tailored to simd commands as they have intel processors, but could also be extended to gpgpus in the future.\nthe user interface (python) provides a 1:1 image of the system and the processes running in it, which can be controlled with the extensive possibilities of python.\nc++ code structure\nthe functionality of the code is based on systems (mainsystem/csystem) representing the multibody system or similar physical systems to be simulated. parts of the core structure of exudyn are:\ncsystem / mainsystem: a multibody system which consists of nodes, objects, markers, loads, etc.\nsystemcontainer: holds a set of systems; connects to visualization (container)\nnode: used to hold coordinates (unknowns)\n(computational) object: leads to equations, using nodes\nmarker: defines a consistent interface to objects (bodies) and nodes; write access ('accessfunction') -- provides jacobian and read access ('outputvariable')\nload: acts on an object or node via a marker\ncomputational objects: efficient objects for computation = bodies, connectors, connectors, loads, nodes, ...\nvisualization objects: interface between computational objects and 3d graphics\nmain (manager) objects: do all tasks (e.g. interface to visualization objects, gui, python, ...) which are not needed during computation\nstatic solver, kinematic solver, time integration\npython interface via pybind11; items are accessed with a dictionary interface; system structures and settings read/written by direct access to the structure (e.g. simulationsettings, visualizationsettings)\ninterfaces to linear solvers; future: optimizer, eigenvalue solver, ... (mostly external or in python)\nc++ code: modules\nthe following internal modules are used, which are represented by directories in main/src:\nautogenerated: item (nodes, objects, markers and loads) classes split into main (management, python connection), visualization and computation\ngraphics: a general data structure for 2d and 3d graphical objects and a tiny opengl visualization; linkage to glfw\nlinalg: linear algebra with vectors and matrices; separate classes for small vectors (slimvector), large vectors (vector and resizablevector), vectors without copying data (linkeddatavector), and vectors with constant size (constvector)\nmain: mainly contains systemcontainer, system and objectfactory\nobjects: contains the implementation part of the autogenerated items\npymodules: manually created libraries for linkage to python via pybind; remaining linking to python is located in autogenerated folder\npythongenerator: contains python files for automatic generation of c++ interfaces and python interfaces of items;\nsolver: contains all solvers for solving a csystem\nsystem: contains core item files (e.g., mainnode, cnode, mainobject, cobject, ...)\ntests: files for testing of internal linalg (vector/matrix), data structure libraries (array, etc.) and functions\nutilities: array structures for administrative/managing tasks (indexes of objects ... bodies, forces, connectors, ...); basic classes with templates and definitions\nthe following main external libraries are linked to exudyn:\nlest: for testing of internal functions (e.g. linalg)\nglfw: 3d graphics with opengl; cross-platform capabilities\neigen: linear algebra for large matrices, linear solvers, sparse matrices and link to special solvers\npybind11: linking of c++ to python\ncode style and conventions\nthis section provides general coding rules and conventions, partly applicable to the c++ and python parts of the code. many rules follow common conventions (e.g., google code style, but not always -- see notation):\nwrite simple code (no complicated structures or uncommon coding)\nwrite readable code (e.g., variables and functions with names that represent the content or functionality; avoid abbreviations)\nput a header in every file, according to doxygen format\nput a comment to every (global) function, member function, data member, template parameter\nalways use curly brackets for single statements in 'if', 'for', etc.; example: if (i<n) i += 1;\nuse doxygen-style comments (use '//!' qt style and '@ date' with '@' instead of '' for commands)\nuse doxygen (with preceeding '@') 'test' for tests, 'todo' for todos and 'bug' for bugs\nuse 4-spaces-tab\nuse c++11 standards when appropriate, but not exhaustively\none class one file rule (except for some collectors of single implementation functions)\nadd complete unit test to every function (every file has link to lest library)\navoid large classes (>30 member functions; > 15 data members)\nsplit up god classes (>60 member functions)\nmark changed code with your name and date\nreplace tabs by spaces: extras->options->c/c++->tabstopps: tab stopp size = 4 (=standard) + keep spaces=yes\nnotation conventions\nthe following notation conventions are applied (no exceptions!):\nuse lowercamelcase for names of variables (including class member variables), consts, c-define variables, ...; exception: for algorithms following formulas, e.g., f = m*q_tt + k*q, gbar, ...\nuse uppercamelcase for functions, classes, structs, ...\nspecial cases for camelcase: write 'odesystem', but: 'ode1equations'\n'[...]init' ... in arguments, for initialization of variables; e.g. 'valueinit' for initialization of member variable 'value'\nuse american english troughout: visualization, etc.\nfor (abbreviations) in captial letters, e.g. ode, use a lower case letter afterwards:\ndo not use consecutive capitalized words, e.g. do not write 'odeae'\nfor functions use odecomputecoords(), for variables avoid 'ode' at beginning: use node or write odecoords\ndo not use '_' within variable or function names; exception: derivatives\nuse name which exactly describes the function/variable: 'numberofitems' instead of 'size' or 'l'\nexamples for variable names: secondordersize, massmatrix, mthetatheta\nexamples for function/class names: secondordersize, evaluatemassmatrix, position(const vector3d\\& localposition)\nuse the get/set...() convention if data is retrieved from a class (get) or something is set in a class (set); use const t\\& get()/t\\& get if direct access to variables is needed; use get/set for pybind11\nexample get/set: real* getdatapointer(), vector::setall(real), gettransposed(), setrotationalparameters(...), setcolor(...), ...\nuse 'real' instead of double or float: for compatibility, also for avx with sp/dp\nuse 'index' for array/vector size and index instead of size_t or int\nitem: object, node, marker, load: anything handled within the computational/visualization systems\ndo not use numbers (3 for 3d or any other number which represents, e.g., the number of rotation parameters). use const index or constexpr to define constants.\nno-abbreviations-rule\nthe code uses a minimum set of abbreviations; however, the following abbreviation rules are used throughout: in general: do not abbreviate function, class or variable names: getdatapointer() instead of getptr(); exception: cnt, i, j, k, x or v in cases where it is really clear (5-line member functions).\nexceptions to the no-abbreviations-rule:\node ... ordinary differential equations;\node2 ... marks parts related to second order differential equations (sos2, evalf2 in hotint)\node1 ... marks parts related to first order differential equations (es, evalf in hotint)\nae ... algebraic equations (is, evalg in hotint); write 'aecoordinates' for 'algebraicequationscoordinates'\n'c[...]' ... computational, e.g. for computationalnode ==> use 'cnode'\nmin, max ... minimum and maximum\nwrite time derivatives with underscore: _t, _tt; example: position_t, position_tt, ...\nwrite space-wise derivatives ith underscore: _x, _xx, _y, ...\nif a scalar, write coordinate derivative with underscore: _q, _v (derivative w.r.t. velocity coordinates)\nfor components, elements or entries of vectors, arrays, matrices: use 'item' throughout\n'[...]init' ... in arguments, for initialization of variables; e.g. 'valueinit' for initialization of member variable 'value'\ntutorial\nthis section will show:\na basic tutorial for a 1d mass and spring-damper with initial displacements, shortest possible model with practically no special settings\na more advanced rigid-body model, including 3d rigid bodies and revolute joints\nlinks to examples section\na large number of examples, some of them quite advanced, can be found in:\nmain/pythondev/examplesmain/pythondev/testmodels\nmass-spring-damper tutorial\nthe python source code of the first tutorial can be found in the file:\nmain/pythondev/examples/springdampertutorial.py\nthis tutorial will set up a mass point and a spring damper, dynamically compute the solution and evaluate the reference solution.\nwe import the exudyn library and the interface for all nodes, objects, markers, loads and sensors:\nimport exudyn as exu\nfrom exudyn.iteminterface import *\nimport numpy as np #for postprocessing\nnext, we need a systemcontainer, which contains all computable systems and add a new mainsystem mbs. per default, you always should name your system 'mbs' (multibody system), in order to copy/paste code parts from other examples, tutorials and other projects:\nsc = exu.systemcontainer()\nmbs = sc.addsystem()\nin order to check, which version you are using, you can printout the current exudyn version. this version is in line with the issue tracker and marks the number of open/closed issues added to exudyn :\nprint('exudyn version='+exu.__version__)\nusing the powerful python language, we can define some variables for our problem, which will also be used for the analytical solution:\nl=0.5 #reference position of mass\nmass = 1.6 #mass in kg\nspring = 4000 #stiffness of spring-damper in n/m\ndamper = 8 #damping constant in n/(m/s)\nf =80 #force on mass\nfor the simple spring-mass-damper system, we need initial displacements and velocities:\nu0=-0.08 #initial displacement\nv0=1 #initial velocity\nx0=f/spring #static displacement\nprint('resonance frequency = '+str(np.sqrt(spring/mass)))\nprint('static displacement = '+str(x0))\nwe first need to add nodes, which provide the coordinates (and the degrees of freedom) to the system. the following line adds a 3d node for 3d mass point (note: point is an abbreviation for nodepoint, defined in iteminterface.py.):\nn1=mbs.addnode(point(referencecoordinates = [l,0,0],\ninitialcoordinates = [u0,0,0],\ninitialvelocities = [v0,0,0]))\nhere, point (=nodepoint) is a python class, which takes a number of arguments defined in the reference manual. the arguments here are referencecoordinates, which are the coordinates for which the system is defined. the initial configuration is given by referencecoordinates + initialcoordinates, while the initial state additionally gets initialvelocities. the command mbs.addnode(...) returns a nodeindex n1, which basically contains an integer, which can only be used as node number. this node number will be used lateron to use the node in the object or in the marker.\nwhile point adds 3 unknown coordinates to the system, which need to be solved, we also can add ground nodes, which can be used similar to nodes, but they do not have unknown coordinates -- and therefore also have no initial displacements or velocities. the advantage of ground nodes (and ground bodies) is that no constraints are needed to fix these nodes. such a ground node is added via:\nnground=mbs.addnode(nodepointground(referencecoordinates = [0,0,0]))\nin the next step, we add an object (sec:programstructure.), which provides equations for coordinates. the masspoint needs at least a mass (kg) and a node number to which the mass point is attached. additionally, graphical objects could be attached:\nmasspoint = mbs.addobject(masspoint(physicsmass = mass, nodenumber = n1))\nin order to apply constraints and loads, we need markers. these markers are used as local positions (and frames), where we can attach a constraint lateron. in this example, we work on the coordinate level, both for forces as well as for constraints. markers are attached to the according ground and regular node number, additionally using a coordinate number (0 ... first coordinate):\ngroundmarker=mbs.addmarker(markernodecoordinate(nodenumber= nground,\ncoordinate = 0))\n#marker for springdamper for first (x-)coordinate:\nnodemarker = mbs.addmarker(markernodecoordinate(nodenumber= n1,\ncoordinate = 0))\nthis means that loads can be applied to the first coordinate of node n1 via marker with number nodemarker, which is in fact of type markerindex.\nnow we add a spring-damper to the markers with numbers groundmarker and the nodemarker, providing stiffness and damping parameters:\nnc = mbs.addobject(coordinatespringdamper(markernumbers = [groundmarker, nodemarker],\nstiffness = spring,\ndamping = damper))\na load is added to marker nodemarker, with a scalar load with value f:\nnload = mbs.addload(loadcoordinate(markernumber = nodemarker,\nload = f))\nfinally, a sensor is added to the coordinate constraint object with number nc, requesting the outputvariabletype force:\nmbs.addsensor(sensorobject(objectnumber=nc, filename='groundforce.txt',\noutputvariabletype=exu.outputvariabletype.force))\nnote that sensors can be attached, e.g., to nodes, bodies, objects (constraints) or loads. as our system is fully set, we can print the overall information and assemble the system to make it ready for simulation:\nprint(mbs)\nmbs.assemble()\nwe will use time integration and therefore define a number of steps (fixed step size; must be provided) and the total time span for the simulation:\ntend = 1 #end time of simulation\nh = 0.001 #step size; leads to 1000 steps\nall settings for simulation, see according reference section, can be provided in a structure given from exu.simulationsettings(). note that this structure will contain all default values, and only non-default values need to be provided:\nsimulationsettings = exu.simulationsettings()\nsimulationsettings.solutionsettings.solutionwriteperiod = 5e-3 #output interval general\nsimulationsettings.solutionsettings.sensorswriteperiod = 5e-3 #output interval of sensors\nsimulationsettings.timeintegration.numberofsteps = int(tend/h) #must be integer\nsimulationsettings.timeintegration.endtime = tend\nwe are using a generalized alpha solver, where numerical damping is needed for index 3 constraints. as we have only spring-dampers, we can set the spectral radius to 1, meaning no numerical damping:\nsimulationsettings.timeintegration.generalizedalpha.spectralradius = 1\nin order to visualize the results online, a renderer can be started. as our computation will be very fast, it is a good idea to wait for the user to press space, before starting the simulation (uncomment second line):\nexu.startrenderer() #start graphics visualization\n#mbs.waitforusertocontinue() #wait for pressing space bar to continue (in render window!)\nas the simulation is still very fast, we will not see the motion of our node. using e.g.steps=10000000 in the lines above allows you online visualize the resulting oscillations.\nfinally, we start the solver, by telling which system to be solved, solver type and the simulation settings:\nexu.solvedynamic(mbs, simulationsettings)\nafter simulation, our renderer needs to be stopped (otherwise it would stay in background and prohibit further simulations). sometimes you would like to wait until closing the render window, using waitforrenderenginestopflag():\n#sc.waitforrenderenginestopflag()#wait for pressing 'q' to quit\nexu.stoprenderer() #safely close rendering window!\nthere are several ways to evaluate results, see the reference pages. in the following we take the final value of node n1 and read its 3d position vector:\n#evaluate final (=current) output values\nu = mbs.getnodeoutput(n1, exu.outputvariabletype.position)\nprint('displacement=',u)\nthe following code generates a reference (exact) solution for our example:\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nomega0 = np.sqrt(spring/mass) #eigen frequency of undamped system\ndrel = damper/(2*np.sqrt(spring*mass)) #dimensionless damping\nomega = omega0*np.sqrt(1-drel**2) #eigen freq of damped system\nc1 = u0-x0 #static solution needs to be considered!\nc2 = (v0+omega0*drel*c1) / omega #c1, c2 are coeffs for solution\nsteps = int(tend/h) #use same steps for reference solution\nrefsol = np.zeros((steps+1,2))\nfor i in range(0,steps+1):\nt = tend*i/steps\nrefsol[i,0] = t\nrefsol[i,1] = np.exp(-omega0*drel*t)*(c1*np.cos(omega*t)+c2*np.sin(omega*t))+x0\nplt.plot(refsol[:,0], refsol[:,1], 'r-', label='displacement (m); exact solution')\nnow we can load our results from the default solution file coordinatessolution.txt, which is in the same directory as your python tutorial file. for convenient reading the file containing commented lines, we use a numpy feature and finally plot the displacement of coordinate 0 or our mass point (data[:,0] contains the simulation time, data[:,1] contains displacement of (global) coordinate 0, data[:,2] contains displacement of (global) coordinate 1, ...)):\ndata = np.loadtxt('coordinatessolution.txt', comments='#', delimiter=',')\nplt.plot(data[:,0], data[:,1], 'b-', label='displacement (m); numerical solution')\nthe sensor result can be loaded in the same way. the sensor output format contains time in the first column and sensor values in the remaining columns. the number of columns depends on the sensor and the output quantity (scalar, vector, ...):\ndata = np.loadtxt('groundforce.txt', comments='#', delimiter=',')\nplt.plot(data[:,0], data[:,1]*1e-3, 'g-', label='force (kn)')\nin order to get a nice plot within spyder, the following options can be used (note, in some environments you need finally the command plt.show()):\nax=plt.gca() # get current axes\nax.grid(true, 'major', 'both')\nax.xaxis.set_major_locator(ticker.maxnlocator(10))\nax.yaxis.set_major_locator(ticker.maxnlocator(10))\nplt.legend() #show labels as legend\nplt.tight_layout()\nplt.show()\nthe matplotlib output should look like this:\nrigid body and joints tutorial\nthe python source code of the first tutorial can be found in the file:\nmain/pythondev/examples/rigidbodytutorial3.py\nthis tutorial will set up a multibody system containing a ground, two rigid bodies and two revolute joints driven by gravity, compare a 3d view of the example in the figure above.\nwe first import the exudyn library and the interface for all nodes, objects, markers, loads and sensors:\nimport exudyn as exu\nfrom exudyn.iteminterface import *\nfrom exudyn.utilities import *\nimport numpy as np #for postprocessing\nthe submodule exudyn.utilities contains helper functions for graphics representation, 3d rigid bodies and joints.\nas in the first tutorial, we need a systemcontainer and add a new mainsystem mbs:\nsc = exu.systemcontainer()\nmbs = sc.addsystem()\nwe define some geometrical parameters for lateron use.\n#physical parameters\ng = [0,-9.81,0] #gravity\nl = 1 #length\nw = 0.1 #width\nbodydim=[l,w,w] #body dimensions\np0 = [0,0,0] #origin of pendulum\npmid0 = np.array([l*0.5,0,0]) #center of mass, body0\nwe add an empty ground body, using default values. it's origin is at [0,0,0] and here we use no visualization.\n#ground body\noground = mbs.addobject(objectground())\nfor physical parameters of the rigid body, we can use the class rigidbodyinertia, which allows to define mass, center of mass (com) and inertia parameters, as well as shifting com or adding inertias. the rigidbodyinertia can be used directly to create rigid bodies. special derived classes can be use to define rigid body inertias for cylinders, cubes, etc., so we use a cube here:\n#first link:\nicube0 = inertiacuboid(density=5000, sidelengths=bodydim)\nicube0 = icube0.translated([-0.25*l,0,0]) #transform com, com not at reference point!\nnote that the com is translated in axial direction, while it would be at the body's local position [0,0,0] by default!\nfor visualization, we need to add some graphics for the body defined as a 3d rigidlink object and we additionally draw a basis (three rgb-vectors) at the com:\n#graphics for body\ngraphicsbody0 = graphicsdatarigidlink(p0=[-0.5*l,0,0],p1=[0.5*l,0,0],\naxis0=[0,0,1], axis1=[0,0,0], radius=[0.5*w,0.5*w],\nthickness=w, width=[1.2*w,1.2*w], color=color4red)\ngraphicscom0 = graphicsdatabasis(origin=icube0.com, length=2*w)\nnow we have defined all data for the link (rigid body). we could use mbs.addnode(noderigidbodyep(...)) and mbs.addobject(objectrigidbody(...)) to create a node and a body, but the exudyn.rigidbodyutilities offer a much more comfortable function:\n[n0,b0]=addrigidbody(mainsys = mbs,\ninertia = icube0, #includes com\nnodetype = exu.nodetype.rotationeulerparameters,\nposition = pmid0,\nrotationmatrix = np.diag([1,1,1]),\ngravity = g,\ngraphicsdatalist = [graphicsbody0, graphicscom0])\nwhich also adds a gravity load and could also set initial velocities, if wanted. the nodetype specifies the underlying model for the rigid body node, see thedoc.pdf. we can use\nrotationeulerparameters: for fast computation, but leads to an additional algebraic equation and thus needs an implicit solver\nrotationrxyz: contains a singularity if the second angle reaches +/- 90 degrees, but no algebraic equations\nrotationrotationvector: basically contains a singularity for 0 degrees, but if used in combination with lie group integrators, singularities are bypassed\nwe now add a revolute joint around the (global) z-axis. we have several possibilities, which are shown in the following. for the first two possibilities only, we need the following markers\n#markers for ground and rigid body (not needed for option 3):\nmarkerground = mbs.addmarker(markerbodyrigid(bodynumber=oground, localposition=[0,0,0]))\nmarkerbody0j0 = mbs.addmarker(markerbodyrigid(bodynumber=b0, localposition=[-0.5*l,0,0]))\nthe very general option 1 is to use the genericjoint, that can be used to define any kind of joint with translations and rotations fixed or free,\n#revolute joint option 1:\nmbs.addobject(genericjoint(markernumbers=[markerground, markerbody0j0],\nconstrainedaxes=[1,1,1,1,1,0],\nvisualization=vobjectjointgeneric(axesradius=0.2*w,\naxeslength=1.4*w)))\nin addition, transformation matrices (rotationmarker0/1) can be added, see the joint description.\noption 2 is using the revolute joint, which allows a free rotation around the local z-axis of marker 0 (markerground in our example)\n#revolute joint option 2:\nmbs.addobject(objectjointrevolutez(markernumbers = [markerground, markerbody0j0],\nrotationmarker0=np.eye(3),\nrotationmarker1=np.eye(3),\nvisualization=vobjectjointrevolutez(axisradius=0.2*w,\naxislength=1.4*w)\n))\nadditional transformation matrices (rotationmarker0/1) can be added in order to chose any rotation axis.\nnote that an error in the definition of markers for the joints can be also detected in the render window (if you completed the example), e.g., if you change the following marker in the lines above,\n#example if wrong marker position is chosen:\nmarkerbody0j0 = mbs.addmarker(markerbodyrigid(bodynumber=b0, localposition=[-0.4*l,0,0]))\n=> you will see a misalignment of the two parts of the joint by 0.1*l.\ndue to the fact that the definition of markers for general joints is tedious, there is a utility function, which allows to attach revolute joints immediately to bodies and defining the rotation axis only once for the joint:\n#revolute joint option 3:\naddrevolutejoint(mbs, body0=oground, body1=b0, point=[0,0,0],\naxis=[0,0,1], useglobalframe=true, showjoint=true,\naxisradius=0.2*w, axislength=1.4*w)\nthe second link and the according joint can be set up in a very similar way:\n#second link:\ngraphicsbody1 = graphicsdatarigidlink(p0=[0,0,-0.5*l],p1=[0,0,0.5*l],\naxis0=[1,0,0], axis1=[0,0,0], radius=[0.06,0.05],\nthickness = 0.1, width = [0.12,0.12],\ncolor=color4lightgreen)\nicube1 = inertiacuboid(density=5000, sidelengths=[0.1,0.1,1])\npmid1 = np.array([l,0,0]) + np.array([0,0,0.5*l]) #center of mass, body1\n[n1,b1]=addrigidbody(mainsys = mbs,\ninertia = icube1,\nnodetype = exu.nodetype.rotationeulerparameters,\nposition = pmid1,\nrotationmatrix = np.diag([1,1,1]),\nangularvelocity = [0,0,0],\ngravity = g,\ngraphicsdatalist = [graphicsbody1])\nthe revolute joint in this case has a free rotation around the global x-axis:\n#revolute joint (free x-axis)\naddrevolutejoint(mbs, body0=b0, body1=b1, point=[l,0,0],\naxis=[1,0,0], useglobalframe=true, showjoint=true,\naxisradius=0.2*w, axislength=1.4*w)\nfinally, we also add a sensor for some output of the double pendulum:\n#position sensor at tip of body1\nsens1=mbs.addsensor(sensorbody(bodynumber=b1, localposition=[0,0,0.5*l],\nfilename='solution/sensorpos.txt',\noutputvariabletype = exu.outputvariabletype.position))\nbefore simulation, we need to call assemble() for our system, which links objects, nodes, ..., assigns initial values and does further pre-computations and checks:\nmbs.assemble()\nafter assemble(), markers, nodes, objects, etc. are linked and we can analyze the internal structure. first, we can print out useful information, either just typing mbs in the ipython console to print out overal information:\n<systemdata:\nnumber of nodes= 2\nnumber of objects = 5\nnumber of markers = 8\nnumber of loads = 2\nnumber of sensors = 1\nnumber of ode2 coordinates = 14\nnumber of ode1 coordinates = 0\nnumber of ae coordinates = 12\nnumber of data coordinates = 0\nfor details see mbs.systemdata, mbs.sys and mbs.variables\n>\nnote that there are 2 nodes for the two rigid bodies. the five objects are due to ground object, 2 rigid bodies and 2 revolute joints. the meaning of markers can be seen in the graphical representation described below.\nalternatively we can print the full internal information as a dictionary using:\nmbs.systemdata.info() #show detailed information\nwhich results in the following output:\nnode0:\n'nodetype': 'rigidbodyep', 'referencecoordinates': [0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0], 'addconstraintequation': true, 'initialcoordinates': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'initialvelocities': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'name': 'node0', 'vshow': true, 'vdrawsize': -1.0, 'vcolor': [-1.0, -1.0, -1.0, -1.0]\nnode1:\n'nodetype': 'rigidbodyep', 'referencecoordinates': [1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0], 'addconstraintequation': true, 'initialcoordinates': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'initialvelocities': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'name': 'node1', 'vshow': true, 'vdrawsize': -1.0, 'vcolor': [-1.0, -1.0, -1.0, -1.0]\nobject0:\n'objecttype': 'ground', 'referenceposition': [0.0, 0.0, 0.0], 'name': 'object0', 'vshow': true, 'vgraphicsdatauserfunction': 0, 'vcolor': [-1.0, -1.0, -1.0, -1.0], 'vgraphicsdata': 'todo': 'get graphics data to be implemented'\nobject1:\n'objecttype': 'rigidbody', 'physicsmass': 50.0, 'physicsinertia': [0.08333333333333336, 7.333333333333334, 7.333333333333334, 0.0, 0.0, 0.0], 'physicscenterofmass': [-0.25, 0.0, 0.0], 'nodenumber': 0, 'name': 'object1', 'vshow': true, 'vgraphicsdatauserfunction': 0, 'vgraphicsdata': 'todo': 'get graphics data to be implemented'\nobject2:\n'objecttype': 'jointrevolute', 'markernumbers': [3, 4], 'rotationmarker0': [[0.0, 1.0, 0.0], [-1.0, 0.0, 0.0], [0.0, 0.0, 1.0]], 'rotationmarker1': [[0.0, 1.0, 0.0], [-1.0, 0.0, 0.0], [0.0, 0.0, 1.0]], 'activeconnector': true, 'name': 'object2', 'vshow': true, 'vaxisradius': 0.019999999552965164, 'vaxislength': 0.14000000059604645, 'vcolor': [-1.0, -1.0, -1.0, -1.0]\nobject3:\n...\na graphical representation of the internal structure of the model can be shown using the command drawsystemgraph:\ndrawsystemgraph(mbs, useitemtypes=true) #draw nice graph of system\nfor the output see the figure below. note that obviously, markers are always needed to connect objects (or nodes) as well as loads. we can also see, that 2 markers markerbodyrigid1 and markerbodyrigid2 are unused, which is no further problem for the model and also does not require additional computational resources (except for some bytes of memory). having isolated nodes or joints that are not connected (or having too many connections) may indicate that you did something wrong in setting up your model.\nbefore starting our simulation, we should adjust the solver parameters, especially the end time and the step size (no automatic step size for implicit solvers available!):\nsimulationsettings = exu.simulationsettings() #takes currently set values or default values\ntend = 4 #simulation time\nh = 1e-3 #step size\nsimulationsettings.timeintegration.numberofsteps = int(tend/h)\nsimulationsettings.timeintegration.endtime = tend\nsimulationsettings.timeintegration.verbosemode = 1\n#simulationsettings.timeintegration.simulateinrealtime = true\nsimulationsettings.solutionsettings.solutionwriteperiod = 0.005 #store every 5 ms\nthe verbosemode tells the solver the amount of output during solving. higher values (2, 3, ...) show residual vectors, jacobians, etc. for every time step, but slow down simulation significantly. the option simulateinrealtime is used to view the model during simulation, while setting this false, the simulation finishes after fractions of a second. it should be set to false in general, while solution can be viewed using the solutionviewer(). with solutionwriteperiod you can adjust the frequency which is used to store the solution of the whole model, which may lead to very large files and may slow down simulation, but is used in the solutionviewer() to reload the solution after simulation.\nin order to improve visualization, there are hundreds of options, see visualization settings in thedoc.pdf, some of them used here:\nsc.visualizationsettings.window.renderwindowsize=[1600,1200]\nsc.visualizationsettings.opengl.multisampling = 4 #improved opengl rendering\nsc.visualizationsettings.general.autofitscene = false\nsc.visualizationsettings.nodes.drawnodesaspoint=false\nsc.visualizationsettings.nodes.showbasis=true #shows three rgb (=xyz) lines for node basis\nthe option autofitscene is used in order to avoid zooming while loading the last saved render state, see below.\nwe can start the 3d visualization (renderer) now:\nexu.startrenderer()\nin order to reload the model view of the last simulation (if there is any), we can use the following commands:\nif 'renderstate' in exu.sys: #reload old view\nsc.setrenderstate(exu.sys['renderstate'])\nmbs.waitforusertocontinue() #stop before simulating\nthe function waitforusertocontinue() waits with simulation until we press space bar. this allows us to make some pre-checks.\nfinally, implicit time integration (simulation) is started with:\nexu.solvedynamic(mbs, simulationsettings = simulationsettings,\nsolvertype=exu.dynamicsolvertype.trapezoidalindex2)\nafter simulation, the library would immediately exit (and jump back to ipython or close the terminal window). in order to avoid this, we can use waitforrenderenginestopflag() to wait until we press key 'q'.\nsc.waitforrenderenginestopflag() #stop before closing\nexu.stoprenderer() #safely close rendering window!\nif you entered everything correctly, the render window should show a nice animation of the 3d double pendulum after pressing the space key. if we do not stop the renderer (stoprenderer()), it will stay open for further simulations. however, it is safer to always close the renderer at the end.\nas the simulation will run very fast, if you did not set simulateinrealtime to true. however, you can reload the stored solution and view the stored steps interactively:\nsol = loadsolutionfile('coordinatessolution.txt')\nfrom exudyn.interactive import solutionviewer\nsolutionviewer(mbs, sol)\nfinally, we can plot our sensor, drawing the y-component of the sensor:\nfrom exudyn.plot import plotsensor\nplotsensor(mbs, [sens1],[1])\ncongratulations! you completed the rigid body tutorial, which gives you the ability to model multibody systems. note that much more complicated models are possible, including feedback control or flexible bodies, see the examples!\nfor further information go to thedoc.pdf !!!", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000942, "year": null}, {"Unnamed: 0": 1989, "autor": 969, "date": null, "content": "RL-botics\nRL-botics is a toolbox with highly optimized implementations of Deep Reinforcement Learning algorithms for robotics developed with Keras and TensorFlow in Python3.\nThe objective was to have modular, clean and easy to read codebase so that the research community may build on top with ease. The implementations can be integrated with OpenAI Gym environments. The majority of the algorithms are Policy Search Methods as the toolbox is targetted for robotic applications.\nRequirements\nRequirements:\npython3 (>=3.5)\nScipy\nNumpy\nPandas\nMatplotlib\nTensorFlow\nTensorFlow Probability\nKeras\nOpenAI Gym\nConda Environment\nIt is highly recommended to install this package in a virtual environment, such as Miniconda. Please find the Conda installation here.\nTo create a new conda environment called RL:\nconda create -n RL python=3\nTo activate the environment:\nsource activate RL\nTo deactivate the environment:\nsource deactivate\nInstallation\nTo install the package, we recommend cloning the original package:\ngit clone https://github.com/Suman7495/rl-botics.git\ncd rl-botics\npip install -e .\nUsage\nTo run any algorithm in the default setting, simply run:\ncd rl_botics/<algo>/\npython main.py\nFor example, to run TRPO:\ncd rl_botics/trpo/\npython main.py\nNumerous other options can be added too, but it is recommended to modify the hyerperparameters in hyperparameters.py.\nAlgorithms\nThe algorithms implemented are:\nQ-Learning\nDeep Q-Network\nVanilla Policy Gradient\nDeep Deterministic Policy Gradient\nTrust Region Policy Optimization\nProximal Policy Optimization\nProximal Policy Optmization with Intrinsic Curiosity Module (ICM)\nCompatible Natural Policy Gradient\nTo be added:\nRelative Entropy Search\nSoft Actor Critic\nA3C\nHER\nEnvironments\nAll environments are in the envs directory. The environments available currently are:\nField Vision Rock Sampling (FVRS): A POMDP environment where the agent has to collect good rocks from partial observability.\nTable Continuous: A POMDP environment emulation Human Robot Collaboration. The objective of the robot is to remove dirty dishes from the table without colliding with the human.\nToolbox Structure\nAll the algorithms are in the rl_botics directory. Each algorithm specified above has an individual directory.\nCommon\nThe directory common contains common modular classes to easily build new algorithms.\napproximators: Basic Deep Neural Networks (Dense, Conv, LSTM).\ndata_collection: Performs rollouts and collect observations and rewards\nlogger: Log training data and other information\nplotter: Plot graphs\npolicies: Common policies such as Random, Softmax, Parametrized Softmax and Gaussian Policy\nutils: Functions to compute the expected return, the Generalized Advantage Estimation (GAE), etc.\nAlgorithm Directories\nEach algorithm directory contains at least 3 files:\nmain.py: Main script to run the algorithm\nhyperparameters.py: File to contain the default hyperparameters\n<algo>.py: Implementation of the algorithm\nutils.py: (Optional) File containing some utility functions\nSome algorithm directories may have additional files specific to the algorithm.\nContributing\nTo contribute to this package, it is recommended to follow this structure:\nThe new algorithm directory should at least contain the 3 files mentioned above.\nmain.py should contain at least the following functions:\nmain: Parses input argument, builds the environment and agent, and train the agent.\nargparse: Parses input argument and loads default hyperparameters from hyperparameter.py.\n<algo>.py should contain at least the following methods:\n__init__: Initializes the classes\n_build_graph: Calls the following methods to build the TensorFlow graph:\n_init_placeholders: Initialize TensorFlow placeholders\n_build_policy: Build policy TensorFlow graph\n_build_value_function: Build value function TensorFlow graph\n_loss: Build policy loss function TensorFlwo graph\ntrain: Main training loop called by main.py\nupdate_policy: Update the policy\nupdate_value: Update the value function\nprint_results: Print the training results\nprocess_paths: (optional) Process collected trajectories to return the feed dictionary for TensorFlow\nIt is recommended to check the structure of ppo.py and follow a similar structure.\nCredits\nSuman Pal\nLicense\nMIT License.", "link": "https://github.com/Suman7495/rl-botics", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "rl-botics\nrl-botics is a toolbox with highly optimized implementations of deep reinforcement learning algorithms for robotics developed with keras and tensorflow in python3.\nthe objective was to have modular, clean and easy to read codebase so that the research community may build on top with ease. the implementations can be integrated with openai gym environments. the majority of the algorithms are policy search methods as the toolbox is targetted for robotic applications.\nrequirements\nrequirements:\npython3 (>=3.5)\nscipy\nnumpy\npandas\nmatplotlib\ntensorflow\ntensorflow probability\nkeras\nopenai gym\nconda environment\nit is highly recommended to install this package in a virtual environment, such as miniconda. please find the conda installation here.\nto create a new conda environment called rl:\nconda create -n rl python=3\nto activate the environment:\nsource activate rl\nto deactivate the environment:\nsource deactivate\ninstallation\nto install the package, we recommend cloning the original package:\ngit clone https://github.com/suman7495/rl-botics.git\ncd rl-botics\npip install -e .\nusage\nto run any algorithm in the default setting, simply run:\ncd rl_botics/<algo>/\npython main.py\nfor example, to run trpo:\ncd rl_botics/trpo/\npython main.py\nnumerous other options can be added too, but it is recommended to modify the hyerperparameters in hyperparameters.py.\nalgorithms\nthe algorithms implemented are:\nq-learning\ndeep q-network\nvanilla policy gradient\ndeep deterministic policy gradient\ntrust region policy optimization\nproximal policy optimization\nproximal policy optmization with intrinsic curiosity module (icm)\ncompatible natural policy gradient\nto be added:\nrelative entropy search\nsoft actor critic\na3c\nher\nenvironments\nall environments are in the envs directory. the environments available currently are:\nfield vision rock sampling (fvrs): a pomdp environment where the agent has to collect good rocks from partial observability.\ntable continuous: a pomdp environment emulation human robot collaboration. the objective of the robot is to remove dirty dishes from the table without colliding with the human.\ntoolbox structure\nall the algorithms are in the rl_botics directory. each algorithm specified above has an individual directory.\ncommon\nthe directory common contains common modular classes to easily build new algorithms.\napproximators: basic deep neural networks (dense, conv, lstm).\ndata_collection: performs rollouts and collect observations and rewards\nlogger: log training data and other information\nplotter: -----> plot !!!  graphs\npolicies: common policies such as random, softmax, parametrized softmax and gaussian policy\nutils: functions to compute the expected return, the generalized advantage estimation (gae), etc.\nalgorithm directories\neach algorithm directory contains at least 3 files:\nmain.py: main script to run the algorithm\nhyperparameters.py: file to contain the default hyperparameters\n<algo>.py: implementation of the algorithm\nutils.py: (optional) file containing some utility functions\nsome algorithm directories may have additional files specific to the algorithm.\ncontributing\nto contribute to this package, it is recommended to follow this structure:\nthe new algorithm directory should at least contain the 3 files mentioned above.\nmain.py should contain at least the following functions:\nmain: parses input argument, builds the environment and agent, and train the agent.\nargparse: parses input argument and loads default hyperparameters from hyperparameter.py.\n<algo>.py should contain at least the following methods:\n__init__: initializes the classes\n_build_graph: calls the following methods to build the tensorflow graph:\n_init_placeholders: initialize tensorflow placeholders\n_build_policy: build policy tensorflow graph\n_build_value_function: build value function tensorflow graph\n_loss: build policy loss function tensorflwo graph\ntrain: main training loop called by main.py\nupdate_policy: update the policy\nupdate_value: update the value function\nprint_results: print the training results\nprocess_paths: (optional) process collected trajectories to return the feed dictionary for tensorflow\nit is recommended to check the structure of ppo.py and follow a similar structure.\ncredits\nsuman pal\nlicense\nmit license.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000969, "year": null}, {"Unnamed: 0": 2019, "autor": 999, "date": null, "content": "my-matlab-robotics-toolbox\nA simple and straightforward implementation of the DH-parameters in MATLAB\nThere are two different conventions on the implementation of the DH-parameters. This one uses the standard DH-parameters which can as well be found in this book: https://www.cs.duke.edu/brd/Teaching/Bio/asmb/current/Papers/chap3-forward-kinematics.pdf. The results have been validated with Peter Corke's RVC toolbox (https://github.com/petercorke/robotics-toolbox-matlab).\ncgr prefix means the code is code-generation ready.\nncgr means the code is NOT code-generation ready.\nFeatures:\nForward kinematics\nHomogenous transformation of each link of the robot\nNumerical Jacobian\nSimple visualization, it can also be animated\nInverse kinematics with the pseudo-inverse method and damped least square method.\nCode generation ready.\nHow to use:\nCreate a global variable N_DOFS and define the number of degree-of-freedom of the robot in it. The reason why global variable is used is because I keep having problems in using the dynamic memory allocation for MATLAB coder. Therefore, I use global variable to define the dimension of the necessary static arrays.\nCreate the robot structure with cgr_create.\nActuate and update the joint with cgr_self_update functions.\nIf necessary, plot the robot with ncgr_plot by first calling ncgr_graphic once at the beginning of the program.\nTo create the compiled MEX or DLL files, two examples of MATLAB Coder project files are provided. This is where the global variable is used.\nAuralius Manurung manurunga@yandex.com", "link": "https://github.com/auralius/my-matlab-robotics-toolbox", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "plot", "selectorShort": "plot", "MarkedSent": "my-matlab-robotics-toolbox\na simple and straightforward implementation of the dh-parameters in matlab\nthere are two different conventions on the implementation of the dh-parameters. this one uses the standard dh-parameters which can as well be found in this book: https://www.cs.duke.edu/brd/teaching/bio/asmb/current/papers/chap3-forward-kinematics.pdf. the results have been validated with peter corke's rvc toolbox (https://github.com/petercorke/robotics-toolbox-matlab).\ncgr prefix means the code is code-generation ready.\nncgr means the code is not code-generation ready.\nfeatures:\nforward kinematics\nhomogenous transformation of each link of the robot\nnumerical jacobian\nsimple visualization, it can also be animated\ninverse kinematics with the pseudo-inverse method and damped least square method.\ncode generation ready.\nhow to use:\ncreate a global variable n_dofs and define the number of degree-of-freedom of the robot in it. the reason why global variable is used is because i keep having problems in using the dynamic memory allocation for matlab coder. therefore, i use global variable to define the dimension of the necessary static arrays.\ncreate the robot structure with cgr_create.\nactuate and update the joint with cgr_self_update functions.\nif necessary, -----> plot !!!  the robot with ncgr_plot by first calling ncgr_graphic once at the beginning of the program.\nto create the compiled mex or dll files, two examples of matlab coder project files are provided. this is where the global variable is used.\nauralius manurung manurunga@yandex.com", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000999, "year": null}], "name": "plotrobotics"}
{"interestingcomments": [{"Unnamed: 0": 1043, "autor": 23, "date": null, "content": "The Autonomous Driving Cookbook (Preview)\nNOTE:\nThis project is developed and being maintained by Project Road Runner at Microsoft Garage. This is currently a work in progress. We will continue to add more tutorials and scenarios based on requests from our users and the availability of our collaborators.\nAutonomous driving has transcended far beyond being a crazy moonshot idea over the last half decade or so. It has quickly become one of the biggest technologies today that promises to shape our tomorrow, not very unlike when cars first came into existence. A big driver powering this change is the recent advances in software (Artificial Intelligence), hardware (GPUs, FPGAs etc.) and cloud computing, which have enabled ingest and processing of large amounts of data, making it possible for companies to push for levels 4 and 5 of autonomy. Achieving those levels of autonomy though, require training on hundreds of millions and sometimes hundreds of billions of miles worth of training data to demonstrate reliability, according to a report from RAND.\nDespite the large amount of data collected every day, it is still insufficient to meet the demands of the ever increasing AI model complexity required by autonomous vehicles. One way to collect such huge amounts of data is through the use of simulation. Simulation makes it easy to not only collect data from a variety of different scenarios which would take days, if not months in the real world (like different weather conditions, varying daylight etc.), it also provides a safe test bed for trained models. With behavioral cloning, you can easily prepare highly efficient models in simulation and fine tune them using a relatively low amount of real world data. Then there are models built using techniques like Reinforcement Learning, which can only be trained in simulation. With simulators such as AirSim, working on these scenarios has become very easy.\nWe believe that the best way to make a technology grow is by making it easily available and accessible to everyone. This is best achieved by making the barrier of entry to it as low as possible. At Microsoft, our mission is to empower every person and organization on the planet to achieve more. That has been our primary motivation behind preparing this cookbook. Our aim with this project is to help you get quickly acquainted and familiarized with different onboarding scenarios in autonomous driving so you can take what you learn here and employ it in your everyday job with a minimal barrier to entry.\nWho is this cookbook for?\nOur plan is to make this cookbook a valuable resource for beginners, researchers and industry experts alike. Tutorials in the cookbook are presented as Jupyter notebooks, making it very easy for you to download the instructions and get started without a lot of setup time. To help this further, wherever needed, tutorials come with their own datasets, helper scripts and binaries. While the tutorials leverage popular open-source tools (like Keras, TensorFlow etc.) as well as Microsoft open-source and commercial technology (like AirSim, Azure virtual machines, Batch AI, CNTK etc.), the primary focus is on the content and learning, enabling you to take what you learn here and apply it to your work using tools of your choice.\nWe would love to hear your feedback on how we can evolve this project to reach that goal. Please use the GitHub Issues section to get in touch with us regarding ideas and suggestions.\nTutorials available\nCurrently, the following tutorials are available:\nAutonomous Driving using End-to-End Deep Learning: an AirSim tutorial\nDistributed Deep Reinforcement Learning for Autonomous Driving\nFollowing tutorials will be available soon:\nLane Detection using Deep Learning\nContributing\nPlease read the instructions and guidelines for collaborators if you wish to add a new tutorial to the cookbook.\nThis project welcomes and encourages contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.microsoft.com.\nWhen you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.\nThis project has adopted the Microsoft Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.", "link": "https://github.com/microsoft/AutonomousDrivingCookbook", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "bed", "selectorShort": "bed", "MarkedSent": "the autonomous driving cookbook (preview)\nnote:\nthis project is developed and being maintained by project road runner at microsoft garage. this is currently a work in progress. we will continue to add more tutorials and scenarios based on requests from our users and the availability of our collaborators.\nautonomous driving has transcended far beyond being a crazy moonshot idea over the last half decade or so. it has quickly become one of the biggest technologies today that promises to shape our tomorrow, not very unlike when cars first came into existence. a big driver powering this change is the recent advances in software (artificial intelligence), hardware (gpus, fpgas etc.) and cloud computing, which have enabled ingest and processing of large amounts of data, making it possible for companies to push for levels 4 and 5 of autonomy. achieving those levels of autonomy though, require training on hundreds of millions and sometimes hundreds of billions of miles worth of training data to demonstrate reliability, according to a report from rand.\ndespite the large amount of data collected every day, it is still insufficient to meet the demands of the ever increasing ai model complexity required by autonomous vehicles. one way to collect such huge amounts of data is through the use of simulation. simulation makes it easy to not only collect data from a variety of different scenarios which would take days, if not months in the real world (like different weather conditions, varying daylight etc.), it also provides a safe test -----> bed !!!  for trained models. with behavioral cloning, you can easily prepare highly efficient models in simulation and fine tune them using a relatively low amount of real world data. then there are models built using techniques like reinforcement learning, which can only be trained in simulation. with simulators such as airsim, working on these scenarios has become very easy.\nwe believe that the best way to make a technology grow is by making it easily available and accessible to everyone. this is best achieved by making the barrier of entry to it as low as possible. at microsoft, our mission is to empower every person and organization on the planet to achieve more. that has been our primary motivation behind preparing this cookbook. our aim with this project is to help you get quickly acquainted and familiarized with different onboarding scenarios in autonomous driving so you can take what you learn here and employ it in your everyday job with a minimal barrier to entry.\nwho is this cookbook for?\nour plan is to make this cookbook a valuable resource for beginners, researchers and industry experts alike. tutorials in the cookbook are presented as jupyter notebooks, making it very easy for you to download the instructions and get started without a lot of setup time. to help this further, wherever needed, tutorials come with their own datasets, helper scripts and binaries. while the tutorials leverage popular open-source tools (like keras, tensorflow etc.) as well as microsoft open-source and commercial technology (like airsim, azure virtual machines, batch ai, cntk etc.), the primary focus is on the content and learning, enabling you to take what you learn here and apply it to your work using tools of your choice.\nwe would love to hear your feedback on how we can evolve this project to reach that goal. please use the github issues section to get in touch with us regarding ideas and suggestions.\ntutorials available\ncurrently, the following tutorials are available:\nautonomous driving using end-to-end deep learning: an airsim tutorial\ndistributed deep reinforcement learning for autonomous driving\nfollowing tutorials will be available soon:\nlane detection using deep learning\ncontributing\nplease read the instructions and guidelines for collaborators if you wish to add a new tutorial to the cookbook.\nthis project welcomes and encourages contributions and suggestions. most contributions require you to agree to a contributor license agreement (cla) declaring that you have the right to, and actually do, grant us the rights to use your contribution. for details, visit https://cla.microsoft.com.\nwhen you submit a pull request, a cla-bot will automatically determine whether you need to provide a cla and decorate the pr appropriately (e.g., label, comment). simply follow the instructions provided by the bot. you will only need to do this once across all repos using our cla.\nthis project has adopted the microsoft open source code of conduct. for more information see the code of conduct faq or contact opencode@microsoft.com with any additional questions or comments.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000023, "year": null}, {"Unnamed: 0": 1435, "autor": 415, "date": null, "content": "ManiSkill Benchmark\nObject manipulation from 3D visual inputs poses many challenges on building generalizable perception and policy models. However, 3D assets in existing benchmarks mostly lack the diversity of 3D shapes that align with real-world intra-class complexity in topology and geometry. Here we propose SAPIEN Manipulation Skill Benchmark (ManiSkill) to benchmark manipulation skills over diverse objects in a full-physics simulator. 3D assets in ManiSkill include large intra-class topological and geometric variations. Tasks are carefully chosen to cover distinct types of manipulation challenges. Latest progress in 3D vision also makes us believe that we should customize the benchmark so that the challenge is inviting to researchers working on 3D deep learning. To this end, we simulate a moving panoramic camera that returns ego-centric point clouds or RGB-D images. In addition, we would like ManiSkill to serve a broad set of researchers interested in manipulation research. Besides supporting the learning of policies from interactions, we also support learning-from-demonstrations (LfD) methods, by providing a large number of high-quality demonstrations (~36,000 successful trajectories, ~1.5M point cloud/RGB-D frames in total). We provide baselines using 3D deep learning and LfD algorithms. All code of our benchmark (simulator, environment, SDK, and baselines) is open-sourced.\nCurrently, ManiSkill has released 4 different tasks: OpenCabinetDoor, OpenCabinetDrawer, PushChair, and MoveBucket, with 162 objects.\nClick here for our paper.\nClick here for SAPIEN Open-Source Manipulation Skill Challenge (ManiSkill Challenge) website.\nThis README describes how to install ManiSkill, how to run a basic example, and relevant environment details.\nTable of Contents:\nManiSkill Benchmark\nUpdates and Announcements\nPreliminary Knowledge\nWhat Can I Do with ManiSkill? (TL;DR)\nFor Computer Vision People\nFor Reinforcement Learning People\nFor Robot Learning People\nGetting Started\nSystem Requirements\nInstallation\nBasic Example\nViewer Tutorial\nUtilize Demonstrations\nBaselines and Training Framework\nFAQ\nEnvironment Details\nTasks\nRobots and Actions\nObservations\nObservation Structure for Each Mode\nSegmentation Masks\nRewards\nTermination\nEvaluation\nVisualization\nAvailable Environments\nAdvanced Usage\nCustom Split\nVisualization inside Docker\nOperational Space Control\nGet Pose of End Effector\nConclusion\nAcknowledgements\nCitation\nUpdates and Announcements\nNov 15, 2021: We provided a detailed explanation of the action space, see here.\nNov 6, 2021: We provided a more detailed explanation of the observations, see here.\nOct 11, 2021: ManiSkill has been accepted to NeurIPS 2021 Track on Datasets and Benchmarks!\nSep 5, 2021: The treatment of background points in observations are slightly different in the demonstration data and our old environments. We have changed the environments so that the background treatment matches exactly. Please pull the latest codes from the ManiSkill repo and rerun the evaluation scripts to measure performance. You do not need to update the demonstration data or retrain an existing model if it has been trained solely on our demonstration data (but you need to re-evaluate the model).\nAug 16, 2021: ManiSkill now supports operational space control.\nJuly 29, 2021: The initial version of ManiSkill is released!\nPreliminary Knowledge\nManiSkill environment is built on the gym interface. If you have used OpenAI gym or have worked on RL previously, you can skip this section.\nIn ManiSkill environments, you are controlling a robot to accomplish certain predefined tasks in simulated environments. The tasks are defined by rewards. Every timestep you are given an observation (e.g., point cloud / image) from the environment, and you are required to output an action (a vector) to control the robot.\n(* image credits to OpenAI Gym)\nExplanations about the above terminologies:\nObservation: a description about the current state of the simulated environment, which may not contain complete information. Observation is usually represented by several arrays (e.g., point clouds, images, vectors).\nAction: how an agent interacts with the environment, which is usually represented by a vector.\nReward: used to define the goal of an agent, which is usually represented by a scalar value.\nWhat Can I Do with ManiSkill? (TL;DR)\nFor Computer Vision People\nBased on the current visual observations (point clouds / RGBD images), your job is to output an action (a vector). We provide supervision for actions, so training an agent is just supervised learning (more specifically, imitation learning). You can start with little knowledge on robotics and policy learning.\nFor Reinforcement Learning People\nManiSkill is designed for the generalization of policies and learning-from-demonstrations methods. Some topics you might be interested:\nhow to combine offline RL and online RL\ngeneralize a manipulation policy to unseen objects\n...\nFor Robot Learning People\nIn simulated environments, large-scale learning and planning are feasible. We provide four meaningful daily-life tasks for you as a good test bed.\nGetting Started\nThis section introduces benchmark installation, basic examples, demonstrations, and baselines we provide.\nSystem Requirements\nMinimum requirements\nUbuntu 18.04 / Ubuntu 20.04 or equivalent Linux distribution. 16.04 is not supported.\nNvidia GPU with > 6G memory\nNvidia Graphics Driver 460+ (lower versions may work but are untested)\nInstallation\nFirst, clone this repository and cd into it.\ngit clone https://github.com/haosulab/ManiSkill.git\ncd ManiSkill\nSecond, install dependencies listed in environment.yml. It is recommended to use the latest (mini)conda to manage the environment, but you can also choose to manually install the dependencies.\nconda env create -f environment.yml\nconda activate mani_skill\nLastly, install ManiSkill.\npip install -e .\nBasic Example\nHere is a basic example for making an environment in ManiSkill and running a random policy in it. You can also run the full script using basic_example.py. ManiSkill environment is built on the OpenAI Gym interface. If you have not used OpenAI Gym before, we strongly recommend reading their documentation first.\nimport gym\nimport mani_skill.env\nenv = gym.make('OpenCabinetDoor-v0')\n# full environment list can be found in available_environments.txt\nenv.set_env_mode(obs_mode='state', reward_type='sparse')\n# obs_mode can be 'state', 'pointcloud' or 'rgbd'\n# reward_type can be 'sparse' or 'dense'\nprint(env.observation_space) # this shows the observation structure in Openai Gym's format\nprint(env.action_space) # this shows the action space in Openai Gym's format\nfor level_idx in range(0, 5): # level_idx is a random seed\nobs = env.reset(level=level_idx)\nprint('#### Level {:d}'.format(level_idx))\nfor i_step in range(100000):\n# env.render('human') # a display is required to use this function; note that rendering will slow down the running speed\naction = env.action_space.sample()\nobs, reward, done, info = env.step(action) # take a random action\nprint('{:d}: reward {:.4f}, done {}'.format(i_step, reward, done))\nif done:\nbreak\nenv.close()\nViewer Tutorial\nThe env.render('human') line above opens the SAPIEN viewer for interactively debugging the environment. Here is a short tutorial.\nNavigation:\nUse wasd keys to move around (just like in FPS games).\nHold Right Mouse Button to rotate the view.\nClick on any object to select it. Now press f to enter the focus mode. In the focus mode, hold Right Mouse Button to rotate around the object origin. Use wasd to exit the focus mode.\nImportant limitation: do not reset a level while an object is selected, otherwise the program will crash.\nInspection:\nPause will keep the rendering running in a loop and pause the simulation. You can look around with the navigation keys when paused.\nYou can use the Scene Hierarchy tool to select objects.\nUse the Actor/Entity tab and the Articulation tab to view the properties of the selected object.\nYou can find a more detailed tutorial here.\nUtilize Demonstrations\nWe provide demonstration datasets for each task to facilitate learning-from-demonstrations approaches. Please refer to the documentation here.\nBaselines and Training Framework\nWe provide a high-quality framework for training agents on the ManiSkill Benchmark at ManiSkill-Learn. The framework supports various imitation learning and offline-RL baselines implemented using point-cloud based network architectures. Try it out!\nIn our challenge, ManiSkill (this repo) contains the environments you need to work on, and ManiSkill-Learn framework contains the baselines provided by us. ManiSkill is a required component for this challenge, but ManiSkill-Learn is not required. However, we encourage you to use ManiSkill-Learn to develop your algorithms and it will help you start quicklier and easier.\nFAQ\nFAQ page is hosted here.\nEnvironment Details\nThis section describes some details of the environments in the ManiSkill Benchmark. ManiSkill environments are built on the Gym interface. If you have not used OpenAI Gym before, we strongly recommend reading their documentation first.\nTasks\nManiSkill Benchmark currently contains 4 tasks: OpenCabinetDoor, OpenCabinetDrawer, PushChair, and MoveBucket.\nOpenCabinetDoor and OpenCabinetDrawer are examples of manipulating articulated objects with revolute and prismatic joints respectively. The agent is required to open the target door or drawer through the coordination between arm and body.\nPushChair exemplifies the ability to manipulate complex underactuated systems. The agent needs to push a swivel chair to a target location. Each chair is typically equipped with several omni-directional wheels and a rotating seat.\nMoveBucket is an example of manipulation that heavily relies on two-arm coordination. The agent is required to lift a bucket with a ball in it from the ground onto a platform.\nThese environments can be constructed by changing the environment name passed to gym.make. Keep reading for more details.\nRobots and Actions\nThe state of the robot is a vector, and the action is also a vector. We have implemented modules compiling the state of the robot into a vector, and modules converting the action vector into the robot control signals. While you do not need to worry about them, the details are provided below in case of you are curious. All the tasks in ManiSkill use similar robots, which are composed of three parts: moving platform, Sciurus robot body, and one or two Franka Panda arm(s). The moving platform can move and rotate on the ground plane, and its height is also adjustable. The robot body is fixed on top of the platform, providing support for the arms. Depending on the task, one or two robot arm(s) are connected to the robot body. There are 22 joints in the dual-arm robot and 13 for the single-arm robot. To match with the realistic robotics setup, we use PID controllers to control the joints of the robots. The robot fingers use position controllers, while all other joints, including the moving platform joints and the arm joints, use velocity controllers. The controllers are internally implemented as augmented PD and PID controllers. The action space corresponds to the normalized target values of all controllers. A detailed exaplanation of the action space can be found here.\nWe also provide another action interface based on operational space control, please see Operational Space Control for more details.\nObservations\nManiSkill supports three observation modes: state, pointcloud and rgbd, which can be set by env.set_env_mode(obs_mode=obs_mode). For all observation modes, the observation consist of three components: 1) A vector that describes the current state of the robot, including pose, velocity, angular velocity of the moving platform of the robot, joint angles and joint velocities of all robot joints, as well as states of all controllers; 2) A vector that describes task-relevant information, if necessary; 3) Perception of the scene, which has different representations according to the observation modes. In state mode, the perception information is a vector that encodes the full ground truth physical state of the environment (e.g. pose of the manipulated objects); in pointcloud mode, the perception information is a point cloud captured from the mounted cameras on the robot; in rgbd mode, the perception information is RGB-D images captured from the cameras.\nObservation Structure for Each Mode\nThe following script shows the structure of the observations in different observation modes.\n# Observation structure for pointcloud mode\nobs = {\n'agent': ... , # a vector that describes the agent's state, including pose, velocity, angular velocity of the\n# moving platform of the robot, joint angles and joint velocities of all robot joints,\n# positions and velocities of the robot fingers\n'pointcloud': {\n'rgb': ... , # (N, 3) array, RGB values for each point\n'xyz': ... , # (N, 3) array, position for each point, recorded in the world frame\n'seg': ... , # (N, k) array, k task-relevant segmentation masks, e.g. handle of a cabinet door, each mask is a binary array\n}\n}\n# Observation structure for rgbd mode\nobs = {\n'agent': ... , # a vector that describes agent's state, including pose, velocity, angular velocity of the\n# moving platform of the robot, joint angles and joint velocities of all robot joints,\n# positions and velocities of the robot fingers\n'rgbd': {\n'rgb': ... , # (160, 400, 3*3) array, three RGB images concatenated on the last dimension, captured by three cameras on robot\n'depth': ... , # (160, 400, 3) array, three depth images concatenated on the last dimension\n'seg': ... , # (160, 400, k*3) array, k task-relevant segmentation masks, e.g. handle of a cabinet door, each mask is a binary array\n}\n}\n# Observation structure for state mode\nobs = ... # a vector that describes agent's state, task-relevant information, and object-relevant information;\n# the object-relevant information includes pose, velocity, angular velocity of the object,\n# as well as joint angles and joint velocities if it is an articulated object (e.g, cabinet).\n# State mode is commonly used when training and test on the same object,\n# but is not suitable for studying the generalization to unseen objects,\n# as different objects may have completely different state representations.\nA detailed explanation of the agent vector can be found here.\nThe observations obs are typically obtained when resetting and stepping the environment as shown below\n# reset\nobs = env.reset(level=level_idx)\n# step\nobs, reward, done, info = env.step(action)\nSegmentation Masks\nAs mentioned in the codes above, we provide task-relevant segmentation masks in pointcloud and rgbd modes. Here are the details about our segmentation masks for each task:\nOpenCabinetDoor: handle of the target door, target door, robot (3 masks in total)\nOpenCabinetDrawer: handle of the target drawer, target drawer, robot (3 masks in total)\nPushChair: robot (1 mask in total)\nMoveBucket: robot (1 mask in total)\nBasically, we provide the robot mask and any mask that is necessary for specifying the target. For example, in OpenCabinetDoor/Drawer environments, a cabinet might have many doors/drawers, so we provide the door/drawer mask such that users know which door/drawer to open. We also provide handle mask such that the users know from which direction the door/drawer should be opened.\nRewards\nThe reward for the next step can be obtained by obs, reward, done, info = env.step(action). ManiSkill supports two kinds of rewards: sparse and dense. The sparse reward is a binary signal which is equivalent to the task-specific success condition. Learning with sparse reward is very difficult. To alleviate such difficulty, we carefully designed well-shaped dense reward functions for each task. The type of reward can be configured by env.set_env_mode(reward_type=reward_type).\nTermination\nThe agent-environment interaction process is composed of subsequences, each containing a starting point and an ending point, which we call episodes. Examples include plays of a game and trips through a maze.\nIn ManiSkill tasks, an episode will be terminated if either of the following conditions is satisfied:\nGo beyond the time limit\nIn all tasks, the time limit for each episode is 200, which should be sufficient to solve the task.\nTask is solved\nWe design several success metrics for each task, which can be accessed from info['eval_info'].\nEach metric will be True if and only if some certain conditions are satisfied for 10 consecutive steps.\nThe task is regarded as solved when all the metrics are True at the same time.\nEvaluation\nWe evaluate the performance of a policy (agent) on each task by the mean success rate. A formal description of the challenge submission and evaluation processes can be found here.\nUsers can also evaluate their policies using the evaluation tools provided by us. Please go through the following steps:\nImplement your solution following this example\nIf your codes include file paths, please use the relative paths with respect to your code file. (Check this example)\nName your solution file user_solution.py\nRun PYTHONPATH=YOUR_SOLUTION_DIRECTORY:$PYTHONPATH python mani_skill/tools/evaluate_policy.py --env ENV_NAME\nYOUR_SOLUTION_DIRECTORY is the directory containing your user_solution.py\nSpecify the levels on which you want to evaluate: --level-range 100-200\nNote that you should active a python environment supporting your user_solution.py before running the script\nResult will be exported to ./eval_results.csv\nVisualization\nThe environment normally runs in off-screen mode. If you want to visualize the scene in a window and interactively inspect the scene, you need to call\nenv.render(\"human\")\nThis function requires your machine to be connected to a display screen, or more specifically, a running x-server. It opens a visualization window that you can interact with. Do note that using this visualization can add additional helper objects or change the appearance of objects in the scene, so you should NOT generate any data for training purposes while the visualizer is open.\nThe visualizer is based on the SAPIEN viewer, and it provides a lot of debugging functionalities. You can read more about how to use this viewer here. Note: the render function must be called repeatedly to interact with the viewer, and the viewer will not run by itself when the program is paused.\nAvailable Environments\nWe registered three kinds of environments:\nRandom-object environment\nIf you call env.reset(), you may get a different object instance (e.g., a different chair in PushChair task).\nEnvironment names: OpenCabinetDoor-v0, OpenCabinetDrawer-v0, PushChair-v0, and MoveBucket-v0.\nFixed-object environment\nOnly one object instance will be presented in the environment, and it will never be replaced by other object instances.\nThese environments are registered as simpler versions of the multi-object environments, and they can be used for debugging.\nEnvironment name examples: PushChair_3000-v0, OpenCabinetDoor_1000-v0, ... .\nFixed-link environment\nOnly OpenCabinetDoor and OpenCabinetDrawer have fixed-link environments, since a cabinet can have multiple target links (door or drawer).\nIn a fixed-link environment, the target link (door or drawer) to be opened is fixed, and it will not change by calling env.reset().\nEnvironment name examples: OpenCabinetDoor_1000_link_0-v0\nThe full list of available environments can be found in available_environments.txt.\nAdvanced Usage\nCustom Split\nIf you want to select some objects to be used in a task (e.g., create training/validation split), we provide an example for you. Let us take the PushChair task as an example. You can create a file such as mani_skill/assets/config_files/chair_models_custom_split_example.yml to specify the objects you want to use. You also need to modify these lines accordingly to register new environments.\nVisualization inside Docker\nIf you want to visualize the environment while running in a Docker, you should give the Docker container access to the graphics card and let the Docker container access your local x-server. When starting the Docker, make sure to pass --gpus all -e DISPLAY=$DISPLAY -e QT_X11_NO_MITSHM=1 -e XAUTHORITY -e NVIDIA_DRIVER_CAPABILITIES=all -v /tmp/.X11-unix:/tmp/.X11-unix as arguments. For example,\ndocker run -i -d --gpus all --name maniskill_container \\\n-e DISPLAY=$DISPLAY -e QT_X11_NO_MITSHM=1 -e XAUTHORITY -e NVIDIA_DRIVER_CAPABILITIES=all -v /tmp/.X11-unix:/tmp/.X11-unix \\\nDOCKER_IMAGE_NAME\nNext, connect the x-server by\nxhost +local:`docker inspect --format='{{ .Config.Hostname }}' maniskill_container`\nYou can replace the maniskill_container with your container name.\nOperational Space Control\nThe action passed into env.step() is a control signal in the joint space. In addition, we provide another action interface based on operational space control. In maniskill/utils/osc.py, we implement a class OperationalSpaceControlInterface, which is used to convert actions between the joint space and the operational space. We also provide some basic examples.\nOperationalSpaceControlInterface takes the name of a task as input, which is used to determine the type of robot and its related information. It provides two functions (joint_space_to_operational_space_and_null_space and operational_space_and_null_space_to_joint_space) to map between actions in the joint space and actions in the operational space and the null space.\nThe operational space of our robot contain three parts: the joints of the moving platform (4 dimensions: translation (x, y, z) and rotation about z), the joints of the robot fingers (2 * num_of_arms), and the 6D velocities of end effectors in their local (i.e. end effectors') frames (6 * num_of_arms). To be more specific, the 6D velocity of end effector is a body twist at the end effector origin frame ordered as [v omega].\nThe null space contains 7 * num_of_arms degrees of freedom. An action in the null space (i.e. whose operational space component is a zero vector) provides movements in the null space of end effectors, which means that it only moves the links on the robot arm(s) but keeps the end effector (link left_panda_hand or right_panda_hand) static.\nWe also provide some basic examples in the test() function of maniskill/utils/osc.py. Please check it for further understanding.\nGet Pose of End Effector\nIf you want to manually design some controllers, then the pose(s) of the end effector(s) might be needed. We provided an example to compute the pose(s) of the end effector(s) in the world frame from the observation dict. Specifically, the poses of the end effectors refer to the poses of right_panda_hand / left_panda_hand links in our robot.\nConclusion\nNow that you have familiarized yourself with the ManiSkill benchmark, you can train and visualize policies on the ManiSkill environments. You may want to play with our baselines and get started with learning-from-demonstrations algorithms.\nAcknowledgements\nWe thank Qualcomm for sponsoring the associated challenge, Sergey Levine and Ashvin Nair for insightful discussions during the whole development process, Yuzhe Qin for the suggestions on building robots, Jiayuan Gu for providing technical support on SAPIEN, and Rui Chen, Songfang Han, Wei Jiang for testing our system.\nCitation\n@inproceedings{mu2021maniskill,\ntitle={ManiSkill: Generalizable Manipulation Skill Benchmark with Large-Scale Demonstrations},\nauthor={Mu, Tongzhou and Ling, Zhan and Xiang, Fanbo and Yang, Derek Cathera and Li, Xuanlin and Tao, Stone and Huang, Zhiao and Jia, Zhiwei and Su, Hao},\nbooktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},\nyear={2021}\n}", "link": "https://github.com/haosulab/ManiSkill", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "bed", "selectorShort": "bed", "MarkedSent": "maniskill benchmark\nobject manipulation from 3d visual inputs poses many challenges on building generalizable perception and policy models. however, 3d assets in existing benchmarks mostly lack the diversity of 3d shapes that align with real-world intra-class complexity in topology and geometry. here we propose sapien manipulation skill benchmark (maniskill) to benchmark manipulation skills over diverse objects in a full-physics simulator. 3d assets in maniskill include large intra-class topological and geometric variations. tasks are carefully chosen to cover distinct types of manipulation challenges. latest progress in 3d vision also makes us believe that we should customize the benchmark so that the challenge is inviting to researchers working on 3d deep learning. to this end, we simulate a moving panoramic camera that returns ego-centric point clouds or rgb-d images. in addition, we would like maniskill to serve a broad set of researchers interested in manipulation research. besides supporting the learning of policies from interactions, we also support learning-from-demonstrations (lfd) methods, by providing a large number of high-quality demonstrations (~36,000 successful trajectories, ~1.5m point cloud/rgb-d frames in total). we provide baselines using 3d deep learning and lfd algorithms. all code of our benchmark (simulator, environment, sdk, and baselines) is open-sourced.\ncurrently, maniskill has released 4 different tasks: opencabinetdoor, opencabinetdrawer, pushchair, and movebucket, with 162 objects.\nclick here for our paper.\nclick here for sapien open-source manipulation skill challenge (maniskill challenge) website.\nthis readme describes how to install maniskill, how to run a basic example, and relevant environment details.\ntable of contents:\nmaniskill benchmark\nupdates and announcements\npreliminary knowledge\nwhat can i do with maniskill? (tl;dr)\nfor computer vision people\nfor reinforcement learning people\nfor robot learning people\ngetting started\nsystem requirements\ninstallation\nbasic example\nviewer tutorial\nutilize demonstrations\nbaselines and training framework\nfaq\nenvironment details\ntasks\nrobots and actions\nobservations\nobservation structure for each mode\nsegmentation masks\nrewards\ntermination\nevaluation\nvisualization\navailable environments\nadvanced usage\ncustom split\nvisualization inside docker\noperational space control\nget pose of end effector\nconclusion\nacknowledgements\ncitation\nupdates and announcements\nnov 15, 2021: we provided a detailed explanation of the action space, see here.\nnov 6, 2021: we provided a more detailed explanation of the observations, see here.\noct 11, 2021: maniskill has been accepted to neurips 2021 track on datasets and benchmarks!\nsep 5, 2021: the treatment of background points in observations are slightly different in the demonstration data and our old environments. we have changed the environments so that the background treatment matches exactly. please pull the latest codes from the maniskill repo and rerun the evaluation scripts to measure performance. you do not need to update the demonstration data or retrain an existing model if it has been trained solely on our demonstration data (but you need to re-evaluate the model).\naug 16, 2021: maniskill now supports operational space control.\njuly 29, 2021: the initial version of maniskill is released!\npreliminary knowledge\nmaniskill environment is built on the gym interface. if you have used openai gym or have worked on rl previously, you can skip this section.\nin maniskill environments, you are controlling a robot to accomplish certain predefined tasks in simulated environments. the tasks are defined by rewards. every timestep you are given an observation (e.g., point cloud / image) from the environment, and you are required to output an action (a vector) to control the robot.\n(* image credits to openai gym)\nexplanations about the above terminologies:\nobservation: a description about the current state of the simulated environment, which may not contain complete information. observation is usually represented by several arrays (e.g., point clouds, images, vectors).\naction: how an agent interacts with the environment, which is usually represented by a vector.\nreward: used to define the goal of an agent, which is usually represented by a scalar value.\nwhat can i do with maniskill? (tl;dr)\nfor computer vision people\nbased on the current visual observations (point clouds / rgbd images), your job is to output an action (a vector). we provide supervision for actions, so training an agent is just supervised learning (more specifically, imitation learning). you can start with little knowledge on robotics and policy learning.\nfor reinforcement learning people\nmaniskill is designed for the generalization of policies and learning-from-demonstrations methods. some topics you might be interested:\nhow to combine offline rl and online rl\ngeneralize a manipulation policy to unseen objects\n...\nfor robot learning people\nin simulated environments, large-scale learning and planning are feasible. we provide four meaningful daily-life tasks for you as a good test -----> bed !!! .\ngetting started\nthis section introduces benchmark installation, basic examples, demonstrations, and baselines we provide.\nsystem requirements\nminimum requirements\nubuntu 18.04 / ubuntu 20.04 or equivalent linux distribution. 16.04 is not supported.\nnvidia gpu with > 6g memory\nnvidia graphics driver 460+ (lower versions may work but are untested)\ninstallation\nfirst, clone this repository and cd into it.\ngit clone https://github.com/haosulab/maniskill.git\ncd maniskill\nsecond, install dependencies listed in environment.yml. it is recommended to use the latest (mini)conda to manage the environment, but you can also choose to manually install the dependencies.\nconda env create -f environment.yml\nconda activate mani_skill\nlastly, install maniskill.\npip install -e .\nbasic example\nhere is a basic example for making an environment in maniskill and running a random policy in it. you can also run the full script using basic_example.py. maniskill environment is built on the openai gym interface. if you have not used openai gym before, we strongly recommend reading their documentation first.\nimport gym\nimport mani_skill.env\nenv = gym.make('opencabinetdoor-v0')\n# full environment list can be found in available_environments.txt\nenv.set_env_mode(obs_mode='state', reward_type='sparse')\n# obs_mode can be 'state', 'pointcloud' or 'rgbd'\n# reward_type can be 'sparse' or 'dense'\nprint(env.observation_space) # this shows the observation structure in openai gym's format\nprint(env.action_space) # this shows the action space in openai gym's format\nfor level_idx in range(0, 5): # level_idx is a random seed\nobs = env.reset(level=level_idx)\nprint('#### level {:d}'.format(level_idx))\nfor i_step in range(100000):\n# env.render('human') # a display is required to use this function; note that rendering will slow down the running speed\naction = env.action_space.sample()\nobs, reward, done, info = env.step(action) # take a random action\nprint('{:d}: reward {:.4f}, done {}'.format(i_step, reward, done))\nif done:\nbreak\nenv.close()\nviewer tutorial\nthe env.render('human') line above opens the sapien viewer for interactively debugging the environment. here is a short tutorial.\nnavigation:\nuse wasd keys to move around (just like in fps games).\nhold right mouse button to rotate the view.\nclick on any object to select it. now press f to enter the focus mode. in the focus mode, hold right mouse button to rotate around the object origin. use wasd to exit the focus mode.\nimportant limitation: do not reset a level while an object is selected, otherwise the program will crash.\ninspection:\npause will keep the rendering running in a loop and pause the simulation. you can look around with the navigation keys when paused.\nyou can use the scene hierarchy tool to select objects.\nuse the actor/entity tab and the articulation tab to view the properties of the selected object.\nyou can find a more detailed tutorial here.\nutilize demonstrations\nwe provide demonstration datasets for each task to facilitate learning-from-demonstrations approaches. please refer to the documentation here.\nbaselines and training framework\nwe provide a high-quality framework for training agents on the maniskill benchmark at maniskill-learn. the framework supports various imitation learning and offline-rl baselines implemented using point-cloud based network architectures. try it out!\nin our challenge, maniskill (this repo) contains the environments you need to work on, and maniskill-learn framework contains the baselines provided by us. maniskill is a required component for this challenge, but maniskill-learn is not required. however, we encourage you to use maniskill-learn to develop your algorithms and it will help you start quicklier and easier.\nfaq\nfaq page is hosted here.\nenvironment details\nthis section describes some details of the environments in the maniskill benchmark. maniskill environments are built on the gym interface. if you have not used openai gym before, we strongly recommend reading their documentation first.\ntasks\nmaniskill benchmark currently contains 4 tasks: opencabinetdoor, opencabinetdrawer, pushchair, and movebucket.\nopencabinetdoor and opencabinetdrawer are examples of manipulating articulated objects with revolute and prismatic joints respectively. the agent is required to open the target door or drawer through the coordination between arm and body.\npushchair exemplifies the ability to manipulate complex underactuated systems. the agent needs to push a swivel chair to a target location. each chair is typically equipped with several omni-directional wheels and a rotating seat.\nmovebucket is an example of manipulation that heavily relies on two-arm coordination. the agent is required to lift a bucket with a ball in it from the ground onto a platform.\nthese environments can be constructed by changing the environment name passed to gym.make. keep reading for more details.\nrobots and actions\nthe state of the robot is a vector, and the action is also a vector. we have implemented modules compiling the state of the robot into a vector, and modules converting the action vector into the robot control signals. while you do not need to worry about them, the details are provided below in case of you are curious. all the tasks in maniskill use similar robots, which are composed of three parts: moving platform, sciurus robot body, and one or two franka panda arm(s). the moving platform can move and rotate on the ground plane, and its height is also adjustable. the robot body is fixed on top of the platform, providing support for the arms. depending on the task, one or two robot arm(s) are connected to the robot body. there are 22 joints in the dual-arm robot and 13 for the single-arm robot. to match with the realistic robotics setup, we use pid controllers to control the joints of the robots. the robot fingers use position controllers, while all other joints, including the moving platform joints and the arm joints, use velocity controllers. the controllers are internally implemented as augmented pd and pid controllers. the action space corresponds to the normalized target values of all controllers. a detailed exaplanation of the action space can be found here.\nwe also provide another action interface based on operational space control, please see operational space control for more details.\nobservations\nmaniskill supports three observation modes: state, pointcloud and rgbd, which can be set by env.set_env_mode(obs_mode=obs_mode). for all observation modes, the observation consist of three components: 1) a vector that describes the current state of the robot, including pose, velocity, angular velocity of the moving platform of the robot, joint angles and joint velocities of all robot joints, as well as states of all controllers; 2) a vector that describes task-relevant information, if necessary; 3) perception of the scene, which has different representations according to the observation modes. in state mode, the perception information is a vector that encodes the full ground truth physical state of the environment (e.g. pose of the manipulated objects); in pointcloud mode, the perception information is a point cloud captured from the mounted cameras on the robot; in rgbd mode, the perception information is rgb-d images captured from the cameras.\nobservation structure for each mode\nthe following script shows the structure of the observations in different observation modes.\n# observation structure for pointcloud mode\nobs = {\n'agent': ... , # a vector that describes the agent's state, including pose, velocity, angular velocity of the\n# moving platform of the robot, joint angles and joint velocities of all robot joints,\n# positions and velocities of the robot fingers\n'pointcloud': {\n'rgb': ... , # (n, 3) array, rgb values for each point\n'xyz': ... , # (n, 3) array, position for each point, recorded in the world frame\n'seg': ... , # (n, k) array, k task-relevant segmentation masks, e.g. handle of a cabinet door, each mask is a binary array\n}\n}\n# observation structure for rgbd mode\nobs = {\n'agent': ... , # a vector that describes agent's state, including pose, velocity, angular velocity of the\n# moving platform of the robot, joint angles and joint velocities of all robot joints,\n# positions and velocities of the robot fingers\n'rgbd': {\n'rgb': ... , # (160, 400, 3*3) array, three rgb images concatenated on the last dimension, captured by three cameras on robot\n'depth': ... , # (160, 400, 3) array, three depth images concatenated on the last dimension\n'seg': ... , # (160, 400, k*3) array, k task-relevant segmentation masks, e.g. handle of a cabinet door, each mask is a binary array\n}\n}\n# observation structure for state mode\nobs = ... # a vector that describes agent's state, task-relevant information, and object-relevant information;\n# the object-relevant information includes pose, velocity, angular velocity of the object,\n# as well as joint angles and joint velocities if it is an articulated object (e.g, cabinet).\n# state mode is commonly used when training and test on the same object,\n# but is not suitable for studying the generalization to unseen objects,\n# as different objects may have completely different state representations.\na detailed explanation of the agent vector can be found here.\nthe observations obs are typically obtained when resetting and stepping the environment as shown below\n# reset\nobs = env.reset(level=level_idx)\n# step\nobs, reward, done, info = env.step(action)\nsegmentation masks\nas mentioned in the codes above, we provide task-relevant segmentation masks in pointcloud and rgbd modes. here are the details about our segmentation masks for each task:\nopencabinetdoor: handle of the target door, target door, robot (3 masks in total)\nopencabinetdrawer: handle of the target drawer, target drawer, robot (3 masks in total)\npushchair: robot (1 mask in total)\nmovebucket: robot (1 mask in total)\nbasically, we provide the robot mask and any mask that is necessary for specifying the target. for example, in opencabinetdoor/drawer environments, a cabinet might have many doors/drawers, so we provide the door/drawer mask such that users know which door/drawer to open. we also provide handle mask such that the users know from which direction the door/drawer should be opened.\nrewards\nthe reward for the next step can be obtained by obs, reward, done, info = env.step(action). maniskill supports two kinds of rewards: sparse and dense. the sparse reward is a binary signal which is equivalent to the task-specific success condition. learning with sparse reward is very difficult. to alleviate such difficulty, we carefully designed well-shaped dense reward functions for each task. the type of reward can be configured by env.set_env_mode(reward_type=reward_type).\ntermination\nthe agent-environment interaction process is composed of subsequences, each containing a starting point and an ending point, which we call episodes. examples include plays of a game and trips through a maze.\nin maniskill tasks, an episode will be terminated if either of the following conditions is satisfied:\ngo beyond the time limit\nin all tasks, the time limit for each episode is 200, which should be sufficient to solve the task.\ntask is solved\nwe design several success metrics for each task, which can be accessed from info['eval_info'].\neach metric will be true if and only if some certain conditions are satisfied for 10 consecutive steps.\nthe task is regarded as solved when all the metrics are true at the same time.\nevaluation\nwe evaluate the performance of a policy (agent) on each task by the mean success rate. a formal description of the challenge submission and evaluation processes can be found here.\nusers can also evaluate their policies using the evaluation tools provided by us. please go through the following steps:\nimplement your solution following this example\nif your codes include file paths, please use the relative paths with respect to your code file. (check this example)\nname your solution file user_solution.py\nrun pythonpath=your_solution_directory:$pythonpath python mani_skill/tools/evaluate_policy.py --env env_name\nyour_solution_directory is the directory containing your user_solution.py\nspecify the levels on which you want to evaluate: --level-range 100-200\nnote that you should active a python environment supporting your user_solution.py before running the script\nresult will be exported to ./eval_results.csv\nvisualization\nthe environment normally runs in off-screen mode. if you want to visualize the scene in a window and interactively inspect the scene, you need to call\nenv.render(\"human\")\nthis function requires your machine to be connected to a display screen, or more specifically, a running x-server. it opens a visualization window that you can interact with. do note that using this visualization can add additional helper objects or change the appearance of objects in the scene, so you should not generate any data for training purposes while the visualizer is open.\nthe visualizer is based on the sapien viewer, and it provides a lot of debugging functionalities. you can read more about how to use this viewer here. note: the render function must be called repeatedly to interact with the viewer, and the viewer will not run by itself when the program is paused.\navailable environments\nwe registered three kinds of environments:\nrandom-object environment\nif you call env.reset(), you may get a different object instance (e.g., a different chair in pushchair task).\nenvironment names: opencabinetdoor-v0, opencabinetdrawer-v0, pushchair-v0, and movebucket-v0.\nfixed-object environment\nonly one object instance will be presented in the environment, and it will never be replaced by other object instances.\nthese environments are registered as simpler versions of the multi-object environments, and they can be used for debugging.\nenvironment name examples: pushchair_3000-v0, opencabinetdoor_1000-v0, ... .\nfixed-link environment\nonly opencabinetdoor and opencabinetdrawer have fixed-link environments, since a cabinet can have multiple target links (door or drawer).\nin a fixed-link environment, the target link (door or drawer) to be opened is fixed, and it will not change by calling env.reset().\nenvironment name examples: opencabinetdoor_1000_link_0-v0\nthe full list of available environments can be found in available_environments.txt.\nadvanced usage\ncustom split\nif you want to select some objects to be used in a task (e.g., create training/validation split), we provide an example for you. let us take the pushchair task as an example. you can create a file such as mani_skill/assets/config_files/chair_models_custom_split_example.yml to specify the objects you want to use. you also need to modify these lines accordingly to register new environments.\nvisualization inside docker\nif you want to visualize the environment while running in a docker, you should give the docker container access to the graphics card and let the docker container access your local x-server. when starting the docker, make sure to pass --gpus all -e display=$display -e qt_x11_no_mitshm=1 -e xauthority -e nvidia_driver_capabilities=all -v /tmp/.x11-unix:/tmp/.x11-unix as arguments. for example,\ndocker run -i -d --gpus all --name maniskill_container \\\n-e display=$display -e qt_x11_no_mitshm=1 -e xauthority -e nvidia_driver_capabilities=all -v /tmp/.x11-unix:/tmp/.x11-unix \\\ndocker_image_name\nnext, connect the x-server by\nxhost +local:`docker inspect --format='{{ .config.hostname }}' maniskill_container`\nyou can replace the maniskill_container with your container name.\noperational space control\nthe action passed into env.step() is a control signal in the joint space. in addition, we provide another action interface based on operational space control. in maniskill/utils/osc.py, we implement a class operationalspacecontrolinterface, which is used to convert actions between the joint space and the operational space. we also provide some basic examples.\noperationalspacecontrolinterface takes the name of a task as input, which is used to determine the type of robot and its related information. it provides two functions (joint_space_to_operational_space_and_null_space and operational_space_and_null_space_to_joint_space) to map between actions in the joint space and actions in the operational space and the null space.\nthe operational space of our robot contain three parts: the joints of the moving platform (4 dimensions: translation (x, y, z) and rotation about z), the joints of the robot fingers (2 * num_of_arms), and the 6d velocities of end effectors in their local (i.e. end effectors') frames (6 * num_of_arms). to be more specific, the 6d velocity of end effector is a body twist at the end effector origin frame ordered as [v omega].\nthe null space contains 7 * num_of_arms degrees of freedom. an action in the null space (i.e. whose operational space component is a zero vector) provides movements in the null space of end effectors, which means that it only moves the links on the robot arm(s) but keeps the end effector (link left_panda_hand or right_panda_hand) static.\nwe also provide some basic examples in the test() function of maniskill/utils/osc.py. please check it for further understanding.\nget pose of end effector\nif you want to manually design some controllers, then the pose(s) of the end effector(s) might be needed. we provided an example to compute the pose(s) of the end effector(s) in the world frame from the observation dict. specifically, the poses of the end effectors refer to the poses of right_panda_hand / left_panda_hand links in our robot.\nconclusion\nnow that you have familiarized yourself with the maniskill benchmark, you can train and visualize policies on the maniskill environments. you may want to play with our baselines and get started with learning-from-demonstrations algorithms.\nacknowledgements\nwe thank qualcomm for sponsoring the associated challenge, sergey levine and ashvin nair for insightful discussions during the whole development process, yuzhe qin for the suggestions on building robots, jiayuan gu for providing technical support on sapien, and rui chen, songfang han, wei jiang for testing our system.\ncitation\n@inproceedings{mu2021maniskill,\ntitle={maniskill: generalizable manipulation skill benchmark with large-scale demonstrations},\nauthor={mu, tongzhou and ling, zhan and xiang, fanbo and yang, derek cathera and li, xuanlin and tao, stone and huang, zhiao and jia, zhiwei and su, hao},\nbooktitle={thirty-fifth conference on neural information processing systems datasets and benchmarks track (round 2)},\nyear={2021}\n}", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000415, "year": null}, {"Unnamed: 0": 1878, "autor": 858, "date": null, "content": "Marlin2ForPipetBot 3D Printer and Lab Robot CNC Firmware\nAdditional documentation can be found in the repository DerAndere1/Marlin at https://github.com, the Wiki or on the PipetBot-A8 project homepage that is part of the authors homepage. For CNC machines with additional axes (I, J, K, U, V, W) that can be used for indexed machining or to drive pumps or other tools, e.g. lab robots (liquid handling robots, \"pipetting robots\"). Please test this firmware and let us know if it misbehaves in any way. Volunteers are standing by!\nMarlin2ForPipetBot Branch\nNot for production use. Use with caution!\nMarlin2forPipetBot is a branch of the Marlin fork by DerAndere (based on https://github.com/MarlinFirmware/Marlin/commit/d6b332f4c7a79553a41c4a6166f21f802be930c8).\nThis branch is for patches to the latest Marlin2ForPipetBot release version.\nMarlin2ForPipetBot supports up to nine non-extruder axes (LINEAR_AXES) plus extruders (e.g. XYZABCUVW+E or XYZABCW+E or XYZCUVW+E or XYZABC+E or XYZUVW+E).\nDefault axis names are:\nLINEAR_AXES Axis codes\n3 X, Y, Z, E\n4 X, Y, Z, A, E\n5 X, Y, Z, A, B, E\n6 X, Y, Z, A, B, C, E\n7 X, Y, Z, A, B, C, U, E\n8 X, Y, Z, A, B, C, U, V, E\n9 X, Y, Z, A, B, C, U, V, W, E\nExample syntax for movement (G-code G1) with LINEAR_AXES 9:\nG1 [Xx.xxxx] [Yy.yyyy] [Zz.zzzz] [Aa.aaaa] Bb.bbbb] [Cc.cccc] [Uu.uuuu] [Vv.vvvv] [Ww.wwww] [Ee.eeee] [Ff.ffff]\nParameters:\nX, Y, Z: position in the cartesian coordinate system consisting of primary linear axes X, Y and Z. Unit: mm (after G-code G21) or imperial inch (after G-code G20)\nA, B, C: angular position in the pseudo-cartesian coordinate system consisting of rotational axes A, B, and C that are parallel (wrapped around) axes axes X, Y and Z. Unit: degrees\nU, V, W: position in the cartesian coordinate system consisting of secondary linear axes U, V and W that are parallel to axes X, Y and Z. Unit: mm (after G-code G21) or imperial inch (after G-code G20)\nE: distance the E stepper should move. Unit: mm (after G-code G21) or imperial inch (after G-code G20)\nF: Feedrate as defined by LinuxCNC (extension of NIST RS274NGC interpreter - version 3):\nFor motion involving one or more of the X, Y, and Z axes (with or without motion of other axes), the feed rate means length units per minute along the programmed XYZ path, as if the other axes were not moving.\nFor motion of one or more of the secondary linear axes (axis names 'U', 'V', or 'W') with the X, Y , and Z axes not moving (with or without motion of rotational axes), the feed rate means length units per minute along the programmed UVW path (using the usual Euclidean metric in the UVW coordinate system), as if the rotational axes were not moving.\nFor motion of one or more of the rotational axes (axis names 'A', 'B' or 'C') with linear axes not moving, the rate is applied as follows. Let dA, dB, and dC be the angles in degrees through which the A, B, and C axes, respectively, must move. Let D = sqrt((dA)^2 + (dB)^2 + (dC)^2). Conceptually, D is a measure of total angular motion, using the usual Euclidean metric. Let T be the amount of time required to move through D degrees at the current feed rate in degrees per minute. The rotational axes should be moved in coordinated linear motion so that the elapsed time from the start to the end of the motion is T plus any time required for acceleration or deceleration.\nConfiguration\nConfiguration is done by editing the file Marlin/Configuration.h. E.g. change\ndefine LINEAR_AXES 3\nto:\ndefine LINEAR_AXES 4\nImportant options are:\nFOAMCUTTER_XYUV\nDefine FOAMCUTTER_XYUV kinematics for a hot wire cutter with parallel horizontal axes X, U where the hights of the two wire ends are controlled by parallel axes Y, V.\nLINEAR_AXES\nLINEAR_AXES: The number of axes that are not used for extruders (axes that benefit from endstops and homing). LINEAR_AXES > 3 requires definition of [[I, [J, [K...]]]_STEP_PIN, [I, [J, [K...]]]_ENABLE_PIN, [I, [J, [K...]]]_DIR_PIN, [I, [J, [K...]]]_STOP_PIN, USE_[I, [J, [K...]]][MIN || MAX]_PLUG, [I, [J, [K...]]]_ENABLE_ON, DISABLE_[I, [J, [K...]]], [I, [J, [K...]]]_MIN_POS, [I, [J, [K...]]]_MAX_POS, [I, [J, [K...]]]_HOME_DIR, possibly DEFAULT_[I, [J, [K...]]]JERK, and definition of the respective values of DEFAULT_AXIS_STEPS_PER_UNIT, DEFAULT_MAX_FEEDRATE, DEFAULT_MAX_ACCELERATION, HOMING_FEEDRATE_MM_M, AXIS_RELATIVE_MODES, MICROSTEP_MODES, MANUAL_FEEDRATE and possibly also values of HOMING_BUMP_DIVISOR,\nHOMING_BACKOFF_POST_MM, BACKLASH_DISTANCE_MM. For bed-leveling, NOZZLE_TO_PROBE_OFFSETS has to be extended with elemets of value 0 until the number of elements is equal to the value of LINEAR_AXES.\nAllowed values: [2, 3, 4, 5, 6, 7, 8, 9]\nAXIS4_NAME\nAXIS4_NAME, AXIS5_NAME, AXIS6_NAME, AXIS7_NAME``AXIS8_NAME``AXIS9_NAME: Axis codes for additional axes: This defines the axis code that is used in G-code commands to reference a specific axis. Axes with name 'A', 'B' or 'C' are rotational axes for which distances and positions must be specified in degrees. Other axes are linear axes for which distances and positions must be specified in length units (mm in default mode (after G21) or imperial inches in inch mode (after G20))\n'A' for rotational axis parallel to X\n'B' for rotational axis parallel to Y\n'C' for rotational axis parallel to Z\n'U' for secondary linear axis parallel to X\n'V' for secondary linear axis parallel to Y\n'W' for secondary linear axis parallel to Z\nRegardless of the settings, firmware-internal axis names are I (AXIS4), J (AXIS5), K (AXIS6), U (AXIS7), V (AXIS8), W (AXIS9).\nAllowed values: ['A', 'B', 'C', 'U', 'V', 'W']\nSAFE_BED_LEVELING_START_X\nSAFE_BED_LEVELING_START_X, SAFE_BED_LEVELING_START_Y, SAFE_BED_LEVELING_START_Z, SAFE_BED_LEVELING_START_I, SAFE_BED_LEVELING_START_J, SAFE_BED_LEVELING_START_K, SAFE_BED_LEVELING_START_U, SAFE_BED_LEVELING_START_V, SAFE_BED_LEVELING_START_W: Safe bed leveling start coordinates. If enabled, the respective axis is moved to the specified position at the beginning of the bed leveling procedure. Required e.g. with LINEAR_AXES >= 4, if Z probe is not perpendicular to the bed after homing. Values must be chosen so that the bed is oriented horizontally and so that the Z-probe is oriented vertically.\nBuilding Marlin2ForPipetBot\nTo build Marlin2ForPipetBot you'll need PlatformIO. The Marlin team has posted detailed instructions on Building Marlin with PlatformIO. Marlin2ForPipetBot is preconfigured for the Anet-V1.0 board of the PipetBot-A8. When using the default build environment (default_env = melzi_optiboot), upload of the compiled Marlin2ForPipetBot firmware to the board via USB using the optiboot bootloader requires burning of the optiboot bootloader onto the board as described in the SkyNet3D/anet-board documentation.\nThe different branches in the git repository https://github.com/DerAndere1/Marlin reflect different stages of development:\nMarlin2ForPipetBot branch is the stable release branch for tagged releases of Marlin2ForPipetBot firmware. It is optimized and preconfigured for the PipetBot-A8 by default. Currently it is based on Marlin bugfix-2.0.x from 2021-12-05, https://github.com/MarlinFirmware/Marlin/commit/d6b332f4c7a79553a41c4a6166f21f802be930c8 or later. Adds support for 9 axes, including rotational axes (AXIS*_NAME 'A', 'B', or 'C')\nMarlin2ForPipetBot_dev branch is used to develop and test bugfixes for Marlin2ForPipetBot. After successful testing, it will be merged into Marlin2ForPipetBot.\n6axis_PR1 branch was merged into upstream MarlinFirmware/Marlin (pull request https://github.com/MarlinFirmware/Marlin/pull/19112). This branch is now outdated. Use current MarlinFirmware/Marlin instead.\n9axis_PR2 branch is used to develop support for up to 9 non-extruder axes. A pull request targeting https://github.com/MarlinFirmware/Marlin/tree/bugfix-2.0 was opened. This branch needs to be rebased onto https://github.com/MarlinFirmware/Marlin/tree/bugfix-2.0.\nOther branches: Deprecated legacy code. Use current MarlinFirmware/Marlin instead.\nHardware Abstraction Layer (HAL)\nMarlin 2.0 introduces a layer of abstraction so that all the existing high-level code can be built for 32-bit platforms while still retaining full 8-bit AVR compatibility. Retaining AVR compatibility and a single code-base is important to us, because we want to make sure that features and patches get as much testing and attention as possible, and that all platforms always benefit from the latest improvements.\nCurrent HALs\nAVR (8-bit)\nboard processor speed flash sram logic fpu\nArduino AVR ATmega, ATTiny, etc. 16-20MHz 64-256k 2-16k 5V no\nDUE\nboards processor speed flash sram logic fpu\nArduino Due, RAMPS-FD, etc. SAM3X8E ARM-Cortex M3 84MHz 512k 64+32k 3.3V no\nESP32\nboard processor speed flash sram logic fpu\nESP32 Tensilica Xtensa LX6 160-240MHz variants --- --- 3.3V ---\nLPC1768 / LPC1769\nboards processor speed flash sram logic fpu\nRe-ARM LPC1768 ARM-Cortex M3 100MHz 512k 32+16+16k 3.3-5V no\nMKS SBASE LPC1768 ARM-Cortex M3 100MHz 512k 32+16+16k 3.3-5V no\nSelena Compact LPC1768 ARM-Cortex M3 100MHz 512k 32+16+16k 3.3-5V no\nAzteeg X5 GT LPC1769 ARM-Cortex M3 120MHz 512k 32+16+16k 3.3-5V no\nSmoothieboard LPC1769 ARM-Cortex M3 120MHz 512k 64k 3.3-5V no\nSAMD51\nboards processor speed flash sram logic fpu\nAdafruit Grand Central M4 SAMD51P20A ARM-Cortex M4 120MHz 1M 256k 3.3V yes\nSTM32F1\nboards processor speed flash sram logic fpu\nArduino STM32 STM32F1 ARM-Cortex M3 72MHz 256-512k 48-64k 3.3V no\nGeeetech3D GTM32 STM32F1 ARM-Cortex M3 72MHz 256-512k 48-64k 3.3V no\nSTM32F4\nboards processor speed flash sram logic fpu\nSTEVAL-3DP001V1 STM32F401VE Arm-Cortex M4 84MHz 512k 64+32k 3.3-5V yes\nTeensy++ 2.0\nboards processor speed flash sram logic fpu\nTeensy++ 2.0 AT90USB1286 16MHz 128k 8k 5V no\nTeensy 3.1 / 3.2\nboards processor speed flash sram logic fpu\nTeensy 3.2 MK20DX256VLH7 ARM-Cortex M4 72MHz 256k 32k 3.3V-5V yes\nTeensy 3.5 / 3.6\nboards processor speed flash sram logic fpu\nTeensy 3.5 MK64FX512VMD12 ARM-Cortex M4 120MHz 512k 192k 3.3-5V yes\nTeensy 3.6 MK66FX1M0VMD18 ARM-Cortex M4 180MHz 1M 256k 3.3V yes\nTeensy 4.0 / 4.1\nboards processor speed flash sram logic fpu\nTeensy 4.0 IMXRT1062DVL6A ARM-Cortex M7 600MHz 1M 2M 3.3V yes\nTeensy 4.1 IMXRT1062DVJ6A ARM-Cortex M7 600MHz 1M 2M 3.3V yes\nSubmitting Patches\nProposed patches should be submitted as a Pull Request against the (bugfix-2.0.x) branch.\nThis branch is for fixing bugs and integrating any new features for the duration of the Marlin 2.0.x life-cycle.\nFollow the Coding Standards to gain points with the maintainers.\nPlease submit Feature Requests and Bug Reports to the Issue Queue. Support resources are also listed there.\nWhenever you add new features, be sure to add tests to buildroot/tests and then run your tests locally, if possible.\nIt's optional: Running all the tests on Windows might take a long time, and they will run anyway on GitHub.\nIf you're running the tests on Linux (or on WSL with the code on a Linux volume) the speed is much faster.\nYou can use make tests-all-local or make tests-single-local TEST_TARGET=....\nIf you prefer Docker you can use make tests-all-local-docker or make tests-all-local-docker TEST_TARGET=....\nRepRap.org Wiki Page\nCredits\nThe current Marlin dev team consists of:\nScott Lahteine [@thinkyhead] - USA\nRoxanne Neufeld [@Roxy-3D] - USA\nBob Kuhn [@Bob-the-Kuhn] - USA\nChris Pepper [@p3p] - UK\nJo\u00e3o Brazio [@jbrazio] - Portugal\nErik van der Zalm [@ErikZalm] - Netherlands\nMarlin2ForPipetBot is modified by:\nDerAndere [@DerAndere1] - Germany\nGarbriel Beraldo @GabrielBeraldo] - Brasil\nOlivier Briand @hobiseven] - France\nWolverine @MohammadSDGHN - Undisclosed\nbilsef @bilsef - Undisclosed\nFNeo31 @FNeo31 - Undisclosed\nHendrikJan-5D @HendrikJan-5D - Undisclosed\nLicense\nMarlin2ForPipetBot is published under the GPL license because we believe in open development. The GPL comes with both rights and obligations. Whether you use Marlin firmware as the driver for your open or closed-source product, you must keep Marlin open, and you must provide your compatible Marlin source code to end users upon request. The most straightforward way to comply with the Marlin license is to make a fork of Marlin on Github, perform your modifications, and direct users to your modified fork.\nWhile we can't prevent the use of this code in products (3D printers, CNC, etc.) that are closed source or crippled by a patent, we would prefer that you choose another firmware or, better yet, make your own.", "link": "https://github.com/DerAndere1/Marlin", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "bed", "selectorShort": "bed", "MarkedSent": "marlin2forpipetbot 3d printer and lab robot cnc firmware\nadditional documentation can be found in the repository derandere1/marlin at https://github.com, the wiki or on the pipetbot-a8 project homepage that is part of the authors homepage. for cnc machines with additional axes (i, j, k, u, v, w) that can be used for indexed machining or to drive pumps or other tools, e.g. lab robots (liquid handling robots, \"pipetting robots\"). please test this firmware and let us know if it misbehaves in any way. volunteers are standing by!\nmarlin2forpipetbot branch\nnot for production use. use with caution!\nmarlin2forpipetbot is a branch of the marlin fork by derandere (based on https://github.com/marlinfirmware/marlin/commit/d6b332f4c7a79553a41c4a6166f21f802be930c8).\nthis branch is for patches to the latest marlin2forpipetbot release version.\nmarlin2forpipetbot supports up to nine non-extruder axes (linear_axes) plus extruders (e.g. xyzabcuvw+e or xyzabcw+e or xyzcuvw+e or xyzabc+e or xyzuvw+e).\ndefault axis names are:\nlinear_axes axis codes\n3 x, y, z, e\n4 x, y, z, a, e\n5 x, y, z, a, b, e\n6 x, y, z, a, b, c, e\n7 x, y, z, a, b, c, u, e\n8 x, y, z, a, b, c, u, v, e\n9 x, y, z, a, b, c, u, v, w, e\nexample syntax for movement (g-code g1) with linear_axes 9:\ng1 [xx.xxxx] [yy.yyyy] [zz.zzzz] [aa.aaaa] bb.bbbb] [cc.cccc] [uu.uuuu] [vv.vvvv] [ww.wwww] [ee.eeee] [ff.ffff]\nparameters:\nx, y, z: position in the cartesian coordinate system consisting of primary linear axes x, y and z. unit: mm (after g-code g21) or imperial inch (after g-code g20)\na, b, c: angular position in the pseudo-cartesian coordinate system consisting of rotational axes a, b, and c that are parallel (wrapped around) axes axes x, y and z. unit: degrees\nu, v, w: position in the cartesian coordinate system consisting of secondary linear axes u, v and w that are parallel to axes x, y and z. unit: mm (after g-code g21) or imperial inch (after g-code g20)\ne: distance the e stepper should move. unit: mm (after g-code g21) or imperial inch (after g-code g20)\nf: feedrate as defined by linuxcnc (extension of nist rs274ngc interpreter - version 3):\nfor motion involving one or more of the x, y, and z axes (with or without motion of other axes), the feed rate means length units per minute along the programmed xyz path, as if the other axes were not moving.\nfor motion of one or more of the secondary linear axes (axis names 'u', 'v', or 'w') with the x, y , and z axes not moving (with or without motion of rotational axes), the feed rate means length units per minute along the programmed uvw path (using the usual euclidean metric in the uvw coordinate system), as if the rotational axes were not moving.\nfor motion of one or more of the rotational axes (axis names 'a', 'b' or 'c') with linear axes not moving, the rate is applied as follows. let da, db, and dc be the angles in degrees through which the a, b, and c axes, respectively, must move. let d = sqrt((da)^2 + (db)^2 + (dc)^2). conceptually, d is a measure of total angular motion, using the usual euclidean metric. let t be the amount of time required to move through d degrees at the current feed rate in degrees per minute. the rotational axes should be moved in coordinated linear motion so that the elapsed time from the start to the end of the motion is t plus any time required for acceleration or deceleration.\nconfiguration\nconfiguration is done by editing the file marlin/configuration.h. e.g. change\ndefine linear_axes 3\nto:\ndefine linear_axes 4\nimportant options are:\nfoamcutter_xyuv\ndefine foamcutter_xyuv kinematics for a hot wire cutter with parallel horizontal axes x, u where the hights of the two wire ends are controlled by parallel axes y, v.\nlinear_axes\nlinear_axes: the number of axes that are not used for extruders (axes that benefit from endstops and homing). linear_axes > 3 requires definition of [[i, [j, [k...]]]_step_pin, [i, [j, [k...]]]_enable_pin, [i, [j, [k...]]]_dir_pin, [i, [j, [k...]]]_stop_pin, use_[i, [j, [k...]]][min || max]_plug, [i, [j, [k...]]]_enable_on, disable_[i, [j, [k...]]], [i, [j, [k...]]]_min_pos, [i, [j, [k...]]]_max_pos, [i, [j, [k...]]]_home_dir, possibly default_[i, [j, [k...]]]jerk, and definition of the respective values of default_axis_steps_per_unit, default_max_feedrate, default_max_acceleration, homing_feedrate_mm_m, axis_relative_modes, microstep_modes, manual_feedrate and possibly also values of homing_bump_divisor,\nhoming_backoff_post_mm, backlash_distance_mm. for -----> bed !!! -leveling, nozzle_to_probe_offsets has to be extended with elemets of value 0 until the number of elements is equal to the value of linear_axes.\nallowed values: [2, 3, 4, 5, 6, 7, 8, 9]\naxis4_name\naxis4_name, axis5_name, axis6_name, axis7_name``axis8_name``axis9_name: axis codes for additional axes: this defines the axis code that is used in g-code commands to reference a specific axis. axes with name 'a', 'b' or 'c' are rotational axes for which distances and positions must be specified in degrees. other axes are linear axes for which distances and positions must be specified in length units (mm in default mode (after g21) or imperial inches in inch mode (after g20))\n'a' for rotational axis parallel to x\n'b' for rotational axis parallel to y\n'c' for rotational axis parallel to z\n'u' for secondary linear axis parallel to x\n'v' for secondary linear axis parallel to y\n'w' for secondary linear axis parallel to z\nregardless of the settings, firmware-internal axis names are i (axis4), j (axis5), k (axis6), u (axis7), v (axis8), w (axis9).\nallowed values: ['a', 'b', 'c', 'u', 'v', 'w']\nsafe_bed_leveling_start_x\nsafe_bed_leveling_start_x, safe_bed_leveling_start_y, safe_bed_leveling_start_z, safe_bed_leveling_start_i, safe_bed_leveling_start_j, safe_bed_leveling_start_k, safe_bed_leveling_start_u, safe_bed_leveling_start_v, safe_bed_leveling_start_w: safe bed leveling start coordinates. if enabled, the respective axis is moved to the specified position at the beginning of the bed leveling procedure. required e.g. with linear_axes >= 4, if z probe is not perpendicular to the bed after homing. values must be chosen so that the bed is oriented horizontally and so that the z-probe is oriented vertically.\nbuilding marlin2forpipetbot\nto build marlin2forpipetbot you'll need platformio. the marlin team has posted detailed instructions on building marlin with platformio. marlin2forpipetbot is preconfigured for the anet-v1.0 board of the pipetbot-a8. when using the default build environment (default_env = melzi_optiboot), upload of the compiled marlin2forpipetbot firmware to the board via usb using the optiboot bootloader requires burning of the optiboot bootloader onto the board as described in the skynet3d/anet-board documentation.\nthe different branches in the git repository https://github.com/derandere1/marlin reflect different stages of development:\nmarlin2forpipetbot branch is the stable release branch for tagged releases of marlin2forpipetbot firmware. it is optimized and preconfigured for the pipetbot-a8 by default. currently it is based on marlin bugfix-2.0.x from 2021-12-05, https://github.com/marlinfirmware/marlin/commit/d6b332f4c7a79553a41c4a6166f21f802be930c8 or later. adds support for 9 axes, including rotational axes (axis*_name 'a', 'b', or 'c')\nmarlin2forpipetbot_dev branch is used to develop and test bugfixes for marlin2forpipetbot. after successful testing, it will be merged into marlin2forpipetbot.\n6axis_pr1 branch was merged into upstream marlinfirmware/marlin (pull request https://github.com/marlinfirmware/marlin/pull/19112). this branch is now outdated. use current marlinfirmware/marlin instead.\n9axis_pr2 branch is used to develop support for up to 9 non-extruder axes. a pull request targeting https://github.com/marlinfirmware/marlin/tree/bugfix-2.0 was opened. this branch needs to be rebased onto https://github.com/marlinfirmware/marlin/tree/bugfix-2.0.\nother branches: deprecated legacy code. use current marlinfirmware/marlin instead.\nhardware abstraction layer (hal)\nmarlin 2.0 introduces a layer of abstraction so that all the existing high-level code can be built for 32-bit platforms while still retaining full 8-bit avr compatibility. retaining avr compatibility and a single code-base is important to us, because we want to make sure that features and patches get as much testing and attention as possible, and that all platforms always benefit from the latest improvements.\ncurrent hals\navr (8-bit)\nboard processor speed flash sram logic fpu\narduino avr atmega, attiny, etc. 16-20mhz 64-256k 2-16k 5v no\ndue\nboards processor speed flash sram logic fpu\narduino due, ramps-fd, etc. sam3x8e arm-cortex m3 84mhz 512k 64+32k 3.3v no\nesp32\nboard processor speed flash sram logic fpu\nesp32 tensilica xtensa lx6 160-240mhz variants --- --- 3.3v ---\nlpc1768 / lpc1769\nboards processor speed flash sram logic fpu\nre-arm lpc1768 arm-cortex m3 100mhz 512k 32+16+16k 3.3-5v no\nmks sbase lpc1768 arm-cortex m3 100mhz 512k 32+16+16k 3.3-5v no\nselena compact lpc1768 arm-cortex m3 100mhz 512k 32+16+16k 3.3-5v no\nazteeg x5 gt lpc1769 arm-cortex m3 120mhz 512k 32+16+16k 3.3-5v no\nsmoothieboard lpc1769 arm-cortex m3 120mhz 512k 64k 3.3-5v no\nsamd51\nboards processor speed flash sram logic fpu\nadafruit grand central m4 samd51p20a arm-cortex m4 120mhz 1m 256k 3.3v yes\nstm32f1\nboards processor speed flash sram logic fpu\narduino stm32 stm32f1 arm-cortex m3 72mhz 256-512k 48-64k 3.3v no\ngeeetech3d gtm32 stm32f1 arm-cortex m3 72mhz 256-512k 48-64k 3.3v no\nstm32f4\nboards processor speed flash sram logic fpu\nsteval-3dp001v1 stm32f401ve arm-cortex m4 84mhz 512k 64+32k 3.3-5v yes\nteensy++ 2.0\nboards processor speed flash sram logic fpu\nteensy++ 2.0 at90usb1286 16mhz 128k 8k 5v no\nteensy 3.1 / 3.2\nboards processor speed flash sram logic fpu\nteensy 3.2 mk20dx256vlh7 arm-cortex m4 72mhz 256k 32k 3.3v-5v yes\nteensy 3.5 / 3.6\nboards processor speed flash sram logic fpu\nteensy 3.5 mk64fx512vmd12 arm-cortex m4 120mhz 512k 192k 3.3-5v yes\nteensy 3.6 mk66fx1m0vmd18 arm-cortex m4 180mhz 1m 256k 3.3v yes\nteensy 4.0 / 4.1\nboards processor speed flash sram logic fpu\nteensy 4.0 imxrt1062dvl6a arm-cortex m7 600mhz 1m 2m 3.3v yes\nteensy 4.1 imxrt1062dvj6a arm-cortex m7 600mhz 1m 2m 3.3v yes\nsubmitting patches\nproposed patches should be submitted as a pull request against the (bugfix-2.0.x) branch.\nthis branch is for fixing bugs and integrating any new features for the duration of the marlin 2.0.x life-cycle.\nfollow the coding standards to gain points with the maintainers.\nplease submit feature requests and bug reports to the issue queue. support resources are also listed there.\nwhenever you add new features, be sure to add tests to buildroot/tests and then run your tests locally, if possible.\nit's optional: running all the tests on windows might take a long time, and they will run anyway on github.\nif you're running the tests on linux (or on wsl with the code on a linux volume) the speed is much faster.\nyou can use make tests-all-local or make tests-single-local test_target=....\nif you prefer docker you can use make tests-all-local-docker or make tests-all-local-docker test_target=....\nreprap.org wiki page\ncredits\nthe current marlin dev team consists of:\nscott lahteine [@thinkyhead] - usa\nroxanne neufeld [@roxy-3d] - usa\nbob kuhn [@bob-the-kuhn] - usa\nchris pepper [@p3p] - uk\njo\u00e3o brazio [@jbrazio] - portugal\nerik van der zalm [@erikzalm] - netherlands\nmarlin2forpipetbot is modified by:\nderandere [@derandere1] - germany\ngarbriel beraldo @gabrielberaldo] - brasil\nolivier briand @hobiseven] - france\nwolverine @mohammadsdghn - undisclosed\nbilsef @bilsef - undisclosed\nfneo31 @fneo31 - undisclosed\nhendrikjan-5d @hendrikjan-5d - undisclosed\nlicense\nmarlin2forpipetbot is published under the gpl license because we believe in open development. the gpl comes with both rights and obligations. whether you use marlin firmware as the driver for your open or closed-source product, you must keep marlin open, and you must provide your compatible marlin source code to end users upon request. the most straightforward way to comply with the marlin license is to make a fork of marlin on github, perform your modifications, and direct users to your modified fork.\nwhile we can't prevent the use of this code in products (3d printers, cnc, etc.) that are closed source or crippled by a patent, we would prefer that you choose another firmware or, better yet, make your own.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000858, "year": null}], "name": "bedrobotics"}